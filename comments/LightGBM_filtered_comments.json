{
  "v4.6.0": [
    "coding: utf-8",
    "now that the relevant information has been pulled out of params, it's safe to overwrite it",
    "with the content that should be used for training (i.e. with aliases resolved)",
    "if there were not multiple boosting rounds configurations provided in params,",
    "then by definition they cannot have conflicting values... no need to warn",
    "if all the aliases have the same value, no need to warn",
    "if this line is reached, lightgbm should warn",
    "create predictor first",
    "setting early stopping via global params should be possible",
    "reduce cost for prediction training data",
    "process callbacks",
    "construct booster",
    "start training",
    "check evaluation result.",
    "ranking task, split according to groups",
    "run preprocessing on the data set if needed",
    "build up 2 maps, of the form:",
    "",
    "OrderedDict{",
    "(<dataset_name>, <metric_name>): <is_higher_better>",
    "}",
    "",
    "OrderedDict{",
    "(<dataset_name>, <metric_name>): list[<metric_value>]",
    "}",
    "",
    "turn that into a list of tuples of the form:",
    "",
    "[",
    "(<dataset_name>, <metric_name>, mean(<values>), <is_higher_better>, std_dev(<values>))",
    "]",
    "setting early stopping via global params should be possible",
    "setup callbacks",
    "coding: utf-8",
    "scikit-learn is intentionally imported first here,",
    "see https://github.com/microsoft/LightGBM/issues/6509",
    "dummy function to support older version of scikit-learn",
    "validate_data() was added in scikit-learn 1.6, this function roughly imitates it for older versions.",
    "It can be removed when lightgbm's minimum scikit-learn version is at least 1.6.",
    "'force_all_finite' was renamed to 'ensure_all_finite' in scikit-learn 1.6",
    "trap other keyword arguments that only work on scikit-learn >=1.6, like 'reset'",
    "it's safe to import _num_features unconditionally because:",
    "",
    "* it was first added in scikit-learn 0.24.2",
    "* lightgbm cannot be used with scikit-learn versions older than that",
    "* this validate_data() re-implementation will not be called in scikit-learn>=1.6",
    "",
    "_num_features() raises a TypeError on 1-dimensional input. That's a problem",
    "because scikit-learn's 'check_fit1d' estimator check sets that expectation that",
    "estimators must raise a ValueError when a 1-dimensional input is passed to fit().",
    "",
    "So here, lightgbm avoids calling _num_features() on 1-dimensional inputs.",
    "NOTE: check_X_y() calls check_array() internally, so only need to call one or the other of them here",
    "this only needs to be updated at fit() time",
    "raise the same error that scikit-learn's `validate_data()` does on scikit-learn>=1.6",
    "additional scikit-learn imports only for type hints",
    "sklearn.utils.Tags can be imported unconditionally once",
    "lightgbm's minimum scikit-learn version is 1.6 or higher",
    "catching 'ValueError' here because of this:",
    "https://github.com/microsoft/LightGBM/issues/6365#issuecomment-2002330003",
    "",
    "That's potentially risky as dask does some significant import-time processing,",
    "like loading configuration from environment variables and files, and catching",
    "ValueError here might hide issues with that config-loading.",
    "",
    "But in exchange, it's less likely that 'import lightgbm' will fail for",
    "dask-related reasons, which is beneficial for any workloads that are using",
    "lightgbm but not its Dask functionality.",
    "coding: utf-8",
    "f(labels, preds)",
    "f(labels, preds, weights)",
    "f(labels, preds, weights, group)",
    "f(labels, preds)",
    "f(labels, preds, weights)",
    "f(labels, preds, weights, group)",
    "documentation templates for LGBMModel methods are shared between the classes in",
    "this module and those in the ``dask`` module",
    "It's possible, for example, to pass 3 eval sets through `eval_set`,",
    "but only 1 init_score through `eval_init_score`.",
    "",
    "This if-else accounts for that possibility.",
    "scikit-learn 1.6 introduced an __sklearn__tags() method intended to replace _more_tags().",
    "_more_tags() can be removed whenever lightgbm's minimum supported scikit-learn version",
    "is >=1.6.",
    "ref: https://github.com/microsoft/LightGBM/pull/6651",
    "\"check_sample_weight_equivalence\" can be removed when lightgbm's",
    "minimum supported scikit-learn version is at least 1.6",
    "ref: https://github.com/scikit-learn/scikit-learn/pull/30137",
    "_LGBMModelBase.__sklearn_tags__() cannot be called unconditionally,",
    "because that method isn't defined for scikit-learn<1.6",
    "take whatever tags are provided by BaseEstimator, then modify",
    "them with LightGBM-specific values",
    "Based on: https://github.com/dmlc/xgboost/blob/bd92b1c9c0db3e75ec3dfa513e1435d518bb535d/python-package/xgboost/sklearn.py#L941",
    "which was based on: https://stackoverflow.com/questions/59248211",
    "",
    "`get_params()` flows like this:",
    "",
    "0. Get parameters in subclass (self.__class__) first, by using inspect.",
    "1. Get parameters in all parent classes (especially `LGBMModel`).",
    "2. Get whatever was passed via `**kwargs`.",
    "3. Merge them.",
    "",
    "This needs to accommodate being called recursively in the following",
    "inheritance graphs (and similar for classification and ranking):",
    "",
    "DaskLGBMRegressor -> LGBMRegressor     -> LGBMModel -> BaseEstimator",
    "(custom subclass) -> LGBMRegressor     -> LGBMModel -> BaseEstimator",
    "LGBMRegressor     -> LGBMModel -> BaseEstimator",
    "(custom subclass) -> LGBMModel -> BaseEstimator",
    "LGBMModel -> BaseEstimator",
    "",
    "If the immediate parent defines get_params(), use that.",
    "Otherwise, skip it and assume the next class will have it.",
    "This is here primarily for cases where the first class in MRO is a scikit-learn mixin.",
    "register default metric for consistency with callable eval_metric case",
    "try to deduce from class instance",
    "overwrite default metric by explicitly set metric",
    "use joblib conventions for negative n_jobs, just like scikit-learn",
    "at predict time, this is handled later due to the order of parameter updates",
    "Do not modify original args in fit function",
    "Refer to https://github.com/microsoft/LightGBM/pull/2619",
    "Separate built-in from callable evaluation metrics",
    "concatenate metric from params (or default if not provided in params) and eval_metric",
    "allow any input type (this validation is done further down, in lgb.Dataset())",
    "do not raise an error if Inf of NaN values are found (LightGBM handles these internally)",
    "raise an error on 0-row and 1-row inputs",
    "for other data types, setting n_features_in_ is handled by _LGBMValidateData() in the branch above",
    "reduce cost for prediction training data",
    "This populates the property self.n_features_, the number of features in the fitted model,",
    "and so should only be set after fitting.",
    "",
    "The related property self._n_features_in, which populates self.n_features_in_,",
    "is set BEFORE fitting.",
    "free dataset",
    "'y' being omitted = run scikit-learn's check_array() instead of check_X_y()",
    "",
    "Prevent scikit-learn from deleting or modifying attributes like 'feature_names_in_' and 'n_features_in_'.",
    "These shouldn't be changed at predict() time.",
    "allow any input type (this validation is done further down, in lgb.Dataset())",
    "do not raise an error if Inf of NaN values are found (LightGBM handles these internally)",
    "raise an error on 0-row inputs",
    "retrieve original params that possibly can be used in both training and prediction",
    "and then overwrite them (considering aliases) with params that were passed directly in prediction",
    "number of threads can have values with special meaning which is only applied",
    "in the scikit-learn interface, these should not reach the c++ side as-is",
    "NOTE: all args from LGBMModel.__init__() are intentionally repeated here for",
    "docs, help(), and tab completion.",
    "handle the case where RegressorMixin possibly provides _more_tags()",
    "override those with LightGBM-specific preferences",
    "NOTE: all args from LGBMModel.__init__() are intentionally repeated here for",
    "docs, help(), and tab completion.",
    "handle the case where ClassifierMixin possibly provides _more_tags()",
    "override those with LightGBM-specific preferences",
    "adjust eval metrics to match whether binary or multiclass",
    "classification is being performed",
    "do not modify args, as it causes errors in model selection tools",
    "NOTE: all args from LGBMModel.__init__() are intentionally repeated here for",
    "docs, help(), and tab completion.",
    "check group data",
    "coding: utf-8",
    "we don't need lib_lightgbm while building docs",
    "coding: utf-8",
    "\"#ddffdd\" is light green, \"#ffdddd\" is light red",
    "coding: utf-8",
    ".basic is intentionally loaded as early as possible, to dlopen() lib_lightgbm.{dll,dylib,so}",
    "and its dependencies as early as possible",
    "coding: utf-8",
    "This import causes lib_lightgbm.{dll,dylib,so} to be loaded.",
    "It's intentionally done here, as early as possible, to avoid issues like",
    "\"libgomp.so.1: cannot allocate memory in static TLS block\" on aarch64 Linux.",
    "",
    "For details, see the \"cannot allocate memory in static TLS block\" entry in docs/FAQ.rst.",
    "typing.TypeGuard was only introduced in Python 3.10",
    "ensure dtype and order, copies if either do not match",
    "flatten array without copying",
    "connect the Python logger to logging in lib_lightgbm",
    "TypeError: obj is not a string or a number",
    "ValueError: invalid literal",
    "Obtain objects to export",
    "Prepare export",
    "Export all objects",
    "DeprecationWarning is not shown by default, so let's create our own with higher level",
    "ref: https://peps.python.org/pep-0565/#additional-use-case-for-futurewarning",
    "lazy evaluation to allow import without dynamic library, e.g., for docs generation",
    "if buffer length is not long enough, re-allocate a buffer",
    "avoid side effects on passed-in parameters",
    "if main_param_name was provided, keep that value and remove all aliases",
    "if main param name was not found, search for an alias",
    "neither of main_param_name, aliases were found",
    "most common case (no nullable dtypes)",
    "1.0 <= pd version < 1.1 and nullable dtypes, least common case",
    "raises error because array is casted to type(pd.NA) and there's no na_value argument",
    "data has nullable dtypes, but we can specify na_value argument and copy will be made",
    "take shallow copy in case we modify categorical columns",
    "whole column modifications don't change the original df",
    "determine feature names",
    "determine categorical features",
    "use cat cols from DataFrame",
    "so that the target dtype considers floats",
    "Get total row number.",
    "Random access by row index. Used for data sampling.",
    "Range data access. Used to read data in batch when constructing Dataset.",
    "Optionally specify batch_size to control range data read size.",
    "Only required if using ``Dataset.subset()``.",
    "__get_num_preds() cannot work with nrow > MAX_INT32, so calculate overall number of predictions piecemeal",
    "avoid memory consumption by arrays concatenation operations",
    "create numpy array from output arrays",
    "break up indptr based on number of rows (note more than one matrix in multiclass case)",
    "for CSC there is extra column added",
    "reformat output into a csr or csc matrix or list of csr or csc matrices",
    "same shape as input csr or csc matrix except extra column for expected value",
    "note: make sure we copy data as it will be deallocated next",
    "free the temporary native indptr, indices, and data",
    "__get_num_preds() cannot work with nrow > MAX_INT32, so calculate overall number of predictions piecemeal",
    "avoid memory consumption by arrays concatenation operations",
    "Check that the input is valid: we only handle numbers (for now)",
    "Prepare prediction output array",
    "Export Arrow table to C and run prediction",
    "c type: double**",
    "each double* element points to start of each column of sample data.",
    "c type int**",
    "each int* points to start of indices for each column",
    "no min_data, nthreads and verbose in this function",
    "check data has header or not",
    "need to regroup init_score",
    "process for args",
    "get categorical features",
    "If the params[cat_alias] is equal to categorical_indices, do not report the warning.",
    "process for reference dataset",
    "start construct data",
    "set feature names",
    "Select sampled rows, transpose to column order.",
    "create validation dataset from ref_dataset",
    "Check that the input is valid: we only handle numbers (for now)",
    "Export Arrow table to C",
    "create valid",
    "construct subset",
    "create train",
    "could be updated if data is not freed",
    "set to None",
    "If the data is a arrow data, we can just pass it to C",
    "If a table is being passed, we concatenate the columns. This is only valid for",
    "'init_score'.",
    "we're done if self and reference share a common upstream reference",
    "Check if the weight contains values other than one",
    "Set field",
    "original values can be modified at cpp side",
    "if buffer length is not long enough, reallocate buffers",
    "group data from LightGBM is boundaries data, need to convert to group size",
    "Training task",
    "if \"machines\" is given, assume user wants to do distributed learning, and set up network",
    "construct booster object",
    "copy the parameters from train_set",
    "save reference to data",
    "buffer for inner predict",
    "Prediction task",
    "if buffer length is not long enough, re-allocate a buffer",
    "if a single node tree it won't have `leaf_index` so return 0",
    "Create the node record, and populate universal data members",
    "Update values to reflect node type (leaf or split)",
    "traverse the next level of the tree",
    "In tree format, \"subtree_list\" is a list of node records (dicts),",
    "and we add node to the list.",
    "need reset training data",
    "need to push new valid data",
    "ensure that existing Booster is freed before replacing it",
    "with a new one createdfrom file",
    "if buffer length is not long enough, re-allocate a buffer",
    "if buffer length is not long enough, reallocate a buffer",
    "Copy models",
    "Get name of features",
    "if buffer length is not long enough, reallocate buffers",
    "avoid to predict many time in one iteration",
    "Get num of inner evals",
    "Get name of eval metrics",
    "if buffer length is not long enough, reallocate buffers",
    "coding: utf-8",
    "Callback environment used by callbacks",
    "this import is here to avoid a circular import",
    "tuples from cv() sometimes have a 5th item, with standard deviation of",
    "the evaluation metric (taken over all cross-validation folds)",
    "for cv(), 'metric_value' is actually a mean of metric values over all CV folds",
    "train()",
    "cv()",
    "CVBooster holds a list of Booster objects, each needs to be updated",
    "for lgb.cv() with eval_train_metric=True, evaluation is also done on the training set",
    "and those metrics are considered for early stopping",
    "for lgb.train(), it's possible to pass the training data via valid_sets with any eval_name",
    "get details of the first dataset",
    "validation sets are guaranteed to not be identical to the training data in cv()",
    "self.best_score_list is initialized to an empty list",
    "coding: utf-8",
    "Acquire port in worker",
    "schedule futures to retrieve each element of the tuple",
    "retrieve ports",
    "Concatenate many parts into one",
    "construct local eval_set data.",
    "store indices of eval_set components that were not contained within local parts.",
    "consolidate parts of each individual eval component.",
    "require that eval_name exists in evaluated result data in case dropped due to padding.",
    "in distributed training the 'training' eval_set is not detected, will have name 'valid_<index>'.",
    "filter padding from eval parts then _concat each eval_set component.",
    "reconstruct eval_set fit args/kwargs depending on which components of eval_set are on worker.",
    "ensure that expected keys for evals_result_ and best_score_ exist regardless of padding.",
    "capture whether local_listen_port or its aliases were provided",
    "capture whether machines or its aliases were provided",
    "Some passed-in parameters can be removed:",
    "* 'num_machines': set automatically from Dask worker list",
    "* 'num_threads': overridden to match nthreads on each Dask process",
    "Split arrays/dataframes into parts. Arrange parts into dicts to enforce co-locality",
    "evals_set will to be re-constructed into smaller lists of (X, y) tuples, where",
    "X and y are each delayed sub-lists of original eval dask Collections.",
    "find maximum number of parts in an individual eval set so that we can",
    "pad eval sets when they come in different sizes.",
    "when individual eval set is equivalent to training data, skip recomputing parts.",
    "add None-padding for individual eval_set member if it is smaller than the largest member.",
    "first time a chunk of this eval set is added to this part.",
    "append additional chunks of this eval set to this part.",
    "ensure that all evaluation parts map uniquely to one part.",
    "assign sub-eval_set components to worker parts.",
    "Start computation in the background",
    "trigger error locally",
    "Find locations of all parts and map them to particular Dask workers",
    "Check that all workers were provided some of eval_set. Otherwise warn user that validation",
    "data artifacts may not be populated depending on worker returning final estimator.",
    "assign general validation set settings to fit kwargs.",
    "resolve aliases for network parameters and pop the result off params.",
    "these values are added back in calls to `_train_part()`",
    "figure out network params",
    "Tell each worker to train on the parts that it has locally",
    "",
    "This code treats ``_train_part()`` calls as not \"pure\" because:",
    "1. there is randomness in the training process unless parameters ``seed``",
    "and ``deterministic`` are set",
    "2. even with those parameters set, the output of one ``_train_part()`` call",
    "relies on global state (it and all the other LightGBM training processes",
    "coordinate with each other)",
    "if network parameters were changed during training, remove them from the",
    "returned model so that they're generated dynamically on every run based",
    "on the Dask cluster you're connected to and which workers have pieces of",
    "the training data",
    "dask.DataFrame.map_partitions() expects each call to return a pandas DataFrame or Series",
    "for multi-class classification with sparse matrices, pred_contrib predictions",
    "are returned as a list of sparse matrices (one per class)",
    "pred_contrib output will have one column per feature,",
    "plus one more for the base value",
    "need to tell Dask the expected type and shape of individual preds",
    "by default, dask.array.concatenate() concatenates sparse arrays into a COO matrix",
    "the code below is used instead to ensure that the sparse type is preserved during concatenation",
    "At this point, `out` is a list of lists of delayeds (each of which points to a matrix).",
    "Concatenate them to return a list of Dask Arrays.",
    "DaskLGBMClassifier does not support group, eval_group.",
    "DaskLGBMClassifier support for callbacks and init_model is not tested",
    "DaskLGBMRegressor does not support group, eval_class_weight, eval_group.",
    "DaskLGBMRegressor support for callbacks and init_model is not tested",
    "DaskLGBMRanker does not support eval_class_weight or early stopping",
    "DaskLGBMRanker support for callbacks and init_model is not tested",
    "coding: utf-8",
    "load or create your dataset",
    "generate feature names",
    "create dataset for lightgbm",
    "if you want to re-use data, remember to set free_raw_data=False",
    "specify your configurations as a dict",
    "feature_name and categorical_feature",
    "check feature name",
    "save model to file",
    "dump model to JSON (and save to file)",
    "feature names",
    "feature importances",
    "load model to predict",
    "can only predict with the best iteration (or the saving iteration)",
    "eval with loaded model",
    "dump model with pickle",
    "load model with pickle to predict",
    "can predict with any iteration when loaded in pickle way",
    "eval with loaded model",
    "continue training",
    "init_model accepts:",
    "1. model file name",
    "2. Booster()",
    "decay learning rates",
    "reset_parameter callback accepts:",
    "1. list with length = num_boost_round",
    "2. function(curr_iter)",
    "change other parameters during training",
    "self-defined objective function",
    "f(preds: array, train_data: Dataset) -> grad: array, hess: array",
    "log likelihood loss",
    "self-defined eval metric",
    "f(preds: array, train_data: Dataset) -> name: str, eval_result: float, is_higher_better: bool",
    "binary error",
    "NOTE: when you do customized loss function, the default prediction value is margin",
    "This may make built-in evaluation metric calculate wrong results",
    "For example, we are doing log likelihood loss, the prediction is score before logistic transformation",
    "Keep this in mind when you use the customization",
    "Pass custom objective function through params",
    "another self-defined eval metric",
    "f(preds: array, train_data: Dataset) -> name: str, eval_result: float, is_higher_better: bool",
    "accuracy",
    "NOTE: when you do customized loss function, the default prediction value is margin",
    "This may make built-in evaluation metric calculate wrong results",
    "For example, we are doing log likelihood loss, the prediction is score before logistic transformation",
    "Keep this in mind when you use the customization",
    "Pass custom objective function through params",
    "callback",
    "coding: utf-8",
    "load or create your dataset",
    "train",
    "predict",
    "eval",
    "feature importances",
    "self-defined eval metric",
    "f(y_true: array, y_pred: array) -> name: str, eval_result: float, is_higher_better: bool",
    "Root Mean Squared Logarithmic Error (RMSLE)",
    "train",
    "another self-defined eval metric",
    "f(y_true: array, y_pred: array) -> name: str, eval_result: float, is_higher_better: bool",
    "Relative Absolute Error (RAE)",
    "train",
    "predict",
    "eval",
    "other scikit-learn modules",
    "coding: utf-8",
    "load or create your dataset",
    "create dataset for lightgbm",
    "specify your configurations as a dict",
    "train",
    "coding: utf-8",
    "################",
    "Simulate some binary data with a single categorical and",
    "single continuous predictor",
    "################",
    "Set up a couple of utilities for our experiments",
    "################",
    "Observe the behavior of `binary` and `xentropy` objectives",
    "Trying this throws an error on non-binary values of y:",
    "experiment('binary', label_type='probability', DATA)",
    "The speed of `binary` is not drastically different than",
    "`xentropy`. `xentropy` runs faster than `binary` in many cases, although",
    "there are reasons to suspect that `binary` should run faster when the",
    "label is an integer instead of a float",
    "coding: utf-8",
    "load or create your dataset",
    "create dataset for lightgbm",
    "specify your configurations as a dict",
    "train",
    "save model to file",
    "predict",
    "eval",
    "We can also open HDF5 file once and get access to",
    "With binary dataset created, we can use either Python API or cmdline version to train.",
    "",
    "Note: in order to create exactly the same dataset with the one created in simple_example.py, we need",
    "to modify simple_example.py to pass numpy array instead of pandas DataFrame to Dataset constructor.",
    "The reason is that DataFrame column names will be used in Dataset. For a DataFrame with Int64Index",
    "as columns, Dataset will use column names like [\"0\", \"1\", \"2\", ...]. While for numpy array, column names",
    "are using the default one assigned in C++ code (dataset_loader.cpp), like [\"Column_0\", \"Column_1\", ...].",
    "Y has a single column and we read it in single shot. So store it as an 1-d array.",
    "We use random access for data sampling when creating LightGBM Dataset from Sequence.",
    "When accessing any element in a HDF5 chunk, it's read entirely.",
    "To save I/O for sampling, we should keep number of total chunks much larger than sample count.",
    "Here we are just creating a chunk size that matches with batch_size.",
    "",
    "Also note that the data is stored in row major order to avoid extra copy when passing to",
    "lightgbm Dataset.",
    "Save to 2 HDF5 files for demonstration.",
    "We can store multiple datasets inside a single HDF5 file.",
    "Separating X and Y for choosing best chunk size for data loading.",
    "split training data into two partitions",
    "make this array dense because we're splitting across",
    "a sparse boundary to partition the data",
    "the code below uses sklearn.metrics, but this requires pulling all of the",
    "predictions and target values back from workers to the client",
    "",
    "for larger datasets, consider the metrics from dask-ml instead",
    "https://ml.dask.org/modules/api.html#dask-ml-metrics-metrics",
    "!/usr/bin/env python3",
    "-*- coding: utf-8 -*-",
    "",
    "LightGBM documentation build configuration file, created by",
    "sphinx-quickstart on Thu May  4 14:30:58 2017.",
    "",
    "This file is execfile()d with the current directory set to its",
    "containing dir.",
    "",
    "Note that not all possible configuration values are present in this",
    "autogenerated file.",
    "",
    "All configuration values have a default; values that are commented out",
    "serve to show the default.",
    "If extensions (or modules to document with autodoc) are in another directory,",
    "add these directories to sys.path here. If the directory is relative to the",
    "documentation root, use os.path.abspath to make it absolute.",
    "-- General configuration ------------------------------------------------",
    "If your documentation needs a minimal Sphinx version, state it here.",
    "Add any Sphinx extension module names here, as strings. They can be",
    "extensions coming with Sphinx (named 'sphinx.ext.*') or your custom",
    "ones.",
    "mock out modules",
    "hide type hints in API docs",
    "Generate autosummary pages. Output should be set with: `:toctree: pythonapi/`",
    "Only the class' docstring is inserted.",
    "If true, `todo` and `todoList` produce output, else they produce nothing.",
    "The master toctree document.",
    "General information about the project.",
    "The name of an image file (relative to this directory) to place at the top",
    "of the sidebar.",
    "The name of an image file (relative to this directory) to use as a favicon of",
    "the docs.  This file should be a Windows icon file (.ico) being 16x16 or 32x32",
    "pixels large.",
    "The version info for the project you're documenting, acts as replacement for",
    "|version| and |release|, also used in various other places throughout the",
    "built documents.",
    "The short X.Y version.",
    "The full version, including alpha/beta/rc tags.",
    "The language for content autogenerated by Sphinx. Refer to documentation",
    "for a list of supported languages.",
    "",
    "This is also used if you do content translation via gettext catalogs.",
    "Usually you set \"language\" from the command line for these cases.",
    "List of patterns, relative to source directory, that match files and",
    "directories to ignore when looking for source files.",
    "This patterns also effect to html_static_path and html_extra_path",
    "The name of the Pygments (syntax highlighting) style to use.",
    "-- Configuration for C API docs generation ------------------------------",
    "-- Options for HTML output ----------------------------------------------",
    "The theme to use for HTML and HTML Help pages.  See the documentation for",
    "a list of builtin themes.",
    "Theme options are theme-specific and customize the look and feel of a theme",
    "further.  For a list of options available for each theme, see the",
    "documentation.",
    "Add any paths that contain custom static files (such as style sheets) here,",
    "relative to this directory. They are copied after the builtin static files,",
    "so a file named \"default.css\" will overwrite the builtin \"default.css\".",
    "-- Options for HTMLHelp output ------------------------------------------",
    "Output file base name for HTML help builder.",
    "-- Options for LaTeX output ---------------------------------------------",
    "The name of an image file (relative to this directory) to place at the top of",
    "the title page.",
    "intersphinx configuration",
    "Warning! The following code can cause buffer overflows on RTD.",
    "Consider suppressing output completely if RTD project silently fails.",
    "Refer to https://github.com/svenevs/exhale",
    "/blob/fe7644829057af622e467bb529db6c03a830da99/exhale/deploy.py#L99-L111",
    "Warning! The following code can cause buffer overflows on RTD.",
    "Consider suppressing output completely if RTD project silently fails.",
    "Refer to https://github.com/svenevs/exhale",
    "/blob/fe7644829057af622e467bb529db6c03a830da99/exhale/deploy.py#L99-L111",
    "coding: utf-8",
    "This is a basic test for floating number parsing.",
    "Most of the test cases come from:",
    "https://github.com/dmlc/xgboost/blob/master/tests/cpp/common/test_charconv.cc",
    "https://github.com/Alexhuszagh/rust-lexical/blob/master/data/test-parse-unittests/strtod_tests.toml",
    "FLT_MAX",
    "FLT_MIN",
    "DBL_MAX (1 + (1 - 2^-52)) * 2^1023 = (2^53 - 1) * 2^971",
    "2^971 * (2^53 - 1 + 1/2) : the smallest number resolving to inf",
    "Near DBL_MIN",
    "DBL_MIN 2^-1022",
    "The behavior for parsing -nan depends on implementation.",
    "Thus we skip binary check for negative nan.",
    "See comment in test_cases.",
    "construct sample data first (use all data for convenience and since size is small)",
    "Load some test data",
    "Use the smaller \".test\" data because we don't care about the actual data and it's smaller",
    "Add some fake initial_scores and groups so we can test streaming them",
    "Now use the reference dataset schema to make some testable Datasets with N rows each",
    "Load some test data",
    "Use the smaller \".test\" data because we don't care about the actual data and it's smaller",
    "Add some fake initial_scores and groups so we can test streaming them",
    "Now use the reference dataset schema to make some testable Datasets with N rows each",
    "This code is copied and adapted from the official Arrow producer examples:",
    "https://arrow.apache.org/docs/format/CDataInterface.html#exporting-a-struct-float32-utf8-array",
    "Free children",
    "Finalize",
    "Free children",
    "Free buffers",
    "Finalize",
    "NOTE: Arrow arrays have 64-bit alignment but we can safely ignore this in tests",
    "By using `calloc` above, we only need to set 'true' values",
    "Arithmetic",
    "Subscripts",
    "End",
    "Check for values in first chunk",
    "Check for some values in second chunk",
    "Check end",
    "Load some test data",
    "Serialize the reference",
    "Deserialize the reference",
    "Confirm 1 successful API call",
    "Free memory",
    "Load some test data",
    "Run a single row prediction and compare with regular Mat prediction:",
    "Drop the result from the dataset, we only care about checking that prediction results are equal",
    "in both cases",
    "Test LGBM_BoosterPredictForMat in multi-threaded mode",
    "Now let's run with the single row fast prediction API:",
    "Free all:",
    "Test that Data() points to first value written",
    "Constants",
    "Start with some content:",
    "Clear & re-use:",
    "Output should match new content:",
    "Number of trials for each new ChunkedArray configuration. Pass 100 times over the search space:",
    "Each outer loop iteration changes the test by adding +1 chunk. We start with 1 chunk only:",
    "Sweep valid and invalid addresses with a ChunkedArray with `chunks` chunks:",
    "Compute a new trial address & value & if it is a valid address:",
    "Insert item. If at a valid address, 0 is returned, otherwise, -1 is returned:",
    "If at valid address, check that the stored value is correct & remember it for the future:",
    "Check the just-stored value with getitem():",
    "Also store the just-stored value for future tracking:",
    "Final check: ensure even with overrides, all valid insertions store the latest value at that address:",
    "Test in 2 ways that the values are correctly laid out in memory:",
    "Since init_scores are in a column format, but need to be pushed as rows, we have to extract each batch",
    "Use multiple threads to test concurrency",
    "Since init_scores are in a column format, but need to be pushed as rows, we have to extract each batch",
    "Calculate expected boundaries",
    "Extract a set of rows from the column-based format (still maintaining column based format)",
    "coding: utf-8",
    "at initialization, should be -1",
    "updating that value through the C API should work",
    "resetting to any negative number should set it to -1",
    "coding: utf-8",
    "check saved model persistence",
    "we need to check the consistency of model file here, so test for exact equal",
    "check early stopping is working. Make it stop very early, so the scores should be very close to zero",
    "scores likely to be different, but prediction should still be the same",
    "test that shape is checked during prediction",
    "The simple implementation is just a single \"return self.ndarray[idx]\"",
    "The following is for demo and testing purpose.",
    "whole col",
    "half col",
    "Create dataset from numpy array directly.",
    "Create dataset using Sequence.",
    "Test for validation set.",
    "Select some random rows as valid data.",
    "From Dataset constructor, with dataset from numpy array.",
    "From Dataset.create_valid, with dataset from sequence.",
    "test that method works even with free_raw_data=True",
    "test that method works but sets raw data to None in case of immergeable data types",
    "test that method works for different data types",
    "Set extremely harsh penalties, so CEGB will block most splits.",
    "Compare pairs of penalties, to ensure scaling works as intended",
    "Reset booster1's parameters to p2, so the parameter section of the file matches.",
    "unconstructed, get_* methods should return whatever was provided",
    "before construction, get_field() should raise an exception",
    "constructed, get_* methods should return numpy arrays, even when the provided",
    "input was a list of floats or ints",
    "get_field(\"group\") returns a numpy array with boundaries, instead of size",
    "NOTE: \"position\" is converted to int32 on the C++ side",
    "should resolve duplicate aliases, and prefer the main parameter",
    "should choose the highest priority alias and set that value on main param",
    "if only aliases are used",
    "should use the default if main param and aliases are missing",
    "all changes should be made on copies and not modify the original",
    "preserves None found for main param and still removes aliases",
    "correctly chooses value when only an alias is provided",
    "adds None if that's given as the default and param not found",
    "If callable is found in objective",
    "Value in params should be preferred to the default_value passed from keyword arguments",
    "None of objective or its aliases in params, but default_value is callable.",
    "\"bad\" = 1 element too many",
    "copy=False is necessary because starting with pandas 3.0, pd.DataFrame() creates",
    "a copy of the input numpy array by default",
    "ref: https://github.com/pandas-dev/pandas/issues/58913",
    "check that the original data wasn't modified",
    "check that the built data has the codes",
    "if all categories were seen during training we just take the codes",
    "if we only saw 'a' during training we just replace its code",
    "and leave the rest as nan",
    "test using defined feature names",
    "test using default feature names",
    "check for feature indices outside of range",
    "NOTE: this intentionally contains values where num_leaves <, ==, and > (max_depth^2)",
    "NOTE: max_depth < 5 is significant here because the default for num_leaves=31. With max_depth=5,",
    "a full depth-wise tree would have 2^5 = 32 leaves.",
    "makes a copy",
    "row-major dataset",
    "col-major dataset",
    "check datasets are equal",
    "several matrices",
    "one matrix",
    "coding: utf-8",
    "add target, weight, and group to DataFrame so that partitions abide by group boundaries.",
    "set_index ensures partitions are based on group id.",
    "See https://stackoverflow.com/questions/49532824/dask-dataframe-split-partitions-based-on-a-column-or-function.",
    "separate target, weight from features.",
    "encode group identifiers into run-length encoding, the format LightGBMRanker is expecting",
    "so that within each partition, sum(g) = n_samples.",
    "ranking arrays: one chunk per group. Each chunk must include all columns.",
    "make one categorical feature relevant to the target",
    "https://github.com/microsoft/LightGBM/issues/4118",
    "extra predict() parameters should be passed through correctly",
    "pref_leaf values should have the right shape",
    "and values that look like valid tree nodes",
    "be sure LightGBM actually used at least one categorical column,",
    "and that it was correctly treated as a categorical feature",
    "shape depends on whether it is binary or multiclass classification",
    "in the special case of multi-class classification using scipy sparse matrices,",
    "the output of `.predict(..., pred_contrib=True)` is a list of sparse matrices (one per class)",
    "",
    "since that case is so different than all other cases, check the relevant things here",
    "and then return early",
    "raw scores will probably be different, but at least check that all predicted classes are the same",
    "be sure LightGBM actually used at least one categorical column,",
    "and that it was correctly treated as a categorical feature",
    "* shape depends on whether it is binary or multiclass classification",
    "* matrix for binary classification is of the form [feature_contrib, base_value],",
    "for multi-class it's [feat_contrib_class1, base_value_class1, feat_contrib_class2, base_value_class2, etc.]",
    "* contrib outputs for distributed training are different than from local training, so we can just test",
    "that the output has the right shape and base values are in the right position",
    "with a custom objective, prediction result is a raw score instead of predicted class",
    "function should have been preserved",
    "should correctly classify every sample",
    "probability estimates should be similar",
    "Scores should be the same",
    "Predictions should be roughly the same.",
    "pref_leaf values should have the right shape",
    "and values that look like valid tree nodes",
    "extra predict() parameters should be passed through correctly",
    "be sure LightGBM actually used at least one categorical column,",
    "and that it was correctly treated as a categorical feature",
    "contrib outputs for distributed training are different than from local training, so we can just test",
    "that the output has the right shape and base values are in the right position",
    "be sure LightGBM actually used at least one categorical column,",
    "and that it was correctly treated as a categorical feature",
    "Quantiles should be right",
    "be sure LightGBM actually used at least one categorical column,",
    "and that it was correctly treated as a categorical feature",
    "function should have been preserved",
    "Scores should be the same",
    "local and Dask predictions should be the same",
    "predictions should be better than random",
    "rebalance small dask.Array dataset for better performance.",
    "use many trees + leaves to overfit, help ensure that Dask data-parallel strategy matches that of",
    "serial learner. See https://github.com/microsoft/LightGBM/issues/3292#issuecomment-671288210.",
    "distributed ranker should be able to rank decently well and should",
    "have high rank correlation with scores from serial ranker.",
    "extra predict() parameters should be passed through correctly",
    "pref_leaf values should have the right shape",
    "and values that look like valid tree nodes",
    "be sure LightGBM actually used at least one categorical column,",
    "and that it was correctly treated as a categorical feature",
    "rebalance small dask.Array dataset for better performance.",
    "distributed ranker should be able to rank decently well with the least-squares objective",
    "and should have high rank correlation with scores from serial ranker.",
    "function should have been preserved",
    "Use larger trainset to prevent premature stopping due to zero loss, causing num_trees() < n_estimators.",
    "Use small chunk_size to avoid single-worker allocation of eval data partitions.",
    "test eval_class_weight, eval_init_score on binary-classification task.",
    "Note: objective's default `metric` will be evaluated in evals_result_ in addition to all eval_metrics.",
    "create eval_sets by creating new datasets or copying training data.",
    "total number of trees scales up for ova classifier.",
    "check that early stopping was not applied.",
    "checks that evals_result_ and best_score_ contain expected data and eval_set names.",
    "check that each eval_name and metric exists for all eval sets, allowing for the",
    "case when a worker receives a fully-padded eval_set component which is not evaluated.",
    "should be able to use the class without specifying a client",
    "should be able to set client after construction",
    "data on cluster1",
    "create identical data on cluster2",
    "at this point, the result of default_client() is client2 since it was the most recently",
    "created. So setting client to client1 here to test that you can select a non-default client",
    "unfitted model should survive pickling round trip, and pickling",
    "shouldn't have side effects on the model object",
    "client will always be None after unpickling",
    "fitted model should survive pickling round trip, and pickling",
    "shouldn't have side effects on the model object",
    "client will always be None after unpickling",
    "rebalance data to be sure that each worker has a piece of the data",
    "model 1 - no network parameters given",
    "model 2 - machines given",
    "model 3 - local_listen_port given",
    "training should fail because LightGBM will try to use the same",
    "port for multiple worker processes on the same machine",
    "rebalance data to be sure that each worker has a piece of the data",
    "test that \"machines\" is actually respected by creating a socket that uses",
    "one of the ports mentioned in \"machines\"",
    "The above error leaves a worker waiting",
    "an informative error should be raised if \"machines\" has duplicates",
    "should not allow for any varargs",
    "the only varkw should be **kwargs,",
    "for pass-through to parent classes' __init__()",
    "\"client\" should be the only different, and the final argument",
    "default values for all constructor arguments should be identical",
    "",
    "NOTE: if LGBMClassifier / LGBMRanker / LGBMRegressor ever override",
    "any of LGBMModel's constructor arguments, this will need to be updated",
    "only positional argument should be 'self'",
    "get_params() should be identical, except for \"client\"",
    "check if init score changes predictions",
    "this test is separate because it takes a not-yet-constructed estimator",
    "if init_score was provided, a model of stumps should predict all 0s",
    "if init_score was not provided, prediction for a model of stumps should be",
    "the \"average\" of the labels",
    "coding: utf-8",
    "coding: utf-8",
    "build target, group ID vectors.",
    "build y/target and group-id vectors with user-specified group sizes.",
    "build y/target and group-id vectors according to n_samples, avg_gs, and random_gs.",
    "groups should contain > 1 element for pairwise learning objective.",
    "build feature data, X. Transform first few into informative features.",
    "doing this here, at import time, to ensure it only runs once_per import",
    "instead of once per assertion",
    "coding: utf-8",
    "NOTE: In the AppVeyor CI, importing pyarrow fails due to an old Visual Studio version. Hence,",
    "we conditionally import pyarrow here (and skip tests if it cannot be imported). However, we",
    "don't want these tests to silently be skipped, hence, we only conditionally import when a",
    "specific env var is set.",
    "----------------------------------------------------------------------------------------------- #",
    "UTILITIES                                            #",
    "----------------------------------------------------------------------------------------------- #",
    "Set random nulls",
    "Split data into <=2 random chunks",
    "Turn chunks into array",
    "----------------------------------------------------------------------------------------------- #",
    "UNIT TESTS                                           #",
    "----------------------------------------------------------------------------------------------- #",
    "------------------------------------------- DATASET ------------------------------------------- #",
    "-------------------------------------------- FIELDS ------------------------------------------- #",
    "Check for equality",
    "-------------------------------------------- LABELS ------------------------------------------- #",
    "------------------------------------------- WEIGHTS ------------------------------------------- #",
    "-------------------------------------------- GROUPS ------------------------------------------- #",
    "----------------------------------------- INIT SCORES ----------------------------------------- #",
    "------------------------------------------ PREDICTION ----------------------------------------- #",
    "coding: utf-8",
    "check that really dummy objective was used and estimator didn't learn anything",
    "prediction result is actually not transformed (is raw) due to custom objective",
    "original estimator is unaffected",
    "new estimator is unfitted, but has the same parameters",
    "should not allow for any varargs",
    "the only varkw should be **kwargs,",
    "default values for all constructor arguments should be identical",
    "",
    "NOTE: if LGBMClassifier / LGBMRanker / LGBMRegressor ever override",
    "any of LGBMModel's constructor arguments, this will need to be updated",
    "only positional argument should be 'self'",
    "get_params() should be identical",
    "Overrides, used to test that passing through **kwargs works as expected.",
    "",
    "why these?",
    "",
    "- 'n_estimators' directly matches a keyword arg for the scikit-learn estimators",
    "- 'eta' is a parameter alias for 'learning_rate'",
    "lightgbm-official classes",
    "custom sub-classes",
    "param values to make training deterministic and",
    "just train a small, cheap model",
    "scikit-learn estimators should remember every parameter passed",
    "via keyword arguments in the estimator constructor, but then",
    "only pass the correct value down to LightGBM's C++ side",
    "scikit-learn get_params()",
    "lgb.Booster's 'params' attribute",
    "Config in the 'LightGBM::Booster' on the C++ side",
    "Test if random_state is properly stored",
    "Test if two random states produce identical models",
    "Test if subsequent fits sample from random_state object and produce different models",
    "Test that the largest element is NOT the same, the smallest can be the same, i.e. zero",
    "why fixed seed?",
    "sometimes there is no difference how cols are treated (cat or not cat)",
    "With default params",
    "Tests same probabilities",
    "Tests same predictions",
    "Tests same raw scores",
    "Tests same leaf indices",
    "Tests same feature contributions",
    "Tests other parameters for the prediction works",
    "Tests start_iteration",
    "Tests same probabilities, starting from iteration 10",
    "Tests same predictions, starting from iteration 10",
    "Tests same raw scores, starting from iteration 10",
    "Tests same leaf indices, starting from iteration 10",
    "Tests same feature contributions, starting from iteration 10",
    "Tests other parameters for the prediction works, starting from iteration 10",
    "Test multiclass binary classification",
    "test that params passed in predict have higher priority",
    "no custom objective, no custom metric",
    "default metric",
    "non-default metric",
    "no metric",
    "non-default metric in eval_metric",
    "non-default metric with non-default metric in eval_metric",
    "non-default metric with multiple metrics in eval_metric",
    "non-default metric with multiple metrics in eval_metric for LGBMClassifier",
    "default metric for non-default objective",
    "non-default metric for non-default objective",
    "no metric",
    "non-default metric in eval_metric for non-default objective",
    "non-default metric with non-default metric in eval_metric for non-default objective",
    "non-default metric with multiple metrics in eval_metric for non-default objective",
    "custom objective, no custom metric",
    "default regression metric for custom objective",
    "non-default regression metric for custom objective",
    "multiple regression metrics for custom objective",
    "no metric",
    "default regression metric with non-default metric in eval_metric for custom objective",
    "non-default regression metric with metric in eval_metric for custom objective",
    "multiple regression metrics with metric in eval_metric for custom objective",
    "multiple regression metrics with multiple metrics in eval_metric for custom objective",
    "no custom objective, custom metric",
    "default metric with custom metric",
    "non-default metric with custom metric",
    "multiple metrics with custom metric",
    "custom metric (disable default metric)",
    "default metric for non-default objective with custom metric",
    "non-default metric for non-default objective with custom metric",
    "multiple metrics for non-default objective with custom metric",
    "custom metric (disable default metric for non-default objective)",
    "custom objective, custom metric",
    "custom metric for custom objective",
    "non-default regression metric with custom metric for custom objective",
    "multiple regression metrics with custom metric for custom objective",
    "default metric and invalid binary metric is replaced with multiclass alternative",
    "invalid binary metric is replaced with multiclass alternative",
    "default metric for non-default multiclass objective",
    "and invalid binary metric is replaced with multiclass alternative",
    "default metric and invalid multiclass metric is replaced with binary alternative",
    "invalid multiclass metric is replaced with binary alternative for custom objective",
    "the evaluation metric changes to multiclass metric even num classes is 2 for multiclass objective",
    "the evaluation metric changes to multiclass metric even num classes is 2 for ovr objective",
    "Verify that can receive a list of metrics, only callable",
    "Verify that can receive a list of custom and built-in metrics",
    "Verify that works as expected when eval_metric is empty",
    "Verify that can receive a list of metrics, only built-in",
    "Verify that eval_metric is robust to receiving a list with None",
    "feval",
    "single eval_set",
    "two eval_set",
    "no warning: no aliases, all defaults",
    "no warning: no aliases, just n_estimators",
    "no warning: 1 alias + n_estimators (both same value)",
    "no warning: 1 alias + n_estimators (different values... value from params should win)",
    "no warning: 2 aliases (both same value)",
    "no warning: 4 aliases (all same value)",
    "warning: 2 aliases (different values... \"num_iterations\" wins because it's the main param name)",
    "should not be any other logs (except the warning, intercepted by pytest)",
    "warning: 2 aliases (different values... first one in the order from Config::parameter2aliases() wins)",
    "should not be any other logs (except the warning, intercepted by pytest)",
    "input is a numpy array, which doesn't have feature names. LightGBM adds",
    "feature names to the fitted model, which is inconsistent with sklearn's behavior",
    "Starting with scikit-learn 1.6 (https://github.com/scikit-learn/scikit-learn/pull/30149),",
    "the only API for marking estimator tests as expected to fail is to pass a keyword argument",
    "to parametrize_with_checks(). That function didn't accept additional arguments in earlier",
    "versions.",
    "",
    "This block defines a patched version of parametrize_with_checks() so lightgbm's tests",
    "can be compatible with scikit-learn <1.6 and >=1.6.",
    "",
    "This should be removed once minimum supported scikit-learn version is at least 1.6.",
    "the try-except part of this should be removed once lightgbm's",
    "minimum supported scikit-learn version is at least 1.6",
    "only the exact error we expected to be raised should be raised",
    "if no AttributeError was thrown, we must be using scikit-learn>=1.6,",
    "and so the actual effects of __sklearn_tags__() should be tested",
    "'val_minus_two' here is the expected number of threads for n_jobs=-2",
    "Note: according to joblib's formula, a value of n_jobs=-2 means",
    "\"use all but one thread\" (formula: n_cpus + 1 + n_jobs)",
    "try to predict with a different feature",
    "check that disabling the check doesn't raise the error",
    "LightGBM's 'predict_disable_shape_check' mechanism is intentionally not respected by",
    "its scikit-learn estimators, for consistency with scikit-learn's own behavior.",
    "train on the first 3 features",
    "more cols in X than features: error",
    "fewer cols in X than features: error",
    "same number of columns in both: no error",
    "make weights and init_score same types as y, just to avoid",
    "a huge number of combinations and therefore test cases",
    "make weights and init_score same types as y, just to avoid",
    "a huge number of combinations and therefore test cases",
    "coding: utf-8",
    "we're in a leaf now",
    "check that the rest of the elements have black color",
    "check that we got to the expected leaf",
    "coding: utf-8",
    "coding: utf-8",
    "check that default gives same result as k = 1",
    "check against independent calculation for k = 1",
    "check against independent calculation for k = 2",
    "check against independent calculation for k = 10",
    "check cases where predictions are equal",
    "should give same result as binary auc for 2 classes",
    "test the case where all predictions are equal",
    "test that weighted data gives different auc_mu",
    "test that equal data weights give same auc_mu as unweighted data",
    "should give 1 when accuracy = 1",
    "test loading class weights",
    "Simulates position bias for a given ranking dataset.",
    "The output dataset is identical to the input one with the exception for the relevance labels.",
    "The new labels are generated according to an instance of a cascade user model:",
    "for each query, the user is simulated to be traversing the list of documents ranked by a baseline ranker",
    "(in our example it is simply the ordering by some feature correlated with relevance, e.g., 34)",
    "and clicks on that document (new_label=1) with some probability 'pclick' depending on its true relevance;",
    "at each position the user may stop the traversal with some probability pstop. For the non-clicked documents,",
    "new_label=0. Thus the generated new labels are biased towards the baseline ranker.",
    "The positions of the documents in the ranked lists produced by the baseline, are returned.",
    "a mapping of a document's true relevance (defined on a 5-grade scale) into the probability of clicking it",
    "an instantiation of a cascade model where the user stops with probability 0.2 after observing each document",
    "simulate position bias for the train dataset and put the train dataset with biased labels to temp directory",
    "the performance of the unbiased LambdaMART should outperform the plain LambdaMART on the dataset with position bias",
    "add extra row to position file",
    "simulate position bias for the train dataset and put the train dataset with biased labels to temp directory",
    "test setting positions through Dataset constructor with numpy array",
    "the performance of the unbiased LambdaMART should outperform the plain LambdaMART on the dataset with position bias",
    "test setting positions through Dataset constructor with pandas Series",
    "test setting positions through set_position",
    "test get_position works",
    "no early stopping",
    "early stopping occurs",
    "regular early stopping",
    "positive min_delta",
    "test custom eval metrics",
    "shuffle = False, override metric in params",
    "shuffle = True, callbacks",
    "enable display training loss",
    "self defined folds",
    "LambdaRank",
    "... with l2 metric",
    "... with NDCG (default) metric",
    "self defined folds with lambdarank",
    "init_model from an in-memory Booster",
    "init_model from a text file",
    "predictions should be identical",
    "with early stopping",
    "predict by each fold booster",
    "check that each booster predicted using the best iteration",
    "fold averaging",
    "without early stopping",
    "test feature_names with whitespaces",
    "This has non-ascii strings.",
    "check that passing parameters to the constructor raises warning and ignores them",
    "check inference isn't affected by unknown parameter",
    "entries whose values should reflect params passed to lgb.train()",
    "'l1' was passed in with alias 'mae'",
    "NOTE: this was passed in with alias 'sub_row'",
    "entries with default values of params",
    "add device-specific entries",
    "",
    "passed-in force_col_wise / force_row_wise parameters are ignored on CUDA and GPU builds...",
    "https://github.com/microsoft/LightGBM/blob/1d7ee63686272bceffd522284127573b511df6be/src/io/config.cpp#L375-L377",
    "check that model text has all expected param entries",
    "since Booster.model_to_string() is used when pickling, check that parameters all",
    "roundtrip pickling successfully too",
    "why fixed seed?",
    "sometimes there is no difference how cols are treated (cat or not cat)",
    "take subsets and train",
    "generate CSR sparse dataset",
    "convert data to dense and get back same contribs",
    "validate the values are the same",
    "validate using CSC matrix",
    "validate the values are the same",
    "generate CSR sparse dataset",
    "convert data to dense and get back same contribs",
    "validate the values are the same",
    "validate using CSC matrix",
    "validate the values are the same",
    "@pytest.mark.skipif(psutil.virtual_memory().available / 1024 / 1024 / 1024 < 3, reason=\"not enough RAM\")",
    "def test_int32_max_sparse_contribs(rng):",
    "params = {\"objective\": \"binary\"}",
    "train_features = rng.uniform(size=(100, 1000))",
    "train_targets = [0] * 50 + [1] * 50",
    "lgb_train = lgb.Dataset(train_features, train_targets)",
    "gbm = lgb.train(params, lgb_train, num_boost_round=2)",
    "csr_input_shape = (3000000, 1000)",
    "test_features = csr_matrix(csr_input_shape)",
    "for i in range(0, csr_input_shape[0], csr_input_shape[0] // 6):",
    "for j in range(0, 1000, 100):",
    "test_features[i, j] = random.random()",
    "y_pred_csr = gbm.predict(test_features, pred_contrib=True)",
    "# Note there is an extra column added to the output for the expected value",
    "csr_output_shape = (csr_input_shape[0], csr_input_shape[1] + 1)",
    "assert y_pred_csr.shape == csr_output_shape",
    "y_pred_csc = gbm.predict(test_features.tocsc(), pred_contrib=True)",
    "# Note output CSC shape should be same as CSR output shape",
    "assert y_pred_csc.shape == csr_output_shape",
    "test sliced labels",
    "append some columns",
    "append some rows",
    "test sliced 2d matrix",
    "test sliced CSR",
    "trees start at position 1.",
    "split_features are in 4th line.",
    "test if a penalty as high as the depth indeed prohibits all monotone splits",
    "The penalization is so high that the first 2 features should not be used here",
    "Check that a very high penalization is the same as not using the features at all",
    "check refit accepts dataset_params",
    "the following checks that dart and rf with mape can predict outside the 0-1 range",
    "https://github.com/microsoft/LightGBM/issues/1579",
    "no custom objective, no feval",
    "default metric",
    "non-default metric in params",
    "default metric in args",
    "non-default metric in args",
    "metric in args overwrites one in params",
    "metric in args overwrites one in params",
    "multiple metrics in params",
    "multiple metrics in args",
    "remove default metric by 'None' in list",
    "remove default metric by 'None' aliases",
    "custom objective, no feval",
    "no default metric",
    "metric in params",
    "metric in args",
    "metric in args overwrites its' alias in params",
    "multiple metrics in params",
    "multiple metrics in args",
    "no custom objective, feval",
    "default metric with custom one",
    "non-default metric in params with custom one",
    "default metric in args with custom one",
    "default metric in args with 1 custom function returning a list of 2 metrics",
    "non-default metric in args with custom one",
    "metric in args overwrites one in params, custom one is evaluated too",
    "multiple metrics in params with custom one",
    "multiple metrics in args with custom one",
    "custom metric is evaluated despite 'None' is passed",
    "custom objective, feval",
    "no default metric, only custom one",
    "metric in params with custom one",
    "metric in args with custom one",
    "metric in args overwrites one in params, custom one is evaluated too",
    "multiple metrics in params with custom one",
    "multiple metrics in args with custom one",
    "custom metric is evaluated despite 'None' is passed",
    "no custom objective, no feval",
    "default metric",
    "default metric in params",
    "non-default metric in params",
    "multiple metrics in params",
    "remove default metric by 'None' aliases",
    "custom objective, no feval",
    "no default metric",
    "metric in params",
    "multiple metrics in params",
    "no custom objective, feval",
    "default metric with custom one",
    "default metric in params with custom one",
    "default metric in params with custom function returning a list of 2 metrics",
    "non-default metric in params with custom one",
    "multiple metrics in params with custom one",
    "custom metric is evaluated despite 'None' is passed",
    "custom objective, feval",
    "no default metric, only custom one",
    "metric in params with custom one",
    "multiple metrics in params with custom one",
    "custom metric is evaluated despite 'None' is passed",
    "Custom objective replaces multiclass",
    "multiclass default metric",
    "multiclass default metric with custom one",
    "multiclass metric alias with custom one for custom objective",
    "no metric for invalid class_num",
    "custom metric for invalid class_num",
    "multiclass metric alias with custom one with invalid class_num",
    "multiclass default metric without num_class",
    "multiclass metric alias",
    "multiclass metric",
    "non-valid metric for multiclass objective",
    "non-default num_class for default objective",
    "no metric with non-default num_class for custom objective",
    "multiclass metric alias for custom objective",
    "multiclass metric for custom objective",
    "binary metric with non-default num_class for custom objective",
    "Expect three metrics but mean and stdv for each metric",
    "test XGBoost-style return value",
    "test numpy-style return value",
    "test bins string type",
    "test histogram is disabled for categorical features",
    "test for lgb.train",
    "test feval for lgb.train",
    "test with two valid data for lgb.train",
    "test for lgb.cv",
    "test feval for lgb.cv",
    "test that binning works properly for features with only positive or only negative values",
    "decreasing without freeing raw data is allowed",
    "decreasing before lazy init is allowed",
    "increasing is allowed",
    "decreasing with disabled filter is allowed",
    "decreasing with enabled filter is disallowed;",
    "also changes of other params are disallowed",
    "check extra trees increases regularization",
    "check path smoothing increases regularization",
    "test edge case with one leaf",
    "check that constraint containing all features is equivalent to no constraint",
    "check that constraint partitioning the features reduces train accuracy",
    "check that constraints consisting of single features reduce accuracy further",
    "test that interaction constraints work when not all features are used",
    "check that number of threads does not affect result",
    "check that setting linear_tree=True fits better than ordinary trees when data has linear relationship",
    "test again with nans in data",
    "test again with bagging",
    "test with a feature that has only one non-nan value",
    "test with a categorical feature",
    "test refit: same results on same data",
    "test refit with save and load",
    "test refit: different results training on different data",
    "test when num_leaves - 1 < num_features and when num_leaves - 1 > num_features",
    "test that the predict once with all iterations equals summed results with start_iteration and num_iteration",
    "test the case where start_iteration <= 0, and num_iteration is None",
    "test the case where start_iteration > 0, and num_iteration <= 0",
    "test the case where start_iteration > 0, and num_iteration <= 0, with pred_leaf=True",
    "test the case where start_iteration > 0, and num_iteration <= 0, with pred_contrib=True",
    "test for regression",
    "test both with and without early stopping",
    "test for multi-class",
    "test both with and without early stopping",
    "test for binary",
    "test both with and without early stopping",
    "checking prediction from 1 iteration and the whole model, to prevent bugs",
    "of the form \"a model of n stumps predicts n * initial_score\"",
    "if init_score was provided, a model of stumps should predict all 0s",
    "if init_score was not provided, prediction for a model of stumps should be",
    "the \"average\" of the labels",
    "1-round model",
    "2-round model",
    "1-round model",
    "2-round model",
    "1-round model",
    "2-round model",
    "test against sklearn average precision metric",
    "test that average precision is 1 where model predicts perfectly",
    "data as float64",
    "test all features were used",
    "test the score is better than predicting the mean",
    "test all predictions are equal using different input dtypes",
    "introduce some missing values",
    "in recent versions of pandas, type 'bool' is incompatible with nan values in x4",
    "train with regular dtypes",
    "convert to nullable dtypes",
    "test training succeeds",
    "test all features were used",
    "test the score is better than predicting the mean",
    "test equal predictions",
    "test data are taken from bug report",
    "https://github.com/microsoft/LightGBM/issues/4708",
    "modified from https://github.com/microsoft/LightGBM/issues/3679#issuecomment-938652811",
    "and https://github.com/microsoft/LightGBM/pull/5087",
    "test that the ``splits_per_leaf_`` of CEGB is cleaned before training a new tree",
    "which is done in the fix #5164",
    "without the fix:",
    "Check failed: (best_split_info.left_count) > (0)",
    "no warning: no aliases, all defaults",
    "no warning: no aliases, just num_boost_round",
    "no warning: 1 alias + num_boost_round (both same value)",
    "no warning: 1 alias + num_boost_round (different values... value from params should win)",
    "no warning: 2 aliases (both same value)",
    "no warning: 4 aliases (all same value)",
    "warning: 2 aliases (different values... \"num_iterations\" wins because it's the main param name)",
    "should not be any other logs (except the warning, intercepted by pytest)",
    "warning: 2 aliases (different values... first one in the order from Config::parameter2aliases() wins)",
    "should not be any other logs (except the warning, intercepted by pytest)",
    "no warning: no aliases, all defaults",
    "no warning: no aliases, just num_boost_round",
    "no warning: 1 alias + num_boost_round (both same value)",
    "no warning: 1 alias + num_boost_round (different values... value from params should win)",
    "no warning: 2 aliases (both same value)",
    "no warning: 4 aliases (all same value)",
    "warning: 2 aliases (different values... \"num_iterations\" wins because it's the main param name)",
    "should not be any other logs (except the warning, intercepted by pytest)",
    "warning: 2 aliases (different values... first one in the order from Config::parameter2aliases() wins)",
    "should not be any other logs (except the warning, intercepted by pytest)",
    "try to predict with a different feature",
    "check that disabling the check doesn't raise the error",
    "try to refit with a different feature",
    "check that disabling the check doesn't raise the error",
    "coding: utf-8",
    "If compiled appropriately, the same installation will support both GPU and CPU.",
    "Double-precision floats are only supported on x86_64 with PoCL",
    "coding: utf-8",
    "coding: utf-8",
    "coding: utf-8",
    "coding: utf-8",
    "alias table",
    "names",
    "from strings",
    "tails",
    "tails",
    "the following are stored as comma separated strings but are arrays in the wrappers",
    "coding: utf-8",
    "Note: MSVC has issues with Altrep classes, so they are disabled for it.",
    "See: https://github.com/microsoft/LightGBM/pull/6213#issuecomment-2111025768",
    "These are helper functions to allow doing a stack unwind",
    "after an R allocation error, which would trigger a long jump.",
    "convert from one-based to zero-based index",
    "if any feature names were larger than allocated size,",
    "allow for a larger size and try again",
    "convert from boundaries to size",
    "--- start Booster interfaces",
    "if any eval names were larger than allocated size,",
    "allow for a larger size and try again",
    "Note: for some reason, MSVC crashes when an error is thrown here",
    "if the buffer variable is defined as 'std::unique_ptr<std::vector<char>>',",
    "but not if it is defined as '<std::vector<char>'.",
    "if the model string was larger than the initial buffer, call the function again, writing directly to the R object",
    "if the model string was larger than the initial buffer, allocate a bigger buffer and try again",
    "if aliases string was larger than the initial buffer, allocate a bigger buffer and try again",
    "if aliases string was larger than the initial buffer, allocate a bigger buffer and try again",
    ".Call() calls",
    "Single row predictor to abstract away caching logic",
    "Prevent the booster from being modified while we have a predictor relying on it during prediction",
    "If several threads try to predict at the same time using the same SingleRowPredictor",
    "we want them to still provide correct values, so the mutex is necessary due to the shared",
    "resources in the predictor.",
    "However the recommended approach is to instantiate one SingleRowPredictor per thread,",
    "to avoid contention here.",
    "create boosting",
    "initialize the boosting",
    "create objective function",
    "initialize the objective function",
    "create training metric",
    "reset the boosting",
    "create objective function",
    "initialize the objective function",
    "Workaround https://github.com/microsoft/LightGBM/issues/6142 by locking here",
    "This is only a workaround because if predictors are initialized differently it may still behave incorrectly,",
    "and because multiple racing Predictor initializations through LGBM_BoosterPredictForMat suffers from that same issue of Predictor init writing things in the booster.",
    "Once #6142 is fixed (predictor doesn't write in the Booster as should have been the case since 1c35c3b9ede9adab8ccc5fd7b4b2b6af188a79f0), this line can be removed.",
    "calculate the nonzero data and indices size",
    "allocate data and indices arrays",
    "Get the number of trees per iteration (for multiclass scenario we output multiple sparse matrices)",
    "aggregated per row feature contribution results",
    "keep track of the row_vector sizes for parallelization",
    "copy vector results to output for each row",
    "Get the number of trees per iteration (for multiclass scenario we output multiple sparse matrices)",
    "aggregated per row feature contribution results",
    "calculate number of elements per column to construct",
    "the CSC matrix with random access",
    "keep track of column counts",
    "keep track of beginning index for each column",
    "keep track of beginning index for each matrix",
    "Note: we parallelize across matrices instead of rows because of the column_counts[m][col_idx] increment inside the loop",
    "store the row index",
    "update column count",
    "explicitly declare symbols from LightGBM namespace",
    "some help functions used to convert data",
    "Row iterator of on column for CSC matrix",
    "return value at idx, only can access by ascent order",
    "return next non-zero pair, if index < 0, means no more data",
    "start of c_api functions",
    "This API is to keep python binding's behavior the same with C++ implementation.",
    "Sample count, random seed etc. should be provided in parameters.",
    "convert internal thread id to be unique based on external thread id",
    "convert internal thread id to be unique based on external thread id",
    "sample data first",
    "sample data first",
    "sample data first",
    "local buffer to re-use memory",
    "sample data first",
    "no more data",
    "Prepare the Arrow data",
    "Initialize the dataset",
    "If there is no reference dataset, we first sample indices",
    "Then, we obtain sample values by parallelizing across columns",
    "Values need to be copied from the record batches.",
    "The chunks are iterated over in the inner loop as columns can be treated independently.",
    "Finally, we initialize a loader from the sampled values",
    "After sampling and properly initializing all bins, we can add our data to the dataset. Here,",
    "we parallelize across rows.",
    "---- start of booster",
    "Naming: In future versions of LightGBM, public API named around `FastConfig` should be made named around",
    "`SingleRowPredictor`, because it is specific to single row prediction, and doesn't actually hold only config.",
    "For now this is kept as `FastConfig` for backwards compatibility.",
    "At the same time, one should consider removing the old non-fast single row public API that stores its Predictor",
    "in the Booster, because that will enable removing these Predictors from the Booster, and associated initialization",
    "code.",
    "Single row in row-major format:",
    "Apply the configuration",
    "Set up chunked array and iterators for all columns",
    "Build row function",
    "Run prediction",
    "---- start of some help functions",
    "data is array of pointers to individual rows",
    "set number of threads for openmp",
    "read parameters from config file",
    "remove str after \"#\"",
    "de-duplicate params",
    "prediction is needed if using input initial model(continued train)",
    "need to continue training",
    "sync up random seed for data partition",
    "load Training data",
    "load data for distributed training",
    "load data for single machine",
    "need save binary file",
    "create training metric",
    "only when have metrics then need to construct validation data",
    "Add validation data, if it exists",
    "add",
    "need save binary file",
    "add metric for validation data",
    "output used time on each iteration",
    "need init network",
    "create boosting",
    "create objective function",
    "load training data",
    "initialize the objective function",
    "initialize the boosting",
    "add validation data into boosting",
    "convert model to if-else statement code",
    "create predictor",
    "Free memory",
    "create predictor",
    "label_gain = 2^i - 1, may overflow, so we use 31 here",
    "counts for all labels",
    "start from top label, and accumulate DCG",
    "counts for all labels",
    "calculate k Max DCG by one pass",
    "get sorted indices by score",
    "calculate multi dcg by one pass",
    "wait for all client start up",
    "Don't call MPI_Finalize() here: If the destructor was called because only this node had an exception, calling MPI_Finalize() will cause all nodes to hang.",
    "Instead we will handle finalize/abort for MPI in main().",
    "default set to -1",
    "distance at k-th communication, distance[k] = 2^k",
    "set incoming rank at k-th communication",
    "set outgoing rank at k-th communication",
    "default set as -1",
    "construct all recursive halving map for all machines",
    "let 1 << k <= num_machines",
    "distance of each communication",
    "if num_machines = 2^k, don't need to group machines",
    "communication direction, %2 == 0 is positive",
    "neighbor at k-th communication",
    "receive data block at k-th communication",
    "send data block at k-th communication",
    "if num_machines != 2^k, need to group machines",
    "group, two machine in one group, total \"rest\" groups will have 2 machines.",
    "let left machine as group leader",
    "cache block information for groups, group with 2 machines will have double block size",
    "convert from group to node leader",
    "convert from node to group",
    "meet new group",
    "add block len for this group",
    "calculate the group block start",
    "not need to construct",
    "get receive block information",
    "accumulate block len",
    "get send block information",
    "accumulate block len",
    "static member definition",
    "if small package or small count , do it by all gather.(reduce the communication times.)",
    "assign the blocks to every rank.",
    "do reduce scatter",
    "do all gather",
    "assign blocks",
    "need use buffer here, since size of \"output\" is smaller than size after all gather",
    "copy back",
    "assign blocks",
    "start all gather",
    "when num_machines is small and data is large",
    "use output as receive buffer",
    "get current local block size",
    "get out rank",
    "get in rank",
    "get send information",
    "get recv information",
    "send and recv at same time",
    "rotate in-place",
    "use output as receive buffer",
    "get current local block size",
    "get send information",
    "get recv information",
    "send and recv at same time",
    "use output as receive buffer",
    "send and recv at same time",
    "send local data to neighbor first",
    "receive neighbor data first",
    "reduce",
    "get target",
    "get send information",
    "get recv information",
    "send and recv at same time",
    "reduce",
    "send result to neighbor",
    "receive result from neighbor",
    "copy result",
    "start up socket",
    "parse clients from file",
    "get ip list of local machine",
    "get local rank",
    "construct listener",
    "construct communication topo",
    "construct linkers",
    "free listener",
    "set timeout",
    "accept incoming socket",
    "receive rank",
    "add new socket",
    "save ranks that need to connect with",
    "start listener",
    "start connect",
    "let smaller rank connect to larger rank",
    "send local rank",
    "wait for listener",
    "print connected linkers",
    "only need to copy subset",
    "avoid to copy subset many times",
    "avoid out of range",
    "may need to recopy subset",
    "valid the type",
    "parser factory implementation.",
    "customized parser add-on.",
    "save header to parser config in case needed.",
    "save label id to parser config in case needed.",
    "Constructors",
    "Get type tag",
    "Comparisons",
    "This has to be separate, not in Statics, because Json() accesses",
    "statics().null.",
    "advance until next line, or end of input",
    "advance until closing tokens",
    "The usual case: non-escaped characters",
    "Handle escapes",
    "Extract 4-byte escape sequence",
    "Explicitly check length of the substring. The following loop",
    "relies on std::string returning the terminating NUL when",
    "accessing str[length]. Checking here reduces brittleness.",
    "JSON specifies that characters outside the BMP shall be encoded as a",
    "pair of 4-hex-digit \\u escapes encoding their surrogate pair",
    "components. Check whether we're in the middle of such a beast: the",
    "previous codepoint was an escaped lead (high) surrogate, and this is",
    "a trail (low) surrogate.",
    "Reassemble the two surrogate pairs into one astral-plane character,",
    "per the UTF-16 algorithm.",
    "Integer part",
    "Decimal part",
    "Exponent part",
    "Check for any trailing garbage",
    "Documented in json11.hpp",
    "Check for another object",
    "get column names",
    "support to get header from parser config, so could utilize following label name to id mapping logic.",
    "load label idx first",
    "if parser config file exists, feature names may be changed after customized parser applied.",
    "clear here so could use default filled feature names during dataset construction.",
    "may improve by saving real feature names defined in parser in the future.",
    "erase label column name",
    "load ignore columns",
    "load weight idx",
    "load group idx",
    "don't support query id in data file when using distributed training",
    "read data to memory",
    "sample data",
    "construct feature bin mappers & clear sample data",
    "initialize label",
    "extract features",
    "sample data from file",
    "construct feature bin mappers & clear sample data",
    "initialize label",
    "extract features",
    "load data from binary file",
    "checks whether there's a initial score file when loaded from binary data files",
    "the initial score file should with suffix \".bin.init\"",
    "check meta data",
    "need to check training data",
    "read data in memory",
    "initialize label",
    "extract features",
    "Get number of lines of data file",
    "initialize label",
    "extract features",
    "load data from binary file",
    "checks whether there's a initial score file when loaded from binary data files",
    "the initial score file should with suffix \".bin.init\"",
    "not need to check validation data",
    "check meta data",
    "check token",
    "read feature group definitions",
    "read feature size",
    "buffer to read binary file",
    "check token",
    "read size of header",
    "re-allocate space if not enough",
    "read header",
    "get header",
    "read size of meta data",
    "re-allocate space if not enough",
    "read meta data",
    "load meta data",
    "sample local used data if need to partition",
    "if not contain query file, minimal sample unit is one record",
    "if contain query file, minimal sample unit is one query",
    "if is new query",
    "read feature data",
    "read feature size",
    "re-allocate space if not enough",
    "raw data",
    "fill feature_names_ if not header",
    "get forced split",
    "if only one machine, find bin locally",
    "if have multi-machines, need to find bin distributed",
    "different machines will find bin for different features",
    "start and len will store the process feature indices for different machines",
    "machine i will find bins for features in [ start[i], start[i] + len[i] )",
    "free",
    "gather global feature bin mappers",
    "restore features bins from buffer",
    "---- private functions ----",
    "get header",
    "num_groups",
    "real_feature_idx_",
    "feature2group",
    "feature2subfeature",
    "group_bin_boundaries",
    "group_feature_start_",
    "group_feature_cnt_",
    "get feature names",
    "get forced_bin_bounds_",
    "if features are ordered, not need to use hist_buf",
    "read all lines",
    "get query data",
    "if not contain query data, minimal sample unit is one record",
    "if contain query data, minimal sample unit is one query",
    "if is new query",
    "get query data",
    "if not contain query file, minimal sample unit is one record",
    "if contain query file, minimal sample unit is one query",
    "if is new query",
    "parse features",
    "get forced split",
    "check the range of label_idx, weight_idx and group_idx",
    "skip label check if user input parser config file,",
    "because label id is got from raw features while dataset features are consistent with customized parser.",
    "fill feature_names_ if not header",
    "start find bins",
    "if only one machine, find bin locally",
    "start and len will store the process feature indices for different machines",
    "machine i will find bins for features in [ start[i], start[i] + len[i] )",
    "free",
    "gather global feature bin mappers",
    "restore features bins from buffer",
    "if doesn't need to prediction with initial model",
    "parser",
    "set label",
    "free processed line:",
    "shrink_to_fit will be very slow in linux, and seems not free memory, disable for now",
    "text_reader_->Lines()[i].shrink_to_fit();",
    "push data",
    "if is used feature",
    "if need to prediction with initial model",
    "parser",
    "set initial score",
    "set label",
    "free processed line:",
    "shrink_to_fit will be very slow in Linux, and seems not free memory, disable for now",
    "text_reader_->Lines()[i].shrink_to_fit();",
    "push data",
    "if is used feature",
    "metadata_ will manage space of init_score",
    "text data can be free after loaded feature values",
    "parser",
    "set initial score",
    "set label",
    "push data",
    "if is used feature",
    "only need part of data",
    "need full data",
    "metadata_ will manage space of init_score",
    "read size of token",
    "remove duplicates",
    "deep copy function for BinMapper",
    "mean size for one bin",
    "need a new bin",
    "update bin upper bound",
    "last bin upper bound",
    "get number of positive and negative distinct values",
    "include zero bounds and infinity bound",
    "add forced bounds, excluding zeros since we have already added zero bounds",
    "find remaining bounds",
    "find distinct_values first",
    "push zero in the front",
    "use the large value",
    "push zero in the back",
    "convert to int type first",
    "sort by counts in descending order",
    "will ignore the categorical of small counts",
    "Push the dummy bin for NaN",
    "Use MissingType::None to represent this bin contains all categoricals",
    "fix count of NaN bin",
    "check trivial(num_bin_ == 1) feature",
    "check useless bin",
    "When most_freq_bin_ != default_bin_, there are some additional data loading costs.",
    "so use most_freq_bin_ = default_bin_ when there is not so sparse",
    "calculate max bin of all features to select the int type in MultiValDenseBin",
    "for lambdarank, it needs query data for partition data in distributed learning",
    "need convert query_id to boundaries",
    "check weights",
    "check positions",
    "check query boundaries",
    "contain initial score file",
    "check weights",
    "get local weights",
    "check positions",
    "get local positions",
    "check query boundaries",
    "get local query boundaries",
    "contain initial score file",
    "get local initial scores",
    "re-calculate query weight",
    "Clear init scores on empty input",
    "Note that len here is row count, not num_init_score, so we compare against num_data",
    "We need to use source_size here, because len might not equal size (due to a partially loaded dataset)",
    "CUDA is handled after all insertions are complete",
    "CUDA is handled after all insertions are complete",
    "Clear weights on empty input",
    "CUDA is handled after all insertions are complete",
    "Clear query boundaries on empty input",
    "save to nullptr",
    "CUDA is handled after all insertions are complete",
    "default weight file name",
    "default position file name",
    "default init_score file name",
    "use first line to count number class",
    "default query file name",
    "root is in the depth 0",
    "non-leaf",
    "leaf",
    "use this for the missing value conversion",
    "Predict func by Map to ifelse",
    "use this for the missing value conversion",
    "non-leaf",
    "left subtree",
    "right subtree",
    "leaf",
    "non-leaf",
    "left subtree",
    "right subtree",
    "leaf",
    "recursive computation of SHAP values for a decision tree",
    "extend the unique path",
    "leaf node",
    "internal node",
    "see if we have already split on this feature,",
    "if so we undo that split so we can redo it for this node",
    "recursive sparse computation of SHAP values for a decision tree",
    "extend the unique path",
    "leaf node",
    "internal node",
    "see if we have already split on this feature,",
    "if so we undo that split so we can redo it for this node",
    "if \"verbosity\" was found in params, prefer that to any other aliases",
    "if \"verbose\" was found in params and \"verbosity\" was not, use that value",
    "if \"verbosity\" and \"verbose\" were both missing from params, don't modify LightGBM's log level",
    "otherwise, update LightGBM's log level based on the passed-in value",
    "add names of objective function if not providing metric",
    "equal weights for all classes",
    "generate seeds by seed.",
    "sort eval_at",
    "Only push the non-training data",
    "check for conflicts",
    "check if objective, metric, and num_class match",
    "Change pool size to -1 (no limit) when using data parallel to reduce communication costs",
    "max_depth defaults to -1, so max_depth>0 implies \"you explicitly overrode the default\"",
    "",
    "Changing max_depth while leaving num_leaves at its default (31) can lead to 2 undesirable situations:",
    "",
    "* (0 <= max_depth <= 4) it's not possible to produce a tree with 31 leaves",
    "- this block reduces num_leaves to 2^max_depth",
    "* (max_depth > 4) 31 leaves is less than a full depth-wise tree, which might lead to underfitting",
    "- this block warns about that",
    "ref: https://github.com/microsoft/LightGBM/issues/2898#issuecomment-1002860601",
    "Fits in an int, and is more restrictive than the current num_leaves",
    "force col-wise for gpu version",
    "force row-wise for cuda version",
    "linear tree learner must be serial type and run on CPU device",
    "min_data_in_leaf must be at least 2 if path smoothing is active. This is because when the split is calculated",
    "the count is calculated using the proportion of hessian in the leaf which is rounded up to nearest int, so it can",
    "be 1 when there is actually no data in the leaf. In rare cases this can cause a bug because with path smoothing the",
    "calculated split gain can be positive even with zero gradient and hessian.",
    "In distributed mode, local node doesn't have histograms on all features, cannot perform \"intermediate\" monotone constraints.",
    "\"intermediate\" monotone constraints need to recompute splits. If the features are sampled when computing the",
    "split initially, then the sampling needs to be recorded or done once again, which is currently not supported",
    "first round: fill the single val group",
    "always push the last group",
    "put dense feature first",
    "sort by non zero cnt",
    "sort by non zero cnt, bigger first",
    "shuffle groups",
    "Using std::swap for vector<bool> will cause the wrong result.",
    "get num_features",
    "get bin_mappers",
    "for sparse multi value bin, we store the feature bin values with offset added",
    "for dense multi value bin, the feature bin values without offsets are used",
    "copy feature bin mapper data",
    "copy feature bin mapper data",
    "update CUDA storage for column data and metadata",
    "if not pass a filename, just append \".bin\" of original file",
    "Write the basic header information for the dataset",
    "get size of meta data",
    "write meta data",
    "write feature data",
    "get size of feature",
    "write feature",
    "write raw data; use row-major order so we can read row-by-row",
    "Calculate approximate size of output and reserve space",
    "write feature group definitions",
    "Give a little extra just in case, to avoid unnecessary resizes",
    "Write token that marks the data as binary reference, and the version",
    "Write the basic definition of the overall dataset",
    "write feature group definitions",
    "get size of feature",
    "write feature",
    "size of feature names and forced bins",
    "write header",
    "write feature names",
    "write forced bins",
    "explicitly initialize template methods, for cross module call",
    "explicitly initialize template methods, for cross module call",
    "Only one multi-val group, just simply merge",
    "Skip the leading 0 when copying group_bin_boundaries.",
    "regenerate other fields",
    "need to iterate bin iterator",
    "is dense column",
    "is sparse column",
    "initialize the subset cuda column data",
    "if one column has too many bins, use a separate partition for that column",
    "try if adding this column exceed the maximum number per partition",
    "if one column has too many bins, use a separate partition for that column",
    "try if adding this column exceed the maximum number per partition",
    "if LightGBM-specific default has been set, ignore OpenMP-global config",
    "otherwise, default to OpenMP-global config",
    "ensure that if LGBM_SetMaxThreads() was ever called, LightGBM doesn't",
    "use more than that many threads",
    "store the importance first",
    "PredictRaw",
    "PredictRawByMap",
    "Predict",
    "PredictByMap",
    "PredictLeafIndex",
    "PredictLeafIndexByMap",
    "output model type",
    "output number of class",
    "output label index",
    "output max_feature_idx",
    "output objective",
    "output tree models",
    "store the importance first",
    "sort the importance",
    "use serialized string to restore this object",
    "Use first 128 chars to avoid exceed the message buffer.",
    "get number of classes",
    "get index of label",
    "get max_feature_idx first",
    "get average_output",
    "get feature names",
    "get monotone_constraints",
    "set zero",
    "predict all the trees for one iteration",
    "check early stopping",
    "set zero",
    "predict all the trees for one iteration",
    "check early stopping",
    "margin_threshold will be captured by value",
    "copy and sort",
    "margin_threshold will be captured by value",
    "Fix for compiler warnings about reaching end of control",
    "load forced_splits file",
    "init tree learner",
    "push training metrics",
    "get max feature index",
    "get label index",
    "get feature names",
    "get parser config file content",
    "check that forced splits does not use feature indices larger than dataset size",
    "if need bagging, create buffer",
    "for a validation dataset, we need its score and metric",
    "update score",
    "objective function will calculate gradients and hessians",
    "output used time per iteration",
    "boosting from average label; or customized \"average\" if implemented for the current objective",
    "boosting first",
    "use customized objective function",
    "the check below fails unless objective=custom is provided in the parameters on Booster creation",
    "need to copy customized gradients when using GOSS",
    "bagging logic",
    "need to copy gradients for bagging subset.",
    "shrinkage by learning rate",
    "update score",
    "only add default score one-time",
    "updates scores",
    "extend init_scores with zeros",
    "add model",
    "reset score",
    "remove model",
    "print message for metric",
    "pop last early_stopping_round_ models",
    "update training score",
    "we need to predict out-of-bag scores of data for boosting",
    "update validation score",
    "print training metric",
    "print validation metric",
    "set zero",
    "predict all the trees for one iteration",
    "predict all the trees for one iteration",
    "push training metrics",
    "not same training data, need reset score and others",
    "create score tracker",
    "update score",
    "resize gradient vectors to copy the customized gradients for goss or bagging with subset",
    "load forced_splits file",
    "if exists initial score, will start from it",
    "clear host score buffer",
    "Need special case for no smoothing to preserve existing behaviour. If no smoothing, the parent output is calculated",
    "with the larger categorical l2, whereas min_split_gain uses the original l2.",
    "if data not enough, or sum hessian too small",
    "if data not enough",
    "if sum hessian too small",
    "current split gain",
    "gain with split is worse than without split",
    "mark as able to be split",
    "better split point",
    "recover sum of gradient and hessian from the sum of quantized gradient and hessian",
    "Need special case for no smoothing to preserve existing behaviour. If no smoothing, the parent output is calculated",
    "with the larger categorical l2, whereas min_split_gain uses the original l2.",
    "if data not enough, or sum hessian too small",
    "if data not enough",
    "if sum hessian too small",
    "current split gain",
    "gain with split is worse than without split",
    "mark as able to be split",
    "better split point",
    "Get the max size of pool",
    "at least need 2 leaves",
    "push split information for all leaves",
    "initialize splits for leaf",
    "initialize data partition",
    "initialize ordered gradients and hessians",
    "cannot change is_hist_col_wise during training",
    "initialize splits for leaf",
    "initialize data partition",
    "initialize ordered gradients and hessians",
    "Get the max size of pool",
    "at least need 2 leaves",
    "push split information for all leaves",
    "some initial works before training",
    "set the root value by hand, as it is not handled by splits",
    "root leaf",
    "only root leaf can be splitted on first time",
    "some initial works before finding best split",
    "find best threshold for every feature",
    "Get a leaf with max split gain",
    "Get split information for best leaf",
    "cannot split, quit",
    "split tree with best leaf",
    "reset histogram pool",
    "initialize data partition",
    "reset the splits for leaves",
    "Sumup for root",
    "use all data",
    "use bagging, only use part of data",
    "check depth of current leaf",
    "only need to check left leaf, since right leaf is in same level of left leaf",
    "no enough data to continue",
    "only have root",
    "put parent(left) leaf's histograms into larger leaf's histograms",
    "put parent(left) leaf's histograms to larger leaf's histograms",
    "construct smaller leaf",
    "construct larger leaf",
    "find splits",
    "only has root leaf",
    "start at root leaf",
    "Histogram construction require parent features.",
    "then, compute own splits",
    "split info should exist because searching in bfs fashion - should have added from parent",
    "update before tree split",
    "don't need to update this in data-based parallel model",
    "split tree, will return right leaf",
    "store the true split gain in tree model",
    "don't need to update this in data-based parallel model",
    "store the true split gain in tree model",
    "init the leaves that used on next iteration",
    "update leave outputs if needed",
    "bag_mapper[index_mapper[i]]",
    "it is needed to filter the features after the above code.",
    "Otherwise, the `is_splittable` in `FeatureHistogram` will be wrong, and cause some features being accidentally filtered in the later nodes.",
    "for root leaf the \"parent\" output is its own output because we don't apply any smoothing to the root",
    "can't use GetParentOutput because leaf_splits doesn't have weight property set",
    "find splits",
    "identify features containing nans",
    "preallocate the matrix used to calculate linear model coefficients",
    "store only upper triangular half of matrix as an array, in row-major order",
    "this requires (max_num_feat + 1) * (max_num_feat + 2) / 2 entries (including the constant terms of the regression)",
    "we add another 8 to ensure cache lines are not shared among processors",
    "some initial works before training",
    "root leaf",
    "only root leaf can be splitted on first time",
    "some initial works before finding best split",
    "find best threshold for every feature",
    "Get a leaf with max split gain",
    "Get split information for best leaf",
    "cannot split, quit",
    "split tree with best leaf",
    "map data to leaf number",
    "calculate coefficients using the method described in Eq 3 of https://arxiv.org/pdf/1802.05640.pdf",
    "the coefficients vector is given by",
    "- (X_T * H * X + lambda) ^ (-1) * (X_T * g)",
    "where:",
    "X is the matrix where the first column is the feature values and the second is all ones,",
    "H is the diagonal matrix of the hessian,",
    "lambda is the diagonal matrix with diagonal entries equal to the regularisation term linear_lambda",
    "g is the vector of gradients",
    "the subscript _T denotes the transpose",
    "create array of pointers to raw data, and coefficient matrices, for each leaf",
    "clear the coefficient matrices",
    "aggregate results from different threads",
    "copy into eigen matrices and solve",
    "update the tree properties",
    "need to be able to hold smaller and larger best splits in SyncUpGlobalBestSplit",
    "get feature partition",
    "get local used features",
    "get best split at smaller leaf",
    "find local best split for larger leaf",
    "sync global best info",
    "update best split",
    "instantiate template classes, otherwise linker cannot find the code",
    "initialize SerialTreeLearner",
    "Get local rank and global machine size",
    "need to be able to hold smaller and larger best splits in SyncUpGlobalBestSplit",
    "allocate buffer for communication",
    "get block start and block len for reduce scatter",
    "get buffer_write_start_pos",
    "get buffer_read_start_pos",
    "generate feature partition for current tree",
    "get local used feature",
    "get block start and block len for reduce scatter",
    "sync global data sumup info",
    "global sumup reduce",
    "copy back",
    "set global sumup info",
    "init global data count in leaf",
    "reset hist num bits according to global num data",
    "sync global data sumup info",
    "global sumup reduce",
    "copy back",
    "set global sumup info",
    "init global data count in leaf",
    "clear histogram buffer before synchronizing",
    "otherwise histogram contents from the previous iteration will be sent",
    "construct local histograms",
    "copy to buffer",
    "Reduce scatter for histogram",
    "restore global histograms from buffer",
    "only root leaf",
    "construct histgroms for large leaf, we init larger leaf as the parent, so we can just subtract the smaller leaf's histograms",
    "find local best split for larger leaf",
    "sync global best info",
    "set best split",
    "need update global number of data in leaf",
    "reset hist num bits according to global num data",
    "instantiate template classes, otherwise linker cannot find the code",
    "initialize SerialTreeLearner",
    "some additional variables needed for GPU trainer",
    "Initialize GPU buffers and kernels",
    "some functions used for debugging the GPU histogram construction",
    "printf(\"grad %g != %g (%d ULPs)\\n\", GET_GRAD(h1, i), GET_GRAD(h2, i), ulps);",
    "goto err;",
    "we roughly want 256 workgroups per device, and we have num_dense_feature4_ feature tuples.",
    "also guarantee that there are at least 2K examples per workgroup",
    "return 0;",
    "we have already copied ordered gradients, ordered Hessians and indices to GPU",
    "decide the best number of workgroups working on one feature4 tuple",
    "set work group size based on feature size",
    "each 2^exp_workgroups_per_feature workgroups work on a feature4 tuple",
    "we need to refresh the kernel arguments after reallocating",
    "The only argument that needs to be changed later is num_data_",
    "the GPU kernel will process all features in one call, and each",
    "2^exp_workgroups_per_feature (compile time constant) workgroup will",
    "process one feature4 tuple",
    "for the root node, indices are not copied",
    "for constant hessian, hessians are not copied except for the root node",
    "there will be 2^exp_workgroups_per_feature = num_workgroups / num_dense_feature4 sub-histogram per feature4",
    "and we will launch num_feature workgroups for this kernel",
    "will launch threads for all features",
    "the queue should be asynchronous, and we will can WaitAndGetHistograms() before we start processing dense feature groups",
    "copy the results asynchronously. Size depends on if double precision is used",
    "we will wait for this object in WaitAndGetHistograms",
    "when the output is ready, the computation is done",
    "values of this feature has been redistributed to multiple bins; need a reduction here",
    "how many feature-group tuples we have",
    "leave some safe margin for prefetching",
    "256 work-items per workgroup. Each work-item prefetches one tuple for that feature",
    "clear sparse/dense maps",
    "do nothing if no features can be processed on GPU",
    "allocate memory for all features (FIXME: 4 GB barrier on some devices, need to split to multiple buffers)",
    "unpin old buffer if necessary before destructing them",
    "make ordered_gradients and Hessians larger (including extra room for prefetching), and pin them",
    "allocate space for gradients and Hessians on device",
    "we will copy gradients and Hessians in after ordered_gradients_ and ordered_hessians_ are constructed",
    "allocate feature mask, for disabling some feature-groups' histogram calculation",
    "copy indices to the device",
    "histogram bin entry size depends on the precision (single/double)",
    "create output buffer, each feature has a histogram with device_bin_size_ bins,",
    "each work group generates a sub-histogram of dword_features_ features.",
    "only initialize once here, as this will not need to change when ResetTrainingData() is called",
    "create atomic counters for inter-group coordination",
    "The output buffer is allocated to host directly, to overlap compute and data transfer",
    "find the dense feature-groups and group then into Feature4 data structure (several feature-groups packed into 4 bytes)",
    "looking for dword_features_ non-sparse feature-groups",
    "decide if we need to redistribute the bin",
    "multiplier must be a power of 2",
    "device_bin_mults_.push_back(1);",
    "found",
    "for data transfer time",
    "Now generate new data structure feature4, and copy data to the device",
    "preallocate arrays for all threads, and pin them",
    "building Feature4 bundles; each thread handles dword_features_ features",
    "one feature datapoint is 4 bits",
    "this guarantees that the RawGet() function is inlined, rather than using virtual function dispatching",
    "one feature datapoint is one byte",
    "this guarantees that the RawGet() function is inlined, rather than using virtual function dispatching",
    "Dense bin",
    "Dense 4-bit bin",
    "working on the remaining (less than dword_features_) feature groups",
    "fill the leftover features",
    "fill this empty feature with some \"random\" value",
    "fill this empty feature with some \"random\" value",
    "copying the last 1 to (dword_features - 1) feature-groups in the last tuple",
    "deallocate pinned space for feature copying",
    "data transfer time",
    "for other types of failure, build log might not be available; program.build_log() can crash",
    "Something bad happened. Just return \"No log available.\"",
    "build is okay, log may contain warnings",
    "destroy any old kernels",
    "create OpenCL kernels for different number of workgroups per feature",
    "currently we don't use constant memory",
    "compile the GPU kernel depending if double precision is used, constant hessian is used, etc.",
    "kernel with indices in an array",
    "kernel with all features enabled, with eliminated branches",
    "kernel with all data indices (for root node, and assumes that root node always uses all features)",
    "do nothing if no features can be processed on GPU",
    "The only argument that needs to be changed later is num_data_",
    "hessian is passed as a parameter, but it is not available now.",
    "hessian will be set in BeforeTrain()",
    "Get the max bin size, used for selecting best GPU kernel",
    "initialize GPU",
    "determine which kernel to use based on the max number of bins",
    "the +9 skips extra characters \")\", newline, \"#endif\" and newline at the beginning",
    "the +9 skips extra characters \")\", newline, \"#endif\" and newline at the beginning",
    "the +9 skips extra characters \")\", newline, \"#endif\" and newline at the beginning",
    "ignore the feature groups that contain categorical features when producing warnings about max_bin.",
    "these groups may contain larger number of bins due to categorical features, but not due to the setting of max_bin.",
    "setup GPU kernel arguments after we allocating all the buffers",
    "GPU memory has to been reallocated because data may have been changed",
    "setup GPU kernel arguments after we allocating all the buffers",
    "Copy initial full hessians and gradients to GPU.",
    "We start copying as early as possible, instead of at ConstructHistogram().",
    "setup hessian parameters only",
    "hessian is passed as a parameter",
    "use bagging",
    "On GPU, we start copying indices, gradients and Hessians now, instead at ConstructHistogram()",
    "copy used gradients and Hessians to ordered buffer",
    "transfer the indices to GPU",
    "transfer hessian to GPU",
    "setup hessian parameters only",
    "hessian is passed as a parameter",
    "transfer gradients to GPU",
    "only have root",
    "Copy indices, gradients and Hessians as early as possible",
    "only need to initialize for smaller leaf",
    "Get leaf boundary",
    "copy indices to the GPU:",
    "copy ordered Hessians to the GPU:",
    "copy ordered gradients to the GPU:",
    "do nothing if no features can be processed on GPU",
    "copy data indices if it is not null",
    "generate and copy ordered_gradients if gradients is not null",
    "generate and copy ordered_hessians if Hessians is not null",
    "converted indices in is_feature_used to feature-group indices",
    "construct the feature masks for dense feature-groups",
    "if no feature group is used, just return and do not use GPU",
    "if not all feature groups are used, we need to transfer the feature mask to GPU",
    "otherwise, we will use a specialized GPU kernel with all feature groups enabled",
    "All data have been prepared, now run the GPU kernel",
    "construct smaller leaf",
    "ConstructGPUHistogramsAsync will return true if there are available feature groups dispatched to GPU",
    "then construct sparse features on CPU",
    "wait for GPU to finish, only if GPU is actually used",
    "use double precision",
    "use single precision",
    "Compare GPU histogram with CPU histogram, useful for debugging GPU code problem",
    "#define GPU_DEBUG_COMPARE",
    "construct larger leaf",
    "then construct sparse features on CPU",
    "wait for GPU to finish, only if GPU is actually used",
    "use double precision",
    "use single precision",
    "do some sanity check for the GPU algorithm",
    "limit top k",
    "get max bin",
    "calculate buffer size",
    "need to be able to hold smaller and larger best splits in SyncUpGlobalBestSplit",
    "left and right on same time, so need double size",
    "initialize histograms for global",
    "sync global data sumup info",
    "set global sumup info",
    "init global data count in leaf",
    "get local sumup",
    "get local sumup",
    "get mean number on machines",
    "weighted gain",
    "get top k",
    "Copy histogram to buffer, and Get local aggregate features",
    "copy histograms.",
    "copy smaller leaf histograms first",
    "mark local aggregated feature",
    "copy",
    "then copy larger leaf histograms",
    "mark local aggregated feature",
    "copy",
    "use local data to find local best splits",
    "clear histogram buffer before synchronizing",
    "otherwise histogram contents from the previous iteration will be sent",
    "find splits",
    "only has root leaf",
    "local voting",
    "gather",
    "get all top-k from all machines",
    "global voting",
    "copy local histgrams to buffer",
    "Reduce scatter for histogram",
    "find best split from local aggregated histograms",
    "restore from buffer",
    "restore from buffer",
    "find local best",
    "find local best split for larger leaf",
    "sync global best info",
    "copy back",
    "set the global number of data for leaves",
    "init the global sumup info",
    "instantiate template classes, otherwise linker cannot find the code",
    "allocate CUDA memory",
    "leave some space for alignment",
    "input best split info",
    "for leaf information update",
    "gather information for CPU, used for launching kernels",
    "for leaf splits information update",
    "we need restore the order of indices in cuda_data_indices_",
    "allocate more memory for sum reduction in CUDA",
    "only the first element records the final sum",
    "initialize split find task information (a split find task is one pass through the histogram of a feature)",
    "need to double the size of histogram buffer in global memory when using double precision in histogram construction",
    "use only half the size of histogram buffer in global memory when quantized training since each gradient and hessian takes only 2 bytes",
    "use the first gpu by default",
    "set the root value by hand, as it is not handled by splits",
    "std::max(..., 1UL) to avoid error in the case when there are NaN's in the categorical values",
    "use feature interaction constraint or sample features by node"
  ],
  "v4.5.0": [
    "coding: utf-8",
    "raise deprecation warnings if necessary",
    "ref: https://github.com/microsoft/LightGBM/issues/6435",
    "create predictor first",
    "setting early stopping via global params should be possible",
    "reduce cost for prediction training data",
    "process callbacks",
    "construct booster",
    "start training",
    "check evaluation result.",
    "ranking task, split according to groups",
    "run preprocessing on the data set if needed",
    "raise deprecation warnings if necessary",
    "ref: https://github.com/microsoft/LightGBM/issues/6435",
    "setting early stopping via global params should be possible",
    "setup callbacks",
    "coding: utf-8",
    "scikit-learn is intentionally imported first here,",
    "see https://github.com/microsoft/LightGBM/issues/6509",
    "dummy function to support older version of scikit-learn",
    "catching 'ValueError' here because of this:",
    "https://github.com/microsoft/LightGBM/issues/6365#issuecomment-2002330003",
    "",
    "That's potentially risky as dask does some significant import-time processing,",
    "like loading configuration from environment variables and files, and catching",
    "ValueError here might hide issues with that config-loading.",
    "",
    "But in exchange, it's less likely that 'import lightgbm' will fail for",
    "dask-related reasons, which is beneficial for any workloads that are using",
    "lightgbm but not its Dask functionality.",
    "coding: utf-8",
    "f(labels, preds)",
    "f(labels, preds, weights)",
    "f(labels, preds, weights, group)",
    "f(labels, preds)",
    "f(labels, preds, weights)",
    "f(labels, preds, weights, group)",
    "documentation templates for LGBMModel methods are shared between the classes in",
    "this module and those in the ``dask`` module",
    "It's possible, for example, to pass 3 eval sets through `eval_set`,",
    "but only 1 init_score through `eval_init_score`.",
    "",
    "This if-else accounts for that possibility.",
    "register default metric for consistency with callable eval_metric case",
    "try to deduce from class instance",
    "overwrite default metric by explicitly set metric",
    "use joblib conventions for negative n_jobs, just like scikit-learn",
    "at predict time, this is handled later due to the order of parameter updates",
    "Do not modify original args in fit function",
    "Refer to https://github.com/microsoft/LightGBM/pull/2619",
    "Separate built-in from callable evaluation metrics",
    "concatenate metric from params (or default if not provided in params) and eval_metric",
    "copy for consistency",
    "reduce cost for prediction training data",
    "free dataset",
    "retrieve original params that possibly can be used in both training and prediction",
    "and then overwrite them (considering aliases) with params that were passed directly in prediction",
    "number of threads can have values with special meaning which is only applied",
    "in the scikit-learn interface, these should not reach the c++ side as-is",
    "adjust eval metrics to match whether binary or multiclass",
    "classification is being performed",
    "do not modify args, as it causes errors in model selection tools",
    "check group data",
    "coding: utf-8",
    "coding: utf-8",
    "\"#ddffdd\" is light green, \"#ffdddd\" is light red",
    "coding: utf-8",
    "coding: utf-8",
    "typing.TypeGuard was only introduced in Python 3.10",
    "we don't need lib_lightgbm while building docs",
    "TypeError: obj is not a string or a number",
    "ValueError: invalid literal",
    "Obtain objects to export",
    "Prepare export",
    "Export all objects",
    "DeprecationWarning is not shown by default, so let's create our own with higher level",
    "ref: https://peps.python.org/pep-0565/#additional-use-case-for-futurewarning",
    "lazy evaluation to allow import without dynamic library, e.g., for docs generation",
    "if buffer length is not long enough, re-allocate a buffer",
    "avoid side effects on passed-in parameters",
    "if main_param_name was provided, keep that value and remove all aliases",
    "if main param name was not found, search for an alias",
    "neither of main_param_name, aliases were found",
    "most common case (no nullable dtypes)",
    "1.0 <= pd version < 1.1 and nullable dtypes, least common case",
    "raises error because array is casted to type(pd.NA) and there's no na_value argument",
    "data has nullable dtypes, but we can specify na_value argument and copy will be made",
    "take shallow copy in case we modify categorical columns",
    "whole column modifications don't change the original df",
    "determine feature names",
    "determine categorical features",
    "use cat cols from DataFrame",
    "so that the target dtype considers floats",
    "Get total row number.",
    "Random access by row index. Used for data sampling.",
    "Range data access. Used to read data in batch when constructing Dataset.",
    "Optionally specify batch_size to control range data read size.",
    "Only required if using ``Dataset.subset()``.",
    "__get_num_preds() cannot work with nrow > MAX_INT32, so calculate overall number of predictions piecemeal",
    "avoid memory consumption by arrays concatenation operations",
    "create numpy array from output arrays",
    "break up indptr based on number of rows (note more than one matrix in multiclass case)",
    "for CSC there is extra column added",
    "reformat output into a csr or csc matrix or list of csr or csc matrices",
    "same shape as input csr or csc matrix except extra column for expected value",
    "note: make sure we copy data as it will be deallocated next",
    "free the temporary native indptr, indices, and data",
    "__get_num_preds() cannot work with nrow > MAX_INT32, so calculate overall number of predictions piecemeal",
    "avoid memory consumption by arrays concatenation operations",
    "Check that the input is valid: we only handle numbers (for now)",
    "Prepare prediction output array",
    "Export Arrow table to C and run prediction",
    "c type: double**",
    "each double* element points to start of each column of sample data.",
    "c type int**",
    "each int* points to start of indices for each column",
    "no min_data, nthreads and verbose in this function",
    "check data has header or not",
    "need to regroup init_score",
    "process for args",
    "get categorical features",
    "If the params[cat_alias] is equal to categorical_indices, do not report the warning.",
    "process for reference dataset",
    "start construct data",
    "set feature names",
    "Select sampled rows, transpose to column order.",
    "create validation dataset from ref_dataset",
    "Check that the input is valid: we only handle numbers (for now)",
    "Export Arrow table to C",
    "create valid",
    "construct subset",
    "create train",
    "could be updated if data is not freed",
    "set to None",
    "If the data is a arrow data, we can just pass it to C",
    "If a table is being passed, we concatenate the columns. This is only valid for",
    "'init_score'.",
    "we're done if self and reference share a common upstream reference",
    "Check if the weight contains values other than one",
    "Set field",
    "original values can be modified at cpp side",
    "if buffer length is not long enough, reallocate buffers",
    "group data from LightGBM is boundaries data, need to convert to group size",
    "Training task",
    "if \"machines\" is given, assume user wants to do distributed learning, and set up network",
    "construct booster object",
    "copy the parameters from train_set",
    "save reference to data",
    "buffer for inner predict",
    "Prediction task",
    "if buffer length is not long enough, re-allocate a buffer",
    "if a single node tree it won't have `leaf_index` so return 0",
    "Create the node record, and populate universal data members",
    "Update values to reflect node type (leaf or split)",
    "traverse the next level of the tree",
    "In tree format, \"subtree_list\" is a list of node records (dicts),",
    "and we add node to the list.",
    "need reset training data",
    "need to push new valid data",
    "ensure that existing Booster is freed before replacing it",
    "with a new one createdfrom file",
    "if buffer length is not long enough, re-allocate a buffer",
    "if buffer length is not long enough, reallocate a buffer",
    "Copy models",
    "Get name of features",
    "if buffer length is not long enough, reallocate buffers",
    "avoid to predict many time in one iteration",
    "Get num of inner evals",
    "Get name of eval metrics",
    "if buffer length is not long enough, reallocate buffers",
    "coding: utf-8",
    "Callback environment used by callbacks",
    "CVBooster holds a list of Booster objects, each needs to be updated",
    "for lgb.cv() with eval_train_metric=True, evaluation is also done on the training set",
    "and those metrics are considered for early stopping",
    "for lgb.train(), it's possible to pass the training data via valid_sets with any eval_name",
    "validation sets are guaranteed to not be identical to the training data in cv()",
    "split is needed for \"<dataset type> <metric>\" case (e.g. \"train l1\")",
    "self.best_score_list is initialized to an empty list",
    "split is needed for \"<dataset type> <metric>\" case (e.g. \"train l1\")",
    "coding: utf-8",
    "Acquire port in worker",
    "schedule futures to retrieve each element of the tuple",
    "retrieve ports",
    "Concatenate many parts into one",
    "construct local eval_set data.",
    "store indices of eval_set components that were not contained within local parts.",
    "consolidate parts of each individual eval component.",
    "require that eval_name exists in evaluated result data in case dropped due to padding.",
    "in distributed training the 'training' eval_set is not detected, will have name 'valid_<index>'.",
    "filter padding from eval parts then _concat each eval_set component.",
    "reconstruct eval_set fit args/kwargs depending on which components of eval_set are on worker.",
    "ensure that expected keys for evals_result_ and best_score_ exist regardless of padding.",
    "capture whether local_listen_port or its aliases were provided",
    "capture whether machines or its aliases were provided",
    "Some passed-in parameters can be removed:",
    "* 'num_machines': set automatically from Dask worker list",
    "* 'num_threads': overridden to match nthreads on each Dask process",
    "Split arrays/dataframes into parts. Arrange parts into dicts to enforce co-locality",
    "evals_set will to be re-constructed into smaller lists of (X, y) tuples, where",
    "X and y are each delayed sub-lists of original eval dask Collections.",
    "find maximum number of parts in an individual eval set so that we can",
    "pad eval sets when they come in different sizes.",
    "when individual eval set is equivalent to training data, skip recomputing parts.",
    "add None-padding for individual eval_set member if it is smaller than the largest member.",
    "first time a chunk of this eval set is added to this part.",
    "append additional chunks of this eval set to this part.",
    "ensure that all evaluation parts map uniquely to one part.",
    "assign sub-eval_set components to worker parts.",
    "Start computation in the background",
    "trigger error locally",
    "Find locations of all parts and map them to particular Dask workers",
    "Check that all workers were provided some of eval_set. Otherwise warn user that validation",
    "data artifacts may not be populated depending on worker returning final estimator.",
    "assign general validation set settings to fit kwargs.",
    "resolve aliases for network parameters and pop the result off params.",
    "these values are added back in calls to `_train_part()`",
    "figure out network params",
    "Tell each worker to train on the parts that it has locally",
    "",
    "This code treats ``_train_part()`` calls as not \"pure\" because:",
    "1. there is randomness in the training process unless parameters ``seed``",
    "and ``deterministic`` are set",
    "2. even with those parameters set, the output of one ``_train_part()`` call",
    "relies on global state (it and all the other LightGBM training processes",
    "coordinate with each other)",
    "if network parameters were changed during training, remove them from the",
    "returned model so that they're generated dynamically on every run based",
    "on the Dask cluster you're connected to and which workers have pieces of",
    "the training data",
    "dask.DataFrame.map_partitions() expects each call to return a pandas DataFrame or Series",
    "for multi-class classification with sparse matrices, pred_contrib predictions",
    "are returned as a list of sparse matrices (one per class)",
    "pred_contrib output will have one column per feature,",
    "plus one more for the base value",
    "need to tell Dask the expected type and shape of individual preds",
    "by default, dask.array.concatenate() concatenates sparse arrays into a COO matrix",
    "the code below is used instead to ensure that the sparse type is preserved during concatentation",
    "At this point, `out` is a list of lists of delayeds (each of which points to a matrix).",
    "Concatenate them to return a list of Dask Arrays.",
    "DaskLGBMClassifier does not support group, eval_group.",
    "DaskLGBMClassifier support for callbacks and init_model is not tested",
    "DaskLGBMRegressor does not support group, eval_class_weight, eval_group.",
    "DaskLGBMRegressor support for callbacks and init_model is not tested",
    "DaskLGBMRanker does not support eval_class_weight or early stopping",
    "DaskLGBMRanker support for callbacks and init_model is not tested",
    "coding: utf-8",
    "load or create your dataset",
    "generate feature names",
    "create dataset for lightgbm",
    "if you want to re-use data, remember to set free_raw_data=False",
    "specify your configurations as a dict",
    "feature_name and categorical_feature",
    "check feature name",
    "save model to file",
    "dump model to JSON (and save to file)",
    "feature names",
    "feature importances",
    "load model to predict",
    "can only predict with the best iteration (or the saving iteration)",
    "eval with loaded model",
    "dump model with pickle",
    "load model with pickle to predict",
    "can predict with any iteration when loaded in pickle way",
    "eval with loaded model",
    "continue training",
    "init_model accepts:",
    "1. model file name",
    "2. Booster()",
    "decay learning rates",
    "reset_parameter callback accepts:",
    "1. list with length = num_boost_round",
    "2. function(curr_iter)",
    "change other parameters during training",
    "self-defined objective function",
    "f(preds: array, train_data: Dataset) -> grad: array, hess: array",
    "log likelihood loss",
    "self-defined eval metric",
    "f(preds: array, train_data: Dataset) -> name: str, eval_result: float, is_higher_better: bool",
    "binary error",
    "NOTE: when you do customized loss function, the default prediction value is margin",
    "This may make built-in evaluation metric calculate wrong results",
    "For example, we are doing log likelihood loss, the prediction is score before logistic transformation",
    "Keep this in mind when you use the customization",
    "Pass custom objective function through params",
    "another self-defined eval metric",
    "f(preds: array, train_data: Dataset) -> name: str, eval_result: float, is_higher_better: bool",
    "accuracy",
    "NOTE: when you do customized loss function, the default prediction value is margin",
    "This may make built-in evaluation metric calculate wrong results",
    "For example, we are doing log likelihood loss, the prediction is score before logistic transformation",
    "Keep this in mind when you use the customization",
    "Pass custom objective function through params",
    "callback",
    "coding: utf-8",
    "load or create your dataset",
    "train",
    "predict",
    "eval",
    "feature importances",
    "self-defined eval metric",
    "f(y_true: array, y_pred: array) -> name: str, eval_result: float, is_higher_better: bool",
    "Root Mean Squared Logarithmic Error (RMSLE)",
    "train",
    "another self-defined eval metric",
    "f(y_true: array, y_pred: array) -> name: str, eval_result: float, is_higher_better: bool",
    "Relative Absolute Error (RAE)",
    "train",
    "predict",
    "eval",
    "other scikit-learn modules",
    "coding: utf-8",
    "load or create your dataset",
    "create dataset for lightgbm",
    "specify your configurations as a dict",
    "train",
    "coding: utf-8",
    "################",
    "Simulate some binary data with a single categorical and",
    "single continuous predictor",
    "################",
    "Set up a couple of utilities for our experiments",
    "################",
    "Observe the behavior of `binary` and `xentropy` objectives",
    "Trying this throws an error on non-binary values of y:",
    "experiment('binary', label_type='probability', DATA)",
    "The speed of `binary` is not drastically different than",
    "`xentropy`. `xentropy` runs faster than `binary` in many cases, although",
    "there are reasons to suspect that `binary` should run faster when the",
    "label is an integer instead of a float",
    "coding: utf-8",
    "load or create your dataset",
    "create dataset for lightgbm",
    "specify your configurations as a dict",
    "train",
    "save model to file",
    "predict",
    "eval",
    "We can also open HDF5 file once and get access to",
    "With binary dataset created, we can use either Python API or cmdline version to train.",
    "",
    "Note: in order to create exactly the same dataset with the one created in simple_example.py, we need",
    "to modify simple_example.py to pass numpy array instead of pandas DataFrame to Dataset constructor.",
    "The reason is that DataFrame column names will be used in Dataset. For a DataFrame with Int64Index",
    "as columns, Dataset will use column names like [\"0\", \"1\", \"2\", ...]. While for numpy array, column names",
    "are using the default one assigned in C++ code (dataset_loader.cpp), like [\"Column_0\", \"Column_1\", ...].",
    "Y has a single column and we read it in single shot. So store it as an 1-d array.",
    "We use random access for data sampling when creating LightGBM Dataset from Sequence.",
    "When accessing any element in a HDF5 chunk, it's read entirely.",
    "To save I/O for sampling, we should keep number of total chunks much larger than sample count.",
    "Here we are just creating a chunk size that matches with batch_size.",
    "",
    "Also note that the data is stored in row major order to avoid extra copy when passing to",
    "lightgbm Dataset.",
    "Save to 2 HDF5 files for demonstration.",
    "We can store multiple datasets inside a single HDF5 file.",
    "Separating X and Y for choosing best chunk size for data loading.",
    "split training data into two partitions",
    "make this array dense because we're splitting across",
    "a sparse boundary to partition the data",
    "the code below uses sklearn.metrics, but this requires pulling all of the",
    "predictions and target values back from workers to the client",
    "",
    "for larger datasets, consider the metrics from dask-ml instead",
    "https://ml.dask.org/modules/api.html#dask-ml-metrics-metrics",
    "coding: utf-8",
    "!/usr/bin/env python3",
    "-*- coding: utf-8 -*-",
    "",
    "LightGBM documentation build configuration file, created by",
    "sphinx-quickstart on Thu May  4 14:30:58 2017.",
    "",
    "This file is execfile()d with the current directory set to its",
    "containing dir.",
    "",
    "Note that not all possible configuration values are present in this",
    "autogenerated file.",
    "",
    "All configuration values have a default; values that are commented out",
    "serve to show the default.",
    "If extensions (or modules to document with autodoc) are in another directory,",
    "add these directories to sys.path here. If the directory is relative to the",
    "documentation root, use os.path.abspath to make it absolute.",
    "-- General configuration ------------------------------------------------",
    "If your documentation needs a minimal Sphinx version, state it here.",
    "Add any Sphinx extension module names here, as strings. They can be",
    "extensions coming with Sphinx (named 'sphinx.ext.*') or your custom",
    "ones.",
    "mock out modules",
    "hide type hints in API docs",
    "Generate autosummary pages. Output should be set with: `:toctree: pythonapi/`",
    "Only the class' docstring is inserted.",
    "If true, `todo` and `todoList` produce output, else they produce nothing.",
    "The master toctree document.",
    "General information about the project.",
    "The name of an image file (relative to this directory) to place at the top",
    "of the sidebar.",
    "The name of an image file (relative to this directory) to use as a favicon of",
    "the docs.  This file should be a Windows icon file (.ico) being 16x16 or 32x32",
    "pixels large.",
    "The version info for the project you're documenting, acts as replacement for",
    "|version| and |release|, also used in various other places throughout the",
    "built documents.",
    "The short X.Y version.",
    "The full version, including alpha/beta/rc tags.",
    "The language for content autogenerated by Sphinx. Refer to documentation",
    "for a list of supported languages.",
    "",
    "This is also used if you do content translation via gettext catalogs.",
    "Usually you set \"language\" from the command line for these cases.",
    "List of patterns, relative to source directory, that match files and",
    "directories to ignore when looking for source files.",
    "This patterns also effect to html_static_path and html_extra_path",
    "The name of the Pygments (syntax highlighting) style to use.",
    "-- Configuration for C API docs generation ------------------------------",
    "-- Options for HTML output ----------------------------------------------",
    "The theme to use for HTML and HTML Help pages.  See the documentation for",
    "a list of builtin themes.",
    "Theme options are theme-specific and customize the look and feel of a theme",
    "further.  For a list of options available for each theme, see the",
    "documentation.",
    "Add any paths that contain custom static files (such as style sheets) here,",
    "relative to this directory. They are copied after the builtin static files,",
    "so a file named \"default.css\" will overwrite the builtin \"default.css\".",
    "-- Options for HTMLHelp output ------------------------------------------",
    "Output file base name for HTML help builder.",
    "-- Options for LaTeX output ---------------------------------------------",
    "The name of an image file (relative to this directory) to place at the top of",
    "the title page.",
    "intersphinx configuration",
    "Warning! The following code can cause buffer overflows on RTD.",
    "Consider suppressing output completely if RTD project silently fails.",
    "Refer to https://github.com/svenevs/exhale",
    "/blob/fe7644829057af622e467bb529db6c03a830da99/exhale/deploy.py#L99-L111",
    "Warning! The following code can cause buffer overflows on RTD.",
    "Consider suppressing output completely if RTD project silently fails.",
    "Refer to https://github.com/svenevs/exhale",
    "/blob/fe7644829057af622e467bb529db6c03a830da99/exhale/deploy.py#L99-L111",
    "coding: utf-8",
    "This is a basic test for floating number parsing.",
    "Most of the test cases come from:",
    "https://github.com/dmlc/xgboost/blob/master/tests/cpp/common/test_charconv.cc",
    "https://github.com/Alexhuszagh/rust-lexical/blob/master/data/test-parse-unittests/strtod_tests.toml",
    "FLT_MAX",
    "FLT_MIN",
    "DBL_MAX (1 + (1 - 2^-52)) * 2^1023 = (2^53 - 1) * 2^971",
    "2^971 * (2^53 - 1 + 1/2) : the smallest number resolving to inf",
    "Near DBL_MIN",
    "DBL_MIN 2^-1022",
    "The behavior for parsing -nan depends on implementation.",
    "Thus we skip binary check for negative nan.",
    "See comment in test_cases.",
    "construct sample data first (use all data for convenience and since size is small)",
    "Load some test data",
    "Use the smaller \".test\" data because we don't care about the actual data and it's smaller",
    "Add some fake initial_scores and groups so we can test streaming them",
    "Now use the reference dataset schema to make some testable Datasets with N rows each",
    "Load some test data",
    "Use the smaller \".test\" data because we don't care about the actual data and it's smaller",
    "Add some fake initial_scores and groups so we can test streaming them",
    "Now use the reference dataset schema to make some testable Datasets with N rows each",
    "This code is copied and adapted from the official Arrow producer examples:",
    "https://arrow.apache.org/docs/format/CDataInterface.html#exporting-a-struct-float32-utf8-array",
    "Free children",
    "Finalize",
    "Free children",
    "Free buffers",
    "Finalize",
    "NOTE: Arrow arrays have 64-bit alignment but we can safely ignore this in tests",
    "By using `calloc` above, we only need to set 'true' values",
    "Arithmetic",
    "Subscripts",
    "End",
    "Check for values in first chunk",
    "Check for some values in second chunk",
    "Check end",
    "Load some test data",
    "Serialize the reference",
    "Deserialize the reference",
    "Confirm 1 successful API call",
    "Free memory",
    "Load some test data",
    "Run a single row prediction and compare with regular Mat prediction:",
    "Drop the result from the dataset, we only care about checking that prediction results are equal",
    "in both cases",
    "Now let's run with the single row fast prediction API:",
    "Free all:",
    "Test that Data() points to first value written",
    "Constants",
    "Start with some content:",
    "Clear & re-use:",
    "Output should match new content:",
    "Number of trials for each new ChunkedArray configuration. Pass 100 times over the search space:",
    "Each outer loop iteration changes the test by adding +1 chunk. We start with 1 chunk only:",
    "Sweep valid and invalid addresses with a ChunkedArray with `chunks` chunks:",
    "Compute a new trial address & value & if it is a valid address:",
    "Insert item. If at a valid address, 0 is returned, otherwise, -1 is returned:",
    "If at valid address, check that the stored value is correct & remember it for the future:",
    "Check the just-stored value with getitem():",
    "Also store the just-stored value for future tracking:",
    "Final check: ensure even with overrides, all valid insertions store the latest value at that address:",
    "Test in 2 ways that the values are correctly laid out in memory:",
    "Since init_scores are in a column format, but need to be pushed as rows, we have to extract each batch",
    "Use multiple threads to test concurrency",
    "Since init_scores are in a column format, but need to be pushed as rows, we have to extract each batch",
    "Calculate expected boundaries",
    "Extract a set of rows from the column-based format (still maintaining column based format)",
    "coding: utf-8",
    "at initialization, should be -1",
    "updating that value through the C API should work",
    "resetting to any negative number should set it to -1",
    "coding: utf-8",
    "check saved model persistence",
    "we need to check the consistency of model file here, so test for exact equal",
    "check early stopping is working. Make it stop very early, so the scores should be very close to zero",
    "scores likely to be different, but prediction should still be the same",
    "test that shape is checked during prediction",
    "The simple implementation is just a single \"return self.ndarray[idx]\"",
    "The following is for demo and testing purpose.",
    "whole col",
    "half col",
    "Create dataset from numpy array directly.",
    "Create dataset using Sequence.",
    "Test for validation set.",
    "Select some random rows as valid data.",
    "From Dataset constructor, with dataset from numpy array.",
    "From Dataset.create_valid, with dataset from sequence.",
    "test that method works even with free_raw_data=True",
    "test that method works but sets raw data to None in case of immergeable data types",
    "test that method works for different data types",
    "Set extremely harsh penalties, so CEGB will block most splits.",
    "Compare pairs of penalties, to ensure scaling works as intended",
    "Reset booster1's parameters to p2, so the parameter section of the file matches.",
    "unconstructed, get_* methods should return whatever was provided",
    "before construction, get_field() should raise an exception",
    "constructed, get_* methods should return numpy arrays, even when the provided",
    "input was a list of floats or ints",
    "get_field(\"group\") returns a numpy array with boundaries, instead of size",
    "NOTE: \"position\" is converted to int32 on the C++ side",
    "should resolve duplicate aliases, and prefer the main parameter",
    "should choose the highest priority alias and set that value on main param",
    "if only aliases are used",
    "should use the default if main param and aliases are missing",
    "all changes should be made on copies and not modify the original",
    "preserves None found for main param and still removes aliases",
    "correctly chooses value when only an alias is provided",
    "adds None if that's given as the default and param not found",
    "If callable is found in objective",
    "Value in params should be preferred to the default_value passed from keyword arguments",
    "None of objective or its aliases in params, but default_value is callable.",
    "\"bad\" = 1 element too many",
    "copy=False is necessary because starting with pandas 3.0, pd.DataFrame() creates",
    "a copy of the input numpy array by default",
    "ref: https://github.com/pandas-dev/pandas/issues/58913",
    "check that the original data wasn't modified",
    "check that the built data has the codes",
    "if all categories were seen during training we just take the codes",
    "if we only saw 'a' during training we just replace its code",
    "and leave the rest as nan",
    "test using defined feature names",
    "test using default feature names",
    "check for feature indices outside of range",
    "NOTE: this intentionally contains values where num_leaves <, ==, and > (max_depth^2)",
    "NOTE: max_depth < 5 is significant here because the default for num_leaves=31. With max_depth=5,",
    "a full depth-wise tree would have 2^5 = 32 leaves.",
    "coding: utf-8",
    "add target, weight, and group to DataFrame so that partitions abide by group boundaries.",
    "set_index ensures partitions are based on group id.",
    "See https://stackoverflow.com/questions/49532824/dask-dataframe-split-partitions-based-on-a-column-or-function.",
    "separate target, weight from features.",
    "encode group identifiers into run-length encoding, the format LightGBMRanker is expecting",
    "so that within each partition, sum(g) = n_samples.",
    "ranking arrays: one chunk per group. Each chunk must include all columns.",
    "make one categorical feature relevant to the target",
    "https://github.com/microsoft/LightGBM/issues/4118",
    "extra predict() parameters should be passed through correctly",
    "pref_leaf values should have the right shape",
    "and values that look like valid tree nodes",
    "be sure LightGBM actually used at least one categorical column,",
    "and that it was correctly treated as a categorical feature",
    "shape depends on whether it is binary or multiclass classification",
    "in the special case of multi-class classification using scipy sparse matrices,",
    "the output of `.predict(..., pred_contrib=True)` is a list of sparse matrices (one per class)",
    "",
    "since that case is so different than all other cases, check the relevant things here",
    "and then return early",
    "raw scores will probably be different, but at least check that all predicted classes are the same",
    "be sure LightGBM actually used at least one categorical column,",
    "and that it was correctly treated as a categorical feature",
    "* shape depends on whether it is binary or multiclass classification",
    "* matrix for binary classification is of the form [feature_contrib, base_value],",
    "for multi-class it's [feat_contrib_class1, base_value_class1, feat_contrib_class2, base_value_class2, etc.]",
    "* contrib outputs for distributed training are different than from local training, so we can just test",
    "that the output has the right shape and base values are in the right position",
    "with a custom objective, prediction result is a raw score instead of predicted class",
    "function should have been preserved",
    "should correctly classify every sample",
    "probability estimates should be similar",
    "Scores should be the same",
    "Predictions should be roughly the same.",
    "pref_leaf values should have the right shape",
    "and values that look like valid tree nodes",
    "extra predict() parameters should be passed through correctly",
    "be sure LightGBM actually used at least one categorical column,",
    "and that it was correctly treated as a categorical feature",
    "contrib outputs for distributed training are different than from local training, so we can just test",
    "that the output has the right shape and base values are in the right position",
    "be sure LightGBM actually used at least one categorical column,",
    "and that it was correctly treated as a categorical feature",
    "Quantiles should be right",
    "be sure LightGBM actually used at least one categorical column,",
    "and that it was correctly treated as a categorical feature",
    "function should have been preserved",
    "Scores should be the same",
    "local and Dask predictions should be the same",
    "predictions should be better than random",
    "rebalance small dask.Array dataset for better performance.",
    "use many trees + leaves to overfit, help ensure that Dask data-parallel strategy matches that of",
    "serial learner. See https://github.com/microsoft/LightGBM/issues/3292#issuecomment-671288210.",
    "distributed ranker should be able to rank decently well and should",
    "have high rank correlation with scores from serial ranker.",
    "extra predict() parameters should be passed through correctly",
    "pref_leaf values should have the right shape",
    "and values that look like valid tree nodes",
    "be sure LightGBM actually used at least one categorical column,",
    "and that it was correctly treated as a categorical feature",
    "rebalance small dask.Array dataset for better performance.",
    "distributed ranker should be able to rank decently well with the least-squares objective",
    "and should have high rank correlation with scores from serial ranker.",
    "function should have been preserved",
    "Use larger trainset to prevent premature stopping due to zero loss, causing num_trees() < n_estimators.",
    "Use small chunk_size to avoid single-worker allocation of eval data partitions.",
    "test eval_class_weight, eval_init_score on binary-classification task.",
    "Note: objective's default `metric` will be evaluated in evals_result_ in addition to all eval_metrics.",
    "create eval_sets by creating new datasets or copying training data.",
    "total number of trees scales up for ova classifier.",
    "check that early stopping was not applied.",
    "checks that evals_result_ and best_score_ contain expected data and eval_set names.",
    "check that each eval_name and metric exists for all eval sets, allowing for the",
    "case when a worker receives a fully-padded eval_set component which is not evaluated.",
    "should be able to use the class without specifying a client",
    "should be able to set client after construction",
    "data on cluster1",
    "create identical data on cluster2",
    "at this point, the result of default_client() is client2 since it was the most recently",
    "created. So setting client to client1 here to test that you can select a non-default client",
    "unfitted model should survive pickling round trip, and pickling",
    "shouldn't have side effects on the model object",
    "client will always be None after unpickling",
    "fitted model should survive pickling round trip, and pickling",
    "shouldn't have side effects on the model object",
    "client will always be None after unpickling",
    "rebalance data to be sure that each worker has a piece of the data",
    "model 1 - no network parameters given",
    "model 2 - machines given",
    "model 3 - local_listen_port given",
    "training should fail because LightGBM will try to use the same",
    "port for multiple worker processes on the same machine",
    "rebalance data to be sure that each worker has a piece of the data",
    "test that \"machines\" is actually respected by creating a socket that uses",
    "one of the ports mentioned in \"machines\"",
    "The above error leaves a worker waiting",
    "an informative error should be raised if \"machines\" has duplicates",
    "\"client\" should be the only different, and the final argument",
    "value of the root node is 0 when init_score is set",
    "this test is separate because it takes a not-yet-constructed estimator",
    "coding: utf-8",
    "coding: utf-8",
    "build target, group ID vectors.",
    "build y/target and group-id vectors with user-specified group sizes.",
    "build y/target and group-id vectors according to n_samples, avg_gs, and random_gs.",
    "groups should contain > 1 element for pairwise learning objective.",
    "build feature data, X. Transform first few into informative features.",
    "doing this here, at import time, to ensure it only runs once_per import",
    "instead of once per assertion",
    "coding: utf-8",
    "NOTE: In the AppVeyor CI, importing pyarrow fails due to an old Visual Studio version. Hence,",
    "we conditionally import pyarrow here (and skip tests if it cannot be imported). However, we",
    "don't want these tests to silently be skipped, hence, we only conditionally import when a",
    "specific env var is set.",
    "----------------------------------------------------------------------------------------------- #",
    "UTILITIES                                            #",
    "----------------------------------------------------------------------------------------------- #",
    "Set random nulls",
    "Split data into <=2 random chunks",
    "Turn chunks into array",
    "----------------------------------------------------------------------------------------------- #",
    "UNIT TESTS                                           #",
    "----------------------------------------------------------------------------------------------- #",
    "------------------------------------------- DATASET ------------------------------------------- #",
    "-------------------------------------------- FIELDS ------------------------------------------- #",
    "Check for equality",
    "-------------------------------------------- LABELS ------------------------------------------- #",
    "------------------------------------------- WEIGHTS ------------------------------------------- #",
    "-------------------------------------------- GROUPS ------------------------------------------- #",
    "----------------------------------------- INIT SCORES ----------------------------------------- #",
    "------------------------------------------ PREDICTION ----------------------------------------- #",
    "coding: utf-8",
    "check that really dummy objective was used and estimator didn't learn anything",
    "prediction result is actually not transformed (is raw) due to custom objective",
    "original estimator is unaffected",
    "new estimator is unfitted, but has the same parameters",
    "Test if random_state is properly stored",
    "Test if two random states produce identical models",
    "Test if subsequent fits sample from random_state object and produce different models",
    "Test that the largest element is NOT the same, the smallest can be the same, i.e. zero",
    "why fixed seed?",
    "sometimes there is no difference how cols are treated (cat or not cat)",
    "With default params",
    "Tests same probabilities",
    "Tests same predictions",
    "Tests same raw scores",
    "Tests same leaf indices",
    "Tests same feature contributions",
    "Tests other parameters for the prediction works",
    "Tests start_iteration",
    "Tests same probabilities, starting from iteration 10",
    "Tests same predictions, starting from iteration 10",
    "Tests same raw scores, starting from iteration 10",
    "Tests same leaf indices, starting from iteration 10",
    "Tests same feature contributions, starting from iteration 10",
    "Tests other parameters for the prediction works, starting from iteration 10",
    "Test multiclass binary classification",
    "test that params passed in predict have higher priority",
    "no custom objective, no custom metric",
    "default metric",
    "non-default metric",
    "no metric",
    "non-default metric in eval_metric",
    "non-default metric with non-default metric in eval_metric",
    "non-default metric with multiple metrics in eval_metric",
    "non-default metric with multiple metrics in eval_metric for LGBMClassifier",
    "default metric for non-default objective",
    "non-default metric for non-default objective",
    "no metric",
    "non-default metric in eval_metric for non-default objective",
    "non-default metric with non-default metric in eval_metric for non-default objective",
    "non-default metric with multiple metrics in eval_metric for non-default objective",
    "custom objective, no custom metric",
    "default regression metric for custom objective",
    "non-default regression metric for custom objective",
    "multiple regression metrics for custom objective",
    "no metric",
    "default regression metric with non-default metric in eval_metric for custom objective",
    "non-default regression metric with metric in eval_metric for custom objective",
    "multiple regression metrics with metric in eval_metric for custom objective",
    "multiple regression metrics with multiple metrics in eval_metric for custom objective",
    "no custom objective, custom metric",
    "default metric with custom metric",
    "non-default metric with custom metric",
    "multiple metrics with custom metric",
    "custom metric (disable default metric)",
    "default metric for non-default objective with custom metric",
    "non-default metric for non-default objective with custom metric",
    "multiple metrics for non-default objective with custom metric",
    "custom metric (disable default metric for non-default objective)",
    "custom objective, custom metric",
    "custom metric for custom objective",
    "non-default regression metric with custom metric for custom objective",
    "multiple regression metrics with custom metric for custom objective",
    "default metric and invalid binary metric is replaced with multiclass alternative",
    "invalid binary metric is replaced with multiclass alternative",
    "default metric for non-default multiclass objective",
    "and invalid binary metric is replaced with multiclass alternative",
    "default metric and invalid multiclass metric is replaced with binary alternative",
    "invalid multiclass metric is replaced with binary alternative for custom objective",
    "the evaluation metric changes to multiclass metric even num classes is 2 for multiclass objective",
    "the evaluation metric changes to multiclass metric even num classes is 2 for ovr objective",
    "Verify that can receive a list of metrics, only callable",
    "Verify that can receive a list of custom and built-in metrics",
    "Verify that works as expected when eval_metric is empty",
    "Verify that can receive a list of metrics, only built-in",
    "Verify that eval_metric is robust to receiving a list with None",
    "feval",
    "single eval_set",
    "two eval_set",
    "input is a numpy array, which doesn't have feature names. LightGBM adds",
    "feature names to the fitted model, which is inconsistent with sklearn's behavior",
    "'val_minus_two' here is the expected number of threads for n_jobs=-2",
    "Note: according to joblib's formula, a value of n_jobs=-2 means",
    "\"use all but one thread\" (formula: n_cpus + 1 + n_jobs)",
    "try to predict with a different feature",
    "check that disabling the check doesn't raise the error",
    "make weights and init_score same types as y, just to avoid",
    "a huge number of combinations and therefore test cases",
    "make weights and init_score same types as y, just to avoid",
    "a huge number of combinations and therefore test cases",
    "coding: utf-8",
    "we're in a leaf now",
    "check that the rest of the elements have black color",
    "check that we got to the expected leaf",
    "coding: utf-8",
    "coding: utf-8",
    "check that default gives same result as k = 1",
    "check against independent calculation for k = 1",
    "check against independent calculation for k = 2",
    "check against independent calculation for k = 10",
    "check cases where predictions are equal",
    "should give same result as binary auc for 2 classes",
    "test the case where all predictions are equal",
    "test that weighted data gives different auc_mu",
    "test that equal data weights give same auc_mu as unweighted data",
    "should give 1 when accuracy = 1",
    "test loading class weights",
    "Simulates position bias for a given ranking dataset.",
    "The ouput dataset is identical to the input one with the exception for the relevance labels.",
    "The new labels are generated according to an instance of a cascade user model:",
    "for each query, the user is simulated to be traversing the list of documents ranked by a baseline ranker",
    "(in our example it is simply the ordering by some feature correlated with relevance, e.g., 34)",
    "and clicks on that document (new_label=1) with some probability 'pclick' depending on its true relevance;",
    "at each position the user may stop the traversal with some probability pstop. For the non-clicked documents,",
    "new_label=0. Thus the generated new labels are biased towards the baseline ranker.",
    "The positions of the documents in the ranked lists produced by the baseline, are returned.",
    "a mapping of a document's true relevance (defined on a 5-grade scale) into the probability of clicking it",
    "an instantiation of a cascade model where the user stops with probability 0.2 after observing each document",
    "simulate position bias for the train dataset and put the train dataset with biased labels to temp directory",
    "the performance of the unbiased LambdaMART should outperform the plain LambdaMART on the dataset with position bias",
    "add extra row to position file",
    "simulate position bias for the train dataset and put the train dataset with biased labels to temp directory",
    "test setting positions through Dataset constructor with numpy array",
    "the performance of the unbiased LambdaMART should outperform the plain LambdaMART on the dataset with position bias",
    "test setting positions through Dataset constructor with pandas Series",
    "test setting positions through set_position",
    "test get_position works",
    "no early stopping",
    "early stopping occurs",
    "regular early stopping",
    "positive min_delta",
    "test custom eval metrics",
    "shuffle = False, override metric in params",
    "shuffle = True, callbacks",
    "enable display training loss",
    "self defined folds",
    "LambdaRank",
    "... with l2 metric",
    "... with NDCG (default) metric",
    "self defined folds with lambdarank",
    "init_model from an in-memory Booster",
    "init_model from a text file",
    "predictions should be identical",
    "with early stopping",
    "predict by each fold booster",
    "check that each booster predicted using the best iteration",
    "fold averaging",
    "without early stopping",
    "test feature_names with whitespaces",
    "This has non-ascii strings.",
    "check that passing parameters to the constructor raises warning and ignores them",
    "check inference isn't affected by unknown parameter",
    "entries whose values should reflect params passed to lgb.train()",
    "'l1' was passed in with alias 'mae'",
    "NOTE: this was passed in with alias 'sub_row'",
    "entries with default values of params",
    "add device-specific entries",
    "",
    "passed-in force_col_wise / force_row_wise parameters are ignored on CUDA and GPU builds...",
    "https://github.com/microsoft/LightGBM/blob/1d7ee63686272bceffd522284127573b511df6be/src/io/config.cpp#L375-L377",
    "check that model text has all expected param entries",
    "since Booster.model_to_string() is used when pickling, check that parameters all",
    "roundtrip pickling successfully too",
    "why fixed seed?",
    "sometimes there is no difference how cols are treated (cat or not cat)",
    "take subsets and train",
    "generate CSR sparse dataset",
    "convert data to dense and get back same contribs",
    "validate the values are the same",
    "validate using CSC matrix",
    "validate the values are the same",
    "generate CSR sparse dataset",
    "convert data to dense and get back same contribs",
    "validate the values are the same",
    "validate using CSC matrix",
    "validate the values are the same",
    "@pytest.mark.skipif(psutil.virtual_memory().available / 1024 / 1024 / 1024 < 3, reason=\"not enough RAM\")",
    "def test_int32_max_sparse_contribs(rng):",
    "params = {\"objective\": \"binary\"}",
    "train_features = rng.uniform(size=(100, 1000))",
    "train_targets = [0] * 50 + [1] * 50",
    "lgb_train = lgb.Dataset(train_features, train_targets)",
    "gbm = lgb.train(params, lgb_train, num_boost_round=2)",
    "csr_input_shape = (3000000, 1000)",
    "test_features = csr_matrix(csr_input_shape)",
    "for i in range(0, csr_input_shape[0], csr_input_shape[0] // 6):",
    "for j in range(0, 1000, 100):",
    "test_features[i, j] = random.random()",
    "y_pred_csr = gbm.predict(test_features, pred_contrib=True)",
    "# Note there is an extra column added to the output for the expected value",
    "csr_output_shape = (csr_input_shape[0], csr_input_shape[1] + 1)",
    "assert y_pred_csr.shape == csr_output_shape",
    "y_pred_csc = gbm.predict(test_features.tocsc(), pred_contrib=True)",
    "# Note output CSC shape should be same as CSR output shape",
    "assert y_pred_csc.shape == csr_output_shape",
    "test sliced labels",
    "append some columns",
    "append some rows",
    "test sliced 2d matrix",
    "test sliced CSR",
    "trees start at position 1.",
    "split_features are in 4th line.",
    "test if a penalty as high as the depth indeed prohibits all monotone splits",
    "The penalization is so high that the first 2 features should not be used here",
    "Check that a very high penalization is the same as not using the features at all",
    "check refit accepts dataset_params",
    "the following checks that dart and rf with mape can predict outside the 0-1 range",
    "https://github.com/microsoft/LightGBM/issues/1579",
    "no custom objective, no feval",
    "default metric",
    "non-default metric in params",
    "default metric in args",
    "non-default metric in args",
    "metric in args overwrites one in params",
    "metric in args overwrites one in params",
    "multiple metrics in params",
    "multiple metrics in args",
    "remove default metric by 'None' in list",
    "remove default metric by 'None' aliases",
    "custom objective, no feval",
    "no default metric",
    "metric in params",
    "metric in args",
    "metric in args overwrites its' alias in params",
    "multiple metrics in params",
    "multiple metrics in args",
    "no custom objective, feval",
    "default metric with custom one",
    "non-default metric in params with custom one",
    "default metric in args with custom one",
    "non-default metric in args with custom one",
    "metric in args overwrites one in params, custom one is evaluated too",
    "multiple metrics in params with custom one",
    "multiple metrics in args with custom one",
    "custom metric is evaluated despite 'None' is passed",
    "custom objective, feval",
    "no default metric, only custom one",
    "metric in params with custom one",
    "metric in args with custom one",
    "metric in args overwrites one in params, custom one is evaluated too",
    "multiple metrics in params with custom one",
    "multiple metrics in args with custom one",
    "custom metric is evaluated despite 'None' is passed",
    "no custom objective, no feval",
    "default metric",
    "default metric in params",
    "non-default metric in params",
    "multiple metrics in params",
    "remove default metric by 'None' aliases",
    "custom objective, no feval",
    "no default metric",
    "metric in params",
    "multiple metrics in params",
    "no custom objective, feval",
    "default metric with custom one",
    "default metric in params with custom one",
    "non-default metric in params with custom one",
    "multiple metrics in params with custom one",
    "custom metric is evaluated despite 'None' is passed",
    "custom objective, feval",
    "no default metric, only custom one",
    "metric in params with custom one",
    "multiple metrics in params with custom one",
    "custom metric is evaluated despite 'None' is passed",
    "Custom objective replaces multiclass",
    "multiclass default metric",
    "multiclass default metric with custom one",
    "multiclass metric alias with custom one for custom objective",
    "no metric for invalid class_num",
    "custom metric for invalid class_num",
    "multiclass metric alias with custom one with invalid class_num",
    "multiclass default metric without num_class",
    "multiclass metric alias",
    "multiclass metric",
    "non-valid metric for multiclass objective",
    "non-default num_class for default objective",
    "no metric with non-default num_class for custom objective",
    "multiclass metric alias for custom objective",
    "multiclass metric for custom objective",
    "binary metric with non-default num_class for custom objective",
    "Expect three metrics but mean and stdv for each metric",
    "test XGBoost-style return value",
    "test numpy-style return value",
    "test bins string type",
    "test histogram is disabled for categorical features",
    "test for lgb.train",
    "test feval for lgb.train",
    "test with two valid data for lgb.train",
    "test for lgb.cv",
    "test feval for lgb.cv",
    "test that binning works properly for features with only positive or only negative values",
    "decreasing without freeing raw data is allowed",
    "decreasing before lazy init is allowed",
    "increasing is allowed",
    "decreasing with disabled filter is allowed",
    "decreasing with enabled filter is disallowed;",
    "also changes of other params are disallowed",
    "check extra trees increases regularization",
    "check path smoothing increases regularization",
    "test edge case with one leaf",
    "check that constraint containing all features is equivalent to no constraint",
    "check that constraint partitioning the features reduces train accuracy",
    "check that constraints consisting of single features reduce accuracy further",
    "test that interaction constraints work when not all features are used",
    "check that number of threads does not affect result",
    "check that setting linear_tree=True fits better than ordinary trees when data has linear relationship",
    "test again with nans in data",
    "test again with bagging",
    "test with a feature that has only one non-nan value",
    "test with a categorical feature",
    "test refit: same results on same data",
    "test refit with save and load",
    "test refit: different results training on different data",
    "test when num_leaves - 1 < num_features and when num_leaves - 1 > num_features",
    "test that the predict once with all iterations equals summed results with start_iteration and num_iteration",
    "test the case where start_iteration <= 0, and num_iteration is None",
    "test the case where start_iteration > 0, and num_iteration <= 0",
    "test the case where start_iteration > 0, and num_iteration <= 0, with pred_leaf=True",
    "test the case where start_iteration > 0, and num_iteration <= 0, with pred_contrib=True",
    "test for regression",
    "test both with and without early stopping",
    "test for multi-class",
    "test both with and without early stopping",
    "test for binary",
    "test both with and without early stopping",
    "test against sklearn average precision metric",
    "test that average precision is 1 where model predicts perfectly",
    "data as float64",
    "test all features were used",
    "test the score is better than predicting the mean",
    "test all predictions are equal using different input dtypes",
    "introduce some missing values",
    "in recent versions of pandas, type 'bool' is incompatible with nan values in x4",
    "train with regular dtypes",
    "convert to nullable dtypes",
    "test training succeeds",
    "test all features were used",
    "test the score is better than predicting the mean",
    "test equal predictions",
    "test data are taken from bug report",
    "https://github.com/microsoft/LightGBM/issues/4708",
    "modified from https://github.com/microsoft/LightGBM/issues/3679#issuecomment-938652811",
    "and https://github.com/microsoft/LightGBM/pull/5087",
    "test that the ``splits_per_leaf_`` of CEGB is cleaned before training a new tree",
    "which is done in the fix #5164",
    "without the fix:",
    "Check failed: (best_split_info.left_count) > (0)",
    "try to predict with a different feature",
    "check that disabling the check doesn't raise the error",
    "try to refit with a different feature",
    "check that disabling the check doesn't raise the error",
    "coding: utf-8",
    "If compiled appropriately, the same installation will support both GPU and CPU.",
    "Double-precision floats are only supported on x86_64 with PoCL",
    "coding: utf-8",
    "coding: utf-8",
    "Note: MSVC has issues with Altrep classes, so they are disabled for it.",
    "See: https://github.com/microsoft/LightGBM/pull/6213#issuecomment-2111025768",
    "These are helper functions to allow doing a stack unwind",
    "after an R allocation error, which would trigger a long jump.",
    "convert from one-based to zero-based index",
    "if any feature names were larger than allocated size,",
    "allow for a larger size and try again",
    "convert from boundaries to size",
    "--- start Booster interfaces",
    "if any eval names were larger than allocated size,",
    "allow for a larger size and try again",
    "Note: for some reason, MSVC crashes when an error is thrown here",
    "if the buffer variable is defined as 'std::unique_ptr<std::vector<char>>',",
    "but not if it is defined as '<std::vector<char>'.",
    "if the model string was larger than the initial buffer, call the function again, writing directly to the R object",
    "if the model string was larger than the initial buffer, allocate a bigger buffer and try again",
    "if aliases string was larger than the initial buffer, allocate a bigger buffer and try again",
    "if aliases string was larger than the initial buffer, allocate a bigger buffer and try again",
    ".Call() calls",
    "coding: utf-8",
    "alias table",
    "names",
    "from strings",
    "tails",
    "tails",
    "the following are stored as comma separated strings but are arrays in the wrappers",
    "coding: utf-8",
    "Single row predictor to abstract away caching logic",
    "Prevent the booster from being modified while we have a predictor relying on it during prediction",
    "If several threads try to predict at the same time using the same SingleRowPredictor",
    "we want them to still provide correct values, so the mutex is necessary due to the shared",
    "resources in the predictor.",
    "However the recommended approach is to instantiate one SingleRowPredictor per thread,",
    "to avoid contention here.",
    "create boosting",
    "initialize the boosting",
    "create objective function",
    "initialize the objective function",
    "create training metric",
    "reset the boosting",
    "create objective function",
    "initialize the objective function",
    "Workaround https://github.com/microsoft/LightGBM/issues/6142 by locking here",
    "This is only a workaround because if predictors are initialized differently it may still behave incorrectly,",
    "and because multiple racing Predictor initializations through LGBM_BoosterPredictForMat suffers from that same issue of Predictor init writing things in the booster.",
    "Once #6142 is fixed (predictor doesn't write in the Booster as should have been the case since 1c35c3b9ede9adab8ccc5fd7b4b2b6af188a79f0), this line can be removed.",
    "calculate the nonzero data and indices size",
    "allocate data and indices arrays",
    "Get the number of trees per iteration (for multiclass scenario we output multiple sparse matrices)",
    "aggregated per row feature contribution results",
    "keep track of the row_vector sizes for parallelization",
    "copy vector results to output for each row",
    "Get the number of trees per iteration (for multiclass scenario we output multiple sparse matrices)",
    "aggregated per row feature contribution results",
    "calculate number of elements per column to construct",
    "the CSC matrix with random access",
    "keep track of column counts",
    "keep track of beginning index for each column",
    "keep track of beginning index for each matrix",
    "Note: we parallelize across matrices instead of rows because of the column_counts[m][col_idx] increment inside the loop",
    "store the row index",
    "update column count",
    "explicitly declare symbols from LightGBM namespace",
    "some help functions used to convert data",
    "Row iterator of on column for CSC matrix",
    "return value at idx, only can access by ascent order",
    "return next non-zero pair, if index < 0, means no more data",
    "start of c_api functions",
    "This API is to keep python binding's behavior the same with C++ implementation.",
    "Sample count, random seed etc. should be provided in parameters.",
    "convert internal thread id to be unique based on external thread id",
    "convert internal thread id to be unique based on external thread id",
    "sample data first",
    "sample data first",
    "sample data first",
    "local buffer to re-use memory",
    "sample data first",
    "no more data",
    "Prepare the Arrow data",
    "Initialize the dataset",
    "If there is no reference dataset, we first sample indices",
    "Then, we obtain sample values by parallelizing across columns",
    "Values need to be copied from the record batches.",
    "The chunks are iterated over in the inner loop as columns can be treated independently.",
    "Finally, we initialize a loader from the sampled values",
    "After sampling and properly initializing all bins, we can add our data to the dataset. Here,",
    "we parallelize across rows.",
    "---- start of booster",
    "Naming: In future versions of LightGBM, public API named around `FastConfig` should be made named around",
    "`SingleRowPredictor`, because it is specific to single row prediction, and doesn't actually hold only config.",
    "For now this is kept as `FastConfig` for backwards compatibility.",
    "At the same time, one should consider removing the old non-fast single row public API that stores its Predictor",
    "in the Booster, because that will enable removing these Predictors from the Booster, and associated initialization",
    "code.",
    "Single row in row-major format:",
    "Apply the configuration",
    "Set up chunked array and iterators for all columns",
    "Build row function",
    "Run prediction",
    "---- start of some help functions",
    "data is array of pointers to individual rows",
    "set number of threads for openmp",
    "read parameters from config file",
    "remove str after \"#\"",
    "de-duplicate params",
    "prediction is needed if using input initial model(continued train)",
    "need to continue training",
    "sync up random seed for data partition",
    "load Training data",
    "load data for distributed training",
    "load data for single machine",
    "need save binary file",
    "create training metric",
    "only when have metrics then need to construct validation data",
    "Add validation data, if it exists",
    "add",
    "need save binary file",
    "add metric for validation data",
    "output used time on each iteration",
    "need init network",
    "create boosting",
    "create objective function",
    "load training data",
    "initialize the objective function",
    "initialize the boosting",
    "add validation data into boosting",
    "convert model to if-else statement code",
    "create predictor",
    "Free memory",
    "create predictor",
    "label_gain = 2^i - 1, may overflow, so we use 31 here",
    "counts for all labels",
    "start from top label, and accumulate DCG",
    "counts for all labels",
    "calculate k Max DCG by one pass",
    "get sorted indices by score",
    "calculate multi dcg by one pass",
    "wait for all client start up",
    "Don't call MPI_Finalize() here: If the destructor was called because only this node had an exception, calling MPI_Finalize() will cause all nodes to hang.",
    "Instead we will handle finalize/abort for MPI in main().",
    "default set to -1",
    "distance at k-th communication, distance[k] = 2^k",
    "set incoming rank at k-th commuication",
    "set outgoing rank at k-th commuication",
    "default set as -1",
    "construct all recursive halving map for all machines",
    "let 1 << k <= num_machines",
    "distance of each communication",
    "if num_machines = 2^k, don't need to group machines",
    "communication direction, %2 == 0 is positive",
    "neighbor at k-th communication",
    "receive data block at k-th communication",
    "send data block at k-th communication",
    "if num_machines != 2^k, need to group machines",
    "group, two machine in one group, total \"rest\" groups will have 2 machines.",
    "let left machine as group leader",
    "cache block information for groups, group with 2 machines will have double block size",
    "convert from group to node leader",
    "convert from node to group",
    "meet new group",
    "add block len for this group",
    "calculate the group block start",
    "not need to construct",
    "get receive block information",
    "accumulate block len",
    "get send block information",
    "accumulate block len",
    "static member definition",
    "if small package or small count , do it by all gather.(reduce the communication times.)",
    "assign the blocks to every rank.",
    "do reduce scatter",
    "do all gather",
    "assign blocks",
    "need use buffer here, since size of \"output\" is smaller than size after all gather",
    "copy back",
    "assign blocks",
    "start all gather",
    "when num_machines is small and data is large",
    "use output as receive buffer",
    "get current local block size",
    "get out rank",
    "get in rank",
    "get send information",
    "get recv information",
    "send and recv at same time",
    "rotate in-place",
    "use output as receive buffer",
    "get current local block size",
    "get send information",
    "get recv information",
    "send and recv at same time",
    "use output as receive buffer",
    "send and recv at same time",
    "send local data to neighbor first",
    "receive neighbor data first",
    "reduce",
    "get target",
    "get send information",
    "get recv information",
    "send and recv at same time",
    "reduce",
    "send result to neighbor",
    "receive result from neighbor",
    "copy result",
    "start up socket",
    "parse clients from file",
    "get ip list of local machine",
    "get local rank",
    "construct listener",
    "construct communication topo",
    "construct linkers",
    "free listener",
    "set timeout",
    "accept incoming socket",
    "receive rank",
    "add new socket",
    "save ranks that need to connect with",
    "start listener",
    "start connect",
    "let smaller rank connect to larger rank",
    "send local rank",
    "wait for listener",
    "print connected linkers",
    "only need to copy subset",
    "avoid to copy subset many times",
    "avoid out of range",
    "may need to recopy subset",
    "valid the type",
    "parser factory implementation.",
    "customized parser add-on.",
    "save header to parser config in case needed.",
    "save label id to parser config in case needed.",
    "Constructors",
    "Get type tag",
    "Comparisons",
    "This has to be separate, not in Statics, because Json() accesses",
    "statics().null.",
    "advance until next line, or end of input",
    "advance until closing tokens",
    "The usual case: non-escaped characters",
    "Handle escapes",
    "Extract 4-byte escape sequence",
    "Explicitly check length of the substring. The following loop",
    "relies on std::string returning the terminating NUL when",
    "accessing str[length]. Checking here reduces brittleness.",
    "JSON specifies that characters outside the BMP shall be encoded as a",
    "pair of 4-hex-digit \\u escapes encoding their surrogate pair",
    "components. Check whether we're in the middle of such a beast: the",
    "previous codepoint was an escaped lead (high) surrogate, and this is",
    "a trail (low) surrogate.",
    "Reassemble the two surrogate pairs into one astral-plane character,",
    "per the UTF-16 algorithm.",
    "Integer part",
    "Decimal part",
    "Exponent part",
    "Check for any trailing garbage",
    "Documented in json11.hpp",
    "Check for another object",
    "get column names",
    "support to get header from parser config, so could utilize following label name to id mapping logic.",
    "load label idx first",
    "if parser config file exists, feature names may be changed after customized parser applied.",
    "clear here so could use default filled feature names during dataset construction.",
    "may improve by saving real feature names defined in parser in the future.",
    "erase label column name",
    "load ignore columns",
    "load weight idx",
    "load group idx",
    "don't support query id in data file when using distributed training",
    "read data to memory",
    "sample data",
    "construct feature bin mappers & clear sample data",
    "initialize label",
    "extract features",
    "sample data from file",
    "construct feature bin mappers & clear sample data",
    "initialize label",
    "extract features",
    "load data from binary file",
    "checks whether there's a initial score file when loaded from binary data files",
    "the initial score file should with suffix \".bin.init\"",
    "check meta data",
    "need to check training data",
    "read data in memory",
    "initialize label",
    "extract features",
    "Get number of lines of data file",
    "initialize label",
    "extract features",
    "load data from binary file",
    "checks whether there's a initial score file when loaded from binary data files",
    "the initial score file should with suffix \".bin.init\"",
    "not need to check validation data",
    "check meta data",
    "check token",
    "read feature group definitions",
    "read feature size",
    "buffer to read binary file",
    "check token",
    "read size of header",
    "re-allocate space if not enough",
    "read header",
    "get header",
    "read size of meta data",
    "re-allocate space if not enough",
    "read meta data",
    "load meta data",
    "sample local used data if need to partition",
    "if not contain query file, minimal sample unit is one record",
    "if contain query file, minimal sample unit is one query",
    "if is new query",
    "read feature data",
    "read feature size",
    "re-allocate space if not enough",
    "raw data",
    "fill feature_names_ if not header",
    "get forced split",
    "if only one machine, find bin locally",
    "if have multi-machines, need to find bin distributed",
    "different machines will find bin for different features",
    "start and len will store the process feature indices for different machines",
    "machine i will find bins for features in [ start[i], start[i] + len[i] )",
    "free",
    "gather global feature bin mappers",
    "restore features bins from buffer",
    "---- private functions ----",
    "get header",
    "num_groups",
    "real_feature_idx_",
    "feature2group",
    "feature2subfeature",
    "group_bin_boundaries",
    "group_feature_start_",
    "group_feature_cnt_",
    "get feature names",
    "get forced_bin_bounds_",
    "if features are ordered, not need to use hist_buf",
    "read all lines",
    "get query data",
    "if not contain query data, minimal sample unit is one record",
    "if contain query data, minimal sample unit is one query",
    "if is new query",
    "get query data",
    "if not contain query file, minimal sample unit is one record",
    "if contain query file, minimal sample unit is one query",
    "if is new query",
    "parse features",
    "get forced split",
    "check the range of label_idx, weight_idx and group_idx",
    "skip label check if user input parser config file,",
    "because label id is got from raw features while dataset features are consistent with customized parser.",
    "fill feature_names_ if not header",
    "start find bins",
    "if only one machine, find bin locally",
    "start and len will store the process feature indices for different machines",
    "machine i will find bins for features in [ start[i], start[i] + len[i] )",
    "free",
    "gather global feature bin mappers",
    "restore features bins from buffer",
    "if doesn't need to prediction with initial model",
    "parser",
    "set label",
    "free processed line:",
    "shrink_to_fit will be very slow in linux, and seems not free memory, disable for now",
    "text_reader_->Lines()[i].shrink_to_fit();",
    "push data",
    "if is used feature",
    "if need to prediction with initial model",
    "parser",
    "set initial score",
    "set label",
    "free processed line:",
    "shrink_to_fit will be very slow in Linux, and seems not free memory, disable for now",
    "text_reader_->Lines()[i].shrink_to_fit();",
    "push data",
    "if is used feature",
    "metadata_ will manage space of init_score",
    "text data can be free after loaded feature values",
    "parser",
    "set initial score",
    "set label",
    "push data",
    "if is used feature",
    "only need part of data",
    "need full data",
    "metadata_ will manage space of init_score",
    "read size of token",
    "remove duplicates",
    "deep copy function for BinMapper",
    "mean size for one bin",
    "need a new bin",
    "update bin upper bound",
    "last bin upper bound",
    "get number of positive and negative distinct values",
    "include zero bounds and infinity bound",
    "add forced bounds, excluding zeros since we have already added zero bounds",
    "find remaining bounds",
    "find distinct_values first",
    "push zero in the front",
    "use the large value",
    "push zero in the back",
    "convert to int type first",
    "sort by counts in descending order",
    "will ignore the categorical of small counts",
    "Push the dummy bin for NaN",
    "Use MissingType::None to represent this bin contains all categoricals",
    "fix count of NaN bin",
    "check trivial(num_bin_ == 1) feature",
    "check useless bin",
    "When most_freq_bin_ != default_bin_, there are some additional data loading costs.",
    "so use most_freq_bin_ = default_bin_ when there is not so sparse",
    "calculate max bin of all features to select the int type in MultiValDenseBin",
    "for lambdarank, it needs query data for partition data in distributed learning",
    "need convert query_id to boundaries",
    "check weights",
    "check positions",
    "check query boundries",
    "contain initial score file",
    "check weights",
    "get local weights",
    "check positions",
    "get local positions",
    "check query boundries",
    "get local query boundaries",
    "contain initial score file",
    "get local initial scores",
    "re-calculate query weight",
    "Clear init scores on empty input",
    "Note that len here is row count, not num_init_score, so we compare against num_data",
    "We need to use source_size here, because len might not equal size (due to a partially loaded dataset)",
    "CUDA is handled after all insertions are complete",
    "CUDA is handled after all insertions are complete",
    "Clear weights on empty input",
    "CUDA is handled after all insertions are complete",
    "Clear query boundaries on empty input",
    "save to nullptr",
    "CUDA is handled after all insertions are complete",
    "default weight file name",
    "default position file name",
    "default init_score file name",
    "use first line to count number class",
    "default query file name",
    "root is in the depth 0",
    "non-leaf",
    "leaf",
    "use this for the missing value conversion",
    "Predict func by Map to ifelse",
    "use this for the missing value conversion",
    "non-leaf",
    "left subtree",
    "right subtree",
    "leaf",
    "non-leaf",
    "left subtree",
    "right subtree",
    "leaf",
    "recursive computation of SHAP values for a decision tree",
    "extend the unique path",
    "leaf node",
    "internal node",
    "see if we have already split on this feature,",
    "if so we undo that split so we can redo it for this node",
    "recursive sparse computation of SHAP values for a decision tree",
    "extend the unique path",
    "leaf node",
    "internal node",
    "see if we have already split on this feature,",
    "if so we undo that split so we can redo it for this node",
    "if \"verbosity\" was found in params, prefer that to any other aliases",
    "if \"verbose\" was found in params and \"verbosity\" was not, use that value",
    "if \"verbosity\" and \"verbose\" were both missing from params, don't modify LightGBM's log level",
    "otherwise, update LightGBM's log level based on the passed-in value",
    "add names of objective function if not providing metric",
    "equal weights for all classes",
    "generate seeds by seed.",
    "sort eval_at",
    "Only push the non-training data",
    "check for conflicts",
    "check if objective, metric, and num_class match",
    "Change pool size to -1 (no limit) when using data parallel to reduce communication costs",
    "max_depth defaults to -1, so max_depth>0 implies \"you explicitly overrode the default\"",
    "",
    "Changing max_depth while leaving num_leaves at its default (31) can lead to 2 undesirable situations:",
    "",
    "* (0 <= max_depth <= 4) it's not possible to produce a tree with 31 leaves",
    "- this block reduces num_leaves to 2^max_depth",
    "* (max_depth > 4) 31 leaves is less than a full depth-wise tree, which might lead to underfitting",
    "- this block warns about that",
    "ref: https://github.com/microsoft/LightGBM/issues/2898#issuecomment-1002860601",
    "Fits in an int, and is more restrictive than the current num_leaves",
    "force col-wise for gpu, and cuda version",
    "force row-wise for cuda version",
    "linear tree learner must be serial type and run on CPU device",
    "min_data_in_leaf must be at least 2 if path smoothing is active. This is because when the split is calculated",
    "the count is calculated using the proportion of hessian in the leaf which is rounded up to nearest int, so it can",
    "be 1 when there is actually no data in the leaf. In rare cases this can cause a bug because with path smoothing the",
    "calculated split gain can be positive even with zero gradient and hessian.",
    "In distributed mode, local node doesn't have histograms on all features, cannot perform \"intermediate\" monotone constraints.",
    "\"intermediate\" monotone constraints need to recompute splits. If the features are sampled when computing the",
    "split initially, then the sampling needs to be recorded or done once again, which is currently not supported",
    "first round: fill the single val group",
    "always push the last group",
    "put dense feature first",
    "sort by non zero cnt",
    "sort by non zero cnt, bigger first",
    "shuffle groups",
    "Using std::swap for vector<bool> will cause the wrong result.",
    "get num_features",
    "get bin_mappers",
    "for sparse multi value bin, we store the feature bin values with offset added",
    "for dense multi value bin, the feature bin values without offsets are used",
    "copy feature bin mapper data",
    "copy feature bin mapper data",
    "update CUDA storage for column data and metadata",
    "if not pass a filename, just append \".bin\" of original file",
    "Write the basic header information for the dataset",
    "get size of meta data",
    "write meta data",
    "write feature data",
    "get size of feature",
    "write feature",
    "write raw data; use row-major order so we can read row-by-row",
    "Calculate approximate size of output and reserve space",
    "write feature group definitions",
    "Give a little extra just in case, to avoid unnecessary resizes",
    "Write token that marks the data as binary reference, and the version",
    "Write the basic definition of the overall dataset",
    "write feature group definitions",
    "get size of feature",
    "write feature",
    "size of feature names and forced bins",
    "write header",
    "write feature names",
    "write forced bins",
    "explicitly initialize template methods, for cross module call",
    "explicitly initialize template methods, for cross module call",
    "Only one multi-val group, just simply merge",
    "Skip the leading 0 when copying group_bin_boundaries.",
    "regenerate other fields",
    "need to iterate bin iterator",
    "is dense column",
    "is sparse column",
    "initialize the subset cuda column data",
    "if one column has too many bins, use a separate partition for that column",
    "try if adding this column exceed the maximum number per partition",
    "if one column has too many bins, use a separate partition for that column",
    "try if adding this column exceed the maximum number per partition",
    "if LightGBM-specific default has been set, ignore OpenMP-global config",
    "otherwise, default to OpenMP-global config",
    "ensure that if LGBM_SetMaxThreads() was ever called, LightGBM doesn't",
    "use more than that many threads",
    "store the importance first",
    "PredictRaw",
    "PredictRawByMap",
    "Predict",
    "PredictByMap",
    "PredictLeafIndex",
    "PredictLeafIndexByMap",
    "output model type",
    "output number of class",
    "output label index",
    "output max_feature_idx",
    "output objective",
    "output tree models",
    "store the importance first",
    "sort the importance",
    "use serialized string to restore this object",
    "Use first 128 chars to avoid exceed the message buffer.",
    "get number of classes",
    "get index of label",
    "get max_feature_idx first",
    "get average_output",
    "get feature names",
    "get monotone_constraints",
    "set zero",
    "predict all the trees for one iteration",
    "check early stopping",
    "set zero",
    "predict all the trees for one iteration",
    "check early stopping",
    "margin_threshold will be captured by value",
    "copy and sort",
    "margin_threshold will be captured by value",
    "Fix for compiler warnings about reaching end of control",
    "load forced_splits file",
    "init tree learner",
    "push training metrics",
    "get max feature index",
    "get label index",
    "get feature names",
    "get parser config file content",
    "check that forced splits does not use feature indices larger than dataset size",
    "if need bagging, create buffer",
    "for a validation dataset, we need its score and metric",
    "update score",
    "objective function will calculate gradients and hessians",
    "output used time per iteration",
    "boosting from average label; or customized \"average\" if implemented for the current objective",
    "boosting first",
    "use customized objective function",
    "the check below fails unless objective=custom is provided in the parameters on Booster creation",
    "need to copy customized gradients when using GOSS",
    "bagging logic",
    "need to copy gradients for bagging subset.",
    "shrinkage by learning rate",
    "update score",
    "only add default score one-time",
    "updates scores",
    "add model",
    "reset score",
    "remove model",
    "print message for metric",
    "pop last early_stopping_round_ models",
    "update training score",
    "we need to predict out-of-bag scores of data for boosting",
    "update validation score",
    "print training metric",
    "print validation metric",
    "set zero",
    "predict all the trees for one iteration",
    "predict all the trees for one iteration",
    "push training metrics",
    "not same training data, need reset score and others",
    "create score tracker",
    "update score",
    "resize gradient vectors to copy the customized gradients for goss or bagging with subset",
    "load forced_splits file",
    "if exists initial score, will start from it",
    "clear host score buffer",
    "Need special case for no smoothing to preserve existing behaviour. If no smoothing, the parent output is calculated",
    "with the larger categorical l2, whereas min_split_gain uses the original l2.",
    "if data not enough, or sum hessian too small",
    "if data not enough",
    "if sum hessian too small",
    "current split gain",
    "gain with split is worse than without split",
    "mark as able to be split",
    "better split point",
    "recover sum of gradient and hessian from the sum of quantized gradient and hessian",
    "Need special case for no smoothing to preserve existing behaviour. If no smoothing, the parent output is calculated",
    "with the larger categorical l2, whereas min_split_gain uses the original l2.",
    "if data not enough, or sum hessian too small",
    "if data not enough",
    "if sum hessian too small",
    "current split gain",
    "gain with split is worse than without split",
    "mark as able to be split",
    "better split point",
    "Get the max size of pool",
    "at least need 2 leaves",
    "push split information for all leaves",
    "initialize splits for leaf",
    "initialize data partition",
    "initialize ordered gradients and hessians",
    "cannot change is_hist_col_wise during training",
    "initialize splits for leaf",
    "initialize data partition",
    "initialize ordered gradients and hessians",
    "Get the max size of pool",
    "at least need 2 leaves",
    "push split information for all leaves",
    "some initial works before training",
    "root leaf",
    "only root leaf can be splitted on first time",
    "some initial works before finding best split",
    "find best threshold for every feature",
    "Get a leaf with max split gain",
    "Get split information for best leaf",
    "cannot split, quit",
    "split tree with best leaf",
    "reset histogram pool",
    "initialize data partition",
    "reset the splits for leaves",
    "Sumup for root",
    "use all data",
    "use bagging, only use part of data",
    "check depth of current leaf",
    "only need to check left leaf, since right leaf is in same level of left leaf",
    "no enough data to continue",
    "only have root",
    "put parent(left) leaf's histograms into larger leaf's histograms",
    "put parent(left) leaf's histograms to larger leaf's histograms",
    "construct smaller leaf",
    "construct larger leaf",
    "find splits",
    "only has root leaf",
    "start at root leaf",
    "Histogram construction require parent features.",
    "then, compute own splits",
    "split info should exist because searching in bfs fashion - should have added from parent",
    "update before tree split",
    "don't need to update this in data-based parallel model",
    "split tree, will return right leaf",
    "store the true split gain in tree model",
    "don't need to update this in data-based parallel model",
    "store the true split gain in tree model",
    "init the leaves that used on next iteration",
    "update leave outputs if needed",
    "bag_mapper[index_mapper[i]]",
    "it is needed to filter the features after the above code.",
    "Otherwise, the `is_splittable` in `FeatureHistogram` will be wrong, and cause some features being accidentally filtered in the later nodes.",
    "for root leaf the \"parent\" output is its own output because we don't apply any smoothing to the root",
    "can't use GetParentOutput because leaf_splits doesn't have weight property set",
    "find splits",
    "identify features containing nans",
    "preallocate the matrix used to calculate linear model coefficients",
    "store only upper triangular half of matrix as an array, in row-major order",
    "this requires (max_num_feat + 1) * (max_num_feat + 2) / 2 entries (including the constant terms of the regression)",
    "we add another 8 to ensure cache lines are not shared among processors",
    "some initial works before training",
    "root leaf",
    "only root leaf can be splitted on first time",
    "some initial works before finding best split",
    "find best threshold for every feature",
    "Get a leaf with max split gain",
    "Get split information for best leaf",
    "cannot split, quit",
    "split tree with best leaf",
    "map data to leaf number",
    "calculate coefficients using the method described in Eq 3 of https://arxiv.org/pdf/1802.05640.pdf",
    "the coefficients vector is given by",
    "- (X_T * H * X + lambda) ^ (-1) * (X_T * g)",
    "where:",
    "X is the matrix where the first column is the feature values and the second is all ones,",
    "H is the diagonal matrix of the hessian,",
    "lambda is the diagonal matrix with diagonal entries equal to the regularisation term linear_lambda",
    "g is the vector of gradients",
    "the subscript _T denotes the transpose",
    "create array of pointers to raw data, and coefficient matrices, for each leaf",
    "clear the coefficient matrices",
    "aggregate results from different threads",
    "copy into eigen matrices and solve",
    "update the tree properties",
    "need to be able to hold smaller and larger best splits in SyncUpGlobalBestSplit",
    "get feature partition",
    "get local used features",
    "get best split at smaller leaf",
    "find local best split for larger leaf",
    "sync global best info",
    "update best split",
    "instantiate template classes, otherwise linker cannot find the code",
    "initialize SerialTreeLearner",
    "Get local rank and global machine size",
    "need to be able to hold smaller and larger best splits in SyncUpGlobalBestSplit",
    "allocate buffer for communication",
    "get block start and block len for reduce scatter",
    "get buffer_write_start_pos",
    "get buffer_read_start_pos",
    "generate feature partition for current tree",
    "get local used feature",
    "get block start and block len for reduce scatter",
    "sync global data sumup info",
    "global sumup reduce",
    "copy back",
    "set global sumup info",
    "init global data count in leaf",
    "reset hist num bits according to global num data",
    "sync global data sumup info",
    "global sumup reduce",
    "copy back",
    "set global sumup info",
    "init global data count in leaf",
    "clear histogram buffer before synchronizing",
    "otherwise histogram contents from the previous iteration will be sent",
    "construct local histograms",
    "copy to buffer",
    "Reduce scatter for histogram",
    "restore global histograms from buffer",
    "only root leaf",
    "construct histgroms for large leaf, we init larger leaf as the parent, so we can just subtract the smaller leaf's histograms",
    "find local best split for larger leaf",
    "sync global best info",
    "set best split",
    "need update global number of data in leaf",
    "reset hist num bits according to global num data",
    "instantiate template classes, otherwise linker cannot find the code",
    "initialize SerialTreeLearner",
    "some additional variables needed for GPU trainer",
    "Initialize GPU buffers and kernels",
    "some functions used for debugging the GPU histogram construction",
    "printf(\"grad %g != %g (%d ULPs)\\n\", GET_GRAD(h1, i), GET_GRAD(h2, i), ulps);",
    "goto err;",
    "we roughly want 256 workgroups per device, and we have num_dense_feature4_ feature tuples.",
    "also guarantee that there are at least 2K examples per workgroup",
    "return 0;",
    "we have already copied ordered gradients, ordered Hessians and indices to GPU",
    "decide the best number of workgroups working on one feature4 tuple",
    "set work group size based on feature size",
    "each 2^exp_workgroups_per_feature workgroups work on a feature4 tuple",
    "we need to refresh the kernel arguments after reallocating",
    "The only argument that needs to be changed later is num_data_",
    "the GPU kernel will process all features in one call, and each",
    "2^exp_workgroups_per_feature (compile time constant) workgroup will",
    "process one feature4 tuple",
    "for the root node, indices are not copied",
    "for constant hessian, hessians are not copied except for the root node",
    "there will be 2^exp_workgroups_per_feature = num_workgroups / num_dense_feature4 sub-histogram per feature4",
    "and we will launch num_feature workgroups for this kernel",
    "will launch threads for all features",
    "the queue should be asynchronous, and we will can WaitAndGetHistograms() before we start processing dense feature groups",
    "copy the results asynchronously. Size depends on if double precision is used",
    "we will wait for this object in WaitAndGetHistograms",
    "when the output is ready, the computation is done",
    "values of this feature has been redistributed to multiple bins; need a reduction here",
    "how many feature-group tuples we have",
    "leave some safe margin for prefetching",
    "256 work-items per workgroup. Each work-item prefetches one tuple for that feature",
    "clear sparse/dense maps",
    "do nothing if no features can be processed on GPU",
    "allocate memory for all features (FIXME: 4 GB barrier on some devices, need to split to multiple buffers)",
    "unpin old buffer if necessary before destructing them",
    "make ordered_gradients and Hessians larger (including extra room for prefetching), and pin them",
    "allocate space for gradients and Hessians on device",
    "we will copy gradients and Hessians in after ordered_gradients_ and ordered_hessians_ are constructed",
    "allocate feature mask, for disabling some feature-groups' histogram calculation",
    "copy indices to the device",
    "histogram bin entry size depends on the precision (single/double)",
    "create output buffer, each feature has a histogram with device_bin_size_ bins,",
    "each work group generates a sub-histogram of dword_features_ features.",
    "only initialize once here, as this will not need to change when ResetTrainingData() is called",
    "create atomic counters for inter-group coordination",
    "The output buffer is allocated to host directly, to overlap compute and data transfer",
    "find the dense feature-groups and group then into Feature4 data structure (several feature-groups packed into 4 bytes)",
    "looking for dword_features_ non-sparse feature-groups",
    "decide if we need to redistribute the bin",
    "multiplier must be a power of 2",
    "device_bin_mults_.push_back(1);",
    "found",
    "for data transfer time",
    "Now generate new data structure feature4, and copy data to the device",
    "preallocate arrays for all threads, and pin them",
    "building Feature4 bundles; each thread handles dword_features_ features",
    "one feature datapoint is 4 bits",
    "this guarantees that the RawGet() function is inlined, rather than using virtual function dispatching",
    "one feature datapoint is one byte",
    "this guarantees that the RawGet() function is inlined, rather than using virtual function dispatching",
    "Dense bin",
    "Dense 4-bit bin",
    "working on the remaining (less than dword_features_) feature groups",
    "fill the leftover features",
    "fill this empty feature with some \"random\" value",
    "fill this empty feature with some \"random\" value",
    "copying the last 1 to (dword_features - 1) feature-groups in the last tuple",
    "deallocate pinned space for feature copying",
    "data transfer time",
    "for other types of failure, build log might not be available; program.build_log() can crash",
    "Something bad happened. Just return \"No log available.\"",
    "build is okay, log may contain warnings",
    "destroy any old kernels",
    "create OpenCL kernels for different number of workgroups per feature",
    "currently we don't use constant memory",
    "compile the GPU kernel depending if double precision is used, constant hessian is used, etc.",
    "kernel with indices in an array",
    "kernel with all features enabled, with eliminated branches",
    "kernel with all data indices (for root node, and assumes that root node always uses all features)",
    "do nothing if no features can be processed on GPU",
    "The only argument that needs to be changed later is num_data_",
    "hessian is passed as a parameter, but it is not available now.",
    "hessian will be set in BeforeTrain()",
    "Get the max bin size, used for selecting best GPU kernel",
    "initialize GPU",
    "determine which kernel to use based on the max number of bins",
    "the +9 skips extra characters \")\", newline, \"#endif\" and newline at the beginning",
    "the +9 skips extra characters \")\", newline, \"#endif\" and newline at the beginning",
    "the +9 skips extra characters \")\", newline, \"#endif\" and newline at the beginning",
    "ignore the feature groups that contain categorical features when producing warnings about max_bin.",
    "these groups may contain larger number of bins due to categorical features, but not due to the setting of max_bin.",
    "setup GPU kernel arguments after we allocating all the buffers",
    "GPU memory has to been reallocated because data may have been changed",
    "setup GPU kernel arguments after we allocating all the buffers",
    "Copy initial full hessians and gradients to GPU.",
    "We start copying as early as possible, instead of at ConstructHistogram().",
    "setup hessian parameters only",
    "hessian is passed as a parameter",
    "use bagging",
    "On GPU, we start copying indices, gradients and Hessians now, instead at ConstructHistogram()",
    "copy used gradients and Hessians to ordered buffer",
    "transfer the indices to GPU",
    "transfer hessian to GPU",
    "setup hessian parameters only",
    "hessian is passed as a parameter",
    "transfer gradients to GPU",
    "only have root",
    "Copy indices, gradients and Hessians as early as possible",
    "only need to initialize for smaller leaf",
    "Get leaf boundary",
    "copy indices to the GPU:",
    "copy ordered Hessians to the GPU:",
    "copy ordered gradients to the GPU:",
    "do nothing if no features can be processed on GPU",
    "copy data indices if it is not null",
    "generate and copy ordered_gradients if gradients is not null",
    "generate and copy ordered_hessians if Hessians is not null",
    "converted indices in is_feature_used to feature-group indices",
    "construct the feature masks for dense feature-groups",
    "if no feature group is used, just return and do not use GPU",
    "if not all feature groups are used, we need to transfer the feature mask to GPU",
    "otherwise, we will use a specialized GPU kernel with all feature groups enabled",
    "All data have been prepared, now run the GPU kernel",
    "construct smaller leaf",
    "ConstructGPUHistogramsAsync will return true if there are available feature groups dispatched to GPU",
    "then construct sparse features on CPU",
    "wait for GPU to finish, only if GPU is actually used",
    "use double precision",
    "use single precision",
    "Compare GPU histogram with CPU histogram, useful for debugging GPU code problem",
    "#define GPU_DEBUG_COMPARE",
    "construct larger leaf",
    "then construct sparse features on CPU",
    "wait for GPU to finish, only if GPU is actually used",
    "use double precision",
    "use single precision",
    "do some sanity check for the GPU algorithm",
    "limit top k",
    "get max bin",
    "calculate buffer size",
    "need to be able to hold smaller and larger best splits in SyncUpGlobalBestSplit",
    "left and right on same time, so need double size",
    "initialize histograms for global",
    "sync global data sumup info",
    "set global sumup info",
    "init global data count in leaf",
    "get local sumup",
    "get local sumup",
    "get mean number on machines",
    "weighted gain",
    "get top k",
    "Copy histogram to buffer, and Get local aggregate features",
    "copy histograms.",
    "copy smaller leaf histograms first",
    "mark local aggregated feature",
    "copy",
    "then copy larger leaf histograms",
    "mark local aggregated feature",
    "copy",
    "use local data to find local best splits",
    "clear histogram buffer before synchronizing",
    "otherwise histogram contents from the previous iteration will be sent",
    "find splits",
    "only has root leaf",
    "local voting",
    "gather",
    "get all top-k from all machines",
    "global voting",
    "copy local histgrams to buffer",
    "Reduce scatter for histogram",
    "find best split from local aggregated histograms",
    "restore from buffer",
    "restore from buffer",
    "find local best",
    "find local best split for larger leaf",
    "sync global best info",
    "copy back",
    "set the global number of data for leaves",
    "init the global sumup info",
    "instantiate template classes, otherwise linker cannot find the code",
    "allocate CUDA memory",
    "leave some space for alignment",
    "input best split info",
    "for leaf information update",
    "gather information for CPU, used for launching kernels",
    "for leaf splits information update",
    "we need restore the order of indices in cuda_data_indices_",
    "allocate more memory for sum reduction in CUDA",
    "only the first element records the final sum",
    "intialize split find task information (a split find task is one pass through the histogram of a feature)",
    "need to double the size of histogram buffer in global memory when using double precision in histogram construction",
    "use only half the size of histogram buffer in global memory when quantized training since each gradient and hessian takes only 2 bytes",
    "use the first gpu by default",
    "std::max(..., 1UL) to avoid error in the case when there are NaN's in the categorical values",
    "use feature interaction constraint or sample features by node"
  ],
  "v4.4.0": [
    "coding: utf-8",
    "raise deprecation warnings if necessary",
    "ref: https://github.com/microsoft/LightGBM/issues/6435",
    "create predictor first",
    "setting early stopping via global params should be possible",
    "reduce cost for prediction training data",
    "process callbacks",
    "construct booster",
    "start training",
    "check evaluation result.",
    "ranking task, split according to groups",
    "run preprocessing on the data set if needed",
    "raise deprecation warnings if necessary",
    "ref: https://github.com/microsoft/LightGBM/issues/6435",
    "setting early stopping via global params should be possible",
    "setup callbacks",
    "coding: utf-8",
    "dummy function to support older version of scikit-learn",
    "catching 'ValueError' here because of this:",
    "https://github.com/microsoft/LightGBM/issues/6365#issuecomment-2002330003",
    "",
    "That's potentially risky as dask does some significant import-time processing,",
    "like loading configuration from environment variables and files, and catching",
    "ValueError here might hide issues with that config-loading.",
    "",
    "But in exchange, it's less likely that 'import lightgbm' will fail for",
    "dask-related reasons, which is beneficial for any workloads that are using",
    "lightgbm but not its Dask functionality.",
    "coding: utf-8",
    "f(labels, preds)",
    "f(labels, preds, weights)",
    "f(labels, preds, weights, group)",
    "f(labels, preds)",
    "f(labels, preds, weights)",
    "f(labels, preds, weights, group)",
    "documentation templates for LGBMModel methods are shared between the classes in",
    "this module and those in the ``dask`` module",
    "It's possible, for example, to pass 3 eval sets through `eval_set`,",
    "but only 1 init_score through `eval_init_score`.",
    "",
    "This if-else accounts for that possiblity.",
    "register default metric for consistency with callable eval_metric case",
    "try to deduce from class instance",
    "overwrite default metric by explicitly set metric",
    "use joblib conventions for negative n_jobs, just like scikit-learn",
    "at predict time, this is handled later due to the order of parameter updates",
    "Do not modify original args in fit function",
    "Refer to https://github.com/microsoft/LightGBM/pull/2619",
    "Separate built-in from callable evaluation metrics",
    "concatenate metric from params (or default if not provided in params) and eval_metric",
    "copy for consistency",
    "reduce cost for prediction training data",
    "free dataset",
    "retrive original params that possibly can be used in both training and prediction",
    "and then overwrite them (considering aliases) with params that were passed directly in prediction",
    "number of threads can have values with special meaning which is only applied",
    "in the scikit-learn interface, these should not reach the c++ side as-is",
    "adjust eval metrics to match whether binary or multiclass",
    "classification is being performed",
    "do not modify args, as it causes errors in model selection tools",
    "check group data",
    "coding: utf-8",
    "coding: utf-8",
    "\"#ddffdd\" is light green, \"#ffdddd\" is light red",
    "coding: utf-8",
    "coding: utf-8",
    "typing.TypeGuard was only introduced in Python 3.10",
    "we don't need lib_lightgbm while building docs",
    "TypeError: obj is not a string or a number",
    "ValueError: invalid literal",
    "Obtain objects to export",
    "Prepare export",
    "Export all objects",
    "DeprecationWarning is not shown by default, so let's create our own with higher level",
    "ref: https://peps.python.org/pep-0565/#additional-use-case-for-futurewarning",
    "lazy evaluation to allow import without dynamic library, e.g., for docs generation",
    "if buffer length is not long enough, re-allocate a buffer",
    "avoid side effects on passed-in parameters",
    "if main_param_name was provided, keep that value and remove all aliases",
    "if main param name was not found, search for an alias",
    "neither of main_param_name, aliases were found",
    "most common case (no nullable dtypes)",
    "1.0 <= pd version < 1.1 and nullable dtypes, least common case",
    "raises error because array is casted to type(pd.NA) and there's no na_value argument",
    "data has nullable dtypes, but we can specify na_value argument and copy will be made",
    "take shallow copy in case we modify categorical columns",
    "whole column modifications don't change the original df",
    "determine feature names",
    "determine categorical features",
    "use cat cols from DataFrame",
    "so that the target dtype considers floats",
    "Get total row number.",
    "Random access by row index. Used for data sampling.",
    "Range data access. Used to read data in batch when constructing Dataset.",
    "Optionally specify batch_size to control range data read size.",
    "Only required if using ``Dataset.subset()``.",
    "__get_num_preds() cannot work with nrow > MAX_INT32, so calculate overall number of predictions piecemeal",
    "avoid memory consumption by arrays concatenation operations",
    "create numpy array from output arrays",
    "break up indptr based on number of rows (note more than one matrix in multiclass case)",
    "for CSC there is extra column added",
    "reformat output into a csr or csc matrix or list of csr or csc matrices",
    "same shape as input csr or csc matrix except extra column for expected value",
    "note: make sure we copy data as it will be deallocated next",
    "free the temporary native indptr, indices, and data",
    "__get_num_preds() cannot work with nrow > MAX_INT32, so calculate overall number of predictions piecemeal",
    "avoid memory consumption by arrays concatenation operations",
    "Check that the input is valid: we only handle numbers (for now)",
    "Prepare prediction output array",
    "Export Arrow table to C and run prediction",
    "c type: double**",
    "each double* element points to start of each column of sample data.",
    "c type int**",
    "each int* points to start of indices for each column",
    "no min_data, nthreads and verbose in this function",
    "check data has header or not",
    "need to regroup init_score",
    "process for args",
    "get categorical features",
    "If the params[cat_alias] is equal to categorical_indices, do not report the warning.",
    "process for reference dataset",
    "start construct data",
    "set feature names",
    "Select sampled rows, transpose to column order.",
    "create validation dataset from ref_dataset",
    "Check that the input is valid: we only handle numbers (for now)",
    "Export Arrow table to C",
    "create valid",
    "construct subset",
    "create train",
    "could be updated if data is not freed",
    "set to None",
    "If the data is a arrow data, we can just pass it to C",
    "If a table is being passed, we concatenate the columns. This is only valid for",
    "'init_score'.",
    "we're done if self and reference share a common upstream reference",
    "Check if the weight contains values other than one",
    "Set field",
    "original values can be modified at cpp side",
    "if buffer length is not long enough, reallocate buffers",
    "group data from LightGBM is boundaries data, need to convert to group size",
    "Training task",
    "if \"machines\" is given, assume user wants to do distributed learning, and set up network",
    "construct booster object",
    "copy the parameters from train_set",
    "save reference to data",
    "buffer for inner predict",
    "Prediction task",
    "if buffer length is not long enough, re-allocate a buffer",
    "if a single node tree it won't have `leaf_index` so return 0",
    "Create the node record, and populate universal data members",
    "Update values to reflect node type (leaf or split)",
    "traverse the next level of the tree",
    "In tree format, \"subtree_list\" is a list of node records (dicts),",
    "and we add node to the list.",
    "need reset training data",
    "need to push new valid data",
    "ensure that existing Booster is freed before replacing it",
    "with a new one createdfrom file",
    "if buffer length is not long enough, re-allocate a buffer",
    "if buffer length is not long enough, reallocate a buffer",
    "Copy models",
    "Get name of features",
    "if buffer length is not long enough, reallocate buffers",
    "avoid to predict many time in one iteration",
    "Get num of inner evals",
    "Get name of eval metrics",
    "if buffer length is not long enough, reallocate buffers",
    "coding: utf-8",
    "Callback environment used by callbacks",
    "CVBooster holds a list of Booster objects, each needs to be updated",
    "for lgb.cv() with eval_train_metric=True, evaluation is also done on the training set",
    "and those metrics are considered for early stopping",
    "for lgb.train(), it's possible to pass the training data via valid_sets with any eval_name",
    "validation sets are guaranteed to not be identical to the training data in cv()",
    "split is needed for \"<dataset type> <metric>\" case (e.g. \"train l1\")",
    "self.best_score_list is initialized to an empty list",
    "split is needed for \"<dataset type> <metric>\" case (e.g. \"train l1\")",
    "coding: utf-8",
    "Acquire port in worker",
    "schedule futures to retrieve each element of the tuple",
    "retrieve ports",
    "Concatenate many parts into one",
    "construct local eval_set data.",
    "store indices of eval_set components that were not contained within local parts.",
    "consolidate parts of each individual eval component.",
    "require that eval_name exists in evaluated result data in case dropped due to padding.",
    "in distributed training the 'training' eval_set is not detected, will have name 'valid_<index>'.",
    "filter padding from eval parts then _concat each eval_set component.",
    "reconstruct eval_set fit args/kwargs depending on which components of eval_set are on worker.",
    "ensure that expected keys for evals_result_ and best_score_ exist regardless of padding.",
    "capture whether local_listen_port or its aliases were provided",
    "capture whether machines or its aliases were provided",
    "Some passed-in parameters can be removed:",
    "* 'num_machines': set automatically from Dask worker list",
    "* 'num_threads': overridden to match nthreads on each Dask process",
    "Split arrays/dataframes into parts. Arrange parts into dicts to enforce co-locality",
    "evals_set will to be re-constructed into smaller lists of (X, y) tuples, where",
    "X and y are each delayed sub-lists of original eval dask Collections.",
    "find maximum number of parts in an individual eval set so that we can",
    "pad eval sets when they come in different sizes.",
    "when individual eval set is equivalent to training data, skip recomputing parts.",
    "add None-padding for individual eval_set member if it is smaller than the largest member.",
    "first time a chunk of this eval set is added to this part.",
    "append additional chunks of this eval set to this part.",
    "ensure that all evaluation parts map uniquely to one part.",
    "assign sub-eval_set components to worker parts.",
    "Start computation in the background",
    "trigger error locally",
    "Find locations of all parts and map them to particular Dask workers",
    "Check that all workers were provided some of eval_set. Otherwise warn user that validation",
    "data artifacts may not be populated depending on worker returning final estimator.",
    "assign general validation set settings to fit kwargs.",
    "resolve aliases for network parameters and pop the result off params.",
    "these values are added back in calls to `_train_part()`",
    "figure out network params",
    "Tell each worker to train on the parts that it has locally",
    "",
    "This code treats ``_train_part()`` calls as not \"pure\" because:",
    "1. there is randomness in the training process unless parameters ``seed``",
    "and ``deterministic`` are set",
    "2. even with those parameters set, the output of one ``_train_part()`` call",
    "relies on global state (it and all the other LightGBM training processes",
    "coordinate with each other)",
    "if network parameters were changed during training, remove them from the",
    "returned model so that they're generated dynamically on every run based",
    "on the Dask cluster you're connected to and which workers have pieces of",
    "the training data",
    "dask.DataFrame.map_partitions() expects each call to return a pandas DataFrame or Series",
    "for multi-class classification with sparse matrices, pred_contrib predictions",
    "are returned as a list of sparse matrices (one per class)",
    "pred_contrib output will have one column per feature,",
    "plus one more for the base value",
    "need to tell Dask the expected type and shape of individual preds",
    "by default, dask.array.concatenate() concatenates sparse arrays into a COO matrix",
    "the code below is used instead to ensure that the sparse type is preserved during concatentation",
    "At this point, `out` is a list of lists of delayeds (each of which points to a matrix).",
    "Concatenate them to return a list of Dask Arrays.",
    "DaskLGBMClassifier does not support group, eval_group.",
    "DaskLGBMClassifier support for callbacks and init_model is not tested",
    "DaskLGBMRegressor does not support group, eval_class_weight, eval_group.",
    "DaskLGBMRegressor support for callbacks and init_model is not tested",
    "DaskLGBMRanker does not support eval_class_weight or early stopping",
    "DaskLGBMRanker support for callbacks and init_model is not tested",
    "coding: utf-8",
    "load or create your dataset",
    "generate feature names",
    "create dataset for lightgbm",
    "if you want to re-use data, remember to set free_raw_data=False",
    "specify your configurations as a dict",
    "feature_name and categorical_feature",
    "check feature name",
    "save model to file",
    "dump model to JSON (and save to file)",
    "feature names",
    "feature importances",
    "load model to predict",
    "can only predict with the best iteration (or the saving iteration)",
    "eval with loaded model",
    "dump model with pickle",
    "load model with pickle to predict",
    "can predict with any iteration when loaded in pickle way",
    "eval with loaded model",
    "continue training",
    "init_model accepts:",
    "1. model file name",
    "2. Booster()",
    "decay learning rates",
    "reset_parameter callback accepts:",
    "1. list with length = num_boost_round",
    "2. function(curr_iter)",
    "change other parameters during training",
    "self-defined objective function",
    "f(preds: array, train_data: Dataset) -> grad: array, hess: array",
    "log likelihood loss",
    "self-defined eval metric",
    "f(preds: array, train_data: Dataset) -> name: str, eval_result: float, is_higher_better: bool",
    "binary error",
    "NOTE: when you do customized loss function, the default prediction value is margin",
    "This may make built-in evaluation metric calculate wrong results",
    "For example, we are doing log likelihood loss, the prediction is score before logistic transformation",
    "Keep this in mind when you use the customization",
    "Pass custom objective function through params",
    "another self-defined eval metric",
    "f(preds: array, train_data: Dataset) -> name: str, eval_result: float, is_higher_better: bool",
    "accuracy",
    "NOTE: when you do customized loss function, the default prediction value is margin",
    "This may make built-in evaluation metric calculate wrong results",
    "For example, we are doing log likelihood loss, the prediction is score before logistic transformation",
    "Keep this in mind when you use the customization",
    "Pass custom objective function through params",
    "callback",
    "coding: utf-8",
    "load or create your dataset",
    "train",
    "predict",
    "eval",
    "feature importances",
    "self-defined eval metric",
    "f(y_true: array, y_pred: array) -> name: str, eval_result: float, is_higher_better: bool",
    "Root Mean Squared Logarithmic Error (RMSLE)",
    "train",
    "another self-defined eval metric",
    "f(y_true: array, y_pred: array) -> name: str, eval_result: float, is_higher_better: bool",
    "Relative Absolute Error (RAE)",
    "train",
    "predict",
    "eval",
    "other scikit-learn modules",
    "coding: utf-8",
    "load or create your dataset",
    "create dataset for lightgbm",
    "specify your configurations as a dict",
    "train",
    "coding: utf-8",
    "################",
    "Simulate some binary data with a single categorical and",
    "single continuous predictor",
    "################",
    "Set up a couple of utilities for our experiments",
    "################",
    "Observe the behavior of `binary` and `xentropy` objectives",
    "Trying this throws an error on non-binary values of y:",
    "experiment('binary', label_type='probability', DATA)",
    "The speed of `binary` is not drastically different than",
    "`xentropy`. `xentropy` runs faster than `binary` in many cases, although",
    "there are reasons to suspect that `binary` should run faster when the",
    "label is an integer instead of a float",
    "coding: utf-8",
    "load or create your dataset",
    "create dataset for lightgbm",
    "specify your configurations as a dict",
    "train",
    "save model to file",
    "predict",
    "eval",
    "We can also open HDF5 file once and get access to",
    "With binary dataset created, we can use either Python API or cmdline version to train.",
    "",
    "Note: in order to create exactly the same dataset with the one created in simple_example.py, we need",
    "to modify simple_example.py to pass numpy array instead of pandas DataFrame to Dataset constructor.",
    "The reason is that DataFrame column names will be used in Dataset. For a DataFrame with Int64Index",
    "as columns, Dataset will use column names like [\"0\", \"1\", \"2\", ...]. While for numpy array, column names",
    "are using the default one assigned in C++ code (dataset_loader.cpp), like [\"Column_0\", \"Column_1\", ...].",
    "Y has a single column and we read it in single shot. So store it as an 1-d array.",
    "We use random access for data sampling when creating LightGBM Dataset from Sequence.",
    "When accessing any element in a HDF5 chunk, it's read entirely.",
    "To save I/O for sampling, we should keep number of total chunks much larger than sample count.",
    "Here we are just creating a chunk size that matches with batch_size.",
    "",
    "Also note that the data is stored in row major order to avoid extra copy when passing to",
    "lightgbm Dataset.",
    "Save to 2 HDF5 files for demonstration.",
    "We can store multiple datasets inside a single HDF5 file.",
    "Separating X and Y for choosing best chunk size for data loading.",
    "split training data into two partitions",
    "make this array dense because we're splitting across",
    "a sparse boundary to partition the data",
    "the code below uses sklearn.metrics, but this requires pulling all of the",
    "predictions and target values back from workers to the client",
    "",
    "for larger datasets, consider the metrics from dask-ml instead",
    "https://ml.dask.org/modules/api.html#dask-ml-metrics-metrics",
    "coding: utf-8",
    "!/usr/bin/env python3",
    "-*- coding: utf-8 -*-",
    "",
    "LightGBM documentation build configuration file, created by",
    "sphinx-quickstart on Thu May  4 14:30:58 2017.",
    "",
    "This file is execfile()d with the current directory set to its",
    "containing dir.",
    "",
    "Note that not all possible configuration values are present in this",
    "autogenerated file.",
    "",
    "All configuration values have a default; values that are commented out",
    "serve to show the default.",
    "If extensions (or modules to document with autodoc) are in another directory,",
    "add these directories to sys.path here. If the directory is relative to the",
    "documentation root, use os.path.abspath to make it absolute.",
    "-- General configuration ------------------------------------------------",
    "If your documentation needs a minimal Sphinx version, state it here.",
    "Add any Sphinx extension module names here, as strings. They can be",
    "extensions coming with Sphinx (named 'sphinx.ext.*') or your custom",
    "ones.",
    "mock out modules",
    "hide type hints in API docs",
    "Generate autosummary pages. Output should be set with: `:toctree: pythonapi/`",
    "Only the class' docstring is inserted.",
    "If true, `todo` and `todoList` produce output, else they produce nothing.",
    "The master toctree document.",
    "General information about the project.",
    "The name of an image file (relative to this directory) to place at the top",
    "of the sidebar.",
    "The name of an image file (relative to this directory) to use as a favicon of",
    "the docs.  This file should be a Windows icon file (.ico) being 16x16 or 32x32",
    "pixels large.",
    "The version info for the project you're documenting, acts as replacement for",
    "|version| and |release|, also used in various other places throughout the",
    "built documents.",
    "The short X.Y version.",
    "The full version, including alpha/beta/rc tags.",
    "The language for content autogenerated by Sphinx. Refer to documentation",
    "for a list of supported languages.",
    "",
    "This is also used if you do content translation via gettext catalogs.",
    "Usually you set \"language\" from the command line for these cases.",
    "List of patterns, relative to source directory, that match files and",
    "directories to ignore when looking for source files.",
    "This patterns also effect to html_static_path and html_extra_path",
    "The name of the Pygments (syntax highlighting) style to use.",
    "-- Configuration for C API docs generation ------------------------------",
    "-- Options for HTML output ----------------------------------------------",
    "The theme to use for HTML and HTML Help pages.  See the documentation for",
    "a list of builtin themes.",
    "Theme options are theme-specific and customize the look and feel of a theme",
    "further.  For a list of options available for each theme, see the",
    "documentation.",
    "Add any paths that contain custom static files (such as style sheets) here,",
    "relative to this directory. They are copied after the builtin static files,",
    "so a file named \"default.css\" will overwrite the builtin \"default.css\".",
    "-- Options for HTMLHelp output ------------------------------------------",
    "Output file base name for HTML help builder.",
    "-- Options for LaTeX output ---------------------------------------------",
    "The name of an image file (relative to this directory) to place at the top of",
    "the title page.",
    "intersphinx configuration",
    "Warning! The following code can cause buffer overflows on RTD.",
    "Consider suppressing output completely if RTD project silently fails.",
    "Refer to https://github.com/svenevs/exhale",
    "/blob/fe7644829057af622e467bb529db6c03a830da99/exhale/deploy.py#L99-L111",
    "Warning! The following code can cause buffer overflows on RTD.",
    "Consider suppressing output completely if RTD project silently fails.",
    "Refer to https://github.com/svenevs/exhale",
    "/blob/fe7644829057af622e467bb529db6c03a830da99/exhale/deploy.py#L99-L111",
    "coding: utf-8",
    "This is a basic test for floating number parsing.",
    "Most of the test cases come from:",
    "https://github.com/dmlc/xgboost/blob/master/tests/cpp/common/test_charconv.cc",
    "https://github.com/Alexhuszagh/rust-lexical/blob/master/data/test-parse-unittests/strtod_tests.toml",
    "FLT_MAX",
    "FLT_MIN",
    "DBL_MAX (1 + (1 - 2^-52)) * 2^1023 = (2^53 - 1) * 2^971",
    "2^971 * (2^53 - 1 + 1/2) : the smallest number resolving to inf",
    "Near DBL_MIN",
    "DBL_MIN 2^-1022",
    "The behavior for parsing -nan depends on implementation.",
    "Thus we skip binary check for negative nan.",
    "See comment in test_cases.",
    "construct sample data first (use all data for convenience and since size is small)",
    "Load some test data",
    "Use the smaller \".test\" data because we don't care about the actual data and it's smaller",
    "Add some fake initial_scores and groups so we can test streaming them",
    "Now use the reference dataset schema to make some testable Datasets with N rows each",
    "Load some test data",
    "Use the smaller \".test\" data because we don't care about the actual data and it's smaller",
    "Add some fake initial_scores and groups so we can test streaming them",
    "Now use the reference dataset schema to make some testable Datasets with N rows each",
    "This code is copied and adapted from the official Arrow producer examples:",
    "https://arrow.apache.org/docs/format/CDataInterface.html#exporting-a-struct-float32-utf8-array",
    "Free children",
    "Finalize",
    "Free children",
    "Free buffers",
    "Finalize",
    "NOTE: Arrow arrays have 64-bit alignment but we can safely ignore this in tests",
    "By using `calloc` above, we only need to set 'true' values",
    "Arithmetic",
    "Subscripts",
    "End",
    "Check for values in first chunk",
    "Check for some values in second chunk",
    "Check end",
    "Load some test data",
    "Serialize the reference",
    "Deserialize the reference",
    "Confirm 1 successful API call",
    "Free memory",
    "Load some test data",
    "Run a single row prediction and compare with regular Mat prediction:",
    "Drop the result from the dataset, we only care about checking that prediction results are equal",
    "in both cases",
    "Now let's run with the single row fast prediction API:",
    "Free all:",
    "Test that Data() points to first value written",
    "Constants",
    "Start with some content:",
    "Clear & re-use:",
    "Output should match new content:",
    "Number of trials for each new ChunkedArray configuration. Pass 100 times over the search space:",
    "Each outer loop iteration changes the test by adding +1 chunk. We start with 1 chunk only:",
    "Sweep valid and invalid addresses with a ChunkedArray with `chunks` chunks:",
    "Compute a new trial address & value & if it is a valid address:",
    "Insert item. If at a valid address, 0 is returned, otherwise, -1 is returned:",
    "If at valid address, check that the stored value is correct & remember it for the future:",
    "Check the just-stored value with getitem():",
    "Also store the just-stored value for future tracking:",
    "Final check: ensure even with overrides, all valid insertions store the latest value at that address:",
    "Test in 2 ways that the values are correctly laid out in memory:",
    "Since init_scores are in a column format, but need to be pushed as rows, we have to extract each batch",
    "Use multiple threads to test concurrency",
    "Since init_scores are in a column format, but need to be pushed as rows, we have to extract each batch",
    "Calculate expected boundaries",
    "Extract a set of rows from the column-based format (still maintaining column based format)",
    "coding: utf-8",
    "at initialization, should be -1",
    "updating that value through the C API should work",
    "resetting to any negative number should set it to -1",
    "coding: utf-8",
    "check saved model persistence",
    "we need to check the consistency of model file here, so test for exact equal",
    "check early stopping is working. Make it stop very early, so the scores should be very close to zero",
    "scores likely to be different, but prediction should still be the same",
    "test that shape is checked during prediction",
    "The simple implementation is just a single \"return self.ndarray[idx]\"",
    "The following is for demo and testing purpose.",
    "whole col",
    "half col",
    "Create dataset from numpy array directly.",
    "Create dataset using Sequence.",
    "Test for validation set.",
    "Select some random rows as valid data.",
    "From Dataset constructor, with dataset from numpy array.",
    "From Dataset.create_valid, with dataset from sequence.",
    "test that method works even with free_raw_data=True",
    "test that method works but sets raw data to None in case of immergeable data types",
    "test that method works for different data types",
    "Set extremely harsh penalties, so CEGB will block most splits.",
    "Compare pairs of penalties, to ensure scaling works as intended",
    "Reset booster1's parameters to p2, so the parameter section of the file matches.",
    "unconstructed, get_* methods should return whatever was provided",
    "before construction, get_field() should raise an exception",
    "constructed, get_* methods should return numpy arrays, even when the provided",
    "input was a list of floats or ints",
    "get_field(\"group\") returns a numpy array with boundaries, instead of size",
    "NOTE: \"position\" is converted to int32 on the C++ side",
    "should resolve duplicate aliases, and prefer the main parameter",
    "should choose the highest priority alias and set that value on main param",
    "if only aliases are used",
    "should use the default if main param and aliases are missing",
    "all changes should be made on copies and not modify the original",
    "preserves None found for main param and still removes aliases",
    "correctly chooses value when only an alias is provided",
    "adds None if that's given as the default and param not found",
    "If callable is found in objective",
    "Value in params should be preferred to the default_value passed from keyword arguments",
    "None of objective or its aliases in params, but default_value is callable.",
    "\"bad\" = 1 element too many",
    "copy=False is necessary because starting with pandas 3.0, pd.DataFrame() creates",
    "a copy of the input numpy array by default",
    "ref: https://github.com/pandas-dev/pandas/issues/58913",
    "check that the original data wasn't modified",
    "check that the built data has the codes",
    "if all categories were seen during training we just take the codes",
    "if we only saw 'a' during training we just replace its code",
    "and leave the rest as nan",
    "test using defined feature names",
    "test using default feature names",
    "check for feature indices outside of range",
    "NOTE: this intentionally contains values where num_leaves <, ==, and > (max_depth^2)",
    "NOTE: max_depth < 5 is significant here because the default for num_leaves=31. With max_depth=5,",
    "a full depth-wise tree would have 2^5 = 32 leaves.",
    "coding: utf-8",
    "add target, weight, and group to DataFrame so that partitions abide by group boundaries.",
    "set_index ensures partitions are based on group id.",
    "See https://stackoverflow.com/questions/49532824/dask-dataframe-split-partitions-based-on-a-column-or-function.",
    "separate target, weight from features.",
    "encode group identifiers into run-length encoding, the format LightGBMRanker is expecting",
    "so that within each partition, sum(g) = n_samples.",
    "ranking arrays: one chunk per group. Each chunk must include all columns.",
    "make one categorical feature relevant to the target",
    "https://github.com/microsoft/LightGBM/issues/4118",
    "extra predict() parameters should be passed through correctly",
    "pref_leaf values should have the right shape",
    "and values that look like valid tree nodes",
    "be sure LightGBM actually used at least one categorical column,",
    "and that it was correctly treated as a categorical feature",
    "shape depends on whether it is binary or multiclass classification",
    "in the special case of multi-class classification using scipy sparse matrices,",
    "the output of `.predict(..., pred_contrib=True)` is a list of sparse matrices (one per class)",
    "",
    "since that case is so different than all other cases, check the relevant things here",
    "and then return early",
    "raw scores will probably be different, but at least check that all predicted classes are the same",
    "be sure LightGBM actually used at least one categorical column,",
    "and that it was correctly treated as a categorical feature",
    "* shape depends on whether it is binary or multiclass classification",
    "* matrix for binary classification is of the form [feature_contrib, base_value],",
    "for multi-class it's [feat_contrib_class1, base_value_class1, feat_contrib_class2, base_value_class2, etc.]",
    "* contrib outputs for distributed training are different than from local training, so we can just test",
    "that the output has the right shape and base values are in the right position",
    "with a custom objective, prediction result is a raw score instead of predicted class",
    "function should have been preserved",
    "should correctly classify every sample",
    "probability estimates should be similar",
    "Scores should be the same",
    "Predictions should be roughly the same.",
    "pref_leaf values should have the right shape",
    "and values that look like valid tree nodes",
    "extra predict() parameters should be passed through correctly",
    "be sure LightGBM actually used at least one categorical column,",
    "and that it was correctly treated as a categorical feature",
    "contrib outputs for distributed training are different than from local training, so we can just test",
    "that the output has the right shape and base values are in the right position",
    "be sure LightGBM actually used at least one categorical column,",
    "and that it was correctly treated as a categorical feature",
    "Quantiles should be right",
    "be sure LightGBM actually used at least one categorical column,",
    "and that it was correctly treated as a categorical feature",
    "function should have been preserved",
    "Scores should be the same",
    "local and Dask predictions should be the same",
    "predictions should be better than random",
    "rebalance small dask.Array dataset for better performance.",
    "use many trees + leaves to overfit, help ensure that Dask data-parallel strategy matches that of",
    "serial learner. See https://github.com/microsoft/LightGBM/issues/3292#issuecomment-671288210.",
    "distributed ranker should be able to rank decently well and should",
    "have high rank correlation with scores from serial ranker.",
    "extra predict() parameters should be passed through correctly",
    "pref_leaf values should have the right shape",
    "and values that look like valid tree nodes",
    "be sure LightGBM actually used at least one categorical column,",
    "and that it was correctly treated as a categorical feature",
    "rebalance small dask.Array dataset for better performance.",
    "distributed ranker should be able to rank decently well with the least-squares objective",
    "and should have high rank correlation with scores from serial ranker.",
    "function should have been preserved",
    "Use larger trainset to prevent premature stopping due to zero loss, causing num_trees() < n_estimators.",
    "Use small chunk_size to avoid single-worker allocation of eval data partitions.",
    "test eval_class_weight, eval_init_score on binary-classification task.",
    "Note: objective's default `metric` will be evaluated in evals_result_ in addition to all eval_metrics.",
    "create eval_sets by creating new datasets or copying training data.",
    "total number of trees scales up for ova classifier.",
    "check that early stopping was not applied.",
    "checks that evals_result_ and best_score_ contain expected data and eval_set names.",
    "check that each eval_name and metric exists for all eval sets, allowing for the",
    "case when a worker receives a fully-padded eval_set component which is not evaluated.",
    "should be able to use the class without specifying a client",
    "should be able to set client after construction",
    "data on cluster1",
    "create identical data on cluster2",
    "at this point, the result of default_client() is client2 since it was the most recently",
    "created. So setting client to client1 here to test that you can select a non-default client",
    "unfitted model should survive pickling round trip, and pickling",
    "shouldn't have side effects on the model object",
    "client will always be None after unpickling",
    "fitted model should survive pickling round trip, and pickling",
    "shouldn't have side effects on the model object",
    "client will always be None after unpickling",
    "rebalance data to be sure that each worker has a piece of the data",
    "model 1 - no network parameters given",
    "model 2 - machines given",
    "model 3 - local_listen_port given",
    "training should fail because LightGBM will try to use the same",
    "port for multiple worker processes on the same machine",
    "rebalance data to be sure that each worker has a piece of the data",
    "test that \"machines\" is actually respected by creating a socket that uses",
    "one of the ports mentioned in \"machines\"",
    "The above error leaves a worker waiting",
    "an informative error should be raised if \"machines\" has duplicates",
    "\"client\" should be the only different, and the final argument",
    "value of the root node is 0 when init_score is set",
    "this test is separate because it takes a not-yet-constructed estimator",
    "coding: utf-8",
    "coding: utf-8",
    "build target, group ID vectors.",
    "build y/target and group-id vectors with user-specified group sizes.",
    "build y/target and group-id vectors according to n_samples, avg_gs, and random_gs.",
    "groups should contain > 1 element for pairwise learning objective.",
    "build feature data, X. Transform first few into informative features.",
    "doing this here, at import time, to ensure it only runs once_per import",
    "instead of once per assertion",
    "coding: utf-8",
    "NOTE: In the AppVeyor CI, importing pyarrow fails due to an old Visual Studio version. Hence,",
    "we conditionally import pyarrow here (and skip tests if it cannot be imported). However, we",
    "don't want these tests to silently be skipped, hence, we only conditionally import when a",
    "specific env var is set.",
    "----------------------------------------------------------------------------------------------- #",
    "UTILITIES                                            #",
    "----------------------------------------------------------------------------------------------- #",
    "Set random nulls",
    "Split data into <=2 random chunks",
    "Turn chunks into array",
    "----------------------------------------------------------------------------------------------- #",
    "UNIT TESTS                                           #",
    "----------------------------------------------------------------------------------------------- #",
    "------------------------------------------- DATASET ------------------------------------------- #",
    "-------------------------------------------- FIELDS ------------------------------------------- #",
    "Check for equality",
    "-------------------------------------------- LABELS ------------------------------------------- #",
    "------------------------------------------- WEIGHTS ------------------------------------------- #",
    "-------------------------------------------- GROUPS ------------------------------------------- #",
    "----------------------------------------- INIT SCORES ----------------------------------------- #",
    "------------------------------------------ PREDICTION ----------------------------------------- #",
    "coding: utf-8",
    "check that really dummy objective was used and estimator didn't learn anything",
    "prediction result is actually not transformed (is raw) due to custom objective",
    "original estimator is unaffected",
    "new estimator is unfitted, but has the same parameters",
    "Test if random_state is properly stored",
    "Test if two random states produce identical models",
    "Test if subsequent fits sample from random_state object and produce different models",
    "Test that the largest element is NOT the same, the smallest can be the same, i.e. zero",
    "why fixed seed?",
    "sometimes there is no difference how cols are treated (cat or not cat)",
    "With default params",
    "Tests same probabilities",
    "Tests same predictions",
    "Tests same raw scores",
    "Tests same leaf indices",
    "Tests same feature contributions",
    "Tests other parameters for the prediction works",
    "Tests start_iteration",
    "Tests same probabilities, starting from iteration 10",
    "Tests same predictions, starting from iteration 10",
    "Tests same raw scores, starting from iteration 10",
    "Tests same leaf indices, starting from iteration 10",
    "Tests same feature contributions, starting from iteration 10",
    "Tests other parameters for the prediction works, starting from iteration 10",
    "test that params passed in predict have higher priority",
    "no custom objective, no custom metric",
    "default metric",
    "non-default metric",
    "no metric",
    "non-default metric in eval_metric",
    "non-default metric with non-default metric in eval_metric",
    "non-default metric with multiple metrics in eval_metric",
    "non-default metric with multiple metrics in eval_metric for LGBMClassifier",
    "default metric for non-default objective",
    "non-default metric for non-default objective",
    "no metric",
    "non-default metric in eval_metric for non-default objective",
    "non-default metric with non-default metric in eval_metric for non-default objective",
    "non-default metric with multiple metrics in eval_metric for non-default objective",
    "custom objective, no custom metric",
    "default regression metric for custom objective",
    "non-default regression metric for custom objective",
    "multiple regression metrics for custom objective",
    "no metric",
    "default regression metric with non-default metric in eval_metric for custom objective",
    "non-default regression metric with metric in eval_metric for custom objective",
    "multiple regression metrics with metric in eval_metric for custom objective",
    "multiple regression metrics with multiple metrics in eval_metric for custom objective",
    "no custom objective, custom metric",
    "default metric with custom metric",
    "non-default metric with custom metric",
    "multiple metrics with custom metric",
    "custom metric (disable default metric)",
    "default metric for non-default objective with custom metric",
    "non-default metric for non-default objective with custom metric",
    "multiple metrics for non-default objective with custom metric",
    "custom metric (disable default metric for non-default objective)",
    "custom objective, custom metric",
    "custom metric for custom objective",
    "non-default regression metric with custom metric for custom objective",
    "multiple regression metrics with custom metric for custom objective",
    "default metric and invalid binary metric is replaced with multiclass alternative",
    "invalid binary metric is replaced with multiclass alternative",
    "default metric for non-default multiclass objective",
    "and invalid binary metric is replaced with multiclass alternative",
    "default metric and invalid multiclass metric is replaced with binary alternative",
    "invalid multiclass metric is replaced with binary alternative for custom objective",
    "Verify that can receive a list of metrics, only callable",
    "Verify that can receive a list of custom and built-in metrics",
    "Verify that works as expected when eval_metric is empty",
    "Verify that can receive a list of metrics, only built-in",
    "Verify that eval_metric is robust to receiving a list with None",
    "feval",
    "single eval_set",
    "two eval_set",
    "'val_minus_two' here is the expected number of threads for n_jobs=-2",
    "Note: according to joblib's formula, a value of n_jobs=-2 means",
    "\"use all but one thread\" (formula: n_cpus + 1 + n_jobs)",
    "try to predict with a different feature",
    "check that disabling the check doesn't raise the error",
    "make weights and init_score same types as y, just to avoid",
    "a huge number of combinations and therefore test cases",
    "make weights and init_score same types as y, just to avoid",
    "a huge number of combinations and therefore test cases",
    "coding: utf-8",
    "we're in a leaf now",
    "check that the rest of the elements have black color",
    "check that we got to the expected leaf",
    "coding: utf-8",
    "coding: utf-8",
    "check that default gives same result as k = 1",
    "check against independent calculation for k = 1",
    "check against independent calculation for k = 2",
    "check against independent calculation for k = 10",
    "check cases where predictions are equal",
    "should give same result as binary auc for 2 classes",
    "test the case where all predictions are equal",
    "test that weighted data gives different auc_mu",
    "test that equal data weights give same auc_mu as unweighted data",
    "should give 1 when accuracy = 1",
    "test loading class weights",
    "Simulates position bias for a given ranking dataset.",
    "The ouput dataset is identical to the input one with the exception for the relevance labels.",
    "The new labels are generated according to an instance of a cascade user model:",
    "for each query, the user is simulated to be traversing the list of documents ranked by a baseline ranker",
    "(in our example it is simply the ordering by some feature correlated with relevance, e.g., 34)",
    "and clicks on that document (new_label=1) with some probability 'pclick' depending on its true relevance;",
    "at each position the user may stop the traversal with some probability pstop. For the non-clicked documents,",
    "new_label=0. Thus the generated new labels are biased towards the baseline ranker.",
    "The positions of the documents in the ranked lists produced by the baseline, are returned.",
    "a mapping of a document's true relevance (defined on a 5-grade scale) into the probability of clicking it",
    "an instantiation of a cascade model where the user stops with probability 0.2 after observing each document",
    "simulate position bias for the train dataset and put the train dataset with biased labels to temp directory",
    "the performance of the unbiased LambdaMART should outperform the plain LambdaMART on the dataset with position bias",
    "add extra row to position file",
    "simulate position bias for the train dataset and put the train dataset with biased labels to temp directory",
    "test setting positions through Dataset constructor with numpy array",
    "the performance of the unbiased LambdaMART should outperform the plain LambdaMART on the dataset with position bias",
    "test setting positions through Dataset constructor with pandas Series",
    "test setting positions through set_position",
    "test get_position works",
    "no early stopping",
    "early stopping occurs",
    "regular early stopping",
    "positive min_delta",
    "test custom eval metrics",
    "shuffle = False, override metric in params",
    "shuffle = True, callbacks",
    "enable display training loss",
    "self defined folds",
    "LambdaRank",
    "... with l2 metric",
    "... with NDCG (default) metric",
    "self defined folds with lambdarank",
    "init_model from an in-memory Booster",
    "init_model from a text file",
    "predictions should be identical",
    "with early stopping",
    "predict by each fold booster",
    "check that each booster predicted using the best iteration",
    "fold averaging",
    "without early stopping",
    "test feature_names with whitespaces",
    "This has non-ascii strings.",
    "check that passing parameters to the constructor raises warning and ignores them",
    "check inference isn't affected by unknown parameter",
    "entries whose values should reflect params passed to lgb.train()",
    "'l1' was passed in with alias 'mae'",
    "NOTE: this was passed in with alias 'sub_row'",
    "entries with default values of params",
    "add device-specific entries",
    "",
    "passed-in force_col_wise / force_row_wise parameters are ignored on CUDA and GPU builds...",
    "https://github.com/microsoft/LightGBM/blob/1d7ee63686272bceffd522284127573b511df6be/src/io/config.cpp#L375-L377",
    "check that model text has all expected param entries",
    "since Booster.model_to_string() is used when pickling, check that parameters all",
    "roundtrip pickling successfully too",
    "why fixed seed?",
    "sometimes there is no difference how cols are treated (cat or not cat)",
    "take subsets and train",
    "generate CSR sparse dataset",
    "convert data to dense and get back same contribs",
    "validate the values are the same",
    "validate using CSC matrix",
    "validate the values are the same",
    "generate CSR sparse dataset",
    "convert data to dense and get back same contribs",
    "validate the values are the same",
    "validate using CSC matrix",
    "validate the values are the same",
    "@pytest.mark.skipif(psutil.virtual_memory().available / 1024 / 1024 / 1024 < 3, reason=\"not enough RAM\")",
    "def test_int32_max_sparse_contribs(rng):",
    "params = {\"objective\": \"binary\"}",
    "train_features = rng.uniform(size=(100, 1000))",
    "train_targets = [0] * 50 + [1] * 50",
    "lgb_train = lgb.Dataset(train_features, train_targets)",
    "gbm = lgb.train(params, lgb_train, num_boost_round=2)",
    "csr_input_shape = (3000000, 1000)",
    "test_features = csr_matrix(csr_input_shape)",
    "for i in range(0, csr_input_shape[0], csr_input_shape[0] // 6):",
    "for j in range(0, 1000, 100):",
    "test_features[i, j] = random.random()",
    "y_pred_csr = gbm.predict(test_features, pred_contrib=True)",
    "# Note there is an extra column added to the output for the expected value",
    "csr_output_shape = (csr_input_shape[0], csr_input_shape[1] + 1)",
    "assert y_pred_csr.shape == csr_output_shape",
    "y_pred_csc = gbm.predict(test_features.tocsc(), pred_contrib=True)",
    "# Note output CSC shape should be same as CSR output shape",
    "assert y_pred_csc.shape == csr_output_shape",
    "test sliced labels",
    "append some columns",
    "append some rows",
    "test sliced 2d matrix",
    "test sliced CSR",
    "trees start at position 1.",
    "split_features are in 4th line.",
    "test if a penalty as high as the depth indeed prohibits all monotone splits",
    "The penalization is so high that the first 2 features should not be used here",
    "Check that a very high penalization is the same as not using the features at all",
    "check refit accepts dataset_params",
    "the following checks that dart and rf with mape can predict outside the 0-1 range",
    "https://github.com/microsoft/LightGBM/issues/1579",
    "no custom objective, no feval",
    "default metric",
    "non-default metric in params",
    "default metric in args",
    "non-default metric in args",
    "metric in args overwrites one in params",
    "metric in args overwrites one in params",
    "multiple metrics in params",
    "multiple metrics in args",
    "remove default metric by 'None' in list",
    "remove default metric by 'None' aliases",
    "custom objective, no feval",
    "no default metric",
    "metric in params",
    "metric in args",
    "metric in args overwrites its' alias in params",
    "multiple metrics in params",
    "multiple metrics in args",
    "no custom objective, feval",
    "default metric with custom one",
    "non-default metric in params with custom one",
    "default metric in args with custom one",
    "non-default metric in args with custom one",
    "metric in args overwrites one in params, custom one is evaluated too",
    "multiple metrics in params with custom one",
    "multiple metrics in args with custom one",
    "custom metric is evaluated despite 'None' is passed",
    "custom objective, feval",
    "no default metric, only custom one",
    "metric in params with custom one",
    "metric in args with custom one",
    "metric in args overwrites one in params, custom one is evaluated too",
    "multiple metrics in params with custom one",
    "multiple metrics in args with custom one",
    "custom metric is evaluated despite 'None' is passed",
    "no custom objective, no feval",
    "default metric",
    "default metric in params",
    "non-default metric in params",
    "multiple metrics in params",
    "remove default metric by 'None' aliases",
    "custom objective, no feval",
    "no default metric",
    "metric in params",
    "multiple metrics in params",
    "no custom objective, feval",
    "default metric with custom one",
    "default metric in params with custom one",
    "non-default metric in params with custom one",
    "multiple metrics in params with custom one",
    "custom metric is evaluated despite 'None' is passed",
    "custom objective, feval",
    "no default metric, only custom one",
    "metric in params with custom one",
    "multiple metrics in params with custom one",
    "custom metric is evaluated despite 'None' is passed",
    "Custom objective replaces multiclass",
    "multiclass default metric",
    "multiclass default metric with custom one",
    "multiclass metric alias with custom one for custom objective",
    "no metric for invalid class_num",
    "custom metric for invalid class_num",
    "multiclass metric alias with custom one with invalid class_num",
    "multiclass default metric without num_class",
    "multiclass metric alias",
    "multiclass metric",
    "non-valid metric for multiclass objective",
    "non-default num_class for default objective",
    "no metric with non-default num_class for custom objective",
    "multiclass metric alias for custom objective",
    "multiclass metric for custom objective",
    "binary metric with non-default num_class for custom objective",
    "Expect three metrics but mean and stdv for each metric",
    "test XGBoost-style return value",
    "test numpy-style return value",
    "test bins string type",
    "test histogram is disabled for categorical features",
    "test for lgb.train",
    "test feval for lgb.train",
    "test with two valid data for lgb.train",
    "test for lgb.cv",
    "test feval for lgb.cv",
    "test that binning works properly for features with only positive or only negative values",
    "decreasing without freeing raw data is allowed",
    "decreasing before lazy init is allowed",
    "increasing is allowed",
    "decreasing with disabled filter is allowed",
    "decreasing with enabled filter is disallowed;",
    "also changes of other params are disallowed",
    "check extra trees increases regularization",
    "check path smoothing increases regularization",
    "test edge case with one leaf",
    "check that constraint containing all features is equivalent to no constraint",
    "check that constraint partitioning the features reduces train accuracy",
    "check that constraints consisting of single features reduce accuracy further",
    "test that interaction constraints work when not all features are used",
    "check that number of threads does not affect result",
    "check that setting linear_tree=True fits better than ordinary trees when data has linear relationship",
    "test again with nans in data",
    "test again with bagging",
    "test with a feature that has only one non-nan value",
    "test with a categorical feature",
    "test refit: same results on same data",
    "test refit with save and load",
    "test refit: different results training on different data",
    "test when num_leaves - 1 < num_features and when num_leaves - 1 > num_features",
    "test that the predict once with all iterations equals summed results with start_iteration and num_iteration",
    "test the case where start_iteration <= 0, and num_iteration is None",
    "test the case where start_iteration > 0, and num_iteration <= 0",
    "test the case where start_iteration > 0, and num_iteration <= 0, with pred_leaf=True",
    "test the case where start_iteration > 0, and num_iteration <= 0, with pred_contrib=True",
    "test for regression",
    "test both with and without early stopping",
    "test for multi-class",
    "test both with and without early stopping",
    "test for binary",
    "test both with and without early stopping",
    "test against sklearn average precision metric",
    "test that average precision is 1 where model predicts perfectly",
    "data as float64",
    "test all features were used",
    "test the score is better than predicting the mean",
    "test all predictions are equal using different input dtypes",
    "introduce some missing values",
    "the previous line turns x3 into object dtype in recent versions of pandas",
    "train with regular dtypes",
    "convert to nullable dtypes",
    "test training succeeds",
    "test all features were used",
    "test the score is better than predicting the mean",
    "test equal predictions",
    "test data are taken from bug report",
    "https://github.com/microsoft/LightGBM/issues/4708",
    "modified from https://github.com/microsoft/LightGBM/issues/3679#issuecomment-938652811",
    "and https://github.com/microsoft/LightGBM/pull/5087",
    "test that the ``splits_per_leaf_`` of CEGB is cleaned before training a new tree",
    "which is done in the fix #5164",
    "without the fix:",
    "Check failed: (best_split_info.left_count) > (0)",
    "try to predict with a different feature",
    "check that disabling the check doesn't raise the error",
    "try to refit with a different feature",
    "check that disabling the check doesn't raise the error",
    "coding: utf-8",
    "If compiled appropriately, the same installation will support both GPU and CPU.",
    "Double-precision floats are only supported on x86_64 with PoCL",
    "coding: utf-8",
    "coding: utf-8",
    "Note: MSVC has issues with Altrep classes, so they are disabled for it.",
    "See: https://github.com/microsoft/LightGBM/pull/6213#issuecomment-2111025768",
    "These are helper functions to allow doing a stack unwind",
    "after an R allocation error, which would trigger a long jump.",
    "convert from one-based to zero-based index",
    "if any feature names were larger than allocated size,",
    "allow for a larger size and try again",
    "convert from boundaries to size",
    "--- start Booster interfaces",
    "if any eval names were larger than allocated size,",
    "allow for a larger size and try again",
    "Note: for some reason, MSVC crashes when an error is thrown here",
    "if the buffer variable is defined as 'std::unique_ptr<std::vector<char>>',",
    "but not if it is defined as '<std::vector<char>'.",
    "if the model string was larger than the initial buffer, call the function again, writing directly to the R object",
    "if the model string was larger than the initial buffer, allocate a bigger buffer and try again",
    "if aliases string was larger than the initial buffer, allocate a bigger buffer and try again",
    "if aliases string was larger than the initial buffer, allocate a bigger buffer and try again",
    ".Call() calls",
    "coding: utf-8",
    "alias table",
    "names",
    "from strings",
    "tails",
    "tails",
    "the following are stored as comma separated strings but are arrays in the wrappers",
    "coding: utf-8",
    "Single row predictor to abstract away caching logic",
    "Prevent the booster from being modified while we have a predictor relying on it during prediction",
    "If several threads try to predict at the same time using the same SingleRowPredictor",
    "we want them to still provide correct values, so the mutex is necessary due to the shared",
    "resources in the predictor.",
    "However the recommended approach is to instantiate one SingleRowPredictor per thread,",
    "to avoid contention here.",
    "create boosting",
    "initialize the boosting",
    "create objective function",
    "initialize the objective function",
    "create training metric",
    "reset the boosting",
    "create objective function",
    "initialize the objective function",
    "Workaround https://github.com/microsoft/LightGBM/issues/6142 by locking here",
    "This is only a workaround because if predictors are initialized differently it may still behave incorrectly,",
    "and because multiple racing Predictor initializations through LGBM_BoosterPredictForMat suffers from that same issue of Predictor init writing things in the booster.",
    "Once #6142 is fixed (predictor doesn't write in the Booster as should have been the case since 1c35c3b9ede9adab8ccc5fd7b4b2b6af188a79f0), this line can be removed.",
    "calculate the nonzero data and indices size",
    "allocate data and indices arrays",
    "Get the number of trees per iteration (for multiclass scenario we output multiple sparse matrices)",
    "aggregated per row feature contribution results",
    "keep track of the row_vector sizes for parallelization",
    "copy vector results to output for each row",
    "Get the number of trees per iteration (for multiclass scenario we output multiple sparse matrices)",
    "aggregated per row feature contribution results",
    "calculate number of elements per column to construct",
    "the CSC matrix with random access",
    "keep track of column counts",
    "keep track of beginning index for each column",
    "keep track of beginning index for each matrix",
    "Note: we parallelize across matrices instead of rows because of the column_counts[m][col_idx] increment inside the loop",
    "store the row index",
    "update column count",
    "explicitly declare symbols from LightGBM namespace",
    "some help functions used to convert data",
    "Row iterator of on column for CSC matrix",
    "return value at idx, only can access by ascent order",
    "return next non-zero pair, if index < 0, means no more data",
    "start of c_api functions",
    "This API is to keep python binding's behavior the same with C++ implementation.",
    "Sample count, random seed etc. should be provided in parameters.",
    "convert internal thread id to be unique based on external thread id",
    "convert internal thread id to be unique based on external thread id",
    "sample data first",
    "sample data first",
    "sample data first",
    "local buffer to re-use memory",
    "sample data first",
    "no more data",
    "Prepare the Arrow data",
    "Initialize the dataset",
    "If there is no reference dataset, we first sample indices",
    "Then, we obtain sample values by parallelizing across columns",
    "Values need to be copied from the record batches.",
    "The chunks are iterated over in the inner loop as columns can be treated independently.",
    "Finally, we initialize a loader from the sampled values",
    "After sampling and properly initializing all bins, we can add our data to the dataset. Here,",
    "we parallelize across rows.",
    "---- start of booster",
    "Naming: In future versions of LightGBM, public API named around `FastConfig` should be made named around",
    "`SingleRowPredictor`, because it is specific to single row prediction, and doesn't actually hold only config.",
    "For now this is kept as `FastConfig` for backwards compatibility.",
    "At the same time, one should consider removing the old non-fast single row public API that stores its Predictor",
    "in the Booster, because that will enable removing these Predictors from the Booster, and associated initialization",
    "code.",
    "Single row in row-major format:",
    "Apply the configuration",
    "Set up chunked array and iterators for all columns",
    "Build row function",
    "Run prediction",
    "---- start of some help functions",
    "data is array of pointers to individual rows",
    "set number of threads for openmp",
    "read parameters from config file",
    "remove str after \"#\"",
    "de-duplicate params",
    "prediction is needed if using input initial model(continued train)",
    "need to continue training",
    "sync up random seed for data partition",
    "load Training data",
    "load data for distributed training",
    "load data for single machine",
    "need save binary file",
    "create training metric",
    "only when have metrics then need to construct validation data",
    "Add validation data, if it exists",
    "add",
    "need save binary file",
    "add metric for validation data",
    "output used time on each iteration",
    "need init network",
    "create boosting",
    "create objective function",
    "load training data",
    "initialize the objective function",
    "initialize the boosting",
    "add validation data into boosting",
    "convert model to if-else statement code",
    "create predictor",
    "Free memory",
    "create predictor",
    "label_gain = 2^i - 1, may overflow, so we use 31 here",
    "counts for all labels",
    "start from top label, and accumulate DCG",
    "counts for all labels",
    "calculate k Max DCG by one pass",
    "get sorted indices by score",
    "calculate multi dcg by one pass",
    "wait for all client start up",
    "Don't call MPI_Finalize() here: If the destructor was called because only this node had an exception, calling MPI_Finalize() will cause all nodes to hang.",
    "Instead we will handle finalize/abort for MPI in main().",
    "default set to -1",
    "distance at k-th communication, distance[k] = 2^k",
    "set incoming rank at k-th commuication",
    "set outgoing rank at k-th commuication",
    "default set as -1",
    "construct all recursive halving map for all machines",
    "let 1 << k <= num_machines",
    "distance of each communication",
    "if num_machines = 2^k, don't need to group machines",
    "communication direction, %2 == 0 is positive",
    "neighbor at k-th communication",
    "receive data block at k-th communication",
    "send data block at k-th communication",
    "if num_machines != 2^k, need to group machines",
    "group, two machine in one group, total \"rest\" groups will have 2 machines.",
    "let left machine as group leader",
    "cache block information for groups, group with 2 machines will have double block size",
    "convert from group to node leader",
    "convert from node to group",
    "meet new group",
    "add block len for this group",
    "calculate the group block start",
    "not need to construct",
    "get receive block information",
    "accumulate block len",
    "get send block information",
    "accumulate block len",
    "static member definition",
    "if small package or small count , do it by all gather.(reduce the communication times.)",
    "assign the blocks to every rank.",
    "do reduce scatter",
    "do all gather",
    "assign blocks",
    "need use buffer here, since size of \"output\" is smaller than size after all gather",
    "copy back",
    "assign blocks",
    "start all gather",
    "when num_machines is small and data is large",
    "use output as receive buffer",
    "get current local block size",
    "get out rank",
    "get in rank",
    "get send information",
    "get recv information",
    "send and recv at same time",
    "rotate in-place",
    "use output as receive buffer",
    "get current local block size",
    "get send information",
    "get recv information",
    "send and recv at same time",
    "use output as receive buffer",
    "send and recv at same time",
    "send local data to neighbor first",
    "receive neighbor data first",
    "reduce",
    "get target",
    "get send information",
    "get recv information",
    "send and recv at same time",
    "reduce",
    "send result to neighbor",
    "receive result from neighbor",
    "copy result",
    "start up socket",
    "parse clients from file",
    "get ip list of local machine",
    "get local rank",
    "construct listener",
    "construct communication topo",
    "construct linkers",
    "free listener",
    "set timeout",
    "accept incoming socket",
    "receive rank",
    "add new socket",
    "save ranks that need to connect with",
    "start listener",
    "start connect",
    "let smaller rank connect to larger rank",
    "send local rank",
    "wait for listener",
    "print connected linkers",
    "only need to copy subset",
    "avoid to copy subset many times",
    "avoid out of range",
    "may need to recopy subset",
    "valid the type",
    "parser factory implementation.",
    "customized parser add-on.",
    "save header to parser config in case needed.",
    "save label id to parser config in case needed.",
    "Constructors",
    "Get type tag",
    "Comparisons",
    "This has to be separate, not in Statics, because Json() accesses",
    "statics().null.",
    "advance until next line, or end of input",
    "advance until closing tokens",
    "The usual case: non-escaped characters",
    "Handle escapes",
    "Extract 4-byte escape sequence",
    "Explicitly check length of the substring. The following loop",
    "relies on std::string returning the terminating NUL when",
    "accessing str[length]. Checking here reduces brittleness.",
    "JSON specifies that characters outside the BMP shall be encoded as a",
    "pair of 4-hex-digit \\u escapes encoding their surrogate pair",
    "components. Check whether we're in the middle of such a beast: the",
    "previous codepoint was an escaped lead (high) surrogate, and this is",
    "a trail (low) surrogate.",
    "Reassemble the two surrogate pairs into one astral-plane character,",
    "per the UTF-16 algorithm.",
    "Integer part",
    "Decimal part",
    "Exponent part",
    "Check for any trailing garbage",
    "Documented in json11.hpp",
    "Check for another object",
    "get column names",
    "support to get header from parser config, so could utilize following label name to id mapping logic.",
    "load label idx first",
    "if parser config file exists, feature names may be changed after customized parser applied.",
    "clear here so could use default filled feature names during dataset construction.",
    "may improve by saving real feature names defined in parser in the future.",
    "erase label column name",
    "load ignore columns",
    "load weight idx",
    "load group idx",
    "don't support query id in data file when using distributed training",
    "read data to memory",
    "sample data",
    "construct feature bin mappers & clear sample data",
    "initialize label",
    "extract features",
    "sample data from file",
    "construct feature bin mappers & clear sample data",
    "initialize label",
    "extract features",
    "load data from binary file",
    "checks whether there's a initial score file when loaded from binary data files",
    "the intial score file should with suffix \".bin.init\"",
    "check meta data",
    "need to check training data",
    "read data in memory",
    "initialize label",
    "extract features",
    "Get number of lines of data file",
    "initialize label",
    "extract features",
    "load data from binary file",
    "checks whether there's a initial score file when loaded from binary data files",
    "the intial score file should with suffix \".bin.init\"",
    "not need to check validation data",
    "check meta data",
    "check token",
    "read feature group definitions",
    "read feature size",
    "buffer to read binary file",
    "check token",
    "read size of header",
    "re-allocate space if not enough",
    "read header",
    "get header",
    "read size of meta data",
    "re-allocate space if not enough",
    "read meta data",
    "load meta data",
    "sample local used data if need to partition",
    "if not contain query file, minimal sample unit is one record",
    "if contain query file, minimal sample unit is one query",
    "if is new query",
    "read feature data",
    "read feature size",
    "re-allocate space if not enough",
    "raw data",
    "fill feature_names_ if not header",
    "get forced split",
    "if only one machine, find bin locally",
    "if have multi-machines, need to find bin distributed",
    "different machines will find bin for different features",
    "start and len will store the process feature indices for different machines",
    "machine i will find bins for features in [ start[i], start[i] + len[i] )",
    "free",
    "gather global feature bin mappers",
    "restore features bins from buffer",
    "---- private functions ----",
    "get header",
    "num_groups",
    "real_feature_idx_",
    "feature2group",
    "feature2subfeature",
    "group_bin_boundaries",
    "group_feature_start_",
    "group_feature_cnt_",
    "get feature names",
    "get forced_bin_bounds_",
    "if features are ordered, not need to use hist_buf",
    "read all lines",
    "get query data",
    "if not contain query data, minimal sample unit is one record",
    "if contain query data, minimal sample unit is one query",
    "if is new query",
    "get query data",
    "if not contain query file, minimal sample unit is one record",
    "if contain query file, minimal sample unit is one query",
    "if is new query",
    "parse features",
    "get forced split",
    "check the range of label_idx, weight_idx and group_idx",
    "skip label check if user input parser config file,",
    "because label id is got from raw features while dataset features are consistent with customized parser.",
    "fill feature_names_ if not header",
    "start find bins",
    "if only one machine, find bin locally",
    "start and len will store the process feature indices for different machines",
    "machine i will find bins for features in [ start[i], start[i] + len[i] )",
    "free",
    "gather global feature bin mappers",
    "restore features bins from buffer",
    "if doesn't need to prediction with initial model",
    "parser",
    "set label",
    "free processed line:",
    "shrink_to_fit will be very slow in linux, and seems not free memory, disable for now",
    "text_reader_->Lines()[i].shrink_to_fit();",
    "push data",
    "if is used feature",
    "if need to prediction with initial model",
    "parser",
    "set initial score",
    "set label",
    "free processed line:",
    "shrink_to_fit will be very slow in Linux, and seems not free memory, disable for now",
    "text_reader_->Lines()[i].shrink_to_fit();",
    "push data",
    "if is used feature",
    "metadata_ will manage space of init_score",
    "text data can be free after loaded feature values",
    "parser",
    "set initial score",
    "set label",
    "push data",
    "if is used feature",
    "only need part of data",
    "need full data",
    "metadata_ will manage space of init_score",
    "read size of token",
    "remove duplicates",
    "deep copy function for BinMapper",
    "mean size for one bin",
    "need a new bin",
    "update bin upper bound",
    "last bin upper bound",
    "get number of positive and negative distinct values",
    "include zero bounds and infinity bound",
    "add forced bounds, excluding zeros since we have already added zero bounds",
    "find remaining bounds",
    "find distinct_values first",
    "push zero in the front",
    "use the large value",
    "push zero in the back",
    "convert to int type first",
    "sort by counts in descending order",
    "will ignore the categorical of small counts",
    "Push the dummy bin for NaN",
    "Use MissingType::None to represent this bin contains all categoricals",
    "fix count of NaN bin",
    "check trivial(num_bin_ == 1) feature",
    "check useless bin",
    "When most_freq_bin_ != default_bin_, there are some additional data loading costs.",
    "so use most_freq_bin_ = default_bin_ when there is not so sparse",
    "calculate max bin of all features to select the int type in MultiValDenseBin",
    "for lambdarank, it needs query data for partition data in distributed learning",
    "need convert query_id to boundaries",
    "check weights",
    "check positions",
    "check query boundries",
    "contain initial score file",
    "check weights",
    "get local weights",
    "check positions",
    "get local positions",
    "check query boundries",
    "get local query boundaries",
    "contain initial score file",
    "get local initial scores",
    "re-calculate query weight",
    "Clear init scores on empty input",
    "Note that len here is row count, not num_init_score, so we compare against num_data",
    "We need to use source_size here, because len might not equal size (due to a partially loaded dataset)",
    "CUDA is handled after all insertions are complete",
    "CUDA is handled after all insertions are complete",
    "Clear weights on empty input",
    "CUDA is handled after all insertions are complete",
    "Clear query boundaries on empty input",
    "save to nullptr",
    "CUDA is handled after all insertions are complete",
    "default weight file name",
    "default position file name",
    "default init_score file name",
    "use first line to count number class",
    "default query file name",
    "root is in the depth 0",
    "non-leaf",
    "leaf",
    "use this for the missing value conversion",
    "Predict func by Map to ifelse",
    "use this for the missing value conversion",
    "non-leaf",
    "left subtree",
    "right subtree",
    "leaf",
    "non-leaf",
    "left subtree",
    "right subtree",
    "leaf",
    "recursive computation of SHAP values for a decision tree",
    "extend the unique path",
    "leaf node",
    "internal node",
    "see if we have already split on this feature,",
    "if so we undo that split so we can redo it for this node",
    "recursive sparse computation of SHAP values for a decision tree",
    "extend the unique path",
    "leaf node",
    "internal node",
    "see if we have already split on this feature,",
    "if so we undo that split so we can redo it for this node",
    "add names of objective function if not providing metric",
    "equal weights for all classes",
    "generate seeds by seed.",
    "sort eval_at",
    "Only push the non-training data",
    "check for conflicts",
    "check if objective, metric, and num_class match",
    "Change pool size to -1 (no limit) when using data parallel to reduce communication costs",
    "max_depth defaults to -1, so max_depth>0 implies \"you explicitly overrode the default\"",
    "",
    "Changing max_depth while leaving num_leaves at its default (31) can lead to 2 undesirable situations:",
    "",
    "* (0 <= max_depth <= 4) it's not possible to produce a tree with 31 leaves",
    "- this block reduces num_leaves to 2^max_depth",
    "* (max_depth > 4) 31 leaves is less than a full depth-wise tree, which might lead to underfitting",
    "- this block warns about that",
    "ref: https://github.com/microsoft/LightGBM/issues/2898#issuecomment-1002860601",
    "Fits in an int, and is more restrictive than the current num_leaves",
    "force col-wise for gpu, and cuda version",
    "force row-wise for cuda version",
    "linear tree learner must be serial type and run on CPU device",
    "min_data_in_leaf must be at least 2 if path smoothing is active. This is because when the split is calculated",
    "the count is calculated using the proportion of hessian in the leaf which is rounded up to nearest int, so it can",
    "be 1 when there is actually no data in the leaf. In rare cases this can cause a bug because with path smoothing the",
    "calculated split gain can be positive even with zero gradient and hessian.",
    "In distributed mode, local node doesn't have histograms on all features, cannot perform \"intermediate\" monotone constraints.",
    "\"intermediate\" monotone constraints need to recompute splits. If the features are sampled when computing the",
    "split initially, then the sampling needs to be recorded or done once again, which is currently not supported",
    "first round: fill the single val group",
    "always push the last group",
    "put dense feature first",
    "sort by non zero cnt",
    "sort by non zero cnt, bigger first",
    "shuffle groups",
    "Using std::swap for vector<bool> will cause the wrong result.",
    "get num_features",
    "get bin_mappers",
    "for sparse multi value bin, we store the feature bin values with offset added",
    "for dense multi value bin, the feature bin values without offsets are used",
    "copy feature bin mapper data",
    "copy feature bin mapper data",
    "update CUDA storage for column data and metadata",
    "if not pass a filename, just append \".bin\" of original file",
    "Write the basic header information for the dataset",
    "get size of meta data",
    "write meta data",
    "write feature data",
    "get size of feature",
    "write feature",
    "write raw data; use row-major order so we can read row-by-row",
    "Calculate approximate size of output and reserve space",
    "write feature group definitions",
    "Give a little extra just in case, to avoid unnecessary resizes",
    "Write token that marks the data as binary reference, and the version",
    "Write the basic definition of the overall dataset",
    "write feature group definitions",
    "get size of feature",
    "write feature",
    "size of feature names and forced bins",
    "write header",
    "write feature names",
    "write forced bins",
    "explicitly initialize template methods, for cross module call",
    "explicitly initialize template methods, for cross module call",
    "Only one multi-val group, just simply merge",
    "Skip the leading 0 when copying group_bin_boundaries.",
    "regenerate other fields",
    "need to iterate bin iterator",
    "is dense column",
    "is sparse column",
    "initialize the subset cuda column data",
    "if one column has too many bins, use a separate partition for that column",
    "try if adding this column exceed the maximum number per partition",
    "if one column has too many bins, use a separate partition for that column",
    "try if adding this column exceed the maximum number per partition",
    "if LightGBM-specific default has been set, ignore OpenMP-global config",
    "otherwise, default to OpenMP-global config",
    "ensure that if LGBM_SetMaxThreads() was ever called, LightGBM doesn't",
    "use more than that many threads",
    "store the importance first",
    "PredictRaw",
    "PredictRawByMap",
    "Predict",
    "PredictByMap",
    "PredictLeafIndex",
    "PredictLeafIndexByMap",
    "output model type",
    "output number of class",
    "output label index",
    "output max_feature_idx",
    "output objective",
    "output tree models",
    "store the importance first",
    "sort the importance",
    "use serialized string to restore this object",
    "Use first 128 chars to avoid exceed the message buffer.",
    "get number of classes",
    "get index of label",
    "get max_feature_idx first",
    "get average_output",
    "get feature names",
    "get monotone_constraints",
    "set zero",
    "predict all the trees for one iteration",
    "check early stopping",
    "set zero",
    "predict all the trees for one iteration",
    "check early stopping",
    "margin_threshold will be captured by value",
    "copy and sort",
    "margin_threshold will be captured by value",
    "Fix for compiler warnings about reaching end of control",
    "load forced_splits file",
    "init tree learner",
    "push training metrics",
    "get max feature index",
    "get label index",
    "get feature names",
    "get parser config file content",
    "check that forced splits does not use feature indices larger than dataset size",
    "if need bagging, create buffer",
    "for a validation dataset, we need its score and metric",
    "update score",
    "objective function will calculate gradients and hessians",
    "output used time per iteration",
    "boosting from average label; or customized \"average\" if implemented for the current objective",
    "boosting first",
    "use customized objective function",
    "the check below fails unless objective=custom is provided in the parameters on Booster creation",
    "need to copy customized gradients when using GOSS",
    "bagging logic",
    "need to copy gradients for bagging subset.",
    "shrinkage by learning rate",
    "update score",
    "only add default score one-time",
    "updates scores",
    "add model",
    "reset score",
    "remove model",
    "print message for metric",
    "pop last early_stopping_round_ models",
    "update training score",
    "we need to predict out-of-bag scores of data for boosting",
    "update validation score",
    "print training metric",
    "print validation metric",
    "set zero",
    "predict all the trees for one iteration",
    "predict all the trees for one iteration",
    "push training metrics",
    "not same training data, need reset score and others",
    "create score tracker",
    "update score",
    "resize gradient vectors to copy the customized gradients for goss or bagging with subset",
    "load forced_splits file",
    "if exists initial score, will start from it",
    "clear host score buffer",
    "Need special case for no smoothing to preserve existing behaviour. If no smoothing, the parent output is calculated",
    "with the larger categorical l2, whereas min_split_gain uses the original l2.",
    "if data not enough, or sum hessian too small",
    "if data not enough",
    "if sum hessian too small",
    "current split gain",
    "gain with split is worse than without split",
    "mark as able to be split",
    "better split point",
    "recover sum of gradient and hessian from the sum of quantized gradient and hessian",
    "Need special case for no smoothing to preserve existing behaviour. If no smoothing, the parent output is calculated",
    "with the larger categorical l2, whereas min_split_gain uses the original l2.",
    "if data not enough, or sum hessian too small",
    "if data not enough",
    "if sum hessian too small",
    "current split gain",
    "gain with split is worse than without split",
    "mark as able to be split",
    "better split point",
    "Get the max size of pool",
    "at least need 2 leaves",
    "push split information for all leaves",
    "initialize splits for leaf",
    "initialize data partition",
    "initialize ordered gradients and hessians",
    "cannot change is_hist_col_wise during training",
    "initialize splits for leaf",
    "initialize data partition",
    "initialize ordered gradients and hessians",
    "Get the max size of pool",
    "at least need 2 leaves",
    "push split information for all leaves",
    "some initial works before training",
    "root leaf",
    "only root leaf can be splitted on first time",
    "some initial works before finding best split",
    "find best threshold for every feature",
    "Get a leaf with max split gain",
    "Get split information for best leaf",
    "cannot split, quit",
    "split tree with best leaf",
    "reset histogram pool",
    "initialize data partition",
    "reset the splits for leaves",
    "Sumup for root",
    "use all data",
    "use bagging, only use part of data",
    "check depth of current leaf",
    "only need to check left leaf, since right leaf is in same level of left leaf",
    "no enough data to continue",
    "only have root",
    "put parent(left) leaf's histograms into larger leaf's histograms",
    "put parent(left) leaf's histograms to larger leaf's histograms",
    "construct smaller leaf",
    "construct larger leaf",
    "find splits",
    "only has root leaf",
    "start at root leaf",
    "Histogram construction require parent features.",
    "then, compute own splits",
    "split info should exist because searching in bfs fashion - should have added from parent",
    "update before tree split",
    "don't need to update this in data-based parallel model",
    "split tree, will return right leaf",
    "store the true split gain in tree model",
    "don't need to update this in data-based parallel model",
    "store the true split gain in tree model",
    "init the leaves that used on next iteration",
    "update leave outputs if needed",
    "bag_mapper[index_mapper[i]]",
    "it is needed to filter the features after the above code.",
    "Otherwise, the `is_splittable` in `FeatureHistogram` will be wrong, and cause some features being accidentally filtered in the later nodes.",
    "for root leaf the \"parent\" output is its own output because we don't apply any smoothing to the root",
    "can't use GetParentOutput because leaf_splits doesn't have weight property set",
    "find splits",
    "identify features containing nans",
    "preallocate the matrix used to calculate linear model coefficients",
    "store only upper triangular half of matrix as an array, in row-major order",
    "this requires (max_num_feat + 1) * (max_num_feat + 2) / 2 entries (including the constant terms of the regression)",
    "we add another 8 to ensure cache lines are not shared among processors",
    "some initial works before training",
    "root leaf",
    "only root leaf can be splitted on first time",
    "some initial works before finding best split",
    "find best threshold for every feature",
    "Get a leaf with max split gain",
    "Get split information for best leaf",
    "cannot split, quit",
    "split tree with best leaf",
    "map data to leaf number",
    "calculate coefficients using the method described in Eq 3 of https://arxiv.org/pdf/1802.05640.pdf",
    "the coefficients vector is given by",
    "- (X_T * H * X + lambda) ^ (-1) * (X_T * g)",
    "where:",
    "X is the matrix where the first column is the feature values and the second is all ones,",
    "H is the diagonal matrix of the hessian,",
    "lambda is the diagonal matrix with diagonal entries equal to the regularisation term linear_lambda",
    "g is the vector of gradients",
    "the subscript _T denotes the transpose",
    "create array of pointers to raw data, and coefficient matrices, for each leaf",
    "clear the coefficient matrices",
    "aggregate results from different threads",
    "copy into eigen matrices and solve",
    "update the tree properties",
    "need to be able to hold smaller and larger best splits in SyncUpGlobalBestSplit",
    "get feature partition",
    "get local used features",
    "get best split at smaller leaf",
    "find local best split for larger leaf",
    "sync global best info",
    "update best split",
    "instantiate template classes, otherwise linker cannot find the code",
    "initialize SerialTreeLearner",
    "Get local rank and global machine size",
    "need to be able to hold smaller and larger best splits in SyncUpGlobalBestSplit",
    "allocate buffer for communication",
    "get block start and block len for reduce scatter",
    "get buffer_write_start_pos",
    "get buffer_read_start_pos",
    "generate feature partition for current tree",
    "get local used feature",
    "get block start and block len for reduce scatter",
    "sync global data sumup info",
    "global sumup reduce",
    "copy back",
    "set global sumup info",
    "init global data count in leaf",
    "reset hist num bits according to global num data",
    "sync global data sumup info",
    "global sumup reduce",
    "copy back",
    "set global sumup info",
    "init global data count in leaf",
    "clear histogram buffer before synchronizing",
    "otherwise histogram contents from the previous iteration will be sent",
    "construct local histograms",
    "copy to buffer",
    "Reduce scatter for histogram",
    "restore global histograms from buffer",
    "only root leaf",
    "construct histgroms for large leaf, we init larger leaf as the parent, so we can just subtract the smaller leaf's histograms",
    "find local best split for larger leaf",
    "sync global best info",
    "set best split",
    "need update global number of data in leaf",
    "reset hist num bits according to global num data",
    "instantiate template classes, otherwise linker cannot find the code",
    "initialize SerialTreeLearner",
    "some additional variables needed for GPU trainer",
    "Initialize GPU buffers and kernels",
    "some functions used for debugging the GPU histogram construction",
    "printf(\"grad %g != %g (%d ULPs)\\n\", GET_GRAD(h1, i), GET_GRAD(h2, i), ulps);",
    "goto err;",
    "we roughly want 256 workgroups per device, and we have num_dense_feature4_ feature tuples.",
    "also guarantee that there are at least 2K examples per workgroup",
    "return 0;",
    "we have already copied ordered gradients, ordered Hessians and indices to GPU",
    "decide the best number of workgroups working on one feature4 tuple",
    "set work group size based on feature size",
    "each 2^exp_workgroups_per_feature workgroups work on a feature4 tuple",
    "we need to refresh the kernel arguments after reallocating",
    "The only argument that needs to be changed later is num_data_",
    "the GPU kernel will process all features in one call, and each",
    "2^exp_workgroups_per_feature (compile time constant) workgroup will",
    "process one feature4 tuple",
    "for the root node, indices are not copied",
    "for constant hessian, hessians are not copied except for the root node",
    "there will be 2^exp_workgroups_per_feature = num_workgroups / num_dense_feature4 sub-histogram per feature4",
    "and we will launch num_feature workgroups for this kernel",
    "will launch threads for all features",
    "the queue should be asynchronous, and we will can WaitAndGetHistograms() before we start processing dense feature groups",
    "copy the results asynchronously. Size depends on if double precision is used",
    "we will wait for this object in WaitAndGetHistograms",
    "when the output is ready, the computation is done",
    "values of this feature has been redistributed to multiple bins; need a reduction here",
    "how many feature-group tuples we have",
    "leave some safe margin for prefetching",
    "256 work-items per workgroup. Each work-item prefetches one tuple for that feature",
    "clear sparse/dense maps",
    "do nothing if no features can be processed on GPU",
    "allocate memory for all features (FIXME: 4 GB barrier on some devices, need to split to multiple buffers)",
    "unpin old buffer if necessary before destructing them",
    "make ordered_gradients and Hessians larger (including extra room for prefetching), and pin them",
    "allocate space for gradients and Hessians on device",
    "we will copy gradients and Hessians in after ordered_gradients_ and ordered_hessians_ are constructed",
    "allocate feature mask, for disabling some feature-groups' histogram calculation",
    "copy indices to the device",
    "histogram bin entry size depends on the precision (single/double)",
    "create output buffer, each feature has a histogram with device_bin_size_ bins,",
    "each work group generates a sub-histogram of dword_features_ features.",
    "only initialize once here, as this will not need to change when ResetTrainingData() is called",
    "create atomic counters for inter-group coordination",
    "The output buffer is allocated to host directly, to overlap compute and data transfer",
    "find the dense feature-groups and group then into Feature4 data structure (several feature-groups packed into 4 bytes)",
    "looking for dword_features_ non-sparse feature-groups",
    "decide if we need to redistribute the bin",
    "multiplier must be a power of 2",
    "device_bin_mults_.push_back(1);",
    "found",
    "for data transfer time",
    "Now generate new data structure feature4, and copy data to the device",
    "preallocate arrays for all threads, and pin them",
    "building Feature4 bundles; each thread handles dword_features_ features",
    "one feature datapoint is 4 bits",
    "this guarantees that the RawGet() function is inlined, rather than using virtual function dispatching",
    "one feature datapoint is one byte",
    "this guarantees that the RawGet() function is inlined, rather than using virtual function dispatching",
    "Dense bin",
    "Dense 4-bit bin",
    "working on the remaining (less than dword_features_) feature groups",
    "fill the leftover features",
    "fill this empty feature with some \"random\" value",
    "fill this empty feature with some \"random\" value",
    "copying the last 1 to (dword_features - 1) feature-groups in the last tuple",
    "deallocate pinned space for feature copying",
    "data transfer time",
    "for other types of failure, build log might not be available; program.build_log() can crash",
    "Something bad happened. Just return \"No log available.\"",
    "build is okay, log may contain warnings",
    "destroy any old kernels",
    "create OpenCL kernels for different number of workgroups per feature",
    "currently we don't use constant memory",
    "compile the GPU kernel depending if double precision is used, constant hessian is used, etc.",
    "kernel with indices in an array",
    "kernel with all features enabled, with eliminated branches",
    "kernel with all data indices (for root node, and assumes that root node always uses all features)",
    "do nothing if no features can be processed on GPU",
    "The only argument that needs to be changed later is num_data_",
    "hessian is passed as a parameter, but it is not available now.",
    "hessian will be set in BeforeTrain()",
    "Get the max bin size, used for selecting best GPU kernel",
    "initialize GPU",
    "determine which kernel to use based on the max number of bins",
    "the +9 skips extra characters \")\", newline, \"#endif\" and newline at the beginning",
    "the +9 skips extra characters \")\", newline, \"#endif\" and newline at the beginning",
    "the +9 skips extra characters \")\", newline, \"#endif\" and newline at the beginning",
    "ignore the feature groups that contain categorical features when producing warnings about max_bin.",
    "these groups may contain larger number of bins due to categorical features, but not due to the setting of max_bin.",
    "setup GPU kernel arguments after we allocating all the buffers",
    "GPU memory has to been reallocated because data may have been changed",
    "setup GPU kernel arguments after we allocating all the buffers",
    "Copy initial full hessians and gradients to GPU.",
    "We start copying as early as possible, instead of at ConstructHistogram().",
    "setup hessian parameters only",
    "hessian is passed as a parameter",
    "use bagging",
    "On GPU, we start copying indices, gradients and Hessians now, instead at ConstructHistogram()",
    "copy used gradients and Hessians to ordered buffer",
    "transfer the indices to GPU",
    "transfer hessian to GPU",
    "setup hessian parameters only",
    "hessian is passed as a parameter",
    "transfer gradients to GPU",
    "only have root",
    "Copy indices, gradients and Hessians as early as possible",
    "only need to initialize for smaller leaf",
    "Get leaf boundary",
    "copy indices to the GPU:",
    "copy ordered Hessians to the GPU:",
    "copy ordered gradients to the GPU:",
    "do nothing if no features can be processed on GPU",
    "copy data indices if it is not null",
    "generate and copy ordered_gradients if gradients is not null",
    "generate and copy ordered_hessians if Hessians is not null",
    "converted indices in is_feature_used to feature-group indices",
    "construct the feature masks for dense feature-groups",
    "if no feature group is used, just return and do not use GPU",
    "if not all feature groups are used, we need to transfer the feature mask to GPU",
    "otherwise, we will use a specialized GPU kernel with all feature groups enabled",
    "All data have been prepared, now run the GPU kernel",
    "construct smaller leaf",
    "ConstructGPUHistogramsAsync will return true if there are available feature groups dispatched to GPU",
    "then construct sparse features on CPU",
    "wait for GPU to finish, only if GPU is actually used",
    "use double precision",
    "use single precision",
    "Compare GPU histogram with CPU histogram, useful for debugging GPU code problem",
    "#define GPU_DEBUG_COMPARE",
    "construct larger leaf",
    "then construct sparse features on CPU",
    "wait for GPU to finish, only if GPU is actually used",
    "use double precision",
    "use single precision",
    "do some sanity check for the GPU algorithm",
    "limit top k",
    "get max bin",
    "calculate buffer size",
    "need to be able to hold smaller and larger best splits in SyncUpGlobalBestSplit",
    "left and right on same time, so need double size",
    "initialize histograms for global",
    "sync global data sumup info",
    "set global sumup info",
    "init global data count in leaf",
    "get local sumup",
    "get local sumup",
    "get mean number on machines",
    "weighted gain",
    "get top k",
    "Copy histogram to buffer, and Get local aggregate features",
    "copy histograms.",
    "copy smaller leaf histograms first",
    "mark local aggregated feature",
    "copy",
    "then copy larger leaf histograms",
    "mark local aggregated feature",
    "copy",
    "use local data to find local best splits",
    "clear histogram buffer before synchronizing",
    "otherwise histogram contents from the previous iteration will be sent",
    "find splits",
    "only has root leaf",
    "local voting",
    "gather",
    "get all top-k from all machines",
    "global voting",
    "copy local histgrams to buffer",
    "Reduce scatter for histogram",
    "find best split from local aggregated histograms",
    "restore from buffer",
    "restore from buffer",
    "find local best",
    "find local best split for larger leaf",
    "sync global best info",
    "copy back",
    "set the global number of data for leaves",
    "init the global sumup info",
    "instantiate template classes, otherwise linker cannot find the code",
    "allocate CUDA memory",
    "leave some space for alignment",
    "input best split info",
    "for leaf information update",
    "gather information for CPU, used for launching kernels",
    "for leaf splits information update",
    "we need restore the order of indices in cuda_data_indices_",
    "allocate more memory for sum reduction in CUDA",
    "only the first element records the final sum",
    "intialize split find task information (a split find task is one pass through the histogram of a feature)",
    "need to double the size of histogram buffer in global memory when using double precision in histogram construction",
    "use only half the size of histogram buffer in global memory when quantized training since each gradient and hessian takes only 2 bytes",
    "use the first gpu by default",
    "std::max(..., 1UL) to avoid error in the case when there are NaN's in the categorical values",
    "use feature interaction constraint or sample features by node"
  ],
  "v4.3.0": [
    "coding: utf-8",
    "create predictor first",
    "setting early stopping via global params should be possible",
    "reduce cost for prediction training data",
    "process callbacks",
    "construct booster",
    "start training",
    "check evaluation result.",
    "ranking task, split according to groups",
    "run preprocessing on the data set if needed",
    "setting early stopping via global params should be possible",
    "setup callbacks",
    "coding: utf-8",
    "dummy function to support older version of scikit-learn",
    "coding: utf-8",
    "f(labels, preds)",
    "f(labels, preds, weights)",
    "f(labels, preds, weights, group)",
    "f(labels, preds)",
    "f(labels, preds, weights)",
    "f(labels, preds, weights, group)",
    "documentation templates for LGBMModel methods are shared between the classes in",
    "this module and those in the ``dask`` module",
    "register default metric for consistency with callable eval_metric case",
    "try to deduce from class instance",
    "overwrite default metric by explicitly set metric",
    "use joblib conventions for negative n_jobs, just like scikit-learn",
    "at predict time, this is handled later due to the order of parameter updates",
    "Do not modify original args in fit function",
    "Refer to https://github.com/microsoft/LightGBM/pull/2619",
    "Separate built-in from callable evaluation metrics",
    "concatenate metric from params (or default if not provided in params) and eval_metric",
    "copy for consistency",
    "reduce cost for prediction training data",
    "free dataset",
    "retrive original params that possibly can be used in both training and prediction",
    "and then overwrite them (considering aliases) with params that were passed directly in prediction",
    "number of threads can have values with special meaning which is only applied",
    "in the scikit-learn interface, these should not reach the c++ side as-is",
    "adjust eval metrics to match whether binary or multiclass",
    "classification is being performed",
    "do not modify args, as it causes errors in model selection tools",
    "check group data",
    "coding: utf-8",
    "coding: utf-8",
    "\"#ddffdd\" is light green, \"#ffdddd\" is light red",
    "coding: utf-8",
    "coding: utf-8",
    "typing.TypeGuard was only introduced in Python 3.10",
    "we don't need lib_lightgbm while building docs",
    "TypeError: obj is not a string or a number",
    "ValueError: invalid literal",
    "Obtain objects to export",
    "Prepare export",
    "Export all objects",
    "DeprecationWarning is not shown by default, so let's create our own with higher level",
    "lazy evaluation to allow import without dynamic library, e.g., for docs generation",
    "if buffer length is not long enough, re-allocate a buffer",
    "avoid side effects on passed-in parameters",
    "if main_param_name was provided, keep that value and remove all aliases",
    "if main param name was not found, search for an alias",
    "neither of main_param_name, aliases were found",
    "most common case (no nullable dtypes)",
    "1.0 <= pd version < 1.1 and nullable dtypes, least common case",
    "raises error because array is casted to type(pd.NA) and there's no na_value argument",
    "data has nullable dtypes, but we can specify na_value argument and copy will be made",
    "take shallow copy in case we modify categorical columns",
    "whole column modifications don't change the original df",
    "determine feature names",
    "determine categorical features",
    "use cat cols from DataFrame",
    "so that the target dtype considers floats",
    "Get total row number.",
    "Random access by row index. Used for data sampling.",
    "Range data access. Used to read data in batch when constructing Dataset.",
    "Optionally specify batch_size to control range data read size.",
    "Only required if using ``Dataset.subset()``.",
    "__get_num_preds() cannot work with nrow > MAX_INT32, so calculate overall number of predictions piecemeal",
    "avoid memory consumption by arrays concatenation operations",
    "create numpy array from output arrays",
    "break up indptr based on number of rows (note more than one matrix in multiclass case)",
    "for CSC there is extra column added",
    "reformat output into a csr or csc matrix or list of csr or csc matrices",
    "same shape as input csr or csc matrix except extra column for expected value",
    "note: make sure we copy data as it will be deallocated next",
    "free the temporary native indptr, indices, and data",
    "__get_num_preds() cannot work with nrow > MAX_INT32, so calculate overall number of predictions piecemeal",
    "avoid memory consumption by arrays concatenation operations",
    "Check that the input is valid: we only handle numbers (for now)",
    "Prepare prediction output array",
    "Export Arrow table to C and run prediction",
    "c type: double**",
    "each double* element points to start of each column of sample data.",
    "c type int**",
    "each int* points to start of indices for each column",
    "no min_data, nthreads and verbose in this function",
    "check data has header or not",
    "need to regroup init_score",
    "process for args",
    "get categorical features",
    "If the params[cat_alias] is equal to categorical_indices, do not report the warning.",
    "process for reference dataset",
    "start construct data",
    "set feature names",
    "Select sampled rows, transpose to column order.",
    "create validation dataset from ref_dataset",
    "Check that the input is valid: we only handle numbers (for now)",
    "Export Arrow table to C",
    "create valid",
    "construct subset",
    "create train",
    "could be updated if data is not freed",
    "set to None",
    "If the data is a arrow data, we can just pass it to C",
    "If a table is being passed, we concatenate the columns. This is only valid for",
    "'init_score'.",
    "we're done if self and reference share a common upstream reference",
    "Check if the weight contains values other than one",
    "Set field",
    "original values can be modified at cpp side",
    "if buffer length is not long enough, reallocate buffers",
    "group data from LightGBM is boundaries data, need to convert to group size",
    "Training task",
    "if \"machines\" is given, assume user wants to do distributed learning, and set up network",
    "construct booster object",
    "copy the parameters from train_set",
    "save reference to data",
    "buffer for inner predict",
    "Prediction task",
    "if buffer length is not long enough, re-allocate a buffer",
    "if a single node tree it won't have `leaf_index` so return 0",
    "Create the node record, and populate universal data members",
    "Update values to reflect node type (leaf or split)",
    "traverse the next level of the tree",
    "In tree format, \"subtree_list\" is a list of node records (dicts),",
    "and we add node to the list.",
    "need reset training data",
    "need to push new valid data",
    "ensure that existing Booster is freed before replacing it",
    "with a new one createdfrom file",
    "if buffer length is not long enough, re-allocate a buffer",
    "if buffer length is not long enough, reallocate a buffer",
    "Copy models",
    "Get name of features",
    "if buffer length is not long enough, reallocate buffers",
    "avoid to predict many time in one iteration",
    "Get num of inner evals",
    "Get name of eval metrics",
    "if buffer length is not long enough, reallocate buffers",
    "coding: utf-8",
    "Callback environment used by callbacks",
    "CVBooster holds a list of Booster objects, each needs to be updated",
    "for lgb.cv() with eval_train_metric=True, evaluation is also done on the training set",
    "and those metrics are considered for early stopping",
    "for lgb.train(), it's possible to pass the training data via valid_sets with any eval_name",
    "validation sets are guaranteed to not be identical to the training data in cv()",
    "split is needed for \"<dataset type> <metric>\" case (e.g. \"train l1\")",
    "self.best_score_list is initialized to an empty list",
    "split is needed for \"<dataset type> <metric>\" case (e.g. \"train l1\")",
    "coding: utf-8",
    "Acquire port in worker",
    "schedule futures to retrieve each element of the tuple",
    "retrieve ports",
    "Concatenate many parts into one",
    "construct local eval_set data.",
    "store indices of eval_set components that were not contained within local parts.",
    "consolidate parts of each individual eval component.",
    "require that eval_name exists in evaluated result data in case dropped due to padding.",
    "in distributed training the 'training' eval_set is not detected, will have name 'valid_<index>'.",
    "filter padding from eval parts then _concat each eval_set component.",
    "reconstruct eval_set fit args/kwargs depending on which components of eval_set are on worker.",
    "ensure that expected keys for evals_result_ and best_score_ exist regardless of padding.",
    "capture whether local_listen_port or its aliases were provided",
    "capture whether machines or its aliases were provided",
    "Some passed-in parameters can be removed:",
    "* 'num_machines': set automatically from Dask worker list",
    "* 'num_threads': overridden to match nthreads on each Dask process",
    "Split arrays/dataframes into parts. Arrange parts into dicts to enforce co-locality",
    "evals_set will to be re-constructed into smaller lists of (X, y) tuples, where",
    "X and y are each delayed sub-lists of original eval dask Collections.",
    "find maximum number of parts in an individual eval set so that we can",
    "pad eval sets when they come in different sizes.",
    "when individual eval set is equivalent to training data, skip recomputing parts.",
    "add None-padding for individual eval_set member if it is smaller than the largest member.",
    "first time a chunk of this eval set is added to this part.",
    "append additional chunks of this eval set to this part.",
    "ensure that all evaluation parts map uniquely to one part.",
    "assign sub-eval_set components to worker parts.",
    "Start computation in the background",
    "trigger error locally",
    "Find locations of all parts and map them to particular Dask workers",
    "Check that all workers were provided some of eval_set. Otherwise warn user that validation",
    "data artifacts may not be populated depending on worker returning final estimator.",
    "assign general validation set settings to fit kwargs.",
    "resolve aliases for network parameters and pop the result off params.",
    "these values are added back in calls to `_train_part()`",
    "figure out network params",
    "Tell each worker to train on the parts that it has locally",
    "",
    "This code treats ``_train_part()`` calls as not \"pure\" because:",
    "1. there is randomness in the training process unless parameters ``seed``",
    "and ``deterministic`` are set",
    "2. even with those parameters set, the output of one ``_train_part()`` call",
    "relies on global state (it and all the other LightGBM training processes",
    "coordinate with each other)",
    "if network parameters were changed during training, remove them from the",
    "returned model so that they're generated dynamically on every run based",
    "on the Dask cluster you're connected to and which workers have pieces of",
    "the training data",
    "dask.DataFrame.map_partitions() expects each call to return a pandas DataFrame or Series",
    "for multi-class classification with sparse matrices, pred_contrib predictions",
    "are returned as a list of sparse matrices (one per class)",
    "pred_contrib output will have one column per feature,",
    "plus one more for the base value",
    "need to tell Dask the expected type and shape of individual preds",
    "by default, dask.array.concatenate() concatenates sparse arrays into a COO matrix",
    "the code below is used instead to ensure that the sparse type is preserved during concatentation",
    "At this point, `out` is a list of lists of delayeds (each of which points to a matrix).",
    "Concatenate them to return a list of Dask Arrays.",
    "DaskLGBMClassifier does not support group, eval_group.",
    "DaskLGBMClassifier support for callbacks and init_model is not tested",
    "DaskLGBMRegressor does not support group, eval_class_weight, eval_group.",
    "DaskLGBMRegressor support for callbacks and init_model is not tested",
    "DaskLGBMRanker does not support eval_class_weight or early stopping",
    "DaskLGBMRanker support for callbacks and init_model is not tested",
    "coding: utf-8",
    "load or create your dataset",
    "create dataset for lightgbm",
    "if you want to re-use data, remember to set free_raw_data=False",
    "specify your configurations as a dict",
    "generate feature names",
    "feature_name and categorical_feature",
    "check feature name",
    "save model to file",
    "dump model to JSON (and save to file)",
    "feature names",
    "feature importances",
    "load model to predict",
    "can only predict with the best iteration (or the saving iteration)",
    "eval with loaded model",
    "dump model with pickle",
    "load model with pickle to predict",
    "can predict with any iteration when loaded in pickle way",
    "eval with loaded model",
    "continue training",
    "init_model accepts:",
    "1. model file name",
    "2. Booster()",
    "decay learning rates",
    "reset_parameter callback accepts:",
    "1. list with length = num_boost_round",
    "2. function(curr_iter)",
    "change other parameters during training",
    "self-defined objective function",
    "f(preds: array, train_data: Dataset) -> grad: array, hess: array",
    "log likelihood loss",
    "self-defined eval metric",
    "f(preds: array, train_data: Dataset) -> name: str, eval_result: float, is_higher_better: bool",
    "binary error",
    "NOTE: when you do customized loss function, the default prediction value is margin",
    "This may make built-in evaluation metric calculate wrong results",
    "For example, we are doing log likelihood loss, the prediction is score before logistic transformation",
    "Keep this in mind when you use the customization",
    "Pass custom objective function through params",
    "another self-defined eval metric",
    "f(preds: array, train_data: Dataset) -> name: str, eval_result: float, is_higher_better: bool",
    "accuracy",
    "NOTE: when you do customized loss function, the default prediction value is margin",
    "This may make built-in evaluation metric calculate wrong results",
    "For example, we are doing log likelihood loss, the prediction is score before logistic transformation",
    "Keep this in mind when you use the customization",
    "Pass custom objective function through params",
    "callback",
    "coding: utf-8",
    "load or create your dataset",
    "train",
    "predict",
    "eval",
    "feature importances",
    "self-defined eval metric",
    "f(y_true: array, y_pred: array) -> name: str, eval_result: float, is_higher_better: bool",
    "Root Mean Squared Logarithmic Error (RMSLE)",
    "train",
    "another self-defined eval metric",
    "f(y_true: array, y_pred: array) -> name: str, eval_result: float, is_higher_better: bool",
    "Relative Absolute Error (RAE)",
    "train",
    "predict",
    "eval",
    "other scikit-learn modules",
    "coding: utf-8",
    "load or create your dataset",
    "create dataset for lightgbm",
    "specify your configurations as a dict",
    "train",
    "coding: utf-8",
    "################",
    "Simulate some binary data with a single categorical and",
    "single continuous predictor",
    "################",
    "Set up a couple of utilities for our experiments",
    "################",
    "Observe the behavior of `binary` and `xentropy` objectives",
    "Trying this throws an error on non-binary values of y:",
    "experiment('binary', label_type='probability', DATA)",
    "The speed of `binary` is not drastically different than",
    "`xentropy`. `xentropy` runs faster than `binary` in many cases, although",
    "there are reasons to suspect that `binary` should run faster when the",
    "label is an integer instead of a float",
    "coding: utf-8",
    "load or create your dataset",
    "create dataset for lightgbm",
    "specify your configurations as a dict",
    "train",
    "save model to file",
    "predict",
    "eval",
    "We can also open HDF5 file once and get access to",
    "With binary dataset created, we can use either Python API or cmdline version to train.",
    "",
    "Note: in order to create exactly the same dataset with the one created in simple_example.py, we need",
    "to modify simple_example.py to pass numpy array instead of pandas DataFrame to Dataset constructor.",
    "The reason is that DataFrame column names will be used in Dataset. For a DataFrame with Int64Index",
    "as columns, Dataset will use column names like [\"0\", \"1\", \"2\", ...]. While for numpy array, column names",
    "are using the default one assigned in C++ code (dataset_loader.cpp), like [\"Column_0\", \"Column_1\", ...].",
    "Y has a single column and we read it in single shot. So store it as an 1-d array.",
    "We use random access for data sampling when creating LightGBM Dataset from Sequence.",
    "When accessing any element in a HDF5 chunk, it's read entirely.",
    "To save I/O for sampling, we should keep number of total chunks much larger than sample count.",
    "Here we are just creating a chunk size that matches with batch_size.",
    "",
    "Also note that the data is stored in row major order to avoid extra copy when passing to",
    "lightgbm Dataset.",
    "Save to 2 HDF5 files for demonstration.",
    "We can store multiple datasets inside a single HDF5 file.",
    "Separating X and Y for choosing best chunk size for data loading.",
    "split training data into two partitions",
    "make this array dense because we're splitting across",
    "a sparse boundary to partition the data",
    "the code below uses sklearn.metrics, but this requires pulling all of the",
    "predictions and target values back from workers to the client",
    "",
    "for larger datasets, consider the metrics from dask-ml instead",
    "https://ml.dask.org/modules/api.html#dask-ml-metrics-metrics",
    "coding: utf-8",
    "!/usr/bin/env python3",
    "-*- coding: utf-8 -*-",
    "",
    "LightGBM documentation build configuration file, created by",
    "sphinx-quickstart on Thu May  4 14:30:58 2017.",
    "",
    "This file is execfile()d with the current directory set to its",
    "containing dir.",
    "",
    "Note that not all possible configuration values are present in this",
    "autogenerated file.",
    "",
    "All configuration values have a default; values that are commented out",
    "serve to show the default.",
    "If extensions (or modules to document with autodoc) are in another directory,",
    "add these directories to sys.path here. If the directory is relative to the",
    "documentation root, use os.path.abspath to make it absolute.",
    "-- General configuration ------------------------------------------------",
    "If your documentation needs a minimal Sphinx version, state it here.",
    "Add any Sphinx extension module names here, as strings. They can be",
    "extensions coming with Sphinx (named 'sphinx.ext.*') or your custom",
    "ones.",
    "mock out modules",
    "hide type hints in API docs",
    "Generate autosummary pages. Output should be set with: `:toctree: pythonapi/`",
    "Only the class' docstring is inserted.",
    "If true, `todo` and `todoList` produce output, else they produce nothing.",
    "The master toctree document.",
    "General information about the project.",
    "The name of an image file (relative to this directory) to place at the top",
    "of the sidebar.",
    "The name of an image file (relative to this directory) to use as a favicon of",
    "the docs.  This file should be a Windows icon file (.ico) being 16x16 or 32x32",
    "pixels large.",
    "The version info for the project you're documenting, acts as replacement for",
    "|version| and |release|, also used in various other places throughout the",
    "built documents.",
    "The short X.Y version.",
    "The full version, including alpha/beta/rc tags.",
    "The language for content autogenerated by Sphinx. Refer to documentation",
    "for a list of supported languages.",
    "",
    "This is also used if you do content translation via gettext catalogs.",
    "Usually you set \"language\" from the command line for these cases.",
    "List of patterns, relative to source directory, that match files and",
    "directories to ignore when looking for source files.",
    "This patterns also effect to html_static_path and html_extra_path",
    "The name of the Pygments (syntax highlighting) style to use.",
    "-- Configuration for C API docs generation ------------------------------",
    "-- Options for HTML output ----------------------------------------------",
    "The theme to use for HTML and HTML Help pages.  See the documentation for",
    "a list of builtin themes.",
    "Theme options are theme-specific and customize the look and feel of a theme",
    "further.  For a list of options available for each theme, see the",
    "documentation.",
    "Add any paths that contain custom static files (such as style sheets) here,",
    "relative to this directory. They are copied after the builtin static files,",
    "so a file named \"default.css\" will overwrite the builtin \"default.css\".",
    "-- Options for HTMLHelp output ------------------------------------------",
    "Output file base name for HTML help builder.",
    "-- Options for LaTeX output ---------------------------------------------",
    "The name of an image file (relative to this directory) to place at the top of",
    "the title page.",
    "intersphinx configuration",
    "Warning! The following code can cause buffer overflows on RTD.",
    "Consider suppressing output completely if RTD project silently fails.",
    "Refer to https://github.com/svenevs/exhale",
    "/blob/fe7644829057af622e467bb529db6c03a830da99/exhale/deploy.py#L99-L111",
    "Warning! The following code can cause buffer overflows on RTD.",
    "Consider suppressing output completely if RTD project silently fails.",
    "Refer to https://github.com/svenevs/exhale",
    "/blob/fe7644829057af622e467bb529db6c03a830da99/exhale/deploy.py#L99-L111",
    "coding: utf-8",
    "This is a basic test for floating number parsing.",
    "Most of the test cases come from:",
    "https://github.com/dmlc/xgboost/blob/master/tests/cpp/common/test_charconv.cc",
    "https://github.com/Alexhuszagh/rust-lexical/blob/master/data/test-parse-unittests/strtod_tests.toml",
    "FLT_MAX",
    "FLT_MIN",
    "DBL_MAX (1 + (1 - 2^-52)) * 2^1023 = (2^53 - 1) * 2^971",
    "2^971 * (2^53 - 1 + 1/2) : the smallest number resolving to inf",
    "Near DBL_MIN",
    "DBL_MIN 2^-1022",
    "The behavior for parsing -nan depends on implementation.",
    "Thus we skip binary check for negative nan.",
    "See comment in test_cases.",
    "construct sample data first (use all data for convenience and since size is small)",
    "Load some test data",
    "Use the smaller \".test\" data because we don't care about the actual data and it's smaller",
    "Add some fake initial_scores and groups so we can test streaming them",
    "Now use the reference dataset schema to make some testable Datasets with N rows each",
    "Load some test data",
    "Use the smaller \".test\" data because we don't care about the actual data and it's smaller",
    "Add some fake initial_scores and groups so we can test streaming them",
    "Now use the reference dataset schema to make some testable Datasets with N rows each",
    "NOTE: Arrow arrays have 64-bit alignment but we can safely ignore this in tests",
    "1) Create validity bitmap",
    "2) Create buffers",
    "Create arrow array",
    "Arithmetic",
    "Subscripts",
    "End",
    "Load some test data",
    "Serialize the reference",
    "Deserialize the reference",
    "Confirm 1 successful API call",
    "Free memory",
    "Test that Data() points to first value written",
    "Constants",
    "Start with some content:",
    "Clear & re-use:",
    "Output should match new content:",
    "Number of trials for each new ChunkedArray configuration. Pass 100 times over the search space:",
    "Each outer loop iteration changes the test by adding +1 chunk. We start with 1 chunk only:",
    "Sweep valid and invalid addresses with a ChunkedArray with `chunks` chunks:",
    "Compute a new trial address & value & if it is a valid address:",
    "Insert item. If at a valid address, 0 is returned, otherwise, -1 is returned:",
    "If at valid address, check that the stored value is correct & remember it for the future:",
    "Check the just-stored value with getitem():",
    "Also store the just-stored value for future tracking:",
    "Final check: ensure even with overrides, all valid insertions store the latest value at that address:",
    "Test in 2 ways that the values are correctly laid out in memory:",
    "Since init_scores are in a column format, but need to be pushed as rows, we have to extract each batch",
    "Use multiple threads to test concurrency",
    "Since init_scores are in a column format, but need to be pushed as rows, we have to extract each batch",
    "Calculate expected boundaries",
    "Extract a set of rows from the column-based format (still maintaining column based format)",
    "coding: utf-8",
    "at initialization, should be -1",
    "updating that value through the C API should work",
    "resetting to any negative number should set it to -1",
    "coding: utf-8",
    "check saved model persistence",
    "we need to check the consistency of model file here, so test for exact equal",
    "check early stopping is working. Make it stop very early, so the scores should be very close to zero",
    "scores likely to be different, but prediction should still be the same",
    "test that shape is checked during prediction",
    "The simple implementation is just a single \"return self.ndarray[idx]\"",
    "The following is for demo and testing purpose.",
    "whole col",
    "half col",
    "Create dataset from numpy array directly.",
    "Create dataset using Sequence.",
    "Test for validation set.",
    "Select some random rows as valid data.",
    "From Dataset constructor, with dataset from numpy array.",
    "From Dataset.create_valid, with dataset from sequence.",
    "test that method works even with free_raw_data=True",
    "test that method works but sets raw data to None in case of immergeable data types",
    "test that method works for different data types",
    "Set extremely harsh penalties, so CEGB will block most splits.",
    "Compare pairs of penalties, to ensure scaling works as intended",
    "Reset booster1's parameters to p2, so the parameter section of the file matches.",
    "unconstructed, get_* methods should return whatever was provided",
    "before construction, get_field() should raise an exception",
    "constructed, get_* methods should return numpy arrays, even when the provided",
    "input was a list of floats or ints",
    "get_field(\"group\") returns a numpy array with boundaries, instead of size",
    "NOTE: \"position\" is converted to int32 on the C++ side",
    "should resolve duplicate aliases, and prefer the main parameter",
    "should choose the highest priority alias and set that value on main param",
    "if only aliases are used",
    "should use the default if main param and aliases are missing",
    "all changes should be made on copies and not modify the original",
    "preserves None found for main param and still removes aliases",
    "correctly chooses value when only an alias is provided",
    "adds None if that's given as the default and param not found",
    "If callable is found in objective",
    "Value in params should be preferred to the default_value passed from keyword arguments",
    "None of objective or its aliases in params, but default_value is callable.",
    "check that the original data wasn't modified",
    "check that the built data has the codes",
    "if all categories were seen during training we just take the codes",
    "if we only saw 'a' during training we just replace its code",
    "and leave the rest as nan",
    "test using defined feature names",
    "test using default feature names",
    "check for feature indices outside of range",
    "coding: utf-8",
    "add target, weight, and group to DataFrame so that partitions abide by group boundaries.",
    "set_index ensures partitions are based on group id.",
    "See https://stackoverflow.com/questions/49532824/dask-dataframe-split-partitions-based-on-a-column-or-function.",
    "separate target, weight from features.",
    "encode group identifiers into run-length encoding, the format LightGBMRanker is expecting",
    "so that within each partition, sum(g) = n_samples.",
    "ranking arrays: one chunk per group. Each chunk must include all columns.",
    "make one categorical feature relevant to the target",
    "https://github.com/microsoft/LightGBM/issues/4118",
    "extra predict() parameters should be passed through correctly",
    "pref_leaf values should have the right shape",
    "and values that look like valid tree nodes",
    "be sure LightGBM actually used at least one categorical column,",
    "and that it was correctly treated as a categorical feature",
    "shape depends on whether it is binary or multiclass classification",
    "in the special case of multi-class classification using scipy sparse matrices,",
    "the output of `.predict(..., pred_contrib=True)` is a list of sparse matrices (one per class)",
    "",
    "since that case is so different than all other cases, check the relevant things here",
    "and then return early",
    "raw scores will probably be different, but at least check that all predicted classes are the same",
    "be sure LightGBM actually used at least one categorical column,",
    "and that it was correctly treated as a categorical feature",
    "* shape depends on whether it is binary or multiclass classification",
    "* matrix for binary classification is of the form [feature_contrib, base_value],",
    "for multi-class it's [feat_contrib_class1, base_value_class1, feat_contrib_class2, base_value_class2, etc.]",
    "* contrib outputs for distributed training are different than from local training, so we can just test",
    "that the output has the right shape and base values are in the right position",
    "with a custom objective, prediction result is a raw score instead of predicted class",
    "function should have been preserved",
    "should correctly classify every sample",
    "probability estimates should be similar",
    "Scores should be the same",
    "Predictions should be roughly the same.",
    "pref_leaf values should have the right shape",
    "and values that look like valid tree nodes",
    "extra predict() parameters should be passed through correctly",
    "be sure LightGBM actually used at least one categorical column,",
    "and that it was correctly treated as a categorical feature",
    "contrib outputs for distributed training are different than from local training, so we can just test",
    "that the output has the right shape and base values are in the right position",
    "be sure LightGBM actually used at least one categorical column,",
    "and that it was correctly treated as a categorical feature",
    "Quantiles should be right",
    "be sure LightGBM actually used at least one categorical column,",
    "and that it was correctly treated as a categorical feature",
    "function should have been preserved",
    "Scores should be the same",
    "local and Dask predictions should be the same",
    "predictions should be better than random",
    "rebalance small dask.Array dataset for better performance.",
    "use many trees + leaves to overfit, help ensure that Dask data-parallel strategy matches that of",
    "serial learner. See https://github.com/microsoft/LightGBM/issues/3292#issuecomment-671288210.",
    "distributed ranker should be able to rank decently well and should",
    "have high rank correlation with scores from serial ranker.",
    "extra predict() parameters should be passed through correctly",
    "pref_leaf values should have the right shape",
    "and values that look like valid tree nodes",
    "be sure LightGBM actually used at least one categorical column,",
    "and that it was correctly treated as a categorical feature",
    "rebalance small dask.Array dataset for better performance.",
    "distributed ranker should be able to rank decently well with the least-squares objective",
    "and should have high rank correlation with scores from serial ranker.",
    "function should have been preserved",
    "Use larger trainset to prevent premature stopping due to zero loss, causing num_trees() < n_estimators.",
    "Use small chunk_size to avoid single-worker allocation of eval data partitions.",
    "test eval_class_weight, eval_init_score on binary-classification task.",
    "Note: objective's default `metric` will be evaluated in evals_result_ in addition to all eval_metrics.",
    "create eval_sets by creating new datasets or copying training data.",
    "total number of trees scales up for ova classifier.",
    "check that early stopping was not applied.",
    "checks that evals_result_ and best_score_ contain expected data and eval_set names.",
    "check that each eval_name and metric exists for all eval sets, allowing for the",
    "case when a worker receives a fully-padded eval_set component which is not evaluated.",
    "should be able to use the class without specifying a client",
    "should be able to set client after construction",
    "data on cluster1",
    "create identical data on cluster2",
    "at this point, the result of default_client() is client2 since it was the most recently",
    "created. So setting client to client1 here to test that you can select a non-default client",
    "unfitted model should survive pickling round trip, and pickling",
    "shouldn't have side effects on the model object",
    "client will always be None after unpickling",
    "fitted model should survive pickling round trip, and pickling",
    "shouldn't have side effects on the model object",
    "client will always be None after unpickling",
    "rebalance data to be sure that each worker has a piece of the data",
    "model 1 - no network parameters given",
    "model 2 - machines given",
    "model 3 - local_listen_port given",
    "training should fail because LightGBM will try to use the same",
    "port for multiple worker processes on the same machine",
    "rebalance data to be sure that each worker has a piece of the data",
    "test that \"machines\" is actually respected by creating a socket that uses",
    "one of the ports mentioned in \"machines\"",
    "The above error leaves a worker waiting",
    "an informative error should be raised if \"machines\" has duplicates",
    "\"client\" should be the only different, and the final argument",
    "value of the root node is 0 when init_score is set",
    "this test is separate because it takes a not-yet-constructed estimator",
    "coding: utf-8",
    "coding: utf-8",
    "build target, group ID vectors.",
    "build y/target and group-id vectors with user-specified group sizes.",
    "build y/target and group-id vectors according to n_samples, avg_gs, and random_gs.",
    "groups should contain > 1 element for pairwise learning objective.",
    "build feature data, X. Transform first few into informative features.",
    "doing this here, at import time, to ensure it only runs once_per import",
    "instead of once per assertion",
    "coding: utf-8",
    "----------------------------------------------------------------------------------------------- #",
    "UTILITIES                                            #",
    "----------------------------------------------------------------------------------------------- #",
    "Set random nulls",
    "Split data into <=2 random chunks",
    "Turn chunks into array",
    "----------------------------------------------------------------------------------------------- #",
    "UNIT TESTS                                           #",
    "----------------------------------------------------------------------------------------------- #",
    "------------------------------------------- DATASET ------------------------------------------- #",
    "-------------------------------------------- FIELDS ------------------------------------------- #",
    "Check for equality",
    "-------------------------------------------- LABELS ------------------------------------------- #",
    "------------------------------------------- WEIGHTS ------------------------------------------- #",
    "-------------------------------------------- GROUPS ------------------------------------------- #",
    "----------------------------------------- INIT SCORES ----------------------------------------- #",
    "------------------------------------------ PREDICTION ----------------------------------------- #",
    "coding: utf-8",
    "check that really dummy objective was used and estimator didn't learn anything",
    "prediction result is actually not transformed (is raw) due to custom objective",
    "original estimator is unaffected",
    "new estimator is unfitted, but has the same parameters",
    "Test if random_state is properly stored",
    "Test if two random states produce identical models",
    "Test if subsequent fits sample from random_state object and produce different models",
    "Test that the largest element is NOT the same, the smallest can be the same, i.e. zero",
    "With default params",
    "Tests same probabilities",
    "Tests same predictions",
    "Tests same raw scores",
    "Tests same leaf indices",
    "Tests same feature contributions",
    "Tests other parameters for the prediction works",
    "Tests start_iteration",
    "Tests same probabilities, starting from iteration 10",
    "Tests same predictions, starting from iteration 10",
    "Tests same raw scores, starting from iteration 10",
    "Tests same leaf indices, starting from iteration 10",
    "Tests same feature contributions, starting from iteration 10",
    "Tests other parameters for the prediction works, starting from iteration 10",
    "test that params passed in predict have higher priority",
    "no custom objective, no custom metric",
    "default metric",
    "non-default metric",
    "no metric",
    "non-default metric in eval_metric",
    "non-default metric with non-default metric in eval_metric",
    "non-default metric with multiple metrics in eval_metric",
    "non-default metric with multiple metrics in eval_metric for LGBMClassifier",
    "default metric for non-default objective",
    "non-default metric for non-default objective",
    "no metric",
    "non-default metric in eval_metric for non-default objective",
    "non-default metric with non-default metric in eval_metric for non-default objective",
    "non-default metric with multiple metrics in eval_metric for non-default objective",
    "custom objective, no custom metric",
    "default regression metric for custom objective",
    "non-default regression metric for custom objective",
    "multiple regression metrics for custom objective",
    "no metric",
    "default regression metric with non-default metric in eval_metric for custom objective",
    "non-default regression metric with metric in eval_metric for custom objective",
    "multiple regression metrics with metric in eval_metric for custom objective",
    "multiple regression metrics with multiple metrics in eval_metric for custom objective",
    "no custom objective, custom metric",
    "default metric with custom metric",
    "non-default metric with custom metric",
    "multiple metrics with custom metric",
    "custom metric (disable default metric)",
    "default metric for non-default objective with custom metric",
    "non-default metric for non-default objective with custom metric",
    "multiple metrics for non-default objective with custom metric",
    "custom metric (disable default metric for non-default objective)",
    "custom objective, custom metric",
    "custom metric for custom objective",
    "non-default regression metric with custom metric for custom objective",
    "multiple regression metrics with custom metric for custom objective",
    "default metric and invalid binary metric is replaced with multiclass alternative",
    "invalid binary metric is replaced with multiclass alternative",
    "default metric for non-default multiclass objective",
    "and invalid binary metric is replaced with multiclass alternative",
    "default metric and invalid multiclass metric is replaced with binary alternative",
    "invalid multiclass metric is replaced with binary alternative for custom objective",
    "Verify that can receive a list of metrics, only callable",
    "Verify that can receive a list of custom and built-in metrics",
    "Verify that works as expected when eval_metric is empty",
    "Verify that can receive a list of metrics, only built-in",
    "Verify that eval_metric is robust to receiving a list with None",
    "feval",
    "single eval_set",
    "two eval_set",
    "'val_minus_two' here is the expected number of threads for n_jobs=-2",
    "Note: according to joblib's formula, a value of n_jobs=-2 means",
    "\"use all but one thread\" (formula: n_cpus + 1 + n_jobs)",
    "try to predict with a different feature",
    "check that disabling the check doesn't raise the error",
    "make weights and init_score same types as y, just to avoid",
    "a huge number of combinations and therefore test cases",
    "make weights and init_score same types as y, just to avoid",
    "a huge number of combinations and therefore test cases",
    "coding: utf-8",
    "we're in a leaf now",
    "check that the rest of the elements have black color",
    "check that we got to the expected leaf",
    "coding: utf-8",
    "coding: utf-8",
    "check that default gives same result as k = 1",
    "check against independent calculation for k = 1",
    "check against independent calculation for k = 2",
    "check against independent calculation for k = 10",
    "check cases where predictions are equal",
    "should give same result as binary auc for 2 classes",
    "test the case where all predictions are equal",
    "test that weighted data gives different auc_mu",
    "test that equal data weights give same auc_mu as unweighted data",
    "should give 1 when accuracy = 1",
    "test loading class weights",
    "Simulates position bias for a given ranking dataset.",
    "The ouput dataset is identical to the input one with the exception for the relevance labels.",
    "The new labels are generated according to an instance of a cascade user model:",
    "for each query, the user is simulated to be traversing the list of documents ranked by a baseline ranker",
    "(in our example it is simply the ordering by some feature correlated with relevance, e.g., 34)",
    "and clicks on that document (new_label=1) with some probability 'pclick' depending on its true relevance;",
    "at each position the user may stop the traversal with some probability pstop. For the non-clicked documents,",
    "new_label=0. Thus the generated new labels are biased towards the baseline ranker.",
    "The positions of the documents in the ranked lists produced by the baseline, are returned.",
    "a mapping of a document's true relevance (defined on a 5-grade scale) into the probability of clicking it",
    "an instantiation of a cascade model where the user stops with probability 0.2 after observing each document",
    "simulate position bias for the train dataset and put the train dataset with biased labels to temp directory",
    "the performance of the unbiased LambdaMART should outperform the plain LambdaMART on the dataset with position bias",
    "add extra row to position file",
    "simulate position bias for the train dataset and put the train dataset with biased labels to temp directory",
    "test setting positions through Dataset constructor with numpy array",
    "the performance of the unbiased LambdaMART should outperform the plain LambdaMART on the dataset with position bias",
    "test setting positions through Dataset constructor with pandas Series",
    "test setting positions through set_position",
    "test get_position works",
    "no early stopping",
    "early stopping occurs",
    "regular early stopping",
    "positive min_delta",
    "test custom eval metrics",
    "shuffle = False, override metric in params",
    "shuffle = True, callbacks",
    "enable display training loss",
    "self defined folds",
    "LambdaRank",
    "... with l2 metric",
    "... with NDCG (default) metric",
    "self defined folds with lambdarank",
    "init_model from an in-memory Booster",
    "init_model from a text file",
    "predictions should be identical",
    "with early stopping",
    "predict by each fold booster",
    "check that each booster predicted using the best iteration",
    "fold averaging",
    "without early stopping",
    "test feature_names with whitespaces",
    "This has non-ascii strings.",
    "check that passing parameters to the constructor raises warning and ignores them",
    "check inference isn't affected by unknown parameter",
    "entries whose values should reflect params passed to lgb.train()",
    "'l1' was passed in with alias 'mae'",
    "NOTE: this was passed in with alias 'sub_row'",
    "entries with default values of params",
    "add device-specific entries",
    "",
    "passed-in force_col_wise / force_row_wise parameters are ignored on CUDA and GPU builds...",
    "https://github.com/microsoft/LightGBM/blob/1d7ee63686272bceffd522284127573b511df6be/src/io/config.cpp#L375-L377",
    "check that model text has all expected param entries",
    "since Booster.model_to_string() is used when pickling, check that parameters all",
    "roundtrip pickling successfully too",
    "take subsets and train",
    "generate CSR sparse dataset",
    "convert data to dense and get back same contribs",
    "validate the values are the same",
    "validate using CSC matrix",
    "validate the values are the same",
    "generate CSR sparse dataset",
    "convert data to dense and get back same contribs",
    "validate the values are the same",
    "validate using CSC matrix",
    "validate the values are the same",
    "Note there is an extra column added to the output for the expected value",
    "Note output CSC shape should be same as CSR output shape",
    "test sliced labels",
    "append some columns",
    "append some rows",
    "test sliced 2d matrix",
    "test sliced CSR",
    "trees start at position 1.",
    "split_features are in 4th line.",
    "test if a penalty as high as the depth indeed prohibits all monotone splits",
    "The penalization is so high that the first 2 features should not be used here",
    "Check that a very high penalization is the same as not using the features at all",
    "check refit accepts dataset_params",
    "the following checks that dart and rf with mape can predict outside the 0-1 range",
    "https://github.com/microsoft/LightGBM/issues/1579",
    "no custom objective, no feval",
    "default metric",
    "non-default metric in params",
    "default metric in args",
    "non-default metric in args",
    "metric in args overwrites one in params",
    "metric in args overwrites one in params",
    "multiple metrics in params",
    "multiple metrics in args",
    "remove default metric by 'None' in list",
    "remove default metric by 'None' aliases",
    "custom objective, no feval",
    "no default metric",
    "metric in params",
    "metric in args",
    "metric in args overwrites its' alias in params",
    "multiple metrics in params",
    "multiple metrics in args",
    "no custom objective, feval",
    "default metric with custom one",
    "non-default metric in params with custom one",
    "default metric in args with custom one",
    "non-default metric in args with custom one",
    "metric in args overwrites one in params, custom one is evaluated too",
    "multiple metrics in params with custom one",
    "multiple metrics in args with custom one",
    "custom metric is evaluated despite 'None' is passed",
    "custom objective, feval",
    "no default metric, only custom one",
    "metric in params with custom one",
    "metric in args with custom one",
    "metric in args overwrites one in params, custom one is evaluated too",
    "multiple metrics in params with custom one",
    "multiple metrics in args with custom one",
    "custom metric is evaluated despite 'None' is passed",
    "no custom objective, no feval",
    "default metric",
    "default metric in params",
    "non-default metric in params",
    "multiple metrics in params",
    "remove default metric by 'None' aliases",
    "custom objective, no feval",
    "no default metric",
    "metric in params",
    "multiple metrics in params",
    "no custom objective, feval",
    "default metric with custom one",
    "default metric in params with custom one",
    "non-default metric in params with custom one",
    "multiple metrics in params with custom one",
    "custom metric is evaluated despite 'None' is passed",
    "custom objective, feval",
    "no default metric, only custom one",
    "metric in params with custom one",
    "multiple metrics in params with custom one",
    "custom metric is evaluated despite 'None' is passed",
    "Custom objective replaces multiclass",
    "multiclass default metric",
    "multiclass default metric with custom one",
    "multiclass metric alias with custom one for custom objective",
    "no metric for invalid class_num",
    "custom metric for invalid class_num",
    "multiclass metric alias with custom one with invalid class_num",
    "multiclass default metric without num_class",
    "multiclass metric alias",
    "multiclass metric",
    "non-valid metric for multiclass objective",
    "non-default num_class for default objective",
    "no metric with non-default num_class for custom objective",
    "multiclass metric alias for custom objective",
    "multiclass metric for custom objective",
    "binary metric with non-default num_class for custom objective",
    "Expect three metrics but mean and stdv for each metric",
    "test XGBoost-style return value",
    "test numpy-style return value",
    "test bins string type",
    "test histogram is disabled for categorical features",
    "test for lgb.train",
    "test feval for lgb.train",
    "test with two valid data for lgb.train",
    "test for lgb.cv",
    "test feval for lgb.cv",
    "test that binning works properly for features with only positive or only negative values",
    "decreasing without freeing raw data is allowed",
    "decreasing before lazy init is allowed",
    "increasing is allowed",
    "decreasing with disabled filter is allowed",
    "decreasing with enabled filter is disallowed;",
    "also changes of other params are disallowed",
    "check extra trees increases regularization",
    "check path smoothing increases regularization",
    "test edge case with one leaf",
    "check that constraint containing all features is equivalent to no constraint",
    "check that constraint partitioning the features reduces train accuracy",
    "check that constraints consisting of single features reduce accuracy further",
    "test that interaction constraints work when not all features are used",
    "check that number of threads does not affect result",
    "check that setting linear_tree=True fits better than ordinary trees when data has linear relationship",
    "test again with nans in data",
    "test again with bagging",
    "test with a feature that has only one non-nan value",
    "test with a categorical feature",
    "test refit: same results on same data",
    "test refit with save and load",
    "test refit: different results training on different data",
    "test when num_leaves - 1 < num_features and when num_leaves - 1 > num_features",
    "test that the predict once with all iterations equals summed results with start_iteration and num_iteration",
    "test the case where start_iteration <= 0, and num_iteration is None",
    "test the case where start_iteration > 0, and num_iteration <= 0",
    "test the case where start_iteration > 0, and num_iteration <= 0, with pred_leaf=True",
    "test the case where start_iteration > 0, and num_iteration <= 0, with pred_contrib=True",
    "test for regression",
    "test both with and without early stopping",
    "test for multi-class",
    "test both with and without early stopping",
    "test for binary",
    "test both with and without early stopping",
    "test against sklearn average precision metric",
    "test that average precision is 1 where model predicts perfectly",
    "data as float64",
    "test all features were used",
    "test the score is better than predicting the mean",
    "test all predictions are equal using different input dtypes",
    "introduce some missing values",
    "the previous line turns x3 into object dtype in recent versions of pandas",
    "train with regular dtypes",
    "convert to nullable dtypes",
    "test training succeeds",
    "test all features were used",
    "test the score is better than predicting the mean",
    "test equal predictions",
    "test data are taken from bug report",
    "https://github.com/microsoft/LightGBM/issues/4708",
    "modified from https://github.com/microsoft/LightGBM/issues/3679#issuecomment-938652811",
    "and https://github.com/microsoft/LightGBM/pull/5087",
    "test that the ``splits_per_leaf_`` of CEGB is cleaned before training a new tree",
    "which is done in the fix #5164",
    "without the fix:",
    "Check failed: (best_split_info.left_count) > (0)",
    "try to predict with a different feature",
    "check that disabling the check doesn't raise the error",
    "try to refit with a different feature",
    "check that disabling the check doesn't raise the error",
    "coding: utf-8",
    "If compiled appropriately, the same installation will support both GPU and CPU.",
    "Double-precision floats are only supported on x86_64 with PoCL",
    "coding: utf-8",
    "coding: utf-8",
    "These are helper functions to allow doing a stack unwind",
    "after an R allocation error, which would trigger a long jump.",
    "convert from one-based to zero-based index",
    "if any feature names were larger than allocated size,",
    "allow for a larger size and try again",
    "convert from boundaries to size",
    "--- start Booster interfaces",
    "if any eval names were larger than allocated size,",
    "allow for a larger size and try again",
    "if the model string was larger than the initial buffer, call the function again, writing directly to the R object",
    "if the model string was larger than the initial buffer, allocate a bigger buffer and try again",
    "if aliases string was larger than the initial buffer, allocate a bigger buffer and try again",
    "if aliases string was larger than the initial buffer, allocate a bigger buffer and try again",
    ".Call() calls",
    "coding: utf-8",
    "alias table",
    "names",
    "from strings",
    "tails",
    "tails",
    "the following are stored as comma separated strings but are arrays in the wrappers",
    "coding: utf-8",
    "Single row predictor to abstract away caching logic",
    "create boosting",
    "initialize the boosting",
    "create objective function",
    "initialize the objective function",
    "create training metric",
    "reset the boosting",
    "create objective function",
    "initialize the objective function",
    "calculate the nonzero data and indices size",
    "allocate data and indices arrays",
    "Get the number of trees per iteration (for multiclass scenario we output multiple sparse matrices)",
    "aggregated per row feature contribution results",
    "keep track of the row_vector sizes for parallelization",
    "copy vector results to output for each row",
    "Get the number of trees per iteration (for multiclass scenario we output multiple sparse matrices)",
    "aggregated per row feature contribution results",
    "calculate number of elements per column to construct",
    "the CSC matrix with random access",
    "keep track of column counts",
    "keep track of beginning index for each column",
    "keep track of beginning index for each matrix",
    "Note: we parallelize across matrices instead of rows because of the column_counts[m][col_idx] increment inside the loop",
    "store the row index",
    "update column count",
    "explicitly declare symbols from LightGBM namespace",
    "some help functions used to convert data",
    "Row iterator of on column for CSC matrix",
    "return value at idx, only can access by ascent order",
    "return next non-zero pair, if index < 0, means no more data",
    "start of c_api functions",
    "This API is to keep python binding's behavior the same with C++ implementation.",
    "Sample count, random seed etc. should be provided in parameters.",
    "convert internal thread id to be unique based on external thread id",
    "convert internal thread id to be unique based on external thread id",
    "sample data first",
    "sample data first",
    "sample data first",
    "local buffer to re-use memory",
    "sample data first",
    "no more data",
    "Prepare the Arrow data",
    "Initialize the dataset",
    "If there is no reference dataset, we first sample indices",
    "Then, we obtain sample values by parallelizing across columns",
    "Values need to be copied from the record batches.",
    "The chunks are iterated over in the inner loop as columns can be treated independently.",
    "Finally, we initialize a loader from the sampled values",
    "After sampling and properly initializing all bins, we can add our data to the dataset. Here,",
    "we parallelize across rows.",
    "---- start of booster",
    "Single row in row-major format:",
    "Apply the configuration",
    "Set up chunked array and iterators for all columns",
    "Build row function",
    "Run prediction",
    "---- start of some help functions",
    "data is array of pointers to individual rows",
    "set number of threads for openmp",
    "read parameters from config file",
    "remove str after \"#\"",
    "de-duplicate params",
    "prediction is needed if using input initial model(continued train)",
    "need to continue training",
    "sync up random seed for data partition",
    "load Training data",
    "load data for distributed training",
    "load data for single machine",
    "need save binary file",
    "create training metric",
    "only when have metrics then need to construct validation data",
    "Add validation data, if it exists",
    "add",
    "need save binary file",
    "add metric for validation data",
    "output used time on each iteration",
    "need init network",
    "create boosting",
    "create objective function",
    "load training data",
    "initialize the objective function",
    "initialize the boosting",
    "add validation data into boosting",
    "convert model to if-else statement code",
    "create predictor",
    "Free memory",
    "create predictor",
    "label_gain = 2^i - 1, may overflow, so we use 31 here",
    "counts for all labels",
    "start from top label, and accumulate DCG",
    "counts for all labels",
    "calculate k Max DCG by one pass",
    "get sorted indices by score",
    "calculate multi dcg by one pass",
    "wait for all client start up",
    "Don't call MPI_Finalize() here: If the destructor was called because only this node had an exception, calling MPI_Finalize() will cause all nodes to hang.",
    "Instead we will handle finalize/abort for MPI in main().",
    "default set to -1",
    "distance at k-th communication, distance[k] = 2^k",
    "set incoming rank at k-th commuication",
    "set outgoing rank at k-th commuication",
    "default set as -1",
    "construct all recursive halving map for all machines",
    "let 1 << k <= num_machines",
    "distance of each communication",
    "if num_machines = 2^k, don't need to group machines",
    "communication direction, %2 == 0 is positive",
    "neighbor at k-th communication",
    "receive data block at k-th communication",
    "send data block at k-th communication",
    "if num_machines != 2^k, need to group machines",
    "group, two machine in one group, total \"rest\" groups will have 2 machines.",
    "let left machine as group leader",
    "cache block information for groups, group with 2 machines will have double block size",
    "convert from group to node leader",
    "convert from node to group",
    "meet new group",
    "add block len for this group",
    "calculate the group block start",
    "not need to construct",
    "get receive block information",
    "accumulate block len",
    "get send block information",
    "accumulate block len",
    "static member definition",
    "if small package or small count , do it by all gather.(reduce the communication times.)",
    "assign the blocks to every rank.",
    "do reduce scatter",
    "do all gather",
    "assign blocks",
    "need use buffer here, since size of \"output\" is smaller than size after all gather",
    "copy back",
    "assign blocks",
    "start all gather",
    "when num_machines is small and data is large",
    "use output as receive buffer",
    "get current local block size",
    "get out rank",
    "get in rank",
    "get send information",
    "get recv information",
    "send and recv at same time",
    "rotate in-place",
    "use output as receive buffer",
    "get current local block size",
    "get send information",
    "get recv information",
    "send and recv at same time",
    "use output as receive buffer",
    "send and recv at same time",
    "send local data to neighbor first",
    "receive neighbor data first",
    "reduce",
    "get target",
    "get send information",
    "get recv information",
    "send and recv at same time",
    "reduce",
    "send result to neighbor",
    "receive result from neighbor",
    "copy result",
    "start up socket",
    "parse clients from file",
    "get ip list of local machine",
    "get local rank",
    "construct listener",
    "construct communication topo",
    "construct linkers",
    "free listener",
    "set timeout",
    "accept incoming socket",
    "receive rank",
    "add new socket",
    "save ranks that need to connect with",
    "start listener",
    "start connect",
    "let smaller rank connect to larger rank",
    "send local rank",
    "wait for listener",
    "print connected linkers",
    "only need to copy subset",
    "avoid to copy subset many times",
    "avoid out of range",
    "may need to recopy subset",
    "valid the type",
    "parser factory implementation.",
    "customized parser add-on.",
    "save header to parser config in case needed.",
    "save label id to parser config in case needed.",
    "Constructors",
    "Get type tag",
    "Comparisons",
    "This has to be separate, not in Statics, because Json() accesses",
    "statics().null.",
    "advance until next line, or end of input",
    "advance until closing tokens",
    "The usual case: non-escaped characters",
    "Handle escapes",
    "Extract 4-byte escape sequence",
    "Explicitly check length of the substring. The following loop",
    "relies on std::string returning the terminating NUL when",
    "accessing str[length]. Checking here reduces brittleness.",
    "JSON specifies that characters outside the BMP shall be encoded as a",
    "pair of 4-hex-digit \\u escapes encoding their surrogate pair",
    "components. Check whether we're in the middle of such a beast: the",
    "previous codepoint was an escaped lead (high) surrogate, and this is",
    "a trail (low) surrogate.",
    "Reassemble the two surrogate pairs into one astral-plane character,",
    "per the UTF-16 algorithm.",
    "Integer part",
    "Decimal part",
    "Exponent part",
    "Check for any trailing garbage",
    "Documented in json11.hpp",
    "Check for another object",
    "get column names",
    "support to get header from parser config, so could utilize following label name to id mapping logic.",
    "load label idx first",
    "if parser config file exists, feature names may be changed after customized parser applied.",
    "clear here so could use default filled feature names during dataset construction.",
    "may improve by saving real feature names defined in parser in the future.",
    "erase label column name",
    "load ignore columns",
    "load weight idx",
    "load group idx",
    "don't support query id in data file when using distributed training",
    "read data to memory",
    "sample data",
    "construct feature bin mappers & clear sample data",
    "initialize label",
    "extract features",
    "sample data from file",
    "construct feature bin mappers & clear sample data",
    "initialize label",
    "extract features",
    "load data from binary file",
    "checks whether there's a initial score file when loaded from binary data files",
    "the intial score file should with suffix \".bin.init\"",
    "check meta data",
    "need to check training data",
    "read data in memory",
    "initialize label",
    "extract features",
    "Get number of lines of data file",
    "initialize label",
    "extract features",
    "load data from binary file",
    "checks whether there's a initial score file when loaded from binary data files",
    "the intial score file should with suffix \".bin.init\"",
    "not need to check validation data",
    "check meta data",
    "check token",
    "read feature group definitions",
    "read feature size",
    "buffer to read binary file",
    "check token",
    "read size of header",
    "re-allocate space if not enough",
    "read header",
    "get header",
    "read size of meta data",
    "re-allocate space if not enough",
    "read meta data",
    "load meta data",
    "sample local used data if need to partition",
    "if not contain query file, minimal sample unit is one record",
    "if contain query file, minimal sample unit is one query",
    "if is new query",
    "read feature data",
    "read feature size",
    "re-allocate space if not enough",
    "raw data",
    "fill feature_names_ if not header",
    "get forced split",
    "if only one machine, find bin locally",
    "if have multi-machines, need to find bin distributed",
    "different machines will find bin for different features",
    "start and len will store the process feature indices for different machines",
    "machine i will find bins for features in [ start[i], start[i] + len[i] )",
    "free",
    "gather global feature bin mappers",
    "restore features bins from buffer",
    "---- private functions ----",
    "get header",
    "num_groups",
    "real_feature_idx_",
    "feature2group",
    "feature2subfeature",
    "group_bin_boundaries",
    "group_feature_start_",
    "group_feature_cnt_",
    "get feature names",
    "get forced_bin_bounds_",
    "if features are ordered, not need to use hist_buf",
    "read all lines",
    "get query data",
    "if not contain query data, minimal sample unit is one record",
    "if contain query data, minimal sample unit is one query",
    "if is new query",
    "get query data",
    "if not contain query file, minimal sample unit is one record",
    "if contain query file, minimal sample unit is one query",
    "if is new query",
    "parse features",
    "get forced split",
    "check the range of label_idx, weight_idx and group_idx",
    "skip label check if user input parser config file,",
    "because label id is got from raw features while dataset features are consistent with customized parser.",
    "fill feature_names_ if not header",
    "start find bins",
    "if only one machine, find bin locally",
    "start and len will store the process feature indices for different machines",
    "machine i will find bins for features in [ start[i], start[i] + len[i] )",
    "free",
    "gather global feature bin mappers",
    "restore features bins from buffer",
    "if doesn't need to prediction with initial model",
    "parser",
    "set label",
    "free processed line:",
    "shrink_to_fit will be very slow in linux, and seems not free memory, disable for now",
    "text_reader_->Lines()[i].shrink_to_fit();",
    "push data",
    "if is used feature",
    "if need to prediction with initial model",
    "parser",
    "set initial score",
    "set label",
    "free processed line:",
    "shrink_to_fit will be very slow in Linux, and seems not free memory, disable for now",
    "text_reader_->Lines()[i].shrink_to_fit();",
    "push data",
    "if is used feature",
    "metadata_ will manage space of init_score",
    "text data can be free after loaded feature values",
    "parser",
    "set initial score",
    "set label",
    "push data",
    "if is used feature",
    "only need part of data",
    "need full data",
    "metadata_ will manage space of init_score",
    "read size of token",
    "remove duplicates",
    "deep copy function for BinMapper",
    "mean size for one bin",
    "need a new bin",
    "update bin upper bound",
    "last bin upper bound",
    "get number of positive and negative distinct values",
    "include zero bounds and infinity bound",
    "add forced bounds, excluding zeros since we have already added zero bounds",
    "find remaining bounds",
    "find distinct_values first",
    "push zero in the front",
    "use the large value",
    "push zero in the back",
    "convert to int type first",
    "sort by counts in descending order",
    "will ignore the categorical of small counts",
    "Push the dummy bin for NaN",
    "Use MissingType::None to represent this bin contains all categoricals",
    "fix count of NaN bin",
    "check trivial(num_bin_ == 1) feature",
    "check useless bin",
    "When most_freq_bin_ != default_bin_, there are some additional data loading costs.",
    "so use most_freq_bin_ = default_bin_ when there is not so sparse",
    "calculate max bin of all features to select the int type in MultiValDenseBin",
    "for lambdarank, it needs query data for partition data in distributed learning",
    "need convert query_id to boundaries",
    "check weights",
    "check positions",
    "check query boundries",
    "contain initial score file",
    "check weights",
    "get local weights",
    "check positions",
    "get local positions",
    "check query boundries",
    "get local query boundaries",
    "contain initial score file",
    "get local initial scores",
    "re-calculate query weight",
    "Clear init scores on empty input",
    "Note that len here is row count, not num_init_score, so we compare against num_data",
    "We need to use source_size here, because len might not equal size (due to a partially loaded dataset)",
    "CUDA is handled after all insertions are complete",
    "CUDA is handled after all insertions are complete",
    "Clear weights on empty input",
    "CUDA is handled after all insertions are complete",
    "Clear query boundaries on empty input",
    "save to nullptr",
    "CUDA is handled after all insertions are complete",
    "default weight file name",
    "default position file name",
    "default init_score file name",
    "use first line to count number class",
    "default query file name",
    "root is in the depth 0",
    "non-leaf",
    "leaf",
    "use this for the missing value conversion",
    "Predict func by Map to ifelse",
    "use this for the missing value conversion",
    "non-leaf",
    "left subtree",
    "right subtree",
    "leaf",
    "non-leaf",
    "left subtree",
    "right subtree",
    "leaf",
    "recursive computation of SHAP values for a decision tree",
    "extend the unique path",
    "leaf node",
    "internal node",
    "see if we have already split on this feature,",
    "if so we undo that split so we can redo it for this node",
    "recursive sparse computation of SHAP values for a decision tree",
    "extend the unique path",
    "leaf node",
    "internal node",
    "see if we have already split on this feature,",
    "if so we undo that split so we can redo it for this node",
    "add names of objective function if not providing metric",
    "equal weights for all classes",
    "generate seeds by seed.",
    "sort eval_at",
    "Only push the non-training data",
    "check for conflicts",
    "check if objective, metric, and num_class match",
    "Change pool size to -1 (no limit) when using data parallel to reduce communication costs",
    "Check max_depth and num_leaves",
    "Fits in an int, and is more restrictive than the current num_leaves",
    "force col-wise for gpu, and cuda version",
    "force row-wise for cuda version",
    "linear tree learner must be serial type and run on CPU device",
    "min_data_in_leaf must be at least 2 if path smoothing is active. This is because when the split is calculated",
    "the count is calculated using the proportion of hessian in the leaf which is rounded up to nearest int, so it can",
    "be 1 when there is actually no data in the leaf. In rare cases this can cause a bug because with path smoothing the",
    "calculated split gain can be positive even with zero gradient and hessian.",
    "In distributed mode, local node doesn't have histograms on all features, cannot perform \"intermediate\" monotone constraints.",
    "\"intermediate\" monotone constraints need to recompute splits. If the features are sampled when computing the",
    "split initially, then the sampling needs to be recorded or done once again, which is currently not supported",
    "first round: fill the single val group",
    "always push the last group",
    "put dense feature first",
    "sort by non zero cnt",
    "sort by non zero cnt, bigger first",
    "shuffle groups",
    "Using std::swap for vector<bool> will cause the wrong result.",
    "get num_features",
    "get bin_mappers",
    "for sparse multi value bin, we store the feature bin values with offset added",
    "for dense multi value bin, the feature bin values without offsets are used",
    "copy feature bin mapper data",
    "copy feature bin mapper data",
    "update CUDA storage for column data and metadata",
    "if not pass a filename, just append \".bin\" of original file",
    "Write the basic header information for the dataset",
    "get size of meta data",
    "write meta data",
    "write feature data",
    "get size of feature",
    "write feature",
    "write raw data; use row-major order so we can read row-by-row",
    "Calculate approximate size of output and reserve space",
    "write feature group definitions",
    "Give a little extra just in case, to avoid unnecessary resizes",
    "Write token that marks the data as binary reference, and the version",
    "Write the basic definition of the overall dataset",
    "write feature group definitions",
    "get size of feature",
    "write feature",
    "size of feature names and forced bins",
    "write header",
    "write feature names",
    "write forced bins",
    "explicitly initialize template methods, for cross module call",
    "explicitly initialize template methods, for cross module call",
    "Only one multi-val group, just simply merge",
    "Skip the leading 0 when copying group_bin_boundaries.",
    "regenerate other fields",
    "need to iterate bin iterator",
    "is dense column",
    "is sparse column",
    "initialize the subset cuda column data",
    "if one column has too many bins, use a separate partition for that column",
    "try if adding this column exceed the maximum number per partition",
    "if one column has too many bins, use a separate partition for that column",
    "try if adding this column exceed the maximum number per partition",
    "if LightGBM-specific default has been set, ignore OpenMP-global config",
    "otherwise, default to OpenMP-global config",
    "ensure that if LGBM_SetMaxThreads() was ever called, LightGBM doesn't",
    "use more than that many threads",
    "store the importance first",
    "PredictRaw",
    "PredictRawByMap",
    "Predict",
    "PredictByMap",
    "PredictLeafIndex",
    "PredictLeafIndexByMap",
    "output model type",
    "output number of class",
    "output label index",
    "output max_feature_idx",
    "output objective",
    "output tree models",
    "store the importance first",
    "sort the importance",
    "use serialized string to restore this object",
    "Use first 128 chars to avoid exceed the message buffer.",
    "get number of classes",
    "get index of label",
    "get max_feature_idx first",
    "get average_output",
    "get feature names",
    "get monotone_constraints",
    "set zero",
    "predict all the trees for one iteration",
    "check early stopping",
    "set zero",
    "predict all the trees for one iteration",
    "check early stopping",
    "margin_threshold will be captured by value",
    "copy and sort",
    "margin_threshold will be captured by value",
    "Fix for compiler warnings about reaching end of control",
    "load forced_splits file",
    "init tree learner",
    "push training metrics",
    "get max feature index",
    "get label index",
    "get feature names",
    "get parser config file content",
    "check that forced splits does not use feature indices larger than dataset size",
    "if need bagging, create buffer",
    "for a validation dataset, we need its score and metric",
    "update score",
    "objective function will calculate gradients and hessians",
    "output used time per iteration",
    "boosting from average label; or customized \"average\" if implemented for the current objective",
    "boosting first",
    "use customized objective function",
    "need to copy customized gradients when using GOSS",
    "bagging logic",
    "need to copy gradients for bagging subset.",
    "shrinkage by learning rate",
    "update score",
    "only add default score one-time",
    "updates scores",
    "add model",
    "reset score",
    "remove model",
    "print message for metric",
    "pop last early_stopping_round_ models",
    "update training score",
    "we need to predict out-of-bag scores of data for boosting",
    "update validation score",
    "print training metric",
    "print validation metric",
    "set zero",
    "predict all the trees for one iteration",
    "predict all the trees for one iteration",
    "push training metrics",
    "not same training data, need reset score and others",
    "create score tracker",
    "update score",
    "resize gradient vectors to copy the customized gradients for goss or bagging with subset",
    "load forced_splits file",
    "if exists initial score, will start from it",
    "clear host score buffer",
    "Get the max size of pool",
    "at least need 2 leaves",
    "push split information for all leaves",
    "initialize splits for leaf",
    "initialize data partition",
    "initialize ordered gradients and hessians",
    "cannot change is_hist_col_wise during training",
    "initialize splits for leaf",
    "initialize data partition",
    "initialize ordered gradients and hessians",
    "Get the max size of pool",
    "at least need 2 leaves",
    "push split information for all leaves",
    "some initial works before training",
    "root leaf",
    "only root leaf can be splitted on first time",
    "some initial works before finding best split",
    "find best threshold for every feature",
    "Get a leaf with max split gain",
    "Get split information for best leaf",
    "cannot split, quit",
    "split tree with best leaf",
    "reset histogram pool",
    "initialize data partition",
    "reset the splits for leaves",
    "Sumup for root",
    "use all data",
    "use bagging, only use part of data",
    "check depth of current leaf",
    "only need to check left leaf, since right leaf is in same level of left leaf",
    "no enough data to continue",
    "only have root",
    "put parent(left) leaf's histograms into larger leaf's histograms",
    "put parent(left) leaf's histograms to larger leaf's histograms",
    "construct smaller leaf",
    "construct larger leaf",
    "find splits",
    "only has root leaf",
    "start at root leaf",
    "Histogram construction require parent features.",
    "then, compute own splits",
    "split info should exist because searching in bfs fashion - should have added from parent",
    "update before tree split",
    "don't need to update this in data-based parallel model",
    "split tree, will return right leaf",
    "store the true split gain in tree model",
    "don't need to update this in data-based parallel model",
    "store the true split gain in tree model",
    "init the leaves that used on next iteration",
    "update leave outputs if needed",
    "bag_mapper[index_mapper[i]]",
    "it is needed to filter the features after the above code.",
    "Otherwise, the `is_splittable` in `FeatureHistogram` will be wrong, and cause some features being accidentally filtered in the later nodes.",
    "for root leaf the \"parent\" output is its own output because we don't apply any smoothing to the root",
    "can't use GetParentOutput because leaf_splits doesn't have weight property set",
    "find splits",
    "identify features containing nans",
    "preallocate the matrix used to calculate linear model coefficients",
    "store only upper triangular half of matrix as an array, in row-major order",
    "this requires (max_num_feat + 1) * (max_num_feat + 2) / 2 entries (including the constant terms of the regression)",
    "we add another 8 to ensure cache lines are not shared among processors",
    "some initial works before training",
    "root leaf",
    "only root leaf can be splitted on first time",
    "some initial works before finding best split",
    "find best threshold for every feature",
    "Get a leaf with max split gain",
    "Get split information for best leaf",
    "cannot split, quit",
    "split tree with best leaf",
    "map data to leaf number",
    "calculate coefficients using the method described in Eq 3 of https://arxiv.org/pdf/1802.05640.pdf",
    "the coefficients vector is given by",
    "- (X_T * H * X + lambda) ^ (-1) * (X_T * g)",
    "where:",
    "X is the matrix where the first column is the feature values and the second is all ones,",
    "H is the diagonal matrix of the hessian,",
    "lambda is the diagonal matrix with diagonal entries equal to the regularisation term linear_lambda",
    "g is the vector of gradients",
    "the subscript _T denotes the transpose",
    "create array of pointers to raw data, and coefficient matrices, for each leaf",
    "clear the coefficient matrices",
    "aggregate results from different threads",
    "copy into eigen matrices and solve",
    "update the tree properties",
    "need to be able to hold smaller and larger best splits in SyncUpGlobalBestSplit",
    "get feature partition",
    "get local used features",
    "get best split at smaller leaf",
    "find local best split for larger leaf",
    "sync global best info",
    "update best split",
    "instantiate template classes, otherwise linker cannot find the code",
    "initialize SerialTreeLearner",
    "Get local rank and global machine size",
    "need to be able to hold smaller and larger best splits in SyncUpGlobalBestSplit",
    "allocate buffer for communication",
    "get block start and block len for reduce scatter",
    "get buffer_write_start_pos",
    "get buffer_read_start_pos",
    "generate feature partition for current tree",
    "get local used feature",
    "get block start and block len for reduce scatter",
    "sync global data sumup info",
    "global sumup reduce",
    "copy back",
    "set global sumup info",
    "init global data count in leaf",
    "reset hist num bits according to global num data",
    "sync global data sumup info",
    "global sumup reduce",
    "copy back",
    "set global sumup info",
    "init global data count in leaf",
    "clear histogram buffer before synchronizing",
    "otherwise histogram contents from the previous iteration will be sent",
    "construct local histograms",
    "copy to buffer",
    "Reduce scatter for histogram",
    "restore global histograms from buffer",
    "only root leaf",
    "construct histgroms for large leaf, we init larger leaf as the parent, so we can just subtract the smaller leaf's histograms",
    "find local best split for larger leaf",
    "sync global best info",
    "set best split",
    "need update global number of data in leaf",
    "reset hist num bits according to global num data",
    "instantiate template classes, otherwise linker cannot find the code",
    "initialize SerialTreeLearner",
    "some additional variables needed for GPU trainer",
    "Initialize GPU buffers and kernels",
    "some functions used for debugging the GPU histogram construction",
    "printf(\"grad %g != %g (%d ULPs)\\n\", GET_GRAD(h1, i), GET_GRAD(h2, i), ulps);",
    "goto err;",
    "we roughly want 256 workgroups per device, and we have num_dense_feature4_ feature tuples.",
    "also guarantee that there are at least 2K examples per workgroup",
    "return 0;",
    "we have already copied ordered gradients, ordered Hessians and indices to GPU",
    "decide the best number of workgroups working on one feature4 tuple",
    "set work group size based on feature size",
    "each 2^exp_workgroups_per_feature workgroups work on a feature4 tuple",
    "we need to refresh the kernel arguments after reallocating",
    "The only argument that needs to be changed later is num_data_",
    "the GPU kernel will process all features in one call, and each",
    "2^exp_workgroups_per_feature (compile time constant) workgroup will",
    "process one feature4 tuple",
    "for the root node, indices are not copied",
    "for constant hessian, hessians are not copied except for the root node",
    "there will be 2^exp_workgroups_per_feature = num_workgroups / num_dense_feature4 sub-histogram per feature4",
    "and we will launch num_feature workgroups for this kernel",
    "will launch threads for all features",
    "the queue should be asynchronous, and we will can WaitAndGetHistograms() before we start processing dense feature groups",
    "copy the results asynchronously. Size depends on if double precision is used",
    "we will wait for this object in WaitAndGetHistograms",
    "when the output is ready, the computation is done",
    "values of this feature has been redistributed to multiple bins; need a reduction here",
    "how many feature-group tuples we have",
    "leave some safe margin for prefetching",
    "256 work-items per workgroup. Each work-item prefetches one tuple for that feature",
    "clear sparse/dense maps",
    "do nothing if no features can be processed on GPU",
    "allocate memory for all features (FIXME: 4 GB barrier on some devices, need to split to multiple buffers)",
    "unpin old buffer if necessary before destructing them",
    "make ordered_gradients and Hessians larger (including extra room for prefetching), and pin them",
    "allocate space for gradients and Hessians on device",
    "we will copy gradients and Hessians in after ordered_gradients_ and ordered_hessians_ are constructed",
    "allocate feature mask, for disabling some feature-groups' histogram calculation",
    "copy indices to the device",
    "histogram bin entry size depends on the precision (single/double)",
    "create output buffer, each feature has a histogram with device_bin_size_ bins,",
    "each work group generates a sub-histogram of dword_features_ features.",
    "only initialize once here, as this will not need to change when ResetTrainingData() is called",
    "create atomic counters for inter-group coordination",
    "The output buffer is allocated to host directly, to overlap compute and data transfer",
    "find the dense feature-groups and group then into Feature4 data structure (several feature-groups packed into 4 bytes)",
    "looking for dword_features_ non-sparse feature-groups",
    "decide if we need to redistribute the bin",
    "multiplier must be a power of 2",
    "device_bin_mults_.push_back(1);",
    "found",
    "for data transfer time",
    "Now generate new data structure feature4, and copy data to the device",
    "preallocate arrays for all threads, and pin them",
    "building Feature4 bundles; each thread handles dword_features_ features",
    "one feature datapoint is 4 bits",
    "this guarantees that the RawGet() function is inlined, rather than using virtual function dispatching",
    "one feature datapoint is one byte",
    "this guarantees that the RawGet() function is inlined, rather than using virtual function dispatching",
    "Dense bin",
    "Dense 4-bit bin",
    "working on the remaining (less than dword_features_) feature groups",
    "fill the leftover features",
    "fill this empty feature with some \"random\" value",
    "fill this empty feature with some \"random\" value",
    "copying the last 1 to (dword_features - 1) feature-groups in the last tuple",
    "deallocate pinned space for feature copying",
    "data transfer time",
    "for other types of failure, build log might not be available; program.build_log() can crash",
    "Something bad happened. Just return \"No log available.\"",
    "build is okay, log may contain warnings",
    "destroy any old kernels",
    "create OpenCL kernels for different number of workgroups per feature",
    "currently we don't use constant memory",
    "compile the GPU kernel depending if double precision is used, constant hessian is used, etc.",
    "kernel with indices in an array",
    "kernel with all features enabled, with eliminated branches",
    "kernel with all data indices (for root node, and assumes that root node always uses all features)",
    "do nothing if no features can be processed on GPU",
    "The only argument that needs to be changed later is num_data_",
    "hessian is passed as a parameter, but it is not available now.",
    "hessian will be set in BeforeTrain()",
    "Get the max bin size, used for selecting best GPU kernel",
    "initialize GPU",
    "determine which kernel to use based on the max number of bins",
    "the +9 skips extra characters \")\", newline, \"#endif\" and newline at the beginning",
    "the +9 skips extra characters \")\", newline, \"#endif\" and newline at the beginning",
    "the +9 skips extra characters \")\", newline, \"#endif\" and newline at the beginning",
    "ignore the feature groups that contain categorical features when producing warnings about max_bin.",
    "these groups may contain larger number of bins due to categorical features, but not due to the setting of max_bin.",
    "setup GPU kernel arguments after we allocating all the buffers",
    "GPU memory has to been reallocated because data may have been changed",
    "setup GPU kernel arguments after we allocating all the buffers",
    "Copy initial full hessians and gradients to GPU.",
    "We start copying as early as possible, instead of at ConstructHistogram().",
    "setup hessian parameters only",
    "hessian is passed as a parameter",
    "use bagging",
    "On GPU, we start copying indices, gradients and Hessians now, instead at ConstructHistogram()",
    "copy used gradients and Hessians to ordered buffer",
    "transfer the indices to GPU",
    "transfer hessian to GPU",
    "setup hessian parameters only",
    "hessian is passed as a parameter",
    "transfer gradients to GPU",
    "only have root",
    "Copy indices, gradients and Hessians as early as possible",
    "only need to initialize for smaller leaf",
    "Get leaf boundary",
    "copy indices to the GPU:",
    "copy ordered Hessians to the GPU:",
    "copy ordered gradients to the GPU:",
    "do nothing if no features can be processed on GPU",
    "copy data indices if it is not null",
    "generate and copy ordered_gradients if gradients is not null",
    "generate and copy ordered_hessians if Hessians is not null",
    "converted indices in is_feature_used to feature-group indices",
    "construct the feature masks for dense feature-groups",
    "if no feature group is used, just return and do not use GPU",
    "if not all feature groups are used, we need to transfer the feature mask to GPU",
    "otherwise, we will use a specialized GPU kernel with all feature groups enabled",
    "All data have been prepared, now run the GPU kernel",
    "construct smaller leaf",
    "ConstructGPUHistogramsAsync will return true if there are available feature groups dispatched to GPU",
    "then construct sparse features on CPU",
    "wait for GPU to finish, only if GPU is actually used",
    "use double precision",
    "use single precision",
    "Compare GPU histogram with CPU histogram, useful for debugging GPU code problem",
    "#define GPU_DEBUG_COMPARE",
    "construct larger leaf",
    "then construct sparse features on CPU",
    "wait for GPU to finish, only if GPU is actually used",
    "use double precision",
    "use single precision",
    "do some sanity check for the GPU algorithm",
    "limit top k",
    "get max bin",
    "calculate buffer size",
    "need to be able to hold smaller and larger best splits in SyncUpGlobalBestSplit",
    "left and right on same time, so need double size",
    "initialize histograms for global",
    "sync global data sumup info",
    "set global sumup info",
    "init global data count in leaf",
    "get local sumup",
    "get local sumup",
    "get mean number on machines",
    "weighted gain",
    "get top k",
    "Copy histogram to buffer, and Get local aggregate features",
    "copy histograms.",
    "copy smaller leaf histograms first",
    "mark local aggregated feature",
    "copy",
    "then copy larger leaf histograms",
    "mark local aggregated feature",
    "copy",
    "use local data to find local best splits",
    "clear histogram buffer before synchronizing",
    "otherwise histogram contents from the previous iteration will be sent",
    "find splits",
    "only has root leaf",
    "local voting",
    "gather",
    "get all top-k from all machines",
    "global voting",
    "copy local histgrams to buffer",
    "Reduce scatter for histogram",
    "find best split from local aggregated histograms",
    "restore from buffer",
    "restore from buffer",
    "find local best",
    "find local best split for larger leaf",
    "sync global best info",
    "copy back",
    "set the global number of data for leaves",
    "init the global sumup info",
    "instantiate template classes, otherwise linker cannot find the code",
    "allocate CUDA memory",
    "leave some space for alignment",
    "input best split info",
    "for leaf information update",
    "gather information for CPU, used for launching kernels",
    "for leaf splits information update",
    "we need restore the order of indices in cuda_data_indices_",
    "allocate more memory for sum reduction in CUDA",
    "only the first element records the final sum",
    "intialize split find task information (a split find task is one pass through the histogram of a feature)",
    "need to double the size of histogram buffer in global memory when using double precision in histogram construction",
    "use only half the size of histogram buffer in global memory when quantized training since each gradient and hessian takes only 2 bytes",
    "use the first gpu by default",
    "std::max(..., 1UL) to avoid error in the case when there are NaN's in the categorical values",
    "use feature interaction constraint or sample features by node"
  ],
  "v4.2.0": [
    "coding: utf-8",
    "create predictor first",
    "setting early stopping via global params should be possible",
    "reduce cost for prediction training data",
    "process callbacks",
    "construct booster",
    "start training",
    "check evaluation result.",
    "ranking task, split according to groups",
    "run preprocessing on the data set if needed",
    "setting early stopping via global params should be possible",
    "setup callbacks",
    "coding: utf-8",
    "dummy function to support older version of scikit-learn",
    "coding: utf-8",
    "f(labels, preds)",
    "f(labels, preds, weights)",
    "f(labels, preds, weights, group)",
    "f(labels, preds)",
    "f(labels, preds, weights)",
    "f(labels, preds, weights, group)",
    "documentation templates for LGBMModel methods are shared between the classes in",
    "this module and those in the ``dask`` module",
    "register default metric for consistency with callable eval_metric case",
    "try to deduce from class instance",
    "overwrite default metric by explicitly set metric",
    "use joblib conventions for negative n_jobs, just like scikit-learn",
    "at predict time, this is handled later due to the order of parameter updates",
    "Do not modify original args in fit function",
    "Refer to https://github.com/microsoft/LightGBM/pull/2619",
    "Separate built-in from callable evaluation metrics",
    "concatenate metric from params (or default if not provided in params) and eval_metric",
    "copy for consistency",
    "reduce cost for prediction training data",
    "free dataset",
    "retrive original params that possibly can be used in both training and prediction",
    "and then overwrite them (considering aliases) with params that were passed directly in prediction",
    "number of threads can have values with special meaning which is only applied",
    "in the scikit-learn interface, these should not reach the c++ side as-is",
    "adjust eval metrics to match whether binary or multiclass",
    "classification is being performed",
    "do not modify args, as it causes errors in model selection tools",
    "check group data",
    "coding: utf-8",
    "coding: utf-8",
    "\"#ddffdd\" is light green, \"#ffdddd\" is light red",
    "coding: utf-8",
    "coding: utf-8",
    "typing.TypeGuard was only introduced in Python 3.10",
    "we don't need lib_lightgbm while building docs",
    "TypeError: obj is not a string or a number",
    "ValueError: invalid literal",
    "Obtain objects to export",
    "Prepare export",
    "Export all objects",
    "DeprecationWarning is not shown by default, so let's create our own with higher level",
    "lazy evaluation to allow import without dynamic library, e.g., for docs generation",
    "if buffer length is not long enough, re-allocate a buffer",
    "avoid side effects on passed-in parameters",
    "if main_param_name was provided, keep that value and remove all aliases",
    "if main param name was not found, search for an alias",
    "neither of main_param_name, aliases were found",
    "most common case (no nullable dtypes)",
    "1.0 <= pd version < 1.1 and nullable dtypes, least common case",
    "raises error because array is casted to type(pd.NA) and there's no na_value argument",
    "data has nullable dtypes, but we can specify na_value argument and copy will be made",
    "take shallow copy in case we modify categorical columns",
    "whole column modifications don't change the original df",
    "determine feature names",
    "determine categorical features",
    "so that the target dtype considers floats",
    "Get total row number.",
    "Random access by row index. Used for data sampling.",
    "Range data access. Used to read data in batch when constructing Dataset.",
    "Optionally specify batch_size to control range data read size.",
    "Only required if using ``Dataset.subset()``.",
    "__get_num_preds() cannot work with nrow > MAX_INT32, so calculate overall number of predictions piecemeal",
    "avoid memory consumption by arrays concatenation operations",
    "create numpy array from output arrays",
    "break up indptr based on number of rows (note more than one matrix in multiclass case)",
    "for CSC there is extra column added",
    "reformat output into a csr or csc matrix or list of csr or csc matrices",
    "same shape as input csr or csc matrix except extra column for expected value",
    "note: make sure we copy data as it will be deallocated next",
    "free the temporary native indptr, indices, and data",
    "__get_num_preds() cannot work with nrow > MAX_INT32, so calculate overall number of predictions piecemeal",
    "avoid memory consumption by arrays concatenation operations",
    "Check that the input is valid: we only handle numbers (for now)",
    "Prepare prediction output array",
    "Export Arrow table to C and run prediction",
    "c type: double**",
    "each double* element points to start of each column of sample data.",
    "c type int**",
    "each int* points to start of indices for each column",
    "no min_data, nthreads and verbose in this function",
    "check data has header or not",
    "need to regroup init_score",
    "process for args",
    "get categorical features",
    "If the params[cat_alias] is equal to categorical_indices, do not report the warning.",
    "process for reference dataset",
    "start construct data",
    "set feature names",
    "Select sampled rows, transpose to column order.",
    "create validation dataset from ref_dataset",
    "Check that the input is valid: we only handle numbers (for now)",
    "Export Arrow table to C",
    "create valid",
    "construct subset",
    "create train",
    "could be updated if data is not freed",
    "set to None",
    "If the data is a arrow data, we can just pass it to C",
    "If a table is being passed, we concatenate the columns. This is only valid for",
    "'init_score'.",
    "we're done if self and reference share a common upstream reference",
    "Check if the weight contains values other than one",
    "Set field",
    "original values can be modified at cpp side",
    "if buffer length is not long enough, reallocate buffers",
    "group data from LightGBM is boundaries data, need to convert to group size",
    "Training task",
    "if \"machines\" is given, assume user wants to do distributed learning, and set up network",
    "construct booster object",
    "copy the parameters from train_set",
    "save reference to data",
    "buffer for inner predict",
    "Prediction task",
    "if buffer length is not long enough, re-allocate a buffer",
    "if a single node tree it won't have `leaf_index` so return 0",
    "Create the node record, and populate universal data members",
    "Update values to reflect node type (leaf or split)",
    "traverse the next level of the tree",
    "In tree format, \"subtree_list\" is a list of node records (dicts),",
    "and we add node to the list.",
    "need reset training data",
    "need to push new valid data",
    "ensure that existing Booster is freed before replacing it",
    "with a new one createdfrom file",
    "if buffer length is not long enough, re-allocate a buffer",
    "if buffer length is not long enough, reallocate a buffer",
    "Copy models",
    "Get name of features",
    "if buffer length is not long enough, reallocate buffers",
    "avoid to predict many time in one iteration",
    "Get num of inner evals",
    "Get name of eval metrics",
    "if buffer length is not long enough, reallocate buffers",
    "coding: utf-8",
    "Callback environment used by callbacks",
    "CVBooster holds a list of Booster objects, each needs to be updated",
    "for lgb.cv() with eval_train_metric=True, evaluation is also done on the training set",
    "and those metrics are considered for early stopping",
    "for lgb.train(), it's possible to pass the training data via valid_sets with any eval_name",
    "validation sets are guaranteed to not be identical to the training data in cv()",
    "split is needed for \"<dataset type> <metric>\" case (e.g. \"train l1\")",
    "self.best_score_list is initialized to an empty list",
    "split is needed for \"<dataset type> <metric>\" case (e.g. \"train l1\")",
    "coding: utf-8",
    "Acquire port in worker",
    "schedule futures to retrieve each element of the tuple",
    "retrieve ports",
    "Concatenate many parts into one",
    "construct local eval_set data.",
    "store indices of eval_set components that were not contained within local parts.",
    "consolidate parts of each individual eval component.",
    "require that eval_name exists in evaluated result data in case dropped due to padding.",
    "in distributed training the 'training' eval_set is not detected, will have name 'valid_<index>'.",
    "filter padding from eval parts then _concat each eval_set component.",
    "reconstruct eval_set fit args/kwargs depending on which components of eval_set are on worker.",
    "ensure that expected keys for evals_result_ and best_score_ exist regardless of padding.",
    "capture whether local_listen_port or its aliases were provided",
    "capture whether machines or its aliases were provided",
    "Some passed-in parameters can be removed:",
    "* 'num_machines': set automatically from Dask worker list",
    "* 'num_threads': overridden to match nthreads on each Dask process",
    "Split arrays/dataframes into parts. Arrange parts into dicts to enforce co-locality",
    "evals_set will to be re-constructed into smaller lists of (X, y) tuples, where",
    "X and y are each delayed sub-lists of original eval dask Collections.",
    "find maximum number of parts in an individual eval set so that we can",
    "pad eval sets when they come in different sizes.",
    "when individual eval set is equivalent to training data, skip recomputing parts.",
    "add None-padding for individual eval_set member if it is smaller than the largest member.",
    "first time a chunk of this eval set is added to this part.",
    "append additional chunks of this eval set to this part.",
    "ensure that all evaluation parts map uniquely to one part.",
    "assign sub-eval_set components to worker parts.",
    "Start computation in the background",
    "trigger error locally",
    "Find locations of all parts and map them to particular Dask workers",
    "Check that all workers were provided some of eval_set. Otherwise warn user that validation",
    "data artifacts may not be populated depending on worker returning final estimator.",
    "assign general validation set settings to fit kwargs.",
    "resolve aliases for network parameters and pop the result off params.",
    "these values are added back in calls to `_train_part()`",
    "figure out network params",
    "Tell each worker to train on the parts that it has locally",
    "",
    "This code treats ``_train_part()`` calls as not \"pure\" because:",
    "1. there is randomness in the training process unless parameters ``seed``",
    "and ``deterministic`` are set",
    "2. even with those parameters set, the output of one ``_train_part()`` call",
    "relies on global state (it and all the other LightGBM training processes",
    "coordinate with each other)",
    "if network parameters were changed during training, remove them from the",
    "returned model so that they're generated dynamically on every run based",
    "on the Dask cluster you're connected to and which workers have pieces of",
    "the training data",
    "dask.DataFrame.map_partitions() expects each call to return a pandas DataFrame or Series",
    "for multi-class classification with sparse matrices, pred_contrib predictions",
    "are returned as a list of sparse matrices (one per class)",
    "pred_contrib output will have one column per feature,",
    "plus one more for the base value",
    "need to tell Dask the expected type and shape of individual preds",
    "by default, dask.array.concatenate() concatenates sparse arrays into a COO matrix",
    "the code below is used instead to ensure that the sparse type is preserved during concatentation",
    "At this point, `out` is a list of lists of delayeds (each of which points to a matrix).",
    "Concatenate them to return a list of Dask Arrays.",
    "DaskLGBMClassifier does not support group, eval_group.",
    "DaskLGBMClassifier support for callbacks and init_model is not tested",
    "DaskLGBMRegressor does not support group, eval_class_weight, eval_group.",
    "DaskLGBMRegressor support for callbacks and init_model is not tested",
    "DaskLGBMRanker does not support eval_class_weight or early stopping",
    "DaskLGBMRanker support for callbacks and init_model is not tested",
    "coding: utf-8",
    "load or create your dataset",
    "create dataset for lightgbm",
    "if you want to re-use data, remember to set free_raw_data=False",
    "specify your configurations as a dict",
    "generate feature names",
    "feature_name and categorical_feature",
    "check feature name",
    "save model to file",
    "dump model to JSON (and save to file)",
    "feature names",
    "feature importances",
    "load model to predict",
    "can only predict with the best iteration (or the saving iteration)",
    "eval with loaded model",
    "dump model with pickle",
    "load model with pickle to predict",
    "can predict with any iteration when loaded in pickle way",
    "eval with loaded model",
    "continue training",
    "init_model accepts:",
    "1. model file name",
    "2. Booster()",
    "decay learning rates",
    "reset_parameter callback accepts:",
    "1. list with length = num_boost_round",
    "2. function(curr_iter)",
    "change other parameters during training",
    "self-defined objective function",
    "f(preds: array, train_data: Dataset) -> grad: array, hess: array",
    "log likelihood loss",
    "self-defined eval metric",
    "f(preds: array, train_data: Dataset) -> name: str, eval_result: float, is_higher_better: bool",
    "binary error",
    "NOTE: when you do customized loss function, the default prediction value is margin",
    "This may make built-in evaluation metric calculate wrong results",
    "For example, we are doing log likelihood loss, the prediction is score before logistic transformation",
    "Keep this in mind when you use the customization",
    "Pass custom objective function through params",
    "another self-defined eval metric",
    "f(preds: array, train_data: Dataset) -> name: str, eval_result: float, is_higher_better: bool",
    "accuracy",
    "NOTE: when you do customized loss function, the default prediction value is margin",
    "This may make built-in evaluation metric calculate wrong results",
    "For example, we are doing log likelihood loss, the prediction is score before logistic transformation",
    "Keep this in mind when you use the customization",
    "Pass custom objective function through params",
    "callback",
    "coding: utf-8",
    "load or create your dataset",
    "train",
    "predict",
    "eval",
    "feature importances",
    "self-defined eval metric",
    "f(y_true: array, y_pred: array) -> name: str, eval_result: float, is_higher_better: bool",
    "Root Mean Squared Logarithmic Error (RMSLE)",
    "train",
    "another self-defined eval metric",
    "f(y_true: array, y_pred: array) -> name: str, eval_result: float, is_higher_better: bool",
    "Relative Absolute Error (RAE)",
    "train",
    "predict",
    "eval",
    "other scikit-learn modules",
    "coding: utf-8",
    "load or create your dataset",
    "create dataset for lightgbm",
    "specify your configurations as a dict",
    "train",
    "coding: utf-8",
    "################",
    "Simulate some binary data with a single categorical and",
    "single continuous predictor",
    "################",
    "Set up a couple of utilities for our experiments",
    "################",
    "Observe the behavior of `binary` and `xentropy` objectives",
    "Trying this throws an error on non-binary values of y:",
    "experiment('binary', label_type='probability', DATA)",
    "The speed of `binary` is not drastically different than",
    "`xentropy`. `xentropy` runs faster than `binary` in many cases, although",
    "there are reasons to suspect that `binary` should run faster when the",
    "label is an integer instead of a float",
    "coding: utf-8",
    "load or create your dataset",
    "create dataset for lightgbm",
    "specify your configurations as a dict",
    "train",
    "save model to file",
    "predict",
    "eval",
    "We can also open HDF5 file once and get access to",
    "With binary dataset created, we can use either Python API or cmdline version to train.",
    "",
    "Note: in order to create exactly the same dataset with the one created in simple_example.py, we need",
    "to modify simple_example.py to pass numpy array instead of pandas DataFrame to Dataset constructor.",
    "The reason is that DataFrame column names will be used in Dataset. For a DataFrame with Int64Index",
    "as columns, Dataset will use column names like [\"0\", \"1\", \"2\", ...]. While for numpy array, column names",
    "are using the default one assigned in C++ code (dataset_loader.cpp), like [\"Column_0\", \"Column_1\", ...].",
    "Y has a single column and we read it in single shot. So store it as an 1-d array.",
    "We use random access for data sampling when creating LightGBM Dataset from Sequence.",
    "When accessing any element in a HDF5 chunk, it's read entirely.",
    "To save I/O for sampling, we should keep number of total chunks much larger than sample count.",
    "Here we are just creating a chunk size that matches with batch_size.",
    "",
    "Also note that the data is stored in row major order to avoid extra copy when passing to",
    "lightgbm Dataset.",
    "Save to 2 HDF5 files for demonstration.",
    "We can store multiple datasets inside a single HDF5 file.",
    "Separating X and Y for choosing best chunk size for data loading.",
    "split training data into two partitions",
    "make this array dense because we're splitting across",
    "a sparse boundary to partition the data",
    "the code below uses sklearn.metrics, but this requires pulling all of the",
    "predictions and target values back from workers to the client",
    "",
    "for larger datasets, consider the metrics from dask-ml instead",
    "https://ml.dask.org/modules/api.html#dask-ml-metrics-metrics",
    "coding: utf-8",
    "!/usr/bin/env python3",
    "-*- coding: utf-8 -*-",
    "",
    "LightGBM documentation build configuration file, created by",
    "sphinx-quickstart on Thu May  4 14:30:58 2017.",
    "",
    "This file is execfile()d with the current directory set to its",
    "containing dir.",
    "",
    "Note that not all possible configuration values are present in this",
    "autogenerated file.",
    "",
    "All configuration values have a default; values that are commented out",
    "serve to show the default.",
    "If extensions (or modules to document with autodoc) are in another directory,",
    "add these directories to sys.path here. If the directory is relative to the",
    "documentation root, use os.path.abspath to make it absolute.",
    "-- General configuration ------------------------------------------------",
    "If your documentation needs a minimal Sphinx version, state it here.",
    "Add any Sphinx extension module names here, as strings. They can be",
    "extensions coming with Sphinx (named 'sphinx.ext.*') or your custom",
    "ones.",
    "mock out modules",
    "hide type hints in API docs",
    "Generate autosummary pages. Output should be set with: `:toctree: pythonapi/`",
    "Only the class' docstring is inserted.",
    "If true, `todo` and `todoList` produce output, else they produce nothing.",
    "The master toctree document.",
    "General information about the project.",
    "The name of an image file (relative to this directory) to place at the top",
    "of the sidebar.",
    "The name of an image file (relative to this directory) to use as a favicon of",
    "the docs.  This file should be a Windows icon file (.ico) being 16x16 or 32x32",
    "pixels large.",
    "The version info for the project you're documenting, acts as replacement for",
    "|version| and |release|, also used in various other places throughout the",
    "built documents.",
    "The short X.Y version.",
    "The full version, including alpha/beta/rc tags.",
    "The language for content autogenerated by Sphinx. Refer to documentation",
    "for a list of supported languages.",
    "",
    "This is also used if you do content translation via gettext catalogs.",
    "Usually you set \"language\" from the command line for these cases.",
    "List of patterns, relative to source directory, that match files and",
    "directories to ignore when looking for source files.",
    "This patterns also effect to html_static_path and html_extra_path",
    "The name of the Pygments (syntax highlighting) style to use.",
    "-- Configuration for C API docs generation ------------------------------",
    "-- Options for HTML output ----------------------------------------------",
    "The theme to use for HTML and HTML Help pages.  See the documentation for",
    "a list of builtin themes.",
    "Theme options are theme-specific and customize the look and feel of a theme",
    "further.  For a list of options available for each theme, see the",
    "documentation.",
    "Add any paths that contain custom static files (such as style sheets) here,",
    "relative to this directory. They are copied after the builtin static files,",
    "so a file named \"default.css\" will overwrite the builtin \"default.css\".",
    "-- Options for HTMLHelp output ------------------------------------------",
    "Output file base name for HTML help builder.",
    "-- Options for LaTeX output ---------------------------------------------",
    "The name of an image file (relative to this directory) to place at the top of",
    "the title page.",
    "intersphinx configuration",
    "Warning! The following code can cause buffer overflows on RTD.",
    "Consider suppressing output completely if RTD project silently fails.",
    "Refer to https://github.com/svenevs/exhale",
    "/blob/fe7644829057af622e467bb529db6c03a830da99/exhale/deploy.py#L99-L111",
    "Warning! The following code can cause buffer overflows on RTD.",
    "Consider suppressing output completely if RTD project silently fails.",
    "Refer to https://github.com/svenevs/exhale",
    "/blob/fe7644829057af622e467bb529db6c03a830da99/exhale/deploy.py#L99-L111",
    "coding: utf-8",
    "This is a basic test for floating number parsing.",
    "Most of the test cases come from:",
    "https://github.com/dmlc/xgboost/blob/master/tests/cpp/common/test_charconv.cc",
    "https://github.com/Alexhuszagh/rust-lexical/blob/master/data/test-parse-unittests/strtod_tests.toml",
    "FLT_MAX",
    "FLT_MIN",
    "DBL_MAX (1 + (1 - 2^-52)) * 2^1023 = (2^53 - 1) * 2^971",
    "2^971 * (2^53 - 1 + 1/2) : the smallest number resolving to inf",
    "Near DBL_MIN",
    "DBL_MIN 2^-1022",
    "The behavior for parsing -nan depends on implementation.",
    "Thus we skip binary check for negative nan.",
    "See comment in test_cases.",
    "construct sample data first (use all data for convenience and since size is small)",
    "Load some test data",
    "Use the smaller \".test\" data because we don't care about the actual data and it's smaller",
    "Add some fake initial_scores and groups so we can test streaming them",
    "Now use the reference dataset schema to make some testable Datasets with N rows each",
    "Load some test data",
    "Use the smaller \".test\" data because we don't care about the actual data and it's smaller",
    "Add some fake initial_scores and groups so we can test streaming them",
    "Now use the reference dataset schema to make some testable Datasets with N rows each",
    "NOTE: Arrow arrays have 64-bit alignment but we can safely ignore this in tests",
    "1) Create validity bitmap",
    "2) Create buffers",
    "Create arrow array",
    "Arithmetic",
    "Subscripts",
    "End",
    "Load some test data",
    "Serialize the reference",
    "Deserialize the reference",
    "Confirm 1 successful API call",
    "Free memory",
    "Test that Data() points to first value written",
    "Constants",
    "Start with some content:",
    "Clear & re-use:",
    "Output should match new content:",
    "Number of trials for each new ChunkedArray configuration. Pass 100 times over the search space:",
    "Each outer loop iteration changes the test by adding +1 chunk. We start with 1 chunk only:",
    "Sweep valid and invalid addresses with a ChunkedArray with `chunks` chunks:",
    "Compute a new trial address & value & if it is a valid address:",
    "Insert item. If at a valid address, 0 is returned, otherwise, -1 is returned:",
    "If at valid address, check that the stored value is correct & remember it for the future:",
    "Check the just-stored value with getitem():",
    "Also store the just-stored value for future tracking:",
    "Final check: ensure even with overrides, all valid insertions store the latest value at that address:",
    "Test in 2 ways that the values are correctly laid out in memory:",
    "Since init_scores are in a column format, but need to be pushed as rows, we have to extract each batch",
    "Use multiple threads to test concurrency",
    "Since init_scores are in a column format, but need to be pushed as rows, we have to extract each batch",
    "Calculate expected boundaries",
    "Extract a set of rows from the column-based format (still maintaining column based format)",
    "coding: utf-8",
    "at initialization, should be -1",
    "updating that value through the C API should work",
    "resetting to any negative number should set it to -1",
    "coding: utf-8",
    "check saved model persistence",
    "we need to check the consistency of model file here, so test for exact equal",
    "check early stopping is working. Make it stop very early, so the scores should be very close to zero",
    "scores likely to be different, but prediction should still be the same",
    "test that shape is checked during prediction",
    "The simple implementation is just a single \"return self.ndarray[idx]\"",
    "The following is for demo and testing purpose.",
    "whole col",
    "half col",
    "Create dataset from numpy array directly.",
    "Create dataset using Sequence.",
    "Test for validation set.",
    "Select some random rows as valid data.",
    "From Dataset constructor, with dataset from numpy array.",
    "From Dataset.create_valid, with dataset from sequence.",
    "test that method works even with free_raw_data=True",
    "test that method works but sets raw data to None in case of immergeable data types",
    "test that method works for different data types",
    "Set extremely harsh penalties, so CEGB will block most splits.",
    "Compare pairs of penalties, to ensure scaling works as intended",
    "Reset booster1's parameters to p2, so the parameter section of the file matches.",
    "unconstructed, get_* methods should return whatever was provided",
    "before construction, get_field() should raise an exception",
    "constructed, get_* methods should return numpy arrays, even when the provided",
    "input was a list of floats or ints",
    "get_field(\"group\") returns a numpy array with boundaries, instead of size",
    "NOTE: \"position\" is converted to int32 on the C++ side",
    "should resolve duplicate aliases, and prefer the main parameter",
    "should choose the highest priority alias and set that value on main param",
    "if only aliases are used",
    "should use the default if main param and aliases are missing",
    "all changes should be made on copies and not modify the original",
    "preserves None found for main param and still removes aliases",
    "correctly chooses value when only an alias is provided",
    "adds None if that's given as the default and param not found",
    "If callable is found in objective",
    "Value in params should be preferred to the default_value passed from keyword arguments",
    "None of objective or its aliases in params, but default_value is callable.",
    "check that the original data wasn't modified",
    "check that the built data has the codes",
    "if all categories were seen during training we just take the codes",
    "if we only saw 'a' during training we just replace its code",
    "and leave the rest as nan",
    "test using defined feature names",
    "test using default feature names",
    "check for feature indices outside of range",
    "coding: utf-8",
    "add target, weight, and group to DataFrame so that partitions abide by group boundaries.",
    "set_index ensures partitions are based on group id.",
    "See https://stackoverflow.com/questions/49532824/dask-dataframe-split-partitions-based-on-a-column-or-function.",
    "separate target, weight from features.",
    "encode group identifiers into run-length encoding, the format LightGBMRanker is expecting",
    "so that within each partition, sum(g) = n_samples.",
    "ranking arrays: one chunk per group. Each chunk must include all columns.",
    "make one categorical feature relevant to the target",
    "https://github.com/microsoft/LightGBM/issues/4118",
    "extra predict() parameters should be passed through correctly",
    "pref_leaf values should have the right shape",
    "and values that look like valid tree nodes",
    "be sure LightGBM actually used at least one categorical column,",
    "and that it was correctly treated as a categorical feature",
    "shape depends on whether it is binary or multiclass classification",
    "in the special case of multi-class classification using scipy sparse matrices,",
    "the output of `.predict(..., pred_contrib=True)` is a list of sparse matrices (one per class)",
    "",
    "since that case is so different than all other cases, check the relevant things here",
    "and then return early",
    "raw scores will probably be different, but at least check that all predicted classes are the same",
    "be sure LightGBM actually used at least one categorical column,",
    "and that it was correctly treated as a categorical feature",
    "* shape depends on whether it is binary or multiclass classification",
    "* matrix for binary classification is of the form [feature_contrib, base_value],",
    "for multi-class it's [feat_contrib_class1, base_value_class1, feat_contrib_class2, base_value_class2, etc.]",
    "* contrib outputs for distributed training are different than from local training, so we can just test",
    "that the output has the right shape and base values are in the right position",
    "with a custom objective, prediction result is a raw score instead of predicted class",
    "function should have been preserved",
    "should correctly classify every sample",
    "probability estimates should be similar",
    "Scores should be the same",
    "Predictions should be roughly the same.",
    "pref_leaf values should have the right shape",
    "and values that look like valid tree nodes",
    "extra predict() parameters should be passed through correctly",
    "be sure LightGBM actually used at least one categorical column,",
    "and that it was correctly treated as a categorical feature",
    "contrib outputs for distributed training are different than from local training, so we can just test",
    "that the output has the right shape and base values are in the right position",
    "be sure LightGBM actually used at least one categorical column,",
    "and that it was correctly treated as a categorical feature",
    "Quantiles should be right",
    "be sure LightGBM actually used at least one categorical column,",
    "and that it was correctly treated as a categorical feature",
    "function should have been preserved",
    "Scores should be the same",
    "local and Dask predictions should be the same",
    "predictions should be better than random",
    "rebalance small dask.Array dataset for better performance.",
    "use many trees + leaves to overfit, help ensure that Dask data-parallel strategy matches that of",
    "serial learner. See https://github.com/microsoft/LightGBM/issues/3292#issuecomment-671288210.",
    "distributed ranker should be able to rank decently well and should",
    "have high rank correlation with scores from serial ranker.",
    "extra predict() parameters should be passed through correctly",
    "pref_leaf values should have the right shape",
    "and values that look like valid tree nodes",
    "be sure LightGBM actually used at least one categorical column,",
    "and that it was correctly treated as a categorical feature",
    "rebalance small dask.Array dataset for better performance.",
    "distributed ranker should be able to rank decently well with the least-squares objective",
    "and should have high rank correlation with scores from serial ranker.",
    "function should have been preserved",
    "Use larger trainset to prevent premature stopping due to zero loss, causing num_trees() < n_estimators.",
    "Use small chunk_size to avoid single-worker allocation of eval data partitions.",
    "test eval_class_weight, eval_init_score on binary-classification task.",
    "Note: objective's default `metric` will be evaluated in evals_result_ in addition to all eval_metrics.",
    "create eval_sets by creating new datasets or copying training data.",
    "total number of trees scales up for ova classifier.",
    "check that early stopping was not applied.",
    "checks that evals_result_ and best_score_ contain expected data and eval_set names.",
    "check that each eval_name and metric exists for all eval sets, allowing for the",
    "case when a worker receives a fully-padded eval_set component which is not evaluated.",
    "should be able to use the class without specifying a client",
    "should be able to set client after construction",
    "data on cluster1",
    "create identical data on cluster2",
    "at this point, the result of default_client() is client2 since it was the most recently",
    "created. So setting client to client1 here to test that you can select a non-default client",
    "unfitted model should survive pickling round trip, and pickling",
    "shouldn't have side effects on the model object",
    "client will always be None after unpickling",
    "fitted model should survive pickling round trip, and pickling",
    "shouldn't have side effects on the model object",
    "client will always be None after unpickling",
    "rebalance data to be sure that each worker has a piece of the data",
    "model 1 - no network parameters given",
    "model 2 - machines given",
    "model 3 - local_listen_port given",
    "training should fail because LightGBM will try to use the same",
    "port for multiple worker processes on the same machine",
    "rebalance data to be sure that each worker has a piece of the data",
    "test that \"machines\" is actually respected by creating a socket that uses",
    "one of the ports mentioned in \"machines\"",
    "The above error leaves a worker waiting",
    "an informative error should be raised if \"machines\" has duplicates",
    "\"client\" should be the only different, and the final argument",
    "value of the root node is 0 when init_score is set",
    "this test is separate because it takes a not-yet-constructed estimator",
    "coding: utf-8",
    "coding: utf-8",
    "build target, group ID vectors.",
    "build y/target and group-id vectors with user-specified group sizes.",
    "build y/target and group-id vectors according to n_samples, avg_gs, and random_gs.",
    "groups should contain > 1 element for pairwise learning objective.",
    "build feature data, X. Transform first few into informative features.",
    "doing this here, at import time, to ensure it only runs once_per import",
    "instead of once per assertion",
    "coding: utf-8",
    "----------------------------------------------------------------------------------------------- #",
    "UTILITIES                                            #",
    "----------------------------------------------------------------------------------------------- #",
    "Set random nulls",
    "Split data into <=2 random chunks",
    "Turn chunks into array",
    "----------------------------------------------------------------------------------------------- #",
    "UNIT TESTS                                           #",
    "----------------------------------------------------------------------------------------------- #",
    "------------------------------------------- DATASET ------------------------------------------- #",
    "-------------------------------------------- FIELDS ------------------------------------------- #",
    "Check for equality",
    "-------------------------------------------- LABELS ------------------------------------------- #",
    "------------------------------------------- WEIGHTS ------------------------------------------- #",
    "-------------------------------------------- GROUPS ------------------------------------------- #",
    "----------------------------------------- INIT SCORES ----------------------------------------- #",
    "------------------------------------------ PREDICTION ----------------------------------------- #",
    "coding: utf-8",
    "check that really dummy objective was used and estimator didn't learn anything",
    "prediction result is actually not transformed (is raw) due to custom objective",
    "original estimator is unaffected",
    "new estimator is unfitted, but has the same parameters",
    "Test if random_state is properly stored",
    "Test if two random states produce identical models",
    "Test if subsequent fits sample from random_state object and produce different models",
    "Test that the largest element is NOT the same, the smallest can be the same, i.e. zero",
    "With default params",
    "Tests same probabilities",
    "Tests same predictions",
    "Tests same raw scores",
    "Tests same leaf indices",
    "Tests same feature contributions",
    "Tests other parameters for the prediction works",
    "Tests start_iteration",
    "Tests same probabilities, starting from iteration 10",
    "Tests same predictions, starting from iteration 10",
    "Tests same raw scores, starting from iteration 10",
    "Tests same leaf indices, starting from iteration 10",
    "Tests same feature contributions, starting from iteration 10",
    "Tests other parameters for the prediction works, starting from iteration 10",
    "test that params passed in predict have higher priority",
    "no custom objective, no custom metric",
    "default metric",
    "non-default metric",
    "no metric",
    "non-default metric in eval_metric",
    "non-default metric with non-default metric in eval_metric",
    "non-default metric with multiple metrics in eval_metric",
    "non-default metric with multiple metrics in eval_metric for LGBMClassifier",
    "default metric for non-default objective",
    "non-default metric for non-default objective",
    "no metric",
    "non-default metric in eval_metric for non-default objective",
    "non-default metric with non-default metric in eval_metric for non-default objective",
    "non-default metric with multiple metrics in eval_metric for non-default objective",
    "custom objective, no custom metric",
    "default regression metric for custom objective",
    "non-default regression metric for custom objective",
    "multiple regression metrics for custom objective",
    "no metric",
    "default regression metric with non-default metric in eval_metric for custom objective",
    "non-default regression metric with metric in eval_metric for custom objective",
    "multiple regression metrics with metric in eval_metric for custom objective",
    "multiple regression metrics with multiple metrics in eval_metric for custom objective",
    "no custom objective, custom metric",
    "default metric with custom metric",
    "non-default metric with custom metric",
    "multiple metrics with custom metric",
    "custom metric (disable default metric)",
    "default metric for non-default objective with custom metric",
    "non-default metric for non-default objective with custom metric",
    "multiple metrics for non-default objective with custom metric",
    "custom metric (disable default metric for non-default objective)",
    "custom objective, custom metric",
    "custom metric for custom objective",
    "non-default regression metric with custom metric for custom objective",
    "multiple regression metrics with custom metric for custom objective",
    "default metric and invalid binary metric is replaced with multiclass alternative",
    "invalid binary metric is replaced with multiclass alternative",
    "default metric for non-default multiclass objective",
    "and invalid binary metric is replaced with multiclass alternative",
    "default metric and invalid multiclass metric is replaced with binary alternative",
    "invalid multiclass metric is replaced with binary alternative for custom objective",
    "Verify that can receive a list of metrics, only callable",
    "Verify that can receive a list of custom and built-in metrics",
    "Verify that works as expected when eval_metric is empty",
    "Verify that can receive a list of metrics, only built-in",
    "Verify that eval_metric is robust to receiving a list with None",
    "feval",
    "single eval_set",
    "two eval_set",
    "'val_minus_two' here is the expected number of threads for n_jobs=-2",
    "Note: according to joblib's formula, a value of n_jobs=-2 means",
    "\"use all but one thread\" (formula: n_cpus + 1 + n_jobs)",
    "try to predict with a different feature",
    "check that disabling the check doesn't raise the error",
    "make weights and init_score same types as y, just to avoid",
    "a huge number of combinations and therefore test cases",
    "make weights and init_score same types as y, just to avoid",
    "a huge number of combinations and therefore test cases",
    "coding: utf-8",
    "we're in a leaf now",
    "check that the rest of the elements have black color",
    "check that we got to the expected leaf",
    "coding: utf-8",
    "coding: utf-8",
    "check that default gives same result as k = 1",
    "check against independent calculation for k = 1",
    "check against independent calculation for k = 2",
    "check against independent calculation for k = 10",
    "check cases where predictions are equal",
    "should give same result as binary auc for 2 classes",
    "test the case where all predictions are equal",
    "test that weighted data gives different auc_mu",
    "test that equal data weights give same auc_mu as unweighted data",
    "should give 1 when accuracy = 1",
    "test loading class weights",
    "Simulates position bias for a given ranking dataset.",
    "The ouput dataset is identical to the input one with the exception for the relevance labels.",
    "The new labels are generated according to an instance of a cascade user model:",
    "for each query, the user is simulated to be traversing the list of documents ranked by a baseline ranker",
    "(in our example it is simply the ordering by some feature correlated with relevance, e.g., 34)",
    "and clicks on that document (new_label=1) with some probability 'pclick' depending on its true relevance;",
    "at each position the user may stop the traversal with some probability pstop. For the non-clicked documents,",
    "new_label=0. Thus the generated new labels are biased towards the baseline ranker.",
    "The positions of the documents in the ranked lists produced by the baseline, are returned.",
    "a mapping of a document's true relevance (defined on a 5-grade scale) into the probability of clicking it",
    "an instantiation of a cascade model where the user stops with probability 0.2 after observing each document",
    "simulate position bias for the train dataset and put the train dataset with biased labels to temp directory",
    "the performance of the unbiased LambdaMART should outperform the plain LambdaMART on the dataset with position bias",
    "add extra row to position file",
    "simulate position bias for the train dataset and put the train dataset with biased labels to temp directory",
    "test setting positions through Dataset constructor with numpy array",
    "the performance of the unbiased LambdaMART should outperform the plain LambdaMART on the dataset with position bias",
    "test setting positions through Dataset constructor with pandas Series",
    "test setting positions through set_position",
    "test get_position works",
    "no early stopping",
    "early stopping occurs",
    "regular early stopping",
    "positive min_delta",
    "test custom eval metrics",
    "shuffle = False, override metric in params",
    "shuffle = True, callbacks",
    "enable display training loss",
    "self defined folds",
    "LambdaRank",
    "... with l2 metric",
    "... with NDCG (default) metric",
    "self defined folds with lambdarank",
    "init_model from an in-memory Booster",
    "init_model from a text file",
    "predictions should be identical",
    "with early stopping",
    "predict by each fold booster",
    "check that each booster predicted using the best iteration",
    "fold averaging",
    "without early stopping",
    "test feature_names with whitespaces",
    "This has non-ascii strings.",
    "check that passing parameters to the constructor raises warning and ignores them",
    "check inference isn't affected by unknown parameter",
    "entries whose values should reflect params passed to lgb.train()",
    "'l1' was passed in with alias 'mae'",
    "NOTE: this was passed in with alias 'sub_row'",
    "entries with default values of params",
    "add device-specific entries",
    "",
    "passed-in force_col_wise / force_row_wise parameters are ignored on CUDA and GPU builds...",
    "https://github.com/microsoft/LightGBM/blob/1d7ee63686272bceffd522284127573b511df6be/src/io/config.cpp#L375-L377",
    "check that model text has all expected param entries",
    "since Booster.model_to_string() is used when pickling, check that parameters all",
    "roundtrip pickling successfully too",
    "take subsets and train",
    "generate CSR sparse dataset",
    "convert data to dense and get back same contribs",
    "validate the values are the same",
    "validate using CSC matrix",
    "validate the values are the same",
    "generate CSR sparse dataset",
    "convert data to dense and get back same contribs",
    "validate the values are the same",
    "validate using CSC matrix",
    "validate the values are the same",
    "Note there is an extra column added to the output for the expected value",
    "Note output CSC shape should be same as CSR output shape",
    "test sliced labels",
    "append some columns",
    "append some rows",
    "test sliced 2d matrix",
    "test sliced CSR",
    "trees start at position 1.",
    "split_features are in 4th line.",
    "test if a penalty as high as the depth indeed prohibits all monotone splits",
    "The penalization is so high that the first 2 features should not be used here",
    "Check that a very high penalization is the same as not using the features at all",
    "check refit accepts dataset_params",
    "the following checks that dart and rf with mape can predict outside the 0-1 range",
    "https://github.com/microsoft/LightGBM/issues/1579",
    "no custom objective, no feval",
    "default metric",
    "non-default metric in params",
    "default metric in args",
    "non-default metric in args",
    "metric in args overwrites one in params",
    "metric in args overwrites one in params",
    "multiple metrics in params",
    "multiple metrics in args",
    "remove default metric by 'None' in list",
    "remove default metric by 'None' aliases",
    "custom objective, no feval",
    "no default metric",
    "metric in params",
    "metric in args",
    "metric in args overwrites its' alias in params",
    "multiple metrics in params",
    "multiple metrics in args",
    "no custom objective, feval",
    "default metric with custom one",
    "non-default metric in params with custom one",
    "default metric in args with custom one",
    "non-default metric in args with custom one",
    "metric in args overwrites one in params, custom one is evaluated too",
    "multiple metrics in params with custom one",
    "multiple metrics in args with custom one",
    "custom metric is evaluated despite 'None' is passed",
    "custom objective, feval",
    "no default metric, only custom one",
    "metric in params with custom one",
    "metric in args with custom one",
    "metric in args overwrites one in params, custom one is evaluated too",
    "multiple metrics in params with custom one",
    "multiple metrics in args with custom one",
    "custom metric is evaluated despite 'None' is passed",
    "no custom objective, no feval",
    "default metric",
    "default metric in params",
    "non-default metric in params",
    "multiple metrics in params",
    "remove default metric by 'None' aliases",
    "custom objective, no feval",
    "no default metric",
    "metric in params",
    "multiple metrics in params",
    "no custom objective, feval",
    "default metric with custom one",
    "default metric in params with custom one",
    "non-default metric in params with custom one",
    "multiple metrics in params with custom one",
    "custom metric is evaluated despite 'None' is passed",
    "custom objective, feval",
    "no default metric, only custom one",
    "metric in params with custom one",
    "multiple metrics in params with custom one",
    "custom metric is evaluated despite 'None' is passed",
    "Custom objective replaces multiclass",
    "multiclass default metric",
    "multiclass default metric with custom one",
    "multiclass metric alias with custom one for custom objective",
    "no metric for invalid class_num",
    "custom metric for invalid class_num",
    "multiclass metric alias with custom one with invalid class_num",
    "multiclass default metric without num_class",
    "multiclass metric alias",
    "multiclass metric",
    "non-valid metric for multiclass objective",
    "non-default num_class for default objective",
    "no metric with non-default num_class for custom objective",
    "multiclass metric alias for custom objective",
    "multiclass metric for custom objective",
    "binary metric with non-default num_class for custom objective",
    "Expect three metrics but mean and stdv for each metric",
    "test XGBoost-style return value",
    "test numpy-style return value",
    "test bins string type",
    "test histogram is disabled for categorical features",
    "test for lgb.train",
    "test feval for lgb.train",
    "test with two valid data for lgb.train",
    "test for lgb.cv",
    "test feval for lgb.cv",
    "test that binning works properly for features with only positive or only negative values",
    "decreasing without freeing raw data is allowed",
    "decreasing before lazy init is allowed",
    "increasing is allowed",
    "decreasing with disabled filter is allowed",
    "decreasing with enabled filter is disallowed;",
    "also changes of other params are disallowed",
    "check extra trees increases regularization",
    "check path smoothing increases regularization",
    "test edge case with one leaf",
    "check that constraint containing all features is equivalent to no constraint",
    "check that constraint partitioning the features reduces train accuracy",
    "check that constraints consisting of single features reduce accuracy further",
    "test that interaction constraints work when not all features are used",
    "check that number of threads does not affect result",
    "check that setting linear_tree=True fits better than ordinary trees when data has linear relationship",
    "test again with nans in data",
    "test again with bagging",
    "test with a feature that has only one non-nan value",
    "test with a categorical feature",
    "test refit: same results on same data",
    "test refit with save and load",
    "test refit: different results training on different data",
    "test when num_leaves - 1 < num_features and when num_leaves - 1 > num_features",
    "test that the predict once with all iterations equals summed results with start_iteration and num_iteration",
    "test the case where start_iteration <= 0, and num_iteration is None",
    "test the case where start_iteration > 0, and num_iteration <= 0",
    "test the case where start_iteration > 0, and num_iteration <= 0, with pred_leaf=True",
    "test the case where start_iteration > 0, and num_iteration <= 0, with pred_contrib=True",
    "test for regression",
    "test both with and without early stopping",
    "test for multi-class",
    "test both with and without early stopping",
    "test for binary",
    "test both with and without early stopping",
    "test against sklearn average precision metric",
    "test that average precision is 1 where model predicts perfectly",
    "data as float64",
    "test all features were used",
    "test the score is better than predicting the mean",
    "test all predictions are equal using different input dtypes",
    "introduce some missing values",
    "the previous line turns x3 into object dtype in recent versions of pandas",
    "train with regular dtypes",
    "convert to nullable dtypes",
    "test training succeeds",
    "test all features were used",
    "test the score is better than predicting the mean",
    "test equal predictions",
    "test data are taken from bug report",
    "https://github.com/microsoft/LightGBM/issues/4708",
    "modified from https://github.com/microsoft/LightGBM/issues/3679#issuecomment-938652811",
    "and https://github.com/microsoft/LightGBM/pull/5087",
    "test that the ``splits_per_leaf_`` of CEGB is cleaned before training a new tree",
    "which is done in the fix #5164",
    "without the fix:",
    "Check failed: (best_split_info.left_count) > (0)",
    "try to predict with a different feature",
    "check that disabling the check doesn't raise the error",
    "try to refit with a different feature",
    "check that disabling the check doesn't raise the error",
    "coding: utf-8",
    "If compiled appropriately, the same installation will support both GPU and CPU.",
    "Double-precision floats are only supported on x86_64 with PoCL",
    "coding: utf-8",
    "coding: utf-8",
    "These are helper functions to allow doing a stack unwind",
    "after an R allocation error, which would trigger a long jump.",
    "convert from one-based to zero-based index",
    "if any feature names were larger than allocated size,",
    "allow for a larger size and try again",
    "convert from boundaries to size",
    "--- start Booster interfaces",
    "if any eval names were larger than allocated size,",
    "allow for a larger size and try again",
    "if the model string was larger than the initial buffer, call the function again, writing directly to the R object",
    "if the model string was larger than the initial buffer, allocate a bigger buffer and try again",
    "if aliases string was larger than the initial buffer, allocate a bigger buffer and try again",
    "if aliases string was larger than the initial buffer, allocate a bigger buffer and try again",
    ".Call() calls",
    "coding: utf-8",
    "alias table",
    "names",
    "from strings",
    "tails",
    "tails",
    "the following are stored as comma separated strings but are arrays in the wrappers",
    "coding: utf-8",
    "Single row predictor to abstract away caching logic",
    "create boosting",
    "initialize the boosting",
    "create objective function",
    "initialize the objective function",
    "create training metric",
    "reset the boosting",
    "create objective function",
    "initialize the objective function",
    "calculate the nonzero data and indices size",
    "allocate data and indices arrays",
    "Get the number of trees per iteration (for multiclass scenario we output multiple sparse matrices)",
    "aggregated per row feature contribution results",
    "keep track of the row_vector sizes for parallelization",
    "copy vector results to output for each row",
    "Get the number of trees per iteration (for multiclass scenario we output multiple sparse matrices)",
    "aggregated per row feature contribution results",
    "calculate number of elements per column to construct",
    "the CSC matrix with random access",
    "keep track of column counts",
    "keep track of beginning index for each column",
    "keep track of beginning index for each matrix",
    "Note: we parallelize across matrices instead of rows because of the column_counts[m][col_idx] increment inside the loop",
    "store the row index",
    "update column count",
    "explicitly declare symbols from LightGBM namespace",
    "some help functions used to convert data",
    "Row iterator of on column for CSC matrix",
    "return value at idx, only can access by ascent order",
    "return next non-zero pair, if index < 0, means no more data",
    "start of c_api functions",
    "This API is to keep python binding's behavior the same with C++ implementation.",
    "Sample count, random seed etc. should be provided in parameters.",
    "convert internal thread id to be unique based on external thread id",
    "convert internal thread id to be unique based on external thread id",
    "sample data first",
    "sample data first",
    "sample data first",
    "local buffer to re-use memory",
    "sample data first",
    "no more data",
    "Prepare the Arrow data",
    "Initialize the dataset",
    "If there is no reference dataset, we first sample indices",
    "Then, we obtain sample values by parallelizing across columns",
    "Values need to be copied from the record batches.",
    "The chunks are iterated over in the inner loop as columns can be treated independently.",
    "Finally, we initialize a loader from the sampled values",
    "After sampling and properly initializing all bins, we can add our data to the dataset. Here,",
    "we parallelize across rows.",
    "---- start of booster",
    "Single row in row-major format:",
    "Apply the configuration",
    "Set up chunked array and iterators for all columns",
    "Build row function",
    "Run prediction",
    "---- start of some help functions",
    "data is array of pointers to individual rows",
    "set number of threads for openmp",
    "read parameters from config file",
    "remove str after \"#\"",
    "de-duplicate params",
    "prediction is needed if using input initial model(continued train)",
    "need to continue training",
    "sync up random seed for data partition",
    "load Training data",
    "load data for distributed training",
    "load data for single machine",
    "need save binary file",
    "create training metric",
    "only when have metrics then need to construct validation data",
    "Add validation data, if it exists",
    "add",
    "need save binary file",
    "add metric for validation data",
    "output used time on each iteration",
    "need init network",
    "create boosting",
    "create objective function",
    "load training data",
    "initialize the objective function",
    "initialize the boosting",
    "add validation data into boosting",
    "convert model to if-else statement code",
    "create predictor",
    "Free memory",
    "create predictor",
    "label_gain = 2^i - 1, may overflow, so we use 31 here",
    "counts for all labels",
    "start from top label, and accumulate DCG",
    "counts for all labels",
    "calculate k Max DCG by one pass",
    "get sorted indices by score",
    "calculate multi dcg by one pass",
    "wait for all client start up",
    "Don't call MPI_Finalize() here: If the destructor was called because only this node had an exception, calling MPI_Finalize() will cause all nodes to hang.",
    "Instead we will handle finalize/abort for MPI in main().",
    "default set to -1",
    "distance at k-th communication, distance[k] = 2^k",
    "set incoming rank at k-th commuication",
    "set outgoing rank at k-th commuication",
    "default set as -1",
    "construct all recursive halving map for all machines",
    "let 1 << k <= num_machines",
    "distance of each communication",
    "if num_machines = 2^k, don't need to group machines",
    "communication direction, %2 == 0 is positive",
    "neighbor at k-th communication",
    "receive data block at k-th communication",
    "send data block at k-th communication",
    "if num_machines != 2^k, need to group machines",
    "group, two machine in one group, total \"rest\" groups will have 2 machines.",
    "let left machine as group leader",
    "cache block information for groups, group with 2 machines will have double block size",
    "convert from group to node leader",
    "convert from node to group",
    "meet new group",
    "add block len for this group",
    "calculate the group block start",
    "not need to construct",
    "get receive block information",
    "accumulate block len",
    "get send block information",
    "accumulate block len",
    "static member definition",
    "if small package or small count , do it by all gather.(reduce the communication times.)",
    "assign the blocks to every rank.",
    "do reduce scatter",
    "do all gather",
    "assign blocks",
    "need use buffer here, since size of \"output\" is smaller than size after all gather",
    "copy back",
    "assign blocks",
    "start all gather",
    "when num_machines is small and data is large",
    "use output as receive buffer",
    "get current local block size",
    "get out rank",
    "get in rank",
    "get send information",
    "get recv information",
    "send and recv at same time",
    "rotate in-place",
    "use output as receive buffer",
    "get current local block size",
    "get send information",
    "get recv information",
    "send and recv at same time",
    "use output as receive buffer",
    "send and recv at same time",
    "send local data to neighbor first",
    "receive neighbor data first",
    "reduce",
    "get target",
    "get send information",
    "get recv information",
    "send and recv at same time",
    "reduce",
    "send result to neighbor",
    "receive result from neighbor",
    "copy result",
    "start up socket",
    "parse clients from file",
    "get ip list of local machine",
    "get local rank",
    "construct listener",
    "construct communication topo",
    "construct linkers",
    "free listener",
    "set timeout",
    "accept incoming socket",
    "receive rank",
    "add new socket",
    "save ranks that need to connect with",
    "start listener",
    "start connect",
    "let smaller rank connect to larger rank",
    "send local rank",
    "wait for listener",
    "print connected linkers",
    "only need to copy subset",
    "avoid to copy subset many times",
    "avoid out of range",
    "may need to recopy subset",
    "valid the type",
    "parser factory implementation.",
    "customized parser add-on.",
    "save header to parser config in case needed.",
    "save label id to parser config in case needed.",
    "Constructors",
    "Get type tag",
    "Comparisons",
    "This has to be separate, not in Statics, because Json() accesses",
    "statics().null.",
    "advance until next line, or end of input",
    "advance until closing tokens",
    "The usual case: non-escaped characters",
    "Handle escapes",
    "Extract 4-byte escape sequence",
    "Explicitly check length of the substring. The following loop",
    "relies on std::string returning the terminating NUL when",
    "accessing str[length]. Checking here reduces brittleness.",
    "JSON specifies that characters outside the BMP shall be encoded as a",
    "pair of 4-hex-digit \\u escapes encoding their surrogate pair",
    "components. Check whether we're in the middle of such a beast: the",
    "previous codepoint was an escaped lead (high) surrogate, and this is",
    "a trail (low) surrogate.",
    "Reassemble the two surrogate pairs into one astral-plane character,",
    "per the UTF-16 algorithm.",
    "Integer part",
    "Decimal part",
    "Exponent part",
    "Check for any trailing garbage",
    "Documented in json11.hpp",
    "Check for another object",
    "get column names",
    "support to get header from parser config, so could utilize following label name to id mapping logic.",
    "load label idx first",
    "if parser config file exists, feature names may be changed after customized parser applied.",
    "clear here so could use default filled feature names during dataset construction.",
    "may improve by saving real feature names defined in parser in the future.",
    "erase label column name",
    "load ignore columns",
    "load weight idx",
    "load group idx",
    "don't support query id in data file when using distributed training",
    "read data to memory",
    "sample data",
    "construct feature bin mappers & clear sample data",
    "initialize label",
    "extract features",
    "sample data from file",
    "construct feature bin mappers & clear sample data",
    "initialize label",
    "extract features",
    "load data from binary file",
    "checks whether there's a initial score file when loaded from binary data files",
    "the intial score file should with suffix \".bin.init\"",
    "check meta data",
    "need to check training data",
    "read data in memory",
    "initialize label",
    "extract features",
    "Get number of lines of data file",
    "initialize label",
    "extract features",
    "load data from binary file",
    "checks whether there's a initial score file when loaded from binary data files",
    "the intial score file should with suffix \".bin.init\"",
    "not need to check validation data",
    "check meta data",
    "check token",
    "read feature group definitions",
    "read feature size",
    "buffer to read binary file",
    "check token",
    "read size of header",
    "re-allocate space if not enough",
    "read header",
    "get header",
    "read size of meta data",
    "re-allocate space if not enough",
    "read meta data",
    "load meta data",
    "sample local used data if need to partition",
    "if not contain query file, minimal sample unit is one record",
    "if contain query file, minimal sample unit is one query",
    "if is new query",
    "read feature data",
    "read feature size",
    "re-allocate space if not enough",
    "raw data",
    "fill feature_names_ if not header",
    "get forced split",
    "if only one machine, find bin locally",
    "if have multi-machines, need to find bin distributed",
    "different machines will find bin for different features",
    "start and len will store the process feature indices for different machines",
    "machine i will find bins for features in [ start[i], start[i] + len[i] )",
    "free",
    "gather global feature bin mappers",
    "restore features bins from buffer",
    "---- private functions ----",
    "get header",
    "num_groups",
    "real_feature_idx_",
    "feature2group",
    "feature2subfeature",
    "group_bin_boundaries",
    "group_feature_start_",
    "group_feature_cnt_",
    "get feature names",
    "get forced_bin_bounds_",
    "if features are ordered, not need to use hist_buf",
    "read all lines",
    "get query data",
    "if not contain query data, minimal sample unit is one record",
    "if contain query data, minimal sample unit is one query",
    "if is new query",
    "get query data",
    "if not contain query file, minimal sample unit is one record",
    "if contain query file, minimal sample unit is one query",
    "if is new query",
    "parse features",
    "get forced split",
    "check the range of label_idx, weight_idx and group_idx",
    "skip label check if user input parser config file,",
    "because label id is got from raw features while dataset features are consistent with customized parser.",
    "fill feature_names_ if not header",
    "start find bins",
    "if only one machine, find bin locally",
    "start and len will store the process feature indices for different machines",
    "machine i will find bins for features in [ start[i], start[i] + len[i] )",
    "free",
    "gather global feature bin mappers",
    "restore features bins from buffer",
    "if doesn't need to prediction with initial model",
    "parser",
    "set label",
    "free processed line:",
    "shrink_to_fit will be very slow in linux, and seems not free memory, disable for now",
    "text_reader_->Lines()[i].shrink_to_fit();",
    "push data",
    "if is used feature",
    "if need to prediction with initial model",
    "parser",
    "set initial score",
    "set label",
    "free processed line:",
    "shrink_to_fit will be very slow in Linux, and seems not free memory, disable for now",
    "text_reader_->Lines()[i].shrink_to_fit();",
    "push data",
    "if is used feature",
    "metadata_ will manage space of init_score",
    "text data can be free after loaded feature values",
    "parser",
    "set initial score",
    "set label",
    "push data",
    "if is used feature",
    "only need part of data",
    "need full data",
    "metadata_ will manage space of init_score",
    "read size of token",
    "remove duplicates",
    "deep copy function for BinMapper",
    "mean size for one bin",
    "need a new bin",
    "update bin upper bound",
    "last bin upper bound",
    "get number of positive and negative distinct values",
    "include zero bounds and infinity bound",
    "add forced bounds, excluding zeros since we have already added zero bounds",
    "find remaining bounds",
    "find distinct_values first",
    "push zero in the front",
    "use the large value",
    "push zero in the back",
    "convert to int type first",
    "sort by counts in descending order",
    "will ignore the categorical of small counts",
    "Push the dummy bin for NaN",
    "Use MissingType::None to represent this bin contains all categoricals",
    "fix count of NaN bin",
    "check trivial(num_bin_ == 1) feature",
    "check useless bin",
    "When most_freq_bin_ != default_bin_, there are some additional data loading costs.",
    "so use most_freq_bin_ = default_bin_ when there is not so sparse",
    "calculate max bin of all features to select the int type in MultiValDenseBin",
    "for lambdarank, it needs query data for partition data in distributed learning",
    "need convert query_id to boundaries",
    "check weights",
    "check positions",
    "check query boundries",
    "contain initial score file",
    "check weights",
    "get local weights",
    "check positions",
    "get local positions",
    "check query boundries",
    "get local query boundaries",
    "contain initial score file",
    "get local initial scores",
    "re-calculate query weight",
    "Clear init scores on empty input",
    "Note that len here is row count, not num_init_score, so we compare against num_data",
    "We need to use source_size here, because len might not equal size (due to a partially loaded dataset)",
    "CUDA is handled after all insertions are complete",
    "CUDA is handled after all insertions are complete",
    "Clear weights on empty input",
    "CUDA is handled after all insertions are complete",
    "Clear query boundaries on empty input",
    "save to nullptr",
    "CUDA is handled after all insertions are complete",
    "default weight file name",
    "default position file name",
    "default init_score file name",
    "use first line to count number class",
    "default query file name",
    "root is in the depth 0",
    "non-leaf",
    "leaf",
    "use this for the missing value conversion",
    "Predict func by Map to ifelse",
    "use this for the missing value conversion",
    "non-leaf",
    "left subtree",
    "right subtree",
    "leaf",
    "non-leaf",
    "left subtree",
    "right subtree",
    "leaf",
    "recursive computation of SHAP values for a decision tree",
    "extend the unique path",
    "leaf node",
    "internal node",
    "see if we have already split on this feature,",
    "if so we undo that split so we can redo it for this node",
    "recursive sparse computation of SHAP values for a decision tree",
    "extend the unique path",
    "leaf node",
    "internal node",
    "see if we have already split on this feature,",
    "if so we undo that split so we can redo it for this node",
    "add names of objective function if not providing metric",
    "equal weights for all classes",
    "generate seeds by seed.",
    "sort eval_at",
    "Only push the non-training data",
    "check for conflicts",
    "check if objective, metric, and num_class match",
    "Change pool size to -1 (no limit) when using data parallel to reduce communication costs",
    "Check max_depth and num_leaves",
    "Fits in an int, and is more restrictive than the current num_leaves",
    "force col-wise for gpu, and cuda version",
    "force row-wise for cuda version",
    "linear tree learner must be serial type and run on CPU device",
    "min_data_in_leaf must be at least 2 if path smoothing is active. This is because when the split is calculated",
    "the count is calculated using the proportion of hessian in the leaf which is rounded up to nearest int, so it can",
    "be 1 when there is actually no data in the leaf. In rare cases this can cause a bug because with path smoothing the",
    "calculated split gain can be positive even with zero gradient and hessian.",
    "In distributed mode, local node doesn't have histograms on all features, cannot perform \"intermediate\" monotone constraints.",
    "\"intermediate\" monotone constraints need to recompute splits. If the features are sampled when computing the",
    "split initially, then the sampling needs to be recorded or done once again, which is currently not supported",
    "first round: fill the single val group",
    "always push the last group",
    "put dense feature first",
    "sort by non zero cnt",
    "sort by non zero cnt, bigger first",
    "shuffle groups",
    "Using std::swap for vector<bool> will cause the wrong result.",
    "get num_features",
    "get bin_mappers",
    "for sparse multi value bin, we store the feature bin values with offset added",
    "for dense multi value bin, the feature bin values without offsets are used",
    "copy feature bin mapper data",
    "copy feature bin mapper data",
    "update CUDA storage for column data and metadata",
    "if not pass a filename, just append \".bin\" of original file",
    "Write the basic header information for the dataset",
    "get size of meta data",
    "write meta data",
    "write feature data",
    "get size of feature",
    "write feature",
    "write raw data; use row-major order so we can read row-by-row",
    "Calculate approximate size of output and reserve space",
    "write feature group definitions",
    "Give a little extra just in case, to avoid unnecessary resizes",
    "Write token that marks the data as binary reference, and the version",
    "Write the basic definition of the overall dataset",
    "write feature group definitions",
    "get size of feature",
    "write feature",
    "size of feature names and forced bins",
    "write header",
    "write feature names",
    "write forced bins",
    "explicitly initialize template methods, for cross module call",
    "explicitly initialize template methods, for cross module call",
    "Only one multi-val group, just simply merge",
    "Skip the leading 0 when copying group_bin_boundaries.",
    "regenerate other fields",
    "need to iterate bin iterator",
    "is dense column",
    "is sparse column",
    "initialize the subset cuda column data",
    "if one column has too many bins, use a separate partition for that column",
    "try if adding this column exceed the maximum number per partition",
    "if one column has too many bins, use a separate partition for that column",
    "try if adding this column exceed the maximum number per partition",
    "if LightGBM-specific default has been set, ignore OpenMP-global config",
    "otherwise, default to OpenMP-global config",
    "ensure that if LGBM_SetMaxThreads() was ever called, LightGBM doesn't",
    "use more than that many threads",
    "store the importance first",
    "PredictRaw",
    "PredictRawByMap",
    "Predict",
    "PredictByMap",
    "PredictLeafIndex",
    "PredictLeafIndexByMap",
    "output model type",
    "output number of class",
    "output label index",
    "output max_feature_idx",
    "output objective",
    "output tree models",
    "store the importance first",
    "sort the importance",
    "use serialized string to restore this object",
    "Use first 128 chars to avoid exceed the message buffer.",
    "get number of classes",
    "get index of label",
    "get max_feature_idx first",
    "get average_output",
    "get feature names",
    "get monotone_constraints",
    "set zero",
    "predict all the trees for one iteration",
    "check early stopping",
    "set zero",
    "predict all the trees for one iteration",
    "check early stopping",
    "margin_threshold will be captured by value",
    "copy and sort",
    "margin_threshold will be captured by value",
    "Fix for compiler warnings about reaching end of control",
    "load forced_splits file",
    "init tree learner",
    "push training metrics",
    "get max feature index",
    "get label index",
    "get feature names",
    "get parser config file content",
    "check that forced splits does not use feature indices larger than dataset size",
    "if need bagging, create buffer",
    "for a validation dataset, we need its score and metric",
    "update score",
    "objective function will calculate gradients and hessians",
    "output used time per iteration",
    "boosting from average label; or customized \"average\" if implemented for the current objective",
    "boosting first",
    "use customized objective function",
    "need to copy customized gradients when using GOSS",
    "bagging logic",
    "need to copy gradients for bagging subset.",
    "shrinkage by learning rate",
    "update score",
    "only add default score one-time",
    "updates scores",
    "add model",
    "reset score",
    "remove model",
    "print message for metric",
    "pop last early_stopping_round_ models",
    "update training score",
    "we need to predict out-of-bag scores of data for boosting",
    "update validation score",
    "print training metric",
    "print validation metric",
    "set zero",
    "predict all the trees for one iteration",
    "predict all the trees for one iteration",
    "push training metrics",
    "not same training data, need reset score and others",
    "create score tracker",
    "update score",
    "resize gradient vectors to copy the customized gradients for goss or bagging with subset",
    "load forced_splits file",
    "if exists initial score, will start from it",
    "clear host score buffer",
    "Get the max size of pool",
    "at least need 2 leaves",
    "push split information for all leaves",
    "initialize splits for leaf",
    "initialize data partition",
    "initialize ordered gradients and hessians",
    "cannot change is_hist_col_wise during training",
    "initialize splits for leaf",
    "initialize data partition",
    "initialize ordered gradients and hessians",
    "Get the max size of pool",
    "at least need 2 leaves",
    "push split information for all leaves",
    "some initial works before training",
    "root leaf",
    "only root leaf can be splitted on first time",
    "some initial works before finding best split",
    "find best threshold for every feature",
    "Get a leaf with max split gain",
    "Get split information for best leaf",
    "cannot split, quit",
    "split tree with best leaf",
    "reset histogram pool",
    "initialize data partition",
    "reset the splits for leaves",
    "Sumup for root",
    "use all data",
    "use bagging, only use part of data",
    "check depth of current leaf",
    "only need to check left leaf, since right leaf is in same level of left leaf",
    "no enough data to continue",
    "only have root",
    "put parent(left) leaf's histograms into larger leaf's histograms",
    "put parent(left) leaf's histograms to larger leaf's histograms",
    "construct smaller leaf",
    "construct larger leaf",
    "find splits",
    "only has root leaf",
    "start at root leaf",
    "Histogram construction require parent features.",
    "then, compute own splits",
    "split info should exist because searching in bfs fashion - should have added from parent",
    "update before tree split",
    "don't need to update this in data-based parallel model",
    "split tree, will return right leaf",
    "store the true split gain in tree model",
    "don't need to update this in data-based parallel model",
    "store the true split gain in tree model",
    "init the leaves that used on next iteration",
    "update leave outputs if needed",
    "bag_mapper[index_mapper[i]]",
    "it is needed to filter the features after the above code.",
    "Otherwise, the `is_splittable` in `FeatureHistogram` will be wrong, and cause some features being accidentally filtered in the later nodes.",
    "for root leaf the \"parent\" output is its own output because we don't apply any smoothing to the root",
    "can't use GetParentOutput because leaf_splits doesn't have weight property set",
    "find splits",
    "identify features containing nans",
    "preallocate the matrix used to calculate linear model coefficients",
    "store only upper triangular half of matrix as an array, in row-major order",
    "this requires (max_num_feat + 1) * (max_num_feat + 2) / 2 entries (including the constant terms of the regression)",
    "we add another 8 to ensure cache lines are not shared among processors",
    "some initial works before training",
    "root leaf",
    "only root leaf can be splitted on first time",
    "some initial works before finding best split",
    "find best threshold for every feature",
    "Get a leaf with max split gain",
    "Get split information for best leaf",
    "cannot split, quit",
    "split tree with best leaf",
    "map data to leaf number",
    "calculate coefficients using the method described in Eq 3 of https://arxiv.org/pdf/1802.05640.pdf",
    "the coefficients vector is given by",
    "- (X_T * H * X + lambda) ^ (-1) * (X_T * g)",
    "where:",
    "X is the matrix where the first column is the feature values and the second is all ones,",
    "H is the diagonal matrix of the hessian,",
    "lambda is the diagonal matrix with diagonal entries equal to the regularisation term linear_lambda",
    "g is the vector of gradients",
    "the subscript _T denotes the transpose",
    "create array of pointers to raw data, and coefficient matrices, for each leaf",
    "clear the coefficient matrices",
    "aggregate results from different threads",
    "copy into eigen matrices and solve",
    "update the tree properties",
    "need to be able to hold smaller and larger best splits in SyncUpGlobalBestSplit",
    "get feature partition",
    "get local used features",
    "get best split at smaller leaf",
    "find local best split for larger leaf",
    "sync global best info",
    "update best split",
    "instantiate template classes, otherwise linker cannot find the code",
    "initialize SerialTreeLearner",
    "Get local rank and global machine size",
    "need to be able to hold smaller and larger best splits in SyncUpGlobalBestSplit",
    "allocate buffer for communication",
    "get block start and block len for reduce scatter",
    "get buffer_write_start_pos",
    "get buffer_read_start_pos",
    "generate feature partition for current tree",
    "get local used feature",
    "get block start and block len for reduce scatter",
    "sync global data sumup info",
    "global sumup reduce",
    "copy back",
    "set global sumup info",
    "init global data count in leaf",
    "reset hist num bits according to global num data",
    "sync global data sumup info",
    "global sumup reduce",
    "copy back",
    "set global sumup info",
    "init global data count in leaf",
    "clear histogram buffer before synchronizing",
    "otherwise histogram contents from the previous iteration will be sent",
    "construct local histograms",
    "copy to buffer",
    "Reduce scatter for histogram",
    "restore global histograms from buffer",
    "only root leaf",
    "construct histgroms for large leaf, we init larger leaf as the parent, so we can just subtract the smaller leaf's histograms",
    "find local best split for larger leaf",
    "sync global best info",
    "set best split",
    "need update global number of data in leaf",
    "reset hist num bits according to global num data",
    "instantiate template classes, otherwise linker cannot find the code",
    "initialize SerialTreeLearner",
    "some additional variables needed for GPU trainer",
    "Initialize GPU buffers and kernels",
    "some functions used for debugging the GPU histogram construction",
    "printf(\"grad %g != %g (%d ULPs)\\n\", GET_GRAD(h1, i), GET_GRAD(h2, i), ulps);",
    "goto err;",
    "we roughly want 256 workgroups per device, and we have num_dense_feature4_ feature tuples.",
    "also guarantee that there are at least 2K examples per workgroup",
    "return 0;",
    "we have already copied ordered gradients, ordered Hessians and indices to GPU",
    "decide the best number of workgroups working on one feature4 tuple",
    "set work group size based on feature size",
    "each 2^exp_workgroups_per_feature workgroups work on a feature4 tuple",
    "we need to refresh the kernel arguments after reallocating",
    "The only argument that needs to be changed later is num_data_",
    "the GPU kernel will process all features in one call, and each",
    "2^exp_workgroups_per_feature (compile time constant) workgroup will",
    "process one feature4 tuple",
    "for the root node, indices are not copied",
    "for constant hessian, hessians are not copied except for the root node",
    "there will be 2^exp_workgroups_per_feature = num_workgroups / num_dense_feature4 sub-histogram per feature4",
    "and we will launch num_feature workgroups for this kernel",
    "will launch threads for all features",
    "the queue should be asynchronous, and we will can WaitAndGetHistograms() before we start processing dense feature groups",
    "copy the results asynchronously. Size depends on if double precision is used",
    "we will wait for this object in WaitAndGetHistograms",
    "when the output is ready, the computation is done",
    "values of this feature has been redistributed to multiple bins; need a reduction here",
    "how many feature-group tuples we have",
    "leave some safe margin for prefetching",
    "256 work-items per workgroup. Each work-item prefetches one tuple for that feature",
    "clear sparse/dense maps",
    "do nothing if no features can be processed on GPU",
    "allocate memory for all features (FIXME: 4 GB barrier on some devices, need to split to multiple buffers)",
    "unpin old buffer if necessary before destructing them",
    "make ordered_gradients and Hessians larger (including extra room for prefetching), and pin them",
    "allocate space for gradients and Hessians on device",
    "we will copy gradients and Hessians in after ordered_gradients_ and ordered_hessians_ are constructed",
    "allocate feature mask, for disabling some feature-groups' histogram calculation",
    "copy indices to the device",
    "histogram bin entry size depends on the precision (single/double)",
    "create output buffer, each feature has a histogram with device_bin_size_ bins,",
    "each work group generates a sub-histogram of dword_features_ features.",
    "only initialize once here, as this will not need to change when ResetTrainingData() is called",
    "create atomic counters for inter-group coordination",
    "The output buffer is allocated to host directly, to overlap compute and data transfer",
    "find the dense feature-groups and group then into Feature4 data structure (several feature-groups packed into 4 bytes)",
    "looking for dword_features_ non-sparse feature-groups",
    "decide if we need to redistribute the bin",
    "multiplier must be a power of 2",
    "device_bin_mults_.push_back(1);",
    "found",
    "for data transfer time",
    "Now generate new data structure feature4, and copy data to the device",
    "preallocate arrays for all threads, and pin them",
    "building Feature4 bundles; each thread handles dword_features_ features",
    "one feature datapoint is 4 bits",
    "this guarantees that the RawGet() function is inlined, rather than using virtual function dispatching",
    "one feature datapoint is one byte",
    "this guarantees that the RawGet() function is inlined, rather than using virtual function dispatching",
    "Dense bin",
    "Dense 4-bit bin",
    "working on the remaining (less than dword_features_) feature groups",
    "fill the leftover features",
    "fill this empty feature with some \"random\" value",
    "fill this empty feature with some \"random\" value",
    "copying the last 1 to (dword_features - 1) feature-groups in the last tuple",
    "deallocate pinned space for feature copying",
    "data transfer time",
    "for other types of failure, build log might not be available; program.build_log() can crash",
    "Something bad happened. Just return \"No log available.\"",
    "build is okay, log may contain warnings",
    "destroy any old kernels",
    "create OpenCL kernels for different number of workgroups per feature",
    "currently we don't use constant memory",
    "compile the GPU kernel depending if double precision is used, constant hessian is used, etc.",
    "kernel with indices in an array",
    "kernel with all features enabled, with eliminated branches",
    "kernel with all data indices (for root node, and assumes that root node always uses all features)",
    "do nothing if no features can be processed on GPU",
    "The only argument that needs to be changed later is num_data_",
    "hessian is passed as a parameter, but it is not available now.",
    "hessian will be set in BeforeTrain()",
    "Get the max bin size, used for selecting best GPU kernel",
    "initialize GPU",
    "determine which kernel to use based on the max number of bins",
    "the +9 skips extra characters \")\", newline, \"#endif\" and newline at the beginning",
    "the +9 skips extra characters \")\", newline, \"#endif\" and newline at the beginning",
    "the +9 skips extra characters \")\", newline, \"#endif\" and newline at the beginning",
    "ignore the feature groups that contain categorical features when producing warnings about max_bin.",
    "these groups may contain larger number of bins due to categorical features, but not due to the setting of max_bin.",
    "setup GPU kernel arguments after we allocating all the buffers",
    "GPU memory has to been reallocated because data may have been changed",
    "setup GPU kernel arguments after we allocating all the buffers",
    "Copy initial full hessians and gradients to GPU.",
    "We start copying as early as possible, instead of at ConstructHistogram().",
    "setup hessian parameters only",
    "hessian is passed as a parameter",
    "use bagging",
    "On GPU, we start copying indices, gradients and Hessians now, instead at ConstructHistogram()",
    "copy used gradients and Hessians to ordered buffer",
    "transfer the indices to GPU",
    "transfer hessian to GPU",
    "setup hessian parameters only",
    "hessian is passed as a parameter",
    "transfer gradients to GPU",
    "only have root",
    "Copy indices, gradients and Hessians as early as possible",
    "only need to initialize for smaller leaf",
    "Get leaf boundary",
    "copy indices to the GPU:",
    "copy ordered Hessians to the GPU:",
    "copy ordered gradients to the GPU:",
    "do nothing if no features can be processed on GPU",
    "copy data indices if it is not null",
    "generate and copy ordered_gradients if gradients is not null",
    "generate and copy ordered_hessians if Hessians is not null",
    "converted indices in is_feature_used to feature-group indices",
    "construct the feature masks for dense feature-groups",
    "if no feature group is used, just return and do not use GPU",
    "if not all feature groups are used, we need to transfer the feature mask to GPU",
    "otherwise, we will use a specialized GPU kernel with all feature groups enabled",
    "All data have been prepared, now run the GPU kernel",
    "construct smaller leaf",
    "ConstructGPUHistogramsAsync will return true if there are available feature groups dispatched to GPU",
    "then construct sparse features on CPU",
    "wait for GPU to finish, only if GPU is actually used",
    "use double precision",
    "use single precision",
    "Compare GPU histogram with CPU histogram, useful for debugging GPU code problem",
    "#define GPU_DEBUG_COMPARE",
    "construct larger leaf",
    "then construct sparse features on CPU",
    "wait for GPU to finish, only if GPU is actually used",
    "use double precision",
    "use single precision",
    "do some sanity check for the GPU algorithm",
    "limit top k",
    "get max bin",
    "calculate buffer size",
    "need to be able to hold smaller and larger best splits in SyncUpGlobalBestSplit",
    "left and right on same time, so need double size",
    "initialize histograms for global",
    "sync global data sumup info",
    "set global sumup info",
    "init global data count in leaf",
    "get local sumup",
    "get local sumup",
    "get mean number on machines",
    "weighted gain",
    "get top k",
    "Copy histogram to buffer, and Get local aggregate features",
    "copy histograms.",
    "copy smaller leaf histograms first",
    "mark local aggregated feature",
    "copy",
    "then copy larger leaf histograms",
    "mark local aggregated feature",
    "copy",
    "use local data to find local best splits",
    "clear histogram buffer before synchronizing",
    "otherwise histogram contents from the previous iteration will be sent",
    "find splits",
    "only has root leaf",
    "local voting",
    "gather",
    "get all top-k from all machines",
    "global voting",
    "copy local histgrams to buffer",
    "Reduce scatter for histogram",
    "find best split from local aggregated histograms",
    "restore from buffer",
    "restore from buffer",
    "find local best",
    "find local best split for larger leaf",
    "sync global best info",
    "copy back",
    "set the global number of data for leaves",
    "init the global sumup info",
    "instantiate template classes, otherwise linker cannot find the code",
    "allocate CUDA memory",
    "leave some space for alignment",
    "input best split info",
    "for leaf information update",
    "gather information for CPU, used for launching kernels",
    "for leaf splits information update",
    "we need restore the order of indices in cuda_data_indices_",
    "allocate more memory for sum reduction in CUDA",
    "only the first element records the final sum",
    "intialize split find task information (a split find task is one pass through the histogram of a feature)",
    "need to double the size of histogram buffer in global memory when using double precision in histogram construction",
    "use only half the size of histogram buffer in global memory when quantized training since each gradient and hessian takes only 2 bytes",
    "use the first gpu by default",
    "std::max(..., 1UL) to avoid error in the case when there are NaN's in the categorical values",
    "use feature interaction constraint or sample features by node"
  ],
  "v4.1.0": [
    "coding: utf-8",
    "create predictor first",
    "setting early stopping via global params should be possible",
    "reduce cost for prediction training data",
    "process callbacks",
    "construct booster",
    "start training",
    "check evaluation result.",
    "ranking task, split according to groups",
    "run preprocessing on the data set if needed",
    "setting early stopping via global params should be possible",
    "setup callbacks",
    "coding: utf-8",
    "dummy function to support older version of scikit-learn",
    "coding: utf-8",
    "f(labels, preds)",
    "f(labels, preds, weights)",
    "f(labels, preds, weights, group)",
    "f(labels, preds)",
    "f(labels, preds, weights)",
    "f(labels, preds, weights, group)",
    "documentation templates for LGBMModel methods are shared between the classes in",
    "this module and those in the ``dask`` module",
    "register default metric for consistency with callable eval_metric case",
    "try to deduce from class instance",
    "overwrite default metric by explicitly set metric",
    "use joblib conventions for negative n_jobs, just like scikit-learn",
    "at predict time, this is handled later due to the order of parameter updates",
    "Do not modify original args in fit function",
    "Refer to https://github.com/microsoft/LightGBM/pull/2619",
    "Separate built-in from callable evaluation metrics",
    "concatenate metric from params (or default if not provided in params) and eval_metric",
    "copy for consistency",
    "reduce cost for prediction training data",
    "free dataset",
    "retrive original params that possibly can be used in both training and prediction",
    "and then overwrite them (considering aliases) with params that were passed directly in prediction",
    "number of threads can have values with special meaning which is only applied",
    "in the scikit-learn interface, these should not reach the c++ side as-is",
    "adjust eval metrics to match whether binary or multiclass",
    "classification is being performed",
    "do not modify args, as it causes errors in model selection tools",
    "check group data",
    "coding: utf-8",
    "coding: utf-8",
    "\"#ddffdd\" is light green, \"#ffdddd\" is light red",
    "coding: utf-8",
    "coding: utf-8",
    "we don't need lib_lightgbm while building docs",
    "TypeError: obj is not a string or a number",
    "ValueError: invalid literal",
    "DeprecationWarning is not shown by default, so let's create our own with higher level",
    "lazy evaluation to allow import without dynamic library, e.g., for docs generation",
    "if buffer length is not long enough, re-allocate a buffer",
    "avoid side effects on passed-in parameters",
    "if main_param_name was provided, keep that value and remove all aliases",
    "if main param name was not found, search for an alias",
    "neither of main_param_name, aliases were found",
    "determine feature names",
    "determine categorical features",
    "get numpy representation of the data",
    "most common case (no nullable dtypes)",
    "1.0 <= pd version < 1.1 and nullable dtypes, least common case",
    "raises error because array is casted to type(pd.NA) and there's no na_value argument",
    "data has nullable dtypes, but we can specify na_value argument and copy will be made",
    "Get total row number.",
    "Random access by row index. Used for data sampling.",
    "Range data access. Used to read data in batch when constructing Dataset.",
    "Optionally specify batch_size to control range data read size.",
    "Only required if using ``Dataset.subset()``.",
    "__get_num_preds() cannot work with nrow > MAX_INT32, so calculate overall number of predictions piecemeal",
    "avoid memory consumption by arrays concatenation operations",
    "create numpy array from output arrays",
    "break up indptr based on number of rows (note more than one matrix in multiclass case)",
    "for CSC there is extra column added",
    "reformat output into a csr or csc matrix or list of csr or csc matrices",
    "same shape as input csr or csc matrix except extra column for expected value",
    "note: make sure we copy data as it will be deallocated next",
    "free the temporary native indptr, indices, and data",
    "__get_num_preds() cannot work with nrow > MAX_INT32, so calculate overall number of predictions piecemeal",
    "avoid memory consumption by arrays concatenation operations",
    "c type: double**",
    "each double* element points to start of each column of sample data.",
    "c type int**",
    "each int* points to start of indices for each column",
    "no min_data, nthreads and verbose in this function",
    "check data has header or not",
    "need to regroup init_score",
    "process for args",
    "get categorical features",
    "If the params[cat_alias] is equal to categorical_indices, do not report the warning.",
    "process for reference dataset",
    "start construct data",
    "set feature names",
    "Select sampled rows, transpose to column order.",
    "create validation dataset from ref_dataset",
    "create valid",
    "construct subset",
    "create train",
    "could be updated if data is not freed",
    "set to None",
    "we're done if self and reference share a common upstream reference",
    "most common case (no nullable dtypes)",
    "1.0 <= pd version < 1.1 and nullable dtypes, least common case",
    "raises error because array is casted to type(pd.NA) and there's no na_value argument",
    "data has nullable dtypes, but we can specify na_value argument and copy will be made",
    "if buffer length is not long enough, reallocate buffers",
    "group data from LightGBM is boundaries data, need to convert to group size",
    "Training task",
    "if \"machines\" is given, assume user wants to do distributed learning, and set up network",
    "construct booster object",
    "copy the parameters from train_set",
    "save reference to data",
    "buffer for inner predict",
    "Prediction task",
    "if buffer length is not long enough, re-allocate a buffer",
    "if a single node tree it won't have `leaf_index` so return 0",
    "Create the node record, and populate universal data members",
    "Update values to reflect node type (leaf or split)",
    "traverse the next level of the tree",
    "In tree format, \"subtree_list\" is a list of node records (dicts),",
    "and we add node to the list.",
    "need reset training data",
    "need to push new valid data",
    "ensure that existing Booster is freed before replacing it",
    "with a new one createdfrom file",
    "if buffer length is not long enough, re-allocate a buffer",
    "if buffer length is not long enough, reallocate a buffer",
    "Copy models",
    "Get name of features",
    "if buffer length is not long enough, reallocate buffers",
    "avoid to predict many time in one iteration",
    "Get num of inner evals",
    "Get name of eval metrics",
    "if buffer length is not long enough, reallocate buffers",
    "coding: utf-8",
    "Callback environment used by callbacks",
    "split is needed for \"<dataset type> <metric>\" case (e.g. \"train l1\")",
    "self.best_score_list is initialized to an empty list",
    "split is needed for \"<dataset type> <metric>\" case (e.g. \"train l1\")",
    "coding: utf-8",
    "Acquire port in worker",
    "schedule futures to retrieve each element of the tuple",
    "retrieve ports",
    "Concatenate many parts into one",
    "construct local eval_set data.",
    "store indices of eval_set components that were not contained within local parts.",
    "consolidate parts of each individual eval component.",
    "require that eval_name exists in evaluated result data in case dropped due to padding.",
    "in distributed training the 'training' eval_set is not detected, will have name 'valid_<index>'.",
    "filter padding from eval parts then _concat each eval_set component.",
    "reconstruct eval_set fit args/kwargs depending on which components of eval_set are on worker.",
    "ensure that expected keys for evals_result_ and best_score_ exist regardless of padding.",
    "capture whether local_listen_port or its aliases were provided",
    "capture whether machines or its aliases were provided",
    "Some passed-in parameters can be removed:",
    "* 'num_machines': set automatically from Dask worker list",
    "* 'num_threads': overridden to match nthreads on each Dask process",
    "Split arrays/dataframes into parts. Arrange parts into dicts to enforce co-locality",
    "evals_set will to be re-constructed into smaller lists of (X, y) tuples, where",
    "X and y are each delayed sub-lists of original eval dask Collections.",
    "find maximum number of parts in an individual eval set so that we can",
    "pad eval sets when they come in different sizes.",
    "when individual eval set is equivalent to training data, skip recomputing parts.",
    "add None-padding for individual eval_set member if it is smaller than the largest member.",
    "first time a chunk of this eval set is added to this part.",
    "append additional chunks of this eval set to this part.",
    "ensure that all evaluation parts map uniquely to one part.",
    "assign sub-eval_set components to worker parts.",
    "Start computation in the background",
    "trigger error locally",
    "Find locations of all parts and map them to particular Dask workers",
    "Check that all workers were provided some of eval_set. Otherwise warn user that validation",
    "data artifacts may not be populated depending on worker returning final estimator.",
    "assign general validation set settings to fit kwargs.",
    "resolve aliases for network parameters and pop the result off params.",
    "these values are added back in calls to `_train_part()`",
    "figure out network params",
    "Tell each worker to train on the parts that it has locally",
    "",
    "This code treats ``_train_part()`` calls as not \"pure\" because:",
    "1. there is randomness in the training process unless parameters ``seed``",
    "and ``deterministic`` are set",
    "2. even with those parameters set, the output of one ``_train_part()`` call",
    "relies on global state (it and all the other LightGBM training processes",
    "coordinate with each other)",
    "if network parameters were changed during training, remove them from the",
    "returned model so that they're generated dynamically on every run based",
    "on the Dask cluster you're connected to and which workers have pieces of",
    "the training data",
    "dask.DataFrame.map_partitions() expects each call to return a pandas DataFrame or Series",
    "for multi-class classification with sparse matrices, pred_contrib predictions",
    "are returned as a list of sparse matrices (one per class)",
    "pred_contrib output will have one column per feature,",
    "plus one more for the base value",
    "need to tell Dask the expected type and shape of individual preds",
    "by default, dask.array.concatenate() concatenates sparse arrays into a COO matrix",
    "the code below is used instead to ensure that the sparse type is preserved during concatentation",
    "At this point, `out` is a list of lists of delayeds (each of which points to a matrix).",
    "Concatenate them to return a list of Dask Arrays.",
    "DaskLGBMClassifier does not support group, eval_group.",
    "DaskLGBMClassifier support for callbacks and init_model is not tested",
    "DaskLGBMRegressor does not support group, eval_class_weight, eval_group.",
    "DaskLGBMRegressor support for callbacks and init_model is not tested",
    "DaskLGBMRanker does not support eval_class_weight or early stopping",
    "DaskLGBMRanker support for callbacks and init_model is not tested",
    "coding: utf-8",
    "load or create your dataset",
    "create dataset for lightgbm",
    "if you want to re-use data, remember to set free_raw_data=False",
    "specify your configurations as a dict",
    "generate feature names",
    "feature_name and categorical_feature",
    "check feature name",
    "save model to file",
    "dump model to JSON (and save to file)",
    "feature names",
    "feature importances",
    "load model to predict",
    "can only predict with the best iteration (or the saving iteration)",
    "eval with loaded model",
    "dump model with pickle",
    "load model with pickle to predict",
    "can predict with any iteration when loaded in pickle way",
    "eval with loaded model",
    "continue training",
    "init_model accepts:",
    "1. model file name",
    "2. Booster()",
    "decay learning rates",
    "reset_parameter callback accepts:",
    "1. list with length = num_boost_round",
    "2. function(curr_iter)",
    "change other parameters during training",
    "self-defined objective function",
    "f(preds: array, train_data: Dataset) -> grad: array, hess: array",
    "log likelihood loss",
    "self-defined eval metric",
    "f(preds: array, train_data: Dataset) -> name: str, eval_result: float, is_higher_better: bool",
    "binary error",
    "NOTE: when you do customized loss function, the default prediction value is margin",
    "This may make built-in evaluation metric calculate wrong results",
    "For example, we are doing log likelihood loss, the prediction is score before logistic transformation",
    "Keep this in mind when you use the customization",
    "Pass custom objective function through params",
    "another self-defined eval metric",
    "f(preds: array, train_data: Dataset) -> name: str, eval_result: float, is_higher_better: bool",
    "accuracy",
    "NOTE: when you do customized loss function, the default prediction value is margin",
    "This may make built-in evaluation metric calculate wrong results",
    "For example, we are doing log likelihood loss, the prediction is score before logistic transformation",
    "Keep this in mind when you use the customization",
    "Pass custom objective function through params",
    "callback",
    "coding: utf-8",
    "load or create your dataset",
    "train",
    "predict",
    "eval",
    "feature importances",
    "self-defined eval metric",
    "f(y_true: array, y_pred: array) -> name: str, eval_result: float, is_higher_better: bool",
    "Root Mean Squared Logarithmic Error (RMSLE)",
    "train",
    "another self-defined eval metric",
    "f(y_true: array, y_pred: array) -> name: str, eval_result: float, is_higher_better: bool",
    "Relative Absolute Error (RAE)",
    "train",
    "predict",
    "eval",
    "other scikit-learn modules",
    "coding: utf-8",
    "load or create your dataset",
    "create dataset for lightgbm",
    "specify your configurations as a dict",
    "train",
    "coding: utf-8",
    "################",
    "Simulate some binary data with a single categorical and",
    "single continuous predictor",
    "################",
    "Set up a couple of utilities for our experiments",
    "################",
    "Observe the behavior of `binary` and `xentropy` objectives",
    "Trying this throws an error on non-binary values of y:",
    "experiment('binary', label_type='probability', DATA)",
    "The speed of `binary` is not drastically different than",
    "`xentropy`. `xentropy` runs faster than `binary` in many cases, although",
    "there are reasons to suspect that `binary` should run faster when the",
    "label is an integer instead of a float",
    "coding: utf-8",
    "load or create your dataset",
    "create dataset for lightgbm",
    "specify your configurations as a dict",
    "train",
    "save model to file",
    "predict",
    "eval",
    "We can also open HDF5 file once and get access to",
    "With binary dataset created, we can use either Python API or cmdline version to train.",
    "",
    "Note: in order to create exactly the same dataset with the one created in simple_example.py, we need",
    "to modify simple_example.py to pass numpy array instead of pandas DataFrame to Dataset constructor.",
    "The reason is that DataFrame column names will be used in Dataset. For a DataFrame with Int64Index",
    "as columns, Dataset will use column names like [\"0\", \"1\", \"2\", ...]. While for numpy array, column names",
    "are using the default one assigned in C++ code (dataset_loader.cpp), like [\"Column_0\", \"Column_1\", ...].",
    "Y has a single column and we read it in single shot. So store it as an 1-d array.",
    "We use random access for data sampling when creating LightGBM Dataset from Sequence.",
    "When accessing any element in a HDF5 chunk, it's read entirely.",
    "To save I/O for sampling, we should keep number of total chunks much larger than sample count.",
    "Here we are just creating a chunk size that matches with batch_size.",
    "",
    "Also note that the data is stored in row major order to avoid extra copy when passing to",
    "lightgbm Dataset.",
    "Save to 2 HDF5 files for demonstration.",
    "We can store multiple datasets inside a single HDF5 file.",
    "Separating X and Y for choosing best chunk size for data loading.",
    "split training data into two partitions",
    "make this array dense because we're splitting across",
    "a sparse boundary to partition the data",
    "the code below uses sklearn.metrics, but this requires pulling all of the",
    "predictions and target values back from workers to the client",
    "",
    "for larger datasets, consider the metrics from dask-ml instead",
    "https://ml.dask.org/modules/api.html#dask-ml-metrics-metrics",
    "coding: utf-8",
    "!/usr/bin/env python3",
    "-*- coding: utf-8 -*-",
    "",
    "LightGBM documentation build configuration file, created by",
    "sphinx-quickstart on Thu May  4 14:30:58 2017.",
    "",
    "This file is execfile()d with the current directory set to its",
    "containing dir.",
    "",
    "Note that not all possible configuration values are present in this",
    "autogenerated file.",
    "",
    "All configuration values have a default; values that are commented out",
    "serve to show the default.",
    "If extensions (or modules to document with autodoc) are in another directory,",
    "add these directories to sys.path here. If the directory is relative to the",
    "documentation root, use os.path.abspath to make it absolute.",
    "-- General configuration ------------------------------------------------",
    "If your documentation needs a minimal Sphinx version, state it here.",
    "Add any Sphinx extension module names here, as strings. They can be",
    "extensions coming with Sphinx (named 'sphinx.ext.*') or your custom",
    "ones.",
    "mock out modules",
    "hide type hints in API docs",
    "Generate autosummary pages. Output should be set with: `:toctree: pythonapi/`",
    "Only the class' docstring is inserted.",
    "If true, `todo` and `todoList` produce output, else they produce nothing.",
    "The master toctree document.",
    "General information about the project.",
    "The name of an image file (relative to this directory) to place at the top",
    "of the sidebar.",
    "The name of an image file (relative to this directory) to use as a favicon of",
    "the docs.  This file should be a Windows icon file (.ico) being 16x16 or 32x32",
    "pixels large.",
    "The version info for the project you're documenting, acts as replacement for",
    "|version| and |release|, also used in various other places throughout the",
    "built documents.",
    "The short X.Y version.",
    "The full version, including alpha/beta/rc tags.",
    "The language for content autogenerated by Sphinx. Refer to documentation",
    "for a list of supported languages.",
    "",
    "This is also used if you do content translation via gettext catalogs.",
    "Usually you set \"language\" from the command line for these cases.",
    "List of patterns, relative to source directory, that match files and",
    "directories to ignore when looking for source files.",
    "This patterns also effect to html_static_path and html_extra_path",
    "The name of the Pygments (syntax highlighting) style to use.",
    "-- Configuration for C API docs generation ------------------------------",
    "-- Options for HTML output ----------------------------------------------",
    "The theme to use for HTML and HTML Help pages.  See the documentation for",
    "a list of builtin themes.",
    "Theme options are theme-specific and customize the look and feel of a theme",
    "further.  For a list of options available for each theme, see the",
    "documentation.",
    "Add any paths that contain custom static files (such as style sheets) here,",
    "relative to this directory. They are copied after the builtin static files,",
    "so a file named \"default.css\" will overwrite the builtin \"default.css\".",
    "-- Options for HTMLHelp output ------------------------------------------",
    "Output file base name for HTML help builder.",
    "-- Options for LaTeX output ---------------------------------------------",
    "The name of an image file (relative to this directory) to place at the top of",
    "the title page.",
    "intersphinx configuration",
    "Warning! The following code can cause buffer overflows on RTD.",
    "Consider suppressing output completely if RTD project silently fails.",
    "Refer to https://github.com/svenevs/exhale",
    "/blob/fe7644829057af622e467bb529db6c03a830da99/exhale/deploy.py#L99-L111",
    "Warning! The following code can cause buffer overflows on RTD.",
    "Consider suppressing output completely if RTD project silently fails.",
    "Refer to https://github.com/svenevs/exhale",
    "/blob/fe7644829057af622e467bb529db6c03a830da99/exhale/deploy.py#L99-L111",
    "coding: utf-8",
    "This is a basic test for floating number parsing.",
    "Most of the test cases come from:",
    "https://github.com/dmlc/xgboost/blob/master/tests/cpp/common/test_charconv.cc",
    "https://github.com/Alexhuszagh/rust-lexical/blob/master/data/test-parse-unittests/strtod_tests.toml",
    "FLT_MAX",
    "FLT_MIN",
    "DBL_MAX (1 + (1 - 2^-52)) * 2^1023 = (2^53 - 1) * 2^971",
    "2^971 * (2^53 - 1 + 1/2) : the smallest number resolving to inf",
    "Near DBL_MIN",
    "DBL_MIN 2^-1022",
    "The behavior for parsing -nan depends on implementation.",
    "Thus we skip binary check for negative nan.",
    "See comment in test_cases.",
    "construct sample data first (use all data for convenience and since size is small)",
    "Load some test data",
    "Use the smaller \".test\" data because we don't care about the actual data and it's smaller",
    "Add some fake initial_scores and groups so we can test streaming them",
    "Now use the reference dataset schema to make some testable Datasets with N rows each",
    "Load some test data",
    "Use the smaller \".test\" data because we don't care about the actual data and it's smaller",
    "Add some fake initial_scores and groups so we can test streaming them",
    "Now use the reference dataset schema to make some testable Datasets with N rows each",
    "Load some test data",
    "Serialize the reference",
    "Deserialize the reference",
    "Confirm 1 successful API call",
    "Free memory",
    "Test that Data() points to first value written",
    "Constants",
    "Start with some content:",
    "Clear & re-use:",
    "Output should match new content:",
    "Number of trials for each new ChunkedArray configuration. Pass 100 times over the search space:",
    "Each outer loop iteration changes the test by adding +1 chunk. We start with 1 chunk only:",
    "Sweep valid and invalid addresses with a ChunkedArray with `chunks` chunks:",
    "Compute a new trial address & value & if it is a valid address:",
    "Insert item. If at a valid address, 0 is returned, otherwise, -1 is returned:",
    "If at valid address, check that the stored value is correct & remember it for the future:",
    "Check the just-stored value with getitem():",
    "Also store the just-stored value for future tracking:",
    "Final check: ensure even with overrides, all valid insertions store the latest value at that address:",
    "Test in 2 ways that the values are correctly laid out in memory:",
    "Since init_scores are in a column format, but need to be pushed as rows, we have to extract each batch",
    "Use multiple threads to test concurrency",
    "Since init_scores are in a column format, but need to be pushed as rows, we have to extract each batch",
    "Calculate expected boundaries",
    "Extract a set of rows from the column-based format (still maintaining column based format)",
    "coding: utf-8",
    "coding: utf-8",
    "check saved model persistence",
    "we need to check the consistency of model file here, so test for exact equal",
    "check early stopping is working. Make it stop very early, so the scores should be very close to zero",
    "scores likely to be different, but prediction should still be the same",
    "test that shape is checked during prediction",
    "The simple implementation is just a single \"return self.ndarray[idx]\"",
    "The following is for demo and testing purpose.",
    "whole col",
    "half col",
    "Create dataset from numpy array directly.",
    "Create dataset using Sequence.",
    "Test for validation set.",
    "Select some random rows as valid data.",
    "From Dataset constructor, with dataset from numpy array.",
    "From Dataset.create_valid, with dataset from sequence.",
    "test that method works even with free_raw_data=True",
    "test that method works but sets raw data to None in case of immergeable data types",
    "test that method works for different data types",
    "Set extremely harsh penalties, so CEGB will block most splits.",
    "Compare pairs of penalties, to ensure scaling works as intended",
    "Reset booster1's parameters to p2, so the parameter section of the file matches.",
    "should resolve duplicate aliases, and prefer the main parameter",
    "should choose the highest priority alias and set that value on main param",
    "if only aliases are used",
    "should use the default if main param and aliases are missing",
    "all changes should be made on copies and not modify the original",
    "preserves None found for main param and still removes aliases",
    "correctly chooses value when only an alias is provided",
    "adds None if that's given as the default and param not found",
    "If callable is found in objective",
    "Value in params should be preferred to the default_value passed from keyword arguments",
    "None of objective or its aliases in params, but default_value is callable.",
    "check that the original data wasn't modified",
    "check that the built data has the codes",
    "test using defined feature names",
    "test using default feature names",
    "check for feature indices outside of range",
    "coding: utf-8",
    "add target, weight, and group to DataFrame so that partitions abide by group boundaries.",
    "set_index ensures partitions are based on group id.",
    "See https://stackoverflow.com/questions/49532824/dask-dataframe-split-partitions-based-on-a-column-or-function.",
    "separate target, weight from features.",
    "encode group identifiers into run-length encoding, the format LightGBMRanker is expecting",
    "so that within each partition, sum(g) = n_samples.",
    "ranking arrays: one chunk per group. Each chunk must include all columns.",
    "make one categorical feature relevant to the target",
    "https://github.com/microsoft/LightGBM/issues/4118",
    "extra predict() parameters should be passed through correctly",
    "pref_leaf values should have the right shape",
    "and values that look like valid tree nodes",
    "be sure LightGBM actually used at least one categorical column,",
    "and that it was correctly treated as a categorical feature",
    "shape depends on whether it is binary or multiclass classification",
    "in the special case of multi-class classification using scipy sparse matrices,",
    "the output of `.predict(..., pred_contrib=True)` is a list of sparse matrices (one per class)",
    "",
    "since that case is so different than all other cases, check the relevant things here",
    "and then return early",
    "raw scores will probably be different, but at least check that all predicted classes are the same",
    "be sure LightGBM actually used at least one categorical column,",
    "and that it was correctly treated as a categorical feature",
    "* shape depends on whether it is binary or multiclass classification",
    "* matrix for binary classification is of the form [feature_contrib, base_value],",
    "for multi-class it's [feat_contrib_class1, base_value_class1, feat_contrib_class2, base_value_class2, etc.]",
    "* contrib outputs for distributed training are different than from local training, so we can just test",
    "that the output has the right shape and base values are in the right position",
    "with a custom objective, prediction result is a raw score instead of predicted class",
    "function should have been preserved",
    "should correctly classify every sample",
    "probability estimates should be similar",
    "Scores should be the same",
    "Predictions should be roughly the same.",
    "pref_leaf values should have the right shape",
    "and values that look like valid tree nodes",
    "extra predict() parameters should be passed through correctly",
    "be sure LightGBM actually used at least one categorical column,",
    "and that it was correctly treated as a categorical feature",
    "contrib outputs for distributed training are different than from local training, so we can just test",
    "that the output has the right shape and base values are in the right position",
    "be sure LightGBM actually used at least one categorical column,",
    "and that it was correctly treated as a categorical feature",
    "Quantiles should be right",
    "be sure LightGBM actually used at least one categorical column,",
    "and that it was correctly treated as a categorical feature",
    "function should have been preserved",
    "Scores should be the same",
    "local and Dask predictions should be the same",
    "predictions should be better than random",
    "rebalance small dask.Array dataset for better performance.",
    "use many trees + leaves to overfit, help ensure that Dask data-parallel strategy matches that of",
    "serial learner. See https://github.com/microsoft/LightGBM/issues/3292#issuecomment-671288210.",
    "distributed ranker should be able to rank decently well and should",
    "have high rank correlation with scores from serial ranker.",
    "extra predict() parameters should be passed through correctly",
    "pref_leaf values should have the right shape",
    "and values that look like valid tree nodes",
    "be sure LightGBM actually used at least one categorical column,",
    "and that it was correctly treated as a categorical feature",
    "rebalance small dask.Array dataset for better performance.",
    "distributed ranker should be able to rank decently well with the least-squares objective",
    "and should have high rank correlation with scores from serial ranker.",
    "function should have been preserved",
    "Use larger trainset to prevent premature stopping due to zero loss, causing num_trees() < n_estimators.",
    "Use small chunk_size to avoid single-worker allocation of eval data partitions.",
    "test eval_class_weight, eval_init_score on binary-classification task.",
    "Note: objective's default `metric` will be evaluated in evals_result_ in addition to all eval_metrics.",
    "create eval_sets by creating new datasets or copying training data.",
    "total number of trees scales up for ova classifier.",
    "check that early stopping was not applied.",
    "checks that evals_result_ and best_score_ contain expected data and eval_set names.",
    "check that each eval_name and metric exists for all eval sets, allowing for the",
    "case when a worker receives a fully-padded eval_set component which is not evaluated.",
    "should be able to use the class without specifying a client",
    "should be able to set client after construction",
    "data on cluster1",
    "create identical data on cluster2",
    "at this point, the result of default_client() is client2 since it was the most recently",
    "created. So setting client to client1 here to test that you can select a non-default client",
    "unfitted model should survive pickling round trip, and pickling",
    "shouldn't have side effects on the model object",
    "client will always be None after unpickling",
    "fitted model should survive pickling round trip, and pickling",
    "shouldn't have side effects on the model object",
    "client will always be None after unpickling",
    "rebalance data to be sure that each worker has a piece of the data",
    "model 1 - no network parameters given",
    "model 2 - machines given",
    "model 3 - local_listen_port given",
    "training should fail because LightGBM will try to use the same",
    "port for multiple worker processes on the same machine",
    "rebalance data to be sure that each worker has a piece of the data",
    "test that \"machines\" is actually respected by creating a socket that uses",
    "one of the ports mentioned in \"machines\"",
    "The above error leaves a worker waiting",
    "an informative error should be raised if \"machines\" has duplicates",
    "\"client\" should be the only different, and the final argument",
    "value of the root node is 0 when init_score is set",
    "this test is separate because it takes a not-yet-constructed estimator",
    "coding: utf-8",
    "coding: utf-8",
    "build target, group ID vectors.",
    "build y/target and group-id vectors with user-specified group sizes.",
    "build y/target and group-id vectors according to n_samples, avg_gs, and random_gs.",
    "groups should contain > 1 element for pairwise learning objective.",
    "build feature data, X. Transform first few into informative features.",
    "coding: utf-8",
    "check that really dummy objective was used and estimator didn't learn anything",
    "prediction result is actually not transformed (is raw) due to custom objective",
    "original estimator is unaffected",
    "new estimator is unfitted, but has the same parameters",
    "Test if random_state is properly stored",
    "Test if two random states produce identical models",
    "Test if subsequent fits sample from random_state object and produce different models",
    "Test that the largest element is NOT the same, the smallest can be the same, i.e. zero",
    "With default params",
    "Tests same probabilities",
    "Tests same predictions",
    "Tests same raw scores",
    "Tests same leaf indices",
    "Tests same feature contributions",
    "Tests other parameters for the prediction works",
    "Tests start_iteration",
    "Tests same probabilities, starting from iteration 10",
    "Tests same predictions, starting from iteration 10",
    "Tests same raw scores, starting from iteration 10",
    "Tests same leaf indices, starting from iteration 10",
    "Tests same feature contributions, starting from iteration 10",
    "Tests other parameters for the prediction works, starting from iteration 10",
    "test that params passed in predict have higher priority",
    "no custom objective, no custom metric",
    "default metric",
    "non-default metric",
    "no metric",
    "non-default metric in eval_metric",
    "non-default metric with non-default metric in eval_metric",
    "non-default metric with multiple metrics in eval_metric",
    "non-default metric with multiple metrics in eval_metric for LGBMClassifier",
    "default metric for non-default objective",
    "non-default metric for non-default objective",
    "no metric",
    "non-default metric in eval_metric for non-default objective",
    "non-default metric with non-default metric in eval_metric for non-default objective",
    "non-default metric with multiple metrics in eval_metric for non-default objective",
    "custom objective, no custom metric",
    "default regression metric for custom objective",
    "non-default regression metric for custom objective",
    "multiple regression metrics for custom objective",
    "no metric",
    "default regression metric with non-default metric in eval_metric for custom objective",
    "non-default regression metric with metric in eval_metric for custom objective",
    "multiple regression metrics with metric in eval_metric for custom objective",
    "multiple regression metrics with multiple metrics in eval_metric for custom objective",
    "no custom objective, custom metric",
    "default metric with custom metric",
    "non-default metric with custom metric",
    "multiple metrics with custom metric",
    "custom metric (disable default metric)",
    "default metric for non-default objective with custom metric",
    "non-default metric for non-default objective with custom metric",
    "multiple metrics for non-default objective with custom metric",
    "custom metric (disable default metric for non-default objective)",
    "custom objective, custom metric",
    "custom metric for custom objective",
    "non-default regression metric with custom metric for custom objective",
    "multiple regression metrics with custom metric for custom objective",
    "default metric and invalid binary metric is replaced with multiclass alternative",
    "invalid binary metric is replaced with multiclass alternative",
    "default metric for non-default multiclass objective",
    "and invalid binary metric is replaced with multiclass alternative",
    "default metric and invalid multiclass metric is replaced with binary alternative",
    "invalid multiclass metric is replaced with binary alternative for custom objective",
    "Verify that can receive a list of metrics, only callable",
    "Verify that can receive a list of custom and built-in metrics",
    "Verify that works as expected when eval_metric is empty",
    "Verify that can receive a list of metrics, only built-in",
    "Verify that eval_metric is robust to receiving a list with None",
    "feval",
    "single eval_set",
    "two eval_set",
    "'val_minus_two' here is the expected number of threads for n_jobs=-2",
    "Note: according to joblib's formula, a value of n_jobs=-2 means",
    "\"use all but one thread\" (formula: n_cpus + 1 + n_jobs)",
    "try to predict with a different feature",
    "check that disabling the check doesn't raise the error",
    "make weights and init_score same types as y, just to avoid",
    "a huge number of combinations and therefore test cases",
    "make weights and init_score same types as y, just to avoid",
    "a huge number of combinations and therefore test cases",
    "coding: utf-8",
    "we're in a leaf now",
    "check that the rest of the elements have black color",
    "check that we got to the expected leaf",
    "coding: utf-8",
    "coding: utf-8",
    "check that default gives same result as k = 1",
    "check against independent calculation for k = 1",
    "check against independent calculation for k = 2",
    "check against independent calculation for k = 10",
    "check cases where predictions are equal",
    "should give same result as binary auc for 2 classes",
    "test the case where all predictions are equal",
    "test that weighted data gives different auc_mu",
    "test that equal data weights give same auc_mu as unweighted data",
    "should give 1 when accuracy = 1",
    "test loading class weights",
    "Simulates position bias for a given ranking dataset.",
    "The ouput dataset is identical to the input one with the exception for the relevance labels.",
    "The new labels are generated according to an instance of a cascade user model:",
    "for each query, the user is simulated to be traversing the list of documents ranked by a baseline ranker",
    "(in our example it is simply the ordering by some feature correlated with relevance, e.g., 34)",
    "and clicks on that document (new_label=1) with some probability 'pclick' depending on its true relevance;",
    "at each position the user may stop the traversal with some probability pstop. For the non-clicked documents,",
    "new_label=0. Thus the generated new labels are biased towards the baseline ranker.",
    "The positions of the documents in the ranked lists produced by the baseline, are returned.",
    "a mapping of a document's true relevance (defined on a 5-grade scale) into the probability of clicking it",
    "an instantiation of a cascade model where the user stops with probability 0.2 after observing each document",
    "simulate position bias for the train dataset and put the train dataset with biased labels to temp directory",
    "the performance of the unbiased LambdaMART should outperform the plain LambdaMART on the dataset with position bias",
    "add extra row to position file",
    "simulate position bias for the train dataset and put the train dataset with biased labels to temp directory",
    "test setting positions through Dataset constructor with numpy array",
    "the performance of the unbiased LambdaMART should outperform the plain LambdaMART on the dataset with position bias",
    "test setting positions through Dataset constructor with pandas Series",
    "test setting positions through set_position",
    "test get_position works",
    "no early stopping",
    "early stopping occurs",
    "regular early stopping",
    "positive min_delta",
    "test custom eval metrics",
    "shuffle = False, override metric in params",
    "shuffle = True, callbacks",
    "enable display training loss",
    "self defined folds",
    "LambdaRank",
    "... with l2 metric",
    "... with NDCG (default) metric",
    "self defined folds with lambdarank",
    "init_model from an in-memory Booster",
    "init_model from a text file",
    "predictions should be identical",
    "with early stopping",
    "predict by each fold booster",
    "check that each booster predicted using the best iteration",
    "fold averaging",
    "without early stopping",
    "test feature_names with whitespaces",
    "This has non-ascii strings.",
    "check that passing parameters to the constructor raises warning and ignores them",
    "take subsets and train",
    "generate CSR sparse dataset",
    "convert data to dense and get back same contribs",
    "validate the values are the same",
    "validate using CSC matrix",
    "validate the values are the same",
    "generate CSR sparse dataset",
    "convert data to dense and get back same contribs",
    "validate the values are the same",
    "validate using CSC matrix",
    "validate the values are the same",
    "Note there is an extra column added to the output for the expected value",
    "Note output CSC shape should be same as CSR output shape",
    "test sliced labels",
    "append some columns",
    "append some rows",
    "test sliced 2d matrix",
    "test sliced CSR",
    "trees start at position 1.",
    "split_features are in 4th line.",
    "test if a penalty as high as the depth indeed prohibits all monotone splits",
    "The penalization is so high that the first 2 features should not be used here",
    "Check that a very high penalization is the same as not using the features at all",
    "check refit accepts dataset_params",
    "the following checks that dart and rf with mape can predict outside the 0-1 range",
    "https://github.com/microsoft/LightGBM/issues/1579",
    "no custom objective, no feval",
    "default metric",
    "non-default metric in params",
    "default metric in args",
    "non-default metric in args",
    "metric in args overwrites one in params",
    "metric in args overwrites one in params",
    "multiple metrics in params",
    "multiple metrics in args",
    "remove default metric by 'None' in list",
    "remove default metric by 'None' aliases",
    "custom objective, no feval",
    "no default metric",
    "metric in params",
    "metric in args",
    "metric in args overwrites its' alias in params",
    "multiple metrics in params",
    "multiple metrics in args",
    "no custom objective, feval",
    "default metric with custom one",
    "non-default metric in params with custom one",
    "default metric in args with custom one",
    "non-default metric in args with custom one",
    "metric in args overwrites one in params, custom one is evaluated too",
    "multiple metrics in params with custom one",
    "multiple metrics in args with custom one",
    "custom metric is evaluated despite 'None' is passed",
    "custom objective, feval",
    "no default metric, only custom one",
    "metric in params with custom one",
    "metric in args with custom one",
    "metric in args overwrites one in params, custom one is evaluated too",
    "multiple metrics in params with custom one",
    "multiple metrics in args with custom one",
    "custom metric is evaluated despite 'None' is passed",
    "no custom objective, no feval",
    "default metric",
    "default metric in params",
    "non-default metric in params",
    "multiple metrics in params",
    "remove default metric by 'None' aliases",
    "custom objective, no feval",
    "no default metric",
    "metric in params",
    "multiple metrics in params",
    "no custom objective, feval",
    "default metric with custom one",
    "default metric in params with custom one",
    "non-default metric in params with custom one",
    "multiple metrics in params with custom one",
    "custom metric is evaluated despite 'None' is passed",
    "custom objective, feval",
    "no default metric, only custom one",
    "metric in params with custom one",
    "multiple metrics in params with custom one",
    "custom metric is evaluated despite 'None' is passed",
    "Custom objective replaces multiclass",
    "multiclass default metric",
    "multiclass default metric with custom one",
    "multiclass metric alias with custom one for custom objective",
    "no metric for invalid class_num",
    "custom metric for invalid class_num",
    "multiclass metric alias with custom one with invalid class_num",
    "multiclass default metric without num_class",
    "multiclass metric alias",
    "multiclass metric",
    "non-valid metric for multiclass objective",
    "non-default num_class for default objective",
    "no metric with non-default num_class for custom objective",
    "multiclass metric alias for custom objective",
    "multiclass metric for custom objective",
    "binary metric with non-default num_class for custom objective",
    "Expect three metrics but mean and stdv for each metric",
    "test XGBoost-style return value",
    "test numpy-style return value",
    "test bins string type",
    "test histogram is disabled for categorical features",
    "test for lgb.train",
    "test feval for lgb.train",
    "test with two valid data for lgb.train",
    "test for lgb.cv",
    "test feval for lgb.cv",
    "test that binning works properly for features with only positive or only negative values",
    "decreasing without freeing raw data is allowed",
    "decreasing before lazy init is allowed",
    "increasing is allowed",
    "decreasing with disabled filter is allowed",
    "decreasing with enabled filter is disallowed;",
    "also changes of other params are disallowed",
    "check extra trees increases regularization",
    "check path smoothing increases regularization",
    "test edge case with one leaf",
    "check that constraint containing all features is equivalent to no constraint",
    "check that constraint partitioning the features reduces train accuracy",
    "check that constraints consisting of single features reduce accuracy further",
    "test that interaction constraints work when not all features are used",
    "check that number of threads does not affect result",
    "check that setting linear_tree=True fits better than ordinary trees when data has linear relationship",
    "test again with nans in data",
    "test again with bagging",
    "test with a feature that has only one non-nan value",
    "test with a categorical feature",
    "test refit: same results on same data",
    "test refit with save and load",
    "test refit: different results training on different data",
    "test when num_leaves - 1 < num_features and when num_leaves - 1 > num_features",
    "test that the predict once with all iterations equals summed results with start_iteration and num_iteration",
    "test the case where start_iteration <= 0, and num_iteration is None",
    "test the case where start_iteration > 0, and num_iteration <= 0",
    "test the case where start_iteration > 0, and num_iteration <= 0, with pred_leaf=True",
    "test the case where start_iteration > 0, and num_iteration <= 0, with pred_contrib=True",
    "test for regression",
    "test both with and without early stopping",
    "test for multi-class",
    "test both with and without early stopping",
    "test for binary",
    "test both with and without early stopping",
    "test against sklearn average precision metric",
    "test that average precision is 1 where model predicts perfectly",
    "data as float64",
    "test all features were used",
    "test the score is better than predicting the mean",
    "test all predictions are equal using different input dtypes",
    "introduce some missing values",
    "the previous line turns x3 into object dtype in recent versions of pandas",
    "train with regular dtypes",
    "convert to nullable dtypes",
    "test training succeeds",
    "test all features were used",
    "test the score is better than predicting the mean",
    "test equal predictions",
    "test data are taken from bug report",
    "https://github.com/microsoft/LightGBM/issues/4708",
    "modified from https://github.com/microsoft/LightGBM/issues/3679#issuecomment-938652811",
    "and https://github.com/microsoft/LightGBM/pull/5087",
    "test that the ``splits_per_leaf_`` of CEGB is cleaned before training a new tree",
    "which is done in the fix #5164",
    "without the fix:",
    "Check failed: (best_split_info.left_count) > (0)",
    "try to predict with a different feature",
    "check that disabling the check doesn't raise the error",
    "try to refit with a different feature",
    "check that disabling the check doesn't raise the error",
    "coding: utf-8",
    "If compiled appropriately, the same installation will support both GPU and CPU.",
    "Double-precision floats are only supported on x86_64 with PoCL",
    "coding: utf-8",
    "coding: utf-8",
    "These are helper functions to allow doing a stack unwind",
    "after an R allocation error, which would trigger a long jump.",
    "convert from one-based to zero-based index",
    "if any feature names were larger than allocated size,",
    "allow for a larger size and try again",
    "convert from boundaries to size",
    "--- start Booster interfaces",
    "if any eval names were larger than allocated size,",
    "allow for a larger size and try again",
    "if the model string was larger than the initial buffer, call the function again, writing directly to the R object",
    "if the model string was larger than the initial buffer, allocate a bigger buffer and try again",
    "if aliases string was larger than the initial buffer, allocate a bigger buffer and try again",
    "if aliases string was larger than the initial buffer, allocate a bigger buffer and try again",
    ".Call() calls",
    "coding: utf-8",
    "alias table",
    "names",
    "from strings",
    "tails",
    "tails",
    "the following are stored as comma separated strings but are arrays in the wrappers",
    "coding: utf-8",
    "Single row predictor to abstract away caching logic",
    "create boosting",
    "initialize the boosting",
    "create objective function",
    "initialize the objective function",
    "create training metric",
    "reset the boosting",
    "create objective function",
    "initialize the objective function",
    "calculate the nonzero data and indices size",
    "allocate data and indices arrays",
    "Get the number of trees per iteration (for multiclass scenario we output multiple sparse matrices)",
    "aggregated per row feature contribution results",
    "keep track of the row_vector sizes for parallelization",
    "copy vector results to output for each row",
    "Get the number of trees per iteration (for multiclass scenario we output multiple sparse matrices)",
    "aggregated per row feature contribution results",
    "calculate number of elements per column to construct",
    "the CSC matrix with random access",
    "keep track of column counts",
    "keep track of beginning index for each column",
    "keep track of beginning index for each matrix",
    "Note: we parallelize across matrices instead of rows because of the column_counts[m][col_idx] increment inside the loop",
    "store the row index",
    "update column count",
    "explicitly declare symbols from LightGBM namespace",
    "some help functions used to convert data",
    "Row iterator of on column for CSC matrix",
    "return value at idx, only can access by ascent order",
    "return next non-zero pair, if index < 0, means no more data",
    "start of c_api functions",
    "This API is to keep python binding's behavior the same with C++ implementation.",
    "Sample count, random seed etc. should be provided in parameters.",
    "convert internal thread id to be unique based on external thread id",
    "convert internal thread id to be unique based on external thread id",
    "sample data first",
    "sample data first",
    "sample data first",
    "local buffer to re-use memory",
    "sample data first",
    "no more data",
    "---- start of booster",
    "Single row in row-major format:",
    "---- start of some help functions",
    "data is array of pointers to individual rows",
    "set number of threads for openmp",
    "read parameters from config file",
    "remove str after \"#\"",
    "de-duplicate params",
    "prediction is needed if using input initial model(continued train)",
    "need to continue training",
    "sync up random seed for data partition",
    "load Training data",
    "load data for distributed training",
    "load data for single machine",
    "need save binary file",
    "create training metric",
    "only when have metrics then need to construct validation data",
    "Add validation data, if it exists",
    "add",
    "need save binary file",
    "add metric for validation data",
    "output used time on each iteration",
    "need init network",
    "create boosting",
    "create objective function",
    "load training data",
    "initialize the objective function",
    "initialize the boosting",
    "add validation data into boosting",
    "convert model to if-else statement code",
    "create predictor",
    "Free memory",
    "create predictor",
    "label_gain = 2^i - 1, may overflow, so we use 31 here",
    "counts for all labels",
    "start from top label, and accumulate DCG",
    "counts for all labels",
    "calculate k Max DCG by one pass",
    "get sorted indices by score",
    "calculate multi dcg by one pass",
    "wait for all client start up",
    "Don't call MPI_Finalize() here: If the destructor was called because only this node had an exception, calling MPI_Finalize() will cause all nodes to hang.",
    "Instead we will handle finalize/abort for MPI in main().",
    "default set to -1",
    "distance at k-th communication, distance[k] = 2^k",
    "set incoming rank at k-th commuication",
    "set outgoing rank at k-th commuication",
    "default set as -1",
    "construct all recursive halving map for all machines",
    "let 1 << k <= num_machines",
    "distance of each communication",
    "if num_machines = 2^k, don't need to group machines",
    "communication direction, %2 == 0 is positive",
    "neighbor at k-th communication",
    "receive data block at k-th communication",
    "send data block at k-th communication",
    "if num_machines != 2^k, need to group machines",
    "group, two machine in one group, total \"rest\" groups will have 2 machines.",
    "let left machine as group leader",
    "cache block information for groups, group with 2 machines will have double block size",
    "convert from group to node leader",
    "convert from node to group",
    "meet new group",
    "add block len for this group",
    "calculate the group block start",
    "not need to construct",
    "get receive block information",
    "accumulate block len",
    "get send block information",
    "accumulate block len",
    "static member definition",
    "if small package or small count , do it by all gather.(reduce the communication times.)",
    "assign the blocks to every rank.",
    "do reduce scatter",
    "do all gather",
    "assign blocks",
    "need use buffer here, since size of \"output\" is smaller than size after all gather",
    "copy back",
    "assign blocks",
    "start all gather",
    "when num_machines is small and data is large",
    "use output as receive buffer",
    "get current local block size",
    "get out rank",
    "get in rank",
    "get send information",
    "get recv information",
    "send and recv at same time",
    "rotate in-place",
    "use output as receive buffer",
    "get current local block size",
    "get send information",
    "get recv information",
    "send and recv at same time",
    "use output as receive buffer",
    "send and recv at same time",
    "send local data to neighbor first",
    "receive neighbor data first",
    "reduce",
    "get target",
    "get send information",
    "get recv information",
    "send and recv at same time",
    "reduce",
    "send result to neighbor",
    "receive result from neighbor",
    "copy result",
    "start up socket",
    "parse clients from file",
    "get ip list of local machine",
    "get local rank",
    "construct listener",
    "construct communication topo",
    "construct linkers",
    "free listener",
    "set timeout",
    "accept incoming socket",
    "receive rank",
    "add new socket",
    "save ranks that need to connect with",
    "start listener",
    "start connect",
    "let smaller rank connect to larger rank",
    "send local rank",
    "wait for listener",
    "print connected linkers",
    "only need to copy subset",
    "avoid to copy subset many times",
    "avoid out of range",
    "may need to recopy subset",
    "valid the type",
    "parser factory implementation.",
    "customized parser add-on.",
    "save header to parser config in case needed.",
    "save label id to parser config in case needed.",
    "Constructors",
    "Get type tag",
    "Comparisons",
    "This has to be separate, not in Statics, because Json() accesses",
    "statics().null.",
    "advance until next line, or end of input",
    "advance until closing tokens",
    "The usual case: non-escaped characters",
    "Handle escapes",
    "Extract 4-byte escape sequence",
    "Explicitly check length of the substring. The following loop",
    "relies on std::string returning the terminating NUL when",
    "accessing str[length]. Checking here reduces brittleness.",
    "JSON specifies that characters outside the BMP shall be encoded as a",
    "pair of 4-hex-digit \\u escapes encoding their surrogate pair",
    "components. Check whether we're in the middle of such a beast: the",
    "previous codepoint was an escaped lead (high) surrogate, and this is",
    "a trail (low) surrogate.",
    "Reassemble the two surrogate pairs into one astral-plane character,",
    "per the UTF-16 algorithm.",
    "Integer part",
    "Decimal part",
    "Exponent part",
    "Check for any trailing garbage",
    "Documented in json11.hpp",
    "Check for another object",
    "get column names",
    "support to get header from parser config, so could utilize following label name to id mapping logic.",
    "load label idx first",
    "if parser config file exists, feature names may be changed after customized parser applied.",
    "clear here so could use default filled feature names during dataset construction.",
    "may improve by saving real feature names defined in parser in the future.",
    "erase label column name",
    "load ignore columns",
    "load weight idx",
    "load group idx",
    "don't support query id in data file when using distributed training",
    "read data to memory",
    "sample data",
    "construct feature bin mappers & clear sample data",
    "initialize label",
    "extract features",
    "sample data from file",
    "construct feature bin mappers & clear sample data",
    "initialize label",
    "extract features",
    "load data from binary file",
    "checks whether there's a initial score file when loaded from binary data files",
    "the intial score file should with suffix \".bin.init\"",
    "check meta data",
    "need to check training data",
    "read data in memory",
    "initialize label",
    "extract features",
    "Get number of lines of data file",
    "initialize label",
    "extract features",
    "load data from binary file",
    "checks whether there's a initial score file when loaded from binary data files",
    "the intial score file should with suffix \".bin.init\"",
    "not need to check validation data",
    "check meta data",
    "check token",
    "read feature group definitions",
    "read feature size",
    "buffer to read binary file",
    "check token",
    "read size of header",
    "re-allocate space if not enough",
    "read header",
    "get header",
    "read size of meta data",
    "re-allocate space if not enough",
    "read meta data",
    "load meta data",
    "sample local used data if need to partition",
    "if not contain query file, minimal sample unit is one record",
    "if contain query file, minimal sample unit is one query",
    "if is new query",
    "read feature data",
    "read feature size",
    "re-allocate space if not enough",
    "raw data",
    "fill feature_names_ if not header",
    "get forced split",
    "if only one machine, find bin locally",
    "if have multi-machines, need to find bin distributed",
    "different machines will find bin for different features",
    "start and len will store the process feature indices for different machines",
    "machine i will find bins for features in [ start[i], start[i] + len[i] )",
    "free",
    "gather global feature bin mappers",
    "restore features bins from buffer",
    "---- private functions ----",
    "get header",
    "num_groups",
    "real_feature_idx_",
    "feature2group",
    "feature2subfeature",
    "group_bin_boundaries",
    "group_feature_start_",
    "group_feature_cnt_",
    "get feature names",
    "get forced_bin_bounds_",
    "if features are ordered, not need to use hist_buf",
    "read all lines",
    "get query data",
    "if not contain query data, minimal sample unit is one record",
    "if contain query data, minimal sample unit is one query",
    "if is new query",
    "get query data",
    "if not contain query file, minimal sample unit is one record",
    "if contain query file, minimal sample unit is one query",
    "if is new query",
    "parse features",
    "get forced split",
    "check the range of label_idx, weight_idx and group_idx",
    "skip label check if user input parser config file,",
    "because label id is got from raw features while dataset features are consistent with customized parser.",
    "fill feature_names_ if not header",
    "start find bins",
    "if only one machine, find bin locally",
    "start and len will store the process feature indices for different machines",
    "machine i will find bins for features in [ start[i], start[i] + len[i] )",
    "free",
    "gather global feature bin mappers",
    "restore features bins from buffer",
    "if doesn't need to prediction with initial model",
    "parser",
    "set label",
    "free processed line:",
    "shrink_to_fit will be very slow in linux, and seems not free memory, disable for now",
    "text_reader_->Lines()[i].shrink_to_fit();",
    "push data",
    "if is used feature",
    "if need to prediction with initial model",
    "parser",
    "set initial score",
    "set label",
    "free processed line:",
    "shrink_to_fit will be very slow in Linux, and seems not free memory, disable for now",
    "text_reader_->Lines()[i].shrink_to_fit();",
    "push data",
    "if is used feature",
    "metadata_ will manage space of init_score",
    "text data can be free after loaded feature values",
    "parser",
    "set initial score",
    "set label",
    "push data",
    "if is used feature",
    "only need part of data",
    "need full data",
    "metadata_ will manage space of init_score",
    "read size of token",
    "remove duplicates",
    "deep copy function for BinMapper",
    "mean size for one bin",
    "need a new bin",
    "update bin upper bound",
    "last bin upper bound",
    "get number of positive and negative distinct values",
    "include zero bounds and infinity bound",
    "add forced bounds, excluding zeros since we have already added zero bounds",
    "find remaining bounds",
    "find distinct_values first",
    "push zero in the front",
    "use the large value",
    "push zero in the back",
    "convert to int type first",
    "sort by counts in descending order",
    "will ignore the categorical of small counts",
    "Push the dummy bin for NaN",
    "Use MissingType::None to represent this bin contains all categoricals",
    "fix count of NaN bin",
    "check trivial(num_bin_ == 1) feature",
    "check useless bin",
    "When most_freq_bin_ != default_bin_, there are some additional data loading costs.",
    "so use most_freq_bin_ = default_bin_ when there is not so sparse",
    "calculate max bin of all features to select the int type in MultiValDenseBin",
    "for lambdarank, it needs query data for partition data in distributed learning",
    "need convert query_id to boundaries",
    "check weights",
    "check positions",
    "check query boundries",
    "contain initial score file",
    "check weights",
    "get local weights",
    "check positions",
    "get local positions",
    "check query boundries",
    "get local query boundaries",
    "contain initial score file",
    "get local initial scores",
    "re-calculate query weight",
    "save to nullptr",
    "Note that len here is row count, not num_init_score, so we compare against num_data",
    "We need to use source_size here, because len might not equal size (due to a partially loaded dataset)",
    "CUDA is handled after all insertions are complete",
    "CUDA is handled after all insertions are complete",
    "save to nullptr",
    "CUDA is handled after all insertions are complete",
    "save to nullptr",
    "save to nullptr",
    "CUDA is handled after all insertions are complete",
    "default weight file name",
    "default position file name",
    "default init_score file name",
    "use first line to count number class",
    "default query file name",
    "root is in the depth 0",
    "non-leaf",
    "leaf",
    "use this for the missing value conversion",
    "Predict func by Map to ifelse",
    "use this for the missing value conversion",
    "non-leaf",
    "left subtree",
    "right subtree",
    "leaf",
    "non-leaf",
    "left subtree",
    "right subtree",
    "leaf",
    "recursive computation of SHAP values for a decision tree",
    "extend the unique path",
    "leaf node",
    "internal node",
    "see if we have already split on this feature,",
    "if so we undo that split so we can redo it for this node",
    "recursive sparse computation of SHAP values for a decision tree",
    "extend the unique path",
    "leaf node",
    "internal node",
    "see if we have already split on this feature,",
    "if so we undo that split so we can redo it for this node",
    "add names of objective function if not providing metric",
    "equal weights for all classes",
    "generate seeds by seed.",
    "sort eval_at",
    "Only push the non-training data",
    "check for conflicts",
    "check if objective, metric, and num_class match",
    "Change pool size to -1 (no limit) when using data parallel to reduce communication costs",
    "Check max_depth and num_leaves",
    "Fits in an int, and is more restrictive than the current num_leaves",
    "force col-wise for gpu, and cuda version",
    "force row-wise for cuda version",
    "linear tree learner must be serial type and run on CPU device",
    "min_data_in_leaf must be at least 2 if path smoothing is active. This is because when the split is calculated",
    "the count is calculated using the proportion of hessian in the leaf which is rounded up to nearest int, so it can",
    "be 1 when there is actually no data in the leaf. In rare cases this can cause a bug because with path smoothing the",
    "calculated split gain can be positive even with zero gradient and hessian.",
    "In distributed mode, local node doesn't have histograms on all features, cannot perform \"intermediate\" monotone constraints.",
    "\"intermediate\" monotone constraints need to recompute splits. If the features are sampled when computing the",
    "split initially, then the sampling needs to be recorded or done once again, which is currently not supported",
    "first round: fill the single val group",
    "always push the last group",
    "put dense feature first",
    "sort by non zero cnt",
    "sort by non zero cnt, bigger first",
    "shuffle groups",
    "Using std::swap for vector<bool> will cause the wrong result.",
    "get num_features",
    "get bin_mappers",
    "for sparse multi value bin, we store the feature bin values with offset added",
    "for dense multi value bin, the feature bin values without offsets are used",
    "copy feature bin mapper data",
    "copy feature bin mapper data",
    "update CUDA storage for column data and metadata",
    "if not pass a filename, just append \".bin\" of original file",
    "Write the basic header information for the dataset",
    "get size of meta data",
    "write meta data",
    "write feature data",
    "get size of feature",
    "write feature",
    "write raw data; use row-major order so we can read row-by-row",
    "Calculate approximate size of output and reserve space",
    "write feature group definitions",
    "Give a little extra just in case, to avoid unnecessary resizes",
    "Write token that marks the data as binary reference, and the version",
    "Write the basic definition of the overall dataset",
    "write feature group definitions",
    "get size of feature",
    "write feature",
    "size of feature names and forced bins",
    "write header",
    "write feature names",
    "write forced bins",
    "explicitly initialize template methods, for cross module call",
    "explicitly initialize template methods, for cross module call",
    "Only one multi-val group, just simply merge",
    "Skip the leading 0 when copying group_bin_boundaries.",
    "regenerate other fields",
    "need to iterate bin iterator",
    "is dense column",
    "is sparse column",
    "initialize the subset cuda column data",
    "if one column has too many bins, use a separate partition for that column",
    "try if adding this column exceed the maximum number per partition",
    "if one column has too many bins, use a separate partition for that column",
    "try if adding this column exceed the maximum number per partition",
    "store the importance first",
    "PredictRaw",
    "PredictRawByMap",
    "Predict",
    "PredictByMap",
    "PredictLeafIndex",
    "PredictLeafIndexByMap",
    "output model type",
    "output number of class",
    "output label index",
    "output max_feature_idx",
    "output objective",
    "output tree models",
    "store the importance first",
    "sort the importance",
    "use serialized string to restore this object",
    "Use first 128 chars to avoid exceed the message buffer.",
    "get number of classes",
    "get index of label",
    "get max_feature_idx first",
    "get average_output",
    "get feature names",
    "get monotone_constraints",
    "set zero",
    "predict all the trees for one iteration",
    "check early stopping",
    "set zero",
    "predict all the trees for one iteration",
    "check early stopping",
    "margin_threshold will be captured by value",
    "copy and sort",
    "margin_threshold will be captured by value",
    "Fix for compiler warnings about reaching end of control",
    "load forced_splits file",
    "init tree learner",
    "push training metrics",
    "get max feature index",
    "get label index",
    "get feature names",
    "get parser config file content",
    "check that forced splits does not use feature indices larger than dataset size",
    "if need bagging, create buffer",
    "for a validation dataset, we need its score and metric",
    "update score",
    "objective function will calculate gradients and hessians",
    "output used time per iteration",
    "boosting from average label; or customized \"average\" if implemented for the current objective",
    "boosting first",
    "use customized objective function",
    "need to copy customized gradients when using GOSS",
    "bagging logic",
    "need to copy gradients for bagging subset.",
    "shrinkage by learning rate",
    "update score",
    "only add default score one-time",
    "updates scores",
    "add model",
    "reset score",
    "remove model",
    "print message for metric",
    "pop last early_stopping_round_ models",
    "update training score",
    "we need to predict out-of-bag scores of data for boosting",
    "update validation score",
    "print training metric",
    "print validation metric",
    "set zero",
    "predict all the trees for one iteration",
    "predict all the trees for one iteration",
    "push training metrics",
    "not same training data, need reset score and others",
    "create score tracker",
    "update score",
    "resize gradient vectors to copy the customized gradients for goss or bagging with subset",
    "load forced_splits file",
    "if exists initial score, will start from it",
    "clear host score buffer",
    "Get the max size of pool",
    "at least need 2 leaves",
    "push split information for all leaves",
    "initialize splits for leaf",
    "initialize data partition",
    "initialize ordered gradients and hessians",
    "cannot change is_hist_col_wise during training",
    "initialize splits for leaf",
    "initialize data partition",
    "initialize ordered gradients and hessians",
    "Get the max size of pool",
    "at least need 2 leaves",
    "push split information for all leaves",
    "some initial works before training",
    "root leaf",
    "only root leaf can be splitted on first time",
    "some initial works before finding best split",
    "find best threshold for every feature",
    "Get a leaf with max split gain",
    "Get split information for best leaf",
    "cannot split, quit",
    "split tree with best leaf",
    "reset histogram pool",
    "initialize data partition",
    "reset the splits for leaves",
    "Sumup for root",
    "use all data",
    "use bagging, only use part of data",
    "check depth of current leaf",
    "only need to check left leaf, since right leaf is in same level of left leaf",
    "no enough data to continue",
    "only have root",
    "put parent(left) leaf's histograms into larger leaf's histograms",
    "put parent(left) leaf's histograms to larger leaf's histograms",
    "construct smaller leaf",
    "construct larger leaf",
    "find splits",
    "only has root leaf",
    "start at root leaf",
    "Histogram construction require parent features.",
    "then, compute own splits",
    "split info should exist because searching in bfs fashion - should have added from parent",
    "update before tree split",
    "don't need to update this in data-based parallel model",
    "split tree, will return right leaf",
    "store the true split gain in tree model",
    "don't need to update this in data-based parallel model",
    "store the true split gain in tree model",
    "init the leaves that used on next iteration",
    "update leave outputs if needed",
    "bag_mapper[index_mapper[i]]",
    "it is needed to filter the features after the above code.",
    "Otherwise, the `is_splittable` in `FeatureHistogram` will be wrong, and cause some features being accidentally filtered in the later nodes.",
    "for root leaf the \"parent\" output is its own output because we don't apply any smoothing to the root",
    "can't use GetParentOutput because leaf_splits doesn't have weight property set",
    "find splits",
    "identify features containing nans",
    "preallocate the matrix used to calculate linear model coefficients",
    "store only upper triangular half of matrix as an array, in row-major order",
    "this requires (max_num_feat + 1) * (max_num_feat + 2) / 2 entries (including the constant terms of the regression)",
    "we add another 8 to ensure cache lines are not shared among processors",
    "some initial works before training",
    "root leaf",
    "only root leaf can be splitted on first time",
    "some initial works before finding best split",
    "find best threshold for every feature",
    "Get a leaf with max split gain",
    "Get split information for best leaf",
    "cannot split, quit",
    "split tree with best leaf",
    "map data to leaf number",
    "calculate coefficients using the method described in Eq 3 of https://arxiv.org/pdf/1802.05640.pdf",
    "the coefficients vector is given by",
    "- (X_T * H * X + lambda) ^ (-1) * (X_T * g)",
    "where:",
    "X is the matrix where the first column is the feature values and the second is all ones,",
    "H is the diagonal matrix of the hessian,",
    "lambda is the diagonal matrix with diagonal entries equal to the regularisation term linear_lambda",
    "g is the vector of gradients",
    "the subscript _T denotes the transpose",
    "create array of pointers to raw data, and coefficient matrices, for each leaf",
    "clear the coefficient matrices",
    "aggregate results from different threads",
    "copy into eigen matrices and solve",
    "update the tree properties",
    "need to be able to hold smaller and larger best splits in SyncUpGlobalBestSplit",
    "get feature partition",
    "get local used features",
    "get best split at smaller leaf",
    "find local best split for larger leaf",
    "sync global best info",
    "update best split",
    "instantiate template classes, otherwise linker cannot find the code",
    "initialize SerialTreeLearner",
    "Get local rank and global machine size",
    "need to be able to hold smaller and larger best splits in SyncUpGlobalBestSplit",
    "allocate buffer for communication",
    "get block start and block len for reduce scatter",
    "get buffer_write_start_pos",
    "get buffer_read_start_pos",
    "generate feature partition for current tree",
    "get local used feature",
    "get block start and block len for reduce scatter",
    "sync global data sumup info",
    "global sumup reduce",
    "copy back",
    "set global sumup info",
    "init global data count in leaf",
    "reset hist num bits according to global num data",
    "sync global data sumup info",
    "global sumup reduce",
    "copy back",
    "set global sumup info",
    "init global data count in leaf",
    "clear histogram buffer before synchronizing",
    "otherwise histogram contents from the previous iteration will be sent",
    "construct local histograms",
    "copy to buffer",
    "Reduce scatter for histogram",
    "restore global histograms from buffer",
    "only root leaf",
    "construct histgroms for large leaf, we init larger leaf as the parent, so we can just subtract the smaller leaf's histograms",
    "find local best split for larger leaf",
    "sync global best info",
    "set best split",
    "need update global number of data in leaf",
    "reset hist num bits according to global num data",
    "instantiate template classes, otherwise linker cannot find the code",
    "initialize SerialTreeLearner",
    "some additional variables needed for GPU trainer",
    "Initialize GPU buffers and kernels",
    "some functions used for debugging the GPU histogram construction",
    "printf(\"grad %g != %g (%d ULPs)\\n\", GET_GRAD(h1, i), GET_GRAD(h2, i), ulps);",
    "goto err;",
    "we roughly want 256 workgroups per device, and we have num_dense_feature4_ feature tuples.",
    "also guarantee that there are at least 2K examples per workgroup",
    "return 0;",
    "we have already copied ordered gradients, ordered Hessians and indices to GPU",
    "decide the best number of workgroups working on one feature4 tuple",
    "set work group size based on feature size",
    "each 2^exp_workgroups_per_feature workgroups work on a feature4 tuple",
    "we need to refresh the kernel arguments after reallocating",
    "The only argument that needs to be changed later is num_data_",
    "the GPU kernel will process all features in one call, and each",
    "2^exp_workgroups_per_feature (compile time constant) workgroup will",
    "process one feature4 tuple",
    "for the root node, indices are not copied",
    "for constant hessian, hessians are not copied except for the root node",
    "there will be 2^exp_workgroups_per_feature = num_workgroups / num_dense_feature4 sub-histogram per feature4",
    "and we will launch num_feature workgroups for this kernel",
    "will launch threads for all features",
    "the queue should be asynchronous, and we will can WaitAndGetHistograms() before we start processing dense feature groups",
    "copy the results asynchronously. Size depends on if double precision is used",
    "we will wait for this object in WaitAndGetHistograms",
    "when the output is ready, the computation is done",
    "values of this feature has been redistributed to multiple bins; need a reduction here",
    "how many feature-group tuples we have",
    "leave some safe margin for prefetching",
    "256 work-items per workgroup. Each work-item prefetches one tuple for that feature",
    "clear sparse/dense maps",
    "do nothing if no features can be processed on GPU",
    "allocate memory for all features (FIXME: 4 GB barrier on some devices, need to split to multiple buffers)",
    "unpin old buffer if necessary before destructing them",
    "make ordered_gradients and Hessians larger (including extra room for prefetching), and pin them",
    "allocate space for gradients and Hessians on device",
    "we will copy gradients and Hessians in after ordered_gradients_ and ordered_hessians_ are constructed",
    "allocate feature mask, for disabling some feature-groups' histogram calculation",
    "copy indices to the device",
    "histogram bin entry size depends on the precision (single/double)",
    "create output buffer, each feature has a histogram with device_bin_size_ bins,",
    "each work group generates a sub-histogram of dword_features_ features.",
    "only initialize once here, as this will not need to change when ResetTrainingData() is called",
    "create atomic counters for inter-group coordination",
    "The output buffer is allocated to host directly, to overlap compute and data transfer",
    "find the dense feature-groups and group then into Feature4 data structure (several feature-groups packed into 4 bytes)",
    "looking for dword_features_ non-sparse feature-groups",
    "decide if we need to redistribute the bin",
    "multiplier must be a power of 2",
    "device_bin_mults_.push_back(1);",
    "found",
    "for data transfer time",
    "Now generate new data structure feature4, and copy data to the device",
    "preallocate arrays for all threads, and pin them",
    "building Feature4 bundles; each thread handles dword_features_ features",
    "one feature datapoint is 4 bits",
    "this guarantees that the RawGet() function is inlined, rather than using virtual function dispatching",
    "one feature datapoint is one byte",
    "this guarantees that the RawGet() function is inlined, rather than using virtual function dispatching",
    "Dense bin",
    "Dense 4-bit bin",
    "working on the remaining (less than dword_features_) feature groups",
    "fill the leftover features",
    "fill this empty feature with some \"random\" value",
    "fill this empty feature with some \"random\" value",
    "copying the last 1 to (dword_features - 1) feature-groups in the last tuple",
    "deallocate pinned space for feature copying",
    "data transfer time",
    "for other types of failure, build log might not be available; program.build_log() can crash",
    "Something bad happened. Just return \"No log available.\"",
    "build is okay, log may contain warnings",
    "destroy any old kernels",
    "create OpenCL kernels for different number of workgroups per feature",
    "currently we don't use constant memory",
    "compile the GPU kernel depending if double precision is used, constant hessian is used, etc.",
    "kernel with indices in an array",
    "kernel with all features enabled, with eliminated branches",
    "kernel with all data indices (for root node, and assumes that root node always uses all features)",
    "do nothing if no features can be processed on GPU",
    "The only argument that needs to be changed later is num_data_",
    "hessian is passed as a parameter, but it is not available now.",
    "hessian will be set in BeforeTrain()",
    "Get the max bin size, used for selecting best GPU kernel",
    "initialize GPU",
    "determine which kernel to use based on the max number of bins",
    "the +9 skips extra characters \")\", newline, \"#endif\" and newline at the beginning",
    "the +9 skips extra characters \")\", newline, \"#endif\" and newline at the beginning",
    "the +9 skips extra characters \")\", newline, \"#endif\" and newline at the beginning",
    "ignore the feature groups that contain categorical features when producing warnings about max_bin.",
    "these groups may contain larger number of bins due to categorical features, but not due to the setting of max_bin.",
    "setup GPU kernel arguments after we allocating all the buffers",
    "GPU memory has to been reallocated because data may have been changed",
    "setup GPU kernel arguments after we allocating all the buffers",
    "Copy initial full hessians and gradients to GPU.",
    "We start copying as early as possible, instead of at ConstructHistogram().",
    "setup hessian parameters only",
    "hessian is passed as a parameter",
    "use bagging",
    "On GPU, we start copying indices, gradients and Hessians now, instead at ConstructHistogram()",
    "copy used gradients and Hessians to ordered buffer",
    "transfer the indices to GPU",
    "transfer hessian to GPU",
    "setup hessian parameters only",
    "hessian is passed as a parameter",
    "transfer gradients to GPU",
    "only have root",
    "Copy indices, gradients and Hessians as early as possible",
    "only need to initialize for smaller leaf",
    "Get leaf boundary",
    "copy indices to the GPU:",
    "copy ordered Hessians to the GPU:",
    "copy ordered gradients to the GPU:",
    "do nothing if no features can be processed on GPU",
    "copy data indices if it is not null",
    "generate and copy ordered_gradients if gradients is not null",
    "generate and copy ordered_hessians if Hessians is not null",
    "converted indices in is_feature_used to feature-group indices",
    "construct the feature masks for dense feature-groups",
    "if no feature group is used, just return and do not use GPU",
    "if not all feature groups are used, we need to transfer the feature mask to GPU",
    "otherwise, we will use a specialized GPU kernel with all feature groups enabled",
    "All data have been prepared, now run the GPU kernel",
    "construct smaller leaf",
    "ConstructGPUHistogramsAsync will return true if there are available feature groups dispatched to GPU",
    "then construct sparse features on CPU",
    "wait for GPU to finish, only if GPU is actually used",
    "use double precision",
    "use single precision",
    "Compare GPU histogram with CPU histogram, useful for debugging GPU code problem",
    "#define GPU_DEBUG_COMPARE",
    "construct larger leaf",
    "then construct sparse features on CPU",
    "wait for GPU to finish, only if GPU is actually used",
    "use double precision",
    "use single precision",
    "do some sanity check for the GPU algorithm",
    "limit top k",
    "get max bin",
    "calculate buffer size",
    "need to be able to hold smaller and larger best splits in SyncUpGlobalBestSplit",
    "left and right on same time, so need double size",
    "initialize histograms for global",
    "sync global data sumup info",
    "set global sumup info",
    "init global data count in leaf",
    "get local sumup",
    "get local sumup",
    "get mean number on machines",
    "weighted gain",
    "get top k",
    "Copy histogram to buffer, and Get local aggregate features",
    "copy histograms.",
    "copy smaller leaf histograms first",
    "mark local aggregated feature",
    "copy",
    "then copy larger leaf histograms",
    "mark local aggregated feature",
    "copy",
    "use local data to find local best splits",
    "clear histogram buffer before synchronizing",
    "otherwise histogram contents from the previous iteration will be sent",
    "find splits",
    "only has root leaf",
    "local voting",
    "gather",
    "get all top-k from all machines",
    "global voting",
    "copy local histgrams to buffer",
    "Reduce scatter for histogram",
    "find best split from local aggregated histograms",
    "restore from buffer",
    "restore from buffer",
    "find local best",
    "find local best split for larger leaf",
    "sync global best info",
    "copy back",
    "set the global number of data for leaves",
    "init the global sumup info",
    "instantiate template classes, otherwise linker cannot find the code",
    "allocate CUDA memory",
    "leave some space for alignment",
    "input best split info",
    "for leaf information update",
    "gather information for CPU, used for launching kernels",
    "for leaf splits information update",
    "we need restore the order of indices in cuda_data_indices_",
    "allocate more memory for sum reduction in CUDA",
    "only the first element records the final sum",
    "intialize split find task information (a split find task is one pass through the histogram of a feature)",
    "use the first gpu by default",
    "std::max(..., 1UL) to avoid error in the case when there are NaN's in the categorical values",
    "use feature interaction constraint or sample features by node"
  ],
  "v4.0.0": [
    "coding: utf-8",
    "create predictor first",
    "setting early stopping via global params should be possible",
    "reduce cost for prediction training data",
    "process callbacks",
    "construct booster",
    "start training",
    "check evaluation result.",
    "ranking task, split according to groups",
    "run preprocessing on the data set if needed",
    "setting early stopping via global params should be possible",
    "setup callbacks",
    "coding: utf-8",
    "dummy function to support older version of scikit-learn",
    "coding: utf-8",
    "f(labels, preds)",
    "f(labels, preds, weights)",
    "f(labels, preds, weights, group)",
    "f(labels, preds)",
    "f(labels, preds, weights)",
    "f(labels, preds, weights, group)",
    "documentation templates for LGBMModel methods are shared between the classes in",
    "this module and those in the ``dask`` module",
    "register default metric for consistency with callable eval_metric case",
    "try to deduce from class instance",
    "overwrite default metric by explicitly set metric",
    "use joblib conventions for negative n_jobs, just like scikit-learn",
    "at predict time, this is handled later due to the order of parameter updates",
    "Do not modify original args in fit function",
    "Refer to https://github.com/microsoft/LightGBM/pull/2619",
    "Separate built-in from callable evaluation metrics",
    "concatenate metric from params (or default if not provided in params) and eval_metric",
    "copy for consistency",
    "reduce cost for prediction training data",
    "free dataset",
    "retrive original params that possibly can be used in both training and prediction",
    "and then overwrite them (considering aliases) with params that were passed directly in prediction",
    "number of threads can have values with special meaning which is only applied",
    "in the scikit-learn interface, these should not reach the c++ side as-is",
    "adjust eval metrics to match whether binary or multiclass",
    "classification is being performed",
    "do not modify args, as it causes errors in model selection tools",
    "check group data",
    "coding: utf-8",
    "coding: utf-8",
    "\"#ddffdd\" is light green, \"#ffdddd\" is light red",
    "coding: utf-8",
    "coding: utf-8",
    "we don't need lib_lightgbm while building docs",
    "TypeError: obj is not a string or a number",
    "ValueError: invalid literal",
    "DeprecationWarning is not shown by default, so let's create our own with higher level",
    "lazy evaluation to allow import without dynamic library, e.g., for docs generation",
    "if buffer length is not long enough, re-allocate a buffer",
    "avoid side effects on passed-in parameters",
    "if main_param_name was provided, keep that value and remove all aliases",
    "if main param name was not found, search for an alias",
    "neither of main_param_name, aliases were found",
    "most common case (no nullable dtypes)",
    "1.0 <= pd version < 1.1 and nullable dtypes, least common case",
    "raises error because array is casted to type(pd.NA) and there's no na_value argument",
    "data has nullable dtypes, but we can specify na_value argument and copy will be made",
    "Get total row number.",
    "Random access by row index. Used for data sampling.",
    "Range data access. Used to read data in batch when constructing Dataset.",
    "Optionally specify batch_size to control range data read size.",
    "Only required if using ``Dataset.subset()``.",
    "__get_num_preds() cannot work with nrow > MAX_INT32, so calculate overall number of predictions piecemeal",
    "avoid memory consumption by arrays concatenation operations",
    "create numpy array from output arrays",
    "break up indptr based on number of rows (note more than one matrix in multiclass case)",
    "for CSC there is extra column added",
    "reformat output into a csr or csc matrix or list of csr or csc matrices",
    "same shape as input csr or csc matrix except extra column for expected value",
    "note: make sure we copy data as it will be deallocated next",
    "free the temporary native indptr, indices, and data",
    "__get_num_preds() cannot work with nrow > MAX_INT32, so calculate overall number of predictions piecemeal",
    "avoid memory consumption by arrays concatenation operations",
    "c type: double**",
    "each double* element points to start of each column of sample data.",
    "c type int**",
    "each int* points to start of indices for each column",
    "no min_data, nthreads and verbose in this function",
    "check data has header or not",
    "need to regroup init_score",
    "process for args",
    "get categorical features",
    "If the params[cat_alias] is equal to categorical_indices, do not report the warning.",
    "process for reference dataset",
    "start construct data",
    "set feature names",
    "Select sampled rows, transpose to column order.",
    "create validation dataset from ref_dataset",
    "create valid",
    "construct subset",
    "create train",
    "could be updated if data is not freed",
    "set to None",
    "we're done if self and reference share a common upstream reference",
    "most common case (no nullable dtypes)",
    "1.0 <= pd version < 1.1 and nullable dtypes, least common case",
    "raises error because array is casted to type(pd.NA) and there's no na_value argument",
    "data has nullable dtypes, but we can specify na_value argument and copy will be made",
    "if buffer length is not long enough, reallocate buffers",
    "group data from LightGBM is boundaries data, need to convert to group size",
    "Training task",
    "if \"machines\" is given, assume user wants to do distributed learning, and set up network",
    "construct booster object",
    "copy the parameters from train_set",
    "save reference to data",
    "buffer for inner predict",
    "Prediction task",
    "if buffer length is not long enough, re-allocate a buffer",
    "if a single node tree it won't have `leaf_index` so return 0",
    "Create the node record, and populate universal data members",
    "Update values to reflect node type (leaf or split)",
    "traverse the next level of the tree",
    "In tree format, \"subtree_list\" is a list of node records (dicts),",
    "and we add node to the list.",
    "need reset training data",
    "need to push new valid data",
    "if buffer length is not long enough, re-allocate a buffer",
    "if buffer length is not long enough, reallocate a buffer",
    "Copy models",
    "Get name of features",
    "if buffer length is not long enough, reallocate buffers",
    "avoid to predict many time in one iteration",
    "Get num of inner evals",
    "Get name of eval metrics",
    "if buffer length is not long enough, reallocate buffers",
    "coding: utf-8",
    "Callback environment used by callbacks",
    "split is needed for \"<dataset type> <metric>\" case (e.g. \"train l1\")",
    "self.best_score_list is initialized to an empty list",
    "split is needed for \"<dataset type> <metric>\" case (e.g. \"train l1\")",
    "coding: utf-8",
    "Acquire port in worker",
    "schedule futures to retrieve each element of the tuple",
    "retrieve ports",
    "Concatenate many parts into one",
    "construct local eval_set data.",
    "store indices of eval_set components that were not contained within local parts.",
    "consolidate parts of each individual eval component.",
    "require that eval_name exists in evaluated result data in case dropped due to padding.",
    "in distributed training the 'training' eval_set is not detected, will have name 'valid_<index>'.",
    "filter padding from eval parts then _concat each eval_set component.",
    "reconstruct eval_set fit args/kwargs depending on which components of eval_set are on worker.",
    "ensure that expected keys for evals_result_ and best_score_ exist regardless of padding.",
    "capture whether local_listen_port or its aliases were provided",
    "capture whether machines or its aliases were provided",
    "Some passed-in parameters can be removed:",
    "* 'num_machines': set automatically from Dask worker list",
    "* 'num_threads': overridden to match nthreads on each Dask process",
    "Split arrays/dataframes into parts. Arrange parts into dicts to enforce co-locality",
    "evals_set will to be re-constructed into smaller lists of (X, y) tuples, where",
    "X and y are each delayed sub-lists of original eval dask Collections.",
    "find maximum number of parts in an individual eval set so that we can",
    "pad eval sets when they come in different sizes.",
    "when individual eval set is equivalent to training data, skip recomputing parts.",
    "add None-padding for individual eval_set member if it is smaller than the largest member.",
    "first time a chunk of this eval set is added to this part.",
    "append additional chunks of this eval set to this part.",
    "ensure that all evaluation parts map uniquely to one part.",
    "assign sub-eval_set components to worker parts.",
    "Start computation in the background",
    "trigger error locally",
    "Find locations of all parts and map them to particular Dask workers",
    "Check that all workers were provided some of eval_set. Otherwise warn user that validation",
    "data artifacts may not be populated depending on worker returning final estimator.",
    "assign general validation set settings to fit kwargs.",
    "resolve aliases for network parameters and pop the result off params.",
    "these values are added back in calls to `_train_part()`",
    "figure out network params",
    "Tell each worker to train on the parts that it has locally",
    "",
    "This code treats ``_train_part()`` calls as not \"pure\" because:",
    "1. there is randomness in the training process unless parameters ``seed``",
    "and ``deterministic`` are set",
    "2. even with those parameters set, the output of one ``_train_part()`` call",
    "relies on global state (it and all the other LightGBM training processes",
    "coordinate with each other)",
    "if network parameters were changed during training, remove them from the",
    "returned model so that they're generated dynamically on every run based",
    "on the Dask cluster you're connected to and which workers have pieces of",
    "the training data",
    "dask.DataFrame.map_partitions() expects each call to return a pandas DataFrame or Series",
    "for multi-class classification with sparse matrices, pred_contrib predictions",
    "are returned as a list of sparse matrices (one per class)",
    "pred_contrib output will have one column per feature,",
    "plus one more for the base value",
    "need to tell Dask the expected type and shape of individual preds",
    "by default, dask.array.concatenate() concatenates sparse arrays into a COO matrix",
    "the code below is used instead to ensure that the sparse type is preserved during concatentation",
    "At this point, `out` is a list of lists of delayeds (each of which points to a matrix).",
    "Concatenate them to return a list of Dask Arrays.",
    "DaskLGBMClassifier does not support group, eval_group.",
    "DaskLGBMClassifier support for callbacks and init_model is not tested",
    "DaskLGBMRegressor does not support group, eval_class_weight, eval_group.",
    "DaskLGBMRegressor support for callbacks and init_model is not tested",
    "DaskLGBMRanker does not support eval_class_weight or early stopping",
    "DaskLGBMRanker support for callbacks and init_model is not tested",
    "coding: utf-8",
    "load or create your dataset",
    "create dataset for lightgbm",
    "if you want to re-use data, remember to set free_raw_data=False",
    "specify your configurations as a dict",
    "generate feature names",
    "feature_name and categorical_feature",
    "check feature name",
    "save model to file",
    "dump model to JSON (and save to file)",
    "feature names",
    "feature importances",
    "load model to predict",
    "can only predict with the best iteration (or the saving iteration)",
    "eval with loaded model",
    "dump model with pickle",
    "load model with pickle to predict",
    "can predict with any iteration when loaded in pickle way",
    "eval with loaded model",
    "continue training",
    "init_model accepts:",
    "1. model file name",
    "2. Booster()",
    "decay learning rates",
    "reset_parameter callback accepts:",
    "1. list with length = num_boost_round",
    "2. function(curr_iter)",
    "change other parameters during training",
    "self-defined objective function",
    "f(preds: array, train_data: Dataset) -> grad: array, hess: array",
    "log likelihood loss",
    "self-defined eval metric",
    "f(preds: array, train_data: Dataset) -> name: str, eval_result: float, is_higher_better: bool",
    "binary error",
    "NOTE: when you do customized loss function, the default prediction value is margin",
    "This may make built-in evaluation metric calculate wrong results",
    "For example, we are doing log likelihood loss, the prediction is score before logistic transformation",
    "Keep this in mind when you use the customization",
    "Pass custom objective function through params",
    "another self-defined eval metric",
    "f(preds: array, train_data: Dataset) -> name: str, eval_result: float, is_higher_better: bool",
    "accuracy",
    "NOTE: when you do customized loss function, the default prediction value is margin",
    "This may make built-in evaluation metric calculate wrong results",
    "For example, we are doing log likelihood loss, the prediction is score before logistic transformation",
    "Keep this in mind when you use the customization",
    "Pass custom objective function through params",
    "callback",
    "coding: utf-8",
    "load or create your dataset",
    "train",
    "predict",
    "eval",
    "feature importances",
    "self-defined eval metric",
    "f(y_true: array, y_pred: array) -> name: str, eval_result: float, is_higher_better: bool",
    "Root Mean Squared Logarithmic Error (RMSLE)",
    "train",
    "another self-defined eval metric",
    "f(y_true: array, y_pred: array) -> name: str, eval_result: float, is_higher_better: bool",
    "Relative Absolute Error (RAE)",
    "train",
    "predict",
    "eval",
    "other scikit-learn modules",
    "coding: utf-8",
    "load or create your dataset",
    "create dataset for lightgbm",
    "specify your configurations as a dict",
    "train",
    "coding: utf-8",
    "################",
    "Simulate some binary data with a single categorical and",
    "single continuous predictor",
    "################",
    "Set up a couple of utilities for our experiments",
    "################",
    "Observe the behavior of `binary` and `xentropy` objectives",
    "Trying this throws an error on non-binary values of y:",
    "experiment('binary', label_type='probability', DATA)",
    "The speed of `binary` is not drastically different than",
    "`xentropy`. `xentropy` runs faster than `binary` in many cases, although",
    "there are reasons to suspect that `binary` should run faster when the",
    "label is an integer instead of a float",
    "coding: utf-8",
    "load or create your dataset",
    "create dataset for lightgbm",
    "specify your configurations as a dict",
    "train",
    "save model to file",
    "predict",
    "eval",
    "We can also open HDF5 file once and get access to",
    "With binary dataset created, we can use either Python API or cmdline version to train.",
    "",
    "Note: in order to create exactly the same dataset with the one created in simple_example.py, we need",
    "to modify simple_example.py to pass numpy array instead of pandas DataFrame to Dataset constructor.",
    "The reason is that DataFrame column names will be used in Dataset. For a DataFrame with Int64Index",
    "as columns, Dataset will use column names like [\"0\", \"1\", \"2\", ...]. While for numpy array, column names",
    "are using the default one assigned in C++ code (dataset_loader.cpp), like [\"Column_0\", \"Column_1\", ...].",
    "Y has a single column and we read it in single shot. So store it as an 1-d array.",
    "We use random access for data sampling when creating LightGBM Dataset from Sequence.",
    "When accessing any element in a HDF5 chunk, it's read entirely.",
    "To save I/O for sampling, we should keep number of total chunks much larger than sample count.",
    "Here we are just creating a chunk size that matches with batch_size.",
    "",
    "Also note that the data is stored in row major order to avoid extra copy when passing to",
    "lightgbm Dataset.",
    "Save to 2 HDF5 files for demonstration.",
    "We can store multiple datasets inside a single HDF5 file.",
    "Separating X and Y for choosing best chunk size for data loading.",
    "split training data into two partitions",
    "make this array dense because we're splitting across",
    "a sparse boundary to partition the data",
    "the code below uses sklearn.metrics, but this requires pulling all of the",
    "predictions and target values back from workers to the client",
    "",
    "for larger datasets, consider the metrics from dask-ml instead",
    "https://ml.dask.org/modules/api.html#dask-ml-metrics-metrics",
    "coding: utf-8",
    "!/usr/bin/env python3",
    "-*- coding: utf-8 -*-",
    "",
    "LightGBM documentation build configuration file, created by",
    "sphinx-quickstart on Thu May  4 14:30:58 2017.",
    "",
    "This file is execfile()d with the current directory set to its",
    "containing dir.",
    "",
    "Note that not all possible configuration values are present in this",
    "autogenerated file.",
    "",
    "All configuration values have a default; values that are commented out",
    "serve to show the default.",
    "If extensions (or modules to document with autodoc) are in another directory,",
    "add these directories to sys.path here. If the directory is relative to the",
    "documentation root, use os.path.abspath to make it absolute.",
    "-- General configuration ------------------------------------------------",
    "If your documentation needs a minimal Sphinx version, state it here.",
    "Add any Sphinx extension module names here, as strings. They can be",
    "extensions coming with Sphinx (named 'sphinx.ext.*') or your custom",
    "ones.",
    "mock out modules",
    "hide type hints in API docs",
    "Generate autosummary pages. Output should be set with: `:toctree: pythonapi/`",
    "Only the class' docstring is inserted.",
    "If true, `todo` and `todoList` produce output, else they produce nothing.",
    "The master toctree document.",
    "General information about the project.",
    "The name of an image file (relative to this directory) to place at the top",
    "of the sidebar.",
    "The name of an image file (relative to this directory) to use as a favicon of",
    "the docs.  This file should be a Windows icon file (.ico) being 16x16 or 32x32",
    "pixels large.",
    "The version info for the project you're documenting, acts as replacement for",
    "|version| and |release|, also used in various other places throughout the",
    "built documents.",
    "The short X.Y version.",
    "The full version, including alpha/beta/rc tags.",
    "The language for content autogenerated by Sphinx. Refer to documentation",
    "for a list of supported languages.",
    "",
    "This is also used if you do content translation via gettext catalogs.",
    "Usually you set \"language\" from the command line for these cases.",
    "List of patterns, relative to source directory, that match files and",
    "directories to ignore when looking for source files.",
    "This patterns also effect to html_static_path and html_extra_path",
    "The name of the Pygments (syntax highlighting) style to use.",
    "-- Configuration for C API docs generation ------------------------------",
    "-- Options for HTML output ----------------------------------------------",
    "The theme to use for HTML and HTML Help pages.  See the documentation for",
    "a list of builtin themes.",
    "Theme options are theme-specific and customize the look and feel of a theme",
    "further.  For a list of options available for each theme, see the",
    "documentation.",
    "Add any paths that contain custom static files (such as style sheets) here,",
    "relative to this directory. They are copied after the builtin static files,",
    "so a file named \"default.css\" will overwrite the builtin \"default.css\".",
    "-- Options for HTMLHelp output ------------------------------------------",
    "Output file base name for HTML help builder.",
    "-- Options for LaTeX output ---------------------------------------------",
    "The name of an image file (relative to this directory) to place at the top of",
    "the title page.",
    "intersphinx configuration",
    "Warning! The following code can cause buffer overflows on RTD.",
    "Consider suppressing output completely if RTD project silently fails.",
    "Refer to https://github.com/svenevs/exhale",
    "/blob/fe7644829057af622e467bb529db6c03a830da99/exhale/deploy.py#L99-L111",
    "Warning! The following code can cause buffer overflows on RTD.",
    "Consider suppressing output completely if RTD project silently fails.",
    "Refer to https://github.com/svenevs/exhale",
    "/blob/fe7644829057af622e467bb529db6c03a830da99/exhale/deploy.py#L99-L111",
    "coding: utf-8",
    "This is a basic test for floating number parsing.",
    "Most of the test cases come from:",
    "https://github.com/dmlc/xgboost/blob/master/tests/cpp/common/test_charconv.cc",
    "https://github.com/Alexhuszagh/rust-lexical/blob/master/data/test-parse-unittests/strtod_tests.toml",
    "FLT_MAX",
    "FLT_MIN",
    "DBL_MAX (1 + (1 - 2^-52)) * 2^1023 = (2^53 - 1) * 2^971",
    "2^971 * (2^53 - 1 + 1/2) : the smallest number resolving to inf",
    "Near DBL_MIN",
    "DBL_MIN 2^-1022",
    "The behavior for parsing -nan depends on implementation.",
    "Thus we skip binary check for negative nan.",
    "See comment in test_cases.",
    "construct sample data first (use all data for convenience and since size is small)",
    "Load some test data",
    "Use the smaller \".test\" data because we don't care about the actual data and it's smaller",
    "Add some fake initial_scores and groups so we can test streaming them",
    "Now use the reference dataset schema to make some testable Datasets with N rows each",
    "Load some test data",
    "Use the smaller \".test\" data because we don't care about the actual data and it's smaller",
    "Add some fake initial_scores and groups so we can test streaming them",
    "Now use the reference dataset schema to make some testable Datasets with N rows each",
    "Load some test data",
    "Serialize the reference",
    "Deserialize the reference",
    "Confirm 1 successful API call",
    "Free memory",
    "Test that Data() points to first value written",
    "Constants",
    "Start with some content:",
    "Clear & re-use:",
    "Output should match new content:",
    "Number of trials for each new ChunkedArray configuration. Pass 100 times over the search space:",
    "Each outer loop iteration changes the test by adding +1 chunk. We start with 1 chunk only:",
    "Sweep valid and invalid addresses with a ChunkedArray with `chunks` chunks:",
    "Compute a new trial address & value & if it is a valid address:",
    "Insert item. If at a valid address, 0 is returned, otherwise, -1 is returned:",
    "If at valid address, check that the stored value is correct & remember it for the future:",
    "Check the just-stored value with getitem():",
    "Also store the just-stored value for future tracking:",
    "Final check: ensure even with overrides, all valid insertions store the latest value at that address:",
    "Test in 2 ways that the values are correctly laid out in memory:",
    "Since init_scores are in a column format, but need to be pushed as rows, we have to extract each batch",
    "Use multiple threads to test concurrency",
    "Since init_scores are in a column format, but need to be pushed as rows, we have to extract each batch",
    "Calculate expected boundaries",
    "Extract a set of rows from the column-based format (still maintaining column based format)",
    "coding: utf-8",
    "coding: utf-8",
    "check saved model persistence",
    "we need to check the consistency of model file here, so test for exact equal",
    "check early stopping is working. Make it stop very early, so the scores should be very close to zero",
    "scores likely to be different, but prediction should still be the same",
    "test that shape is checked during prediction",
    "The simple implementation is just a single \"return self.ndarray[idx]\"",
    "The following is for demo and testing purpose.",
    "whole col",
    "half col",
    "Create dataset from numpy array directly.",
    "Create dataset using Sequence.",
    "Test for validation set.",
    "Select some random rows as valid data.",
    "From Dataset constructor, with dataset from numpy array.",
    "From Dataset.create_valid, with dataset from sequence.",
    "test that method works even with free_raw_data=True",
    "test that method works but sets raw data to None in case of immergeable data types",
    "test that method works for different data types",
    "Set extremely harsh penalties, so CEGB will block most splits.",
    "Compare pairs of penalties, to ensure scaling works as intended",
    "Reset booster1's parameters to p2, so the parameter section of the file matches.",
    "should resolve duplicate aliases, and prefer the main parameter",
    "should choose the highest priority alias and set that value on main param",
    "if only aliases are used",
    "should use the default if main param and aliases are missing",
    "all changes should be made on copies and not modify the original",
    "preserves None found for main param and still removes aliases",
    "correctly chooses value when only an alias is provided",
    "adds None if that's given as the default and param not found",
    "If callable is found in objective",
    "Value in params should be preferred to the default_value passed from keyword arguments",
    "None of objective or its aliases in params, but default_value is callable.",
    "check that the original data wasn't modified",
    "check that the built data has the codes",
    "test using defined feature names",
    "test using default feature names",
    "check for feature indices outside of range",
    "coding: utf-8",
    "add target, weight, and group to DataFrame so that partitions abide by group boundaries.",
    "set_index ensures partitions are based on group id.",
    "See https://stackoverflow.com/questions/49532824/dask-dataframe-split-partitions-based-on-a-column-or-function.",
    "separate target, weight from features.",
    "encode group identifiers into run-length encoding, the format LightGBMRanker is expecting",
    "so that within each partition, sum(g) = n_samples.",
    "ranking arrays: one chunk per group. Each chunk must include all columns.",
    "make one categorical feature relevant to the target",
    "https://github.com/microsoft/LightGBM/issues/4118",
    "extra predict() parameters should be passed through correctly",
    "pref_leaf values should have the right shape",
    "and values that look like valid tree nodes",
    "be sure LightGBM actually used at least one categorical column,",
    "and that it was correctly treated as a categorical feature",
    "shape depends on whether it is binary or multiclass classification",
    "in the special case of multi-class classification using scipy sparse matrices,",
    "the output of `.predict(..., pred_contrib=True)` is a list of sparse matrices (one per class)",
    "",
    "since that case is so different than all other cases, check the relevant things here",
    "and then return early",
    "raw scores will probably be different, but at least check that all predicted classes are the same",
    "be sure LightGBM actually used at least one categorical column,",
    "and that it was correctly treated as a categorical feature",
    "* shape depends on whether it is binary or multiclass classification",
    "* matrix for binary classification is of the form [feature_contrib, base_value],",
    "for multi-class it's [feat_contrib_class1, base_value_class1, feat_contrib_class2, base_value_class2, etc.]",
    "* contrib outputs for distributed training are different than from local training, so we can just test",
    "that the output has the right shape and base values are in the right position",
    "with a custom objective, prediction result is a raw score instead of predicted class",
    "function should have been preserved",
    "should correctly classify every sample",
    "probability estimates should be similar",
    "Scores should be the same",
    "Predictions should be roughly the same.",
    "pref_leaf values should have the right shape",
    "and values that look like valid tree nodes",
    "extra predict() parameters should be passed through correctly",
    "be sure LightGBM actually used at least one categorical column,",
    "and that it was correctly treated as a categorical feature",
    "contrib outputs for distributed training are different than from local training, so we can just test",
    "that the output has the right shape and base values are in the right position",
    "be sure LightGBM actually used at least one categorical column,",
    "and that it was correctly treated as a categorical feature",
    "Quantiles should be right",
    "be sure LightGBM actually used at least one categorical column,",
    "and that it was correctly treated as a categorical feature",
    "function should have been preserved",
    "Scores should be the same",
    "local and Dask predictions should be the same",
    "predictions should be better than random",
    "rebalance small dask.Array dataset for better performance.",
    "use many trees + leaves to overfit, help ensure that Dask data-parallel strategy matches that of",
    "serial learner. See https://github.com/microsoft/LightGBM/issues/3292#issuecomment-671288210.",
    "distributed ranker should be able to rank decently well and should",
    "have high rank correlation with scores from serial ranker.",
    "extra predict() parameters should be passed through correctly",
    "pref_leaf values should have the right shape",
    "and values that look like valid tree nodes",
    "be sure LightGBM actually used at least one categorical column,",
    "and that it was correctly treated as a categorical feature",
    "rebalance small dask.Array dataset for better performance.",
    "distributed ranker should be able to rank decently well with the least-squares objective",
    "and should have high rank correlation with scores from serial ranker.",
    "function should have been preserved",
    "Use larger trainset to prevent premature stopping due to zero loss, causing num_trees() < n_estimators.",
    "Use small chunk_size to avoid single-worker allocation of eval data partitions.",
    "test eval_class_weight, eval_init_score on binary-classification task.",
    "Note: objective's default `metric` will be evaluated in evals_result_ in addition to all eval_metrics.",
    "create eval_sets by creating new datasets or copying training data.",
    "total number of trees scales up for ova classifier.",
    "check that early stopping was not applied.",
    "checks that evals_result_ and best_score_ contain expected data and eval_set names.",
    "check that each eval_name and metric exists for all eval sets, allowing for the",
    "case when a worker receives a fully-padded eval_set component which is not evaluated.",
    "should be able to use the class without specifying a client",
    "should be able to set client after construction",
    "data on cluster1",
    "create identical data on cluster2",
    "at this point, the result of default_client() is client2 since it was the most recently",
    "created. So setting client to client1 here to test that you can select a non-default client",
    "unfitted model should survive pickling round trip, and pickling",
    "shouldn't have side effects on the model object",
    "client will always be None after unpickling",
    "fitted model should survive pickling round trip, and pickling",
    "shouldn't have side effects on the model object",
    "client will always be None after unpickling",
    "rebalance data to be sure that each worker has a piece of the data",
    "model 1 - no network parameters given",
    "model 2 - machines given",
    "model 3 - local_listen_port given",
    "training should fail because LightGBM will try to use the same",
    "port for multiple worker processes on the same machine",
    "rebalance data to be sure that each worker has a piece of the data",
    "test that \"machines\" is actually respected by creating a socket that uses",
    "one of the ports mentioned in \"machines\"",
    "The above error leaves a worker waiting",
    "an informative error should be raised if \"machines\" has duplicates",
    "\"client\" should be the only different, and the final argument",
    "value of the root node is 0 when init_score is set",
    "this test is separate because it takes a not-yet-constructed estimator",
    "coding: utf-8",
    "coding: utf-8",
    "build target, group ID vectors.",
    "build y/target and group-id vectors with user-specified group sizes.",
    "build y/target and group-id vectors according to n_samples, avg_gs, and random_gs.",
    "groups should contain > 1 element for pairwise learning objective.",
    "build feature data, X. Transform first few into informative features.",
    "coding: utf-8",
    "check that really dummy objective was used and estimator didn't learn anything",
    "prediction result is actually not transformed (is raw) due to custom objective",
    "original estimator is unaffected",
    "new estimator is unfitted, but has the same parameters",
    "Test if random_state is properly stored",
    "Test if two random states produce identical models",
    "Test if subsequent fits sample from random_state object and produce different models",
    "Test that the largest element is NOT the same, the smallest can be the same, i.e. zero",
    "With default params",
    "Tests same probabilities",
    "Tests same predictions",
    "Tests same raw scores",
    "Tests same leaf indices",
    "Tests same feature contributions",
    "Tests other parameters for the prediction works",
    "Tests start_iteration",
    "Tests same probabilities, starting from iteration 10",
    "Tests same predictions, starting from iteration 10",
    "Tests same raw scores, starting from iteration 10",
    "Tests same leaf indices, starting from iteration 10",
    "Tests same feature contributions, starting from iteration 10",
    "Tests other parameters for the prediction works, starting from iteration 10",
    "test that params passed in predict have higher priority",
    "no custom objective, no custom metric",
    "default metric",
    "non-default metric",
    "no metric",
    "non-default metric in eval_metric",
    "non-default metric with non-default metric in eval_metric",
    "non-default metric with multiple metrics in eval_metric",
    "non-default metric with multiple metrics in eval_metric for LGBMClassifier",
    "default metric for non-default objective",
    "non-default metric for non-default objective",
    "no metric",
    "non-default metric in eval_metric for non-default objective",
    "non-default metric with non-default metric in eval_metric for non-default objective",
    "non-default metric with multiple metrics in eval_metric for non-default objective",
    "custom objective, no custom metric",
    "default regression metric for custom objective",
    "non-default regression metric for custom objective",
    "multiple regression metrics for custom objective",
    "no metric",
    "default regression metric with non-default metric in eval_metric for custom objective",
    "non-default regression metric with metric in eval_metric for custom objective",
    "multiple regression metrics with metric in eval_metric for custom objective",
    "multiple regression metrics with multiple metrics in eval_metric for custom objective",
    "no custom objective, custom metric",
    "default metric with custom metric",
    "non-default metric with custom metric",
    "multiple metrics with custom metric",
    "custom metric (disable default metric)",
    "default metric for non-default objective with custom metric",
    "non-default metric for non-default objective with custom metric",
    "multiple metrics for non-default objective with custom metric",
    "custom metric (disable default metric for non-default objective)",
    "custom objective, custom metric",
    "custom metric for custom objective",
    "non-default regression metric with custom metric for custom objective",
    "multiple regression metrics with custom metric for custom objective",
    "default metric and invalid binary metric is replaced with multiclass alternative",
    "invalid binary metric is replaced with multiclass alternative",
    "default metric for non-default multiclass objective",
    "and invalid binary metric is replaced with multiclass alternative",
    "default metric and invalid multiclass metric is replaced with binary alternative",
    "invalid multiclass metric is replaced with binary alternative for custom objective",
    "Verify that can receive a list of metrics, only callable",
    "Verify that can receive a list of custom and built-in metrics",
    "Verify that works as expected when eval_metric is empty",
    "Verify that can receive a list of metrics, only built-in",
    "Verify that eval_metric is robust to receiving a list with None",
    "feval",
    "single eval_set",
    "two eval_set",
    "'val_minus_two' here is the expected number of threads for n_jobs=-2",
    "Note: according to joblib's formula, a value of n_jobs=-2 means",
    "\"use all but one thread\" (formula: n_cpus + 1 + n_jobs)",
    "try to predict with a different feature",
    "check that disabling the check doesn't raise the error",
    "make weights and init_score same types as y, just to avoid",
    "a huge number of combinations and therefore test cases",
    "make weights and init_score same types as y, just to avoid",
    "a huge number of combinations and therefore test cases",
    "coding: utf-8",
    "we're in a leaf now",
    "check that the rest of the elements have black color",
    "check that we got to the expected leaf",
    "coding: utf-8",
    "coding: utf-8",
    "check that default gives same result as k = 1",
    "check against independent calculation for k = 1",
    "check against independent calculation for k = 2",
    "check against independent calculation for k = 10",
    "check cases where predictions are equal",
    "should give same result as binary auc for 2 classes",
    "test the case where all predictions are equal",
    "test that weighted data gives different auc_mu",
    "test that equal data weights give same auc_mu as unweighted data",
    "should give 1 when accuracy = 1",
    "test loading class weights",
    "no early stopping",
    "early stopping occurs",
    "regular early stopping",
    "positive min_delta",
    "test custom eval metrics",
    "shuffle = False, override metric in params",
    "shuffle = True, callbacks",
    "enable display training loss",
    "self defined folds",
    "LambdaRank",
    "... with l2 metric",
    "... with NDCG (default) metric",
    "self defined folds with lambdarank",
    "init_model from an in-memory Booster",
    "init_model from a text file",
    "predictions should be identical",
    "with early stopping",
    "predict by each fold booster",
    "check that each booster predicted using the best iteration",
    "fold averaging",
    "without early stopping",
    "test feature_names with whitespaces",
    "This has non-ascii strings.",
    "check that passing parameters to the constructor raises warning and ignores them",
    "take subsets and train",
    "generate CSR sparse dataset",
    "convert data to dense and get back same contribs",
    "validate the values are the same",
    "validate using CSC matrix",
    "validate the values are the same",
    "generate CSR sparse dataset",
    "convert data to dense and get back same contribs",
    "validate the values are the same",
    "validate using CSC matrix",
    "validate the values are the same",
    "Note there is an extra column added to the output for the expected value",
    "Note output CSC shape should be same as CSR output shape",
    "test sliced labels",
    "append some columns",
    "append some rows",
    "test sliced 2d matrix",
    "test sliced CSR",
    "trees start at position 1.",
    "split_features are in 4th line.",
    "test if a penalty as high as the depth indeed prohibits all monotone splits",
    "The penalization is so high that the first 2 features should not be used here",
    "Check that a very high penalization is the same as not using the features at all",
    "check refit accepts dataset_params",
    "the following checks that dart and rf with mape can predict outside the 0-1 range",
    "https://github.com/microsoft/LightGBM/issues/1579",
    "no custom objective, no feval",
    "default metric",
    "non-default metric in params",
    "default metric in args",
    "non-default metric in args",
    "metric in args overwrites one in params",
    "metric in args overwrites one in params",
    "multiple metrics in params",
    "multiple metrics in args",
    "remove default metric by 'None' in list",
    "remove default metric by 'None' aliases",
    "custom objective, no feval",
    "no default metric",
    "metric in params",
    "metric in args",
    "metric in args overwrites its' alias in params",
    "multiple metrics in params",
    "multiple metrics in args",
    "no custom objective, feval",
    "default metric with custom one",
    "non-default metric in params with custom one",
    "default metric in args with custom one",
    "non-default metric in args with custom one",
    "metric in args overwrites one in params, custom one is evaluated too",
    "multiple metrics in params with custom one",
    "multiple metrics in args with custom one",
    "custom metric is evaluated despite 'None' is passed",
    "custom objective, feval",
    "no default metric, only custom one",
    "metric in params with custom one",
    "metric in args with custom one",
    "metric in args overwrites one in params, custom one is evaluated too",
    "multiple metrics in params with custom one",
    "multiple metrics in args with custom one",
    "custom metric is evaluated despite 'None' is passed",
    "no custom objective, no feval",
    "default metric",
    "default metric in params",
    "non-default metric in params",
    "multiple metrics in params",
    "remove default metric by 'None' aliases",
    "custom objective, no feval",
    "no default metric",
    "metric in params",
    "multiple metrics in params",
    "no custom objective, feval",
    "default metric with custom one",
    "default metric in params with custom one",
    "non-default metric in params with custom one",
    "multiple metrics in params with custom one",
    "custom metric is evaluated despite 'None' is passed",
    "custom objective, feval",
    "no default metric, only custom one",
    "metric in params with custom one",
    "multiple metrics in params with custom one",
    "custom metric is evaluated despite 'None' is passed",
    "Custom objective replaces multiclass",
    "multiclass default metric",
    "multiclass default metric with custom one",
    "multiclass metric alias with custom one for custom objective",
    "no metric for invalid class_num",
    "custom metric for invalid class_num",
    "multiclass metric alias with custom one with invalid class_num",
    "multiclass default metric without num_class",
    "multiclass metric alias",
    "multiclass metric",
    "non-valid metric for multiclass objective",
    "non-default num_class for default objective",
    "no metric with non-default num_class for custom objective",
    "multiclass metric alias for custom objective",
    "multiclass metric for custom objective",
    "binary metric with non-default num_class for custom objective",
    "Expect three metrics but mean and stdv for each metric",
    "test XGBoost-style return value",
    "test numpy-style return value",
    "test bins string type",
    "test histogram is disabled for categorical features",
    "test for lgb.train",
    "test feval for lgb.train",
    "test with two valid data for lgb.train",
    "test for lgb.cv",
    "test feval for lgb.cv",
    "test that binning works properly for features with only positive or only negative values",
    "decreasing without freeing raw data is allowed",
    "decreasing before lazy init is allowed",
    "increasing is allowed",
    "decreasing with disabled filter is allowed",
    "decreasing with enabled filter is disallowed;",
    "also changes of other params are disallowed",
    "check extra trees increases regularization",
    "check path smoothing increases regularization",
    "test edge case with one leaf",
    "check that constraint containing all features is equivalent to no constraint",
    "check that constraint partitioning the features reduces train accuracy",
    "check that constraints consisting of single features reduce accuracy further",
    "test that interaction constraints work when not all features are used",
    "check that number of threads does not affect result",
    "check that setting linear_tree=True fits better than ordinary trees when data has linear relationship",
    "test again with nans in data",
    "test again with bagging",
    "test with a feature that has only one non-nan value",
    "test with a categorical feature",
    "test refit: same results on same data",
    "test refit with save and load",
    "test refit: different results training on different data",
    "test when num_leaves - 1 < num_features and when num_leaves - 1 > num_features",
    "test that the predict once with all iterations equals summed results with start_iteration and num_iteration",
    "test the case where start_iteration <= 0, and num_iteration is None",
    "test the case where start_iteration > 0, and num_iteration <= 0",
    "test the case where start_iteration > 0, and num_iteration <= 0, with pred_leaf=True",
    "test the case where start_iteration > 0, and num_iteration <= 0, with pred_contrib=True",
    "test for regression",
    "test both with and without early stopping",
    "test for multi-class",
    "test both with and without early stopping",
    "test for binary",
    "test both with and without early stopping",
    "test against sklearn average precision metric",
    "test that average precision is 1 where model predicts perfectly",
    "data as float64",
    "test all features were used",
    "test the score is better than predicting the mean",
    "test all predictions are equal using different input dtypes",
    "introduce some missing values",
    "the previous line turns x3 into object dtype in recent versions of pandas",
    "train with regular dtypes",
    "convert to nullable dtypes",
    "test training succeeds",
    "test all features were used",
    "test the score is better than predicting the mean",
    "test equal predictions",
    "test data are taken from bug report",
    "https://github.com/microsoft/LightGBM/issues/4708",
    "modified from https://github.com/microsoft/LightGBM/issues/3679#issuecomment-938652811",
    "and https://github.com/microsoft/LightGBM/pull/5087",
    "test that the ``splits_per_leaf_`` of CEGB is cleaned before training a new tree",
    "which is done in the fix #5164",
    "without the fix:",
    "Check failed: (best_split_info.left_count) > (0)",
    "try to predict with a different feature",
    "check that disabling the check doesn't raise the error",
    "try to refit with a different feature",
    "check that disabling the check doesn't raise the error",
    "coding: utf-8",
    "If compiled appropriately, the same installation will support both GPU and CPU.",
    "Double-precision floats are only supported on x86_64 with PoCL",
    "coding: utf-8",
    "coding: utf-8",
    "These are helper functions to allow doing a stack unwind",
    "after an R allocation error, which would trigger a long jump.",
    "convert from one-based to zero-based index",
    "if any feature names were larger than allocated size,",
    "allow for a larger size and try again",
    "convert from boundaries to size",
    "--- start Booster interfaces",
    "if any eval names were larger than allocated size,",
    "allow for a larger size and try again",
    "if the model string was larger than the initial buffer, call the function again, writing directly to the R object",
    "if the model string was larger than the initial buffer, allocate a bigger buffer and try again",
    "if aliases string was larger than the initial buffer, allocate a bigger buffer and try again",
    "if aliases string was larger than the initial buffer, allocate a bigger buffer and try again",
    ".Call() calls",
    "coding: utf-8",
    "alias table",
    "names",
    "from strings",
    "tails",
    "tails",
    "the following are stored as comma separated strings but are arrays in the wrappers",
    "coding: utf-8",
    "Single row predictor to abstract away caching logic",
    "create boosting",
    "initialize the boosting",
    "create objective function",
    "initialize the objective function",
    "create training metric",
    "reset the boosting",
    "create objective function",
    "initialize the objective function",
    "calculate the nonzero data and indices size",
    "allocate data and indices arrays",
    "Get the number of trees per iteration (for multiclass scenario we output multiple sparse matrices)",
    "aggregated per row feature contribution results",
    "keep track of the row_vector sizes for parallelization",
    "copy vector results to output for each row",
    "Get the number of trees per iteration (for multiclass scenario we output multiple sparse matrices)",
    "aggregated per row feature contribution results",
    "calculate number of elements per column to construct",
    "the CSC matrix with random access",
    "keep track of column counts",
    "keep track of beginning index for each column",
    "keep track of beginning index for each matrix",
    "Note: we parallelize across matrices instead of rows because of the column_counts[m][col_idx] increment inside the loop",
    "store the row index",
    "update column count",
    "explicitly declare symbols from LightGBM namespace",
    "some help functions used to convert data",
    "Row iterator of on column for CSC matrix",
    "return value at idx, only can access by ascent order",
    "return next non-zero pair, if index < 0, means no more data",
    "start of c_api functions",
    "This API is to keep python binding's behavior the same with C++ implementation.",
    "Sample count, random seed etc. should be provided in parameters.",
    "convert internal thread id to be unique based on external thread id",
    "convert internal thread id to be unique based on external thread id",
    "sample data first",
    "sample data first",
    "sample data first",
    "local buffer to re-use memory",
    "sample data first",
    "no more data",
    "---- start of booster",
    "Single row in row-major format:",
    "---- start of some help functions",
    "data is array of pointers to individual rows",
    "set number of threads for openmp",
    "read parameters from config file",
    "remove str after \"#\"",
    "de-duplicate params",
    "prediction is needed if using input initial model(continued train)",
    "need to continue training",
    "sync up random seed for data partition",
    "load Training data",
    "load data for distributed training",
    "load data for single machine",
    "need save binary file",
    "create training metric",
    "only when have metrics then need to construct validation data",
    "Add validation data, if it exists",
    "add",
    "need save binary file",
    "add metric for validation data",
    "output used time on each iteration",
    "need init network",
    "create boosting",
    "create objective function",
    "load training data",
    "initialize the objective function",
    "initialize the boosting",
    "add validation data into boosting",
    "convert model to if-else statement code",
    "create predictor",
    "Free memory",
    "create predictor",
    "label_gain = 2^i - 1, may overflow, so we use 31 here",
    "counts for all labels",
    "start from top label, and accumulate DCG",
    "counts for all labels",
    "calculate k Max DCG by one pass",
    "get sorted indices by score",
    "calculate multi dcg by one pass",
    "wait for all client start up",
    "Don't call MPI_Finalize() here: If the destructor was called because only this node had an exception, calling MPI_Finalize() will cause all nodes to hang.",
    "Instead we will handle finalize/abort for MPI in main().",
    "default set to -1",
    "distance at k-th communication, distance[k] = 2^k",
    "set incoming rank at k-th commuication",
    "set outgoing rank at k-th commuication",
    "default set as -1",
    "construct all recursive halving map for all machines",
    "let 1 << k <= num_machines",
    "distance of each communication",
    "if num_machines = 2^k, don't need to group machines",
    "communication direction, %2 == 0 is positive",
    "neighbor at k-th communication",
    "receive data block at k-th communication",
    "send data block at k-th communication",
    "if num_machines != 2^k, need to group machines",
    "group, two machine in one group, total \"rest\" groups will have 2 machines.",
    "let left machine as group leader",
    "cache block information for groups, group with 2 machines will have double block size",
    "convert from group to node leader",
    "convert from node to group",
    "meet new group",
    "add block len for this group",
    "calculate the group block start",
    "not need to construct",
    "get receive block information",
    "accumulate block len",
    "get send block information",
    "accumulate block len",
    "static member definition",
    "if small package or small count , do it by all gather.(reduce the communication times.)",
    "assign the blocks to every rank.",
    "do reduce scatter",
    "do all gather",
    "assign blocks",
    "need use buffer here, since size of \"output\" is smaller than size after all gather",
    "copy back",
    "assign blocks",
    "start all gather",
    "when num_machines is small and data is large",
    "use output as receive buffer",
    "get current local block size",
    "get out rank",
    "get in rank",
    "get send information",
    "get recv information",
    "send and recv at same time",
    "rotate in-place",
    "use output as receive buffer",
    "get current local block size",
    "get send information",
    "get recv information",
    "send and recv at same time",
    "use output as receive buffer",
    "send and recv at same time",
    "send local data to neighbor first",
    "receive neighbor data first",
    "reduce",
    "get target",
    "get send information",
    "get recv information",
    "send and recv at same time",
    "reduce",
    "send result to neighbor",
    "receive result from neighbor",
    "copy result",
    "start up socket",
    "parse clients from file",
    "get ip list of local machine",
    "get local rank",
    "construct listener",
    "construct communication topo",
    "construct linkers",
    "free listener",
    "set timeout",
    "accept incoming socket",
    "receive rank",
    "add new socket",
    "save ranks that need to connect with",
    "start listener",
    "start connect",
    "let smaller rank connect to larger rank",
    "send local rank",
    "wait for listener",
    "print connected linkers",
    "only need to copy subset",
    "avoid to copy subset many times",
    "avoid out of range",
    "may need to recopy subset",
    "valid the type",
    "parser factory implementation.",
    "customized parser add-on.",
    "save header to parser config in case needed.",
    "save label id to parser config in case needed.",
    "Constructors",
    "Get type tag",
    "Comparisons",
    "This has to be separate, not in Statics, because Json() accesses",
    "statics().null.",
    "advance until next line, or end of input",
    "advance until closing tokens",
    "The usual case: non-escaped characters",
    "Handle escapes",
    "Extract 4-byte escape sequence",
    "Explicitly check length of the substring. The following loop",
    "relies on std::string returning the terminating NUL when",
    "accessing str[length]. Checking here reduces brittleness.",
    "JSON specifies that characters outside the BMP shall be encoded as a",
    "pair of 4-hex-digit \\u escapes encoding their surrogate pair",
    "components. Check whether we're in the middle of such a beast: the",
    "previous codepoint was an escaped lead (high) surrogate, and this is",
    "a trail (low) surrogate.",
    "Reassemble the two surrogate pairs into one astral-plane character,",
    "per the UTF-16 algorithm.",
    "Integer part",
    "Decimal part",
    "Exponent part",
    "Check for any trailing garbage",
    "Documented in json11.hpp",
    "Check for another object",
    "get column names",
    "support to get header from parser config, so could utilize following label name to id mapping logic.",
    "load label idx first",
    "if parser config file exists, feature names may be changed after customized parser applied.",
    "clear here so could use default filled feature names during dataset construction.",
    "may improve by saving real feature names defined in parser in the future.",
    "erase label column name",
    "load ignore columns",
    "load weight idx",
    "load group idx",
    "don't support query id in data file when using distributed training",
    "read data to memory",
    "sample data",
    "construct feature bin mappers & clear sample data",
    "initialize label",
    "extract features",
    "sample data from file",
    "construct feature bin mappers & clear sample data",
    "initialize label",
    "extract features",
    "load data from binary file",
    "checks whether there's a initial score file when loaded from binary data files",
    "the intial score file should with suffix \".bin.init\"",
    "check meta data",
    "need to check training data",
    "read data in memory",
    "initialize label",
    "extract features",
    "Get number of lines of data file",
    "initialize label",
    "extract features",
    "load data from binary file",
    "checks whether there's a initial score file when loaded from binary data files",
    "the intial score file should with suffix \".bin.init\"",
    "not need to check validation data",
    "check meta data",
    "check token",
    "read feature group definitions",
    "read feature size",
    "buffer to read binary file",
    "check token",
    "read size of header",
    "re-allocate space if not enough",
    "read header",
    "get header",
    "read size of meta data",
    "re-allocate space if not enough",
    "read meta data",
    "load meta data",
    "sample local used data if need to partition",
    "if not contain query file, minimal sample unit is one record",
    "if contain query file, minimal sample unit is one query",
    "if is new query",
    "read feature data",
    "read feature size",
    "re-allocate space if not enough",
    "raw data",
    "fill feature_names_ if not header",
    "get forced split",
    "if only one machine, find bin locally",
    "if have multi-machines, need to find bin distributed",
    "different machines will find bin for different features",
    "start and len will store the process feature indices for different machines",
    "machine i will find bins for features in [ start[i], start[i] + len[i] )",
    "free",
    "gather global feature bin mappers",
    "restore features bins from buffer",
    "---- private functions ----",
    "get header",
    "num_groups",
    "real_feature_idx_",
    "feature2group",
    "feature2subfeature",
    "group_bin_boundaries",
    "group_feature_start_",
    "group_feature_cnt_",
    "get feature names",
    "get forced_bin_bounds_",
    "if features are ordered, not need to use hist_buf",
    "read all lines",
    "get query data",
    "if not contain query data, minimal sample unit is one record",
    "if contain query data, minimal sample unit is one query",
    "if is new query",
    "get query data",
    "if not contain query file, minimal sample unit is one record",
    "if contain query file, minimal sample unit is one query",
    "if is new query",
    "parse features",
    "get forced split",
    "check the range of label_idx, weight_idx and group_idx",
    "skip label check if user input parser config file,",
    "because label id is got from raw features while dataset features are consistent with customized parser.",
    "fill feature_names_ if not header",
    "start find bins",
    "if only one machine, find bin locally",
    "start and len will store the process feature indices for different machines",
    "machine i will find bins for features in [ start[i], start[i] + len[i] )",
    "free",
    "gather global feature bin mappers",
    "restore features bins from buffer",
    "if doesn't need to prediction with initial model",
    "parser",
    "set label",
    "free processed line:",
    "shrink_to_fit will be very slow in linux, and seems not free memory, disable for now",
    "text_reader_->Lines()[i].shrink_to_fit();",
    "push data",
    "if is used feature",
    "if need to prediction with initial model",
    "parser",
    "set initial score",
    "set label",
    "free processed line:",
    "shrink_to_fit will be very slow in Linux, and seems not free memory, disable for now",
    "text_reader_->Lines()[i].shrink_to_fit();",
    "push data",
    "if is used feature",
    "metadata_ will manage space of init_score",
    "text data can be free after loaded feature values",
    "parser",
    "set initial score",
    "set label",
    "push data",
    "if is used feature",
    "only need part of data",
    "need full data",
    "metadata_ will manage space of init_score",
    "read size of token",
    "remove duplicates",
    "deep copy function for BinMapper",
    "mean size for one bin",
    "need a new bin",
    "update bin upper bound",
    "last bin upper bound",
    "get number of positive and negative distinct values",
    "include zero bounds and infinity bound",
    "add forced bounds, excluding zeros since we have already added zero bounds",
    "find remaining bounds",
    "find distinct_values first",
    "push zero in the front",
    "use the large value",
    "push zero in the back",
    "convert to int type first",
    "sort by counts in descending order",
    "will ignore the categorical of small counts",
    "Push the dummy bin for NaN",
    "Use MissingType::None to represent this bin contains all categoricals",
    "fix count of NaN bin",
    "check trivial(num_bin_ == 1) feature",
    "check useless bin",
    "When most_freq_bin_ != default_bin_, there are some additional data loading costs.",
    "so use most_freq_bin_ = default_bin_ when there is not so sparse",
    "calculate max bin of all features to select the int type in MultiValDenseBin",
    "for lambdarank, it needs query data for partition data in distributed learning",
    "need convert query_id to boundaries",
    "check weights",
    "check query boundries",
    "contain initial score file",
    "check weights",
    "get local weights",
    "check query boundries",
    "get local query boundaries",
    "contain initial score file",
    "get local initial scores",
    "re-calculate query weight",
    "save to nullptr",
    "Note that len here is row count, not num_init_score, so we compare against num_data",
    "We need to use source_size here, because len might not equal size (due to a partially loaded dataset)",
    "CUDA is handled after all insertions are complete",
    "CUDA is handled after all insertions are complete",
    "save to nullptr",
    "CUDA is handled after all insertions are complete",
    "save to nullptr",
    "CUDA is handled after all insertions are complete",
    "default weight file name",
    "default init_score file name",
    "use first line to count number class",
    "default query file name",
    "root is in the depth 0",
    "non-leaf",
    "leaf",
    "use this for the missing value conversion",
    "Predict func by Map to ifelse",
    "use this for the missing value conversion",
    "non-leaf",
    "left subtree",
    "right subtree",
    "leaf",
    "non-leaf",
    "left subtree",
    "right subtree",
    "leaf",
    "recursive computation of SHAP values for a decision tree",
    "extend the unique path",
    "leaf node",
    "internal node",
    "see if we have already split on this feature,",
    "if so we undo that split so we can redo it for this node",
    "recursive sparse computation of SHAP values for a decision tree",
    "extend the unique path",
    "leaf node",
    "internal node",
    "see if we have already split on this feature,",
    "if so we undo that split so we can redo it for this node",
    "add names of objective function if not providing metric",
    "equal weights for all classes",
    "generate seeds by seed.",
    "sort eval_at",
    "Only push the non-training data",
    "check for conflicts",
    "check if objective, metric, and num_class match",
    "Change pool size to -1 (no limit) when using data parallel to reduce communication costs",
    "Check max_depth and num_leaves",
    "Fits in an int, and is more restrictive than the current num_leaves",
    "force col-wise for gpu, and cuda version",
    "force row-wise for cuda version",
    "linear tree learner must be serial type and run on CPU device",
    "min_data_in_leaf must be at least 2 if path smoothing is active. This is because when the split is calculated",
    "the count is calculated using the proportion of hessian in the leaf which is rounded up to nearest int, so it can",
    "be 1 when there is actually no data in the leaf. In rare cases this can cause a bug because with path smoothing the",
    "calculated split gain can be positive even with zero gradient and hessian.",
    "In distributed mode, local node doesn't have histograms on all features, cannot perform \"intermediate\" monotone constraints.",
    "\"intermediate\" monotone constraints need to recompute splits. If the features are sampled when computing the",
    "split initially, then the sampling needs to be recorded or done once again, which is currently not supported",
    "first round: fill the single val group",
    "always push the last group",
    "put dense feature first",
    "sort by non zero cnt",
    "sort by non zero cnt, bigger first",
    "shuffle groups",
    "Using std::swap for vector<bool> will cause the wrong result.",
    "get num_features",
    "get bin_mappers",
    "for sparse multi value bin, we store the feature bin values with offset added",
    "for dense multi value bin, the feature bin values without offsets are used",
    "copy feature bin mapper data",
    "copy feature bin mapper data",
    "update CUDA storage for column data and metadata",
    "if not pass a filename, just append \".bin\" of original file",
    "Write the basic header information for the dataset",
    "get size of meta data",
    "write meta data",
    "write feature data",
    "get size of feature",
    "write feature",
    "write raw data; use row-major order so we can read row-by-row",
    "Calculate approximate size of output and reserve space",
    "write feature group definitions",
    "Give a little extra just in case, to avoid unnecessary resizes",
    "Write token that marks the data as binary reference, and the version",
    "Write the basic definition of the overall dataset",
    "write feature group definitions",
    "get size of feature",
    "write feature",
    "size of feature names and forced bins",
    "write header",
    "write feature names",
    "write forced bins",
    "explicitly initialize template methods, for cross module call",
    "explicitly initialize template methods, for cross module call",
    "Only one multi-val group, just simply merge",
    "Skip the leading 0 when copying group_bin_boundaries.",
    "regenerate other fields",
    "need to iterate bin iterator",
    "is dense column",
    "is sparse column",
    "initialize the subset cuda column data",
    "if one column has too many bins, use a separate partition for that column",
    "try if adding this column exceed the maximum number per partition",
    "if one column has too many bins, use a separate partition for that column",
    "try if adding this column exceed the maximum number per partition",
    "store the importance first",
    "PredictRaw",
    "PredictRawByMap",
    "Predict",
    "PredictByMap",
    "PredictLeafIndex",
    "PredictLeafIndexByMap",
    "output model type",
    "output number of class",
    "output label index",
    "output max_feature_idx",
    "output objective",
    "output tree models",
    "store the importance first",
    "sort the importance",
    "use serialized string to restore this object",
    "Use first 128 chars to avoid exceed the message buffer.",
    "get number of classes",
    "get index of label",
    "get max_feature_idx first",
    "get average_output",
    "get feature names",
    "get monotone_constraints",
    "set zero",
    "predict all the trees for one iteration",
    "check early stopping",
    "set zero",
    "predict all the trees for one iteration",
    "check early stopping",
    "margin_threshold will be captured by value",
    "copy and sort",
    "margin_threshold will be captured by value",
    "Fix for compiler warnings about reaching end of control",
    "load forced_splits file",
    "init tree learner",
    "push training metrics",
    "get max feature index",
    "get label index",
    "get feature names",
    "get parser config file content",
    "check that forced splits does not use feature indices larger than dataset size",
    "if need bagging, create buffer",
    "for a validation dataset, we need its score and metric",
    "update score",
    "objective function will calculate gradients and hessians",
    "output used time per iteration",
    "boosting from average label; or customized \"average\" if implemented for the current objective",
    "boosting first",
    "use customized objective function",
    "need to copy customized gradients when using GOSS",
    "bagging logic",
    "need to copy gradients for bagging subset.",
    "shrinkage by learning rate",
    "update score",
    "only add default score one-time",
    "updates scores",
    "add model",
    "reset score",
    "remove model",
    "print message for metric",
    "pop last early_stopping_round_ models",
    "update training score",
    "we need to predict out-of-bag scores of data for boosting",
    "update validation score",
    "print training metric",
    "print validation metric",
    "set zero",
    "predict all the trees for one iteration",
    "predict all the trees for one iteration",
    "push training metrics",
    "not same training data, need reset score and others",
    "create score tracker",
    "update score",
    "resize gradient vectors to copy the customized gradients for goss or bagging with subset",
    "load forced_splits file",
    "if exists initial score, will start from it",
    "clear host score buffer",
    "Get the max size of pool",
    "at least need 2 leaves",
    "push split information for all leaves",
    "initialize splits for leaf",
    "initialize data partition",
    "initialize ordered gradients and hessians",
    "cannot change is_hist_col_wise during training",
    "initialize splits for leaf",
    "initialize data partition",
    "initialize ordered gradients and hessians",
    "Get the max size of pool",
    "at least need 2 leaves",
    "push split information for all leaves",
    "some initial works before training",
    "root leaf",
    "only root leaf can be splitted on first time",
    "some initial works before finding best split",
    "find best threshold for every feature",
    "Get a leaf with max split gain",
    "Get split information for best leaf",
    "cannot split, quit",
    "split tree with best leaf",
    "reset histogram pool",
    "initialize data partition",
    "reset the splits for leaves",
    "Sumup for root",
    "use all data",
    "use bagging, only use part of data",
    "check depth of current leaf",
    "only need to check left leaf, since right leaf is in same level of left leaf",
    "no enough data to continue",
    "only have root",
    "put parent(left) leaf's histograms into larger leaf's histograms",
    "put parent(left) leaf's histograms to larger leaf's histograms",
    "construct smaller leaf",
    "construct larger leaf",
    "find splits",
    "only has root leaf",
    "start at root leaf",
    "Histogram construction require parent features.",
    "then, compute own splits",
    "split info should exist because searching in bfs fashion - should have added from parent",
    "update before tree split",
    "don't need to update this in data-based parallel model",
    "split tree, will return right leaf",
    "store the true split gain in tree model",
    "don't need to update this in data-based parallel model",
    "store the true split gain in tree model",
    "init the leaves that used on next iteration",
    "update leave outputs if needed",
    "bag_mapper[index_mapper[i]]",
    "it is needed to filter the features after the above code.",
    "Otherwise, the `is_splittable` in `FeatureHistogram` will be wrong, and cause some features being accidentally filtered in the later nodes.",
    "for root leaf the \"parent\" output is its own output because we don't apply any smoothing to the root",
    "can't use GetParentOutput because leaf_splits doesn't have weight property set",
    "find splits",
    "identify features containing nans",
    "preallocate the matrix used to calculate linear model coefficients",
    "store only upper triangular half of matrix as an array, in row-major order",
    "this requires (max_num_feat + 1) * (max_num_feat + 2) / 2 entries (including the constant terms of the regression)",
    "we add another 8 to ensure cache lines are not shared among processors",
    "some initial works before training",
    "root leaf",
    "only root leaf can be splitted on first time",
    "some initial works before finding best split",
    "find best threshold for every feature",
    "Get a leaf with max split gain",
    "Get split information for best leaf",
    "cannot split, quit",
    "split tree with best leaf",
    "map data to leaf number",
    "calculate coefficients using the method described in Eq 3 of https://arxiv.org/pdf/1802.05640.pdf",
    "the coefficients vector is given by",
    "- (X_T * H * X + lambda) ^ (-1) * (X_T * g)",
    "where:",
    "X is the matrix where the first column is the feature values and the second is all ones,",
    "H is the diagonal matrix of the hessian,",
    "lambda is the diagonal matrix with diagonal entries equal to the regularisation term linear_lambda",
    "g is the vector of gradients",
    "the subscript _T denotes the transpose",
    "create array of pointers to raw data, and coefficient matrices, for each leaf",
    "clear the coefficient matrices",
    "aggregate results from different threads",
    "copy into eigen matrices and solve",
    "update the tree properties",
    "need to be able to hold smaller and larger best splits in SyncUpGlobalBestSplit",
    "get feature partition",
    "get local used features",
    "get best split at smaller leaf",
    "find local best split for larger leaf",
    "sync global best info",
    "update best split",
    "instantiate template classes, otherwise linker cannot find the code",
    "initialize SerialTreeLearner",
    "Get local rank and global machine size",
    "need to be able to hold smaller and larger best splits in SyncUpGlobalBestSplit",
    "allocate buffer for communication",
    "get block start and block len for reduce scatter",
    "get buffer_write_start_pos",
    "get buffer_read_start_pos",
    "generate feature partition for current tree",
    "get local used feature",
    "get block start and block len for reduce scatter",
    "sync global data sumup info",
    "global sumup reduce",
    "copy back",
    "set global sumup info",
    "init global data count in leaf",
    "reset hist num bits according to global num data",
    "sync global data sumup info",
    "global sumup reduce",
    "copy back",
    "set global sumup info",
    "init global data count in leaf",
    "clear histogram buffer before synchronizing",
    "otherwise histogram contents from the previous iteration will be sent",
    "construct local histograms",
    "copy to buffer",
    "Reduce scatter for histogram",
    "restore global histograms from buffer",
    "only root leaf",
    "construct histgroms for large leaf, we init larger leaf as the parent, so we can just subtract the smaller leaf's histograms",
    "find local best split for larger leaf",
    "sync global best info",
    "set best split",
    "need update global number of data in leaf",
    "reset hist num bits according to global num data",
    "instantiate template classes, otherwise linker cannot find the code",
    "initialize SerialTreeLearner",
    "some additional variables needed for GPU trainer",
    "Initialize GPU buffers and kernels",
    "some functions used for debugging the GPU histogram construction",
    "printf(\"grad %g != %g (%d ULPs)\\n\", GET_GRAD(h1, i), GET_GRAD(h2, i), ulps);",
    "goto err;",
    "we roughly want 256 workgroups per device, and we have num_dense_feature4_ feature tuples.",
    "also guarantee that there are at least 2K examples per workgroup",
    "return 0;",
    "we have already copied ordered gradients, ordered Hessians and indices to GPU",
    "decide the best number of workgroups working on one feature4 tuple",
    "set work group size based on feature size",
    "each 2^exp_workgroups_per_feature workgroups work on a feature4 tuple",
    "we need to refresh the kernel arguments after reallocating",
    "The only argument that needs to be changed later is num_data_",
    "the GPU kernel will process all features in one call, and each",
    "2^exp_workgroups_per_feature (compile time constant) workgroup will",
    "process one feature4 tuple",
    "for the root node, indices are not copied",
    "for constant hessian, hessians are not copied except for the root node",
    "there will be 2^exp_workgroups_per_feature = num_workgroups / num_dense_feature4 sub-histogram per feature4",
    "and we will launch num_feature workgroups for this kernel",
    "will launch threads for all features",
    "the queue should be asynchronous, and we will can WaitAndGetHistograms() before we start processing dense feature groups",
    "copy the results asynchronously. Size depends on if double precision is used",
    "we will wait for this object in WaitAndGetHistograms",
    "when the output is ready, the computation is done",
    "values of this feature has been redistributed to multiple bins; need a reduction here",
    "how many feature-group tuples we have",
    "leave some safe margin for prefetching",
    "256 work-items per workgroup. Each work-item prefetches one tuple for that feature",
    "clear sparse/dense maps",
    "do nothing if no features can be processed on GPU",
    "allocate memory for all features (FIXME: 4 GB barrier on some devices, need to split to multiple buffers)",
    "unpin old buffer if necessary before destructing them",
    "make ordered_gradients and Hessians larger (including extra room for prefetching), and pin them",
    "allocate space for gradients and Hessians on device",
    "we will copy gradients and Hessians in after ordered_gradients_ and ordered_hessians_ are constructed",
    "allocate feature mask, for disabling some feature-groups' histogram calculation",
    "copy indices to the device",
    "histogram bin entry size depends on the precision (single/double)",
    "create output buffer, each feature has a histogram with device_bin_size_ bins,",
    "each work group generates a sub-histogram of dword_features_ features.",
    "only initialize once here, as this will not need to change when ResetTrainingData() is called",
    "create atomic counters for inter-group coordination",
    "The output buffer is allocated to host directly, to overlap compute and data transfer",
    "find the dense feature-groups and group then into Feature4 data structure (several feature-groups packed into 4 bytes)",
    "looking for dword_features_ non-sparse feature-groups",
    "decide if we need to redistribute the bin",
    "multiplier must be a power of 2",
    "device_bin_mults_.push_back(1);",
    "found",
    "for data transfer time",
    "Now generate new data structure feature4, and copy data to the device",
    "preallocate arrays for all threads, and pin them",
    "building Feature4 bundles; each thread handles dword_features_ features",
    "one feature datapoint is 4 bits",
    "this guarantees that the RawGet() function is inlined, rather than using virtual function dispatching",
    "one feature datapoint is one byte",
    "this guarantees that the RawGet() function is inlined, rather than using virtual function dispatching",
    "Dense bin",
    "Dense 4-bit bin",
    "working on the remaining (less than dword_features_) feature groups",
    "fill the leftover features",
    "fill this empty feature with some \"random\" value",
    "fill this empty feature with some \"random\" value",
    "copying the last 1 to (dword_features - 1) feature-groups in the last tuple",
    "deallocate pinned space for feature copying",
    "data transfer time",
    "for other types of failure, build log might not be available; program.build_log() can crash",
    "Something bad happened. Just return \"No log available.\"",
    "build is okay, log may contain warnings",
    "destroy any old kernels",
    "create OpenCL kernels for different number of workgroups per feature",
    "currently we don't use constant memory",
    "compile the GPU kernel depending if double precision is used, constant hessian is used, etc.",
    "kernel with indices in an array",
    "kernel with all features enabled, with eliminated branches",
    "kernel with all data indices (for root node, and assumes that root node always uses all features)",
    "do nothing if no features can be processed on GPU",
    "The only argument that needs to be changed later is num_data_",
    "hessian is passed as a parameter, but it is not available now.",
    "hessian will be set in BeforeTrain()",
    "Get the max bin size, used for selecting best GPU kernel",
    "initialize GPU",
    "determine which kernel to use based on the max number of bins",
    "the +9 skips extra characters \")\", newline, \"#endif\" and newline at the beginning",
    "the +9 skips extra characters \")\", newline, \"#endif\" and newline at the beginning",
    "the +9 skips extra characters \")\", newline, \"#endif\" and newline at the beginning",
    "ignore the feature groups that contain categorical features when producing warnings about max_bin.",
    "these groups may contain larger number of bins due to categorical features, but not due to the setting of max_bin.",
    "setup GPU kernel arguments after we allocating all the buffers",
    "GPU memory has to been reallocated because data may have been changed",
    "setup GPU kernel arguments after we allocating all the buffers",
    "Copy initial full hessians and gradients to GPU.",
    "We start copying as early as possible, instead of at ConstructHistogram().",
    "setup hessian parameters only",
    "hessian is passed as a parameter",
    "use bagging",
    "On GPU, we start copying indices, gradients and Hessians now, instead at ConstructHistogram()",
    "copy used gradients and Hessians to ordered buffer",
    "transfer the indices to GPU",
    "transfer hessian to GPU",
    "setup hessian parameters only",
    "hessian is passed as a parameter",
    "transfer gradients to GPU",
    "only have root",
    "Copy indices, gradients and Hessians as early as possible",
    "only need to initialize for smaller leaf",
    "Get leaf boundary",
    "copy indices to the GPU:",
    "copy ordered Hessians to the GPU:",
    "copy ordered gradients to the GPU:",
    "do nothing if no features can be processed on GPU",
    "copy data indices if it is not null",
    "generate and copy ordered_gradients if gradients is not null",
    "generate and copy ordered_hessians if Hessians is not null",
    "converted indices in is_feature_used to feature-group indices",
    "construct the feature masks for dense feature-groups",
    "if no feature group is used, just return and do not use GPU",
    "if not all feature groups are used, we need to transfer the feature mask to GPU",
    "otherwise, we will use a specialized GPU kernel with all feature groups enabled",
    "All data have been prepared, now run the GPU kernel",
    "construct smaller leaf",
    "ConstructGPUHistogramsAsync will return true if there are available feature groups dispatched to GPU",
    "then construct sparse features on CPU",
    "wait for GPU to finish, only if GPU is actually used",
    "use double precision",
    "use single precision",
    "Compare GPU histogram with CPU histogram, useful for debugging GPU code problem",
    "#define GPU_DEBUG_COMPARE",
    "construct larger leaf",
    "then construct sparse features on CPU",
    "wait for GPU to finish, only if GPU is actually used",
    "use double precision",
    "use single precision",
    "do some sanity check for the GPU algorithm",
    "limit top k",
    "get max bin",
    "calculate buffer size",
    "need to be able to hold smaller and larger best splits in SyncUpGlobalBestSplit",
    "left and right on same time, so need double size",
    "initialize histograms for global",
    "sync global data sumup info",
    "set global sumup info",
    "init global data count in leaf",
    "get local sumup",
    "get local sumup",
    "get mean number on machines",
    "weighted gain",
    "get top k",
    "Copy histogram to buffer, and Get local aggregate features",
    "copy histograms.",
    "copy smaller leaf histograms first",
    "mark local aggregated feature",
    "copy",
    "then copy larger leaf histograms",
    "mark local aggregated feature",
    "copy",
    "use local data to find local best splits",
    "clear histogram buffer before synchronizing",
    "otherwise histogram contents from the previous iteration will be sent",
    "find splits",
    "only has root leaf",
    "local voting",
    "gather",
    "get all top-k from all machines",
    "global voting",
    "copy local histgrams to buffer",
    "Reduce scatter for histogram",
    "find best split from local aggregated histograms",
    "restore from buffer",
    "restore from buffer",
    "find local best",
    "find local best split for larger leaf",
    "sync global best info",
    "copy back",
    "set the global number of data for leaves",
    "init the global sumup info",
    "instantiate template classes, otherwise linker cannot find the code",
    "allocate CUDA memory",
    "leave some space for alignment",
    "input best split info",
    "for leaf information update",
    "gather information for CPU, used for launching kernels",
    "for leaf splits information update",
    "we need restore the order of indices in cuda_data_indices_",
    "allocate more memory for sum reduction in CUDA",
    "only the first element records the final sum",
    "intialize split find task information (a split find task is one pass through the histogram of a feature)",
    "use the first gpu by default",
    "std::max(..., 1UL) to avoid error in the case when there are NaN's in the categorical values",
    "use feature interaction constraint or sample features by node"
  ],
  "v3.3.5": [
    "coding: utf-8",
    "coding: utf-8",
    "create predictor first",
    "show deprecation warning only for early stop argument, setting early stop via global params should still be possible",
    "check dataset",
    "reduce cost for prediction training data",
    "process callbacks",
    "Most of legacy advanced options becomes callbacks",
    "construct booster",
    "start training",
    "check evaluation result.",
    "ranking task, split according to groups",
    "run preprocessing on the data set if needed",
    "setup callbacks",
    "coding: utf-8",
    "dummy function to support older version of scikit-learn",
    "coding: utf-8",
    "documentation templates for LGBMModel methods are shared between the classes in",
    "this module and those in the ``dask`` module",
    "user can set verbose with kwargs, it has higher priority",
    "Do not modify original args in fit function",
    "Refer to https://github.com/microsoft/LightGBM/pull/2619",
    "Separate built-in from callable evaluation metrics",
    "register default metric for consistency with callable eval_metric case",
    "try to deduce from class instance",
    "overwrite default metric by explicitly set metric",
    "concatenate metric from params (or default if not provided in params) and eval_metric",
    "copy for consistency",
    "reduce cost for prediction training data",
    "free dataset",
    "Switch to using a multiclass objective in the underlying LGBM instance",
    "do not modify args, as it causes errors in model selection tools",
    "check group data",
    "coding: utf-8",
    "we don't need lib_lightgbm while building docs",
    "coding: utf-8",
    "\"#ddffdd\" is light green, \"#ffdddd\" is light red",
    "coding: utf-8",
    "coding: utf-8",
    "TypeError: obj is not a string or a number",
    "ValueError: invalid literal",
    "DeprecationWarning is not shown by default, so let's create our own with higher level",
    "avoid side effects on passed-in parameters",
    "find a value, and remove other aliases with .pop()",
    "prefer the value of 'main_param_name' if it exists, otherwise search the aliases",
    "Get total row number.",
    "Random access by row index. Used for data sampling.",
    "Range data access. Used to read data in batch when constructing Dataset.",
    "Optionally specify batch_size to control range data read size.",
    "Only required if using ``Dataset.subset()``.",
    "__get_num_preds() cannot work with nrow > MAX_INT32, so calculate overall number of predictions piecemeal",
    "avoid memory consumption by arrays concatenation operations",
    "create numpy array from output arrays",
    "break up indptr based on number of rows (note more than one matrix in multiclass case)",
    "for CSC there is extra column added",
    "reformat output into a csr or csc matrix or list of csr or csc matrices",
    "same shape as input csr or csc matrix except extra column for expected value",
    "note: make sure we copy data as it will be deallocated next",
    "free the temporary native indptr, indices, and data",
    "__get_num_preds() cannot work with nrow > MAX_INT32, so calculate overall number of predictions piecemeal",
    "avoid memory consumption by arrays concatenation operations",
    "c type: double**",
    "each double* element points to start of each column of sample data.",
    "c type int**",
    "each int* points to start of indices for each column",
    "no min_data, nthreads and verbose in this function",
    "check data has header or not",
    "need to regroup init_score",
    "process for args",
    "user can set verbose with params, it has higher priority",
    "get categorical features",
    "process for reference dataset",
    "start construct data",
    "set feature names",
    "Select sampled rows, transpose to column order.",
    "create validation dataset from ref_dataset",
    "create valid",
    "construct subset",
    "create train",
    "could be updated if data is not freed",
    "set to None",
    "we're done if self and reference share a common upstream reference",
    "if buffer length is not long enough, reallocate buffers",
    "group data from LightGBM is boundaries data, need to convert to group size",
    "user can set verbose with params, it has higher priority",
    "Training task",
    "if \"machines\" is given, assume user wants to do distributed learning, and set up network",
    "construct booster object",
    "copy the parameters from train_set",
    "save reference to data",
    "buffer for inner predict",
    "Prediction task",
    "if a single node tree it won't have `leaf_index` so return 0",
    "Create the node record, and populate universal data members",
    "Update values to reflect node type (leaf or split)",
    "traverse the next level of the tree",
    "In tree format, \"subtree_list\" is a list of node records (dicts),",
    "and we add node to the list.",
    "need reset training data",
    "need to push new valid data",
    "if buffer length is not long enough, re-allocate a buffer",
    "if buffer length is not long enough, reallocate a buffer",
    "Copy models",
    "Get name of features",
    "if buffer length is not long enough, reallocate buffers",
    "avoid to predict many time in one iteration",
    "Get num of inner evals",
    "Get name of eval metrics",
    "if buffer length is not long enough, reallocate buffers",
    "coding: utf-8",
    "Callback environment used by callbacks",
    "split is needed for \"<dataset type> <metric>\" case (e.g. \"train l1\")",
    "split is needed for \"<dataset type> <metric>\" case (e.g. \"train l1\")",
    "coding: utf-8",
    "Concatenate many parts into one",
    "construct local eval_set data.",
    "store indices of eval_set components that were not contained within local parts.",
    "consolidate parts of each individual eval component.",
    "require that eval_name exists in evaluated result data in case dropped due to padding.",
    "in distributed training the 'training' eval_set is not detected, will have name 'valid_<index>'.",
    "filter padding from eval parts then _concat each eval_set component.",
    "reconstruct eval_set fit args/kwargs depending on which components of eval_set are on worker.",
    "ensure that expected keys for evals_result_ and best_score_ exist regardless of padding.",
    "capture whether local_listen_port or its aliases were provided",
    "capture whether machines or its aliases were provided",
    "Some passed-in parameters can be removed:",
    "* 'num_machines': set automatically from Dask worker list",
    "* 'num_threads': overridden to match nthreads on each Dask process",
    "Split arrays/dataframes into parts. Arrange parts into dicts to enforce co-locality",
    "evals_set will to be re-constructed into smaller lists of (X, y) tuples, where",
    "X and y are each delayed sub-lists of original eval dask Collections.",
    "find maximum number of parts in an individual eval set so that we can",
    "pad eval sets when they come in different sizes.",
    "when individual eval set is equivalent to training data, skip recomputing parts.",
    "add None-padding for individual eval_set member if it is smaller than the largest member.",
    "first time a chunk of this eval set is added to this part.",
    "append additional chunks of this eval set to this part.",
    "ensure that all evaluation parts map uniquely to one part.",
    "assign sub-eval_set components to worker parts.",
    "Start computation in the background",
    "Find locations of all parts and map them to particular Dask workers",
    "Check that all workers were provided some of eval_set. Otherwise warn user that validation",
    "data artifacts may not be populated depending on worker returning final estimator.",
    "assign general validation set settings to fit kwargs.",
    "resolve aliases for network parameters and pop the result off params.",
    "these values are added back in calls to `_train_part()`",
    "figure out network params",
    "Tell each worker to train on the parts that it has locally",
    "",
    "This code treats ``_train_part()`` calls as not \"pure\" because:",
    "1. there is randomness in the training process unless parameters ``seed``",
    "and ``deterministic`` are set",
    "2. even with those parameters set, the output of one ``_train_part()`` call",
    "relies on global state (it and all the other LightGBM training processes",
    "coordinate with each other)",
    "if network parameters were changed during training, remove them from the",
    "returned model so that they're generated dynamically on every run based",
    "on the Dask cluster you're connected to and which workers have pieces of",
    "the training data",
    "dask.DataFrame.map_partitions() expects each call to return a pandas DataFrame or Series",
    "for multi-class classification with sparse matrices, pred_contrib predictions",
    "are returned as a list of sparse matrices (one per class)",
    "pred_contrib output will have one column per feature,",
    "plus one more for the base value",
    "need to tell Dask the expected type and shape of individual preds",
    "by default, dask.array.concatenate() concatenates sparse arrays into a COO matrix",
    "the code below is used instead to ensure that the sparse type is preserved during concatentation",
    "At this point, `out` is a list of lists of delayeds (each of which points to a matrix).",
    "Concatenate them to return a list of Dask Arrays.",
    "the note on custom objective functions in LGBMModel.__init__ is not",
    "currently relevant for the Dask estimators",
    "DaskLGBMClassifier does not support group, eval_group, early_stopping_rounds.",
    "DaskLGBMClassifier support for callbacks and init_model is not tested",
    "the note on custom objective functions in LGBMModel.__init__ is not",
    "currently relevant for the Dask estimators",
    "DaskLGBMRegressor does not support group, eval_class_weight, eval_group, early_stopping_rounds.",
    "DaskLGBMRegressor support for callbacks and init_model is not tested",
    "the note on custom objective functions in LGBMModel.__init__ is not",
    "currently relevant for the Dask estimators",
    "DaskLGBMRanker does not support eval_class_weight or early stopping",
    "DaskLGBMRanker support for callbacks and init_model is not tested",
    "coding: utf-8",
    "load or create your dataset",
    "create dataset for lightgbm",
    "if you want to re-use data, remember to set free_raw_data=False",
    "specify your configurations as a dict",
    "generate feature names",
    "feature_name and categorical_feature",
    "check feature name",
    "save model to file",
    "dump model to JSON (and save to file)",
    "feature names",
    "feature importances",
    "load model to predict",
    "can only predict with the best iteration (or the saving iteration)",
    "eval with loaded model",
    "dump model with pickle",
    "load model with pickle to predict",
    "can predict with any iteration when loaded in pickle way",
    "eval with loaded model",
    "continue training",
    "init_model accepts:",
    "1. model file name",
    "2. Booster()",
    "decay learning rates",
    "learning_rates accepts:",
    "1. list/tuple with length = num_boost_round",
    "2. function(curr_iter)",
    "change other parameters during training",
    "self-defined objective function",
    "f(preds: array, train_data: Dataset) -> grad: array, hess: array",
    "log likelihood loss",
    "self-defined eval metric",
    "f(preds: array, train_data: Dataset) -> name: str, eval_result: float, is_higher_better: bool",
    "binary error",
    "NOTE: when you do customized loss function, the default prediction value is margin",
    "This may make built-in evaluation metric calculate wrong results",
    "For example, we are doing log likelihood loss, the prediction is score before logistic transformation",
    "Keep this in mind when you use the customization",
    "another self-defined eval metric",
    "f(preds: array, train_data: Dataset) -> name: str, eval_result: float, is_higher_better: bool",
    "accuracy",
    "NOTE: when you do customized loss function, the default prediction value is margin",
    "This may make built-in evaluation metric calculate wrong results",
    "For example, we are doing log likelihood loss, the prediction is score before logistic transformation",
    "Keep this in mind when you use the customization",
    "callback",
    "coding: utf-8",
    "load or create your dataset",
    "train",
    "predict",
    "eval",
    "feature importances",
    "self-defined eval metric",
    "f(y_true: array, y_pred: array) -> name: str, eval_result: float, is_higher_better: bool",
    "Root Mean Squared Logarithmic Error (RMSLE)",
    "train",
    "another self-defined eval metric",
    "f(y_true: array, y_pred: array) -> name: str, eval_result: float, is_higher_better: bool",
    "Relative Absolute Error (RAE)",
    "train",
    "predict",
    "eval",
    "other scikit-learn modules",
    "coding: utf-8",
    "load or create your dataset",
    "create dataset for lightgbm",
    "specify your configurations as a dict",
    "train",
    "coding: utf-8",
    "################",
    "Simulate some binary data with a single categorical and",
    "single continuous predictor",
    "################",
    "Set up a couple of utilities for our experiments",
    "################",
    "Observe the behavior of `binary` and `xentropy` objectives",
    "Trying this throws an error on non-binary values of y:",
    "experiment('binary', label_type='probability', DATA)",
    "The speed of `binary` is not drastically different than",
    "`xentropy`. `xentropy` runs faster than `binary` in many cases, although",
    "there are reasons to suspect that `binary` should run faster when the",
    "label is an integer instead of a float",
    "coding: utf-8",
    "load or create your dataset",
    "create dataset for lightgbm",
    "specify your configurations as a dict",
    "train",
    "save model to file",
    "predict",
    "eval",
    "We can also open HDF5 file once and get access to",
    "With binary dataset created, we can use either Python API or cmdline version to train.",
    "",
    "Note: in order to create exactly the same dataset with the one created in simple_example.py, we need",
    "to modify simple_example.py to pass numpy array instead of pandas DataFrame to Dataset constructor.",
    "The reason is that DataFrame column names will be used in Dataset. For a DataFrame with Int64Index",
    "as columns, Dataset will use column names like [\"0\", \"1\", \"2\", ...]. While for numpy array, column names",
    "are using the default one assigned in C++ code (dataset_loader.cpp), like [\"Column_0\", \"Column_1\", ...].",
    "Y has a single column and we read it in single shot. So store it as an 1-d array.",
    "We use random access for data sampling when creating LightGBM Dataset from Sequence.",
    "When accessing any element in a HDF5 chunk, it's read entirely.",
    "To save I/O for sampling, we should keep number of total chunks much larger than sample count.",
    "Here we are just creating a chunk size that matches with batch_size.",
    "",
    "Also note that the data is stored in row major order to avoid extra copy when passing to",
    "lightgbm Dataset.",
    "Save to 2 HDF5 files for demonstration.",
    "We can store multiple datasets inside a single HDF5 file.",
    "Separating X and Y for choosing best chunk size for data loading.",
    "split training data into two partitions",
    "make this array dense because we're splitting across",
    "a sparse boundary to partition the data",
    "the code below uses sklearn.metrics, but this requires pulling all of the",
    "predictions and target values back from workers to the client",
    "",
    "for larger datasets, consider the metrics from dask-ml instead",
    "https://ml.dask.org/modules/api.html#dask-ml-metrics-metrics",
    "coding: utf-8",
    "!/usr/bin/env python3",
    "-*- coding: utf-8 -*-",
    "",
    "LightGBM documentation build configuration file, created by",
    "sphinx-quickstart on Thu May  4 14:30:58 2017.",
    "",
    "This file is execfile()d with the current directory set to its",
    "containing dir.",
    "",
    "Note that not all possible configuration values are present in this",
    "autogenerated file.",
    "",
    "All configuration values have a default; values that are commented out",
    "serve to show the default.",
    "If extensions (or modules to document with autodoc) are in another directory,",
    "add these directories to sys.path here. If the directory is relative to the",
    "documentation root, use os.path.abspath to make it absolute.",
    "-- General configuration ------------------------------------------------",
    "If your documentation needs a minimal Sphinx version, state it here.",
    "Add any Sphinx extension module names here, as strings. They can be",
    "extensions coming with Sphinx (named 'sphinx.ext.*') or your custom",
    "ones.",
    "mock out modules",
    "hide type hints in API docs",
    "Generate autosummary pages. Output should be set with: `:toctree: pythonapi/`",
    "Only the class' docstring is inserted.",
    "If true, `todo` and `todoList` produce output, else they produce nothing.",
    "The master toctree document.",
    "General information about the project.",
    "The name of an image file (relative to this directory) to place at the top",
    "of the sidebar.",
    "The name of an image file (relative to this directory) to use as a favicon of",
    "the docs.  This file should be a Windows icon file (.ico) being 16x16 or 32x32",
    "pixels large.",
    "The version info for the project you're documenting, acts as replacement for",
    "|version| and |release|, also used in various other places throughout the",
    "built documents.",
    "The short X.Y version.",
    "The full version, including alpha/beta/rc tags.",
    "The language for content autogenerated by Sphinx. Refer to documentation",
    "for a list of supported languages.",
    "",
    "This is also used if you do content translation via gettext catalogs.",
    "Usually you set \"language\" from the command line for these cases.",
    "List of patterns, relative to source directory, that match files and",
    "directories to ignore when looking for source files.",
    "This patterns also effect to html_static_path and html_extra_path",
    "The name of the Pygments (syntax highlighting) style to use.",
    "-- Configuration for C API docs generation ------------------------------",
    "-- Options for HTML output ----------------------------------------------",
    "The theme to use for HTML and HTML Help pages.  See the documentation for",
    "a list of builtin themes.",
    "Theme options are theme-specific and customize the look and feel of a theme",
    "further.  For a list of options available for each theme, see the",
    "documentation.",
    "Add any paths that contain custom static files (such as style sheets) here,",
    "relative to this directory. They are copied after the builtin static files,",
    "so a file named \"default.css\" will overwrite the builtin \"default.css\".",
    "-- Options for HTMLHelp output ------------------------------------------",
    "Output file base name for HTML help builder.",
    "-- Options for LaTeX output ---------------------------------------------",
    "The name of an image file (relative to this directory) to place at the top of",
    "the title page.",
    "Warning! The following code can cause buffer overflows on RTD.",
    "Consider suppressing output completely if RTD project silently fails.",
    "Refer to https://github.com/svenevs/exhale",
    "/blob/fe7644829057af622e467bb529db6c03a830da99/exhale/deploy.py#L99-L111",
    "Warning! The following code can cause buffer overflows on RTD.",
    "Consider suppressing output completely if RTD project silently fails.",
    "Refer to https://github.com/svenevs/exhale",
    "/blob/fe7644829057af622e467bb529db6c03a830da99/exhale/deploy.py#L99-L111",
    "coding: utf-8",
    "This is a basic test for floating number parsing.",
    "Most of the test cases come from:",
    "https://github.com/dmlc/xgboost/blob/master/tests/cpp/common/test_charconv.cc",
    "https://github.com/Alexhuszagh/rust-lexical/blob/master/data/test-parse-unittests/strtod_tests.toml",
    "FLT_MAX",
    "FLT_MIN",
    "DBL_MAX (1 + (1 - 2^-52)) * 2^1023 = (2^53 - 1) * 2^971",
    "2^971 * (2^53 - 1 + 1/2) : the smallest number resolving to inf",
    "Near DBL_MIN",
    "DBL_MIN 2^-1022",
    "The behavior for parsing -nan depends on implementation.",
    "Thus we skip binary check for negative nan.",
    "See comment in test_cases.",
    "Constants",
    "Start with some content:",
    "Clear & re-use:",
    "Output should match new content:",
    "Number of trials for each new ChunkedArray configuration. Pass 100 times over the search space:",
    "Each outer loop iteration changes the test by adding +1 chunk. We start with 1 chunk only:",
    "Sweep valid and invalid addresses with a ChunkedArray with `chunks` chunks:",
    "Compute a new trial address & value & if it is a valid address:",
    "Insert item. If at a valid address, 0 is returned, otherwise, -1 is returned:",
    "If at valid address, check that the stored value is correct & remember it for the future:",
    "Check the just-stored value with getitem():",
    "Also store the just-stored value for future tracking:",
    "Final check: ensure even with overrides, all valid insertions store the latest value at that address:",
    "Test in 2 ways that the values are correctly laid out in memory:",
    "coding: utf-8",
    "we don't need lib_lightgbm while building docs",
    "coding: utf-8",
    "check saved model persistence",
    "we need to check the consistency of model file here, so test for exact equal",
    "check early stopping is working. Make it stop very early, so the scores should be very close to zero",
    "scores likely to be different, but prediction should still be the same",
    "test that shape is checked during prediction",
    "The simple implementation is just a single \"return self.ndarray[idx]\"",
    "The following is for demo and testing purpose.",
    "whole col",
    "half col",
    "Create dataset from numpy array directly.",
    "Create dataset using Sequence.",
    "Test for validation set.",
    "Select some random rows as valid data.",
    "From Dataset constructor, with dataset from numpy array.",
    "From Dataset.create_valid, with dataset from sequence.",
    "test that method works even with free_raw_data=True",
    "test that method works but sets raw data to None in case of immergeable data types",
    "test that method works for different data types",
    "Set extremely harsh penalties, so CEGB will block most splits.",
    "Compare pairs of penalties, to ensure scaling works as intended",
    "Reset booster1's parameters to p2, so the parameter section of the file matches.",
    "should resolve duplicate aliases, and prefer the main parameter",
    "should choose a value from an alias and set that value on main param",
    "if only an alias is used",
    "should use the default if main param and aliases are missing",
    "all changes should be made on copies and not modify the original",
    "coding: utf-8",
    "add target, weight, and group to DataFrame so that partitions abide by group boundaries.",
    "set_index ensures partitions are based on group id.",
    "See https://stackoverflow.com/questions/49532824/dask-dataframe-split-partitions-based-on-a-column-or-function.",
    "separate target, weight from features.",
    "encode group identifiers into run-length encoding, the format LightGBMRanker is expecting",
    "so that within each partition, sum(g) = n_samples.",
    "ranking arrays: one chunk per group. Each chunk must include all columns.",
    "make one categorical feature relevant to the target",
    "https://github.com/microsoft/LightGBM/issues/4118",
    "extra predict() parameters should be passed through correctly",
    "pref_leaf values should have the right shape",
    "and values that look like valid tree nodes",
    "be sure LightGBM actually used at least one categorical column,",
    "and that it was correctly treated as a categorical feature",
    "shape depends on whether it is binary or multiclass classification",
    "in the special case of multi-class classification using scipy sparse matrices,",
    "the output of `.predict(..., pred_contrib=True)` is a list of sparse matrices (one per class)",
    "",
    "since that case is so different than all other cases, check the relevant things here",
    "and then return early",
    "raw scores will probably be different, but at least check that all predicted classes are the same",
    "be sure LightGBM actually used at least one categorical column,",
    "and that it was correctly treated as a categorical feature",
    "* shape depends on whether it is binary or multiclass classification",
    "* matrix for binary classification is of the form [feature_contrib, base_value],",
    "for multi-class it's [feat_contrib_class1, base_value_class1, feat_contrib_class2, base_value_class2, etc.]",
    "* contrib outputs for distributed training are different than from local training, so we can just test",
    "that the output has the right shape and base values are in the right position",
    "check that found ports are different for same address (LocalCluster)",
    "check that the ports are indeed open",
    "Scores should be the same",
    "Predictions should be roughly the same.",
    "pref_leaf values should have the right shape",
    "and values that look like valid tree nodes",
    "extra predict() parameters should be passed through correctly",
    "be sure LightGBM actually used at least one categorical column,",
    "and that it was correctly treated as a categorical feature",
    "contrib outputs for distributed training are different than from local training, so we can just test",
    "that the output has the right shape and base values are in the right position",
    "be sure LightGBM actually used at least one categorical column,",
    "and that it was correctly treated as a categorical feature",
    "Quantiles should be right",
    "be sure LightGBM actually used at least one categorical column,",
    "and that it was correctly treated as a categorical feature",
    "rebalance small dask.Array dataset for better performance.",
    "use many trees + leaves to overfit, help ensure that Dask data-parallel strategy matches that of",
    "serial learner. See https://github.com/microsoft/LightGBM/issues/3292#issuecomment-671288210.",
    "distributed ranker should be able to rank decently well and should",
    "have high rank correlation with scores from serial ranker.",
    "extra predict() parameters should be passed through correctly",
    "pref_leaf values should have the right shape",
    "and values that look like valid tree nodes",
    "be sure LightGBM actually used at least one categorical column,",
    "and that it was correctly treated as a categorical feature",
    "Use larger trainset to prevent premature stopping due to zero loss, causing num_trees() < n_estimators.",
    "Use small chunk_size to avoid single-worker allocation of eval data partitions.",
    "test eval_class_weight, eval_init_score on binary-classification task.",
    "Note: objective's default `metric` will be evaluated in evals_result_ in addition to all eval_metrics.",
    "create eval_sets by creating new datasets or copying training data.",
    "total number of trees scales up for ova classifier.",
    "check that early stopping was not applied.",
    "checks that evals_result_ and best_score_ contain expected data and eval_set names.",
    "check that each eval_name and metric exists for all eval sets, allowing for the",
    "case when a worker receives a fully-padded eval_set component which is not evaluated.",
    "should be able to use the class without specifying a client",
    "should be able to set client after construction",
    "data on cluster1",
    "create identical data on cluster2",
    "at this point, the result of default_client() is client2 since it was the most recently",
    "created. So setting client to client1 here to test that you can select a non-default client",
    "unfitted model should survive pickling round trip, and pickling",
    "shouldn't have side effects on the model object",
    "client will always be None after unpickling",
    "fitted model should survive pickling round trip, and pickling",
    "shouldn't have side effects on the model object",
    "client will always be None after unpickling",
    "rebalance data to be sure that each worker has a piece of the data",
    "model 1 - no network parameters given",
    "model 2 - machines given",
    "model 3 - local_listen_port given",
    "training should fail because LightGBM will try to use the same",
    "port for multiple worker processes on the same machine",
    "rebalance data to be sure that each worker has a piece of the data",
    "test that \"machines\" is actually respected by creating a socket that uses",
    "one of the ports mentioned in \"machines\"",
    "The above error leaves a worker waiting",
    "an informative error should be raised if \"machines\" has duplicates",
    "\"client\" should be the only different, and the final argument",
    "value of the root node is 0 when init_score is set",
    "this test is separate because it takes a not-yet-constructed estimator",
    "coding: utf-8",
    "coding: utf-8",
    "build target, group ID vectors.",
    "build y/target and group-id vectors with user-specified group sizes.",
    "build y/target and group-id vectors according to n_samples, avg_gs, and random_gs.",
    "groups should contain > 1 element for pairwise learning objective.",
    "build feature data, X. Transform first few into informative features.",
    "coding: utf-8",
    "prediction result is actually not transformed (is raw) due to custom objective",
    "sklearn <0.23 does not have a stacking classifier and n_features_in_ property",
    "sklearn <0.23 does not have a stacking regressor and n_features_in_ property",
    "sklearn < 0.22 does not have the post fit attribute: classes_",
    "sklearn < 0.23 does not have as_frame parameter",
    "sklearn < 0.22 does not have the post fit attribute: classes_",
    "sklearn < 0.23 does not have as_frame parameter",
    "Test if random_state is properly stored",
    "Test if two random states produce identical models",
    "Test if subsequent fits sample from random_state object and produce different models",
    "Test that the largest element is NOT the same, the smallest can be the same, i.e. zero",
    "With default params",
    "Tests same probabilities",
    "Tests same predictions",
    "Tests same raw scores",
    "Tests same leaf indices",
    "Tests same feature contributions",
    "Tests other parameters for the prediction works",
    "Tests start_iteration",
    "Tests same probabilities, starting from iteration 10",
    "Tests same predictions, starting from iteration 10",
    "Tests same raw scores, starting from iteration 10",
    "Tests same leaf indices, starting from iteration 10",
    "Tests same feature contributions, starting from iteration 10",
    "Tests other parameters for the prediction works, starting from iteration 10",
    "no custom objective, no custom metric",
    "default metric",
    "non-default metric",
    "no metric",
    "non-default metric in eval_metric",
    "non-default metric with non-default metric in eval_metric",
    "non-default metric with multiple metrics in eval_metric",
    "non-default metric with multiple metrics in eval_metric for LGBMClassifier",
    "default metric for non-default objective",
    "non-default metric for non-default objective",
    "no metric",
    "non-default metric in eval_metric for non-default objective",
    "non-default metric with non-default metric in eval_metric for non-default objective",
    "non-default metric with multiple metrics in eval_metric for non-default objective",
    "custom objective, no custom metric",
    "default regression metric for custom objective",
    "non-default regression metric for custom objective",
    "multiple regression metrics for custom objective",
    "no metric",
    "default regression metric with non-default metric in eval_metric for custom objective",
    "non-default regression metric with metric in eval_metric for custom objective",
    "multiple regression metrics with metric in eval_metric for custom objective",
    "multiple regression metrics with multiple metrics in eval_metric for custom objective",
    "no custom objective, custom metric",
    "default metric with custom metric",
    "non-default metric with custom metric",
    "multiple metrics with custom metric",
    "custom metric (disable default metric)",
    "default metric for non-default objective with custom metric",
    "non-default metric for non-default objective with custom metric",
    "multiple metrics for non-default objective with custom metric",
    "custom metric (disable default metric for non-default objective)",
    "custom objective, custom metric",
    "custom metric for custom objective",
    "non-default regression metric with custom metric for custom objective",
    "multiple regression metrics with custom metric for custom objective",
    "default metric and invalid binary metric is replaced with multiclass alternative",
    "invalid objective is replaced with default multiclass one",
    "and invalid binary metric is replaced with multiclass alternative",
    "default metric for non-default multiclass objective",
    "and invalid binary metric is replaced with multiclass alternative",
    "default metric and invalid multiclass metric is replaced with binary alternative",
    "invalid multiclass metric is replaced with binary alternative for custom objective",
    "Verify that can receive a list of metrics, only callable",
    "Verify that can receive a list of custom and built-in metrics",
    "Verify that works as expected when eval_metric is empty",
    "Verify that can receive a list of metrics, only built-in",
    "Verify that eval_metric is robust to receiving a list with None",
    "training data as eval_set",
    "feval",
    "single eval_set",
    "two eval_set",
    "sklearn < 0.22 requires passing \"attributes\" argument",
    "Test that estimators are default-constructible",
    "coding: utf-8",
    "coding: utf-8",
    "check that default gives same result as k = 1",
    "check against independent calculation for k = 1",
    "check against independent calculation for k = 2",
    "check against independent calculation for k = 10",
    "check cases where predictions are equal",
    "should give same result as binary auc for 2 classes",
    "test the case where all predictions are equal",
    "test that weighted data gives different auc_mu",
    "test that equal data weights give same auc_mu as unweighted data",
    "should give 1 when accuracy = 1",
    "test loading class weights",
    "no early stopping",
    "early stopping occurs",
    "test custom eval metrics",
    "shuffle = False, override metric in params",
    "shuffle = True, callbacks",
    "enable display training loss",
    "self defined folds",
    "LambdaRank",
    "... with l2 metric",
    "... with NDCG (default) metric",
    "self defined folds with lambdarank",
    "with early stopping",
    "predict by each fold booster",
    "fold averaging",
    "without early stopping",
    "test feature_names with whitespaces",
    "This has non-ascii strings.",
    "take subsets and train",
    "generate CSR sparse dataset",
    "convert data to dense and get back same contribs",
    "validate the values are the same",
    "validate using CSC matrix",
    "validate the values are the same",
    "generate CSR sparse dataset",
    "convert data to dense and get back same contribs",
    "validate the values are the same",
    "validate using CSC matrix",
    "validate the values are the same",
    "Note there is an extra column added to the output for the expected value",
    "Note output CSC shape should be same as CSR output shape",
    "test sliced labels",
    "append some columns",
    "append some rows",
    "test sliced 2d matrix",
    "test sliced CSR",
    "trees start at position 1.",
    "split_features are in 4th line.",
    "test if a penalty as high as the depth indeed prohibits all monotone splits",
    "The penalization is so high that the first 2 features should not be used here",
    "Check that a very high penalization is the same as not using the features at all",
    "no fobj, no feval",
    "default metric",
    "non-default metric in params",
    "default metric in args",
    "non-default metric in args",
    "metric in args overwrites one in params",
    "multiple metrics in params",
    "multiple metrics in args",
    "remove default metric by 'None' in list",
    "remove default metric by 'None' aliases",
    "fobj, no feval",
    "no default metric",
    "metric in params",
    "metric in args",
    "metric in args overwrites its' alias in params",
    "multiple metrics in params",
    "multiple metrics in args",
    "no fobj, feval",
    "default metric with custom one",
    "non-default metric in params with custom one",
    "default metric in args with custom one",
    "non-default metric in args with custom one",
    "metric in args overwrites one in params, custom one is evaluated too",
    "multiple metrics in params with custom one",
    "multiple metrics in args with custom one",
    "custom metric is evaluated despite 'None' is passed",
    "fobj, feval",
    "no default metric, only custom one",
    "metric in params with custom one",
    "metric in args with custom one",
    "metric in args overwrites one in params, custom one is evaluated too",
    "multiple metrics in params with custom one",
    "multiple metrics in args with custom one",
    "custom metric is evaluated despite 'None' is passed",
    "no fobj, no feval",
    "default metric",
    "default metric in params",
    "non-default metric in params",
    "multiple metrics in params",
    "remove default metric by 'None' aliases",
    "fobj, no feval",
    "no default metric",
    "metric in params",
    "multiple metrics in params",
    "no fobj, feval",
    "default metric with custom one",
    "default metric in params with custom one",
    "non-default metric in params with custom one",
    "multiple metrics in params with custom one",
    "custom metric is evaluated despite 'None' is passed",
    "fobj, feval",
    "no default metric, only custom one",
    "metric in params with custom one",
    "multiple metrics in params with custom one",
    "custom metric is evaluated despite 'None' is passed",
    "multiclass default metric",
    "multiclass default metric with custom one",
    "multiclass metric alias with custom one for custom objective",
    "no metric for invalid class_num",
    "custom metric for invalid class_num",
    "multiclass metric alias with custom one with invalid class_num",
    "multiclass default metric without num_class",
    "multiclass metric alias",
    "multiclass metric",
    "non-valid metric for multiclass objective",
    "non-default num_class for default objective",
    "no metric with non-default num_class for custom objective",
    "multiclass metric alias for custom objective",
    "multiclass metric for custom objective",
    "binary metric with non-default num_class for custom objective",
    "Expect three metrics but mean and stdv for each metric",
    "test XGBoost-style return value",
    "test numpy-style return value",
    "test bins string type",
    "test histogram is disabled for categorical features",
    "test for lgb.train",
    "test feval for lgb.train",
    "test with two valid data for lgb.train",
    "test for lgb.cv",
    "test feval for lgb.cv",
    "test that binning works properly for features with only positive or only negative values",
    "decreasing without freeing raw data is allowed",
    "decreasing before lazy init is allowed",
    "increasing is allowed",
    "decreasing with disabled filter is allowed",
    "decreasing with enabled filter is disallowed;",
    "also changes of other params are disallowed",
    "check extra trees increases regularization",
    "check path smoothing increases regularization",
    "test edge case with one leaf",
    "check that constraint containing all features is equivalent to no constraint",
    "check that constraint partitioning the features reduces train accuracy",
    "check that constraints consisting of single features reduce accuracy further",
    "test that interaction constraints work when not all features are used",
    "check that setting linear_tree=True fits better than ordinary trees when data has linear relationship",
    "test again with nans in data",
    "test again with bagging",
    "test with a feature that has only one non-nan value",
    "test with a categorical feature",
    "test refit: same results on same data",
    "test refit with save and load",
    "test refit: different results training on different data",
    "test when num_leaves - 1 < num_features and when num_leaves - 1 > num_features",
    "test that the predict once with all iterations equals summed results with start_iteration and num_iteration",
    "test the case where start_iteration <= 0, and num_iteration is None",
    "test the case where start_iteration > 0, and num_iteration <= 0",
    "test the case where start_iteration > 0, and num_iteration <= 0, with pred_leaf=True",
    "test the case where start_iteration > 0, and num_iteration <= 0, with pred_contrib=True",
    "test for regression",
    "test both with and without early stopping",
    "test for multi-class",
    "test both with and without early stopping",
    "test for binary",
    "test both with and without early stopping",
    "test against sklearn average precision metric",
    "test that average precision is 1 where model predicts perfectly",
    "coding: utf-8",
    "If compiled appropriately, the same installation will support both GPU and CPU.",
    "coding: utf-8",
    "coding: utf-8",
    "These are helper functions to allow doing a stack unwind",
    "after an R allocation error, which would trigger a long jump.",
    "convert from one-based to zero-based index",
    "if any feature names were larger than allocated size,",
    "allow for a larger size and try again",
    "convert from boundaries to size",
    "--- start Booster interfaces",
    "if any eval names were larger than allocated size,",
    "allow for a larger size and try again",
    "if the model string was larger than the initial buffer, allocate a bigger buffer and try again",
    "if the model string was larger than the initial buffer, allocate a bigger buffer and try again",
    ".Call() calls",
    "coding: utf-8",
    "alias table",
    "names",
    "from strings",
    "tails",
    "tails",
    "coding: utf-8",
    "Single row predictor to abstract away caching logic",
    "create boosting",
    "initialize the boosting",
    "create objective function",
    "initialize the objective function",
    "create training metric",
    "reset the boosting",
    "create objective function",
    "initialize the objective function",
    "calculate the nonzero data and indices size",
    "allocate data and indices arrays",
    "Get the number of trees per iteration (for multiclass scenario we output multiple sparse matrices)",
    "aggregated per row feature contribution results",
    "keep track of the row_vector sizes for parallelization",
    "copy vector results to output for each row",
    "Get the number of trees per iteration (for multiclass scenario we output multiple sparse matrices)",
    "aggregated per row feature contribution results",
    "calculate number of elements per column to construct",
    "the CSC matrix with random access",
    "keep track of column counts",
    "keep track of beginning index for each column",
    "keep track of beginning index for each matrix",
    "Note: we parallelize across matrices instead of rows because of the column_counts[m][col_idx] increment inside the loop",
    "store the row index",
    "update column count",
    "explicitly declare symbols from LightGBM namespace",
    "some help functions used to convert data",
    "Row iterator of on column for CSC matrix",
    "return value at idx, only can access by ascent order",
    "return next non-zero pair, if index < 0, means no more data",
    "start of c_api functions",
    "This API is to keep python binding's behavior the same with C++ implementation.",
    "Sample count, random seed etc. should be provided in parameters.",
    "sample data first",
    "sample data first",
    "sample data first",
    "local buffer to re-use memory",
    "sample data first",
    "no more data",
    "---- start of booster",
    "Single row in row-major format:",
    "---- start of some help functions",
    "data is array of pointers to individual rows",
    "set number of threads for openmp",
    "check for alias",
    "read parameters from config file",
    "remove str after \"#\"",
    "check for alias again",
    "load configs",
    "prediction is needed if using input initial model(continued train)",
    "need to continue training",
    "sync up random seed for data partition",
    "load Training data",
    "load data for distributed training",
    "load data for single machine",
    "need save binary file",
    "create training metric",
    "only when have metrics then need to construct validation data",
    "Add validation data, if it exists",
    "add",
    "need save binary file",
    "add metric for validation data",
    "output used time on each iteration",
    "need init network",
    "create boosting",
    "create objective function",
    "load training data",
    "initialize the objective function",
    "initialize the boosting",
    "add validation data into boosting",
    "convert model to if-else statement code",
    "create predictor",
    "Free memory",
    "create predictor",
    "label_gain = 2^i - 1, may overflow, so we use 31 here",
    "counts for all labels",
    "start from top label, and accumulate DCG",
    "counts for all labels",
    "calculate k Max DCG by one pass",
    "get sorted indices by score",
    "calculate multi dcg by one pass",
    "wait for all client start up",
    "Don't call MPI_Finalize() here: If the destructor was called because only this node had an exception, calling MPI_Finalize() will cause all nodes to hang.",
    "Instead we will handle finalize/abort for MPI in main().",
    "default set to -1",
    "distance at k-th communication, distance[k] = 2^k",
    "set incoming rank at k-th commuication",
    "set outgoing rank at k-th commuication",
    "default set as -1",
    "construct all recursive halving map for all machines",
    "let 1 << k <= num_machines",
    "distance of each communication",
    "if num_machines = 2^k, don't need to group machines",
    "communication direction, %2 == 0 is positive",
    "neighbor at k-th communication",
    "receive data block at k-th communication",
    "send data block at k-th communication",
    "if num_machines != 2^k, need to group machines",
    "group, two machine in one group, total \"rest\" groups will have 2 machines.",
    "let left machine as group leader",
    "cache block information for groups, group with 2 machines will have double block size",
    "convert from group to node leader",
    "convert from node to group",
    "meet new group",
    "add block len for this group",
    "calculate the group block start",
    "not need to construct",
    "get receive block information",
    "accumulate block len",
    "get send block information",
    "accumulate block len",
    "static member definition",
    "if small package or small count , do it by all gather.(reduce the communication times.)",
    "assign the blocks to every rank.",
    "do reduce scatter",
    "do all gather",
    "assign blocks",
    "need use buffer here, since size of \"output\" is smaller than size after all gather",
    "copy back",
    "assign blocks",
    "start all gather",
    "when num_machines is small and data is large",
    "use output as receive buffer",
    "get current local block size",
    "get out rank",
    "get in rank",
    "get send information",
    "get recv information",
    "send and recv at same time",
    "rotate in-place",
    "use output as receive buffer",
    "get current local block size",
    "get send information",
    "get recv information",
    "send and recv at same time",
    "use output as receive buffer",
    "send and recv at same time",
    "send local data to neighbor first",
    "receive neighbor data first",
    "reduce",
    "get target",
    "get send information",
    "get recv information",
    "send and recv at same time",
    "reduce",
    "send result to neighbor",
    "receive result from neighbor",
    "copy result",
    "start up socket",
    "parse clients from file",
    "get ip list of local machine",
    "get local rank",
    "construct listener",
    "construct communication topo",
    "construct linkers",
    "free listener",
    "set timeout",
    "accept incoming socket",
    "receive rank",
    "add new socket",
    "save ranks that need to connect with",
    "start listener",
    "start connect",
    "let smaller rank connect to larger rank",
    "send local rank",
    "wait for listener",
    "print connected linkers",
    "only need to copy subset",
    "avoid to copy subset many times",
    "avoid out of range",
    "may need to recopy subset",
    "valid the type",
    "Constructors",
    "Get type tag",
    "Comparisons",
    "This has to be separate, not in Statics, because Json() accesses",
    "statics().null.",
    "advance until next line, or end of input",
    "advance until closing tokens",
    "The usual case: non-escaped characters",
    "Handle escapes",
    "Extract 4-byte escape sequence",
    "Explicitly check length of the substring. The following loop",
    "relies on std::string returning the terminating NUL when",
    "accessing str[length]. Checking here reduces brittleness.",
    "JSON specifies that characters outside the BMP shall be encoded as a",
    "pair of 4-hex-digit \\u escapes encoding their surrogate pair",
    "components. Check whether we're in the middle of such a beast: the",
    "previous codepoint was an escaped lead (high) surrogate, and this is",
    "a trail (low) surrogate.",
    "Reassemble the two surrogate pairs into one astral-plane character,",
    "per the UTF-16 algorithm.",
    "Integer part",
    "Decimal part",
    "Exponent part",
    "Check for any trailing garbage",
    "Documented in json11.hpp",
    "Check for another object",
    "get column names",
    "load label idx first",
    "erase label column name",
    "load ignore columns",
    "load weight idx",
    "load group idx",
    "don't support query id in data file when using distributed training",
    "read data to memory",
    "sample data",
    "construct feature bin mappers",
    "initialize label",
    "extract features",
    "sample data from file",
    "construct feature bin mappers",
    "initialize label",
    "extract features",
    "load data from binary file",
    "check meta data",
    "need to check training data",
    "read data in memory",
    "initialize label",
    "extract features",
    "Get number of lines of data file",
    "initialize label",
    "extract features",
    "load data from binary file",
    "not need to check validation data",
    "check meta data",
    "buffer to read binary file",
    "check token",
    "read size of header",
    "re-allocmate space if not enough",
    "read header",
    "get header",
    "num_groups",
    "real_feature_idx_",
    "feature2group",
    "feature2subfeature",
    "group_bin_boundaries",
    "group_feature_start_",
    "group_feature_cnt_",
    "get feature names",
    "write feature names",
    "get forced_bin_bounds_",
    "read size of meta data",
    "re-allocate space if not enough",
    "read meta data",
    "load meta data",
    "sample local used data if need to partition",
    "if not contain query file, minimal sample unit is one record",
    "if contain query file, minimal sample unit is one query",
    "if is new query",
    "read feature data",
    "read feature size",
    "re-allocate space if not enough",
    "raw data",
    "fill feature_names_ if not header",
    "get forced split",
    "if only one machine, find bin locally",
    "if have multi-machines, need to find bin distributed",
    "different machines will find bin for different features",
    "start and len will store the process feature indices for different machines",
    "machine i will find bins for features in [ start[i], start[i] + len[i] )",
    "free",
    "gather global feature bin mappers",
    "restore features bins from buffer",
    "---- private functions ----",
    "if features are ordered, not need to use hist_buf",
    "read all lines",
    "get query data",
    "if not contain query data, minimal sample unit is one record",
    "if contain query data, minimal sample unit is one query",
    "if is new query",
    "get query data",
    "if not contain query file, minimal sample unit is one record",
    "if contain query file, minimal sample unit is one query",
    "if is new query",
    "parse features",
    "get forced split",
    "check the range of label_idx, weight_idx and group_idx",
    "fill feature_names_ if not header",
    "start find bins",
    "if only one machine, find bin locally",
    "start and len will store the process feature indices for different machines",
    "machine i will find bins for features in [ start[i], start[i] + len[i] )",
    "free",
    "gather global feature bin mappers",
    "restore features bins from buffer",
    "if doesn't need to prediction with initial model",
    "parser",
    "set label",
    "free processed line:",
    "shrink_to_fit will be very slow in linux, and seems not free memory, disable for now",
    "text_reader_->Lines()[i].shrink_to_fit();",
    "push data",
    "if is used feature",
    "if need to prediction with initial model",
    "parser",
    "set initial score",
    "set label",
    "free processed line:",
    "shrink_to_fit will be very slow in Linux, and seems not free memory, disable for now",
    "text_reader_->Lines()[i].shrink_to_fit();",
    "push data",
    "if is used feature",
    "metadata_ will manage space of init_score",
    "text data can be free after loaded feature values",
    "parser",
    "set initial score",
    "set label",
    "push data",
    "if is used feature",
    "only need part of data",
    "need full data",
    "metadata_ will manage space of init_score",
    "read size of token",
    "remove duplicates",
    "deep copy function for BinMapper",
    "mean size for one bin",
    "need a new bin",
    "update bin upper bound",
    "last bin upper bound",
    "get list of distinct values",
    "get number of positive and negative distinct values",
    "include zero bounds and infinity bound",
    "add forced bounds, excluding zeros since we have already added zero bounds",
    "find remaining bounds",
    "find distinct_values first",
    "push zero in the front",
    "use the large value",
    "push zero in the back",
    "convert to int type first",
    "sort by counts",
    "will ignore the categorical of small counts",
    "Push the dummy bin for NaN",
    "Use MissingType::None to represent this bin contains all categoricals",
    "fix count of NaN bin",
    "check trivial(num_bin_ == 1) feature",
    "check useless bin",
    "When most_freq_bin_ != default_bin_, there are some additional data loading costs.",
    "so use most_freq_bin_  = default_bin_ when there is not so sparse",
    "calculate max bin of all features to select the int type in MultiValDenseBin",
    "for lambdarank, it needs query data for partition data in distributed learning",
    "need convert query_id to boundaries",
    "check weights",
    "check query boundries",
    "contain initial score file",
    "check weights",
    "get local weights",
    "check query boundries",
    "get local query boundaries",
    "contain initial score file",
    "get local initial scores",
    "re-load query weight",
    "save to nullptr",
    "save to nullptr",
    "save to nullptr",
    "default weight file name",
    "default init_score file name",
    "use first line to count number class",
    "default query file name",
    "root is in the depth 0",
    "non-leaf",
    "leaf",
    "use this for the missing value conversion",
    "Predict func by Map to ifelse",
    "use this for the missing value conversion",
    "non-leaf",
    "left subtree",
    "right subtree",
    "leaf",
    "non-leaf",
    "left subtree",
    "right subtree",
    "leaf",
    "recursive computation of SHAP values for a decision tree",
    "extend the unique path",
    "leaf node",
    "internal node",
    "see if we have already split on this feature,",
    "if so we undo that split so we can redo it for this node",
    "recursive sparse computation of SHAP values for a decision tree",
    "extend the unique path",
    "leaf node",
    "internal node",
    "see if we have already split on this feature,",
    "if so we undo that split so we can redo it for this node",
    "add names of objective function if not providing metric",
    "equal weights for all classes",
    "generate seeds by seed.",
    "sort eval_at",
    "Only push the non-training data",
    "check for conflicts",
    "check if objective, metric, and num_class match",
    "Change pool size to -1 (no limit) when using data parallel to reduce communication costs",
    "Check max_depth and num_leaves",
    "Fits in an int, and is more restrictive than the current num_leaves",
    "force col-wise for gpu & CUDA",
    "force gpu_use_dp for CUDA",
    "linear tree learner must be serial type and run on CPU device",
    "min_data_in_leaf must be at least 2 if path smoothing is active. This is because when the split is calculated",
    "the count is calculated using the proportion of hessian in the leaf which is rounded up to nearest int, so it can",
    "be 1 when there is actually no data in the leaf. In rare cases this can cause a bug because with path smoothing the",
    "calculated split gain can be positive even with zero gradient and hessian.",
    "In distributed mode, local node doesn't have histograms on all features, cannot perform \"intermediate\" monotone constraints.",
    "\"intermediate\" monotone constraints need to recompute splits. If the features are sampled when computing the",
    "split initially, then the sampling needs to be recorded or done once again, which is currently not supported",
    "first round: fill the single val group",
    "always push the last group",
    "put dense feature first",
    "sort by non zero cnt",
    "sort by non zero cnt, bigger first",
    "shuffle groups",
    "Using std::swap for vector<bool> will cause the wrong result.",
    "get num_features",
    "get bin_mappers",
    "for sparse multi value bin, we store the feature bin values with offset added",
    "for dense multi value bin, the feature bin values without offsets are used",
    "copy feature bin mapper data",
    "copy feature bin mapper data",
    "if not pass a filename, just append \".bin\" of original file",
    "get size of header",
    "size of feature names",
    "size of forced bins",
    "write header",
    "write feature names",
    "write forced bins",
    "get size of meta data",
    "write meta data",
    "write feature data",
    "get size of feature",
    "write feature",
    "write raw data; use row-major order so we can read row-by-row",
    "explicitly initialize template methods, for cross module call",
    "Only one multi-val group, just simply merge",
    "Skip the leading 0 when copying group_bin_boundaries.",
    "regenerate other fields",
    "store the importance first",
    "PredictRaw",
    "PredictRawByMap",
    "Predict",
    "PredictByMap",
    "PredictLeafIndex",
    "PredictLeafIndexByMap",
    "output model type",
    "output number of class",
    "output label index",
    "output max_feature_idx",
    "output objective",
    "output tree models",
    "store the importance first",
    "sort the importance",
    "use serialized string to restore this object",
    "Use first 128 chars to avoid exceed the message buffer.",
    "get number of classes",
    "get index of label",
    "get max_feature_idx first",
    "get average_output",
    "get feature names",
    "get monotone_constraints",
    "set zero",
    "predict all the trees for one iteration",
    "check early stopping",
    "set zero",
    "predict all the trees for one iteration",
    "check early stopping",
    "margin_threshold will be captured by value",
    "copy and sort",
    "margin_threshold will be captured by value",
    "Fix for compiler warnings about reaching end of control",
    "load forced_splits file",
    "init tree learner",
    "push training metrics",
    "create buffer for gradients and Hessians",
    "get max feature index",
    "get label index",
    "get feature names",
    "if need bagging, create buffer",
    "for a validation dataset, we need its score and metric",
    "update score",
    "objective function will calculate gradients and hessians",
    "random bagging, minimal unit is one record",
    "random bagging, minimal unit is one record",
    "if need bagging",
    "set bagging data to tree learner",
    "get subset",
    "output used time per iteration",
    "boosting from average label; or customized \"average\" if implemented for the current objective",
    "boosting first",
    "bagging logic",
    "need to copy gradients for bagging subset.",
    "shrinkage by learning rate",
    "update score",
    "only add default score one-time",
    "updates scores",
    "add model",
    "reset score",
    "remove model",
    "print message for metric",
    "pop last early_stopping_round_ models",
    "update training score",
    "we need to predict out-of-bag scores of data for boosting",
    "update validation score",
    "print training metric",
    "print validation metric",
    "set zero",
    "predict all the trees for one iteration",
    "predict all the trees for one iteration",
    "push training metrics",
    "not same training data, need reset score and others",
    "create score tracker",
    "update score",
    "create buffer for gradients and hessians",
    "load forced_splits file",
    "if need bagging, create buffer",
    "Get the max size of pool",
    "at least need 2 leaves",
    "push split information for all leaves",
    "initialize splits for leaf",
    "initialize data partition",
    "initialize ordered gradients and hessians",
    "cannot change is_hist_col_wise during training",
    "initialize splits for leaf",
    "initialize data partition",
    "initialize ordered gradients and hessians",
    "Get the max size of pool",
    "at least need 2 leaves",
    "push split information for all leaves",
    "some initial works before training",
    "root leaf",
    "only root leaf can be splitted on first time",
    "some initial works before finding best split",
    "find best threshold for every feature",
    "Get a leaf with max split gain",
    "Get split information for best leaf",
    "cannot split, quit",
    "split tree with best leaf",
    "reset histogram pool",
    "initialize data partition",
    "reset the splits for leaves",
    "Sumup for root",
    "use all data",
    "use bagging, only use part of data",
    "check depth of current leaf",
    "only need to check left leaf, since right leaf is in same level of left leaf",
    "no enough data to continue",
    "only have root",
    "put parent(left) leaf's histograms into larger leaf's histograms",
    "put parent(left) leaf's histograms to larger leaf's histograms",
    "construct smaller leaf",
    "construct larger leaf",
    "find splits",
    "only has root leaf",
    "start at root leaf",
    "before processing next node from queue, store info for current left/right leaf",
    "store \"best split\" for left and right, even if they might be overwritten by forced split",
    "then, compute own splits",
    "split info should exist because searching in bfs fashion - should have added from parent",
    "update before tree split",
    "don't need to update this in data-based parallel model",
    "split tree, will return right leaf",
    "store the true split gain in tree model",
    "don't need to update this in data-based parallel model",
    "store the true split gain in tree model",
    "init the leaves that used on next iteration",
    "update leave outputs if needed",
    "bag_mapper[index_mapper[i]]",
    "it is needed to filter the features after the above code.",
    "Otherwise, the `is_splittable` in `FeatureHistogram` will be wrong, and cause some features being accidentally filtered in the later nodes.",
    "for root leaf the \"parent\" output is its own output because we don't apply any smoothing to the root",
    "can't use GetParentOutput because leaf_splits doesn't have weight property set",
    "find splits",
    "identify features containing nans",
    "preallocate the matrix used to calculate linear model coefficients",
    "store only upper triangular half of matrix as an array, in row-major order",
    "this requires (max_num_feat + 1) * (max_num_feat + 2) / 2 entries (including the constant terms of the regression)",
    "we add another 8 to ensure cache lines are not shared among processors",
    "some initial works before training",
    "root leaf",
    "only root leaf can be splitted on first time",
    "some initial works before finding best split",
    "find best threshold for every feature",
    "Get a leaf with max split gain",
    "Get split information for best leaf",
    "cannot split, quit",
    "split tree with best leaf",
    "map data to leaf number",
    "calculate coefficients using the method described in Eq 3 of https://arxiv.org/pdf/1802.05640.pdf",
    "the coefficients vector is given by",
    "- (X_T * H * X + lambda) ^ (-1) * (X_T * g)",
    "where:",
    "X is the matrix where the first column is the feature values and the second is all ones,",
    "H is the diagonal matrix of the hessian,",
    "lambda is the diagonal matrix with diagonal entries equal to the regularisation term linear_lambda",
    "g is the vector of gradients",
    "the subscript _T denotes the transpose",
    "create array of pointers to raw data, and coefficient matrices, for each leaf",
    "clear the coefficient matrices",
    "aggregate results from different threads",
    "copy into eigen matrices and solve",
    "update the tree properties",
    "need to be able to hold smaller and larger best splits in SyncUpGlobalBestSplit",
    "get feature partition",
    "get local used features",
    "get best split at smaller leaf",
    "find local best split for larger leaf",
    "sync global best info",
    "update best split",
    "instantiate template classes, otherwise linker cannot find the code",
    "initialize SerialTreeLearner",
    "Get local rank and global machine size",
    "need to be able to hold smaller and larger best splits in SyncUpGlobalBestSplit",
    "allocate buffer for communication",
    "generate feature partition for current tree",
    "get local used feature",
    "get block start and block len for reduce scatter",
    "get buffer_write_start_pos_",
    "get buffer_read_start_pos_",
    "sync global data sumup info",
    "global sumup reduce",
    "copy back",
    "set global sumup info",
    "init global data count in leaf",
    "clear histogram buffer before synchronizing",
    "otherwise histogram contents from the previous iteration will be sent",
    "construct local histograms",
    "copy to buffer",
    "Reduce scatter for histogram",
    "restore global histograms from buffer",
    "only root leaf",
    "construct histgroms for large leaf, we init larger leaf as the parent, so we can just subtract the smaller leaf's histograms",
    "find local best split for larger leaf",
    "sync global best info",
    "set best split",
    "need update global number of data in leaf",
    "instantiate template classes, otherwise linker cannot find the code",
    "initialize SerialTreeLearner",
    "some additional variables needed for GPU trainer",
    "Initialize GPU buffers and kernels",
    "some functions used for debugging the GPU histogram construction",
    "printf(\"grad %g != %g (%d ULPs)\\n\", GET_GRAD(h1, i), GET_GRAD(h2, i), ulps);",
    "goto err;",
    "we roughly want 256 workgroups per device, and we have num_dense_feature4_ feature tuples.",
    "also guarantee that there are at least 2K examples per workgroup",
    "return 0;",
    "we have already copied ordered gradients, ordered Hessians and indices to GPU",
    "decide the best number of workgroups working on one feature4 tuple",
    "set work group size based on feature size",
    "each 2^exp_workgroups_per_feature workgroups work on a feature4 tuple",
    "we need to refresh the kernel arguments after reallocating",
    "The only argument that needs to be changed later is num_data_",
    "the GPU kernel will process all features in one call, and each",
    "2^exp_workgroups_per_feature (compile time constant) workgroup will",
    "process one feature4 tuple",
    "for the root node, indices are not copied",
    "for constant hessian, hessians are not copied except for the root node",
    "there will be 2^exp_workgroups_per_feature = num_workgroups / num_dense_feature4 sub-histogram per feature4",
    "and we will launch num_feature workgroups for this kernel",
    "will launch threads for all features",
    "the queue should be asynchronous, and we will can WaitAndGetHistograms() before we start processing dense feature groups",
    "copy the results asynchronously. Size depends on if double precision is used",
    "we will wait for this object in WaitAndGetHistograms",
    "when the output is ready, the computation is done",
    "values of this feature has been redistributed to multiple bins; need a reduction here",
    "how many feature-group tuples we have",
    "leave some safe margin for prefetching",
    "256 work-items per workgroup. Each work-item prefetches one tuple for that feature",
    "clear sparse/dense maps",
    "do nothing if no features can be processed on GPU",
    "allocate memory for all features (FIXME: 4 GB barrier on some devices, need to split to multiple buffers)",
    "unpin old buffer if necessary before destructing them",
    "make ordered_gradients and Hessians larger (including extra room for prefetching), and pin them",
    "allocate space for gradients and Hessians on device",
    "we will copy gradients and Hessians in after ordered_gradients_ and ordered_hessians_ are constructed",
    "allocate feature mask, for disabling some feature-groups' histogram calculation",
    "copy indices to the device",
    "histogram bin entry size depends on the precision (single/double)",
    "create output buffer, each feature has a histogram with device_bin_size_ bins,",
    "each work group generates a sub-histogram of dword_features_ features.",
    "only initialize once here, as this will not need to change when ResetTrainingData() is called",
    "create atomic counters for inter-group coordination",
    "The output buffer is allocated to host directly, to overlap compute and data transfer",
    "find the dense feature-groups and group then into Feature4 data structure (several feature-groups packed into 4 bytes)",
    "looking for dword_features_ non-sparse feature-groups",
    "decide if we need to redistribute the bin",
    "multiplier must be a power of 2",
    "device_bin_mults_.push_back(1);",
    "found",
    "for data transfer time",
    "Now generate new data structure feature4, and copy data to the device",
    "preallocate arrays for all threads, and pin them",
    "building Feature4 bundles; each thread handles dword_features_ features",
    "one feature datapoint is 4 bits",
    "this guarantees that the RawGet() function is inlined, rather than using virtual function dispatching",
    "one feature datapoint is one byte",
    "this guarantees that the RawGet() function is inlined, rather than using virtual function dispatching",
    "Dense bin",
    "Dense 4-bit bin",
    "working on the remaining (less than dword_features_) feature groups",
    "fill the leftover features",
    "fill this empty feature with some \"random\" value",
    "fill this empty feature with some \"random\" value",
    "copying the last 1 to (dword_features - 1) feature-groups in the last tuple",
    "deallocate pinned space for feature copying",
    "data transfer time",
    "for other types of failure, build log might not be available; program.build_log() can crash",
    "Something bad happened. Just return \"No log available.\"",
    "build is okay, log may contain warnings",
    "destroy any old kernels",
    "create OpenCL kernels for different number of workgroups per feature",
    "currently we don't use constant memory",
    "compile the GPU kernel depending if double precision is used, constant hessian is used, etc.",
    "kernel with indices in an array",
    "kernel with all features enabled, with eliminated branches",
    "kernel with all data indices (for root node, and assumes that root node always uses all features)",
    "do nothing if no features can be processed on GPU",
    "The only argument that needs to be changed later is num_data_",
    "hessian is passed as a parameter, but it is not available now.",
    "hessian will be set in BeforeTrain()",
    "Get the max bin size, used for selecting best GPU kernel",
    "initialize GPU",
    "determine which kernel to use based on the max number of bins",
    "the +9 skips extra characters \")\", newline, \"#endif\" and newline at the beginning",
    "the +9 skips extra characters \")\", newline, \"#endif\" and newline at the beginning",
    "the +9 skips extra characters \")\", newline, \"#endif\" and newline at the beginning",
    "setup GPU kernel arguments after we allocating all the buffers",
    "GPU memory has to been reallocated because data may have been changed",
    "setup GPU kernel arguments after we allocating all the buffers",
    "Copy initial full hessians and gradients to GPU.",
    "We start copying as early as possible, instead of at ConstructHistogram().",
    "setup hessian parameters only",
    "hessian is passed as a parameter",
    "use bagging",
    "On GPU, we start copying indices, gradients and Hessians now, instead at ConstructHistogram()",
    "copy used gradients and Hessians to ordered buffer",
    "transfer the indices to GPU",
    "transfer hessian to GPU",
    "setup hessian parameters only",
    "hessian is passed as a parameter",
    "transfer gradients to GPU",
    "only have root",
    "Copy indices, gradients and Hessians as early as possible",
    "only need to initialize for smaller leaf",
    "Get leaf boundary",
    "copy indices to the GPU:",
    "copy ordered Hessians to the GPU:",
    "copy ordered gradients to the GPU:",
    "do nothing if no features can be processed on GPU",
    "copy data indices if it is not null",
    "generate and copy ordered_gradients if gradients is not null",
    "generate and copy ordered_hessians if Hessians is not null",
    "converted indices in is_feature_used to feature-group indices",
    "construct the feature masks for dense feature-groups",
    "if no feature group is used, just return and do not use GPU",
    "if not all feature groups are used, we need to transfer the feature mask to GPU",
    "otherwise, we will use a specialized GPU kernel with all feature groups enabled",
    "All data have been prepared, now run the GPU kernel",
    "construct smaller leaf",
    "ConstructGPUHistogramsAsync will return true if there are available feature groups dispatched to GPU",
    "then construct sparse features on CPU",
    "wait for GPU to finish, only if GPU is actually used",
    "use double precision",
    "use single precision",
    "Compare GPU histogram with CPU histogram, useful for debugging GPU code problem",
    "#define GPU_DEBUG_COMPARE",
    "construct larger leaf",
    "then construct sparse features on CPU",
    "wait for GPU to finish, only if GPU is actually used",
    "use double precision",
    "use single precision",
    "do some sanity check for the GPU algorithm",
    "limit top k",
    "get max bin",
    "calculate buffer size",
    "need to be able to hold smaller and larger best splits in SyncUpGlobalBestSplit",
    "left and right on same time, so need double size",
    "initialize histograms for global",
    "sync global data sumup info",
    "set global sumup info",
    "init global data count in leaf",
    "get local sumup",
    "get local sumup",
    "get mean number on machines",
    "weighted gain",
    "get top k",
    "Copy histogram to buffer, and Get local aggregate features",
    "copy histograms.",
    "copy smaller leaf histograms first",
    "mark local aggregated feature",
    "copy",
    "then copy larger leaf histograms",
    "mark local aggregated feature",
    "copy",
    "use local data to find local best splits",
    "clear histogram buffer before synchronizing",
    "otherwise histogram contents from the previous iteration will be sent",
    "find splits",
    "only has root leaf",
    "local voting",
    "gather",
    "get all top-k from all machines",
    "global voting",
    "copy local histgrams to buffer",
    "Reduce scatter for histogram",
    "find best split from local aggregated histograms",
    "restore from buffer",
    "restore from buffer",
    "find local best",
    "find local best split for larger leaf",
    "sync global best info",
    "copy back",
    "set the global number of data for leaves",
    "init the global sumup info",
    "instantiate template classes, otherwise linker cannot find the code",
    "launch cuda kernel",
    "initialize SerialTreeLearner",
    "some additional variables needed for GPU trainer",
    "Initialize GPU buffers and kernels: get device info",
    "some functions used for debugging the GPU histogram construction",
    "we roughly want 256 workgroups per device, and we have num_dense_feature4_ feature tuples.",
    "also guarantee that there are at least 2K examples per workgroup",
    "we have already copied ordered gradients, ordered hessians and indices to GPU",
    "decide the best number of workgroups working on one feature4 tuple",
    "set work group size based on feature size",
    "each 2^exp_workgroups_per_feature workgroups work on a feature4 tuple",
    "set thread_data",
    "copy the results asynchronously. Size depends on if double precision is used",
    "when the output is ready, the computation is done",
    "how many feature-group tuples we have",
    "leave some safe margin for prefetching",
    "256 work-items per workgroup. Each work-item prefetches one tuple for that feature",
    "clear sparse/dense maps",
    "do nothing it there is no dense feature",
    "calculate number of feature groups per gpu",
    "histogram bin entry size depends on the precision (single/double)",
    "allocate GPU memory for each GPU",
    "do nothing it there is no gpu feature",
    "allocate memory for all features",
    "allocate space for gradients and hessians on device",
    "we will copy gradients and hessians in after ordered_gradients_ and ordered_hessians_ are constructed",
    "copy indices to the device",
    "create output buffer, each feature has a histogram with device_bin_size_ bins,",
    "each work group generates a sub-histogram of dword_features_ features.",
    "only initialize once here, as this will not need to change when ResetTrainingData() is called",
    "create atomic counters for inter-group coordination",
    "The output buffer is allocated to host directly, to overlap compute and data transfer",
    "clear sparse/dense maps",
    "find the dense feature-groups and group then into Feature4 data structure (several feature-groups packed into 4 bytes)",
    "set device info",
    "looking for dword_features_ non-sparse feature-groups",
    "reset device info",
    "InitGPU w/ num_gpu",
    "Get the max bin size, used for selecting best GPU kernel",
    "get num_dense_feature_groups_",
    "initialize GPU",
    "set cpu threads",
    "resize device memory pointers",
    "create stream & events to handle multiple GPUs",
    "check data size",
    "GPU memory has to been reallocated because data may have been changed",
    "AllocateGPUMemory only when the number of data increased",
    "setup GPU kernel arguments after we allocating all the buffers",
    "Copy initial full hessians and gradients to GPU.",
    "We start copying as early as possible, instead of at ConstructHistogram().",
    "use bagging",
    "On GPU, we start copying indices, gradients and hessians now, instead at ConstructHistogram()",
    "copy used gradients and hessians to ordered buffer",
    "transfer the indices to GPU",
    "only have root",
    "Copy indices, gradients and hessians as early as possible",
    "only need to initialize for smaller leaf",
    "Get leaf boundary",
    "do nothing if no features can be processed on GPU",
    "copy data indices if it is not null",
    "converted indices in is_feature_used to feature-group indices",
    "construct the feature masks for dense feature-groups",
    "if no feature group is used, just return and do not use GPU",
    "if not all feature groups are used, we need to transfer the feature mask to GPU",
    "otherwise, we will use a specialized GPU kernel with all feature groups enabled",
    "We now copy even if all features are used.",
    "All data have been prepared, now run the GPU kernel",
    "construct smaller leaf",
    "Check workgroups per feature4 tuple..",
    "if the workgroup per feature is 1 (2^0), return as the work is too small for a GPU",
    "ConstructGPUHistogramsAsync will return true if there are availabe feature groups dispatched to GPU",
    "then construct sparse features on CPU",
    "We set data_indices to null to avoid rebuilding ordered gradients/hessians",
    "wait for GPU to finish, only if GPU is actually used",
    "use double precision",
    "use single precision",
    "Compare GPU histogram with CPU histogram, useful for debuggin GPU code problem",
    "#define CUDA_DEBUG_COMPARE",
    "construct larger leaf",
    "then construct sparse features on CPU",
    "We set data_indices to null to avoid rebuilding ordered gradients/hessians",
    "wait for GPU to finish, only if GPU is actually used",
    "use double precision",
    "use single precision",
    "do some sanity check for the GPU algorithm"
  ],
  "v3.3.4": [
    "coding: utf-8",
    "coding: utf-8",
    "create predictor first",
    "show deprecation warning only for early stop argument, setting early stop via global params should still be possible",
    "check dataset",
    "reduce cost for prediction training data",
    "process callbacks",
    "Most of legacy advanced options becomes callbacks",
    "construct booster",
    "start training",
    "check evaluation result.",
    "ranking task, split according to groups",
    "run preprocessing on the data set if needed",
    "setup callbacks",
    "coding: utf-8",
    "dummy function to support older version of scikit-learn",
    "coding: utf-8",
    "documentation templates for LGBMModel methods are shared between the classes in",
    "this module and those in the ``dask`` module",
    "user can set verbose with kwargs, it has higher priority",
    "Do not modify original args in fit function",
    "Refer to https://github.com/microsoft/LightGBM/pull/2619",
    "Separate built-in from callable evaluation metrics",
    "register default metric for consistency with callable eval_metric case",
    "try to deduce from class instance",
    "overwrite default metric by explicitly set metric",
    "concatenate metric from params (or default if not provided in params) and eval_metric",
    "copy for consistency",
    "reduce cost for prediction training data",
    "free dataset",
    "Switch to using a multiclass objective in the underlying LGBM instance",
    "do not modify args, as it causes errors in model selection tools",
    "check group data",
    "coding: utf-8",
    "we don't need lib_lightgbm while building docs",
    "coding: utf-8",
    "\"#ddffdd\" is light green, \"#ffdddd\" is light red",
    "coding: utf-8",
    "coding: utf-8",
    "TypeError: obj is not a string or a number",
    "ValueError: invalid literal",
    "DeprecationWarning is not shown by default, so let's create our own with higher level",
    "avoid side effects on passed-in parameters",
    "find a value, and remove other aliases with .pop()",
    "prefer the value of 'main_param_name' if it exists, otherwise search the aliases",
    "Get total row number.",
    "Random access by row index. Used for data sampling.",
    "Range data access. Used to read data in batch when constructing Dataset.",
    "Optionally specify batch_size to control range data read size.",
    "Only required if using ``Dataset.subset()``.",
    "__get_num_preds() cannot work with nrow > MAX_INT32, so calculate overall number of predictions piecemeal",
    "avoid memory consumption by arrays concatenation operations",
    "create numpy array from output arrays",
    "break up indptr based on number of rows (note more than one matrix in multiclass case)",
    "for CSC there is extra column added",
    "reformat output into a csr or csc matrix or list of csr or csc matrices",
    "same shape as input csr or csc matrix except extra column for expected value",
    "note: make sure we copy data as it will be deallocated next",
    "free the temporary native indptr, indices, and data",
    "__get_num_preds() cannot work with nrow > MAX_INT32, so calculate overall number of predictions piecemeal",
    "avoid memory consumption by arrays concatenation operations",
    "c type: double**",
    "each double* element points to start of each column of sample data.",
    "c type int**",
    "each int* points to start of indices for each column",
    "no min_data, nthreads and verbose in this function",
    "check data has header or not",
    "need to regroup init_score",
    "process for args",
    "user can set verbose with params, it has higher priority",
    "get categorical features",
    "process for reference dataset",
    "start construct data",
    "set feature names",
    "Select sampled rows, transpose to column order.",
    "create validation dataset from ref_dataset",
    "create valid",
    "construct subset",
    "create train",
    "could be updated if data is not freed",
    "set to None",
    "we're done if self and reference share a common upstream reference",
    "if buffer length is not long enough, reallocate buffers",
    "group data from LightGBM is boundaries data, need to convert to group size",
    "user can set verbose with params, it has higher priority",
    "Training task",
    "if \"machines\" is given, assume user wants to do distributed learning, and set up network",
    "construct booster object",
    "copy the parameters from train_set",
    "save reference to data",
    "buffer for inner predict",
    "Prediction task",
    "if a single node tree it won't have `leaf_index` so return 0",
    "Create the node record, and populate universal data members",
    "Update values to reflect node type (leaf or split)",
    "traverse the next level of the tree",
    "In tree format, \"subtree_list\" is a list of node records (dicts),",
    "and we add node to the list.",
    "need reset training data",
    "need to push new valid data",
    "if buffer length is not long enough, re-allocate a buffer",
    "if buffer length is not long enough, reallocate a buffer",
    "Copy models",
    "Get name of features",
    "if buffer length is not long enough, reallocate buffers",
    "avoid to predict many time in one iteration",
    "Get num of inner evals",
    "Get name of eval metrics",
    "if buffer length is not long enough, reallocate buffers",
    "coding: utf-8",
    "Callback environment used by callbacks",
    "split is needed for \"<dataset type> <metric>\" case (e.g. \"train l1\")",
    "split is needed for \"<dataset type> <metric>\" case (e.g. \"train l1\")",
    "coding: utf-8",
    "Concatenate many parts into one",
    "construct local eval_set data.",
    "store indices of eval_set components that were not contained within local parts.",
    "consolidate parts of each individual eval component.",
    "require that eval_name exists in evaluated result data in case dropped due to padding.",
    "in distributed training the 'training' eval_set is not detected, will have name 'valid_<index>'.",
    "filter padding from eval parts then _concat each eval_set component.",
    "reconstruct eval_set fit args/kwargs depending on which components of eval_set are on worker.",
    "ensure that expected keys for evals_result_ and best_score_ exist regardless of padding.",
    "capture whether local_listen_port or its aliases were provided",
    "capture whether machines or its aliases were provided",
    "Some passed-in parameters can be removed:",
    "* 'num_machines': set automatically from Dask worker list",
    "* 'num_threads': overridden to match nthreads on each Dask process",
    "Split arrays/dataframes into parts. Arrange parts into dicts to enforce co-locality",
    "evals_set will to be re-constructed into smaller lists of (X, y) tuples, where",
    "X and y are each delayed sub-lists of original eval dask Collections.",
    "find maximum number of parts in an individual eval set so that we can",
    "pad eval sets when they come in different sizes.",
    "when individual eval set is equivalent to training data, skip recomputing parts.",
    "add None-padding for individual eval_set member if it is smaller than the largest member.",
    "first time a chunk of this eval set is added to this part.",
    "append additional chunks of this eval set to this part.",
    "ensure that all evaluation parts map uniquely to one part.",
    "assign sub-eval_set components to worker parts.",
    "Start computation in the background",
    "Find locations of all parts and map them to particular Dask workers",
    "Check that all workers were provided some of eval_set. Otherwise warn user that validation",
    "data artifacts may not be populated depending on worker returning final estimator.",
    "assign general validation set settings to fit kwargs.",
    "resolve aliases for network parameters and pop the result off params.",
    "these values are added back in calls to `_train_part()`",
    "figure out network params",
    "Tell each worker to train on the parts that it has locally",
    "",
    "This code treats ``_train_part()`` calls as not \"pure\" because:",
    "1. there is randomness in the training process unless parameters ``seed``",
    "and ``deterministic`` are set",
    "2. even with those parameters set, the output of one ``_train_part()`` call",
    "relies on global state (it and all the other LightGBM training processes",
    "coordinate with each other)",
    "if network parameters were changed during training, remove them from the",
    "returned model so that they're generated dynamically on every run based",
    "on the Dask cluster you're connected to and which workers have pieces of",
    "the training data",
    "dask.DataFrame.map_partitions() expects each call to return a pandas DataFrame or Series",
    "for multi-class classification with sparse matrices, pred_contrib predictions",
    "are returned as a list of sparse matrices (one per class)",
    "pred_contrib output will have one column per feature,",
    "plus one more for the base value",
    "need to tell Dask the expected type and shape of individual preds",
    "by default, dask.array.concatenate() concatenates sparse arrays into a COO matrix",
    "the code below is used instead to ensure that the sparse type is preserved during concatentation",
    "At this point, `out` is a list of lists of delayeds (each of which points to a matrix).",
    "Concatenate them to return a list of Dask Arrays.",
    "the note on custom objective functions in LGBMModel.__init__ is not",
    "currently relevant for the Dask estimators",
    "DaskLGBMClassifier does not support group, eval_group, early_stopping_rounds.",
    "DaskLGBMClassifier support for callbacks and init_model is not tested",
    "the note on custom objective functions in LGBMModel.__init__ is not",
    "currently relevant for the Dask estimators",
    "DaskLGBMRegressor does not support group, eval_class_weight, eval_group, early_stopping_rounds.",
    "DaskLGBMRegressor support for callbacks and init_model is not tested",
    "the note on custom objective functions in LGBMModel.__init__ is not",
    "currently relevant for the Dask estimators",
    "DaskLGBMRanker does not support eval_class_weight or early stopping",
    "DaskLGBMRanker support for callbacks and init_model is not tested",
    "coding: utf-8",
    "load or create your dataset",
    "create dataset for lightgbm",
    "if you want to re-use data, remember to set free_raw_data=False",
    "specify your configurations as a dict",
    "generate feature names",
    "feature_name and categorical_feature",
    "check feature name",
    "save model to file",
    "dump model to JSON (and save to file)",
    "feature names",
    "feature importances",
    "load model to predict",
    "can only predict with the best iteration (or the saving iteration)",
    "eval with loaded model",
    "dump model with pickle",
    "load model with pickle to predict",
    "can predict with any iteration when loaded in pickle way",
    "eval with loaded model",
    "continue training",
    "init_model accepts:",
    "1. model file name",
    "2. Booster()",
    "decay learning rates",
    "learning_rates accepts:",
    "1. list/tuple with length = num_boost_round",
    "2. function(curr_iter)",
    "change other parameters during training",
    "self-defined objective function",
    "f(preds: array, train_data: Dataset) -> grad: array, hess: array",
    "log likelihood loss",
    "self-defined eval metric",
    "f(preds: array, train_data: Dataset) -> name: str, eval_result: float, is_higher_better: bool",
    "binary error",
    "NOTE: when you do customized loss function, the default prediction value is margin",
    "This may make built-in evaluation metric calculate wrong results",
    "For example, we are doing log likelihood loss, the prediction is score before logistic transformation",
    "Keep this in mind when you use the customization",
    "another self-defined eval metric",
    "f(preds: array, train_data: Dataset) -> name: str, eval_result: float, is_higher_better: bool",
    "accuracy",
    "NOTE: when you do customized loss function, the default prediction value is margin",
    "This may make built-in evaluation metric calculate wrong results",
    "For example, we are doing log likelihood loss, the prediction is score before logistic transformation",
    "Keep this in mind when you use the customization",
    "callback",
    "coding: utf-8",
    "load or create your dataset",
    "train",
    "predict",
    "eval",
    "feature importances",
    "self-defined eval metric",
    "f(y_true: array, y_pred: array) -> name: str, eval_result: float, is_higher_better: bool",
    "Root Mean Squared Logarithmic Error (RMSLE)",
    "train",
    "another self-defined eval metric",
    "f(y_true: array, y_pred: array) -> name: str, eval_result: float, is_higher_better: bool",
    "Relative Absolute Error (RAE)",
    "train",
    "predict",
    "eval",
    "other scikit-learn modules",
    "coding: utf-8",
    "load or create your dataset",
    "create dataset for lightgbm",
    "specify your configurations as a dict",
    "train",
    "coding: utf-8",
    "################",
    "Simulate some binary data with a single categorical and",
    "single continuous predictor",
    "################",
    "Set up a couple of utilities for our experiments",
    "################",
    "Observe the behavior of `binary` and `xentropy` objectives",
    "Trying this throws an error on non-binary values of y:",
    "experiment('binary', label_type='probability', DATA)",
    "The speed of `binary` is not drastically different than",
    "`xentropy`. `xentropy` runs faster than `binary` in many cases, although",
    "there are reasons to suspect that `binary` should run faster when the",
    "label is an integer instead of a float",
    "coding: utf-8",
    "load or create your dataset",
    "create dataset for lightgbm",
    "specify your configurations as a dict",
    "train",
    "save model to file",
    "predict",
    "eval",
    "We can also open HDF5 file once and get access to",
    "With binary dataset created, we can use either Python API or cmdline version to train.",
    "",
    "Note: in order to create exactly the same dataset with the one created in simple_example.py, we need",
    "to modify simple_example.py to pass numpy array instead of pandas DataFrame to Dataset constructor.",
    "The reason is that DataFrame column names will be used in Dataset. For a DataFrame with Int64Index",
    "as columns, Dataset will use column names like [\"0\", \"1\", \"2\", ...]. While for numpy array, column names",
    "are using the default one assigned in C++ code (dataset_loader.cpp), like [\"Column_0\", \"Column_1\", ...].",
    "Y has a single column and we read it in single shot. So store it as an 1-d array.",
    "We use random access for data sampling when creating LightGBM Dataset from Sequence.",
    "When accessing any element in a HDF5 chunk, it's read entirely.",
    "To save I/O for sampling, we should keep number of total chunks much larger than sample count.",
    "Here we are just creating a chunk size that matches with batch_size.",
    "",
    "Also note that the data is stored in row major order to avoid extra copy when passing to",
    "lightgbm Dataset.",
    "Save to 2 HDF5 files for demonstration.",
    "We can store multiple datasets inside a single HDF5 file.",
    "Separating X and Y for choosing best chunk size for data loading.",
    "split training data into two partitions",
    "make this array dense because we're splitting across",
    "a sparse boundary to partition the data",
    "the code below uses sklearn.metrics, but this requires pulling all of the",
    "predictions and target values back from workers to the client",
    "",
    "for larger datasets, consider the metrics from dask-ml instead",
    "https://ml.dask.org/modules/api.html#dask-ml-metrics-metrics",
    "coding: utf-8",
    "!/usr/bin/env python3",
    "-*- coding: utf-8 -*-",
    "",
    "LightGBM documentation build configuration file, created by",
    "sphinx-quickstart on Thu May  4 14:30:58 2017.",
    "",
    "This file is execfile()d with the current directory set to its",
    "containing dir.",
    "",
    "Note that not all possible configuration values are present in this",
    "autogenerated file.",
    "",
    "All configuration values have a default; values that are commented out",
    "serve to show the default.",
    "If extensions (or modules to document with autodoc) are in another directory,",
    "add these directories to sys.path here. If the directory is relative to the",
    "documentation root, use os.path.abspath to make it absolute.",
    "-- General configuration ------------------------------------------------",
    "If your documentation needs a minimal Sphinx version, state it here.",
    "Add any Sphinx extension module names here, as strings. They can be",
    "extensions coming with Sphinx (named 'sphinx.ext.*') or your custom",
    "ones.",
    "mock out modules",
    "hide type hints in API docs",
    "Generate autosummary pages. Output should be set with: `:toctree: pythonapi/`",
    "Only the class' docstring is inserted.",
    "If true, `todo` and `todoList` produce output, else they produce nothing.",
    "The master toctree document.",
    "General information about the project.",
    "The name of an image file (relative to this directory) to place at the top",
    "of the sidebar.",
    "The name of an image file (relative to this directory) to use as a favicon of",
    "the docs.  This file should be a Windows icon file (.ico) being 16x16 or 32x32",
    "pixels large.",
    "The version info for the project you're documenting, acts as replacement for",
    "|version| and |release|, also used in various other places throughout the",
    "built documents.",
    "The short X.Y version.",
    "The full version, including alpha/beta/rc tags.",
    "The language for content autogenerated by Sphinx. Refer to documentation",
    "for a list of supported languages.",
    "",
    "This is also used if you do content translation via gettext catalogs.",
    "Usually you set \"language\" from the command line for these cases.",
    "List of patterns, relative to source directory, that match files and",
    "directories to ignore when looking for source files.",
    "This patterns also effect to html_static_path and html_extra_path",
    "The name of the Pygments (syntax highlighting) style to use.",
    "-- Configuration for C API docs generation ------------------------------",
    "-- Options for HTML output ----------------------------------------------",
    "The theme to use for HTML and HTML Help pages.  See the documentation for",
    "a list of builtin themes.",
    "Theme options are theme-specific and customize the look and feel of a theme",
    "further.  For a list of options available for each theme, see the",
    "documentation.",
    "Add any paths that contain custom static files (such as style sheets) here,",
    "relative to this directory. They are copied after the builtin static files,",
    "so a file named \"default.css\" will overwrite the builtin \"default.css\".",
    "-- Options for HTMLHelp output ------------------------------------------",
    "Output file base name for HTML help builder.",
    "-- Options for LaTeX output ---------------------------------------------",
    "The name of an image file (relative to this directory) to place at the top of",
    "the title page.",
    "Warning! The following code can cause buffer overflows on RTD.",
    "Consider suppressing output completely if RTD project silently fails.",
    "Refer to https://github.com/svenevs/exhale",
    "/blob/fe7644829057af622e467bb529db6c03a830da99/exhale/deploy.py#L99-L111",
    "Warning! The following code can cause buffer overflows on RTD.",
    "Consider suppressing output completely if RTD project silently fails.",
    "Refer to https://github.com/svenevs/exhale",
    "/blob/fe7644829057af622e467bb529db6c03a830da99/exhale/deploy.py#L99-L111",
    "coding: utf-8",
    "This is a basic test for floating number parsing.",
    "Most of the test cases come from:",
    "https://github.com/dmlc/xgboost/blob/master/tests/cpp/common/test_charconv.cc",
    "https://github.com/Alexhuszagh/rust-lexical/blob/master/data/test-parse-unittests/strtod_tests.toml",
    "FLT_MAX",
    "FLT_MIN",
    "DBL_MAX (1 + (1 - 2^-52)) * 2^1023 = (2^53 - 1) * 2^971",
    "2^971 * (2^53 - 1 + 1/2) : the smallest number resolving to inf",
    "Near DBL_MIN",
    "DBL_MIN 2^-1022",
    "The behavior for parsing -nan depends on implementation.",
    "Thus we skip binary check for negative nan.",
    "See comment in test_cases.",
    "Constants",
    "Start with some content:",
    "Clear & re-use:",
    "Output should match new content:",
    "Number of trials for each new ChunkedArray configuration. Pass 100 times over the search space:",
    "Each outer loop iteration changes the test by adding +1 chunk. We start with 1 chunk only:",
    "Sweep valid and invalid addresses with a ChunkedArray with `chunks` chunks:",
    "Compute a new trial address & value & if it is a valid address:",
    "Insert item. If at a valid address, 0 is returned, otherwise, -1 is returned:",
    "If at valid address, check that the stored value is correct & remember it for the future:",
    "Check the just-stored value with getitem():",
    "Also store the just-stored value for future tracking:",
    "Final check: ensure even with overrides, all valid insertions store the latest value at that address:",
    "Test in 2 ways that the values are correctly laid out in memory:",
    "coding: utf-8",
    "we don't need lib_lightgbm while building docs",
    "coding: utf-8",
    "check saved model persistence",
    "we need to check the consistency of model file here, so test for exact equal",
    "check early stopping is working. Make it stop very early, so the scores should be very close to zero",
    "scores likely to be different, but prediction should still be the same",
    "test that shape is checked during prediction",
    "The simple implementation is just a single \"return self.ndarray[idx]\"",
    "The following is for demo and testing purpose.",
    "whole col",
    "half col",
    "Create dataset from numpy array directly.",
    "Create dataset using Sequence.",
    "Test for validation set.",
    "Select some random rows as valid data.",
    "From Dataset constructor, with dataset from numpy array.",
    "From Dataset.create_valid, with dataset from sequence.",
    "test that method works even with free_raw_data=True",
    "test that method works but sets raw data to None in case of immergeable data types",
    "test that method works for different data types",
    "Set extremely harsh penalties, so CEGB will block most splits.",
    "Compare pairs of penalties, to ensure scaling works as intended",
    "Reset booster1's parameters to p2, so the parameter section of the file matches.",
    "should resolve duplicate aliases, and prefer the main parameter",
    "should choose a value from an alias and set that value on main param",
    "if only an alias is used",
    "should use the default if main param and aliases are missing",
    "all changes should be made on copies and not modify the original",
    "coding: utf-8",
    "add target, weight, and group to DataFrame so that partitions abide by group boundaries.",
    "set_index ensures partitions are based on group id.",
    "See https://stackoverflow.com/questions/49532824/dask-dataframe-split-partitions-based-on-a-column-or-function.",
    "separate target, weight from features.",
    "encode group identifiers into run-length encoding, the format LightGBMRanker is expecting",
    "so that within each partition, sum(g) = n_samples.",
    "ranking arrays: one chunk per group. Each chunk must include all columns.",
    "make one categorical feature relevant to the target",
    "https://github.com/microsoft/LightGBM/issues/4118",
    "extra predict() parameters should be passed through correctly",
    "pref_leaf values should have the right shape",
    "and values that look like valid tree nodes",
    "be sure LightGBM actually used at least one categorical column,",
    "and that it was correctly treated as a categorical feature",
    "shape depends on whether it is binary or multiclass classification",
    "in the special case of multi-class classification using scipy sparse matrices,",
    "the output of `.predict(..., pred_contrib=True)` is a list of sparse matrices (one per class)",
    "",
    "since that case is so different than all other cases, check the relevant things here",
    "and then return early",
    "raw scores will probably be different, but at least check that all predicted classes are the same",
    "be sure LightGBM actually used at least one categorical column,",
    "and that it was correctly treated as a categorical feature",
    "* shape depends on whether it is binary or multiclass classification",
    "* matrix for binary classification is of the form [feature_contrib, base_value],",
    "for multi-class it's [feat_contrib_class1, base_value_class1, feat_contrib_class2, base_value_class2, etc.]",
    "* contrib outputs for distributed training are different than from local training, so we can just test",
    "that the output has the right shape and base values are in the right position",
    "check that found ports are different for same address (LocalCluster)",
    "check that the ports are indeed open",
    "Scores should be the same",
    "Predictions should be roughly the same.",
    "pref_leaf values should have the right shape",
    "and values that look like valid tree nodes",
    "extra predict() parameters should be passed through correctly",
    "be sure LightGBM actually used at least one categorical column,",
    "and that it was correctly treated as a categorical feature",
    "contrib outputs for distributed training are different than from local training, so we can just test",
    "that the output has the right shape and base values are in the right position",
    "be sure LightGBM actually used at least one categorical column,",
    "and that it was correctly treated as a categorical feature",
    "Quantiles should be right",
    "be sure LightGBM actually used at least one categorical column,",
    "and that it was correctly treated as a categorical feature",
    "rebalance small dask.Array dataset for better performance.",
    "use many trees + leaves to overfit, help ensure that Dask data-parallel strategy matches that of",
    "serial learner. See https://github.com/microsoft/LightGBM/issues/3292#issuecomment-671288210.",
    "distributed ranker should be able to rank decently well and should",
    "have high rank correlation with scores from serial ranker.",
    "extra predict() parameters should be passed through correctly",
    "pref_leaf values should have the right shape",
    "and values that look like valid tree nodes",
    "be sure LightGBM actually used at least one categorical column,",
    "and that it was correctly treated as a categorical feature",
    "Use larger trainset to prevent premature stopping due to zero loss, causing num_trees() < n_estimators.",
    "Use small chunk_size to avoid single-worker allocation of eval data partitions.",
    "test eval_class_weight, eval_init_score on binary-classification task.",
    "Note: objective's default `metric` will be evaluated in evals_result_ in addition to all eval_metrics.",
    "create eval_sets by creating new datasets or copying training data.",
    "total number of trees scales up for ova classifier.",
    "check that early stopping was not applied.",
    "checks that evals_result_ and best_score_ contain expected data and eval_set names.",
    "check that each eval_name and metric exists for all eval sets, allowing for the",
    "case when a worker receives a fully-padded eval_set component which is not evaluated.",
    "should be able to use the class without specifying a client",
    "should be able to set client after construction",
    "data on cluster1",
    "create identical data on cluster2",
    "at this point, the result of default_client() is client2 since it was the most recently",
    "created. So setting client to client1 here to test that you can select a non-default client",
    "unfitted model should survive pickling round trip, and pickling",
    "shouldn't have side effects on the model object",
    "client will always be None after unpickling",
    "fitted model should survive pickling round trip, and pickling",
    "shouldn't have side effects on the model object",
    "client will always be None after unpickling",
    "rebalance data to be sure that each worker has a piece of the data",
    "model 1 - no network parameters given",
    "model 2 - machines given",
    "model 3 - local_listen_port given",
    "training should fail because LightGBM will try to use the same",
    "port for multiple worker processes on the same machine",
    "rebalance data to be sure that each worker has a piece of the data",
    "test that \"machines\" is actually respected by creating a socket that uses",
    "one of the ports mentioned in \"machines\"",
    "The above error leaves a worker waiting",
    "an informative error should be raised if \"machines\" has duplicates",
    "\"client\" should be the only different, and the final argument",
    "value of the root node is 0 when init_score is set",
    "this test is separate because it takes a not-yet-constructed estimator",
    "coding: utf-8",
    "coding: utf-8",
    "build target, group ID vectors.",
    "build y/target and group-id vectors with user-specified group sizes.",
    "build y/target and group-id vectors according to n_samples, avg_gs, and random_gs.",
    "groups should contain > 1 element for pairwise learning objective.",
    "build feature data, X. Transform first few into informative features.",
    "coding: utf-8",
    "prediction result is actually not transformed (is raw) due to custom objective",
    "sklearn <0.23 does not have a stacking classifier and n_features_in_ property",
    "sklearn <0.23 does not have a stacking regressor and n_features_in_ property",
    "sklearn < 0.22 does not have the post fit attribute: classes_",
    "sklearn < 0.23 does not have as_frame parameter",
    "sklearn < 0.22 does not have the post fit attribute: classes_",
    "sklearn < 0.23 does not have as_frame parameter",
    "Test if random_state is properly stored",
    "Test if two random states produce identical models",
    "Test if subsequent fits sample from random_state object and produce different models",
    "Test that the largest element is NOT the same, the smallest can be the same, i.e. zero",
    "With default params",
    "Tests same probabilities",
    "Tests same predictions",
    "Tests same raw scores",
    "Tests same leaf indices",
    "Tests same feature contributions",
    "Tests other parameters for the prediction works",
    "Tests start_iteration",
    "Tests same probabilities, starting from iteration 10",
    "Tests same predictions, starting from iteration 10",
    "Tests same raw scores, starting from iteration 10",
    "Tests same leaf indices, starting from iteration 10",
    "Tests same feature contributions, starting from iteration 10",
    "Tests other parameters for the prediction works, starting from iteration 10",
    "no custom objective, no custom metric",
    "default metric",
    "non-default metric",
    "no metric",
    "non-default metric in eval_metric",
    "non-default metric with non-default metric in eval_metric",
    "non-default metric with multiple metrics in eval_metric",
    "non-default metric with multiple metrics in eval_metric for LGBMClassifier",
    "default metric for non-default objective",
    "non-default metric for non-default objective",
    "no metric",
    "non-default metric in eval_metric for non-default objective",
    "non-default metric with non-default metric in eval_metric for non-default objective",
    "non-default metric with multiple metrics in eval_metric for non-default objective",
    "custom objective, no custom metric",
    "default regression metric for custom objective",
    "non-default regression metric for custom objective",
    "multiple regression metrics for custom objective",
    "no metric",
    "default regression metric with non-default metric in eval_metric for custom objective",
    "non-default regression metric with metric in eval_metric for custom objective",
    "multiple regression metrics with metric in eval_metric for custom objective",
    "multiple regression metrics with multiple metrics in eval_metric for custom objective",
    "no custom objective, custom metric",
    "default metric with custom metric",
    "non-default metric with custom metric",
    "multiple metrics with custom metric",
    "custom metric (disable default metric)",
    "default metric for non-default objective with custom metric",
    "non-default metric for non-default objective with custom metric",
    "multiple metrics for non-default objective with custom metric",
    "custom metric (disable default metric for non-default objective)",
    "custom objective, custom metric",
    "custom metric for custom objective",
    "non-default regression metric with custom metric for custom objective",
    "multiple regression metrics with custom metric for custom objective",
    "default metric and invalid binary metric is replaced with multiclass alternative",
    "invalid objective is replaced with default multiclass one",
    "and invalid binary metric is replaced with multiclass alternative",
    "default metric for non-default multiclass objective",
    "and invalid binary metric is replaced with multiclass alternative",
    "default metric and invalid multiclass metric is replaced with binary alternative",
    "invalid multiclass metric is replaced with binary alternative for custom objective",
    "Verify that can receive a list of metrics, only callable",
    "Verify that can receive a list of custom and built-in metrics",
    "Verify that works as expected when eval_metric is empty",
    "Verify that can receive a list of metrics, only built-in",
    "Verify that eval_metric is robust to receiving a list with None",
    "training data as eval_set",
    "feval",
    "single eval_set",
    "two eval_set",
    "sklearn < 0.22 requires passing \"attributes\" argument",
    "Test that estimators are default-constructible",
    "coding: utf-8",
    "coding: utf-8",
    "check that default gives same result as k = 1",
    "check against independent calculation for k = 1",
    "check against independent calculation for k = 2",
    "check against independent calculation for k = 10",
    "check cases where predictions are equal",
    "should give same result as binary auc for 2 classes",
    "test the case where all predictions are equal",
    "test that weighted data gives different auc_mu",
    "test that equal data weights give same auc_mu as unweighted data",
    "should give 1 when accuracy = 1",
    "test loading class weights",
    "no early stopping",
    "early stopping occurs",
    "test custom eval metrics",
    "shuffle = False, override metric in params",
    "shuffle = True, callbacks",
    "enable display training loss",
    "self defined folds",
    "LambdaRank",
    "... with l2 metric",
    "... with NDCG (default) metric",
    "self defined folds with lambdarank",
    "with early stopping",
    "predict by each fold booster",
    "fold averaging",
    "without early stopping",
    "test feature_names with whitespaces",
    "This has non-ascii strings.",
    "take subsets and train",
    "generate CSR sparse dataset",
    "convert data to dense and get back same contribs",
    "validate the values are the same",
    "validate using CSC matrix",
    "validate the values are the same",
    "generate CSR sparse dataset",
    "convert data to dense and get back same contribs",
    "validate the values are the same",
    "validate using CSC matrix",
    "validate the values are the same",
    "Note there is an extra column added to the output for the expected value",
    "Note output CSC shape should be same as CSR output shape",
    "test sliced labels",
    "append some columns",
    "append some rows",
    "test sliced 2d matrix",
    "test sliced CSR",
    "trees start at position 1.",
    "split_features are in 4th line.",
    "test if a penalty as high as the depth indeed prohibits all monotone splits",
    "The penalization is so high that the first 2 features should not be used here",
    "Check that a very high penalization is the same as not using the features at all",
    "no fobj, no feval",
    "default metric",
    "non-default metric in params",
    "default metric in args",
    "non-default metric in args",
    "metric in args overwrites one in params",
    "multiple metrics in params",
    "multiple metrics in args",
    "remove default metric by 'None' in list",
    "remove default metric by 'None' aliases",
    "fobj, no feval",
    "no default metric",
    "metric in params",
    "metric in args",
    "metric in args overwrites its' alias in params",
    "multiple metrics in params",
    "multiple metrics in args",
    "no fobj, feval",
    "default metric with custom one",
    "non-default metric in params with custom one",
    "default metric in args with custom one",
    "non-default metric in args with custom one",
    "metric in args overwrites one in params, custom one is evaluated too",
    "multiple metrics in params with custom one",
    "multiple metrics in args with custom one",
    "custom metric is evaluated despite 'None' is passed",
    "fobj, feval",
    "no default metric, only custom one",
    "metric in params with custom one",
    "metric in args with custom one",
    "metric in args overwrites one in params, custom one is evaluated too",
    "multiple metrics in params with custom one",
    "multiple metrics in args with custom one",
    "custom metric is evaluated despite 'None' is passed",
    "no fobj, no feval",
    "default metric",
    "default metric in params",
    "non-default metric in params",
    "multiple metrics in params",
    "remove default metric by 'None' aliases",
    "fobj, no feval",
    "no default metric",
    "metric in params",
    "multiple metrics in params",
    "no fobj, feval",
    "default metric with custom one",
    "default metric in params with custom one",
    "non-default metric in params with custom one",
    "multiple metrics in params with custom one",
    "custom metric is evaluated despite 'None' is passed",
    "fobj, feval",
    "no default metric, only custom one",
    "metric in params with custom one",
    "multiple metrics in params with custom one",
    "custom metric is evaluated despite 'None' is passed",
    "multiclass default metric",
    "multiclass default metric with custom one",
    "multiclass metric alias with custom one for custom objective",
    "no metric for invalid class_num",
    "custom metric for invalid class_num",
    "multiclass metric alias with custom one with invalid class_num",
    "multiclass default metric without num_class",
    "multiclass metric alias",
    "multiclass metric",
    "non-valid metric for multiclass objective",
    "non-default num_class for default objective",
    "no metric with non-default num_class for custom objective",
    "multiclass metric alias for custom objective",
    "multiclass metric for custom objective",
    "binary metric with non-default num_class for custom objective",
    "Expect three metrics but mean and stdv for each metric",
    "test XGBoost-style return value",
    "test numpy-style return value",
    "test bins string type",
    "test histogram is disabled for categorical features",
    "test for lgb.train",
    "test feval for lgb.train",
    "test with two valid data for lgb.train",
    "test for lgb.cv",
    "test feval for lgb.cv",
    "test that binning works properly for features with only positive or only negative values",
    "decreasing without freeing raw data is allowed",
    "decreasing before lazy init is allowed",
    "increasing is allowed",
    "decreasing with disabled filter is allowed",
    "decreasing with enabled filter is disallowed;",
    "also changes of other params are disallowed",
    "check extra trees increases regularization",
    "check path smoothing increases regularization",
    "test edge case with one leaf",
    "check that constraint containing all features is equivalent to no constraint",
    "check that constraint partitioning the features reduces train accuracy",
    "check that constraints consisting of single features reduce accuracy further",
    "test that interaction constraints work when not all features are used",
    "check that setting linear_tree=True fits better than ordinary trees when data has linear relationship",
    "test again with nans in data",
    "test again with bagging",
    "test with a feature that has only one non-nan value",
    "test with a categorical feature",
    "test refit: same results on same data",
    "test refit with save and load",
    "test refit: different results training on different data",
    "test when num_leaves - 1 < num_features and when num_leaves - 1 > num_features",
    "test that the predict once with all iterations equals summed results with start_iteration and num_iteration",
    "test the case where start_iteration <= 0, and num_iteration is None",
    "test the case where start_iteration > 0, and num_iteration <= 0",
    "test the case where start_iteration > 0, and num_iteration <= 0, with pred_leaf=True",
    "test the case where start_iteration > 0, and num_iteration <= 0, with pred_contrib=True",
    "test for regression",
    "test both with and without early stopping",
    "test for multi-class",
    "test both with and without early stopping",
    "test for binary",
    "test both with and without early stopping",
    "test against sklearn average precision metric",
    "test that average precision is 1 where model predicts perfectly",
    "coding: utf-8",
    "If compiled appropriately, the same installation will support both GPU and CPU.",
    "coding: utf-8",
    "coding: utf-8",
    "These are helper functions to allow doing a stack unwind",
    "after an R allocation error, which would trigger a long jump.",
    "convert from one-based to zero-based index",
    "if any feature names were larger than allocated size,",
    "allow for a larger size and try again",
    "convert from boundaries to size",
    "--- start Booster interfaces",
    "if any eval names were larger than allocated size,",
    "allow for a larger size and try again",
    "if the model string was larger than the initial buffer, allocate a bigger buffer and try again",
    "if the model string was larger than the initial buffer, allocate a bigger buffer and try again",
    ".Call() calls",
    "coding: utf-8",
    "alias table",
    "names",
    "from strings",
    "tails",
    "tails",
    "coding: utf-8",
    "Single row predictor to abstract away caching logic",
    "create boosting",
    "initialize the boosting",
    "create objective function",
    "initialize the objective function",
    "create training metric",
    "reset the boosting",
    "create objective function",
    "initialize the objective function",
    "calculate the nonzero data and indices size",
    "allocate data and indices arrays",
    "Get the number of trees per iteration (for multiclass scenario we output multiple sparse matrices)",
    "aggregated per row feature contribution results",
    "keep track of the row_vector sizes for parallelization",
    "copy vector results to output for each row",
    "Get the number of trees per iteration (for multiclass scenario we output multiple sparse matrices)",
    "aggregated per row feature contribution results",
    "calculate number of elements per column to construct",
    "the CSC matrix with random access",
    "keep track of column counts",
    "keep track of beginning index for each column",
    "keep track of beginning index for each matrix",
    "Note: we parallelize across matrices instead of rows because of the column_counts[m][col_idx] increment inside the loop",
    "store the row index",
    "update column count",
    "explicitly declare symbols from LightGBM namespace",
    "some help functions used to convert data",
    "Row iterator of on column for CSC matrix",
    "return value at idx, only can access by ascent order",
    "return next non-zero pair, if index < 0, means no more data",
    "start of c_api functions",
    "This API is to keep python binding's behavior the same with C++ implementation.",
    "Sample count, random seed etc. should be provided in parameters.",
    "sample data first",
    "sample data first",
    "sample data first",
    "local buffer to re-use memory",
    "sample data first",
    "no more data",
    "---- start of booster",
    "Single row in row-major format:",
    "---- start of some help functions",
    "data is array of pointers to individual rows",
    "set number of threads for openmp",
    "check for alias",
    "read parameters from config file",
    "remove str after \"#\"",
    "check for alias again",
    "load configs",
    "prediction is needed if using input initial model(continued train)",
    "need to continue training",
    "sync up random seed for data partition",
    "load Training data",
    "load data for distributed training",
    "load data for single machine",
    "need save binary file",
    "create training metric",
    "only when have metrics then need to construct validation data",
    "Add validation data, if it exists",
    "add",
    "need save binary file",
    "add metric for validation data",
    "output used time on each iteration",
    "need init network",
    "create boosting",
    "create objective function",
    "load training data",
    "initialize the objective function",
    "initialize the boosting",
    "add validation data into boosting",
    "convert model to if-else statement code",
    "create predictor",
    "Free memory",
    "create predictor",
    "label_gain = 2^i - 1, may overflow, so we use 31 here",
    "counts for all labels",
    "start from top label, and accumulate DCG",
    "counts for all labels",
    "calculate k Max DCG by one pass",
    "get sorted indices by score",
    "calculate multi dcg by one pass",
    "wait for all client start up",
    "Don't call MPI_Finalize() here: If the destructor was called because only this node had an exception, calling MPI_Finalize() will cause all nodes to hang.",
    "Instead we will handle finalize/abort for MPI in main().",
    "default set to -1",
    "distance at k-th communication, distance[k] = 2^k",
    "set incoming rank at k-th commuication",
    "set outgoing rank at k-th commuication",
    "default set as -1",
    "construct all recursive halving map for all machines",
    "let 1 << k <= num_machines",
    "distance of each communication",
    "if num_machines = 2^k, don't need to group machines",
    "communication direction, %2 == 0 is positive",
    "neighbor at k-th communication",
    "receive data block at k-th communication",
    "send data block at k-th communication",
    "if num_machines != 2^k, need to group machines",
    "group, two machine in one group, total \"rest\" groups will have 2 machines.",
    "let left machine as group leader",
    "cache block information for groups, group with 2 machines will have double block size",
    "convert from group to node leader",
    "convert from node to group",
    "meet new group",
    "add block len for this group",
    "calculate the group block start",
    "not need to construct",
    "get receive block information",
    "accumulate block len",
    "get send block information",
    "accumulate block len",
    "static member definition",
    "if small package or small count , do it by all gather.(reduce the communication times.)",
    "assign the blocks to every rank.",
    "do reduce scatter",
    "do all gather",
    "assign blocks",
    "need use buffer here, since size of \"output\" is smaller than size after all gather",
    "copy back",
    "assign blocks",
    "start all gather",
    "when num_machines is small and data is large",
    "use output as receive buffer",
    "get current local block size",
    "get out rank",
    "get in rank",
    "get send information",
    "get recv information",
    "send and recv at same time",
    "rotate in-place",
    "use output as receive buffer",
    "get current local block size",
    "get send information",
    "get recv information",
    "send and recv at same time",
    "use output as receive buffer",
    "send and recv at same time",
    "send local data to neighbor first",
    "receive neighbor data first",
    "reduce",
    "get target",
    "get send information",
    "get recv information",
    "send and recv at same time",
    "reduce",
    "send result to neighbor",
    "receive result from neighbor",
    "copy result",
    "start up socket",
    "parse clients from file",
    "get ip list of local machine",
    "get local rank",
    "construct listener",
    "construct communication topo",
    "construct linkers",
    "free listener",
    "set timeout",
    "accept incoming socket",
    "receive rank",
    "add new socket",
    "save ranks that need to connect with",
    "start listener",
    "start connect",
    "let smaller rank connect to larger rank",
    "send local rank",
    "wait for listener",
    "print connected linkers",
    "only need to copy subset",
    "avoid to copy subset many times",
    "avoid out of range",
    "may need to recopy subset",
    "valid the type",
    "Constructors",
    "Get type tag",
    "Comparisons",
    "This has to be separate, not in Statics, because Json() accesses",
    "statics().null.",
    "advance until next line, or end of input",
    "advance until closing tokens",
    "The usual case: non-escaped characters",
    "Handle escapes",
    "Extract 4-byte escape sequence",
    "Explicitly check length of the substring. The following loop",
    "relies on std::string returning the terminating NUL when",
    "accessing str[length]. Checking here reduces brittleness.",
    "JSON specifies that characters outside the BMP shall be encoded as a",
    "pair of 4-hex-digit \\u escapes encoding their surrogate pair",
    "components. Check whether we're in the middle of such a beast: the",
    "previous codepoint was an escaped lead (high) surrogate, and this is",
    "a trail (low) surrogate.",
    "Reassemble the two surrogate pairs into one astral-plane character,",
    "per the UTF-16 algorithm.",
    "Integer part",
    "Decimal part",
    "Exponent part",
    "Check for any trailing garbage",
    "Documented in json11.hpp",
    "Check for another object",
    "get column names",
    "load label idx first",
    "erase label column name",
    "load ignore columns",
    "load weight idx",
    "load group idx",
    "don't support query id in data file when using distributed training",
    "read data to memory",
    "sample data",
    "construct feature bin mappers",
    "initialize label",
    "extract features",
    "sample data from file",
    "construct feature bin mappers",
    "initialize label",
    "extract features",
    "load data from binary file",
    "check meta data",
    "need to check training data",
    "read data in memory",
    "initialize label",
    "extract features",
    "Get number of lines of data file",
    "initialize label",
    "extract features",
    "load data from binary file",
    "not need to check validation data",
    "check meta data",
    "buffer to read binary file",
    "check token",
    "read size of header",
    "re-allocmate space if not enough",
    "read header",
    "get header",
    "num_groups",
    "real_feature_idx_",
    "feature2group",
    "feature2subfeature",
    "group_bin_boundaries",
    "group_feature_start_",
    "group_feature_cnt_",
    "get feature names",
    "write feature names",
    "get forced_bin_bounds_",
    "read size of meta data",
    "re-allocate space if not enough",
    "read meta data",
    "load meta data",
    "sample local used data if need to partition",
    "if not contain query file, minimal sample unit is one record",
    "if contain query file, minimal sample unit is one query",
    "if is new query",
    "read feature data",
    "read feature size",
    "re-allocate space if not enough",
    "raw data",
    "fill feature_names_ if not header",
    "get forced split",
    "if only one machine, find bin locally",
    "if have multi-machines, need to find bin distributed",
    "different machines will find bin for different features",
    "start and len will store the process feature indices for different machines",
    "machine i will find bins for features in [ start[i], start[i] + len[i] )",
    "free",
    "gather global feature bin mappers",
    "restore features bins from buffer",
    "---- private functions ----",
    "if features are ordered, not need to use hist_buf",
    "read all lines",
    "get query data",
    "if not contain query data, minimal sample unit is one record",
    "if contain query data, minimal sample unit is one query",
    "if is new query",
    "get query data",
    "if not contain query file, minimal sample unit is one record",
    "if contain query file, minimal sample unit is one query",
    "if is new query",
    "parse features",
    "get forced split",
    "check the range of label_idx, weight_idx and group_idx",
    "fill feature_names_ if not header",
    "start find bins",
    "if only one machine, find bin locally",
    "start and len will store the process feature indices for different machines",
    "machine i will find bins for features in [ start[i], start[i] + len[i] )",
    "free",
    "gather global feature bin mappers",
    "restore features bins from buffer",
    "if doesn't need to prediction with initial model",
    "parser",
    "set label",
    "free processed line:",
    "shrink_to_fit will be very slow in linux, and seems not free memory, disable for now",
    "text_reader_->Lines()[i].shrink_to_fit();",
    "push data",
    "if is used feature",
    "if need to prediction with initial model",
    "parser",
    "set initial score",
    "set label",
    "free processed line:",
    "shrink_to_fit will be very slow in Linux, and seems not free memory, disable for now",
    "text_reader_->Lines()[i].shrink_to_fit();",
    "push data",
    "if is used feature",
    "metadata_ will manage space of init_score",
    "text data can be free after loaded feature values",
    "parser",
    "set initial score",
    "set label",
    "push data",
    "if is used feature",
    "only need part of data",
    "need full data",
    "metadata_ will manage space of init_score",
    "read size of token",
    "remove duplicates",
    "deep copy function for BinMapper",
    "mean size for one bin",
    "need a new bin",
    "update bin upper bound",
    "last bin upper bound",
    "get list of distinct values",
    "get number of positive and negative distinct values",
    "include zero bounds and infinity bound",
    "add forced bounds, excluding zeros since we have already added zero bounds",
    "find remaining bounds",
    "find distinct_values first",
    "push zero in the front",
    "use the large value",
    "push zero in the back",
    "convert to int type first",
    "sort by counts",
    "will ignore the categorical of small counts",
    "Push the dummy bin for NaN",
    "Use MissingType::None to represent this bin contains all categoricals",
    "fix count of NaN bin",
    "check trivial(num_bin_ == 1) feature",
    "check useless bin",
    "When most_freq_bin_ != default_bin_, there are some additional data loading costs.",
    "so use most_freq_bin_  = default_bin_ when there is not so sparse",
    "calculate max bin of all features to select the int type in MultiValDenseBin",
    "for lambdarank, it needs query data for partition data in distributed learning",
    "need convert query_id to boundaries",
    "check weights",
    "check query boundries",
    "contain initial score file",
    "check weights",
    "get local weights",
    "check query boundries",
    "get local query boundaries",
    "contain initial score file",
    "get local initial scores",
    "re-load query weight",
    "save to nullptr",
    "save to nullptr",
    "save to nullptr",
    "default weight file name",
    "default init_score file name",
    "use first line to count number class",
    "default query file name",
    "root is in the depth 0",
    "non-leaf",
    "leaf",
    "use this for the missing value conversion",
    "Predict func by Map to ifelse",
    "use this for the missing value conversion",
    "non-leaf",
    "left subtree",
    "right subtree",
    "leaf",
    "non-leaf",
    "left subtree",
    "right subtree",
    "leaf",
    "recursive computation of SHAP values for a decision tree",
    "extend the unique path",
    "leaf node",
    "internal node",
    "see if we have already split on this feature,",
    "if so we undo that split so we can redo it for this node",
    "recursive sparse computation of SHAP values for a decision tree",
    "extend the unique path",
    "leaf node",
    "internal node",
    "see if we have already split on this feature,",
    "if so we undo that split so we can redo it for this node",
    "add names of objective function if not providing metric",
    "equal weights for all classes",
    "generate seeds by seed.",
    "sort eval_at",
    "Only push the non-training data",
    "check for conflicts",
    "check if objective, metric, and num_class match",
    "Change pool size to -1 (no limit) when using data parallel to reduce communication costs",
    "Check max_depth and num_leaves",
    "Fits in an int, and is more restrictive than the current num_leaves",
    "force col-wise for gpu & CUDA",
    "force gpu_use_dp for CUDA",
    "linear tree learner must be serial type and run on CPU device",
    "min_data_in_leaf must be at least 2 if path smoothing is active. This is because when the split is calculated",
    "the count is calculated using the proportion of hessian in the leaf which is rounded up to nearest int, so it can",
    "be 1 when there is actually no data in the leaf. In rare cases this can cause a bug because with path smoothing the",
    "calculated split gain can be positive even with zero gradient and hessian.",
    "In distributed mode, local node doesn't have histograms on all features, cannot perform \"intermediate\" monotone constraints.",
    "\"intermediate\" monotone constraints need to recompute splits. If the features are sampled when computing the",
    "split initially, then the sampling needs to be recorded or done once again, which is currently not supported",
    "first round: fill the single val group",
    "always push the last group",
    "put dense feature first",
    "sort by non zero cnt",
    "sort by non zero cnt, bigger first",
    "shuffle groups",
    "Using std::swap for vector<bool> will cause the wrong result.",
    "get num_features",
    "get bin_mappers",
    "for sparse multi value bin, we store the feature bin values with offset added",
    "for dense multi value bin, the feature bin values without offsets are used",
    "copy feature bin mapper data",
    "copy feature bin mapper data",
    "if not pass a filename, just append \".bin\" of original file",
    "get size of header",
    "size of feature names",
    "size of forced bins",
    "write header",
    "write feature names",
    "write forced bins",
    "get size of meta data",
    "write meta data",
    "write feature data",
    "get size of feature",
    "write feature",
    "write raw data; use row-major order so we can read row-by-row",
    "explicitly initialize template methods, for cross module call",
    "Only one multi-val group, just simply merge",
    "Skip the leading 0 when copying group_bin_boundaries.",
    "regenerate other fields",
    "store the importance first",
    "PredictRaw",
    "PredictRawByMap",
    "Predict",
    "PredictByMap",
    "PredictLeafIndex",
    "PredictLeafIndexByMap",
    "output model type",
    "output number of class",
    "output label index",
    "output max_feature_idx",
    "output objective",
    "output tree models",
    "store the importance first",
    "sort the importance",
    "use serialized string to restore this object",
    "Use first 128 chars to avoid exceed the message buffer.",
    "get number of classes",
    "get index of label",
    "get max_feature_idx first",
    "get average_output",
    "get feature names",
    "get monotone_constraints",
    "set zero",
    "predict all the trees for one iteration",
    "check early stopping",
    "set zero",
    "predict all the trees for one iteration",
    "check early stopping",
    "margin_threshold will be captured by value",
    "copy and sort",
    "margin_threshold will be captured by value",
    "Fix for compiler warnings about reaching end of control",
    "load forced_splits file",
    "init tree learner",
    "push training metrics",
    "create buffer for gradients and Hessians",
    "get max feature index",
    "get label index",
    "get feature names",
    "if need bagging, create buffer",
    "for a validation dataset, we need its score and metric",
    "update score",
    "objective function will calculate gradients and hessians",
    "random bagging, minimal unit is one record",
    "random bagging, minimal unit is one record",
    "if need bagging",
    "set bagging data to tree learner",
    "get subset",
    "output used time per iteration",
    "boosting from average label; or customized \"average\" if implemented for the current objective",
    "boosting first",
    "bagging logic",
    "need to copy gradients for bagging subset.",
    "shrinkage by learning rate",
    "update score",
    "only add default score one-time",
    "updates scores",
    "add model",
    "reset score",
    "remove model",
    "print message for metric",
    "pop last early_stopping_round_ models",
    "update training score",
    "we need to predict out-of-bag scores of data for boosting",
    "update validation score",
    "print training metric",
    "print validation metric",
    "set zero",
    "predict all the trees for one iteration",
    "predict all the trees for one iteration",
    "push training metrics",
    "not same training data, need reset score and others",
    "create score tracker",
    "update score",
    "create buffer for gradients and hessians",
    "load forced_splits file",
    "if need bagging, create buffer",
    "Get the max size of pool",
    "at least need 2 leaves",
    "push split information for all leaves",
    "initialize splits for leaf",
    "initialize data partition",
    "initialize ordered gradients and hessians",
    "cannot change is_hist_col_wise during training",
    "initialize splits for leaf",
    "initialize data partition",
    "initialize ordered gradients and hessians",
    "Get the max size of pool",
    "at least need 2 leaves",
    "push split information for all leaves",
    "some initial works before training",
    "root leaf",
    "only root leaf can be splitted on first time",
    "some initial works before finding best split",
    "find best threshold for every feature",
    "Get a leaf with max split gain",
    "Get split information for best leaf",
    "cannot split, quit",
    "split tree with best leaf",
    "reset histogram pool",
    "initialize data partition",
    "reset the splits for leaves",
    "Sumup for root",
    "use all data",
    "use bagging, only use part of data",
    "check depth of current leaf",
    "only need to check left leaf, since right leaf is in same level of left leaf",
    "no enough data to continue",
    "only have root",
    "put parent(left) leaf's histograms into larger leaf's histograms",
    "put parent(left) leaf's histograms to larger leaf's histograms",
    "construct smaller leaf",
    "construct larger leaf",
    "find splits",
    "only has root leaf",
    "start at root leaf",
    "before processing next node from queue, store info for current left/right leaf",
    "store \"best split\" for left and right, even if they might be overwritten by forced split",
    "then, compute own splits",
    "split info should exist because searching in bfs fashion - should have added from parent",
    "update before tree split",
    "don't need to update this in data-based parallel model",
    "split tree, will return right leaf",
    "store the true split gain in tree model",
    "don't need to update this in data-based parallel model",
    "store the true split gain in tree model",
    "init the leaves that used on next iteration",
    "update leave outputs if needed",
    "bag_mapper[index_mapper[i]]",
    "it is needed to filter the features after the above code.",
    "Otherwise, the `is_splittable` in `FeatureHistogram` will be wrong, and cause some features being accidentally filtered in the later nodes.",
    "for root leaf the \"parent\" output is its own output because we don't apply any smoothing to the root",
    "can't use GetParentOutput because leaf_splits doesn't have weight property set",
    "find splits",
    "identify features containing nans",
    "preallocate the matrix used to calculate linear model coefficients",
    "store only upper triangular half of matrix as an array, in row-major order",
    "this requires (max_num_feat + 1) * (max_num_feat + 2) / 2 entries (including the constant terms of the regression)",
    "we add another 8 to ensure cache lines are not shared among processors",
    "some initial works before training",
    "root leaf",
    "only root leaf can be splitted on first time",
    "some initial works before finding best split",
    "find best threshold for every feature",
    "Get a leaf with max split gain",
    "Get split information for best leaf",
    "cannot split, quit",
    "split tree with best leaf",
    "map data to leaf number",
    "calculate coefficients using the method described in Eq 3 of https://arxiv.org/pdf/1802.05640.pdf",
    "the coefficients vector is given by",
    "- (X_T * H * X + lambda) ^ (-1) * (X_T * g)",
    "where:",
    "X is the matrix where the first column is the feature values and the second is all ones,",
    "H is the diagonal matrix of the hessian,",
    "lambda is the diagonal matrix with diagonal entries equal to the regularisation term linear_lambda",
    "g is the vector of gradients",
    "the subscript _T denotes the transpose",
    "create array of pointers to raw data, and coefficient matrices, for each leaf",
    "clear the coefficient matrices",
    "aggregate results from different threads",
    "copy into eigen matrices and solve",
    "update the tree properties",
    "need to be able to hold smaller and larger best splits in SyncUpGlobalBestSplit",
    "get feature partition",
    "get local used features",
    "get best split at smaller leaf",
    "find local best split for larger leaf",
    "sync global best info",
    "update best split",
    "instantiate template classes, otherwise linker cannot find the code",
    "initialize SerialTreeLearner",
    "Get local rank and global machine size",
    "need to be able to hold smaller and larger best splits in SyncUpGlobalBestSplit",
    "allocate buffer for communication",
    "generate feature partition for current tree",
    "get local used feature",
    "get block start and block len for reduce scatter",
    "get buffer_write_start_pos_",
    "get buffer_read_start_pos_",
    "sync global data sumup info",
    "global sumup reduce",
    "copy back",
    "set global sumup info",
    "init global data count in leaf",
    "clear histogram buffer before synchronizing",
    "otherwise histogram contents from the previous iteration will be sent",
    "construct local histograms",
    "copy to buffer",
    "Reduce scatter for histogram",
    "restore global histograms from buffer",
    "only root leaf",
    "construct histgroms for large leaf, we init larger leaf as the parent, so we can just subtract the smaller leaf's histograms",
    "find local best split for larger leaf",
    "sync global best info",
    "set best split",
    "need update global number of data in leaf",
    "instantiate template classes, otherwise linker cannot find the code",
    "initialize SerialTreeLearner",
    "some additional variables needed for GPU trainer",
    "Initialize GPU buffers and kernels",
    "some functions used for debugging the GPU histogram construction",
    "printf(\"grad %g != %g (%d ULPs)\\n\", GET_GRAD(h1, i), GET_GRAD(h2, i), ulps);",
    "goto err;",
    "we roughly want 256 workgroups per device, and we have num_dense_feature4_ feature tuples.",
    "also guarantee that there are at least 2K examples per workgroup",
    "return 0;",
    "we have already copied ordered gradients, ordered Hessians and indices to GPU",
    "decide the best number of workgroups working on one feature4 tuple",
    "set work group size based on feature size",
    "each 2^exp_workgroups_per_feature workgroups work on a feature4 tuple",
    "we need to refresh the kernel arguments after reallocating",
    "The only argument that needs to be changed later is num_data_",
    "the GPU kernel will process all features in one call, and each",
    "2^exp_workgroups_per_feature (compile time constant) workgroup will",
    "process one feature4 tuple",
    "for the root node, indices are not copied",
    "for constant hessian, hessians are not copied except for the root node",
    "there will be 2^exp_workgroups_per_feature = num_workgroups / num_dense_feature4 sub-histogram per feature4",
    "and we will launch num_feature workgroups for this kernel",
    "will launch threads for all features",
    "the queue should be asynchronous, and we will can WaitAndGetHistograms() before we start processing dense feature groups",
    "copy the results asynchronously. Size depends on if double precision is used",
    "we will wait for this object in WaitAndGetHistograms",
    "when the output is ready, the computation is done",
    "values of this feature has been redistributed to multiple bins; need a reduction here",
    "how many feature-group tuples we have",
    "leave some safe margin for prefetching",
    "256 work-items per workgroup. Each work-item prefetches one tuple for that feature",
    "clear sparse/dense maps",
    "do nothing if no features can be processed on GPU",
    "allocate memory for all features (FIXME: 4 GB barrier on some devices, need to split to multiple buffers)",
    "unpin old buffer if necessary before destructing them",
    "make ordered_gradients and Hessians larger (including extra room for prefetching), and pin them",
    "allocate space for gradients and Hessians on device",
    "we will copy gradients and Hessians in after ordered_gradients_ and ordered_hessians_ are constructed",
    "allocate feature mask, for disabling some feature-groups' histogram calculation",
    "copy indices to the device",
    "histogram bin entry size depends on the precision (single/double)",
    "create output buffer, each feature has a histogram with device_bin_size_ bins,",
    "each work group generates a sub-histogram of dword_features_ features.",
    "only initialize once here, as this will not need to change when ResetTrainingData() is called",
    "create atomic counters for inter-group coordination",
    "The output buffer is allocated to host directly, to overlap compute and data transfer",
    "find the dense feature-groups and group then into Feature4 data structure (several feature-groups packed into 4 bytes)",
    "looking for dword_features_ non-sparse feature-groups",
    "decide if we need to redistribute the bin",
    "multiplier must be a power of 2",
    "device_bin_mults_.push_back(1);",
    "found",
    "for data transfer time",
    "Now generate new data structure feature4, and copy data to the device",
    "preallocate arrays for all threads, and pin them",
    "building Feature4 bundles; each thread handles dword_features_ features",
    "one feature datapoint is 4 bits",
    "this guarantees that the RawGet() function is inlined, rather than using virtual function dispatching",
    "one feature datapoint is one byte",
    "this guarantees that the RawGet() function is inlined, rather than using virtual function dispatching",
    "Dense bin",
    "Dense 4-bit bin",
    "working on the remaining (less than dword_features_) feature groups",
    "fill the leftover features",
    "fill this empty feature with some \"random\" value",
    "fill this empty feature with some \"random\" value",
    "copying the last 1 to (dword_features - 1) feature-groups in the last tuple",
    "deallocate pinned space for feature copying",
    "data transfer time",
    "for other types of failure, build log might not be available; program.build_log() can crash",
    "Something bad happened. Just return \"No log available.\"",
    "build is okay, log may contain warnings",
    "destroy any old kernels",
    "create OpenCL kernels for different number of workgroups per feature",
    "currently we don't use constant memory",
    "compile the GPU kernel depending if double precision is used, constant hessian is used, etc.",
    "kernel with indices in an array",
    "kernel with all features enabled, with eliminated branches",
    "kernel with all data indices (for root node, and assumes that root node always uses all features)",
    "do nothing if no features can be processed on GPU",
    "The only argument that needs to be changed later is num_data_",
    "hessian is passed as a parameter, but it is not available now.",
    "hessian will be set in BeforeTrain()",
    "Get the max bin size, used for selecting best GPU kernel",
    "initialize GPU",
    "determine which kernel to use based on the max number of bins",
    "the +9 skips extra characters \")\", newline, \"#endif\" and newline at the beginning",
    "the +9 skips extra characters \")\", newline, \"#endif\" and newline at the beginning",
    "the +9 skips extra characters \")\", newline, \"#endif\" and newline at the beginning",
    "setup GPU kernel arguments after we allocating all the buffers",
    "GPU memory has to been reallocated because data may have been changed",
    "setup GPU kernel arguments after we allocating all the buffers",
    "Copy initial full hessians and gradients to GPU.",
    "We start copying as early as possible, instead of at ConstructHistogram().",
    "setup hessian parameters only",
    "hessian is passed as a parameter",
    "use bagging",
    "On GPU, we start copying indices, gradients and Hessians now, instead at ConstructHistogram()",
    "copy used gradients and Hessians to ordered buffer",
    "transfer the indices to GPU",
    "transfer hessian to GPU",
    "setup hessian parameters only",
    "hessian is passed as a parameter",
    "transfer gradients to GPU",
    "only have root",
    "Copy indices, gradients and Hessians as early as possible",
    "only need to initialize for smaller leaf",
    "Get leaf boundary",
    "copy indices to the GPU:",
    "copy ordered Hessians to the GPU:",
    "copy ordered gradients to the GPU:",
    "do nothing if no features can be processed on GPU",
    "copy data indices if it is not null",
    "generate and copy ordered_gradients if gradients is not null",
    "generate and copy ordered_hessians if Hessians is not null",
    "converted indices in is_feature_used to feature-group indices",
    "construct the feature masks for dense feature-groups",
    "if no feature group is used, just return and do not use GPU",
    "if not all feature groups are used, we need to transfer the feature mask to GPU",
    "otherwise, we will use a specialized GPU kernel with all feature groups enabled",
    "All data have been prepared, now run the GPU kernel",
    "construct smaller leaf",
    "ConstructGPUHistogramsAsync will return true if there are available feature groups dispatched to GPU",
    "then construct sparse features on CPU",
    "wait for GPU to finish, only if GPU is actually used",
    "use double precision",
    "use single precision",
    "Compare GPU histogram with CPU histogram, useful for debugging GPU code problem",
    "#define GPU_DEBUG_COMPARE",
    "construct larger leaf",
    "then construct sparse features on CPU",
    "wait for GPU to finish, only if GPU is actually used",
    "use double precision",
    "use single precision",
    "do some sanity check for the GPU algorithm",
    "limit top k",
    "get max bin",
    "calculate buffer size",
    "need to be able to hold smaller and larger best splits in SyncUpGlobalBestSplit",
    "left and right on same time, so need double size",
    "initialize histograms for global",
    "sync global data sumup info",
    "set global sumup info",
    "init global data count in leaf",
    "get local sumup",
    "get local sumup",
    "get mean number on machines",
    "weighted gain",
    "get top k",
    "Copy histogram to buffer, and Get local aggregate features",
    "copy histograms.",
    "copy smaller leaf histograms first",
    "mark local aggregated feature",
    "copy",
    "then copy larger leaf histograms",
    "mark local aggregated feature",
    "copy",
    "use local data to find local best splits",
    "clear histogram buffer before synchronizing",
    "otherwise histogram contents from the previous iteration will be sent",
    "find splits",
    "only has root leaf",
    "local voting",
    "gather",
    "get all top-k from all machines",
    "global voting",
    "copy local histgrams to buffer",
    "Reduce scatter for histogram",
    "find best split from local aggregated histograms",
    "restore from buffer",
    "restore from buffer",
    "find local best",
    "find local best split for larger leaf",
    "sync global best info",
    "copy back",
    "set the global number of data for leaves",
    "init the global sumup info",
    "instantiate template classes, otherwise linker cannot find the code",
    "launch cuda kernel",
    "initialize SerialTreeLearner",
    "some additional variables needed for GPU trainer",
    "Initialize GPU buffers and kernels: get device info",
    "some functions used for debugging the GPU histogram construction",
    "we roughly want 256 workgroups per device, and we have num_dense_feature4_ feature tuples.",
    "also guarantee that there are at least 2K examples per workgroup",
    "we have already copied ordered gradients, ordered hessians and indices to GPU",
    "decide the best number of workgroups working on one feature4 tuple",
    "set work group size based on feature size",
    "each 2^exp_workgroups_per_feature workgroups work on a feature4 tuple",
    "set thread_data",
    "copy the results asynchronously. Size depends on if double precision is used",
    "when the output is ready, the computation is done",
    "how many feature-group tuples we have",
    "leave some safe margin for prefetching",
    "256 work-items per workgroup. Each work-item prefetches one tuple for that feature",
    "clear sparse/dense maps",
    "do nothing it there is no dense feature",
    "calculate number of feature groups per gpu",
    "histogram bin entry size depends on the precision (single/double)",
    "allocate GPU memory for each GPU",
    "do nothing it there is no gpu feature",
    "allocate memory for all features",
    "allocate space for gradients and hessians on device",
    "we will copy gradients and hessians in after ordered_gradients_ and ordered_hessians_ are constructed",
    "copy indices to the device",
    "create output buffer, each feature has a histogram with device_bin_size_ bins,",
    "each work group generates a sub-histogram of dword_features_ features.",
    "only initialize once here, as this will not need to change when ResetTrainingData() is called",
    "create atomic counters for inter-group coordination",
    "The output buffer is allocated to host directly, to overlap compute and data transfer",
    "clear sparse/dense maps",
    "find the dense feature-groups and group then into Feature4 data structure (several feature-groups packed into 4 bytes)",
    "set device info",
    "looking for dword_features_ non-sparse feature-groups",
    "reset device info",
    "InitGPU w/ num_gpu",
    "Get the max bin size, used for selecting best GPU kernel",
    "get num_dense_feature_groups_",
    "initialize GPU",
    "set cpu threads",
    "resize device memory pointers",
    "create stream & events to handle multiple GPUs",
    "check data size",
    "GPU memory has to been reallocated because data may have been changed",
    "AllocateGPUMemory only when the number of data increased",
    "setup GPU kernel arguments after we allocating all the buffers",
    "Copy initial full hessians and gradients to GPU.",
    "We start copying as early as possible, instead of at ConstructHistogram().",
    "use bagging",
    "On GPU, we start copying indices, gradients and hessians now, instead at ConstructHistogram()",
    "copy used gradients and hessians to ordered buffer",
    "transfer the indices to GPU",
    "only have root",
    "Copy indices, gradients and hessians as early as possible",
    "only need to initialize for smaller leaf",
    "Get leaf boundary",
    "do nothing if no features can be processed on GPU",
    "copy data indices if it is not null",
    "converted indices in is_feature_used to feature-group indices",
    "construct the feature masks for dense feature-groups",
    "if no feature group is used, just return and do not use GPU",
    "if not all feature groups are used, we need to transfer the feature mask to GPU",
    "otherwise, we will use a specialized GPU kernel with all feature groups enabled",
    "We now copy even if all features are used.",
    "All data have been prepared, now run the GPU kernel",
    "construct smaller leaf",
    "Check workgroups per feature4 tuple..",
    "if the workgroup per feature is 1 (2^0), return as the work is too small for a GPU",
    "ConstructGPUHistogramsAsync will return true if there are availabe feature groups dispatched to GPU",
    "then construct sparse features on CPU",
    "We set data_indices to null to avoid rebuilding ordered gradients/hessians",
    "wait for GPU to finish, only if GPU is actually used",
    "use double precision",
    "use single precision",
    "Compare GPU histogram with CPU histogram, useful for debuggin GPU code problem",
    "#define CUDA_DEBUG_COMPARE",
    "construct larger leaf",
    "then construct sparse features on CPU",
    "We set data_indices to null to avoid rebuilding ordered gradients/hessians",
    "wait for GPU to finish, only if GPU is actually used",
    "use double precision",
    "use single precision",
    "do some sanity check for the GPU algorithm"
  ],
  "v3.3.3": [
    "coding: utf-8",
    "coding: utf-8",
    "create predictor first",
    "show deprecation warning only for early stop argument, setting early stop via global params should still be possible",
    "check dataset",
    "reduce cost for prediction training data",
    "process callbacks",
    "Most of legacy advanced options becomes callbacks",
    "construct booster",
    "start training",
    "check evaluation result.",
    "ranking task, split according to groups",
    "run preprocessing on the data set if needed",
    "setup callbacks",
    "coding: utf-8",
    "dummy function to support older version of scikit-learn",
    "coding: utf-8",
    "documentation templates for LGBMModel methods are shared between the classes in",
    "this module and those in the ``dask`` module",
    "user can set verbose with kwargs, it has higher priority",
    "Do not modify original args in fit function",
    "Refer to https://github.com/microsoft/LightGBM/pull/2619",
    "Separate built-in from callable evaluation metrics",
    "register default metric for consistency with callable eval_metric case",
    "try to deduce from class instance",
    "overwrite default metric by explicitly set metric",
    "concatenate metric from params (or default if not provided in params) and eval_metric",
    "copy for consistency",
    "reduce cost for prediction training data",
    "free dataset",
    "Switch to using a multiclass objective in the underlying LGBM instance",
    "do not modify args, as it causes errors in model selection tools",
    "check group data",
    "coding: utf-8",
    "we don't need lib_lightgbm while building docs",
    "coding: utf-8",
    "\"#ddffdd\" is light green, \"#ffdddd\" is light red",
    "coding: utf-8",
    "coding: utf-8",
    "TypeError: obj is not a string or a number",
    "ValueError: invalid literal",
    "DeprecationWarning is not shown by default, so let's create our own with higher level",
    "avoid side effects on passed-in parameters",
    "find a value, and remove other aliases with .pop()",
    "prefer the value of 'main_param_name' if it exists, otherwise search the aliases",
    "Get total row number.",
    "Random access by row index. Used for data sampling.",
    "Range data access. Used to read data in batch when constructing Dataset.",
    "Optionally specify batch_size to control range data read size.",
    "Only required if using ``Dataset.subset()``.",
    "__get_num_preds() cannot work with nrow > MAX_INT32, so calculate overall number of predictions piecemeal",
    "avoid memory consumption by arrays concatenation operations",
    "create numpy array from output arrays",
    "break up indptr based on number of rows (note more than one matrix in multiclass case)",
    "for CSC there is extra column added",
    "reformat output into a csr or csc matrix or list of csr or csc matrices",
    "same shape as input csr or csc matrix except extra column for expected value",
    "note: make sure we copy data as it will be deallocated next",
    "free the temporary native indptr, indices, and data",
    "__get_num_preds() cannot work with nrow > MAX_INT32, so calculate overall number of predictions piecemeal",
    "avoid memory consumption by arrays concatenation operations",
    "c type: double**",
    "each double* element points to start of each column of sample data.",
    "c type int**",
    "each int* points to start of indices for each column",
    "no min_data, nthreads and verbose in this function",
    "check data has header or not",
    "need to regroup init_score",
    "process for args",
    "user can set verbose with params, it has higher priority",
    "get categorical features",
    "process for reference dataset",
    "start construct data",
    "set feature names",
    "Select sampled rows, transpose to column order.",
    "create validation dataset from ref_dataset",
    "create valid",
    "construct subset",
    "create train",
    "could be updated if data is not freed",
    "set to None",
    "we're done if self and reference share a common upstream reference",
    "if buffer length is not long enough, reallocate buffers",
    "group data from LightGBM is boundaries data, need to convert to group size",
    "user can set verbose with params, it has higher priority",
    "Training task",
    "if \"machines\" is given, assume user wants to do distributed learning, and set up network",
    "construct booster object",
    "copy the parameters from train_set",
    "save reference to data",
    "buffer for inner predict",
    "Prediction task",
    "if a single node tree it won't have `leaf_index` so return 0",
    "Create the node record, and populate universal data members",
    "Update values to reflect node type (leaf or split)",
    "traverse the next level of the tree",
    "In tree format, \"subtree_list\" is a list of node records (dicts),",
    "and we add node to the list.",
    "need reset training data",
    "need to push new valid data",
    "if buffer length is not long enough, re-allocate a buffer",
    "if buffer length is not long enough, reallocate a buffer",
    "Copy models",
    "Get name of features",
    "if buffer length is not long enough, reallocate buffers",
    "avoid to predict many time in one iteration",
    "Get num of inner evals",
    "Get name of eval metrics",
    "if buffer length is not long enough, reallocate buffers",
    "coding: utf-8",
    "Callback environment used by callbacks",
    "split is needed for \"<dataset type> <metric>\" case (e.g. \"train l1\")",
    "split is needed for \"<dataset type> <metric>\" case (e.g. \"train l1\")",
    "coding: utf-8",
    "Concatenate many parts into one",
    "construct local eval_set data.",
    "store indices of eval_set components that were not contained within local parts.",
    "consolidate parts of each individual eval component.",
    "require that eval_name exists in evaluated result data in case dropped due to padding.",
    "in distributed training the 'training' eval_set is not detected, will have name 'valid_<index>'.",
    "filter padding from eval parts then _concat each eval_set component.",
    "reconstruct eval_set fit args/kwargs depending on which components of eval_set are on worker.",
    "ensure that expected keys for evals_result_ and best_score_ exist regardless of padding.",
    "capture whether local_listen_port or its aliases were provided",
    "capture whether machines or its aliases were provided",
    "Some passed-in parameters can be removed:",
    "* 'num_machines': set automatically from Dask worker list",
    "* 'num_threads': overridden to match nthreads on each Dask process",
    "Split arrays/dataframes into parts. Arrange parts into dicts to enforce co-locality",
    "evals_set will to be re-constructed into smaller lists of (X, y) tuples, where",
    "X and y are each delayed sub-lists of original eval dask Collections.",
    "find maximum number of parts in an individual eval set so that we can",
    "pad eval sets when they come in different sizes.",
    "when individual eval set is equivalent to training data, skip recomputing parts.",
    "add None-padding for individual eval_set member if it is smaller than the largest member.",
    "first time a chunk of this eval set is added to this part.",
    "append additional chunks of this eval set to this part.",
    "ensure that all evaluation parts map uniquely to one part.",
    "assign sub-eval_set components to worker parts.",
    "Start computation in the background",
    "Find locations of all parts and map them to particular Dask workers",
    "Check that all workers were provided some of eval_set. Otherwise warn user that validation",
    "data artifacts may not be populated depending on worker returning final estimator.",
    "assign general validation set settings to fit kwargs.",
    "resolve aliases for network parameters and pop the result off params.",
    "these values are added back in calls to `_train_part()`",
    "figure out network params",
    "Tell each worker to train on the parts that it has locally",
    "",
    "This code treats ``_train_part()`` calls as not \"pure\" because:",
    "1. there is randomness in the training process unless parameters ``seed``",
    "and ``deterministic`` are set",
    "2. even with those parameters set, the output of one ``_train_part()`` call",
    "relies on global state (it and all the other LightGBM training processes",
    "coordinate with each other)",
    "if network parameters were changed during training, remove them from the",
    "returned model so that they're generated dynamically on every run based",
    "on the Dask cluster you're connected to and which workers have pieces of",
    "the training data",
    "dask.DataFrame.map_partitions() expects each call to return a pandas DataFrame or Series",
    "for multi-class classification with sparse matrices, pred_contrib predictions",
    "are returned as a list of sparse matrices (one per class)",
    "pred_contrib output will have one column per feature,",
    "plus one more for the base value",
    "need to tell Dask the expected type and shape of individual preds",
    "by default, dask.array.concatenate() concatenates sparse arrays into a COO matrix",
    "the code below is used instead to ensure that the sparse type is preserved during concatentation",
    "At this point, `out` is a list of lists of delayeds (each of which points to a matrix).",
    "Concatenate them to return a list of Dask Arrays.",
    "the note on custom objective functions in LGBMModel.__init__ is not",
    "currently relevant for the Dask estimators",
    "DaskLGBMClassifier does not support group, eval_group, early_stopping_rounds.",
    "DaskLGBMClassifier support for callbacks and init_model is not tested",
    "the note on custom objective functions in LGBMModel.__init__ is not",
    "currently relevant for the Dask estimators",
    "DaskLGBMRegressor does not support group, eval_class_weight, eval_group, early_stopping_rounds.",
    "DaskLGBMRegressor support for callbacks and init_model is not tested",
    "the note on custom objective functions in LGBMModel.__init__ is not",
    "currently relevant for the Dask estimators",
    "DaskLGBMRanker does not support eval_class_weight or early stopping",
    "DaskLGBMRanker support for callbacks and init_model is not tested",
    "coding: utf-8",
    "load or create your dataset",
    "create dataset for lightgbm",
    "if you want to re-use data, remember to set free_raw_data=False",
    "specify your configurations as a dict",
    "generate feature names",
    "feature_name and categorical_feature",
    "check feature name",
    "save model to file",
    "dump model to JSON (and save to file)",
    "feature names",
    "feature importances",
    "load model to predict",
    "can only predict with the best iteration (or the saving iteration)",
    "eval with loaded model",
    "dump model with pickle",
    "load model with pickle to predict",
    "can predict with any iteration when loaded in pickle way",
    "eval with loaded model",
    "continue training",
    "init_model accepts:",
    "1. model file name",
    "2. Booster()",
    "decay learning rates",
    "learning_rates accepts:",
    "1. list/tuple with length = num_boost_round",
    "2. function(curr_iter)",
    "change other parameters during training",
    "self-defined objective function",
    "f(preds: array, train_data: Dataset) -> grad: array, hess: array",
    "log likelihood loss",
    "self-defined eval metric",
    "f(preds: array, train_data: Dataset) -> name: str, eval_result: float, is_higher_better: bool",
    "binary error",
    "NOTE: when you do customized loss function, the default prediction value is margin",
    "This may make built-in evaluation metric calculate wrong results",
    "For example, we are doing log likelihood loss, the prediction is score before logistic transformation",
    "Keep this in mind when you use the customization",
    "another self-defined eval metric",
    "f(preds: array, train_data: Dataset) -> name: str, eval_result: float, is_higher_better: bool",
    "accuracy",
    "NOTE: when you do customized loss function, the default prediction value is margin",
    "This may make built-in evaluation metric calculate wrong results",
    "For example, we are doing log likelihood loss, the prediction is score before logistic transformation",
    "Keep this in mind when you use the customization",
    "callback",
    "coding: utf-8",
    "load or create your dataset",
    "train",
    "predict",
    "eval",
    "feature importances",
    "self-defined eval metric",
    "f(y_true: array, y_pred: array) -> name: str, eval_result: float, is_higher_better: bool",
    "Root Mean Squared Logarithmic Error (RMSLE)",
    "train",
    "another self-defined eval metric",
    "f(y_true: array, y_pred: array) -> name: str, eval_result: float, is_higher_better: bool",
    "Relative Absolute Error (RAE)",
    "train",
    "predict",
    "eval",
    "other scikit-learn modules",
    "coding: utf-8",
    "load or create your dataset",
    "create dataset for lightgbm",
    "specify your configurations as a dict",
    "train",
    "coding: utf-8",
    "################",
    "Simulate some binary data with a single categorical and",
    "single continuous predictor",
    "################",
    "Set up a couple of utilities for our experiments",
    "################",
    "Observe the behavior of `binary` and `xentropy` objectives",
    "Trying this throws an error on non-binary values of y:",
    "experiment('binary', label_type='probability', DATA)",
    "The speed of `binary` is not drastically different than",
    "`xentropy`. `xentropy` runs faster than `binary` in many cases, although",
    "there are reasons to suspect that `binary` should run faster when the",
    "label is an integer instead of a float",
    "coding: utf-8",
    "load or create your dataset",
    "create dataset for lightgbm",
    "specify your configurations as a dict",
    "train",
    "save model to file",
    "predict",
    "eval",
    "We can also open HDF5 file once and get access to",
    "With binary dataset created, we can use either Python API or cmdline version to train.",
    "",
    "Note: in order to create exactly the same dataset with the one created in simple_example.py, we need",
    "to modify simple_example.py to pass numpy array instead of pandas DataFrame to Dataset constructor.",
    "The reason is that DataFrame column names will be used in Dataset. For a DataFrame with Int64Index",
    "as columns, Dataset will use column names like [\"0\", \"1\", \"2\", ...]. While for numpy array, column names",
    "are using the default one assigned in C++ code (dataset_loader.cpp), like [\"Column_0\", \"Column_1\", ...].",
    "Y has a single column and we read it in single shot. So store it as an 1-d array.",
    "We use random access for data sampling when creating LightGBM Dataset from Sequence.",
    "When accessing any element in a HDF5 chunk, it's read entirely.",
    "To save I/O for sampling, we should keep number of total chunks much larger than sample count.",
    "Here we are just creating a chunk size that matches with batch_size.",
    "",
    "Also note that the data is stored in row major order to avoid extra copy when passing to",
    "lightgbm Dataset.",
    "Save to 2 HDF5 files for demonstration.",
    "We can store multiple datasets inside a single HDF5 file.",
    "Separating X and Y for choosing best chunk size for data loading.",
    "split training data into two partitions",
    "make this array dense because we're splitting across",
    "a sparse boundary to partition the data",
    "the code below uses sklearn.metrics, but this requires pulling all of the",
    "predictions and target values back from workers to the client",
    "",
    "for larger datasets, consider the metrics from dask-ml instead",
    "https://ml.dask.org/modules/api.html#dask-ml-metrics-metrics",
    "coding: utf-8",
    "!/usr/bin/env python3",
    "-*- coding: utf-8 -*-",
    "",
    "LightGBM documentation build configuration file, created by",
    "sphinx-quickstart on Thu May  4 14:30:58 2017.",
    "",
    "This file is execfile()d with the current directory set to its",
    "containing dir.",
    "",
    "Note that not all possible configuration values are present in this",
    "autogenerated file.",
    "",
    "All configuration values have a default; values that are commented out",
    "serve to show the default.",
    "If extensions (or modules to document with autodoc) are in another directory,",
    "add these directories to sys.path here. If the directory is relative to the",
    "documentation root, use os.path.abspath to make it absolute.",
    "-- General configuration ------------------------------------------------",
    "If your documentation needs a minimal Sphinx version, state it here.",
    "Add any Sphinx extension module names here, as strings. They can be",
    "extensions coming with Sphinx (named 'sphinx.ext.*') or your custom",
    "ones.",
    "mock out modules",
    "hide type hints in API docs",
    "Generate autosummary pages. Output should be set with: `:toctree: pythonapi/`",
    "Only the class' docstring is inserted.",
    "If true, `todo` and `todoList` produce output, else they produce nothing.",
    "The master toctree document.",
    "General information about the project.",
    "The name of an image file (relative to this directory) to place at the top",
    "of the sidebar.",
    "The name of an image file (relative to this directory) to use as a favicon of",
    "the docs.  This file should be a Windows icon file (.ico) being 16x16 or 32x32",
    "pixels large.",
    "The version info for the project you're documenting, acts as replacement for",
    "|version| and |release|, also used in various other places throughout the",
    "built documents.",
    "The short X.Y version.",
    "The full version, including alpha/beta/rc tags.",
    "The language for content autogenerated by Sphinx. Refer to documentation",
    "for a list of supported languages.",
    "",
    "This is also used if you do content translation via gettext catalogs.",
    "Usually you set \"language\" from the command line for these cases.",
    "List of patterns, relative to source directory, that match files and",
    "directories to ignore when looking for source files.",
    "This patterns also effect to html_static_path and html_extra_path",
    "The name of the Pygments (syntax highlighting) style to use.",
    "-- Configuration for C API docs generation ------------------------------",
    "-- Options for HTML output ----------------------------------------------",
    "The theme to use for HTML and HTML Help pages.  See the documentation for",
    "a list of builtin themes.",
    "Theme options are theme-specific and customize the look and feel of a theme",
    "further.  For a list of options available for each theme, see the",
    "documentation.",
    "Add any paths that contain custom static files (such as style sheets) here,",
    "relative to this directory. They are copied after the builtin static files,",
    "so a file named \"default.css\" will overwrite the builtin \"default.css\".",
    "-- Options for HTMLHelp output ------------------------------------------",
    "Output file base name for HTML help builder.",
    "-- Options for LaTeX output ---------------------------------------------",
    "The name of an image file (relative to this directory) to place at the top of",
    "the title page.",
    "Warning! The following code can cause buffer overflows on RTD.",
    "Consider suppressing output completely if RTD project silently fails.",
    "Refer to https://github.com/svenevs/exhale",
    "/blob/fe7644829057af622e467bb529db6c03a830da99/exhale/deploy.py#L99-L111",
    "Warning! The following code can cause buffer overflows on RTD.",
    "Consider suppressing output completely if RTD project silently fails.",
    "Refer to https://github.com/svenevs/exhale",
    "/blob/fe7644829057af622e467bb529db6c03a830da99/exhale/deploy.py#L99-L111",
    "coding: utf-8",
    "This is a basic test for floating number parsing.",
    "Most of the test cases come from:",
    "https://github.com/dmlc/xgboost/blob/master/tests/cpp/common/test_charconv.cc",
    "https://github.com/Alexhuszagh/rust-lexical/blob/master/data/test-parse-unittests/strtod_tests.toml",
    "FLT_MAX",
    "FLT_MIN",
    "DBL_MAX (1 + (1 - 2^-52)) * 2^1023 = (2^53 - 1) * 2^971",
    "2^971 * (2^53 - 1 + 1/2) : the smallest number resolving to inf",
    "Near DBL_MIN",
    "DBL_MIN 2^-1022",
    "The behavior for parsing -nan depends on implementation.",
    "Thus we skip binary check for negative nan.",
    "See comment in test_cases.",
    "Constants",
    "Start with some content:",
    "Clear & re-use:",
    "Output should match new content:",
    "Number of trials for each new ChunkedArray configuration. Pass 100 times over the search space:",
    "Each outer loop iteration changes the test by adding +1 chunk. We start with 1 chunk only:",
    "Sweep valid and invalid addresses with a ChunkedArray with `chunks` chunks:",
    "Compute a new trial address & value & if it is a valid address:",
    "Insert item. If at a valid address, 0 is returned, otherwise, -1 is returned:",
    "If at valid address, check that the stored value is correct & remember it for the future:",
    "Check the just-stored value with getitem():",
    "Also store the just-stored value for future tracking:",
    "Final check: ensure even with overrides, all valid insertions store the latest value at that address:",
    "Test in 2 ways that the values are correctly laid out in memory:",
    "coding: utf-8",
    "we don't need lib_lightgbm while building docs",
    "coding: utf-8",
    "check saved model persistence",
    "we need to check the consistency of model file here, so test for exact equal",
    "check early stopping is working. Make it stop very early, so the scores should be very close to zero",
    "scores likely to be different, but prediction should still be the same",
    "test that shape is checked during prediction",
    "The simple implementation is just a single \"return self.ndarray[idx]\"",
    "The following is for demo and testing purpose.",
    "whole col",
    "half col",
    "Create dataset from numpy array directly.",
    "Create dataset using Sequence.",
    "Test for validation set.",
    "Select some random rows as valid data.",
    "From Dataset constructor, with dataset from numpy array.",
    "From Dataset.create_valid, with dataset from sequence.",
    "test that method works even with free_raw_data=True",
    "test that method works but sets raw data to None in case of immergeable data types",
    "test that method works for different data types",
    "Set extremely harsh penalties, so CEGB will block most splits.",
    "Compare pairs of penalties, to ensure scaling works as intended",
    "Reset booster1's parameters to p2, so the parameter section of the file matches.",
    "should resolve duplicate aliases, and prefer the main parameter",
    "should choose a value from an alias and set that value on main param",
    "if only an alias is used",
    "should use the default if main param and aliases are missing",
    "all changes should be made on copies and not modify the original",
    "coding: utf-8",
    "add target, weight, and group to DataFrame so that partitions abide by group boundaries.",
    "set_index ensures partitions are based on group id.",
    "See https://stackoverflow.com/questions/49532824/dask-dataframe-split-partitions-based-on-a-column-or-function.",
    "separate target, weight from features.",
    "encode group identifiers into run-length encoding, the format LightGBMRanker is expecting",
    "so that within each partition, sum(g) = n_samples.",
    "ranking arrays: one chunk per group. Each chunk must include all columns.",
    "make one categorical feature relevant to the target",
    "https://github.com/microsoft/LightGBM/issues/4118",
    "extra predict() parameters should be passed through correctly",
    "pref_leaf values should have the right shape",
    "and values that look like valid tree nodes",
    "be sure LightGBM actually used at least one categorical column,",
    "and that it was correctly treated as a categorical feature",
    "shape depends on whether it is binary or multiclass classification",
    "in the special case of multi-class classification using scipy sparse matrices,",
    "the output of `.predict(..., pred_contrib=True)` is a list of sparse matrices (one per class)",
    "",
    "since that case is so different than all other cases, check the relevant things here",
    "and then return early",
    "raw scores will probably be different, but at least check that all predicted classes are the same",
    "be sure LightGBM actually used at least one categorical column,",
    "and that it was correctly treated as a categorical feature",
    "* shape depends on whether it is binary or multiclass classification",
    "* matrix for binary classification is of the form [feature_contrib, base_value],",
    "for multi-class it's [feat_contrib_class1, base_value_class1, feat_contrib_class2, base_value_class2, etc.]",
    "* contrib outputs for distributed training are different than from local training, so we can just test",
    "that the output has the right shape and base values are in the right position",
    "check that found ports are different for same address (LocalCluster)",
    "check that the ports are indeed open",
    "Scores should be the same",
    "Predictions should be roughly the same.",
    "pref_leaf values should have the right shape",
    "and values that look like valid tree nodes",
    "extra predict() parameters should be passed through correctly",
    "be sure LightGBM actually used at least one categorical column,",
    "and that it was correctly treated as a categorical feature",
    "contrib outputs for distributed training are different than from local training, so we can just test",
    "that the output has the right shape and base values are in the right position",
    "be sure LightGBM actually used at least one categorical column,",
    "and that it was correctly treated as a categorical feature",
    "Quantiles should be right",
    "be sure LightGBM actually used at least one categorical column,",
    "and that it was correctly treated as a categorical feature",
    "rebalance small dask.Array dataset for better performance.",
    "use many trees + leaves to overfit, help ensure that Dask data-parallel strategy matches that of",
    "serial learner. See https://github.com/microsoft/LightGBM/issues/3292#issuecomment-671288210.",
    "distributed ranker should be able to rank decently well and should",
    "have high rank correlation with scores from serial ranker.",
    "extra predict() parameters should be passed through correctly",
    "pref_leaf values should have the right shape",
    "and values that look like valid tree nodes",
    "be sure LightGBM actually used at least one categorical column,",
    "and that it was correctly treated as a categorical feature",
    "Use larger trainset to prevent premature stopping due to zero loss, causing num_trees() < n_estimators.",
    "Use small chunk_size to avoid single-worker allocation of eval data partitions.",
    "test eval_class_weight, eval_init_score on binary-classification task.",
    "Note: objective's default `metric` will be evaluated in evals_result_ in addition to all eval_metrics.",
    "create eval_sets by creating new datasets or copying training data.",
    "total number of trees scales up for ova classifier.",
    "check that early stopping was not applied.",
    "checks that evals_result_ and best_score_ contain expected data and eval_set names.",
    "check that each eval_name and metric exists for all eval sets, allowing for the",
    "case when a worker receives a fully-padded eval_set component which is not evaluated.",
    "should be able to use the class without specifying a client",
    "should be able to set client after construction",
    "data on cluster1",
    "create identical data on cluster2",
    "at this point, the result of default_client() is client2 since it was the most recently",
    "created. So setting client to client1 here to test that you can select a non-default client",
    "unfitted model should survive pickling round trip, and pickling",
    "shouldn't have side effects on the model object",
    "client will always be None after unpickling",
    "fitted model should survive pickling round trip, and pickling",
    "shouldn't have side effects on the model object",
    "client will always be None after unpickling",
    "rebalance data to be sure that each worker has a piece of the data",
    "model 1 - no network parameters given",
    "model 2 - machines given",
    "model 3 - local_listen_port given",
    "training should fail because LightGBM will try to use the same",
    "port for multiple worker processes on the same machine",
    "rebalance data to be sure that each worker has a piece of the data",
    "test that \"machines\" is actually respected by creating a socket that uses",
    "one of the ports mentioned in \"machines\"",
    "The above error leaves a worker waiting",
    "an informative error should be raised if \"machines\" has duplicates",
    "\"client\" should be the only different, and the final argument",
    "value of the root node is 0 when init_score is set",
    "this test is separate because it takes a not-yet-constructed estimator",
    "coding: utf-8",
    "coding: utf-8",
    "build target, group ID vectors.",
    "build y/target and group-id vectors with user-specified group sizes.",
    "build y/target and group-id vectors according to n_samples, avg_gs, and random_gs.",
    "groups should contain > 1 element for pairwise learning objective.",
    "build feature data, X. Transform first few into informative features.",
    "coding: utf-8",
    "prediction result is actually not transformed (is raw) due to custom objective",
    "sklearn <0.23 does not have a stacking classifier and n_features_in_ property",
    "sklearn <0.23 does not have a stacking regressor and n_features_in_ property",
    "sklearn < 0.22 does not have the post fit attribute: classes_",
    "sklearn < 0.23 does not have as_frame parameter",
    "sklearn < 0.22 does not have the post fit attribute: classes_",
    "sklearn < 0.23 does not have as_frame parameter",
    "Test if random_state is properly stored",
    "Test if two random states produce identical models",
    "Test if subsequent fits sample from random_state object and produce different models",
    "Test that the largest element is NOT the same, the smallest can be the same, i.e. zero",
    "With default params",
    "Tests same probabilities",
    "Tests same predictions",
    "Tests same raw scores",
    "Tests same leaf indices",
    "Tests same feature contributions",
    "Tests other parameters for the prediction works",
    "Tests start_iteration",
    "Tests same probabilities, starting from iteration 10",
    "Tests same predictions, starting from iteration 10",
    "Tests same raw scores, starting from iteration 10",
    "Tests same leaf indices, starting from iteration 10",
    "Tests same feature contributions, starting from iteration 10",
    "Tests other parameters for the prediction works, starting from iteration 10",
    "no custom objective, no custom metric",
    "default metric",
    "non-default metric",
    "no metric",
    "non-default metric in eval_metric",
    "non-default metric with non-default metric in eval_metric",
    "non-default metric with multiple metrics in eval_metric",
    "non-default metric with multiple metrics in eval_metric for LGBMClassifier",
    "default metric for non-default objective",
    "non-default metric for non-default objective",
    "no metric",
    "non-default metric in eval_metric for non-default objective",
    "non-default metric with non-default metric in eval_metric for non-default objective",
    "non-default metric with multiple metrics in eval_metric for non-default objective",
    "custom objective, no custom metric",
    "default regression metric for custom objective",
    "non-default regression metric for custom objective",
    "multiple regression metrics for custom objective",
    "no metric",
    "default regression metric with non-default metric in eval_metric for custom objective",
    "non-default regression metric with metric in eval_metric for custom objective",
    "multiple regression metrics with metric in eval_metric for custom objective",
    "multiple regression metrics with multiple metrics in eval_metric for custom objective",
    "no custom objective, custom metric",
    "default metric with custom metric",
    "non-default metric with custom metric",
    "multiple metrics with custom metric",
    "custom metric (disable default metric)",
    "default metric for non-default objective with custom metric",
    "non-default metric for non-default objective with custom metric",
    "multiple metrics for non-default objective with custom metric",
    "custom metric (disable default metric for non-default objective)",
    "custom objective, custom metric",
    "custom metric for custom objective",
    "non-default regression metric with custom metric for custom objective",
    "multiple regression metrics with custom metric for custom objective",
    "default metric and invalid binary metric is replaced with multiclass alternative",
    "invalid objective is replaced with default multiclass one",
    "and invalid binary metric is replaced with multiclass alternative",
    "default metric for non-default multiclass objective",
    "and invalid binary metric is replaced with multiclass alternative",
    "default metric and invalid multiclass metric is replaced with binary alternative",
    "invalid multiclass metric is replaced with binary alternative for custom objective",
    "Verify that can receive a list of metrics, only callable",
    "Verify that can receive a list of custom and built-in metrics",
    "Verify that works as expected when eval_metric is empty",
    "Verify that can receive a list of metrics, only built-in",
    "Verify that eval_metric is robust to receiving a list with None",
    "training data as eval_set",
    "feval",
    "single eval_set",
    "two eval_set",
    "sklearn < 0.22 requires passing \"attributes\" argument",
    "Test that estimators are default-constructible",
    "coding: utf-8",
    "coding: utf-8",
    "check that default gives same result as k = 1",
    "check against independent calculation for k = 1",
    "check against independent calculation for k = 2",
    "check against independent calculation for k = 10",
    "check cases where predictions are equal",
    "should give same result as binary auc for 2 classes",
    "test the case where all predictions are equal",
    "test that weighted data gives different auc_mu",
    "test that equal data weights give same auc_mu as unweighted data",
    "should give 1 when accuracy = 1",
    "test loading class weights",
    "no early stopping",
    "early stopping occurs",
    "test custom eval metrics",
    "shuffle = False, override metric in params",
    "shuffle = True, callbacks",
    "enable display training loss",
    "self defined folds",
    "LambdaRank",
    "... with l2 metric",
    "... with NDCG (default) metric",
    "self defined folds with lambdarank",
    "with early stopping",
    "predict by each fold booster",
    "fold averaging",
    "without early stopping",
    "test feature_names with whitespaces",
    "This has non-ascii strings.",
    "take subsets and train",
    "generate CSR sparse dataset",
    "convert data to dense and get back same contribs",
    "validate the values are the same",
    "validate using CSC matrix",
    "validate the values are the same",
    "generate CSR sparse dataset",
    "convert data to dense and get back same contribs",
    "validate the values are the same",
    "validate using CSC matrix",
    "validate the values are the same",
    "Note there is an extra column added to the output for the expected value",
    "Note output CSC shape should be same as CSR output shape",
    "test sliced labels",
    "append some columns",
    "append some rows",
    "test sliced 2d matrix",
    "test sliced CSR",
    "trees start at position 1.",
    "split_features are in 4th line.",
    "test if a penalty as high as the depth indeed prohibits all monotone splits",
    "The penalization is so high that the first 2 features should not be used here",
    "Check that a very high penalization is the same as not using the features at all",
    "no fobj, no feval",
    "default metric",
    "non-default metric in params",
    "default metric in args",
    "non-default metric in args",
    "metric in args overwrites one in params",
    "multiple metrics in params",
    "multiple metrics in args",
    "remove default metric by 'None' in list",
    "remove default metric by 'None' aliases",
    "fobj, no feval",
    "no default metric",
    "metric in params",
    "metric in args",
    "metric in args overwrites its' alias in params",
    "multiple metrics in params",
    "multiple metrics in args",
    "no fobj, feval",
    "default metric with custom one",
    "non-default metric in params with custom one",
    "default metric in args with custom one",
    "non-default metric in args with custom one",
    "metric in args overwrites one in params, custom one is evaluated too",
    "multiple metrics in params with custom one",
    "multiple metrics in args with custom one",
    "custom metric is evaluated despite 'None' is passed",
    "fobj, feval",
    "no default metric, only custom one",
    "metric in params with custom one",
    "metric in args with custom one",
    "metric in args overwrites one in params, custom one is evaluated too",
    "multiple metrics in params with custom one",
    "multiple metrics in args with custom one",
    "custom metric is evaluated despite 'None' is passed",
    "no fobj, no feval",
    "default metric",
    "default metric in params",
    "non-default metric in params",
    "multiple metrics in params",
    "remove default metric by 'None' aliases",
    "fobj, no feval",
    "no default metric",
    "metric in params",
    "multiple metrics in params",
    "no fobj, feval",
    "default metric with custom one",
    "default metric in params with custom one",
    "non-default metric in params with custom one",
    "multiple metrics in params with custom one",
    "custom metric is evaluated despite 'None' is passed",
    "fobj, feval",
    "no default metric, only custom one",
    "metric in params with custom one",
    "multiple metrics in params with custom one",
    "custom metric is evaluated despite 'None' is passed",
    "multiclass default metric",
    "multiclass default metric with custom one",
    "multiclass metric alias with custom one for custom objective",
    "no metric for invalid class_num",
    "custom metric for invalid class_num",
    "multiclass metric alias with custom one with invalid class_num",
    "multiclass default metric without num_class",
    "multiclass metric alias",
    "multiclass metric",
    "non-valid metric for multiclass objective",
    "non-default num_class for default objective",
    "no metric with non-default num_class for custom objective",
    "multiclass metric alias for custom objective",
    "multiclass metric for custom objective",
    "binary metric with non-default num_class for custom objective",
    "Expect three metrics but mean and stdv for each metric",
    "test XGBoost-style return value",
    "test numpy-style return value",
    "test bins string type",
    "test histogram is disabled for categorical features",
    "test for lgb.train",
    "test feval for lgb.train",
    "test with two valid data for lgb.train",
    "test for lgb.cv",
    "test feval for lgb.cv",
    "test that binning works properly for features with only positive or only negative values",
    "decreasing without freeing raw data is allowed",
    "decreasing before lazy init is allowed",
    "increasing is allowed",
    "decreasing with disabled filter is allowed",
    "decreasing with enabled filter is disallowed;",
    "also changes of other params are disallowed",
    "check extra trees increases regularization",
    "check path smoothing increases regularization",
    "test edge case with one leaf",
    "check that constraint containing all features is equivalent to no constraint",
    "check that constraint partitioning the features reduces train accuracy",
    "check that constraints consisting of single features reduce accuracy further",
    "test that interaction constraints work when not all features are used",
    "check that setting linear_tree=True fits better than ordinary trees when data has linear relationship",
    "test again with nans in data",
    "test again with bagging",
    "test with a feature that has only one non-nan value",
    "test with a categorical feature",
    "test refit: same results on same data",
    "test refit with save and load",
    "test refit: different results training on different data",
    "test when num_leaves - 1 < num_features and when num_leaves - 1 > num_features",
    "test that the predict once with all iterations equals summed results with start_iteration and num_iteration",
    "test the case where start_iteration <= 0, and num_iteration is None",
    "test the case where start_iteration > 0, and num_iteration <= 0",
    "test the case where start_iteration > 0, and num_iteration <= 0, with pred_leaf=True",
    "test the case where start_iteration > 0, and num_iteration <= 0, with pred_contrib=True",
    "test for regression",
    "test both with and without early stopping",
    "test for multi-class",
    "test both with and without early stopping",
    "test for binary",
    "test both with and without early stopping",
    "test against sklearn average precision metric",
    "test that average precision is 1 where model predicts perfectly",
    "coding: utf-8",
    "If compiled appropriately, the same installation will support both GPU and CPU.",
    "coding: utf-8",
    "coding: utf-8",
    "These are helper functions to allow doing a stack unwind",
    "after an R allocation error, which would trigger a long jump.",
    "convert from one-based to zero-based index",
    "if any feature names were larger than allocated size,",
    "allow for a larger size and try again",
    "convert from boundaries to size",
    "--- start Booster interfaces",
    "if any eval names were larger than allocated size,",
    "allow for a larger size and try again",
    "if the model string was larger than the initial buffer, allocate a bigger buffer and try again",
    "if the model string was larger than the initial buffer, allocate a bigger buffer and try again",
    ".Call() calls",
    "coding: utf-8",
    "alias table",
    "names",
    "from strings",
    "tails",
    "tails",
    "coding: utf-8",
    "Single row predictor to abstract away caching logic",
    "create boosting",
    "initialize the boosting",
    "create objective function",
    "initialize the objective function",
    "create training metric",
    "reset the boosting",
    "create objective function",
    "initialize the objective function",
    "calculate the nonzero data and indices size",
    "allocate data and indices arrays",
    "Get the number of trees per iteration (for multiclass scenario we output multiple sparse matrices)",
    "aggregated per row feature contribution results",
    "keep track of the row_vector sizes for parallelization",
    "copy vector results to output for each row",
    "Get the number of trees per iteration (for multiclass scenario we output multiple sparse matrices)",
    "aggregated per row feature contribution results",
    "calculate number of elements per column to construct",
    "the CSC matrix with random access",
    "keep track of column counts",
    "keep track of beginning index for each column",
    "keep track of beginning index for each matrix",
    "Note: we parallelize across matrices instead of rows because of the column_counts[m][col_idx] increment inside the loop",
    "store the row index",
    "update column count",
    "explicitly declare symbols from LightGBM namespace",
    "some help functions used to convert data",
    "Row iterator of on column for CSC matrix",
    "return value at idx, only can access by ascent order",
    "return next non-zero pair, if index < 0, means no more data",
    "start of c_api functions",
    "This API is to keep python binding's behavior the same with C++ implementation.",
    "Sample count, random seed etc. should be provided in parameters.",
    "sample data first",
    "sample data first",
    "sample data first",
    "local buffer to re-use memory",
    "sample data first",
    "no more data",
    "---- start of booster",
    "Single row in row-major format:",
    "---- start of some help functions",
    "data is array of pointers to individual rows",
    "set number of threads for openmp",
    "check for alias",
    "read parameters from config file",
    "remove str after \"#\"",
    "check for alias again",
    "load configs",
    "prediction is needed if using input initial model(continued train)",
    "need to continue training",
    "sync up random seed for data partition",
    "load Training data",
    "load data for distributed training",
    "load data for single machine",
    "need save binary file",
    "create training metric",
    "only when have metrics then need to construct validation data",
    "Add validation data, if it exists",
    "add",
    "need save binary file",
    "add metric for validation data",
    "output used time on each iteration",
    "need init network",
    "create boosting",
    "create objective function",
    "load training data",
    "initialize the objective function",
    "initialize the boosting",
    "add validation data into boosting",
    "convert model to if-else statement code",
    "create predictor",
    "Free memory",
    "create predictor",
    "label_gain = 2^i - 1, may overflow, so we use 31 here",
    "counts for all labels",
    "start from top label, and accumulate DCG",
    "counts for all labels",
    "calculate k Max DCG by one pass",
    "get sorted indices by score",
    "calculate multi dcg by one pass",
    "wait for all client start up",
    "Don't call MPI_Finalize() here: If the destructor was called because only this node had an exception, calling MPI_Finalize() will cause all nodes to hang.",
    "Instead we will handle finalize/abort for MPI in main().",
    "default set to -1",
    "distance at k-th communication, distance[k] = 2^k",
    "set incoming rank at k-th commuication",
    "set outgoing rank at k-th commuication",
    "default set as -1",
    "construct all recursive halving map for all machines",
    "let 1 << k <= num_machines",
    "distance of each communication",
    "if num_machines = 2^k, don't need to group machines",
    "communication direction, %2 == 0 is positive",
    "neighbor at k-th communication",
    "receive data block at k-th communication",
    "send data block at k-th communication",
    "if num_machines != 2^k, need to group machines",
    "group, two machine in one group, total \"rest\" groups will have 2 machines.",
    "let left machine as group leader",
    "cache block information for groups, group with 2 machines will have double block size",
    "convert from group to node leader",
    "convert from node to group",
    "meet new group",
    "add block len for this group",
    "calculate the group block start",
    "not need to construct",
    "get receive block information",
    "accumulate block len",
    "get send block information",
    "accumulate block len",
    "static member definition",
    "if small package or small count , do it by all gather.(reduce the communication times.)",
    "assign the blocks to every rank.",
    "do reduce scatter",
    "do all gather",
    "assign blocks",
    "need use buffer here, since size of \"output\" is smaller than size after all gather",
    "copy back",
    "assign blocks",
    "start all gather",
    "when num_machines is small and data is large",
    "use output as receive buffer",
    "get current local block size",
    "get out rank",
    "get in rank",
    "get send information",
    "get recv information",
    "send and recv at same time",
    "rotate in-place",
    "use output as receive buffer",
    "get current local block size",
    "get send information",
    "get recv information",
    "send and recv at same time",
    "use output as receive buffer",
    "send and recv at same time",
    "send local data to neighbor first",
    "receive neighbor data first",
    "reduce",
    "get target",
    "get send information",
    "get recv information",
    "send and recv at same time",
    "reduce",
    "send result to neighbor",
    "receive result from neighbor",
    "copy result",
    "start up socket",
    "parse clients from file",
    "get ip list of local machine",
    "get local rank",
    "construct listener",
    "construct communication topo",
    "construct linkers",
    "free listener",
    "set timeout",
    "accept incoming socket",
    "receive rank",
    "add new socket",
    "save ranks that need to connect with",
    "start listener",
    "start connect",
    "let smaller rank connect to larger rank",
    "send local rank",
    "wait for listener",
    "print connected linkers",
    "only need to copy subset",
    "avoid to copy subset many times",
    "avoid out of range",
    "may need to recopy subset",
    "valid the type",
    "Constructors",
    "Get type tag",
    "Comparisons",
    "This has to be separate, not in Statics, because Json() accesses",
    "statics().null.",
    "advance until next line, or end of input",
    "advance until closing tokens",
    "The usual case: non-escaped characters",
    "Handle escapes",
    "Extract 4-byte escape sequence",
    "Explicitly check length of the substring. The following loop",
    "relies on std::string returning the terminating NUL when",
    "accessing str[length]. Checking here reduces brittleness.",
    "JSON specifies that characters outside the BMP shall be encoded as a",
    "pair of 4-hex-digit \\u escapes encoding their surrogate pair",
    "components. Check whether we're in the middle of such a beast: the",
    "previous codepoint was an escaped lead (high) surrogate, and this is",
    "a trail (low) surrogate.",
    "Reassemble the two surrogate pairs into one astral-plane character,",
    "per the UTF-16 algorithm.",
    "Integer part",
    "Decimal part",
    "Exponent part",
    "Check for any trailing garbage",
    "Documented in json11.hpp",
    "Check for another object",
    "get column names",
    "load label idx first",
    "erase label column name",
    "load ignore columns",
    "load weight idx",
    "load group idx",
    "don't support query id in data file when using distributed training",
    "read data to memory",
    "sample data",
    "construct feature bin mappers",
    "initialize label",
    "extract features",
    "sample data from file",
    "construct feature bin mappers",
    "initialize label",
    "extract features",
    "load data from binary file",
    "check meta data",
    "need to check training data",
    "read data in memory",
    "initialize label",
    "extract features",
    "Get number of lines of data file",
    "initialize label",
    "extract features",
    "load data from binary file",
    "not need to check validation data",
    "check meta data",
    "buffer to read binary file",
    "check token",
    "read size of header",
    "re-allocmate space if not enough",
    "read header",
    "get header",
    "num_groups",
    "real_feature_idx_",
    "feature2group",
    "feature2subfeature",
    "group_bin_boundaries",
    "group_feature_start_",
    "group_feature_cnt_",
    "get feature names",
    "write feature names",
    "get forced_bin_bounds_",
    "read size of meta data",
    "re-allocate space if not enough",
    "read meta data",
    "load meta data",
    "sample local used data if need to partition",
    "if not contain query file, minimal sample unit is one record",
    "if contain query file, minimal sample unit is one query",
    "if is new query",
    "read feature data",
    "read feature size",
    "re-allocate space if not enough",
    "raw data",
    "fill feature_names_ if not header",
    "get forced split",
    "if only one machine, find bin locally",
    "if have multi-machines, need to find bin distributed",
    "different machines will find bin for different features",
    "start and len will store the process feature indices for different machines",
    "machine i will find bins for features in [ start[i], start[i] + len[i] )",
    "free",
    "gather global feature bin mappers",
    "restore features bins from buffer",
    "---- private functions ----",
    "if features are ordered, not need to use hist_buf",
    "read all lines",
    "get query data",
    "if not contain query data, minimal sample unit is one record",
    "if contain query data, minimal sample unit is one query",
    "if is new query",
    "get query data",
    "if not contain query file, minimal sample unit is one record",
    "if contain query file, minimal sample unit is one query",
    "if is new query",
    "parse features",
    "get forced split",
    "check the range of label_idx, weight_idx and group_idx",
    "fill feature_names_ if not header",
    "start find bins",
    "if only one machine, find bin locally",
    "start and len will store the process feature indices for different machines",
    "machine i will find bins for features in [ start[i], start[i] + len[i] )",
    "free",
    "gather global feature bin mappers",
    "restore features bins from buffer",
    "if doesn't need to prediction with initial model",
    "parser",
    "set label",
    "free processed line:",
    "shrink_to_fit will be very slow in linux, and seems not free memory, disable for now",
    "text_reader_->Lines()[i].shrink_to_fit();",
    "push data",
    "if is used feature",
    "if need to prediction with initial model",
    "parser",
    "set initial score",
    "set label",
    "free processed line:",
    "shrink_to_fit will be very slow in Linux, and seems not free memory, disable for now",
    "text_reader_->Lines()[i].shrink_to_fit();",
    "push data",
    "if is used feature",
    "metadata_ will manage space of init_score",
    "text data can be free after loaded feature values",
    "parser",
    "set initial score",
    "set label",
    "push data",
    "if is used feature",
    "only need part of data",
    "need full data",
    "metadata_ will manage space of init_score",
    "read size of token",
    "remove duplicates",
    "deep copy function for BinMapper",
    "mean size for one bin",
    "need a new bin",
    "update bin upper bound",
    "last bin upper bound",
    "get list of distinct values",
    "get number of positive and negative distinct values",
    "include zero bounds and infinity bound",
    "add forced bounds, excluding zeros since we have already added zero bounds",
    "find remaining bounds",
    "find distinct_values first",
    "push zero in the front",
    "use the large value",
    "push zero in the back",
    "convert to int type first",
    "sort by counts",
    "will ignore the categorical of small counts",
    "Push the dummy bin for NaN",
    "Use MissingType::None to represent this bin contains all categoricals",
    "fix count of NaN bin",
    "check trivial(num_bin_ == 1) feature",
    "check useless bin",
    "When most_freq_bin_ != default_bin_, there are some additional data loading costs.",
    "so use most_freq_bin_  = default_bin_ when there is not so sparse",
    "calculate max bin of all features to select the int type in MultiValDenseBin",
    "for lambdarank, it needs query data for partition data in distributed learning",
    "need convert query_id to boundaries",
    "check weights",
    "check query boundries",
    "contain initial score file",
    "check weights",
    "get local weights",
    "check query boundries",
    "get local query boundaries",
    "contain initial score file",
    "get local initial scores",
    "re-load query weight",
    "save to nullptr",
    "save to nullptr",
    "save to nullptr",
    "default weight file name",
    "default init_score file name",
    "use first line to count number class",
    "default query file name",
    "root is in the depth 0",
    "non-leaf",
    "leaf",
    "use this for the missing value conversion",
    "Predict func by Map to ifelse",
    "use this for the missing value conversion",
    "non-leaf",
    "left subtree",
    "right subtree",
    "leaf",
    "non-leaf",
    "left subtree",
    "right subtree",
    "leaf",
    "recursive computation of SHAP values for a decision tree",
    "extend the unique path",
    "leaf node",
    "internal node",
    "see if we have already split on this feature,",
    "if so we undo that split so we can redo it for this node",
    "recursive sparse computation of SHAP values for a decision tree",
    "extend the unique path",
    "leaf node",
    "internal node",
    "see if we have already split on this feature,",
    "if so we undo that split so we can redo it for this node",
    "add names of objective function if not providing metric",
    "equal weights for all classes",
    "generate seeds by seed.",
    "sort eval_at",
    "Only push the non-training data",
    "check for conflicts",
    "check if objective, metric, and num_class match",
    "Change pool size to -1 (no limit) when using data parallel to reduce communication costs",
    "Check max_depth and num_leaves",
    "Fits in an int, and is more restrictive than the current num_leaves",
    "force col-wise for gpu & CUDA",
    "force gpu_use_dp for CUDA",
    "linear tree learner must be serial type and run on CPU device",
    "min_data_in_leaf must be at least 2 if path smoothing is active. This is because when the split is calculated",
    "the count is calculated using the proportion of hessian in the leaf which is rounded up to nearest int, so it can",
    "be 1 when there is actually no data in the leaf. In rare cases this can cause a bug because with path smoothing the",
    "calculated split gain can be positive even with zero gradient and hessian.",
    "In distributed mode, local node doesn't have histograms on all features, cannot perform \"intermediate\" monotone constraints.",
    "\"intermediate\" monotone constraints need to recompute splits. If the features are sampled when computing the",
    "split initially, then the sampling needs to be recorded or done once again, which is currently not supported",
    "first round: fill the single val group",
    "always push the last group",
    "put dense feature first",
    "sort by non zero cnt",
    "sort by non zero cnt, bigger first",
    "shuffle groups",
    "Using std::swap for vector<bool> will cause the wrong result.",
    "get num_features",
    "get bin_mappers",
    "for sparse multi value bin, we store the feature bin values with offset added",
    "for dense multi value bin, the feature bin values without offsets are used",
    "copy feature bin mapper data",
    "copy feature bin mapper data",
    "if not pass a filename, just append \".bin\" of original file",
    "get size of header",
    "size of feature names",
    "size of forced bins",
    "write header",
    "write feature names",
    "write forced bins",
    "get size of meta data",
    "write meta data",
    "write feature data",
    "get size of feature",
    "write feature",
    "write raw data; use row-major order so we can read row-by-row",
    "explicitly initialize template methods, for cross module call",
    "Only one multi-val group, just simply merge",
    "Skip the leading 0 when copying group_bin_boundaries.",
    "regenerate other fields",
    "store the importance first",
    "PredictRaw",
    "PredictRawByMap",
    "Predict",
    "PredictByMap",
    "PredictLeafIndex",
    "PredictLeafIndexByMap",
    "output model type",
    "output number of class",
    "output label index",
    "output max_feature_idx",
    "output objective",
    "output tree models",
    "store the importance first",
    "sort the importance",
    "use serialized string to restore this object",
    "Use first 128 chars to avoid exceed the message buffer.",
    "get number of classes",
    "get index of label",
    "get max_feature_idx first",
    "get average_output",
    "get feature names",
    "get monotone_constraints",
    "set zero",
    "predict all the trees for one iteration",
    "check early stopping",
    "set zero",
    "predict all the trees for one iteration",
    "check early stopping",
    "margin_threshold will be captured by value",
    "copy and sort",
    "margin_threshold will be captured by value",
    "Fix for compiler warnings about reaching end of control",
    "load forced_splits file",
    "init tree learner",
    "push training metrics",
    "create buffer for gradients and Hessians",
    "get max feature index",
    "get label index",
    "get feature names",
    "if need bagging, create buffer",
    "for a validation dataset, we need its score and metric",
    "update score",
    "objective function will calculate gradients and hessians",
    "random bagging, minimal unit is one record",
    "random bagging, minimal unit is one record",
    "if need bagging",
    "set bagging data to tree learner",
    "get subset",
    "output used time per iteration",
    "boosting from average label; or customized \"average\" if implemented for the current objective",
    "boosting first",
    "bagging logic",
    "need to copy gradients for bagging subset.",
    "shrinkage by learning rate",
    "update score",
    "only add default score one-time",
    "updates scores",
    "add model",
    "reset score",
    "remove model",
    "print message for metric",
    "pop last early_stopping_round_ models",
    "update training score",
    "we need to predict out-of-bag scores of data for boosting",
    "update validation score",
    "print training metric",
    "print validation metric",
    "set zero",
    "predict all the trees for one iteration",
    "predict all the trees for one iteration",
    "push training metrics",
    "not same training data, need reset score and others",
    "create score tracker",
    "update score",
    "create buffer for gradients and hessians",
    "load forced_splits file",
    "if need bagging, create buffer",
    "Get the max size of pool",
    "at least need 2 leaves",
    "push split information for all leaves",
    "initialize splits for leaf",
    "initialize data partition",
    "initialize ordered gradients and hessians",
    "cannot change is_hist_col_wise during training",
    "initialize splits for leaf",
    "initialize data partition",
    "initialize ordered gradients and hessians",
    "Get the max size of pool",
    "at least need 2 leaves",
    "push split information for all leaves",
    "some initial works before training",
    "root leaf",
    "only root leaf can be splitted on first time",
    "some initial works before finding best split",
    "find best threshold for every feature",
    "Get a leaf with max split gain",
    "Get split information for best leaf",
    "cannot split, quit",
    "split tree with best leaf",
    "reset histogram pool",
    "initialize data partition",
    "reset the splits for leaves",
    "Sumup for root",
    "use all data",
    "use bagging, only use part of data",
    "check depth of current leaf",
    "only need to check left leaf, since right leaf is in same level of left leaf",
    "no enough data to continue",
    "only have root",
    "put parent(left) leaf's histograms into larger leaf's histograms",
    "put parent(left) leaf's histograms to larger leaf's histograms",
    "construct smaller leaf",
    "construct larger leaf",
    "find splits",
    "only has root leaf",
    "start at root leaf",
    "before processing next node from queue, store info for current left/right leaf",
    "store \"best split\" for left and right, even if they might be overwritten by forced split",
    "then, compute own splits",
    "split info should exist because searching in bfs fashion - should have added from parent",
    "update before tree split",
    "don't need to update this in data-based parallel model",
    "split tree, will return right leaf",
    "store the true split gain in tree model",
    "don't need to update this in data-based parallel model",
    "store the true split gain in tree model",
    "init the leaves that used on next iteration",
    "update leave outputs if needed",
    "bag_mapper[index_mapper[i]]",
    "it is needed to filter the features after the above code.",
    "Otherwise, the `is_splittable` in `FeatureHistogram` will be wrong, and cause some features being accidentally filtered in the later nodes.",
    "for root leaf the \"parent\" output is its own output because we don't apply any smoothing to the root",
    "can't use GetParentOutput because leaf_splits doesn't have weight property set",
    "find splits",
    "identify features containing nans",
    "preallocate the matrix used to calculate linear model coefficients",
    "store only upper triangular half of matrix as an array, in row-major order",
    "this requires (max_num_feat + 1) * (max_num_feat + 2) / 2 entries (including the constant terms of the regression)",
    "we add another 8 to ensure cache lines are not shared among processors",
    "some initial works before training",
    "root leaf",
    "only root leaf can be splitted on first time",
    "some initial works before finding best split",
    "find best threshold for every feature",
    "Get a leaf with max split gain",
    "Get split information for best leaf",
    "cannot split, quit",
    "split tree with best leaf",
    "map data to leaf number",
    "calculate coefficients using the method described in Eq 3 of https://arxiv.org/pdf/1802.05640.pdf",
    "the coefficients vector is given by",
    "- (X_T * H * X + lambda) ^ (-1) * (X_T * g)",
    "where:",
    "X is the matrix where the first column is the feature values and the second is all ones,",
    "H is the diagonal matrix of the hessian,",
    "lambda is the diagonal matrix with diagonal entries equal to the regularisation term linear_lambda",
    "g is the vector of gradients",
    "the subscript _T denotes the transpose",
    "create array of pointers to raw data, and coefficient matrices, for each leaf",
    "clear the coefficient matrices",
    "aggregate results from different threads",
    "copy into eigen matrices and solve",
    "update the tree properties",
    "need to be able to hold smaller and larger best splits in SyncUpGlobalBestSplit",
    "get feature partition",
    "get local used features",
    "get best split at smaller leaf",
    "find local best split for larger leaf",
    "sync global best info",
    "update best split",
    "instantiate template classes, otherwise linker cannot find the code",
    "initialize SerialTreeLearner",
    "Get local rank and global machine size",
    "need to be able to hold smaller and larger best splits in SyncUpGlobalBestSplit",
    "allocate buffer for communication",
    "generate feature partition for current tree",
    "get local used feature",
    "get block start and block len for reduce scatter",
    "get buffer_write_start_pos_",
    "get buffer_read_start_pos_",
    "sync global data sumup info",
    "global sumup reduce",
    "copy back",
    "set global sumup info",
    "init global data count in leaf",
    "clear histogram buffer before synchronizing",
    "otherwise histogram contents from the previous iteration will be sent",
    "construct local histograms",
    "copy to buffer",
    "Reduce scatter for histogram",
    "restore global histograms from buffer",
    "only root leaf",
    "construct histgroms for large leaf, we init larger leaf as the parent, so we can just subtract the smaller leaf's histograms",
    "find local best split for larger leaf",
    "sync global best info",
    "set best split",
    "need update global number of data in leaf",
    "instantiate template classes, otherwise linker cannot find the code",
    "initialize SerialTreeLearner",
    "some additional variables needed for GPU trainer",
    "Initialize GPU buffers and kernels",
    "some functions used for debugging the GPU histogram construction",
    "printf(\"grad %g != %g (%d ULPs)\\n\", GET_GRAD(h1, i), GET_GRAD(h2, i), ulps);",
    "goto err;",
    "we roughly want 256 workgroups per device, and we have num_dense_feature4_ feature tuples.",
    "also guarantee that there are at least 2K examples per workgroup",
    "return 0;",
    "we have already copied ordered gradients, ordered Hessians and indices to GPU",
    "decide the best number of workgroups working on one feature4 tuple",
    "set work group size based on feature size",
    "each 2^exp_workgroups_per_feature workgroups work on a feature4 tuple",
    "we need to refresh the kernel arguments after reallocating",
    "The only argument that needs to be changed later is num_data_",
    "the GPU kernel will process all features in one call, and each",
    "2^exp_workgroups_per_feature (compile time constant) workgroup will",
    "process one feature4 tuple",
    "for the root node, indices are not copied",
    "for constant hessian, hessians are not copied except for the root node",
    "there will be 2^exp_workgroups_per_feature = num_workgroups / num_dense_feature4 sub-histogram per feature4",
    "and we will launch num_feature workgroups for this kernel",
    "will launch threads for all features",
    "the queue should be asynchronous, and we will can WaitAndGetHistograms() before we start processing dense feature groups",
    "copy the results asynchronously. Size depends on if double precision is used",
    "we will wait for this object in WaitAndGetHistograms",
    "when the output is ready, the computation is done",
    "values of this feature has been redistributed to multiple bins; need a reduction here",
    "how many feature-group tuples we have",
    "leave some safe margin for prefetching",
    "256 work-items per workgroup. Each work-item prefetches one tuple for that feature",
    "clear sparse/dense maps",
    "do nothing if no features can be processed on GPU",
    "allocate memory for all features (FIXME: 4 GB barrier on some devices, need to split to multiple buffers)",
    "unpin old buffer if necessary before destructing them",
    "make ordered_gradients and Hessians larger (including extra room for prefetching), and pin them",
    "allocate space for gradients and Hessians on device",
    "we will copy gradients and Hessians in after ordered_gradients_ and ordered_hessians_ are constructed",
    "allocate feature mask, for disabling some feature-groups' histogram calculation",
    "copy indices to the device",
    "histogram bin entry size depends on the precision (single/double)",
    "create output buffer, each feature has a histogram with device_bin_size_ bins,",
    "each work group generates a sub-histogram of dword_features_ features.",
    "only initialize once here, as this will not need to change when ResetTrainingData() is called",
    "create atomic counters for inter-group coordination",
    "The output buffer is allocated to host directly, to overlap compute and data transfer",
    "find the dense feature-groups and group then into Feature4 data structure (several feature-groups packed into 4 bytes)",
    "looking for dword_features_ non-sparse feature-groups",
    "decide if we need to redistribute the bin",
    "multiplier must be a power of 2",
    "device_bin_mults_.push_back(1);",
    "found",
    "for data transfer time",
    "Now generate new data structure feature4, and copy data to the device",
    "preallocate arrays for all threads, and pin them",
    "building Feature4 bundles; each thread handles dword_features_ features",
    "one feature datapoint is 4 bits",
    "this guarantees that the RawGet() function is inlined, rather than using virtual function dispatching",
    "one feature datapoint is one byte",
    "this guarantees that the RawGet() function is inlined, rather than using virtual function dispatching",
    "Dense bin",
    "Dense 4-bit bin",
    "working on the remaining (less than dword_features_) feature groups",
    "fill the leftover features",
    "fill this empty feature with some \"random\" value",
    "fill this empty feature with some \"random\" value",
    "copying the last 1 to (dword_features - 1) feature-groups in the last tuple",
    "deallocate pinned space for feature copying",
    "data transfer time",
    "for other types of failure, build log might not be available; program.build_log() can crash",
    "Something bad happened. Just return \"No log available.\"",
    "build is okay, log may contain warnings",
    "destroy any old kernels",
    "create OpenCL kernels for different number of workgroups per feature",
    "currently we don't use constant memory",
    "compile the GPU kernel depending if double precision is used, constant hessian is used, etc.",
    "kernel with indices in an array",
    "kernel with all features enabled, with eliminated branches",
    "kernel with all data indices (for root node, and assumes that root node always uses all features)",
    "do nothing if no features can be processed on GPU",
    "The only argument that needs to be changed later is num_data_",
    "hessian is passed as a parameter, but it is not available now.",
    "hessian will be set in BeforeTrain()",
    "Get the max bin size, used for selecting best GPU kernel",
    "initialize GPU",
    "determine which kernel to use based on the max number of bins",
    "the +9 skips extra characters \")\", newline, \"#endif\" and newline at the beginning",
    "the +9 skips extra characters \")\", newline, \"#endif\" and newline at the beginning",
    "the +9 skips extra characters \")\", newline, \"#endif\" and newline at the beginning",
    "setup GPU kernel arguments after we allocating all the buffers",
    "GPU memory has to been reallocated because data may have been changed",
    "setup GPU kernel arguments after we allocating all the buffers",
    "Copy initial full hessians and gradients to GPU.",
    "We start copying as early as possible, instead of at ConstructHistogram().",
    "setup hessian parameters only",
    "hessian is passed as a parameter",
    "use bagging",
    "On GPU, we start copying indices, gradients and Hessians now, instead at ConstructHistogram()",
    "copy used gradients and Hessians to ordered buffer",
    "transfer the indices to GPU",
    "transfer hessian to GPU",
    "setup hessian parameters only",
    "hessian is passed as a parameter",
    "transfer gradients to GPU",
    "only have root",
    "Copy indices, gradients and Hessians as early as possible",
    "only need to initialize for smaller leaf",
    "Get leaf boundary",
    "copy indices to the GPU:",
    "copy ordered Hessians to the GPU:",
    "copy ordered gradients to the GPU:",
    "do nothing if no features can be processed on GPU",
    "copy data indices if it is not null",
    "generate and copy ordered_gradients if gradients is not null",
    "generate and copy ordered_hessians if Hessians is not null",
    "converted indices in is_feature_used to feature-group indices",
    "construct the feature masks for dense feature-groups",
    "if no feature group is used, just return and do not use GPU",
    "if not all feature groups are used, we need to transfer the feature mask to GPU",
    "otherwise, we will use a specialized GPU kernel with all feature groups enabled",
    "All data have been prepared, now run the GPU kernel",
    "construct smaller leaf",
    "ConstructGPUHistogramsAsync will return true if there are available feature groups dispatched to GPU",
    "then construct sparse features on CPU",
    "wait for GPU to finish, only if GPU is actually used",
    "use double precision",
    "use single precision",
    "Compare GPU histogram with CPU histogram, useful for debugging GPU code problem",
    "#define GPU_DEBUG_COMPARE",
    "construct larger leaf",
    "then construct sparse features on CPU",
    "wait for GPU to finish, only if GPU is actually used",
    "use double precision",
    "use single precision",
    "do some sanity check for the GPU algorithm",
    "limit top k",
    "get max bin",
    "calculate buffer size",
    "need to be able to hold smaller and larger best splits in SyncUpGlobalBestSplit",
    "left and right on same time, so need double size",
    "initialize histograms for global",
    "sync global data sumup info",
    "set global sumup info",
    "init global data count in leaf",
    "get local sumup",
    "get local sumup",
    "get mean number on machines",
    "weighted gain",
    "get top k",
    "Copy histogram to buffer, and Get local aggregate features",
    "copy histograms.",
    "copy smaller leaf histograms first",
    "mark local aggregated feature",
    "copy",
    "then copy larger leaf histograms",
    "mark local aggregated feature",
    "copy",
    "use local data to find local best splits",
    "clear histogram buffer before synchronizing",
    "otherwise histogram contents from the previous iteration will be sent",
    "find splits",
    "only has root leaf",
    "local voting",
    "gather",
    "get all top-k from all machines",
    "global voting",
    "copy local histgrams to buffer",
    "Reduce scatter for histogram",
    "find best split from local aggregated histograms",
    "restore from buffer",
    "restore from buffer",
    "find local best",
    "find local best split for larger leaf",
    "sync global best info",
    "copy back",
    "set the global number of data for leaves",
    "init the global sumup info",
    "instantiate template classes, otherwise linker cannot find the code",
    "launch cuda kernel",
    "initialize SerialTreeLearner",
    "some additional variables needed for GPU trainer",
    "Initialize GPU buffers and kernels: get device info",
    "some functions used for debugging the GPU histogram construction",
    "we roughly want 256 workgroups per device, and we have num_dense_feature4_ feature tuples.",
    "also guarantee that there are at least 2K examples per workgroup",
    "we have already copied ordered gradients, ordered hessians and indices to GPU",
    "decide the best number of workgroups working on one feature4 tuple",
    "set work group size based on feature size",
    "each 2^exp_workgroups_per_feature workgroups work on a feature4 tuple",
    "set thread_data",
    "copy the results asynchronously. Size depends on if double precision is used",
    "when the output is ready, the computation is done",
    "how many feature-group tuples we have",
    "leave some safe margin for prefetching",
    "256 work-items per workgroup. Each work-item prefetches one tuple for that feature",
    "clear sparse/dense maps",
    "do nothing it there is no dense feature",
    "calculate number of feature groups per gpu",
    "histogram bin entry size depends on the precision (single/double)",
    "allocate GPU memory for each GPU",
    "do nothing it there is no gpu feature",
    "allocate memory for all features",
    "allocate space for gradients and hessians on device",
    "we will copy gradients and hessians in after ordered_gradients_ and ordered_hessians_ are constructed",
    "copy indices to the device",
    "create output buffer, each feature has a histogram with device_bin_size_ bins,",
    "each work group generates a sub-histogram of dword_features_ features.",
    "only initialize once here, as this will not need to change when ResetTrainingData() is called",
    "create atomic counters for inter-group coordination",
    "The output buffer is allocated to host directly, to overlap compute and data transfer",
    "clear sparse/dense maps",
    "find the dense feature-groups and group then into Feature4 data structure (several feature-groups packed into 4 bytes)",
    "set device info",
    "looking for dword_features_ non-sparse feature-groups",
    "reset device info",
    "InitGPU w/ num_gpu",
    "Get the max bin size, used for selecting best GPU kernel",
    "get num_dense_feature_groups_",
    "initialize GPU",
    "set cpu threads",
    "resize device memory pointers",
    "create stream & events to handle multiple GPUs",
    "check data size",
    "GPU memory has to been reallocated because data may have been changed",
    "AllocateGPUMemory only when the number of data increased",
    "setup GPU kernel arguments after we allocating all the buffers",
    "Copy initial full hessians and gradients to GPU.",
    "We start copying as early as possible, instead of at ConstructHistogram().",
    "use bagging",
    "On GPU, we start copying indices, gradients and hessians now, instead at ConstructHistogram()",
    "copy used gradients and hessians to ordered buffer",
    "transfer the indices to GPU",
    "only have root",
    "Copy indices, gradients and hessians as early as possible",
    "only need to initialize for smaller leaf",
    "Get leaf boundary",
    "do nothing if no features can be processed on GPU",
    "copy data indices if it is not null",
    "converted indices in is_feature_used to feature-group indices",
    "construct the feature masks for dense feature-groups",
    "if no feature group is used, just return and do not use GPU",
    "if not all feature groups are used, we need to transfer the feature mask to GPU",
    "otherwise, we will use a specialized GPU kernel with all feature groups enabled",
    "We now copy even if all features are used.",
    "All data have been prepared, now run the GPU kernel",
    "construct smaller leaf",
    "Check workgroups per feature4 tuple..",
    "if the workgroup per feature is 1 (2^0), return as the work is too small for a GPU",
    "ConstructGPUHistogramsAsync will return true if there are availabe feature groups dispatched to GPU",
    "then construct sparse features on CPU",
    "We set data_indices to null to avoid rebuilding ordered gradients/hessians",
    "wait for GPU to finish, only if GPU is actually used",
    "use double precision",
    "use single precision",
    "Compare GPU histogram with CPU histogram, useful for debuggin GPU code problem",
    "#define CUDA_DEBUG_COMPARE",
    "construct larger leaf",
    "then construct sparse features on CPU",
    "We set data_indices to null to avoid rebuilding ordered gradients/hessians",
    "wait for GPU to finish, only if GPU is actually used",
    "use double precision",
    "use single precision",
    "do some sanity check for the GPU algorithm"
  ],
  "v3.3.2": [
    "coding: utf-8",
    "coding: utf-8",
    "create predictor first",
    "show deprecation warning only for early stop argument, setting early stop via global params should still be possible",
    "check dataset",
    "reduce cost for prediction training data",
    "process callbacks",
    "Most of legacy advanced options becomes callbacks",
    "construct booster",
    "start training",
    "check evaluation result.",
    "ranking task, split according to groups",
    "run preprocessing on the data set if needed",
    "setup callbacks",
    "coding: utf-8",
    "dummy function to support older version of scikit-learn",
    "coding: utf-8",
    "documentation templates for LGBMModel methods are shared between the classes in",
    "this module and those in the ``dask`` module",
    "user can set verbose with kwargs, it has higher priority",
    "Do not modify original args in fit function",
    "Refer to https://github.com/microsoft/LightGBM/pull/2619",
    "Separate built-in from callable evaluation metrics",
    "register default metric for consistency with callable eval_metric case",
    "try to deduce from class instance",
    "overwrite default metric by explicitly set metric",
    "concatenate metric from params (or default if not provided in params) and eval_metric",
    "copy for consistency",
    "reduce cost for prediction training data",
    "free dataset",
    "Switch to using a multiclass objective in the underlying LGBM instance",
    "do not modify args, as it causes errors in model selection tools",
    "check group data",
    "coding: utf-8",
    "we don't need lib_lightgbm while building docs",
    "coding: utf-8",
    "\"#ddffdd\" is light green, \"#ffdddd\" is light red",
    "coding: utf-8",
    "coding: utf-8",
    "TypeError: obj is not a string or a number",
    "ValueError: invalid literal",
    "DeprecationWarning is not shown by default, so let's create our own with higher level",
    "avoid side effects on passed-in parameters",
    "find a value, and remove other aliases with .pop()",
    "prefer the value of 'main_param_name' if it exists, otherwise search the aliases",
    "Get total row number.",
    "Random access by row index. Used for data sampling.",
    "Range data access. Used to read data in batch when constructing Dataset.",
    "Optionally specify batch_size to control range data read size.",
    "Only required if using ``Dataset.subset()``.",
    "__get_num_preds() cannot work with nrow > MAX_INT32, so calculate overall number of predictions piecemeal",
    "avoid memory consumption by arrays concatenation operations",
    "create numpy array from output arrays",
    "break up indptr based on number of rows (note more than one matrix in multiclass case)",
    "for CSC there is extra column added",
    "reformat output into a csr or csc matrix or list of csr or csc matrices",
    "same shape as input csr or csc matrix except extra column for expected value",
    "note: make sure we copy data as it will be deallocated next",
    "free the temporary native indptr, indices, and data",
    "__get_num_preds() cannot work with nrow > MAX_INT32, so calculate overall number of predictions piecemeal",
    "avoid memory consumption by arrays concatenation operations",
    "c type: double**",
    "each double* element points to start of each column of sample data.",
    "c type int**",
    "each int* points to start of indices for each column",
    "no min_data, nthreads and verbose in this function",
    "check data has header or not",
    "need to regroup init_score",
    "process for args",
    "user can set verbose with params, it has higher priority",
    "get categorical features",
    "process for reference dataset",
    "start construct data",
    "set feature names",
    "Select sampled rows, transpose to column order.",
    "create validation dataset from ref_dataset",
    "create valid",
    "construct subset",
    "create train",
    "could be updated if data is not freed",
    "set to None",
    "we're done if self and reference share a common upstream reference",
    "if buffer length is not long enough, reallocate buffers",
    "group data from LightGBM is boundaries data, need to convert to group size",
    "user can set verbose with params, it has higher priority",
    "Training task",
    "if \"machines\" is given, assume user wants to do distributed learning, and set up network",
    "construct booster object",
    "copy the parameters from train_set",
    "save reference to data",
    "buffer for inner predict",
    "Prediction task",
    "if a single node tree it won't have `leaf_index` so return 0",
    "Create the node record, and populate universal data members",
    "Update values to reflect node type (leaf or split)",
    "traverse the next level of the tree",
    "In tree format, \"subtree_list\" is a list of node records (dicts),",
    "and we add node to the list.",
    "need reset training data",
    "need to push new valid data",
    "if buffer length is not long enough, re-allocate a buffer",
    "if buffer length is not long enough, reallocate a buffer",
    "Copy models",
    "Get name of features",
    "if buffer length is not long enough, reallocate buffers",
    "avoid to predict many time in one iteration",
    "Get num of inner evals",
    "Get name of eval metrics",
    "if buffer length is not long enough, reallocate buffers",
    "coding: utf-8",
    "Callback environment used by callbacks",
    "split is needed for \"<dataset type> <metric>\" case (e.g. \"train l1\")",
    "split is needed for \"<dataset type> <metric>\" case (e.g. \"train l1\")",
    "coding: utf-8",
    "Concatenate many parts into one",
    "construct local eval_set data.",
    "store indices of eval_set components that were not contained within local parts.",
    "consolidate parts of each individual eval component.",
    "require that eval_name exists in evaluated result data in case dropped due to padding.",
    "in distributed training the 'training' eval_set is not detected, will have name 'valid_<index>'.",
    "filter padding from eval parts then _concat each eval_set component.",
    "reconstruct eval_set fit args/kwargs depending on which components of eval_set are on worker.",
    "ensure that expected keys for evals_result_ and best_score_ exist regardless of padding.",
    "capture whether local_listen_port or its aliases were provided",
    "capture whether machines or its aliases were provided",
    "Some passed-in parameters can be removed:",
    "* 'num_machines': set automatically from Dask worker list",
    "* 'num_threads': overridden to match nthreads on each Dask process",
    "Split arrays/dataframes into parts. Arrange parts into dicts to enforce co-locality",
    "evals_set will to be re-constructed into smaller lists of (X, y) tuples, where",
    "X and y are each delayed sub-lists of original eval dask Collections.",
    "find maximum number of parts in an individual eval set so that we can",
    "pad eval sets when they come in different sizes.",
    "when individual eval set is equivalent to training data, skip recomputing parts.",
    "add None-padding for individual eval_set member if it is smaller than the largest member.",
    "first time a chunk of this eval set is added to this part.",
    "append additional chunks of this eval set to this part.",
    "ensure that all evaluation parts map uniquely to one part.",
    "assign sub-eval_set components to worker parts.",
    "Start computation in the background",
    "Find locations of all parts and map them to particular Dask workers",
    "Check that all workers were provided some of eval_set. Otherwise warn user that validation",
    "data artifacts may not be populated depending on worker returning final estimator.",
    "assign general validation set settings to fit kwargs.",
    "resolve aliases for network parameters and pop the result off params.",
    "these values are added back in calls to `_train_part()`",
    "figure out network params",
    "Tell each worker to train on the parts that it has locally",
    "",
    "This code treats ``_train_part()`` calls as not \"pure\" because:",
    "1. there is randomness in the training process unless parameters ``seed``",
    "and ``deterministic`` are set",
    "2. even with those parameters set, the output of one ``_train_part()`` call",
    "relies on global state (it and all the other LightGBM training processes",
    "coordinate with each other)",
    "if network parameters were changed during training, remove them from the",
    "returned model so that they're generated dynamically on every run based",
    "on the Dask cluster you're connected to and which workers have pieces of",
    "the training data",
    "dask.DataFrame.map_partitions() expects each call to return a pandas DataFrame or Series",
    "for multi-class classification with sparse matrices, pred_contrib predictions",
    "are returned as a list of sparse matrices (one per class)",
    "pred_contrib output will have one column per feature,",
    "plus one more for the base value",
    "need to tell Dask the expected type and shape of individual preds",
    "by default, dask.array.concatenate() concatenates sparse arrays into a COO matrix",
    "the code below is used instead to ensure that the sparse type is preserved during concatentation",
    "At this point, `out` is a list of lists of delayeds (each of which points to a matrix).",
    "Concatenate them to return a list of Dask Arrays.",
    "the note on custom objective functions in LGBMModel.__init__ is not",
    "currently relevant for the Dask estimators",
    "DaskLGBMClassifier does not support group, eval_group, early_stopping_rounds.",
    "DaskLGBMClassifier support for callbacks and init_model is not tested",
    "the note on custom objective functions in LGBMModel.__init__ is not",
    "currently relevant for the Dask estimators",
    "DaskLGBMRegressor does not support group, eval_class_weight, eval_group, early_stopping_rounds.",
    "DaskLGBMRegressor support for callbacks and init_model is not tested",
    "the note on custom objective functions in LGBMModel.__init__ is not",
    "currently relevant for the Dask estimators",
    "DaskLGBMRanker does not support eval_class_weight or early stopping",
    "DaskLGBMRanker support for callbacks and init_model is not tested",
    "coding: utf-8",
    "load or create your dataset",
    "create dataset for lightgbm",
    "if you want to re-use data, remember to set free_raw_data=False",
    "specify your configurations as a dict",
    "generate feature names",
    "feature_name and categorical_feature",
    "check feature name",
    "save model to file",
    "dump model to JSON (and save to file)",
    "feature names",
    "feature importances",
    "load model to predict",
    "can only predict with the best iteration (or the saving iteration)",
    "eval with loaded model",
    "dump model with pickle",
    "load model with pickle to predict",
    "can predict with any iteration when loaded in pickle way",
    "eval with loaded model",
    "continue training",
    "init_model accepts:",
    "1. model file name",
    "2. Booster()",
    "decay learning rates",
    "learning_rates accepts:",
    "1. list/tuple with length = num_boost_round",
    "2. function(curr_iter)",
    "change other parameters during training",
    "self-defined objective function",
    "f(preds: array, train_data: Dataset) -> grad: array, hess: array",
    "log likelihood loss",
    "self-defined eval metric",
    "f(preds: array, train_data: Dataset) -> name: str, eval_result: float, is_higher_better: bool",
    "binary error",
    "NOTE: when you do customized loss function, the default prediction value is margin",
    "This may make built-in evaluation metric calculate wrong results",
    "For example, we are doing log likelihood loss, the prediction is score before logistic transformation",
    "Keep this in mind when you use the customization",
    "another self-defined eval metric",
    "f(preds: array, train_data: Dataset) -> name: str, eval_result: float, is_higher_better: bool",
    "accuracy",
    "NOTE: when you do customized loss function, the default prediction value is margin",
    "This may make built-in evaluation metric calculate wrong results",
    "For example, we are doing log likelihood loss, the prediction is score before logistic transformation",
    "Keep this in mind when you use the customization",
    "callback",
    "coding: utf-8",
    "load or create your dataset",
    "train",
    "predict",
    "eval",
    "feature importances",
    "self-defined eval metric",
    "f(y_true: array, y_pred: array) -> name: str, eval_result: float, is_higher_better: bool",
    "Root Mean Squared Logarithmic Error (RMSLE)",
    "train",
    "another self-defined eval metric",
    "f(y_true: array, y_pred: array) -> name: str, eval_result: float, is_higher_better: bool",
    "Relative Absolute Error (RAE)",
    "train",
    "predict",
    "eval",
    "other scikit-learn modules",
    "coding: utf-8",
    "load or create your dataset",
    "create dataset for lightgbm",
    "specify your configurations as a dict",
    "train",
    "coding: utf-8",
    "################",
    "Simulate some binary data with a single categorical and",
    "single continuous predictor",
    "################",
    "Set up a couple of utilities for our experiments",
    "################",
    "Observe the behavior of `binary` and `xentropy` objectives",
    "Trying this throws an error on non-binary values of y:",
    "experiment('binary', label_type='probability', DATA)",
    "The speed of `binary` is not drastically different than",
    "`xentropy`. `xentropy` runs faster than `binary` in many cases, although",
    "there are reasons to suspect that `binary` should run faster when the",
    "label is an integer instead of a float",
    "coding: utf-8",
    "load or create your dataset",
    "create dataset for lightgbm",
    "specify your configurations as a dict",
    "train",
    "save model to file",
    "predict",
    "eval",
    "We can also open HDF5 file once and get access to",
    "With binary dataset created, we can use either Python API or cmdline version to train.",
    "",
    "Note: in order to create exactly the same dataset with the one created in simple_example.py, we need",
    "to modify simple_example.py to pass numpy array instead of pandas DataFrame to Dataset constructor.",
    "The reason is that DataFrame column names will be used in Dataset. For a DataFrame with Int64Index",
    "as columns, Dataset will use column names like [\"0\", \"1\", \"2\", ...]. While for numpy array, column names",
    "are using the default one assigned in C++ code (dataset_loader.cpp), like [\"Column_0\", \"Column_1\", ...].",
    "Y has a single column and we read it in single shot. So store it as an 1-d array.",
    "We use random access for data sampling when creating LightGBM Dataset from Sequence.",
    "When accessing any element in a HDF5 chunk, it's read entirely.",
    "To save I/O for sampling, we should keep number of total chunks much larger than sample count.",
    "Here we are just creating a chunk size that matches with batch_size.",
    "",
    "Also note that the data is stored in row major order to avoid extra copy when passing to",
    "lightgbm Dataset.",
    "Save to 2 HDF5 files for demonstration.",
    "We can store multiple datasets inside a single HDF5 file.",
    "Separating X and Y for choosing best chunk size for data loading.",
    "split training data into two partitions",
    "make this array dense because we're splitting across",
    "a sparse boundary to partition the data",
    "the code below uses sklearn.metrics, but this requires pulling all of the",
    "predictions and target values back from workers to the client",
    "",
    "for larger datasets, consider the metrics from dask-ml instead",
    "https://ml.dask.org/modules/api.html#dask-ml-metrics-metrics",
    "coding: utf-8",
    "!/usr/bin/env python3",
    "-*- coding: utf-8 -*-",
    "",
    "LightGBM documentation build configuration file, created by",
    "sphinx-quickstart on Thu May  4 14:30:58 2017.",
    "",
    "This file is execfile()d with the current directory set to its",
    "containing dir.",
    "",
    "Note that not all possible configuration values are present in this",
    "autogenerated file.",
    "",
    "All configuration values have a default; values that are commented out",
    "serve to show the default.",
    "If extensions (or modules to document with autodoc) are in another directory,",
    "add these directories to sys.path here. If the directory is relative to the",
    "documentation root, use os.path.abspath to make it absolute.",
    "-- mock out modules",
    "-- General configuration ------------------------------------------------",
    "If your documentation needs a minimal Sphinx version, state it here.",
    "Add any Sphinx extension module names here, as strings. They can be",
    "extensions coming with Sphinx (named 'sphinx.ext.*') or your custom",
    "ones.",
    "hide type hints in API docs",
    "Generate autosummary pages. Output should be set with: `:toctree: pythonapi/`",
    "Only the class' docstring is inserted.",
    "If true, `todo` and `todoList` produce output, else they produce nothing.",
    "The master toctree document.",
    "General information about the project.",
    "The name of an image file (relative to this directory) to place at the top",
    "of the sidebar.",
    "The name of an image file (relative to this directory) to use as a favicon of",
    "the docs.  This file should be a Windows icon file (.ico) being 16x16 or 32x32",
    "pixels large.",
    "The version info for the project you're documenting, acts as replacement for",
    "|version| and |release|, also used in various other places throughout the",
    "built documents.",
    "The short X.Y version.",
    "The full version, including alpha/beta/rc tags.",
    "The language for content autogenerated by Sphinx. Refer to documentation",
    "for a list of supported languages.",
    "",
    "This is also used if you do content translation via gettext catalogs.",
    "Usually you set \"language\" from the command line for these cases.",
    "List of patterns, relative to source directory, that match files and",
    "directories to ignore when looking for source files.",
    "This patterns also effect to html_static_path and html_extra_path",
    "The name of the Pygments (syntax highlighting) style to use.",
    "-- Configuration for C API docs generation ------------------------------",
    "-- Options for HTML output ----------------------------------------------",
    "The theme to use for HTML and HTML Help pages.  See the documentation for",
    "a list of builtin themes.",
    "Theme options are theme-specific and customize the look and feel of a theme",
    "further.  For a list of options available for each theme, see the",
    "documentation.",
    "Add any paths that contain custom static files (such as style sheets) here,",
    "relative to this directory. They are copied after the builtin static files,",
    "so a file named \"default.css\" will overwrite the builtin \"default.css\".",
    "-- Options for HTMLHelp output ------------------------------------------",
    "Output file base name for HTML help builder.",
    "-- Options for LaTeX output ---------------------------------------------",
    "The name of an image file (relative to this directory) to place at the top of",
    "the title page.",
    "Warning! The following code can cause buffer overflows on RTD.",
    "Consider suppressing output completely if RTD project silently fails.",
    "Refer to https://github.com/svenevs/exhale",
    "/blob/fe7644829057af622e467bb529db6c03a830da99/exhale/deploy.py#L99-L111",
    "Warning! The following code can cause buffer overflows on RTD.",
    "Consider suppressing output completely if RTD project silently fails.",
    "Refer to https://github.com/svenevs/exhale",
    "/blob/fe7644829057af622e467bb529db6c03a830da99/exhale/deploy.py#L99-L111",
    "coding: utf-8",
    "This is a basic test for floating number parsing.",
    "Most of the test cases come from:",
    "https://github.com/dmlc/xgboost/blob/master/tests/cpp/common/test_charconv.cc",
    "https://github.com/Alexhuszagh/rust-lexical/blob/master/data/test-parse-unittests/strtod_tests.toml",
    "FLT_MAX",
    "FLT_MIN",
    "DBL_MAX (1 + (1 - 2^-52)) * 2^1023 = (2^53 - 1) * 2^971",
    "2^971 * (2^53 - 1 + 1/2) : the smallest number resolving to inf",
    "Near DBL_MIN",
    "DBL_MIN 2^-1022",
    "The behavior for parsing -nan depends on implementation.",
    "Thus we skip binary check for negative nan.",
    "See comment in test_cases.",
    "Constants",
    "Start with some content:",
    "Clear & re-use:",
    "Output should match new content:",
    "Number of trials for each new ChunkedArray configuration. Pass 100 times over the search space:",
    "Each outer loop iteration changes the test by adding +1 chunk. We start with 1 chunk only:",
    "Sweep valid and invalid addresses with a ChunkedArray with `chunks` chunks:",
    "Compute a new trial address & value & if it is a valid address:",
    "Insert item. If at a valid address, 0 is returned, otherwise, -1 is returned:",
    "If at valid address, check that the stored value is correct & remember it for the future:",
    "Check the just-stored value with getitem():",
    "Also store the just-stored value for future tracking:",
    "Final check: ensure even with overrides, all valid insertions store the latest value at that address:",
    "Test in 2 ways that the values are correctly laid out in memory:",
    "coding: utf-8",
    "we don't need lib_lightgbm while building docs",
    "coding: utf-8",
    "check saved model persistence",
    "we need to check the consistency of model file here, so test for exact equal",
    "check early stopping is working. Make it stop very early, so the scores should be very close to zero",
    "scores likely to be different, but prediction should still be the same",
    "test that shape is checked during prediction",
    "The simple implementation is just a single \"return self.ndarray[idx]\"",
    "The following is for demo and testing purpose.",
    "whole col",
    "half col",
    "Create dataset from numpy array directly.",
    "Create dataset using Sequence.",
    "Test for validation set.",
    "Select some random rows as valid data.",
    "From Dataset constructor, with dataset from numpy array.",
    "From Dataset.create_valid, with dataset from sequence.",
    "test that method works even with free_raw_data=True",
    "test that method works but sets raw data to None in case of immergeable data types",
    "test that method works for different data types",
    "Set extremely harsh penalties, so CEGB will block most splits.",
    "Compare pairs of penalties, to ensure scaling works as intended",
    "Reset booster1's parameters to p2, so the parameter section of the file matches.",
    "should resolve duplicate aliases, and prefer the main parameter",
    "should choose a value from an alias and set that value on main param",
    "if only an alias is used",
    "should use the default if main param and aliases are missing",
    "all changes should be made on copies and not modify the original",
    "coding: utf-8",
    "add target, weight, and group to DataFrame so that partitions abide by group boundaries.",
    "set_index ensures partitions are based on group id.",
    "See https://stackoverflow.com/questions/49532824/dask-dataframe-split-partitions-based-on-a-column-or-function.",
    "separate target, weight from features.",
    "encode group identifiers into run-length encoding, the format LightGBMRanker is expecting",
    "so that within each partition, sum(g) = n_samples.",
    "ranking arrays: one chunk per group. Each chunk must include all columns.",
    "make one categorical feature relevant to the target",
    "https://github.com/microsoft/LightGBM/issues/4118",
    "extra predict() parameters should be passed through correctly",
    "pref_leaf values should have the right shape",
    "and values that look like valid tree nodes",
    "be sure LightGBM actually used at least one categorical column,",
    "and that it was correctly treated as a categorical feature",
    "shape depends on whether it is binary or multiclass classification",
    "in the special case of multi-class classification using scipy sparse matrices,",
    "the output of `.predict(..., pred_contrib=True)` is a list of sparse matrices (one per class)",
    "",
    "since that case is so different than all other cases, check the relevant things here",
    "and then return early",
    "raw scores will probably be different, but at least check that all predicted classes are the same",
    "be sure LightGBM actually used at least one categorical column,",
    "and that it was correctly treated as a categorical feature",
    "* shape depends on whether it is binary or multiclass classification",
    "* matrix for binary classification is of the form [feature_contrib, base_value],",
    "for multi-class it's [feat_contrib_class1, base_value_class1, feat_contrib_class2, base_value_class2, etc.]",
    "* contrib outputs for distributed training are different than from local training, so we can just test",
    "that the output has the right shape and base values are in the right position",
    "check that found ports are different for same address (LocalCluster)",
    "check that the ports are indeed open",
    "Scores should be the same",
    "Predictions should be roughly the same.",
    "pref_leaf values should have the right shape",
    "and values that look like valid tree nodes",
    "extra predict() parameters should be passed through correctly",
    "be sure LightGBM actually used at least one categorical column,",
    "and that it was correctly treated as a categorical feature",
    "contrib outputs for distributed training are different than from local training, so we can just test",
    "that the output has the right shape and base values are in the right position",
    "be sure LightGBM actually used at least one categorical column,",
    "and that it was correctly treated as a categorical feature",
    "Quantiles should be right",
    "be sure LightGBM actually used at least one categorical column,",
    "and that it was correctly treated as a categorical feature",
    "rebalance small dask.Array dataset for better performance.",
    "use many trees + leaves to overfit, help ensure that Dask data-parallel strategy matches that of",
    "serial learner. See https://github.com/microsoft/LightGBM/issues/3292#issuecomment-671288210.",
    "distributed ranker should be able to rank decently well and should",
    "have high rank correlation with scores from serial ranker.",
    "extra predict() parameters should be passed through correctly",
    "pref_leaf values should have the right shape",
    "and values that look like valid tree nodes",
    "be sure LightGBM actually used at least one categorical column,",
    "and that it was correctly treated as a categorical feature",
    "Use larger trainset to prevent premature stopping due to zero loss, causing num_trees() < n_estimators.",
    "Use small chunk_size to avoid single-worker allocation of eval data partitions.",
    "test eval_class_weight, eval_init_score on binary-classification task.",
    "Note: objective's default `metric` will be evaluated in evals_result_ in addition to all eval_metrics.",
    "create eval_sets by creating new datasets or copying training data.",
    "total number of trees scales up for ova classifier.",
    "check that early stopping was not applied.",
    "checks that evals_result_ and best_score_ contain expected data and eval_set names.",
    "check that each eval_name and metric exists for all eval sets, allowing for the",
    "case when a worker receives a fully-padded eval_set component which is not evaluated.",
    "should be able to use the class without specifying a client",
    "should be able to set client after construction",
    "data on cluster1",
    "create identical data on cluster2",
    "at this point, the result of default_client() is client2 since it was the most recently",
    "created. So setting client to client1 here to test that you can select a non-default client",
    "unfitted model should survive pickling round trip, and pickling",
    "shouldn't have side effects on the model object",
    "client will always be None after unpickling",
    "fitted model should survive pickling round trip, and pickling",
    "shouldn't have side effects on the model object",
    "client will always be None after unpickling",
    "rebalance data to be sure that each worker has a piece of the data",
    "model 1 - no network parameters given",
    "model 2 - machines given",
    "model 3 - local_listen_port given",
    "training should fail because LightGBM will try to use the same",
    "port for multiple worker processes on the same machine",
    "rebalance data to be sure that each worker has a piece of the data",
    "test that \"machines\" is actually respected by creating a socket that uses",
    "one of the ports mentioned in \"machines\"",
    "The above error leaves a worker waiting",
    "an informative error should be raised if \"machines\" has duplicates",
    "\"client\" should be the only different, and the final argument",
    "value of the root node is 0 when init_score is set",
    "this test is separate because it takes a not-yet-constructed estimator",
    "coding: utf-8",
    "coding: utf-8",
    "build target, group ID vectors.",
    "build y/target and group-id vectors with user-specified group sizes.",
    "build y/target and group-id vectors according to n_samples, avg_gs, and random_gs.",
    "groups should contain > 1 element for pairwise learning objective.",
    "build feature data, X. Transform first few into informative features.",
    "coding: utf-8",
    "prediction result is actually not transformed (is raw) due to custom objective",
    "sklearn <0.23 does not have a stacking classifier and n_features_in_ property",
    "sklearn <0.23 does not have a stacking regressor and n_features_in_ property",
    "sklearn < 0.22 does not have the post fit attribute: classes_",
    "sklearn < 0.23 does not have as_frame parameter",
    "sklearn < 0.22 does not have the post fit attribute: classes_",
    "sklearn < 0.23 does not have as_frame parameter",
    "Test if random_state is properly stored",
    "Test if two random states produce identical models",
    "Test if subsequent fits sample from random_state object and produce different models",
    "Test that the largest element is NOT the same, the smallest can be the same, i.e. zero",
    "With default params",
    "Tests same probabilities",
    "Tests same predictions",
    "Tests same raw scores",
    "Tests same leaf indices",
    "Tests same feature contributions",
    "Tests other parameters for the prediction works",
    "Tests start_iteration",
    "Tests same probabilities, starting from iteration 10",
    "Tests same predictions, starting from iteration 10",
    "Tests same raw scores, starting from iteration 10",
    "Tests same leaf indices, starting from iteration 10",
    "Tests same feature contributions, starting from iteration 10",
    "Tests other parameters for the prediction works, starting from iteration 10",
    "no custom objective, no custom metric",
    "default metric",
    "non-default metric",
    "no metric",
    "non-default metric in eval_metric",
    "non-default metric with non-default metric in eval_metric",
    "non-default metric with multiple metrics in eval_metric",
    "non-default metric with multiple metrics in eval_metric for LGBMClassifier",
    "default metric for non-default objective",
    "non-default metric for non-default objective",
    "no metric",
    "non-default metric in eval_metric for non-default objective",
    "non-default metric with non-default metric in eval_metric for non-default objective",
    "non-default metric with multiple metrics in eval_metric for non-default objective",
    "custom objective, no custom metric",
    "default regression metric for custom objective",
    "non-default regression metric for custom objective",
    "multiple regression metrics for custom objective",
    "no metric",
    "default regression metric with non-default metric in eval_metric for custom objective",
    "non-default regression metric with metric in eval_metric for custom objective",
    "multiple regression metrics with metric in eval_metric for custom objective",
    "multiple regression metrics with multiple metrics in eval_metric for custom objective",
    "no custom objective, custom metric",
    "default metric with custom metric",
    "non-default metric with custom metric",
    "multiple metrics with custom metric",
    "custom metric (disable default metric)",
    "default metric for non-default objective with custom metric",
    "non-default metric for non-default objective with custom metric",
    "multiple metrics for non-default objective with custom metric",
    "custom metric (disable default metric for non-default objective)",
    "custom objective, custom metric",
    "custom metric for custom objective",
    "non-default regression metric with custom metric for custom objective",
    "multiple regression metrics with custom metric for custom objective",
    "default metric and invalid binary metric is replaced with multiclass alternative",
    "invalid objective is replaced with default multiclass one",
    "and invalid binary metric is replaced with multiclass alternative",
    "default metric for non-default multiclass objective",
    "and invalid binary metric is replaced with multiclass alternative",
    "default metric and invalid multiclass metric is replaced with binary alternative",
    "invalid multiclass metric is replaced with binary alternative for custom objective",
    "Verify that can receive a list of metrics, only callable",
    "Verify that can receive a list of custom and built-in metrics",
    "Verify that works as expected when eval_metric is empty",
    "Verify that can receive a list of metrics, only built-in",
    "Verify that eval_metric is robust to receiving a list with None",
    "training data as eval_set",
    "feval",
    "single eval_set",
    "two eval_set",
    "sklearn < 0.22 requires passing \"attributes\" argument",
    "Test that estimators are default-constructible",
    "coding: utf-8",
    "coding: utf-8",
    "check that default gives same result as k = 1",
    "check against independent calculation for k = 1",
    "check against independent calculation for k = 2",
    "check against independent calculation for k = 10",
    "check cases where predictions are equal",
    "should give same result as binary auc for 2 classes",
    "test the case where all predictions are equal",
    "test that weighted data gives different auc_mu",
    "test that equal data weights give same auc_mu as unweighted data",
    "should give 1 when accuracy = 1",
    "test loading class weights",
    "no early stopping",
    "early stopping occurs",
    "test custom eval metrics",
    "shuffle = False, override metric in params",
    "shuffle = True, callbacks",
    "enable display training loss",
    "self defined folds",
    "LambdaRank",
    "... with l2 metric",
    "... with NDCG (default) metric",
    "self defined folds with lambdarank",
    "with early stopping",
    "predict by each fold booster",
    "fold averaging",
    "without early stopping",
    "test feature_names with whitespaces",
    "This has non-ascii strings.",
    "take subsets and train",
    "generate CSR sparse dataset",
    "convert data to dense and get back same contribs",
    "validate the values are the same",
    "validate using CSC matrix",
    "validate the values are the same",
    "generate CSR sparse dataset",
    "convert data to dense and get back same contribs",
    "validate the values are the same",
    "validate using CSC matrix",
    "validate the values are the same",
    "Note there is an extra column added to the output for the expected value",
    "Note output CSC shape should be same as CSR output shape",
    "test sliced labels",
    "append some columns",
    "append some rows",
    "test sliced 2d matrix",
    "test sliced CSR",
    "trees start at position 1.",
    "split_features are in 4th line.",
    "test if a penalty as high as the depth indeed prohibits all monotone splits",
    "The penalization is so high that the first 2 features should not be used here",
    "Check that a very high penalization is the same as not using the features at all",
    "no fobj, no feval",
    "default metric",
    "non-default metric in params",
    "default metric in args",
    "non-default metric in args",
    "metric in args overwrites one in params",
    "multiple metrics in params",
    "multiple metrics in args",
    "remove default metric by 'None' in list",
    "remove default metric by 'None' aliases",
    "fobj, no feval",
    "no default metric",
    "metric in params",
    "metric in args",
    "metric in args overwrites its' alias in params",
    "multiple metrics in params",
    "multiple metrics in args",
    "no fobj, feval",
    "default metric with custom one",
    "non-default metric in params with custom one",
    "default metric in args with custom one",
    "non-default metric in args with custom one",
    "metric in args overwrites one in params, custom one is evaluated too",
    "multiple metrics in params with custom one",
    "multiple metrics in args with custom one",
    "custom metric is evaluated despite 'None' is passed",
    "fobj, feval",
    "no default metric, only custom one",
    "metric in params with custom one",
    "metric in args with custom one",
    "metric in args overwrites one in params, custom one is evaluated too",
    "multiple metrics in params with custom one",
    "multiple metrics in args with custom one",
    "custom metric is evaluated despite 'None' is passed",
    "no fobj, no feval",
    "default metric",
    "default metric in params",
    "non-default metric in params",
    "multiple metrics in params",
    "remove default metric by 'None' aliases",
    "fobj, no feval",
    "no default metric",
    "metric in params",
    "multiple metrics in params",
    "no fobj, feval",
    "default metric with custom one",
    "default metric in params with custom one",
    "non-default metric in params with custom one",
    "multiple metrics in params with custom one",
    "custom metric is evaluated despite 'None' is passed",
    "fobj, feval",
    "no default metric, only custom one",
    "metric in params with custom one",
    "multiple metrics in params with custom one",
    "custom metric is evaluated despite 'None' is passed",
    "multiclass default metric",
    "multiclass default metric with custom one",
    "multiclass metric alias with custom one for custom objective",
    "no metric for invalid class_num",
    "custom metric for invalid class_num",
    "multiclass metric alias with custom one with invalid class_num",
    "multiclass default metric without num_class",
    "multiclass metric alias",
    "multiclass metric",
    "non-valid metric for multiclass objective",
    "non-default num_class for default objective",
    "no metric with non-default num_class for custom objective",
    "multiclass metric alias for custom objective",
    "multiclass metric for custom objective",
    "binary metric with non-default num_class for custom objective",
    "Expect three metrics but mean and stdv for each metric",
    "test XGBoost-style return value",
    "test numpy-style return value",
    "test bins string type",
    "test histogram is disabled for categorical features",
    "test for lgb.train",
    "test feval for lgb.train",
    "test with two valid data for lgb.train",
    "test for lgb.cv",
    "test feval for lgb.cv",
    "test that binning works properly for features with only positive or only negative values",
    "decreasing without freeing raw data is allowed",
    "decreasing before lazy init is allowed",
    "increasing is allowed",
    "decreasing with disabled filter is allowed",
    "decreasing with enabled filter is disallowed;",
    "also changes of other params are disallowed",
    "check extra trees increases regularization",
    "check path smoothing increases regularization",
    "test edge case with one leaf",
    "check that constraint containing all features is equivalent to no constraint",
    "check that constraint partitioning the features reduces train accuracy",
    "check that constraints consisting of single features reduce accuracy further",
    "test that interaction constraints work when not all features are used",
    "check that setting linear_tree=True fits better than ordinary trees when data has linear relationship",
    "test again with nans in data",
    "test again with bagging",
    "test with a feature that has only one non-nan value",
    "test with a categorical feature",
    "test refit: same results on same data",
    "test refit with save and load",
    "test refit: different results training on different data",
    "test when num_leaves - 1 < num_features and when num_leaves - 1 > num_features",
    "test that the predict once with all iterations equals summed results with start_iteration and num_iteration",
    "test the case where start_iteration <= 0, and num_iteration is None",
    "test the case where start_iteration > 0, and num_iteration <= 0",
    "test the case where start_iteration > 0, and num_iteration <= 0, with pred_leaf=True",
    "test the case where start_iteration > 0, and num_iteration <= 0, with pred_contrib=True",
    "test for regression",
    "test both with and without early stopping",
    "test for multi-class",
    "test both with and without early stopping",
    "test for binary",
    "test both with and without early stopping",
    "test against sklearn average precision metric",
    "test that average precision is 1 where model predicts perfectly",
    "coding: utf-8",
    "If compiled appropriately, the same installation will support both GPU and CPU.",
    "coding: utf-8",
    "coding: utf-8",
    "These are helper functions to allow doing a stack unwind",
    "after an R allocation error, which would trigger a long jump.",
    "convert from one-based to zero-based index",
    "if any feature names were larger than allocated size,",
    "allow for a larger size and try again",
    "convert from boundaries to size",
    "--- start Booster interfaces",
    "if any eval names were larger than allocated size,",
    "allow for a larger size and try again",
    "if the model string was larger than the initial buffer, allocate a bigger buffer and try again",
    "if the model string was larger than the initial buffer, allocate a bigger buffer and try again",
    ".Call() calls",
    "coding: utf-8",
    "alias table",
    "names",
    "from strings",
    "tails",
    "tails",
    "coding: utf-8",
    "Single row predictor to abstract away caching logic",
    "create boosting",
    "initialize the boosting",
    "create objective function",
    "initialize the objective function",
    "create training metric",
    "reset the boosting",
    "create objective function",
    "initialize the objective function",
    "calculate the nonzero data and indices size",
    "allocate data and indices arrays",
    "Get the number of trees per iteration (for multiclass scenario we output multiple sparse matrices)",
    "aggregated per row feature contribution results",
    "keep track of the row_vector sizes for parallelization",
    "copy vector results to output for each row",
    "Get the number of trees per iteration (for multiclass scenario we output multiple sparse matrices)",
    "aggregated per row feature contribution results",
    "calculate number of elements per column to construct",
    "the CSC matrix with random access",
    "keep track of column counts",
    "keep track of beginning index for each column",
    "keep track of beginning index for each matrix",
    "Note: we parallelize across matrices instead of rows because of the column_counts[m][col_idx] increment inside the loop",
    "store the row index",
    "update column count",
    "explicitly declare symbols from LightGBM namespace",
    "some help functions used to convert data",
    "Row iterator of on column for CSC matrix",
    "return value at idx, only can access by ascent order",
    "return next non-zero pair, if index < 0, means no more data",
    "start of c_api functions",
    "This API is to keep python binding's behavior the same with C++ implementation.",
    "Sample count, random seed etc. should be provided in parameters.",
    "sample data first",
    "sample data first",
    "sample data first",
    "local buffer to re-use memory",
    "sample data first",
    "no more data",
    "---- start of booster",
    "Single row in row-major format:",
    "---- start of some help functions",
    "data is array of pointers to individual rows",
    "set number of threads for openmp",
    "check for alias",
    "read parameters from config file",
    "remove str after \"#\"",
    "check for alias again",
    "load configs",
    "prediction is needed if using input initial model(continued train)",
    "need to continue training",
    "sync up random seed for data partition",
    "load Training data",
    "load data for distributed training",
    "load data for single machine",
    "need save binary file",
    "create training metric",
    "only when have metrics then need to construct validation data",
    "Add validation data, if it exists",
    "add",
    "need save binary file",
    "add metric for validation data",
    "output used time on each iteration",
    "need init network",
    "create boosting",
    "create objective function",
    "load training data",
    "initialize the objective function",
    "initialize the boosting",
    "add validation data into boosting",
    "convert model to if-else statement code",
    "create predictor",
    "Free memory",
    "create predictor",
    "label_gain = 2^i - 1, may overflow, so we use 31 here",
    "counts for all labels",
    "start from top label, and accumulate DCG",
    "counts for all labels",
    "calculate k Max DCG by one pass",
    "get sorted indices by score",
    "calculate multi dcg by one pass",
    "wait for all client start up",
    "Don't call MPI_Finalize() here: If the destructor was called because only this node had an exception, calling MPI_Finalize() will cause all nodes to hang.",
    "Instead we will handle finalize/abort for MPI in main().",
    "default set to -1",
    "distance at k-th communication, distance[k] = 2^k",
    "set incoming rank at k-th commuication",
    "set outgoing rank at k-th commuication",
    "default set as -1",
    "construct all recursive halving map for all machines",
    "let 1 << k <= num_machines",
    "distance of each communication",
    "if num_machines = 2^k, don't need to group machines",
    "communication direction, %2 == 0 is positive",
    "neighbor at k-th communication",
    "receive data block at k-th communication",
    "send data block at k-th communication",
    "if num_machines != 2^k, need to group machines",
    "group, two machine in one group, total \"rest\" groups will have 2 machines.",
    "let left machine as group leader",
    "cache block information for groups, group with 2 machines will have double block size",
    "convert from group to node leader",
    "convert from node to group",
    "meet new group",
    "add block len for this group",
    "calculate the group block start",
    "not need to construct",
    "get receive block information",
    "accumulate block len",
    "get send block information",
    "accumulate block len",
    "static member definition",
    "if small package or small count , do it by all gather.(reduce the communication times.)",
    "assign the blocks to every rank.",
    "do reduce scatter",
    "do all gather",
    "assign blocks",
    "need use buffer here, since size of \"output\" is smaller than size after all gather",
    "copy back",
    "assign blocks",
    "start all gather",
    "when num_machines is small and data is large",
    "use output as receive buffer",
    "get current local block size",
    "get out rank",
    "get in rank",
    "get send information",
    "get recv information",
    "send and recv at same time",
    "rotate in-place",
    "use output as receive buffer",
    "get current local block size",
    "get send information",
    "get recv information",
    "send and recv at same time",
    "use output as receive buffer",
    "send and recv at same time",
    "send local data to neighbor first",
    "receive neighbor data first",
    "reduce",
    "get target",
    "get send information",
    "get recv information",
    "send and recv at same time",
    "reduce",
    "send result to neighbor",
    "receive result from neighbor",
    "copy result",
    "start up socket",
    "parse clients from file",
    "get ip list of local machine",
    "get local rank",
    "construct listener",
    "construct communication topo",
    "construct linkers",
    "free listener",
    "set timeout",
    "accept incoming socket",
    "receive rank",
    "add new socket",
    "save ranks that need to connect with",
    "start listener",
    "start connect",
    "let smaller rank connect to larger rank",
    "send local rank",
    "wait for listener",
    "print connected linkers",
    "only need to copy subset",
    "avoid to copy subset many times",
    "avoid out of range",
    "may need to recopy subset",
    "valid the type",
    "Constructors",
    "Get type tag",
    "Comparisons",
    "This has to be separate, not in Statics, because Json() accesses",
    "statics().null.",
    "advance until next line, or end of input",
    "advance until closing tokens",
    "The usual case: non-escaped characters",
    "Handle escapes",
    "Extract 4-byte escape sequence",
    "Explicitly check length of the substring. The following loop",
    "relies on std::string returning the terminating NUL when",
    "accessing str[length]. Checking here reduces brittleness.",
    "JSON specifies that characters outside the BMP shall be encoded as a",
    "pair of 4-hex-digit \\u escapes encoding their surrogate pair",
    "components. Check whether we're in the middle of such a beast: the",
    "previous codepoint was an escaped lead (high) surrogate, and this is",
    "a trail (low) surrogate.",
    "Reassemble the two surrogate pairs into one astral-plane character,",
    "per the UTF-16 algorithm.",
    "Integer part",
    "Decimal part",
    "Exponent part",
    "Check for any trailing garbage",
    "Documented in json11.hpp",
    "Check for another object",
    "get column names",
    "load label idx first",
    "erase label column name",
    "load ignore columns",
    "load weight idx",
    "load group idx",
    "don't support query id in data file when using distributed training",
    "read data to memory",
    "sample data",
    "construct feature bin mappers",
    "initialize label",
    "extract features",
    "sample data from file",
    "construct feature bin mappers",
    "initialize label",
    "extract features",
    "load data from binary file",
    "check meta data",
    "need to check training data",
    "read data in memory",
    "initialize label",
    "extract features",
    "Get number of lines of data file",
    "initialize label",
    "extract features",
    "load data from binary file",
    "not need to check validation data",
    "check meta data",
    "buffer to read binary file",
    "check token",
    "read size of header",
    "re-allocmate space if not enough",
    "read header",
    "get header",
    "num_groups",
    "real_feature_idx_",
    "feature2group",
    "feature2subfeature",
    "group_bin_boundaries",
    "group_feature_start_",
    "group_feature_cnt_",
    "get feature names",
    "write feature names",
    "get forced_bin_bounds_",
    "read size of meta data",
    "re-allocate space if not enough",
    "read meta data",
    "load meta data",
    "sample local used data if need to partition",
    "if not contain query file, minimal sample unit is one record",
    "if contain query file, minimal sample unit is one query",
    "if is new query",
    "read feature data",
    "read feature size",
    "re-allocate space if not enough",
    "raw data",
    "fill feature_names_ if not header",
    "get forced split",
    "if only one machine, find bin locally",
    "if have multi-machines, need to find bin distributed",
    "different machines will find bin for different features",
    "start and len will store the process feature indices for different machines",
    "machine i will find bins for features in [ start[i], start[i] + len[i] )",
    "free",
    "gather global feature bin mappers",
    "restore features bins from buffer",
    "---- private functions ----",
    "if features are ordered, not need to use hist_buf",
    "read all lines",
    "get query data",
    "if not contain query data, minimal sample unit is one record",
    "if contain query data, minimal sample unit is one query",
    "if is new query",
    "get query data",
    "if not contain query file, minimal sample unit is one record",
    "if contain query file, minimal sample unit is one query",
    "if is new query",
    "parse features",
    "get forced split",
    "check the range of label_idx, weight_idx and group_idx",
    "fill feature_names_ if not header",
    "start find bins",
    "if only one machine, find bin locally",
    "start and len will store the process feature indices for different machines",
    "machine i will find bins for features in [ start[i], start[i] + len[i] )",
    "free",
    "gather global feature bin mappers",
    "restore features bins from buffer",
    "if doesn't need to prediction with initial model",
    "parser",
    "set label",
    "free processed line:",
    "shrink_to_fit will be very slow in linux, and seems not free memory, disable for now",
    "text_reader_->Lines()[i].shrink_to_fit();",
    "push data",
    "if is used feature",
    "if need to prediction with initial model",
    "parser",
    "set initial score",
    "set label",
    "free processed line:",
    "shrink_to_fit will be very slow in Linux, and seems not free memory, disable for now",
    "text_reader_->Lines()[i].shrink_to_fit();",
    "push data",
    "if is used feature",
    "metadata_ will manage space of init_score",
    "text data can be free after loaded feature values",
    "parser",
    "set initial score",
    "set label",
    "push data",
    "if is used feature",
    "only need part of data",
    "need full data",
    "metadata_ will manage space of init_score",
    "read size of token",
    "remove duplicates",
    "deep copy function for BinMapper",
    "mean size for one bin",
    "need a new bin",
    "update bin upper bound",
    "last bin upper bound",
    "get list of distinct values",
    "get number of positive and negative distinct values",
    "include zero bounds and infinity bound",
    "add forced bounds, excluding zeros since we have already added zero bounds",
    "find remaining bounds",
    "find distinct_values first",
    "push zero in the front",
    "use the large value",
    "push zero in the back",
    "convert to int type first",
    "sort by counts",
    "will ignore the categorical of small counts",
    "Push the dummy bin for NaN",
    "Use MissingType::None to represent this bin contains all categoricals",
    "fix count of NaN bin",
    "check trivial(num_bin_ == 1) feature",
    "check useless bin",
    "When most_freq_bin_ != default_bin_, there are some additional data loading costs.",
    "so use most_freq_bin_  = default_bin_ when there is not so sparse",
    "calculate max bin of all features to select the int type in MultiValDenseBin",
    "for lambdarank, it needs query data for partition data in distributed learning",
    "need convert query_id to boundaries",
    "check weights",
    "check query boundries",
    "contain initial score file",
    "check weights",
    "get local weights",
    "check query boundries",
    "get local query boundaries",
    "contain initial score file",
    "get local initial scores",
    "re-load query weight",
    "save to nullptr",
    "save to nullptr",
    "save to nullptr",
    "default weight file name",
    "default init_score file name",
    "use first line to count number class",
    "default query file name",
    "root is in the depth 0",
    "non-leaf",
    "leaf",
    "use this for the missing value conversion",
    "Predict func by Map to ifelse",
    "use this for the missing value conversion",
    "non-leaf",
    "left subtree",
    "right subtree",
    "leaf",
    "non-leaf",
    "left subtree",
    "right subtree",
    "leaf",
    "recursive computation of SHAP values for a decision tree",
    "extend the unique path",
    "leaf node",
    "internal node",
    "see if we have already split on this feature,",
    "if so we undo that split so we can redo it for this node",
    "recursive sparse computation of SHAP values for a decision tree",
    "extend the unique path",
    "leaf node",
    "internal node",
    "see if we have already split on this feature,",
    "if so we undo that split so we can redo it for this node",
    "add names of objective function if not providing metric",
    "equal weights for all classes",
    "generate seeds by seed.",
    "sort eval_at",
    "Only push the non-training data",
    "check for conflicts",
    "check if objective, metric, and num_class match",
    "Change pool size to -1 (no limit) when using data parallel to reduce communication costs",
    "Check max_depth and num_leaves",
    "Fits in an int, and is more restrictive than the current num_leaves",
    "force col-wise for gpu & CUDA",
    "force gpu_use_dp for CUDA",
    "linear tree learner must be serial type and run on CPU device",
    "min_data_in_leaf must be at least 2 if path smoothing is active. This is because when the split is calculated",
    "the count is calculated using the proportion of hessian in the leaf which is rounded up to nearest int, so it can",
    "be 1 when there is actually no data in the leaf. In rare cases this can cause a bug because with path smoothing the",
    "calculated split gain can be positive even with zero gradient and hessian.",
    "In distributed mode, local node doesn't have histograms on all features, cannot perform \"intermediate\" monotone constraints.",
    "\"intermediate\" monotone constraints need to recompute splits. If the features are sampled when computing the",
    "split initially, then the sampling needs to be recorded or done once again, which is currently not supported",
    "first round: fill the single val group",
    "always push the last group",
    "put dense feature first",
    "sort by non zero cnt",
    "sort by non zero cnt, bigger first",
    "shuffle groups",
    "Using std::swap for vector<bool> will cause the wrong result.",
    "get num_features",
    "get bin_mappers",
    "for sparse multi value bin, we store the feature bin values with offset added",
    "for dense multi value bin, the feature bin values without offsets are used",
    "copy feature bin mapper data",
    "copy feature bin mapper data",
    "if not pass a filename, just append \".bin\" of original file",
    "get size of header",
    "size of feature names",
    "size of forced bins",
    "write header",
    "write feature names",
    "write forced bins",
    "get size of meta data",
    "write meta data",
    "write feature data",
    "get size of feature",
    "write feature",
    "write raw data; use row-major order so we can read row-by-row",
    "explicitly initialize template methods, for cross module call",
    "Only one multi-val group, just simply merge",
    "Skip the leading 0 when copying group_bin_boundaries.",
    "regenerate other fields",
    "store the importance first",
    "PredictRaw",
    "PredictRawByMap",
    "Predict",
    "PredictByMap",
    "PredictLeafIndex",
    "PredictLeafIndexByMap",
    "output model type",
    "output number of class",
    "output label index",
    "output max_feature_idx",
    "output objective",
    "output tree models",
    "store the importance first",
    "sort the importance",
    "use serialized string to restore this object",
    "Use first 128 chars to avoid exceed the message buffer.",
    "get number of classes",
    "get index of label",
    "get max_feature_idx first",
    "get average_output",
    "get feature names",
    "get monotone_constraints",
    "set zero",
    "predict all the trees for one iteration",
    "check early stopping",
    "set zero",
    "predict all the trees for one iteration",
    "check early stopping",
    "margin_threshold will be captured by value",
    "copy and sort",
    "margin_threshold will be captured by value",
    "Fix for compiler warnings about reaching end of control",
    "load forced_splits file",
    "init tree learner",
    "push training metrics",
    "create buffer for gradients and Hessians",
    "get max feature index",
    "get label index",
    "get feature names",
    "if need bagging, create buffer",
    "for a validation dataset, we need its score and metric",
    "update score",
    "objective function will calculate gradients and hessians",
    "random bagging, minimal unit is one record",
    "random bagging, minimal unit is one record",
    "if need bagging",
    "set bagging data to tree learner",
    "get subset",
    "output used time per iteration",
    "boosting from average label; or customized \"average\" if implemented for the current objective",
    "boosting first",
    "bagging logic",
    "need to copy gradients for bagging subset.",
    "shrinkage by learning rate",
    "update score",
    "only add default score one-time",
    "updates scores",
    "add model",
    "reset score",
    "remove model",
    "print message for metric",
    "pop last early_stopping_round_ models",
    "update training score",
    "we need to predict out-of-bag scores of data for boosting",
    "update validation score",
    "print training metric",
    "print validation metric",
    "set zero",
    "predict all the trees for one iteration",
    "predict all the trees for one iteration",
    "push training metrics",
    "not same training data, need reset score and others",
    "create score tracker",
    "update score",
    "create buffer for gradients and hessians",
    "load forced_splits file",
    "if need bagging, create buffer",
    "Get the max size of pool",
    "at least need 2 leaves",
    "push split information for all leaves",
    "initialize splits for leaf",
    "initialize data partition",
    "initialize ordered gradients and hessians",
    "cannot change is_hist_col_wise during training",
    "initialize splits for leaf",
    "initialize data partition",
    "initialize ordered gradients and hessians",
    "Get the max size of pool",
    "at least need 2 leaves",
    "push split information for all leaves",
    "some initial works before training",
    "root leaf",
    "only root leaf can be splitted on first time",
    "some initial works before finding best split",
    "find best threshold for every feature",
    "Get a leaf with max split gain",
    "Get split information for best leaf",
    "cannot split, quit",
    "split tree with best leaf",
    "reset histogram pool",
    "initialize data partition",
    "reset the splits for leaves",
    "Sumup for root",
    "use all data",
    "use bagging, only use part of data",
    "check depth of current leaf",
    "only need to check left leaf, since right leaf is in same level of left leaf",
    "no enough data to continue",
    "only have root",
    "put parent(left) leaf's histograms into larger leaf's histograms",
    "put parent(left) leaf's histograms to larger leaf's histograms",
    "construct smaller leaf",
    "construct larger leaf",
    "find splits",
    "only has root leaf",
    "start at root leaf",
    "before processing next node from queue, store info for current left/right leaf",
    "store \"best split\" for left and right, even if they might be overwritten by forced split",
    "then, compute own splits",
    "split info should exist because searching in bfs fashion - should have added from parent",
    "update before tree split",
    "don't need to update this in data-based parallel model",
    "split tree, will return right leaf",
    "store the true split gain in tree model",
    "don't need to update this in data-based parallel model",
    "store the true split gain in tree model",
    "init the leaves that used on next iteration",
    "update leave outputs if needed",
    "bag_mapper[index_mapper[i]]",
    "it is needed to filter the features after the above code.",
    "Otherwise, the `is_splittable` in `FeatureHistogram` will be wrong, and cause some features being accidentally filtered in the later nodes.",
    "for root leaf the \"parent\" output is its own output because we don't apply any smoothing to the root",
    "can't use GetParentOutput because leaf_splits doesn't have weight property set",
    "find splits",
    "identify features containing nans",
    "preallocate the matrix used to calculate linear model coefficients",
    "store only upper triangular half of matrix as an array, in row-major order",
    "this requires (max_num_feat + 1) * (max_num_feat + 2) / 2 entries (including the constant terms of the regression)",
    "we add another 8 to ensure cache lines are not shared among processors",
    "some initial works before training",
    "root leaf",
    "only root leaf can be splitted on first time",
    "some initial works before finding best split",
    "find best threshold for every feature",
    "Get a leaf with max split gain",
    "Get split information for best leaf",
    "cannot split, quit",
    "split tree with best leaf",
    "map data to leaf number",
    "calculate coefficients using the method described in Eq 3 of https://arxiv.org/pdf/1802.05640.pdf",
    "the coefficients vector is given by",
    "- (X_T * H * X + lambda) ^ (-1) * (X_T * g)",
    "where:",
    "X is the matrix where the first column is the feature values and the second is all ones,",
    "H is the diagonal matrix of the hessian,",
    "lambda is the diagonal matrix with diagonal entries equal to the regularisation term linear_lambda",
    "g is the vector of gradients",
    "the subscript _T denotes the transpose",
    "create array of pointers to raw data, and coefficient matrices, for each leaf",
    "clear the coefficient matrices",
    "aggregate results from different threads",
    "copy into eigen matrices and solve",
    "update the tree properties",
    "need to be able to hold smaller and larger best splits in SyncUpGlobalBestSplit",
    "get feature partition",
    "get local used features",
    "get best split at smaller leaf",
    "find local best split for larger leaf",
    "sync global best info",
    "update best split",
    "instantiate template classes, otherwise linker cannot find the code",
    "initialize SerialTreeLearner",
    "Get local rank and global machine size",
    "need to be able to hold smaller and larger best splits in SyncUpGlobalBestSplit",
    "allocate buffer for communication",
    "generate feature partition for current tree",
    "get local used feature",
    "get block start and block len for reduce scatter",
    "get buffer_write_start_pos_",
    "get buffer_read_start_pos_",
    "sync global data sumup info",
    "global sumup reduce",
    "copy back",
    "set global sumup info",
    "init global data count in leaf",
    "clear histogram buffer before synchronizing",
    "otherwise histogram contents from the previous iteration will be sent",
    "construct local histograms",
    "copy to buffer",
    "Reduce scatter for histogram",
    "restore global histograms from buffer",
    "only root leaf",
    "construct histgroms for large leaf, we init larger leaf as the parent, so we can just subtract the smaller leaf's histograms",
    "find local best split for larger leaf",
    "sync global best info",
    "set best split",
    "need update global number of data in leaf",
    "instantiate template classes, otherwise linker cannot find the code",
    "initialize SerialTreeLearner",
    "some additional variables needed for GPU trainer",
    "Initialize GPU buffers and kernels",
    "some functions used for debugging the GPU histogram construction",
    "printf(\"grad %g != %g (%d ULPs)\\n\", GET_GRAD(h1, i), GET_GRAD(h2, i), ulps);",
    "goto err;",
    "we roughly want 256 workgroups per device, and we have num_dense_feature4_ feature tuples.",
    "also guarantee that there are at least 2K examples per workgroup",
    "return 0;",
    "we have already copied ordered gradients, ordered Hessians and indices to GPU",
    "decide the best number of workgroups working on one feature4 tuple",
    "set work group size based on feature size",
    "each 2^exp_workgroups_per_feature workgroups work on a feature4 tuple",
    "we need to refresh the kernel arguments after reallocating",
    "The only argument that needs to be changed later is num_data_",
    "the GPU kernel will process all features in one call, and each",
    "2^exp_workgroups_per_feature (compile time constant) workgroup will",
    "process one feature4 tuple",
    "for the root node, indices are not copied",
    "for constant hessian, hessians are not copied except for the root node",
    "there will be 2^exp_workgroups_per_feature = num_workgroups / num_dense_feature4 sub-histogram per feature4",
    "and we will launch num_feature workgroups for this kernel",
    "will launch threads for all features",
    "the queue should be asynchronous, and we will can WaitAndGetHistograms() before we start processing dense feature groups",
    "copy the results asynchronously. Size depends on if double precision is used",
    "we will wait for this object in WaitAndGetHistograms",
    "when the output is ready, the computation is done",
    "values of this feature has been redistributed to multiple bins; need a reduction here",
    "how many feature-group tuples we have",
    "leave some safe margin for prefetching",
    "256 work-items per workgroup. Each work-item prefetches one tuple for that feature",
    "clear sparse/dense maps",
    "do nothing if no features can be processed on GPU",
    "allocate memory for all features (FIXME: 4 GB barrier on some devices, need to split to multiple buffers)",
    "unpin old buffer if necessary before destructing them",
    "make ordered_gradients and Hessians larger (including extra room for prefetching), and pin them",
    "allocate space for gradients and Hessians on device",
    "we will copy gradients and Hessians in after ordered_gradients_ and ordered_hessians_ are constructed",
    "allocate feature mask, for disabling some feature-groups' histogram calculation",
    "copy indices to the device",
    "histogram bin entry size depends on the precision (single/double)",
    "create output buffer, each feature has a histogram with device_bin_size_ bins,",
    "each work group generates a sub-histogram of dword_features_ features.",
    "only initialize once here, as this will not need to change when ResetTrainingData() is called",
    "create atomic counters for inter-group coordination",
    "The output buffer is allocated to host directly, to overlap compute and data transfer",
    "find the dense feature-groups and group then into Feature4 data structure (several feature-groups packed into 4 bytes)",
    "looking for dword_features_ non-sparse feature-groups",
    "decide if we need to redistribute the bin",
    "multiplier must be a power of 2",
    "device_bin_mults_.push_back(1);",
    "found",
    "for data transfer time",
    "Now generate new data structure feature4, and copy data to the device",
    "preallocate arrays for all threads, and pin them",
    "building Feature4 bundles; each thread handles dword_features_ features",
    "one feature datapoint is 4 bits",
    "this guarantees that the RawGet() function is inlined, rather than using virtual function dispatching",
    "one feature datapoint is one byte",
    "this guarantees that the RawGet() function is inlined, rather than using virtual function dispatching",
    "Dense bin",
    "Dense 4-bit bin",
    "working on the remaining (less than dword_features_) feature groups",
    "fill the leftover features",
    "fill this empty feature with some \"random\" value",
    "fill this empty feature with some \"random\" value",
    "copying the last 1 to (dword_features - 1) feature-groups in the last tuple",
    "deallocate pinned space for feature copying",
    "data transfer time",
    "for other types of failure, build log might not be available; program.build_log() can crash",
    "Something bad happened. Just return \"No log available.\"",
    "build is okay, log may contain warnings",
    "destroy any old kernels",
    "create OpenCL kernels for different number of workgroups per feature",
    "currently we don't use constant memory",
    "compile the GPU kernel depending if double precision is used, constant hessian is used, etc.",
    "kernel with indices in an array",
    "kernel with all features enabled, with eliminated branches",
    "kernel with all data indices (for root node, and assumes that root node always uses all features)",
    "do nothing if no features can be processed on GPU",
    "The only argument that needs to be changed later is num_data_",
    "hessian is passed as a parameter, but it is not available now.",
    "hessian will be set in BeforeTrain()",
    "Get the max bin size, used for selecting best GPU kernel",
    "initialize GPU",
    "determine which kernel to use based on the max number of bins",
    "the +9 skips extra characters \")\", newline, \"#endif\" and newline at the beginning",
    "the +9 skips extra characters \")\", newline, \"#endif\" and newline at the beginning",
    "the +9 skips extra characters \")\", newline, \"#endif\" and newline at the beginning",
    "setup GPU kernel arguments after we allocating all the buffers",
    "GPU memory has to been reallocated because data may have been changed",
    "setup GPU kernel arguments after we allocating all the buffers",
    "Copy initial full hessians and gradients to GPU.",
    "We start copying as early as possible, instead of at ConstructHistogram().",
    "setup hessian parameters only",
    "hessian is passed as a parameter",
    "use bagging",
    "On GPU, we start copying indices, gradients and Hessians now, instead at ConstructHistogram()",
    "copy used gradients and Hessians to ordered buffer",
    "transfer the indices to GPU",
    "transfer hessian to GPU",
    "setup hessian parameters only",
    "hessian is passed as a parameter",
    "transfer gradients to GPU",
    "only have root",
    "Copy indices, gradients and Hessians as early as possible",
    "only need to initialize for smaller leaf",
    "Get leaf boundary",
    "copy indices to the GPU:",
    "copy ordered Hessians to the GPU:",
    "copy ordered gradients to the GPU:",
    "do nothing if no features can be processed on GPU",
    "copy data indices if it is not null",
    "generate and copy ordered_gradients if gradients is not null",
    "generate and copy ordered_hessians if Hessians is not null",
    "converted indices in is_feature_used to feature-group indices",
    "construct the feature masks for dense feature-groups",
    "if no feature group is used, just return and do not use GPU",
    "if not all feature groups are used, we need to transfer the feature mask to GPU",
    "otherwise, we will use a specialized GPU kernel with all feature groups enabled",
    "All data have been prepared, now run the GPU kernel",
    "construct smaller leaf",
    "ConstructGPUHistogramsAsync will return true if there are available feature groups dispatched to GPU",
    "then construct sparse features on CPU",
    "wait for GPU to finish, only if GPU is actually used",
    "use double precision",
    "use single precision",
    "Compare GPU histogram with CPU histogram, useful for debugging GPU code problem",
    "#define GPU_DEBUG_COMPARE",
    "construct larger leaf",
    "then construct sparse features on CPU",
    "wait for GPU to finish, only if GPU is actually used",
    "use double precision",
    "use single precision",
    "do some sanity check for the GPU algorithm",
    "limit top k",
    "get max bin",
    "calculate buffer size",
    "need to be able to hold smaller and larger best splits in SyncUpGlobalBestSplit",
    "left and right on same time, so need double size",
    "initialize histograms for global",
    "sync global data sumup info",
    "set global sumup info",
    "init global data count in leaf",
    "get local sumup",
    "get local sumup",
    "get mean number on machines",
    "weighted gain",
    "get top k",
    "Copy histogram to buffer, and Get local aggregate features",
    "copy histograms.",
    "copy smaller leaf histograms first",
    "mark local aggregated feature",
    "copy",
    "then copy larger leaf histograms",
    "mark local aggregated feature",
    "copy",
    "use local data to find local best splits",
    "clear histogram buffer before synchronizing",
    "otherwise histogram contents from the previous iteration will be sent",
    "find splits",
    "only has root leaf",
    "local voting",
    "gather",
    "get all top-k from all machines",
    "global voting",
    "copy local histgrams to buffer",
    "Reduce scatter for histogram",
    "find best split from local aggregated histograms",
    "restore from buffer",
    "restore from buffer",
    "find local best",
    "find local best split for larger leaf",
    "sync global best info",
    "copy back",
    "set the global number of data for leaves",
    "init the global sumup info",
    "instantiate template classes, otherwise linker cannot find the code",
    "launch cuda kernel",
    "initialize SerialTreeLearner",
    "some additional variables needed for GPU trainer",
    "Initialize GPU buffers and kernels: get device info",
    "some functions used for debugging the GPU histogram construction",
    "we roughly want 256 workgroups per device, and we have num_dense_feature4_ feature tuples.",
    "also guarantee that there are at least 2K examples per workgroup",
    "we have already copied ordered gradients, ordered hessians and indices to GPU",
    "decide the best number of workgroups working on one feature4 tuple",
    "set work group size based on feature size",
    "each 2^exp_workgroups_per_feature workgroups work on a feature4 tuple",
    "set thread_data",
    "copy the results asynchronously. Size depends on if double precision is used",
    "when the output is ready, the computation is done",
    "how many feature-group tuples we have",
    "leave some safe margin for prefetching",
    "256 work-items per workgroup. Each work-item prefetches one tuple for that feature",
    "clear sparse/dense maps",
    "do nothing it there is no dense feature",
    "calculate number of feature groups per gpu",
    "histogram bin entry size depends on the precision (single/double)",
    "allocate GPU memory for each GPU",
    "do nothing it there is no gpu feature",
    "allocate memory for all features",
    "allocate space for gradients and hessians on device",
    "we will copy gradients and hessians in after ordered_gradients_ and ordered_hessians_ are constructed",
    "copy indices to the device",
    "create output buffer, each feature has a histogram with device_bin_size_ bins,",
    "each work group generates a sub-histogram of dword_features_ features.",
    "only initialize once here, as this will not need to change when ResetTrainingData() is called",
    "create atomic counters for inter-group coordination",
    "The output buffer is allocated to host directly, to overlap compute and data transfer",
    "clear sparse/dense maps",
    "find the dense feature-groups and group then into Feature4 data structure (several feature-groups packed into 4 bytes)",
    "set device info",
    "looking for dword_features_ non-sparse feature-groups",
    "reset device info",
    "InitGPU w/ num_gpu",
    "Get the max bin size, used for selecting best GPU kernel",
    "get num_dense_feature_groups_",
    "initialize GPU",
    "set cpu threads",
    "resize device memory pointers",
    "create stream & events to handle multiple GPUs",
    "check data size",
    "GPU memory has to been reallocated because data may have been changed",
    "AllocateGPUMemory only when the number of data increased",
    "setup GPU kernel arguments after we allocating all the buffers",
    "Copy initial full hessians and gradients to GPU.",
    "We start copying as early as possible, instead of at ConstructHistogram().",
    "use bagging",
    "On GPU, we start copying indices, gradients and hessians now, instead at ConstructHistogram()",
    "copy used gradients and hessians to ordered buffer",
    "transfer the indices to GPU",
    "only have root",
    "Copy indices, gradients and hessians as early as possible",
    "only need to initialize for smaller leaf",
    "Get leaf boundary",
    "do nothing if no features can be processed on GPU",
    "copy data indices if it is not null",
    "converted indices in is_feature_used to feature-group indices",
    "construct the feature masks for dense feature-groups",
    "if no feature group is used, just return and do not use GPU",
    "if not all feature groups are used, we need to transfer the feature mask to GPU",
    "otherwise, we will use a specialized GPU kernel with all feature groups enabled",
    "We now copy even if all features are used.",
    "All data have been prepared, now run the GPU kernel",
    "construct smaller leaf",
    "Check workgroups per feature4 tuple..",
    "if the workgroup per feature is 1 (2^0), return as the work is too small for a GPU",
    "ConstructGPUHistogramsAsync will return true if there are availabe feature groups dispatched to GPU",
    "then construct sparse features on CPU",
    "We set data_indices to null to avoid rebuilding ordered gradients/hessians",
    "wait for GPU to finish, only if GPU is actually used",
    "use double precision",
    "use single precision",
    "Compare GPU histogram with CPU histogram, useful for debuggin GPU code problem",
    "#define CUDA_DEBUG_COMPARE",
    "construct larger leaf",
    "then construct sparse features on CPU",
    "We set data_indices to null to avoid rebuilding ordered gradients/hessians",
    "wait for GPU to finish, only if GPU is actually used",
    "use double precision",
    "use single precision",
    "do some sanity check for the GPU algorithm"
  ],
  "v3.3.1": [
    "coding: utf-8",
    "coding: utf-8",
    "create predictor first",
    "show deprecation warning only for early stop argument, setting early stop via global params should still be possible",
    "check dataset",
    "reduce cost for prediction training data",
    "process callbacks",
    "Most of legacy advanced options becomes callbacks",
    "construct booster",
    "start training",
    "check evaluation result.",
    "ranking task, split according to groups",
    "run preprocessing on the data set if needed",
    "setup callbacks",
    "coding: utf-8",
    "dummy function to support older version of scikit-learn",
    "coding: utf-8",
    "documentation templates for LGBMModel methods are shared between the classes in",
    "this module and those in the ``dask`` module",
    "user can set verbose with kwargs, it has higher priority",
    "Do not modify original args in fit function",
    "Refer to https://github.com/microsoft/LightGBM/pull/2619",
    "Separate built-in from callable evaluation metrics",
    "register default metric for consistency with callable eval_metric case",
    "try to deduce from class instance",
    "overwrite default metric by explicitly set metric",
    "concatenate metric from params (or default if not provided in params) and eval_metric",
    "copy for consistency",
    "reduce cost for prediction training data",
    "free dataset",
    "Switch to using a multiclass objective in the underlying LGBM instance",
    "do not modify args, as it causes errors in model selection tools",
    "check group data",
    "coding: utf-8",
    "we don't need lib_lightgbm while building docs",
    "coding: utf-8",
    "\"#ddffdd\" is light green, \"#ffdddd\" is light red",
    "coding: utf-8",
    "coding: utf-8",
    "TypeError: obj is not a string or a number",
    "ValueError: invalid literal",
    "DeprecationWarning is not shown by default, so let's create our own with higher level",
    "avoid side effects on passed-in parameters",
    "find a value, and remove other aliases with .pop()",
    "prefer the value of 'main_param_name' if it exists, otherwise search the aliases",
    "Get total row number.",
    "Random access by row index. Used for data sampling.",
    "Range data access. Used to read data in batch when constructing Dataset.",
    "Optionally specify batch_size to control range data read size.",
    "Only required if using ``Dataset.subset()``.",
    "__get_num_preds() cannot work with nrow > MAX_INT32, so calculate overall number of predictions piecemeal",
    "avoid memory consumption by arrays concatenation operations",
    "create numpy array from output arrays",
    "break up indptr based on number of rows (note more than one matrix in multiclass case)",
    "for CSC there is extra column added",
    "reformat output into a csr or csc matrix or list of csr or csc matrices",
    "same shape as input csr or csc matrix except extra column for expected value",
    "note: make sure we copy data as it will be deallocated next",
    "free the temporary native indptr, indices, and data",
    "__get_num_preds() cannot work with nrow > MAX_INT32, so calculate overall number of predictions piecemeal",
    "avoid memory consumption by arrays concatenation operations",
    "c type: double**",
    "each double* element points to start of each column of sample data.",
    "c type int**",
    "each int* points to start of indices for each column",
    "no min_data, nthreads and verbose in this function",
    "check data has header or not",
    "need to regroup init_score",
    "process for args",
    "user can set verbose with params, it has higher priority",
    "get categorical features",
    "process for reference dataset",
    "start construct data",
    "set feature names",
    "Select sampled rows, transpose to column order.",
    "create validation dataset from ref_dataset",
    "create valid",
    "construct subset",
    "create train",
    "could be updated if data is not freed",
    "set to None",
    "we're done if self and reference share a common upstream reference",
    "if buffer length is not long enough, reallocate buffers",
    "group data from LightGBM is boundaries data, need to convert to group size",
    "user can set verbose with params, it has higher priority",
    "Training task",
    "if \"machines\" is given, assume user wants to do distributed learning, and set up network",
    "construct booster object",
    "copy the parameters from train_set",
    "save reference to data",
    "buffer for inner predict",
    "Prediction task",
    "if a single node tree it won't have `leaf_index` so return 0",
    "Create the node record, and populate universal data members",
    "Update values to reflect node type (leaf or split)",
    "traverse the next level of the tree",
    "In tree format, \"subtree_list\" is a list of node records (dicts),",
    "and we add node to the list.",
    "need reset training data",
    "need to push new valid data",
    "if buffer length is not long enough, re-allocate a buffer",
    "if buffer length is not long enough, reallocate a buffer",
    "Copy models",
    "Get name of features",
    "if buffer length is not long enough, reallocate buffers",
    "avoid to predict many time in one iteration",
    "Get num of inner evals",
    "Get name of eval metrics",
    "if buffer length is not long enough, reallocate buffers",
    "coding: utf-8",
    "Callback environment used by callbacks",
    "split is needed for \"<dataset type> <metric>\" case (e.g. \"train l1\")",
    "split is needed for \"<dataset type> <metric>\" case (e.g. \"train l1\")",
    "coding: utf-8",
    "Concatenate many parts into one",
    "construct local eval_set data.",
    "store indices of eval_set components that were not contained within local parts.",
    "consolidate parts of each individual eval component.",
    "require that eval_name exists in evaluated result data in case dropped due to padding.",
    "in distributed training the 'training' eval_set is not detected, will have name 'valid_<index>'.",
    "filter padding from eval parts then _concat each eval_set component.",
    "reconstruct eval_set fit args/kwargs depending on which components of eval_set are on worker.",
    "ensure that expected keys for evals_result_ and best_score_ exist regardless of padding.",
    "capture whether local_listen_port or its aliases were provided",
    "capture whether machines or its aliases were provided",
    "Some passed-in parameters can be removed:",
    "* 'num_machines': set automatically from Dask worker list",
    "* 'num_threads': overridden to match nthreads on each Dask process",
    "Split arrays/dataframes into parts. Arrange parts into dicts to enforce co-locality",
    "evals_set will to be re-constructed into smaller lists of (X, y) tuples, where",
    "X and y are each delayed sub-lists of original eval dask Collections.",
    "find maximum number of parts in an individual eval set so that we can",
    "pad eval sets when they come in different sizes.",
    "when individual eval set is equivalent to training data, skip recomputing parts.",
    "add None-padding for individual eval_set member if it is smaller than the largest member.",
    "first time a chunk of this eval set is added to this part.",
    "append additional chunks of this eval set to this part.",
    "ensure that all evaluation parts map uniquely to one part.",
    "assign sub-eval_set components to worker parts.",
    "Start computation in the background",
    "Find locations of all parts and map them to particular Dask workers",
    "Check that all workers were provided some of eval_set. Otherwise warn user that validation",
    "data artifacts may not be populated depending on worker returning final estimator.",
    "assign general validation set settings to fit kwargs.",
    "resolve aliases for network parameters and pop the result off params.",
    "these values are added back in calls to `_train_part()`",
    "figure out network params",
    "Tell each worker to train on the parts that it has locally",
    "",
    "This code treats ``_train_part()`` calls as not \"pure\" because:",
    "1. there is randomness in the training process unless parameters ``seed``",
    "and ``deterministic`` are set",
    "2. even with those parameters set, the output of one ``_train_part()`` call",
    "relies on global state (it and all the other LightGBM training processes",
    "coordinate with each other)",
    "if network parameters were changed during training, remove them from the",
    "returned model so that they're generated dynamically on every run based",
    "on the Dask cluster you're connected to and which workers have pieces of",
    "the training data",
    "dask.DataFrame.map_partitions() expects each call to return a pandas DataFrame or Series",
    "for multi-class classification with sparse matrices, pred_contrib predictions",
    "are returned as a list of sparse matrices (one per class)",
    "pred_contrib output will have one column per feature,",
    "plus one more for the base value",
    "need to tell Dask the expected type and shape of individual preds",
    "by default, dask.array.concatenate() concatenates sparse arrays into a COO matrix",
    "the code below is used instead to ensure that the sparse type is preserved during concatentation",
    "At this point, `out` is a list of lists of delayeds (each of which points to a matrix).",
    "Concatenate them to return a list of Dask Arrays.",
    "the note on custom objective functions in LGBMModel.__init__ is not",
    "currently relevant for the Dask estimators",
    "DaskLGBMClassifier does not support group, eval_group, early_stopping_rounds.",
    "DaskLGBMClassifier support for callbacks and init_model is not tested",
    "the note on custom objective functions in LGBMModel.__init__ is not",
    "currently relevant for the Dask estimators",
    "DaskLGBMRegressor does not support group, eval_class_weight, eval_group, early_stopping_rounds.",
    "DaskLGBMRegressor support for callbacks and init_model is not tested",
    "the note on custom objective functions in LGBMModel.__init__ is not",
    "currently relevant for the Dask estimators",
    "DaskLGBMRanker does not support eval_class_weight or early stopping",
    "DaskLGBMRanker support for callbacks and init_model is not tested",
    "coding: utf-8",
    "load or create your dataset",
    "create dataset for lightgbm",
    "if you want to re-use data, remember to set free_raw_data=False",
    "specify your configurations as a dict",
    "generate feature names",
    "feature_name and categorical_feature",
    "check feature name",
    "save model to file",
    "dump model to JSON (and save to file)",
    "feature names",
    "feature importances",
    "load model to predict",
    "can only predict with the best iteration (or the saving iteration)",
    "eval with loaded model",
    "dump model with pickle",
    "load model with pickle to predict",
    "can predict with any iteration when loaded in pickle way",
    "eval with loaded model",
    "continue training",
    "init_model accepts:",
    "1. model file name",
    "2. Booster()",
    "decay learning rates",
    "learning_rates accepts:",
    "1. list/tuple with length = num_boost_round",
    "2. function(curr_iter)",
    "change other parameters during training",
    "self-defined objective function",
    "f(preds: array, train_data: Dataset) -> grad: array, hess: array",
    "log likelihood loss",
    "self-defined eval metric",
    "f(preds: array, train_data: Dataset) -> name: str, eval_result: float, is_higher_better: bool",
    "binary error",
    "NOTE: when you do customized loss function, the default prediction value is margin",
    "This may make built-in evaluation metric calculate wrong results",
    "For example, we are doing log likelihood loss, the prediction is score before logistic transformation",
    "Keep this in mind when you use the customization",
    "another self-defined eval metric",
    "f(preds: array, train_data: Dataset) -> name: str, eval_result: float, is_higher_better: bool",
    "accuracy",
    "NOTE: when you do customized loss function, the default prediction value is margin",
    "This may make built-in evaluation metric calculate wrong results",
    "For example, we are doing log likelihood loss, the prediction is score before logistic transformation",
    "Keep this in mind when you use the customization",
    "callback",
    "coding: utf-8",
    "load or create your dataset",
    "train",
    "predict",
    "eval",
    "feature importances",
    "self-defined eval metric",
    "f(y_true: array, y_pred: array) -> name: str, eval_result: float, is_higher_better: bool",
    "Root Mean Squared Logarithmic Error (RMSLE)",
    "train",
    "another self-defined eval metric",
    "f(y_true: array, y_pred: array) -> name: str, eval_result: float, is_higher_better: bool",
    "Relative Absolute Error (RAE)",
    "train",
    "predict",
    "eval",
    "other scikit-learn modules",
    "coding: utf-8",
    "load or create your dataset",
    "create dataset for lightgbm",
    "specify your configurations as a dict",
    "train",
    "coding: utf-8",
    "################",
    "Simulate some binary data with a single categorical and",
    "single continuous predictor",
    "################",
    "Set up a couple of utilities for our experiments",
    "################",
    "Observe the behavior of `binary` and `xentropy` objectives",
    "Trying this throws an error on non-binary values of y:",
    "experiment('binary', label_type='probability', DATA)",
    "The speed of `binary` is not drastically different than",
    "`xentropy`. `xentropy` runs faster than `binary` in many cases, although",
    "there are reasons to suspect that `binary` should run faster when the",
    "label is an integer instead of a float",
    "coding: utf-8",
    "load or create your dataset",
    "create dataset for lightgbm",
    "specify your configurations as a dict",
    "train",
    "save model to file",
    "predict",
    "eval",
    "We can also open HDF5 file once and get access to",
    "With binary dataset created, we can use either Python API or cmdline version to train.",
    "",
    "Note: in order to create exactly the same dataset with the one created in simple_example.py, we need",
    "to modify simple_example.py to pass numpy array instead of pandas DataFrame to Dataset constructor.",
    "The reason is that DataFrame column names will be used in Dataset. For a DataFrame with Int64Index",
    "as columns, Dataset will use column names like [\"0\", \"1\", \"2\", ...]. While for numpy array, column names",
    "are using the default one assigned in C++ code (dataset_loader.cpp), like [\"Column_0\", \"Column_1\", ...].",
    "Y has a single column and we read it in single shot. So store it as an 1-d array.",
    "We use random access for data sampling when creating LightGBM Dataset from Sequence.",
    "When accessing any element in a HDF5 chunk, it's read entirely.",
    "To save I/O for sampling, we should keep number of total chunks much larger than sample count.",
    "Here we are just creating a chunk size that matches with batch_size.",
    "",
    "Also note that the data is stored in row major order to avoid extra copy when passing to",
    "lightgbm Dataset.",
    "Save to 2 HDF5 files for demonstration.",
    "We can store multiple datasets inside a single HDF5 file.",
    "Separating X and Y for choosing best chunk size for data loading.",
    "split training data into two partitions",
    "make this array dense because we're splitting across",
    "a sparse boundary to partition the data",
    "the code below uses sklearn.metrics, but this requires pulling all of the",
    "predictions and target values back from workers to the client",
    "",
    "for larger datasets, consider the metrics from dask-ml instead",
    "https://ml.dask.org/modules/api.html#dask-ml-metrics-metrics",
    "coding: utf-8",
    "!/usr/bin/env python3",
    "-*- coding: utf-8 -*-",
    "",
    "LightGBM documentation build configuration file, created by",
    "sphinx-quickstart on Thu May  4 14:30:58 2017.",
    "",
    "This file is execfile()d with the current directory set to its",
    "containing dir.",
    "",
    "Note that not all possible configuration values are present in this",
    "autogenerated file.",
    "",
    "All configuration values have a default; values that are commented out",
    "serve to show the default.",
    "If extensions (or modules to document with autodoc) are in another directory,",
    "add these directories to sys.path here. If the directory is relative to the",
    "documentation root, use os.path.abspath to make it absolute.",
    "-- mock out modules",
    "-- General configuration ------------------------------------------------",
    "If your documentation needs a minimal Sphinx version, state it here.",
    "Add any Sphinx extension module names here, as strings. They can be",
    "extensions coming with Sphinx (named 'sphinx.ext.*') or your custom",
    "ones.",
    "hide type hints in API docs",
    "Generate autosummary pages. Output should be set with: `:toctree: pythonapi/`",
    "Only the class' docstring is inserted.",
    "If true, `todo` and `todoList` produce output, else they produce nothing.",
    "The master toctree document.",
    "General information about the project.",
    "The name of an image file (relative to this directory) to place at the top",
    "of the sidebar.",
    "The name of an image file (relative to this directory) to use as a favicon of",
    "the docs.  This file should be a Windows icon file (.ico) being 16x16 or 32x32",
    "pixels large.",
    "The version info for the project you're documenting, acts as replacement for",
    "|version| and |release|, also used in various other places throughout the",
    "built documents.",
    "The short X.Y version.",
    "The full version, including alpha/beta/rc tags.",
    "The language for content autogenerated by Sphinx. Refer to documentation",
    "for a list of supported languages.",
    "",
    "This is also used if you do content translation via gettext catalogs.",
    "Usually you set \"language\" from the command line for these cases.",
    "List of patterns, relative to source directory, that match files and",
    "directories to ignore when looking for source files.",
    "This patterns also effect to html_static_path and html_extra_path",
    "The name of the Pygments (syntax highlighting) style to use.",
    "-- Configuration for C API docs generation ------------------------------",
    "-- Options for HTML output ----------------------------------------------",
    "The theme to use for HTML and HTML Help pages.  See the documentation for",
    "a list of builtin themes.",
    "Theme options are theme-specific and customize the look and feel of a theme",
    "further.  For a list of options available for each theme, see the",
    "documentation.",
    "Add any paths that contain custom static files (such as style sheets) here,",
    "relative to this directory. They are copied after the builtin static files,",
    "so a file named \"default.css\" will overwrite the builtin \"default.css\".",
    "-- Options for HTMLHelp output ------------------------------------------",
    "Output file base name for HTML help builder.",
    "-- Options for LaTeX output ---------------------------------------------",
    "The name of an image file (relative to this directory) to place at the top of",
    "the title page.",
    "Warning! The following code can cause buffer overflows on RTD.",
    "Consider suppressing output completely if RTD project silently fails.",
    "Refer to https://github.com/svenevs/exhale",
    "/blob/fe7644829057af622e467bb529db6c03a830da99/exhale/deploy.py#L99-L111",
    "Warning! The following code can cause buffer overflows on RTD.",
    "Consider suppressing output completely if RTD project silently fails.",
    "Refer to https://github.com/svenevs/exhale",
    "/blob/fe7644829057af622e467bb529db6c03a830da99/exhale/deploy.py#L99-L111",
    "coding: utf-8",
    "This is a basic test for floating number parsing.",
    "Most of the test cases come from:",
    "https://github.com/dmlc/xgboost/blob/master/tests/cpp/common/test_charconv.cc",
    "https://github.com/Alexhuszagh/rust-lexical/blob/master/data/test-parse-unittests/strtod_tests.toml",
    "FLT_MAX",
    "FLT_MIN",
    "DBL_MAX (1 + (1 - 2^-52)) * 2^1023 = (2^53 - 1) * 2^971",
    "2^971 * (2^53 - 1 + 1/2) : the smallest number resolving to inf",
    "Near DBL_MIN",
    "DBL_MIN 2^-1022",
    "The behavior for parsing -nan depends on implementation.",
    "Thus we skip binary check for negative nan.",
    "See comment in test_cases.",
    "Constants",
    "Start with some content:",
    "Clear & re-use:",
    "Output should match new content:",
    "Number of trials for each new ChunkedArray configuration. Pass 100 times over the search space:",
    "Each outer loop iteration changes the test by adding +1 chunk. We start with 1 chunk only:",
    "Sweep valid and invalid addresses with a ChunkedArray with `chunks` chunks:",
    "Compute a new trial address & value & if it is a valid address:",
    "Insert item. If at a valid address, 0 is returned, otherwise, -1 is returned:",
    "If at valid address, check that the stored value is correct & remember it for the future:",
    "Check the just-stored value with getitem():",
    "Also store the just-stored value for future tracking:",
    "Final check: ensure even with overrides, all valid insertions store the latest value at that address:",
    "Test in 2 ways that the values are correctly laid out in memory:",
    "coding: utf-8",
    "we don't need lib_lightgbm while building docs",
    "coding: utf-8",
    "check saved model persistence",
    "we need to check the consistency of model file here, so test for exact equal",
    "check early stopping is working. Make it stop very early, so the scores should be very close to zero",
    "scores likely to be different, but prediction should still be the same",
    "test that shape is checked during prediction",
    "The simple implementation is just a single \"return self.ndarray[idx]\"",
    "The following is for demo and testing purpose.",
    "whole col",
    "half col",
    "Create dataset from numpy array directly.",
    "Create dataset using Sequence.",
    "Test for validation set.",
    "Select some random rows as valid data.",
    "From Dataset constructor, with dataset from numpy array.",
    "From Dataset.create_valid, with dataset from sequence.",
    "test that method works even with free_raw_data=True",
    "test that method works but sets raw data to None in case of immergeable data types",
    "test that method works for different data types",
    "Set extremely harsh penalties, so CEGB will block most splits.",
    "Compare pairs of penalties, to ensure scaling works as intended",
    "Reset booster1's parameters to p2, so the parameter section of the file matches.",
    "should resolve duplicate aliases, and prefer the main parameter",
    "should choose a value from an alias and set that value on main param",
    "if only an alias is used",
    "should use the default if main param and aliases are missing",
    "all changes should be made on copies and not modify the original",
    "coding: utf-8",
    "add target, weight, and group to DataFrame so that partitions abide by group boundaries.",
    "set_index ensures partitions are based on group id.",
    "See https://stackoverflow.com/questions/49532824/dask-dataframe-split-partitions-based-on-a-column-or-function.",
    "separate target, weight from features.",
    "encode group identifiers into run-length encoding, the format LightGBMRanker is expecting",
    "so that within each partition, sum(g) = n_samples.",
    "ranking arrays: one chunk per group. Each chunk must include all columns.",
    "make one categorical feature relevant to the target",
    "https://github.com/microsoft/LightGBM/issues/4118",
    "extra predict() parameters should be passed through correctly",
    "pref_leaf values should have the right shape",
    "and values that look like valid tree nodes",
    "be sure LightGBM actually used at least one categorical column,",
    "and that it was correctly treated as a categorical feature",
    "shape depends on whether it is binary or multiclass classification",
    "in the special case of multi-class classification using scipy sparse matrices,",
    "the output of `.predict(..., pred_contrib=True)` is a list of sparse matrices (one per class)",
    "",
    "since that case is so different than all other cases, check the relevant things here",
    "and then return early",
    "raw scores will probably be different, but at least check that all predicted classes are the same",
    "be sure LightGBM actually used at least one categorical column,",
    "and that it was correctly treated as a categorical feature",
    "* shape depends on whether it is binary or multiclass classification",
    "* matrix for binary classification is of the form [feature_contrib, base_value],",
    "for multi-class it's [feat_contrib_class1, base_value_class1, feat_contrib_class2, base_value_class2, etc.]",
    "* contrib outputs for distributed training are different than from local training, so we can just test",
    "that the output has the right shape and base values are in the right position",
    "check that found ports are different for same address (LocalCluster)",
    "check that the ports are indeed open",
    "Scores should be the same",
    "Predictions should be roughly the same.",
    "pref_leaf values should have the right shape",
    "and values that look like valid tree nodes",
    "extra predict() parameters should be passed through correctly",
    "be sure LightGBM actually used at least one categorical column,",
    "and that it was correctly treated as a categorical feature",
    "contrib outputs for distributed training are different than from local training, so we can just test",
    "that the output has the right shape and base values are in the right position",
    "be sure LightGBM actually used at least one categorical column,",
    "and that it was correctly treated as a categorical feature",
    "Quantiles should be right",
    "be sure LightGBM actually used at least one categorical column,",
    "and that it was correctly treated as a categorical feature",
    "rebalance small dask.Array dataset for better performance.",
    "use many trees + leaves to overfit, help ensure that Dask data-parallel strategy matches that of",
    "serial learner. See https://github.com/microsoft/LightGBM/issues/3292#issuecomment-671288210.",
    "distributed ranker should be able to rank decently well and should",
    "have high rank correlation with scores from serial ranker.",
    "extra predict() parameters should be passed through correctly",
    "pref_leaf values should have the right shape",
    "and values that look like valid tree nodes",
    "be sure LightGBM actually used at least one categorical column,",
    "and that it was correctly treated as a categorical feature",
    "Use larger trainset to prevent premature stopping due to zero loss, causing num_trees() < n_estimators.",
    "Use small chunk_size to avoid single-worker allocation of eval data partitions.",
    "test eval_class_weight, eval_init_score on binary-classification task.",
    "Note: objective's default `metric` will be evaluated in evals_result_ in addition to all eval_metrics.",
    "create eval_sets by creating new datasets or copying training data.",
    "total number of trees scales up for ova classifier.",
    "check that early stopping was not applied.",
    "checks that evals_result_ and best_score_ contain expected data and eval_set names.",
    "check that each eval_name and metric exists for all eval sets, allowing for the",
    "case when a worker receives a fully-padded eval_set component which is not evaluated.",
    "should be able to use the class without specifying a client",
    "should be able to set client after construction",
    "data on cluster1",
    "create identical data on cluster2",
    "at this point, the result of default_client() is client2 since it was the most recently",
    "created. So setting client to client1 here to test that you can select a non-default client",
    "unfitted model should survive pickling round trip, and pickling",
    "shouldn't have side effects on the model object",
    "client will always be None after unpickling",
    "fitted model should survive pickling round trip, and pickling",
    "shouldn't have side effects on the model object",
    "client will always be None after unpickling",
    "rebalance data to be sure that each worker has a piece of the data",
    "model 1 - no network parameters given",
    "model 2 - machines given",
    "model 3 - local_listen_port given",
    "training should fail because LightGBM will try to use the same",
    "port for multiple worker processes on the same machine",
    "rebalance data to be sure that each worker has a piece of the data",
    "test that \"machines\" is actually respected by creating a socket that uses",
    "one of the ports mentioned in \"machines\"",
    "The above error leaves a worker waiting",
    "an informative error should be raised if \"machines\" has duplicates",
    "\"client\" should be the only different, and the final argument",
    "value of the root node is 0 when init_score is set",
    "this test is separate because it takes a not-yet-constructed estimator",
    "coding: utf-8",
    "coding: utf-8",
    "build target, group ID vectors.",
    "build y/target and group-id vectors with user-specified group sizes.",
    "build y/target and group-id vectors according to n_samples, avg_gs, and random_gs.",
    "groups should contain > 1 element for pairwise learning objective.",
    "build feature data, X. Transform first few into informative features.",
    "coding: utf-8",
    "prediction result is actually not transformed (is raw) due to custom objective",
    "sklearn <0.23 does not have a stacking classifier and n_features_in_ property",
    "sklearn <0.23 does not have a stacking regressor and n_features_in_ property",
    "sklearn < 0.22 does not have the post fit attribute: classes_",
    "sklearn < 0.23 does not have as_frame parameter",
    "sklearn < 0.22 does not have the post fit attribute: classes_",
    "sklearn < 0.23 does not have as_frame parameter",
    "Test if random_state is properly stored",
    "Test if two random states produce identical models",
    "Test if subsequent fits sample from random_state object and produce different models",
    "Test that the largest element is NOT the same, the smallest can be the same, i.e. zero",
    "With default params",
    "Tests same probabilities",
    "Tests same predictions",
    "Tests same raw scores",
    "Tests same leaf indices",
    "Tests same feature contributions",
    "Tests other parameters for the prediction works",
    "Tests start_iteration",
    "Tests same probabilities, starting from iteration 10",
    "Tests same predictions, starting from iteration 10",
    "Tests same raw scores, starting from iteration 10",
    "Tests same leaf indices, starting from iteration 10",
    "Tests same feature contributions, starting from iteration 10",
    "Tests other parameters for the prediction works, starting from iteration 10",
    "no custom objective, no custom metric",
    "default metric",
    "non-default metric",
    "no metric",
    "non-default metric in eval_metric",
    "non-default metric with non-default metric in eval_metric",
    "non-default metric with multiple metrics in eval_metric",
    "non-default metric with multiple metrics in eval_metric for LGBMClassifier",
    "default metric for non-default objective",
    "non-default metric for non-default objective",
    "no metric",
    "non-default metric in eval_metric for non-default objective",
    "non-default metric with non-default metric in eval_metric for non-default objective",
    "non-default metric with multiple metrics in eval_metric for non-default objective",
    "custom objective, no custom metric",
    "default regression metric for custom objective",
    "non-default regression metric for custom objective",
    "multiple regression metrics for custom objective",
    "no metric",
    "default regression metric with non-default metric in eval_metric for custom objective",
    "non-default regression metric with metric in eval_metric for custom objective",
    "multiple regression metrics with metric in eval_metric for custom objective",
    "multiple regression metrics with multiple metrics in eval_metric for custom objective",
    "no custom objective, custom metric",
    "default metric with custom metric",
    "non-default metric with custom metric",
    "multiple metrics with custom metric",
    "custom metric (disable default metric)",
    "default metric for non-default objective with custom metric",
    "non-default metric for non-default objective with custom metric",
    "multiple metrics for non-default objective with custom metric",
    "custom metric (disable default metric for non-default objective)",
    "custom objective, custom metric",
    "custom metric for custom objective",
    "non-default regression metric with custom metric for custom objective",
    "multiple regression metrics with custom metric for custom objective",
    "default metric and invalid binary metric is replaced with multiclass alternative",
    "invalid objective is replaced with default multiclass one",
    "and invalid binary metric is replaced with multiclass alternative",
    "default metric for non-default multiclass objective",
    "and invalid binary metric is replaced with multiclass alternative",
    "default metric and invalid multiclass metric is replaced with binary alternative",
    "invalid multiclass metric is replaced with binary alternative for custom objective",
    "Verify that can receive a list of metrics, only callable",
    "Verify that can receive a list of custom and built-in metrics",
    "Verify that works as expected when eval_metric is empty",
    "Verify that can receive a list of metrics, only built-in",
    "Verify that eval_metric is robust to receiving a list with None",
    "training data as eval_set",
    "feval",
    "single eval_set",
    "two eval_set",
    "sklearn < 0.22 requires passing \"attributes\" argument",
    "Test that estimators are default-constructible",
    "coding: utf-8",
    "coding: utf-8",
    "check that default gives same result as k = 1",
    "check against independent calculation for k = 1",
    "check against independent calculation for k = 2",
    "check against independent calculation for k = 10",
    "check cases where predictions are equal",
    "should give same result as binary auc for 2 classes",
    "test the case where all predictions are equal",
    "test that weighted data gives different auc_mu",
    "test that equal data weights give same auc_mu as unweighted data",
    "should give 1 when accuracy = 1",
    "test loading class weights",
    "no early stopping",
    "early stopping occurs",
    "test custom eval metrics",
    "shuffle = False, override metric in params",
    "shuffle = True, callbacks",
    "enable display training loss",
    "self defined folds",
    "LambdaRank",
    "... with l2 metric",
    "... with NDCG (default) metric",
    "self defined folds with lambdarank",
    "with early stopping",
    "predict by each fold booster",
    "fold averaging",
    "without early stopping",
    "test feature_names with whitespaces",
    "This has non-ascii strings.",
    "take subsets and train",
    "generate CSR sparse dataset",
    "convert data to dense and get back same contribs",
    "validate the values are the same",
    "validate using CSC matrix",
    "validate the values are the same",
    "generate CSR sparse dataset",
    "convert data to dense and get back same contribs",
    "validate the values are the same",
    "validate using CSC matrix",
    "validate the values are the same",
    "Note there is an extra column added to the output for the expected value",
    "Note output CSC shape should be same as CSR output shape",
    "test sliced labels",
    "append some columns",
    "append some rows",
    "test sliced 2d matrix",
    "test sliced CSR",
    "trees start at position 1.",
    "split_features are in 4th line.",
    "test if a penalty as high as the depth indeed prohibits all monotone splits",
    "The penalization is so high that the first 2 features should not be used here",
    "Check that a very high penalization is the same as not using the features at all",
    "no fobj, no feval",
    "default metric",
    "non-default metric in params",
    "default metric in args",
    "non-default metric in args",
    "metric in args overwrites one in params",
    "multiple metrics in params",
    "multiple metrics in args",
    "remove default metric by 'None' in list",
    "remove default metric by 'None' aliases",
    "fobj, no feval",
    "no default metric",
    "metric in params",
    "metric in args",
    "metric in args overwrites its' alias in params",
    "multiple metrics in params",
    "multiple metrics in args",
    "no fobj, feval",
    "default metric with custom one",
    "non-default metric in params with custom one",
    "default metric in args with custom one",
    "non-default metric in args with custom one",
    "metric in args overwrites one in params, custom one is evaluated too",
    "multiple metrics in params with custom one",
    "multiple metrics in args with custom one",
    "custom metric is evaluated despite 'None' is passed",
    "fobj, feval",
    "no default metric, only custom one",
    "metric in params with custom one",
    "metric in args with custom one",
    "metric in args overwrites one in params, custom one is evaluated too",
    "multiple metrics in params with custom one",
    "multiple metrics in args with custom one",
    "custom metric is evaluated despite 'None' is passed",
    "no fobj, no feval",
    "default metric",
    "default metric in params",
    "non-default metric in params",
    "multiple metrics in params",
    "remove default metric by 'None' aliases",
    "fobj, no feval",
    "no default metric",
    "metric in params",
    "multiple metrics in params",
    "no fobj, feval",
    "default metric with custom one",
    "default metric in params with custom one",
    "non-default metric in params with custom one",
    "multiple metrics in params with custom one",
    "custom metric is evaluated despite 'None' is passed",
    "fobj, feval",
    "no default metric, only custom one",
    "metric in params with custom one",
    "multiple metrics in params with custom one",
    "custom metric is evaluated despite 'None' is passed",
    "multiclass default metric",
    "multiclass default metric with custom one",
    "multiclass metric alias with custom one for custom objective",
    "no metric for invalid class_num",
    "custom metric for invalid class_num",
    "multiclass metric alias with custom one with invalid class_num",
    "multiclass default metric without num_class",
    "multiclass metric alias",
    "multiclass metric",
    "non-valid metric for multiclass objective",
    "non-default num_class for default objective",
    "no metric with non-default num_class for custom objective",
    "multiclass metric alias for custom objective",
    "multiclass metric for custom objective",
    "binary metric with non-default num_class for custom objective",
    "Expect three metrics but mean and stdv for each metric",
    "test XGBoost-style return value",
    "test numpy-style return value",
    "test bins string type",
    "test histogram is disabled for categorical features",
    "test for lgb.train",
    "test feval for lgb.train",
    "test with two valid data for lgb.train",
    "test for lgb.cv",
    "test feval for lgb.cv",
    "test that binning works properly for features with only positive or only negative values",
    "decreasing without freeing raw data is allowed",
    "decreasing before lazy init is allowed",
    "increasing is allowed",
    "decreasing with disabled filter is allowed",
    "decreasing with enabled filter is disallowed;",
    "also changes of other params are disallowed",
    "check extra trees increases regularization",
    "check path smoothing increases regularization",
    "test edge case with one leaf",
    "check that constraint containing all features is equivalent to no constraint",
    "check that constraint partitioning the features reduces train accuracy",
    "check that constraints consisting of single features reduce accuracy further",
    "test that interaction constraints work when not all features are used",
    "check that setting linear_tree=True fits better than ordinary trees when data has linear relationship",
    "test again with nans in data",
    "test again with bagging",
    "test with a feature that has only one non-nan value",
    "test with a categorical feature",
    "test refit: same results on same data",
    "test refit with save and load",
    "test refit: different results training on different data",
    "test when num_leaves - 1 < num_features and when num_leaves - 1 > num_features",
    "test that the predict once with all iterations equals summed results with start_iteration and num_iteration",
    "test the case where start_iteration <= 0, and num_iteration is None",
    "test the case where start_iteration > 0, and num_iteration <= 0",
    "test the case where start_iteration > 0, and num_iteration <= 0, with pred_leaf=True",
    "test the case where start_iteration > 0, and num_iteration <= 0, with pred_contrib=True",
    "test for regression",
    "test both with and without early stopping",
    "test for multi-class",
    "test both with and without early stopping",
    "test for binary",
    "test both with and without early stopping",
    "test against sklearn average precision metric",
    "test that average precision is 1 where model predicts perfectly",
    "coding: utf-8",
    "If compiled appropriately, the same installation will support both GPU and CPU.",
    "coding: utf-8",
    "coding: utf-8",
    "These are helper functions to allow doing a stack unwind",
    "after an R allocation error, which would trigger a long jump.",
    "convert from one-based to zero-based index",
    "if any feature names were larger than allocated size,",
    "allow for a larger size and try again",
    "convert from boundaries to size",
    "--- start Booster interfaces",
    "if any eval names were larger than allocated size,",
    "allow for a larger size and try again",
    "if the model string was larger than the initial buffer, allocate a bigger buffer and try again",
    "if the model string was larger than the initial buffer, allocate a bigger buffer and try again",
    ".Call() calls",
    "coding: utf-8",
    "alias table",
    "names",
    "from strings",
    "tails",
    "tails",
    "coding: utf-8",
    "Single row predictor to abstract away caching logic",
    "create boosting",
    "initialize the boosting",
    "create objective function",
    "initialize the objective function",
    "create training metric",
    "reset the boosting",
    "create objective function",
    "initialize the objective function",
    "calculate the nonzero data and indices size",
    "allocate data and indices arrays",
    "Get the number of trees per iteration (for multiclass scenario we output multiple sparse matrices)",
    "aggregated per row feature contribution results",
    "keep track of the row_vector sizes for parallelization",
    "copy vector results to output for each row",
    "Get the number of trees per iteration (for multiclass scenario we output multiple sparse matrices)",
    "aggregated per row feature contribution results",
    "calculate number of elements per column to construct",
    "the CSC matrix with random access",
    "keep track of column counts",
    "keep track of beginning index for each column",
    "keep track of beginning index for each matrix",
    "Note: we parallelize across matrices instead of rows because of the column_counts[m][col_idx] increment inside the loop",
    "store the row index",
    "update column count",
    "explicitly declare symbols from LightGBM namespace",
    "some help functions used to convert data",
    "Row iterator of on column for CSC matrix",
    "return value at idx, only can access by ascent order",
    "return next non-zero pair, if index < 0, means no more data",
    "start of c_api functions",
    "This API is to keep python binding's behavior the same with C++ implementation.",
    "Sample count, random seed etc. should be provided in parameters.",
    "sample data first",
    "sample data first",
    "sample data first",
    "local buffer to re-use memory",
    "sample data first",
    "no more data",
    "---- start of booster",
    "Single row in row-major format:",
    "---- start of some help functions",
    "data is array of pointers to individual rows",
    "set number of threads for openmp",
    "check for alias",
    "read parameters from config file",
    "remove str after \"#\"",
    "check for alias again",
    "load configs",
    "prediction is needed if using input initial model(continued train)",
    "need to continue training",
    "sync up random seed for data partition",
    "load Training data",
    "load data for distributed training",
    "load data for single machine",
    "need save binary file",
    "create training metric",
    "only when have metrics then need to construct validation data",
    "Add validation data, if it exists",
    "add",
    "need save binary file",
    "add metric for validation data",
    "output used time on each iteration",
    "need init network",
    "create boosting",
    "create objective function",
    "load training data",
    "initialize the objective function",
    "initialize the boosting",
    "add validation data into boosting",
    "convert model to if-else statement code",
    "create predictor",
    "Free memory",
    "create predictor",
    "label_gain = 2^i - 1, may overflow, so we use 31 here",
    "counts for all labels",
    "start from top label, and accumulate DCG",
    "counts for all labels",
    "calculate k Max DCG by one pass",
    "get sorted indices by score",
    "calculate multi dcg by one pass",
    "wait for all client start up",
    "Don't call MPI_Finalize() here: If the destructor was called because only this node had an exception, calling MPI_Finalize() will cause all nodes to hang.",
    "Instead we will handle finalize/abort for MPI in main().",
    "default set to -1",
    "distance at k-th communication, distance[k] = 2^k",
    "set incoming rank at k-th commuication",
    "set outgoing rank at k-th commuication",
    "default set as -1",
    "construct all recursive halving map for all machines",
    "let 1 << k <= num_machines",
    "distance of each communication",
    "if num_machines = 2^k, don't need to group machines",
    "communication direction, %2 == 0 is positive",
    "neighbor at k-th communication",
    "receive data block at k-th communication",
    "send data block at k-th communication",
    "if num_machines != 2^k, need to group machines",
    "group, two machine in one group, total \"rest\" groups will have 2 machines.",
    "let left machine as group leader",
    "cache block information for groups, group with 2 machines will have double block size",
    "convert from group to node leader",
    "convert from node to group",
    "meet new group",
    "add block len for this group",
    "calculate the group block start",
    "not need to construct",
    "get receive block information",
    "accumulate block len",
    "get send block information",
    "accumulate block len",
    "static member definition",
    "if small package or small count , do it by all gather.(reduce the communication times.)",
    "assign the blocks to every rank.",
    "do reduce scatter",
    "do all gather",
    "assign blocks",
    "need use buffer here, since size of \"output\" is smaller than size after all gather",
    "copy back",
    "assign blocks",
    "start all gather",
    "when num_machines is small and data is large",
    "use output as receive buffer",
    "get current local block size",
    "get out rank",
    "get in rank",
    "get send information",
    "get recv information",
    "send and recv at same time",
    "rotate in-place",
    "use output as receive buffer",
    "get current local block size",
    "get send information",
    "get recv information",
    "send and recv at same time",
    "use output as receive buffer",
    "send and recv at same time",
    "send local data to neighbor first",
    "receive neighbor data first",
    "reduce",
    "get target",
    "get send information",
    "get recv information",
    "send and recv at same time",
    "reduce",
    "send result to neighbor",
    "receive result from neighbor",
    "copy result",
    "start up socket",
    "parse clients from file",
    "get ip list of local machine",
    "get local rank",
    "construct listener",
    "construct communication topo",
    "construct linkers",
    "free listener",
    "set timeout",
    "accept incoming socket",
    "receive rank",
    "add new socket",
    "save ranks that need to connect with",
    "start listener",
    "start connect",
    "let smaller rank connect to larger rank",
    "send local rank",
    "wait for listener",
    "print connected linkers",
    "only need to copy subset",
    "avoid to copy subset many times",
    "avoid out of range",
    "may need to recopy subset",
    "valid the type",
    "Constructors",
    "Get type tag",
    "Comparisons",
    "This has to be separate, not in Statics, because Json() accesses",
    "statics().null.",
    "advance until next line, or end of input",
    "advance until closing tokens",
    "The usual case: non-escaped characters",
    "Handle escapes",
    "Extract 4-byte escape sequence",
    "Explicitly check length of the substring. The following loop",
    "relies on std::string returning the terminating NUL when",
    "accessing str[length]. Checking here reduces brittleness.",
    "JSON specifies that characters outside the BMP shall be encoded as a",
    "pair of 4-hex-digit \\u escapes encoding their surrogate pair",
    "components. Check whether we're in the middle of such a beast: the",
    "previous codepoint was an escaped lead (high) surrogate, and this is",
    "a trail (low) surrogate.",
    "Reassemble the two surrogate pairs into one astral-plane character,",
    "per the UTF-16 algorithm.",
    "Integer part",
    "Decimal part",
    "Exponent part",
    "Check for any trailing garbage",
    "Documented in json11.hpp",
    "Check for another object",
    "get column names",
    "load label idx first",
    "erase label column name",
    "load ignore columns",
    "load weight idx",
    "load group idx",
    "don't support query id in data file when using distributed training",
    "read data to memory",
    "sample data",
    "construct feature bin mappers",
    "initialize label",
    "extract features",
    "sample data from file",
    "construct feature bin mappers",
    "initialize label",
    "extract features",
    "load data from binary file",
    "check meta data",
    "need to check training data",
    "read data in memory",
    "initialize label",
    "extract features",
    "Get number of lines of data file",
    "initialize label",
    "extract features",
    "load data from binary file",
    "not need to check validation data",
    "check meta data",
    "buffer to read binary file",
    "check token",
    "read size of header",
    "re-allocmate space if not enough",
    "read header",
    "get header",
    "num_groups",
    "real_feature_idx_",
    "feature2group",
    "feature2subfeature",
    "group_bin_boundaries",
    "group_feature_start_",
    "group_feature_cnt_",
    "get feature names",
    "write feature names",
    "get forced_bin_bounds_",
    "read size of meta data",
    "re-allocate space if not enough",
    "read meta data",
    "load meta data",
    "sample local used data if need to partition",
    "if not contain query file, minimal sample unit is one record",
    "if contain query file, minimal sample unit is one query",
    "if is new query",
    "read feature data",
    "read feature size",
    "re-allocate space if not enough",
    "raw data",
    "fill feature_names_ if not header",
    "get forced split",
    "if only one machine, find bin locally",
    "if have multi-machines, need to find bin distributed",
    "different machines will find bin for different features",
    "start and len will store the process feature indices for different machines",
    "machine i will find bins for features in [ start[i], start[i] + len[i] )",
    "free",
    "gather global feature bin mappers",
    "restore features bins from buffer",
    "---- private functions ----",
    "if features are ordered, not need to use hist_buf",
    "read all lines",
    "get query data",
    "if not contain query data, minimal sample unit is one record",
    "if contain query data, minimal sample unit is one query",
    "if is new query",
    "get query data",
    "if not contain query file, minimal sample unit is one record",
    "if contain query file, minimal sample unit is one query",
    "if is new query",
    "parse features",
    "get forced split",
    "check the range of label_idx, weight_idx and group_idx",
    "fill feature_names_ if not header",
    "start find bins",
    "if only one machine, find bin locally",
    "start and len will store the process feature indices for different machines",
    "machine i will find bins for features in [ start[i], start[i] + len[i] )",
    "free",
    "gather global feature bin mappers",
    "restore features bins from buffer",
    "if doesn't need to prediction with initial model",
    "parser",
    "set label",
    "free processed line:",
    "shrink_to_fit will be very slow in linux, and seems not free memory, disable for now",
    "text_reader_->Lines()[i].shrink_to_fit();",
    "push data",
    "if is used feature",
    "if need to prediction with initial model",
    "parser",
    "set initial score",
    "set label",
    "free processed line:",
    "shrink_to_fit will be very slow in Linux, and seems not free memory, disable for now",
    "text_reader_->Lines()[i].shrink_to_fit();",
    "push data",
    "if is used feature",
    "metadata_ will manage space of init_score",
    "text data can be free after loaded feature values",
    "parser",
    "set initial score",
    "set label",
    "push data",
    "if is used feature",
    "only need part of data",
    "need full data",
    "metadata_ will manage space of init_score",
    "read size of token",
    "remove duplicates",
    "deep copy function for BinMapper",
    "mean size for one bin",
    "need a new bin",
    "update bin upper bound",
    "last bin upper bound",
    "get list of distinct values",
    "get number of positive and negative distinct values",
    "include zero bounds and infinity bound",
    "add forced bounds, excluding zeros since we have already added zero bounds",
    "find remaining bounds",
    "find distinct_values first",
    "push zero in the front",
    "use the large value",
    "push zero in the back",
    "convert to int type first",
    "sort by counts",
    "will ignore the categorical of small counts",
    "Push the dummy bin for NaN",
    "Use MissingType::None to represent this bin contains all categoricals",
    "fix count of NaN bin",
    "check trivial(num_bin_ == 1) feature",
    "check useless bin",
    "When most_freq_bin_ != default_bin_, there are some additional data loading costs.",
    "so use most_freq_bin_  = default_bin_ when there is not so sparse",
    "calculate max bin of all features to select the int type in MultiValDenseBin",
    "for lambdarank, it needs query data for partition data in distributed learning",
    "need convert query_id to boundaries",
    "check weights",
    "check query boundries",
    "contain initial score file",
    "check weights",
    "get local weights",
    "check query boundries",
    "get local query boundaries",
    "contain initial score file",
    "get local initial scores",
    "re-load query weight",
    "save to nullptr",
    "save to nullptr",
    "save to nullptr",
    "default weight file name",
    "default init_score file name",
    "use first line to count number class",
    "default query file name",
    "root is in the depth 0",
    "non-leaf",
    "leaf",
    "use this for the missing value conversion",
    "Predict func by Map to ifelse",
    "use this for the missing value conversion",
    "non-leaf",
    "left subtree",
    "right subtree",
    "leaf",
    "non-leaf",
    "left subtree",
    "right subtree",
    "leaf",
    "recursive computation of SHAP values for a decision tree",
    "extend the unique path",
    "leaf node",
    "internal node",
    "see if we have already split on this feature,",
    "if so we undo that split so we can redo it for this node",
    "recursive sparse computation of SHAP values for a decision tree",
    "extend the unique path",
    "leaf node",
    "internal node",
    "see if we have already split on this feature,",
    "if so we undo that split so we can redo it for this node",
    "add names of objective function if not providing metric",
    "equal weights for all classes",
    "generate seeds by seed.",
    "sort eval_at",
    "Only push the non-training data",
    "check for conflicts",
    "check if objective, metric, and num_class match",
    "Change pool size to -1 (no limit) when using data parallel to reduce communication costs",
    "Check max_depth and num_leaves",
    "Fits in an int, and is more restrictive than the current num_leaves",
    "force col-wise for gpu & CUDA",
    "force gpu_use_dp for CUDA",
    "linear tree learner must be serial type and run on CPU device",
    "min_data_in_leaf must be at least 2 if path smoothing is active. This is because when the split is calculated",
    "the count is calculated using the proportion of hessian in the leaf which is rounded up to nearest int, so it can",
    "be 1 when there is actually no data in the leaf. In rare cases this can cause a bug because with path smoothing the",
    "calculated split gain can be positive even with zero gradient and hessian.",
    "In distributed mode, local node doesn't have histograms on all features, cannot perform \"intermediate\" monotone constraints.",
    "\"intermediate\" monotone constraints need to recompute splits. If the features are sampled when computing the",
    "split initially, then the sampling needs to be recorded or done once again, which is currently not supported",
    "first round: fill the single val group",
    "always push the last group",
    "put dense feature first",
    "sort by non zero cnt",
    "sort by non zero cnt, bigger first",
    "shuffle groups",
    "Using std::swap for vector<bool> will cause the wrong result.",
    "get num_features",
    "get bin_mappers",
    "for sparse multi value bin, we store the feature bin values with offset added",
    "for dense multi value bin, the feature bin values without offsets are used",
    "copy feature bin mapper data",
    "copy feature bin mapper data",
    "if not pass a filename, just append \".bin\" of original file",
    "get size of header",
    "size of feature names",
    "size of forced bins",
    "write header",
    "write feature names",
    "write forced bins",
    "get size of meta data",
    "write meta data",
    "write feature data",
    "get size of feature",
    "write feature",
    "write raw data; use row-major order so we can read row-by-row",
    "explicitly initialize template methods, for cross module call",
    "Only one multi-val group, just simply merge",
    "Skip the leading 0 when copying group_bin_boundaries.",
    "regenerate other fields",
    "store the importance first",
    "PredictRaw",
    "PredictRawByMap",
    "Predict",
    "PredictByMap",
    "PredictLeafIndex",
    "PredictLeafIndexByMap",
    "output model type",
    "output number of class",
    "output label index",
    "output max_feature_idx",
    "output objective",
    "output tree models",
    "store the importance first",
    "sort the importance",
    "use serialized string to restore this object",
    "Use first 128 chars to avoid exceed the message buffer.",
    "get number of classes",
    "get index of label",
    "get max_feature_idx first",
    "get average_output",
    "get feature names",
    "get monotone_constraints",
    "set zero",
    "predict all the trees for one iteration",
    "check early stopping",
    "set zero",
    "predict all the trees for one iteration",
    "check early stopping",
    "margin_threshold will be captured by value",
    "copy and sort",
    "margin_threshold will be captured by value",
    "Fix for compiler warnings about reaching end of control",
    "load forced_splits file",
    "init tree learner",
    "push training metrics",
    "create buffer for gradients and Hessians",
    "get max feature index",
    "get label index",
    "get feature names",
    "if need bagging, create buffer",
    "for a validation dataset, we need its score and metric",
    "update score",
    "objective function will calculate gradients and hessians",
    "random bagging, minimal unit is one record",
    "random bagging, minimal unit is one record",
    "if need bagging",
    "set bagging data to tree learner",
    "get subset",
    "output used time per iteration",
    "boosting from average label; or customized \"average\" if implemented for the current objective",
    "boosting first",
    "bagging logic",
    "need to copy gradients for bagging subset.",
    "shrinkage by learning rate",
    "update score",
    "only add default score one-time",
    "updates scores",
    "add model",
    "reset score",
    "remove model",
    "print message for metric",
    "pop last early_stopping_round_ models",
    "update training score",
    "we need to predict out-of-bag scores of data for boosting",
    "update validation score",
    "print training metric",
    "print validation metric",
    "set zero",
    "predict all the trees for one iteration",
    "predict all the trees for one iteration",
    "push training metrics",
    "not same training data, need reset score and others",
    "create score tracker",
    "update score",
    "create buffer for gradients and hessians",
    "load forced_splits file",
    "if need bagging, create buffer",
    "Get the max size of pool",
    "at least need 2 leaves",
    "push split information for all leaves",
    "initialize splits for leaf",
    "initialize data partition",
    "initialize ordered gradients and hessians",
    "cannot change is_hist_col_wise during training",
    "initialize splits for leaf",
    "initialize data partition",
    "initialize ordered gradients and hessians",
    "Get the max size of pool",
    "at least need 2 leaves",
    "push split information for all leaves",
    "some initial works before training",
    "root leaf",
    "only root leaf can be splitted on first time",
    "some initial works before finding best split",
    "find best threshold for every feature",
    "Get a leaf with max split gain",
    "Get split information for best leaf",
    "cannot split, quit",
    "split tree with best leaf",
    "reset histogram pool",
    "initialize data partition",
    "reset the splits for leaves",
    "Sumup for root",
    "use all data",
    "use bagging, only use part of data",
    "check depth of current leaf",
    "only need to check left leaf, since right leaf is in same level of left leaf",
    "no enough data to continue",
    "only have root",
    "put parent(left) leaf's histograms into larger leaf's histograms",
    "put parent(left) leaf's histograms to larger leaf's histograms",
    "construct smaller leaf",
    "construct larger leaf",
    "find splits",
    "only has root leaf",
    "start at root leaf",
    "before processing next node from queue, store info for current left/right leaf",
    "store \"best split\" for left and right, even if they might be overwritten by forced split",
    "then, compute own splits",
    "split info should exist because searching in bfs fashion - should have added from parent",
    "update before tree split",
    "don't need to update this in data-based parallel model",
    "split tree, will return right leaf",
    "store the true split gain in tree model",
    "don't need to update this in data-based parallel model",
    "store the true split gain in tree model",
    "init the leaves that used on next iteration",
    "update leave outputs if needed",
    "bag_mapper[index_mapper[i]]",
    "it is needed to filter the features after the above code.",
    "Otherwise, the `is_splittable` in `FeatureHistogram` will be wrong, and cause some features being accidentally filtered in the later nodes.",
    "for root leaf the \"parent\" output is its own output because we don't apply any smoothing to the root",
    "can't use GetParentOutput because leaf_splits doesn't have weight property set",
    "find splits",
    "identify features containing nans",
    "preallocate the matrix used to calculate linear model coefficients",
    "store only upper triangular half of matrix as an array, in row-major order",
    "this requires (max_num_feat + 1) * (max_num_feat + 2) / 2 entries (including the constant terms of the regression)",
    "we add another 8 to ensure cache lines are not shared among processors",
    "some initial works before training",
    "root leaf",
    "only root leaf can be splitted on first time",
    "some initial works before finding best split",
    "find best threshold for every feature",
    "Get a leaf with max split gain",
    "Get split information for best leaf",
    "cannot split, quit",
    "split tree with best leaf",
    "map data to leaf number",
    "calculate coefficients using the method described in Eq 3 of https://arxiv.org/pdf/1802.05640.pdf",
    "the coefficients vector is given by",
    "- (X_T * H * X + lambda) ^ (-1) * (X_T * g)",
    "where:",
    "X is the matrix where the first column is the feature values and the second is all ones,",
    "H is the diagonal matrix of the hessian,",
    "lambda is the diagonal matrix with diagonal entries equal to the regularisation term linear_lambda",
    "g is the vector of gradients",
    "the subscript _T denotes the transpose",
    "create array of pointers to raw data, and coefficient matrices, for each leaf",
    "clear the coefficient matrices",
    "aggregate results from different threads",
    "copy into eigen matrices and solve",
    "update the tree properties",
    "need to be able to hold smaller and larger best splits in SyncUpGlobalBestSplit",
    "get feature partition",
    "get local used features",
    "get best split at smaller leaf",
    "find local best split for larger leaf",
    "sync global best info",
    "update best split",
    "instantiate template classes, otherwise linker cannot find the code",
    "initialize SerialTreeLearner",
    "Get local rank and global machine size",
    "need to be able to hold smaller and larger best splits in SyncUpGlobalBestSplit",
    "allocate buffer for communication",
    "generate feature partition for current tree",
    "get local used feature",
    "get block start and block len for reduce scatter",
    "get buffer_write_start_pos_",
    "get buffer_read_start_pos_",
    "sync global data sumup info",
    "global sumup reduce",
    "copy back",
    "set global sumup info",
    "init global data count in leaf",
    "clear histogram buffer before synchronizing",
    "otherwise histogram contents from the previous iteration will be sent",
    "construct local histograms",
    "copy to buffer",
    "Reduce scatter for histogram",
    "restore global histograms from buffer",
    "only root leaf",
    "construct histgroms for large leaf, we init larger leaf as the parent, so we can just subtract the smaller leaf's histograms",
    "find local best split for larger leaf",
    "sync global best info",
    "set best split",
    "need update global number of data in leaf",
    "instantiate template classes, otherwise linker cannot find the code",
    "initialize SerialTreeLearner",
    "some additional variables needed for GPU trainer",
    "Initialize GPU buffers and kernels",
    "some functions used for debugging the GPU histogram construction",
    "printf(\"grad %g != %g (%d ULPs)\\n\", GET_GRAD(h1, i), GET_GRAD(h2, i), ulps);",
    "goto err;",
    "we roughly want 256 workgroups per device, and we have num_dense_feature4_ feature tuples.",
    "also guarantee that there are at least 2K examples per workgroup",
    "return 0;",
    "we have already copied ordered gradients, ordered Hessians and indices to GPU",
    "decide the best number of workgroups working on one feature4 tuple",
    "set work group size based on feature size",
    "each 2^exp_workgroups_per_feature workgroups work on a feature4 tuple",
    "we need to refresh the kernel arguments after reallocating",
    "The only argument that needs to be changed later is num_data_",
    "the GPU kernel will process all features in one call, and each",
    "2^exp_workgroups_per_feature (compile time constant) workgroup will",
    "process one feature4 tuple",
    "for the root node, indices are not copied",
    "for constant hessian, hessians are not copied except for the root node",
    "there will be 2^exp_workgroups_per_feature = num_workgroups / num_dense_feature4 sub-histogram per feature4",
    "and we will launch num_feature workgroups for this kernel",
    "will launch threads for all features",
    "the queue should be asynchronous, and we will can WaitAndGetHistograms() before we start processing dense feature groups",
    "copy the results asynchronously. Size depends on if double precision is used",
    "we will wait for this object in WaitAndGetHistograms",
    "when the output is ready, the computation is done",
    "values of this feature has been redistributed to multiple bins; need a reduction here",
    "how many feature-group tuples we have",
    "leave some safe margin for prefetching",
    "256 work-items per workgroup. Each work-item prefetches one tuple for that feature",
    "clear sparse/dense maps",
    "do nothing if no features can be processed on GPU",
    "allocate memory for all features (FIXME: 4 GB barrier on some devices, need to split to multiple buffers)",
    "unpin old buffer if necessary before destructing them",
    "make ordered_gradients and Hessians larger (including extra room for prefetching), and pin them",
    "allocate space for gradients and Hessians on device",
    "we will copy gradients and Hessians in after ordered_gradients_ and ordered_hessians_ are constructed",
    "allocate feature mask, for disabling some feature-groups' histogram calculation",
    "copy indices to the device",
    "histogram bin entry size depends on the precision (single/double)",
    "create output buffer, each feature has a histogram with device_bin_size_ bins,",
    "each work group generates a sub-histogram of dword_features_ features.",
    "only initialize once here, as this will not need to change when ResetTrainingData() is called",
    "create atomic counters for inter-group coordination",
    "The output buffer is allocated to host directly, to overlap compute and data transfer",
    "find the dense feature-groups and group then into Feature4 data structure (several feature-groups packed into 4 bytes)",
    "looking for dword_features_ non-sparse feature-groups",
    "decide if we need to redistribute the bin",
    "multiplier must be a power of 2",
    "device_bin_mults_.push_back(1);",
    "found",
    "for data transfer time",
    "Now generate new data structure feature4, and copy data to the device",
    "preallocate arrays for all threads, and pin them",
    "building Feature4 bundles; each thread handles dword_features_ features",
    "one feature datapoint is 4 bits",
    "this guarantees that the RawGet() function is inlined, rather than using virtual function dispatching",
    "one feature datapoint is one byte",
    "this guarantees that the RawGet() function is inlined, rather than using virtual function dispatching",
    "Dense bin",
    "Dense 4-bit bin",
    "working on the remaining (less than dword_features_) feature groups",
    "fill the leftover features",
    "fill this empty feature with some \"random\" value",
    "fill this empty feature with some \"random\" value",
    "copying the last 1 to (dword_features - 1) feature-groups in the last tuple",
    "deallocate pinned space for feature copying",
    "data transfer time",
    "for other types of failure, build log might not be available; program.build_log() can crash",
    "Something bad happened. Just return \"No log available.\"",
    "build is okay, log may contain warnings",
    "destroy any old kernels",
    "create OpenCL kernels for different number of workgroups per feature",
    "currently we don't use constant memory",
    "compile the GPU kernel depending if double precision is used, constant hessian is used, etc.",
    "kernel with indices in an array",
    "kernel with all features enabled, with eliminated branches",
    "kernel with all data indices (for root node, and assumes that root node always uses all features)",
    "do nothing if no features can be processed on GPU",
    "The only argument that needs to be changed later is num_data_",
    "hessian is passed as a parameter, but it is not available now.",
    "hessian will be set in BeforeTrain()",
    "Get the max bin size, used for selecting best GPU kernel",
    "initialize GPU",
    "determine which kernel to use based on the max number of bins",
    "the +9 skips extra characters \")\", newline, \"#endif\" and newline at the beginning",
    "the +9 skips extra characters \")\", newline, \"#endif\" and newline at the beginning",
    "the +9 skips extra characters \")\", newline, \"#endif\" and newline at the beginning",
    "setup GPU kernel arguments after we allocating all the buffers",
    "GPU memory has to been reallocated because data may have been changed",
    "setup GPU kernel arguments after we allocating all the buffers",
    "Copy initial full hessians and gradients to GPU.",
    "We start copying as early as possible, instead of at ConstructHistogram().",
    "setup hessian parameters only",
    "hessian is passed as a parameter",
    "use bagging",
    "On GPU, we start copying indices, gradients and Hessians now, instead at ConstructHistogram()",
    "copy used gradients and Hessians to ordered buffer",
    "transfer the indices to GPU",
    "transfer hessian to GPU",
    "setup hessian parameters only",
    "hessian is passed as a parameter",
    "transfer gradients to GPU",
    "only have root",
    "Copy indices, gradients and Hessians as early as possible",
    "only need to initialize for smaller leaf",
    "Get leaf boundary",
    "copy indices to the GPU:",
    "copy ordered Hessians to the GPU:",
    "copy ordered gradients to the GPU:",
    "do nothing if no features can be processed on GPU",
    "copy data indices if it is not null",
    "generate and copy ordered_gradients if gradients is not null",
    "generate and copy ordered_hessians if Hessians is not null",
    "converted indices in is_feature_used to feature-group indices",
    "construct the feature masks for dense feature-groups",
    "if no feature group is used, just return and do not use GPU",
    "if not all feature groups are used, we need to transfer the feature mask to GPU",
    "otherwise, we will use a specialized GPU kernel with all feature groups enabled",
    "All data have been prepared, now run the GPU kernel",
    "construct smaller leaf",
    "ConstructGPUHistogramsAsync will return true if there are available feature groups dispatched to GPU",
    "then construct sparse features on CPU",
    "wait for GPU to finish, only if GPU is actually used",
    "use double precision",
    "use single precision",
    "Compare GPU histogram with CPU histogram, useful for debugging GPU code problem",
    "#define GPU_DEBUG_COMPARE",
    "construct larger leaf",
    "then construct sparse features on CPU",
    "wait for GPU to finish, only if GPU is actually used",
    "use double precision",
    "use single precision",
    "do some sanity check for the GPU algorithm",
    "limit top k",
    "get max bin",
    "calculate buffer size",
    "need to be able to hold smaller and larger best splits in SyncUpGlobalBestSplit",
    "left and right on same time, so need double size",
    "initialize histograms for global",
    "sync global data sumup info",
    "set global sumup info",
    "init global data count in leaf",
    "get local sumup",
    "get local sumup",
    "get mean number on machines",
    "weighted gain",
    "get top k",
    "Copy histogram to buffer, and Get local aggregate features",
    "copy histograms.",
    "copy smaller leaf histograms first",
    "mark local aggregated feature",
    "copy",
    "then copy larger leaf histograms",
    "mark local aggregated feature",
    "copy",
    "use local data to find local best splits",
    "clear histogram buffer before synchronizing",
    "otherwise histogram contents from the previous iteration will be sent",
    "find splits",
    "only has root leaf",
    "local voting",
    "gather",
    "get all top-k from all machines",
    "global voting",
    "copy local histgrams to buffer",
    "Reduce scatter for histogram",
    "find best split from local aggregated histograms",
    "restore from buffer",
    "restore from buffer",
    "find local best",
    "find local best split for larger leaf",
    "sync global best info",
    "copy back",
    "set the global number of data for leaves",
    "init the global sumup info",
    "instantiate template classes, otherwise linker cannot find the code",
    "launch cuda kernel",
    "initialize SerialTreeLearner",
    "some additional variables needed for GPU trainer",
    "Initialize GPU buffers and kernels: get device info",
    "some functions used for debugging the GPU histogram construction",
    "we roughly want 256 workgroups per device, and we have num_dense_feature4_ feature tuples.",
    "also guarantee that there are at least 2K examples per workgroup",
    "we have already copied ordered gradients, ordered hessians and indices to GPU",
    "decide the best number of workgroups working on one feature4 tuple",
    "set work group size based on feature size",
    "each 2^exp_workgroups_per_feature workgroups work on a feature4 tuple",
    "set thread_data",
    "copy the results asynchronously. Size depends on if double precision is used",
    "when the output is ready, the computation is done",
    "how many feature-group tuples we have",
    "leave some safe margin for prefetching",
    "256 work-items per workgroup. Each work-item prefetches one tuple for that feature",
    "clear sparse/dense maps",
    "do nothing it there is no dense feature",
    "calculate number of feature groups per gpu",
    "histogram bin entry size depends on the precision (single/double)",
    "allocate GPU memory for each GPU",
    "do nothing it there is no gpu feature",
    "allocate memory for all features",
    "allocate space for gradients and hessians on device",
    "we will copy gradients and hessians in after ordered_gradients_ and ordered_hessians_ are constructed",
    "copy indices to the device",
    "create output buffer, each feature has a histogram with device_bin_size_ bins,",
    "each work group generates a sub-histogram of dword_features_ features.",
    "only initialize once here, as this will not need to change when ResetTrainingData() is called",
    "create atomic counters for inter-group coordination",
    "The output buffer is allocated to host directly, to overlap compute and data transfer",
    "clear sparse/dense maps",
    "find the dense feature-groups and group then into Feature4 data structure (several feature-groups packed into 4 bytes)",
    "set device info",
    "looking for dword_features_ non-sparse feature-groups",
    "reset device info",
    "InitGPU w/ num_gpu",
    "Get the max bin size, used for selecting best GPU kernel",
    "get num_dense_feature_groups_",
    "initialize GPU",
    "set cpu threads",
    "resize device memory pointers",
    "create stream & events to handle multiple GPUs",
    "check data size",
    "GPU memory has to been reallocated because data may have been changed",
    "AllocateGPUMemory only when the number of data increased",
    "setup GPU kernel arguments after we allocating all the buffers",
    "Copy initial full hessians and gradients to GPU.",
    "We start copying as early as possible, instead of at ConstructHistogram().",
    "use bagging",
    "On GPU, we start copying indices, gradients and hessians now, instead at ConstructHistogram()",
    "copy used gradients and hessians to ordered buffer",
    "transfer the indices to GPU",
    "only have root",
    "Copy indices, gradients and hessians as early as possible",
    "only need to initialize for smaller leaf",
    "Get leaf boundary",
    "do nothing if no features can be processed on GPU",
    "copy data indices if it is not null",
    "converted indices in is_feature_used to feature-group indices",
    "construct the feature masks for dense feature-groups",
    "if no feature group is used, just return and do not use GPU",
    "if not all feature groups are used, we need to transfer the feature mask to GPU",
    "otherwise, we will use a specialized GPU kernel with all feature groups enabled",
    "We now copy even if all features are used.",
    "All data have been prepared, now run the GPU kernel",
    "construct smaller leaf",
    "Check workgroups per feature4 tuple..",
    "if the workgroup per feature is 1 (2^0), return as the work is too small for a GPU",
    "ConstructGPUHistogramsAsync will return true if there are availabe feature groups dispatched to GPU",
    "then construct sparse features on CPU",
    "We set data_indices to null to avoid rebuilding ordered gradients/hessians",
    "wait for GPU to finish, only if GPU is actually used",
    "use double precision",
    "use single precision",
    "Compare GPU histogram with CPU histogram, useful for debuggin GPU code problem",
    "#define CUDA_DEBUG_COMPARE",
    "construct larger leaf",
    "then construct sparse features on CPU",
    "We set data_indices to null to avoid rebuilding ordered gradients/hessians",
    "wait for GPU to finish, only if GPU is actually used",
    "use double precision",
    "use single precision",
    "do some sanity check for the GPU algorithm"
  ],
  "v3.3.0": [
    "coding: utf-8",
    "coding: utf-8",
    "create predictor first",
    "show deprecation warning only for early stop argument, setting early stop via global params should still be possible",
    "check dataset",
    "reduce cost for prediction training data",
    "process callbacks",
    "Most of legacy advanced options becomes callbacks",
    "construct booster",
    "start training",
    "check evaluation result.",
    "ranking task, split according to groups",
    "run preprocessing on the data set if needed",
    "setup callbacks",
    "coding: utf-8",
    "dummy function to support older version of scikit-learn",
    "coding: utf-8",
    "documentation templates for LGBMModel methods are shared between the classes in",
    "this module and those in the ``dask`` module",
    "user can set verbose with kwargs, it has higher priority",
    "Do not modify original args in fit function",
    "Refer to https://github.com/microsoft/LightGBM/pull/2619",
    "Separate built-in from callable evaluation metrics",
    "register default metric for consistency with callable eval_metric case",
    "try to deduce from class instance",
    "overwrite default metric by explicitly set metric",
    "concatenate metric from params (or default if not provided in params) and eval_metric",
    "copy for consistency",
    "reduce cost for prediction training data",
    "free dataset",
    "Switch to using a multiclass objective in the underlying LGBM instance",
    "do not modify args, as it causes errors in model selection tools",
    "check group data",
    "coding: utf-8",
    "we don't need lib_lightgbm while building docs",
    "coding: utf-8",
    "\"#ddffdd\" is light green, \"#ffdddd\" is light red",
    "coding: utf-8",
    "coding: utf-8",
    "TypeError: obj is not a string or a number",
    "ValueError: invalid literal",
    "DeprecationWarning is not shown by default, so let's create our own with higher level",
    "avoid side effects on passed-in parameters",
    "find a value, and remove other aliases with .pop()",
    "prefer the value of 'main_param_name' if it exists, otherwise search the aliases",
    "Get total row number.",
    "Random access by row index. Used for data sampling.",
    "Range data access. Used to read data in batch when constructing Dataset.",
    "Optionally specify batch_size to control range data read size.",
    "Only required if using ``Dataset.subset()``.",
    "__get_num_preds() cannot work with nrow > MAX_INT32, so calculate overall number of predictions piecemeal",
    "avoid memory consumption by arrays concatenation operations",
    "create numpy array from output arrays",
    "break up indptr based on number of rows (note more than one matrix in multiclass case)",
    "for CSC there is extra column added",
    "reformat output into a csr or csc matrix or list of csr or csc matrices",
    "same shape as input csr or csc matrix except extra column for expected value",
    "note: make sure we copy data as it will be deallocated next",
    "free the temporary native indptr, indices, and data",
    "__get_num_preds() cannot work with nrow > MAX_INT32, so calculate overall number of predictions piecemeal",
    "avoid memory consumption by arrays concatenation operations",
    "c type: double**",
    "each double* element points to start of each column of sample data.",
    "c type int**",
    "each int* points to start of indices for each column",
    "no min_data, nthreads and verbose in this function",
    "check data has header or not",
    "need to regroup init_score",
    "process for args",
    "user can set verbose with params, it has higher priority",
    "get categorical features",
    "process for reference dataset",
    "start construct data",
    "set feature names",
    "Select sampled rows, transpose to column order.",
    "create validation dataset from ref_dataset",
    "create valid",
    "construct subset",
    "create train",
    "could be updated if data is not freed",
    "set to None",
    "we're done if self and reference share a common upstream reference",
    "if buffer length is not long enough, reallocate buffers",
    "group data from LightGBM is boundaries data, need to convert to group size",
    "user can set verbose with params, it has higher priority",
    "Training task",
    "if \"machines\" is given, assume user wants to do distributed learning, and set up network",
    "construct booster object",
    "copy the parameters from train_set",
    "save reference to data",
    "buffer for inner predict",
    "Prediction task",
    "if a single node tree it won't have `leaf_index` so return 0",
    "Create the node record, and populate universal data members",
    "Update values to reflect node type (leaf or split)",
    "traverse the next level of the tree",
    "In tree format, \"subtree_list\" is a list of node records (dicts),",
    "and we add node to the list.",
    "need reset training data",
    "need to push new valid data",
    "if buffer length is not long enough, re-allocate a buffer",
    "if buffer length is not long enough, reallocate a buffer",
    "Copy models",
    "Get name of features",
    "if buffer length is not long enough, reallocate buffers",
    "avoid to predict many time in one iteration",
    "Get num of inner evals",
    "Get name of eval metrics",
    "if buffer length is not long enough, reallocate buffers",
    "coding: utf-8",
    "Callback environment used by callbacks",
    "split is needed for \"<dataset type> <metric>\" case (e.g. \"train l1\")",
    "split is needed for \"<dataset type> <metric>\" case (e.g. \"train l1\")",
    "coding: utf-8",
    "Concatenate many parts into one",
    "construct local eval_set data.",
    "store indices of eval_set components that were not contained within local parts.",
    "consolidate parts of each individual eval component.",
    "require that eval_name exists in evaluated result data in case dropped due to padding.",
    "in distributed training the 'training' eval_set is not detected, will have name 'valid_<index>'.",
    "filter padding from eval parts then _concat each eval_set component.",
    "reconstruct eval_set fit args/kwargs depending on which components of eval_set are on worker.",
    "ensure that expected keys for evals_result_ and best_score_ exist regardless of padding.",
    "capture whether local_listen_port or its aliases were provided",
    "capture whether machines or its aliases were provided",
    "Some passed-in parameters can be removed:",
    "* 'num_machines': set automatically from Dask worker list",
    "* 'num_threads': overridden to match nthreads on each Dask process",
    "Split arrays/dataframes into parts. Arrange parts into dicts to enforce co-locality",
    "evals_set will to be re-constructed into smaller lists of (X, y) tuples, where",
    "X and y are each delayed sub-lists of original eval dask Collections.",
    "find maximum number of parts in an individual eval set so that we can",
    "pad eval sets when they come in different sizes.",
    "when individual eval set is equivalent to training data, skip recomputing parts.",
    "add None-padding for individual eval_set member if it is smaller than the largest member.",
    "first time a chunk of this eval set is added to this part.",
    "append additional chunks of this eval set to this part.",
    "ensure that all evaluation parts map uniquely to one part.",
    "assign sub-eval_set components to worker parts.",
    "Start computation in the background",
    "Find locations of all parts and map them to particular Dask workers",
    "Check that all workers were provided some of eval_set. Otherwise warn user that validation",
    "data artifacts may not be populated depending on worker returning final estimator.",
    "assign general validation set settings to fit kwargs.",
    "resolve aliases for network parameters and pop the result off params.",
    "these values are added back in calls to `_train_part()`",
    "figure out network params",
    "Tell each worker to train on the parts that it has locally",
    "",
    "This code treats ``_train_part()`` calls as not \"pure\" because:",
    "1. there is randomness in the training process unless parameters ``seed``",
    "and ``deterministic`` are set",
    "2. even with those parameters set, the output of one ``_train_part()`` call",
    "relies on global state (it and all the other LightGBM training processes",
    "coordinate with each other)",
    "if network parameters were changed during training, remove them from the",
    "returned model so that they're generated dynamically on every run based",
    "on the Dask cluster you're connected to and which workers have pieces of",
    "the training data",
    "dask.DataFrame.map_partitions() expects each call to return a pandas DataFrame or Series",
    "for multi-class classification with sparse matrices, pred_contrib predictions",
    "are returned as a list of sparse matrices (one per class)",
    "pred_contrib output will have one column per feature,",
    "plus one more for the base value",
    "need to tell Dask the expected type and shape of individual preds",
    "by default, dask.array.concatenate() concatenates sparse arrays into a COO matrix",
    "the code below is used instead to ensure that the sparse type is preserved during concatentation",
    "At this point, `out` is a list of lists of delayeds (each of which points to a matrix).",
    "Concatenate them to return a list of Dask Arrays.",
    "the note on custom objective functions in LGBMModel.__init__ is not",
    "currently relevant for the Dask estimators",
    "DaskLGBMClassifier does not support group, eval_group, early_stopping_rounds.",
    "DaskLGBMClassifier support for callbacks and init_model is not tested",
    "the note on custom objective functions in LGBMModel.__init__ is not",
    "currently relevant for the Dask estimators",
    "DaskLGBMRegressor does not support group, eval_class_weight, eval_group, early_stopping_rounds.",
    "DaskLGBMRegressor support for callbacks and init_model is not tested",
    "the note on custom objective functions in LGBMModel.__init__ is not",
    "currently relevant for the Dask estimators",
    "DaskLGBMRanker does not support eval_class_weight or early stopping",
    "DaskLGBMRanker support for callbacks and init_model is not tested",
    "coding: utf-8",
    "load or create your dataset",
    "create dataset for lightgbm",
    "if you want to re-use data, remember to set free_raw_data=False",
    "specify your configurations as a dict",
    "generate feature names",
    "feature_name and categorical_feature",
    "check feature name",
    "save model to file",
    "dump model to JSON (and save to file)",
    "feature names",
    "feature importances",
    "load model to predict",
    "can only predict with the best iteration (or the saving iteration)",
    "eval with loaded model",
    "dump model with pickle",
    "load model with pickle to predict",
    "can predict with any iteration when loaded in pickle way",
    "eval with loaded model",
    "continue training",
    "init_model accepts:",
    "1. model file name",
    "2. Booster()",
    "decay learning rates",
    "learning_rates accepts:",
    "1. list/tuple with length = num_boost_round",
    "2. function(curr_iter)",
    "change other parameters during training",
    "self-defined objective function",
    "f(preds: array, train_data: Dataset) -> grad: array, hess: array",
    "log likelihood loss",
    "self-defined eval metric",
    "f(preds: array, train_data: Dataset) -> name: str, eval_result: float, is_higher_better: bool",
    "binary error",
    "NOTE: when you do customized loss function, the default prediction value is margin",
    "This may make built-in evaluation metric calculate wrong results",
    "For example, we are doing log likelihood loss, the prediction is score before logistic transformation",
    "Keep this in mind when you use the customization",
    "another self-defined eval metric",
    "f(preds: array, train_data: Dataset) -> name: str, eval_result: float, is_higher_better: bool",
    "accuracy",
    "NOTE: when you do customized loss function, the default prediction value is margin",
    "This may make built-in evaluation metric calculate wrong results",
    "For example, we are doing log likelihood loss, the prediction is score before logistic transformation",
    "Keep this in mind when you use the customization",
    "callback",
    "coding: utf-8",
    "load or create your dataset",
    "train",
    "predict",
    "eval",
    "feature importances",
    "self-defined eval metric",
    "f(y_true: array, y_pred: array) -> name: str, eval_result: float, is_higher_better: bool",
    "Root Mean Squared Logarithmic Error (RMSLE)",
    "train",
    "another self-defined eval metric",
    "f(y_true: array, y_pred: array) -> name: str, eval_result: float, is_higher_better: bool",
    "Relative Absolute Error (RAE)",
    "train",
    "predict",
    "eval",
    "other scikit-learn modules",
    "coding: utf-8",
    "load or create your dataset",
    "create dataset for lightgbm",
    "specify your configurations as a dict",
    "train",
    "coding: utf-8",
    "################",
    "Simulate some binary data with a single categorical and",
    "single continuous predictor",
    "################",
    "Set up a couple of utilities for our experiments",
    "################",
    "Observe the behavior of `binary` and `xentropy` objectives",
    "Trying this throws an error on non-binary values of y:",
    "experiment('binary', label_type='probability', DATA)",
    "The speed of `binary` is not drastically different than",
    "`xentropy`. `xentropy` runs faster than `binary` in many cases, although",
    "there are reasons to suspect that `binary` should run faster when the",
    "label is an integer instead of a float",
    "coding: utf-8",
    "load or create your dataset",
    "create dataset for lightgbm",
    "specify your configurations as a dict",
    "train",
    "save model to file",
    "predict",
    "eval",
    "We can also open HDF5 file once and get access to",
    "With binary dataset created, we can use either Python API or cmdline version to train.",
    "",
    "Note: in order to create exactly the same dataset with the one created in simple_example.py, we need",
    "to modify simple_example.py to pass numpy array instead of pandas DataFrame to Dataset constructor.",
    "The reason is that DataFrame column names will be used in Dataset. For a DataFrame with Int64Index",
    "as columns, Dataset will use column names like [\"0\", \"1\", \"2\", ...]. While for numpy array, column names",
    "are using the default one assigned in C++ code (dataset_loader.cpp), like [\"Column_0\", \"Column_1\", ...].",
    "Y has a single column and we read it in single shot. So store it as an 1-d array.",
    "We use random access for data sampling when creating LightGBM Dataset from Sequence.",
    "When accessing any element in a HDF5 chunk, it's read entirely.",
    "To save I/O for sampling, we should keep number of total chunks much larger than sample count.",
    "Here we are just creating a chunk size that matches with batch_size.",
    "",
    "Also note that the data is stored in row major order to avoid extra copy when passing to",
    "lightgbm Dataset.",
    "Save to 2 HDF5 files for demonstration.",
    "We can store multiple datasets inside a single HDF5 file.",
    "Separating X and Y for choosing best chunk size for data loading.",
    "split training data into two partitions",
    "make this array dense because we're splitting across",
    "a sparse boundary to partition the data",
    "the code below uses sklearn.metrics, but this requires pulling all of the",
    "predictions and target values back from workers to the client",
    "",
    "for larger datasets, consider the metrics from dask-ml instead",
    "https://ml.dask.org/modules/api.html#dask-ml-metrics-metrics",
    "coding: utf-8",
    "!/usr/bin/env python3",
    "-*- coding: utf-8 -*-",
    "",
    "LightGBM documentation build configuration file, created by",
    "sphinx-quickstart on Thu May  4 14:30:58 2017.",
    "",
    "This file is execfile()d with the current directory set to its",
    "containing dir.",
    "",
    "Note that not all possible configuration values are present in this",
    "autogenerated file.",
    "",
    "All configuration values have a default; values that are commented out",
    "serve to show the default.",
    "If extensions (or modules to document with autodoc) are in another directory,",
    "add these directories to sys.path here. If the directory is relative to the",
    "documentation root, use os.path.abspath to make it absolute.",
    "-- mock out modules",
    "-- General configuration ------------------------------------------------",
    "If your documentation needs a minimal Sphinx version, state it here.",
    "Add any Sphinx extension module names here, as strings. They can be",
    "extensions coming with Sphinx (named 'sphinx.ext.*') or your custom",
    "ones.",
    "hide type hints in API docs",
    "Generate autosummary pages. Output should be set with: `:toctree: pythonapi/`",
    "Only the class' docstring is inserted.",
    "If true, `todo` and `todoList` produce output, else they produce nothing.",
    "The master toctree document.",
    "General information about the project.",
    "The name of an image file (relative to this directory) to place at the top",
    "of the sidebar.",
    "The name of an image file (relative to this directory) to use as a favicon of",
    "the docs.  This file should be a Windows icon file (.ico) being 16x16 or 32x32",
    "pixels large.",
    "The version info for the project you're documenting, acts as replacement for",
    "|version| and |release|, also used in various other places throughout the",
    "built documents.",
    "The short X.Y version.",
    "The full version, including alpha/beta/rc tags.",
    "The language for content autogenerated by Sphinx. Refer to documentation",
    "for a list of supported languages.",
    "",
    "This is also used if you do content translation via gettext catalogs.",
    "Usually you set \"language\" from the command line for these cases.",
    "List of patterns, relative to source directory, that match files and",
    "directories to ignore when looking for source files.",
    "This patterns also effect to html_static_path and html_extra_path",
    "The name of the Pygments (syntax highlighting) style to use.",
    "-- Configuration for C API docs generation ------------------------------",
    "-- Options for HTML output ----------------------------------------------",
    "The theme to use for HTML and HTML Help pages.  See the documentation for",
    "a list of builtin themes.",
    "Theme options are theme-specific and customize the look and feel of a theme",
    "further.  For a list of options available for each theme, see the",
    "documentation.",
    "Add any paths that contain custom static files (such as style sheets) here,",
    "relative to this directory. They are copied after the builtin static files,",
    "so a file named \"default.css\" will overwrite the builtin \"default.css\".",
    "-- Options for HTMLHelp output ------------------------------------------",
    "Output file base name for HTML help builder.",
    "-- Options for LaTeX output ---------------------------------------------",
    "The name of an image file (relative to this directory) to place at the top of",
    "the title page.",
    "Warning! The following code can cause buffer overflows on RTD.",
    "Consider suppressing output completely if RTD project silently fails.",
    "Refer to https://github.com/svenevs/exhale",
    "/blob/fe7644829057af622e467bb529db6c03a830da99/exhale/deploy.py#L99-L111",
    "Warning! The following code can cause buffer overflows on RTD.",
    "Consider suppressing output completely if RTD project silently fails.",
    "Refer to https://github.com/svenevs/exhale",
    "/blob/fe7644829057af622e467bb529db6c03a830da99/exhale/deploy.py#L99-L111",
    "coding: utf-8",
    "This is a basic test for floating number parsing.",
    "Most of the test cases come from:",
    "https://github.com/dmlc/xgboost/blob/master/tests/cpp/common/test_charconv.cc",
    "https://github.com/Alexhuszagh/rust-lexical/blob/master/data/test-parse-unittests/strtod_tests.toml",
    "FLT_MAX",
    "FLT_MIN",
    "DBL_MAX (1 + (1 - 2^-52)) * 2^1023 = (2^53 - 1) * 2^971",
    "2^971 * (2^53 - 1 + 1/2) : the smallest number resolving to inf",
    "Near DBL_MIN",
    "DBL_MIN 2^-1022",
    "The behavior for parsing -nan depends on implementation.",
    "Thus we skip binary check for negative nan.",
    "See comment in test_cases.",
    "Constants",
    "Start with some content:",
    "Clear & re-use:",
    "Output should match new content:",
    "Number of trials for each new ChunkedArray configuration. Pass 100 times over the search space:",
    "Each outer loop iteration changes the test by adding +1 chunk. We start with 1 chunk only:",
    "Sweep valid and invalid addresses with a ChunkedArray with `chunks` chunks:",
    "Compute a new trial address & value & if it is a valid address:",
    "Insert item. If at a valid address, 0 is returned, otherwise, -1 is returned:",
    "If at valid address, check that the stored value is correct & remember it for the future:",
    "Check the just-stored value with getitem():",
    "Also store the just-stored value for future tracking:",
    "Final check: ensure even with overrides, all valid insertions store the latest value at that address:",
    "Test in 2 ways that the values are correctly laid out in memory:",
    "coding: utf-8",
    "we don't need lib_lightgbm while building docs",
    "coding: utf-8",
    "check saved model persistence",
    "we need to check the consistency of model file here, so test for exact equal",
    "check early stopping is working. Make it stop very early, so the scores should be very close to zero",
    "scores likely to be different, but prediction should still be the same",
    "test that shape is checked during prediction",
    "The simple implementation is just a single \"return self.ndarray[idx]\"",
    "The following is for demo and testing purpose.",
    "whole col",
    "half col",
    "Create dataset from numpy array directly.",
    "Create dataset using Sequence.",
    "Test for validation set.",
    "Select some random rows as valid data.",
    "From Dataset constructor, with dataset from numpy array.",
    "From Dataset.create_valid, with dataset from sequence.",
    "test that method works even with free_raw_data=True",
    "test that method works but sets raw data to None in case of immergeable data types",
    "test that method works for different data types",
    "Set extremely harsh penalties, so CEGB will block most splits.",
    "Compare pairs of penalties, to ensure scaling works as intended",
    "Reset booster1's parameters to p2, so the parameter section of the file matches.",
    "should resolve duplicate aliases, and prefer the main parameter",
    "should choose a value from an alias and set that value on main param",
    "if only an alias is used",
    "should use the default if main param and aliases are missing",
    "all changes should be made on copies and not modify the original",
    "coding: utf-8",
    "add target, weight, and group to DataFrame so that partitions abide by group boundaries.",
    "set_index ensures partitions are based on group id.",
    "See https://stackoverflow.com/questions/49532824/dask-dataframe-split-partitions-based-on-a-column-or-function.",
    "separate target, weight from features.",
    "encode group identifiers into run-length encoding, the format LightGBMRanker is expecting",
    "so that within each partition, sum(g) = n_samples.",
    "ranking arrays: one chunk per group. Each chunk must include all columns.",
    "make one categorical feature relevant to the target",
    "https://github.com/microsoft/LightGBM/issues/4118",
    "extra predict() parameters should be passed through correctly",
    "pref_leaf values should have the right shape",
    "and values that look like valid tree nodes",
    "be sure LightGBM actually used at least one categorical column,",
    "and that it was correctly treated as a categorical feature",
    "shape depends on whether it is binary or multiclass classification",
    "in the special case of multi-class classification using scipy sparse matrices,",
    "the output of `.predict(..., pred_contrib=True)` is a list of sparse matrices (one per class)",
    "",
    "since that case is so different than all other cases, check the relevant things here",
    "and then return early",
    "raw scores will probably be different, but at least check that all predicted classes are the same",
    "be sure LightGBM actually used at least one categorical column,",
    "and that it was correctly treated as a categorical feature",
    "* shape depends on whether it is binary or multiclass classification",
    "* matrix for binary classification is of the form [feature_contrib, base_value],",
    "for multi-class it's [feat_contrib_class1, base_value_class1, feat_contrib_class2, base_value_class2, etc.]",
    "* contrib outputs for distributed training are different than from local training, so we can just test",
    "that the output has the right shape and base values are in the right position",
    "check that found ports are different for same address (LocalCluster)",
    "check that the ports are indeed open",
    "Scores should be the same",
    "Predictions should be roughly the same.",
    "pref_leaf values should have the right shape",
    "and values that look like valid tree nodes",
    "extra predict() parameters should be passed through correctly",
    "be sure LightGBM actually used at least one categorical column,",
    "and that it was correctly treated as a categorical feature",
    "contrib outputs for distributed training are different than from local training, so we can just test",
    "that the output has the right shape and base values are in the right position",
    "be sure LightGBM actually used at least one categorical column,",
    "and that it was correctly treated as a categorical feature",
    "Quantiles should be right",
    "be sure LightGBM actually used at least one categorical column,",
    "and that it was correctly treated as a categorical feature",
    "rebalance small dask.Array dataset for better performance.",
    "use many trees + leaves to overfit, help ensure that Dask data-parallel strategy matches that of",
    "serial learner. See https://github.com/microsoft/LightGBM/issues/3292#issuecomment-671288210.",
    "distributed ranker should be able to rank decently well and should",
    "have high rank correlation with scores from serial ranker.",
    "extra predict() parameters should be passed through correctly",
    "pref_leaf values should have the right shape",
    "and values that look like valid tree nodes",
    "be sure LightGBM actually used at least one categorical column,",
    "and that it was correctly treated as a categorical feature",
    "Use larger trainset to prevent premature stopping due to zero loss, causing num_trees() < n_estimators.",
    "Use small chunk_size to avoid single-worker allocation of eval data partitions.",
    "test eval_class_weight, eval_init_score on binary-classification task.",
    "Note: objective's default `metric` will be evaluated in evals_result_ in addition to all eval_metrics.",
    "create eval_sets by creating new datasets or copying training data.",
    "total number of trees scales up for ova classifier.",
    "check that early stopping was not applied.",
    "checks that evals_result_ and best_score_ contain expected data and eval_set names.",
    "check that each eval_name and metric exists for all eval sets, allowing for the",
    "case when a worker receives a fully-padded eval_set component which is not evaluated.",
    "should be able to use the class without specifying a client",
    "should be able to set client after construction",
    "data on cluster1",
    "create identical data on cluster2",
    "at this point, the result of default_client() is client2 since it was the most recently",
    "created. So setting client to client1 here to test that you can select a non-default client",
    "unfitted model should survive pickling round trip, and pickling",
    "shouldn't have side effects on the model object",
    "client will always be None after unpickling",
    "fitted model should survive pickling round trip, and pickling",
    "shouldn't have side effects on the model object",
    "client will always be None after unpickling",
    "rebalance data to be sure that each worker has a piece of the data",
    "model 1 - no network parameters given",
    "model 2 - machines given",
    "model 3 - local_listen_port given",
    "training should fail because LightGBM will try to use the same",
    "port for multiple worker processes on the same machine",
    "rebalance data to be sure that each worker has a piece of the data",
    "test that \"machines\" is actually respected by creating a socket that uses",
    "one of the ports mentioned in \"machines\"",
    "The above error leaves a worker waiting",
    "an informative error should be raised if \"machines\" has duplicates",
    "\"client\" should be the only different, and the final argument",
    "value of the root node is 0 when init_score is set",
    "this test is separate because it takes a not-yet-constructed estimator",
    "coding: utf-8",
    "coding: utf-8",
    "build target, group ID vectors.",
    "build y/target and group-id vectors with user-specified group sizes.",
    "build y/target and group-id vectors according to n_samples, avg_gs, and random_gs.",
    "groups should contain > 1 element for pairwise learning objective.",
    "build feature data, X. Transform first few into informative features.",
    "coding: utf-8",
    "prediction result is actually not transformed (is raw) due to custom objective",
    "sklearn <0.23 does not have a stacking classifier and n_features_in_ property",
    "sklearn <0.23 does not have a stacking regressor and n_features_in_ property",
    "sklearn < 0.22 does not have the post fit attribute: classes_",
    "sklearn < 0.23 does not have as_frame parameter",
    "sklearn < 0.22 does not have the post fit attribute: classes_",
    "sklearn < 0.23 does not have as_frame parameter",
    "Test if random_state is properly stored",
    "Test if two random states produce identical models",
    "Test if subsequent fits sample from random_state object and produce different models",
    "Test that the largest element is NOT the same, the smallest can be the same, i.e. zero",
    "With default params",
    "Tests same probabilities",
    "Tests same predictions",
    "Tests same raw scores",
    "Tests same leaf indices",
    "Tests same feature contributions",
    "Tests other parameters for the prediction works",
    "Tests start_iteration",
    "Tests same probabilities, starting from iteration 10",
    "Tests same predictions, starting from iteration 10",
    "Tests same raw scores, starting from iteration 10",
    "Tests same leaf indices, starting from iteration 10",
    "Tests same feature contributions, starting from iteration 10",
    "Tests other parameters for the prediction works, starting from iteration 10",
    "no custom objective, no custom metric",
    "default metric",
    "non-default metric",
    "no metric",
    "non-default metric in eval_metric",
    "non-default metric with non-default metric in eval_metric",
    "non-default metric with multiple metrics in eval_metric",
    "non-default metric with multiple metrics in eval_metric for LGBMClassifier",
    "default metric for non-default objective",
    "non-default metric for non-default objective",
    "no metric",
    "non-default metric in eval_metric for non-default objective",
    "non-default metric with non-default metric in eval_metric for non-default objective",
    "non-default metric with multiple metrics in eval_metric for non-default objective",
    "custom objective, no custom metric",
    "default regression metric for custom objective",
    "non-default regression metric for custom objective",
    "multiple regression metrics for custom objective",
    "no metric",
    "default regression metric with non-default metric in eval_metric for custom objective",
    "non-default regression metric with metric in eval_metric for custom objective",
    "multiple regression metrics with metric in eval_metric for custom objective",
    "multiple regression metrics with multiple metrics in eval_metric for custom objective",
    "no custom objective, custom metric",
    "default metric with custom metric",
    "non-default metric with custom metric",
    "multiple metrics with custom metric",
    "custom metric (disable default metric)",
    "default metric for non-default objective with custom metric",
    "non-default metric for non-default objective with custom metric",
    "multiple metrics for non-default objective with custom metric",
    "custom metric (disable default metric for non-default objective)",
    "custom objective, custom metric",
    "custom metric for custom objective",
    "non-default regression metric with custom metric for custom objective",
    "multiple regression metrics with custom metric for custom objective",
    "default metric and invalid binary metric is replaced with multiclass alternative",
    "invalid objective is replaced with default multiclass one",
    "and invalid binary metric is replaced with multiclass alternative",
    "default metric for non-default multiclass objective",
    "and invalid binary metric is replaced with multiclass alternative",
    "default metric and invalid multiclass metric is replaced with binary alternative",
    "invalid multiclass metric is replaced with binary alternative for custom objective",
    "Verify that can receive a list of metrics, only callable",
    "Verify that can receive a list of custom and built-in metrics",
    "Verify that works as expected when eval_metric is empty",
    "Verify that can receive a list of metrics, only built-in",
    "Verify that eval_metric is robust to receiving a list with None",
    "training data as eval_set",
    "feval",
    "single eval_set",
    "two eval_set",
    "sklearn < 0.22 requires passing \"attributes\" argument",
    "Test that estimators are default-constructible",
    "coding: utf-8",
    "coding: utf-8",
    "check that default gives same result as k = 1",
    "check against independent calculation for k = 1",
    "check against independent calculation for k = 2",
    "check against independent calculation for k = 10",
    "check cases where predictions are equal",
    "should give same result as binary auc for 2 classes",
    "test the case where all predictions are equal",
    "test that weighted data gives different auc_mu",
    "test that equal data weights give same auc_mu as unweighted data",
    "should give 1 when accuracy = 1",
    "test loading class weights",
    "no early stopping",
    "early stopping occurs",
    "test custom eval metrics",
    "shuffle = False, override metric in params",
    "shuffle = True, callbacks",
    "enable display training loss",
    "self defined folds",
    "LambdaRank",
    "... with l2 metric",
    "... with NDCG (default) metric",
    "self defined folds with lambdarank",
    "with early stopping",
    "predict by each fold booster",
    "fold averaging",
    "without early stopping",
    "test feature_names with whitespaces",
    "This has non-ascii strings.",
    "take subsets and train",
    "generate CSR sparse dataset",
    "convert data to dense and get back same contribs",
    "validate the values are the same",
    "validate using CSC matrix",
    "validate the values are the same",
    "generate CSR sparse dataset",
    "convert data to dense and get back same contribs",
    "validate the values are the same",
    "validate using CSC matrix",
    "validate the values are the same",
    "Note there is an extra column added to the output for the expected value",
    "Note output CSC shape should be same as CSR output shape",
    "test sliced labels",
    "append some columns",
    "append some rows",
    "test sliced 2d matrix",
    "test sliced CSR",
    "trees start at position 1.",
    "split_features are in 4th line.",
    "test if a penalty as high as the depth indeed prohibits all monotone splits",
    "The penalization is so high that the first 2 features should not be used here",
    "Check that a very high penalization is the same as not using the features at all",
    "no fobj, no feval",
    "default metric",
    "non-default metric in params",
    "default metric in args",
    "non-default metric in args",
    "metric in args overwrites one in params",
    "multiple metrics in params",
    "multiple metrics in args",
    "remove default metric by 'None' in list",
    "remove default metric by 'None' aliases",
    "fobj, no feval",
    "no default metric",
    "metric in params",
    "metric in args",
    "metric in args overwrites its' alias in params",
    "multiple metrics in params",
    "multiple metrics in args",
    "no fobj, feval",
    "default metric with custom one",
    "non-default metric in params with custom one",
    "default metric in args with custom one",
    "non-default metric in args with custom one",
    "metric in args overwrites one in params, custom one is evaluated too",
    "multiple metrics in params with custom one",
    "multiple metrics in args with custom one",
    "custom metric is evaluated despite 'None' is passed",
    "fobj, feval",
    "no default metric, only custom one",
    "metric in params with custom one",
    "metric in args with custom one",
    "metric in args overwrites one in params, custom one is evaluated too",
    "multiple metrics in params with custom one",
    "multiple metrics in args with custom one",
    "custom metric is evaluated despite 'None' is passed",
    "no fobj, no feval",
    "default metric",
    "default metric in params",
    "non-default metric in params",
    "multiple metrics in params",
    "remove default metric by 'None' aliases",
    "fobj, no feval",
    "no default metric",
    "metric in params",
    "multiple metrics in params",
    "no fobj, feval",
    "default metric with custom one",
    "default metric in params with custom one",
    "non-default metric in params with custom one",
    "multiple metrics in params with custom one",
    "custom metric is evaluated despite 'None' is passed",
    "fobj, feval",
    "no default metric, only custom one",
    "metric in params with custom one",
    "multiple metrics in params with custom one",
    "custom metric is evaluated despite 'None' is passed",
    "multiclass default metric",
    "multiclass default metric with custom one",
    "multiclass metric alias with custom one for custom objective",
    "no metric for invalid class_num",
    "custom metric for invalid class_num",
    "multiclass metric alias with custom one with invalid class_num",
    "multiclass default metric without num_class",
    "multiclass metric alias",
    "multiclass metric",
    "non-valid metric for multiclass objective",
    "non-default num_class for default objective",
    "no metric with non-default num_class for custom objective",
    "multiclass metric alias for custom objective",
    "multiclass metric for custom objective",
    "binary metric with non-default num_class for custom objective",
    "Expect three metrics but mean and stdv for each metric",
    "test XGBoost-style return value",
    "test numpy-style return value",
    "test bins string type",
    "test histogram is disabled for categorical features",
    "test for lgb.train",
    "test feval for lgb.train",
    "test with two valid data for lgb.train",
    "test for lgb.cv",
    "test feval for lgb.cv",
    "test that binning works properly for features with only positive or only negative values",
    "decreasing without freeing raw data is allowed",
    "decreasing before lazy init is allowed",
    "increasing is allowed",
    "decreasing with disabled filter is allowed",
    "decreasing with enabled filter is disallowed;",
    "also changes of other params are disallowed",
    "check extra trees increases regularization",
    "check path smoothing increases regularization",
    "test edge case with one leaf",
    "check that constraint containing all features is equivalent to no constraint",
    "check that constraint partitioning the features reduces train accuracy",
    "check that constraints consisting of single features reduce accuracy further",
    "test that interaction constraints work when not all features are used",
    "check that setting linear_tree=True fits better than ordinary trees when data has linear relationship",
    "test again with nans in data",
    "test again with bagging",
    "test with a feature that has only one non-nan value",
    "test with a categorical feature",
    "test refit: same results on same data",
    "test refit with save and load",
    "test refit: different results training on different data",
    "test when num_leaves - 1 < num_features and when num_leaves - 1 > num_features",
    "test that the predict once with all iterations equals summed results with start_iteration and num_iteration",
    "test the case where start_iteration <= 0, and num_iteration is None",
    "test the case where start_iteration > 0, and num_iteration <= 0",
    "test the case where start_iteration > 0, and num_iteration <= 0, with pred_leaf=True",
    "test the case where start_iteration > 0, and num_iteration <= 0, with pred_contrib=True",
    "test for regression",
    "test both with and without early stopping",
    "test for multi-class",
    "test both with and without early stopping",
    "test for binary",
    "test both with and without early stopping",
    "test against sklearn average precision metric",
    "test that average precision is 1 where model predicts perfectly",
    "coding: utf-8",
    "If compiled appropriately, the same installation will support both GPU and CPU.",
    "coding: utf-8",
    "coding: utf-8",
    "These are helper functions to allow doing a stack unwind",
    "after an R allocation error, which would trigger a long jump.",
    "convert from one-based to zero-based index",
    "if any feature names were larger than allocated size,",
    "allow for a larger size and try again",
    "convert from boundaries to size",
    "--- start Booster interfaces",
    "if any eval names were larger than allocated size,",
    "allow for a larger size and try again",
    "if the model string was larger than the initial buffer, allocate a bigger buffer and try again",
    "if the model string was larger than the initial buffer, allocate a bigger buffer and try again",
    ".Call() calls",
    "coding: utf-8",
    "alias table",
    "names",
    "from strings",
    "tails",
    "tails",
    "coding: utf-8",
    "Single row predictor to abstract away caching logic",
    "create boosting",
    "initialize the boosting",
    "create objective function",
    "initialize the objective function",
    "create training metric",
    "reset the boosting",
    "create objective function",
    "initialize the objective function",
    "calculate the nonzero data and indices size",
    "allocate data and indices arrays",
    "Get the number of trees per iteration (for multiclass scenario we output multiple sparse matrices)",
    "aggregated per row feature contribution results",
    "keep track of the row_vector sizes for parallelization",
    "copy vector results to output for each row",
    "Get the number of trees per iteration (for multiclass scenario we output multiple sparse matrices)",
    "aggregated per row feature contribution results",
    "calculate number of elements per column to construct",
    "the CSC matrix with random access",
    "keep track of column counts",
    "keep track of beginning index for each column",
    "keep track of beginning index for each matrix",
    "Note: we parallelize across matrices instead of rows because of the column_counts[m][col_idx] increment inside the loop",
    "store the row index",
    "update column count",
    "explicitly declare symbols from LightGBM namespace",
    "some help functions used to convert data",
    "Row iterator of on column for CSC matrix",
    "return value at idx, only can access by ascent order",
    "return next non-zero pair, if index < 0, means no more data",
    "start of c_api functions",
    "This API is to keep python binding's behavior the same with C++ implementation.",
    "Sample count, random seed etc. should be provided in parameters.",
    "sample data first",
    "sample data first",
    "sample data first",
    "local buffer to re-use memory",
    "sample data first",
    "no more data",
    "---- start of booster",
    "Single row in row-major format:",
    "---- start of some help functions",
    "data is array of pointers to individual rows",
    "set number of threads for openmp",
    "check for alias",
    "read parameters from config file",
    "remove str after \"#\"",
    "check for alias again",
    "load configs",
    "prediction is needed if using input initial model(continued train)",
    "need to continue training",
    "sync up random seed for data partition",
    "load Training data",
    "load data for distributed training",
    "load data for single machine",
    "need save binary file",
    "create training metric",
    "only when have metrics then need to construct validation data",
    "Add validation data, if it exists",
    "add",
    "need save binary file",
    "add metric for validation data",
    "output used time on each iteration",
    "need init network",
    "create boosting",
    "create objective function",
    "load training data",
    "initialize the objective function",
    "initialize the boosting",
    "add validation data into boosting",
    "convert model to if-else statement code",
    "create predictor",
    "Free memory",
    "create predictor",
    "label_gain = 2^i - 1, may overflow, so we use 31 here",
    "counts for all labels",
    "start from top label, and accumulate DCG",
    "counts for all labels",
    "calculate k Max DCG by one pass",
    "get sorted indices by score",
    "calculate multi dcg by one pass",
    "wait for all client start up",
    "Don't call MPI_Finalize() here: If the destructor was called because only this node had an exception, calling MPI_Finalize() will cause all nodes to hang.",
    "Instead we will handle finalize/abort for MPI in main().",
    "default set to -1",
    "distance at k-th communication, distance[k] = 2^k",
    "set incoming rank at k-th commuication",
    "set outgoing rank at k-th commuication",
    "default set as -1",
    "construct all recursive halving map for all machines",
    "let 1 << k <= num_machines",
    "distance of each communication",
    "if num_machines = 2^k, don't need to group machines",
    "communication direction, %2 == 0 is positive",
    "neighbor at k-th communication",
    "receive data block at k-th communication",
    "send data block at k-th communication",
    "if num_machines != 2^k, need to group machines",
    "group, two machine in one group, total \"rest\" groups will have 2 machines.",
    "let left machine as group leader",
    "cache block information for groups, group with 2 machines will have double block size",
    "convert from group to node leader",
    "convert from node to group",
    "meet new group",
    "add block len for this group",
    "calculate the group block start",
    "not need to construct",
    "get receive block information",
    "accumulate block len",
    "get send block information",
    "accumulate block len",
    "static member definition",
    "if small package or small count , do it by all gather.(reduce the communication times.)",
    "assign the blocks to every rank.",
    "do reduce scatter",
    "do all gather",
    "assign blocks",
    "need use buffer here, since size of \"output\" is smaller than size after all gather",
    "copy back",
    "assign blocks",
    "start all gather",
    "when num_machines is small and data is large",
    "use output as receive buffer",
    "get current local block size",
    "get out rank",
    "get in rank",
    "get send information",
    "get recv information",
    "send and recv at same time",
    "rotate in-place",
    "use output as receive buffer",
    "get current local block size",
    "get send information",
    "get recv information",
    "send and recv at same time",
    "use output as receive buffer",
    "send and recv at same time",
    "send local data to neighbor first",
    "receive neighbor data first",
    "reduce",
    "get target",
    "get send information",
    "get recv information",
    "send and recv at same time",
    "reduce",
    "send result to neighbor",
    "receive result from neighbor",
    "copy result",
    "start up socket",
    "parse clients from file",
    "get ip list of local machine",
    "get local rank",
    "construct listener",
    "construct communication topo",
    "construct linkers",
    "free listener",
    "set timeout",
    "accept incoming socket",
    "receive rank",
    "add new socket",
    "save ranks that need to connect with",
    "start listener",
    "start connect",
    "let smaller rank connect to larger rank",
    "send local rank",
    "wait for listener",
    "print connected linkers",
    "only need to copy subset",
    "avoid to copy subset many times",
    "avoid out of range",
    "may need to recopy subset",
    "valid the type",
    "Constructors",
    "Get type tag",
    "Comparisons",
    "This has to be separate, not in Statics, because Json() accesses",
    "statics().null.",
    "advance until next line, or end of input",
    "advance until closing tokens",
    "The usual case: non-escaped characters",
    "Handle escapes",
    "Extract 4-byte escape sequence",
    "Explicitly check length of the substring. The following loop",
    "relies on std::string returning the terminating NUL when",
    "accessing str[length]. Checking here reduces brittleness.",
    "JSON specifies that characters outside the BMP shall be encoded as a",
    "pair of 4-hex-digit \\u escapes encoding their surrogate pair",
    "components. Check whether we're in the middle of such a beast: the",
    "previous codepoint was an escaped lead (high) surrogate, and this is",
    "a trail (low) surrogate.",
    "Reassemble the two surrogate pairs into one astral-plane character,",
    "per the UTF-16 algorithm.",
    "Integer part",
    "Decimal part",
    "Exponent part",
    "Check for any trailing garbage",
    "Documented in json11.hpp",
    "Check for another object",
    "get column names",
    "load label idx first",
    "erase label column name",
    "load ignore columns",
    "load weight idx",
    "load group idx",
    "don't support query id in data file when using distributed training",
    "read data to memory",
    "sample data",
    "construct feature bin mappers",
    "initialize label",
    "extract features",
    "sample data from file",
    "construct feature bin mappers",
    "initialize label",
    "extract features",
    "load data from binary file",
    "check meta data",
    "need to check training data",
    "read data in memory",
    "initialize label",
    "extract features",
    "Get number of lines of data file",
    "initialize label",
    "extract features",
    "load data from binary file",
    "not need to check validation data",
    "check meta data",
    "buffer to read binary file",
    "check token",
    "read size of header",
    "re-allocmate space if not enough",
    "read header",
    "get header",
    "num_groups",
    "real_feature_idx_",
    "feature2group",
    "feature2subfeature",
    "group_bin_boundaries",
    "group_feature_start_",
    "group_feature_cnt_",
    "get feature names",
    "write feature names",
    "get forced_bin_bounds_",
    "read size of meta data",
    "re-allocate space if not enough",
    "read meta data",
    "load meta data",
    "sample local used data if need to partition",
    "if not contain query file, minimal sample unit is one record",
    "if contain query file, minimal sample unit is one query",
    "if is new query",
    "read feature data",
    "read feature size",
    "re-allocate space if not enough",
    "raw data",
    "fill feature_names_ if not header",
    "get forced split",
    "if only one machine, find bin locally",
    "if have multi-machines, need to find bin distributed",
    "different machines will find bin for different features",
    "start and len will store the process feature indices for different machines",
    "machine i will find bins for features in [ start[i], start[i] + len[i] )",
    "free",
    "gather global feature bin mappers",
    "restore features bins from buffer",
    "---- private functions ----",
    "if features are ordered, not need to use hist_buf",
    "read all lines",
    "get query data",
    "if not contain query data, minimal sample unit is one record",
    "if contain query data, minimal sample unit is one query",
    "if is new query",
    "get query data",
    "if not contain query file, minimal sample unit is one record",
    "if contain query file, minimal sample unit is one query",
    "if is new query",
    "parse features",
    "get forced split",
    "check the range of label_idx, weight_idx and group_idx",
    "fill feature_names_ if not header",
    "start find bins",
    "if only one machine, find bin locally",
    "start and len will store the process feature indices for different machines",
    "machine i will find bins for features in [ start[i], start[i] + len[i] )",
    "free",
    "gather global feature bin mappers",
    "restore features bins from buffer",
    "if doesn't need to prediction with initial model",
    "parser",
    "set label",
    "free processed line:",
    "shrink_to_fit will be very slow in linux, and seems not free memory, disable for now",
    "text_reader_->Lines()[i].shrink_to_fit();",
    "push data",
    "if is used feature",
    "if need to prediction with initial model",
    "parser",
    "set initial score",
    "set label",
    "free processed line:",
    "shrink_to_fit will be very slow in Linux, and seems not free memory, disable for now",
    "text_reader_->Lines()[i].shrink_to_fit();",
    "push data",
    "if is used feature",
    "metadata_ will manage space of init_score",
    "text data can be free after loaded feature values",
    "parser",
    "set initial score",
    "set label",
    "push data",
    "if is used feature",
    "only need part of data",
    "need full data",
    "metadata_ will manage space of init_score",
    "read size of token",
    "remove duplicates",
    "deep copy function for BinMapper",
    "mean size for one bin",
    "need a new bin",
    "update bin upper bound",
    "last bin upper bound",
    "get list of distinct values",
    "get number of positive and negative distinct values",
    "include zero bounds and infinity bound",
    "add forced bounds, excluding zeros since we have already added zero bounds",
    "find remaining bounds",
    "find distinct_values first",
    "push zero in the front",
    "use the large value",
    "push zero in the back",
    "convert to int type first",
    "sort by counts",
    "will ignore the categorical of small counts",
    "Push the dummy bin for NaN",
    "Use MissingType::None to represent this bin contains all categoricals",
    "fix count of NaN bin",
    "check trivial(num_bin_ == 1) feature",
    "check useless bin",
    "When most_freq_bin_ != default_bin_, there are some additional data loading costs.",
    "so use most_freq_bin_  = default_bin_ when there is not so sparse",
    "calculate max bin of all features to select the int type in MultiValDenseBin",
    "for lambdarank, it needs query data for partition data in distributed learning",
    "need convert query_id to boundaries",
    "check weights",
    "check query boundries",
    "contain initial score file",
    "check weights",
    "get local weights",
    "check query boundries",
    "get local query boundaries",
    "contain initial score file",
    "get local initial scores",
    "re-load query weight",
    "save to nullptr",
    "save to nullptr",
    "save to nullptr",
    "default weight file name",
    "default init_score file name",
    "use first line to count number class",
    "default query file name",
    "root is in the depth 0",
    "non-leaf",
    "leaf",
    "use this for the missing value conversion",
    "Predict func by Map to ifelse",
    "use this for the missing value conversion",
    "non-leaf",
    "left subtree",
    "right subtree",
    "leaf",
    "non-leaf",
    "left subtree",
    "right subtree",
    "leaf",
    "recursive computation of SHAP values for a decision tree",
    "extend the unique path",
    "leaf node",
    "internal node",
    "see if we have already split on this feature,",
    "if so we undo that split so we can redo it for this node",
    "recursive sparse computation of SHAP values for a decision tree",
    "extend the unique path",
    "leaf node",
    "internal node",
    "see if we have already split on this feature,",
    "if so we undo that split so we can redo it for this node",
    "add names of objective function if not providing metric",
    "equal weights for all classes",
    "generate seeds by seed.",
    "sort eval_at",
    "Only push the non-training data",
    "check for conflicts",
    "check if objective, metric, and num_class match",
    "Change pool size to -1 (no limit) when using data parallel to reduce communication costs",
    "Check max_depth and num_leaves",
    "Fits in an int, and is more restrictive than the current num_leaves",
    "force col-wise for gpu & CUDA",
    "force gpu_use_dp for CUDA",
    "linear tree learner must be serial type and run on CPU device",
    "min_data_in_leaf must be at least 2 if path smoothing is active. This is because when the split is calculated",
    "the count is calculated using the proportion of hessian in the leaf which is rounded up to nearest int, so it can",
    "be 1 when there is actually no data in the leaf. In rare cases this can cause a bug because with path smoothing the",
    "calculated split gain can be positive even with zero gradient and hessian.",
    "In distributed mode, local node doesn't have histograms on all features, cannot perform \"intermediate\" monotone constraints.",
    "\"intermediate\" monotone constraints need to recompute splits. If the features are sampled when computing the",
    "split initially, then the sampling needs to be recorded or done once again, which is currently not supported",
    "first round: fill the single val group",
    "always push the last group",
    "put dense feature first",
    "sort by non zero cnt",
    "sort by non zero cnt, bigger first",
    "shuffle groups",
    "Using std::swap for vector<bool> will cause the wrong result.",
    "get num_features",
    "get bin_mappers",
    "for sparse multi value bin, we store the feature bin values with offset added",
    "for dense multi value bin, the feature bin values without offsets are used",
    "copy feature bin mapper data",
    "copy feature bin mapper data",
    "if not pass a filename, just append \".bin\" of original file",
    "get size of header",
    "size of feature names",
    "size of forced bins",
    "write header",
    "write feature names",
    "write forced bins",
    "get size of meta data",
    "write meta data",
    "write feature data",
    "get size of feature",
    "write feature",
    "write raw data; use row-major order so we can read row-by-row",
    "explicitly initialize template methods, for cross module call",
    "Only one multi-val group, just simply merge",
    "Skip the leading 0 when copying group_bin_boundaries.",
    "regenerate other fields",
    "store the importance first",
    "PredictRaw",
    "PredictRawByMap",
    "Predict",
    "PredictByMap",
    "PredictLeafIndex",
    "PredictLeafIndexByMap",
    "output model type",
    "output number of class",
    "output label index",
    "output max_feature_idx",
    "output objective",
    "output tree models",
    "store the importance first",
    "sort the importance",
    "use serialized string to restore this object",
    "Use first 128 chars to avoid exceed the message buffer.",
    "get number of classes",
    "get index of label",
    "get max_feature_idx first",
    "get average_output",
    "get feature names",
    "get monotone_constraints",
    "set zero",
    "predict all the trees for one iteration",
    "check early stopping",
    "set zero",
    "predict all the trees for one iteration",
    "check early stopping",
    "margin_threshold will be captured by value",
    "copy and sort",
    "margin_threshold will be captured by value",
    "Fix for compiler warnings about reaching end of control",
    "load forced_splits file",
    "init tree learner",
    "push training metrics",
    "create buffer for gradients and Hessians",
    "get max feature index",
    "get label index",
    "get feature names",
    "if need bagging, create buffer",
    "for a validation dataset, we need its score and metric",
    "update score",
    "objective function will calculate gradients and hessians",
    "random bagging, minimal unit is one record",
    "random bagging, minimal unit is one record",
    "if need bagging",
    "set bagging data to tree learner",
    "get subset",
    "output used time per iteration",
    "boosting from average label; or customized \"average\" if implemented for the current objective",
    "boosting first",
    "bagging logic",
    "need to copy gradients for bagging subset.",
    "shrinkage by learning rate",
    "update score",
    "only add default score one-time",
    "updates scores",
    "add model",
    "reset score",
    "remove model",
    "print message for metric",
    "pop last early_stopping_round_ models",
    "update training score",
    "we need to predict out-of-bag scores of data for boosting",
    "update validation score",
    "print training metric",
    "print validation metric",
    "set zero",
    "predict all the trees for one iteration",
    "predict all the trees for one iteration",
    "push training metrics",
    "not same training data, need reset score and others",
    "create score tracker",
    "update score",
    "create buffer for gradients and hessians",
    "load forced_splits file",
    "if need bagging, create buffer",
    "Get the max size of pool",
    "at least need 2 leaves",
    "push split information for all leaves",
    "initialize splits for leaf",
    "initialize data partition",
    "initialize ordered gradients and hessians",
    "cannot change is_hist_col_wise during training",
    "initialize splits for leaf",
    "initialize data partition",
    "initialize ordered gradients and hessians",
    "Get the max size of pool",
    "at least need 2 leaves",
    "push split information for all leaves",
    "some initial works before training",
    "root leaf",
    "only root leaf can be splitted on first time",
    "some initial works before finding best split",
    "find best threshold for every feature",
    "Get a leaf with max split gain",
    "Get split information for best leaf",
    "cannot split, quit",
    "split tree with best leaf",
    "reset histogram pool",
    "initialize data partition",
    "reset the splits for leaves",
    "Sumup for root",
    "use all data",
    "use bagging, only use part of data",
    "check depth of current leaf",
    "only need to check left leaf, since right leaf is in same level of left leaf",
    "no enough data to continue",
    "only have root",
    "put parent(left) leaf's histograms into larger leaf's histograms",
    "put parent(left) leaf's histograms to larger leaf's histograms",
    "construct smaller leaf",
    "construct larger leaf",
    "find splits",
    "only has root leaf",
    "start at root leaf",
    "before processing next node from queue, store info for current left/right leaf",
    "store \"best split\" for left and right, even if they might be overwritten by forced split",
    "then, compute own splits",
    "split info should exist because searching in bfs fashion - should have added from parent",
    "update before tree split",
    "don't need to update this in data-based parallel model",
    "split tree, will return right leaf",
    "store the true split gain in tree model",
    "don't need to update this in data-based parallel model",
    "store the true split gain in tree model",
    "init the leaves that used on next iteration",
    "update leave outputs if needed",
    "bag_mapper[index_mapper[i]]",
    "it is needed to filter the features after the above code.",
    "Otherwise, the `is_splittable` in `FeatureHistogram` will be wrong, and cause some features being accidentally filtered in the later nodes.",
    "for root leaf the \"parent\" output is its own output because we don't apply any smoothing to the root",
    "can't use GetParentOutput because leaf_splits doesn't have weight property set",
    "find splits",
    "identify features containing nans",
    "preallocate the matrix used to calculate linear model coefficients",
    "store only upper triangular half of matrix as an array, in row-major order",
    "this requires (max_num_feat + 1) * (max_num_feat + 2) / 2 entries (including the constant terms of the regression)",
    "we add another 8 to ensure cache lines are not shared among processors",
    "some initial works before training",
    "root leaf",
    "only root leaf can be splitted on first time",
    "some initial works before finding best split",
    "find best threshold for every feature",
    "Get a leaf with max split gain",
    "Get split information for best leaf",
    "cannot split, quit",
    "split tree with best leaf",
    "map data to leaf number",
    "calculate coefficients using the method described in Eq 3 of https://arxiv.org/pdf/1802.05640.pdf",
    "the coefficients vector is given by",
    "- (X_T * H * X + lambda) ^ (-1) * (X_T * g)",
    "where:",
    "X is the matrix where the first column is the feature values and the second is all ones,",
    "H is the diagonal matrix of the hessian,",
    "lambda is the diagonal matrix with diagonal entries equal to the regularisation term linear_lambda",
    "g is the vector of gradients",
    "the subscript _T denotes the transpose",
    "create array of pointers to raw data, and coefficient matrices, for each leaf",
    "clear the coefficient matrices",
    "aggregate results from different threads",
    "copy into eigen matrices and solve",
    "update the tree properties",
    "need to be able to hold smaller and larger best splits in SyncUpGlobalBestSplit",
    "get feature partition",
    "get local used features",
    "get best split at smaller leaf",
    "find local best split for larger leaf",
    "sync global best info",
    "update best split",
    "instantiate template classes, otherwise linker cannot find the code",
    "initialize SerialTreeLearner",
    "Get local rank and global machine size",
    "need to be able to hold smaller and larger best splits in SyncUpGlobalBestSplit",
    "allocate buffer for communication",
    "generate feature partition for current tree",
    "get local used feature",
    "get block start and block len for reduce scatter",
    "get buffer_write_start_pos_",
    "get buffer_read_start_pos_",
    "sync global data sumup info",
    "global sumup reduce",
    "copy back",
    "set global sumup info",
    "init global data count in leaf",
    "clear histogram buffer before synchronizing",
    "otherwise histogram contents from the previous iteration will be sent",
    "construct local histograms",
    "copy to buffer",
    "Reduce scatter for histogram",
    "restore global histograms from buffer",
    "only root leaf",
    "construct histgroms for large leaf, we init larger leaf as the parent, so we can just subtract the smaller leaf's histograms",
    "find local best split for larger leaf",
    "sync global best info",
    "set best split",
    "need update global number of data in leaf",
    "instantiate template classes, otherwise linker cannot find the code",
    "initialize SerialTreeLearner",
    "some additional variables needed for GPU trainer",
    "Initialize GPU buffers and kernels",
    "some functions used for debugging the GPU histogram construction",
    "printf(\"grad %g != %g (%d ULPs)\\n\", GET_GRAD(h1, i), GET_GRAD(h2, i), ulps);",
    "goto err;",
    "we roughly want 256 workgroups per device, and we have num_dense_feature4_ feature tuples.",
    "also guarantee that there are at least 2K examples per workgroup",
    "return 0;",
    "we have already copied ordered gradients, ordered Hessians and indices to GPU",
    "decide the best number of workgroups working on one feature4 tuple",
    "set work group size based on feature size",
    "each 2^exp_workgroups_per_feature workgroups work on a feature4 tuple",
    "we need to refresh the kernel arguments after reallocating",
    "The only argument that needs to be changed later is num_data_",
    "the GPU kernel will process all features in one call, and each",
    "2^exp_workgroups_per_feature (compile time constant) workgroup will",
    "process one feature4 tuple",
    "for the root node, indices are not copied",
    "for constant hessian, hessians are not copied except for the root node",
    "there will be 2^exp_workgroups_per_feature = num_workgroups / num_dense_feature4 sub-histogram per feature4",
    "and we will launch num_feature workgroups for this kernel",
    "will launch threads for all features",
    "the queue should be asynchronous, and we will can WaitAndGetHistograms() before we start processing dense feature groups",
    "copy the results asynchronously. Size depends on if double precision is used",
    "we will wait for this object in WaitAndGetHistograms",
    "when the output is ready, the computation is done",
    "values of this feature has been redistributed to multiple bins; need a reduction here",
    "how many feature-group tuples we have",
    "leave some safe margin for prefetching",
    "256 work-items per workgroup. Each work-item prefetches one tuple for that feature",
    "clear sparse/dense maps",
    "do nothing if no features can be processed on GPU",
    "allocate memory for all features (FIXME: 4 GB barrier on some devices, need to split to multiple buffers)",
    "unpin old buffer if necessary before destructing them",
    "make ordered_gradients and Hessians larger (including extra room for prefetching), and pin them",
    "allocate space for gradients and Hessians on device",
    "we will copy gradients and Hessians in after ordered_gradients_ and ordered_hessians_ are constructed",
    "allocate feature mask, for disabling some feature-groups' histogram calculation",
    "copy indices to the device",
    "histogram bin entry size depends on the precision (single/double)",
    "create output buffer, each feature has a histogram with device_bin_size_ bins,",
    "each work group generates a sub-histogram of dword_features_ features.",
    "only initialize once here, as this will not need to change when ResetTrainingData() is called",
    "create atomic counters for inter-group coordination",
    "The output buffer is allocated to host directly, to overlap compute and data transfer",
    "find the dense feature-groups and group then into Feature4 data structure (several feature-groups packed into 4 bytes)",
    "looking for dword_features_ non-sparse feature-groups",
    "decide if we need to redistribute the bin",
    "multiplier must be a power of 2",
    "device_bin_mults_.push_back(1);",
    "found",
    "for data transfer time",
    "Now generate new data structure feature4, and copy data to the device",
    "preallocate arrays for all threads, and pin them",
    "building Feature4 bundles; each thread handles dword_features_ features",
    "one feature datapoint is 4 bits",
    "this guarantees that the RawGet() function is inlined, rather than using virtual function dispatching",
    "one feature datapoint is one byte",
    "this guarantees that the RawGet() function is inlined, rather than using virtual function dispatching",
    "Dense bin",
    "Dense 4-bit bin",
    "working on the remaining (less than dword_features_) feature groups",
    "fill the leftover features",
    "fill this empty feature with some \"random\" value",
    "fill this empty feature with some \"random\" value",
    "copying the last 1 to (dword_features - 1) feature-groups in the last tuple",
    "deallocate pinned space for feature copying",
    "data transfer time",
    "for other types of failure, build log might not be available; program.build_log() can crash",
    "Something bad happened. Just return \"No log available.\"",
    "build is okay, log may contain warnings",
    "destroy any old kernels",
    "create OpenCL kernels for different number of workgroups per feature",
    "currently we don't use constant memory",
    "compile the GPU kernel depending if double precision is used, constant hessian is used, etc.",
    "kernel with indices in an array",
    "kernel with all features enabled, with eliminated branches",
    "kernel with all data indices (for root node, and assumes that root node always uses all features)",
    "do nothing if no features can be processed on GPU",
    "The only argument that needs to be changed later is num_data_",
    "hessian is passed as a parameter, but it is not available now.",
    "hessian will be set in BeforeTrain()",
    "Get the max bin size, used for selecting best GPU kernel",
    "initialize GPU",
    "determine which kernel to use based on the max number of bins",
    "the +9 skips extra characters \")\", newline, \"#endif\" and newline at the beginning",
    "the +9 skips extra characters \")\", newline, \"#endif\" and newline at the beginning",
    "the +9 skips extra characters \")\", newline, \"#endif\" and newline at the beginning",
    "setup GPU kernel arguments after we allocating all the buffers",
    "GPU memory has to been reallocated because data may have been changed",
    "setup GPU kernel arguments after we allocating all the buffers",
    "Copy initial full hessians and gradients to GPU.",
    "We start copying as early as possible, instead of at ConstructHistogram().",
    "setup hessian parameters only",
    "hessian is passed as a parameter",
    "use bagging",
    "On GPU, we start copying indices, gradients and Hessians now, instead at ConstructHistogram()",
    "copy used gradients and Hessians to ordered buffer",
    "transfer the indices to GPU",
    "transfer hessian to GPU",
    "setup hessian parameters only",
    "hessian is passed as a parameter",
    "transfer gradients to GPU",
    "only have root",
    "Copy indices, gradients and Hessians as early as possible",
    "only need to initialize for smaller leaf",
    "Get leaf boundary",
    "copy indices to the GPU:",
    "copy ordered Hessians to the GPU:",
    "copy ordered gradients to the GPU:",
    "do nothing if no features can be processed on GPU",
    "copy data indices if it is not null",
    "generate and copy ordered_gradients if gradients is not null",
    "generate and copy ordered_hessians if Hessians is not null",
    "converted indices in is_feature_used to feature-group indices",
    "construct the feature masks for dense feature-groups",
    "if no feature group is used, just return and do not use GPU",
    "if not all feature groups are used, we need to transfer the feature mask to GPU",
    "otherwise, we will use a specialized GPU kernel with all feature groups enabled",
    "All data have been prepared, now run the GPU kernel",
    "construct smaller leaf",
    "ConstructGPUHistogramsAsync will return true if there are available feature groups dispatched to GPU",
    "then construct sparse features on CPU",
    "wait for GPU to finish, only if GPU is actually used",
    "use double precision",
    "use single precision",
    "Compare GPU histogram with CPU histogram, useful for debugging GPU code problem",
    "#define GPU_DEBUG_COMPARE",
    "construct larger leaf",
    "then construct sparse features on CPU",
    "wait for GPU to finish, only if GPU is actually used",
    "use double precision",
    "use single precision",
    "do some sanity check for the GPU algorithm",
    "limit top k",
    "get max bin",
    "calculate buffer size",
    "need to be able to hold smaller and larger best splits in SyncUpGlobalBestSplit",
    "left and right on same time, so need double size",
    "initialize histograms for global",
    "sync global data sumup info",
    "set global sumup info",
    "init global data count in leaf",
    "get local sumup",
    "get local sumup",
    "get mean number on machines",
    "weighted gain",
    "get top k",
    "Copy histogram to buffer, and Get local aggregate features",
    "copy histograms.",
    "copy smaller leaf histograms first",
    "mark local aggregated feature",
    "copy",
    "then copy larger leaf histograms",
    "mark local aggregated feature",
    "copy",
    "use local data to find local best splits",
    "clear histogram buffer before synchronizing",
    "otherwise histogram contents from the previous iteration will be sent",
    "find splits",
    "only has root leaf",
    "local voting",
    "gather",
    "get all top-k from all machines",
    "global voting",
    "copy local histgrams to buffer",
    "Reduce scatter for histogram",
    "find best split from local aggregated histograms",
    "restore from buffer",
    "restore from buffer",
    "find local best",
    "find local best split for larger leaf",
    "sync global best info",
    "copy back",
    "set the global number of data for leaves",
    "init the global sumup info",
    "instantiate template classes, otherwise linker cannot find the code",
    "launch cuda kernel",
    "initialize SerialTreeLearner",
    "some additional variables needed for GPU trainer",
    "Initialize GPU buffers and kernels: get device info",
    "some functions used for debugging the GPU histogram construction",
    "we roughly want 256 workgroups per device, and we have num_dense_feature4_ feature tuples.",
    "also guarantee that there are at least 2K examples per workgroup",
    "we have already copied ordered gradients, ordered hessians and indices to GPU",
    "decide the best number of workgroups working on one feature4 tuple",
    "set work group size based on feature size",
    "each 2^exp_workgroups_per_feature workgroups work on a feature4 tuple",
    "set thread_data",
    "copy the results asynchronously. Size depends on if double precision is used",
    "when the output is ready, the computation is done",
    "how many feature-group tuples we have",
    "leave some safe margin for prefetching",
    "256 work-items per workgroup. Each work-item prefetches one tuple for that feature",
    "clear sparse/dense maps",
    "do nothing it there is no dense feature",
    "calculate number of feature groups per gpu",
    "histogram bin entry size depends on the precision (single/double)",
    "allocate GPU memory for each GPU",
    "do nothing it there is no gpu feature",
    "allocate memory for all features",
    "allocate space for gradients and hessians on device",
    "we will copy gradients and hessians in after ordered_gradients_ and ordered_hessians_ are constructed",
    "copy indices to the device",
    "create output buffer, each feature has a histogram with device_bin_size_ bins,",
    "each work group generates a sub-histogram of dword_features_ features.",
    "only initialize once here, as this will not need to change when ResetTrainingData() is called",
    "create atomic counters for inter-group coordination",
    "The output buffer is allocated to host directly, to overlap compute and data transfer",
    "clear sparse/dense maps",
    "find the dense feature-groups and group then into Feature4 data structure (several feature-groups packed into 4 bytes)",
    "set device info",
    "looking for dword_features_ non-sparse feature-groups",
    "reset device info",
    "InitGPU w/ num_gpu",
    "Get the max bin size, used for selecting best GPU kernel",
    "get num_dense_feature_groups_",
    "initialize GPU",
    "set cpu threads",
    "resize device memory pointers",
    "create stream & events to handle multiple GPUs",
    "check data size",
    "GPU memory has to been reallocated because data may have been changed",
    "AllocateGPUMemory only when the number of data increased",
    "setup GPU kernel arguments after we allocating all the buffers",
    "Copy initial full hessians and gradients to GPU.",
    "We start copying as early as possible, instead of at ConstructHistogram().",
    "use bagging",
    "On GPU, we start copying indices, gradients and hessians now, instead at ConstructHistogram()",
    "copy used gradients and hessians to ordered buffer",
    "transfer the indices to GPU",
    "only have root",
    "Copy indices, gradients and hessians as early as possible",
    "only need to initialize for smaller leaf",
    "Get leaf boundary",
    "do nothing if no features can be processed on GPU",
    "copy data indices if it is not null",
    "converted indices in is_feature_used to feature-group indices",
    "construct the feature masks for dense feature-groups",
    "if no feature group is used, just return and do not use GPU",
    "if not all feature groups are used, we need to transfer the feature mask to GPU",
    "otherwise, we will use a specialized GPU kernel with all feature groups enabled",
    "We now copy even if all features are used.",
    "All data have been prepared, now run the GPU kernel",
    "construct smaller leaf",
    "Check workgroups per feature4 tuple..",
    "if the workgroup per feature is 1 (2^0), return as the work is too small for a GPU",
    "ConstructGPUHistogramsAsync will return true if there are availabe feature groups dispatched to GPU",
    "then construct sparse features on CPU",
    "We set data_indices to null to avoid rebuilding ordered gradients/hessians",
    "wait for GPU to finish, only if GPU is actually used",
    "use double precision",
    "use single precision",
    "Compare GPU histogram with CPU histogram, useful for debuggin GPU code problem",
    "#define CUDA_DEBUG_COMPARE",
    "construct larger leaf",
    "then construct sparse features on CPU",
    "We set data_indices to null to avoid rebuilding ordered gradients/hessians",
    "wait for GPU to finish, only if GPU is actually used",
    "use double precision",
    "use single precision",
    "do some sanity check for the GPU algorithm"
  ],
  "v3.2.1": [
    "coding: utf-8",
    "see https://github.com/pypa/distutils/pull/21",
    "coding: utf-8",
    "create predictor first",
    "check dataset",
    "reduce cost for prediction training data",
    "process callbacks",
    "Most of legacy advanced options becomes callbacks",
    "construct booster",
    "start training",
    "check evaluation result.",
    "ranking task, split according to groups",
    "run preprocessing on the data set if needed",
    "setup callbacks",
    "coding: utf-8",
    "dummy function to support older version of scikit-learn",
    "coding: utf-8",
    "documentation templates for LGBMModel methods are shared between the classes in",
    "this module and those in the ``dask`` module",
    "user can set verbose with kwargs, it has higher priority",
    "Do not modify original args in fit function",
    "Refer to https://github.com/microsoft/LightGBM/pull/2619",
    "Separate built-in from callable evaluation metrics",
    "register default metric for consistency with callable eval_metric case",
    "try to deduce from class instance",
    "overwrite default metric by explicitly set metric",
    "concatenate metric from params (or default if not provided in params) and eval_metric",
    "copy for consistency",
    "reduce cost for prediction training data",
    "free dataset",
    "Switch to using a multiclass objective in the underlying LGBM instance",
    "do not modify args, as it causes errors in model selection tools",
    "check group data",
    "coding: utf-8",
    "we don't need lib_lightgbm while building docs",
    "coding: utf-8",
    "\"#ddffdd\" is light green, \"#ffdddd\" is light red",
    "coding: utf-8",
    "coding: utf-8",
    "TypeError: obj is not a string or a number",
    "ValueError: invalid literal",
    "DeprecationWarning is not shown by default, so let's create our own with higher level",
    "avoid side effects on passed-in parameters",
    "find a value, and remove other aliases with .pop()",
    "prefer the value of 'main_param_name' if it exists, otherwise search the aliases",
    "__get_num_preds() cannot work with nrow > MAX_INT32, so calculate overall number of predictions piecemeal",
    "avoid memory consumption by arrays concatenation operations",
    "create numpy array from output arrays",
    "break up indptr based on number of rows (note more than one matrix in multiclass case)",
    "for CSC there is extra column added",
    "reformat output into a csr or csc matrix or list of csr or csc matrices",
    "same shape as input csr or csc matrix except extra column for expected value",
    "note: make sure we copy data as it will be deallocated next",
    "free the temporary native indptr, indices, and data",
    "__get_num_preds() cannot work with nrow > MAX_INT32, so calculate overall number of predictions piecemeal",
    "avoid memory consumption by arrays concatenation operations",
    "no min_data, nthreads and verbose in this function",
    "check data has header or not",
    "need to regroup init_score",
    "process for args",
    "user can set verbose with params, it has higher priority",
    "get categorical features",
    "process for reference dataset",
    "start construct data",
    "set feature names",
    "create valid",
    "construct subset",
    "create train",
    "could be updated if data is not freed",
    "set to None",
    "we're done if self and reference share a common upstrem reference",
    "group data from LightGBM is boundaries data, need to convert to group size",
    "user can set verbose with params, it has higher priority",
    "Training task",
    "if \"machines\" is given, assume user wants to do distributed learning, and set up network",
    "construct booster object",
    "copy the parameters from train_set",
    "save reference to data",
    "buffer for inner predict",
    "Prediction task",
    "if a single node tree it won't have `leaf_index` so return 0",
    "Create the node record, and populate universal data members",
    "Update values to reflect node type (leaf or split)",
    "traverse the next level of the tree",
    "In tree format, \"subtree_list\" is a list of node records (dicts),",
    "and we add node to the list.",
    "need reset training data",
    "need to push new valid data",
    "if buffer length is not long enough, re-allocate a buffer",
    "if buffer length is not long enough, reallocate a buffer",
    "Copy models",
    "Get name of features",
    "avoid to predict many time in one iteration",
    "Get num of inner evals",
    "Get name of evals",
    "coding: utf-8",
    "Callback environment used by callbacks",
    "split is needed for \"<dataset type> <metric>\" case (e.g. \"train l1\")",
    "split is needed for \"<dataset type> <metric>\" case (e.g. \"train l1\")",
    "coding: utf-8",
    "Concatenate many parts into one",
    "if any duplicates were found, search for new ports one by one",
    "capture whether local_listen_port or its aliases were provided",
    "capture whether machines or its aliases were provided",
    "Some passed-in parameters can be removed:",
    "* 'num_machines': set automatically from Dask worker list",
    "* 'num_threads': overridden to match nthreads on each Dask process",
    "Split arrays/dataframes into parts. Arrange parts into dicts to enforce co-locality",
    "Start computation in the background",
    "Find locations of all parts and map them to particular Dask workers",
    "resolve aliases for network parameters and pop the result off params.",
    "these values are added back in calls to `_train_part()`",
    "figure out network params",
    "this approach with client.run() is faster than searching for ports",
    "serially, but can produce duplicates sometimes. Try the fast approach one",
    "time, then pass it through a function that will use a slower but more reliable",
    "approach if duplicates are found.",
    "Tell each worker to train on the parts that it has locally",
    "",
    "This code treates ``_train_part()`` calls as not \"pure\" because:",
    "1. there is randomness in the training process unless parameters ``seed``",
    "and ``deterministic`` are set",
    "2. even with those parameters set, the output of one ``_train_part()`` call",
    "relies on global state (it and all the other LightGBM training processes",
    "coordinate with each other)",
    "if network parameters were changed during training, remove them from the",
    "returned moodel so that they're generated dynamically on every run based",
    "on the Dask cluster you're connected to and which workers have pieces of",
    "the training data",
    "dask.DataFrame.map_partitions() expects each call to return a pandas DataFrame or Series",
    "the note on custom objective functions in LGBMModel.__init__ is not",
    "currently relevant for the Dask estimators",
    "DaskLGBMClassifier does not support evaluation data, or early stopping",
    "DaskLGBMClassifier support for callbacks and init_model is not tested",
    "the note on custom objective functions in LGBMModel.__init__ is not",
    "currently relevant for the Dask estimators",
    "DaskLGBMRegressor does not support evaluation data, or early stopping",
    "DaskLGBMRegressor support for callbacks and init_model is not tested",
    "the note on custom objective functions in LGBMModel.__init__ is not",
    "currently relevant for the Dask estimators",
    "DaskLGBMRanker does not support evaluation data, or early stopping",
    "DaskLGBMRanker support for callbacks and init_model is not tested",
    "coding: utf-8",
    "load or create your dataset",
    "create dataset for lightgbm",
    "if you want to re-use data, remember to set free_raw_data=False",
    "specify your configurations as a dict",
    "generate feature names",
    "feature_name and categorical_feature",
    "check feature name",
    "save model to file",
    "dump model to JSON (and save to file)",
    "feature names",
    "feature importances",
    "load model to predict",
    "can only predict with the best iteration (or the saving iteration)",
    "eval with loaded model",
    "dump model with pickle",
    "load model with pickle to predict",
    "can predict with any iteration when loaded in pickle way",
    "eval with loaded model",
    "continue training",
    "init_model accepts:",
    "1. model file name",
    "2. Booster()",
    "decay learning rates",
    "learning_rates accepts:",
    "1. list/tuple with length = num_boost_round",
    "2. function(curr_iter)",
    "change other parameters during training",
    "self-defined objective function",
    "f(preds: array, train_data: Dataset) -> grad: array, hess: array",
    "log likelihood loss",
    "self-defined eval metric",
    "f(preds: array, train_data: Dataset) -> name: string, eval_result: float, is_higher_better: bool",
    "binary error",
    "NOTE: when you do customized loss function, the default prediction value is margin",
    "This may make built-in evalution metric calculate wrong results",
    "For example, we are doing log likelihood loss, the prediction is score before logistic transformation",
    "Keep this in mind when you use the customization",
    "another self-defined eval metric",
    "f(preds: array, train_data: Dataset) -> name: string, eval_result: float, is_higher_better: bool",
    "accuracy",
    "NOTE: when you do customized loss function, the default prediction value is margin",
    "This may make built-in evalution metric calculate wrong results",
    "For example, we are doing log likelihood loss, the prediction is score before logistic transformation",
    "Keep this in mind when you use the customization",
    "callback",
    "coding: utf-8",
    "load or create your dataset",
    "train",
    "predict",
    "eval",
    "feature importances",
    "self-defined eval metric",
    "f(y_true: array, y_pred: array) -> name: string, eval_result: float, is_higher_better: bool",
    "Root Mean Squared Logarithmic Error (RMSLE)",
    "train",
    "another self-defined eval metric",
    "f(y_true: array, y_pred: array) -> name: string, eval_result: float, is_higher_better: bool",
    "Relative Absolute Error (RAE)",
    "train",
    "predict",
    "eval",
    "other scikit-learn modules",
    "coding: utf-8",
    "load or create your dataset",
    "create dataset for lightgbm",
    "specify your configurations as a dict",
    "train",
    "coding: utf-8",
    "################",
    "Simulate some binary data with a single categorical and",
    "single continuous predictor",
    "################",
    "Set up a couple of utilities for our experiments",
    "################",
    "Observe the behavior of `binary` and `xentropy` objectives",
    "Trying this throws an error on non-binary values of y:",
    "experiment('binary', label_type='probability', DATA)",
    "The speed of `binary` is not drastically different than",
    "`xentropy`. `xentropy` runs faster than `binary` in many cases, although",
    "there are reasons to suspect that `binary` should run faster when the",
    "label is an integer instead of a float",
    "coding: utf-8",
    "load or create your dataset",
    "create dataset for lightgbm",
    "specify your configurations as a dict",
    "train",
    "save model to file",
    "predict",
    "eval",
    "split training data into two partitions",
    "make this array dense because we're splitting across",
    "a sparse boundary to partition the data",
    "the code below uses sklearn.metrics, but this requires pulling all of the",
    "predictions and target values back from workers to the client",
    "",
    "for larger datasets, consider the metrics from dask-ml instead",
    "https://ml.dask.org/modules/api.html#dask-ml-metrics-metrics",
    "coding: utf-8",
    "!/usr/bin/env python3",
    "-*- coding: utf-8 -*-",
    "",
    "LightGBM documentation build configuration file, created by",
    "sphinx-quickstart on Thu May  4 14:30:58 2017.",
    "",
    "This file is execfile()d with the current directory set to its",
    "containing dir.",
    "",
    "Note that not all possible configuration values are present in this",
    "autogenerated file.",
    "",
    "All configuration values have a default; values that are commented out",
    "serve to show the default.",
    "If extensions (or modules to document with autodoc) are in another directory,",
    "add these directories to sys.path here. If the directory is relative to the",
    "documentation root, use os.path.abspath to make it absolute.",
    "-- mock out modules",
    "-- General configuration ------------------------------------------------",
    "If your documentation needs a minimal Sphinx version, state it here.",
    "Add any Sphinx extension module names here, as strings. They can be",
    "extensions coming with Sphinx (named 'sphinx.ext.*') or your custom",
    "ones.",
    "hide type hints in API docs",
    "Generate autosummary pages. Output should be set with: `:toctree: pythonapi/`",
    "Only the class' docstring is inserted.",
    "If true, `todo` and `todoList` produce output, else they produce nothing.",
    "The master toctree document.",
    "General information about the project.",
    "The name of an image file (relative to this directory) to place at the top",
    "of the sidebar.",
    "The name of an image file (relative to this directory) to use as a favicon of",
    "the docs.  This file should be a Windows icon file (.ico) being 16x16 or 32x32",
    "pixels large.",
    "The version info for the project you're documenting, acts as replacement for",
    "|version| and |release|, also used in various other places throughout the",
    "built documents.",
    "The short X.Y version.",
    "The full version, including alpha/beta/rc tags.",
    "The language for content autogenerated by Sphinx. Refer to documentation",
    "for a list of supported languages.",
    "",
    "This is also used if you do content translation via gettext catalogs.",
    "Usually you set \"language\" from the command line for these cases.",
    "List of patterns, relative to source directory, that match files and",
    "directories to ignore when looking for source files.",
    "This patterns also effect to html_static_path and html_extra_path",
    "The name of the Pygments (syntax highlighting) style to use.",
    "-- Configuration for C API docs generation ------------------------------",
    "-- Options for HTML output ----------------------------------------------",
    "The theme to use for HTML and HTML Help pages.  See the documentation for",
    "a list of builtin themes.",
    "Theme options are theme-specific and customize the look and feel of a theme",
    "further.  For a list of options available for each theme, see the",
    "documentation.",
    "Add any paths that contain custom static files (such as style sheets) here,",
    "relative to this directory. They are copied after the builtin static files,",
    "so a file named \"default.css\" will overwrite the builtin \"default.css\".",
    "-- Options for HTMLHelp output ------------------------------------------",
    "Output file base name for HTML help builder.",
    "-- Options for LaTeX output ---------------------------------------------",
    "The name of an image file (relative to this directory) to place at the top of",
    "the title page.",
    "Warning! The following code can cause buffer overflows on RTD.",
    "Consider suppressing output completely if RTD project silently fails.",
    "Refer to https://github.com/svenevs/exhale",
    "/blob/fe7644829057af622e467bb529db6c03a830da99/exhale/deploy.py#L99-L111",
    "Warning! The following code can cause buffer overflows on RTD.",
    "Consider suppressing output completely if RTD project silently fails.",
    "Refer to https://github.com/svenevs/exhale",
    "/blob/fe7644829057af622e467bb529db6c03a830da99/exhale/deploy.py#L99-L111",
    "coding: utf-8",
    "Constants",
    "Start with some content:",
    "Clear & re-use:",
    "Output should match new content:",
    "Number of trials for each new ChunkedArray configuration. Pass 100 times over the search space:",
    "Each outer loop iteration changes the test by adding +1 chunk. We start with 1 chunk only:",
    "Sweep valid and invalid addresses with a ChunkedArray with `chunks` chunks:",
    "Compute a new trial address & value & if it is a valid address:",
    "Insert item. If at a valid address, 0 is returned, otherwise, -1 is returned:",
    "If at valid address, check that the stored value is correct & remember it for the future:",
    "Check the just-stored value with getitem():",
    "Also store the just-stored value for future tracking:",
    "Final check: ensure even with overrides, all valid insertions store the latest value at that address:",
    "Test in 2 ways that the values are correctly laid out in memory:",
    "coding: utf-8",
    "we don't need lib_lightgbm while building docs",
    "coding: utf-8",
    "check saved model persistence",
    "we need to check the consistency of model file here, so test for exact equal",
    "check early stopping is working. Make it stop very early, so the scores should be very close to zero",
    "scores likely to be different, but prediction should still be the same",
    "test that shape is checked during prediction",
    "test that method works even with free_raw_data=True",
    "test that method works but sets raw data to None in case of immergeable data types",
    "test that method works for different data types",
    "Set extremely harsh penalties, so CEGB will block most splits.",
    "Compare pairs of penalties, to ensure scaling works as intended",
    "Reset booster1's parameters to p2, so the parameter section of the file matches.",
    "should resolve duplicate aliases, and prefer the main parameter",
    "should choose a value from an alias and set that value on main param",
    "if only an alias is used",
    "should use the default if main param and aliases are missing",
    "all changes should be made on copies and not modify the original",
    "coding: utf-8",
    "add target, weight, and group to DataFrame so that partitions abide by group boundaries.",
    "set_index ensures partitions are based on group id.",
    "See https://stackoverflow.com/questions/49532824/dask-dataframe-split-partitions-based-on-a-column-or-function.",
    "separate target, weight from features.",
    "encode group identifiers into run-length encoding, the format LightGBMRanker is expecting",
    "so that within each partition, sum(g) = n_samples.",
    "ranking arrays: one chunk per group. Each chunk must include all columns.",
    "make one categorical feature relevant to the target",
    "https://github.com/microsoft/LightGBM/issues/4118",
    "pref_leaf values should have the right shape",
    "and values that look like valid tree nodes",
    "be sure LightGBM actually used at least one categorical column,",
    "and that it was correctly treated as a categorical feature",
    "be sure LightGBM actually used at least one categorical column,",
    "and that it was correctly treated as a categorical feature",
    "shape depends on whether it is binary or multiclass classification",
    "* shape depends on whether it is binary or multiclass classification",
    "* matrix for binary classification is of the form [feature_contrib, base_value],",
    "for multi-class it's [feat_contrib_class1, base_value_class1, feat_contrib_class2, base_value_class2, etc.]",
    "* contrib outputs for distributed training are different than from local training, so we can just test",
    "that the output has the right shape and base values are in the right position",
    "check that found ports are different for same address (LocalCluster)",
    "check that the ports are indeed open",
    "should handle worker maps without any duplicates",
    "should handle worker maps with duplicates",
    "Scores should be the same",
    "Predictions should be roughly the same.",
    "pref_leaf values should have the right shape",
    "and values that look like valid tree nodes",
    "be sure LightGBM actually used at least one categorical column,",
    "and that it was correctly treated as a categorical feature",
    "contrib outputs for distributed training are different than from local training, so we can just test",
    "that the output has the right shape and base values are in the right position",
    "be sure LightGBM actually used at least one categorical column,",
    "and that it was correctly treated as a categorical feature",
    "Quantiles should be right",
    "be sure LightGBM actually used at least one categorical column,",
    "and that it was correctly treated as a categorical feature",
    "rebalance small dask.Array dataset for better performance.",
    "use many trees + leaves to overfit, help ensure that Dask data-parallel strategy matches that of",
    "serial learner. See https://github.com/microsoft/LightGBM/issues/3292#issuecomment-671288210.",
    "distributed ranker should be able to rank decently well and should",
    "have high rank correlation with scores from serial ranker.",
    "pref_leaf values should have the right shape",
    "and values that look like valid tree nodes",
    "be sure LightGBM actually used at least one categorical column,",
    "and that it was correctly treated as a categorical feature",
    "should be able to use the class without specifying a client",
    "should be able to set client after construction",
    "data on cluster1",
    "create identical data on cluster2",
    "at this point, the result of default_client() is client2 since it was the most recently",
    "created. So setting client to client1 here to test that you can select a non-default client",
    "unfitted model should survive pickling round trip, and pickling",
    "shouldn't have side effects on the model object",
    "client will always be None after unpickling",
    "fitted model should survive pickling round trip, and pickling",
    "shouldn't have side effects on the model object",
    "client will always be None after unpickling",
    "rebalance data to be sure that each worker has a piece of the data",
    "model 1 - no network parameters given",
    "model 2 - machines given",
    "model 3 - local_listen_port given",
    "training should fail because LightGBM will try to use the same",
    "port for multiple worker processes on the same machine",
    "rebalance data to be sure that each worker has a piece of the data",
    "test that \"machines\" is actually respected by creating a socket that uses",
    "one of the ports mentioned in \"machines\"",
    "The above error leaves a worker waiting",
    "an informative error should be raised if \"machines\" has duplicates",
    "\"client\" should be the only different, and the final argument",
    "init_scores must be a 1D array, even for multiclass classification",
    "where you need to provide 1 score per class for each row in X",
    "https://github.com/microsoft/LightGBM/issues/4046",
    "value of the root node is 0 when init_score is set",
    "this test is separate because it takes a not-yet-constructed estimator",
    "coding: utf-8",
    "coding: utf-8",
    "build target, group ID vectors.",
    "build y/target and group-id vectors with user-specified group sizes.",
    "build y/target and group-id vectors according to n_samples, avg_gs, and random_gs.",
    "groups should contain > 1 element for pairwise learning objective.",
    "build feature data, X. Transform first few into informative features.",
    "coding: utf-8",
    "prediction result is actually not transformed (is raw) due to custom objective",
    "sklearn <0.23 does not have a stacking classifier and n_features_in_ property",
    "sklearn <0.23 does not have a stacking regressor and n_features_in_ property",
    "sklearn < 0.22 does not have the post fit attribute: classes_",
    "sklearn < 0.23 does not have as_frame parameter",
    "sklearn < 0.22 does not have the post fit attribute: classes_",
    "sklearn < 0.23 does not have as_frame parameter",
    "Test if random_state is properly stored",
    "Test if two random states produce identical models",
    "Test if subsequent fits sample from random_state object and produce different models",
    "Test that the largest element is NOT the same, the smallest can be the same, i.e. zero",
    "With default params",
    "Tests same probabilities",
    "Tests same predictions",
    "Tests same raw scores",
    "Tests same leaf indices",
    "Tests same feature contributions",
    "Tests other parameters for the prediction works",
    "Tests start_iteration",
    "Tests same probabilities, starting from iteration 10",
    "Tests same predictions, starting from iteration 10",
    "Tests same raw scores, starting from iteration 10",
    "Tests same leaf indices, starting from iteration 10",
    "Tests same feature contributions, starting from iteration 10",
    "Tests other parameters for the prediction works, starting from iteration 10",
    "no custom objective, no custom metric",
    "default metric",
    "non-default metric",
    "no metric",
    "non-default metric in eval_metric",
    "non-default metric with non-default metric in eval_metric",
    "non-default metric with multiple metrics in eval_metric",
    "non-default metric with multiple metrics in eval_metric for LGBMClassifier",
    "default metric for non-default objective",
    "non-default metric for non-default objective",
    "no metric",
    "non-default metric in eval_metric for non-default objective",
    "non-default metric with non-default metric in eval_metric for non-default objective",
    "non-default metric with multiple metrics in eval_metric for non-default objective",
    "custom objective, no custom metric",
    "default regression metric for custom objective",
    "non-default regression metric for custom objective",
    "multiple regression metrics for custom objective",
    "no metric",
    "default regression metric with non-default metric in eval_metric for custom objective",
    "non-default regression metric with metric in eval_metric for custom objective",
    "multiple regression metrics with metric in eval_metric for custom objective",
    "multiple regression metrics with multiple metrics in eval_metric for custom objective",
    "no custom objective, custom metric",
    "default metric with custom metric",
    "non-default metric with custom metric",
    "multiple metrics with custom metric",
    "custom metric (disable default metric)",
    "default metric for non-default objective with custom metric",
    "non-default metric for non-default objective with custom metric",
    "multiple metrics for non-default objective with custom metric",
    "custom metric (disable default metric for non-default objective)",
    "custom objective, custom metric",
    "custom metric for custom objective",
    "non-default regression metric with custom metric for custom objective",
    "multiple regression metrics with custom metric for custom objective",
    "default metric and invalid binary metric is replaced with multiclass alternative",
    "invalid objective is replaced with default multiclass one",
    "and invalid binary metric is replaced with multiclass alternative",
    "default metric for non-default multiclass objective",
    "and invalid binary metric is replaced with multiclass alternative",
    "default metric and invalid multiclass metric is replaced with binary alternative",
    "invalid multiclass metric is replaced with binary alternative for custom objective",
    "Verify that can receive a list of metrics, only callable",
    "Verify that can receive a list of custom and built-in metrics",
    "Verify that works as expected when eval_metric is empty",
    "Verify that can receive a list of metrics, only built-in",
    "Verify that eval_metric is robust to receiving a list with None",
    "training data as eval_set",
    "feval",
    "single eval_set",
    "two eval_set",
    "sklearn < 0.22 requires passing \"attributes\" argument",
    "Test that estimators are default-constructible",
    "coding: utf-8",
    "coding: utf-8",
    "check that default gives same result as k = 1",
    "check against independent calculation for k = 1",
    "check against independent calculation for k = 2",
    "check against independent calculation for k = 10",
    "check cases where predictions are equal",
    "should give same result as binary auc for 2 classes",
    "test the case where all predictions are equal",
    "test that weighted data gives different auc_mu",
    "test that equal data weights give same auc_mu as unweighted data",
    "should give 1 when accuracy = 1",
    "test loading class weights",
    "no early stopping",
    "early stopping occurs",
    "test custom eval metrics",
    "shuffle = False, override metric in params",
    "shuffle = True, callbacks",
    "enable display training loss",
    "self defined folds",
    "lambdarank",
    "... with l2 metric",
    "... with NDCG (default) metric",
    "self defined folds with lambdarank",
    "with early stopping",
    "predict by each fold booster",
    "fold averaging",
    "without early stopping",
    "test feature_names with whitespaces",
    "This has non-ascii strings.",
    "take subsets and train",
    "generate CSR sparse dataset",
    "convert data to dense and get back same contribs",
    "validate the values are the same",
    "validate using CSC matrix",
    "validate the values are the same",
    "generate CSR sparse dataset",
    "convert data to dense and get back same contribs",
    "validate the values are the same",
    "validate using CSC matrix",
    "validate the values are the same",
    "Note there is an extra column added to the output for the expected value",
    "Note output CSC shape should be same as CSR output shape",
    "test sliced labels",
    "append some columns",
    "append some rows",
    "test sliced 2d matrix",
    "test sliced CSR",
    "trees start at position 1.",
    "split_features are in 4th line.",
    "test if a penalty as high as the depth indeed prohibits all monotone splits",
    "The penalization is so high that the first 2 features should not be used here",
    "Check that a very high penalization is the same as not using the features at all",
    "no fobj, no feval",
    "default metric",
    "non-default metric in params",
    "default metric in args",
    "non-default metric in args",
    "metric in args overwrites one in params",
    "multiple metrics in params",
    "multiple metrics in args",
    "remove default metric by 'None' in list",
    "remove default metric by 'None' aliases",
    "fobj, no feval",
    "no default metric",
    "metric in params",
    "metric in args",
    "metric in args overwrites its' alias in params",
    "multiple metrics in params",
    "multiple metrics in args",
    "no fobj, feval",
    "default metric with custom one",
    "non-default metric in params with custom one",
    "default metric in args with custom one",
    "non-default metric in args with custom one",
    "metric in args overwrites one in params, custom one is evaluated too",
    "multiple metrics in params with custom one",
    "multiple metrics in args with custom one",
    "custom metric is evaluated despite 'None' is passed",
    "fobj, feval",
    "no default metric, only custom one",
    "metric in params with custom one",
    "metric in args with custom one",
    "metric in args overwrites one in params, custom one is evaluated too",
    "multiple metrics in params with custom one",
    "multiple metrics in args with custom one",
    "custom metric is evaluated despite 'None' is passed",
    "no fobj, no feval",
    "default metric",
    "default metric in params",
    "non-default metric in params",
    "multiple metrics in params",
    "remove default metric by 'None' aliases",
    "fobj, no feval",
    "no default metric",
    "metric in params",
    "multiple metrics in params",
    "no fobj, feval",
    "default metric with custom one",
    "default metric in params with custom one",
    "non-default metric in params with custom one",
    "multiple metrics in params with custom one",
    "custom metric is evaluated despite 'None' is passed",
    "fobj, feval",
    "no default metric, only custom one",
    "metric in params with custom one",
    "multiple metrics in params with custom one",
    "custom metric is evaluated despite 'None' is passed",
    "multiclass default metric",
    "multiclass default metric with custom one",
    "multiclass metric alias with custom one for custom objective",
    "no metric for invalid class_num",
    "custom metric for invalid class_num",
    "multiclass metric alias with custom one with invalid class_num",
    "multiclass default metric without num_class",
    "multiclass metric alias",
    "multiclass metric",
    "non-valid metric for multiclass objective",
    "non-default num_class for default objective",
    "no metric with non-default num_class for custom objective",
    "multiclass metric alias for custom objective",
    "multiclass metric for custom objective",
    "binary metric with non-default num_class for custom objective",
    "Expect three metrics but mean and stdv for each metric",
    "test XGBoost-style return value",
    "test numpy-style return value",
    "test bins string type",
    "test histogram is disabled for categorical features",
    "test for lgb.train",
    "test feval for lgb.train",
    "test with two valid data for lgb.train",
    "test for lgb.cv",
    "test feval for lgb.cv",
    "test that binning works properly for features with only positive or only negative values",
    "decreasing without freeing raw data is allowed",
    "decreasing before lazy init is allowed",
    "increasing is allowed",
    "decreasing with disabled filter is allowed",
    "decreasing with enabled filter is disallowed;",
    "also changes of other params are disallowed",
    "check extra trees increases regularization",
    "check path smoothing increases regularization",
    "test edge case with one leaf",
    "check that constraint containing all features is equivalent to no constraint",
    "check that constraint partitioning the features reduces train accuracy",
    "check that constraints consisting of single features reduce accuracy further",
    "test that interaction constraints work when not all features are used",
    "check that setting linear_tree=True fits better than ordinary trees when data has linear relationship",
    "test again with nans in data",
    "test again with bagging",
    "test with a feature that has only one non-nan value",
    "test with a categorical feature",
    "test refit: same results on same data",
    "test refit with save and load",
    "test refit: different results training on different data",
    "test when num_leaves - 1 < num_features and when num_leaves - 1 > num_features",
    "test that the predict once with all iterations equals summed results with start_iteration and num_iteration",
    "test the case where start_iteration <= 0, and num_iteration is None",
    "test the case where start_iteration > 0, and num_iteration <= 0",
    "test the case where start_iteration > 0, and num_iteration <= 0, with pred_leaf=True",
    "test the case where start_iteration > 0, and num_iteration <= 0, with pred_contrib=True",
    "test for regression",
    "test both with and without early stopping",
    "test for multi-class",
    "test both with and without early stopping",
    "test for binary",
    "test both with and without early stopping",
    "test against sklearn average precision metric",
    "test that average precision is 1 where model predicts perfectly",
    "coding: utf-8",
    "If compiled appropriately, the same installation will support both GPU and CPU.",
    "coding: utf-8",
    "coding: utf-8",
    "convert from one-based to  zero-based index",
    "convert from boundaries to size",
    "--- start Booster interfaces",
    ".Call() calls",
    "coding: utf-8",
    "alias table",
    "names",
    "from strings",
    "tails",
    "tails",
    "coding: utf-8",
    "Single row predictor to abstract away caching logic",
    "create boosting",
    "initialize the boosting",
    "create objective function",
    "initialize the objective function",
    "create training metric",
    "reset the boosting",
    "create objective function",
    "initialize the objective function",
    "calculate the nonzero data and indices size",
    "allocate data and indices arrays",
    "Get the number of trees per iteration (for multiclass scenario we output multiple sparse matrices)",
    "aggregated per row feature contribution results",
    "keep track of the row_vector sizes for parallelization",
    "copy vector results to output for each row",
    "Get the number of trees per iteration (for multiclass scenario we output multiple sparse matrices)",
    "aggregated per row feature contribution results",
    "calculate number of elements per column to construct",
    "the CSC matrix with random access",
    "keep track of column counts",
    "keep track of beginning index for each column",
    "keep track of beginning index for each matrix",
    "Note: we parallelize across matrices instead of rows because of the column_counts[m][col_idx] increment inside the loop",
    "store the row index",
    "update column count",
    "explicitly declare symbols from LightGBM namespace",
    "some help functions used to convert data",
    "Row iterator of on column for CSC matrix",
    "return value at idx, only can access by ascent order",
    "return next non-zero pair, if index < 0, means no more data",
    "start of c_api functions",
    "sample data first",
    "sample data first",
    "sample data first",
    "local buffer to re-use memory",
    "sample data first",
    "no more data",
    "---- start of booster",
    "Single row in row-major format:",
    "---- start of some help functions",
    "data is array of pointers to individual rows",
    "set number of threads for openmp",
    "check for alias",
    "read parameters from config file",
    "remove str after \"#\"",
    "check for alias again",
    "load configs",
    "prediction is needed if using input initial model(continued train)",
    "need to continue training",
    "sync up random seed for data partition",
    "load Training data",
    "load data for distributed training",
    "load data for single machine",
    "need save binary file",
    "create training metric",
    "only when have metrics then need to construct validation data",
    "Add validation data, if it exists",
    "add",
    "need save binary file",
    "add metric for validation data",
    "output used time on each iteration",
    "need init network",
    "create boosting",
    "create objective function",
    "load training data",
    "initialize the objective function",
    "initialize the boosting",
    "add validation data into boosting",
    "convert model to if-else statement code",
    "create predictor",
    "Free memory",
    "create predictor",
    "label_gain = 2^i - 1, may overflow, so we use 31 here",
    "counts for all labels",
    "start from top label, and accumulate DCG",
    "counts for all labels",
    "calculate k Max DCG by one pass",
    "get sorted indices by score",
    "calculate dcg",
    "get sorted indices by score",
    "calculate multi dcg by one pass",
    "wait for all client start up",
    "Don't call MPI_Finalize() here: If the destructor was called because only this node had an exception, calling MPI_Finalize() will cause all nodes to hang.",
    "Instead we will handle finalize/abort for MPI in main().",
    "default set to -1",
    "distance at k-th communication, distance[k] = 2^k",
    "set incoming rank at k-th commuication",
    "set outgoing rank at k-th commuication",
    "defalut set as -1",
    "construct all recursive halving map for all machines",
    "let 1 << k <= num_machines",
    "distance of each communication",
    "if num_machines = 2^k, don't need to group machines",
    "communication direction, %2 == 0 is positive",
    "neighbor at k-th communication",
    "receive data block at k-th communication",
    "send data block at k-th communication",
    "if num_machines != 2^k, need to group machines",
    "group, two machine in one group, total \"rest\" groups will have 2 machines.",
    "let left machine as group leader",
    "cache block information for groups, group with 2 machines will have double block size",
    "convert from group to node leader",
    "convert from node to group",
    "meet new group",
    "add block len for this group",
    "calculate the group block start",
    "not need to construct",
    "get receive block informations",
    "accumulate block len",
    "get send block informations",
    "accumulate block len",
    "static member definition",
    "if small package or small count , do it by all gather.(reduce the communication times.)",
    "assign the blocks to every rank.",
    "do reduce scatter",
    "do all gather",
    "assign blocks",
    "need use buffer here, since size of \"output\" is smaller than size after all gather",
    "copy back",
    "assign blocks",
    "start all gather",
    "when num_machines is small and data is large",
    "use output as receive buffer",
    "get current local block size",
    "get out rank",
    "get in rank",
    "get send information",
    "get recv information",
    "send and recv at same time",
    "rotate in-place",
    "use output as receive buffer",
    "get current local block size",
    "get send information",
    "get recv information",
    "send and recv at same time",
    "use output as receive buffer",
    "send and recv at same time",
    "send local data to neighbor first",
    "receive neighbor data first",
    "reduce",
    "get target",
    "get send information",
    "get recv information",
    "send and recv at same time",
    "reduce",
    "send result to neighbor",
    "receive result from neighbor",
    "copy result",
    "start up socket",
    "parse clients from file",
    "get ip list of local machine",
    "get local rank",
    "construct listener",
    "construct communication topo",
    "construct linkers",
    "free listener",
    "set timeout",
    "accept incoming socket",
    "receive rank",
    "add new socket",
    "save ranks that need to connect with",
    "start listener",
    "start connect",
    "let smaller rank connect to larger rank",
    "send local rank",
    "wait for listener",
    "print connected linkers",
    "only need to copy subset",
    "avoid to copy subset many times",
    "avoid out of range",
    "may need to recopy subset",
    "valid the type",
    "Constructors",
    "Get type tag",
    "Comparisons",
    "This has to be separate, not in Statics, because Json() accesses",
    "statics().null.",
    "advance until next line, or end of input",
    "advance until closing tokens",
    "The usual case: non-escaped characters",
    "Handle escapes",
    "Extract 4-byte escape sequence",
    "Explicitly check length of the substring. The following loop",
    "relies on std::string returning the terminating NUL when",
    "accessing str[length]. Checking here reduces brittleness.",
    "JSON specifies that characters outside the BMP shall be encoded as a",
    "pair of 4-hex-digit \\u escapes encoding their surrogate pair",
    "components. Check whether we're in the middle of such a beast: the",
    "previous codepoint was an escaped lead (high) surrogate, and this is",
    "a trail (low) surrogate.",
    "Reassemble the two surrogate pairs into one astral-plane character,",
    "per the UTF-16 algorithm.",
    "Integer part",
    "Decimal part",
    "Exponent part",
    "Check for any trailing garbage",
    "Documented in json11.hpp",
    "Check for another object",
    "get column names",
    "load label idx first",
    "erase label column name",
    "load ignore columns",
    "load weight idx",
    "load group idx",
    "don't support query id in data file when using distributed training",
    "read data to memory",
    "sample data",
    "construct feature bin mappers",
    "initialize label",
    "extract features",
    "sample data from file",
    "construct feature bin mappers",
    "initialize label",
    "extract features",
    "load data from binary file",
    "check meta data",
    "need to check training data",
    "read data in memory",
    "initialize label",
    "extract features",
    "Get number of lines of data file",
    "initialize label",
    "extract features",
    "load data from binary file",
    "not need to check validation data",
    "check meta data",
    "buffer to read binary file",
    "check token",
    "read size of header",
    "re-allocmate space if not enough",
    "read header",
    "get header",
    "num_groups",
    "real_feature_idx_",
    "feature2group",
    "feature2subfeature",
    "group_bin_boundaries",
    "group_feature_start_",
    "group_feature_cnt_",
    "get feature names",
    "write feature names",
    "get forced_bin_bounds_",
    "read size of meta data",
    "re-allocate space if not enough",
    "read meta data",
    "load meta data",
    "sample local used data if need to partition",
    "if not contain query file, minimal sample unit is one record",
    "if contain query file, minimal sample unit is one query",
    "if is new query",
    "read feature data",
    "read feature size",
    "re-allocate space if not enough",
    "raw data",
    "fill feature_names_ if not header",
    "get forced split",
    "if only one machine, find bin locally",
    "if have multi-machines, need to find bin distributed",
    "different machines will find bin for different features",
    "start and len will store the process feature indices for different machines",
    "machine i will find bins for features in [ start[i], start[i] + len[i] )",
    "free",
    "gather global feature bin mappers",
    "restore features bins from buffer",
    "---- private functions ----",
    "if features are ordered, not need to use hist_buf",
    "read all lines",
    "get query data",
    "if not contain query data, minimal sample unit is one record",
    "if contain query data, minimal sample unit is one query",
    "if is new query",
    "get query data",
    "if not contain query file, minimal sample unit is one record",
    "if contain query file, minimal sample unit is one query",
    "if is new query",
    "parse features",
    "get forced split",
    "check the range of label_idx, weight_idx and group_idx",
    "fill feature_names_ if not header",
    "start find bins",
    "if only one machine, find bin locally",
    "start and len will store the process feature indices for different machines",
    "machine i will find bins for features in [ start[i], start[i] + len[i] )",
    "free",
    "gather global feature bin mappers",
    "restore features bins from buffer",
    "if doesn't need to prediction with initial model",
    "parser",
    "set label",
    "free processed line:",
    "shrink_to_fit will be very slow in linux, and seems not free memory, disable for now",
    "text_reader_->Lines()[i].shrink_to_fit();",
    "push data",
    "if is used feature",
    "if need to prediction with initial model",
    "parser",
    "set initial score",
    "set label",
    "free processed line:",
    "shrink_to_fit will be very slow in linux, and seems not free memory, disable for now",
    "text_reader_->Lines()[i].shrink_to_fit();",
    "push data",
    "if is used feature",
    "metadata_ will manage space of init_score",
    "text data can be free after loaded feature values",
    "parser",
    "set initial score",
    "set label",
    "push data",
    "if is used feature",
    "only need part of data",
    "need full data",
    "metadata_ will manage space of init_score",
    "read size of token",
    "remove duplicates",
    "deep copy function for BinMapper",
    "mean size for one bin",
    "need a new bin",
    "update bin upper bound",
    "last bin upper bound",
    "get list of distinct values",
    "get number of positive and negative distinct values",
    "include zero bounds and infinity bound",
    "add forced bounds, excluding zeros since we have already added zero bounds",
    "find remaining bounds",
    "find distinct_values first",
    "push zero in the front",
    "use the large value",
    "push zero in the back",
    "convert to int type first",
    "sort by counts",
    "will ignore the categorical of small counts",
    "Push the dummy bin for NaN",
    "Use MissingType::None to represent this bin contains all categoricals",
    "fix count of NaN bin",
    "check trivial(num_bin_ == 1) feature",
    "check useless bin",
    "When most_freq_bin_ != default_bin_, there are some additional data loading costs.",
    "so use most_freq_bin_  = default_bin_ when there is not so sparse",
    "calculate max bin of all features to select the int type in MultiValDenseBin",
    "for lambdarank, it needs query data for partition data in distributed learning",
    "need convert query_id to boundaries",
    "check weights",
    "check query boundries",
    "contain initial score file",
    "check weights",
    "get local weights",
    "check query boundries",
    "get local query boundaries",
    "contain initial score file",
    "get local initial scores",
    "re-load query weight",
    "save to nullptr",
    "save to nullptr",
    "save to nullptr",
    "default weight file name",
    "default init_score file name",
    "use first line to count number class",
    "default query file name",
    "root is in the depth 0",
    "non-leaf",
    "leaf",
    "use this for the missing value conversion",
    "Predict func by Map to ifelse",
    "use this for the missing value conversion",
    "non-leaf",
    "left subtree",
    "right subtree",
    "leaf",
    "non-leaf",
    "left subtree",
    "right subtree",
    "leaf",
    "recursive computation of SHAP values for a decision tree",
    "extend the unique path",
    "leaf node",
    "internal node",
    "see if we have already split on this feature,",
    "if so we undo that split so we can redo it for this node",
    "recursive sparse computation of SHAP values for a decision tree",
    "extend the unique path",
    "leaf node",
    "internal node",
    "see if we have already split on this feature,",
    "if so we undo that split so we can redo it for this node",
    "add names of objective function if not providing metric",
    "equal weights for all classes",
    "generate seeds by seed.",
    "sort eval_at",
    "Only push the non-training data",
    "check for conflicts",
    "check if objective, metric, and num_class match",
    "Change pool size to -1 (no limit) when using data parallel to reduce communication costs",
    "Check max_depth and num_leaves",
    "Fits in an int, and is more restrictive than the current num_leaves",
    "force col-wise for gpu & CUDA",
    "force gpu_use_dp for CUDA",
    "linear tree learner must be serial type and run on cpu device",
    "min_data_in_leaf must be at least 2 if path smoothing is active. This is because when the split is calculated",
    "the count is calculated using the proportion of hessian in the leaf which is rounded up to nearest int, so it can",
    "be 1 when there is actually no data in the leaf. In rare cases this can cause a bug because with path smoothing the",
    "calculated split gain can be positive even with zero gradient and hessian.",
    "In distributed mode, local node doesn't have histograms on all features, cannot perform \"intermediate\" monotone constraints.",
    "\"intermediate\" monotone constraints need to recompute splits. If the features are sampled when computing the",
    "split initially, then the sampling needs to be recorded or done once again, which is currently not supported",
    "first round: fill the single val group",
    "always push the last group",
    "put dense feature first",
    "sort by non zero cnt",
    "sort by non zero cnt, bigger first",
    "shuffle groups",
    "Using std::swap for vector<bool> will cause the wrong result.",
    "get num_features",
    "get bin_mappers",
    "for sparse multi value bin, we store the feature bin values with offset added",
    "for dense multi value bin, the feature bin values without offsets are used",
    "copy feature bin mapper data",
    "copy feature bin mapper data",
    "if not pass a filename, just append \".bin\" of original file",
    "get size of header",
    "size of feature names",
    "size of forced bins",
    "write header",
    "write feature names",
    "write forced bins",
    "get size of meta data",
    "write meta data",
    "write feature data",
    "get size of feature",
    "write feature",
    "write raw data; use row-major order so we can read row-by-row",
    "explicitly initialize template methods, for cross module call",
    "Only one multi-val group, just simply merge",
    "Skip the leading 0 when copying group_bin_boundaries.",
    "regenerate other fields",
    "store the importance first",
    "PredictRaw",
    "PredictRawByMap",
    "Predict",
    "PredictByMap",
    "PredictLeafIndex",
    "PredictLeafIndexByMap",
    "output model type",
    "output number of class",
    "output label index",
    "output max_feature_idx",
    "output objective",
    "output tree models",
    "store the importance first",
    "sort the importance",
    "use serialized string to restore this object",
    "Use first 128 chars to avoid exceed the message buffer.",
    "get number of classes",
    "get index of label",
    "get max_feature_idx first",
    "get average_output",
    "get feature names",
    "get monotone_constraints",
    "set zero",
    "predict all the trees for one iteration",
    "check early stopping",
    "set zero",
    "predict all the trees for one iteration",
    "check early stopping",
    "margin_threshold will be captured by value",
    "copy and sort",
    "margin_threshold will be captured by value",
    "Fix for compiler warnings about reaching end of control",
    "load forced_splits file",
    "init tree learner",
    "push training metrics",
    "create buffer for gradients and hessians",
    "get max feature index",
    "get label index",
    "get feature names",
    "if need bagging, create buffer",
    "for a validation dataset, we need its score and metric",
    "update score",
    "objective function will calculate gradients and hessians",
    "random bagging, minimal unit is one record",
    "random bagging, minimal unit is one record",
    "if need bagging",
    "set bagging data to tree learner",
    "get subset",
    "output used time per iteration",
    "boosting from average label; or customized \"average\" if implemented for the current objective",
    "boosting first",
    "bagging logic",
    "need to copy gradients for bagging subset.",
    "shrinkage by learning rate",
    "update score",
    "only add default score one-time",
    "updates scores",
    "add model",
    "reset score",
    "remove model",
    "print message for metric",
    "pop last early_stopping_round_ models",
    "update training score",
    "we need to predict out-of-bag scores of data for boosting",
    "update validation score",
    "print training metric",
    "print validation metric",
    "set zero",
    "predict all the trees for one iteration",
    "predict all the trees for one iteration",
    "push training metrics",
    "not same training data, need reset score and others",
    "create score tracker",
    "update score",
    "create buffer for gradients and hessians",
    "load forced_splits file",
    "if need bagging, create buffer",
    "Get the max size of pool",
    "at least need 2 leaves",
    "push split information for all leaves",
    "initialize splits for leaf",
    "initialize data partition",
    "initialize ordered gradients and hessians",
    "cannot change is_hist_col_wise during training",
    "initialize splits for leaf",
    "initialize data partition",
    "initialize ordered gradients and hessians",
    "Get the max size of pool",
    "at least need 2 leaves",
    "push split information for all leaves",
    "some initial works before training",
    "root leaf",
    "only root leaf can be splitted on first time",
    "some initial works before finding best split",
    "find best threshold for every feature",
    "Get a leaf with max split gain",
    "Get split information for best leaf",
    "cannot split, quit",
    "split tree with best leaf",
    "reset histogram pool",
    "initialize data partition",
    "reset the splits for leaves",
    "Sumup for root",
    "use all data",
    "use bagging, only use part of data",
    "check depth of current leaf",
    "only need to check left leaf, since right leaf is in same level of left leaf",
    "no enough data to continue",
    "only have root",
    "put parent(left) leaf's histograms into larger leaf's histograms",
    "put parent(left) leaf's histograms to larger leaf's histograms",
    "construct smaller leaf",
    "construct larger leaf",
    "find splits",
    "only has root leaf",
    "start at root leaf",
    "before processing next node from queue, store info for current left/right leaf",
    "store \"best split\" for left and right, even if they might be overwritten by forced split",
    "then, compute own splits",
    "split info should exist because searching in bfs fashion - should have added from parent",
    "update before tree split",
    "don't need to update this in data-based parallel model",
    "split tree, will return right leaf",
    "store the true split gain in tree model",
    "don't need to update this in data-based parallel model",
    "store the true split gain in tree model",
    "init the leaves that used on next iteration",
    "update leave outputs if needed",
    "bag_mapper[index_mapper[i]]",
    "it is needed to filter the features after the above code.",
    "Otherwise, the `is_splittable` in `FeatureHistogram` will be wrong, and cause some features being accidentally filtered in the later nodes.",
    "for root leaf the \"parent\" output is its own output because we don't apply any smoothing to the root",
    "can't use GetParentOutput because leaf_splits doesn't have weight property set",
    "find splits",
    "identify features containing nans",
    "preallocate the matrix used to calculate linear model coefficients",
    "store only upper triangular half of matrix as an array, in row-major order",
    "this requires (max_num_feat + 1) * (max_num_feat + 2) / 2 entries (including the constant terms of the regression)",
    "we add another 8 to ensure cache lines are not shared among processors",
    "some initial works before training",
    "root leaf",
    "only root leaf can be splitted on first time",
    "some initial works before finding best split",
    "find best threshold for every feature",
    "Get a leaf with max split gain",
    "Get split information for best leaf",
    "cannot split, quit",
    "split tree with best leaf",
    "map data to leaf number",
    "calculate coefficients using the method described in Eq 3 of https://arxiv.org/pdf/1802.05640.pdf",
    "the coefficients vector is given by",
    "- (X_T * H * X + lambda) ^ (-1) * (X_T * g)",
    "where:",
    "X is the matrix where the first column is the feature values and the second is all ones,",
    "H is the diagonal matrix of the hessian,",
    "lambda is the diagonal matrix with diagonal entries equal to the regularisation term linear_lambda",
    "g is the vector of gradients",
    "the subscript _T denotes the transpose",
    "create array of pointers to raw data, and coefficient matrices, for each leaf",
    "clear the coefficient matrices",
    "aggregate results from different threads",
    "copy into eigen matrices and solve",
    "update the tree properties",
    "need to be able to hold smaller and larger best splits in SyncUpGlobalBestSplit",
    "get feature partition",
    "get local used features",
    "get best split at smaller leaf",
    "find local best split for larger leaf",
    "sync global best info",
    "update best split",
    "instantiate template classes, otherwise linker cannot find the code",
    "initialize SerialTreeLearner",
    "Get local rank and global machine size",
    "need to be able to hold smaller and larger best splits in SyncUpGlobalBestSplit",
    "allocate buffer for communication",
    "generate feature partition for current tree",
    "get local used feature",
    "get block start and block len for reduce scatter",
    "get buffer_write_start_pos_",
    "get buffer_read_start_pos_",
    "sync global data sumup info",
    "global sumup reduce",
    "copy back",
    "set global sumup info",
    "init global data count in leaf",
    "construct local histograms",
    "copy to buffer",
    "Reduce scatter for histogram",
    "restore global histograms from buffer",
    "only root leaf",
    "construct histgroms for large leaf, we init larger leaf as the parent, so we can just subtract the smaller leaf's histograms",
    "find local best split for larger leaf",
    "sync global best info",
    "set best split",
    "need update global number of data in leaf",
    "instantiate template classes, otherwise linker cannot find the code",
    "initialize SerialTreeLearner",
    "some additional variables needed for GPU trainer",
    "Initialize GPU buffers and kernels",
    "some functions used for debugging the GPU histogram construction",
    "printf(\"grad %g != %g (%d ULPs)\\n\", GET_GRAD(h1, i), GET_GRAD(h2, i), ulps);",
    "goto err;",
    "we roughly want 256 workgroups per device, and we have num_dense_feature4_ feature tuples.",
    "also guarantee that there are at least 2K examples per workgroup",
    "return 0;",
    "we have already copied ordered gradients, ordered hessians and indices to GPU",
    "decide the best number of workgroups working on one feature4 tuple",
    "set work group size based on feature size",
    "each 2^exp_workgroups_per_feature workgroups work on a feature4 tuple",
    "we need to refresh the kernel arguments after reallocating",
    "The only argument that needs to be changed later is num_data_",
    "the GPU kernel will process all features in one call, and each",
    "2^exp_workgroups_per_feature (compile time constant) workgroup will",
    "process one feature4 tuple",
    "for the root node, indices are not copied",
    "for constant hessian, hessians are not copied except for the root node",
    "there will be 2^exp_workgroups_per_feature = num_workgroups / num_dense_feature4 sub-histogram per feature4",
    "and we will launch num_feature workgroups for this kernel",
    "will launch threads for all features",
    "the queue should be asynchrounous, and we will can WaitAndGetHistograms() before we start processing dense feature groups",
    "copy the results asynchronously. Size depends on if double precision is used",
    "we will wait for this object in WaitAndGetHistograms",
    "when the output is ready, the computation is done",
    "values of this feature has been redistributed to multiple bins; need a reduction here",
    "how many feature-group tuples we have",
    "leave some safe margin for prefetching",
    "256 work-items per workgroup. Each work-item prefetches one tuple for that feature",
    "clear sparse/dense maps",
    "do nothing if no features can be processed on GPU",
    "allocate memory for all features (FIXME: 4 GB barrier on some devices, need to split to multiple buffers)",
    "unpin old buffer if necessary before destructing them",
    "make ordered_gradients and hessians larger (including extra room for prefetching), and pin them",
    "allocate space for gradients and hessians on device",
    "we will copy gradients and hessians in after ordered_gradients_ and ordered_hessians_ are constructed",
    "allocate feature mask, for disabling some feature-groups' histogram calculation",
    "copy indices to the device",
    "histogram bin entry size depends on the precision (single/double)",
    "create output buffer, each feature has a histogram with device_bin_size_ bins,",
    "each work group generates a sub-histogram of dword_features_ features.",
    "only initialize once here, as this will not need to change when ResetTrainingData() is called",
    "create atomic counters for inter-group coordination",
    "The output buffer is allocated to host directly, to overlap compute and data transfer",
    "find the dense feature-groups and group then into Feature4 data structure (several feature-groups packed into 4 bytes)",
    "looking for dword_features_ non-sparse feature-groups",
    "decide if we need to redistribute the bin",
    "multiplier must be a power of 2",
    "device_bin_mults_.push_back(1);",
    "found",
    "for data transfer time",
    "Now generate new data structure feature4, and copy data to the device",
    "preallocate arrays for all threads, and pin them",
    "building Feature4 bundles; each thread handles dword_features_ features",
    "one feature datapoint is 4 bits",
    "this guarantees that the RawGet() function is inlined, rather than using virtual function dispatching",
    "one feature datapoint is one byte",
    "this guarantees that the RawGet() function is inlined, rather than using virtual function dispatching",
    "Dense bin",
    "Dense 4-bit bin",
    "working on the remaining (less than dword_features_) feature groups",
    "fill the leftover features",
    "fill this empty feature with some \"random\" value",
    "fill this empty feature with some \"random\" value",
    "copying the last 1 to (dword_features - 1) feature-groups in the last tuple",
    "deallocate pinned space for feature copying",
    "data transfer time",
    "for other types of failure, build log might not be available; program.build_log() can crash",
    "Something bad happened. Just return \"No log available.\"",
    "build is okay, log may contain warnings",
    "destroy any old kernels",
    "create OpenCL kernels for different number of workgroups per feature",
    "currently we don't use constant memory",
    "compile the GPU kernel depending if double precision is used, constant hessian is used, etc.",
    "kernel with indices in an array",
    "kernel with all features enabled, with elimited branches",
    "kernel with all data indices (for root node, and assumes that root node always uses all features)",
    "do nothing if no features can be processed on GPU",
    "The only argument that needs to be changed later is num_data_",
    "hessian is passed as a parameter, but it is not available now.",
    "hessian will be set in BeforeTrain()",
    "Get the max bin size, used for selecting best GPU kernel",
    "initialize GPU",
    "determine which kernel to use based on the max number of bins",
    "the +9 skips extra characters \")\", newline, \"#endif\" and newline at the beginning",
    "the +9 skips extra characters \")\", newline, \"#endif\" and newline at the beginning",
    "the +9 skips extra characters \")\", newline, \"#endif\" and newline at the beginning",
    "setup GPU kernel arguments after we allocating all the buffers",
    "GPU memory has to been reallocated because data may have been changed",
    "setup GPU kernel arguments after we allocating all the buffers",
    "Copy initial full hessians and gradients to GPU.",
    "We start copying as early as possible, instead of at ConstructHistogram().",
    "setup hessian parameters only",
    "hessian is passed as a parameter",
    "use bagging",
    "On GPU, we start copying indices, gradients and hessians now, instead at ConstructHistogram()",
    "copy used gradients and hessians to ordered buffer",
    "transfer the indices to GPU",
    "transfer hessian to GPU",
    "setup hessian parameters only",
    "hessian is passed as a parameter",
    "transfer gradients to GPU",
    "only have root",
    "Copy indices, gradients and hessians as early as possible",
    "only need to initialize for smaller leaf",
    "Get leaf boundary",
    "copy indices to the GPU:",
    "copy ordered hessians to the GPU:",
    "copy ordered gradients to the GPU:",
    "do nothing if no features can be processed on GPU",
    "copy data indices if it is not null",
    "generate and copy ordered_gradients if gradients is not null",
    "generate and copy ordered_hessians if hessians is not null",
    "converted indices in is_feature_used to feature-group indices",
    "construct the feature masks for dense feature-groups",
    "if no feature group is used, just return and do not use GPU",
    "if not all feature groups are used, we need to transfer the feature mask to GPU",
    "otherwise, we will use a specialized GPU kernel with all feature groups enabled",
    "All data have been prepared, now run the GPU kernel",
    "construct smaller leaf",
    "ConstructGPUHistogramsAsync will return true if there are availabe feature gourps dispatched to GPU",
    "then construct sparse features on CPU",
    "wait for GPU to finish, only if GPU is actually used",
    "use double precision",
    "use single precision",
    "Compare GPU histogram with CPU histogram, useful for debuggin GPU code problem",
    "#define GPU_DEBUG_COMPARE",
    "construct larger leaf",
    "then construct sparse features on CPU",
    "wait for GPU to finish, only if GPU is actually used",
    "use double precision",
    "use single precision",
    "do some sanity check for the GPU algorithm",
    "limit top k",
    "get max bin",
    "calculate buffer size",
    "need to be able to hold smaller and larger best splits in SyncUpGlobalBestSplit",
    "left and right on same time, so need double size",
    "initialize histograms for global",
    "sync global data sumup info",
    "set global sumup info",
    "init global data count in leaf",
    "get local sumup",
    "get local sumup",
    "get mean number on machines",
    "weighted gain",
    "get top k",
    "Copy histogram to buffer, and Get local aggregate features",
    "copy histograms.",
    "copy smaller leaf histograms first",
    "mark local aggregated feature",
    "copy",
    "then copy larger leaf histograms",
    "mark local aggregated feature",
    "copy",
    "use local data to find local best splits",
    "find splits",
    "only has root leaf",
    "local voting",
    "gather",
    "get all top-k from all machines",
    "global voting",
    "copy local histgrams to buffer",
    "Reduce scatter for histogram",
    "find best split from local aggregated histograms",
    "restore from buffer",
    "restore from buffer",
    "find local best",
    "find local best split for larger leaf",
    "sync global best info",
    "copy back",
    "set the global number of data for leaves",
    "init the global sumup info",
    "instantiate template classes, otherwise linker cannot find the code",
    "launch cuda kernel",
    "initialize SerialTreeLearner",
    "some additional variables needed for GPU trainer",
    "Initialize GPU buffers and kernels: get device info",
    "some functions used for debugging the GPU histogram construction",
    "we roughly want 256 workgroups per device, and we have num_dense_feature4_ feature tuples.",
    "also guarantee that there are at least 2K examples per workgroup",
    "we have already copied ordered gradients, ordered hessians and indices to GPU",
    "decide the best number of workgroups working on one feature4 tuple",
    "set work group size based on feature size",
    "each 2^exp_workgroups_per_feature workgroups work on a feature4 tuple",
    "set thread_data",
    "copy the results asynchronously. Size depends on if double precision is used",
    "when the output is ready, the computation is done",
    "how many feature-group tuples we have",
    "leave some safe margin for prefetching",
    "256 work-items per workgroup. Each work-item prefetches one tuple for that feature",
    "clear sparse/dense maps",
    "do nothing it there is no dense feature",
    "calculate number of feature groups per gpu",
    "histogram bin entry size depends on the precision (single/double)",
    "allocate GPU memory for each GPU",
    "do nothing it there is no gpu feature",
    "allocate memory for all features",
    "allocate space for gradients and hessians on device",
    "we will copy gradients and hessians in after ordered_gradients_ and ordered_hessians_ are constructed",
    "copy indices to the device",
    "create output buffer, each feature has a histogram with device_bin_size_ bins,",
    "each work group generates a sub-histogram of dword_features_ features.",
    "only initialize once here, as this will not need to change when ResetTrainingData() is called",
    "create atomic counters for inter-group coordination",
    "The output buffer is allocated to host directly, to overlap compute and data transfer",
    "clear sparse/dense maps",
    "find the dense feature-groups and group then into Feature4 data structure (several feature-groups packed into 4 bytes)",
    "set device info",
    "looking for dword_features_ non-sparse feature-groups",
    "reset device info",
    "InitGPU w/ num_gpu",
    "Get the max bin size, used for selecting best GPU kernel",
    "get num_dense_feature_groups_",
    "initialize GPU",
    "set cpu threads",
    "resize device memory pointers",
    "create stream & events to handle multiple GPUs",
    "check data size",
    "GPU memory has to been reallocated because data may have been changed",
    "AllocateGPUMemory only when the number of data increased",
    "setup GPU kernel arguments after we allocating all the buffers",
    "Copy initial full hessians and gradients to GPU.",
    "We start copying as early as possible, instead of at ConstructHistogram().",
    "use bagging",
    "On GPU, we start copying indices, gradients and hessians now, instead at ConstructHistogram()",
    "copy used gradients and hessians to ordered buffer",
    "transfer the indices to GPU",
    "only have root",
    "Copy indices, gradients and hessians as early as possible",
    "only need to initialize for smaller leaf",
    "Get leaf boundary",
    "do nothing if no features can be processed on GPU",
    "copy data indices if it is not null",
    "converted indices in is_feature_used to feature-group indices",
    "construct the feature masks for dense feature-groups",
    "if no feature group is used, just return and do not use GPU",
    "if not all feature groups are used, we need to transfer the feature mask to GPU",
    "otherwise, we will use a specialized GPU kernel with all feature groups enabled",
    "We now copy even if all features are used.",
    "All data have been prepared, now run the GPU kernel",
    "construct smaller leaf",
    "Check workgroups per feature4 tuple..",
    "if the workgroup per feature is 1 (2^0), return as the work is too small for a GPU",
    "ConstructGPUHistogramsAsync will return true if there are availabe feature groups dispatched to GPU",
    "then construct sparse features on CPU",
    "We set data_indices to null to avoid rebuilding ordered gradients/hessians",
    "wait for GPU to finish, only if GPU is actually used",
    "use double precision",
    "use single precision",
    "Compare GPU histogram with CPU histogram, useful for debuggin GPU code problem",
    "#define CUDA_DEBUG_COMPARE",
    "construct larger leaf",
    "then construct sparse features on CPU",
    "We set data_indices to null to avoid rebuilding ordered gradients/hessians",
    "wait for GPU to finish, only if GPU is actually used",
    "use double precision",
    "use single precision",
    "do some sanity check for the GPU algorithm"
  ],
  "v3.2.0": [
    "coding: utf-8",
    "see https://github.com/pypa/distutils/pull/21",
    "coding: utf-8",
    "create predictor first",
    "check dataset",
    "reduce cost for prediction training data",
    "process callbacks",
    "Most of legacy advanced options becomes callbacks",
    "construct booster",
    "start training",
    "check evaluation result.",
    "ranking task, split according to groups",
    "run preprocessing on the data set if needed",
    "setup callbacks",
    "coding: utf-8",
    "dummy function to support older version of scikit-learn",
    "coding: utf-8",
    "documentation templates for LGBMModel methods are shared between the classes in",
    "this module and those in the ``dask`` module",
    "user can set verbose with kwargs, it has higher priority",
    "Do not modify original args in fit function",
    "Refer to https://github.com/microsoft/LightGBM/pull/2619",
    "Separate built-in from callable evaluation metrics",
    "register default metric for consistency with callable eval_metric case",
    "try to deduce from class instance",
    "overwrite default metric by explicitly set metric",
    "concatenate metric from params (or default if not provided in params) and eval_metric",
    "copy for consistency",
    "reduce cost for prediction training data",
    "free dataset",
    "Switch to using a multiclass objective in the underlying LGBM instance",
    "do not modify args, as it causes errors in model selection tools",
    "check group data",
    "coding: utf-8",
    "we don't need lib_lightgbm while building docs",
    "coding: utf-8",
    "\"#ddffdd\" is light green, \"#ffdddd\" is light red",
    "coding: utf-8",
    "coding: utf-8",
    "TypeError: obj is not a string or a number",
    "ValueError: invalid literal",
    "DeprecationWarning is not shown by default, so let's create our own with higher level",
    "avoid side effects on passed-in parameters",
    "find a value, and remove other aliases with .pop()",
    "prefer the value of 'main_param_name' if it exists, otherwise search the aliases",
    "__get_num_preds() cannot work with nrow > MAX_INT32, so calculate overall number of predictions piecemeal",
    "avoid memory consumption by arrays concatenation operations",
    "create numpy array from output arrays",
    "break up indptr based on number of rows (note more than one matrix in multiclass case)",
    "for CSC there is extra column added",
    "reformat output into a csr or csc matrix or list of csr or csc matrices",
    "same shape as input csr or csc matrix except extra column for expected value",
    "note: make sure we copy data as it will be deallocated next",
    "free the temporary native indptr, indices, and data",
    "__get_num_preds() cannot work with nrow > MAX_INT32, so calculate overall number of predictions piecemeal",
    "avoid memory consumption by arrays concatenation operations",
    "no min_data, nthreads and verbose in this function",
    "check data has header or not",
    "need to regroup init_score",
    "process for args",
    "user can set verbose with params, it has higher priority",
    "get categorical features",
    "process for reference dataset",
    "start construct data",
    "set feature names",
    "create valid",
    "construct subset",
    "create train",
    "could be updated if data is not freed",
    "set to None",
    "we're done if self and reference share a common upstrem reference",
    "group data from LightGBM is boundaries data, need to convert to group size",
    "user can set verbose with params, it has higher priority",
    "Training task",
    "if \"machines\" is given, assume user wants to do distributed learning, and set up network",
    "construct booster object",
    "copy the parameters from train_set",
    "save reference to data",
    "buffer for inner predict",
    "Prediction task",
    "if a single node tree it won't have `leaf_index` so return 0",
    "Create the node record, and populate universal data members",
    "Update values to reflect node type (leaf or split)",
    "traverse the next level of the tree",
    "In tree format, \"subtree_list\" is a list of node records (dicts),",
    "and we add node to the list.",
    "need reset training data",
    "need to push new valid data",
    "if buffer length is not long enough, re-allocate a buffer",
    "if buffer length is not long enough, reallocate a buffer",
    "Copy models",
    "Get name of features",
    "avoid to predict many time in one iteration",
    "Get num of inner evals",
    "Get name of evals",
    "coding: utf-8",
    "Callback environment used by callbacks",
    "split is needed for \"<dataset type> <metric>\" case (e.g. \"train l1\")",
    "split is needed for \"<dataset type> <metric>\" case (e.g. \"train l1\")",
    "coding: utf-8",
    "Concatenate many parts into one",
    "capture whether local_listen_port or its aliases were provided",
    "capture whether machines or its aliases were provided",
    "Some passed-in parameters can be removed:",
    "* 'num_machines': set automatically from Dask worker list",
    "* 'num_threads': overridden to match nthreads on each Dask process",
    "Split arrays/dataframes into parts. Arrange parts into dicts to enforce co-locality",
    "Start computation in the background",
    "Find locations of all parts and map them to particular Dask workers",
    "resolve aliases for network parameters and pop the result off params.",
    "these values are added back in calls to `_train_part()`",
    "figure out network params",
    "Tell each worker to train on the parts that it has locally",
    "if network parameters were changed during training, remove them from the",
    "returned moodel so that they're generated dynamically on every run based",
    "on the Dask cluster you're connected to and which workers have pieces of",
    "the training data",
    "dask.DataFrame.map_partitions() expects each call to return a pandas DataFrame or Series",
    "the note on custom objective functions in LGBMModel.__init__ is not",
    "currently relevant for the Dask estimators",
    "DaskLGBMClassifier does not support evaluation data, or early stopping",
    "DaskLGBMClassifier support for callbacks and init_model is not tested",
    "the note on custom objective functions in LGBMModel.__init__ is not",
    "currently relevant for the Dask estimators",
    "DaskLGBMRegressor does not support evaluation data, or early stopping",
    "DaskLGBMRegressor support for callbacks and init_model is not tested",
    "the note on custom objective functions in LGBMModel.__init__ is not",
    "currently relevant for the Dask estimators",
    "DaskLGBMRanker does not support evaluation data, or early stopping",
    "DaskLGBMRanker support for callbacks and init_model is not tested",
    "coding: utf-8",
    "load or create your dataset",
    "create dataset for lightgbm",
    "if you want to re-use data, remember to set free_raw_data=False",
    "specify your configurations as a dict",
    "generate feature names",
    "feature_name and categorical_feature",
    "check feature name",
    "save model to file",
    "dump model to JSON (and save to file)",
    "feature names",
    "feature importances",
    "load model to predict",
    "can only predict with the best iteration (or the saving iteration)",
    "eval with loaded model",
    "dump model with pickle",
    "load model with pickle to predict",
    "can predict with any iteration when loaded in pickle way",
    "eval with loaded model",
    "continue training",
    "init_model accepts:",
    "1. model file name",
    "2. Booster()",
    "decay learning rates",
    "learning_rates accepts:",
    "1. list/tuple with length = num_boost_round",
    "2. function(curr_iter)",
    "change other parameters during training",
    "self-defined objective function",
    "f(preds: array, train_data: Dataset) -> grad: array, hess: array",
    "log likelihood loss",
    "self-defined eval metric",
    "f(preds: array, train_data: Dataset) -> name: string, eval_result: float, is_higher_better: bool",
    "binary error",
    "NOTE: when you do customized loss function, the default prediction value is margin",
    "This may make built-in evalution metric calculate wrong results",
    "For example, we are doing log likelihood loss, the prediction is score before logistic transformation",
    "Keep this in mind when you use the customization",
    "another self-defined eval metric",
    "f(preds: array, train_data: Dataset) -> name: string, eval_result: float, is_higher_better: bool",
    "accuracy",
    "NOTE: when you do customized loss function, the default prediction value is margin",
    "This may make built-in evalution metric calculate wrong results",
    "For example, we are doing log likelihood loss, the prediction is score before logistic transformation",
    "Keep this in mind when you use the customization",
    "callback",
    "coding: utf-8",
    "load or create your dataset",
    "train",
    "predict",
    "eval",
    "feature importances",
    "self-defined eval metric",
    "f(y_true: array, y_pred: array) -> name: string, eval_result: float, is_higher_better: bool",
    "Root Mean Squared Logarithmic Error (RMSLE)",
    "train",
    "another self-defined eval metric",
    "f(y_true: array, y_pred: array) -> name: string, eval_result: float, is_higher_better: bool",
    "Relative Absolute Error (RAE)",
    "train",
    "predict",
    "eval",
    "other scikit-learn modules",
    "coding: utf-8",
    "load or create your dataset",
    "create dataset for lightgbm",
    "specify your configurations as a dict",
    "train",
    "coding: utf-8",
    "################",
    "Simulate some binary data with a single categorical and",
    "single continuous predictor",
    "################",
    "Set up a couple of utilities for our experiments",
    "################",
    "Observe the behavior of `binary` and `xentropy` objectives",
    "Trying this throws an error on non-binary values of y:",
    "experiment('binary', label_type='probability', DATA)",
    "The speed of `binary` is not drastically different than",
    "`xentropy`. `xentropy` runs faster than `binary` in many cases, although",
    "there are reasons to suspect that `binary` should run faster when the",
    "label is an integer instead of a float",
    "coding: utf-8",
    "load or create your dataset",
    "create dataset for lightgbm",
    "specify your configurations as a dict",
    "train",
    "save model to file",
    "predict",
    "eval",
    "split training data into two partitions",
    "make this array dense because we're splitting across",
    "a sparse boundary to partition the data",
    "the code below uses sklearn.metrics, but this requires pulling all of the",
    "predictions and target values back from workers to the client",
    "",
    "for larger datasets, consider the metrics from dask-ml instead",
    "https://ml.dask.org/modules/api.html#dask-ml-metrics-metrics",
    "coding: utf-8",
    "!/usr/bin/env python3",
    "-*- coding: utf-8 -*-",
    "",
    "LightGBM documentation build configuration file, created by",
    "sphinx-quickstart on Thu May  4 14:30:58 2017.",
    "",
    "This file is execfile()d with the current directory set to its",
    "containing dir.",
    "",
    "Note that not all possible configuration values are present in this",
    "autogenerated file.",
    "",
    "All configuration values have a default; values that are commented out",
    "serve to show the default.",
    "If extensions (or modules to document with autodoc) are in another directory,",
    "add these directories to sys.path here. If the directory is relative to the",
    "documentation root, use os.path.abspath to make it absolute.",
    "-- mock out modules",
    "-- General configuration ------------------------------------------------",
    "If your documentation needs a minimal Sphinx version, state it here.",
    "Add any Sphinx extension module names here, as strings. They can be",
    "extensions coming with Sphinx (named 'sphinx.ext.*') or your custom",
    "ones.",
    "hide type hints in API docs",
    "Generate autosummary pages. Output should be set with: `:toctree: pythonapi/`",
    "Only the class' docstring is inserted.",
    "If true, `todo` and `todoList` produce output, else they produce nothing.",
    "The master toctree document.",
    "General information about the project.",
    "The name of an image file (relative to this directory) to place at the top",
    "of the sidebar.",
    "The name of an image file (relative to this directory) to use as a favicon of",
    "the docs.  This file should be a Windows icon file (.ico) being 16x16 or 32x32",
    "pixels large.",
    "The version info for the project you're documenting, acts as replacement for",
    "|version| and |release|, also used in various other places throughout the",
    "built documents.",
    "The short X.Y version.",
    "The full version, including alpha/beta/rc tags.",
    "The language for content autogenerated by Sphinx. Refer to documentation",
    "for a list of supported languages.",
    "",
    "This is also used if you do content translation via gettext catalogs.",
    "Usually you set \"language\" from the command line for these cases.",
    "List of patterns, relative to source directory, that match files and",
    "directories to ignore when looking for source files.",
    "This patterns also effect to html_static_path and html_extra_path",
    "The name of the Pygments (syntax highlighting) style to use.",
    "-- Configuration for C API docs generation ------------------------------",
    "-- Options for HTML output ----------------------------------------------",
    "The theme to use for HTML and HTML Help pages.  See the documentation for",
    "a list of builtin themes.",
    "Theme options are theme-specific and customize the look and feel of a theme",
    "further.  For a list of options available for each theme, see the",
    "documentation.",
    "Add any paths that contain custom static files (such as style sheets) here,",
    "relative to this directory. They are copied after the builtin static files,",
    "so a file named \"default.css\" will overwrite the builtin \"default.css\".",
    "-- Options for HTMLHelp output ------------------------------------------",
    "Output file base name for HTML help builder.",
    "-- Options for LaTeX output ---------------------------------------------",
    "The name of an image file (relative to this directory) to place at the top of",
    "the title page.",
    "Warning! The following code can cause buffer overflows on RTD.",
    "Consider suppressing output completely if RTD project silently fails.",
    "Refer to https://github.com/svenevs/exhale",
    "/blob/fe7644829057af622e467bb529db6c03a830da99/exhale/deploy.py#L99-L111",
    "Warning! The following code can cause buffer overflows on RTD.",
    "Consider suppressing output completely if RTD project silently fails.",
    "Refer to https://github.com/svenevs/exhale",
    "/blob/fe7644829057af622e467bb529db6c03a830da99/exhale/deploy.py#L99-L111",
    "coding: utf-8",
    "Constants",
    "Start with some content:",
    "Clear & re-use:",
    "Output should match new content:",
    "Number of trials for each new ChunkedArray configuration. Pass 100 times over the search space:",
    "Each outer loop iteration changes the test by adding +1 chunk. We start with 1 chunk only:",
    "Sweep valid and invalid addresses with a ChunkedArray with `chunks` chunks:",
    "Compute a new trial address & value & if it is a valid address:",
    "Insert item. If at a valid address, 0 is returned, otherwise, -1 is returned:",
    "If at valid address, check that the stored value is correct & remember it for the future:",
    "Check the just-stored value with getitem():",
    "Also store the just-stored value for future tracking:",
    "Final check: ensure even with overrides, all valid insertions store the latest value at that address:",
    "coding: utf-8",
    "we don't need lib_lightgbm while building docs",
    "coding: utf-8",
    "check saved model persistence",
    "we need to check the consistency of model file here, so test for exact equal",
    "check early stopping is working. Make it stop very early, so the scores should be very close to zero",
    "scores likely to be different, but prediction should still be the same",
    "test that shape is checked during prediction",
    "test that method works even with free_raw_data=True",
    "test that method works but sets raw data to None in case of immergeable data types",
    "test that method works for different data types",
    "Set extremely harsh penalties, so CEGB will block most splits.",
    "Compare pairs of penalties, to ensure scaling works as intended",
    "Reset booster1's parameters to p2, so the parameter section of the file matches.",
    "should resolve duplicate aliases, and prefer the main parameter",
    "should choose a value from an alias and set that value on main param",
    "if only an alias is used",
    "should use the default if main param and aliases are missing",
    "all changes should be made on copies and not modify the original",
    "coding: utf-8",
    "time, in seconds, to wait for the Dask client to close. Used to avoid teardown errors",
    "see https://distributed.dask.org/en/latest/api.html#distributed.Client.close",
    "add target, weight, and group to DataFrame so that partitions abide by group boundaries.",
    "set_index ensures partitions are based on group id.",
    "See https://stackoverflow.com/questions/49532824/dask-dataframe-split-partitions-based-on-a-column-or-function.",
    "separate target, weight from features.",
    "encode group identifiers into run-length encoding, the format LightGBMRanker is expecting",
    "so that within each partition, sum(g) = n_samples.",
    "ranking arrays: one chunk per group. Each chunk must include all columns.",
    "for the small data sizes used in tests, it's hard to get LGBMRegressor to choose",
    "categorical features for splits. So for regression tests with categorical features,",
    "_create_data() returns a DataFrame with ONLY categorical features",
    "pref_leaf values should have the right shape",
    "and values that look like valid tree nodes",
    "be sure LightGBM actually used at least one categorical column,",
    "and that it was correctly treated as a categorical feature",
    "be sure LightGBM actually used at least one categorical column,",
    "and that it was correctly treated as a categorical feature",
    "shape depends on whether it is binary or multiclass classification",
    "* shape depends on whether it is binary or multiclass classification",
    "* matrix for binary classification is of the form [feature_contrib, base_value],",
    "for multi-class it's [feat_contrib_class1, base_value_class1, feat_contrib_class2, base_value_class2, etc.]",
    "* contrib outputs for distributed training are different than from local training, so we can just test",
    "that the output has the right shape and base values are in the right position",
    "check that found ports are different for same address (LocalCluster)",
    "check that the ports are indeed open",
    "Scores should be the same",
    "Predictions should be roughly the same.",
    "pref_leaf values should have the right shape",
    "and values that look like valid tree nodes",
    "The checks below are skipped",
    "for the categorical data case because it's difficult to get",
    "a good fit from just categoricals for a regression problem",
    "with small data",
    "be sure LightGBM actually used at least one categorical column,",
    "and that it was correctly treated as a categorical feature",
    "contrib outputs for distributed training are different than from local training, so we can just test",
    "that the output has the right shape and base values are in the right position",
    "be sure LightGBM actually used at least one categorical column,",
    "and that it was correctly treated as a categorical feature",
    "Quantiles should be right",
    "be sure LightGBM actually used at least one categorical column,",
    "and that it was correctly treated as a categorical feature",
    "rebalance small dask.Array dataset for better performance.",
    "use many trees + leaves to overfit, help ensure that Dask data-parallel strategy matches that of",
    "serial learner. See https://github.com/microsoft/LightGBM/issues/3292#issuecomment-671288210.",
    "distributed ranker should be able to rank decently well and should",
    "have high rank correlation with scores from serial ranker.",
    "pref_leaf values should have the right shape",
    "and values that look like valid tree nodes",
    "be sure LightGBM actually used at least one categorical column,",
    "and that it was correctly treated as a categorical feature",
    "should be able to use the class without specifying a client",
    "should be able to set client after construction",
    "data on cluster1",
    "create identical data on cluster2",
    "at this point, the result of default_client() is client2 since it was the most recently",
    "created. So setting client to client1 here to test that you can select a non-default client",
    "unfitted model should survive pickling round trip, and pickling",
    "shouldn't have side effects on the model object",
    "client will always be None after unpickling",
    "fitted model should survive pickling round trip, and pickling",
    "shouldn't have side effects on the model object",
    "client will always be None after unpickling",
    "rebalance data to be sure that each worker has a piece of the data",
    "model 1 - no network parameters given",
    "model 2 - machines given",
    "model 3 - local_listen_port given",
    "training should fail because LightGBM will try to use the same",
    "port for multiple worker processes on the same machine",
    "rebalance data to be sure that each worker has a piece of the data",
    "test that \"machines\" is actually respected by creating a socket that uses",
    "one of the ports mentioned in \"machines\"",
    "an informative error should be raised if \"machines\" has duplicates",
    "\"client\" should be the only different, and the final argument",
    "init_scores must be a 1D array, even for multiclass classification",
    "where you need to provide 1 score per class for each row in X",
    "https://github.com/microsoft/LightGBM/issues/4046",
    "value of the root node is 0 when init_score is set",
    "this test is separate because it takes a not-yet-constructed estimator",
    "coding: utf-8",
    "coding: utf-8",
    "build target, group ID vectors.",
    "build y/target and group-id vectors with user-specified group sizes.",
    "build y/target and group-id vectors according to n_samples, avg_gs, and random_gs.",
    "groups should contain > 1 element for pairwise learning objective.",
    "build feature data, X. Transform first few into informative features.",
    "coding: utf-8",
    "prediction result is actually not transformed (is raw) due to custom objective",
    "sklearn <0.23 does not have a stacking classifier and n_features_in_ property",
    "sklearn <0.23 does not have a stacking regressor and n_features_in_ property",
    "sklearn < 0.22 does not have the post fit attribute: classes_",
    "sklearn < 0.23 does not have as_frame parameter",
    "sklearn < 0.22 does not have the post fit attribute: classes_",
    "sklearn < 0.23 does not have as_frame parameter",
    "Test if random_state is properly stored",
    "Test if two random states produce identical models",
    "Test if subsequent fits sample from random_state object and produce different models",
    "Test that the largest element is NOT the same, the smallest can be the same, i.e. zero",
    "With default params",
    "Tests same probabilities",
    "Tests same predictions",
    "Tests same raw scores",
    "Tests same leaf indices",
    "Tests same feature contributions",
    "Tests other parameters for the prediction works",
    "Tests start_iteration",
    "Tests same probabilities, starting from iteration 10",
    "Tests same predictions, starting from iteration 10",
    "Tests same raw scores, starting from iteration 10",
    "Tests same leaf indices, starting from iteration 10",
    "Tests same feature contributions, starting from iteration 10",
    "Tests other parameters for the prediction works, starting from iteration 10",
    "no custom objective, no custom metric",
    "default metric",
    "non-default metric",
    "no metric",
    "non-default metric in eval_metric",
    "non-default metric with non-default metric in eval_metric",
    "non-default metric with multiple metrics in eval_metric",
    "non-default metric with multiple metrics in eval_metric for LGBMClassifier",
    "default metric for non-default objective",
    "non-default metric for non-default objective",
    "no metric",
    "non-default metric in eval_metric for non-default objective",
    "non-default metric with non-default metric in eval_metric for non-default objective",
    "non-default metric with multiple metrics in eval_metric for non-default objective",
    "custom objective, no custom metric",
    "default regression metric for custom objective",
    "non-default regression metric for custom objective",
    "multiple regression metrics for custom objective",
    "no metric",
    "default regression metric with non-default metric in eval_metric for custom objective",
    "non-default regression metric with metric in eval_metric for custom objective",
    "multiple regression metrics with metric in eval_metric for custom objective",
    "multiple regression metrics with multiple metrics in eval_metric for custom objective",
    "no custom objective, custom metric",
    "default metric with custom metric",
    "non-default metric with custom metric",
    "multiple metrics with custom metric",
    "custom metric (disable default metric)",
    "default metric for non-default objective with custom metric",
    "non-default metric for non-default objective with custom metric",
    "multiple metrics for non-default objective with custom metric",
    "custom metric (disable default metric for non-default objective)",
    "custom objective, custom metric",
    "custom metric for custom objective",
    "non-default regression metric with custom metric for custom objective",
    "multiple regression metrics with custom metric for custom objective",
    "default metric and invalid binary metric is replaced with multiclass alternative",
    "invalid objective is replaced with default multiclass one",
    "and invalid binary metric is replaced with multiclass alternative",
    "default metric for non-default multiclass objective",
    "and invalid binary metric is replaced with multiclass alternative",
    "default metric and invalid multiclass metric is replaced with binary alternative",
    "invalid multiclass metric is replaced with binary alternative for custom objective",
    "Verify that can receive a list of metrics, only callable",
    "Verify that can receive a list of custom and built-in metrics",
    "Verify that works as expected when eval_metric is empty",
    "Verify that can receive a list of metrics, only built-in",
    "Verify that eval_metric is robust to receiving a list with None",
    "training data as eval_set",
    "feval",
    "single eval_set",
    "two eval_set",
    "sklearn < 0.22 requires passing \"attributes\" argument",
    "Test that estimators are default-constructible",
    "coding: utf-8",
    "coding: utf-8",
    "check that default gives same result as k = 1",
    "check against independent calculation for k = 1",
    "check against independent calculation for k = 2",
    "check against independent calculation for k = 10",
    "check cases where predictions are equal",
    "should give same result as binary auc for 2 classes",
    "test the case where all predictions are equal",
    "test that weighted data gives different auc_mu",
    "test that equal data weights give same auc_mu as unweighted data",
    "should give 1 when accuracy = 1",
    "test loading class weights",
    "no early stopping",
    "early stopping occurs",
    "test custom eval metrics",
    "shuffle = False, override metric in params",
    "shuffle = True, callbacks",
    "enable display training loss",
    "self defined folds",
    "lambdarank",
    "... with l2 metric",
    "... with NDCG (default) metric",
    "self defined folds with lambdarank",
    "with early stopping",
    "predict by each fold booster",
    "fold averaging",
    "without early stopping",
    "test feature_names with whitespaces",
    "This has non-ascii strings.",
    "take subsets and train",
    "generate CSR sparse dataset",
    "convert data to dense and get back same contribs",
    "validate the values are the same",
    "validate using CSC matrix",
    "validate the values are the same",
    "generate CSR sparse dataset",
    "convert data to dense and get back same contribs",
    "validate the values are the same",
    "validate using CSC matrix",
    "validate the values are the same",
    "Note there is an extra column added to the output for the expected value",
    "Note output CSC shape should be same as CSR output shape",
    "test sliced labels",
    "append some columns",
    "append some rows",
    "test sliced 2d matrix",
    "test sliced CSR",
    "test if a penalty as high as the depth indeed prohibits all monotone splits",
    "The penalization is so high that the first 2 features should not be used here",
    "Check that a very high penalization is the same as not using the features at all",
    "no fobj, no feval",
    "default metric",
    "non-default metric in params",
    "default metric in args",
    "non-default metric in args",
    "metric in args overwrites one in params",
    "multiple metrics in params",
    "multiple metrics in args",
    "remove default metric by 'None' in list",
    "remove default metric by 'None' aliases",
    "fobj, no feval",
    "no default metric",
    "metric in params",
    "metric in args",
    "metric in args overwrites its' alias in params",
    "multiple metrics in params",
    "multiple metrics in args",
    "no fobj, feval",
    "default metric with custom one",
    "non-default metric in params with custom one",
    "default metric in args with custom one",
    "non-default metric in args with custom one",
    "metric in args overwrites one in params, custom one is evaluated too",
    "multiple metrics in params with custom one",
    "multiple metrics in args with custom one",
    "custom metric is evaluated despite 'None' is passed",
    "fobj, feval",
    "no default metric, only custom one",
    "metric in params with custom one",
    "metric in args with custom one",
    "metric in args overwrites one in params, custom one is evaluated too",
    "multiple metrics in params with custom one",
    "multiple metrics in args with custom one",
    "custom metric is evaluated despite 'None' is passed",
    "no fobj, no feval",
    "default metric",
    "default metric in params",
    "non-default metric in params",
    "multiple metrics in params",
    "remove default metric by 'None' aliases",
    "fobj, no feval",
    "no default metric",
    "metric in params",
    "multiple metrics in params",
    "no fobj, feval",
    "default metric with custom one",
    "default metric in params with custom one",
    "non-default metric in params with custom one",
    "multiple metrics in params with custom one",
    "custom metric is evaluated despite 'None' is passed",
    "fobj, feval",
    "no default metric, only custom one",
    "metric in params with custom one",
    "multiple metrics in params with custom one",
    "custom metric is evaluated despite 'None' is passed",
    "multiclass default metric",
    "multiclass default metric with custom one",
    "multiclass metric alias with custom one for custom objective",
    "no metric for invalid class_num",
    "custom metric for invalid class_num",
    "multiclass metric alias with custom one with invalid class_num",
    "multiclass default metric without num_class",
    "multiclass metric alias",
    "multiclass metric",
    "non-valid metric for multiclass objective",
    "non-default num_class for default objective",
    "no metric with non-default num_class for custom objective",
    "multiclass metric alias for custom objective",
    "multiclass metric for custom objective",
    "binary metric with non-default num_class for custom objective",
    "Expect three metrics but mean and stdv for each metric",
    "test XGBoost-style return value",
    "test numpy-style return value",
    "test bins string type",
    "test histogram is disabled for categorical features",
    "test for lgb.train",
    "test feval for lgb.train",
    "test with two valid data for lgb.train",
    "test for lgb.cv",
    "test feval for lgb.cv",
    "test that binning works properly for features with only positive or only negative values",
    "decreasing without freeing raw data is allowed",
    "decreasing before lazy init is allowed",
    "increasing is allowed",
    "decreasing with disabled filter is allowed",
    "decreasing with enabled filter is disallowed;",
    "also changes of other params are disallowed",
    "check extra trees increases regularization",
    "check path smoothing increases regularization",
    "test edge case with one leaf",
    "check that constraint containing all features is equivalent to no constraint",
    "check that constraint partitioning the features reduces train accuracy",
    "check that constraints consisting of single features reduce accuracy further",
    "test that interaction constraints work when not all features are used",
    "check that setting linear_tree=True fits better than ordinary trees when data has linear relationship",
    "test again with nans in data",
    "test again with bagging",
    "test with a feature that has only one non-nan value",
    "test with a categorical feature",
    "test refit: same results on same data",
    "test refit with save and load",
    "test refit: different results training on different data",
    "test when num_leaves - 1 < num_features and when num_leaves - 1 > num_features",
    "test that the predict once with all iterations equals summed results with start_iteration and num_iteration",
    "test the case where start_iteration <= 0, and num_iteration is None",
    "test the case where start_iteration > 0, and num_iteration <= 0",
    "test the case where start_iteration > 0, and num_iteration <= 0, with pred_leaf=True",
    "test the case where start_iteration > 0, and num_iteration <= 0, with pred_contrib=True",
    "test for regression",
    "test both with and without early stopping",
    "test for multi-class",
    "test both with and without early stopping",
    "test for binary",
    "test both with and without early stopping",
    "test against sklearn average precision metric",
    "test that average precision is 1 where model predicts perfectly",
    "coding: utf-8",
    "If compiled appropriately, the same installation will support both GPU and CPU.",
    "coding: utf-8",
    "coding: utf-8",
    "convert from one-based to  zero-based index",
    "convert from boundaries to size",
    "--- start Booster interfaces",
    ".Call() calls",
    "coding: utf-8",
    "alias table",
    "names",
    "from strings",
    "tails",
    "tails",
    "coding: utf-8",
    "Single row predictor to abstract away caching logic",
    "create boosting",
    "initialize the boosting",
    "create objective function",
    "initialize the objective function",
    "create training metric",
    "reset the boosting",
    "create objective function",
    "initialize the objective function",
    "calculate the nonzero data and indices size",
    "allocate data and indices arrays",
    "Get the number of trees per iteration (for multiclass scenario we output multiple sparse matrices)",
    "aggregated per row feature contribution results",
    "keep track of the row_vector sizes for parallelization",
    "copy vector results to output for each row",
    "Get the number of trees per iteration (for multiclass scenario we output multiple sparse matrices)",
    "aggregated per row feature contribution results",
    "calculate number of elements per column to construct",
    "the CSC matrix with random access",
    "keep track of column counts",
    "keep track of beginning index for each column",
    "keep track of beginning index for each matrix",
    "Note: we parallelize across matrices instead of rows because of the column_counts[m][col_idx] increment inside the loop",
    "store the row index",
    "update column count",
    "explicitly declare symbols from LightGBM namespace",
    "some help functions used to convert data",
    "Row iterator of on column for CSC matrix",
    "return value at idx, only can access by ascent order",
    "return next non-zero pair, if index < 0, means no more data",
    "start of c_api functions",
    "sample data first",
    "sample data first",
    "sample data first",
    "local buffer to re-use memory",
    "sample data first",
    "no more data",
    "---- start of booster",
    "Single row in row-major format:",
    "---- start of some help functions",
    "data is array of pointers to individual rows",
    "set number of threads for openmp",
    "check for alias",
    "read parameters from config file",
    "remove str after \"#\"",
    "check for alias again",
    "load configs",
    "prediction is needed if using input initial model(continued train)",
    "need to continue training",
    "sync up random seed for data partition",
    "load Training data",
    "load data for distributed training",
    "load data for single machine",
    "need save binary file",
    "create training metric",
    "only when have metrics then need to construct validation data",
    "Add validation data, if it exists",
    "add",
    "need save binary file",
    "add metric for validation data",
    "output used time on each iteration",
    "need init network",
    "create boosting",
    "create objective function",
    "load training data",
    "initialize the objective function",
    "initialize the boosting",
    "add validation data into boosting",
    "convert model to if-else statement code",
    "create predictor",
    "Free memory",
    "create predictor",
    "label_gain = 2^i - 1, may overflow, so we use 31 here",
    "counts for all labels",
    "start from top label, and accumulate DCG",
    "counts for all labels",
    "calculate k Max DCG by one pass",
    "get sorted indices by score",
    "calculate dcg",
    "get sorted indices by score",
    "calculate multi dcg by one pass",
    "wait for all client start up",
    "Don't call MPI_Finalize() here: If the destructor was called because only this node had an exception, calling MPI_Finalize() will cause all nodes to hang.",
    "Instead we will handle finalize/abort for MPI in main().",
    "default set to -1",
    "distance at k-th communication, distance[k] = 2^k",
    "set incoming rank at k-th commuication",
    "set outgoing rank at k-th commuication",
    "defalut set as -1",
    "construct all recursive halving map for all machines",
    "let 1 << k <= num_machines",
    "distance of each communication",
    "if num_machines = 2^k, don't need to group machines",
    "communication direction, %2 == 0 is positive",
    "neighbor at k-th communication",
    "receive data block at k-th communication",
    "send data block at k-th communication",
    "if num_machines != 2^k, need to group machines",
    "group, two machine in one group, total \"rest\" groups will have 2 machines.",
    "let left machine as group leader",
    "cache block information for groups, group with 2 machines will have double block size",
    "convert from group to node leader",
    "convert from node to group",
    "meet new group",
    "add block len for this group",
    "calculate the group block start",
    "not need to construct",
    "get receive block informations",
    "accumulate block len",
    "get send block informations",
    "accumulate block len",
    "static member definition",
    "if small package or small count , do it by all gather.(reduce the communication times.)",
    "assign the blocks to every rank.",
    "do reduce scatter",
    "do all gather",
    "assign blocks",
    "need use buffer here, since size of \"output\" is smaller than size after all gather",
    "copy back",
    "assign blocks",
    "start all gather",
    "when num_machines is small and data is large",
    "use output as receive buffer",
    "get current local block size",
    "get out rank",
    "get in rank",
    "get send information",
    "get recv information",
    "send and recv at same time",
    "rotate in-place",
    "use output as receive buffer",
    "get current local block size",
    "get send information",
    "get recv information",
    "send and recv at same time",
    "use output as receive buffer",
    "send and recv at same time",
    "send local data to neighbor first",
    "receive neighbor data first",
    "reduce",
    "get target",
    "get send information",
    "get recv information",
    "send and recv at same time",
    "reduce",
    "send result to neighbor",
    "receive result from neighbor",
    "copy result",
    "start up socket",
    "parse clients from file",
    "get ip list of local machine",
    "get local rank",
    "construct listener",
    "construct communication topo",
    "construct linkers",
    "free listener",
    "set timeout",
    "accept incoming socket",
    "receive rank",
    "add new socket",
    "save ranks that need to connect with",
    "start listener",
    "start connect",
    "let smaller rank connect to larger rank",
    "send local rank",
    "wait for listener",
    "print connected linkers",
    "only need to copy subset",
    "avoid to copy subset many times",
    "avoid out of range",
    "may need to recopy subset",
    "valid the type",
    "Constructors",
    "Get type tag",
    "Comparisons",
    "This has to be separate, not in Statics, because Json() accesses",
    "statics().null.",
    "advance until next line, or end of input",
    "advance until closing tokens",
    "The usual case: non-escaped characters",
    "Handle escapes",
    "Extract 4-byte escape sequence",
    "Explicitly check length of the substring. The following loop",
    "relies on std::string returning the terminating NUL when",
    "accessing str[length]. Checking here reduces brittleness.",
    "JSON specifies that characters outside the BMP shall be encoded as a",
    "pair of 4-hex-digit \\u escapes encoding their surrogate pair",
    "components. Check whether we're in the middle of such a beast: the",
    "previous codepoint was an escaped lead (high) surrogate, and this is",
    "a trail (low) surrogate.",
    "Reassemble the two surrogate pairs into one astral-plane character,",
    "per the UTF-16 algorithm.",
    "Integer part",
    "Decimal part",
    "Exponent part",
    "Check for any trailing garbage",
    "Documented in json11.hpp",
    "Check for another object",
    "get column names",
    "load label idx first",
    "erase label column name",
    "load ignore columns",
    "load weight idx",
    "load group idx",
    "don't support query id in data file when using distributed training",
    "read data to memory",
    "sample data",
    "construct feature bin mappers",
    "initialize label",
    "extract features",
    "sample data from file",
    "construct feature bin mappers",
    "initialize label",
    "extract features",
    "load data from binary file",
    "check meta data",
    "need to check training data",
    "read data in memory",
    "initialize label",
    "extract features",
    "Get number of lines of data file",
    "initialize label",
    "extract features",
    "load data from binary file",
    "not need to check validation data",
    "check meta data",
    "buffer to read binary file",
    "check token",
    "read size of header",
    "re-allocmate space if not enough",
    "read header",
    "get header",
    "num_groups",
    "real_feature_idx_",
    "feature2group",
    "feature2subfeature",
    "group_bin_boundaries",
    "group_feature_start_",
    "group_feature_cnt_",
    "get feature names",
    "write feature names",
    "get forced_bin_bounds_",
    "read size of meta data",
    "re-allocate space if not enough",
    "read meta data",
    "load meta data",
    "sample local used data if need to partition",
    "if not contain query file, minimal sample unit is one record",
    "if contain query file, minimal sample unit is one query",
    "if is new query",
    "read feature data",
    "read feature size",
    "re-allocate space if not enough",
    "raw data",
    "fill feature_names_ if not header",
    "get forced split",
    "if only one machine, find bin locally",
    "if have multi-machines, need to find bin distributed",
    "different machines will find bin for different features",
    "start and len will store the process feature indices for different machines",
    "machine i will find bins for features in [ start[i], start[i] + len[i] )",
    "free",
    "gather global feature bin mappers",
    "restore features bins from buffer",
    "---- private functions ----",
    "if features are ordered, not need to use hist_buf",
    "read all lines",
    "get query data",
    "if not contain query data, minimal sample unit is one record",
    "if contain query data, minimal sample unit is one query",
    "if is new query",
    "get query data",
    "if not contain query file, minimal sample unit is one record",
    "if contain query file, minimal sample unit is one query",
    "if is new query",
    "parse features",
    "get forced split",
    "check the range of label_idx, weight_idx and group_idx",
    "fill feature_names_ if not header",
    "start find bins",
    "if only one machine, find bin locally",
    "start and len will store the process feature indices for different machines",
    "machine i will find bins for features in [ start[i], start[i] + len[i] )",
    "free",
    "gather global feature bin mappers",
    "restore features bins from buffer",
    "if doesn't need to prediction with initial model",
    "parser",
    "set label",
    "free processed line:",
    "shrink_to_fit will be very slow in linux, and seems not free memory, disable for now",
    "text_reader_->Lines()[i].shrink_to_fit();",
    "push data",
    "if is used feature",
    "if need to prediction with initial model",
    "parser",
    "set initial score",
    "set label",
    "free processed line:",
    "shrink_to_fit will be very slow in linux, and seems not free memory, disable for now",
    "text_reader_->Lines()[i].shrink_to_fit();",
    "push data",
    "if is used feature",
    "metadata_ will manage space of init_score",
    "text data can be free after loaded feature values",
    "parser",
    "set initial score",
    "set label",
    "push data",
    "if is used feature",
    "only need part of data",
    "need full data",
    "metadata_ will manage space of init_score",
    "read size of token",
    "remove duplicates",
    "deep copy function for BinMapper",
    "mean size for one bin",
    "need a new bin",
    "update bin upper bound",
    "last bin upper bound",
    "get list of distinct values",
    "get number of positive and negative distinct values",
    "include zero bounds and infinity bound",
    "add forced bounds, excluding zeros since we have already added zero bounds",
    "find remaining bounds",
    "find distinct_values first",
    "push zero in the front",
    "use the large value",
    "push zero in the back",
    "convert to int type first",
    "sort by counts",
    "will ignore the categorical of small counts",
    "Push the dummy bin for NaN",
    "Use MissingType::None to represent this bin contains all categoricals",
    "fix count of NaN bin",
    "check trivial(num_bin_ == 1) feature",
    "check useless bin",
    "When most_freq_bin_ != default_bin_, there are some additional data loading costs.",
    "so use most_freq_bin_  = default_bin_ when there is not so sparse",
    "calculate max bin of all features to select the int type in MultiValDenseBin",
    "for lambdarank, it needs query data for partition data in distributed learning",
    "need convert query_id to boundaries",
    "check weights",
    "check query boundries",
    "contain initial score file",
    "check weights",
    "get local weights",
    "check query boundries",
    "get local query boundaries",
    "contain initial score file",
    "get local initial scores",
    "re-load query weight",
    "save to nullptr",
    "save to nullptr",
    "save to nullptr",
    "default weight file name",
    "default init_score file name",
    "use first line to count number class",
    "default query file name",
    "root is in the depth 0",
    "non-leaf",
    "leaf",
    "use this for the missing value conversion",
    "Predict func by Map to ifelse",
    "use this for the missing value conversion",
    "non-leaf",
    "left subtree",
    "right subtree",
    "leaf",
    "non-leaf",
    "left subtree",
    "right subtree",
    "leaf",
    "recursive computation of SHAP values for a decision tree",
    "extend the unique path",
    "leaf node",
    "internal node",
    "see if we have already split on this feature,",
    "if so we undo that split so we can redo it for this node",
    "recursive sparse computation of SHAP values for a decision tree",
    "extend the unique path",
    "leaf node",
    "internal node",
    "see if we have already split on this feature,",
    "if so we undo that split so we can redo it for this node",
    "add names of objective function if not providing metric",
    "equal weights for all classes",
    "generate seeds by seed.",
    "sort eval_at",
    "Only push the non-training data",
    "check for conflicts",
    "check if objective, metric, and num_class match",
    "Change pool size to -1 (no limit) when using data parallel to reduce communication costs",
    "Check max_depth and num_leaves",
    "Fits in an int, and is more restrictive than the current num_leaves",
    "force col-wise for gpu & CUDA",
    "force gpu_use_dp for CUDA",
    "linear tree learner must be serial type and run on cpu device",
    "min_data_in_leaf must be at least 2 if path smoothing is active. This is because when the split is calculated",
    "the count is calculated using the proportion of hessian in the leaf which is rounded up to nearest int, so it can",
    "be 1 when there is actually no data in the leaf. In rare cases this can cause a bug because with path smoothing the",
    "calculated split gain can be positive even with zero gradient and hessian.",
    "In distributed mode, local node doesn't have histograms on all features, cannot perform \"intermediate\" monotone constraints.",
    "\"intermediate\" monotone constraints need to recompute splits. If the features are sampled when computing the",
    "split initially, then the sampling needs to be recorded or done once again, which is currently not supported",
    "first round: fill the single val group",
    "always push the last group",
    "put dense feature first",
    "sort by non zero cnt",
    "sort by non zero cnt, bigger first",
    "shuffle groups",
    "Using std::swap for vector<bool> will cause the wrong result.",
    "get num_features",
    "get bin_mappers",
    "for sparse multi value bin, we store the feature bin values with offset added",
    "for dense multi value bin, the feature bin values without offsets are used",
    "copy feature bin mapper data",
    "copy feature bin mapper data",
    "if not pass a filename, just append \".bin\" of original file",
    "get size of header",
    "size of feature names",
    "size of forced bins",
    "write header",
    "write feature names",
    "write forced bins",
    "get size of meta data",
    "write meta data",
    "write feature data",
    "get size of feature",
    "write feature",
    "write raw data; use row-major order so we can read row-by-row",
    "explicitly initialize template methods, for cross module call",
    "Only one multi-val group, just simply merge",
    "Skip the leading 0 when copying group_bin_boundaries.",
    "regenerate other fields",
    "store the importance first",
    "PredictRaw",
    "PredictRawByMap",
    "Predict",
    "PredictByMap",
    "PredictLeafIndex",
    "PredictLeafIndexByMap",
    "output model type",
    "output number of class",
    "output label index",
    "output max_feature_idx",
    "output objective",
    "output tree models",
    "store the importance first",
    "sort the importance",
    "use serialized string to restore this object",
    "Use first 128 chars to avoid exceed the message buffer.",
    "get number of classes",
    "get index of label",
    "get max_feature_idx first",
    "get average_output",
    "get feature names",
    "get monotone_constraints",
    "set zero",
    "predict all the trees for one iteration",
    "check early stopping",
    "set zero",
    "predict all the trees for one iteration",
    "check early stopping",
    "margin_threshold will be captured by value",
    "copy and sort",
    "margin_threshold will be captured by value",
    "Fix for compiler warnings about reaching end of control",
    "load forced_splits file",
    "init tree learner",
    "push training metrics",
    "create buffer for gradients and hessians",
    "get max feature index",
    "get label index",
    "get feature names",
    "if need bagging, create buffer",
    "for a validation dataset, we need its score and metric",
    "update score",
    "objective function will calculate gradients and hessians",
    "random bagging, minimal unit is one record",
    "random bagging, minimal unit is one record",
    "if need bagging",
    "set bagging data to tree learner",
    "get subset",
    "output used time per iteration",
    "boosting from average label; or customized \"average\" if implemented for the current objective",
    "boosting first",
    "bagging logic",
    "need to copy gradients for bagging subset.",
    "shrinkage by learning rate",
    "update score",
    "only add default score one-time",
    "updates scores",
    "add model",
    "reset score",
    "remove model",
    "print message for metric",
    "pop last early_stopping_round_ models",
    "update training score",
    "we need to predict out-of-bag scores of data for boosting",
    "update validation score",
    "print training metric",
    "print validation metric",
    "set zero",
    "predict all the trees for one iteration",
    "predict all the trees for one iteration",
    "push training metrics",
    "not same training data, need reset score and others",
    "create score tracker",
    "update score",
    "create buffer for gradients and hessians",
    "load forced_splits file",
    "if need bagging, create buffer",
    "Get the max size of pool",
    "at least need 2 leaves",
    "push split information for all leaves",
    "initialize splits for leaf",
    "initialize data partition",
    "initialize ordered gradients and hessians",
    "cannot change is_hist_col_wise during training",
    "initialize splits for leaf",
    "initialize data partition",
    "initialize ordered gradients and hessians",
    "Get the max size of pool",
    "at least need 2 leaves",
    "push split information for all leaves",
    "some initial works before training",
    "root leaf",
    "only root leaf can be splitted on first time",
    "some initial works before finding best split",
    "find best threshold for every feature",
    "Get a leaf with max split gain",
    "Get split information for best leaf",
    "cannot split, quit",
    "split tree with best leaf",
    "reset histogram pool",
    "initialize data partition",
    "reset the splits for leaves",
    "Sumup for root",
    "use all data",
    "use bagging, only use part of data",
    "check depth of current leaf",
    "only need to check left leaf, since right leaf is in same level of left leaf",
    "no enough data to continue",
    "only have root",
    "put parent(left) leaf's histograms into larger leaf's histograms",
    "put parent(left) leaf's histograms to larger leaf's histograms",
    "construct smaller leaf",
    "construct larger leaf",
    "find splits",
    "only has root leaf",
    "start at root leaf",
    "before processing next node from queue, store info for current left/right leaf",
    "store \"best split\" for left and right, even if they might be overwritten by forced split",
    "then, compute own splits",
    "split info should exist because searching in bfs fashion - should have added from parent",
    "update before tree split",
    "don't need to update this in data-based parallel model",
    "split tree, will return right leaf",
    "store the true split gain in tree model",
    "don't need to update this in data-based parallel model",
    "store the true split gain in tree model",
    "init the leaves that used on next iteration",
    "update leave outputs if needed",
    "bag_mapper[index_mapper[i]]",
    "it is needed to filter the features after the above code.",
    "Otherwise, the `is_splittable` in `FeatureHistogram` will be wrong, and cause some features being accidentally filtered in the later nodes.",
    "for root leaf the \"parent\" output is its own output because we don't apply any smoothing to the root",
    "can't use GetParentOutput because leaf_splits doesn't have weight property set",
    "find splits",
    "identify features containing nans",
    "preallocate the matrix used to calculate linear model coefficients",
    "store only upper triangular half of matrix as an array, in row-major order",
    "this requires (max_num_feat + 1) * (max_num_feat + 2) / 2 entries (including the constant terms of the regression)",
    "we add another 8 to ensure cache lines are not shared among processors",
    "some initial works before training",
    "root leaf",
    "only root leaf can be splitted on first time",
    "some initial works before finding best split",
    "find best threshold for every feature",
    "Get a leaf with max split gain",
    "Get split information for best leaf",
    "cannot split, quit",
    "split tree with best leaf",
    "map data to leaf number",
    "calculate coefficients using the method described in Eq 3 of https://arxiv.org/pdf/1802.05640.pdf",
    "the coefficients vector is given by",
    "- (X_T * H * X + lambda) ^ (-1) * (X_T * g)",
    "where:",
    "X is the matrix where the first column is the feature values and the second is all ones,",
    "H is the diagonal matrix of the hessian,",
    "lambda is the diagonal matrix with diagonal entries equal to the regularisation term linear_lambda",
    "g is the vector of gradients",
    "the subscript _T denotes the transpose",
    "create array of pointers to raw data, and coefficient matrices, for each leaf",
    "clear the coefficient matrices",
    "aggregate results from different threads",
    "copy into eigen matrices and solve",
    "update the tree properties",
    "need to be able to hold smaller and larger best splits in SyncUpGlobalBestSplit",
    "get feature partition",
    "get local used features",
    "get best split at smaller leaf",
    "find local best split for larger leaf",
    "sync global best info",
    "update best split",
    "instantiate template classes, otherwise linker cannot find the code",
    "initialize SerialTreeLearner",
    "Get local rank and global machine size",
    "need to be able to hold smaller and larger best splits in SyncUpGlobalBestSplit",
    "allocate buffer for communication",
    "generate feature partition for current tree",
    "get local used feature",
    "get block start and block len for reduce scatter",
    "get buffer_write_start_pos_",
    "get buffer_read_start_pos_",
    "sync global data sumup info",
    "global sumup reduce",
    "copy back",
    "set global sumup info",
    "init global data count in leaf",
    "construct local histograms",
    "copy to buffer",
    "Reduce scatter for histogram",
    "restore global histograms from buffer",
    "only root leaf",
    "construct histgroms for large leaf, we init larger leaf as the parent, so we can just subtract the smaller leaf's histograms",
    "find local best split for larger leaf",
    "sync global best info",
    "set best split",
    "need update global number of data in leaf",
    "instantiate template classes, otherwise linker cannot find the code",
    "initialize SerialTreeLearner",
    "some additional variables needed for GPU trainer",
    "Initialize GPU buffers and kernels",
    "some functions used for debugging the GPU histogram construction",
    "printf(\"grad %g != %g (%d ULPs)\\n\", GET_GRAD(h1, i), GET_GRAD(h2, i), ulps);",
    "goto err;",
    "we roughly want 256 workgroups per device, and we have num_dense_feature4_ feature tuples.",
    "also guarantee that there are at least 2K examples per workgroup",
    "return 0;",
    "we have already copied ordered gradients, ordered hessians and indices to GPU",
    "decide the best number of workgroups working on one feature4 tuple",
    "set work group size based on feature size",
    "each 2^exp_workgroups_per_feature workgroups work on a feature4 tuple",
    "we need to refresh the kernel arguments after reallocating",
    "The only argument that needs to be changed later is num_data_",
    "the GPU kernel will process all features in one call, and each",
    "2^exp_workgroups_per_feature (compile time constant) workgroup will",
    "process one feature4 tuple",
    "for the root node, indices are not copied",
    "for constant hessian, hessians are not copied except for the root node",
    "there will be 2^exp_workgroups_per_feature = num_workgroups / num_dense_feature4 sub-histogram per feature4",
    "and we will launch num_feature workgroups for this kernel",
    "will launch threads for all features",
    "the queue should be asynchrounous, and we will can WaitAndGetHistograms() before we start processing dense feature groups",
    "copy the results asynchronously. Size depends on if double precision is used",
    "we will wait for this object in WaitAndGetHistograms",
    "when the output is ready, the computation is done",
    "values of this feature has been redistributed to multiple bins; need a reduction here",
    "how many feature-group tuples we have",
    "leave some safe margin for prefetching",
    "256 work-items per workgroup. Each work-item prefetches one tuple for that feature",
    "clear sparse/dense maps",
    "do nothing if no features can be processed on GPU",
    "allocate memory for all features (FIXME: 4 GB barrier on some devices, need to split to multiple buffers)",
    "unpin old buffer if necessary before destructing them",
    "make ordered_gradients and hessians larger (including extra room for prefetching), and pin them",
    "allocate space for gradients and hessians on device",
    "we will copy gradients and hessians in after ordered_gradients_ and ordered_hessians_ are constructed",
    "allocate feature mask, for disabling some feature-groups' histogram calculation",
    "copy indices to the device",
    "histogram bin entry size depends on the precision (single/double)",
    "create output buffer, each feature has a histogram with device_bin_size_ bins,",
    "each work group generates a sub-histogram of dword_features_ features.",
    "only initialize once here, as this will not need to change when ResetTrainingData() is called",
    "create atomic counters for inter-group coordination",
    "The output buffer is allocated to host directly, to overlap compute and data transfer",
    "find the dense feature-groups and group then into Feature4 data structure (several feature-groups packed into 4 bytes)",
    "looking for dword_features_ non-sparse feature-groups",
    "decide if we need to redistribute the bin",
    "multiplier must be a power of 2",
    "device_bin_mults_.push_back(1);",
    "found",
    "for data transfer time",
    "Now generate new data structure feature4, and copy data to the device",
    "preallocate arrays for all threads, and pin them",
    "building Feature4 bundles; each thread handles dword_features_ features",
    "one feature datapoint is 4 bits",
    "this guarantees that the RawGet() function is inlined, rather than using virtual function dispatching",
    "one feature datapoint is one byte",
    "this guarantees that the RawGet() function is inlined, rather than using virtual function dispatching",
    "Dense bin",
    "Dense 4-bit bin",
    "working on the remaining (less than dword_features_) feature groups",
    "fill the leftover features",
    "fill this empty feature with some \"random\" value",
    "fill this empty feature with some \"random\" value",
    "copying the last 1 to (dword_features - 1) feature-groups in the last tuple",
    "deallocate pinned space for feature copying",
    "data transfer time",
    "for other types of failure, build log might not be available; program.build_log() can crash",
    "Something bad happened. Just return \"No log available.\"",
    "build is okay, log may contain warnings",
    "destroy any old kernels",
    "create OpenCL kernels for different number of workgroups per feature",
    "currently we don't use constant memory",
    "compile the GPU kernel depending if double precision is used, constant hessian is used, etc.",
    "kernel with indices in an array",
    "kernel with all features enabled, with elimited branches",
    "kernel with all data indices (for root node, and assumes that root node always uses all features)",
    "do nothing if no features can be processed on GPU",
    "The only argument that needs to be changed later is num_data_",
    "hessian is passed as a parameter, but it is not available now.",
    "hessian will be set in BeforeTrain()",
    "Get the max bin size, used for selecting best GPU kernel",
    "initialize GPU",
    "determine which kernel to use based on the max number of bins",
    "the +9 skips extra characters \")\", newline, \"#endif\" and newline at the beginning",
    "the +9 skips extra characters \")\", newline, \"#endif\" and newline at the beginning",
    "the +9 skips extra characters \")\", newline, \"#endif\" and newline at the beginning",
    "setup GPU kernel arguments after we allocating all the buffers",
    "GPU memory has to been reallocated because data may have been changed",
    "setup GPU kernel arguments after we allocating all the buffers",
    "Copy initial full hessians and gradients to GPU.",
    "We start copying as early as possible, instead of at ConstructHistogram().",
    "setup hessian parameters only",
    "hessian is passed as a parameter",
    "use bagging",
    "On GPU, we start copying indices, gradients and hessians now, instead at ConstructHistogram()",
    "copy used gradients and hessians to ordered buffer",
    "transfer the indices to GPU",
    "transfer hessian to GPU",
    "setup hessian parameters only",
    "hessian is passed as a parameter",
    "transfer gradients to GPU",
    "only have root",
    "Copy indices, gradients and hessians as early as possible",
    "only need to initialize for smaller leaf",
    "Get leaf boundary",
    "copy indices to the GPU:",
    "copy ordered hessians to the GPU:",
    "copy ordered gradients to the GPU:",
    "do nothing if no features can be processed on GPU",
    "copy data indices if it is not null",
    "generate and copy ordered_gradients if gradients is not null",
    "generate and copy ordered_hessians if hessians is not null",
    "converted indices in is_feature_used to feature-group indices",
    "construct the feature masks for dense feature-groups",
    "if no feature group is used, just return and do not use GPU",
    "if not all feature groups are used, we need to transfer the feature mask to GPU",
    "otherwise, we will use a specialized GPU kernel with all feature groups enabled",
    "All data have been prepared, now run the GPU kernel",
    "construct smaller leaf",
    "ConstructGPUHistogramsAsync will return true if there are availabe feature gourps dispatched to GPU",
    "then construct sparse features on CPU",
    "wait for GPU to finish, only if GPU is actually used",
    "use double precision",
    "use single precision",
    "Compare GPU histogram with CPU histogram, useful for debuggin GPU code problem",
    "#define GPU_DEBUG_COMPARE",
    "construct larger leaf",
    "then construct sparse features on CPU",
    "wait for GPU to finish, only if GPU is actually used",
    "use double precision",
    "use single precision",
    "do some sanity check for the GPU algorithm",
    "limit top k",
    "get max bin",
    "calculate buffer size",
    "need to be able to hold smaller and larger best splits in SyncUpGlobalBestSplit",
    "left and right on same time, so need double size",
    "initialize histograms for global",
    "sync global data sumup info",
    "set global sumup info",
    "init global data count in leaf",
    "get local sumup",
    "get local sumup",
    "get mean number on machines",
    "weighted gain",
    "get top k",
    "Copy histogram to buffer, and Get local aggregate features",
    "copy histograms.",
    "copy smaller leaf histograms first",
    "mark local aggregated feature",
    "copy",
    "then copy larger leaf histograms",
    "mark local aggregated feature",
    "copy",
    "use local data to find local best splits",
    "find splits",
    "only has root leaf",
    "local voting",
    "gather",
    "get all top-k from all machines",
    "global voting",
    "copy local histgrams to buffer",
    "Reduce scatter for histogram",
    "find best split from local aggregated histograms",
    "restore from buffer",
    "restore from buffer",
    "find local best",
    "find local best split for larger leaf",
    "sync global best info",
    "copy back",
    "set the global number of data for leaves",
    "init the global sumup info",
    "instantiate template classes, otherwise linker cannot find the code",
    "launch cuda kernel",
    "initialize SerialTreeLearner",
    "some additional variables needed for GPU trainer",
    "Initialize GPU buffers and kernels: get device info",
    "some functions used for debugging the GPU histogram construction",
    "we roughly want 256 workgroups per device, and we have num_dense_feature4_ feature tuples.",
    "also guarantee that there are at least 2K examples per workgroup",
    "we have already copied ordered gradients, ordered hessians and indices to GPU",
    "decide the best number of workgroups working on one feature4 tuple",
    "set work group size based on feature size",
    "each 2^exp_workgroups_per_feature workgroups work on a feature4 tuple",
    "set thread_data",
    "copy the results asynchronously. Size depends on if double precision is used",
    "when the output is ready, the computation is done",
    "how many feature-group tuples we have",
    "leave some safe margin for prefetching",
    "256 work-items per workgroup. Each work-item prefetches one tuple for that feature",
    "clear sparse/dense maps",
    "do nothing it there is no dense feature",
    "calculate number of feature groups per gpu",
    "histogram bin entry size depends on the precision (single/double)",
    "allocate GPU memory for each GPU",
    "do nothing it there is no gpu feature",
    "allocate memory for all features",
    "allocate space for gradients and hessians on device",
    "we will copy gradients and hessians in after ordered_gradients_ and ordered_hessians_ are constructed",
    "copy indices to the device",
    "create output buffer, each feature has a histogram with device_bin_size_ bins,",
    "each work group generates a sub-histogram of dword_features_ features.",
    "only initialize once here, as this will not need to change when ResetTrainingData() is called",
    "create atomic counters for inter-group coordination",
    "The output buffer is allocated to host directly, to overlap compute and data transfer",
    "clear sparse/dense maps",
    "find the dense feature-groups and group then into Feature4 data structure (several feature-groups packed into 4 bytes)",
    "set device info",
    "looking for dword_features_ non-sparse feature-groups",
    "reset device info",
    "InitGPU w/ num_gpu",
    "Get the max bin size, used for selecting best GPU kernel",
    "get num_dense_feature_groups_",
    "initialize GPU",
    "set cpu threads",
    "resize device memory pointers",
    "create stream & events to handle multiple GPUs",
    "check data size",
    "GPU memory has to been reallocated because data may have been changed",
    "AllocateGPUMemory only when the number of data increased",
    "setup GPU kernel arguments after we allocating all the buffers",
    "Copy initial full hessians and gradients to GPU.",
    "We start copying as early as possible, instead of at ConstructHistogram().",
    "use bagging",
    "On GPU, we start copying indices, gradients and hessians now, instead at ConstructHistogram()",
    "copy used gradients and hessians to ordered buffer",
    "transfer the indices to GPU",
    "only have root",
    "Copy indices, gradients and hessians as early as possible",
    "only need to initialize for smaller leaf",
    "Get leaf boundary",
    "do nothing if no features can be processed on GPU",
    "copy data indices if it is not null",
    "converted indices in is_feature_used to feature-group indices",
    "construct the feature masks for dense feature-groups",
    "if no feature group is used, just return and do not use GPU",
    "if not all feature groups are used, we need to transfer the feature mask to GPU",
    "otherwise, we will use a specialized GPU kernel with all feature groups enabled",
    "We now copy even if all features are used.",
    "All data have been prepared, now run the GPU kernel",
    "construct smaller leaf",
    "Check workgroups per feature4 tuple..",
    "if the workgroup per feature is 1 (2^0), return as the work is too small for a GPU",
    "ConstructGPUHistogramsAsync will return true if there are availabe feature groups dispatched to GPU",
    "then construct sparse features on CPU",
    "We set data_indices to null to avoid rebuilding ordered gradients/hessians",
    "wait for GPU to finish, only if GPU is actually used",
    "use double precision",
    "use single precision",
    "Compare GPU histogram with CPU histogram, useful for debuggin GPU code problem",
    "#define CUDA_DEBUG_COMPARE",
    "construct larger leaf",
    "then construct sparse features on CPU",
    "We set data_indices to null to avoid rebuilding ordered gradients/hessians",
    "wait for GPU to finish, only if GPU is actually used",
    "use double precision",
    "use single precision",
    "do some sanity check for the GPU algorithm"
  ],
  "v3.1.1": [
    "coding: utf-8",
    "see https://github.com/pypa/distutils/pull/21",
    "coding: utf-8",
    "create predictor first",
    "check dataset",
    "reduce cost for prediction training data",
    "process callbacks",
    "Most of legacy advanced options becomes callbacks",
    "construct booster",
    "start training",
    "check evaluation result.",
    "ranking task, split according to groups",
    "run preprocessing on the data set if needed",
    "setup callbacks",
    "coding: utf-8",
    "simplejson does not support Python 3.2, it throws a SyntaxError",
    "because of u'...' Unicode literals.",
    "dummy function to support older version of scikit-learn",
    "DeprecationWarning is not shown by default, so let's create our own with higher level",
    "coding: utf-8",
    "user can set verbose with kwargs, it has higher priority",
    "Do not modify original args in fit function",
    "Refer to https://github.com/microsoft/LightGBM/pull/2619",
    "Separate built-in from callable evaluation metrics",
    "register default metric for consistency with callable eval_metric case",
    "try to deduce from class instance",
    "overwrite default metric by explicitly set metric",
    "concatenate metric from params (or default if not provided in params) and eval_metric",
    "copy for consistency",
    "reduce cost for prediction training data",
    "free dataset",
    "Switch to using a multiclass objective in the underlying LGBM instance",
    "do not modify args, as it causes errors in model selection tools",
    "check group data",
    "coding: utf-8",
    "we don't need lib_lightgbm while building docs",
    "coding: utf-8",
    "\"#ddffdd\" is light green, \"#ffdddd\" is light red",
    "coding: utf-8",
    "REMOVEME: remove warning after 3.1.0 version release",
    "coding: utf-8",
    "TypeError: obj is not a string or a number",
    "ValueError: invalid literal",
    "__get_num_preds() cannot work with nrow > MAX_INT32, so calculate overall number of predictions piecemeal",
    "avoid memory consumption by arrays concatenation operations",
    "create numpy array from output arrays",
    "break up indptr based on number of rows (note more than one matrix in multiclass case)",
    "for CSC there is extra column added",
    "reformat output into a csr or csc matrix or list of csr or csc matrices",
    "same shape as input csr or csc matrix except extra column for expected value",
    "note: make sure we copy data as it will be deallocated next",
    "free the temporary native indptr, indices, and data",
    "__get_num_preds() cannot work with nrow > MAX_INT32, so calculate overall number of predictions piecemeal",
    "avoid memory consumption by arrays concatenation operations",
    "no min_data, nthreads and verbose in this function",
    "check data has header or not",
    "need to regroup init_score",
    "process for args",
    "user can set verbose with params, it has higher priority",
    "get categorical features",
    "process for reference dataset",
    "start construct data",
    "set feature names",
    "create valid",
    "construct subset",
    "create train",
    "could be updated if data is not freed",
    "set to None",
    "we're done if self and reference share a common upstrem reference",
    "group data from LightGBM is boundaries data, need to convert to group size",
    "user can set verbose with params, it has higher priority",
    "Training task",
    "set network if necessary",
    "construct booster object",
    "copy the parameters from train_set",
    "save reference to data",
    "buffer for inner predict",
    "Prediction task",
    "if a single node tree it won't have `leaf_index` so return 0",
    "Create the node record, and populate universal data members",
    "Update values to reflect node type (leaf or split)",
    "traverse the next level of the tree",
    "In tree format, \"subtree_list\" is a list of node records (dicts),",
    "and we add node to the list.",
    "need reset training data",
    "need to push new valid data",
    "if buffer length is not long enough, re-allocate a buffer",
    "if buffer length is not long enough, reallocate a buffer",
    "Copy models",
    "Get name of features",
    "avoid to predict many time in one iteration",
    "Get num of inner evals",
    "Get name of evals",
    "coding: utf-8",
    "Callback environment used by callbacks",
    "split is needed for \"<dataset type> <metric>\" case (e.g. \"train l1\")",
    "split is needed for \"<dataset type> <metric>\" case (e.g. \"train l1\")",
    "coding: utf-8",
    "load or create your dataset",
    "create dataset for lightgbm",
    "if you want to re-use data, remember to set free_raw_data=False",
    "specify your configurations as a dict",
    "generate feature names",
    "feature_name and categorical_feature",
    "check feature name",
    "save model to file",
    "dump model to JSON (and save to file)",
    "feature names",
    "feature importances",
    "load model to predict",
    "can only predict with the best iteration (or the saving iteration)",
    "eval with loaded model",
    "dump model with pickle",
    "load model with pickle to predict",
    "can predict with any iteration when loaded in pickle way",
    "eval with loaded model",
    "continue training",
    "init_model accepts:",
    "1. model file name",
    "2. Booster()",
    "decay learning rates",
    "learning_rates accepts:",
    "1. list/tuple with length = num_boost_round",
    "2. function(curr_iter)",
    "change other parameters during training",
    "self-defined objective function",
    "f(preds: array, train_data: Dataset) -> grad: array, hess: array",
    "log likelihood loss",
    "self-defined eval metric",
    "f(preds: array, train_data: Dataset) -> name: string, eval_result: float, is_higher_better: bool",
    "binary error",
    "NOTE: when you do customized loss function, the default prediction value is margin",
    "This may make built-in evalution metric calculate wrong results",
    "For example, we are doing log likelihood loss, the prediction is score before logistic transformation",
    "Keep this in mind when you use the customization",
    "another self-defined eval metric",
    "f(preds: array, train_data: Dataset) -> name: string, eval_result: float, is_higher_better: bool",
    "accuracy",
    "NOTE: when you do customized loss function, the default prediction value is margin",
    "This may make built-in evalution metric calculate wrong results",
    "For example, we are doing log likelihood loss, the prediction is score before logistic transformation",
    "Keep this in mind when you use the customization",
    "callback",
    "coding: utf-8",
    "load or create your dataset",
    "train",
    "predict",
    "eval",
    "feature importances",
    "self-defined eval metric",
    "f(y_true: array, y_pred: array) -> name: string, eval_result: float, is_higher_better: bool",
    "Root Mean Squared Logarithmic Error (RMSLE)",
    "train",
    "another self-defined eval metric",
    "f(y_true: array, y_pred: array) -> name: string, eval_result: float, is_higher_better: bool",
    "Relative Absolute Error (RAE)",
    "train",
    "predict",
    "eval",
    "other scikit-learn modules",
    "coding: utf-8",
    "load or create your dataset",
    "create dataset for lightgbm",
    "specify your configurations as a dict",
    "train",
    "coding: utf-8",
    "################",
    "Simulate some binary data with a single categorical and",
    "single continuous predictor",
    "################",
    "Set up a couple of utilities for our experiments",
    "################",
    "Observe the behavior of `binary` and `xentropy` objectives",
    "Trying this throws an error on non-binary values of y:",
    "experiment('binary', label_type='probability', DATA)",
    "The speed of `binary` is not drastically different than",
    "`xentropy`. `xentropy` runs faster than `binary` in many cases, although",
    "there are reasons to suspect that `binary` should run faster when the",
    "label is an integer instead of a float",
    "coding: utf-8",
    "load or create your dataset",
    "create dataset for lightgbm",
    "specify your configurations as a dict",
    "train",
    "save model to file",
    "predict",
    "eval",
    "coding: utf-8",
    "!/usr/bin/env python3",
    "-*- coding: utf-8 -*-",
    "",
    "LightGBM documentation build configuration file, created by",
    "sphinx-quickstart on Thu May  4 14:30:58 2017.",
    "",
    "This file is execfile()d with the current directory set to its",
    "containing dir.",
    "",
    "Note that not all possible configuration values are present in this",
    "autogenerated file.",
    "",
    "All configuration values have a default; values that are commented out",
    "serve to show the default.",
    "If extensions (or modules to document with autodoc) are in another directory,",
    "add these directories to sys.path here. If the directory is relative to the",
    "documentation root, use os.path.abspath to make it absolute.",
    "-- mock out modules",
    "-- General configuration ------------------------------------------------",
    "If your documentation needs a minimal Sphinx version, state it here.",
    "Add any Sphinx extension module names here, as strings. They can be",
    "extensions coming with Sphinx (named 'sphinx.ext.*') or your custom",
    "ones.",
    "Generate autosummary pages. Output should be set with: `:toctree: pythonapi/`",
    "Only the class' docstring is inserted.",
    "If true, `todo` and `todoList` produce output, else they produce nothing.",
    "The master toctree document.",
    "General information about the project.",
    "The name of an image file (relative to this directory) to place at the top",
    "of the sidebar.",
    "The name of an image file (relative to this directory) to use as a favicon of",
    "the docs.  This file should be a Windows icon file (.ico) being 16x16 or 32x32",
    "pixels large.",
    "The version info for the project you're documenting, acts as replacement for",
    "|version| and |release|, also used in various other places throughout the",
    "built documents.",
    "The short X.Y version.",
    "The full version, including alpha/beta/rc tags.",
    "The language for content autogenerated by Sphinx. Refer to documentation",
    "for a list of supported languages.",
    "",
    "This is also used if you do content translation via gettext catalogs.",
    "Usually you set \"language\" from the command line for these cases.",
    "List of patterns, relative to source directory, that match files and",
    "directories to ignore when looking for source files.",
    "This patterns also effect to html_static_path and html_extra_path",
    "The name of the Pygments (syntax highlighting) style to use.",
    "-- Configuration for C API docs generation ------------------------------",
    "-- Options for HTML output ----------------------------------------------",
    "The theme to use for HTML and HTML Help pages.  See the documentation for",
    "a list of builtin themes.",
    "Theme options are theme-specific and customize the look and feel of a theme",
    "further.  For a list of options available for each theme, see the",
    "documentation.",
    "Add any paths that contain custom static files (such as style sheets) here,",
    "relative to this directory. They are copied after the builtin static files,",
    "so a file named \"default.css\" will overwrite the builtin \"default.css\".",
    "-- Options for HTMLHelp output ------------------------------------------",
    "Output file base name for HTML help builder.",
    "-- Options for LaTeX output ---------------------------------------------",
    "The name of an image file (relative to this directory) to place at the top of",
    "the title page.",
    "Warning! The following code can cause buffer overflows on RTD.",
    "Consider suppressing output completely if RTD project silently fails.",
    "Refer to https://github.com/svenevs/exhale",
    "/blob/fe7644829057af622e467bb529db6c03a830da99/exhale/deploy.py#L99-L111",
    "Warning! The following code can cause buffer overflows on RTD.",
    "Consider suppressing output completely if RTD project silently fails.",
    "Refer to https://github.com/svenevs/exhale",
    "/blob/fe7644829057af622e467bb529db6c03a830da99/exhale/deploy.py#L99-L111",
    "coding: utf-8",
    "coding: utf-8",
    "we don't need lib_lightgbm while building docs",
    "coding: utf-8",
    "check saved model persistence",
    "we need to check the consistency of model file here, so test for exact equal",
    "check early stopping is working. Make it stop very early, so the scores should be very close to zero",
    "scores likely to be different, but prediction should still be the same",
    "test that shape is checked during prediction",
    "test that method works even with free_raw_data=True",
    "test that method works but sets raw data to None in case of immergeable data types",
    "test that method works for different data types",
    "Set extremely harsh penalties, so CEGB will block most splits.",
    "Compare pairs of penalties, to ensure scaling works as intended",
    "Reset booster1's parameters to p2, so the parameter section of the file matches.",
    "coding: utf-8",
    "coding: utf-8",
    "prediction result is actually not transformed (is raw) due to custom objective",
    "sklearn <0.23 does not have a stacking classifier and n_features_in_ property",
    "sklearn <0.23 does not have a stacking regressor and n_features_in_ property",
    "sklearn < 0.22 does not have the post fit attribute: classes_",
    "sklearn < 0.23 does not have as_frame parameter",
    "sklearn < 0.22 does not have the post fit attribute: classes_",
    "sklearn < 0.23 does not have as_frame parameter",
    "Test if random_state is properly stored",
    "Test if two random states produce identical models",
    "Test if subsequent fits sample from random_state object and produce different models",
    "Test that the largest element is NOT the same, the smallest can be the same, i.e. zero",
    "With default params",
    "Tests same probabilities",
    "Tests same predictions",
    "Tests same raw scores",
    "Tests same leaf indices",
    "Tests same feature contributions",
    "Tests other parameters for the prediction works",
    "Tests start_iteration",
    "Tests same probabilities, starting from iteration 10",
    "Tests same predictions, starting from iteration 10",
    "Tests same raw scores, starting from iteration 10",
    "Tests same leaf indices, starting from iteration 10",
    "Tests same feature contributions, starting from iteration 10",
    "Tests other parameters for the prediction works, starting from iteration 10",
    "no custom objective, no custom metric",
    "default metric",
    "non-default metric",
    "no metric",
    "non-default metric in eval_metric",
    "non-default metric with non-default metric in eval_metric",
    "non-default metric with multiple metrics in eval_metric",
    "non-default metric with multiple metrics in eval_metric for LGBMClassifier",
    "default metric for non-default objective",
    "non-default metric for non-default objective",
    "no metric",
    "non-default metric in eval_metric for non-default objective",
    "non-default metric with non-default metric in eval_metric for non-default objective",
    "non-default metric with multiple metrics in eval_metric for non-default objective",
    "custom objective, no custom metric",
    "default regression metric for custom objective",
    "non-default regression metric for custom objective",
    "multiple regression metrics for custom objective",
    "no metric",
    "default regression metric with non-default metric in eval_metric for custom objective",
    "non-default regression metric with metric in eval_metric for custom objective",
    "multiple regression metrics with metric in eval_metric for custom objective",
    "multiple regression metrics with multiple metrics in eval_metric for custom objective",
    "no custom objective, custom metric",
    "default metric with custom metric",
    "non-default metric with custom metric",
    "multiple metrics with custom metric",
    "custom metric (disable default metric)",
    "default metric for non-default objective with custom metric",
    "non-default metric for non-default objective with custom metric",
    "multiple metrics for non-default objective with custom metric",
    "custom metric (disable default metric for non-default objective)",
    "custom objective, custom metric",
    "custom metric for custom objective",
    "non-default regression metric with custom metric for custom objective",
    "multiple regression metrics with custom metric for custom objective",
    "default metric and invalid binary metric is replaced with multiclass alternative",
    "invalid objective is replaced with default multiclass one",
    "and invalid binary metric is replaced with multiclass alternative",
    "default metric for non-default multiclass objective",
    "and invalid binary metric is replaced with multiclass alternative",
    "default metric and invalid multiclass metric is replaced with binary alternative",
    "invalid multiclass metric is replaced with binary alternative for custom objective",
    "Verify that can receive a list of metrics, only callable",
    "Verify that can receive a list of custom and built-in metrics",
    "Verify that works as expected when eval_metric is empty",
    "Verify that can receive a list of metrics, only built-in",
    "Verify that eval_metric is robust to receiving a list with None",
    "training data as eval_set",
    "feval",
    "single eval_set",
    "two eval_set",
    "sklearn < 0.22 requires passing \"attributes\" argument",
    "Test that estimators are default-constructible",
    "coding: utf-8",
    "coding: utf-8",
    "check that default gives same result as k = 1",
    "check against independent calculation for k = 1",
    "check against independent calculation for k = 2",
    "check against independent calculation for k = 10",
    "check cases where predictions are equal",
    "should give same result as binary auc for 2 classes",
    "test the case where all predictions are equal",
    "test that weighted data gives different auc_mu",
    "test that equal data weights give same auc_mu as unweighted data",
    "should give 1 when accuracy = 1",
    "test loading class weights",
    "no early stopping",
    "early stopping occurs",
    "test custom eval metrics",
    "shuffle = False, override metric in params",
    "shuffle = True, callbacks",
    "enable display training loss",
    "self defined folds",
    "lambdarank",
    "... with l2 metric",
    "... with NDCG (default) metric",
    "self defined folds with lambdarank",
    "with early stopping",
    "predict by each fold booster",
    "fold averaging",
    "without early stopping",
    "test feature_names with whitespaces",
    "This has non-ascii strings.",
    "take subsets and train",
    "generate CSR sparse dataset",
    "convert data to dense and get back same contribs",
    "validate the values are the same",
    "validate using CSC matrix",
    "validate the values are the same",
    "generate CSR sparse dataset",
    "convert data to dense and get back same contribs",
    "validate the values are the same",
    "validate using CSC matrix",
    "validate the values are the same",
    "Note there is an extra column added to the output for the expected value",
    "Note output CSC shape should be same as CSR output shape",
    "test sliced labels",
    "append some columns",
    "append some rows",
    "test sliced 2d matrix",
    "test sliced CSR",
    "test if a penalty as high as the depth indeed prohibits all monotone splits",
    "The penalization is so high that the first 2 features should not be used here",
    "Check that a very high penalization is the same as not using the features at all",
    "no fobj, no feval",
    "default metric",
    "non-default metric in params",
    "default metric in args",
    "non-default metric in args",
    "metric in args overwrites one in params",
    "multiple metrics in params",
    "multiple metrics in args",
    "remove default metric by 'None' in list",
    "remove default metric by 'None' aliases",
    "fobj, no feval",
    "no default metric",
    "metric in params",
    "metric in args",
    "metric in args overwrites its' alias in params",
    "multiple metrics in params",
    "multiple metrics in args",
    "no fobj, feval",
    "default metric with custom one",
    "non-default metric in params with custom one",
    "default metric in args with custom one",
    "non-default metric in args with custom one",
    "metric in args overwrites one in params, custom one is evaluated too",
    "multiple metrics in params with custom one",
    "multiple metrics in args with custom one",
    "custom metric is evaluated despite 'None' is passed",
    "fobj, feval",
    "no default metric, only custom one",
    "metric in params with custom one",
    "metric in args with custom one",
    "metric in args overwrites one in params, custom one is evaluated too",
    "multiple metrics in params with custom one",
    "multiple metrics in args with custom one",
    "custom metric is evaluated despite 'None' is passed",
    "no fobj, no feval",
    "default metric",
    "default metric in params",
    "non-default metric in params",
    "multiple metrics in params",
    "remove default metric by 'None' aliases",
    "fobj, no feval",
    "no default metric",
    "metric in params",
    "multiple metrics in params",
    "no fobj, feval",
    "default metric with custom one",
    "default metric in params with custom one",
    "non-default metric in params with custom one",
    "multiple metrics in params with custom one",
    "custom metric is evaluated despite 'None' is passed",
    "fobj, feval",
    "no default metric, only custom one",
    "metric in params with custom one",
    "multiple metrics in params with custom one",
    "custom metric is evaluated despite 'None' is passed",
    "multiclass default metric",
    "multiclass default metric with custom one",
    "multiclass metric alias with custom one for custom objective",
    "no metric for invalid class_num",
    "custom metric for invalid class_num",
    "multiclass metric alias with custom one with invalid class_num",
    "multiclass default metric without num_class",
    "multiclass metric alias",
    "multiclass metric",
    "non-valid metric for multiclass objective",
    "non-default num_class for default objective",
    "no metric with non-default num_class for custom objective",
    "multiclass metric alias for custom objective",
    "multiclass metric for custom objective",
    "binary metric with non-default num_class for custom objective",
    "Expect three metrics but mean and stdv for each metric",
    "test XGBoost-style return value",
    "test numpy-style return value",
    "test bins string type",
    "test histogram is disabled for categorical features",
    "test for lgb.train",
    "test feval for lgb.train",
    "test with two valid data for lgb.train",
    "test for lgb.cv",
    "test feval for lgb.cv",
    "test that binning works properly for features with only positive or only negative values",
    "decreasing without freeing raw data is allowed",
    "decreasing before lazy init is allowed",
    "increasing is allowed",
    "decreasing with disabled filter is allowed",
    "decreasing with enabled filter is disallowed;",
    "also changes of other params are disallowed",
    "check extra trees increases regularization",
    "check path smoothing increases regularization",
    "test edge case with one leaf",
    "check that constraint containing all features is equivalent to no constraint",
    "check that constraint partitioning the features reduces train accuracy",
    "check that constraints consisting of single features reduce accuracy further",
    "test that interaction constraints work when not all features are used",
    "test that the predict once with all iterations equals summed results with start_iteration and num_iteration",
    "test the case where start_iteration <= 0, and num_iteration is None",
    "test the case where start_iteration > 0, and num_iteration <= 0",
    "test the case where start_iteration > 0, and num_iteration <= 0, with pred_leaf=True",
    "test the case where start_iteration > 0, and num_iteration <= 0, with pred_contrib=True",
    "test for regression",
    "test both with and without early stopping",
    "test for multi-class",
    "test both with and without early stopping",
    "test for binary",
    "test both with and without early stopping",
    "test against sklearn average precision metric",
    "test that average precision is 1 where model predicts perfectly",
    "coding: utf-8",
    "convert from one-based to  zero-based index",
    "convert from boundaries to size",
    "--- start Booster interfaces",
    ".Call() calls",
    "coding: utf-8",
    "alias table",
    "names",
    "from strings",
    "tails",
    "tails",
    "coding: utf-8",
    "Single row predictor to abstract away caching logic",
    "create boosting",
    "initialize the boosting",
    "create objective function",
    "initialize the objective function",
    "create training metric",
    "reset the boosting",
    "create objective function",
    "initialize the objective function",
    "calculate the nonzero data and indices size",
    "allocate data and indices arrays",
    "Get the number of trees per iteration (for multiclass scenario we output multiple sparse matrices)",
    "aggregated per row feature contribution results",
    "keep track of the row_vector sizes for parallelization",
    "copy vector results to output for each row",
    "Get the number of trees per iteration (for multiclass scenario we output multiple sparse matrices)",
    "aggregated per row feature contribution results",
    "calculate number of elements per column to construct",
    "the CSC matrix with random access",
    "keep track of column counts",
    "keep track of beginning index for each column",
    "keep track of beginning index for each matrix",
    "Note: we parallelize across matrices instead of rows because of the column_counts[m][col_idx] increment inside the loop",
    "store the row index",
    "update column count",
    "explicitly declare symbols from LightGBM namespace",
    "some help functions used to convert data",
    "Row iterator of on column for CSC matrix",
    "return value at idx, only can access by ascent order",
    "return next non-zero pair, if index < 0, means no more data",
    "start of c_api functions",
    "sample data first",
    "sample data first",
    "sample data first",
    "local buffer to re-use memory",
    "sample data first",
    "no more data",
    "---- start of booster",
    "Single row in row-major format:",
    "---- start of some help functions",
    "data is array of pointers to individual rows",
    "set number of threads for openmp",
    "check for alias",
    "read parameters from config file",
    "remove str after \"#\"",
    "check for alias again",
    "load configs",
    "prediction is needed if using input initial model(continued train)",
    "need to continue training",
    "sync up random seed for data partition",
    "load Training data",
    "load data for parallel training",
    "load data for single machine",
    "need save binary file",
    "create training metric",
    "only when have metrics then need to construct validation data",
    "Add validation data, if it exists",
    "add",
    "need save binary file",
    "add metric for validation data",
    "output used time on each iteration",
    "need init network",
    "create boosting",
    "create objective function",
    "load training data",
    "initialize the objective function",
    "initialize the boosting",
    "add validation data into boosting",
    "convert model to if-else statement code",
    "create predictor",
    "Free memory",
    "create predictor",
    "label_gain = 2^i - 1, may overflow, so we use 31 here",
    "counts for all labels",
    "start from top label, and accumulate DCG",
    "counts for all labels",
    "calculate k Max DCG by one pass",
    "get sorted indices by score",
    "calculate dcg",
    "get sorted indices by score",
    "calculate multi dcg by one pass",
    "wait for all client start up",
    "Don't call MPI_Finalize() here: If the destructor was called because only this node had an exception, calling MPI_Finalize() will cause all nodes to hang.",
    "Instead we will handle finalize/abort for MPI in main().",
    "default set to -1",
    "distance at k-th communication, distance[k] = 2^k",
    "set incoming rank at k-th commuication",
    "set outgoing rank at k-th commuication",
    "defalut set as -1",
    "construct all recursive halving map for all machines",
    "let 1 << k <= num_machines",
    "distance of each communication",
    "if num_machines = 2^k, don't need to group machines",
    "communication direction, %2 == 0 is positive",
    "neighbor at k-th communication",
    "receive data block at k-th communication",
    "send data block at k-th communication",
    "if num_machines != 2^k, need to group machines",
    "group, two machine in one group, total \"rest\" groups will have 2 machines.",
    "let left machine as group leader",
    "cache block information for groups, group with 2 machines will have double block size",
    "convert from group to node leader",
    "convert from node to group",
    "meet new group",
    "add block len for this group",
    "calculate the group block start",
    "not need to construct",
    "get receive block informations",
    "accumulate block len",
    "get send block informations",
    "accumulate block len",
    "static member definition",
    "if small package or small count , do it by all gather.(reduce the communication times.)",
    "assign the blocks to every rank.",
    "do reduce scatter",
    "do all gather",
    "assign blocks",
    "need use buffer here, since size of \"output\" is smaller than size after all gather",
    "copy back",
    "assign blocks",
    "start all gather",
    "when num_machines is small and data is large",
    "use output as receive buffer",
    "get current local block size",
    "get out rank",
    "get in rank",
    "get send information",
    "get recv information",
    "send and recv at same time",
    "rotate in-place",
    "use output as receive buffer",
    "get current local block size",
    "get send information",
    "get recv information",
    "send and recv at same time",
    "use output as receive buffer",
    "send and recv at same time",
    "send local data to neighbor first",
    "receive neighbor data first",
    "reduce",
    "get target",
    "get send information",
    "get recv information",
    "send and recv at same time",
    "reduce",
    "send result to neighbor",
    "receive result from neighbor",
    "copy result",
    "start up socket",
    "parse clients from file",
    "get ip list of local machine",
    "get local rank",
    "construct listener",
    "construct communication topo",
    "construct linkers",
    "free listener",
    "set timeout",
    "accept incoming socket",
    "receive rank",
    "add new socket",
    "save ranks that need to connect with",
    "start listener",
    "start connect",
    "let smaller rank connect to larger rank",
    "send local rank",
    "wait for listener",
    "print connected linkers",
    "only need to copy subset",
    "avoid to copy subset many times",
    "avoid out of range",
    "may need to recopy subset",
    "valid the type",
    "Constructors",
    "Get type tag",
    "Comparisons",
    "This has to be separate, not in Statics, because Json() accesses",
    "statics().null.",
    "advance until next line, or end of input",
    "advance until closing tokens",
    "The usual case: non-escaped characters",
    "Handle escapes",
    "Extract 4-byte escape sequence",
    "Explicitly check length of the substring. The following loop",
    "relies on std::string returning the terminating NUL when",
    "accessing str[length]. Checking here reduces brittleness.",
    "JSON specifies that characters outside the BMP shall be encoded as a",
    "pair of 4-hex-digit \\u escapes encoding their surrogate pair",
    "components. Check whether we're in the middle of such a beast: the",
    "previous codepoint was an escaped lead (high) surrogate, and this is",
    "a trail (low) surrogate.",
    "Reassemble the two surrogate pairs into one astral-plane character,",
    "per the UTF-16 algorithm.",
    "Integer part",
    "Decimal part",
    "Exponent part",
    "Check for any trailing garbage",
    "Documented in json11.hpp",
    "Check for another object",
    "get column names",
    "load label idx first",
    "erase label column name",
    "load ignore columns",
    "load weight idx",
    "load group idx",
    "don't support query id in data file when training in parallel",
    "read data to memory",
    "sample data",
    "construct feature bin mappers",
    "initialize label",
    "extract features",
    "sample data from file",
    "construct feature bin mappers",
    "initialize label",
    "extract features",
    "load data from binary file",
    "check meta data",
    "need to check training data",
    "read data in memory",
    "initialize label",
    "extract features",
    "Get number of lines of data file",
    "initialize label",
    "extract features",
    "load data from binary file",
    "not need to check validation data",
    "check meta data",
    "buffer to read binary file",
    "check token",
    "read size of header",
    "re-allocmate space if not enough",
    "read header",
    "get header",
    "num_groups",
    "real_feature_idx_",
    "feature2group",
    "feature2subfeature",
    "group_bin_boundaries",
    "group_feature_start_",
    "group_feature_cnt_",
    "get feature names",
    "write feature names",
    "get forced_bin_bounds_",
    "read size of meta data",
    "re-allocate space if not enough",
    "read meta data",
    "load meta data",
    "sample local used data if need to partition",
    "if not contain query file, minimal sample unit is one record",
    "if contain query file, minimal sample unit is one query",
    "if is new query",
    "read feature data",
    "read feature size",
    "re-allocate space if not enough",
    "fill feature_names_ if not header",
    "get forced split",
    "if only one machine, find bin locally",
    "if have multi-machines, need to find bin distributed",
    "different machines will find bin for different features",
    "start and len will store the process feature indices for different machines",
    "machine i will find bins for features in [ start[i], start[i] + len[i] )",
    "free",
    "gather global feature bin mappers",
    "restore features bins from buffer",
    "---- private functions ----",
    "if features are ordered, not need to use hist_buf",
    "read all lines",
    "get query data",
    "if not contain query data, minimal sample unit is one record",
    "if contain query data, minimal sample unit is one query",
    "if is new query",
    "get query data",
    "if not contain query file, minimal sample unit is one record",
    "if contain query file, minimal sample unit is one query",
    "if is new query",
    "parse features",
    "get forced split",
    "check the range of label_idx, weight_idx and group_idx",
    "fill feature_names_ if not header",
    "start find bins",
    "if only one machine, find bin locally",
    "start and len will store the process feature indices for different machines",
    "machine i will find bins for features in [ start[i], start[i] + len[i] )",
    "free",
    "gather global feature bin mappers",
    "restore features bins from buffer",
    "if doesn't need to prediction with initial model",
    "parser",
    "set label",
    "free processed line:",
    "shrink_to_fit will be very slow in linux, and seems not free memory, disable for now",
    "text_reader_->Lines()[i].shrink_to_fit();",
    "push data",
    "if is used feature",
    "if need to prediction with initial model",
    "parser",
    "set initial score",
    "set label",
    "free processed line:",
    "shrink_to_fit will be very slow in linux, and seems not free memory, disable for now",
    "text_reader_->Lines()[i].shrink_to_fit();",
    "push data",
    "if is used feature",
    "metadata_ will manage space of init_score",
    "text data can be free after loaded feature values",
    "parser",
    "set initial score",
    "set label",
    "push data",
    "if is used feature",
    "only need part of data",
    "need full data",
    "metadata_ will manage space of init_score",
    "read size of token",
    "remove duplicates",
    "deep copy function for BinMapper",
    "mean size for one bin",
    "need a new bin",
    "update bin upper bound",
    "last bin upper bound",
    "get list of distinct values",
    "get number of positive and negative distinct values",
    "include zero bounds and infinity bound",
    "add forced bounds, excluding zeros since we have already added zero bounds",
    "find remaining bounds",
    "find distinct_values first",
    "push zero in the front",
    "use the large value",
    "push zero in the back",
    "convert to int type first",
    "sort by counts",
    "will ignore the categorical of small counts",
    "Push the dummy bin for NaN",
    "Use MissingType::None to represent this bin contains all categoricals",
    "fix count of NaN bin",
    "check trivial(num_bin_ == 1) feature",
    "check useless bin",
    "When most_freq_bin_ != default_bin_, there are some additional data loading costs.",
    "so use most_freq_bin_  = default_bin_ when there is not so sparse",
    "calculate max bin of all features to select the int type in MultiValDenseBin",
    "for lambdarank, it needs query data for partition data in parallel learning",
    "need convert query_id to boundaries",
    "check weights",
    "check query boundries",
    "contain initial score file",
    "check weights",
    "get local weights",
    "check query boundries",
    "get local query boundaries",
    "contain initial score file",
    "get local initial scores",
    "re-load query weight",
    "save to nullptr",
    "save to nullptr",
    "save to nullptr",
    "default weight file name",
    "default init_score file name",
    "use first line to count number class",
    "default query file name",
    "root is in the depth 0",
    "non-leaf",
    "leaf",
    "use this for the missing value conversion",
    "Predict func by Map to ifelse",
    "use this for the missing value conversion",
    "non-leaf",
    "left subtree",
    "right subtree",
    "leaf",
    "non-leaf",
    "left subtree",
    "right subtree",
    "leaf",
    "recursive computation of SHAP values for a decision tree",
    "extend the unique path",
    "leaf node",
    "internal node",
    "see if we have already split on this feature,",
    "if so we undo that split so we can redo it for this node",
    "recursive sparse computation of SHAP values for a decision tree",
    "extend the unique path",
    "leaf node",
    "internal node",
    "see if we have already split on this feature,",
    "if so we undo that split so we can redo it for this node",
    "add names of objective function if not providing metric",
    "equal weights for all classes",
    "generate seeds by seed.",
    "sort eval_at",
    "Only push the non-training data",
    "check for conflicts",
    "check if objective, metric, and num_class match",
    "Change pool size to -1 (no limit) when using data parallel to reduce communication costs",
    "Check max_depth and num_leaves",
    "Fits in an int, and is more restrictive than the current num_leaves",
    "force col-wise for gpu & CUDA",
    "force gpu_use_dp for CUDA",
    "min_data_in_leaf must be at least 2 if path smoothing is active. This is because when the split is calculated",
    "the count is calculated using the proportion of hessian in the leaf which is rounded up to nearest int, so it can",
    "be 1 when there is actually no data in the leaf. In rare cases this can cause a bug because with path smoothing the",
    "calculated split gain can be positive even with zero gradient and hessian.",
    "In distributed mode, local node doesn't have histograms on all features, cannot perform \"intermediate\" monotone constraints.",
    "\"intermediate\" monotone constraints need to recompute splits. If the features are sampled when computing the",
    "split initially, then the sampling needs to be recorded or done once again, which is currently not supported",
    "first round: fill the single val group",
    "always push the last group",
    "put dense feature first",
    "sort by non zero cnt",
    "sort by non zero cnt, bigger first",
    "shuffle groups",
    "Using std::swap for vector<bool> will cause the wrong result.",
    "get num_features",
    "get bin_mappers",
    "for sparse multi value bin, we store the feature bin values with offset added",
    "for dense multi value bin, the feature bin values without offsets are used",
    "copy feature bin mapper data",
    "if not pass a filename, just append \".bin\" of original file",
    "get size of header",
    "size of feature names",
    "size of forced bins",
    "write header",
    "write feature names",
    "write forced bins",
    "get size of meta data",
    "write meta data",
    "write feature data",
    "get size of feature",
    "write feature",
    "explicitly initilize template methods, for cross module call",
    "Only one multi-val group, just simply merge",
    "Skip the leading 0 when copying group_bin_boundaries.",
    "regenerate other fields",
    "store the importance first",
    "PredictRaw",
    "PredictRawByMap",
    "Predict",
    "PredictByMap",
    "PredictLeafIndex",
    "PredictLeafIndexByMap",
    "output model type",
    "output number of class",
    "output label index",
    "output max_feature_idx",
    "output objective",
    "output tree models",
    "store the importance first",
    "sort the importance",
    "use serialized string to restore this object",
    "Use first 128 chars to avoid exceed the message buffer.",
    "get number of classes",
    "get index of label",
    "get max_feature_idx first",
    "get average_output",
    "get feature names",
    "get monotone_constraints",
    "set zero",
    "predict all the trees for one iteration",
    "check early stopping",
    "set zero",
    "predict all the trees for one iteration",
    "check early stopping",
    "margin_threshold will be captured by value",
    "copy and sort",
    "margin_threshold will be captured by value",
    "Fix for compiler warnings about reaching end of control",
    "load forced_splits file",
    "init tree learner",
    "push training metrics",
    "create buffer for gradients and hessians",
    "get max feature index",
    "get label index",
    "get feature names",
    "if need bagging, create buffer",
    "for a validation dataset, we need its score and metric",
    "update score",
    "objective function will calculate gradients and hessians",
    "random bagging, minimal unit is one record",
    "random bagging, minimal unit is one record",
    "if need bagging",
    "set bagging data to tree learner",
    "get subset",
    "output used time per iteration",
    "boosting from average label; or customized \"average\" if implemented for the current objective",
    "boosting first",
    "bagging logic",
    "need to copy gradients for bagging subset.",
    "shrinkage by learning rate",
    "update score",
    "only add default score one-time",
    "updates scores",
    "add model",
    "reset score",
    "remove model",
    "print message for metric",
    "pop last early_stopping_round_ models",
    "update training score",
    "we need to predict out-of-bag scores of data for boosting",
    "update validation score",
    "print training metric",
    "print validation metric",
    "set zero",
    "predict all the trees for one iteration",
    "predict all the trees for one iteration",
    "push training metrics",
    "not same training data, need reset score and others",
    "create score tracker",
    "update score",
    "create buffer for gradients and hessians",
    "load forced_splits file",
    "if need bagging, create buffer",
    "Get the max size of pool",
    "at least need 2 leaves",
    "push split information for all leaves",
    "initialize splits for leaf",
    "initialize data partition",
    "initialize ordered gradients and hessians",
    "cannot change is_hist_col_wise during training",
    "initialize splits for leaf",
    "initialize data partition",
    "initialize ordered gradients and hessians",
    "Get the max size of pool",
    "at least need 2 leaves",
    "push split information for all leaves",
    "some initial works before training",
    "root leaf",
    "only root leaf can be splitted on first time",
    "some initial works before finding best split",
    "find best threshold for every feature",
    "Get a leaf with max split gain",
    "Get split information for best leaf",
    "cannot split, quit",
    "split tree with best leaf",
    "reset histogram pool",
    "initialize data partition",
    "reset the splits for leaves",
    "Sumup for root",
    "use all data",
    "use bagging, only use part of data",
    "check depth of current leaf",
    "only need to check left leaf, since right leaf is in same level of left leaf",
    "no enough data to continue",
    "only have root",
    "put parent(left) leaf's histograms into larger leaf's histograms",
    "put parent(left) leaf's histograms to larger leaf's histograms",
    "construct smaller leaf",
    "construct larger leaf",
    "find splits",
    "only has root leaf",
    "start at root leaf",
    "before processing next node from queue, store info for current left/right leaf",
    "store \"best split\" for left and right, even if they might be overwritten by forced split",
    "then, compute own splits",
    "split info should exist because searching in bfs fashion - should have added from parent",
    "update before tree split",
    "don't need to update this in data-based parallel model",
    "split tree, will return right leaf",
    "store the true split gain in tree model",
    "don't need to update this in data-based parallel model",
    "store the true split gain in tree model",
    "init the leaves that used on next iteration",
    "update leave outputs if needed",
    "bag_mapper[index_mapper[i]]",
    "it is needed to filter the features after the above code.",
    "Otherwise, the `is_splittable` in `FeatureHistogram` will be wrong, and cause some features being accidentally filtered in the later nodes.",
    "for root leaf the \"parent\" output is its own output because we don't apply any smoothing to the root",
    "can't use GetParentOutput because leaf_splits doesn't have weight property set",
    "find splits",
    "need to be able to hold smaller and larger best splits in SyncUpGlobalBestSplit",
    "get feature partition",
    "get local used features",
    "get best split at smaller leaf",
    "find local best split for larger leaf",
    "sync global best info",
    "update best split",
    "instantiate template classes, otherwise linker cannot find the code",
    "initialize SerialTreeLearner",
    "Get local rank and global machine size",
    "need to be able to hold smaller and larger best splits in SyncUpGlobalBestSplit",
    "allocate buffer for communication",
    "generate feature partition for current tree",
    "get local used feature",
    "get block start and block len for reduce scatter",
    "get buffer_write_start_pos_",
    "get buffer_read_start_pos_",
    "sync global data sumup info",
    "global sumup reduce",
    "copy back",
    "set global sumup info",
    "init global data count in leaf",
    "construct local histograms",
    "copy to buffer",
    "Reduce scatter for histogram",
    "restore global histograms from buffer",
    "only root leaf",
    "construct histgroms for large leaf, we init larger leaf as the parent, so we can just subtract the smaller leaf's histograms",
    "find local best split for larger leaf",
    "sync global best info",
    "set best split",
    "need update global number of data in leaf",
    "instantiate template classes, otherwise linker cannot find the code",
    "initialize SerialTreeLearner",
    "some additional variables needed for GPU trainer",
    "Initialize GPU buffers and kernels",
    "some functions used for debugging the GPU histogram construction",
    "printf(\"grad %g != %g (%d ULPs)\\n\", GET_GRAD(h1, i), GET_GRAD(h2, i), ulps);",
    "goto err;",
    "we roughly want 256 workgroups per device, and we have num_dense_feature4_ feature tuples.",
    "also guarantee that there are at least 2K examples per workgroup",
    "return 0;",
    "we have already copied ordered gradients, ordered hessians and indices to GPU",
    "decide the best number of workgroups working on one feature4 tuple",
    "set work group size based on feature size",
    "each 2^exp_workgroups_per_feature workgroups work on a feature4 tuple",
    "we need to refresh the kernel arguments after reallocating",
    "The only argument that needs to be changed later is num_data_",
    "the GPU kernel will process all features in one call, and each",
    "2^exp_workgroups_per_feature (compile time constant) workgroup will",
    "process one feature4 tuple",
    "for the root node, indices are not copied",
    "for constant hessian, hessians are not copied except for the root node",
    "there will be 2^exp_workgroups_per_feature = num_workgroups / num_dense_feature4 sub-histogram per feature4",
    "and we will launch num_feature workgroups for this kernel",
    "will launch threads for all features",
    "the queue should be asynchrounous, and we will can WaitAndGetHistograms() before we start processing dense feature groups",
    "copy the results asynchronously. Size depends on if double precision is used",
    "we will wait for this object in WaitAndGetHistograms",
    "when the output is ready, the computation is done",
    "values of this feature has been redistributed to multiple bins; need a reduction here",
    "how many feature-group tuples we have",
    "leave some safe margin for prefetching",
    "256 work-items per workgroup. Each work-item prefetches one tuple for that feature",
    "clear sparse/dense maps",
    "do nothing if no features can be processed on GPU",
    "allocate memory for all features (FIXME: 4 GB barrier on some devices, need to split to multiple buffers)",
    "unpin old buffer if necessary before destructing them",
    "make ordered_gradients and hessians larger (including extra room for prefetching), and pin them",
    "allocate space for gradients and hessians on device",
    "we will copy gradients and hessians in after ordered_gradients_ and ordered_hessians_ are constructed",
    "allocate feature mask, for disabling some feature-groups' histogram calculation",
    "copy indices to the device",
    "histogram bin entry size depends on the precision (single/double)",
    "create output buffer, each feature has a histogram with device_bin_size_ bins,",
    "each work group generates a sub-histogram of dword_features_ features.",
    "only initialize once here, as this will not need to change when ResetTrainingData() is called",
    "create atomic counters for inter-group coordination",
    "The output buffer is allocated to host directly, to overlap compute and data transfer",
    "find the dense feature-groups and group then into Feature4 data structure (several feature-groups packed into 4 bytes)",
    "looking for dword_features_ non-sparse feature-groups",
    "decide if we need to redistribute the bin",
    "multiplier must be a power of 2",
    "device_bin_mults_.push_back(1);",
    "found",
    "for data transfer time",
    "Now generate new data structure feature4, and copy data to the device",
    "preallocate arrays for all threads, and pin them",
    "building Feature4 bundles; each thread handles dword_features_ features",
    "one feature datapoint is 4 bits",
    "this guarantees that the RawGet() function is inlined, rather than using virtual function dispatching",
    "one feature datapoint is one byte",
    "this guarantees that the RawGet() function is inlined, rather than using virtual function dispatching",
    "Dense bin",
    "Dense 4-bit bin",
    "working on the remaining (less than dword_features_) feature groups",
    "fill the leftover features",
    "fill this empty feature with some \"random\" value",
    "fill this empty feature with some \"random\" value",
    "copying the last 1 to (dword_features - 1) feature-groups in the last tuple",
    "deallocate pinned space for feature copying",
    "data transfer time",
    "for other types of failure, build log might not be available; program.build_log() can crash",
    "Something bad happened. Just return \"No log available.\"",
    "build is okay, log may contain warnings",
    "destroy any old kernels",
    "create OpenCL kernels for different number of workgroups per feature",
    "currently we don't use constant memory",
    "compile the GPU kernel depending if double precision is used, constant hessian is used, etc.",
    "kernel with indices in an array",
    "kernel with all features enabled, with elimited branches",
    "kernel with all data indices (for root node, and assumes that root node always uses all features)",
    "do nothing if no features can be processed on GPU",
    "The only argument that needs to be changed later is num_data_",
    "hessian is passed as a parameter, but it is not available now.",
    "hessian will be set in BeforeTrain()",
    "Get the max bin size, used for selecting best GPU kernel",
    "initialize GPU",
    "determine which kernel to use based on the max number of bins",
    "the +9 skips extra characters \")\", newline, \"#endif\" and newline at the beginning",
    "the +9 skips extra characters \")\", newline, \"#endif\" and newline at the beginning",
    "the +9 skips extra characters \")\", newline, \"#endif\" and newline at the beginning",
    "setup GPU kernel arguments after we allocating all the buffers",
    "GPU memory has to been reallocated because data may have been changed",
    "setup GPU kernel arguments after we allocating all the buffers",
    "Copy initial full hessians and gradients to GPU.",
    "We start copying as early as possible, instead of at ConstructHistogram().",
    "setup hessian parameters only",
    "hessian is passed as a parameter",
    "use bagging",
    "On GPU, we start copying indices, gradients and hessians now, instead at ConstructHistogram()",
    "copy used gradients and hessians to ordered buffer",
    "transfer the indices to GPU",
    "transfer hessian to GPU",
    "setup hessian parameters only",
    "hessian is passed as a parameter",
    "transfer gradients to GPU",
    "only have root",
    "Copy indices, gradients and hessians as early as possible",
    "only need to initialize for smaller leaf",
    "Get leaf boundary",
    "copy indices to the GPU:",
    "copy ordered hessians to the GPU:",
    "copy ordered gradients to the GPU:",
    "do nothing if no features can be processed on GPU",
    "copy data indices if it is not null",
    "generate and copy ordered_gradients if gradients is not null",
    "generate and copy ordered_hessians if hessians is not null",
    "converted indices in is_feature_used to feature-group indices",
    "construct the feature masks for dense feature-groups",
    "if no feature group is used, just return and do not use GPU",
    "if not all feature groups are used, we need to transfer the feature mask to GPU",
    "otherwise, we will use a specialized GPU kernel with all feature groups enabled",
    "All data have been prepared, now run the GPU kernel",
    "construct smaller leaf",
    "ConstructGPUHistogramsAsync will return true if there are availabe feature gourps dispatched to GPU",
    "then construct sparse features on CPU",
    "wait for GPU to finish, only if GPU is actually used",
    "use double precision",
    "use single precision",
    "Compare GPU histogram with CPU histogram, useful for debuggin GPU code problem",
    "#define GPU_DEBUG_COMPARE",
    "construct larger leaf",
    "then construct sparse features on CPU",
    "wait for GPU to finish, only if GPU is actually used",
    "use double precision",
    "use single precision",
    "do some sanity check for the GPU algorithm",
    "limit top k",
    "get max bin",
    "calculate buffer size",
    "need to be able to hold smaller and larger best splits in SyncUpGlobalBestSplit",
    "left and right on same time, so need double size",
    "initialize histograms for global",
    "sync global data sumup info",
    "set global sumup info",
    "init global data count in leaf",
    "get local sumup",
    "get local sumup",
    "get mean number on machines",
    "weighted gain",
    "get top k",
    "Copy histogram to buffer, and Get local aggregate features",
    "copy histograms.",
    "copy smaller leaf histograms first",
    "mark local aggregated feature",
    "copy",
    "then copy larger leaf histograms",
    "mark local aggregated feature",
    "copy",
    "use local data to find local best splits",
    "find splits",
    "only has root leaf",
    "local voting",
    "gather",
    "get all top-k from all machines",
    "global voting",
    "copy local histgrams to buffer",
    "Reduce scatter for histogram",
    "find best split from local aggregated histograms",
    "restore from buffer",
    "restore from buffer",
    "find local best",
    "find local best split for larger leaf",
    "sync global best info",
    "copy back",
    "set the global number of data for leaves",
    "init the global sumup info",
    "instantiate template classes, otherwise linker cannot find the code",
    "launch cuda kernel",
    "initialize SerialTreeLearner",
    "some additional variables needed for GPU trainer",
    "Initialize GPU buffers and kernels: get device info",
    "some functions used for debugging the GPU histogram construction",
    "we roughly want 256 workgroups per device, and we have num_dense_feature4_ feature tuples.",
    "also guarantee that there are at least 2K examples per workgroup",
    "we have already copied ordered gradients, ordered hessians and indices to GPU",
    "decide the best number of workgroups working on one feature4 tuple",
    "set work group size based on feature size",
    "each 2^exp_workgroups_per_feature workgroups work on a feature4 tuple",
    "set thread_data",
    "copy the results asynchronously. Size depends on if double precision is used",
    "when the output is ready, the computation is done",
    "how many feature-group tuples we have",
    "leave some safe margin for prefetching",
    "256 work-items per workgroup. Each work-item prefetches one tuple for that feature",
    "clear sparse/dense maps",
    "do nothing it there is no dense feature",
    "calculate number of feature groups per gpu",
    "histogram bin entry size depends on the precision (single/double)",
    "allocate GPU memory for each GPU",
    "do nothing it there is no gpu feature",
    "allocate memory for all features",
    "allocate space for gradients and hessians on device",
    "we will copy gradients and hessians in after ordered_gradients_ and ordered_hessians_ are constructed",
    "copy indices to the device",
    "create output buffer, each feature has a histogram with device_bin_size_ bins,",
    "each work group generates a sub-histogram of dword_features_ features.",
    "only initialize once here, as this will not need to change when ResetTrainingData() is called",
    "create atomic counters for inter-group coordination",
    "The output buffer is allocated to host directly, to overlap compute and data transfer",
    "clear sparse/dense maps",
    "find the dense feature-groups and group then into Feature4 data structure (several feature-groups packed into 4 bytes)",
    "set device info",
    "looking for dword_features_ non-sparse feature-groups",
    "reset device info",
    "InitGPU w/ num_gpu",
    "Get the max bin size, used for selecting best GPU kernel",
    "get num_dense_feature_groups_",
    "initialize GPU",
    "set cpu threads",
    "resize device memory pointers",
    "create stream & events to handle multiple GPUs",
    "check data size",
    "GPU memory has to been reallocated because data may have been changed",
    "AllocateGPUMemory only when the number of data increased",
    "setup GPU kernel arguments after we allocating all the buffers",
    "Copy initial full hessians and gradients to GPU.",
    "We start copying as early as possible, instead of at ConstructHistogram().",
    "use bagging",
    "On GPU, we start copying indices, gradients and hessians now, instead at ConstructHistogram()",
    "copy used gradients and hessians to ordered buffer",
    "transfer the indices to GPU",
    "only have root",
    "Copy indices, gradients and hessians as early as possible",
    "only need to initialize for smaller leaf",
    "Get leaf boundary",
    "do nothing if no features can be processed on GPU",
    "copy data indices if it is not null",
    "converted indices in is_feature_used to feature-group indices",
    "construct the feature masks for dense feature-groups",
    "if no feature group is used, just return and do not use GPU",
    "if not all feature groups are used, we need to transfer the feature mask to GPU",
    "otherwise, we will use a specialized GPU kernel with all feature groups enabled",
    "We now copy even if all features are used.",
    "All data have been prepared, now run the GPU kernel",
    "construct smaller leaf",
    "Check workgroups per feature4 tuple..",
    "if the workgroup per feature is 1 (2^0), return as the work is too small for a GPU",
    "ConstructGPUHistogramsAsync will return true if there are availabe feature groups dispatched to GPU",
    "then construct sparse features on CPU",
    "We set data_indices to null to avoid rebuilding ordered gradients/hessians",
    "wait for GPU to finish, only if GPU is actually used",
    "use double precision",
    "use single precision",
    "Compare GPU histogram with CPU histogram, useful for debuggin GPU code problem",
    "#define CUDA_DEBUG_COMPARE",
    "construct larger leaf",
    "then construct sparse features on CPU",
    "We set data_indices to null to avoid rebuilding ordered gradients/hessians",
    "wait for GPU to finish, only if GPU is actually used",
    "use double precision",
    "use single precision",
    "do some sanity check for the GPU algorithm"
  ],
  "v3.1.0": [
    "coding: utf-8",
    "coding: utf-8",
    "create predictor first",
    "check dataset",
    "reduce cost for prediction training data",
    "process callbacks",
    "Most of legacy advanced options becomes callbacks",
    "construct booster",
    "start training",
    "check evaluation result.",
    "ranking task, split according to groups",
    "run preprocessing on the data set if needed",
    "setup callbacks",
    "coding: utf-8",
    "simplejson does not support Python 3.2, it throws a SyntaxError",
    "because of u'...' Unicode literals.",
    "dummy function to support older version of scikit-learn",
    "DeprecationWarning is not shown by default, so let's create our own with higher level",
    "coding: utf-8",
    "user can set verbose with kwargs, it has higher priority",
    "Do not modify original args in fit function",
    "Refer to https://github.com/microsoft/LightGBM/pull/2619",
    "Separate built-in from callable evaluation metrics",
    "register default metric for consistency with callable eval_metric case",
    "try to deduce from class instance",
    "overwrite default metric by explicitly set metric",
    "concatenate metric from params (or default if not provided in params) and eval_metric",
    "copy for consistency",
    "reduce cost for prediction training data",
    "free dataset",
    "Switch to using a multiclass objective in the underlying LGBM instance",
    "do not modify args, as it causes errors in model selection tools",
    "check group data",
    "coding: utf-8",
    "we don't need lib_lightgbm while building docs",
    "coding: utf-8",
    "\"#ddffdd\" is light green, \"#ffdddd\" is light red",
    "coding: utf-8",
    "REMOVEME: remove warning after 3.1.0 version release",
    "coding: utf-8",
    "TypeError: obj is not a string or a number",
    "ValueError: invalid literal",
    "__get_num_preds() cannot work with nrow > MAX_INT32, so calculate overall number of predictions piecemeal",
    "avoid memory consumption by arrays concatenation operations",
    "create numpy array from output arrays",
    "break up indptr based on number of rows (note more than one matrix in multiclass case)",
    "for CSC there is extra column added",
    "reformat output into a csr or csc matrix or list of csr or csc matrices",
    "same shape as input csr or csc matrix except extra column for expected value",
    "note: make sure we copy data as it will be deallocated next",
    "free the temporary native indptr, indices, and data",
    "__get_num_preds() cannot work with nrow > MAX_INT32, so calculate overall number of predictions piecemeal",
    "avoid memory consumption by arrays concatenation operations",
    "no min_data, nthreads and verbose in this function",
    "check data has header or not",
    "need to regroup init_score",
    "process for args",
    "user can set verbose with params, it has higher priority",
    "get categorical features",
    "process for reference dataset",
    "start construct data",
    "set feature names",
    "create valid",
    "construct subset",
    "create train",
    "could be updated if data is not freed",
    "set to None",
    "we're done if self and reference share a common upstrem reference",
    "group data from LightGBM is boundaries data, need to convert to group size",
    "user can set verbose with params, it has higher priority",
    "Training task",
    "set network if necessary",
    "construct booster object",
    "copy the parameters from train_set",
    "save reference to data",
    "buffer for inner predict",
    "Prediction task",
    "if a single node tree it won't have `leaf_index` so return 0",
    "Create the node record, and populate universal data members",
    "Update values to reflect node type (leaf or split)",
    "traverse the next level of the tree",
    "In tree format, \"subtree_list\" is a list of node records (dicts),",
    "and we add node to the list.",
    "need reset training data",
    "need to push new valid data",
    "if buffer length is not long enough, re-allocate a buffer",
    "if buffer length is not long enough, reallocate a buffer",
    "Copy models",
    "Get name of features",
    "avoid to predict many time in one iteration",
    "Get num of inner evals",
    "Get name of evals",
    "coding: utf-8",
    "Callback environment used by callbacks",
    "split is needed for \"<dataset type> <metric>\" case (e.g. \"train l1\")",
    "split is needed for \"<dataset type> <metric>\" case (e.g. \"train l1\")",
    "coding: utf-8",
    "load or create your dataset",
    "create dataset for lightgbm",
    "if you want to re-use data, remember to set free_raw_data=False",
    "specify your configurations as a dict",
    "generate feature names",
    "feature_name and categorical_feature",
    "check feature name",
    "save model to file",
    "dump model to JSON (and save to file)",
    "feature names",
    "feature importances",
    "load model to predict",
    "can only predict with the best iteration (or the saving iteration)",
    "eval with loaded model",
    "dump model with pickle",
    "load model with pickle to predict",
    "can predict with any iteration when loaded in pickle way",
    "eval with loaded model",
    "continue training",
    "init_model accepts:",
    "1. model file name",
    "2. Booster()",
    "decay learning rates",
    "learning_rates accepts:",
    "1. list/tuple with length = num_boost_round",
    "2. function(curr_iter)",
    "change other parameters during training",
    "self-defined objective function",
    "f(preds: array, train_data: Dataset) -> grad: array, hess: array",
    "log likelihood loss",
    "self-defined eval metric",
    "f(preds: array, train_data: Dataset) -> name: string, eval_result: float, is_higher_better: bool",
    "binary error",
    "NOTE: when you do customized loss function, the default prediction value is margin",
    "This may make built-in evalution metric calculate wrong results",
    "For example, we are doing log likelihood loss, the prediction is score before logistic transformation",
    "Keep this in mind when you use the customization",
    "another self-defined eval metric",
    "f(preds: array, train_data: Dataset) -> name: string, eval_result: float, is_higher_better: bool",
    "accuracy",
    "NOTE: when you do customized loss function, the default prediction value is margin",
    "This may make built-in evalution metric calculate wrong results",
    "For example, we are doing log likelihood loss, the prediction is score before logistic transformation",
    "Keep this in mind when you use the customization",
    "callback",
    "coding: utf-8",
    "load or create your dataset",
    "train",
    "predict",
    "eval",
    "feature importances",
    "self-defined eval metric",
    "f(y_true: array, y_pred: array) -> name: string, eval_result: float, is_higher_better: bool",
    "Root Mean Squared Logarithmic Error (RMSLE)",
    "train",
    "another self-defined eval metric",
    "f(y_true: array, y_pred: array) -> name: string, eval_result: float, is_higher_better: bool",
    "Relative Absolute Error (RAE)",
    "train",
    "predict",
    "eval",
    "other scikit-learn modules",
    "coding: utf-8",
    "load or create your dataset",
    "create dataset for lightgbm",
    "specify your configurations as a dict",
    "train",
    "coding: utf-8",
    "################",
    "Simulate some binary data with a single categorical and",
    "single continuous predictor",
    "################",
    "Set up a couple of utilities for our experiments",
    "################",
    "Observe the behavior of `binary` and `xentropy` objectives",
    "Trying this throws an error on non-binary values of y:",
    "experiment('binary', label_type='probability', DATA)",
    "The speed of `binary` is not drastically different than",
    "`xentropy`. `xentropy` runs faster than `binary` in many cases, although",
    "there are reasons to suspect that `binary` should run faster when the",
    "label is an integer instead of a float",
    "coding: utf-8",
    "load or create your dataset",
    "create dataset for lightgbm",
    "specify your configurations as a dict",
    "train",
    "save model to file",
    "predict",
    "eval",
    "coding: utf-8",
    "!/usr/bin/env python3",
    "-*- coding: utf-8 -*-",
    "",
    "LightGBM documentation build configuration file, created by",
    "sphinx-quickstart on Thu May  4 14:30:58 2017.",
    "",
    "This file is execfile()d with the current directory set to its",
    "containing dir.",
    "",
    "Note that not all possible configuration values are present in this",
    "autogenerated file.",
    "",
    "All configuration values have a default; values that are commented out",
    "serve to show the default.",
    "If extensions (or modules to document with autodoc) are in another directory,",
    "add these directories to sys.path here. If the directory is relative to the",
    "documentation root, use os.path.abspath to make it absolute.",
    "-- mock out modules",
    "-- General configuration ------------------------------------------------",
    "If your documentation needs a minimal Sphinx version, state it here.",
    "Add any Sphinx extension module names here, as strings. They can be",
    "extensions coming with Sphinx (named 'sphinx.ext.*') or your custom",
    "ones.",
    "Generate autosummary pages. Output should be set with: `:toctree: pythonapi/`",
    "Only the class' docstring is inserted.",
    "If true, `todo` and `todoList` produce output, else they produce nothing.",
    "The master toctree document.",
    "General information about the project.",
    "The name of an image file (relative to this directory) to place at the top",
    "of the sidebar.",
    "The name of an image file (relative to this directory) to use as a favicon of",
    "the docs.  This file should be a Windows icon file (.ico) being 16x16 or 32x32",
    "pixels large.",
    "The version info for the project you're documenting, acts as replacement for",
    "|version| and |release|, also used in various other places throughout the",
    "built documents.",
    "The short X.Y version.",
    "The full version, including alpha/beta/rc tags.",
    "The language for content autogenerated by Sphinx. Refer to documentation",
    "for a list of supported languages.",
    "",
    "This is also used if you do content translation via gettext catalogs.",
    "Usually you set \"language\" from the command line for these cases.",
    "List of patterns, relative to source directory, that match files and",
    "directories to ignore when looking for source files.",
    "This patterns also effect to html_static_path and html_extra_path",
    "The name of the Pygments (syntax highlighting) style to use.",
    "-- Configuration for C API docs generation ------------------------------",
    "-- Options for HTML output ----------------------------------------------",
    "The theme to use for HTML and HTML Help pages.  See the documentation for",
    "a list of builtin themes.",
    "Theme options are theme-specific and customize the look and feel of a theme",
    "further.  For a list of options available for each theme, see the",
    "documentation.",
    "Add any paths that contain custom static files (such as style sheets) here,",
    "relative to this directory. They are copied after the builtin static files,",
    "so a file named \"default.css\" will overwrite the builtin \"default.css\".",
    "-- Options for HTMLHelp output ------------------------------------------",
    "Output file base name for HTML help builder.",
    "-- Options for LaTeX output ---------------------------------------------",
    "The name of an image file (relative to this directory) to place at the top of",
    "the title page.",
    "Warning! The following code can cause buffer overflows on RTD.",
    "Consider suppressing output completely if RTD project silently fails.",
    "Refer to https://github.com/svenevs/exhale",
    "/blob/fe7644829057af622e467bb529db6c03a830da99/exhale/deploy.py#L99-L111",
    "Warning! The following code can cause buffer overflows on RTD.",
    "Consider suppressing output completely if RTD project silently fails.",
    "Refer to https://github.com/svenevs/exhale",
    "/blob/fe7644829057af622e467bb529db6c03a830da99/exhale/deploy.py#L99-L111",
    "coding: utf-8",
    "coding: utf-8",
    "we don't need lib_lightgbm while building docs",
    "coding: utf-8",
    "check saved model persistence",
    "we need to check the consistency of model file here, so test for exact equal",
    "check early stopping is working. Make it stop very early, so the scores should be very close to zero",
    "scores likely to be different, but prediction should still be the same",
    "test that shape is checked during prediction",
    "test that method works even with free_raw_data=True",
    "test that method works but sets raw data to None in case of immergeable data types",
    "test that method works for different data types",
    "Set extremely harsh penalties, so CEGB will block most splits.",
    "Compare pairs of penalties, to ensure scaling works as intended",
    "Reset booster1's parameters to p2, so the parameter section of the file matches.",
    "coding: utf-8",
    "coding: utf-8",
    "prediction result is actually not transformed (is raw) due to custom objective",
    "sklearn <0.23 does not have a stacking classifier and n_features_in_ property",
    "sklearn <0.23 does not have a stacking regressor and n_features_in_ property",
    "sklearn < 0.22 does not have the post fit attribute: classes_",
    "sklearn < 0.23 does not have as_frame parameter",
    "sklearn < 0.22 does not have the post fit attribute: classes_",
    "sklearn < 0.23 does not have as_frame parameter",
    "Test if random_state is properly stored",
    "Test if two random states produce identical models",
    "Test if subsequent fits sample from random_state object and produce different models",
    "Test that the largest element is NOT the same, the smallest can be the same, i.e. zero",
    "With default params",
    "Tests same probabilities",
    "Tests same predictions",
    "Tests same raw scores",
    "Tests same leaf indices",
    "Tests same feature contributions",
    "Tests other parameters for the prediction works",
    "Tests start_iteration",
    "Tests same probabilities, starting from iteration 10",
    "Tests same predictions, starting from iteration 10",
    "Tests same raw scores, starting from iteration 10",
    "Tests same leaf indices, starting from iteration 10",
    "Tests same feature contributions, starting from iteration 10",
    "Tests other parameters for the prediction works, starting from iteration 10",
    "no custom objective, no custom metric",
    "default metric",
    "non-default metric",
    "no metric",
    "non-default metric in eval_metric",
    "non-default metric with non-default metric in eval_metric",
    "non-default metric with multiple metrics in eval_metric",
    "non-default metric with multiple metrics in eval_metric for LGBMClassifier",
    "default metric for non-default objective",
    "non-default metric for non-default objective",
    "no metric",
    "non-default metric in eval_metric for non-default objective",
    "non-default metric with non-default metric in eval_metric for non-default objective",
    "non-default metric with multiple metrics in eval_metric for non-default objective",
    "custom objective, no custom metric",
    "default regression metric for custom objective",
    "non-default regression metric for custom objective",
    "multiple regression metrics for custom objective",
    "no metric",
    "default regression metric with non-default metric in eval_metric for custom objective",
    "non-default regression metric with metric in eval_metric for custom objective",
    "multiple regression metrics with metric in eval_metric for custom objective",
    "multiple regression metrics with multiple metrics in eval_metric for custom objective",
    "no custom objective, custom metric",
    "default metric with custom metric",
    "non-default metric with custom metric",
    "multiple metrics with custom metric",
    "custom metric (disable default metric)",
    "default metric for non-default objective with custom metric",
    "non-default metric for non-default objective with custom metric",
    "multiple metrics for non-default objective with custom metric",
    "custom metric (disable default metric for non-default objective)",
    "custom objective, custom metric",
    "custom metric for custom objective",
    "non-default regression metric with custom metric for custom objective",
    "multiple regression metrics with custom metric for custom objective",
    "default metric and invalid binary metric is replaced with multiclass alternative",
    "invalid objective is replaced with default multiclass one",
    "and invalid binary metric is replaced with multiclass alternative",
    "default metric for non-default multiclass objective",
    "and invalid binary metric is replaced with multiclass alternative",
    "default metric and invalid multiclass metric is replaced with binary alternative",
    "invalid multiclass metric is replaced with binary alternative for custom objective",
    "Verify that can receive a list of metrics, only callable",
    "Verify that can receive a list of custom and built-in metrics",
    "Verify that works as expected when eval_metric is empty",
    "Verify that can receive a list of metrics, only built-in",
    "Verify that eval_metric is robust to receiving a list with None",
    "training data as eval_set",
    "feval",
    "single eval_set",
    "two eval_set",
    "sklearn < 0.22 requires passing \"attributes\" argument",
    "Test that estimators are default-constructible",
    "coding: utf-8",
    "coding: utf-8",
    "check that default gives same result as k = 1",
    "check against independent calculation for k = 1",
    "check against independent calculation for k = 2",
    "check against independent calculation for k = 10",
    "check cases where predictions are equal",
    "should give same result as binary auc for 2 classes",
    "test the case where all predictions are equal",
    "test that weighted data gives different auc_mu",
    "test that equal data weights give same auc_mu as unweighted data",
    "should give 1 when accuracy = 1",
    "test loading class weights",
    "no early stopping",
    "early stopping occurs",
    "test custom eval metrics",
    "shuffle = False, override metric in params",
    "shuffle = True, callbacks",
    "enable display training loss",
    "self defined folds",
    "lambdarank",
    "... with l2 metric",
    "... with NDCG (default) metric",
    "self defined folds with lambdarank",
    "with early stopping",
    "predict by each fold booster",
    "fold averaging",
    "without early stopping",
    "test feature_names with whitespaces",
    "This has non-ascii strings.",
    "take subsets and train",
    "generate CSR sparse dataset",
    "convert data to dense and get back same contribs",
    "validate the values are the same",
    "validate using CSC matrix",
    "validate the values are the same",
    "generate CSR sparse dataset",
    "convert data to dense and get back same contribs",
    "validate the values are the same",
    "validate using CSC matrix",
    "validate the values are the same",
    "Note there is an extra column added to the output for the expected value",
    "Note output CSC shape should be same as CSR output shape",
    "test sliced labels",
    "append some columns",
    "append some rows",
    "test sliced 2d matrix",
    "test sliced CSR",
    "test if a penalty as high as the depth indeed prohibits all monotone splits",
    "The penalization is so high that the first 2 features should not be used here",
    "Check that a very high penalization is the same as not using the features at all",
    "no fobj, no feval",
    "default metric",
    "non-default metric in params",
    "default metric in args",
    "non-default metric in args",
    "metric in args overwrites one in params",
    "multiple metrics in params",
    "multiple metrics in args",
    "remove default metric by 'None' in list",
    "remove default metric by 'None' aliases",
    "fobj, no feval",
    "no default metric",
    "metric in params",
    "metric in args",
    "metric in args overwrites its' alias in params",
    "multiple metrics in params",
    "multiple metrics in args",
    "no fobj, feval",
    "default metric with custom one",
    "non-default metric in params with custom one",
    "default metric in args with custom one",
    "non-default metric in args with custom one",
    "metric in args overwrites one in params, custom one is evaluated too",
    "multiple metrics in params with custom one",
    "multiple metrics in args with custom one",
    "custom metric is evaluated despite 'None' is passed",
    "fobj, feval",
    "no default metric, only custom one",
    "metric in params with custom one",
    "metric in args with custom one",
    "metric in args overwrites one in params, custom one is evaluated too",
    "multiple metrics in params with custom one",
    "multiple metrics in args with custom one",
    "custom metric is evaluated despite 'None' is passed",
    "no fobj, no feval",
    "default metric",
    "default metric in params",
    "non-default metric in params",
    "multiple metrics in params",
    "remove default metric by 'None' aliases",
    "fobj, no feval",
    "no default metric",
    "metric in params",
    "multiple metrics in params",
    "no fobj, feval",
    "default metric with custom one",
    "default metric in params with custom one",
    "non-default metric in params with custom one",
    "multiple metrics in params with custom one",
    "custom metric is evaluated despite 'None' is passed",
    "fobj, feval",
    "no default metric, only custom one",
    "metric in params with custom one",
    "multiple metrics in params with custom one",
    "custom metric is evaluated despite 'None' is passed",
    "multiclass default metric",
    "multiclass default metric with custom one",
    "multiclass metric alias with custom one for custom objective",
    "no metric for invalid class_num",
    "custom metric for invalid class_num",
    "multiclass metric alias with custom one with invalid class_num",
    "multiclass default metric without num_class",
    "multiclass metric alias",
    "multiclass metric",
    "non-valid metric for multiclass objective",
    "non-default num_class for default objective",
    "no metric with non-default num_class for custom objective",
    "multiclass metric alias for custom objective",
    "multiclass metric for custom objective",
    "binary metric with non-default num_class for custom objective",
    "Expect three metrics but mean and stdv for each metric",
    "test XGBoost-style return value",
    "test numpy-style return value",
    "test bins string type",
    "test histogram is disabled for categorical features",
    "test for lgb.train",
    "test feval for lgb.train",
    "test with two valid data for lgb.train",
    "test for lgb.cv",
    "test feval for lgb.cv",
    "test that binning works properly for features with only positive or only negative values",
    "decreasing without freeing raw data is allowed",
    "decreasing before lazy init is allowed",
    "increasing is allowed",
    "decreasing with disabled filter is allowed",
    "decreasing with enabled filter is disallowed;",
    "also changes of other params are disallowed",
    "check extra trees increases regularization",
    "check path smoothing increases regularization",
    "test edge case with one leaf",
    "check that constraint containing all features is equivalent to no constraint",
    "check that constraint partitioning the features reduces train accuracy",
    "check that constraints consisting of single features reduce accuracy further",
    "test that interaction constraints work when not all features are used",
    "test that the predict once with all iterations equals summed results with start_iteration and num_iteration",
    "test the case where start_iteration <= 0, and num_iteration is None",
    "test the case where start_iteration > 0, and num_iteration <= 0",
    "test the case where start_iteration > 0, and num_iteration <= 0, with pred_leaf=True",
    "test the case where start_iteration > 0, and num_iteration <= 0, with pred_contrib=True",
    "test for regression",
    "test both with and without early stopping",
    "test for multi-class",
    "test both with and without early stopping",
    "test for binary",
    "test both with and without early stopping",
    "test against sklearn average precision metric",
    "test that average precision is 1 where model predicts perfectly",
    "coding: utf-8",
    "convert from one-based to  zero-based index",
    "convert from boundaries to size",
    "--- start Booster interfaces",
    ".Call() calls",
    "coding: utf-8",
    "alias table",
    "names",
    "from strings",
    "tails",
    "tails",
    "coding: utf-8",
    "Single row predictor to abstract away caching logic",
    "create boosting",
    "initialize the boosting",
    "create objective function",
    "initialize the objective function",
    "create training metric",
    "reset the boosting",
    "create objective function",
    "initialize the objective function",
    "calculate the nonzero data and indices size",
    "allocate data and indices arrays",
    "Get the number of trees per iteration (for multiclass scenario we output multiple sparse matrices)",
    "aggregated per row feature contribution results",
    "keep track of the row_vector sizes for parallelization",
    "copy vector results to output for each row",
    "Get the number of trees per iteration (for multiclass scenario we output multiple sparse matrices)",
    "aggregated per row feature contribution results",
    "calculate number of elements per column to construct",
    "the CSC matrix with random access",
    "keep track of column counts",
    "keep track of beginning index for each column",
    "keep track of beginning index for each matrix",
    "Note: we parallelize across matrices instead of rows because of the column_counts[m][col_idx] increment inside the loop",
    "store the row index",
    "update column count",
    "explicitly declare symbols from LightGBM namespace",
    "some help functions used to convert data",
    "Row iterator of on column for CSC matrix",
    "return value at idx, only can access by ascent order",
    "return next non-zero pair, if index < 0, means no more data",
    "start of c_api functions",
    "sample data first",
    "sample data first",
    "sample data first",
    "local buffer to re-use memory",
    "sample data first",
    "no more data",
    "---- start of booster",
    "Single row in row-major format:",
    "---- start of some help functions",
    "data is array of pointers to individual rows",
    "set number of threads for openmp",
    "check for alias",
    "read parameters from config file",
    "remove str after \"#\"",
    "check for alias again",
    "load configs",
    "prediction is needed if using input initial model(continued train)",
    "need to continue training",
    "sync up random seed for data partition",
    "load Training data",
    "load data for parallel training",
    "load data for single machine",
    "need save binary file",
    "create training metric",
    "only when have metrics then need to construct validation data",
    "Add validation data, if it exists",
    "add",
    "need save binary file",
    "add metric for validation data",
    "output used time on each iteration",
    "need init network",
    "create boosting",
    "create objective function",
    "load training data",
    "initialize the objective function",
    "initialize the boosting",
    "add validation data into boosting",
    "convert model to if-else statement code",
    "create predictor",
    "Free memory",
    "create predictor",
    "label_gain = 2^i - 1, may overflow, so we use 31 here",
    "counts for all labels",
    "start from top label, and accumulate DCG",
    "counts for all labels",
    "calculate k Max DCG by one pass",
    "get sorted indices by score",
    "calculate dcg",
    "get sorted indices by score",
    "calculate multi dcg by one pass",
    "wait for all client start up",
    "Don't call MPI_Finalize() here: If the destructor was called because only this node had an exception, calling MPI_Finalize() will cause all nodes to hang.",
    "Instead we will handle finalize/abort for MPI in main().",
    "default set to -1",
    "distance at k-th communication, distance[k] = 2^k",
    "set incoming rank at k-th commuication",
    "set outgoing rank at k-th commuication",
    "defalut set as -1",
    "construct all recursive halving map for all machines",
    "let 1 << k <= num_machines",
    "distance of each communication",
    "if num_machines = 2^k, don't need to group machines",
    "communication direction, %2 == 0 is positive",
    "neighbor at k-th communication",
    "receive data block at k-th communication",
    "send data block at k-th communication",
    "if num_machines != 2^k, need to group machines",
    "group, two machine in one group, total \"rest\" groups will have 2 machines.",
    "let left machine as group leader",
    "cache block information for groups, group with 2 machines will have double block size",
    "convert from group to node leader",
    "convert from node to group",
    "meet new group",
    "add block len for this group",
    "calculate the group block start",
    "not need to construct",
    "get receive block informations",
    "accumulate block len",
    "get send block informations",
    "accumulate block len",
    "static member definition",
    "if small package or small count , do it by all gather.(reduce the communication times.)",
    "assign the blocks to every rank.",
    "do reduce scatter",
    "do all gather",
    "assign blocks",
    "need use buffer here, since size of \"output\" is smaller than size after all gather",
    "copy back",
    "assign blocks",
    "start all gather",
    "when num_machines is small and data is large",
    "use output as receive buffer",
    "get current local block size",
    "get out rank",
    "get in rank",
    "get send information",
    "get recv information",
    "send and recv at same time",
    "rotate in-place",
    "use output as receive buffer",
    "get current local block size",
    "get send information",
    "get recv information",
    "send and recv at same time",
    "use output as receive buffer",
    "send and recv at same time",
    "send local data to neighbor first",
    "receive neighbor data first",
    "reduce",
    "get target",
    "get send information",
    "get recv information",
    "send and recv at same time",
    "reduce",
    "send result to neighbor",
    "receive result from neighbor",
    "copy result",
    "start up socket",
    "parse clients from file",
    "get ip list of local machine",
    "get local rank",
    "construct listener",
    "construct communication topo",
    "construct linkers",
    "free listener",
    "set timeout",
    "accept incoming socket",
    "receive rank",
    "add new socket",
    "save ranks that need to connect with",
    "start listener",
    "start connect",
    "let smaller rank connect to larger rank",
    "send local rank",
    "wait for listener",
    "print connected linkers",
    "only need to copy subset",
    "avoid to copy subset many times",
    "avoid out of range",
    "may need to recopy subset",
    "valid the type",
    "Constructors",
    "Get type tag",
    "Comparisons",
    "This has to be separate, not in Statics, because Json() accesses",
    "statics().null.",
    "advance until next line, or end of input",
    "advance until closing tokens",
    "The usual case: non-escaped characters",
    "Handle escapes",
    "Extract 4-byte escape sequence",
    "Explicitly check length of the substring. The following loop",
    "relies on std::string returning the terminating NUL when",
    "accessing str[length]. Checking here reduces brittleness.",
    "JSON specifies that characters outside the BMP shall be encoded as a",
    "pair of 4-hex-digit \\u escapes encoding their surrogate pair",
    "components. Check whether we're in the middle of such a beast: the",
    "previous codepoint was an escaped lead (high) surrogate, and this is",
    "a trail (low) surrogate.",
    "Reassemble the two surrogate pairs into one astral-plane character,",
    "per the UTF-16 algorithm.",
    "Integer part",
    "Decimal part",
    "Exponent part",
    "Check for any trailing garbage",
    "Documented in json11.hpp",
    "Check for another object",
    "get column names",
    "load label idx first",
    "erase label column name",
    "load ignore columns",
    "load weight idx",
    "load group idx",
    "don't support query id in data file when training in parallel",
    "read data to memory",
    "sample data",
    "construct feature bin mappers",
    "initialize label",
    "extract features",
    "sample data from file",
    "construct feature bin mappers",
    "initialize label",
    "extract features",
    "load data from binary file",
    "check meta data",
    "need to check training data",
    "read data in memory",
    "initialize label",
    "extract features",
    "Get number of lines of data file",
    "initialize label",
    "extract features",
    "load data from binary file",
    "not need to check validation data",
    "check meta data",
    "buffer to read binary file",
    "check token",
    "read size of header",
    "re-allocmate space if not enough",
    "read header",
    "get header",
    "num_groups",
    "real_feature_idx_",
    "feature2group",
    "feature2subfeature",
    "group_bin_boundaries",
    "group_feature_start_",
    "group_feature_cnt_",
    "get feature names",
    "write feature names",
    "get forced_bin_bounds_",
    "read size of meta data",
    "re-allocate space if not enough",
    "read meta data",
    "load meta data",
    "sample local used data if need to partition",
    "if not contain query file, minimal sample unit is one record",
    "if contain query file, minimal sample unit is one query",
    "if is new query",
    "read feature data",
    "read feature size",
    "re-allocate space if not enough",
    "fill feature_names_ if not header",
    "get forced split",
    "if only one machine, find bin locally",
    "if have multi-machines, need to find bin distributed",
    "different machines will find bin for different features",
    "start and len will store the process feature indices for different machines",
    "machine i will find bins for features in [ start[i], start[i] + len[i] )",
    "free",
    "gather global feature bin mappers",
    "restore features bins from buffer",
    "---- private functions ----",
    "if features are ordered, not need to use hist_buf",
    "read all lines",
    "get query data",
    "if not contain query data, minimal sample unit is one record",
    "if contain query data, minimal sample unit is one query",
    "if is new query",
    "get query data",
    "if not contain query file, minimal sample unit is one record",
    "if contain query file, minimal sample unit is one query",
    "if is new query",
    "parse features",
    "get forced split",
    "check the range of label_idx, weight_idx and group_idx",
    "fill feature_names_ if not header",
    "start find bins",
    "if only one machine, find bin locally",
    "start and len will store the process feature indices for different machines",
    "machine i will find bins for features in [ start[i], start[i] + len[i] )",
    "free",
    "gather global feature bin mappers",
    "restore features bins from buffer",
    "if doesn't need to prediction with initial model",
    "parser",
    "set label",
    "free processed line:",
    "shrink_to_fit will be very slow in linux, and seems not free memory, disable for now",
    "text_reader_->Lines()[i].shrink_to_fit();",
    "push data",
    "if is used feature",
    "if need to prediction with initial model",
    "parser",
    "set initial score",
    "set label",
    "free processed line:",
    "shrink_to_fit will be very slow in linux, and seems not free memory, disable for now",
    "text_reader_->Lines()[i].shrink_to_fit();",
    "push data",
    "if is used feature",
    "metadata_ will manage space of init_score",
    "text data can be free after loaded feature values",
    "parser",
    "set initial score",
    "set label",
    "push data",
    "if is used feature",
    "only need part of data",
    "need full data",
    "metadata_ will manage space of init_score",
    "read size of token",
    "remove duplicates",
    "deep copy function for BinMapper",
    "mean size for one bin",
    "need a new bin",
    "update bin upper bound",
    "last bin upper bound",
    "get list of distinct values",
    "get number of positive and negative distinct values",
    "include zero bounds and infinity bound",
    "add forced bounds, excluding zeros since we have already added zero bounds",
    "find remaining bounds",
    "find distinct_values first",
    "push zero in the front",
    "use the large value",
    "push zero in the back",
    "convert to int type first",
    "sort by counts",
    "will ignore the categorical of small counts",
    "Push the dummy bin for NaN",
    "Use MissingType::None to represent this bin contains all categoricals",
    "fix count of NaN bin",
    "check trivial(num_bin_ == 1) feature",
    "check useless bin",
    "When most_freq_bin_ != default_bin_, there are some additional data loading costs.",
    "so use most_freq_bin_  = default_bin_ when there is not so sparse",
    "calculate max bin of all features to select the int type in MultiValDenseBin",
    "for lambdarank, it needs query data for partition data in parallel learning",
    "need convert query_id to boundaries",
    "check weights",
    "check query boundries",
    "contain initial score file",
    "check weights",
    "get local weights",
    "check query boundries",
    "get local query boundaries",
    "contain initial score file",
    "get local initial scores",
    "re-load query weight",
    "save to nullptr",
    "save to nullptr",
    "save to nullptr",
    "default weight file name",
    "default init_score file name",
    "use first line to count number class",
    "default query file name",
    "root is in the depth 0",
    "non-leaf",
    "leaf",
    "use this for the missing value conversion",
    "Predict func by Map to ifelse",
    "use this for the missing value conversion",
    "non-leaf",
    "left subtree",
    "right subtree",
    "leaf",
    "non-leaf",
    "left subtree",
    "right subtree",
    "leaf",
    "recursive computation of SHAP values for a decision tree",
    "extend the unique path",
    "leaf node",
    "internal node",
    "see if we have already split on this feature,",
    "if so we undo that split so we can redo it for this node",
    "recursive sparse computation of SHAP values for a decision tree",
    "extend the unique path",
    "leaf node",
    "internal node",
    "see if we have already split on this feature,",
    "if so we undo that split so we can redo it for this node",
    "add names of objective function if not providing metric",
    "equal weights for all classes",
    "generate seeds by seed.",
    "sort eval_at",
    "Only push the non-training data",
    "check for conflicts",
    "check if objective, metric, and num_class match",
    "Change pool size to -1 (no limit) when using data parallel to reduce communication costs",
    "Check max_depth and num_leaves",
    "Fits in an int, and is more restrictive than the current num_leaves",
    "force col-wise for gpu & CUDA",
    "force gpu_use_dp for CUDA",
    "min_data_in_leaf must be at least 2 if path smoothing is active. This is because when the split is calculated",
    "the count is calculated using the proportion of hessian in the leaf which is rounded up to nearest int, so it can",
    "be 1 when there is actually no data in the leaf. In rare cases this can cause a bug because with path smoothing the",
    "calculated split gain can be positive even with zero gradient and hessian.",
    "In distributed mode, local node doesn't have histograms on all features, cannot perform \"intermediate\" monotone constraints.",
    "\"intermediate\" monotone constraints need to recompute splits. If the features are sampled when computing the",
    "split initially, then the sampling needs to be recorded or done once again, which is currently not supported",
    "first round: fill the single val group",
    "always push the last group",
    "put dense feature first",
    "sort by non zero cnt",
    "sort by non zero cnt, bigger first",
    "shuffle groups",
    "Using std::swap for vector<bool> will cause the wrong result.",
    "get num_features",
    "get bin_mappers",
    "for sparse multi value bin, we store the feature bin values with offset added",
    "for dense multi value bin, the feature bin values without offsets are used",
    "copy feature bin mapper data",
    "if not pass a filename, just append \".bin\" of original file",
    "get size of header",
    "size of feature names",
    "size of forced bins",
    "write header",
    "write feature names",
    "write forced bins",
    "get size of meta data",
    "write meta data",
    "write feature data",
    "get size of feature",
    "write feature",
    "explicitly initilize template methods, for cross module call",
    "Only one multi-val group, just simply merge",
    "Skip the leading 0 when copying group_bin_boundaries.",
    "regenerate other fields",
    "store the importance first",
    "PredictRaw",
    "PredictRawByMap",
    "Predict",
    "PredictByMap",
    "PredictLeafIndex",
    "PredictLeafIndexByMap",
    "output model type",
    "output number of class",
    "output label index",
    "output max_feature_idx",
    "output objective",
    "output tree models",
    "store the importance first",
    "sort the importance",
    "use serialized string to restore this object",
    "Use first 128 chars to avoid exceed the message buffer.",
    "get number of classes",
    "get index of label",
    "get max_feature_idx first",
    "get average_output",
    "get feature names",
    "get monotone_constraints",
    "set zero",
    "predict all the trees for one iteration",
    "check early stopping",
    "set zero",
    "predict all the trees for one iteration",
    "check early stopping",
    "margin_threshold will be captured by value",
    "copy and sort",
    "margin_threshold will be captured by value",
    "Fix for compiler warnings about reaching end of control",
    "load forced_splits file",
    "init tree learner",
    "push training metrics",
    "create buffer for gradients and hessians",
    "get max feature index",
    "get label index",
    "get feature names",
    "if need bagging, create buffer",
    "for a validation dataset, we need its score and metric",
    "update score",
    "objective function will calculate gradients and hessians",
    "random bagging, minimal unit is one record",
    "random bagging, minimal unit is one record",
    "if need bagging",
    "set bagging data to tree learner",
    "get subset",
    "output used time per iteration",
    "boosting from average label; or customized \"average\" if implemented for the current objective",
    "boosting first",
    "bagging logic",
    "need to copy gradients for bagging subset.",
    "shrinkage by learning rate",
    "update score",
    "only add default score one-time",
    "updates scores",
    "add model",
    "reset score",
    "remove model",
    "print message for metric",
    "pop last early_stopping_round_ models",
    "update training score",
    "we need to predict out-of-bag scores of data for boosting",
    "update validation score",
    "print training metric",
    "print validation metric",
    "set zero",
    "predict all the trees for one iteration",
    "predict all the trees for one iteration",
    "push training metrics",
    "not same training data, need reset score and others",
    "create score tracker",
    "update score",
    "create buffer for gradients and hessians",
    "load forced_splits file",
    "if need bagging, create buffer",
    "Get the max size of pool",
    "at least need 2 leaves",
    "push split information for all leaves",
    "initialize splits for leaf",
    "initialize data partition",
    "initialize ordered gradients and hessians",
    "cannot change is_hist_col_wise during training",
    "initialize splits for leaf",
    "initialize data partition",
    "initialize ordered gradients and hessians",
    "Get the max size of pool",
    "at least need 2 leaves",
    "push split information for all leaves",
    "some initial works before training",
    "root leaf",
    "only root leaf can be splitted on first time",
    "some initial works before finding best split",
    "find best threshold for every feature",
    "Get a leaf with max split gain",
    "Get split information for best leaf",
    "cannot split, quit",
    "split tree with best leaf",
    "reset histogram pool",
    "initialize data partition",
    "reset the splits for leaves",
    "Sumup for root",
    "use all data",
    "use bagging, only use part of data",
    "check depth of current leaf",
    "only need to check left leaf, since right leaf is in same level of left leaf",
    "no enough data to continue",
    "only have root",
    "put parent(left) leaf's histograms into larger leaf's histograms",
    "put parent(left) leaf's histograms to larger leaf's histograms",
    "construct smaller leaf",
    "construct larger leaf",
    "find splits",
    "only has root leaf",
    "start at root leaf",
    "before processing next node from queue, store info for current left/right leaf",
    "store \"best split\" for left and right, even if they might be overwritten by forced split",
    "then, compute own splits",
    "split info should exist because searching in bfs fashion - should have added from parent",
    "update before tree split",
    "don't need to update this in data-based parallel model",
    "split tree, will return right leaf",
    "store the true split gain in tree model",
    "don't need to update this in data-based parallel model",
    "store the true split gain in tree model",
    "init the leaves that used on next iteration",
    "update leave outputs if needed",
    "bag_mapper[index_mapper[i]]",
    "it is needed to filter the features after the above code.",
    "Otherwise, the `is_splittable` in `FeatureHistogram` will be wrong, and cause some features being accidentally filtered in the later nodes.",
    "for root leaf the \"parent\" output is its own output because we don't apply any smoothing to the root",
    "can't use GetParentOutput because leaf_splits doesn't have weight property set",
    "find splits",
    "need to be able to hold smaller and larger best splits in SyncUpGlobalBestSplit",
    "get feature partition",
    "get local used features",
    "get best split at smaller leaf",
    "find local best split for larger leaf",
    "sync global best info",
    "update best split",
    "instantiate template classes, otherwise linker cannot find the code",
    "initialize SerialTreeLearner",
    "Get local rank and global machine size",
    "need to be able to hold smaller and larger best splits in SyncUpGlobalBestSplit",
    "allocate buffer for communication",
    "generate feature partition for current tree",
    "get local used feature",
    "get block start and block len for reduce scatter",
    "get buffer_write_start_pos_",
    "get buffer_read_start_pos_",
    "sync global data sumup info",
    "global sumup reduce",
    "copy back",
    "set global sumup info",
    "init global data count in leaf",
    "construct local histograms",
    "copy to buffer",
    "Reduce scatter for histogram",
    "restore global histograms from buffer",
    "only root leaf",
    "construct histgroms for large leaf, we init larger leaf as the parent, so we can just subtract the smaller leaf's histograms",
    "find local best split for larger leaf",
    "sync global best info",
    "set best split",
    "need update global number of data in leaf",
    "instantiate template classes, otherwise linker cannot find the code",
    "initialize SerialTreeLearner",
    "some additional variables needed for GPU trainer",
    "Initialize GPU buffers and kernels",
    "some functions used for debugging the GPU histogram construction",
    "printf(\"grad %g != %g (%d ULPs)\\n\", GET_GRAD(h1, i), GET_GRAD(h2, i), ulps);",
    "goto err;",
    "we roughly want 256 workgroups per device, and we have num_dense_feature4_ feature tuples.",
    "also guarantee that there are at least 2K examples per workgroup",
    "return 0;",
    "we have already copied ordered gradients, ordered hessians and indices to GPU",
    "decide the best number of workgroups working on one feature4 tuple",
    "set work group size based on feature size",
    "each 2^exp_workgroups_per_feature workgroups work on a feature4 tuple",
    "we need to refresh the kernel arguments after reallocating",
    "The only argument that needs to be changed later is num_data_",
    "the GPU kernel will process all features in one call, and each",
    "2^exp_workgroups_per_feature (compile time constant) workgroup will",
    "process one feature4 tuple",
    "for the root node, indices are not copied",
    "for constant hessian, hessians are not copied except for the root node",
    "there will be 2^exp_workgroups_per_feature = num_workgroups / num_dense_feature4 sub-histogram per feature4",
    "and we will launch num_feature workgroups for this kernel",
    "will launch threads for all features",
    "the queue should be asynchrounous, and we will can WaitAndGetHistograms() before we start processing dense feature groups",
    "copy the results asynchronously. Size depends on if double precision is used",
    "we will wait for this object in WaitAndGetHistograms",
    "when the output is ready, the computation is done",
    "values of this feature has been redistributed to multiple bins; need a reduction here",
    "how many feature-group tuples we have",
    "leave some safe margin for prefetching",
    "256 work-items per workgroup. Each work-item prefetches one tuple for that feature",
    "clear sparse/dense maps",
    "do nothing if no features can be processed on GPU",
    "allocate memory for all features (FIXME: 4 GB barrier on some devices, need to split to multiple buffers)",
    "unpin old buffer if necessary before destructing them",
    "make ordered_gradients and hessians larger (including extra room for prefetching), and pin them",
    "allocate space for gradients and hessians on device",
    "we will copy gradients and hessians in after ordered_gradients_ and ordered_hessians_ are constructed",
    "allocate feature mask, for disabling some feature-groups' histogram calculation",
    "copy indices to the device",
    "histogram bin entry size depends on the precision (single/double)",
    "create output buffer, each feature has a histogram with device_bin_size_ bins,",
    "each work group generates a sub-histogram of dword_features_ features.",
    "only initialize once here, as this will not need to change when ResetTrainingData() is called",
    "create atomic counters for inter-group coordination",
    "The output buffer is allocated to host directly, to overlap compute and data transfer",
    "find the dense feature-groups and group then into Feature4 data structure (several feature-groups packed into 4 bytes)",
    "looking for dword_features_ non-sparse feature-groups",
    "decide if we need to redistribute the bin",
    "multiplier must be a power of 2",
    "device_bin_mults_.push_back(1);",
    "found",
    "for data transfer time",
    "Now generate new data structure feature4, and copy data to the device",
    "preallocate arrays for all threads, and pin them",
    "building Feature4 bundles; each thread handles dword_features_ features",
    "one feature datapoint is 4 bits",
    "this guarantees that the RawGet() function is inlined, rather than using virtual function dispatching",
    "one feature datapoint is one byte",
    "this guarantees that the RawGet() function is inlined, rather than using virtual function dispatching",
    "Dense bin",
    "Dense 4-bit bin",
    "working on the remaining (less than dword_features_) feature groups",
    "fill the leftover features",
    "fill this empty feature with some \"random\" value",
    "fill this empty feature with some \"random\" value",
    "copying the last 1 to (dword_features - 1) feature-groups in the last tuple",
    "deallocate pinned space for feature copying",
    "data transfer time",
    "for other types of failure, build log might not be available; program.build_log() can crash",
    "Something bad happened. Just return \"No log available.\"",
    "build is okay, log may contain warnings",
    "destroy any old kernels",
    "create OpenCL kernels for different number of workgroups per feature",
    "currently we don't use constant memory",
    "compile the GPU kernel depending if double precision is used, constant hessian is used, etc.",
    "kernel with indices in an array",
    "kernel with all features enabled, with elimited branches",
    "kernel with all data indices (for root node, and assumes that root node always uses all features)",
    "do nothing if no features can be processed on GPU",
    "The only argument that needs to be changed later is num_data_",
    "hessian is passed as a parameter, but it is not available now.",
    "hessian will be set in BeforeTrain()",
    "Get the max bin size, used for selecting best GPU kernel",
    "initialize GPU",
    "determine which kernel to use based on the max number of bins",
    "the +9 skips extra characters \")\", newline, \"#endif\" and newline at the beginning",
    "the +9 skips extra characters \")\", newline, \"#endif\" and newline at the beginning",
    "the +9 skips extra characters \")\", newline, \"#endif\" and newline at the beginning",
    "setup GPU kernel arguments after we allocating all the buffers",
    "GPU memory has to been reallocated because data may have been changed",
    "setup GPU kernel arguments after we allocating all the buffers",
    "Copy initial full hessians and gradients to GPU.",
    "We start copying as early as possible, instead of at ConstructHistogram().",
    "setup hessian parameters only",
    "hessian is passed as a parameter",
    "use bagging",
    "On GPU, we start copying indices, gradients and hessians now, instead at ConstructHistogram()",
    "copy used gradients and hessians to ordered buffer",
    "transfer the indices to GPU",
    "transfer hessian to GPU",
    "setup hessian parameters only",
    "hessian is passed as a parameter",
    "transfer gradients to GPU",
    "only have root",
    "Copy indices, gradients and hessians as early as possible",
    "only need to initialize for smaller leaf",
    "Get leaf boundary",
    "copy indices to the GPU:",
    "copy ordered hessians to the GPU:",
    "copy ordered gradients to the GPU:",
    "do nothing if no features can be processed on GPU",
    "copy data indices if it is not null",
    "generate and copy ordered_gradients if gradients is not null",
    "generate and copy ordered_hessians if hessians is not null",
    "converted indices in is_feature_used to feature-group indices",
    "construct the feature masks for dense feature-groups",
    "if no feature group is used, just return and do not use GPU",
    "if not all feature groups are used, we need to transfer the feature mask to GPU",
    "otherwise, we will use a specialized GPU kernel with all feature groups enabled",
    "All data have been prepared, now run the GPU kernel",
    "construct smaller leaf",
    "ConstructGPUHistogramsAsync will return true if there are availabe feature gourps dispatched to GPU",
    "then construct sparse features on CPU",
    "wait for GPU to finish, only if GPU is actually used",
    "use double precision",
    "use single precision",
    "Compare GPU histogram with CPU histogram, useful for debuggin GPU code problem",
    "#define GPU_DEBUG_COMPARE",
    "construct larger leaf",
    "then construct sparse features on CPU",
    "wait for GPU to finish, only if GPU is actually used",
    "use double precision",
    "use single precision",
    "do some sanity check for the GPU algorithm",
    "limit top k",
    "get max bin",
    "calculate buffer size",
    "need to be able to hold smaller and larger best splits in SyncUpGlobalBestSplit",
    "left and right on same time, so need double size",
    "initialize histograms for global",
    "sync global data sumup info",
    "set global sumup info",
    "init global data count in leaf",
    "get local sumup",
    "get local sumup",
    "get mean number on machines",
    "weighted gain",
    "get top k",
    "Copy histogram to buffer, and Get local aggregate features",
    "copy histograms.",
    "copy smaller leaf histograms first",
    "mark local aggregated feature",
    "copy",
    "then copy larger leaf histograms",
    "mark local aggregated feature",
    "copy",
    "use local data to find local best splits",
    "find splits",
    "only has root leaf",
    "local voting",
    "gather",
    "get all top-k from all machines",
    "global voting",
    "copy local histgrams to buffer",
    "Reduce scatter for histogram",
    "find best split from local aggregated histograms",
    "restore from buffer",
    "restore from buffer",
    "find local best",
    "find local best split for larger leaf",
    "sync global best info",
    "copy back",
    "set the global number of data for leaves",
    "init the global sumup info",
    "instantiate template classes, otherwise linker cannot find the code",
    "launch cuda kernel",
    "initialize SerialTreeLearner",
    "some additional variables needed for GPU trainer",
    "Initialize GPU buffers and kernels: get device info",
    "some functions used for debugging the GPU histogram construction",
    "we roughly want 256 workgroups per device, and we have num_dense_feature4_ feature tuples.",
    "also guarantee that there are at least 2K examples per workgroup",
    "we have already copied ordered gradients, ordered hessians and indices to GPU",
    "decide the best number of workgroups working on one feature4 tuple",
    "set work group size based on feature size",
    "each 2^exp_workgroups_per_feature workgroups work on a feature4 tuple",
    "set thread_data",
    "copy the results asynchronously. Size depends on if double precision is used",
    "when the output is ready, the computation is done",
    "how many feature-group tuples we have",
    "leave some safe margin for prefetching",
    "256 work-items per workgroup. Each work-item prefetches one tuple for that feature",
    "clear sparse/dense maps",
    "do nothing it there is no dense feature",
    "calculate number of feature groups per gpu",
    "histogram bin entry size depends on the precision (single/double)",
    "allocate GPU memory for each GPU",
    "do nothing it there is no gpu feature",
    "allocate memory for all features",
    "allocate space for gradients and hessians on device",
    "we will copy gradients and hessians in after ordered_gradients_ and ordered_hessians_ are constructed",
    "copy indices to the device",
    "create output buffer, each feature has a histogram with device_bin_size_ bins,",
    "each work group generates a sub-histogram of dword_features_ features.",
    "only initialize once here, as this will not need to change when ResetTrainingData() is called",
    "create atomic counters for inter-group coordination",
    "The output buffer is allocated to host directly, to overlap compute and data transfer",
    "clear sparse/dense maps",
    "find the dense feature-groups and group then into Feature4 data structure (several feature-groups packed into 4 bytes)",
    "set device info",
    "looking for dword_features_ non-sparse feature-groups",
    "reset device info",
    "InitGPU w/ num_gpu",
    "Get the max bin size, used for selecting best GPU kernel",
    "get num_dense_feature_groups_",
    "initialize GPU",
    "set cpu threads",
    "resize device memory pointers",
    "create stream & events to handle multiple GPUs",
    "check data size",
    "GPU memory has to been reallocated because data may have been changed",
    "AllocateGPUMemory only when the number of data increased",
    "setup GPU kernel arguments after we allocating all the buffers",
    "Copy initial full hessians and gradients to GPU.",
    "We start copying as early as possible, instead of at ConstructHistogram().",
    "use bagging",
    "On GPU, we start copying indices, gradients and hessians now, instead at ConstructHistogram()",
    "copy used gradients and hessians to ordered buffer",
    "transfer the indices to GPU",
    "only have root",
    "Copy indices, gradients and hessians as early as possible",
    "only need to initialize for smaller leaf",
    "Get leaf boundary",
    "do nothing if no features can be processed on GPU",
    "copy data indices if it is not null",
    "converted indices in is_feature_used to feature-group indices",
    "construct the feature masks for dense feature-groups",
    "if no feature group is used, just return and do not use GPU",
    "if not all feature groups are used, we need to transfer the feature mask to GPU",
    "otherwise, we will use a specialized GPU kernel with all feature groups enabled",
    "We now copy even if all features are used.",
    "All data have been prepared, now run the GPU kernel",
    "construct smaller leaf",
    "Check workgroups per feature4 tuple..",
    "if the workgroup per feature is 1 (2^0), return as the work is too small for a GPU",
    "ConstructGPUHistogramsAsync will return true if there are availabe feature groups dispatched to GPU",
    "then construct sparse features on CPU",
    "We set data_indices to null to avoid rebuilding ordered gradients/hessians",
    "wait for GPU to finish, only if GPU is actually used",
    "use double precision",
    "use single precision",
    "Compare GPU histogram with CPU histogram, useful for debuggin GPU code problem",
    "#define CUDA_DEBUG_COMPARE",
    "construct larger leaf",
    "then construct sparse features on CPU",
    "We set data_indices to null to avoid rebuilding ordered gradients/hessians",
    "wait for GPU to finish, only if GPU is actually used",
    "use double precision",
    "use single precision",
    "do some sanity check for the GPU algorithm"
  ],
  "v3.0.0": [
    "coding: utf-8",
    "coding: utf-8",
    "create predictor first",
    "check dataset",
    "reduce cost for prediction training data",
    "process callbacks",
    "Most of legacy advanced options becomes callbacks",
    "construct booster",
    "start training",
    "check evaluation result.",
    "ranking task, split according to groups",
    "run preprocessing on the data set if needed",
    "setup callbacks",
    "coding: utf-8",
    "simplejson does not support Python 3.2, it throws a SyntaxError",
    "because of u'...' Unicode literals.",
    "dummy function to support older version of scikit-learn",
    "DeprecationWarning is not shown by default, so let's create our own with higher level",
    "coding: utf-8",
    "user can set verbose with kwargs, it has higher priority",
    "register default metric for consistency with callable eval_metric case",
    "try to deduce from class instance",
    "overwrite default metric by explicitly set metric",
    "concatenate metric from params (or default if not provided in params) and eval_metric",
    "copy for consistency",
    "reduce cost for prediction training data",
    "free dataset",
    "Switch to using a multiclass objective in the underlying LGBM instance",
    "do not modify args, as it causes errors in model selection tools",
    "check group data",
    "coding: utf-8",
    "we don't need lib_lightgbm while building docs",
    "coding: utf-8",
    "\"#ddffdd\" is light green, \"#ffdddd\" is light red",
    "coding: utf-8",
    "coding: utf-8",
    "TypeError: obj is not a string or a number",
    "ValueError: invalid literal",
    "__get_num_preds() cannot work with nrow > MAX_INT32, so calculate overall number of predictions piecemeal",
    "avoid memory consumption by arrays concatenation operations",
    "create numpy array from output arrays",
    "break up indptr based on number of rows (note more than one matrix in multiclass case)",
    "for CSC there is extra column added",
    "reformat output into a csr or csc matrix or list of csr or csc matrices",
    "same shape as input csr or csc matrix except extra column for expected value",
    "note: make sure we copy data as it will be deallocated next",
    "free the temporary native indptr, indices, and data",
    "__get_num_preds() cannot work with nrow > MAX_INT32, so calculate overall number of predictions piecemeal",
    "avoid memory consumption by arrays concatenation operations",
    "no min_data, nthreads and verbose in this function",
    "check data has header or not",
    "need to regroup init_score",
    "process for args",
    "user can set verbose with params, it has higher priority",
    "get categorical features",
    "process for reference dataset",
    "start construct data",
    "set feature names",
    "create valid",
    "construct subset",
    "create train",
    "could be updated if data is not freed",
    "set to None",
    "we're done if self and reference share a common upstrem reference",
    "group data from LightGBM is boundaries data, need to convert to group size",
    "user can set verbose with params, it has higher priority",
    "Training task",
    "set network if necessary",
    "construct booster object",
    "copy the parameters from train_set",
    "save reference to data",
    "buffer for inner predict",
    "Prediction task",
    "if a single node tree it won't have `leaf_index` so return 0",
    "Create the node record, and populate universal data members",
    "Update values to reflect node type (leaf or split)",
    "traverse the next level of the tree",
    "In tree format, \"subtree_list\" is a list of node records (dicts),",
    "and we add node to the list.",
    "need reset training data",
    "need to push new valid data",
    "if buffer length is not long enough, re-allocate a buffer",
    "if buffer length is not long enough, reallocate a buffer",
    "Copy models",
    "Get name of features",
    "avoid to predict many time in one iteration",
    "Get num of inner evals",
    "Get name of evals",
    "coding: utf-8",
    "Callback environment used by callbacks",
    "split is needed for \"<dataset type> <metric>\" case (e.g. \"train l1\")",
    "split is needed for \"<dataset type> <metric>\" case (e.g. \"train l1\")",
    "coding: utf-8",
    "load or create your dataset",
    "create dataset for lightgbm",
    "if you want to re-use data, remember to set free_raw_data=False",
    "specify your configurations as a dict",
    "generate feature names",
    "feature_name and categorical_feature",
    "check feature name",
    "save model to file",
    "dump model to JSON (and save to file)",
    "feature names",
    "feature importances",
    "load model to predict",
    "can only predict with the best iteration (or the saving iteration)",
    "eval with loaded model",
    "dump model with pickle",
    "load model with pickle to predict",
    "can predict with any iteration when loaded in pickle way",
    "eval with loaded model",
    "continue training",
    "init_model accepts:",
    "1. model file name",
    "2. Booster()",
    "decay learning rates",
    "learning_rates accepts:",
    "1. list/tuple with length = num_boost_round",
    "2. function(curr_iter)",
    "change other parameters during training",
    "self-defined objective function",
    "f(preds: array, train_data: Dataset) -> grad: array, hess: array",
    "log likelihood loss",
    "self-defined eval metric",
    "f(preds: array, train_data: Dataset) -> name: string, eval_result: float, is_higher_better: bool",
    "binary error",
    "NOTE: when you do customized loss function, the default prediction value is margin",
    "This may make built-in evalution metric calculate wrong results",
    "For example, we are doing log likelihood loss, the prediction is score before logistic transformation",
    "Keep this in mind when you use the customization",
    "another self-defined eval metric",
    "f(preds: array, train_data: Dataset) -> name: string, eval_result: float, is_higher_better: bool",
    "accuracy",
    "NOTE: when you do customized loss function, the default prediction value is margin",
    "This may make built-in evalution metric calculate wrong results",
    "For example, we are doing log likelihood loss, the prediction is score before logistic transformation",
    "Keep this in mind when you use the customization",
    "callback",
    "coding: utf-8",
    "load or create your dataset",
    "train",
    "predict",
    "eval",
    "feature importances",
    "self-defined eval metric",
    "f(y_true: array, y_pred: array) -> name: string, eval_result: float, is_higher_better: bool",
    "Root Mean Squared Logarithmic Error (RMSLE)",
    "train",
    "another self-defined eval metric",
    "f(y_true: array, y_pred: array) -> name: string, eval_result: float, is_higher_better: bool",
    "Relative Absolute Error (RAE)",
    "train",
    "predict",
    "eval",
    "other scikit-learn modules",
    "coding: utf-8",
    "load or create your dataset",
    "create dataset for lightgbm",
    "specify your configurations as a dict",
    "train",
    "coding: utf-8",
    "################",
    "Simulate some binary data with a single categorical and",
    "single continuous predictor",
    "################",
    "Set up a couple of utilities for our experiments",
    "################",
    "Observe the behavior of `binary` and `xentropy` objectives",
    "Trying this throws an error on non-binary values of y:",
    "experiment('binary', label_type='probability', DATA)",
    "The speed of `binary` is not drastically different than",
    "`xentropy`. `xentropy` runs faster than `binary` in many cases, although",
    "there are reasons to suspect that `binary` should run faster when the",
    "label is an integer instead of a float",
    "coding: utf-8",
    "load or create your dataset",
    "create dataset for lightgbm",
    "specify your configurations as a dict",
    "train",
    "save model to file",
    "predict",
    "eval",
    "coding: utf-8",
    "!/usr/bin/env python3",
    "-*- coding: utf-8 -*-",
    "",
    "LightGBM documentation build configuration file, created by",
    "sphinx-quickstart on Thu May  4 14:30:58 2017.",
    "",
    "This file is execfile()d with the current directory set to its",
    "containing dir.",
    "",
    "Note that not all possible configuration values are present in this",
    "autogenerated file.",
    "",
    "All configuration values have a default; values that are commented out",
    "serve to show the default.",
    "If extensions (or modules to document with autodoc) are in another directory,",
    "add these directories to sys.path here. If the directory is relative to the",
    "documentation root, use os.path.abspath to make it absolute.",
    "-- mock out modules",
    "-- General configuration ------------------------------------------------",
    "If your documentation needs a minimal Sphinx version, state it here.",
    "Add any Sphinx extension module names here, as strings. They can be",
    "extensions coming with Sphinx (named 'sphinx.ext.*') or your custom",
    "ones.",
    "Generate autosummary pages. Output should be set with: `:toctree: pythonapi/`",
    "Only the class' docstring is inserted.",
    "If true, `todo` and `todoList` produce output, else they produce nothing.",
    "The master toctree document.",
    "General information about the project.",
    "The name of an image file (relative to this directory) to place at the top",
    "of the sidebar.",
    "The name of an image file (relative to this directory) to use as a favicon of",
    "the docs.  This file should be a Windows icon file (.ico) being 16x16 or 32x32",
    "pixels large.",
    "The version info for the project you're documenting, acts as replacement for",
    "|version| and |release|, also used in various other places throughout the",
    "built documents.",
    "The short X.Y version.",
    "The full version, including alpha/beta/rc tags.",
    "The language for content autogenerated by Sphinx. Refer to documentation",
    "for a list of supported languages.",
    "",
    "This is also used if you do content translation via gettext catalogs.",
    "Usually you set \"language\" from the command line for these cases.",
    "List of patterns, relative to source directory, that match files and",
    "directories to ignore when looking for source files.",
    "This patterns also effect to html_static_path and html_extra_path",
    "The name of the Pygments (syntax highlighting) style to use.",
    "-- Configuration for C API docs generation ------------------------------",
    "-- Options for HTML output ----------------------------------------------",
    "The theme to use for HTML and HTML Help pages.  See the documentation for",
    "a list of builtin themes.",
    "Theme options are theme-specific and customize the look and feel of a theme",
    "further.  For a list of options available for each theme, see the",
    "documentation.",
    "Add any paths that contain custom static files (such as style sheets) here,",
    "relative to this directory. They are copied after the builtin static files,",
    "so a file named \"default.css\" will overwrite the builtin \"default.css\".",
    "-- Options for HTMLHelp output ------------------------------------------",
    "Output file base name for HTML help builder.",
    "-- Options for LaTeX output ---------------------------------------------",
    "The name of an image file (relative to this directory) to place at the top of",
    "the title page.",
    "Warning! The following code can cause buffer overflows on RTD.",
    "Consider suppressing output completely if RTD project silently fails.",
    "Refer to https://github.com/svenevs/exhale",
    "/blob/fe7644829057af622e467bb529db6c03a830da99/exhale/deploy.py#L99-L111",
    "Warning! The following code can cause buffer overflows on RTD.",
    "Consider suppressing output completely if RTD project silently fails.",
    "Refer to https://github.com/svenevs/exhale",
    "/blob/fe7644829057af622e467bb529db6c03a830da99/exhale/deploy.py#L99-L111",
    "coding: utf-8",
    "coding: utf-8",
    "we don't need lib_lightgbm while building docs",
    "coding: utf-8",
    "check saved model persistence",
    "we need to check the consistency of model file here, so test for exact equal",
    "check early stopping is working. Make it stop very early, so the scores should be very close to zero",
    "scores likely to be different, but prediction should still be the same",
    "test that shape is checked during prediction",
    "Set extremely harsh penalties, so CEGB will block most splits.",
    "Compare pairs of penalties, to ensure scaling works as intended",
    "Reset booster1's parameters to p2, so the parameter section of the file matches.",
    "coding: utf-8",
    "prediction result is actually not transformed (is raw) due to custom objective",
    "sklearn <0.23 does not have a stacking classifier and n_features_in_ property",
    "sklearn <0.23 does not have a stacking regressor and n_features_in_ property",
    "sklearn < 0.22 does not have the post fit attribute: classes_",
    "sklearn < 0.23 does not have as_frame parameter",
    "sklearn < 0.22 does not have the post fit attribute: classes_",
    "sklearn < 0.23 does not have as_frame parameter",
    "Test if random_state is properly stored",
    "Test if two random states produce identical models",
    "Test if subsequent fits sample from random_state object and produce different models",
    "Test that the largest element is NOT the same, the smallest can be the same, i.e. zero",
    "sklearn <0.19 cannot accept instance, but many tests could be passed only with min_data=1 and min_data_in_bin=1",
    "we cannot use `check_estimator` directly since there is no skip test mechanism",
    "we cannot leave default params (see https://github.com/microsoft/LightGBM/issues/833)",
    "skip test because scikit-learn incorrectly asserts that",
    "private attributes cannot be set in __init__",
    "(see https://github.com/microsoft/LightGBM/issues/2628)",
    "With default params",
    "Tests same probabilities",
    "Tests same predictions",
    "Tests same raw scores",
    "Tests same leaf indices",
    "Tests same feature contributions",
    "Tests other parameters for the prediction works",
    "Tests start_iteration",
    "Tests same probabilities, starting from iteration 10",
    "Tests same predictions, starting from iteration 10",
    "Tests same raw scores, starting from iteration 10",
    "Tests same leaf indices, starting from iteration 10",
    "Tests same feature contributions, starting from iteration 10",
    "Tests other parameters for the prediction works, starting from iteration 10",
    "no custom objective, no custom metric",
    "default metric",
    "non-default metric",
    "no metric",
    "non-default metric in eval_metric",
    "non-default metric with non-default metric in eval_metric",
    "non-default metric with multiple metrics in eval_metric",
    "non-default metric with multiple metrics in eval_metric for LGBMClassifier",
    "default metric for non-default objective",
    "non-default metric for non-default objective",
    "no metric",
    "non-default metric in eval_metric for non-default objective",
    "non-default metric with non-default metric in eval_metric for non-default objective",
    "non-default metric with multiple metrics in eval_metric for non-default objective",
    "custom objective, no custom metric",
    "default regression metric for custom objective",
    "non-default regression metric for custom objective",
    "multiple regression metrics for custom objective",
    "no metric",
    "default regression metric with non-default metric in eval_metric for custom objective",
    "non-default regression metric with metric in eval_metric for custom objective",
    "multiple regression metrics with metric in eval_metric for custom objective",
    "multiple regression metrics with multiple metrics in eval_metric for custom objective",
    "no custom objective, custom metric",
    "default metric with custom metric",
    "non-default metric with custom metric",
    "multiple metrics with custom metric",
    "custom metric (disable default metric)",
    "default metric for non-default objective with custom metric",
    "non-default metric for non-default objective with custom metric",
    "multiple metrics for non-default objective with custom metric",
    "custom metric (disable default metric for non-default objective)",
    "custom objective, custom metric",
    "custom metric for custom objective",
    "non-default regression metric with custom metric for custom objective",
    "multiple regression metrics with custom metric for custom objective",
    "default metric and invalid binary metric is replaced with multiclass alternative",
    "invalid objective is replaced with default multiclass one",
    "and invalid binary metric is replaced with multiclass alternative",
    "default metric for non-default multiclass objective",
    "and invalid binary metric is replaced with multiclass alternative",
    "default metric and invalid multiclass metric is replaced with binary alternative",
    "invalid multiclass metric is replaced with binary alternative for custom objective",
    "training data as eval_set",
    "feval",
    "single eval_set",
    "two eval_set",
    "coding: utf-8",
    "coding: utf-8",
    "check that default gives same result as k = 1",
    "check against independent calculation for k = 1",
    "check against independent calculation for k = 2",
    "check against independent calculation for k = 10",
    "check cases where predictions are equal",
    "should give same result as binary auc for 2 classes",
    "test the case where all predictions are equal",
    "should give 1 when accuracy = 1",
    "test loading weights",
    "no early stopping",
    "early stopping occurs",
    "test custom eval metrics",
    "shuffle = False, override metric in params",
    "shuffle = True, callbacks",
    "enable display training loss",
    "self defined folds",
    "lambdarank",
    "... with l2 metric",
    "... with NDCG (default) metric",
    "self defined folds with lambdarank",
    "with early stopping",
    "predict by each fold booster",
    "fold averaging",
    "without early stopping",
    "test feature_names with whitespaces",
    "This has non-ascii strings.",
    "take subsets and train",
    "generate CSR sparse dataset",
    "convert data to dense and get back same contribs",
    "validate the values are the same",
    "validate using CSC matrix",
    "validate the values are the same",
    "Note there is an extra column added to the output for the expected value",
    "Note output CSC shape should be same as CSR output shape",
    "test sliced labels",
    "append some columns",
    "append some rows",
    "test sliced 2d matrix",
    "test sliced CSR",
    "test if a penalty as high as the depth indeed prohibits all monotone splits",
    "The penalization is so high that the first 2 features should not be used here",
    "Check that a very high penalization is the same as not using the features at all",
    "no fobj, no feval",
    "default metric",
    "non-default metric in params",
    "default metric in args",
    "non-default metric in args",
    "metric in args overwrites one in params",
    "multiple metrics in params",
    "multiple metrics in args",
    "remove default metric by 'None' in list",
    "remove default metric by 'None' aliases",
    "fobj, no feval",
    "no default metric",
    "metric in params",
    "metric in args",
    "metric in args overwrites its' alias in params",
    "multiple metrics in params",
    "multiple metrics in args",
    "no fobj, feval",
    "default metric with custom one",
    "non-default metric in params with custom one",
    "default metric in args with custom one",
    "non-default metric in args with custom one",
    "metric in args overwrites one in params, custom one is evaluated too",
    "multiple metrics in params with custom one",
    "multiple metrics in args with custom one",
    "custom metric is evaluated despite 'None' is passed",
    "fobj, feval",
    "no default metric, only custom one",
    "metric in params with custom one",
    "metric in args with custom one",
    "metric in args overwrites one in params, custom one is evaluated too",
    "multiple metrics in params with custom one",
    "multiple metrics in args with custom one",
    "custom metric is evaluated despite 'None' is passed",
    "no fobj, no feval",
    "default metric",
    "default metric in params",
    "non-default metric in params",
    "multiple metrics in params",
    "remove default metric by 'None' aliases",
    "fobj, no feval",
    "no default metric",
    "metric in params",
    "multiple metrics in params",
    "no fobj, feval",
    "default metric with custom one",
    "default metric in params with custom one",
    "non-default metric in params with custom one",
    "multiple metrics in params with custom one",
    "custom metric is evaluated despite 'None' is passed",
    "fobj, feval",
    "no default metric, only custom one",
    "metric in params with custom one",
    "multiple metrics in params with custom one",
    "custom metric is evaluated despite 'None' is passed",
    "multiclass default metric",
    "multiclass default metric with custom one",
    "multiclass metric alias with custom one for custom objective",
    "no metric for invalid class_num",
    "custom metric for invalid class_num",
    "multiclass metric alias with custom one with invalid class_num",
    "multiclass default metric without num_class",
    "multiclass metric alias",
    "multiclass metric",
    "non-valid metric for multiclass objective",
    "non-default num_class for default objective",
    "no metric with non-default num_class for custom objective",
    "multiclass metric alias for custom objective",
    "multiclass metric for custom objective",
    "binary metric with non-default num_class for custom objective",
    "test XGBoost-style return value",
    "test numpy-style return value",
    "test bins string type",
    "test histogram is disabled for categorical features",
    "test for lgb.train",
    "test feval for lgb.train",
    "test with two valid data for lgb.train",
    "test for lgb.cv",
    "test feval for lgb.cv",
    "test that binning works properly for features with only positive or only negative values",
    "decreasing without freeing raw data is allowed",
    "decreasing before lazy init is allowed",
    "increasing is allowed",
    "decreasing with disabled filter is allowed",
    "decreasing with enabled filter is disallowed;",
    "also changes of other params are disallowed",
    "check extra trees increases regularization",
    "check path smoothing increases regularization",
    "test edge case with one leaf",
    "check that constraint containing all features is equivalent to no constraint",
    "check that constraint partitioning the features reduces train accuracy",
    "check that constraints consisting of single features reduce accuracy further",
    "test that interaction constraints work when not all features are used",
    "test that the predict once with all iterations equals summed results with start_iteration and num_iteration",
    "test the case where start_iteration <= 0, and num_iteration is None",
    "test the case where start_iteration > 0, and num_iteration <= 0",
    "test the case where start_iteration > 0, and num_iteration <= 0, with pred_leaf=True",
    "test the case where start_iteration > 0, and num_iteration <= 0, with pred_contrib=True",
    "test for regression",
    "test both with and without early stopping",
    "test for multi-class",
    "test both with and without early stopping",
    "test for binary",
    "test both with and without early stopping",
    "coding: utf-8",
    "convert from one-based to  zero-based index",
    "convert from boundaries to size",
    "--- start Booster interfaces",
    ".Call() calls",
    "coding: utf-8",
    "alias table",
    "names",
    "from strings",
    "tails",
    "tails",
    "coding: utf-8",
    "Single row predictor to abstract away caching logic",
    "create boosting",
    "initialize the boosting",
    "create objective function",
    "initialize the objective function",
    "create training metric",
    "reset the boosting",
    "create objective function",
    "initialize the objective function",
    "calculate the nonzero data and indices size",
    "allocate data and indices arrays",
    "Get the number of trees per iteration (for multiclass scenario we output multiple sparse matrices)",
    "aggregated per row feature contribution results",
    "keep track of the row_vector sizes for parallelization",
    "copy vector results to output for each row",
    "Get the number of trees per iteration (for multiclass scenario we output multiple sparse matrices)",
    "aggregated per row feature contribution results",
    "calculate number of elements per column to construct",
    "the CSC matrix with random access",
    "keep track of column counts",
    "keep track of beginning index for each column",
    "keep track of beginning index for each matrix",
    "store the row index",
    "update column count",
    "explicitly declare symbols from LightGBM namespace",
    "some help functions used to convert data",
    "Row iterator of on column for CSC matrix",
    "return value at idx, only can access by ascent order",
    "return next non-zero pair, if index < 0, means no more data",
    "start of c_api functions",
    "sample data first",
    "sample data first",
    "sample data first",
    "local buffer to re-use memory",
    "sample data first",
    "no more data",
    "---- start of booster",
    "Single row in row-major format:",
    "---- start of some help functions",
    "data is array of pointers to individual rows",
    "set number of threads for openmp",
    "check for alias",
    "read parameters from config file",
    "remove str after \"#\"",
    "check for alias again",
    "load configs",
    "prediction is needed if using input initial model(continued train)",
    "need to continue training",
    "sync up random seed for data partition",
    "load Training data",
    "load data for parallel training",
    "load data for single machine",
    "need save binary file",
    "create training metric",
    "only when have metrics then need to construct validation data",
    "Add validation data, if it exists",
    "add",
    "need save binary file",
    "add metric for validation data",
    "output used time on each iteration",
    "need init network",
    "create boosting",
    "create objective function",
    "load training data",
    "initialize the objective function",
    "initialize the boosting",
    "add validation data into boosting",
    "convert model to if-else statement code",
    "create predictor",
    "Free memory",
    "create predictor",
    "label_gain = 2^i - 1, may overflow, so we use 31 here",
    "counts for all labels",
    "start from top label, and accumulate DCG",
    "counts for all labels",
    "calculate k Max DCG by one pass",
    "get sorted indices by score",
    "calculate dcg",
    "get sorted indices by score",
    "calculate multi dcg by one pass",
    "wait for all client start up",
    "Don't call MPI_Finalize() here: If the destructor was called because only this node had an exception, calling MPI_Finalize() will cause all nodes to hang.",
    "Instead we will handle finalize/abort for MPI in main().",
    "default set to -1",
    "distance at k-th communication, distance[k] = 2^k",
    "set incoming rank at k-th commuication",
    "set outgoing rank at k-th commuication",
    "defalut set as -1",
    "construct all recursive halving map for all machines",
    "let 1 << k <= num_machines",
    "distance of each communication",
    "if num_machines = 2^k, don't need to group machines",
    "communication direction, %2 == 0 is positive",
    "neighbor at k-th communication",
    "receive data block at k-th communication",
    "send data block at k-th communication",
    "if num_machines != 2^k, need to group machines",
    "group, two machine in one group, total \"rest\" groups will have 2 machines.",
    "let left machine as group leader",
    "cache block information for groups, group with 2 machines will have double block size",
    "convert from group to node leader",
    "convert from node to group",
    "meet new group",
    "add block len for this group",
    "calculate the group block start",
    "not need to construct",
    "get receive block informations",
    "accumulate block len",
    "get send block informations",
    "accumulate block len",
    "static member definition",
    "if small package or small count , do it by all gather.(reduce the communication times.)",
    "assign the blocks to every rank.",
    "do reduce scatter",
    "do all gather",
    "assign blocks",
    "need use buffer here, since size of \"output\" is smaller than size after all gather",
    "copy back",
    "assign blocks",
    "start all gather",
    "when num_machines is small and data is large",
    "use output as receive buffer",
    "get current local block size",
    "get out rank",
    "get in rank",
    "get send information",
    "get recv information",
    "send and recv at same time",
    "rotate in-place",
    "use output as receive buffer",
    "get current local block size",
    "get send information",
    "get recv information",
    "send and recv at same time",
    "use output as receive buffer",
    "send and recv at same time",
    "send local data to neighbor first",
    "receive neighbor data first",
    "reduce",
    "get target",
    "get send information",
    "get recv information",
    "send and recv at same time",
    "reduce",
    "send result to neighbor",
    "receive result from neighbor",
    "copy result",
    "start up socket",
    "parse clients from file",
    "get ip list of local machine",
    "get local rank",
    "construct listener",
    "construct communication topo",
    "construct linkers",
    "free listener",
    "set timeout",
    "accept incoming socket",
    "receive rank",
    "add new socket",
    "save ranks that need to connect with",
    "start listener",
    "start connect",
    "let smaller rank connect to larger rank",
    "send local rank",
    "wait for listener",
    "print connected linkers",
    "valid the type",
    "Constructors",
    "Get type tag",
    "Comparisons",
    "This has to be separate, not in Statics, because Json() accesses",
    "statics().null.",
    "advance until next line, or end of input",
    "advance until closing tokens",
    "The usual case: non-escaped characters",
    "Handle escapes",
    "Extract 4-byte escape sequence",
    "Explicitly check length of the substring. The following loop",
    "relies on std::string returning the terminating NUL when",
    "accessing str[length]. Checking here reduces brittleness.",
    "JSON specifies that characters outside the BMP shall be encoded as a",
    "pair of 4-hex-digit \\u escapes encoding their surrogate pair",
    "components. Check whether we're in the middle of such a beast: the",
    "previous codepoint was an escaped lead (high) surrogate, and this is",
    "a trail (low) surrogate.",
    "Reassemble the two surrogate pairs into one astral-plane character,",
    "per the UTF-16 algorithm.",
    "Integer part",
    "Decimal part",
    "Exponent part",
    "Check for any trailing garbage",
    "Documented in json11.hpp",
    "Check for another object",
    "get column names",
    "load label idx first",
    "erase label column name",
    "load ignore columns",
    "load weight idx",
    "load group idx",
    "don't support query id in data file when training in parallel",
    "read data to memory",
    "sample data",
    "construct feature bin mappers",
    "initialize label",
    "extract features",
    "sample data from file",
    "construct feature bin mappers",
    "initialize label",
    "extract features",
    "load data from binary file",
    "check meta data",
    "need to check training data",
    "read data in memory",
    "initialize label",
    "extract features",
    "Get number of lines of data file",
    "initialize label",
    "extract features",
    "load data from binary file",
    "not need to check validation data",
    "check meta data",
    "buffer to read binary file",
    "check token",
    "read size of header",
    "re-allocmate space if not enough",
    "read header",
    "get header",
    "num_groups",
    "real_feature_idx_",
    "feature2group",
    "feature2subfeature",
    "group_bin_boundaries",
    "group_feature_start_",
    "group_feature_cnt_",
    "get feature names",
    "write feature names",
    "get forced_bin_bounds_",
    "read size of meta data",
    "re-allocate space if not enough",
    "read meta data",
    "load meta data",
    "sample local used data if need to partition",
    "if not contain query file, minimal sample unit is one record",
    "if contain query file, minimal sample unit is one query",
    "if is new query",
    "read feature data",
    "read feature size",
    "re-allocate space if not enough",
    "fill feature_names_ if not header",
    "get forced split",
    "if only one machine, find bin locally",
    "if have multi-machines, need to find bin distributed",
    "different machines will find bin for different features",
    "start and len will store the process feature indices for different machines",
    "machine i will find bins for features in [ start[i], start[i] + len[i] )",
    "free",
    "gather global feature bin mappers",
    "restore features bins from buffer",
    "---- private functions ----",
    "if features are ordered, not need to use hist_buf",
    "read all lines",
    "get query data",
    "if not contain query data, minimal sample unit is one record",
    "if contain query data, minimal sample unit is one query",
    "if is new query",
    "get query data",
    "if not contain query file, minimal sample unit is one record",
    "if contain query file, minimal sample unit is one query",
    "if is new query",
    "parse features",
    "get forced split",
    "check the range of label_idx, weight_idx and group_idx",
    "fill feature_names_ if not header",
    "start find bins",
    "if only one machine, find bin locally",
    "start and len will store the process feature indices for different machines",
    "machine i will find bins for features in [ start[i], start[i] + len[i] )",
    "free",
    "gather global feature bin mappers",
    "restore features bins from buffer",
    "if doesn't need to prediction with initial model",
    "parser",
    "set label",
    "free processed line:",
    "shrink_to_fit will be very slow in linux, and seems not free memory, disable for now",
    "text_reader_->Lines()[i].shrink_to_fit();",
    "push data",
    "if is used feature",
    "if need to prediction with initial model",
    "parser",
    "set initial score",
    "set label",
    "free processed line:",
    "shrink_to_fit will be very slow in linux, and seems not free memory, disable for now",
    "text_reader_->Lines()[i].shrink_to_fit();",
    "push data",
    "if is used feature",
    "metadata_ will manage space of init_score",
    "text data can be free after loaded feature values",
    "parser",
    "set initial score",
    "set label",
    "push data",
    "if is used feature",
    "only need part of data",
    "need full data",
    "metadata_ will manage space of init_score",
    "read size of token",
    "remove duplicates",
    "deep copy function for BinMapper",
    "mean size for one bin",
    "need a new bin",
    "update bin upper bound",
    "last bin upper bound",
    "get list of distinct values",
    "get number of positive and negative distinct values",
    "include zero bounds and infinity bound",
    "add forced bounds, excluding zeros since we have already added zero bounds",
    "find remaining bounds",
    "find distinct_values first",
    "push zero in the front",
    "use the large value",
    "push zero in the back",
    "convert to int type first",
    "sort by counts",
    "will ignore the categorical of small counts",
    "Push the dummy bin for NaN",
    "Use MissingType::None to represent this bin contains all categoricals",
    "fix count of NaN bin",
    "check trivial(num_bin_ == 1) feature",
    "check useless bin",
    "When most_freq_bin_ != default_bin_, there are some additional data loading costs.",
    "so use most_freq_bin_  = default_bin_ when there is not so sparse",
    "for lambdarank, it needs query data for partition data in parallel learning",
    "need convert query_id to boundaries",
    "check weights",
    "check query boundries",
    "contain initial score file",
    "check weights",
    "get local weights",
    "check query boundries",
    "get local query boundaries",
    "contain initial score file",
    "get local initial scores",
    "re-load query weight",
    "save to nullptr",
    "save to nullptr",
    "save to nullptr",
    "default weight file name",
    "default init_score file name",
    "use first line to count number class",
    "default query file name",
    "root is in the depth 0",
    "non-leaf",
    "leaf",
    "use this for the missing value conversion",
    "Predict func by Map to ifelse",
    "use this for the missing value conversion",
    "non-leaf",
    "left subtree",
    "right subtree",
    "leaf",
    "non-leaf",
    "left subtree",
    "right subtree",
    "leaf",
    "recursive computation of SHAP values for a decision tree",
    "extend the unique path",
    "leaf node",
    "internal node",
    "see if we have already split on this feature,",
    "if so we undo that split so we can redo it for this node",
    "recursive sparse computation of SHAP values for a decision tree",
    "extend the unique path",
    "leaf node",
    "internal node",
    "see if we have already split on this feature,",
    "if so we undo that split so we can redo it for this node",
    "add names of objective function if not providing metric",
    "equal weights for all classes",
    "generate seeds by seed.",
    "sort eval_at",
    "Only push the non-training data",
    "check for conflicts",
    "check if objective, metric, and num_class match",
    "Change pool size to -1 (no limit) when using data parallel to reduce communication costs",
    "Check max_depth and num_leaves",
    "Fits in an int, and is more restrictive than the current num_leaves",
    "force col-wise for gpu",
    "min_data_in_leaf must be at least 2 if path smoothing is active. This is because when the split is calculated",
    "the count is calculated using the proportion of hessian in the leaf which is rounded up to nearest int, so it can",
    "be 1 when there is actually no data in the leaf. In rare cases this can cause a bug because with path smoothing the",
    "calculated split gain can be positive even with zero gradient and hessian.",
    "In distributed mode, local node doesn't have histograms on all features, cannot perform \"intermediate\" monotone constraints.",
    "\"intermediate\" monotone constraints need to recompute splits. If the features are sampled when computing the",
    "split initially, then the sampling needs to be recorded or done once again, which is currently not supported",
    "first round: fill the single val group",
    "always push the last group",
    "put dense feature first",
    "sort by non zero cnt",
    "sort by non zero cnt, bigger first",
    "shuffle groups",
    "Using std::swap for vector<bool> will cause the wrong result.",
    "get num_features",
    "get bin_mappers",
    "copy feature bin mapper data",
    "copy feature bin mapper data",
    "if not pass a filename, just append \".bin\" of original file",
    "get size of header",
    "size of feature names",
    "size of forced bins",
    "write header",
    "write feature names",
    "write forced bins",
    "get size of meta data",
    "write meta data",
    "write feature data",
    "get size of feature",
    "write feature",
    "only need to copy subset",
    "avoid to copy subset many times",
    "avoid out of range",
    "may need to recopy subset",
    "explicitly initilize template methods, for cross module call",
    "FIXME: fix the multiple multi-val feature groups, they need to be merged",
    "into one multi-val group",
    "Skip the leading 0 when copying group_bin_boundaries.",
    "store the importance first",
    "PredictRaw",
    "PredictRawByMap",
    "Predict",
    "PredictByMap",
    "PredictLeafIndex",
    "PredictLeafIndexByMap",
    "output model type",
    "output number of class",
    "output label index",
    "output max_feature_idx",
    "output objective",
    "output tree models",
    "store the importance first",
    "sort the importance",
    "use serialized string to restore this object",
    "Use first 128 chars to avoid exceed the message buffer.",
    "get number of classes",
    "get index of label",
    "get max_feature_idx first",
    "get average_output",
    "get feature names",
    "get monotone_constraints",
    "set zero",
    "predict all the trees for one iteration",
    "check early stopping",
    "set zero",
    "predict all the trees for one iteration",
    "check early stopping",
    "margin_threshold will be captured by value",
    "copy and sort",
    "margin_threshold will be captured by value",
    "Fix for compiler warnings about reaching end of control",
    "load forced_splits file",
    "init tree learner",
    "push training metrics",
    "create buffer for gradients and hessians",
    "get max feature index",
    "get label index",
    "get feature names",
    "if need bagging, create buffer",
    "for a validation dataset, we need its score and metric",
    "update score",
    "objective function will calculate gradients and hessians",
    "random bagging, minimal unit is one record",
    "random bagging, minimal unit is one record",
    "if need bagging",
    "set bagging data to tree learner",
    "get subset",
    "output used time per iteration",
    "boosting from average label; or customized \"average\" if implemented for the current objective",
    "boosting first",
    "bagging logic",
    "need to copy gradients for bagging subset.",
    "shrinkage by learning rate",
    "update score",
    "only add default score one-time",
    "updates scores",
    "add model",
    "reset score",
    "remove model",
    "print message for metric",
    "pop last early_stopping_round_ models",
    "update training score",
    "we need to predict out-of-bag scores of data for boosting",
    "update validation score",
    "print training metric",
    "print validation metric",
    "set zero",
    "predict all the trees for one iteration",
    "predict all the trees for one iteration",
    "push training metrics",
    "not same training data, need reset score and others",
    "create score tracker",
    "update score",
    "create buffer for gradients and hessians",
    "load forced_splits file",
    "if need bagging, create buffer",
    "Get the max size of pool",
    "at least need 2 leaves",
    "push split information for all leaves",
    "initialize splits for leaf",
    "initialize data partition",
    "initialize ordered gradients and hessians",
    "cannot change is_hist_col_wise during training",
    "initialize splits for leaf",
    "initialize data partition",
    "initialize ordered gradients and hessians",
    "Get the max size of pool",
    "at least need 2 leaves",
    "push split information for all leaves",
    "some initial works before training",
    "root leaf",
    "only root leaf can be splitted on first time",
    "some initial works before finding best split",
    "find best threshold for every feature",
    "Get a leaf with max split gain",
    "Get split information for best leaf",
    "cannot split, quit",
    "split tree with best leaf",
    "reset histogram pool",
    "initialize data partition",
    "reset the splits for leaves",
    "Sumup for root",
    "use all data",
    "use bagging, only use part of data",
    "check depth of current leaf",
    "only need to check left leaf, since right leaf is in same level of left leaf",
    "no enough data to continue",
    "only have root",
    "put parent(left) leaf's histograms into larger leaf's histograms",
    "put parent(left) leaf's histograms to larger leaf's histograms",
    "construct smaller leaf",
    "construct larger leaf",
    "find splits",
    "only has root leaf",
    "start at root leaf",
    "before processing next node from queue, store info for current left/right leaf",
    "store \"best split\" for left and right, even if they might be overwritten by forced split",
    "then, compute own splits",
    "split info should exist because searching in bfs fashion - should have added from parent",
    "update before tree split",
    "don't need to update this in data-based parallel model",
    "split tree, will return right leaf",
    "store the true split gain in tree model",
    "don't need to update this in data-based parallel model",
    "store the true split gain in tree model",
    "init the leaves that used on next iteration",
    "update leave outputs if needed",
    "bag_mapper[index_mapper[i]]",
    "for root leaf the \"parent\" output is its own output because we don't apply any smoothing to the root",
    "find splits",
    "need to be able to hold smaller and larger best splits in SyncUpGlobalBestSplit",
    "get feature partition",
    "get local used features",
    "get best split at smaller leaf",
    "find local best split for larger leaf",
    "sync global best info",
    "update best split",
    "instantiate template classes, otherwise linker cannot find the code",
    "initialize SerialTreeLearner",
    "Get local rank and global machine size",
    "need to be able to hold smaller and larger best splits in SyncUpGlobalBestSplit",
    "allocate buffer for communication",
    "generate feature partition for current tree",
    "get local used feature",
    "get block start and block len for reduce scatter",
    "get buffer_write_start_pos_",
    "get buffer_read_start_pos_",
    "sync global data sumup info",
    "global sumup reduce",
    "copy back",
    "set global sumup info",
    "init global data count in leaf",
    "construct local histograms",
    "copy to buffer",
    "Reduce scatter for histogram",
    "restore global histograms from buffer",
    "only root leaf",
    "construct histgroms for large leaf, we init larger leaf as the parent, so we can just subtract the smaller leaf's histograms",
    "find local best split for larger leaf",
    "sync global best info",
    "set best split",
    "need update global number of data in leaf",
    "instantiate template classes, otherwise linker cannot find the code",
    "initialize SerialTreeLearner",
    "some additional variables needed for GPU trainer",
    "Initialize GPU buffers and kernels",
    "some functions used for debugging the GPU histogram construction",
    "printf(\"grad %g != %g (%d ULPs)\\n\", GET_GRAD(h1, i), GET_GRAD(h2, i), ulps);",
    "goto err;",
    "we roughly want 256 workgroups per device, and we have num_dense_feature4_ feature tuples.",
    "also guarantee that there are at least 2K examples per workgroup",
    "return 0;",
    "we have already copied ordered gradients, ordered hessians and indices to GPU",
    "decide the best number of workgroups working on one feature4 tuple",
    "set work group size based on feature size",
    "each 2^exp_workgroups_per_feature workgroups work on a feature4 tuple",
    "we need to refresh the kernel arguments after reallocating",
    "The only argument that needs to be changed later is num_data_",
    "the GPU kernel will process all features in one call, and each",
    "2^exp_workgroups_per_feature (compile time constant) workgroup will",
    "process one feature4 tuple",
    "for the root node, indices are not copied",
    "for constant hessian, hessians are not copied except for the root node",
    "there will be 2^exp_workgroups_per_feature = num_workgroups / num_dense_feature4 sub-histogram per feature4",
    "and we will launch num_feature workgroups for this kernel",
    "will launch threads for all features",
    "the queue should be asynchrounous, and we will can WaitAndGetHistograms() before we start processing dense feature groups",
    "copy the results asynchronously. Size depends on if double precision is used",
    "we will wait for this object in WaitAndGetHistograms",
    "when the output is ready, the computation is done",
    "values of this feature has been redistributed to multiple bins; need a reduction here",
    "how many feature-group tuples we have",
    "leave some safe margin for prefetching",
    "256 work-items per workgroup. Each work-item prefetches one tuple for that feature",
    "clear sparse/dense maps",
    "do nothing if no features can be processed on GPU",
    "allocate memory for all features (FIXME: 4 GB barrier on some devices, need to split to multiple buffers)",
    "unpin old buffer if necessary before destructing them",
    "make ordered_gradients and hessians larger (including extra room for prefetching), and pin them",
    "allocate space for gradients and hessians on device",
    "we will copy gradients and hessians in after ordered_gradients_ and ordered_hessians_ are constructed",
    "allocate feature mask, for disabling some feature-groups' histogram calculation",
    "copy indices to the device",
    "histogram bin entry size depends on the precision (single/double)",
    "create output buffer, each feature has a histogram with device_bin_size_ bins,",
    "each work group generates a sub-histogram of dword_features_ features.",
    "only initialize once here, as this will not need to change when ResetTrainingData() is called",
    "create atomic counters for inter-group coordination",
    "The output buffer is allocated to host directly, to overlap compute and data transfer",
    "find the dense feature-groups and group then into Feature4 data structure (several feature-groups packed into 4 bytes)",
    "looking for dword_features_ non-sparse feature-groups",
    "decide if we need to redistribute the bin",
    "multiplier must be a power of 2",
    "device_bin_mults_.push_back(1);",
    "found",
    "for data transfer time",
    "Now generate new data structure feature4, and copy data to the device",
    "preallocate arrays for all threads, and pin them",
    "building Feature4 bundles; each thread handles dword_features_ features",
    "one feature datapoint is 4 bits",
    "this guarantees that the RawGet() function is inlined, rather than using virtual function dispatching",
    "one feature datapoint is one byte",
    "this guarantees that the RawGet() function is inlined, rather than using virtual function dispatching",
    "Dense bin",
    "Dense 4-bit bin",
    "working on the remaining (less than dword_features_) feature groups",
    "fill the leftover features",
    "fill this empty feature with some \"random\" value",
    "fill this empty feature with some \"random\" value",
    "copying the last 1 to (dword_features - 1) feature-groups in the last tuple",
    "deallocate pinned space for feature copying",
    "data transfer time",
    "for other types of failure, build log might not be available; program.build_log() can crash",
    "Something bad happened. Just return \"No log available.\"",
    "build is okay, log may contain warnings",
    "destroy any old kernels",
    "create OpenCL kernels for different number of workgroups per feature",
    "currently we don't use constant memory",
    "compile the GPU kernel depending if double precision is used, constant hessian is used, etc.",
    "kernel with indices in an array",
    "kernel with all features enabled, with elimited branches",
    "kernel with all data indices (for root node, and assumes that root node always uses all features)",
    "do nothing if no features can be processed on GPU",
    "The only argument that needs to be changed later is num_data_",
    "hessian is passed as a parameter, but it is not available now.",
    "hessian will be set in BeforeTrain()",
    "Get the max bin size, used for selecting best GPU kernel",
    "initialize GPU",
    "determine which kernel to use based on the max number of bins",
    "the +9 skips extra characters \")\", newline, \"#endif\" and newline at the beginning",
    "the +9 skips extra characters \")\", newline, \"#endif\" and newline at the beginning",
    "the +9 skips extra characters \")\", newline, \"#endif\" and newline at the beginning",
    "setup GPU kernel arguments after we allocating all the buffers",
    "GPU memory has to been reallocated because data may have been changed",
    "setup GPU kernel arguments after we allocating all the buffers",
    "Copy initial full hessians and gradients to GPU.",
    "We start copying as early as possible, instead of at ConstructHistogram().",
    "setup hessian parameters only",
    "hessian is passed as a parameter",
    "use bagging",
    "On GPU, we start copying indices, gradients and hessians now, instead at ConstructHistogram()",
    "copy used gradients and hessians to ordered buffer",
    "transfer the indices to GPU",
    "transfer hessian to GPU",
    "setup hessian parameters only",
    "hessian is passed as a parameter",
    "transfer gradients to GPU",
    "only have root",
    "Copy indices, gradients and hessians as early as possible",
    "only need to initialize for smaller leaf",
    "Get leaf boundary",
    "copy indices to the GPU:",
    "copy ordered hessians to the GPU:",
    "copy ordered gradients to the GPU:",
    "do nothing if no features can be processed on GPU",
    "copy data indices if it is not null",
    "generate and copy ordered_gradients if gradients is not null",
    "generate and copy ordered_hessians if hessians is not null",
    "converted indices in is_feature_used to feature-group indices",
    "construct the feature masks for dense feature-groups",
    "if no feature group is used, just return and do not use GPU",
    "if not all feature groups are used, we need to transfer the feature mask to GPU",
    "otherwise, we will use a specialized GPU kernel with all feature groups enabled",
    "All data have been prepared, now run the GPU kernel",
    "construct smaller leaf",
    "ConstructGPUHistogramsAsync will return true if there are availabe feature gourps dispatched to GPU",
    "then construct sparse features on CPU",
    "wait for GPU to finish, only if GPU is actually used",
    "use double precision",
    "use single precision",
    "Compare GPU histogram with CPU histogram, useful for debuggin GPU code problem",
    "#define GPU_DEBUG_COMPARE",
    "construct larger leaf",
    "then construct sparse features on CPU",
    "wait for GPU to finish, only if GPU is actually used",
    "use double precision",
    "use single precision",
    "do some sanity check for the GPU algorithm",
    "limit top k",
    "get max bin",
    "calculate buffer size",
    "need to be able to hold smaller and larger best splits in SyncUpGlobalBestSplit",
    "left and right on same time, so need double size",
    "initialize histograms for global",
    "sync global data sumup info",
    "set global sumup info",
    "init global data count in leaf",
    "get local sumup",
    "get local sumup",
    "get mean number on machines",
    "weighted gain",
    "get top k",
    "Copy histogram to buffer, and Get local aggregate features",
    "copy histograms.",
    "copy smaller leaf histograms first",
    "mark local aggregated feature",
    "copy",
    "then copy larger leaf histograms",
    "mark local aggregated feature",
    "copy",
    "use local data to find local best splits",
    "find splits",
    "only has root leaf",
    "local voting",
    "gather",
    "get all top-k from all machines",
    "global voting",
    "copy local histgrams to buffer",
    "Reduce scatter for histogram",
    "find best split from local aggregated histograms",
    "restore from buffer",
    "restore from buffer",
    "find local best",
    "find local best split for larger leaf",
    "sync global best info",
    "copy back",
    "set the global number of data for leaves",
    "init the global sumup info",
    "instantiate template classes, otherwise linker cannot find the code"
  ],
  "v3.0.0rc1": [
    "coding: utf-8",
    "coding: utf-8",
    "create predictor first",
    "check dataset",
    "reduce cost for prediction training data",
    "process callbacks",
    "Most of legacy advanced options becomes callbacks",
    "construct booster",
    "start training",
    "check evaluation result.",
    "ranking task, split according to groups",
    "run preprocessing on the data set if needed",
    "setup callbacks",
    "coding: utf-8",
    "simplejson does not support Python 3.2, it throws a SyntaxError",
    "because of u'...' Unicode literals.",
    "dummy function to support older version of scikit-learn",
    "DeprecationWarning is not shown by default, so let's create our own with higher level",
    "coding: utf-8",
    "user can set verbose with kwargs, it has higher priority",
    "register default metric for consistency with callable eval_metric case",
    "try to deduce from class instance",
    "overwrite default metric by explicitly set metric",
    "concatenate metric from params (or default if not provided in params) and eval_metric",
    "copy for consistency",
    "reduce cost for prediction training data",
    "free dataset",
    "Switch to using a multiclass objective in the underlying LGBM instance",
    "do not modify args, as it causes errors in model selection tools",
    "check group data",
    "coding: utf-8",
    "we don't need lib_lightgbm while building docs",
    "coding: utf-8",
    "\"#ddffdd\" is light green, \"#ffdddd\" is light red",
    "coding: utf-8",
    "coding: utf-8",
    "TypeError: obj is not a string or a number",
    "ValueError: invalid literal",
    "__get_num_preds() cannot work with nrow > MAX_INT32, so calculate overall number of predictions piecemeal",
    "avoid memory consumption by arrays concatenation operations",
    "create numpy array from output arrays",
    "break up indptr based on number of rows (note more than one matrix in multiclass case)",
    "for CSC there is extra column added",
    "reformat output into a csr or csc matrix or list of csr or csc matrices",
    "same shape as input csr or csc matrix except extra column for expected value",
    "note: make sure we copy data as it will be deallocated next",
    "free the temporary native indptr, indices, and data",
    "__get_num_preds() cannot work with nrow > MAX_INT32, so calculate overall number of predictions piecemeal",
    "avoid memory consumption by arrays concatenation operations",
    "no min_data, nthreads and verbose in this function",
    "check data has header or not",
    "need to regroup init_score",
    "process for args",
    "user can set verbose with params, it has higher priority",
    "get categorical features",
    "process for reference dataset",
    "start construct data",
    "set feature names",
    "create valid",
    "construct subset",
    "create train",
    "could be updated if data is not freed",
    "set to None",
    "we're done if self and reference share a common upstrem reference",
    "group data from LightGBM is boundaries data, need to convert to group size",
    "user can set verbose with params, it has higher priority",
    "Training task",
    "set network if necessary",
    "construct booster object",
    "copy the parameters from train_set",
    "save reference to data",
    "buffer for inner predict",
    "Prediction task",
    "if a single node tree it won't have `leaf_index` so return 0",
    "Create the node record, and populate universal data members",
    "Update values to reflect node type (leaf or split)",
    "traverse the next level of the tree",
    "In tree format, \"subtree_list\" is a list of node records (dicts),",
    "and we add node to the list.",
    "need reset training data",
    "need to push new valid data",
    "if buffer length is not long enough, re-allocate a buffer",
    "if buffer length is not long enough, reallocate a buffer",
    "Copy models",
    "Get name of features",
    "avoid to predict many time in one iteration",
    "Get num of inner evals",
    "Get name of evals",
    "coding: utf-8",
    "Callback environment used by callbacks",
    "split is needed for \"<dataset type> <metric>\" case (e.g. \"train l1\")",
    "split is needed for \"<dataset type> <metric>\" case (e.g. \"train l1\")",
    "coding: utf-8",
    "load or create your dataset",
    "create dataset for lightgbm",
    "if you want to re-use data, remember to set free_raw_data=False",
    "specify your configurations as a dict",
    "generate feature names",
    "feature_name and categorical_feature",
    "check feature name",
    "save model to file",
    "dump model to JSON (and save to file)",
    "feature names",
    "feature importances",
    "load model to predict",
    "can only predict with the best iteration (or the saving iteration)",
    "eval with loaded model",
    "dump model with pickle",
    "load model with pickle to predict",
    "can predict with any iteration when loaded in pickle way",
    "eval with loaded model",
    "continue training",
    "init_model accepts:",
    "1. model file name",
    "2. Booster()",
    "decay learning rates",
    "learning_rates accepts:",
    "1. list/tuple with length = num_boost_round",
    "2. function(curr_iter)",
    "change other parameters during training",
    "self-defined objective function",
    "f(preds: array, train_data: Dataset) -> grad: array, hess: array",
    "log likelihood loss",
    "self-defined eval metric",
    "f(preds: array, train_data: Dataset) -> name: string, eval_result: float, is_higher_better: bool",
    "binary error",
    "NOTE: when you do customized loss function, the default prediction value is margin",
    "This may make built-in evalution metric calculate wrong results",
    "For example, we are doing log likelihood loss, the prediction is score before logistic transformation",
    "Keep this in mind when you use the customization",
    "another self-defined eval metric",
    "f(preds: array, train_data: Dataset) -> name: string, eval_result: float, is_higher_better: bool",
    "accuracy",
    "NOTE: when you do customized loss function, the default prediction value is margin",
    "This may make built-in evalution metric calculate wrong results",
    "For example, we are doing log likelihood loss, the prediction is score before logistic transformation",
    "Keep this in mind when you use the customization",
    "callback",
    "coding: utf-8",
    "load or create your dataset",
    "train",
    "predict",
    "eval",
    "feature importances",
    "self-defined eval metric",
    "f(y_true: array, y_pred: array) -> name: string, eval_result: float, is_higher_better: bool",
    "Root Mean Squared Logarithmic Error (RMSLE)",
    "train",
    "another self-defined eval metric",
    "f(y_true: array, y_pred: array) -> name: string, eval_result: float, is_higher_better: bool",
    "Relative Absolute Error (RAE)",
    "train",
    "predict",
    "eval",
    "other scikit-learn modules",
    "coding: utf-8",
    "load or create your dataset",
    "create dataset for lightgbm",
    "specify your configurations as a dict",
    "train",
    "coding: utf-8",
    "################",
    "Simulate some binary data with a single categorical and",
    "single continuous predictor",
    "################",
    "Set up a couple of utilities for our experiments",
    "################",
    "Observe the behavior of `binary` and `xentropy` objectives",
    "Trying this throws an error on non-binary values of y:",
    "experiment('binary', label_type='probability', DATA)",
    "The speed of `binary` is not drastically different than",
    "`xentropy`. `xentropy` runs faster than `binary` in many cases, although",
    "there are reasons to suspect that `binary` should run faster when the",
    "label is an integer instead of a float",
    "coding: utf-8",
    "load or create your dataset",
    "create dataset for lightgbm",
    "specify your configurations as a dict",
    "train",
    "save model to file",
    "predict",
    "eval",
    "coding: utf-8",
    "!/usr/bin/env python3",
    "-*- coding: utf-8 -*-",
    "",
    "LightGBM documentation build configuration file, created by",
    "sphinx-quickstart on Thu May  4 14:30:58 2017.",
    "",
    "This file is execfile()d with the current directory set to its",
    "containing dir.",
    "",
    "Note that not all possible configuration values are present in this",
    "autogenerated file.",
    "",
    "All configuration values have a default; values that are commented out",
    "serve to show the default.",
    "If extensions (or modules to document with autodoc) are in another directory,",
    "add these directories to sys.path here. If the directory is relative to the",
    "documentation root, use os.path.abspath to make it absolute.",
    "-- mock out modules",
    "-- General configuration ------------------------------------------------",
    "If your documentation needs a minimal Sphinx version, state it here.",
    "Add any Sphinx extension module names here, as strings. They can be",
    "extensions coming with Sphinx (named 'sphinx.ext.*') or your custom",
    "ones.",
    "Generate autosummary pages. Output should be set with: `:toctree: pythonapi/`",
    "Only the class' docstring is inserted.",
    "If true, `todo` and `todoList` produce output, else they produce nothing.",
    "The master toctree document.",
    "General information about the project.",
    "The version info for the project you're documenting, acts as replacement for",
    "|version| and |release|, also used in various other places throughout the",
    "built documents.",
    "The short X.Y version.",
    "The full version, including alpha/beta/rc tags.",
    "The language for content autogenerated by Sphinx. Refer to documentation",
    "for a list of supported languages.",
    "",
    "This is also used if you do content translation via gettext catalogs.",
    "Usually you set \"language\" from the command line for these cases.",
    "List of patterns, relative to source directory, that match files and",
    "directories to ignore when looking for source files.",
    "This patterns also effect to html_static_path and html_extra_path",
    "The name of the Pygments (syntax highlighting) style to use.",
    "-- Configuration for C API docs generation ------------------------------",
    "-- Options for HTML output ----------------------------------------------",
    "The theme to use for HTML and HTML Help pages.  See the documentation for",
    "a list of builtin themes.",
    "Theme options are theme-specific and customize the look and feel of a theme",
    "further.  For a list of options available for each theme, see the",
    "documentation.",
    "Add any paths that contain custom static files (such as style sheets) here,",
    "relative to this directory. They are copied after the builtin static files,",
    "so a file named \"default.css\" will overwrite the builtin \"default.css\".",
    "-- Options for HTMLHelp output ------------------------------------------",
    "Output file base name for HTML help builder.",
    "Warning! The following code can cause buffer overflows on RTD.",
    "Consider suppressing output completely if RTD project silently fails.",
    "Refer to https://github.com/svenevs/exhale",
    "/blob/fe7644829057af622e467bb529db6c03a830da99/exhale/deploy.py#L99-L111",
    "Warning! The following code can cause buffer overflows on RTD.",
    "Consider suppressing output completely if RTD project silently fails.",
    "Refer to https://github.com/svenevs/exhale",
    "/blob/fe7644829057af622e467bb529db6c03a830da99/exhale/deploy.py#L99-L111",
    "coding: utf-8",
    "coding: utf-8",
    "we don't need lib_lightgbm while building docs",
    "coding: utf-8",
    "check saved model persistence",
    "we need to check the consistency of model file here, so test for exact equal",
    "check early stopping is working. Make it stop very early, so the scores should be very close to zero",
    "scores likely to be different, but prediction should still be the same",
    "test that shape is checked during prediction",
    "Set extremely harsh penalties, so CEGB will block most splits.",
    "Compare pairs of penalties, to ensure scaling works as intended",
    "Reset booster1's parameters to p2, so the parameter section of the file matches.",
    "coding: utf-8",
    "prediction result is actually not transformed (is raw) due to custom objective",
    "sklearn <0.23 does not have a stacking classifier and n_features_in_ property",
    "sklearn <0.23 does not have a stacking regressor and n_features_in_ property",
    "sklearn < 0.22 does not have the post fit attribute: classes_",
    "sklearn < 0.23 does not have as_frame parameter",
    "sklearn < 0.22 does not have the post fit attribute: classes_",
    "sklearn < 0.23 does not have as_frame parameter",
    "Test if random_state is properly stored",
    "Test if two random states produce identical models",
    "Test if subsequent fits sample from random_state object and produce different models",
    "Test that the largest element is NOT the same, the smallest can be the same, i.e. zero",
    "sklearn <0.19 cannot accept instance, but many tests could be passed only with min_data=1 and min_data_in_bin=1",
    "we cannot use `check_estimator` directly since there is no skip test mechanism",
    "we cannot leave default params (see https://github.com/microsoft/LightGBM/issues/833)",
    "skip test because scikit-learn incorrectly asserts that",
    "private attributes cannot be set in __init__",
    "(see https://github.com/microsoft/LightGBM/issues/2628)",
    "With default params",
    "Tests same probabilities",
    "Tests same predictions",
    "Tests same raw scores",
    "Tests same leaf indices",
    "Tests same feature contributions",
    "Tests other parameters for the prediction works",
    "Tests start_iteration",
    "Tests same probabilities, starting from iteration 10",
    "Tests same predictions, starting from iteration 10",
    "Tests same raw scores, starting from iteration 10",
    "Tests same leaf indices, starting from iteration 10",
    "Tests same feature contributions, starting from iteration 10",
    "Tests other parameters for the prediction works, starting from iteration 10",
    "no custom objective, no custom metric",
    "default metric",
    "non-default metric",
    "no metric",
    "non-default metric in eval_metric",
    "non-default metric with non-default metric in eval_metric",
    "non-default metric with multiple metrics in eval_metric",
    "non-default metric with multiple metrics in eval_metric for LGBMClassifier",
    "default metric for non-default objective",
    "non-default metric for non-default objective",
    "no metric",
    "non-default metric in eval_metric for non-default objective",
    "non-default metric with non-default metric in eval_metric for non-default objective",
    "non-default metric with multiple metrics in eval_metric for non-default objective",
    "custom objective, no custom metric",
    "default regression metric for custom objective",
    "non-default regression metric for custom objective",
    "multiple regression metrics for custom objective",
    "no metric",
    "default regression metric with non-default metric in eval_metric for custom objective",
    "non-default regression metric with metric in eval_metric for custom objective",
    "multiple regression metrics with metric in eval_metric for custom objective",
    "multiple regression metrics with multiple metrics in eval_metric for custom objective",
    "no custom objective, custom metric",
    "default metric with custom metric",
    "non-default metric with custom metric",
    "multiple metrics with custom metric",
    "custom metric (disable default metric)",
    "default metric for non-default objective with custom metric",
    "non-default metric for non-default objective with custom metric",
    "multiple metrics for non-default objective with custom metric",
    "custom metric (disable default metric for non-default objective)",
    "custom objective, custom metric",
    "custom metric for custom objective",
    "non-default regression metric with custom metric for custom objective",
    "multiple regression metrics with custom metric for custom objective",
    "default metric and invalid binary metric is replaced with multiclass alternative",
    "invalid objective is replaced with default multiclass one",
    "and invalid binary metric is replaced with multiclass alternative",
    "default metric for non-default multiclass objective",
    "and invalid binary metric is replaced with multiclass alternative",
    "default metric and invalid multiclass metric is replaced with binary alternative",
    "invalid multiclass metric is replaced with binary alternative for custom objective",
    "training data as eval_set",
    "feval",
    "single eval_set",
    "two eval_set",
    "coding: utf-8",
    "coding: utf-8",
    "check that default gives same result as k = 1",
    "check against independent calculation for k = 1",
    "check against independent calculation for k = 2",
    "check against independent calculation for k = 10",
    "check cases where predictions are equal",
    "should give same result as binary auc for 2 classes",
    "test the case where all predictions are equal",
    "should give 1 when accuracy = 1",
    "test loading weights",
    "no early stopping",
    "early stopping occurs",
    "test custom eval metrics",
    "shuffle = False, override metric in params",
    "shuffle = True, callbacks",
    "enable display training loss",
    "self defined folds",
    "lambdarank",
    "... with l2 metric",
    "... with NDCG (default) metric",
    "self defined folds with lambdarank",
    "with early stopping",
    "predict by each fold booster",
    "fold averaging",
    "without early stopping",
    "test feature_names with whitespaces",
    "This has non-ascii strings.",
    "take subsets and train",
    "generate CSR sparse dataset",
    "convert data to dense and get back same contribs",
    "validate the values are the same",
    "validate using CSC matrix",
    "validate the values are the same",
    "Note there is an extra column added to the output for the expected value",
    "Note output CSC shape should be same as CSR output shape",
    "test sliced labels",
    "append some columns",
    "append some rows",
    "test sliced 2d matrix",
    "test sliced CSR",
    "test if a penalty as high as the depth indeed prohibits all monotone splits",
    "The penalization is so high that the first 2 features should not be used here",
    "Check that a very high penalization is the same as not using the features at all",
    "no fobj, no feval",
    "default metric",
    "non-default metric in params",
    "default metric in args",
    "non-default metric in args",
    "metric in args overwrites one in params",
    "multiple metrics in params",
    "multiple metrics in args",
    "remove default metric by 'None' in list",
    "remove default metric by 'None' aliases",
    "fobj, no feval",
    "no default metric",
    "metric in params",
    "metric in args",
    "metric in args overwrites its' alias in params",
    "multiple metrics in params",
    "multiple metrics in args",
    "no fobj, feval",
    "default metric with custom one",
    "non-default metric in params with custom one",
    "default metric in args with custom one",
    "non-default metric in args with custom one",
    "metric in args overwrites one in params, custom one is evaluated too",
    "multiple metrics in params with custom one",
    "multiple metrics in args with custom one",
    "custom metric is evaluated despite 'None' is passed",
    "fobj, feval",
    "no default metric, only custom one",
    "metric in params with custom one",
    "metric in args with custom one",
    "metric in args overwrites one in params, custom one is evaluated too",
    "multiple metrics in params with custom one",
    "multiple metrics in args with custom one",
    "custom metric is evaluated despite 'None' is passed",
    "no fobj, no feval",
    "default metric",
    "default metric in params",
    "non-default metric in params",
    "multiple metrics in params",
    "remove default metric by 'None' aliases",
    "fobj, no feval",
    "no default metric",
    "metric in params",
    "multiple metrics in params",
    "no fobj, feval",
    "default metric with custom one",
    "default metric in params with custom one",
    "non-default metric in params with custom one",
    "multiple metrics in params with custom one",
    "custom metric is evaluated despite 'None' is passed",
    "fobj, feval",
    "no default metric, only custom one",
    "metric in params with custom one",
    "multiple metrics in params with custom one",
    "custom metric is evaluated despite 'None' is passed",
    "multiclass default metric",
    "multiclass default metric with custom one",
    "multiclass metric alias with custom one for custom objective",
    "no metric for invalid class_num",
    "custom metric for invalid class_num",
    "multiclass metric alias with custom one with invalid class_num",
    "multiclass default metric without num_class",
    "multiclass metric alias",
    "multiclass metric",
    "non-valid metric for multiclass objective",
    "non-default num_class for default objective",
    "no metric with non-default num_class for custom objective",
    "multiclass metric alias for custom objective",
    "multiclass metric for custom objective",
    "binary metric with non-default num_class for custom objective",
    "test XGBoost-style return value",
    "test numpy-style return value",
    "test bins string type",
    "test histogram is disabled for categorical features",
    "test for lgb.train",
    "test feval for lgb.train",
    "test with two valid data for lgb.train",
    "test for lgb.cv",
    "test feval for lgb.cv",
    "test that binning works properly for features with only positive or only negative values",
    "decreasing without freeing raw data is allowed",
    "decreasing before lazy init is allowed",
    "increasing is allowed",
    "decreasing with disabled filter is allowed",
    "decreasing with enabled filter is disallowed;",
    "also changes of other params are disallowed",
    "check extra trees increases regularization",
    "check path smoothing increases regularization",
    "test edge case with one leaf",
    "check that constraint containing all features is equivalent to no constraint",
    "check that constraint partitioning the features reduces train accuracy",
    "check that constraints consisting of single features reduce accuracy further",
    "test that interaction constraints work when not all features are used",
    "test that the predict once with all iterations equals summed results with start_iteration and num_iteration",
    "test the case where start_iteration <= 0, and num_iteration is None",
    "test the case where start_iteration > 0, and num_iteration <= 0",
    "test the case where start_iteration > 0, and num_iteration <= 0, with pred_leaf=True",
    "test the case where start_iteration > 0, and num_iteration <= 0, with pred_contrib=True",
    "test for regression",
    "test both with and without early stopping",
    "test for multi-class",
    "test both with and without early stopping",
    "test for binary",
    "test both with and without early stopping",
    "coding: utf-8",
    "convert from one-based to  zero-based index",
    "convert from boundaries to size",
    "--- start Booster interfaces",
    ".Call() calls",
    "coding: utf-8",
    "alias table",
    "names",
    "from strings",
    "tails",
    "tails",
    "coding: utf-8",
    "Single row predictor to abstract away caching logic",
    "create boosting",
    "initialize the boosting",
    "create objective function",
    "initialize the objective function",
    "create training metric",
    "reset the boosting",
    "create objective function",
    "initialize the objective function",
    "calculate the nonzero data and indices size",
    "allocate data and indices arrays",
    "Get the number of trees per iteration (for multiclass scenario we output multiple sparse matrices)",
    "aggregated per row feature contribution results",
    "keep track of the row_vector sizes for parallelization",
    "copy vector results to output for each row",
    "Get the number of trees per iteration (for multiclass scenario we output multiple sparse matrices)",
    "aggregated per row feature contribution results",
    "calculate number of elements per column to construct",
    "the CSC matrix with random access",
    "keep track of column counts",
    "keep track of beginning index for each column",
    "keep track of beginning index for each matrix",
    "store the row index",
    "update column count",
    "explicitly declare symbols from LightGBM namespace",
    "some help functions used to convert data",
    "Row iterator of on column for CSC matrix",
    "return value at idx, only can access by ascent order",
    "return next non-zero pair, if index < 0, means no more data",
    "start of c_api functions",
    "sample data first",
    "sample data first",
    "sample data first",
    "local buffer to re-use memory",
    "sample data first",
    "no more data",
    "---- start of booster",
    "Single row in row-major format:",
    "---- start of some help functions",
    "data is array of pointers to individual rows",
    "set number of threads for openmp",
    "check for alias",
    "read parameters from config file",
    "remove str after \"#\"",
    "check for alias again",
    "load configs",
    "prediction is needed if using input initial model(continued train)",
    "need to continue training",
    "sync up random seed for data partition",
    "load Training data",
    "load data for parallel training",
    "load data for single machine",
    "need save binary file",
    "create training metric",
    "only when have metrics then need to construct validation data",
    "Add validation data, if it exists",
    "add",
    "need save binary file",
    "add metric for validation data",
    "output used time on each iteration",
    "need init network",
    "create boosting",
    "create objective function",
    "load training data",
    "initialize the objective function",
    "initialize the boosting",
    "add validation data into boosting",
    "convert model to if-else statement code",
    "create predictor",
    "Free memory",
    "create predictor",
    "label_gain = 2^i - 1, may overflow, so we use 31 here",
    "counts for all labels",
    "start from top label, and accumulate DCG",
    "counts for all labels",
    "calculate k Max DCG by one pass",
    "get sorted indices by score",
    "calculate dcg",
    "get sorted indices by score",
    "calculate multi dcg by one pass",
    "wait for all client start up",
    "Don't call MPI_Finalize() here: If the destructor was called because only this node had an exception, calling MPI_Finalize() will cause all nodes to hang.",
    "Instead we will handle finalize/abort for MPI in main().",
    "default set to -1",
    "distance at k-th communication, distance[k] = 2^k",
    "set incoming rank at k-th commuication",
    "set outgoing rank at k-th commuication",
    "defalut set as -1",
    "construct all recursive halving map for all machines",
    "let 1 << k <= num_machines",
    "distance of each communication",
    "if num_machines = 2^k, don't need to group machines",
    "communication direction, %2 == 0 is positive",
    "neighbor at k-th communication",
    "receive data block at k-th communication",
    "send data block at k-th communication",
    "if num_machines != 2^k, need to group machines",
    "group, two machine in one group, total \"rest\" groups will have 2 machines.",
    "let left machine as group leader",
    "cache block information for groups, group with 2 machines will have double block size",
    "convert from group to node leader",
    "convert from node to group",
    "meet new group",
    "add block len for this group",
    "calculate the group block start",
    "not need to construct",
    "get receive block informations",
    "accumulate block len",
    "get send block informations",
    "accumulate block len",
    "static member definition",
    "if small package or small count , do it by all gather.(reduce the communication times.)",
    "assign the blocks to every rank.",
    "do reduce scatter",
    "do all gather",
    "assign blocks",
    "need use buffer here, since size of \"output\" is smaller than size after all gather",
    "copy back",
    "assign blocks",
    "start all gather",
    "when num_machines is small and data is large",
    "use output as receive buffer",
    "get current local block size",
    "get out rank",
    "get in rank",
    "get send information",
    "get recv information",
    "send and recv at same time",
    "rotate in-place",
    "use output as receive buffer",
    "get current local block size",
    "get send information",
    "get recv information",
    "send and recv at same time",
    "use output as receive buffer",
    "send and recv at same time",
    "send local data to neighbor first",
    "receive neighbor data first",
    "reduce",
    "get target",
    "get send information",
    "get recv information",
    "send and recv at same time",
    "reduce",
    "send result to neighbor",
    "receive result from neighbor",
    "copy result",
    "start up socket",
    "parse clients from file",
    "get ip list of local machine",
    "get local rank",
    "construct listener",
    "construct communication topo",
    "construct linkers",
    "free listener",
    "set timeout",
    "accept incoming socket",
    "receive rank",
    "add new socket",
    "save ranks that need to connect with",
    "start listener",
    "start connect",
    "let smaller rank connect to larger rank",
    "send local rank",
    "wait for listener",
    "print connected linkers",
    "valid the type",
    "Constructors",
    "Get type tag",
    "Comparisons",
    "This has to be separate, not in Statics, because Json() accesses",
    "statics().null.",
    "advance until next line, or end of input",
    "advance until closing tokens",
    "The usual case: non-escaped characters",
    "Handle escapes",
    "Extract 4-byte escape sequence",
    "Explicitly check length of the substring. The following loop",
    "relies on std::string returning the terminating NUL when",
    "accessing str[length]. Checking here reduces brittleness.",
    "JSON specifies that characters outside the BMP shall be encoded as a",
    "pair of 4-hex-digit \\u escapes encoding their surrogate pair",
    "components. Check whether we're in the middle of such a beast: the",
    "previous codepoint was an escaped lead (high) surrogate, and this is",
    "a trail (low) surrogate.",
    "Reassemble the two surrogate pairs into one astral-plane character,",
    "per the UTF-16 algorithm.",
    "Integer part",
    "Decimal part",
    "Exponent part",
    "Check for any trailing garbage",
    "Documented in json11.hpp",
    "Check for another object",
    "get column names",
    "load label idx first",
    "erase label column name",
    "load ignore columns",
    "load weight idx",
    "load group idx",
    "don't support query id in data file when training in parallel",
    "read data to memory",
    "sample data",
    "construct feature bin mappers",
    "initialize label",
    "extract features",
    "sample data from file",
    "construct feature bin mappers",
    "initialize label",
    "extract features",
    "load data from binary file",
    "check meta data",
    "need to check training data",
    "read data in memory",
    "initialize label",
    "extract features",
    "Get number of lines of data file",
    "initialize label",
    "extract features",
    "load data from binary file",
    "not need to check validation data",
    "check meta data",
    "buffer to read binary file",
    "check token",
    "read size of header",
    "re-allocmate space if not enough",
    "read header",
    "get header",
    "num_groups",
    "real_feature_idx_",
    "feature2group",
    "feature2subfeature",
    "group_bin_boundaries",
    "group_feature_start_",
    "group_feature_cnt_",
    "get feature names",
    "write feature names",
    "get forced_bin_bounds_",
    "read size of meta data",
    "re-allocate space if not enough",
    "read meta data",
    "load meta data",
    "sample local used data if need to partition",
    "if not contain query file, minimal sample unit is one record",
    "if contain query file, minimal sample unit is one query",
    "if is new query",
    "read feature data",
    "read feature size",
    "re-allocate space if not enough",
    "fill feature_names_ if not header",
    "get forced split",
    "if only one machine, find bin locally",
    "if have multi-machines, need to find bin distributed",
    "different machines will find bin for different features",
    "start and len will store the process feature indices for different machines",
    "machine i will find bins for features in [ start[i], start[i] + len[i] )",
    "free",
    "gather global feature bin mappers",
    "restore features bins from buffer",
    "---- private functions ----",
    "if features are ordered, not need to use hist_buf",
    "read all lines",
    "get query data",
    "if not contain query data, minimal sample unit is one record",
    "if contain query data, minimal sample unit is one query",
    "if is new query",
    "get query data",
    "if not contain query file, minimal sample unit is one record",
    "if contain query file, minimal sample unit is one query",
    "if is new query",
    "parse features",
    "get forced split",
    "check the range of label_idx, weight_idx and group_idx",
    "fill feature_names_ if not header",
    "start find bins",
    "if only one machine, find bin locally",
    "start and len will store the process feature indices for different machines",
    "machine i will find bins for features in [ start[i], start[i] + len[i] )",
    "free",
    "gather global feature bin mappers",
    "restore features bins from buffer",
    "if doesn't need to prediction with initial model",
    "parser",
    "set label",
    "free processed line:",
    "shrink_to_fit will be very slow in linux, and seems not free memory, disable for now",
    "text_reader_->Lines()[i].shrink_to_fit();",
    "push data",
    "if is used feature",
    "if need to prediction with initial model",
    "parser",
    "set initial score",
    "set label",
    "free processed line:",
    "shrink_to_fit will be very slow in linux, and seems not free memory, disable for now",
    "text_reader_->Lines()[i].shrink_to_fit();",
    "push data",
    "if is used feature",
    "metadata_ will manage space of init_score",
    "text data can be free after loaded feature values",
    "parser",
    "set initial score",
    "set label",
    "push data",
    "if is used feature",
    "only need part of data",
    "need full data",
    "metadata_ will manage space of init_score",
    "read size of token",
    "remove duplicates",
    "deep copy function for BinMapper",
    "mean size for one bin",
    "need a new bin",
    "update bin upper bound",
    "last bin upper bound",
    "get list of distinct values",
    "get number of positive and negative distinct values",
    "include zero bounds and infinity bound",
    "add forced bounds, excluding zeros since we have already added zero bounds",
    "find remaining bounds",
    "find distinct_values first",
    "push zero in the front",
    "use the large value",
    "push zero in the back",
    "convert to int type first",
    "sort by counts",
    "avoid first bin is zero",
    "will ignore the categorical of small counts",
    "need an additional bin for NaN",
    "use -1 to represent NaN",
    "Use MissingType::None to represent this bin contains all categoricals",
    "check trivial(num_bin_ == 1) feature",
    "check useless bin",
    "FIXME: how to enable `most_freq_bin_ = 0` for categorical features",
    "When most_freq_bin_ != default_bin_, there are some additional data loading costs.",
    "so use most_freq_bin_  = default_bin_ when there is not so sparse",
    "for lambdarank, it needs query data for partition data in parallel learning",
    "need convert query_id to boundaries",
    "check weights",
    "check query boundries",
    "contain initial score file",
    "check weights",
    "get local weights",
    "check query boundries",
    "get local query boundaries",
    "contain initial score file",
    "get local initial scores",
    "re-load query weight",
    "save to nullptr",
    "save to nullptr",
    "save to nullptr",
    "default weight file name",
    "default init_score file name",
    "use first line to count number class",
    "default query file name",
    "root is in the depth 0",
    "non-leaf",
    "leaf",
    "use this for the missing value conversion",
    "Predict func by Map to ifelse",
    "use this for the missing value conversion",
    "non-leaf",
    "left subtree",
    "right subtree",
    "leaf",
    "non-leaf",
    "left subtree",
    "right subtree",
    "leaf",
    "recursive computation of SHAP values for a decision tree",
    "extend the unique path",
    "leaf node",
    "internal node",
    "see if we have already split on this feature,",
    "if so we undo that split so we can redo it for this node",
    "recursive sparse computation of SHAP values for a decision tree",
    "extend the unique path",
    "leaf node",
    "internal node",
    "see if we have already split on this feature,",
    "if so we undo that split so we can redo it for this node",
    "add names of objective function if not providing metric",
    "equal weights for all classes",
    "generate seeds by seed.",
    "sort eval_at",
    "Only push the non-training data",
    "check for conflicts",
    "check if objective, metric, and num_class match",
    "Change pool size to -1 (no limit) when using data parallel to reduce communication costs",
    "Check max_depth and num_leaves",
    "Fits in an int, and is more restrictive than the current num_leaves",
    "force col-wise for gpu",
    "min_data_in_leaf must be at least 2 if path smoothing is active. This is because when the split is calculated",
    "the count is calculated using the proportion of hessian in the leaf which is rounded up to nearest int, so it can",
    "be 1 when there is actually no data in the leaf. In rare cases this can cause a bug because with path smoothing the",
    "calculated split gain can be positive even with zero gradient and hessian.",
    "In distributed mode, local node doesn't have histograms on all features, cannot perform \"intermediate\" monotone constraints.",
    "\"intermediate\" monotone constraints need to recompute splits. If the features are sampled when computing the",
    "split initially, then the sampling needs to be recorded or done once again, which is currently not supported",
    "first round: fill the single val group",
    "always push the last group",
    "put dense feature first",
    "sort by non zero cnt",
    "sort by non zero cnt, bigger first",
    "shuffle groups",
    "Using std::swap for vector<bool> will cause the wrong result.",
    "get num_features",
    "get bin_mappers",
    "copy feature bin mapper data",
    "copy feature bin mapper data",
    "if not pass a filename, just append \".bin\" of original file",
    "get size of header",
    "size of feature names",
    "size of forced bins",
    "write header",
    "write feature names",
    "write forced bins",
    "get size of meta data",
    "write meta data",
    "write feature data",
    "get size of feature",
    "write feature",
    "only need to copy subset",
    "avoid to copy subset many times",
    "avoid out of range",
    "may need to recopy subset",
    "explicitly initilize template methods, for cross module call",
    "FIXME: fix the multiple multi-val feature groups, they need to be merged",
    "into one multi-val group",
    "Skip the leading 0 when copying group_bin_boundaries.",
    "store the importance first",
    "PredictRaw",
    "PredictRawByMap",
    "Predict",
    "PredictByMap",
    "PredictLeafIndex",
    "PredictLeafIndexByMap",
    "output model type",
    "output number of class",
    "output label index",
    "output max_feature_idx",
    "output objective",
    "output tree models",
    "store the importance first",
    "sort the importance",
    "use serialized string to restore this object",
    "Use first 128 chars to avoid exceed the message buffer.",
    "get number of classes",
    "get index of label",
    "get max_feature_idx first",
    "get average_output",
    "get feature names",
    "get monotone_constraints",
    "set zero",
    "predict all the trees for one iteration",
    "check early stopping",
    "set zero",
    "predict all the trees for one iteration",
    "check early stopping",
    "margin_threshold will be captured by value",
    "copy and sort",
    "margin_threshold will be captured by value",
    "Fix for compiler warnings about reaching end of control",
    "load forced_splits file",
    "init tree learner",
    "push training metrics",
    "create buffer for gradients and hessians",
    "get max feature index",
    "get label index",
    "get feature names",
    "if need bagging, create buffer",
    "for a validation dataset, we need its score and metric",
    "update score",
    "objective function will calculate gradients and hessians",
    "random bagging, minimal unit is one record",
    "random bagging, minimal unit is one record",
    "if need bagging",
    "set bagging data to tree learner",
    "get subset",
    "output used time per iteration",
    "boosting from average label; or customized \"average\" if implemented for the current objective",
    "boosting first",
    "bagging logic",
    "need to copy gradients for bagging subset.",
    "shrinkage by learning rate",
    "update score",
    "only add default score one-time",
    "updates scores",
    "add model",
    "reset score",
    "remove model",
    "print message for metric",
    "pop last early_stopping_round_ models",
    "update training score",
    "we need to predict out-of-bag scores of data for boosting",
    "update validation score",
    "print training metric",
    "print validation metric",
    "set zero",
    "predict all the trees for one iteration",
    "predict all the trees for one iteration",
    "push training metrics",
    "not same training data, need reset score and others",
    "create score tracker",
    "update score",
    "create buffer for gradients and hessians",
    "load forced_splits file",
    "if need bagging, create buffer",
    "Get the max size of pool",
    "at least need 2 leaves",
    "push split information for all leaves",
    "initialize splits for leaf",
    "initialize data partition",
    "initialize ordered gradients and hessians",
    "cannot change is_hist_col_wise during training",
    "initialize splits for leaf",
    "initialize data partition",
    "initialize ordered gradients and hessians",
    "Get the max size of pool",
    "at least need 2 leaves",
    "push split information for all leaves",
    "some initial works before training",
    "root leaf",
    "only root leaf can be splitted on first time",
    "some initial works before finding best split",
    "find best threshold for every feature",
    "Get a leaf with max split gain",
    "Get split information for best leaf",
    "cannot split, quit",
    "split tree with best leaf",
    "reset histogram pool",
    "initialize data partition",
    "reset the splits for leaves",
    "Sumup for root",
    "use all data",
    "use bagging, only use part of data",
    "check depth of current leaf",
    "only need to check left leaf, since right leaf is in same level of left leaf",
    "no enough data to continue",
    "only have root",
    "put parent(left) leaf's histograms into larger leaf's histograms",
    "put parent(left) leaf's histograms to larger leaf's histograms",
    "construct smaller leaf",
    "construct larger leaf",
    "find splits",
    "only has root leaf",
    "start at root leaf",
    "before processing next node from queue, store info for current left/right leaf",
    "store \"best split\" for left and right, even if they might be overwritten by forced split",
    "then, compute own splits",
    "split info should exist because searching in bfs fashion - should have added from parent",
    "update before tree split",
    "don't need to update this in data-based parallel model",
    "split tree, will return right leaf",
    "store the true split gain in tree model",
    "don't need to update this in data-based parallel model",
    "store the true split gain in tree model",
    "init the leaves that used on next iteration",
    "update leave outputs if needed",
    "bag_mapper[index_mapper[i]]",
    "for root leaf the \"parent\" output is its own output because we don't apply any smoothing to the root",
    "find splits",
    "need to be able to hold smaller and larger best splits in SyncUpGlobalBestSplit",
    "get feature partition",
    "get local used features",
    "get best split at smaller leaf",
    "find local best split for larger leaf",
    "sync global best info",
    "update best split",
    "instantiate template classes, otherwise linker cannot find the code",
    "initialize SerialTreeLearner",
    "Get local rank and global machine size",
    "need to be able to hold smaller and larger best splits in SyncUpGlobalBestSplit",
    "allocate buffer for communication",
    "generate feature partition for current tree",
    "get local used feature",
    "get block start and block len for reduce scatter",
    "get buffer_write_start_pos_",
    "get buffer_read_start_pos_",
    "sync global data sumup info",
    "global sumup reduce",
    "copy back",
    "set global sumup info",
    "init global data count in leaf",
    "construct local histograms",
    "copy to buffer",
    "Reduce scatter for histogram",
    "restore global histograms from buffer",
    "only root leaf",
    "construct histgroms for large leaf, we init larger leaf as the parent, so we can just subtract the smaller leaf's histograms",
    "find local best split for larger leaf",
    "sync global best info",
    "set best split",
    "need update global number of data in leaf",
    "instantiate template classes, otherwise linker cannot find the code",
    "initialize SerialTreeLearner",
    "some additional variables needed for GPU trainer",
    "Initialize GPU buffers and kernels",
    "some functions used for debugging the GPU histogram construction",
    "printf(\"grad %g != %g (%d ULPs)\\n\", GET_GRAD(h1, i), GET_GRAD(h2, i), ulps);",
    "goto err;",
    "we roughly want 256 workgroups per device, and we have num_dense_feature4_ feature tuples.",
    "also guarantee that there are at least 2K examples per workgroup",
    "return 0;",
    "we have already copied ordered gradients, ordered hessians and indices to GPU",
    "decide the best number of workgroups working on one feature4 tuple",
    "set work group size based on feature size",
    "each 2^exp_workgroups_per_feature workgroups work on a feature4 tuple",
    "we need to refresh the kernel arguments after reallocating",
    "The only argument that needs to be changed later is num_data_",
    "the GPU kernel will process all features in one call, and each",
    "2^exp_workgroups_per_feature (compile time constant) workgroup will",
    "process one feature4 tuple",
    "for the root node, indices are not copied",
    "for constant hessian, hessians are not copied except for the root node",
    "there will be 2^exp_workgroups_per_feature = num_workgroups / num_dense_feature4 sub-histogram per feature4",
    "and we will launch num_feature workgroups for this kernel",
    "will launch threads for all features",
    "the queue should be asynchrounous, and we will can WaitAndGetHistograms() before we start processing dense feature groups",
    "copy the results asynchronously. Size depends on if double precision is used",
    "we will wait for this object in WaitAndGetHistograms",
    "when the output is ready, the computation is done",
    "values of this feature has been redistributed to multiple bins; need a reduction here",
    "how many feature-group tuples we have",
    "leave some safe margin for prefetching",
    "256 work-items per workgroup. Each work-item prefetches one tuple for that feature",
    "clear sparse/dense maps",
    "do nothing if no features can be processed on GPU",
    "allocate memory for all features (FIXME: 4 GB barrier on some devices, need to split to multiple buffers)",
    "unpin old buffer if necessary before destructing them",
    "make ordered_gradients and hessians larger (including extra room for prefetching), and pin them",
    "allocate space for gradients and hessians on device",
    "we will copy gradients and hessians in after ordered_gradients_ and ordered_hessians_ are constructed",
    "allocate feature mask, for disabling some feature-groups' histogram calculation",
    "copy indices to the device",
    "histogram bin entry size depends on the precision (single/double)",
    "create output buffer, each feature has a histogram with device_bin_size_ bins,",
    "each work group generates a sub-histogram of dword_features_ features.",
    "only initialize once here, as this will not need to change when ResetTrainingData() is called",
    "create atomic counters for inter-group coordination",
    "The output buffer is allocated to host directly, to overlap compute and data transfer",
    "find the dense feature-groups and group then into Feature4 data structure (several feature-groups packed into 4 bytes)",
    "looking for dword_features_ non-sparse feature-groups",
    "decide if we need to redistribute the bin",
    "multiplier must be a power of 2",
    "device_bin_mults_.push_back(1);",
    "found",
    "for data transfer time",
    "Now generate new data structure feature4, and copy data to the device",
    "preallocate arrays for all threads, and pin them",
    "building Feature4 bundles; each thread handles dword_features_ features",
    "one feature datapoint is 4 bits",
    "this guarantees that the RawGet() function is inlined, rather than using virtual function dispatching",
    "one feature datapoint is one byte",
    "this guarantees that the RawGet() function is inlined, rather than using virtual function dispatching",
    "Dense bin",
    "Dense 4-bit bin",
    "working on the remaining (less than dword_features_) feature groups",
    "fill the leftover features",
    "fill this empty feature with some \"random\" value",
    "fill this empty feature with some \"random\" value",
    "copying the last 1 to (dword_features - 1) feature-groups in the last tuple",
    "deallocate pinned space for feature copying",
    "data transfer time",
    "for other types of failure, build log might not be available; program.build_log() can crash",
    "Something bad happened. Just return \"No log available.\"",
    "build is okay, log may contain warnings",
    "destroy any old kernels",
    "create OpenCL kernels for different number of workgroups per feature",
    "currently we don't use constant memory",
    "compile the GPU kernel depending if double precision is used, constant hessian is used, etc.",
    "kernel with indices in an array",
    "kernel with all features enabled, with elimited branches",
    "kernel with all data indices (for root node, and assumes that root node always uses all features)",
    "do nothing if no features can be processed on GPU",
    "The only argument that needs to be changed later is num_data_",
    "hessian is passed as a parameter, but it is not available now.",
    "hessian will be set in BeforeTrain()",
    "Get the max bin size, used for selecting best GPU kernel",
    "initialize GPU",
    "determine which kernel to use based on the max number of bins",
    "the +9 skips extra characters \")\", newline, \"#endif\" and newline at the beginning",
    "the +9 skips extra characters \")\", newline, \"#endif\" and newline at the beginning",
    "the +9 skips extra characters \")\", newline, \"#endif\" and newline at the beginning",
    "setup GPU kernel arguments after we allocating all the buffers",
    "GPU memory has to been reallocated because data may have been changed",
    "setup GPU kernel arguments after we allocating all the buffers",
    "Copy initial full hessians and gradients to GPU.",
    "We start copying as early as possible, instead of at ConstructHistogram().",
    "setup hessian parameters only",
    "hessian is passed as a parameter",
    "use bagging",
    "On GPU, we start copying indices, gradients and hessians now, instead at ConstructHistogram()",
    "copy used gradients and hessians to ordered buffer",
    "transfer the indices to GPU",
    "transfer hessian to GPU",
    "setup hessian parameters only",
    "hessian is passed as a parameter",
    "transfer gradients to GPU",
    "only have root",
    "Copy indices, gradients and hessians as early as possible",
    "only need to initialize for smaller leaf",
    "Get leaf boundary",
    "copy indices to the GPU:",
    "copy ordered hessians to the GPU:",
    "copy ordered gradients to the GPU:",
    "do nothing if no features can be processed on GPU",
    "copy data indices if it is not null",
    "generate and copy ordered_gradients if gradients is not null",
    "generate and copy ordered_hessians if hessians is not null",
    "converted indices in is_feature_used to feature-group indices",
    "construct the feature masks for dense feature-groups",
    "if no feature group is used, just return and do not use GPU",
    "if not all feature groups are used, we need to transfer the feature mask to GPU",
    "otherwise, we will use a specialized GPU kernel with all feature groups enabled",
    "All data have been prepared, now run the GPU kernel",
    "construct smaller leaf",
    "ConstructGPUHistogramsAsync will return true if there are availabe feature gourps dispatched to GPU",
    "then construct sparse features on CPU",
    "wait for GPU to finish, only if GPU is actually used",
    "use double precision",
    "use single precision",
    "Compare GPU histogram with CPU histogram, useful for debuggin GPU code problem",
    "#define GPU_DEBUG_COMPARE",
    "construct larger leaf",
    "then construct sparse features on CPU",
    "wait for GPU to finish, only if GPU is actually used",
    "use double precision",
    "use single precision",
    "do some sanity check for the GPU algorithm",
    "limit top k",
    "get max bin",
    "calculate buffer size",
    "need to be able to hold smaller and larger best splits in SyncUpGlobalBestSplit",
    "left and right on same time, so need double size",
    "initialize histograms for global",
    "sync global data sumup info",
    "set global sumup info",
    "init global data count in leaf",
    "get local sumup",
    "get local sumup",
    "get mean number on machines",
    "weighted gain",
    "get top k",
    "Copy histogram to buffer, and Get local aggregate features",
    "copy histograms.",
    "copy smaller leaf histograms first",
    "mark local aggregated feature",
    "copy",
    "then copy larger leaf histograms",
    "mark local aggregated feature",
    "copy",
    "use local data to find local best splits",
    "find splits",
    "only has root leaf",
    "local voting",
    "gather",
    "get all top-k from all machines",
    "global voting",
    "copy local histgrams to buffer",
    "Reduce scatter for histogram",
    "find best split from local aggregated histograms",
    "restore from buffer",
    "restore from buffer",
    "find local best",
    "find local best split for larger leaf",
    "sync global best info",
    "copy back",
    "set the global number of data for leaves",
    "init the global sumup info",
    "instantiate template classes, otherwise linker cannot find the code"
  ],
  "v2.3.1": [
    "coding: utf-8",
    "Apple Clang with OpenMP",
    "coding: utf-8",
    "create predictor first",
    "check dataset",
    "reduce cost for prediction training data",
    "process callbacks",
    "Most of legacy advanced options becomes callbacks",
    "construct booster",
    "start training",
    "check evaluation result.",
    "lambdarank task, split according to groups",
    "run preprocessing on the data set if needed",
    "setup callbacks",
    "coding: utf-8",
    "simplejson does not support Python 3.2, it throws a SyntaxError",
    "because of u'...' Unicode literals.",
    "DeprecationWarning is not shown by default, so let's create our own with higher level",
    "coding: utf-8",
    "minor change to support `**kwargs`",
    "user can set verbose with kwargs, it has higher priority",
    "register default metric for consistency with callable eval_metric case",
    "try to deduce from class instance",
    "overwrite default metric by explicitly set metric",
    "concatenate metric from params (or default if not provided in params) and eval_metric",
    "reduce cost for prediction training data",
    "free dataset",
    "Switch to using a multiclass objective in the underlying LGBM instance",
    "check group data",
    "coding: utf-8",
    "we don't need lib_lightgbm while building docs",
    "coding: utf-8",
    "\"#ddffdd\" is light green, \"#ffdddd\" is light red",
    "coding: utf-8",
    "coding: utf-8",
    "TypeError: obj is not a string or a number",
    "ValueError: invalid literal",
    "__get_num_preds() cannot work with nrow > MAX_INT32, so calculate overall number of predictions piecemeal",
    "avoid memory consumption by arrays concatenation operations",
    "__get_num_preds() cannot work with nrow > MAX_INT32, so calculate overall number of predictions piecemeal",
    "avoid memory consumption by arrays concatenation operations",
    "check data has header or not",
    "need to regroup init_score",
    "process for args",
    "user can set verbose with params, it has higher priority",
    "get categorical features",
    "process for reference dataset",
    "start construct data",
    "set feature names",
    "create valid",
    "construct subset",
    "create train",
    "set to None",
    "we're done if self and reference share a common upstrem reference",
    "group data from LightGBM is boundaries data, need to convert to group size",
    "user can set verbose with params, it has higher priority",
    "Training task",
    "set network if necessary",
    "construct booster object",
    "save reference to data",
    "buffer for inner predict",
    "Prediction task",
    "need reset training data",
    "need to push new valid data",
    "if buffer length is not long enough, re-allocate a buffer",
    "if buffer length is not long enough, reallocate a buffer",
    "Copy models",
    "Get name of features",
    "avoid to predict many time in one iteration",
    "Get num of inner evals",
    "Get name of evals",
    "coding: utf-8",
    "Callback environment used by callbacks",
    "split is needed for \"<dataset type> <metric>\" case (e.g. \"train l1\")",
    "split is needed for \"<dataset type> <metric>\" case (e.g. \"train l1\")",
    "coding: utf-8",
    "load or create your dataset",
    "create dataset for lightgbm",
    "if you want to re-use data, remember to set free_raw_data=False",
    "specify your configurations as a dict",
    "generate feature names",
    "feature_name and categorical_feature",
    "check feature name",
    "save model to file",
    "dump model to JSON (and save to file)",
    "feature names",
    "feature importances",
    "load model to predict",
    "can only predict with the best iteration (or the saving iteration)",
    "eval with loaded model",
    "dump model with pickle",
    "load model with pickle to predict",
    "can predict with any iteration when loaded in pickle way",
    "eval with loaded model",
    "continue training",
    "init_model accepts:",
    "1. model file name",
    "2. Booster()",
    "decay learning rates",
    "learning_rates accepts:",
    "1. list/tuple with length = num_boost_round",
    "2. function(curr_iter)",
    "change other parameters during training",
    "self-defined objective function",
    "f(preds: array, train_data: Dataset) -> grad: array, hess: array",
    "log likelihood loss",
    "self-defined eval metric",
    "f(preds: array, train_data: Dataset) -> name: string, eval_result: float, is_higher_better: bool",
    "binary error",
    "another self-defined eval metric",
    "f(preds: array, train_data: Dataset) -> name: string, eval_result: float, is_higher_better: bool",
    "accuracy",
    "callback",
    "coding: utf-8",
    "load or create your dataset",
    "train",
    "predict",
    "eval",
    "feature importances",
    "self-defined eval metric",
    "f(y_true: array, y_pred: array) -> name: string, eval_result: float, is_higher_better: bool",
    "Root Mean Squared Logarithmic Error (RMSLE)",
    "train",
    "another self-defined eval metric",
    "f(y_true: array, y_pred: array) -> name: string, eval_result: float, is_higher_better: bool",
    "Relative Absolute Error (RAE)",
    "train",
    "predict",
    "eval",
    "other scikit-learn modules",
    "coding: utf-8",
    "load or create your dataset",
    "create dataset for lightgbm",
    "specify your configurations as a dict",
    "train",
    "coding: utf-8",
    "################",
    "Simulate some binary data with a single categorical and",
    "single continuous predictor",
    "################",
    "Set up a couple of utilities for our experiments",
    "################",
    "Observe the behavior of `binary` and `xentropy` objectives",
    "Trying this throws an error on non-binary values of y:",
    "experiment('binary', label_type='probability', DATA)",
    "The speed of `binary` is not drastically different than",
    "`xentropy`. `xentropy` runs faster than `binary` in many cases, although",
    "there are reasons to suspect that `binary` should run faster when the",
    "label is an integer instead of a float",
    "coding: utf-8",
    "load or create your dataset",
    "create dataset for lightgbm",
    "specify your configurations as a dict",
    "train",
    "save model to file",
    "predict",
    "eval",
    "coding: utf-8",
    "!/usr/bin/env python3",
    "-*- coding: utf-8 -*-",
    "",
    "LightGBM documentation build configuration file, created by",
    "sphinx-quickstart on Thu May  4 14:30:58 2017.",
    "",
    "This file is execfile()d with the current directory set to its",
    "containing dir.",
    "",
    "Note that not all possible configuration values are present in this",
    "autogenerated file.",
    "",
    "All configuration values have a default; values that are commented out",
    "serve to show the default.",
    "If extensions (or modules to document with autodoc) are in another directory,",
    "add these directories to sys.path here. If the directory is relative to the",
    "documentation root, use os.path.abspath to make it absolute.",
    "-- mock out modules",
    "-- General configuration ------------------------------------------------",
    "If your documentation needs a minimal Sphinx version, state it here.",
    "Add any Sphinx extension module names here, as strings. They can be",
    "extensions coming with Sphinx (named 'sphinx.ext.*') or your custom",
    "ones.",
    "Generate autosummary pages. Output should be set with: `:toctree: pythonapi/`",
    "Only the class' docstring is inserted.",
    "If true, `todo` and `todoList` produce output, else they produce nothing.",
    "The master toctree document.",
    "General information about the project.",
    "The version info for the project you're documenting, acts as replacement for",
    "|version| and |release|, also used in various other places throughout the",
    "built documents.",
    "The short X.Y version.",
    "The full version, including alpha/beta/rc tags.",
    "The language for content autogenerated by Sphinx. Refer to documentation",
    "for a list of supported languages.",
    "",
    "This is also used if you do content translation via gettext catalogs.",
    "Usually you set \"language\" from the command line for these cases.",
    "List of patterns, relative to source directory, that match files and",
    "directories to ignore when looking for source files.",
    "This patterns also effect to html_static_path and html_extra_path",
    "The name of the Pygments (syntax highlighting) style to use.",
    "-- Configuration for C API docs generation ------------------------------",
    "-- Options for HTML output ----------------------------------------------",
    "The theme to use for HTML and HTML Help pages.  See the documentation for",
    "a list of builtin themes.",
    "Theme options are theme-specific and customize the look and feel of a theme",
    "further.  For a list of options available for each theme, see the",
    "documentation.",
    "Add any paths that contain custom static files (such as style sheets) here,",
    "relative to this directory. They are copied after the builtin static files,",
    "so a file named \"default.css\" will overwrite the builtin \"default.css\".",
    "-- Options for HTMLHelp output ------------------------------------------",
    "Output file base name for HTML help builder.",
    "Warning! The following code can cause buffer overflows on RTD.",
    "Consider suppressing output completely if RTD project silently fails.",
    "Refer to https://github.com/svenevs/exhale",
    "/blob/fe7644829057af622e467bb529db6c03a830da99/exhale/deploy.py#L99-L111",
    "Warning! The following code can cause buffer overflows on RTD.",
    "Consider suppressing output completely if RTD project silently fails.",
    "Refer to https://github.com/svenevs/exhale",
    "/blob/fe7644829057af622e467bb529db6c03a830da99/exhale/deploy.py#L99-L111",
    "coding: utf-8",
    "coding: utf-8",
    "we don't need lib_lightgbm while building docs",
    "coding: utf-8",
    "check saved model persistence",
    "we need to check the consistency of model file here, so test for exact equal",
    "check early stopping is working. Make it stop very early, so the scores should be very close to zero",
    "scores likely to be different, but prediction should still be the same",
    "test that shape is checked during prediction",
    "Set extremely harsh penalties, so CEGB will block most splits.",
    "Compare pairs of penalties, to ensure scaling works as intended",
    "Reset booster1's parameters to p2, so the parameter section of the file matches.",
    "coding: utf-8",
    "Test that the largest element is NOT the same, the smallest can be the same, i.e. zero",
    "sklearn <0.19 cannot accept instance, but many tests could be passed only with min_data=1 and min_data_in_bin=1",
    "we cannot use `check_estimator` directly since there is no skip test mechanism",
    "we cannot leave default params (see https://github.com/microsoft/LightGBM/issues/833)",
    "With default params",
    "Tests same probabilities",
    "Tests same predictions",
    "Tests same raw scores",
    "Tests same leaf indices",
    "Tests same feature contributions",
    "Tests other parameters for the prediction works",
    "no custom objective, no custom metric",
    "default metric",
    "non-default metric",
    "no metric",
    "non-default metric in eval_metric",
    "non-default metric with non-default metric in eval_metric",
    "non-default metric with multiple metrics in eval_metric",
    "default metric for non-default objective",
    "non-default metric for non-default objective",
    "no metric",
    "non-default metric in eval_metric for non-default objective",
    "non-default metric with non-default metric in eval_metric for non-default objective",
    "non-default metric with multiple metrics in eval_metric for non-default objective",
    "custom objective, no custom metric",
    "default regression metric for custom objective",
    "non-default regression metric for custom objective",
    "multiple regression metrics for custom objective",
    "no metric",
    "default regression metric with non-default metric in eval_metric for custom objective",
    "non-default regression metric with metric in eval_metric for custom objective",
    "multiple regression metrics with metric in eval_metric for custom objective",
    "multiple regression metrics with multiple metrics in eval_metric for custom objective",
    "no custom objective, custom metric",
    "default metric with custom metric",
    "non-default metric with custom metric",
    "multiple metrics with custom metric",
    "custom metric (disable default metric)",
    "default metric for non-default objective with custom metric",
    "non-default metric for non-default objective with custom metric",
    "multiple metrics for non-default objective with custom metric",
    "custom metric (disable default metric for non-default objective)",
    "custom objective, custom metric",
    "custom metric for custom objective",
    "non-default regression metric with custom metric for custom objective",
    "multiple regression metrics with custom metric for custom objective",
    "default metric and invalid binary metric is replaced with multiclass alternative",
    "invalid objective is replaced with default multiclass one",
    "and invalid binary metric is replaced with multiclass alternative",
    "default metric for non-default multiclass objective",
    "and invalid binary metric is replaced with multiclass alternative",
    "default metric and invalid multiclass metric is replaced with binary alternative",
    "invalid multiclass metric is replaced with binary alternative for custom objective",
    "training data as eval_set",
    "feval",
    "single eval_set",
    "two eval_set",
    "coding: utf-8",
    "coding: utf-8",
    "check that default gives same result as k = 1",
    "check against independent calculation for k = 1",
    "check against independent calculation for k = 2",
    "check against independent calculation for k = 10",
    "check cases where predictions are equal",
    "no early stopping",
    "early stopping occurs",
    "test custom eval metrics",
    "shuffle = False, override metric in params",
    "shuffle = True, callbacks",
    "enable display training loss",
    "self defined folds",
    "lambdarank",
    "... with l2 metric",
    "... with NDCG (default) metric",
    "self defined folds with lambdarank",
    "test feature_names with whitespaces",
    "take subsets and train",
    "test sliced labels",
    "append some columns",
    "append some rows",
    "test sliced 2d matrix",
    "test sliced CSR",
    "no fobj, no feval",
    "default metric",
    "non-default metric in params",
    "default metric in args",
    "non-default metric in args",
    "metric in args overwrites one in params",
    "multiple metrics in params",
    "multiple metrics in args",
    "remove default metric by 'None' in list",
    "remove default metric by 'None' aliases",
    "fobj, no feval",
    "no default metric",
    "metric in params",
    "metric in args",
    "metric in args overwrites its' alias in params",
    "multiple metrics in params",
    "multiple metrics in args",
    "no fobj, feval",
    "default metric with custom one",
    "non-default metric in params with custom one",
    "default metric in args with custom one",
    "non-default metric in args with custom one",
    "metric in args overwrites one in params, custom one is evaluated too",
    "multiple metrics in params with custom one",
    "multiple metrics in args with custom one",
    "custom metric is evaluated despite 'None' is passed",
    "fobj, feval",
    "no default metric, only custom one",
    "metric in params with custom one",
    "metric in args with custom one",
    "metric in args overwrites one in params, custom one is evaluated too",
    "multiple metrics in params with custom one",
    "multiple metrics in args with custom one",
    "custom metric is evaluated despite 'None' is passed",
    "no fobj, no feval",
    "default metric",
    "default metric in params",
    "non-default metric in params",
    "multiple metrics in params",
    "remove default metric by 'None' aliases",
    "fobj, no feval",
    "no default metric",
    "metric in params",
    "multiple metrics in params",
    "no fobj, feval",
    "default metric with custom one",
    "default metric in params with custom one",
    "non-default metric in params with custom one",
    "multiple metrics in params with custom one",
    "custom metric is evaluated despite 'None' is passed",
    "fobj, feval",
    "no default metric, only custom one",
    "metric in params with custom one",
    "multiple metrics in params with custom one",
    "custom metric is evaluated despite 'None' is passed",
    "multiclass default metric",
    "multiclass default metric with custom one",
    "multiclass metric alias with custom one for custom objective",
    "no metric for invalid class_num",
    "custom metric for invalid class_num",
    "multiclass metric alias with custom one with invalid class_num",
    "multiclass default metric without num_class",
    "multiclass metric alias",
    "multiclass metric",
    "non-valid metric for multiclass objective",
    "non-default num_class for default objective",
    "no metric with non-default num_class for custom objective",
    "multiclass metric alias for custom objective",
    "multiclass metric for custom objective",
    "binary metric with non-default num_class for custom objective",
    "test XGBoost-style return value",
    "test numpy-style return value",
    "test bins string type",
    "test histogram is disabled for categorical features",
    "test for lgb.train",
    "test feval for lgb.train",
    "test with two valid data for lgb.train",
    "test for lgb.cv",
    "test feval for lgb.cv",
    "test that binning works properly for features with only positive or only negative values",
    "coding: utf-8",
    "coding: utf-8",
    "alias table",
    "names",
    "from strings",
    "tails",
    "tails",
    "coding: utf-8",
    "convert from one-based to  zero-based index",
    "convert from boundaries to size",
    "--- start Booster interfaces",
    "Single row predictor to abstract away caching logic",
    "create boosting",
    "initialize the boosting",
    "create objective function",
    "initialize the objective function",
    "create training metric",
    "reset the boosting",
    "create objective function",
    "initialize the objective function",
    "some help functions used to convert data",
    "Row iterator of on column for CSC matrix",
    "return value at idx, only can access by ascent order",
    "return next non-zero pair, if index < 0, means no more data",
    "start of c_api functions",
    "sample data first",
    "sample data first",
    "sample data first",
    "local buffer to re-use memory",
    "sample data first",
    "no more data",
    "---- start of booster",
    "---- start of some help functions",
    "data is array of pointers to individual rows",
    "set number of threads for openmp",
    "check for alias",
    "read parameters from config file",
    "remove str after \"#\"",
    "check for alias again",
    "load configs",
    "prediction is needed if using input initial model(continued train)",
    "need to continue training",
    "sync up random seed for data partition",
    "load Training data",
    "load data for parallel training",
    "load data for single machine",
    "need save binary file",
    "create training metric",
    "only when have metrics then need to construct validation data",
    "Add validation data, if it exists",
    "add",
    "need save binary file",
    "add metric for validation data",
    "output used time on each iteration",
    "need init network",
    "create boosting",
    "create objective function",
    "load training data",
    "initialize the objective function",
    "initialize the boosting",
    "add validation data into boosting",
    "convert model to if-else statement code",
    "create predictor",
    "Free memory",
    "create predictor",
    "label_gain = 2^i - 1, may overflow, so we use 31 here",
    "counts for all labels",
    "start from top label, and accumulate DCG",
    "counts for all labels",
    "calculate k Max DCG by one pass",
    "get sorted indices by score",
    "calculate dcg",
    "get sorted indices by score",
    "calculate multi dcg by one pass",
    "wait for all client start up",
    "default set to -1",
    "distance at k-th communication, distance[k] = 2^k",
    "set incoming rank at k-th commuication",
    "set outgoing rank at k-th commuication",
    "defalut set as -1",
    "construct all recursive halving map for all machines",
    "let 1 << k <= num_machines",
    "distance of each communication",
    "if num_machines = 2^k, don't need to group machines",
    "communication direction, %2 == 0 is positive",
    "neighbor at k-th communication",
    "receive data block at k-th communication",
    "send data block at k-th communication",
    "if num_machines != 2^k, need to group machines",
    "group, two machine in one group, total \"rest\" groups will have 2 machines.",
    "let left machine as group leader",
    "cache block information for groups, group with 2 machines will have double block size",
    "convert from group to node leader",
    "convert from node to group",
    "meet new group",
    "add block len for this group",
    "calculate the group block start",
    "not need to construct",
    "get receive block informations",
    "accumulate block len",
    "get send block informations",
    "accumulate block len",
    "static member definition",
    "if small package or small count , do it by all gather.(reduce the communication times.)",
    "assign the blocks to every rank.",
    "do reduce scatter",
    "do all gather",
    "assign blocks",
    "need use buffer here, since size of \"output\" is smaller than size after all gather",
    "copy back",
    "assign blocks",
    "start all gather",
    "when num_machines is small and data is large",
    "use output as receive buffer",
    "get current local block size",
    "get out rank",
    "get in rank",
    "get send information",
    "get recv information",
    "send and recv at same time",
    "rotate in-place",
    "use output as receive buffer",
    "get current local block size",
    "get send information",
    "get recv information",
    "send and recv at same time",
    "use output as receive buffer",
    "send and recv at same time",
    "send local data to neighbor first",
    "receive neighbor data first",
    "reduce",
    "get target",
    "get send information",
    "get recv information",
    "send and recv at same time",
    "reduce",
    "send result to neighbor",
    "receive result from neighbor",
    "copy result",
    "start up socket",
    "parse clients from file",
    "get ip list of local machine",
    "get local rank",
    "construct listener",
    "construct communication topo",
    "construct linkers",
    "free listener",
    "set timeout",
    "accept incoming socket",
    "receive rank",
    "add new socket",
    "save ranks that need to connect with",
    "start listener",
    "start connect",
    "let smaller rank connect to larger rank",
    "send local rank",
    "wait for listener",
    "print connected linkers",
    "valid the type",
    "Constructors",
    "Get type tag",
    "Comparisons",
    "This has to be separate, not in Statics, because Json() accesses statics().null.",
    "advance until next line, or end of input",
    "advance until closing tokens",
    "The usual case: non-escaped characters",
    "Handle escapes",
    "Extract 4-byte escape sequence",
    "Explicitly check length of the substring. The following loop",
    "relies on std::string returning the terminating NUL when",
    "accessing str[length]. Checking here reduces brittleness.",
    "JSON specifies that characters outside the BMP shall be encoded as a pair",
    "of 4-hex-digit \\u escapes encoding their surrogate pair components. Check",
    "whether we're in the middle of such a beast: the previous codepoint was an",
    "escaped lead (high) surrogate, and this is a trail (low) surrogate.",
    "Reassemble the two surrogate pairs into one astral-plane character, per",
    "the UTF-16 algorithm.",
    "Integer part",
    "Decimal part",
    "Exponent part",
    "Check for any trailing garbage",
    "Documented in json11.hpp",
    "Check for another object",
    "get column names",
    "load label idx first",
    "erase label column name",
    "load ignore columns",
    "load weight idx",
    "load group idx",
    "don't support query id in data file when training in parallel",
    "read data to memory",
    "sample data",
    "construct feature bin mappers",
    "initialize label",
    "extract features",
    "sample data from file",
    "construct feature bin mappers",
    "initialize label",
    "extract features",
    "load data from binary file",
    "check meta data",
    "need to check training data",
    "read data in memory",
    "initialize label",
    "extract features",
    "Get number of lines of data file",
    "initialize label",
    "extract features",
    "load data from binary file",
    "not need to check validation data",
    "check meta data",
    "buffer to read binary file",
    "check token",
    "read size of header",
    "re-allocmate space if not enough",
    "read header",
    "get header",
    "num_groups",
    "real_feature_idx_",
    "feature2group",
    "feature2subfeature",
    "group_bin_boundaries",
    "group_feature_start_",
    "group_feature_cnt_",
    "get feature names",
    "write feature names",
    "get forced_bin_bounds_",
    "read size of meta data",
    "re-allocate space if not enough",
    "read meta data",
    "load meta data",
    "sample local used data if need to partition",
    "if not contain query file, minimal sample unit is one record",
    "if contain query file, minimal sample unit is one query",
    "if is new query",
    "read feature data",
    "read feature size",
    "re-allocate space if not enough",
    "fill feature_names_ if not header",
    "get forced split",
    "if only one machine, find bin locally",
    "if have multi-machines, need to find bin distributed",
    "different machines will find bin for different features",
    "start and len will store the process feature indices for different machines",
    "machine i will find bins for features in [ start[i], start[i] + len[i] )",
    "free",
    "gather global feature bin mappers",
    "restore features bins from buffer",
    "---- private functions ----",
    "if features are ordered, not need to use hist_buf",
    "read all lines",
    "get query data",
    "if not contain query data, minimal sample unit is one record",
    "if contain query data, minimal sample unit is one query",
    "if is new query",
    "get query data",
    "if not contain query file, minimal sample unit is one record",
    "if contain query file, minimal sample unit is one query",
    "if is new query",
    "parse features",
    "get forced split",
    "check the range of label_idx, weight_idx and group_idx",
    "fill feature_names_ if not header",
    "start find bins",
    "if only one machine, find bin locally",
    "start and len will store the process feature indices for different machines",
    "machine i will find bins for features in [ start[i], start[i] + len[i] )",
    "free",
    "gather global feature bin mappers",
    "restore features bins from buffer",
    "if doesn't need to prediction with initial model",
    "parser",
    "set label",
    "free processed line:",
    "shrink_to_fit will be very slow in linux, and seems not free memory, disable for now",
    "text_reader_->Lines()[i].shrink_to_fit();",
    "push data",
    "if is used feature",
    "if need to prediction with initial model",
    "parser",
    "set initial score",
    "set label",
    "free processed line:",
    "shrink_to_fit will be very slow in linux, and seems not free memory, disable for now",
    "text_reader_->Lines()[i].shrink_to_fit();",
    "push data",
    "if is used feature",
    "metadata_ will manage space of init_score",
    "text data can be free after loaded feature values",
    "parser",
    "set initial score",
    "set label",
    "push data",
    "if is used feature",
    "only need part of data",
    "need full data",
    "metadata_ will manage space of init_score",
    "read size of token",
    "remove duplicates",
    "deep copy function for BinMapper",
    "mean size for one bin",
    "need a new bin",
    "update bin upper bound",
    "last bin upper bound",
    "get list of distinct values",
    "get number of positive and negative distinct values",
    "include zero bounds and infinity bound",
    "add forced bounds, excluding zeros since we have already added zero bounds",
    "find remaining bounds",
    "find distinct_values first",
    "push zero in the front",
    "use the large value",
    "push zero in the back",
    "convert to int type first",
    "sort by counts",
    "avoid first bin is zero",
    "will ignore the categorical of small counts",
    "need an additional bin for NaN",
    "use -1 to represent NaN",
    "Use MissingType::None to represent this bin contains all categoricals",
    "check trivial(num_bin_ == 1) feature",
    "check useless bin",
    "calculate sparse rate",
    "sparse threshold",
    "for lambdarank, it needs query data for partition data in parallel learning",
    "need convert query_id to boundaries",
    "check weights",
    "check query boundries",
    "contain initial score file",
    "check weights",
    "get local weights",
    "check query boundries",
    "get local query boundaries",
    "contain initial score file",
    "get local initial scores",
    "re-load query weight",
    "save to nullptr",
    "save to nullptr",
    "save to nullptr",
    "default weight file name",
    "default weight file name",
    "use first line to count number class",
    "default query file name",
    "root is in the depth 0",
    "non-leaf",
    "leaf",
    "use this for the missing value conversion",
    "Predict func by Map to ifelse",
    "use this for the missing value conversion",
    "non-leaf",
    "left subtree",
    "right subtree",
    "leaf",
    "non-leaf",
    "left subtree",
    "right subtree",
    "leaf",
    "recursive computation of SHAP values for a decision tree",
    "extend the unique path",
    "leaf node",
    "internal node",
    "see if we have already split on this feature,",
    "if so we undo that split so we can redo it for this node",
    "add names of objective function if not providing metric",
    "generate seeds by seed.",
    "sort eval_at",
    "Only push the non-training data",
    "check for conflicts",
    "check if objective, metric, and num_class match",
    "Change pool size to -1 (no limit) when using data parallel to reduce communication costs",
    "Check max_depth and num_leaves",
    "Fits in an int, and is more restrictive than the current num_leaves",
    "filter is based on sampling data, so decrease its range",
    "put dense feature first",
    "sort by non zero cnt",
    "sort by non zero cnt, bigger first",
    "take apart small sparse group, due it will not gain on speed",
    "shuffle groups",
    "get num_features",
    "get bin_mappers",
    "copy feature bin mapper data",
    "copy feature bin mapper data",
    "if not pass a filename, just append \".bin\" of original file",
    "get size of header",
    "size of feature names",
    "size of forced bins",
    "write header",
    "write feature names",
    "write forced bins",
    "get size of meta data",
    "write meta data",
    "write feature data",
    "get size of feature",
    "write feature",
    "feature is not used",
    "construct histograms for smaller leaf",
    "if not use ordered bin",
    "used ordered bin",
    "feature is not used",
    "construct histograms for smaller leaf",
    "if not use ordered bin",
    "used ordered bin",
    "fixed hessian.",
    "feature is not used",
    "construct histograms for smaller leaf",
    "if not use ordered bin",
    "used ordered bin",
    "feature is not used",
    "construct histograms for smaller leaf",
    "if not use ordered bin",
    "used ordered bin",
    "fixed hessian.",
    "Skip the leading 0 when copying group_bin_boundaries.",
    "PredictRaw",
    "PredictRawByMap",
    "Predict",
    "PredictByMap",
    "PredictLeafIndex",
    "PredictLeafIndexByMap",
    "output model type",
    "output number of class",
    "output label index",
    "output max_feature_idx",
    "output objective",
    "output tree models",
    "store the importance first",
    "sort the importance",
    "use serialized string to restore this object",
    "Use first 128 chars to avoid exceed the message buffer.",
    "get number of classes",
    "get index of label",
    "get max_feature_idx first",
    "get average_output",
    "get feature names",
    "get monotone_constraints",
    "set zero",
    "predict all the trees for one iteration",
    "check early stopping",
    "set zero",
    "predict all the trees for one iteration",
    "check early stopping",
    "margin_threshold will be captured by value",
    "copy and sort",
    "margin_threshold will be captured by value",
    "load forced_splits file",
    "init tree learner",
    "push training metrics",
    "create buffer for gradients and hessians",
    "get max feature index",
    "get label index",
    "get feature names",
    "if need bagging, create buffer",
    "for a validation dataset, we need its score and metric",
    "update score",
    "objective function will calculate gradients and hessians",
    "random bagging, minimal unit is one record",
    "from right to left",
    "random bagging, minimal unit is one record",
    "reverse right buffer",
    "if need bagging",
    "set bagging data to tree learner",
    "get subset",
    "output used time per iteration",
    "boosting from average label; or customized \"average\" if implemented for the current objective",
    "boosting first",
    "bagging logic",
    "need to copy gradients for bagging subset.",
    "shrinkage by learning rate",
    "update score",
    "only add default score one-time",
    "updates scores",
    "add model",
    "reset score",
    "remove model",
    "print message for metric",
    "pop last early_stopping_round_ models",
    "update training score",
    "we need to predict out-of-bag scores of data for boosting",
    "update validation score",
    "print training metric",
    "print validation metric",
    "set zero",
    "predict all the trees for one iteration",
    "check early stopping",
    "push training metrics",
    "not same training data, need reset score and others",
    "create score tracker",
    "update score",
    "create buffer for gradients and hessians",
    "if need bagging, create buffer",
    "Get the max size of pool",
    "at least need 2 leaves",
    "push split information for all leaves",
    "get ordered bin",
    "check existing for ordered bin",
    "initialize splits for leaf",
    "initialize data partition",
    "initialize ordered gradients and hessians",
    "if has ordered bin, need to allocate a buffer to fast split",
    "get ordered bin",
    "initialize splits for leaf",
    "initialize data partition",
    "initialize ordered gradients and hessians",
    "if has ordered bin, need to allocate a buffer to fast split",
    "Get the max size of pool",
    "at least need 2 leaves",
    "push split information for all leaves",
    "some initial works before training",
    "root leaf",
    "only root leaf can be splitted on first time",
    "some initial works before finding best split",
    "find best threshold for every feature",
    "Get a leaf with max split gain",
    "Get split information for best leaf",
    "cannot split, quit",
    "split tree with best leaf",
    "reset histogram pool",
    "initialize data partition",
    "reset the splits for leaves",
    "Sumup for root",
    "use all data",
    "use bagging, only use part of data",
    "if has ordered bin, need to initialize the ordered bin",
    "use all data, pass nullptr",
    "bagging, only use part of data",
    "mark used data",
    "initialize ordered bin",
    "check depth of current leaf",
    "only need to check left leaf, since right leaf is in same level of left leaf",
    "no enough data to continue",
    "only have root",
    "put parent(left) leaf's histograms into larger leaf's histograms",
    "put parent(left) leaf's histograms to larger leaf's histograms",
    "split for the ordered bin",
    "mark data that at left-leaf",
    "split the ordered bin",
    "construct smaller leaf",
    "construct larger leaf",
    "find splits",
    "only has root leaf",
    "find best threshold for larger child",
    "start at root leaf",
    "before processing next node from queue, store info for current left/right leaf",
    "store \"best split\" for left and right, even if they might be overwritten by forced split",
    "then, compute own splits",
    "split info should exist because searching in bfs fashion - should have added from parent",
    "split tree, will return right leaf",
    "left = parent",
    "split tree, will return right leaf",
    "init the leaves that used on next iteration",
    "bag_mapper[index_mapper[i]]",
    "get feature partition",
    "get local used features",
    "get best split at smaller leaf",
    "find local best split for larger leaf",
    "sync global best info",
    "update best split",
    "instantiate template classes, otherwise linker cannot find the code",
    "initialize SerialTreeLearner",
    "Get local rank and global machine size",
    "allocate buffer for communication",
    "generate feature partition for current tree",
    "get local used feature",
    "get block start and block len for reduce scatter",
    "get buffer_write_start_pos_",
    "get buffer_read_start_pos_",
    "sync global data sumup info",
    "global sumup reduce",
    "copy back",
    "set global sumup info",
    "init global data count in leaf",
    "construct local histograms",
    "copy to buffer",
    "Reduce scatter for histogram",
    "restore global histograms from buffer",
    "find best threshold for smaller child",
    "only root leaf",
    "construct histgroms for large leaf, we init larger leaf as the parent, so we can just subtract the smaller leaf's histograms",
    "find best threshold for larger child",
    "find local best split for larger leaf",
    "sync global best info",
    "set best split",
    "need update global number of data in leaf",
    "instantiate template classes, otherwise linker cannot find the code",
    "initialize SerialTreeLearner",
    "some additional variables needed for GPU trainer",
    "Initialize GPU buffers and kernels",
    "some functions used for debugging the GPU histogram construction",
    "printf(\"grad %g != %g (%d ULPs)\\n\", h1[i].sum_gradients, h2[i].sum_gradients, ulps);",
    "goto err;",
    "printf(\"hessian %g != %g (%d ULPs)\\n\", h1[i].sum_hessians, h2[i].sum_hessians, ulps);",
    "goto err;",
    "we roughly want 256 workgroups per device, and we have num_dense_feature4_ feature tuples.",
    "also guarantee that there are at least 2K examples per workgroup",
    "return 0;",
    "we have already copied ordered gradients, ordered hessians and indices to GPU",
    "decide the best number of workgroups working on one feature4 tuple",
    "set work group size based on feature size",
    "each 2^exp_workgroups_per_feature workgroups work on a feature4 tuple",
    "we need to refresh the kernel arguments after reallocating",
    "The only argument that needs to be changed later is num_data_",
    "the GPU kernel will process all features in one call, and each",
    "2^exp_workgroups_per_feature (compile time constant) workgroup will",
    "process one feature4 tuple",
    "for the root node, indices are not copied",
    "for constant hessian, hessians are not copied except for the root node",
    "there will be 2^exp_workgroups_per_feature = num_workgroups / num_dense_feature4 sub-histogram per feature4",
    "and we will launch num_feature workgroups for this kernel",
    "will launch threads for all features",
    "the queue should be asynchrounous, and we will can WaitAndGetHistograms() before we start processing dense feature groups",
    "copy the results asynchronously. Size depends on if double precision is used",
    "we will wait for this object in WaitAndGetHistograms",
    "when the output is ready, the computation is done",
    "values of this feature has been redistributed to multiple bins; need a reduction here",
    "how many feature-group tuples we have",
    "leave some safe margin for prefetching",
    "256 work-items per workgroup. Each work-item prefetches one tuple for that feature",
    "clear sparse/dense maps",
    "do nothing if no features can be processed on GPU",
    "allocate memory for all features (FIXME: 4 GB barrier on some devices, need to split to multiple buffers)",
    "unpin old buffer if necessary before destructing them",
    "make ordered_gradients and hessians larger (including extra room for prefetching), and pin them",
    "allocate space for gradients and hessians on device",
    "we will copy gradients and hessians in after ordered_gradients_ and ordered_hessians_ are constructed",
    "allocate feature mask, for disabling some feature-groups' histogram calculation",
    "copy indices to the device",
    "histogram bin entry size depends on the precision (single/double)",
    "create output buffer, each feature has a histogram with device_bin_size_ bins,",
    "each work group generates a sub-histogram of dword_features_ features.",
    "only initialize once here, as this will not need to change when ResetTrainingData() is called",
    "create atomic counters for inter-group coordination",
    "The output buffer is allocated to host directly, to overlap compute and data transfer",
    "find the dense feature-groups and group then into Feature4 data structure (several feature-groups packed into 4 bytes)",
    "looking for dword_features_ non-sparse feature-groups",
    "decide if we need to redistribute the bin",
    "multiplier must be a power of 2",
    "device_bin_mults_.push_back(1);",
    "found",
    "for data transfer time",
    "Now generate new data structure feature4, and copy data to the device",
    "preallocate arrays for all threads, and pin them",
    "building Feature4 bundles; each thread handles dword_features_ features",
    "one feature datapoint is 4 bits",
    "this guarantees that the RawGet() function is inlined, rather than using virtual function dispatching",
    "one feature datapoint is one byte",
    "this guarantees that the RawGet() function is inlined, rather than using virtual function dispatching",
    "Dense bin",
    "Dense 4-bit bin",
    "working on the remaining (less than dword_features_) feature groups",
    "fill the leftover features",
    "fill this empty feature with some \"random\" value",
    "fill this empty feature with some \"random\" value",
    "copying the last 1 to (dword_features - 1) feature-groups in the last tuple",
    "deallocate pinned space for feature copying",
    "data transfer time",
    "for other types of failure, build log might not be available; program.build_log() can crash",
    "Something bad happened. Just return \"No log available.\"",
    "build is okay, log may contain warnings",
    "destroy any old kernels",
    "create OpenCL kernels for different number of workgroups per feature",
    "currently we don't use constant memory",
    "compile the GPU kernel depending if double precision is used, constant hessian is used, etc.",
    "kernel with indices in an array",
    "kernel with all features enabled, with elimited branches",
    "kernel with all data indices (for root node, and assumes that root node always uses all features)",
    "do nothing if no features can be processed on GPU",
    "The only argument that needs to be changed later is num_data_",
    "hessian is passed as a parameter, but it is not available now.",
    "hessian will be set in BeforeTrain()",
    "Get the max bin size, used for selecting best GPU kernel",
    "initialize GPU",
    "determine which kernel to use based on the max number of bins",
    "setup GPU kernel arguments after we allocating all the buffers",
    "check if we need to recompile the GPU kernel (is_constant_hessian changed)",
    "this should rarely occur",
    "GPU memory has to been reallocated because data may have been changed",
    "setup GPU kernel arguments after we allocating all the buffers",
    "Copy initial full hessians and gradients to GPU.",
    "We start copying as early as possible, instead of at ConstructHistogram().",
    "setup hessian parameters only",
    "hessian is passed as a parameter",
    "use bagging",
    "On GPU, we start copying indices, gradients and hessians now, instead at ConstructHistogram()",
    "copy used gradients and hessians to ordered buffer",
    "transfer the indices to GPU",
    "transfer hessian to GPU",
    "setup hessian parameters only",
    "hessian is passed as a parameter",
    "transfer gradients to GPU",
    "only have root",
    "Copy indices, gradients and hessians as early as possible",
    "only need to initialize for smaller leaf",
    "Get leaf boundary",
    "copy indices to the GPU:",
    "copy ordered hessians to the GPU:",
    "copy ordered gradients to the GPU:",
    "do nothing if no features can be processed on GPU",
    "copy data indices if it is not null",
    "generate and copy ordered_gradients if gradients is not null",
    "generate and copy ordered_hessians if hessians is not null",
    "converted indices in is_feature_used to feature-group indices",
    "construct the feature masks for dense feature-groups",
    "if no feature group is used, just return and do not use GPU",
    "if not all feature groups are used, we need to transfer the feature mask to GPU",
    "otherwise, we will use a specialized GPU kernel with all feature groups enabled",
    "All data have been prepared, now run the GPU kernel",
    "construct smaller leaf",
    "ConstructGPUHistogramsAsync will return true if there are availabe feature gourps dispatched to GPU",
    "then construct sparse features on CPU",
    "We set data_indices to null to avoid rebuilding ordered gradients/hessians",
    "wait for GPU to finish, only if GPU is actually used",
    "use double precision",
    "use single precision",
    "Compare GPU histogram with CPU histogram, useful for debuggin GPU code problem",
    "#define GPU_DEBUG_COMPARE",
    "construct larger leaf",
    "then construct sparse features on CPU",
    "We set data_indices to null to avoid rebuilding ordered gradients/hessians",
    "wait for GPU to finish, only if GPU is actually used",
    "use double precision",
    "use single precision",
    "do some sanity check for the GPU algorithm",
    "limit top k",
    "get max bin",
    "calculate buffer size",
    "left and right on same time, so need double size",
    "initialize histograms for global",
    "sync global data sumup info",
    "set global sumup info",
    "init global data count in leaf",
    "get local sumup",
    "get local sumup",
    "get mean number on machines",
    "weighted gain",
    "get top k",
    "Copy histogram to buffer, and Get local aggregate features",
    "copy histograms.",
    "copy smaller leaf histograms first",
    "mark local aggregated feature",
    "copy",
    "then copy larger leaf histograms",
    "mark local aggregated feature",
    "copy",
    "use local data to find local best splits",
    "find splits",
    "only has root leaf",
    "find best threshold for larger child",
    "local voting",
    "gather",
    "get all top-k from all machines",
    "global voting",
    "copy local histgrams to buffer",
    "Reduce scatter for histogram",
    "find best split from local aggregated histograms",
    "restore from buffer",
    "find best threshold",
    "restore from buffer",
    "find best threshold",
    "find local best",
    "find local best split for larger leaf",
    "sync global best info",
    "copy back",
    "set the global number of data for leaves",
    "init the global sumup info",
    "instantiate template classes, otherwise linker cannot find the code"
  ],
  "v2.3.0": [
    "coding: utf-8",
    "pylint: disable=invalid-name, exec-used, C0111",
    "Apple Clang with OpenMP",
    "coding: utf-8",
    "pylint: disable = invalid-name, W0105",
    "create predictor first",
    "check dataset",
    "reduce cost for prediction training data",
    "process callbacks",
    "Most of legacy advanced options becomes callbacks",
    "construct booster",
    "start training",
    "check evaluation result.",
    "lambdarank task, split according to groups",
    "run preprocessing on the data set if needed",
    "setup callbacks",
    "coding: utf-8",
    "pylint: disable = C0103",
    "simplejson does not support Python 3.2, it throws a SyntaxError",
    "because of u'...' Unicode literals.",
    "DeprecationWarning is not shown by default, so let's create our own with higher level",
    "coding: utf-8",
    "pylint: disable = invalid-name, W0105, C0111, C0301",
    "minor change to support `**kwargs`",
    "user can set verbose with kwargs, it has higher priority",
    "register default metric for consistency with callable eval_metric case",
    "try to deduce from class instance",
    "overwrite default metric by explicitly set metric",
    "concatenate metric from params (or default if not provided in params) and eval_metric",
    "reduce cost for prediction training data",
    "free dataset",
    "Switch to using a multiclass objective in the underlying LGBM instance",
    "check group data",
    "coding: utf-8",
    "we don't need lib_lightgbm while building docs",
    "coding: utf-8",
    "pylint: disable = C0103",
    "\"#ddffdd\" is light green, \"#ffdddd\" is light red",
    "coding: utf-8",
    "REMOVEME: remove warning after 2.3.0 version release",
    "coding: utf-8",
    "pylint: disable = invalid-name, C0111, C0301",
    "pylint: disable = R0912, R0913, R0914, W0105, W0201, W0212",
    "TypeError: obj is not a string or a number",
    "ValueError: invalid literal",
    "__get_num_preds() cannot work with nrow > MAX_INT32, so calculate overall number of predictions piecemeal",
    "avoid memory consumption by arrays concatenation operations",
    "__get_num_preds() cannot work with nrow > MAX_INT32, so calculate overall number of predictions piecemeal",
    "avoid memory consumption by arrays concatenation operations",
    "check data has header or not",
    "need to regroup init_score",
    "process for args",
    "user can set verbose with params, it has higher priority",
    "get categorical features",
    "process for reference dataset",
    "start construct data",
    "set feature names",
    "create valid",
    "construct subset",
    "create train",
    "set to None",
    "we're done if self and reference share a common upstrem reference",
    "group data from LightGBM is boundaries data, need to convert to group size",
    "user can set verbose with params, it has higher priority",
    "Training task",
    "set network if necessary",
    "construct booster object",
    "save reference to data",
    "buffer for inner predict",
    "Prediction task",
    "need reset training data",
    "need to push new valid data",
    "if buffer length is not long enough, re-allocate a buffer",
    "if buffer length is not long enough, reallocate a buffer",
    "Copy models",
    "Get name of features",
    "avoid to predict many time in one iteration",
    "Get num of inner evals",
    "Get name of evals",
    "coding: utf-8",
    "pylint: disable = invalid-name, W0105, C0301",
    "Callback environment used by callbacks",
    "split is needed for \"<dataset type> <metric>\" case (e.g. \"train l1\")",
    "split is needed for \"<dataset type> <metric>\" case (e.g. \"train l1\")",
    "coding: utf-8",
    "pylint: disable = invalid-name, C0111",
    "load or create your dataset",
    "create dataset for lightgbm",
    "if you want to re-use data, remember to set free_raw_data=False",
    "specify your configurations as a dict",
    "generate feature names",
    "feature_name and categorical_feature",
    "check feature name",
    "save model to file",
    "dump model to JSON (and save to file)",
    "feature names",
    "feature importances",
    "load model to predict",
    "can only predict with the best iteration (or the saving iteration)",
    "eval with loaded model",
    "dump model with pickle",
    "load model with pickle to predict",
    "can predict with any iteration when loaded in pickle way",
    "eval with loaded model",
    "continue training",
    "init_model accepts:",
    "1. model file name",
    "2. Booster()",
    "decay learning rates",
    "learning_rates accepts:",
    "1. list/tuple with length = num_boost_round",
    "2. function(curr_iter)",
    "change other parameters during training",
    "self-defined objective function",
    "f(preds: array, train_data: Dataset) -> grad: array, hess: array",
    "log likelihood loss",
    "self-defined eval metric",
    "f(preds: array, train_data: Dataset) -> name: string, eval_result: float, is_higher_better: bool",
    "binary error",
    "another self-defined eval metric",
    "f(preds: array, train_data: Dataset) -> name: string, eval_result: float, is_higher_better: bool",
    "accuracy",
    "callback",
    "coding: utf-8",
    "pylint: disable = invalid-name, C0111",
    "load or create your dataset",
    "train",
    "predict",
    "eval",
    "feature importances",
    "self-defined eval metric",
    "f(y_true: array, y_pred: array) -> name: string, eval_result: float, is_higher_better: bool",
    "Root Mean Squared Logarithmic Error (RMSLE)",
    "train",
    "another self-defined eval metric",
    "f(y_true: array, y_pred: array) -> name: string, eval_result: float, is_higher_better: bool",
    "Relative Absolute Error (RAE)",
    "train",
    "predict",
    "eval",
    "other scikit-learn modules",
    "coding: utf-8",
    "pylint: disable = invalid-name, C0111",
    "load or create your dataset",
    "create dataset for lightgbm",
    "specify your configurations as a dict",
    "train",
    "coding: utf-8",
    "pylint: disable = invalid-name, C0111",
    "################",
    "Simulate some binary data with a single categorical and",
    "single continuous predictor",
    "################",
    "Set up a couple of utilities for our experiments",
    "################",
    "Observe the behavior of `binary` and `xentropy` objectives",
    "Trying this throws an error on non-binary values of y:",
    "experiment('binary', label_type='probability', DATA)",
    "The speed of `binary` is not drastically different than",
    "`xentropy`. `xentropy` runs faster than `binary` in many cases, although",
    "there are reasons to suspect that `binary` should run faster when the",
    "label is an integer instead of a float",
    "coding: utf-8",
    "pylint: disable = invalid-name, C0111",
    "load or create your dataset",
    "create dataset for lightgbm",
    "specify your configurations as a dict",
    "train",
    "save model to file",
    "predict",
    "eval",
    "coding: utf-8",
    "!/usr/bin/env python3",
    "-*- coding: utf-8 -*-",
    "",
    "LightGBM documentation build configuration file, created by",
    "sphinx-quickstart on Thu May  4 14:30:58 2017.",
    "",
    "This file is execfile()d with the current directory set to its",
    "containing dir.",
    "",
    "Note that not all possible configuration values are present in this",
    "autogenerated file.",
    "",
    "All configuration values have a default; values that are commented out",
    "serve to show the default.",
    "If extensions (or modules to document with autodoc) are in another directory,",
    "add these directories to sys.path here. If the directory is relative to the",
    "documentation root, use os.path.abspath to make it absolute.",
    "-- mock out modules",
    "-- General configuration ------------------------------------------------",
    "If your documentation needs a minimal Sphinx version, state it here.",
    "Add any Sphinx extension module names here, as strings. They can be",
    "extensions coming with Sphinx (named 'sphinx.ext.*') or your custom",
    "ones.",
    "Generate autosummary pages. Output should be set with: `:toctree: pythonapi/`",
    "Only the class' docstring is inserted.",
    "If true, `todo` and `todoList` produce output, else they produce nothing.",
    "The master toctree document.",
    "General information about the project.",
    "The version info for the project you're documenting, acts as replacement for",
    "|version| and |release|, also used in various other places throughout the",
    "built documents.",
    "The short X.Y version.",
    "The full version, including alpha/beta/rc tags.",
    "The language for content autogenerated by Sphinx. Refer to documentation",
    "for a list of supported languages.",
    "",
    "This is also used if you do content translation via gettext catalogs.",
    "Usually you set \"language\" from the command line for these cases.",
    "List of patterns, relative to source directory, that match files and",
    "directories to ignore when looking for source files.",
    "This patterns also effect to html_static_path and html_extra_path",
    "The name of the Pygments (syntax highlighting) style to use.",
    "-- Configuration for C API docs generation ------------------------------",
    "-- Options for HTML output ----------------------------------------------",
    "The theme to use for HTML and HTML Help pages.  See the documentation for",
    "a list of builtin themes.",
    "Theme options are theme-specific and customize the look and feel of a theme",
    "further.  For a list of options available for each theme, see the",
    "documentation.",
    "Add any paths that contain custom static files (such as style sheets) here,",
    "relative to this directory. They are copied after the builtin static files,",
    "so a file named \"default.css\" will overwrite the builtin \"default.css\".",
    "-- Options for HTMLHelp output ------------------------------------------",
    "Output file base name for HTML help builder.",
    "Warning! The following code can cause buffer overflows on RTD.",
    "Consider suppressing output completely if RTD project silently fails.",
    "Refer to https://github.com/svenevs/exhale",
    "/blob/fe7644829057af622e467bb529db6c03a830da99/exhale/deploy.py#L99-L111",
    "Warning! The following code can cause buffer overflows on RTD.",
    "Consider suppressing output completely if RTD project silently fails.",
    "Refer to https://github.com/svenevs/exhale",
    "/blob/fe7644829057af622e467bb529db6c03a830da99/exhale/deploy.py#L99-L111",
    "coding: utf-8",
    "coding: utf-8",
    "pylint: skip-file",
    "we don't need lib_lightgbm while building docs",
    "coding: utf-8",
    "pylint: skip-file",
    "check saved model persistence",
    "we need to check the consistency of model file here, so test for exact equal",
    "check early stopping is working. Make it stop very early, so the scores should be very close to zero",
    "scores likely to be different, but prediction should still be the same",
    "Set extremely harsh penalties, so CEGB will block most splits.",
    "Compare pairs of penalties, to ensure scaling works as intended",
    "Reset booster1's parameters to p2, so the parameter section of the file matches.",
    "coding: utf-8",
    "pylint: skip-file",
    "Test that the largest element is NOT the same, the smallest can be the same, i.e. zero",
    "sklearn <0.19 cannot accept instance, but many tests could be passed only with min_data=1 and min_data_in_bin=1",
    "we cannot use `check_estimator` directly since there is no skip test mechanism",
    "we cannot leave default params (see https://github.com/microsoft/LightGBM/issues/833)",
    "Tests same probabilities",
    "Tests same predictions",
    "Tests same raw scores",
    "Tests same leaf indices",
    "Tests same feature contributions",
    "Tests other parameters for the prediction works",
    "no custom objective, no custom metric",
    "default metric",
    "non-default metric",
    "no metric",
    "non-default metric in eval_metric",
    "non-default metric with non-default metric in eval_metric",
    "non-default metric with multiple metrics in eval_metric",
    "default metric for non-default objective",
    "non-default metric for non-default objective",
    "no metric",
    "non-default metric in eval_metric for non-default objective",
    "non-default metric with non-default metric in eval_metric for non-default objective",
    "non-default metric with multiple metrics in eval_metric for non-default objective",
    "custom objective, no custom metric",
    "default regression metric for custom objective",
    "non-default regression metric for custom objective",
    "multiple regression metrics for custom objective",
    "no metric",
    "default regression metric with non-default metric in eval_metric for custom objective",
    "non-default regression metric with metric in eval_metric for custom objective",
    "multiple regression metrics with metric in eval_metric for custom objective",
    "multiple regression metrics with multiple metrics in eval_metric for custom objective",
    "no custom objective, custom metric",
    "default metric with custom metric",
    "non-default metric with custom metric",
    "multiple metrics with custom metric",
    "custom metric (disable default metric)",
    "default metric for non-default objective with custom metric",
    "non-default metric for non-default objective with custom metric",
    "multiple metrics for non-default objective with custom metric",
    "custom metric (disable default metric for non-default objective)",
    "custom objective, custom metric",
    "custom metric for custom objective",
    "non-default regression metric with custom metric for custom objective",
    "multiple regression metrics with custom metric for custom objective",
    "default metric and invalid binary metric is replaced with multiclass alternative",
    "invalid objective is replaced with default multiclass one",
    "and invalid binary metric is replaced with multiclass alternative",
    "default metric for non-default multiclass objective",
    "and invalid binary metric is replaced with multiclass alternative",
    "default metric and invalid multiclass metric is replaced with binary alternative",
    "invalid multiclass metric is replaced with binary alternative for custom objective",
    "training data as eval_set",
    "feval",
    "single eval_set",
    "two eval_set",
    "coding: utf-8",
    "pylint: skip-file",
    "coding: utf-8",
    "pylint: skip-file",
    "check that default gives same result as k = 1",
    "check against independent calculation for k = 1",
    "check against independent calculation for k = 2",
    "check against independent calculation for k = 10",
    "check cases where predictions are equal",
    "no early stopping",
    "early stopping occurs",
    "test custom eval metrics",
    "shuffle = False, override metric in params",
    "shuffle = True, callbacks",
    "enable display training loss",
    "self defined folds",
    "lambdarank",
    "... with l2 metric",
    "... with NDCG (default) metric",
    "self defined folds with lambdarank",
    "test feature_names with whitespaces",
    "take subsets and train",
    "test sliced labels",
    "append some columns",
    "append some rows",
    "test sliced 2d matrix",
    "test sliced CSR",
    "no fobj, no feval",
    "default metric",
    "non-default metric in params",
    "default metric in args",
    "non-default metric in args",
    "metric in args overwrites one in params",
    "multiple metrics in params",
    "multiple metrics in args",
    "remove default metric by 'None' in list",
    "remove default metric by 'None' aliases",
    "fobj, no feval",
    "no default metric",
    "metric in params",
    "metric in args",
    "metric in args overwrites its' alias in params",
    "multiple metrics in params",
    "multiple metrics in args",
    "no fobj, feval",
    "default metric with custom one",
    "non-default metric in params with custom one",
    "default metric in args with custom one",
    "non-default metric in args with custom one",
    "metric in args overwrites one in params, custom one is evaluated too",
    "multiple metrics in params with custom one",
    "multiple metrics in args with custom one",
    "custom metric is evaluated despite 'None' is passed",
    "fobj, feval",
    "no default metric, only custom one",
    "metric in params with custom one",
    "metric in args with custom one",
    "metric in args overwrites one in params, custom one is evaluated too",
    "multiple metrics in params with custom one",
    "multiple metrics in args with custom one",
    "custom metric is evaluated despite 'None' is passed",
    "no fobj, no feval",
    "default metric",
    "default metric in params",
    "non-default metric in params",
    "multiple metrics in params",
    "remove default metric by 'None' aliases",
    "fobj, no feval",
    "no default metric",
    "metric in params",
    "multiple metrics in params",
    "no fobj, feval",
    "default metric with custom one",
    "default metric in params with custom one",
    "non-default metric in params with custom one",
    "multiple metrics in params with custom one",
    "custom metric is evaluated despite 'None' is passed",
    "fobj, feval",
    "no default metric, only custom one",
    "metric in params with custom one",
    "multiple metrics in params with custom one",
    "custom metric is evaluated despite 'None' is passed",
    "multiclass default metric",
    "multiclass default metric with custom one",
    "multiclass metric alias with custom one for custom objective",
    "no metric for invalid class_num",
    "custom metric for invalid class_num",
    "multiclass metric alias with custom one with invalid class_num",
    "multiclass default metric without num_class",
    "multiclass metric alias",
    "multiclass metric",
    "non-valid metric for multiclass objective",
    "non-default num_class for default objective",
    "no metric with non-default num_class for custom objective",
    "multiclass metric alias for custom objective",
    "multiclass metric for custom objective",
    "binary metric with non-default num_class for custom objective",
    "test XGBoost-style return value",
    "test numpy-style return value",
    "test bins string type",
    "test histogram is disabled for categorical features",
    "test for lgb.train",
    "test feval for lgb.train",
    "test with two valid data for lgb.train",
    "test for lgb.cv",
    "test feval for lgb.cv",
    "test that binning works properly for features with only positive or only negative values",
    "coding: utf-8",
    "pylint: skip-file",
    "coding: utf-8",
    "alias table",
    "names",
    "from strings",
    "tails",
    "tails",
    "coding: utf-8",
    "convert from one-based to  zero-based index",
    "convert from boundaries to size",
    "--- start Booster interfaces",
    "Single row predictor to abstract away caching logic",
    "create boosting",
    "initialize the boosting",
    "create objective function",
    "initialize the objective function",
    "create training metric",
    "reset the boosting",
    "create objective function",
    "initialize the objective function",
    "some help functions used to convert data",
    "Row iterator of on column for CSC matrix",
    "return value at idx, only can access by ascent order",
    "return next non-zero pair, if index < 0, means no more data",
    "start of c_api functions",
    "sample data first",
    "sample data first",
    "sample data first",
    "local buffer to re-use memory",
    "sample data first",
    "no more data",
    "---- start of booster",
    "---- start of some help functions",
    "data is array of pointers to individual rows",
    "set number of threads for openmp",
    "check for alias",
    "read parameters from config file",
    "remove str after \"#\"",
    "check for alias again",
    "load configs",
    "prediction is needed if using input initial model(continued train)",
    "need to continue training",
    "sync up random seed for data partition",
    "load Training data",
    "load data for parallel training",
    "load data for single machine",
    "need save binary file",
    "create training metric",
    "only when have metrics then need to construct validation data",
    "Add validation data, if it exists",
    "add",
    "need save binary file",
    "add metric for validation data",
    "output used time on each iteration",
    "need init network",
    "create boosting",
    "create objective function",
    "load training data",
    "initialize the objective function",
    "initialize the boosting",
    "add validation data into boosting",
    "convert model to if-else statement code",
    "create predictor",
    "Free memory",
    "create predictor",
    "label_gain = 2^i - 1, may overflow, so we use 31 here",
    "counts for all labels",
    "start from top label, and accumulate DCG",
    "counts for all labels",
    "calculate k Max DCG by one pass",
    "get sorted indices by score",
    "calculate dcg",
    "get sorted indices by score",
    "calculate multi dcg by one pass",
    "wait for all client start up",
    "default set to -1",
    "distance at k-th communication, distance[k] = 2^k",
    "set incoming rank at k-th commuication",
    "set outgoing rank at k-th commuication",
    "defalut set as -1",
    "construct all recursive halving map for all machines",
    "let 1 << k <= num_machines",
    "distance of each communication",
    "if num_machines = 2^k, don't need to group machines",
    "communication direction, %2 == 0 is positive",
    "neighbor at k-th communication",
    "receive data block at k-th communication",
    "send data block at k-th communication",
    "if num_machines != 2^k, need to group machines",
    "group, two machine in one group, total \"rest\" groups will have 2 machines.",
    "let left machine as group leader",
    "cache block information for groups, group with 2 machines will have double block size",
    "convert from group to node leader",
    "convert from node to group",
    "meet new group",
    "add block len for this group",
    "calculate the group block start",
    "not need to construct",
    "get receive block informations",
    "accumulate block len",
    "get send block informations",
    "accumulate block len",
    "static member definition",
    "if small package or small count , do it by all gather.(reduce the communication times.)",
    "assign the blocks to every rank.",
    "do reduce scatter",
    "do all gather",
    "assign blocks",
    "need use buffer here, since size of \"output\" is smaller than size after all gather",
    "copy back",
    "assign blocks",
    "start all gather",
    "when num_machines is small and data is large",
    "use output as receive buffer",
    "get current local block size",
    "get out rank",
    "get in rank",
    "get send information",
    "get recv information",
    "send and recv at same time",
    "rotate in-place",
    "use output as receive buffer",
    "get current local block size",
    "get send information",
    "get recv information",
    "send and recv at same time",
    "use output as receive buffer",
    "send and recv at same time",
    "send local data to neighbor first",
    "receive neighbor data first",
    "reduce",
    "get target",
    "get send information",
    "get recv information",
    "send and recv at same time",
    "reduce",
    "send result to neighbor",
    "receive result from neighbor",
    "copy result",
    "start up socket",
    "parse clients from file",
    "get ip list of local machine",
    "get local rank",
    "construct listener",
    "construct communication topo",
    "construct linkers",
    "free listener",
    "set timeout",
    "accept incoming socket",
    "receive rank",
    "add new socket",
    "save ranks that need to connect with",
    "start listener",
    "start connect",
    "let smaller rank connect to larger rank",
    "send local rank",
    "wait for listener",
    "print connected linkers",
    "Get some statistic from 2 line",
    "if only have one line on file",
    "Constructors",
    "Get type tag",
    "Comparisons",
    "This has to be separate, not in Statics, because Json() accesses statics().null.",
    "advance until next line, or end of input",
    "advance until closing tokens",
    "The usual case: non-escaped characters",
    "Handle escapes",
    "Extract 4-byte escape sequence",
    "Explicitly check length of the substring. The following loop",
    "relies on std::string returning the terminating NUL when",
    "accessing str[length]. Checking here reduces brittleness.",
    "JSON specifies that characters outside the BMP shall be encoded as a pair",
    "of 4-hex-digit \\u escapes encoding their surrogate pair components. Check",
    "whether we're in the middle of such a beast: the previous codepoint was an",
    "escaped lead (high) surrogate, and this is a trail (low) surrogate.",
    "Reassemble the two surrogate pairs into one astral-plane character, per",
    "the UTF-16 algorithm.",
    "Integer part",
    "Decimal part",
    "Exponent part",
    "Check for any trailing garbage",
    "Documented in json11.hpp",
    "Check for another object",
    "get column names",
    "load label idx first",
    "erase label column name",
    "load ignore columns",
    "load weight idx",
    "load group idx",
    "don't support query id in data file when training in parallel",
    "read data to memory",
    "sample data",
    "construct feature bin mappers",
    "initialize label",
    "extract features",
    "sample data from file",
    "construct feature bin mappers",
    "initialize label",
    "extract features",
    "load data from binary file",
    "check meta data",
    "need to check training data",
    "read data in memory",
    "initialize label",
    "extract features",
    "Get number of lines of data file",
    "initialize label",
    "extract features",
    "load data from binary file",
    "not need to check validation data",
    "check meta data",
    "buffer to read binary file",
    "check token",
    "read size of header",
    "re-allocmate space if not enough",
    "read header",
    "get header",
    "num_groups",
    "real_feature_idx_",
    "feature2group",
    "feature2subfeature",
    "group_bin_boundaries",
    "group_feature_start_",
    "group_feature_cnt_",
    "get feature names",
    "write feature names",
    "get forced_bin_bounds_",
    "read size of meta data",
    "re-allocate space if not enough",
    "read meta data",
    "load meta data",
    "sample local used data if need to partition",
    "if not contain query file, minimal sample unit is one record",
    "if contain query file, minimal sample unit is one query",
    "if is new query",
    "read feature data",
    "read feature size",
    "re-allocate space if not enough",
    "fill feature_names_ if not header",
    "get forced split",
    "if only one machine, find bin locally",
    "if have multi-machines, need to find bin distributed",
    "different machines will find bin for different features",
    "start and len will store the process feature indices for different machines",
    "machine i will find bins for features in [ start[i], start[i] + len[i] )",
    "get size of bin mapper with max_bin size",
    "since sizes of different feature may not be same, we expand all bin mapper to type_size",
    "find local feature bins and copy to buffer",
    "free",
    "convert to binary size",
    "gather global feature bin mappers",
    "restore features bins from buffer",
    "---- private functions ----",
    "if features are ordered, not need to use hist_buf",
    "read all lines",
    "get query data",
    "if not contain query data, minimal sample unit is one record",
    "if contain query data, minimal sample unit is one query",
    "if is new query",
    "get query data",
    "if not contain query file, minimal sample unit is one record",
    "if contain query file, minimal sample unit is one query",
    "if is new query",
    "parse features",
    "-1 means doesn't use this feature",
    "get forced split",
    "check the range of label_idx, weight_idx and group_idx",
    "fill feature_names_ if not header",
    "start find bins",
    "if only one machine, find bin locally",
    "if have multi-machines, need to find bin distributed",
    "different machines will find bin for different features",
    "start and len will store the process feature indices for different machines",
    "machine i will find bins for features in [ start[i], start[i] + len[i] )",
    "get size of bin mapper with max_bin size",
    "since sizes of different feature may not be same, we expand all bin mapper to type_size",
    "find local feature bins and copy to buffer",
    "free",
    "convert to binary size",
    "gather global feature bin mappers",
    "restore features bins from buffer",
    "if doesn't need to prediction with initial model",
    "parser",
    "set label",
    "free processed line:",
    "shrink_to_fit will be very slow in linux, and seems not free memory, disable for now",
    "text_reader_->Lines()[i].shrink_to_fit();",
    "push data",
    "if is used feature",
    "if need to prediction with initial model",
    "parser",
    "set initial score",
    "set label",
    "free processed line:",
    "shrink_to_fit will be very slow in linux, and seems not free memory, disable for now",
    "text_reader_->Lines()[i].shrink_to_fit();",
    "push data",
    "if is used feature",
    "metadata_ will manage space of init_score",
    "text data can be free after loaded feature values",
    "parser",
    "set initial score",
    "set label",
    "push data",
    "if is used feature",
    "only need part of data",
    "need full data",
    "metadata_ will manage space of init_score",
    "read size of token",
    "remove duplicates",
    "deep copy function for BinMapper",
    "mean size for one bin",
    "need a new bin",
    "update bin upper bound",
    "last bin upper bound",
    "get list of distinct values",
    "get number of positive and negative distinct values",
    "include zero bounds and infinity bound",
    "add forced bounds, excluding zeros since we have already added zero bounds",
    "find remaining bounds",
    "find distinct_values first",
    "push zero in the front",
    "use the large value",
    "push zero in the back",
    "convert to int type first",
    "sort by counts",
    "avoid first bin is zero",
    "will ignore the categorical of small counts",
    "need an additional bin for NaN",
    "use -1 to represent NaN",
    "Use MissingType::None to represent this bin contains all categoricals",
    "check trivial(num_bin_ == 1) feature",
    "check useless bin",
    "calculate sparse rate",
    "sparse threshold",
    "for lambdarank, it needs query data for partition data in parallel learning",
    "need convert query_id to boundaries",
    "check weights",
    "check query boundries",
    "contain initial score file",
    "check weights",
    "get local weights",
    "check query boundries",
    "get local query boundaries",
    "contain initial score file",
    "get local initial scores",
    "re-load query weight",
    "save to nullptr",
    "save to nullptr",
    "save to nullptr",
    "default weight file name",
    "default weight file name",
    "use first line to count number class",
    "default query file name",
    "root is in the depth 0",
    "non-leaf",
    "leaf",
    "use this for the missing value conversion",
    "Predict func by Map to ifelse",
    "use this for the missing value conversion",
    "non-leaf",
    "left subtree",
    "right subtree",
    "leaf",
    "non-leaf",
    "left subtree",
    "right subtree",
    "leaf",
    "recursive computation of SHAP values for a decision tree",
    "extend the unique path",
    "leaf node",
    "internal node",
    "see if we have already split on this feature,",
    "if so we undo that split so we can redo it for this node",
    "add names of objective function if not providing metric",
    "generate seeds by seed.",
    "sort eval_at",
    "Only push the non-training data",
    "check for conflicts",
    "check if objective, metric, and num_class match",
    "Change pool size to -1 (no limit) when using data parallel to reduce communication costs",
    "Check max_depth and num_leaves",
    "Fits in an int, and is more restrictive than the current num_leaves",
    "filter is based on sampling data, so decrease its range",
    "put dense feature first",
    "sort by non zero cnt",
    "sort by non zero cnt, bigger first",
    "take apart small sparse group, due it will not gain on speed",
    "shuffle groups",
    "get num_features",
    "get bin_mappers",
    "copy feature bin mapper data",
    "copy feature bin mapper data",
    "if not pass a filename, just append \".bin\" of original file",
    "get size of header",
    "size of feature names",
    "size of forced bins",
    "write header",
    "write feature names",
    "write forced bins",
    "get size of meta data",
    "write meta data",
    "write feature data",
    "get size of feature",
    "write feature",
    "feature is not used",
    "construct histograms for smaller leaf",
    "if not use ordered bin",
    "used ordered bin",
    "feature is not used",
    "construct histograms for smaller leaf",
    "if not use ordered bin",
    "used ordered bin",
    "fixed hessian.",
    "feature is not used",
    "construct histograms for smaller leaf",
    "if not use ordered bin",
    "used ordered bin",
    "feature is not used",
    "construct histograms for smaller leaf",
    "if not use ordered bin",
    "used ordered bin",
    "fixed hessian.",
    "Skip the leading 0 when copying group_bin_boundaries.",
    "PredictRaw",
    "PredictRawByMap",
    "Predict",
    "PredictByMap",
    "PredictLeafIndex",
    "PredictLeafIndexByMap",
    "output model type",
    "output number of class",
    "output label index",
    "output max_feature_idx",
    "output objective",
    "output tree models",
    "store the importance first",
    "sort the importance",
    "use serialized string to restore this object",
    "Use first 128 chars to avoid exceed the message buffer.",
    "get number of classes",
    "get index of label",
    "get max_feature_idx first",
    "get average_output",
    "get feature names",
    "get monotone_constraints",
    "set zero",
    "predict all the trees for one iteration",
    "check early stopping",
    "set zero",
    "predict all the trees for one iteration",
    "check early stopping",
    "margin_threshold will be captured by value",
    "copy and sort",
    "margin_threshold will be captured by value",
    "load forced_splits file",
    "init tree learner",
    "push training metrics",
    "create buffer for gradients and hessians",
    "get max feature index",
    "get label index",
    "get feature names",
    "if need bagging, create buffer",
    "for a validation dataset, we need its score and metric",
    "update score",
    "objective function will calculate gradients and hessians",
    "random bagging, minimal unit is one record",
    "from right to left",
    "random bagging, minimal unit is one record",
    "reverse right buffer",
    "if need bagging",
    "set bagging data to tree learner",
    "get subset",
    "output used time per iteration",
    "boosting from average label; or customized \"average\" if implemented for the current objective",
    "boosting first",
    "bagging logic",
    "need to copy gradients for bagging subset.",
    "shrinkage by learning rate",
    "update score",
    "only add default score one-time",
    "updates scores",
    "add model",
    "reset score",
    "remove model",
    "print message for metric",
    "pop last early_stopping_round_ models",
    "update training score",
    "we need to predict out-of-bag scores of data for boosting",
    "update validation score",
    "print training metric",
    "print validation metric",
    "set zero",
    "predict all the trees for one iteration",
    "check early stopping",
    "push training metrics",
    "not same training data, need reset score and others",
    "create score tracker",
    "update score",
    "create buffer for gradients and hessians",
    "if need bagging, create buffer",
    "Get the max size of pool",
    "at least need 2 leaves",
    "push split information for all leaves",
    "get ordered bin",
    "check existing for ordered bin",
    "initialize splits for leaf",
    "initialize data partition",
    "initialize ordered gradients and hessians",
    "if has ordered bin, need to allocate a buffer to fast split",
    "get ordered bin",
    "initialize splits for leaf",
    "initialize data partition",
    "initialize ordered gradients and hessians",
    "if has ordered bin, need to allocate a buffer to fast split",
    "Get the max size of pool",
    "at least need 2 leaves",
    "push split information for all leaves",
    "some initial works before training",
    "root leaf",
    "only root leaf can be splitted on first time",
    "some initial works before finding best split",
    "find best threshold for every feature",
    "Get a leaf with max split gain",
    "Get split information for best leaf",
    "cannot split, quit",
    "split tree with best leaf",
    "reset histogram pool",
    "initialize data partition",
    "reset the splits for leaves",
    "Sumup for root",
    "use all data",
    "use bagging, only use part of data",
    "if has ordered bin, need to initialize the ordered bin",
    "use all data, pass nullptr",
    "bagging, only use part of data",
    "mark used data",
    "initialize ordered bin",
    "check depth of current leaf",
    "only need to check left leaf, since right leaf is in same level of left leaf",
    "no enough data to continue",
    "only have root",
    "put parent(left) leaf's histograms into larger leaf's histograms",
    "put parent(left) leaf's histograms to larger leaf's histograms",
    "split for the ordered bin",
    "mark data that at left-leaf",
    "split the ordered bin",
    "construct smaller leaf",
    "construct larger leaf",
    "find splits",
    "only has root leaf",
    "find best threshold for larger child",
    "start at root leaf",
    "before processing next node from queue, store info for current left/right leaf",
    "store \"best split\" for left and right, even if they might be overwritten by forced split",
    "then, compute own splits",
    "split info should exist because searching in bfs fashion - should have added from parent",
    "split tree, will return right leaf",
    "left = parent",
    "split tree, will return right leaf",
    "init the leaves that used on next iteration",
    "bag_mapper[index_mapper[i]]",
    "get feature partition",
    "get local used features",
    "get best split at smaller leaf",
    "find local best split for larger leaf",
    "sync global best info",
    "update best split",
    "instantiate template classes, otherwise linker cannot find the code",
    "initialize SerialTreeLearner",
    "Get local rank and global machine size",
    "allocate buffer for communication",
    "generate feature partition for current tree",
    "get local used feature",
    "get block start and block len for reduce scatter",
    "get buffer_write_start_pos_",
    "get buffer_read_start_pos_",
    "sync global data sumup info",
    "global sumup reduce",
    "copy back",
    "set global sumup info",
    "init global data count in leaf",
    "construct local histograms",
    "copy to buffer",
    "Reduce scatter for histogram",
    "restore global histograms from buffer",
    "find best threshold for smaller child",
    "only root leaf",
    "construct histgroms for large leaf, we init larger leaf as the parent, so we can just subtract the smaller leaf's histograms",
    "find best threshold for larger child",
    "find local best split for larger leaf",
    "sync global best info",
    "set best split",
    "need update global number of data in leaf",
    "instantiate template classes, otherwise linker cannot find the code",
    "initialize SerialTreeLearner",
    "some additional variables needed for GPU trainer",
    "Initialize GPU buffers and kernels",
    "some functions used for debugging the GPU histogram construction",
    "printf(\"grad %g != %g (%d ULPs)\\n\", h1[i].sum_gradients, h2[i].sum_gradients, ulps);",
    "goto err;",
    "printf(\"hessian %g != %g (%d ULPs)\\n\", h1[i].sum_hessians, h2[i].sum_hessians, ulps);",
    "goto err;",
    "we roughly want 256 workgroups per device, and we have num_dense_feature4_ feature tuples.",
    "also guarantee that there are at least 2K examples per workgroup",
    "return 0;",
    "we have already copied ordered gradients, ordered hessians and indices to GPU",
    "decide the best number of workgroups working on one feature4 tuple",
    "set work group size based on feature size",
    "each 2^exp_workgroups_per_feature workgroups work on a feature4 tuple",
    "we need to refresh the kernel arguments after reallocating",
    "The only argument that needs to be changed later is num_data_",
    "the GPU kernel will process all features in one call, and each",
    "2^exp_workgroups_per_feature (compile time constant) workgroup will",
    "process one feature4 tuple",
    "for the root node, indices are not copied",
    "for constant hessian, hessians are not copied except for the root node",
    "there will be 2^exp_workgroups_per_feature = num_workgroups / num_dense_feature4 sub-histogram per feature4",
    "and we will launch num_feature workgroups for this kernel",
    "will launch threads for all features",
    "the queue should be asynchrounous, and we will can WaitAndGetHistograms() before we start processing dense feature groups",
    "copy the results asynchronously. Size depends on if double precision is used",
    "we will wait for this object in WaitAndGetHistograms",
    "when the output is ready, the computation is done",
    "values of this feature has been redistributed to multiple bins; need a reduction here",
    "how many feature-group tuples we have",
    "leave some safe margin for prefetching",
    "256 work-items per workgroup. Each work-item prefetches one tuple for that feature",
    "clear sparse/dense maps",
    "do nothing if no features can be processed on GPU",
    "allocate memory for all features (FIXME: 4 GB barrier on some devices, need to split to multiple buffers)",
    "unpin old buffer if necessary before destructing them",
    "make ordered_gradients and hessians larger (including extra room for prefetching), and pin them",
    "allocate space for gradients and hessians on device",
    "we will copy gradients and hessians in after ordered_gradients_ and ordered_hessians_ are constructed",
    "allocate feature mask, for disabling some feature-groups' histogram calculation",
    "copy indices to the device",
    "histogram bin entry size depends on the precision (single/double)",
    "create output buffer, each feature has a histogram with device_bin_size_ bins,",
    "each work group generates a sub-histogram of dword_features_ features.",
    "only initialize once here, as this will not need to change when ResetTrainingData() is called",
    "create atomic counters for inter-group coordination",
    "The output buffer is allocated to host directly, to overlap compute and data transfer",
    "find the dense feature-groups and group then into Feature4 data structure (several feature-groups packed into 4 bytes)",
    "looking for dword_features_ non-sparse feature-groups",
    "decide if we need to redistribute the bin",
    "multiplier must be a power of 2",
    "device_bin_mults_.push_back(1);",
    "found",
    "for data transfer time",
    "Now generate new data structure feature4, and copy data to the device",
    "preallocate arrays for all threads, and pin them",
    "building Feature4 bundles; each thread handles dword_features_ features",
    "one feature datapoint is 4 bits",
    "this guarantees that the RawGet() function is inlined, rather than using virtual function dispatching",
    "one feature datapoint is one byte",
    "this guarantees that the RawGet() function is inlined, rather than using virtual function dispatching",
    "Dense bin",
    "Dense 4-bit bin",
    "working on the remaining (less than dword_features_) feature groups",
    "fill the leftover features",
    "fill this empty feature with some \"random\" value",
    "fill this empty feature with some \"random\" value",
    "copying the last 1 to (dword_features - 1) feature-groups in the last tuple",
    "deallocate pinned space for feature copying",
    "data transfer time",
    "for other types of failure, build log might not be available; program.build_log() can crash",
    "Something bad happened. Just return \"No log available.\"",
    "build is okay, log may contain warnings",
    "destroy any old kernels",
    "create OpenCL kernels for different number of workgroups per feature",
    "currently we don't use constant memory",
    "compile the GPU kernel depending if double precision is used, constant hessian is used, etc.",
    "kernel with indices in an array",
    "kernel with all features enabled, with elimited branches",
    "kernel with all data indices (for root node, and assumes that root node always uses all features)",
    "do nothing if no features can be processed on GPU",
    "The only argument that needs to be changed later is num_data_",
    "hessian is passed as a parameter, but it is not available now.",
    "hessian will be set in BeforeTrain()",
    "Get the max bin size, used for selecting best GPU kernel",
    "initialize GPU",
    "determine which kernel to use based on the max number of bins",
    "setup GPU kernel arguments after we allocating all the buffers",
    "check if we need to recompile the GPU kernel (is_constant_hessian changed)",
    "this should rarely occur",
    "GPU memory has to been reallocated because data may have been changed",
    "setup GPU kernel arguments after we allocating all the buffers",
    "Copy initial full hessians and gradients to GPU.",
    "We start copying as early as possible, instead of at ConstructHistogram().",
    "setup hessian parameters only",
    "hessian is passed as a parameter",
    "use bagging",
    "On GPU, we start copying indices, gradients and hessians now, instead at ConstructHistogram()",
    "copy used gradients and hessians to ordered buffer",
    "transfer the indices to GPU",
    "transfer hessian to GPU",
    "setup hessian parameters only",
    "hessian is passed as a parameter",
    "transfer gradients to GPU",
    "only have root",
    "Copy indices, gradients and hessians as early as possible",
    "only need to initialize for smaller leaf",
    "Get leaf boundary",
    "copy indices to the GPU:",
    "copy ordered hessians to the GPU:",
    "copy ordered gradients to the GPU:",
    "do nothing if no features can be processed on GPU",
    "copy data indices if it is not null",
    "generate and copy ordered_gradients if gradients is not null",
    "generate and copy ordered_hessians if hessians is not null",
    "converted indices in is_feature_used to feature-group indices",
    "construct the feature masks for dense feature-groups",
    "if no feature group is used, just return and do not use GPU",
    "if not all feature groups are used, we need to transfer the feature mask to GPU",
    "otherwise, we will use a specialized GPU kernel with all feature groups enabled",
    "All data have been prepared, now run the GPU kernel",
    "construct smaller leaf",
    "ConstructGPUHistogramsAsync will return true if there are availabe feature gourps dispatched to GPU",
    "then construct sparse features on CPU",
    "We set data_indices to null to avoid rebuilding ordered gradients/hessians",
    "wait for GPU to finish, only if GPU is actually used",
    "use double precision",
    "use single precision",
    "Compare GPU histogram with CPU histogram, useful for debuggin GPU code problem",
    "#define GPU_DEBUG_COMPARE",
    "construct larger leaf",
    "then construct sparse features on CPU",
    "We set data_indices to null to avoid rebuilding ordered gradients/hessians",
    "wait for GPU to finish, only if GPU is actually used",
    "use double precision",
    "use single precision",
    "do some sanity check for the GPU algorithm",
    "limit top k",
    "get max bin",
    "calculate buffer size",
    "left and right on same time, so need double size",
    "initialize histograms for global",
    "sync global data sumup info",
    "set global sumup info",
    "init global data count in leaf",
    "get local sumup",
    "get local sumup",
    "get mean number on machines",
    "weighted gain",
    "get top k",
    "Copy histogram to buffer, and Get local aggregate features",
    "copy histograms.",
    "copy smaller leaf histograms first",
    "mark local aggregated feature",
    "copy",
    "then copy larger leaf histograms",
    "mark local aggregated feature",
    "copy",
    "use local data to find local best splits",
    "find splits",
    "only has root leaf",
    "find best threshold for larger child",
    "local voting",
    "gather",
    "get all top-k from all machines",
    "global voting",
    "copy local histgrams to buffer",
    "Reduce scatter for histogram",
    "find best split from local aggregated histograms",
    "restore from buffer",
    "find best threshold",
    "restore from buffer",
    "find best threshold",
    "find local best",
    "find local best split for larger leaf",
    "sync global best info",
    "copy back",
    "set the global number of data for leaves",
    "init the global sumup info",
    "instantiate template classes, otherwise linker cannot find the code"
  ],
  "v2.2.3": [
    "coding: utf-8",
    "pylint: disable=invalid-name, exec-used, C0111",
    "Apple Clang with OpenMP",
    "coding: utf-8",
    "pylint: disable = invalid-name, W0105",
    "create predictor first",
    "check dataset",
    "reduce cost for prediction training data",
    "process callbacks",
    "Most of legacy advanced options becomes callbacks",
    "construct booster",
    "start training",
    "check evaluation result.",
    "lambdarank task, split according to groups",
    "run preprocessing on the data set if needed",
    "setup callbacks",
    "coding: utf-8",
    "pylint: disable = C0103",
    "simplejson does not support Python 3.2, it throws a SyntaxError",
    "because of u'...' Unicode literals.",
    "DeprecationWarning is not shown by default, so let's create our own with higher level",
    "coding: utf-8",
    "pylint: disable = invalid-name, W0105, C0111, C0301",
    "minor change to support `**kwargs`",
    "user can set verbose with kwargs, it has higher priority",
    "register default metric for consistency with callable eval_metric case",
    "try to deduce from class instance",
    "overwrite default metric by explicitly set metric",
    "concatenate metric from params (or default if not provided in params) and eval_metric",
    "reduce cost for prediction training data",
    "free dataset",
    "Switch to using a multiclass objective in the underlying LGBM instance",
    "check group data",
    "coding: utf-8",
    "we don't need lib_lightgbm while building docs",
    "coding: utf-8",
    "pylint: disable = C0103",
    "coding: utf-8",
    "REMOVEME: remove warning after 2.3.0 version release",
    "coding: utf-8",
    "pylint: disable = invalid-name, C0111, C0301",
    "pylint: disable = R0912, R0913, R0914, W0105, W0201, W0212",
    "TypeError: obj is not a string or a number",
    "ValueError: invalid literal",
    "__get_num_preds() cannot work with nrow > MAX_INT32, so calculate overall number of predictions piecemeal",
    "avoid memory consumption by arrays concatenation operations",
    "__get_num_preds() cannot work with nrow > MAX_INT32, so calculate overall number of predictions piecemeal",
    "avoid memory consumption by arrays concatenation operations",
    "process for args",
    "user can set verbose with params, it has higher priority",
    "get categorical features",
    "process for reference dataset",
    "start construct data",
    "check data has header or not",
    "load init score",
    "need re group init score",
    "set feature names",
    "change non-float data to float data, need to copy",
    "change non-float data to float data, need to copy",
    "create valid",
    "construct subset",
    "create train",
    "set to None",
    "we're done if self and reference share a common upstrem reference",
    "group data from LightGBM is boundaries data, need to convert to group size",
    "user can set verbose with params, it has higher priority",
    "Training task",
    "set network if necessary",
    "construct booster object",
    "save reference to data",
    "buffer for inner predict",
    "Prediction task",
    "need reset training data",
    "need to push new valid data",
    "if buffer length is not long enough, re-allocate a buffer",
    "if buffer length is not long enough, reallocate a buffer",
    "Copy models",
    "Get name of features",
    "avoid to predict many time in one iteration",
    "Get num of inner evals",
    "Get name of evals",
    "coding: utf-8",
    "pylint: disable = invalid-name, W0105, C0301",
    "Callback environment used by callbacks",
    "coding: utf-8",
    "pylint: disable = invalid-name, C0111",
    "load or create your dataset",
    "create dataset for lightgbm",
    "if you want to re-use data, remember to set free_raw_data=False",
    "specify your configurations as a dict",
    "generate feature names",
    "feature_name and categorical_feature",
    "check feature name",
    "save model to file",
    "dump model to JSON (and save to file)",
    "feature names",
    "feature importances",
    "load model to predict",
    "can only predict with the best iteration (or the saving iteration)",
    "eval with loaded model",
    "dump model with pickle",
    "load model with pickle to predict",
    "can predict with any iteration when loaded in pickle way",
    "eval with loaded model",
    "continue training",
    "init_model accepts:",
    "1. model file name",
    "2. Booster()",
    "decay learning rates",
    "learning_rates accepts:",
    "1. list/tuple with length = num_boost_round",
    "2. function(curr_iter)",
    "change other parameters during training",
    "self-defined objective function",
    "f(preds: array, train_data: Dataset) -> grad: array, hess: array",
    "log likelihood loss",
    "self-defined eval metric",
    "f(preds: array, train_data: Dataset) -> name: string, eval_result: float, is_higher_better: bool",
    "binary error",
    "callback",
    "coding: utf-8",
    "pylint: disable = invalid-name, C0111",
    "load or create your dataset",
    "train",
    "predict",
    "eval",
    "feature importances",
    "self-defined eval metric",
    "f(y_true: array, y_pred: array) -> name: string, eval_result: float, is_higher_better: bool",
    "Root Mean Squared Logarithmic Error (RMSLE)",
    "train",
    "predict",
    "eval",
    "other scikit-learn modules",
    "coding: utf-8",
    "pylint: disable = invalid-name, C0111",
    "load or create your dataset",
    "create dataset for lightgbm",
    "specify your configurations as a dict",
    "train",
    "coding: utf-8",
    "pylint: disable = invalid-name, C0111",
    "################",
    "Simulate some binary data with a single categorical and",
    "single continuous predictor",
    "################",
    "Set up a couple of utilities for our experiments",
    "################",
    "Observe the behavior of `binary` and `xentropy` objectives",
    "Trying this throws an error on non-binary values of y:",
    "experiment('binary', label_type='probability', DATA)",
    "The speed of `binary` is not drastically different than",
    "`xentropy`. `xentropy` runs faster than `binary` in many cases, although",
    "there are reasons to suspect that `binary` should run faster when the",
    "label is an integer instead of a float",
    "coding: utf-8",
    "pylint: disable = invalid-name, C0111",
    "load or create your dataset",
    "create dataset for lightgbm",
    "specify your configurations as a dict",
    "train",
    "save model to file",
    "predict",
    "eval",
    "coding: utf-8",
    "!/usr/bin/env python3",
    "-*- coding: utf-8 -*-",
    "",
    "LightGBM documentation build configuration file, created by",
    "sphinx-quickstart on Thu May  4 14:30:58 2017.",
    "",
    "This file is execfile()d with the current directory set to its",
    "containing dir.",
    "",
    "Note that not all possible configuration values are present in this",
    "autogenerated file.",
    "",
    "All configuration values have a default; values that are commented out",
    "serve to show the default.",
    "If extensions (or modules to document with autodoc) are in another directory,",
    "add these directories to sys.path here. If the directory is relative to the",
    "documentation root, use os.path.abspath to make it absolute.",
    "-- mock out modules",
    "-- General configuration ------------------------------------------------",
    "If your documentation needs a minimal Sphinx version, state it here.",
    "Add any Sphinx extension module names here, as strings. They can be",
    "extensions coming with Sphinx (named 'sphinx.ext.*') or your custom",
    "ones.",
    "Add any paths that contain templates here, relative to this directory.",
    "The suffix(es) of source filenames.",
    "You can specify multiple suffix as a list of string:",
    "source_suffix = ['.rst', '.md']",
    "The master toctree document.",
    "General information about the project.",
    "The version info for the project you're documenting, acts as replacement for",
    "|version| and |release|, also used in various other places throughout the",
    "built documents.",
    "",
    "The short X.Y version.",
    "The full version, including alpha/beta/rc tags.",
    "The language for content autogenerated by Sphinx. Refer to documentation",
    "for a list of supported languages.",
    "",
    "This is also used if you do content translation via gettext catalogs.",
    "Usually you set \"language\" from the command line for these cases.",
    "List of patterns, relative to source directory, that match files and",
    "directories to ignore when looking for source files.",
    "This patterns also effect to html_static_path and html_extra_path",
    "The name of the Pygments (syntax highlighting) style to use.",
    "If true, `todo` and `todoList` produce output, else they produce nothing.",
    "Both the class' and the __init__ method's docstring are concatenated and inserted.",
    "-- Options for HTML output ----------------------------------------------",
    "The theme to use for HTML and HTML Help pages.  See the documentation for",
    "a list of builtin themes.",
    "Theme options are theme-specific and customize the look and feel of a theme",
    "further.  For a list of options available for each theme, see the",
    "documentation.",
    "Add any paths that contain custom static files (such as style sheets) here,",
    "relative to this directory. They are copied after the builtin static files,",
    "so a file named \"default.css\" will overwrite the builtin \"default.css\".",
    "-- Options for HTMLHelp output ------------------------------------------",
    "Output file base name for HTML help builder.",
    "coding: utf-8",
    "coding: utf-8",
    "pylint: skip-file",
    "we don't need lib_lightgbm while building docs",
    "coding: utf-8",
    "pylint: skip-file",
    "check saved model persistence",
    "we need to check the consistency of model file here, so test for exact equal",
    "check early stopping is working. Make it stop very early, so the scores should be very close to zero",
    "scores likely to be different, but prediction should still be the same",
    "coding: utf-8",
    "pylint: skip-file",
    "Test that the largest element is NOT the same, the smallest can be the same, i.e. zero",
    "sklearn <0.19 cannot accept instance, but many tests could be passed only with min_data=1 and min_data_in_bin=1",
    "we cannot use `check_estimator` directly since there is no skip test mechanism",
    "we cannot leave default params (see https://github.com/Microsoft/LightGBM/issues/833)",
    "Tests same probabilities",
    "Tests same predictions",
    "Tests same raw scores",
    "Tests same leaf indices",
    "Tests same feature contributions",
    "Tests other parameters for the prediction works",
    "no custom objective, no custom metric",
    "default metric",
    "non-default metric",
    "no metric",
    "non-default metric in eval_metric",
    "non-default metric with non-default metric in eval_metric",
    "non-default metric with multiple metrics in eval_metric",
    "default metric for non-default objective",
    "non-default metric for non-default objective",
    "no metric",
    "non-default metric in eval_metric for non-default objective",
    "non-default metric with non-default metric in eval_metric for non-default objective",
    "non-default metric with multiple metrics in eval_metric for non-default objective",
    "custom objective, no custom metric",
    "default regression metric for custom objective",
    "non-default regression metric for custom objective",
    "multiple regression metrics for custom objective",
    "no metric",
    "default regression metric with non-default metric in eval_metric for custom objective",
    "non-default regression metric with metric in eval_metric for custom objective",
    "multiple regression metrics with metric in eval_metric for custom objective",
    "multiple regression metrics with multiple metrics in eval_metric for custom objective",
    "no custom objective, custom metric",
    "default metric with custom metric",
    "non-default metric with custom metric",
    "multiple metrics with custom metric",
    "custom metric (disable default metric)",
    "default metric for non-default objective with custom metric",
    "non-default metric for non-default objective with custom metric",
    "multiple metrics for non-default objective with custom metric",
    "custom metric (disable default metric for non-default objective)",
    "custom objective, custom metric",
    "custom metric for custom objective",
    "non-default regression metric with custom metric for custom objective",
    "multiple regression metrics with custom metric for custom objective",
    "default metric and invalid binary metric is replaced with multiclass alternative",
    "invalid objective is replaced with default multiclass one",
    "and invalid binary metric is replaced with multiclass alternative",
    "default metric for non-default multiclass objective",
    "and invalid binary metric is replaced with multiclass alternative",
    "default metric and invalid multiclass metric is replaced with binary alternative",
    "invalid multiclass metric is replaced with binary alternative for custom objective",
    "coding: utf-8",
    "pylint: skip-file",
    "coding: utf-8",
    "pylint: skip-file",
    "no early stopping",
    "early stopping occurs",
    "test custom eval metrics",
    "shuffle = False, override metric in params",
    "shuffle = True, callbacks",
    "self defined folds",
    "lambdarank",
    "... with l2 metric",
    "... with NDCG (default) metric",
    "self defined folds with lambdarank",
    "test feature_names with whitespaces",
    "take subsets and train",
    "test sliced labels",
    "append some columns",
    "append some rows",
    "test sliced 2d matrix",
    "test sliced CSR",
    "no fobj, no feval",
    "default metric",
    "non-default metric in params",
    "default metric in args",
    "non-default metric in args",
    "metric in args overwrites one in params",
    "multiple metrics in params",
    "multiple metrics in args",
    "remove default metric by 'None' in list",
    "remove default metric by 'None' aliases",
    "fobj, no feval",
    "no default metric",
    "metric in params",
    "metric in args",
    "metric in args overwrites its' alias in params",
    "multiple metrics in params",
    "multiple metrics in args",
    "no fobj, feval",
    "default metric with custom one",
    "non-default metric in params with custom one",
    "default metric in args with custom one",
    "non-default metric in args with custom one",
    "metric in args overwrites one in params, custom one is evaluated too",
    "multiple metrics in params with custom one",
    "multiple metrics in args with custom one",
    "custom metric is evaluated despite 'None' is passed",
    "fobj, feval",
    "no default metric, only custom one",
    "metric in params with custom one",
    "metric in args with custom one",
    "metric in args overwrites one in params, custom one is evaluated too",
    "multiple metrics in params with custom one",
    "multiple metrics in args with custom one",
    "custom metric is evaluated despite 'None' is passed",
    "no fobj, no feval",
    "default metric",
    "default metric in params",
    "non-default metric in params",
    "multiple metrics in params",
    "remove default metric by 'None' aliases",
    "fobj, no feval",
    "no default metric",
    "metric in params",
    "multiple metrics in params",
    "no fobj, feval",
    "default metric with custom one",
    "default metric in params with custom one",
    "non-default metric in params with custom one",
    "multiple metrics in params with custom one",
    "custom metric is evaluated despite 'None' is passed",
    "fobj, feval",
    "no default metric, only custom one",
    "metric in params with custom one",
    "multiple metrics in params with custom one",
    "custom metric is evaluated despite 'None' is passed",
    "multiclass default metric",
    "multiclass default metric with custom one",
    "multiclass metric alias with custom one for custom objective",
    "no metric for invalid class_num",
    "custom metric for invalid class_num",
    "multiclass metric alias with custom one with invalid class_num",
    "multiclass default metric without num_class",
    "multiclass metric alias",
    "multiclass metric",
    "non-valid metric for multiclass objective",
    "non-default num_class for default objective",
    "no metric with non-default num_class for custom objective",
    "multiclass metric alias for custom objective",
    "multiclass metric for custom objective",
    "binary metric with non-default num_class for custom objective",
    "coding: utf-8",
    "pylint: skip-file",
    "Register Dynamic Symbols",
    "coding: utf-8",
    "alias table",
    "names",
    "from strings",
    "tails",
    "tails",
    "coding: utf-8",
    "convert from one-based to  zero-based index",
    "convert from boundaries to size",
    "--- start Booster interfaces",
    "create boosting",
    "initialize the boosting",
    "create objective function",
    "initialize the objective function",
    "create training metric",
    "reset the boosting",
    "create objective function",
    "initialize the objective function",
    "some help functions used to convert data",
    "Row iterator of on column for CSC matrix",
    "return value at idx, only can access by ascent order",
    "return next non-zero pair, if index < 0, means no more data",
    "start of c_api functions",
    "sample data first",
    "sample data first",
    "sample data first",
    "no more data",
    "---- start of booster",
    "---- start of some help functions",
    "set number of threads for openmp",
    "check for alias",
    "read parameters from config file",
    "remove str after \"#\"",
    "check for alias again",
    "load configs",
    "prediction is needed if using input initial model(continued train)",
    "need to continue training",
    "sync up random seed for data partition",
    "load Training data",
    "load data for parallel training",
    "load data for single machine",
    "need save binary file",
    "create training metric",
    "only when have metrics then need to construct validation data",
    "Add validation data, if it exists",
    "add",
    "need save binary file",
    "add metric for validation data",
    "output used time on each iteration",
    "need init network",
    "create boosting",
    "create objective function",
    "load training data",
    "initialize the objective function",
    "initialize the boosting",
    "add validation data into boosting",
    "convert model to if-else statement code",
    "create predictor",
    "Free memory",
    "create predictor",
    "label_gain = 2^i - 1, may overflow, so we use 31 here",
    "counts for all labels",
    "start from top label, and accumulate DCG",
    "counts for all labels",
    "calculate k Max DCG by one pass",
    "get sorted indices by score",
    "calculate dcg",
    "get sorted indices by score",
    "calculate multi dcg by one pass",
    "wait for all client start up",
    "default set to -1",
    "distance at k-th communication, distance[k] = 2^k",
    "set incoming rank at k-th commuication",
    "set outgoing rank at k-th commuication",
    "defalut set as -1",
    "construct all recursive halving map for all machines",
    "let 1 << k <= num_machines",
    "distance of each communication",
    "if num_machines = 2^k, don't need to group machines",
    "communication direction, %2 == 0 is positive",
    "neighbor at k-th communication",
    "receive data block at k-th communication",
    "send data block at k-th communication",
    "if num_machines != 2^k, need to group machines",
    "group, two machine in one group, total \"rest\" groups will have 2 machines.",
    "let left machine as group leader",
    "cache block information for groups, group with 2 machines will have double block size",
    "convert from group to node leader",
    "convert from node to group",
    "meet new group",
    "add block len for this group",
    "calculate the group block start",
    "not need to construct",
    "get receive block informations",
    "accumulate block len",
    "get send block informations",
    "accumulate block len",
    "static member definition",
    "if small package or small count , do it by all gather.(reduce the communication times.)",
    "assign the blocks to every rank.",
    "do reduce scatter",
    "do all gather",
    "assign blocks",
    "need use buffer here, since size of \"output\" is smaller than size after all gather",
    "copy back",
    "assign blocks",
    "start all gather",
    "when num_machines is small and data is large",
    "use output as receive buffer",
    "get current local block size",
    "get out rank",
    "get in rank",
    "get send information",
    "get recv information",
    "send and recv at same time",
    "rotate in-place",
    "use output as receive buffer",
    "get current local block size",
    "get send information",
    "get recv information",
    "send and recv at same time",
    "use output as receive buffer",
    "send and recv at same time",
    "send local data to neighbor first",
    "receive neighbor data first",
    "reduce",
    "get target",
    "get send information",
    "get recv information",
    "send and recv at same time",
    "reduce",
    "send result to neighbor",
    "receive result from neighbor",
    "copy result",
    "start up socket",
    "parse clients from file",
    "get ip list of local machine",
    "get local rank",
    "construct listener",
    "construct communication topo",
    "construct linkers",
    "free listener",
    "set timeout",
    "accept incoming socket",
    "receive rank",
    "add new socket",
    "save ranks that need to connect with",
    "start listener",
    "start connect",
    "let smaller rank connect to larger rank",
    "send local rank",
    "wait for listener",
    "print connected linkers",
    "Get some statistic from 2 line",
    "if only have one line on file",
    "Constructors",
    "Get type tag",
    "Comparisons",
    "This has to be separate, not in Statics, because Json() accesses statics().null.",
    "advance until next line, or end of input",
    "advance until closing tokens",
    "The usual case: non-escaped characters",
    "Handle escapes",
    "Extract 4-byte escape sequence",
    "Explicitly check length of the substring. The following loop",
    "relies on std::string returning the terminating NUL when",
    "accessing str[length]. Checking here reduces brittleness.",
    "JSON specifies that characters outside the BMP shall be encoded as a pair",
    "of 4-hex-digit \\u escapes encoding their surrogate pair components. Check",
    "whether we're in the middle of such a beast: the previous codepoint was an",
    "escaped lead (high) surrogate, and this is a trail (low) surrogate.",
    "Reassemble the two surrogate pairs into one astral-plane character, per",
    "the UTF-16 algorithm.",
    "Integer part",
    "Decimal part",
    "Exponent part",
    "Check for any trailing garbage",
    "Documented in json11.hpp",
    "Check for another object",
    "get column names",
    "load label idx first",
    "erase label column name",
    "load ignore columns",
    "load weight idx",
    "load group idx",
    "don't support query id in data file when training in parallel",
    "read data to memory",
    "sample data",
    "construct feature bin mappers",
    "initialize label",
    "extract features",
    "sample data from file",
    "construct feature bin mappers",
    "initialize label",
    "extract features",
    "load data from binary file",
    "check meta data",
    "need to check training data",
    "read data in memory",
    "initialize label",
    "extract features",
    "Get number of lines of data file",
    "initialize label",
    "extract features",
    "load data from binary file",
    "not need to check validation data",
    "check meta data",
    "buffer to read binary file",
    "check token",
    "read size of header",
    "re-allocmate space if not enough",
    "read header",
    "get header",
    "num_groups",
    "real_feature_idx_",
    "feature2group",
    "feature2subfeature",
    "group_bin_boundaries",
    "group_feature_start_",
    "group_feature_cnt_",
    "get feature names",
    "write feature names",
    "read size of meta data",
    "re-allocate space if not enough",
    "read meta data",
    "load meta data",
    "sample local used data if need to partition",
    "if not contain query file, minimal sample unit is one record",
    "if contain query file, minimal sample unit is one query",
    "if is new query",
    "read feature data",
    "read feature size",
    "re-allocate space if not enough",
    "fill feature_names_ if not header",
    "if only one machine, find bin locally",
    "if have multi-machines, need to find bin distributed",
    "different machines will find bin for different features",
    "start and len will store the process feature indices for different machines",
    "machine i will find bins for features in [ start[i], start[i] + len[i] )",
    "get size of bin mapper with max_bin size",
    "since sizes of different feature may not be same, we expand all bin mapper to type_size",
    "find local feature bins and copy to buffer",
    "free",
    "convert to binary size",
    "gather global feature bin mappers",
    "restore features bins from buffer",
    "---- private functions ----",
    "if features are ordered, not need to use hist_buf",
    "read all lines",
    "get query data",
    "if not contain query data, minimal sample unit is one record",
    "if contain query data, minimal sample unit is one query",
    "if is new query",
    "get query data",
    "if not contain query file, minimal sample unit is one record",
    "if contain query file, minimal sample unit is one query",
    "if is new query",
    "parse features",
    "-1 means doesn't use this feature",
    "check the range of label_idx, weight_idx and group_idx",
    "fill feature_names_ if not header",
    "start find bins",
    "if only one machine, find bin locally",
    "if have multi-machines, need to find bin distributed",
    "different machines will find bin for different features",
    "start and len will store the process feature indices for different machines",
    "machine i will find bins for features in [ start[i], start[i] + len[i] )",
    "get size of bin mapper with max_bin size",
    "since sizes of different feature may not be same, we expand all bin mapper to type_size",
    "find local feature bins and copy to buffer",
    "free",
    "convert to binary size",
    "gather global feature bin mappers",
    "restore features bins from buffer",
    "if doesn't need to prediction with initial model",
    "parser",
    "set label",
    "free processed line:",
    "shrink_to_fit will be very slow in linux, and seems not free memory, disable for now",
    "text_reader_->Lines()[i].shrink_to_fit();",
    "push data",
    "if is used feature",
    "if need to prediction with initial model",
    "parser",
    "set initial score",
    "set label",
    "free processed line:",
    "shrink_to_fit will be very slow in linux, and seems not free memory, disable for now",
    "text_reader_->Lines()[i].shrink_to_fit();",
    "push data",
    "if is used feature",
    "metadata_ will manage space of init_score",
    "text data can be free after loaded feature values",
    "parser",
    "set initial score",
    "set label",
    "push data",
    "if is used feature",
    "only need part of data",
    "need full data",
    "metadata_ will manage space of init_score",
    "read size of token",
    "deep copy function for BinMapper",
    "mean size for one bin",
    "need a new bin",
    "update bin upper bound",
    "last bin upper bound",
    "find distinct_values first",
    "push zero in the front",
    "use the large value",
    "push zero in the back",
    "convert to int type first",
    "sort by counts",
    "avoid first bin is zero",
    "will ignore the categorical of small counts",
    "need an additional bin for NaN",
    "use -1 to represent NaN",
    "Use MissingType::None to represent this bin contains all categoricals",
    "check trivial(num_bin_ == 1) feature",
    "check useless bin",
    "calculate sparse rate",
    "sparse threshold",
    "for lambdarank, it needs query data for partition data in parallel learning",
    "need convert query_id to boundaries",
    "check weights",
    "check query boundries",
    "contain initial score file",
    "check weights",
    "get local weights",
    "check query boundries",
    "get local query boundaries",
    "contain initial score file",
    "get local initial scores",
    "re-load query weight",
    "save to nullptr",
    "save to nullptr",
    "save to nullptr",
    "default weight file name",
    "default weight file name",
    "use first line to count number class",
    "default query file name",
    "/ This file is auto generated by LightGBM\\helpers\\parameter_generator.py from LightGBM\\include\\LightGBM\\config.h file.",
    "root is in the depth 0",
    "non-leaf",
    "leaf",
    "use this for the missing value conversion",
    "Predict func by Map to ifelse",
    "use this for the missing value conversion",
    "non-leaf",
    "left subtree",
    "right subtree",
    "leaf",
    "non-leaf",
    "left subtree",
    "right subtree",
    "leaf",
    "recursive computation of SHAP values for a decision tree",
    "extend the unique path",
    "leaf node",
    "internal node",
    "see if we have already split on this feature,",
    "if so we undo that split so we can redo it for this node",
    "clear old metrics",
    "to lower",
    "split",
    "remove duplicate",
    "add names of objective function if not providing metric",
    "generate seeds by seed.",
    "check for conflicts",
    "check if objective, metric, and num_class match",
    "Change pool size to -1 (no limit) when using data parallel to reduce communication costs",
    "Check max_depth and num_leaves",
    "filter is based on sampling data, so decrease its range",
    "put dense feature first",
    "sort by non zero cnt",
    "sort by non zero cnt, bigger first",
    "take apart small sparse group, due it will not gain on speed",
    "shuffle groups",
    "get num_features",
    "get bin_mappers",
    "copy feature bin mapper data",
    "copy feature bin mapper data",
    "if not pass a filename, just append \".bin\" of original file",
    "get size of header",
    "size of feature names",
    "write header",
    "write feature names",
    "get size of meta data",
    "write meta data",
    "write feature data",
    "get size of feature",
    "write feature",
    "feature is not used",
    "construct histograms for smaller leaf",
    "if not use ordered bin",
    "used ordered bin",
    "feature is not used",
    "construct histograms for smaller leaf",
    "if not use ordered bin",
    "used ordered bin",
    "fixed hessian.",
    "feature is not used",
    "construct histograms for smaller leaf",
    "if not use ordered bin",
    "used ordered bin",
    "feature is not used",
    "construct histograms for smaller leaf",
    "if not use ordered bin",
    "used ordered bin",
    "fixed hessian.",
    "PredictRaw",
    "PredictRawByMap",
    "Predict",
    "PredictByMap",
    "PredictLeafIndex",
    "PredictLeafIndexByMap",
    "output model type",
    "output number of class",
    "output label index",
    "output max_feature_idx",
    "output objective",
    "output tree models",
    "store the importance first",
    "sort the importance",
    "use serialized string to restore this object",
    "Use first 128 chars to avoid exceed the message buffer.",
    "get number of classes",
    "get index of label",
    "get max_feature_idx first",
    "get average_output",
    "get feature names",
    "set zero",
    "predict all the trees for one iteration",
    "check early stopping",
    "set zero",
    "predict all the trees for one iteration",
    "check early stopping",
    "margin_threshold will be captured by value",
    "copy and sort",
    "margin_threshold will be captured by value",
    "load forced_splits file",
    "init tree learner",
    "push training metrics",
    "create buffer for gradients and hessians",
    "get max feature index",
    "get label index",
    "get feature names",
    "if need bagging, create buffer",
    "for a validation dataset, we need its score and metric",
    "update score",
    "objective function will calculate gradients and hessians",
    "random bagging, minimal unit is one record",
    "if need bagging",
    "set bagging data to tree learner",
    "get subset",
    "output used time per iteration",
    "boosting from average label; or customized \"average\" if implemented for the current objective",
    "boosting first",
    "bagging logic",
    "need to copy gradients for bagging subset.",
    "shrinkage by learning rate",
    "update score",
    "only add default score one-time",
    "updates scores",
    "add model",
    "reset score",
    "remove model",
    "print message for metric",
    "pop last early_stopping_round_ models",
    "update training score",
    "we need to predict out-of-bag scores of data for boosting",
    "update validation score",
    "print training metric",
    "print validation metric",
    "set zero",
    "predict all the trees for one iteration",
    "check early stopping",
    "push training metrics",
    "not same training data, need reset score and others",
    "create score tracker",
    "update score",
    "create buffer for gradients and hessians",
    "if need bagging, create buffer",
    "Get the max size of pool",
    "at least need 2 leaves",
    "push split information for all leaves",
    "get ordered bin",
    "check existing for ordered bin",
    "initialize splits for leaf",
    "initialize data partition",
    "initialize ordered gradients and hessians",
    "if has ordered bin, need to allocate a buffer to fast split",
    "get ordered bin",
    "initialize splits for leaf",
    "initialize data partition",
    "initialize ordered gradients and hessians",
    "if has ordered bin, need to allocate a buffer to fast split",
    "Get the max size of pool",
    "at least need 2 leaves",
    "push split information for all leaves",
    "some initial works before training",
    "root leaf",
    "only root leaf can be splitted on first time",
    "some initial works before finding best split",
    "find best threshold for every feature",
    "Get a leaf with max split gain",
    "Get split information for best leaf",
    "cannot split, quit",
    "split tree with best leaf",
    "reset histogram pool",
    "at least use one feature",
    "initialize used features",
    "Get used feature at current tree",
    "initialize data partition",
    "reset the splits for leaves",
    "Sumup for root",
    "use all data",
    "use bagging, only use part of data",
    "if has ordered bin, need to initialize the ordered bin",
    "use all data, pass nullptr",
    "bagging, only use part of data",
    "mark used data",
    "initialize ordered bin",
    "check depth of current leaf",
    "only need to check left leaf, since right leaf is in same level of left leaf",
    "no enough data to continue",
    "only have root",
    "put parent(left) leaf's histograms into larger leaf's histograms",
    "put parent(left) leaf's histograms to larger leaf's histograms",
    "split for the ordered bin",
    "mark data that at left-leaf",
    "split the ordered bin",
    "construct smaller leaf",
    "construct larger leaf",
    "find splits",
    "only has root leaf",
    "find best threshold for larger child",
    "start at root leaf",
    "before processing next node from queue, store info for current left/right leaf",
    "store \"best split\" for left and right, even if they might be overwritten by forced split",
    "then, compute own splits",
    "split info should exist because searching in bfs fashion - should have added from parent",
    "split tree, will return right leaf",
    "left = parent",
    "split tree, will return right leaf",
    "init the leaves that used on next iteration",
    "bag_mapper[index_mapper[i]]",
    "bag_mapper[index_mapper[i]]",
    "get feature partition",
    "get local used features",
    "get best split at smaller leaf",
    "find local best split for larger leaf",
    "sync global best info",
    "update best split",
    "instantiate template classes, otherwise linker cannot find the code",
    "initialize SerialTreeLearner",
    "Get local rank and global machine size",
    "allocate buffer for communication",
    "generate feature partition for current tree",
    "get local used feature",
    "get block start and block len for reduce scatter",
    "get buffer_write_start_pos_",
    "get buffer_read_start_pos_",
    "sync global data sumup info",
    "global sumup reduce",
    "copy back",
    "set global sumup info",
    "init global data count in leaf",
    "construct local histograms",
    "copy to buffer",
    "Reduce scatter for histogram",
    "restore global histograms from buffer",
    "find best threshold for smaller child",
    "only root leaf",
    "construct histgroms for large leaf, we init larger leaf as the parent, so we can just subtract the smaller leaf's histograms",
    "find best threshold for larger child",
    "find local best split for larger leaf",
    "sync global best info",
    "set best split",
    "need update global number of data in leaf",
    "instantiate template classes, otherwise linker cannot find the code",
    "initialize SerialTreeLearner",
    "some additional variables needed for GPU trainer",
    "Initialize GPU buffers and kernels",
    "some functions used for debugging the GPU histogram construction",
    "printf(\"grad %g != %g (%d ULPs)\\n\", h1[i].sum_gradients, h2[i].sum_gradients, ulps);",
    "goto err;",
    "printf(\"hessian %g != %g (%d ULPs)\\n\", h1[i].sum_hessians, h2[i].sum_hessians, ulps);",
    "goto err;",
    "we roughly want 256 workgroups per device, and we have num_dense_feature4_ feature tuples.",
    "also guarantee that there are at least 2K examples per workgroup",
    "return 0;",
    "we have already copied ordered gradients, ordered hessians and indices to GPU",
    "decide the best number of workgroups working on one feature4 tuple",
    "set work group size based on feature size",
    "each 2^exp_workgroups_per_feature workgroups work on a feature4 tuple",
    "we need to refresh the kernel arguments after reallocating",
    "The only argument that needs to be changed later is num_data_",
    "the GPU kernel will process all features in one call, and each",
    "2^exp_workgroups_per_feature (compile time constant) workgroup will",
    "process one feature4 tuple",
    "for the root node, indices are not copied",
    "for constant hessian, hessians are not copied except for the root node",
    "there will be 2^exp_workgroups_per_feature = num_workgroups / num_dense_feature4 sub-histogram per feature4",
    "and we will launch num_feature workgroups for this kernel",
    "will launch threads for all features",
    "the queue should be asynchrounous, and we will can WaitAndGetHistograms() before we start processing dense feature groups",
    "copy the results asynchronously. Size depends on if double precision is used",
    "we will wait for this object in WaitAndGetHistograms",
    "when the output is ready, the computation is done",
    "values of this feature has been redistributed to multiple bins; need a reduction here",
    "how many feature-group tuples we have",
    "leave some safe margin for prefetching",
    "256 work-items per workgroup. Each work-item prefetches one tuple for that feature",
    "clear sparse/dense maps",
    "do nothing if no features can be processed on GPU",
    "allocate memory for all features (FIXME: 4 GB barrier on some devices, need to split to multiple buffers)",
    "unpin old buffer if necessary before destructing them",
    "make ordered_gradients and hessians larger (including extra room for prefetching), and pin them",
    "allocate space for gradients and hessians on device",
    "we will copy gradients and hessians in after ordered_gradients_ and ordered_hessians_ are constructed",
    "allocate feature mask, for disabling some feature-groups' histogram calculation",
    "copy indices to the device",
    "histogram bin entry size depends on the precision (single/double)",
    "create output buffer, each feature has a histogram with device_bin_size_ bins,",
    "each work group generates a sub-histogram of dword_features_ features.",
    "only initialize once here, as this will not need to change when ResetTrainingData() is called",
    "create atomic counters for inter-group coordination",
    "The output buffer is allocated to host directly, to overlap compute and data transfer",
    "find the dense feature-groups and group then into Feature4 data structure (several feature-groups packed into 4 bytes)",
    "looking for dword_features_ non-sparse feature-groups",
    "decide if we need to redistribute the bin",
    "multiplier must be a power of 2",
    "device_bin_mults_.push_back(1);",
    "found",
    "for data transfer time",
    "Now generate new data structure feature4, and copy data to the device",
    "preallocate arrays for all threads, and pin them",
    "building Feature4 bundles; each thread handles dword_features_ features",
    "one feature datapoint is 4 bits",
    "this guarantees that the RawGet() function is inlined, rather than using virtual function dispatching",
    "one feature datapoint is one byte",
    "this guarantees that the RawGet() function is inlined, rather than using virtual function dispatching",
    "Dense bin",
    "Dense 4-bit bin",
    "working on the remaining (less than dword_features_) feature groups",
    "fill the leftover features",
    "fill this empty feature with some \"random\" value",
    "fill this empty feature with some \"random\" value",
    "copying the last 1 to (dword_features - 1) feature-groups in the last tuple",
    "deallocate pinned space for feature copying",
    "data transfer time",
    "for other types of failure, build log might not be available; program.build_log() can crash",
    "Something bad happened. Just return \"No log available.\"",
    "build is okay, log may contain warnings",
    "destroy any old kernels",
    "create OpenCL kernels for different number of workgroups per feature",
    "currently we don't use constant memory",
    "compile the GPU kernel depending if double precision is used, constant hessian is used, etc.",
    "kernel with indices in an array",
    "kernel with all features enabled, with elimited branches",
    "kernel with all data indices (for root node, and assumes that root node always uses all features)",
    "do nothing if no features can be processed on GPU",
    "The only argument that needs to be changed later is num_data_",
    "hessian is passed as a parameter, but it is not available now.",
    "hessian will be set in BeforeTrain()",
    "Get the max bin size, used for selecting best GPU kernel",
    "initialize GPU",
    "determine which kernel to use based on the max number of bins",
    "setup GPU kernel arguments after we allocating all the buffers",
    "check if we need to recompile the GPU kernel (is_constant_hessian changed)",
    "this should rarely occur",
    "GPU memory has to been reallocated because data may have been changed",
    "setup GPU kernel arguments after we allocating all the buffers",
    "Copy initial full hessians and gradients to GPU.",
    "We start copying as early as possible, instead of at ConstructHistogram().",
    "setup hessian parameters only",
    "hessian is passed as a parameter",
    "use bagging",
    "On GPU, we start copying indices, gradients and hessians now, instead at ConstructHistogram()",
    "copy used gradients and hessians to ordered buffer",
    "transfer the indices to GPU",
    "transfer hessian to GPU",
    "setup hessian parameters only",
    "hessian is passed as a parameter",
    "transfer gradients to GPU",
    "only have root",
    "Copy indices, gradients and hessians as early as possible",
    "only need to initialize for smaller leaf",
    "Get leaf boundary",
    "copy indices to the GPU:",
    "copy ordered hessians to the GPU:",
    "copy ordered gradients to the GPU:",
    "do nothing if no features can be processed on GPU",
    "copy data indices if it is not null",
    "generate and copy ordered_gradients if gradients is not null",
    "generate and copy ordered_hessians if hessians is not null",
    "converted indices in is_feature_used to feature-group indices",
    "construct the feature masks for dense feature-groups",
    "if no feature group is used, just return and do not use GPU",
    "if not all feature groups are used, we need to transfer the feature mask to GPU",
    "otherwise, we will use a specialized GPU kernel with all feature groups enabled",
    "All data have been prepared, now run the GPU kernel",
    "construct smaller leaf",
    "ConstructGPUHistogramsAsync will return true if there are availabe feature gourps dispatched to GPU",
    "then construct sparse features on CPU",
    "We set data_indices to null to avoid rebuilding ordered gradients/hessians",
    "wait for GPU to finish, only if GPU is actually used",
    "use double precision",
    "use single precision",
    "Compare GPU histogram with CPU histogram, useful for debuggin GPU code problem",
    "#define GPU_DEBUG_COMPARE",
    "construct larger leaf",
    "then construct sparse features on CPU",
    "We set data_indices to null to avoid rebuilding ordered gradients/hessians",
    "wait for GPU to finish, only if GPU is actually used",
    "use double precision",
    "use single precision",
    "do some sanity check for the GPU algorithm",
    "limit top k",
    "get max bin",
    "calculate buffer size",
    "left and right on same time, so need double size",
    "initialize histograms for global",
    "sync global data sumup info",
    "set global sumup info",
    "init global data count in leaf",
    "get local sumup",
    "get local sumup",
    "get mean number on machines",
    "weighted gain",
    "get top k",
    "Copy histogram to buffer, and Get local aggregate features",
    "copy histograms.",
    "copy smaller leaf histograms first",
    "mark local aggregated feature",
    "copy",
    "then copy larger leaf histograms",
    "mark local aggregated feature",
    "copy",
    "use local data to find local best splits",
    "find splits",
    "only has root leaf",
    "find best threshold for larger child",
    "local voting",
    "gather",
    "get all top-k from all machines",
    "global voting",
    "copy local histgrams to buffer",
    "Reduce scatter for histogram",
    "find best split from local aggregated histograms",
    "restore from buffer",
    "find best threshold",
    "restore from buffer",
    "find best threshold",
    "find local best",
    "find local best split for larger leaf",
    "sync global best info",
    "copy back",
    "set the global number of data for leaves",
    "init the global sumup info",
    "instantiate template classes, otherwise linker cannot find the code"
  ],
  "v2.2.2": [
    "coding: utf-8",
    "pylint: disable=invalid-name, exec-used, C0111",
    "coding: utf-8",
    "pylint: disable = invalid-name, W0105",
    "create predictor first",
    "check dataset",
    "reduce cost for prediction training data",
    "process callbacks",
    "Most of legacy advanced options becomes callbacks",
    "construct booster",
    "start training",
    "check evaluation result.",
    "lambdarank task, split according to groups",
    "run preprocessing on the data set if needed",
    "setup callbacks",
    "coding: utf-8",
    "pylint: disable = C0103",
    "simplejson does not support Python 3.2, it throws a SyntaxError",
    "because of u'...' Unicode literals.",
    "DeprecationWarning is not shown by default, so let's create our own with higher level",
    "coding: utf-8",
    "pylint: disable = invalid-name, W0105, C0111, C0301",
    "minor change to support `**kwargs`",
    "user can set verbose with kwargs, it has higher priority",
    "register default metric for consistency with callable eval_metric case",
    "try to deduce from class instance",
    "overwrite default metric by explicitly set metric",
    "concatenate metric from params (or default if not provided in params) and eval_metric",
    "reduce cost for prediction training data",
    "free dataset",
    "Switch to using a multiclass objective in the underlying LGBM instance",
    "check group data",
    "coding: utf-8",
    "we don't need lib_lightgbm while building docs",
    "coding: utf-8",
    "pylint: disable = C0103",
    "coding: utf-8",
    "REMOVEME: remove warning after 2.3.0 version release",
    "coding: utf-8",
    "pylint: disable = invalid-name, C0111, C0301",
    "pylint: disable = R0912, R0913, R0914, W0105, W0201, W0212",
    "TypeError: obj is not a string or a number",
    "ValueError: invalid literal",
    "__get_num_preds() cannot work with nrow > MAX_INT32, so calculate overall number of predictions piecemeal",
    "avoid memory consumption by arrays concatenation operations",
    "__get_num_preds() cannot work with nrow > MAX_INT32, so calculate overall number of predictions piecemeal",
    "avoid memory consumption by arrays concatenation operations",
    "process for args",
    "user can set verbose with params, it has higher priority",
    "get categorical features",
    "process for reference dataset",
    "start construct data",
    "check data has header or not",
    "load init score",
    "need re group init score",
    "set feature names",
    "change non-float data to float data, need to copy",
    "change non-float data to float data, need to copy",
    "create valid",
    "construct subset",
    "create train",
    "set to None",
    "we're done if self and reference share a common upstrem reference",
    "group data from LightGBM is boundaries data, need to convert to group size",
    "user can set verbose with params, it has higher priority",
    "Training task",
    "set network if necessary",
    "construct booster object",
    "save reference to data",
    "buffer for inner predict",
    "Prediction task",
    "need reset training data",
    "need to push new valid data",
    "if buffer length is not long enough, re-allocate a buffer",
    "if buffer length is not long enough, reallocate a buffer",
    "Copy models",
    "Get name of features",
    "avoid to predict many time in one iteration",
    "Get num of inner evals",
    "Get name of evals",
    "coding: utf-8",
    "pylint: disable = invalid-name, W0105, C0301",
    "Callback environment used by callbacks",
    "coding: utf-8",
    "pylint: disable = invalid-name, C0111",
    "load or create your dataset",
    "create dataset for lightgbm",
    "if you want to re-use data, remember to set free_raw_data=False",
    "specify your configurations as a dict",
    "generate feature names",
    "feature_name and categorical_feature",
    "check feature name",
    "save model to file",
    "dump model to JSON (and save to file)",
    "feature names",
    "feature importances",
    "load model to predict",
    "can only predict with the best iteration (or the saving iteration)",
    "eval with loaded model",
    "dump model with pickle",
    "load model with pickle to predict",
    "can predict with any iteration when loaded in pickle way",
    "eval with loaded model",
    "continue training",
    "init_model accepts:",
    "1. model file name",
    "2. Booster()",
    "decay learning rates",
    "learning_rates accepts:",
    "1. list/tuple with length = num_boost_round",
    "2. function(curr_iter)",
    "change other parameters during training",
    "self-defined objective function",
    "f(preds: array, train_data: Dataset) -> grad: array, hess: array",
    "log likelihood loss",
    "self-defined eval metric",
    "f(preds: array, train_data: Dataset) -> name: string, eval_result: float, is_higher_better: bool",
    "binary error",
    "callback",
    "coding: utf-8",
    "pylint: disable = invalid-name, C0111",
    "load or create your dataset",
    "train",
    "predict",
    "eval",
    "feature importances",
    "self-defined eval metric",
    "f(y_true: array, y_pred: array) -> name: string, eval_result: float, is_higher_better: bool",
    "Root Mean Squared Logarithmic Error (RMSLE)",
    "train",
    "predict",
    "eval",
    "other scikit-learn modules",
    "coding: utf-8",
    "pylint: disable = invalid-name, C0111",
    "load or create your dataset",
    "create dataset for lightgbm",
    "specify your configurations as a dict",
    "train",
    "coding: utf-8",
    "pylint: disable = invalid-name, C0111",
    "################",
    "Simulate some binary data with a single categorical and",
    "single continuous predictor",
    "################",
    "Set up a couple of utilities for our experiments",
    "################",
    "Observe the behavior of `binary` and `xentropy` objectives",
    "Trying this throws an error on non-binary values of y:",
    "experiment('binary', label_type='probability', DATA)",
    "The speed of `binary` is not drastically different than",
    "`xentropy`. `xentropy` runs faster than `binary` in many cases, although",
    "there are reasons to suspect that `binary` should run faster when the",
    "label is an integer instead of a float",
    "coding: utf-8",
    "pylint: disable = invalid-name, C0111",
    "load or create your dataset",
    "create dataset for lightgbm",
    "specify your configurations as a dict",
    "train",
    "save model to file",
    "predict",
    "eval",
    "coding: utf-8",
    "!/usr/bin/env python3",
    "-*- coding: utf-8 -*-",
    "",
    "LightGBM documentation build configuration file, created by",
    "sphinx-quickstart on Thu May  4 14:30:58 2017.",
    "",
    "This file is execfile()d with the current directory set to its",
    "containing dir.",
    "",
    "Note that not all possible configuration values are present in this",
    "autogenerated file.",
    "",
    "All configuration values have a default; values that are commented out",
    "serve to show the default.",
    "If extensions (or modules to document with autodoc) are in another directory,",
    "add these directories to sys.path here. If the directory is relative to the",
    "documentation root, use os.path.abspath to make it absolute.",
    "-- mock out modules",
    "-- General configuration ------------------------------------------------",
    "If your documentation needs a minimal Sphinx version, state it here.",
    "Add any Sphinx extension module names here, as strings. They can be",
    "extensions coming with Sphinx (named 'sphinx.ext.*') or your custom",
    "ones.",
    "Add any paths that contain templates here, relative to this directory.",
    "The suffix(es) of source filenames.",
    "You can specify multiple suffix as a list of string:",
    "source_suffix = ['.rst', '.md']",
    "The master toctree document.",
    "General information about the project.",
    "The version info for the project you're documenting, acts as replacement for",
    "|version| and |release|, also used in various other places throughout the",
    "built documents.",
    "",
    "The short X.Y version.",
    "The full version, including alpha/beta/rc tags.",
    "The language for content autogenerated by Sphinx. Refer to documentation",
    "for a list of supported languages.",
    "",
    "This is also used if you do content translation via gettext catalogs.",
    "Usually you set \"language\" from the command line for these cases.",
    "List of patterns, relative to source directory, that match files and",
    "directories to ignore when looking for source files.",
    "This patterns also effect to html_static_path and html_extra_path",
    "The name of the Pygments (syntax highlighting) style to use.",
    "If true, `todo` and `todoList` produce output, else they produce nothing.",
    "Both the class' and the __init__ method's docstring are concatenated and inserted.",
    "-- Options for HTML output ----------------------------------------------",
    "The theme to use for HTML and HTML Help pages.  See the documentation for",
    "a list of builtin themes.",
    "Theme options are theme-specific and customize the look and feel of a theme",
    "further.  For a list of options available for each theme, see the",
    "documentation.",
    "Add any paths that contain custom static files (such as style sheets) here,",
    "relative to this directory. They are copied after the builtin static files,",
    "so a file named \"default.css\" will overwrite the builtin \"default.css\".",
    "-- Options for HTMLHelp output ------------------------------------------",
    "Output file base name for HTML help builder.",
    "coding: utf-8",
    "coding: utf-8",
    "pylint: skip-file",
    "we don't need lib_lightgbm while building docs",
    "coding: utf-8",
    "pylint: skip-file",
    "check saved model persistence",
    "we need to check the consistency of model file here, so test for exact equal",
    "check early stopping is working. Make it stop very early, so the scores should be very close to zero",
    "scores likely to be different, but prediction should still be the same",
    "coding: utf-8",
    "pylint: skip-file",
    "Test that the largest element is NOT the same, the smallest can be the same, i.e. zero",
    "sklearn <0.19 cannot accept instance, but many tests could be passed only with min_data=1 and min_data_in_bin=1",
    "we cannot use `check_estimator` directly since there is no skip test mechanism",
    "we cannot leave default params (see https://github.com/Microsoft/LightGBM/issues/833)",
    "Tests same probabilities",
    "Tests same predictions",
    "Tests same raw scores",
    "Tests same leaf indices",
    "Tests same feature contributions",
    "Tests other parameters for the prediction works",
    "coding: utf-8",
    "pylint: skip-file",
    "coding: utf-8",
    "pylint: skip-file",
    "no early stopping",
    "early stopping occurs",
    "test custom eval metrics",
    "shuffle = False, override metric in params",
    "shuffle = True, callbacks",
    "self defined folds",
    "lambdarank",
    "... with l2 metric",
    "... with NDCG (default) metric",
    "self defined folds with lambdarank",
    "test feature_names with whitespaces",
    "take subsets and train",
    "test sliced labels",
    "append some columns",
    "append some rows",
    "test sliced 2d matrix",
    "test sliced CSR",
    "coding: utf-8",
    "pylint: skip-file",
    "Register Dynamic Symbols",
    "coding: utf-8",
    "alias table",
    "names",
    "from strings",
    "tails",
    "tails",
    "coding: utf-8",
    "convert from one-based to  zero-based index",
    "convert from boundaries to size",
    "--- start Booster interfaces",
    "create boosting",
    "initialize the boosting",
    "create objective function",
    "initialize the objective function",
    "create training metric",
    "reset the boosting",
    "create objective function",
    "initialize the objective function",
    "some help functions used to convert data",
    "Row iterator of on column for CSC matrix",
    "return value at idx, only can access by ascent order",
    "return next non-zero pair, if index < 0, means no more data",
    "start of c_api functions",
    "sample data first",
    "sample data first",
    "sample data first",
    "no more data",
    "---- start of booster",
    "---- start of some help functions",
    "set number of threads for openmp",
    "check for alias",
    "read parameters from config file",
    "remove str after \"#\"",
    "check for alias again",
    "load configs",
    "prediction is needed if using input initial model(continued train)",
    "need to continue training",
    "sync up random seed for data partition",
    "load Training data",
    "load data for parallel training",
    "load data for single machine",
    "need save binary file",
    "create training metric",
    "only when have metrics then need to construct validation data",
    "Add validation data, if it exists",
    "add",
    "need save binary file",
    "add metric for validation data",
    "output used time on each iteration",
    "need init network",
    "create boosting",
    "create objective function",
    "load training data",
    "initialize the objective function",
    "initialize the boosting",
    "add validation data into boosting",
    "convert model to if-else statement code",
    "create predictor",
    "Free memory",
    "create predictor",
    "label_gain = 2^i - 1, may overflow, so we use 31 here",
    "counts for all labels",
    "start from top label, and accumulate DCG",
    "counts for all labels",
    "calculate k Max DCG by one pass",
    "get sorted indices by score",
    "calculate dcg",
    "get sorted indices by score",
    "calculate multi dcg by one pass",
    "wait for all client start up",
    "default set to -1",
    "distance at k-th communication, distance[k] = 2^k",
    "set incoming rank at k-th commuication",
    "set outgoing rank at k-th commuication",
    "defalut set as -1",
    "construct all recursive halving map for all machines",
    "let 1 << k <= num_machines",
    "distance of each communication",
    "if num_machines = 2^k, don't need to group machines",
    "communication direction, %2 == 0 is positive",
    "neighbor at k-th communication",
    "receive data block at k-th communication",
    "send data block at k-th communication",
    "if num_machines != 2^k, need to group machines",
    "group, two machine in one group, total \"rest\" groups will have 2 machines.",
    "let left machine as group leader",
    "cache block information for groups, group with 2 machines will have double block size",
    "convert from group to node leader",
    "convert from node to group",
    "meet new group",
    "add block len for this group",
    "calculate the group block start",
    "not need to construct",
    "get receive block informations",
    "accumulate block len",
    "get send block informations",
    "accumulate block len",
    "static member definition",
    "if small package or small count , do it by all gather.(reduce the communication times.)",
    "assign the blocks to every rank.",
    "do reduce scatter",
    "do all gather",
    "assign blocks",
    "need use buffer here, since size of \"output\" is smaller than size after all gather",
    "copy back",
    "assign blocks",
    "start all gather",
    "when num_machines is small and data is large",
    "use output as receive buffer",
    "get current local block size",
    "get out rank",
    "get in rank",
    "get send information",
    "get recv information",
    "send and recv at same time",
    "rotate in-place",
    "use output as receive buffer",
    "get current local block size",
    "get send information",
    "get recv information",
    "send and recv at same time",
    "use output as receive buffer",
    "send and recv at same time",
    "send local data to neighbor first",
    "receive neighbor data first",
    "reduce",
    "get target",
    "get send information",
    "get recv information",
    "send and recv at same time",
    "reduce",
    "send result to neighbor",
    "receive result from neighbor",
    "copy result",
    "start up socket",
    "parse clients from file",
    "get ip list of local machine",
    "get local rank",
    "construct listener",
    "construct communication topo",
    "construct linkers",
    "free listener",
    "set timeout",
    "accept incoming socket",
    "receive rank",
    "add new socket",
    "save ranks that need to connect with",
    "start listener",
    "start connect",
    "let smaller rank connect to larger rank",
    "send local rank",
    "wait for listener",
    "print connected linkers",
    "Get some statistic from 2 line",
    "if only have one line on file",
    "Constructors",
    "Get type tag",
    "Comparisons",
    "This has to be separate, not in Statics, because Json() accesses statics().null.",
    "advance until next line, or end of input",
    "advance until closing tokens",
    "The usual case: non-escaped characters",
    "Handle escapes",
    "Extract 4-byte escape sequence",
    "Explicitly check length of the substring. The following loop",
    "relies on std::string returning the terminating NUL when",
    "accessing str[length]. Checking here reduces brittleness.",
    "JSON specifies that characters outside the BMP shall be encoded as a pair",
    "of 4-hex-digit \\u escapes encoding their surrogate pair components. Check",
    "whether we're in the middle of such a beast: the previous codepoint was an",
    "escaped lead (high) surrogate, and this is a trail (low) surrogate.",
    "Reassemble the two surrogate pairs into one astral-plane character, per",
    "the UTF-16 algorithm.",
    "Integer part",
    "Decimal part",
    "Exponent part",
    "Check for any trailing garbage",
    "Documented in json11.hpp",
    "Check for another object",
    "get column names",
    "load label idx first",
    "erase label column name",
    "load ignore columns",
    "load weight idx",
    "load group idx",
    "don't support query id in data file when training in parallel",
    "read data to memory",
    "sample data",
    "construct feature bin mappers",
    "initialize label",
    "extract features",
    "sample data from file",
    "construct feature bin mappers",
    "initialize label",
    "extract features",
    "load data from binary file",
    "check meta data",
    "need to check training data",
    "read data in memory",
    "initialize label",
    "extract features",
    "Get number of lines of data file",
    "initialize label",
    "extract features",
    "load data from binary file",
    "not need to check validation data",
    "check meta data",
    "buffer to read binary file",
    "check token",
    "read size of header",
    "re-allocmate space if not enough",
    "read header",
    "get header",
    "num_groups",
    "real_feature_idx_",
    "feature2group",
    "feature2subfeature",
    "group_bin_boundaries",
    "group_feature_start_",
    "group_feature_cnt_",
    "get feature names",
    "write feature names",
    "read size of meta data",
    "re-allocate space if not enough",
    "read meta data",
    "load meta data",
    "sample local used data if need to partition",
    "if not contain query file, minimal sample unit is one record",
    "if contain query file, minimal sample unit is one query",
    "if is new query",
    "read feature data",
    "read feature size",
    "re-allocate space if not enough",
    "fill feature_names_ if not header",
    "if only one machine, find bin locally",
    "if have multi-machines, need to find bin distributed",
    "different machines will find bin for different features",
    "start and len will store the process feature indices for different machines",
    "machine i will find bins for features in [ start[i], start[i] + len[i] )",
    "get size of bin mapper with max_bin size",
    "since sizes of different feature may not be same, we expand all bin mapper to type_size",
    "find local feature bins and copy to buffer",
    "free",
    "convert to binary size",
    "gather global feature bin mappers",
    "restore features bins from buffer",
    "---- private functions ----",
    "if features are ordered, not need to use hist_buf",
    "read all lines",
    "get query data",
    "if not contain query data, minimal sample unit is one record",
    "if contain query data, minimal sample unit is one query",
    "if is new query",
    "get query data",
    "if not contain query file, minimal sample unit is one record",
    "if contain query file, minimal sample unit is one query",
    "if is new query",
    "parse features",
    "-1 means doesn't use this feature",
    "check the range of label_idx, weight_idx and group_idx",
    "fill feature_names_ if not header",
    "start find bins",
    "if only one machine, find bin locally",
    "if have multi-machines, need to find bin distributed",
    "different machines will find bin for different features",
    "start and len will store the process feature indices for different machines",
    "machine i will find bins for features in [ start[i], start[i] + len[i] )",
    "get size of bin mapper with max_bin size",
    "since sizes of different feature may not be same, we expand all bin mapper to type_size",
    "find local feature bins and copy to buffer",
    "free",
    "convert to binary size",
    "gather global feature bin mappers",
    "restore features bins from buffer",
    "if doesn't need to prediction with initial model",
    "parser",
    "set label",
    "free processed line:",
    "shrink_to_fit will be very slow in linux, and seems not free memory, disable for now",
    "text_reader_->Lines()[i].shrink_to_fit();",
    "push data",
    "if is used feature",
    "if need to prediction with initial model",
    "parser",
    "set initial score",
    "set label",
    "free processed line:",
    "shrink_to_fit will be very slow in linux, and seems not free memory, disable for now",
    "text_reader_->Lines()[i].shrink_to_fit();",
    "push data",
    "if is used feature",
    "metadata_ will manage space of init_score",
    "text data can be free after loaded feature values",
    "parser",
    "set initial score",
    "set label",
    "push data",
    "if is used feature",
    "only need part of data",
    "need full data",
    "metadata_ will manage space of init_score",
    "read size of token",
    "deep copy function for BinMapper",
    "mean size for one bin",
    "need a new bin",
    "update bin upper bound",
    "last bin upper bound",
    "find distinct_values first",
    "push zero in the front",
    "use the large value",
    "push zero in the back",
    "convert to int type first",
    "sort by counts",
    "avoid first bin is zero",
    "will ignore the categorical of small counts",
    "need an additional bin for NaN",
    "use -1 to represent NaN",
    "Use MissingType::None to represent this bin contains all categoricals",
    "check trival(num_bin_ == 1) feature",
    "check useless bin",
    "calculate sparse rate",
    "sparse threshold",
    "for lambdarank, it needs query data for partition data in parallel learning",
    "need convert query_id to boundaries",
    "check weights",
    "check query boundries",
    "contain initial score file",
    "check weights",
    "get local weights",
    "check query boundries",
    "get local query boundaries",
    "contain initial score file",
    "get local initial scores",
    "re-load query weight",
    "save to nullptr",
    "save to nullptr",
    "save to nullptr",
    "default weight file name",
    "default weight file name",
    "use first line to count number class",
    "default query file name",
    "/ This file is auto generated by LightGBM\\helpers\\parameter_generator.py from LightGBM\\include\\LightGBM\\config.h file.",
    "root is in the depth 0",
    "non-leaf",
    "leaf",
    "use this for the missing value conversion",
    "Predict func by Map to ifelse",
    "use this for the missing value conversion",
    "non-leaf",
    "left subtree",
    "right subtree",
    "leaf",
    "non-leaf",
    "left subtree",
    "right subtree",
    "leaf",
    "recursive computation of SHAP values for a decision tree",
    "extend the unique path",
    "leaf node",
    "internal node",
    "see if we have already split on this feature,",
    "if so we undo that split so we can redo it for this node",
    "clear old metrics",
    "to lower",
    "split",
    "remove duplicate",
    "add names of objective function if not providing metric",
    "generate seeds by seed.",
    "check for conflicts",
    "check if objective, metric, and num_class match",
    "Change pool size to -1 (no limit) when using data parallel to reduce communication costs",
    "Check max_depth and num_leaves",
    "filter is based on sampling data, so decrease its range",
    "put dense feature first",
    "sort by non zero cnt",
    "sort by non zero cnt, bigger first",
    "take apart small sparse group, due it will not gain on speed",
    "shuffle groups",
    "get num_features",
    "get bin_mappers",
    "copy feature bin mapper data",
    "copy feature bin mapper data",
    "if not pass a filename, just append \".bin\" of original file",
    "get size of header",
    "size of feature names",
    "write header",
    "write feature names",
    "get size of meta data",
    "write meta data",
    "write feature data",
    "get size of feature",
    "write feature",
    "feature is not used",
    "construct histograms for smaller leaf",
    "if not use ordered bin",
    "used ordered bin",
    "feature is not used",
    "construct histograms for smaller leaf",
    "if not use ordered bin",
    "used ordered bin",
    "fixed hessian.",
    "feature is not used",
    "construct histograms for smaller leaf",
    "if not use ordered bin",
    "used ordered bin",
    "feature is not used",
    "construct histograms for smaller leaf",
    "if not use ordered bin",
    "used ordered bin",
    "fixed hessian.",
    "PredictRaw",
    "PredictRawByMap",
    "Predict",
    "PredictByMap",
    "PredictLeafIndex",
    "PredictLeafIndexByMap",
    "output model type",
    "output number of class",
    "output label index",
    "output max_feature_idx",
    "output objective",
    "output tree models",
    "store the importance first",
    "sort the importance",
    "use serialized string to restore this object",
    "Use first 128 chars to avoid exceed the message buffer.",
    "get number of classes",
    "get index of label",
    "get max_feature_idx first",
    "get average_output",
    "get feature names",
    "set zero",
    "predict all the trees for one iteration",
    "check early stopping",
    "set zero",
    "predict all the trees for one iteration",
    "check early stopping",
    "margin_threshold will be captured by value",
    "copy and sort",
    "margin_threshold will be captured by value",
    "load forced_splits file",
    "init tree learner",
    "push training metrics",
    "create buffer for gradients and hessians",
    "get max feature index",
    "get label index",
    "get feature names",
    "if need bagging, create buffer",
    "for a validation dataset, we need its score and metric",
    "update score",
    "objective function will calculate gradients and hessians",
    "random bagging, minimal unit is one record",
    "if need bagging",
    "set bagging data to tree learner",
    "get subset",
    "output used time per iteration",
    "boosting from average label; or customized \"average\" if implemented for the current objective",
    "boosting first",
    "bagging logic",
    "need to copy gradients for bagging subset.",
    "shrinkage by learning rate",
    "update score",
    "only add default score one-time",
    "updates scores",
    "add model",
    "reset score",
    "remove model",
    "print message for metric",
    "pop last early_stopping_round_ models",
    "update training score",
    "we need to predict out-of-bag scores of data for boosting",
    "update validation score",
    "print training metric",
    "print validation metric",
    "set zero",
    "predict all the trees for one iteration",
    "check early stopping",
    "push training metrics",
    "not same training data, need reset score and others",
    "create score tracker",
    "update score",
    "create buffer for gradients and hessians",
    "if need bagging, create buffer",
    "Get the max size of pool",
    "at least need 2 leaves",
    "push split information for all leaves",
    "get ordered bin",
    "check existing for ordered bin",
    "initialize splits for leaf",
    "initialize data partition",
    "initialize ordered gradients and hessians",
    "if has ordered bin, need to allocate a buffer to fast split",
    "get ordered bin",
    "initialize splits for leaf",
    "initialize data partition",
    "initialize ordered gradients and hessians",
    "if has ordered bin, need to allocate a buffer to fast split",
    "Get the max size of pool",
    "at least need 2 leaves",
    "push split information for all leaves",
    "some initial works before training",
    "root leaf",
    "only root leaf can be splitted on first time",
    "some initial works before finding best split",
    "find best threshold for every feature",
    "Get a leaf with max split gain",
    "Get split information for best leaf",
    "cannot split, quit",
    "split tree with best leaf",
    "reset histogram pool",
    "at least use one feature",
    "initialize used features",
    "Get used feature at current tree",
    "initialize data partition",
    "reset the splits for leaves",
    "Sumup for root",
    "use all data",
    "use bagging, only use part of data",
    "if has ordered bin, need to initialize the ordered bin",
    "use all data, pass nullptr",
    "bagging, only use part of data",
    "mark used data",
    "initialize ordered bin",
    "check depth of current leaf",
    "only need to check left leaf, since right leaf is in same level of left leaf",
    "no enough data to continue",
    "only have root",
    "put parent(left) leaf's histograms into larger leaf's histograms",
    "put parent(left) leaf's histograms to larger leaf's histograms",
    "split for the ordered bin",
    "mark data that at left-leaf",
    "split the ordered bin",
    "construct smaller leaf",
    "construct larger leaf",
    "find splits",
    "only has root leaf",
    "find best threshold for larger child",
    "start at root leaf",
    "before processing next node from queue, store info for current left/right leaf",
    "store \"best split\" for left and right, even if they might be overwritten by forced split",
    "then, compute own splits",
    "split info should exist because searching in bfs fashion - should have added from parent",
    "split tree, will return right leaf",
    "left = parent",
    "split tree, will return right leaf",
    "init the leaves that used on next iteration",
    "bag_mapper[index_mapper[i]]",
    "get feature partition",
    "get local used features",
    "get best split at smaller leaf",
    "find local best split for larger leaf",
    "sync global best info",
    "update best split",
    "instantiate template classes, otherwise linker cannot find the code",
    "initialize SerialTreeLearner",
    "Get local rank and global machine size",
    "allocate buffer for communication",
    "generate feature partition for current tree",
    "get local used feature",
    "get block start and block len for reduce scatter",
    "get buffer_write_start_pos_",
    "get buffer_read_start_pos_",
    "sync global data sumup info",
    "global sumup reduce",
    "copy back",
    "set global sumup info",
    "init global data count in leaf",
    "construct local histograms",
    "copy to buffer",
    "Reduce scatter for histogram",
    "restore global histograms from buffer",
    "find best threshold for smaller child",
    "only root leaf",
    "construct histgroms for large leaf, we init larger leaf as the parent, so we can just subtract the smaller leaf's histograms",
    "find best threshold for larger child",
    "find local best split for larger leaf",
    "sync global best info",
    "set best split",
    "need update global number of data in leaf",
    "instantiate template classes, otherwise linker cannot find the code",
    "initialize SerialTreeLearner",
    "some additional variables needed for GPU trainer",
    "Initialize GPU buffers and kernels",
    "some functions used for debugging the GPU histogram construction",
    "printf(\"grad %g != %g (%d ULPs)\\n\", h1[i].sum_gradients, h2[i].sum_gradients, ulps);",
    "goto err;",
    "printf(\"hessian %g != %g (%d ULPs)\\n\", h1[i].sum_hessians, h2[i].sum_hessians, ulps);",
    "goto err;",
    "we roughly want 256 workgroups per device, and we have num_dense_feature4_ feature tuples.",
    "also guarantee that there are at least 2K examples per workgroup",
    "return 0;",
    "we have already copied ordered gradients, ordered hessians and indices to GPU",
    "decide the best number of workgroups working on one feature4 tuple",
    "set work group size based on feature size",
    "each 2^exp_workgroups_per_feature workgroups work on a feature4 tuple",
    "we need to refresh the kernel arguments after reallocating",
    "The only argument that needs to be changed later is num_data_",
    "the GPU kernel will process all features in one call, and each",
    "2^exp_workgroups_per_feature (compile time constant) workgroup will",
    "process one feature4 tuple",
    "for the root node, indices are not copied",
    "for constant hessian, hessians are not copied except for the root node",
    "there will be 2^exp_workgroups_per_feature = num_workgroups / num_dense_feature4 sub-histogram per feature4",
    "and we will launch num_feature workgroups for this kernel",
    "will launch threads for all features",
    "the queue should be asynchrounous, and we will can WaitAndGetHistograms() before we start processing dense feature groups",
    "copy the results asynchronously. Size depends on if double precision is used",
    "we will wait for this object in WaitAndGetHistograms",
    "when the output is ready, the computation is done",
    "values of this feature has been redistributed to multiple bins; need a reduction here",
    "how many feature-group tuples we have",
    "leave some safe margin for prefetching",
    "256 work-items per workgroup. Each work-item prefetches one tuple for that feature",
    "clear sparse/dense maps",
    "do nothing if no features can be processed on GPU",
    "allocate memory for all features (FIXME: 4 GB barrier on some devices, need to split to multiple buffers)",
    "unpin old buffer if necessary before destructing them",
    "make ordered_gradients and hessians larger (including extra room for prefetching), and pin them",
    "allocate space for gradients and hessians on device",
    "we will copy gradients and hessians in after ordered_gradients_ and ordered_hessians_ are constructed",
    "allocate feature mask, for disabling some feature-groups' histogram calculation",
    "copy indices to the device",
    "histogram bin entry size depends on the precision (single/double)",
    "create output buffer, each feature has a histogram with device_bin_size_ bins,",
    "each work group generates a sub-histogram of dword_features_ features.",
    "only initialize once here, as this will not need to change when ResetTrainingData() is called",
    "create atomic counters for inter-group coordination",
    "The output buffer is allocated to host directly, to overlap compute and data transfer",
    "find the dense feature-groups and group then into Feature4 data structure (several feature-groups packed into 4 bytes)",
    "looking for dword_features_ non-sparse feature-groups",
    "decide if we need to redistribute the bin",
    "multiplier must be a power of 2",
    "device_bin_mults_.push_back(1);",
    "found",
    "for data transfer time",
    "Now generate new data structure feature4, and copy data to the device",
    "preallocate arrays for all threads, and pin them",
    "building Feature4 bundles; each thread handles dword_features_ features",
    "one feature datapoint is 4 bits",
    "this guarantees that the RawGet() function is inlined, rather than using virtual function dispatching",
    "one feature datapoint is one byte",
    "this guarantees that the RawGet() function is inlined, rather than using virtual function dispatching",
    "Dense bin",
    "Dense 4-bit bin",
    "working on the remaining (less than dword_features_) feature groups",
    "fill the leftover features",
    "fill this empty feature with some \"random\" value",
    "fill this empty feature with some \"random\" value",
    "copying the last 1 to (dword_features - 1) feature-groups in the last tuple",
    "deallocate pinned space for feature copying",
    "data transfer time",
    "for other types of failure, build log might not be available; program.build_log() can crash",
    "Something bad happened. Just return \"No log available.\"",
    "build is okay, log may contain warnings",
    "destroy any old kernels",
    "create OpenCL kernels for different number of workgroups per feature",
    "currently we don't use constant memory",
    "compile the GPU kernel depending if double precision is used, constant hessian is used, etc",
    "kernel with indices in an array",
    "kernel with all features enabled, with elimited branches",
    "kernel with all data indices (for root node, and assumes that root node always uses all features)",
    "do nothing if no features can be processed on GPU",
    "The only argument that needs to be changed later is num_data_",
    "hessian is passed as a parameter, but it is not available now.",
    "hessian will be set in BeforeTrain()",
    "Get the max bin size, used for selecting best GPU kernel",
    "initialize GPU",
    "determine which kernel to use based on the max number of bins",
    "setup GPU kernel arguments after we allocating all the buffers",
    "check if we need to recompile the GPU kernel (is_constant_hessian changed)",
    "this should rarely occur",
    "GPU memory has to been reallocated because data may have been changed",
    "setup GPU kernel arguments after we allocating all the buffers",
    "Copy initial full hessians and gradients to GPU.",
    "We start copying as early as possible, instead of at ConstructHistogram().",
    "setup hessian parameters only",
    "hessian is passed as a parameter",
    "use bagging",
    "On GPU, we start copying indices, gradients and hessians now, instead at ConstructHistogram()",
    "copy used gradients and hessians to ordered buffer",
    "transfer the indices to GPU",
    "transfer hessian to GPU",
    "setup hessian parameters only",
    "hessian is passed as a parameter",
    "transfer gradients to GPU",
    "only have root",
    "Copy indices, gradients and hessians as early as possible",
    "only need to initialize for smaller leaf",
    "Get leaf boundary",
    "copy indices to the GPU:",
    "copy ordered hessians to the GPU:",
    "copy ordered gradients to the GPU:",
    "do nothing if no features can be processed on GPU",
    "copy data indices if it is not null",
    "generate and copy ordered_gradients if gradients is not null",
    "generate and copy ordered_hessians if hessians is not null",
    "converted indices in is_feature_used to feature-group indices",
    "construct the feature masks for dense feature-groups",
    "if no feature group is used, just return and do not use GPU",
    "if not all feature groups are used, we need to transfer the feature mask to GPU",
    "otherwise, we will use a specialized GPU kernel with all feature groups enabled",
    "All data have been prepared, now run the GPU kernel",
    "construct smaller leaf",
    "ConstructGPUHistogramsAsync will return true if there are availabe feature gourps dispatched to GPU",
    "then construct sparse features on CPU",
    "We set data_indices to null to avoid rebuilding ordered gradients/hessians",
    "wait for GPU to finish, only if GPU is actually used",
    "use double precision",
    "use single precision",
    "Compare GPU histogram with CPU histogram, useful for debuggin GPU code problem",
    "#define GPU_DEBUG_COMPARE",
    "construct larger leaf",
    "then construct sparse features on CPU",
    "We set data_indices to null to avoid rebuilding ordered gradients/hessians",
    "wait for GPU to finish, only if GPU is actually used",
    "use double precision",
    "use single precision",
    "do some sanity check for the GPU algorithm",
    "limit top k",
    "get max bin",
    "calculate buffer size",
    "left and right on same time, so need double size",
    "initialize histograms for global",
    "sync global data sumup info",
    "set global sumup info",
    "init global data count in leaf",
    "get local sumup",
    "get local sumup",
    "get mean number on machines",
    "weighted gain",
    "get top k",
    "Copy histogram to buffer, and Get local aggregate features",
    "copy histograms.",
    "copy smaller leaf histograms first",
    "mark local aggregated feature",
    "copy",
    "then copy larger leaf histograms",
    "mark local aggregated feature",
    "copy",
    "use local data to find local best splits",
    "find splits",
    "only has root leaf",
    "find best threshold for larger child",
    "local voting",
    "gather",
    "get all top-k from all machines",
    "global voting",
    "copy local histgrams to buffer",
    "Reduce scatter for histogram",
    "find best split from local aggregated histograms",
    "restore from buffer",
    "find best threshold",
    "restore from buffer",
    "find best threshold",
    "find local best",
    "find local best split for larger leaf",
    "sync global best info",
    "copy back",
    "set the global number of data for leaves",
    "init the global sumup info",
    "instantiate template classes, otherwise linker cannot find the code"
  ],
  "v2.2.1": [
    "coding: utf-8",
    "pylint: disable=invalid-name, exec-used, C0111",
    "coding: utf-8",
    "pylint: disable = invalid-name, W0105",
    "create predictor first",
    "check dataset",
    "reduce cost for prediction training data",
    "process callbacks",
    "Most of legacy advanced options becomes callbacks",
    "construct booster",
    "start training",
    "check evaluation result.",
    "lambdarank task, split according to groups",
    "run preprocessing on the data set if needed",
    "setup callbacks",
    "coding: utf-8",
    "pylint: disable = C0103",
    "simplejson does not support Python 3.2, it throws a SyntaxError",
    "because of u'...' Unicode literals.",
    "DeprecationWarning is not shown by default, so let's create our own with higher level",
    "coding: utf-8",
    "pylint: disable = invalid-name, W0105, C0111, C0301",
    "minor change to support `**kwargs`",
    "user can set verbose with kwargs, it has higher priority",
    "register default metric for consistency with callable eval_metric case",
    "try to deduce from class instance",
    "overwrite default metric by explicitly set metric",
    "concatenate metric from params (or default if not provided in params) and eval_metric",
    "reduce cost for prediction training data",
    "free dataset",
    "Switch to using a multiclass objective in the underlying LGBM instance",
    "check group data",
    "coding: utf-8",
    "we don't need lib_lightgbm while building docs",
    "coding: utf-8",
    "pylint: disable = C0103",
    "coding: utf-8",
    "REMOVEME: remove warning after 2.3.0 version release",
    "coding: utf-8",
    "pylint: disable = invalid-name, C0111, C0301",
    "pylint: disable = R0912, R0913, R0914, W0105, W0201, W0212",
    "TypeError: obj is not a string or a number",
    "ValueError: invalid literal",
    "__get_num_preds() cannot work with nrow > MAX_INT32, so calculate overall number of predictions piecemeal",
    "avoid memory consumption by arrays concatenation operations",
    "__get_num_preds() cannot work with nrow > MAX_INT32, so calculate overall number of predictions piecemeal",
    "avoid memory consumption by arrays concatenation operations",
    "process for args",
    "user can set verbose with params, it has higher priority",
    "get categorical features",
    "process for reference dataset",
    "start construct data",
    "check data has header or not",
    "load init score",
    "need re group init score",
    "set feature names",
    "change non-float data to float data, need to copy",
    "change non-float data to float data, need to copy",
    "create valid",
    "construct subset",
    "create train",
    "set to None",
    "we're done if self and reference share a common upstrem reference",
    "group data from LightGBM is boundaries data, need to convert to group size",
    "user can set verbose with params, it has higher priority",
    "Training task",
    "construct booster object",
    "save reference to data",
    "buffer for inner predict",
    "set network if necessary",
    "Prediction task",
    "need reset training data",
    "need to push new valid data",
    "Copy models",
    "Get name of features",
    "avoid to predict many time in one iteration",
    "Get num of inner evals",
    "Get name of evals",
    "coding: utf-8",
    "pylint: disable = invalid-name, W0105, C0301",
    "Callback environment used by callbacks",
    "coding: utf-8",
    "pylint: disable = invalid-name, C0111",
    "load or create your dataset",
    "create dataset for lightgbm",
    "if you want to re-use data, remember to set free_raw_data=False",
    "specify your configurations as a dict",
    "generate a feature name",
    "feature_name and categorical_feature",
    "check feature name",
    "save model to file",
    "dump model to JSON (and save to file)",
    "feature names",
    "feature importances",
    "load model to predict",
    "can only predict with the best iteration (or the saving iteration)",
    "eval with loaded model",
    "dump model with pickle",
    "load model with pickle to predict",
    "can predict with any iteration when loaded in pickle way",
    "eval with loaded model",
    "continue training",
    "init_model accepts:",
    "1. model file name",
    "2. Booster()",
    "decay learning rates",
    "learning_rates accepts:",
    "1. list/tuple with length = num_boost_round",
    "2. function(curr_iter)",
    "change other parameters during training",
    "self-defined objective function",
    "f(preds: array, train_data: Dataset) -> grad: array, hess: array",
    "log likelihood loss",
    "self-defined eval metric",
    "f(preds: array, train_data: Dataset) -> name: string, eval_result: float, is_higher_better: bool",
    "binary error",
    "callback",
    "coding: utf-8",
    "pylint: disable = invalid-name, C0111",
    "load or create your dataset",
    "train",
    "predict",
    "eval",
    "feature importances",
    "self-defined eval metric",
    "f(y_true: array, y_pred: array) -> name: string, eval_result: float, is_higher_better: bool",
    "Root Mean Squared Logarithmic Error (RMSLE)",
    "train",
    "predict",
    "eval",
    "other scikit-learn modules",
    "coding: utf-8",
    "pylint: disable = invalid-name, C0111",
    "load or create your dataset",
    "create dataset for lightgbm",
    "specify your configurations as a dict",
    "train",
    "coding: utf-8",
    "pylint: disable = invalid-name, C0111",
    "################",
    "Simulate some binary data with a single categorical and",
    "single continuous predictor",
    "################",
    "Set up a couple of utilities for our experiments",
    "################",
    "Observe the behavior of `binary` and `xentropy` objectives",
    "Trying this throws an error on non-binary values of y:",
    "experiment('binary', label_type='probability', DATA)",
    "The speed of `binary` is not drastically different than",
    "`xentropy`. `xentropy` runs faster than `binary` in many cases, although",
    "there are reasons to suspect that `binary` should run faster when the",
    "label is an integer instead of a float",
    "coding: utf-8",
    "pylint: disable = invalid-name, C0111",
    "load or create your dataset",
    "create dataset for lightgbm",
    "specify your configurations as a dict",
    "train",
    "save model to file",
    "predict",
    "eval",
    "!/usr/bin/env python3",
    "-*- coding: utf-8 -*-",
    "",
    "LightGBM documentation build configuration file, created by",
    "sphinx-quickstart on Thu May  4 14:30:58 2017.",
    "",
    "This file is execfile()d with the current directory set to its",
    "containing dir.",
    "",
    "Note that not all possible configuration values are present in this",
    "autogenerated file.",
    "",
    "All configuration values have a default; values that are commented out",
    "serve to show the default.",
    "If extensions (or modules to document with autodoc) are in another directory,",
    "add these directories to sys.path here. If the directory is relative to the",
    "documentation root, use os.path.abspath to make it absolute.",
    "-- mock out modules",
    "-- General configuration ------------------------------------------------",
    "If your documentation needs a minimal Sphinx version, state it here.",
    "Add any Sphinx extension module names here, as strings. They can be",
    "extensions coming with Sphinx (named 'sphinx.ext.*') or your custom",
    "ones.",
    "Add any paths that contain templates here, relative to this directory.",
    "The suffix(es) of source filenames.",
    "You can specify multiple suffix as a list of string:",
    "source_suffix = ['.rst', '.md']",
    "The master toctree document.",
    "General information about the project.",
    "The version info for the project you're documenting, acts as replacement for",
    "|version| and |release|, also used in various other places throughout the",
    "built documents.",
    "",
    "The short X.Y version.",
    "The full version, including alpha/beta/rc tags.",
    "The language for content autogenerated by Sphinx. Refer to documentation",
    "for a list of supported languages.",
    "",
    "This is also used if you do content translation via gettext catalogs.",
    "Usually you set \"language\" from the command line for these cases.",
    "List of patterns, relative to source directory, that match files and",
    "directories to ignore when looking for source files.",
    "This patterns also effect to html_static_path and html_extra_path",
    "The name of the Pygments (syntax highlighting) style to use.",
    "If true, `todo` and `todoList` produce output, else they produce nothing.",
    "Both the class' and the __init__ method's docstring are concatenated and inserted.",
    "-- Options for HTML output ----------------------------------------------",
    "The theme to use for HTML and HTML Help pages.  See the documentation for",
    "a list of builtin themes.",
    "Theme options are theme-specific and customize the look and feel of a theme",
    "further.  For a list of options available for each theme, see the",
    "documentation.",
    "Add any paths that contain custom static files (such as style sheets) here,",
    "relative to this directory. They are copied after the builtin static files,",
    "so a file named \"default.css\" will overwrite the builtin \"default.css\".",
    "-- Options for HTMLHelp output ------------------------------------------",
    "Output file base name for HTML help builder.",
    "coding: utf-8",
    "coding: utf-8",
    "pylint: skip-file",
    "we don't need lib_lightgbm while building docs",
    "coding: utf-8",
    "pylint: skip-file",
    "check saved model persistence",
    "we need to check the consistency of model file here, so test for exact equal",
    "check early stopping is working. Make it stop very early, so the scores should be very close to zero",
    "scores likely to be different, but prediction should still be the same",
    "coding: utf-8",
    "pylint: skip-file",
    "Test that the largest element is NOT the same, the smallest can be the same, i.e. zero",
    "sklearn <0.19 cannot accept instance, but many tests could be passed only with min_data=1 and min_data_in_bin=1",
    "we cannot use `check_estimator` directly since there is no skip test mechanism",
    "we cannot leave default params (see https://github.com/Microsoft/LightGBM/issues/833)",
    "Tests same probabilities",
    "Tests same predictions",
    "Tests same raw scores",
    "Tests same leaf indices",
    "Tests same feature contributions",
    "Tests other parameters for the prediction works",
    "coding: utf-8",
    "pylint: skip-file",
    "coding: utf-8",
    "pylint: skip-file",
    "no early stopping",
    "early stopping occurs",
    "test custom eval metrics",
    "shuffle = False, override metric in params",
    "shuffle = True, callbacks",
    "self defined folds",
    "lambdarank",
    "... with l2 metric",
    "... with NDCG (default) metric",
    "self defined folds with lambdarank",
    "test feature_names with whitespaces",
    "take subsets and train",
    "test sliced labels",
    "append some columns",
    "append some rows",
    "test sliced 2d matrix",
    "test sliced CSR",
    "coding: utf-8",
    "pylint: skip-file",
    "Register Dynamic Symbols",
    "convert from one-based to  zero-based index",
    "convert from boundaries to size",
    "--- start Booster interfaces",
    "create boosting",
    "initialize the boosting",
    "create objective function",
    "initialize the objective function",
    "create training metric",
    "reset the boosting",
    "create objective function",
    "initialize the objective function",
    "some help functions used to convert data",
    "Row iterator of on column for CSC matrix",
    "return value at idx, only can access by ascent order",
    "return next non-zero pair, if index < 0, means no more data",
    "start of c_api functions",
    "sample data first",
    "sample data first",
    "sample data first",
    "no more data",
    "---- start of booster",
    "---- start of some help functions",
    "set number of threads for openmp",
    "check for alias",
    "read parameters from config file",
    "remove str after \"#\"",
    "check for alias again",
    "load configs",
    "prediction is needed if using input initial model(continued train)",
    "need to continue training",
    "sync up random seed for data partition",
    "load Training data",
    "load data for parallel training",
    "load data for single machine",
    "need save binary file",
    "create training metric",
    "only when have metrics then need to construct validation data",
    "Add validation data, if it exists",
    "add",
    "need save binary file",
    "add metric for validation data",
    "output used time on each iteration",
    "need init network",
    "create boosting",
    "create objective function",
    "load training data",
    "initialize the objective function",
    "initialize the boosting",
    "add validation data into boosting",
    "convert model to if-else statement code",
    "create predictor",
    "Free memory",
    "create predictor",
    "label_gain = 2^i - 1, may overflow, so we use 31 here",
    "counts for all labels",
    "start from top label, and accumulate DCG",
    "counts for all labels",
    "calculate k Max DCG by one pass",
    "get sorted indices by score",
    "calculate dcg",
    "get sorted indices by score",
    "calculate multi dcg by one pass",
    "wait for all client start up",
    "default set to -1",
    "distance at k-th communication, distance[k] = 2^k",
    "set incoming rank at k-th commuication",
    "set outgoing rank at k-th commuication",
    "defalut set as -1",
    "construct all recursive halving map for all machines",
    "let 1 << k <= num_machines",
    "distance of each communication",
    "if num_machines = 2^k, don't need to group machines",
    "communication direction, %2 == 0 is positive",
    "neighbor at k-th communication",
    "receive data block at k-th communication",
    "send data block at k-th communication",
    "if num_machines != 2^k, need to group machines",
    "group, two machine in one group, total \"rest\" groups will have 2 machines.",
    "let left machine as group leader",
    "cache block information for groups, group with 2 machines will have double block size",
    "convert from group to node leader",
    "convert from node to group",
    "meet new group",
    "add block len for this group",
    "calculate the group block start",
    "not need to construct",
    "get receive block informations",
    "accumulate block len",
    "get send block informations",
    "accumulate block len",
    "static member definition",
    "if small package or small count , do it by all gather.(reduce the communication times.)",
    "assign the blocks to every rank.",
    "do reduce scatter",
    "do all gather",
    "assign blocks",
    "need use buffer here, since size of \"output\" is smaller than size after all gather",
    "copy back",
    "assign blocks",
    "start all gather",
    "when num_machines is small and data is large",
    "use output as receive buffer",
    "get current local block size",
    "get out rank",
    "get in rank",
    "get send information",
    "get recv information",
    "send and recv at same time",
    "rotate in-place",
    "use output as receive buffer",
    "get current local block size",
    "get send information",
    "get recv information",
    "send and recv at same time",
    "use output as receive buffer",
    "send and recv at same time",
    "send local data to neighbor first",
    "receive neighbor data first",
    "reduce",
    "get target",
    "get send information",
    "get recv information",
    "send and recv at same time",
    "reduce",
    "send result to neighbor",
    "receive result from neighbor",
    "copy result",
    "start up socket",
    "parse clients from file",
    "get ip list of local machine",
    "get local rank",
    "construct listener",
    "construct communication topo",
    "construct linkers",
    "free listener",
    "set timeout",
    "accept incoming socket",
    "receive rank",
    "add new socket",
    "save ranks that need to connect with",
    "start listener",
    "start connect",
    "let smaller rank connect to larger rank",
    "send local rank",
    "wait for listener",
    "print connected linkers",
    "Get some statistic from 2 line",
    "if only have one line on file",
    "Constructors",
    "Get type tag",
    "Comparisons",
    "This has to be separate, not in Statics, because Json() accesses statics().null.",
    "advance until next line, or end of input",
    "advance until closing tokens",
    "The usual case: non-escaped characters",
    "Handle escapes",
    "Extract 4-byte escape sequence",
    "Explicitly check length of the substring. The following loop",
    "relies on std::string returning the terminating NUL when",
    "accessing str[length]. Checking here reduces brittleness.",
    "JSON specifies that characters outside the BMP shall be encoded as a pair",
    "of 4-hex-digit \\u escapes encoding their surrogate pair components. Check",
    "whether we're in the middle of such a beast: the previous codepoint was an",
    "escaped lead (high) surrogate, and this is a trail (low) surrogate.",
    "Reassemble the two surrogate pairs into one astral-plane character, per",
    "the UTF-16 algorithm.",
    "Integer part",
    "Decimal part",
    "Exponent part",
    "Check for any trailing garbage",
    "Documented in json11.hpp",
    "Check for another object",
    "get column names",
    "load label idx first",
    "erase label column name",
    "load ignore columns",
    "load weight idx",
    "load group idx",
    "don't support query id in data file when training in parallel",
    "read data to memory",
    "sample data",
    "construct feature bin mappers",
    "initialize label",
    "extract features",
    "sample data from file",
    "construct feature bin mappers",
    "initialize label",
    "extract features",
    "load data from binary file",
    "check meta data",
    "need to check training data",
    "read data in memory",
    "initialize label",
    "extract features",
    "Get number of lines of data file",
    "initialize label",
    "extract features",
    "load data from binary file",
    "not need to check validation data",
    "check meta data",
    "buffer to read binary file",
    "check token",
    "read size of header",
    "re-allocmate space if not enough",
    "read header",
    "get header",
    "num_groups",
    "real_feature_idx_",
    "feature2group",
    "feature2subfeature",
    "group_bin_boundaries",
    "group_feature_start_",
    "group_feature_cnt_",
    "get feature names",
    "write feature names",
    "read size of meta data",
    "re-allocate space if not enough",
    "read meta data",
    "load meta data",
    "sample local used data if need to partition",
    "if not contain query file, minimal sample unit is one record",
    "if contain query file, minimal sample unit is one query",
    "if is new query",
    "read feature data",
    "read feature size",
    "re-allocate space if not enough",
    "fill feature_names_ if not header",
    "if only one machine, find bin locally",
    "if have multi-machines, need to find bin distributed",
    "different machines will find bin for different features",
    "start and len will store the process feature indices for different machines",
    "machine i will find bins for features in [ start[i], start[i] + len[i] )",
    "get size of bin mapper with max_bin size",
    "since sizes of different feature may not be same, we expand all bin mapper to type_size",
    "find local feature bins and copy to buffer",
    "free",
    "convert to binary size",
    "gather global feature bin mappers",
    "restore features bins from buffer",
    "---- private functions ----",
    "if features are ordered, not need to use hist_buf",
    "read all lines",
    "get query data",
    "if not contain query data, minimal sample unit is one record",
    "if contain query data, minimal sample unit is one query",
    "if is new query",
    "get query data",
    "if not contain query file, minimal sample unit is one record",
    "if contain query file, minimal sample unit is one query",
    "if is new query",
    "parse features",
    "-1 means doesn't use this feature",
    "check the range of label_idx, weight_idx and group_idx",
    "fill feature_names_ if not header",
    "start find bins",
    "if only one machine, find bin locally",
    "if have multi-machines, need to find bin distributed",
    "different machines will find bin for different features",
    "start and len will store the process feature indices for different machines",
    "machine i will find bins for features in [ start[i], start[i] + len[i] )",
    "get size of bin mapper with max_bin size",
    "since sizes of different feature may not be same, we expand all bin mapper to type_size",
    "find local feature bins and copy to buffer",
    "free",
    "convert to binary size",
    "gather global feature bin mappers",
    "restore features bins from buffer",
    "if doesn't need to prediction with initial model",
    "parser",
    "set label",
    "free processed line:",
    "shrink_to_fit will be very slow in linux, and seems not free memory, disable for now",
    "text_reader_->Lines()[i].shrink_to_fit();",
    "push data",
    "if is used feature",
    "if need to prediction with initial model",
    "parser",
    "set initial score",
    "set label",
    "free processed line:",
    "shrink_to_fit will be very slow in linux, and seems not free memory, disable for now",
    "text_reader_->Lines()[i].shrink_to_fit();",
    "push data",
    "if is used feature",
    "metadata_ will manage space of init_score",
    "text data can be free after loaded feature values",
    "parser",
    "set initial score",
    "set label",
    "push data",
    "if is used feature",
    "only need part of data",
    "need full data",
    "metadata_ will manage space of init_score",
    "read size of token",
    "deep copy function for BinMapper",
    "mean size for one bin",
    "need a new bin",
    "update bin upper bound",
    "last bin upper bound",
    "find distinct_values first",
    "push zero in the front",
    "use the large value",
    "push zero in the back",
    "convert to int type first",
    "sort by counts",
    "avoid first bin is zero",
    "will ignore the categorical of small counts",
    "need an additional bin for NaN",
    "use -1 to represent NaN",
    "Use MissingType::None to represent this bin contains all categoricals",
    "check trival(num_bin_ == 1) feature",
    "check useless bin",
    "calculate sparse rate",
    "sparse threshold",
    "for lambdarank, it needs query data for partition data in parallel learning",
    "need convert query_id to boundaries",
    "check weights",
    "check query boundries",
    "contain initial score file",
    "check weights",
    "get local weights",
    "check query boundries",
    "get local query boundaries",
    "contain initial score file",
    "get local initial scores",
    "re-load query weight",
    "save to nullptr",
    "save to nullptr",
    "save to nullptr",
    "default weight file name",
    "default weight file name",
    "use first line to count number class",
    "default query file name",
    "/ This file is auto generated by LightGBM\\helper\\parameter_generator.py from LightGBM\\include\\LightGBM\\config.h file.",
    "root is in the depth 0",
    "non-leaf",
    "leaf",
    "use this for the missing value conversion",
    "Predict func by Map to ifelse",
    "use this for the missing value conversion",
    "non-leaf",
    "left subtree",
    "right subtree",
    "leaf",
    "non-leaf",
    "left subtree",
    "right subtree",
    "leaf",
    "recursive computation of SHAP values for a decision tree",
    "extend the unique path",
    "leaf node",
    "internal node",
    "see if we have already split on this feature,",
    "if so we undo that split so we can redo it for this node",
    "clear old metrics",
    "to lower",
    "split",
    "remove duplicate",
    "add names of objective function if not providing metric",
    "generate seeds by seed.",
    "check for conflicts",
    "check if objective, metric, and num_class match",
    "Change pool size to -1 (no limit) when using data parallel to reduce communication costs",
    "Check max_depth and num_leaves",
    "filter is based on sampling data, so decrease its range",
    "put dense feature first",
    "sort by non zero cnt",
    "sort by non zero cnt, bigger first",
    "take apart small sparse group, due it will not gain on speed",
    "shuffle groups",
    "get num_features",
    "get bin_mappers",
    "copy feature bin mapper data",
    "copy feature bin mapper data",
    "if not pass a filename, just append \".bin\" of original file",
    "get size of header",
    "size of feature names",
    "write header",
    "write feature names",
    "get size of meta data",
    "write meta data",
    "write feature data",
    "get size of feature",
    "write feature",
    "feature is not used",
    "construct histograms for smaller leaf",
    "if not use ordered bin",
    "used ordered bin",
    "feature is not used",
    "construct histograms for smaller leaf",
    "if not use ordered bin",
    "used ordered bin",
    "fixed hessian.",
    "feature is not used",
    "construct histograms for smaller leaf",
    "if not use ordered bin",
    "used ordered bin",
    "feature is not used",
    "construct histograms for smaller leaf",
    "if not use ordered bin",
    "used ordered bin",
    "fixed hessian.",
    "PredictRaw",
    "PredictRawByMap",
    "Predict",
    "PredictByMap",
    "PredictLeafIndex",
    "PredictLeafIndexByMap",
    "output model type",
    "output number of class",
    "output label index",
    "output max_feature_idx",
    "output objective",
    "output tree models",
    "store the importance first",
    "sort the importance",
    "use serialized string to restore this object",
    "Use first 128 chars to avoid exceed the message buffer.",
    "get number of classes",
    "get index of label",
    "get max_feature_idx first",
    "get average_output",
    "get feature names",
    "set zero",
    "predict all the trees for one iteration",
    "check early stopping",
    "set zero",
    "predict all the trees for one iteration",
    "check early stopping",
    "margin_threshold will be captured by value",
    "copy and sort",
    "margin_threshold will be captured by value",
    "load forced_splits file",
    "init tree learner",
    "push training metrics",
    "create buffer for gradients and hessians",
    "get max feature index",
    "get label index",
    "get feature names",
    "if need bagging, create buffer",
    "reset config for tree learner",
    "multi-class",
    "binary class",
    "for a validation dataset, we need its score and metric",
    "update score",
    "objective function will calculate gradients and hessians",
    "random bagging, minimal unit is one record",
    "if need bagging",
    "set bagging data to tree learner",
    "get subset",
    "output used time per iteration",
    "boosting from average label; or customized \"average\" if implemented for the current objective",
    "boosting first",
    "bagging logic",
    "need to copy gradients for bagging subset.",
    "shrinkage by learning rate",
    "update score",
    "only add default score one-time",
    "updates scores",
    "add model",
    "reset score",
    "remove model",
    "print message for metric",
    "pop last early_stopping_round_ models",
    "update training score",
    "we need to predict out-of-bag scores of data for boosting",
    "update validation score",
    "print training metric",
    "print validation metric",
    "set zero",
    "predict all the trees for one iteration",
    "check early stopping",
    "push training metrics",
    "not same training data, need reset score and others",
    "create score tracker",
    "update score",
    "create buffer for gradients and hessians",
    "if need bagging, create buffer",
    "Get the max size of pool",
    "at least need 2 leaves",
    "push split information for all leaves",
    "get ordered bin",
    "check existing for ordered bin",
    "initialize splits for leaf",
    "initialize data partition",
    "initialize ordered gradients and hessians",
    "if has ordered bin, need to allocate a buffer to fast split",
    "get ordered bin",
    "initialize splits for leaf",
    "initialize data partition",
    "initialize ordered gradients and hessians",
    "if has ordered bin, need to allocate a buffer to fast split",
    "Get the max size of pool",
    "at least need 2 leaves",
    "push split information for all leaves",
    "some initial works before training",
    "root leaf",
    "only root leaf can be splitted on first time",
    "some initial works before finding best split",
    "find best threshold for every feature",
    "Get a leaf with max split gain",
    "Get split information for best leaf",
    "cannot split, quit",
    "split tree with best leaf",
    "reset histogram pool",
    "at least use one feature",
    "initialize used features",
    "Get used feature at current tree",
    "initialize data partition",
    "reset the splits for leaves",
    "Sumup for root",
    "use all data",
    "use bagging, only use part of data",
    "if has ordered bin, need to initialize the ordered bin",
    "use all data, pass nullptr",
    "bagging, only use part of data",
    "mark used data",
    "initialize ordered bin",
    "check depth of current leaf",
    "only need to check left leaf, since right leaf is in same level of left leaf",
    "no enough data to continue",
    "only have root",
    "put parent(left) leaf's histograms into larger leaf's histograms",
    "put parent(left) leaf's histograms to larger leaf's histograms",
    "split for the ordered bin",
    "mark data that at left-leaf",
    "split the ordered bin",
    "construct smaller leaf",
    "construct larger leaf",
    "find splits",
    "only has root leaf",
    "find best threshold for larger child",
    "start at root leaf",
    "before processing next node from queue, store info for current left/right leaf",
    "store \"best split\" for left and right, even if they might be overwritten by forced split",
    "then, compute own splits",
    "split info should exist because searching in bfs fashion - should have added from parent",
    "split tree, will return right leaf",
    "left = parent",
    "split tree, will return right leaf",
    "init the leaves that used on next iteration",
    "bag_mapper[index_mapper[i]]",
    "get feature partition",
    "get local used features",
    "get best split at smaller leaf",
    "find local best split for larger leaf",
    "sync global best info",
    "update best split",
    "instantiate template classes, otherwise linker cannot find the code",
    "initialize SerialTreeLearner",
    "Get local rank and global machine size",
    "allocate buffer for communication",
    "generate feature partition for current tree",
    "get local used feature",
    "get block start and block len for reduce scatter",
    "get buffer_write_start_pos_",
    "get buffer_read_start_pos_",
    "sync global data sumup info",
    "global sumup reduce",
    "copy back",
    "set global sumup info",
    "init global data count in leaf",
    "construct local histograms",
    "copy to buffer",
    "Reduce scatter for histogram",
    "restore global histograms from buffer",
    "find best threshold for smaller child",
    "only root leaf",
    "construct histgroms for large leaf, we init larger leaf as the parent, so we can just subtract the smaller leaf's histograms",
    "find best threshold for larger child",
    "find local best split for larger leaf",
    "sync global best info",
    "set best split",
    "need update global number of data in leaf",
    "instantiate template classes, otherwise linker cannot find the code",
    "initialize SerialTreeLearner",
    "some additional variables needed for GPU trainer",
    "Initialize GPU buffers and kernels",
    "some functions used for debugging the GPU histogram construction",
    "printf(\"grad %g != %g (%d ULPs)\\n\", h1[i].sum_gradients, h2[i].sum_gradients, ulps);",
    "goto err;",
    "printf(\"hessian %g != %g (%d ULPs)\\n\", h1[i].sum_hessians, h2[i].sum_hessians, ulps);",
    "goto err;",
    "we roughly want 256 workgroups per device, and we have num_dense_feature4_ feature tuples.",
    "also guarantee that there are at least 2K examples per workgroup",
    "return 0;",
    "we have already copied ordered gradients, ordered hessians and indices to GPU",
    "decide the best number of workgroups working on one feature4 tuple",
    "set work group size based on feature size",
    "each 2^exp_workgroups_per_feature workgroups work on a feature4 tuple",
    "we need to refresh the kernel arguments after reallocating",
    "The only argument that needs to be changed later is num_data_",
    "the GPU kernel will process all features in one call, and each",
    "2^exp_workgroups_per_feature (compile time constant) workgroup will",
    "process one feature4 tuple",
    "for the root node, indices are not copied",
    "for constant hessian, hessians are not copied except for the root node",
    "there will be 2^exp_workgroups_per_feature = num_workgroups / num_dense_feature4 sub-histogram per feature4",
    "and we will launch num_feature workgroups for this kernel",
    "will launch threads for all features",
    "the queue should be asynchrounous, and we will can WaitAndGetHistograms() before we start processing dense feature groups",
    "copy the results asynchronously. Size depends on if double precision is used",
    "we will wait for this object in WaitAndGetHistograms",
    "when the output is ready, the computation is done",
    "values of this feature has been redistributed to multiple bins; need a reduction here",
    "how many feature-group tuples we have",
    "leave some safe margin for prefetching",
    "256 work-items per workgroup. Each work-item prefetches one tuple for that feature",
    "clear sparse/dense maps",
    "do nothing if no features can be processed on GPU",
    "allocate memory for all features (FIXME: 4 GB barrier on some devices, need to split to multiple buffers)",
    "unpin old buffer if necessary before destructing them",
    "make ordered_gradients and hessians larger (including extra room for prefetching), and pin them",
    "allocate space for gradients and hessians on device",
    "we will copy gradients and hessians in after ordered_gradients_ and ordered_hessians_ are constructed",
    "allocate feature mask, for disabling some feature-groups' histogram calculation",
    "copy indices to the device",
    "histogram bin entry size depends on the precision (single/double)",
    "create output buffer, each feature has a histogram with device_bin_size_ bins,",
    "each work group generates a sub-histogram of dword_features_ features.",
    "only initialize once here, as this will not need to change when ResetTrainingData() is called",
    "create atomic counters for inter-group coordination",
    "The output buffer is allocated to host directly, to overlap compute and data transfer",
    "find the dense feature-groups and group then into Feature4 data structure (several feature-groups packed into 4 bytes)",
    "looking for dword_features_ non-sparse feature-groups",
    "decide if we need to redistribute the bin",
    "multiplier must be a power of 2",
    "device_bin_mults_.push_back(1);",
    "found",
    "for data transfer time",
    "Now generate new data structure feature4, and copy data to the device",
    "preallocate arrays for all threads, and pin them",
    "building Feature4 bundles; each thread handles dword_features_ features",
    "one feature datapoint is 4 bits",
    "this guarantees that the RawGet() function is inlined, rather than using virtual function dispatching",
    "one feature datapoint is one byte",
    "this guarantees that the RawGet() function is inlined, rather than using virtual function dispatching",
    "Dense bin",
    "Dense 4-bit bin",
    "working on the remaining (less than dword_features_) feature groups",
    "fill the leftover features",
    "fill this empty feature with some \"random\" value",
    "fill this empty feature with some \"random\" value",
    "copying the last 1 to (dword_features - 1) feature-groups in the last tuple",
    "deallocate pinned space for feature copying",
    "data transfer time",
    "for other types of failure, build log might not be available; program.build_log() can crash",
    "Something bad happened. Just return \"No log available.\"",
    "build is okay, log may contain warnings",
    "destroy any old kernels",
    "create OpenCL kernels for different number of workgroups per feature",
    "currently we don't use constant memory",
    "compile the GPU kernel depending if double precision is used, constant hessian is used, etc",
    "kernel with indices in an array",
    "kernel with all features enabled, with elimited branches",
    "kernel with all data indices (for root node, and assumes that root node always uses all features)",
    "do nothing if no features can be processed on GPU",
    "The only argument that needs to be changed later is num_data_",
    "hessian is passed as a parameter, but it is not available now.",
    "hessian will be set in BeforeTrain()",
    "Get the max bin size, used for selecting best GPU kernel",
    "initialize GPU",
    "determine which kernel to use based on the max number of bins",
    "setup GPU kernel arguments after we allocating all the buffers",
    "check if we need to recompile the GPU kernel (is_constant_hessian changed)",
    "this should rarely occur",
    "GPU memory has to been reallocated because data may have been changed",
    "setup GPU kernel arguments after we allocating all the buffers",
    "Copy initial full hessians and gradients to GPU.",
    "We start copying as early as possible, instead of at ConstructHistogram().",
    "setup hessian parameters only",
    "hessian is passed as a parameter",
    "use bagging",
    "On GPU, we start copying indices, gradients and hessians now, instead at ConstructHistogram()",
    "copy used gradients and hessians to ordered buffer",
    "transfer the indices to GPU",
    "transfer hessian to GPU",
    "setup hessian parameters only",
    "hessian is passed as a parameter",
    "transfer gradients to GPU",
    "only have root",
    "Copy indices, gradients and hessians as early as possible",
    "only need to initialize for smaller leaf",
    "Get leaf boundary",
    "copy indices to the GPU:",
    "copy ordered hessians to the GPU:",
    "copy ordered gradients to the GPU:",
    "do nothing if no features can be processed on GPU",
    "copy data indices if it is not null",
    "generate and copy ordered_gradients if gradients is not null",
    "generate and copy ordered_hessians if hessians is not null",
    "converted indices in is_feature_used to feature-group indices",
    "construct the feature masks for dense feature-groups",
    "if no feature group is used, just return and do not use GPU",
    "if not all feature groups are used, we need to transfer the feature mask to GPU",
    "otherwise, we will use a specialized GPU kernel with all feature groups enabled",
    "All data have been prepared, now run the GPU kernel",
    "construct smaller leaf",
    "ConstructGPUHistogramsAsync will return true if there are availabe feature gourps dispatched to GPU",
    "then construct sparse features on CPU",
    "We set data_indices to null to avoid rebuilding ordered gradients/hessians",
    "wait for GPU to finish, only if GPU is actually used",
    "use double precision",
    "use single precision",
    "Compare GPU histogram with CPU histogram, useful for debuggin GPU code problem",
    "#define GPU_DEBUG_COMPARE",
    "construct larger leaf",
    "then construct sparse features on CPU",
    "We set data_indices to null to avoid rebuilding ordered gradients/hessians",
    "wait for GPU to finish, only if GPU is actually used",
    "use double precision",
    "use single precision",
    "do some sanity check for the GPU algorithm",
    "limit top k",
    "get max bin",
    "calculate buffer size",
    "left and right on same time, so need double size",
    "initialize histograms for global",
    "sync global data sumup info",
    "set global sumup info",
    "init global data count in leaf",
    "get local sumup",
    "get local sumup",
    "get mean number on machines",
    "weighted gain",
    "get top k",
    "Copy histogram to buffer, and Get local aggregate features",
    "copy histograms.",
    "copy smaller leaf histograms first",
    "mark local aggregated feature",
    "copy",
    "then copy larger leaf histograms",
    "mark local aggregated feature",
    "copy",
    "use local data to find local best splits",
    "find splits",
    "only has root leaf",
    "find best threshold for larger child",
    "local voting",
    "gather",
    "get all top-k from all machines",
    "global voting",
    "copy local histgrams to buffer",
    "Reduce scatter for histogram",
    "find best split from local aggregated histograms",
    "restore from buffer",
    "find best threshold",
    "restore from buffer",
    "find best threshold",
    "find local best",
    "find local best split for larger leaf",
    "sync global best info",
    "copy back",
    "set the global number of data for leaves",
    "init the global sumup info",
    "instantiate template classes, otherwise linker cannot find the code",
    "coding: utf-8",
    "alias table",
    "names",
    "from strings",
    "tails",
    "tails"
  ],
  "v2.2.0": [
    "coding: utf-8",
    "pylint: disable=invalid-name, exec-used, C0111",
    "coding: utf-8",
    "pylint: disable = invalid-name, W0105",
    "create predictor first",
    "check dataset",
    "reduce cost for prediction training data",
    "process callbacks",
    "Most of legacy advanced options becomes callbacks",
    "construct booster",
    "start training",
    "check evaluation result.",
    "lambdarank task, split according to groups",
    "run preprocessing on the data set if needed",
    "setup callbacks",
    "coding: utf-8",
    "pylint: disable = C0103",
    "simplejson does not support Python 3.2, it throws a SyntaxError",
    "because of u'...' Unicode literals.",
    "DeprecationWarning is not shown by default, so let's create our own with higher level",
    "coding: utf-8",
    "pylint: disable = invalid-name, W0105, C0111, C0301",
    "minor change to support `**kwargs`",
    "user can set verbose with kwargs, it has higher priority",
    "register default metric for consistency with callable eval_metric case",
    "try to deduce from class instance",
    "overwrite default metric by explicitly set metric",
    "concatenate metric from params (or default if not provided in params) and eval_metric",
    "reduce cost for prediction training data",
    "free dataset",
    "Switch to using a multiclass objective in the underlying LGBM instance",
    "check group data",
    "coding: utf-8",
    "we don't need lib_lightgbm while building docs",
    "coding: utf-8",
    "pylint: disable = C0103",
    "coding: utf-8",
    "coding: utf-8",
    "pylint: disable = invalid-name, C0111, C0301",
    "pylint: disable = R0912, R0913, R0914, W0105, W0201, W0212",
    "TypeError: obj is not a string or a number",
    "ValueError: invalid literal",
    "__get_num_preds() cannot work with nrow > MAX_INT32, so calculate overall number of predictions piecemeal",
    "avoid memory consumption by arrays concatenation operations",
    "__get_num_preds() cannot work with nrow > MAX_INT32, so calculate overall number of predictions piecemeal",
    "avoid memory consumption by arrays concatenation operations",
    "process for args",
    "user can set verbose with params, it has higher priority",
    "get categorical features",
    "process for reference dataset",
    "start construct data",
    "check data has header or not",
    "load init score",
    "need re group init score",
    "set feature names",
    "change non-float data to float data, need to copy",
    "change non-float data to float data, need to copy",
    "create valid",
    "construct subset",
    "create train",
    "set to None",
    "we're done if self and reference share a common upstrem reference",
    "group data from LightGBM is boundaries data, need to convert to group size",
    "user can set verbose with params, it has higher priority",
    "Training task",
    "construct booster object",
    "save reference to data",
    "buffer for inner predict",
    "set network if necessary",
    "Prediction task",
    "need reset training data",
    "need to push new valid data",
    "Copy models",
    "Get name of features",
    "avoid to predict many time in one iteration",
    "Get num of inner evals",
    "Get name of evals",
    "coding: utf-8",
    "pylint: disable = invalid-name, W0105, C0301",
    "Callback environment used by callbacks",
    "coding: utf-8",
    "pylint: disable = invalid-name, C0111",
    "load or create your dataset",
    "create dataset for lightgbm",
    "if you want to re-use data, remember to set free_raw_data=False",
    "specify your configurations as a dict",
    "generate a feature name",
    "feature_name and categorical_feature",
    "check feature name",
    "save model to file",
    "dump model to JSON (and save to file)",
    "feature names",
    "feature importances",
    "load model to predict",
    "can only predict with the best iteration (or the saving iteration)",
    "eval with loaded model",
    "dump model with pickle",
    "load model with pickle to predict",
    "can predict with any iteration when loaded in pickle way",
    "eval with loaded model",
    "continue training",
    "init_model accepts:",
    "1. model file name",
    "2. Booster()",
    "decay learning rates",
    "learning_rates accepts:",
    "1. list/tuple with length = num_boost_round",
    "2. function(curr_iter)",
    "change other parameters during training",
    "self-defined objective function",
    "f(preds: array, train_data: Dataset) -> grad: array, hess: array",
    "log likelihood loss",
    "self-defined eval metric",
    "f(preds: array, train_data: Dataset) -> name: string, eval_result: float, is_higher_better: bool",
    "binary error",
    "callback",
    "coding: utf-8",
    "pylint: disable = invalid-name, C0111",
    "load or create your dataset",
    "train",
    "predict",
    "eval",
    "feature importances",
    "self-defined eval metric",
    "f(y_true: array, y_pred: array) -> name: string, eval_result: float, is_higher_better: bool",
    "Root Mean Squared Logarithmic Error (RMSLE)",
    "train",
    "predict",
    "eval",
    "other scikit-learn modules",
    "coding: utf-8",
    "pylint: disable = invalid-name, C0111",
    "load or create your dataset",
    "create dataset for lightgbm",
    "specify your configurations as a dict",
    "train",
    "coding: utf-8",
    "pylint: disable = invalid-name, C0111",
    "################",
    "Simulate some binary data with a single categorical and",
    "single continuous predictor",
    "################",
    "Set up a couple of utilities for our experiments",
    "################",
    "Observe the behavior of `binary` and `xentropy` objectives",
    "Trying this throws an error on non-binary values of y:",
    "experiment('binary', label_type='probability', DATA)",
    "The speed of `binary` is not drastically different than",
    "`xentropy`. `xentropy` runs faster than `binary` in many cases, although",
    "there are reasons to suspect that `binary` should run faster when the",
    "label is an integer instead of a float",
    "coding: utf-8",
    "pylint: disable = invalid-name, C0111",
    "load or create your dataset",
    "create dataset for lightgbm",
    "specify your configurations as a dict",
    "train",
    "save model to file",
    "predict",
    "eval",
    "!/usr/bin/env python3",
    "-*- coding: utf-8 -*-",
    "",
    "LightGBM documentation build configuration file, created by",
    "sphinx-quickstart on Thu May  4 14:30:58 2017.",
    "",
    "This file is execfile()d with the current directory set to its",
    "containing dir.",
    "",
    "Note that not all possible configuration values are present in this",
    "autogenerated file.",
    "",
    "All configuration values have a default; values that are commented out",
    "serve to show the default.",
    "If extensions (or modules to document with autodoc) are in another directory,",
    "add these directories to sys.path here. If the directory is relative to the",
    "documentation root, use os.path.abspath to make it absolute, like shown here.",
    "",
    "-- mock out modules",
    "-- General configuration ------------------------------------------------",
    "If your documentation needs a minimal Sphinx version, state it here.",
    "Add any Sphinx extension module names here, as strings. They can be",
    "extensions coming with Sphinx (named 'sphinx.ext.*') or your custom",
    "ones.",
    "Add any paths that contain templates here, relative to this directory.",
    "The suffix(es) of source filenames.",
    "You can specify multiple suffix as a list of string:",
    "source_suffix = ['.rst', '.md']",
    "The master toctree document.",
    "General information about the project.",
    "The version info for the project you're documenting, acts as replacement for",
    "|version| and |release|, also used in various other places throughout the",
    "built documents.",
    "",
    "The short X.Y version.",
    "The full version, including alpha/beta/rc tags.",
    "The language for content autogenerated by Sphinx. Refer to documentation",
    "for a list of supported languages.",
    "",
    "This is also used if you do content translation via gettext catalogs.",
    "Usually you set \"language\" from the command line for these cases.",
    "List of patterns, relative to source directory, that match files and",
    "directories to ignore when looking for source files.",
    "This patterns also effect to html_static_path and html_extra_path",
    "The name of the Pygments (syntax highlighting) style to use.",
    "If true, `todo` and `todoList` produce output, else they produce nothing.",
    "Both the class' and the __init__ method's docstring are concatenated and inserted.",
    "-- Options for HTML output ----------------------------------------------",
    "The theme to use for HTML and HTML Help pages.  See the documentation for",
    "a list of builtin themes.",
    "",
    "Theme options are theme-specific and customize the look and feel of a theme",
    "further.  For a list of options available for each theme, see the",
    "documentation.",
    "",
    "Add any paths that contain custom static files (such as style sheets) here,",
    "relative to this directory. They are copied after the builtin static files,",
    "so a file named \"default.css\" will overwrite the builtin \"default.css\".",
    "-- Options for HTMLHelp output ------------------------------------------",
    "Output file base name for HTML help builder.",
    "coding: utf-8",
    "coding: utf-8",
    "pylint: skip-file",
    "we don't need lib_lightgbm while building docs",
    "coding: utf-8",
    "pylint: skip-file",
    "check saved model persistence",
    "we need to check the consistency of model file here, so test for exact equal",
    "check early stopping is working. Make it stop very early, so the scores should be very close to zero",
    "scores likely to be different, but prediction should still be the same",
    "coding: utf-8",
    "pylint: skip-file",
    "Test that the largest element is NOT the same, the smallest can be the same, i.e. zero",
    "sklearn <0.19 cannot accept instance, but many tests could be passed only with min_data=1 and min_data_in_bin=1",
    "we cannot use `check_estimator` directly since there is no skip test mechanism",
    "we cannot leave default params (see https://github.com/Microsoft/LightGBM/issues/833)",
    "Tests same probabilities",
    "Tests same predictions",
    "Tests same raw scores",
    "Tests same leaf indices",
    "Tests same feature contributions",
    "Tests other parameters for the prediction works",
    "coding: utf-8",
    "pylint: skip-file",
    "coding: utf-8",
    "pylint: skip-file",
    "no early stopping",
    "early stopping occurs",
    "test custom eval metrics",
    "shuffle = False, override metric in params",
    "shuffle = True, callbacks",
    "self defined folds",
    "lambdarank",
    "... with NDCG (default) metric",
    "... with l2 metric",
    "test feature_names with whitespaces",
    "take subsets and train",
    "test sliced labels",
    "append some columns",
    "append some rows",
    "test sliced 2d matrix",
    "test sliced CSR",
    "coding: utf-8",
    "pylint: skip-file",
    "Register Dynamic Symbols",
    "convert from one-based to  zero-based index",
    "convert from boundaries to size",
    "--- start Booster interfaces",
    "create boosting",
    "initialize the boosting",
    "create objective function",
    "initialize the objective function",
    "create training metric",
    "reset the boosting",
    "create objective function",
    "initialize the objective function",
    "some help functions used to convert data",
    "Row iterator of on column for CSC matrix",
    "return value at idx, only can access by ascent order",
    "return next non-zero pair, if index < 0, means no more data",
    "start of c_api functions",
    "sample data first",
    "sample data first",
    "sample data first",
    "no more data",
    "---- start of booster",
    "---- start of some help functions",
    "set number of threads for openmp",
    "check for alias",
    "read parameters from config file",
    "remove str after \"#\"",
    "check for alias again",
    "load configs",
    "prediction is needed if using input initial model(continued train)",
    "need to continue training",
    "sync up random seed for data partition",
    "load Training data",
    "load data for parallel training",
    "load data for single machine",
    "need save binary file",
    "create training metric",
    "only when have metrics then need to construct validation data",
    "Add validation data, if it exists",
    "add",
    "need save binary file",
    "add metric for validation data",
    "output used time on each iteration",
    "need init network",
    "create boosting",
    "create objective function",
    "load training data",
    "initialize the objective function",
    "initialize the boosting",
    "add validation data into boosting",
    "convert model to if-else statement code",
    "create predictor",
    "Free memory",
    "create predictor",
    "label_gain = 2^i - 1, may overflow, so we use 31 here",
    "counts for all labels",
    "start from top label, and accumulate DCG",
    "counts for all labels",
    "calculate k Max DCG by one pass",
    "get sorted indices by score",
    "calculate dcg",
    "get sorted indices by score",
    "calculate multi dcg by one pass",
    "wait for all client start up",
    "default set to -1",
    "distance at k-th communication, distance[k] = 2^k",
    "set incoming rank at k-th commuication",
    "set outgoing rank at k-th commuication",
    "defalut set as -1",
    "construct all recursive halving map for all machines",
    "let 1 << k <= num_machines",
    "distance of each communication",
    "if num_machines = 2^k, don't need to group machines",
    "communication direction, %2 == 0 is positive",
    "neighbor at k-th communication",
    "receive data block at k-th communication",
    "send data block at k-th communication",
    "if num_machines != 2^k, need to group machines",
    "group, two machine in one group, total \"rest\" groups will have 2 machines.",
    "let left machine as group leader",
    "cache block information for groups, group with 2 machines will have double block size",
    "convert from group to node leader",
    "convert from node to group",
    "meet new group",
    "add block len for this group",
    "calculate the group block start",
    "not need to construct",
    "get receive block informations",
    "accumulate block len",
    "get send block informations",
    "accumulate block len",
    "static member definition",
    "if small package or small count , do it by all gather.(reduce the communication times.)",
    "assign the blocks to every rank.",
    "do reduce scatter",
    "do all gather",
    "assign blocks",
    "need use buffer here, since size of \"output\" is smaller than size after all gather",
    "copy back",
    "assign blocks",
    "start all gather",
    "when num_machines is small and data is large",
    "use output as receive buffer",
    "get current local block size",
    "get out rank",
    "get in rank",
    "get send information",
    "get recv information",
    "send and recv at same time",
    "rotate in-place",
    "use output as receive buffer",
    "get current local block size",
    "get send information",
    "get recv information",
    "send and recv at same time",
    "use output as receive buffer",
    "send and recv at same time",
    "send local data to neighbor first",
    "receive neighbor data first",
    "reduce",
    "get target",
    "get send information",
    "get recv information",
    "send and recv at same time",
    "reduce",
    "send result to neighbor",
    "receive result from neighbor",
    "copy result",
    "start up socket",
    "parse clients from file",
    "get ip list of local machine",
    "get local rank",
    "construct listener",
    "construct communication topo",
    "construct linkers",
    "free listener",
    "set timeout",
    "accept incoming socket",
    "receive rank",
    "add new socket",
    "save ranks that need to connect with",
    "start listener",
    "start connect",
    "let smaller rank connect to larger rank",
    "send local rank",
    "wait for listener",
    "print connected linkers",
    "Get some statistic from 2 line",
    "if only have one line on file",
    "Constructors",
    "Get type tag",
    "Comparisons",
    "This has to be separate, not in Statics, because Json() accesses statics().null.",
    "advance until next line, or end of input",
    "advance until closing tokens",
    "The usual case: non-escaped characters",
    "Handle escapes",
    "Extract 4-byte escape sequence",
    "Explicitly check length of the substring. The following loop",
    "relies on std::string returning the terminating NUL when",
    "accessing str[length]. Checking here reduces brittleness.",
    "JSON specifies that characters outside the BMP shall be encoded as a pair",
    "of 4-hex-digit \\u escapes encoding their surrogate pair components. Check",
    "whether we're in the middle of such a beast: the previous codepoint was an",
    "escaped lead (high) surrogate, and this is a trail (low) surrogate.",
    "Reassemble the two surrogate pairs into one astral-plane character, per",
    "the UTF-16 algorithm.",
    "Integer part",
    "Decimal part",
    "Exponent part",
    "Check for any trailing garbage",
    "Documented in json11.hpp",
    "Check for another object",
    "get column names",
    "load label idx first",
    "erase label column name",
    "load ignore columns",
    "load weight idx",
    "load group idx",
    "don't support query id in data file when training in parallel",
    "read data to memory",
    "sample data",
    "construct feature bin mappers",
    "initialize label",
    "extract features",
    "sample data from file",
    "construct feature bin mappers",
    "initialize label",
    "extract features",
    "load data from binary file",
    "check meta data",
    "need to check training data",
    "read data in memory",
    "initialize label",
    "extract features",
    "Get number of lines of data file",
    "initialize label",
    "extract features",
    "load data from binary file",
    "not need to check validation data",
    "check meta data",
    "buffer to read binary file",
    "check token",
    "read size of header",
    "re-allocmate space if not enough",
    "read header",
    "get header",
    "num_groups",
    "real_feature_idx_",
    "feature2group",
    "feature2subfeature",
    "group_bin_boundaries",
    "group_feature_start_",
    "group_feature_cnt_",
    "get feature names",
    "write feature names",
    "read size of meta data",
    "re-allocate space if not enough",
    "read meta data",
    "load meta data",
    "sample local used data if need to partition",
    "if not contain query file, minimal sample unit is one record",
    "if contain query file, minimal sample unit is one query",
    "if is new query",
    "read feature data",
    "read feature size",
    "re-allocate space if not enough",
    "fill feature_names_ if not header",
    "if only one machine, find bin locally",
    "if have multi-machines, need to find bin distributed",
    "different machines will find bin for different features",
    "start and len will store the process feature indices for different machines",
    "machine i will find bins for features in [ start[i], start[i] + len[i] )",
    "get size of bin mapper with max_bin size",
    "since sizes of different feature may not be same, we expand all bin mapper to type_size",
    "find local feature bins and copy to buffer",
    "free",
    "convert to binary size",
    "gather global feature bin mappers",
    "restore features bins from buffer",
    "---- private functions ----",
    "if features are ordered, not need to use hist_buf",
    "read all lines",
    "get query data",
    "if not contain query data, minimal sample unit is one record",
    "if contain query data, minimal sample unit is one query",
    "if is new query",
    "get query data",
    "if not contain query file, minimal sample unit is one record",
    "if contain query file, minimal sample unit is one query",
    "if is new query",
    "parse features",
    "-1 means doesn't use this feature",
    "check the range of label_idx, weight_idx and group_idx",
    "fill feature_names_ if not header",
    "start find bins",
    "if only one machine, find bin locally",
    "if have multi-machines, need to find bin distributed",
    "different machines will find bin for different features",
    "start and len will store the process feature indices for different machines",
    "machine i will find bins for features in [ start[i], start[i] + len[i] )",
    "get size of bin mapper with max_bin size",
    "since sizes of different feature may not be same, we expand all bin mapper to type_size",
    "find local feature bins and copy to buffer",
    "free",
    "convert to binary size",
    "gather global feature bin mappers",
    "restore features bins from buffer",
    "if doesn't need to prediction with initial model",
    "parser",
    "set label",
    "free processed line:",
    "shrink_to_fit will be very slow in linux, and seems not free memory, disable for now",
    "text_reader_->Lines()[i].shrink_to_fit();",
    "push data",
    "if is used feature",
    "if need to prediction with initial model",
    "parser",
    "set initial score",
    "set label",
    "free processed line:",
    "shrink_to_fit will be very slow in linux, and seems not free memory, disable for now",
    "text_reader_->Lines()[i].shrink_to_fit();",
    "push data",
    "if is used feature",
    "metadata_ will manage space of init_score",
    "text data can be free after loaded feature values",
    "parser",
    "set initial score",
    "set label",
    "push data",
    "if is used feature",
    "only need part of data",
    "need full data",
    "metadata_ will manage space of init_score",
    "read size of token",
    "deep copy function for BinMapper",
    "mean size for one bin",
    "need a new bin",
    "update bin upper bound",
    "last bin upper bound",
    "find distinct_values first",
    "push zero in the front",
    "use the large value",
    "push zero in the back",
    "convert to int type first",
    "sort by counts",
    "avoid first bin is zero",
    "will ignore the categorical of small counts",
    "need an additional bin for NaN",
    "use -1 to represent NaN",
    "Use MissingType::None to represent this bin contains all categoricals",
    "check trival(num_bin_ == 1) feature",
    "check useless bin",
    "calculate sparse rate",
    "sparse threshold",
    "for lambdarank, it needs query data for partition data in parallel learning",
    "need convert query_id to boundaries",
    "check weights",
    "check query boundries",
    "contain initial score file",
    "check weights",
    "get local weights",
    "check query boundries",
    "get local query boundaries",
    "contain initial score file",
    "get local initial scores",
    "re-load query weight",
    "save to nullptr",
    "save to nullptr",
    "save to nullptr",
    "default weight file name",
    "default weight file name",
    "use first line to count number class",
    "default query file name",
    "/ This file is auto generated by LightGBM\\helper\\parameter_generator.py from LightGBM\\include\\LightGBM\\config.h file.",
    "root is in the depth 0",
    "non-leaf",
    "leaf",
    "use this for the missing value conversion",
    "Predict func by Map to ifelse",
    "use this for the missing value conversion",
    "non-leaf",
    "left subtree",
    "right subtree",
    "leaf",
    "non-leaf",
    "left subtree",
    "right subtree",
    "leaf",
    "recursive computation of SHAP values for a decision tree",
    "extend the unique path",
    "leaf node",
    "internal node",
    "see if we have already split on this feature,",
    "if so we undo that split so we can redo it for this node",
    "clear old metrics",
    "to lower",
    "split",
    "remove duplicate",
    "add names of objective function if not providing metric",
    "generate seeds by seed.",
    "check for conflicts",
    "check if objective, metric, and num_class match",
    "Change pool size to -1 (no limit) when using data parallel to reduce communication costs",
    "Check max_depth and num_leaves",
    "filter is based on sampling data, so decrease its range",
    "put dense feature first",
    "sort by non zero cnt",
    "sort by non zero cnt, bigger first",
    "take apart small sparse group, due it will not gain on speed",
    "shuffle groups",
    "get num_features",
    "get bin_mappers",
    "copy feature bin mapper data",
    "copy feature bin mapper data",
    "if not pass a filename, just append \".bin\" of original file",
    "get size of header",
    "size of feature names",
    "write header",
    "write feature names",
    "get size of meta data",
    "write meta data",
    "write feature data",
    "get size of feature",
    "write feature",
    "feature is not used",
    "construct histograms for smaller leaf",
    "if not use ordered bin",
    "used ordered bin",
    "feature is not used",
    "construct histograms for smaller leaf",
    "if not use ordered bin",
    "used ordered bin",
    "fixed hessian.",
    "feature is not used",
    "construct histograms for smaller leaf",
    "if not use ordered bin",
    "used ordered bin",
    "feature is not used",
    "construct histograms for smaller leaf",
    "if not use ordered bin",
    "used ordered bin",
    "fixed hessian.",
    "PredictRaw",
    "PredictRawByMap",
    "Predict",
    "PredictByMap",
    "PredictLeafIndex",
    "PredictLeafIndexByMap",
    "output model type",
    "output number of class",
    "output label index",
    "output max_feature_idx",
    "output objective",
    "output tree models",
    "store the importance first",
    "sort the importance",
    "use serialized string to restore this object",
    "Use first 128 chars to avoid exceed the message buffer.",
    "get number of classes",
    "get index of label",
    "get max_feature_idx first",
    "get average_output",
    "get feature names",
    "set zero",
    "predict all the trees for one iteration",
    "check early stopping",
    "set zero",
    "predict all the trees for one iteration",
    "check early stopping",
    "margin_threshold will be captured by value",
    "copy and sort",
    "margin_threshold will be captured by value",
    "load forced_splits file",
    "init tree learner",
    "push training metrics",
    "create buffer for gradients and hessians",
    "get max feature index",
    "get label index",
    "get feature names",
    "if need bagging, create buffer",
    "reset config for tree learner",
    "multi-class",
    "binary class",
    "for a validation dataset, we need its score and metric",
    "update score",
    "objective function will calculate gradients and hessians",
    "random bagging, minimal unit is one record",
    "if need bagging",
    "set bagging data to tree learner",
    "get subset",
    "output used time per iteration",
    "boosting from average label; or customized \"average\" if implemented for the current objective",
    "boosting first",
    "bagging logic",
    "need to copy gradients for bagging subset.",
    "shrinkage by learning rate",
    "update score",
    "only add default score one-time",
    "updates scores",
    "add model",
    "reset score",
    "remove model",
    "print message for metric",
    "pop last early_stopping_round_ models",
    "update training score",
    "we need to predict out-of-bag scores of data for boosting",
    "update validation score",
    "print training metric",
    "print validation metric",
    "set zero",
    "predict all the trees for one iteration",
    "check early stopping",
    "push training metrics",
    "not same training data, need reset score and others",
    "create score tracker",
    "update score",
    "create buffer for gradients and hessians",
    "if need bagging, create buffer",
    "Get the max size of pool",
    "at least need 2 leaves",
    "push split information for all leaves",
    "get ordered bin",
    "check existing for ordered bin",
    "initialize splits for leaf",
    "initialize data partition",
    "initialize ordered gradients and hessians",
    "if has ordered bin, need to allocate a buffer to fast split",
    "get ordered bin",
    "initialize splits for leaf",
    "initialize data partition",
    "initialize ordered gradients and hessians",
    "if has ordered bin, need to allocate a buffer to fast split",
    "Get the max size of pool",
    "at least need 2 leaves",
    "push split information for all leaves",
    "some initial works before training",
    "root leaf",
    "only root leaf can be splitted on first time",
    "some initial works before finding best split",
    "find best threshold for every feature",
    "Get a leaf with max split gain",
    "Get split information for best leaf",
    "cannot split, quit",
    "split tree with best leaf",
    "reset histogram pool",
    "at least use one feature",
    "initialize used features",
    "Get used feature at current tree",
    "initialize data partition",
    "reset the splits for leaves",
    "Sumup for root",
    "use all data",
    "use bagging, only use part of data",
    "if has ordered bin, need to initialize the ordered bin",
    "use all data, pass nullptr",
    "bagging, only use part of data",
    "mark used data",
    "initialize ordered bin",
    "check depth of current leaf",
    "only need to check left leaf, since right leaf is in same level of left leaf",
    "no enough data to continue",
    "only have root",
    "put parent(left) leaf's histograms into larger leaf's histograms",
    "put parent(left) leaf's histograms to larger leaf's histograms",
    "split for the ordered bin",
    "mark data that at left-leaf",
    "split the ordered bin",
    "construct smaller leaf",
    "construct larger leaf",
    "find splits",
    "only has root leaf",
    "find best threshold for larger child",
    "start at root leaf",
    "before processing next node from queue, store info for current left/right leaf",
    "store \"best split\" for left and right, even if they might be overwritten by forced split",
    "then, compute own splits",
    "split info should exist because searching in bfs fashion - should have added from parent",
    "split tree, will return right leaf",
    "left = parent",
    "split tree, will return right leaf",
    "init the leaves that used on next iteration",
    "bag_mapper[index_mapper[i]]",
    "get feature partition",
    "get local used features",
    "get best split at smaller leaf",
    "find local best split for larger leaf",
    "sync global best info",
    "update best split",
    "instantiate template classes, otherwise linker cannot find the code",
    "initialize SerialTreeLearner",
    "Get local rank and global machine size",
    "allocate buffer for communication",
    "generate feature partition for current tree",
    "get local used feature",
    "get block start and block len for reduce scatter",
    "get buffer_write_start_pos_",
    "get buffer_read_start_pos_",
    "sync global data sumup info",
    "global sumup reduce",
    "copy back",
    "set global sumup info",
    "init global data count in leaf",
    "construct local histograms",
    "copy to buffer",
    "Reduce scatter for histogram",
    "restore global histograms from buffer",
    "find best threshold for smaller child",
    "only root leaf",
    "construct histgroms for large leaf, we init larger leaf as the parent, so we can just subtract the smaller leaf's histograms",
    "find best threshold for larger child",
    "find local best split for larger leaf",
    "sync global best info",
    "set best split",
    "need update global number of data in leaf",
    "instantiate template classes, otherwise linker cannot find the code",
    "initialize SerialTreeLearner",
    "some additional variables needed for GPU trainer",
    "Initialize GPU buffers and kernels",
    "some functions used for debugging the GPU histogram construction",
    "printf(\"grad %g != %g (%d ULPs)\\n\", h1[i].sum_gradients, h2[i].sum_gradients, ulps);",
    "goto err;",
    "printf(\"hessian %g != %g (%d ULPs)\\n\", h1[i].sum_hessians, h2[i].sum_hessians, ulps);",
    "goto err;",
    "we roughly want 256 workgroups per device, and we have num_dense_feature4_ feature tuples.",
    "also guarantee that there are at least 2K examples per workgroup",
    "return 0;",
    "we have already copied ordered gradients, ordered hessians and indices to GPU",
    "decide the best number of workgroups working on one feature4 tuple",
    "set work group size based on feature size",
    "each 2^exp_workgroups_per_feature workgroups work on a feature4 tuple",
    "we need to refresh the kernel arguments after reallocating",
    "The only argument that needs to be changed later is num_data_",
    "the GPU kernel will process all features in one call, and each",
    "2^exp_workgroups_per_feature (compile time constant) workgroup will",
    "process one feature4 tuple",
    "for the root node, indices are not copied",
    "for constant hessian, hessians are not copied except for the root node",
    "there will be 2^exp_workgroups_per_feature = num_workgroups / num_dense_feature4 sub-histogram per feature4",
    "and we will launch num_feature workgroups for this kernel",
    "will launch threads for all features",
    "the queue should be asynchrounous, and we will can WaitAndGetHistograms() before we start processing dense feature groups",
    "copy the results asynchronously. Size depends on if double precision is used",
    "we will wait for this object in WaitAndGetHistograms",
    "when the output is ready, the computation is done",
    "values of this feature has been redistributed to multiple bins; need a reduction here",
    "how many feature-group tuples we have",
    "leave some safe margin for prefetching",
    "256 work-items per workgroup. Each work-item prefetches one tuple for that feature",
    "clear sparse/dense maps",
    "do nothing if no features can be processed on GPU",
    "allocate memory for all features (FIXME: 4 GB barrier on some devices, need to split to multiple buffers)",
    "unpin old buffer if necessary before destructing them",
    "make ordered_gradients and hessians larger (including extra room for prefetching), and pin them",
    "allocate space for gradients and hessians on device",
    "we will copy gradients and hessians in after ordered_gradients_ and ordered_hessians_ are constructed",
    "allocate feature mask, for disabling some feature-groups' histogram calculation",
    "copy indices to the device",
    "histogram bin entry size depends on the precision (single/double)",
    "create output buffer, each feature has a histogram with device_bin_size_ bins,",
    "each work group generates a sub-histogram of dword_features_ features.",
    "only initialize once here, as this will not need to change when ResetTrainingData() is called",
    "create atomic counters for inter-group coordination",
    "The output buffer is allocated to host directly, to overlap compute and data transfer",
    "find the dense feature-groups and group then into Feature4 data structure (several feature-groups packed into 4 bytes)",
    "looking for dword_features_ non-sparse feature-groups",
    "decide if we need to redistribute the bin",
    "multiplier must be a power of 2",
    "device_bin_mults_.push_back(1);",
    "found",
    "for data transfer time",
    "Now generate new data structure feature4, and copy data to the device",
    "preallocate arrays for all threads, and pin them",
    "building Feature4 bundles; each thread handles dword_features_ features",
    "one feature datapoint is 4 bits",
    "this guarantees that the RawGet() function is inlined, rather than using virtual function dispatching",
    "one feature datapoint is one byte",
    "this guarantees that the RawGet() function is inlined, rather than using virtual function dispatching",
    "Dense bin",
    "Dense 4-bit bin",
    "working on the remaining (less than dword_features_) feature groups",
    "fill the leftover features",
    "fill this empty feature with some \"random\" value",
    "fill this empty feature with some \"random\" value",
    "copying the last 1 to (dword_features - 1) feature-groups in the last tuple",
    "deallocate pinned space for feature copying",
    "data transfer time",
    "for other types of failure, build log might not be available; program.build_log() can crash",
    "Something bad happened. Just return \"No log available.\"",
    "build is okay, log may contain warnings",
    "destroy any old kernels",
    "create OpenCL kernels for different number of workgroups per feature",
    "currently we don't use constant memory",
    "compile the GPU kernel depending if double precision is used, constant hessian is used, etc",
    "kernel with indices in an array",
    "kernel with all features enabled, with elimited branches",
    "kernel with all data indices (for root node, and assumes that root node always uses all features)",
    "do nothing if no features can be processed on GPU",
    "The only argument that needs to be changed later is num_data_",
    "hessian is passed as a parameter, but it is not available now.",
    "hessian will be set in BeforeTrain()",
    "Get the max bin size, used for selecting best GPU kernel",
    "initialize GPU",
    "determine which kernel to use based on the max number of bins",
    "setup GPU kernel arguments after we allocating all the buffers",
    "check if we need to recompile the GPU kernel (is_constant_hessian changed)",
    "this should rarely occur",
    "GPU memory has to been reallocated because data may have been changed",
    "setup GPU kernel arguments after we allocating all the buffers",
    "Copy initial full hessians and gradients to GPU.",
    "We start copying as early as possible, instead of at ConstructHistogram().",
    "setup hessian parameters only",
    "hessian is passed as a parameter",
    "use bagging",
    "On GPU, we start copying indices, gradients and hessians now, instead at ConstructHistogram()",
    "copy used gradients and hessians to ordered buffer",
    "transfer the indices to GPU",
    "transfer hessian to GPU",
    "setup hessian parameters only",
    "hessian is passed as a parameter",
    "transfer gradients to GPU",
    "only have root",
    "Copy indices, gradients and hessians as early as possible",
    "only need to initialize for smaller leaf",
    "Get leaf boundary",
    "copy indices to the GPU:",
    "copy ordered hessians to the GPU:",
    "copy ordered gradients to the GPU:",
    "do nothing if no features can be processed on GPU",
    "copy data indices if it is not null",
    "generate and copy ordered_gradients if gradients is not null",
    "generate and copy ordered_hessians if hessians is not null",
    "converted indices in is_feature_used to feature-group indices",
    "construct the feature masks for dense feature-groups",
    "if no feature group is used, just return and do not use GPU",
    "if not all feature groups are used, we need to transfer the feature mask to GPU",
    "otherwise, we will use a specialized GPU kernel with all feature groups enabled",
    "All data have been prepared, now run the GPU kernel",
    "construct smaller leaf",
    "ConstructGPUHistogramsAsync will return true if there are availabe feature gourps dispatched to GPU",
    "then construct sparse features on CPU",
    "We set data_indices to null to avoid rebuilding ordered gradients/hessians",
    "wait for GPU to finish, only if GPU is actually used",
    "use double precision",
    "use single precision",
    "Compare GPU histogram with CPU histogram, useful for debuggin GPU code problem",
    "#define GPU_DEBUG_COMPARE",
    "construct larger leaf",
    "then construct sparse features on CPU",
    "We set data_indices to null to avoid rebuilding ordered gradients/hessians",
    "wait for GPU to finish, only if GPU is actually used",
    "use double precision",
    "use single precision",
    "do some sanity check for the GPU algorithm",
    "limit top k",
    "get max bin",
    "calculate buffer size",
    "left and right on same time, so need double size",
    "initialize histograms for global",
    "sync global data sumup info",
    "set global sumup info",
    "init global data count in leaf",
    "get local sumup",
    "get local sumup",
    "get mean number on machines",
    "weighted gain",
    "get top k",
    "Copy histogram to buffer, and Get local aggregate features",
    "copy histograms.",
    "copy smaller leaf histograms first",
    "mark local aggregated feature",
    "copy",
    "then copy larger leaf histograms",
    "mark local aggregated feature",
    "copy",
    "use local data to find local best splits",
    "find splits",
    "only has root leaf",
    "find best threshold for larger child",
    "local voting",
    "gather",
    "get all top-k from all machines",
    "global voting",
    "copy local histgrams to buffer",
    "Reduce scatter for histogram",
    "find best split from local aggregated histograms",
    "restore from buffer",
    "find best threshold",
    "restore from buffer",
    "find best threshold",
    "find local best",
    "find local best split for larger leaf",
    "sync global best info",
    "copy back",
    "set the global number of data for leaves",
    "init the global sumup info",
    "instantiate template classes, otherwise linker cannot find the code",
    "coding: utf-8",
    "alias table",
    "names",
    "from strings",
    "tails",
    "tails"
  ],
  "v2.1.2": [
    "coding: utf-8",
    "pylint: disable=invalid-name, exec-used, C0111",
    "coding: utf-8",
    "pylint: disable = invalid-name, W0105",
    "create predictor first",
    "check dataset",
    "reduce cost for prediction training data",
    "process callbacks",
    "Most of legacy advanced options becomes callbacks",
    "construct booster",
    "start training",
    "check evaluation result.",
    "lambdarank task, split according to groups",
    "run preprocessing on the data set if needed",
    "setup callbacks",
    "coding: utf-8",
    "pylint: disable = C0103",
    "simplejson does not support Python 3.2, it throws a SyntaxError",
    "because of u'...' Unicode literals.",
    "DeprecationWarning is not shown by default, so let's create our own with higher level",
    "coding: utf-8",
    "pylint: disable = invalid-name, W0105, C0111, C0301",
    "minor change to support `**kwargs`",
    "sklearn interface has another naming convention",
    "user can set verbose with kwargs, it has higher priority",
    "reduce cost for prediction training data",
    "free dataset",
    "Switch to using a multiclass objective in the underlying LGBM instance",
    "check group data",
    "coding: utf-8",
    "we don't need lib_lightgbm while building docs",
    "coding: utf-8",
    "pylint: disable = C0103",
    "coding: utf-8",
    "coding: utf-8",
    "pylint: disable = invalid-name, C0111, C0301",
    "pylint: disable = R0912, R0913, R0914, W0105, W0201, W0212",
    "TypeError: obj is not a string or a number",
    "ValueError: invalid literal",
    "process for args",
    "get categorical features",
    "process for reference dataset",
    "start construct data",
    "check data has header or not",
    "load init score",
    "need re group init score",
    "set feature names",
    "change non-float data to float data, need to copy",
    "create valid",
    "construct subset",
    "create train",
    "set to None",
    "we're done if self and reference share a common upstrem reference",
    "group data from LightGBM is boundaries data, need to convert to group size",
    "Training task",
    "construct booster object",
    "save reference to data",
    "buffer for inner predict",
    "set network if necessary",
    "Prediction task",
    "need reset training data",
    "need to push new valid data",
    "Get name of features",
    "avoid to predict many time in one iteration",
    "Get num of inner evals",
    "Get name of evals",
    "coding: utf-8",
    "pylint: disable = invalid-name, W0105, C0301",
    "Callback environment used by callbacks",
    "coding: utf-8",
    "pylint: disable = invalid-name, C0111",
    "load or create your dataset",
    "create dataset for lightgbm",
    "if you want to re-use data, remember to set free_raw_data=False",
    "specify your configurations as a dict",
    "generate a feature name",
    "feature_name and categorical_feature",
    "check feature name",
    "save model to file",
    "dump model to json (and save to file)",
    "feature names",
    "feature importances",
    "load model to predict",
    "can only predict with the best iteration (or the saving iteration)",
    "eval with loaded model",
    "dump model with pickle",
    "load model with pickle to predict",
    "can predict with any iteration when loaded in pickle way",
    "eval with loaded model",
    "continue training",
    "init_model accepts:",
    "1. model file name",
    "2. Booster()",
    "decay learning rates",
    "learning_rates accepts:",
    "1. list/tuple with length = num_boost_round",
    "2. function(curr_iter)",
    "change other parameters during training",
    "self-defined objective function",
    "f(preds: array, train_data: Dataset) -> grad: array, hess: array",
    "log likelihood loss",
    "self-defined eval metric",
    "f(preds: array, train_data: Dataset) -> name: string, value: array, is_higher_better: bool",
    "binary error",
    "callback",
    "coding: utf-8",
    "pylint: disable = invalid-name, C0111",
    "load or create your dataset",
    "train",
    "predict",
    "eval",
    "feature importances",
    "other scikit-learn modules",
    "coding: utf-8",
    "pylint: disable = invalid-name, C0111",
    "load or create your dataset",
    "create dataset for lightgbm",
    "specify your configurations as a dict",
    "train",
    "coding: utf-8",
    "pylint: disable = invalid-name, C0111",
    "################",
    "Simulate some binary data with a single categorical and",
    "single continuous predictor",
    "################",
    "Set up a couple of utilities for our experiments",
    "################",
    "Observe the behavior of `binary` and `xentropy` objectives",
    "Trying this throws an error on non-binary values of y:",
    "experiment('binary', label_type='probability', DATA)",
    "The speed of `binary` is not drastically different than",
    "`xentropy`. `xentropy` runs faster than `binary` in many cases, although",
    "there are reasons to suspect that `binary` should run faster when the",
    "label is an integer instead of a float",
    "coding: utf-8",
    "pylint: disable = invalid-name, C0111",
    "load or create your dataset",
    "create dataset for lightgbm",
    "specify your configurations as a dict",
    "train",
    "save model to file",
    "predict",
    "eval",
    "!/usr/bin/env python3",
    "-*- coding: utf-8 -*-",
    "",
    "LightGBM documentation build configuration file, created by",
    "sphinx-quickstart on Thu May  4 14:30:58 2017.",
    "",
    "This file is execfile()d with the current directory set to its",
    "containing dir.",
    "",
    "Note that not all possible configuration values are present in this",
    "autogenerated file.",
    "",
    "All configuration values have a default; values that are commented out",
    "serve to show the default.",
    "If extensions (or modules to document with autodoc) are in another directory,",
    "add these directories to sys.path here. If the directory is relative to the",
    "documentation root, use os.path.abspath to make it absolute, like shown here.",
    "",
    "-- mock out modules",
    "-- General configuration ------------------------------------------------",
    "If your documentation needs a minimal Sphinx version, state it here.",
    "Add any Sphinx extension module names here, as strings. They can be",
    "extensions coming with Sphinx (named 'sphinx.ext.*') or your custom",
    "ones.",
    "Add any paths that contain templates here, relative to this directory.",
    "The suffix(es) of source filenames.",
    "You can specify multiple suffix as a list of string:",
    "source_suffix = ['.rst', '.md']",
    "The master toctree document.",
    "General information about the project.",
    "The version info for the project you're documenting, acts as replacement for",
    "|version| and |release|, also used in various other places throughout the",
    "built documents.",
    "",
    "The short X.Y version.",
    "The full version, including alpha/beta/rc tags.",
    "The language for content autogenerated by Sphinx. Refer to documentation",
    "for a list of supported languages.",
    "",
    "This is also used if you do content translation via gettext catalogs.",
    "Usually you set \"language\" from the command line for these cases.",
    "List of patterns, relative to source directory, that match files and",
    "directories to ignore when looking for source files.",
    "This patterns also effect to html_static_path and html_extra_path",
    "The name of the Pygments (syntax highlighting) style to use.",
    "If true, `todo` and `todoList` produce output, else they produce nothing.",
    "Both the class' and the __init__ method's docstring are concatenated and inserted.",
    "-- Options for HTML output ----------------------------------------------",
    "The theme to use for HTML and HTML Help pages.  See the documentation for",
    "a list of builtin themes.",
    "",
    "Theme options are theme-specific and customize the look and feel of a theme",
    "further.  For a list of options available for each theme, see the",
    "documentation.",
    "",
    "Add any paths that contain custom static files (such as style sheets) here,",
    "relative to this directory. They are copied after the builtin static files,",
    "so a file named \"default.css\" will overwrite the builtin \"default.css\".",
    "-- Options for HTMLHelp output ------------------------------------------",
    "Output file base name for HTML help builder.",
    "coding: utf-8",
    "coding: utf-8",
    "pylint: skip-file",
    "we don't need lib_lightgbm while building docs",
    "coding: utf-8",
    "pylint: skip-file",
    "check saved model persistence",
    "we need to check the consistency of model file here, so test for exact equal",
    "check early stopping is working. Make it stop very early, so the scores should be very close to zero",
    "scores likely to be different, but prediction should still be the same",
    "coding: utf-8",
    "pylint: skip-file",
    "Tests that `seed` is the same as `random_state`",
    "sklearn <0.19 cannot accept instance, but many tests could be passed only with min_data=1 and min_data_in_bin=1",
    "we cannot use `check_estimator` directly since there is no skip test mechanism",
    "we cannot leave default params (see https://github.com/Microsoft/LightGBM/issues/833)",
    "Tests same probabilities",
    "Tests same predictions",
    "Tests same raw scores",
    "Tests same leaf indices",
    "Tests same feature contributions",
    "Tests other parameters for the prediction works",
    "coding: utf-8",
    "pylint: skip-file",
    "coding: utf-8",
    "pylint: skip-file",
    "no early stopping",
    "early stopping occurs",
    "test custom eval metrics",
    "shuffle = False, override metric in params",
    "shuffle = True, callbacks",
    "self defined folds",
    "lambdarank",
    "test feature_names with whitespaces",
    "take subsets and train",
    "test sliced labels",
    "append some columns",
    "append some rows",
    "test sliced 2d matrix",
    "test sliced CSR",
    "coding: utf-8",
    "pylint: skip-file",
    "convert from one-based to  zero-based index",
    "convert from boundaries to size",
    "--- start Booster interfaces",
    "create boosting",
    "initialize the boosting",
    "create objective function",
    "initialize the objective function",
    "create training metric",
    "reset the boosting",
    "create objective function",
    "initialize the objective function",
    "some help functions used to convert data",
    "Row iterator of on column for CSC matrix",
    "return value at idx, only can access by ascent order",
    "return next non-zero pair, if index < 0, means no more data",
    "start of c_api functions",
    "sample data first",
    "sample data first",
    "sample data first",
    "no more data",
    "---- start of booster",
    "---- start of some help functions",
    "set number of threads for openmp",
    "check for alias",
    "read parameters from config file",
    "remove str after \"#\"",
    "check for alias again",
    "load configs",
    "prediction is needed if using input initial model(continued train)",
    "need to continue training",
    "sync up random seed for data partition",
    "load Training data",
    "load data for parallel training",
    "load data for single machine",
    "need save binary file",
    "create training metric",
    "only when have metrics then need to construct validation data",
    "Add validation data, if it exists",
    "add",
    "need save binary file",
    "add metric for validation data",
    "output used time on each iteration",
    "need init network",
    "create boosting",
    "create objective function",
    "load training data",
    "initialize the objective function",
    "initialize the boosting",
    "add validation data into boosting",
    "convert model to if-else statement code",
    "create predictor",
    "Free memory",
    "create predictor",
    "label_gain = 2^i - 1, may overflow, so we use 31 here",
    "counts for all labels",
    "start from top label, and accumulate DCG",
    "counts for all labels",
    "calculate k Max DCG by one pass",
    "get sorted indices by score",
    "calculate dcg",
    "get sorted indices by score",
    "calculate multi dcg by one pass",
    "wait for all client start up",
    "default set to -1",
    "distance at k-th communication, distance[k] = 2^k",
    "set incoming rank at k-th commuication",
    "set outgoing rank at k-th commuication",
    "defalut set as -1",
    "construct all recursive halving map for all machines",
    "let 1 << k <= num_machines",
    "distance of each communication",
    "if num_machines = 2^k, don't need to group machines",
    "communication direction, %2 == 0 is positive",
    "neighbor at k-th communication",
    "receive data block at k-th communication",
    "send data block at k-th communication",
    "if num_machines != 2^k, need to group machines",
    "group, two machine in one group, total \"rest\" groups will have 2 machines.",
    "let left machine as group leader",
    "cache block information for groups, group with 2 machines will have double block size",
    "convert from group to node leader",
    "convert from node to group",
    "meet new group",
    "add block len for this group",
    "calculate the group block start",
    "not need to construct",
    "get receive block informations",
    "accumulate block len",
    "get send block informations",
    "accumulate block len",
    "static member definition",
    "if small package or small count , do it by all gather.(reduce the communication times.)",
    "assign the blocks to every rank.",
    "do reduce scatter",
    "do all gather",
    "assign blocks",
    "need use buffer here, since size of \"output\" is smaller than size after all gather",
    "copy back",
    "assign blocks",
    "start all gather",
    "when num_machines is small and data is large",
    "use output as receive buffer",
    "get current local block size",
    "get out rank",
    "get in rank",
    "get send information",
    "get recv information",
    "send and recv at same time",
    "rotate in-place",
    "use output as receive buffer",
    "get current local block size",
    "get send information",
    "get recv information",
    "send and recv at same time",
    "use output as receive buffer",
    "send and recv at same time",
    "send local data to neighbor first",
    "receive neighbor data first",
    "reduce",
    "get target",
    "get send information",
    "get recv information",
    "send and recv at same time",
    "reduce",
    "send result to neighbor",
    "receive result from neighbor",
    "copy result",
    "start up socket",
    "parse clients from file",
    "get ip list of local machine",
    "get local rank",
    "construct listener",
    "construct communication topo",
    "construct linkers",
    "free listener",
    "set timeout",
    "accept incoming socket",
    "receive rank",
    "add new socket",
    "save ranks that need to connect with",
    "start listener",
    "start connect",
    "let smaller rank connect to larger rank",
    "send local rank",
    "wait for listener",
    "print connected linkers",
    "Get some statistic from 2 line",
    "if only have one line on file",
    "Constructors",
    "Get type tag",
    "Comparisons",
    "This has to be separate, not in Statics, because Json() accesses statics().null.",
    "advance until next line, or end of input",
    "advance until closing tokens",
    "The usual case: non-escaped characters",
    "Handle escapes",
    "Extract 4-byte escape sequence",
    "Explicitly check length of the substring. The following loop",
    "relies on std::string returning the terminating NUL when",
    "accessing str[length]. Checking here reduces brittleness.",
    "JSON specifies that characters outside the BMP shall be encoded as a pair",
    "of 4-hex-digit \\u escapes encoding their surrogate pair components. Check",
    "whether we're in the middle of such a beast: the previous codepoint was an",
    "escaped lead (high) surrogate, and this is a trail (low) surrogate.",
    "Reassemble the two surrogate pairs into one astral-plane character, per",
    "the UTF-16 algorithm.",
    "Integer part",
    "Decimal part",
    "Exponent part",
    "Check for any trailing garbage",
    "Documented in json11.hpp",
    "Check for another object",
    "get column names",
    "load label idx first",
    "erase label column name",
    "load ignore columns",
    "load weight idx",
    "load group idx",
    "don't support query id in data file when training in parallel",
    "read data to memory",
    "sample data",
    "construct feature bin mappers",
    "initialize label",
    "extract features",
    "sample data from file",
    "construct feature bin mappers",
    "initialize label",
    "extract features",
    "load data from binary file",
    "check meta data",
    "need to check training data",
    "read data in memory",
    "initialize label",
    "extract features",
    "Get number of lines of data file",
    "initialize label",
    "extract features",
    "load data from binary file",
    "not need to check validation data",
    "check meta data",
    "buffer to read binary file",
    "check token",
    "read size of header",
    "re-allocmate space if not enough",
    "read header",
    "get header",
    "num_groups",
    "real_feature_idx_",
    "feature2group",
    "feature2subfeature",
    "group_bin_boundaries",
    "group_feature_start_",
    "group_feature_cnt_",
    "get feature names",
    "write feature names",
    "read size of meta data",
    "re-allocate space if not enough",
    "read meta data",
    "load meta data",
    "sample local used data if need to partition",
    "if not contain query file, minimal sample unit is one record",
    "if contain query file, minimal sample unit is one query",
    "if is new query",
    "read feature data",
    "read feature size",
    "re-allocate space if not enough",
    "fill feature_names_ if not header",
    "if only one machine, find bin locally",
    "if have multi-machines, need to find bin distributed",
    "different machines will find bin for different features",
    "start and len will store the process feature indices for different machines",
    "machine i will find bins for features in [ start[i], start[i] + len[i] )",
    "get size of bin mapper with max_bin size",
    "since sizes of different feature may not be same, we expand all bin mapper to type_size",
    "find local feature bins and copy to buffer",
    "free",
    "convert to binary size",
    "gather global feature bin mappers",
    "restore features bins from buffer",
    "---- private functions ----",
    "if features are ordered, not need to use hist_buf",
    "read all lines",
    "get query data",
    "if not contain query data, minimal sample unit is one record",
    "if contain query data, minimal sample unit is one query",
    "if is new query",
    "get query data",
    "if not contain query file, minimal sample unit is one record",
    "if contain query file, minimal sample unit is one query",
    "if is new query",
    "parse features",
    "-1 means doesn't use this feature",
    "check the range of label_idx, weight_idx and group_idx",
    "fill feature_names_ if not header",
    "start find bins",
    "if only one machine, find bin locally",
    "if have multi-machines, need to find bin distributed",
    "different machines will find bin for different features",
    "start and len will store the process feature indices for different machines",
    "machine i will find bins for features in [ start[i], start[i] + len[i] )",
    "get size of bin mapper with max_bin size",
    "since sizes of different feature may not be same, we expand all bin mapper to type_size",
    "find local feature bins and copy to buffer",
    "free",
    "convert to binary size",
    "gather global feature bin mappers",
    "restore features bins from buffer",
    "if doesn't need to prediction with initial model",
    "parser",
    "set label",
    "free processed line:",
    "shrink_to_fit will be very slow in linux, and seems not free memory, disable for now",
    "text_reader_->Lines()[i].shrink_to_fit();",
    "push data",
    "if is used feature",
    "if need to prediction with initial model",
    "parser",
    "set initial score",
    "set label",
    "free processed line:",
    "shrink_to_fit will be very slow in linux, and seems not free memory, disable for now",
    "text_reader_->Lines()[i].shrink_to_fit();",
    "push data",
    "if is used feature",
    "metadata_ will manage space of init_score",
    "text data can be free after loaded feature values",
    "parser",
    "set initial score",
    "set label",
    "push data",
    "if is used feature",
    "only need part of data",
    "need full data",
    "metadata_ will manage space of init_score",
    "read size of token",
    "deep copy function for BinMapper",
    "mean size for one bin",
    "need a new bin",
    "update bin upper bound",
    "last bin upper bound",
    "find distinct_values first",
    "push zero in the front",
    "use the large value",
    "push zero in the back",
    "convert to int type first",
    "sort by counts",
    "avoid first bin is zero",
    "will ignore the categorical of small counts",
    "need an additional bin for NaN",
    "use -1 to represent NaN",
    "Use MissingType::None to represent this bin contains all categoricals",
    "check trival(num_bin_ == 1) feature",
    "check useless bin",
    "calculate sparse rate",
    "sparse threshold",
    "for lambdarank, it needs query data for partition data in parallel learning",
    "need convert query_id to boundaries",
    "check weights",
    "check query boundries",
    "contain initial score file",
    "check weights",
    "get local weights",
    "check query boundries",
    "get local query boundaries",
    "contain initial score file",
    "get local initial scores",
    "re-load query weight",
    "save to nullptr",
    "save to nullptr",
    "save to nullptr",
    "default weight file name",
    "default weight file name",
    "use first line to count number class",
    "default query file name",
    "/ This file is auto generated by LightGBM\\helper\\parameter_generator.py from LightGBM\\include\\LightGBM\\config.h file.",
    "root is in the depth 0",
    "non-leaf",
    "leaf",
    "use this for the missing value conversion",
    "Predict func by Map to ifelse",
    "use this for the missing value conversion",
    "non-leaf",
    "left subtree",
    "right subtree",
    "leaf",
    "non-leaf",
    "left subtree",
    "right subtree",
    "leaf",
    "recursive computation of SHAP values for a decision tree",
    "extend the unique path",
    "leaf node",
    "internal node",
    "see if we have already split on this feature,",
    "if so we undo that split so we can redo it for this node",
    "clear old metrics",
    "to lower",
    "split",
    "remove duplicate",
    "add names of objective function if not providing metric",
    "generate seeds by seed.",
    "check for conflicts",
    "check if objective, metric, and num_class match",
    "Change pool size to -1 (no limit) when using data parallel to reduce communication costs",
    "Check max_depth and num_leaves",
    "filter is based on sampling data, so decrease its range",
    "put dense feature first",
    "sort by non zero cnt",
    "sort by non zero cnt, bigger first",
    "take apart small sparse group, due it will not gain on speed",
    "shuffle groups",
    "get num_features",
    "get bin_mappers",
    "copy feature bin mapper data",
    "copy feature bin mapper data",
    "if not pass a filename, just append \".bin\" of original file",
    "get size of header",
    "size of feature names",
    "write header",
    "write feature names",
    "get size of meta data",
    "write meta data",
    "write feature data",
    "get size of feature",
    "write feature",
    "feature is not used",
    "construct histograms for smaller leaf",
    "if not use ordered bin",
    "used ordered bin",
    "feature is not used",
    "construct histograms for smaller leaf",
    "if not use ordered bin",
    "used ordered bin",
    "fixed hessian.",
    "feature is not used",
    "construct histograms for smaller leaf",
    "if not use ordered bin",
    "used ordered bin",
    "feature is not used",
    "construct histograms for smaller leaf",
    "if not use ordered bin",
    "used ordered bin",
    "fixed hessian.",
    "PredictRaw",
    "PredictRawByMap",
    "Predict",
    "PredictByMap",
    "PredictLeafIndex",
    "PredictLeafIndexByMap",
    "output model type",
    "output number of class",
    "output label index",
    "output max_feature_idx",
    "output objective",
    "output tree models",
    "store the importance first",
    "sort the importance",
    "use serialized string to restore this object",
    "Use first 128 chars to avoid exceed the message buffer.",
    "get number of classes",
    "get index of label",
    "get max_feature_idx first",
    "get average_output",
    "get feature names",
    "set zero",
    "predict all the trees for one iteration",
    "check early stopping",
    "set zero",
    "predict all the trees for one iteration",
    "check early stopping",
    "margin_threshold will be captured by value",
    "copy and sort",
    "margin_threshold will be captured by value",
    "load forced_splits file",
    "init tree learner",
    "push training metrics",
    "create buffer for gradients and hessians",
    "get max feature index",
    "get label index",
    "get feature names",
    "if need bagging, create buffer",
    "reset config for tree learner",
    "multi-class",
    "binary class",
    "for a validation dataset, we need its score and metric",
    "update score",
    "objective function will calculate gradients and hessians",
    "random bagging, minimal unit is one record",
    "if need bagging",
    "set bagging data to tree learner",
    "get subset",
    "output used time per iteration",
    "boosting from average label; or customized \"average\" if implemented for the current objective",
    "boosting first",
    "bagging logic",
    "need to copy gradients for bagging subset.",
    "shrinkage by learning rate",
    "update score",
    "only add default score one-time",
    "updates scores",
    "add model",
    "reset score",
    "remove model",
    "print message for metric",
    "pop last early_stopping_round_ models",
    "update training score",
    "we need to predict out-of-bag scores of data for boosting",
    "update validation score",
    "print training metric",
    "print validation metric",
    "set zero",
    "predict all the trees for one iteration",
    "check early stopping",
    "push training metrics",
    "not same training data, need reset score and others",
    "create score tracker",
    "update score",
    "create buffer for gradients and hessians",
    "if need bagging, create buffer",
    "Get the max size of pool",
    "at least need 2 leaves",
    "push split information for all leaves",
    "get ordered bin",
    "check existing for ordered bin",
    "initialize splits for leaf",
    "initialize data partition",
    "initialize ordered gradients and hessians",
    "if has ordered bin, need to allocate a buffer to fast split",
    "get ordered bin",
    "initialize splits for leaf",
    "initialize data partition",
    "initialize ordered gradients and hessians",
    "if has ordered bin, need to allocate a buffer to fast split",
    "Get the max size of pool",
    "at least need 2 leaves",
    "push split information for all leaves",
    "some initial works before training",
    "root leaf",
    "only root leaf can be splitted on first time",
    "some initial works before finding best split",
    "find best threshold for every feature",
    "Get a leaf with max split gain",
    "Get split information for best leaf",
    "cannot split, quit",
    "split tree with best leaf",
    "reset histogram pool",
    "at least use one feature",
    "initialize used features",
    "Get used feature at current tree",
    "initialize data partition",
    "reset the splits for leaves",
    "Sumup for root",
    "use all data",
    "use bagging, only use part of data",
    "if has ordered bin, need to initialize the ordered bin",
    "use all data, pass nullptr",
    "bagging, only use part of data",
    "mark used data",
    "initialize ordered bin",
    "check depth of current leaf",
    "only need to check left leaf, since right leaf is in same level of left leaf",
    "no enough data to continue",
    "only have root",
    "put parent(left) leaf's histograms into larger leaf's histograms",
    "put parent(left) leaf's histograms to larger leaf's histograms",
    "split for the ordered bin",
    "mark data that at left-leaf",
    "split the ordered bin",
    "construct smaller leaf",
    "construct larger leaf",
    "find splits",
    "only has root leaf",
    "find best threshold for larger child",
    "start at root leaf",
    "before processing next node from queue, store info for current left/right leaf",
    "store \"best split\" for left and right, even if they might be overwritten by forced split",
    "then, compute own splits",
    "split info should exist because searching in bfs fashion - should have added from parent",
    "split tree, will return right leaf",
    "left = parent",
    "split tree, will return right leaf",
    "init the leaves that used on next iteration",
    "bag_mapper[index_mapper[i]]",
    "get feature partition",
    "get local used features",
    "get best split at smaller leaf",
    "find local best split for larger leaf",
    "sync global best info",
    "update best split",
    "instantiate template classes, otherwise linker cannot find the code",
    "initialize SerialTreeLearner",
    "Get local rank and global machine size",
    "allocate buffer for communication",
    "generate feature partition for current tree",
    "get local used feature",
    "get block start and block len for reduce scatter",
    "get buffer_write_start_pos_",
    "get buffer_read_start_pos_",
    "sync global data sumup info",
    "global sumup reduce",
    "copy back",
    "set global sumup info",
    "init global data count in leaf",
    "construct local histograms",
    "copy to buffer",
    "Reduce scatter for histogram",
    "restore global histograms from buffer",
    "find best threshold for smaller child",
    "only root leaf",
    "construct histgroms for large leaf, we init larger leaf as the parent, so we can just subtract the smaller leaf's histograms",
    "find best threshold for larger child",
    "find local best split for larger leaf",
    "sync global best info",
    "set best split",
    "need update global number of data in leaf",
    "instantiate template classes, otherwise linker cannot find the code",
    "initialize SerialTreeLearner",
    "some additional variables needed for GPU trainer",
    "Initialize GPU buffers and kernels",
    "some functions used for debugging the GPU histogram construction",
    "printf(\"grad %g != %g (%d ULPs)\\n\", h1[i].sum_gradients, h2[i].sum_gradients, ulps);",
    "goto err;",
    "printf(\"hessian %g != %g (%d ULPs)\\n\", h1[i].sum_hessians, h2[i].sum_hessians, ulps);",
    "goto err;",
    "we roughly want 256 workgroups per device, and we have num_dense_feature4_ feature tuples.",
    "also guarantee that there are at least 2K examples per workgroup",
    "return 0;",
    "we have already copied ordered gradients, ordered hessians and indices to GPU",
    "decide the best number of workgroups working on one feature4 tuple",
    "set work group size based on feature size",
    "each 2^exp_workgroups_per_feature workgroups work on a feature4 tuple",
    "we need to refresh the kernel arguments after reallocating",
    "The only argument that needs to be changed later is num_data_",
    "the GPU kernel will process all features in one call, and each",
    "2^exp_workgroups_per_feature (compile time constant) workgroup will",
    "process one feature4 tuple",
    "for the root node, indices are not copied",
    "for constant hessian, hessians are not copied except for the root node",
    "there will be 2^exp_workgroups_per_feature = num_workgroups / num_dense_feature4 sub-histogram per feature4",
    "and we will launch num_feature workgroups for this kernel",
    "will launch threads for all features",
    "the queue should be asynchrounous, and we will can WaitAndGetHistograms() before we start processing dense feature groups",
    "copy the results asynchronously. Size depends on if double precision is used",
    "we will wait for this object in WaitAndGetHistograms",
    "when the output is ready, the computation is done",
    "values of this feature has been redistributed to multiple bins; need a reduction here",
    "how many feature-group tuples we have",
    "leave some safe margin for prefetching",
    "256 work-items per workgroup. Each work-item prefetches one tuple for that feature",
    "clear sparse/dense maps",
    "do nothing if no features can be processed on GPU",
    "allocate memory for all features (FIXME: 4 GB barrier on some devices, need to split to multiple buffers)",
    "unpin old buffer if necessary before destructing them",
    "make ordered_gradients and hessians larger (including extra room for prefetching), and pin them",
    "allocate space for gradients and hessians on device",
    "we will copy gradients and hessians in after ordered_gradients_ and ordered_hessians_ are constructed",
    "allocate feature mask, for disabling some feature-groups' histogram calculation",
    "copy indices to the device",
    "histogram bin entry size depends on the precision (single/double)",
    "create output buffer, each feature has a histogram with device_bin_size_ bins,",
    "each work group generates a sub-histogram of dword_features_ features.",
    "only initialize once here, as this will not need to change when ResetTrainingData() is called",
    "create atomic counters for inter-group coordination",
    "The output buffer is allocated to host directly, to overlap compute and data transfer",
    "find the dense feature-groups and group then into Feature4 data structure (several feature-groups packed into 4 bytes)",
    "looking for dword_features_ non-sparse feature-groups",
    "decide if we need to redistribute the bin",
    "multiplier must be a power of 2",
    "device_bin_mults_.push_back(1);",
    "found",
    "for data transfer time",
    "Now generate new data structure feature4, and copy data to the device",
    "preallocate arrays for all threads, and pin them",
    "building Feature4 bundles; each thread handles dword_features_ features",
    "one feature datapoint is 4 bits",
    "this guarantees that the RawGet() function is inlined, rather than using virtual function dispatching",
    "one feature datapoint is one byte",
    "this guarantees that the RawGet() function is inlined, rather than using virtual function dispatching",
    "Dense bin",
    "Dense 4-bit bin",
    "working on the remaining (less than dword_features_) feature groups",
    "fill the leftover features",
    "fill this empty feature with some \"random\" value",
    "fill this empty feature with some \"random\" value",
    "copying the last 1 to (dword_features - 1) feature-groups in the last tuple",
    "deallocate pinned space for feature copying",
    "data transfer time",
    "for other types of failure, build log might not be available; program.build_log() can crash",
    "Something bad happened. Just return \"No log available.\"",
    "build is okay, log may contain warnings",
    "destroy any old kernels",
    "create OpenCL kernels for different number of workgroups per feature",
    "currently we don't use constant memory",
    "compile the GPU kernel depending if double precision is used, constant hessian is used, etc",
    "kernel with indices in an array",
    "kernel with all features enabled, with elimited branches",
    "kernel with all data indices (for root node, and assumes that root node always uses all features)",
    "do nothing if no features can be processed on GPU",
    "The only argument that needs to be changed later is num_data_",
    "hessian is passed as a parameter, but it is not available now.",
    "hessian will be set in BeforeTrain()",
    "Get the max bin size, used for selecting best GPU kernel",
    "initialize GPU",
    "determine which kernel to use based on the max number of bins",
    "setup GPU kernel arguments after we allocating all the buffers",
    "check if we need to recompile the GPU kernel (is_constant_hessian changed)",
    "this should rarely occur",
    "GPU memory has to been reallocated because data may have been changed",
    "setup GPU kernel arguments after we allocating all the buffers",
    "Copy initial full hessians and gradients to GPU.",
    "We start copying as early as possible, instead of at ConstructHistogram().",
    "setup hessian parameters only",
    "hessian is passed as a parameter",
    "use bagging",
    "On GPU, we start copying indices, gradients and hessians now, instead at ConstructHistogram()",
    "copy used gradients and hessians to ordered buffer",
    "transfer the indices to GPU",
    "transfer hessian to GPU",
    "setup hessian parameters only",
    "hessian is passed as a parameter",
    "transfer gradients to GPU",
    "only have root",
    "Copy indices, gradients and hessians as early as possible",
    "only need to initialize for smaller leaf",
    "Get leaf boundary",
    "copy indices to the GPU:",
    "copy ordered hessians to the GPU:",
    "copy ordered gradients to the GPU:",
    "do nothing if no features can be processed on GPU",
    "copy data indices if it is not null",
    "generate and copy ordered_gradients if gradients is not null",
    "generate and copy ordered_hessians if hessians is not null",
    "converted indices in is_feature_used to feature-group indices",
    "construct the feature masks for dense feature-groups",
    "if no feature group is used, just return and do not use GPU",
    "if not all feature groups are used, we need to transfer the feature mask to GPU",
    "otherwise, we will use a specialized GPU kernel with all feature groups enabled",
    "All data have been prepared, now run the GPU kernel",
    "construct smaller leaf",
    "ConstructGPUHistogramsAsync will return true if there are availabe feature gourps dispatched to GPU",
    "then construct sparse features on CPU",
    "We set data_indices to null to avoid rebuilding ordered gradients/hessians",
    "wait for GPU to finish, only if GPU is actually used",
    "use double precision",
    "use single precision",
    "Compare GPU histogram with CPU histogram, useful for debuggin GPU code problem",
    "#define GPU_DEBUG_COMPARE",
    "construct larger leaf",
    "then construct sparse features on CPU",
    "We set data_indices to null to avoid rebuilding ordered gradients/hessians",
    "wait for GPU to finish, only if GPU is actually used",
    "use double precision",
    "use single precision",
    "do some sanity check for the GPU algorithm",
    "limit top k",
    "get max bin",
    "calculate buffer size",
    "left and right on same time, so need double size",
    "initialize histograms for global",
    "sync global data sumup info",
    "set global sumup info",
    "init global data count in leaf",
    "get local sumup",
    "get local sumup",
    "get mean number on machines",
    "weighted gain",
    "get top k",
    "Copy histogram to buffer, and Get local aggregate features",
    "copy histograms.",
    "copy smaller leaf histograms first",
    "mark local aggregated feature",
    "copy",
    "then copy larger leaf histograms",
    "mark local aggregated feature",
    "copy",
    "use local data to find local best splits",
    "find splits",
    "only has root leaf",
    "find best threshold for larger child",
    "local voting",
    "gather",
    "get all top-k from all machines",
    "global voting",
    "copy local histgrams to buffer",
    "Reduce scatter for histogram",
    "find best split from local aggregated histograms",
    "restore from buffer",
    "find best threshold",
    "restore from buffer",
    "find best threshold",
    "find local best",
    "find local best split for larger leaf",
    "sync global best info",
    "copy back",
    "set the global number of data for leaves",
    "init the global sumup info",
    "instantiate template classes, otherwise linker cannot find the code",
    "coding: utf-8",
    "alias table",
    "names",
    "from strings",
    "tails",
    "tails"
  ],
  "v2.1.1": [
    "coding: utf-8",
    "pylint: disable=invalid-name, exec-used, C0111",
    "coding: utf-8",
    "pylint: disable = invalid-name, W0105",
    "create predictor first",
    "check dataset",
    "reduce cost for prediction training data",
    "process callbacks",
    "Most of legacy advanced options becomes callbacks",
    "construct booster",
    "start training",
    "check evaluation result.",
    "lambdarank task, split according to groups",
    "run preprocessing on the data set if needed",
    "setup callbacks",
    "coding: utf-8",
    "pylint: disable = C0103",
    "simplejson does not support Python 3.2, it throws a SyntaxError",
    "because of u'...' Unicode literals.",
    "DeprecationWarning is not shown by default, so let's create our own with higher level",
    "coding: utf-8",
    "pylint: disable = invalid-name, W0105, C0111, C0301",
    "minor change to support `**kwargs`",
    "sklearn interface has another naming convention",
    "user can set verbose with kwargs, it has higher priority",
    "reduce cost for prediction training data",
    "free dataset",
    "Switch to using a multiclass objective in the underlying LGBM instance",
    "check group data",
    "coding: utf-8",
    "we don't need lib_lightgbm while building docs",
    "coding: utf-8",
    "pylint: disable = C0103",
    "coding: utf-8",
    "coding: utf-8",
    "pylint: disable = invalid-name, C0111, C0301",
    "pylint: disable = R0912, R0913, R0914, W0105, W0201, W0212",
    "TypeError: obj is not a string or a number",
    "ValueError: invalid literal",
    "process for args",
    "get categorical features",
    "process for reference dataset",
    "start construct data",
    "check data has header or not",
    "load init score",
    "need re group init score",
    "set feature names",
    "change non-float data to float data, need to copy",
    "create valid",
    "construct subset",
    "create train",
    "set to None",
    "we're done if self and reference share a common upstrem reference",
    "group data from LightGBM is boundaries data, need to convert to group size",
    "Training task",
    "construct booster object",
    "save reference to data",
    "buffer for inner predict",
    "set network if necessary",
    "Prediction task",
    "need reset training data",
    "need to push new valid data",
    "Get name of features",
    "avoid to predict many time in one iteration",
    "Get num of inner evals",
    "Get name of evals",
    "coding: utf-8",
    "pylint: disable = invalid-name, W0105, C0301",
    "Callback environment used by callbacks",
    "coding: utf-8",
    "pylint: disable = invalid-name, C0111",
    "load or create your dataset",
    "create dataset for lightgbm",
    "if you want to re-use data, remember to set free_raw_data=False",
    "specify your configurations as a dict",
    "generate a feature name",
    "feature_name and categorical_feature",
    "check feature name",
    "save model to file",
    "dump model to json (and save to file)",
    "feature names",
    "feature importances",
    "load model to predict",
    "can only predict with the best iteration (or the saving iteration)",
    "eval with loaded model",
    "dump model with pickle",
    "load model with pickle to predict",
    "can predict with any iteration when loaded in pickle way",
    "eval with loaded model",
    "continue training",
    "init_model accepts:",
    "1. model file name",
    "2. Booster()",
    "decay learning rates",
    "learning_rates accepts:",
    "1. list/tuple with length = num_boost_round",
    "2. function(curr_iter)",
    "change other parameters during training",
    "self-defined objective function",
    "f(preds: array, train_data: Dataset) -> grad: array, hess: array",
    "log likelihood loss",
    "self-defined eval metric",
    "f(preds: array, train_data: Dataset) -> name: string, value: array, is_higher_better: bool",
    "binary error",
    "callback",
    "coding: utf-8",
    "pylint: disable = invalid-name, C0111",
    "load or create your dataset",
    "train",
    "predict",
    "eval",
    "feature importances",
    "other scikit-learn modules",
    "coding: utf-8",
    "pylint: disable = invalid-name, C0111",
    "load or create your dataset",
    "create dataset for lightgbm",
    "specify your configurations as a dict",
    "train",
    "coding: utf-8",
    "pylint: disable = invalid-name, C0111",
    "load or create your dataset",
    "create dataset for lightgbm",
    "specify your configurations as a dict",
    "train",
    "save model to file",
    "predict",
    "eval",
    "!/usr/bin/env python3",
    "-*- coding: utf-8 -*-",
    "",
    "LightGBM documentation build configuration file, created by",
    "sphinx-quickstart on Thu May  4 14:30:58 2017.",
    "",
    "This file is execfile()d with the current directory set to its",
    "containing dir.",
    "",
    "Note that not all possible configuration values are present in this",
    "autogenerated file.",
    "",
    "All configuration values have a default; values that are commented out",
    "serve to show the default.",
    "If extensions (or modules to document with autodoc) are in another directory,",
    "add these directories to sys.path here. If the directory is relative to the",
    "documentation root, use os.path.abspath to make it absolute, like shown here.",
    "",
    "-- mock out modules",
    "-- General configuration ------------------------------------------------",
    "If your documentation needs a minimal Sphinx version, state it here.",
    "Add any Sphinx extension module names here, as strings. They can be",
    "extensions coming with Sphinx (named 'sphinx.ext.*') or your custom",
    "ones.",
    "Add any paths that contain templates here, relative to this directory.",
    "The suffix(es) of source filenames.",
    "You can specify multiple suffix as a list of string:",
    "source_suffix = ['.rst', '.md']",
    "The master toctree document.",
    "General information about the project.",
    "The version info for the project you're documenting, acts as replacement for",
    "|version| and |release|, also used in various other places throughout the",
    "built documents.",
    "",
    "The short X.Y version.",
    "The full version, including alpha/beta/rc tags.",
    "The language for content autogenerated by Sphinx. Refer to documentation",
    "for a list of supported languages.",
    "",
    "This is also used if you do content translation via gettext catalogs.",
    "Usually you set \"language\" from the command line for these cases.",
    "List of patterns, relative to source directory, that match files and",
    "directories to ignore when looking for source files.",
    "This patterns also effect to html_static_path and html_extra_path",
    "The name of the Pygments (syntax highlighting) style to use.",
    "If true, `todo` and `todoList` produce output, else they produce nothing.",
    "Both the class' and the __init__ method's docstring are concatenated and inserted.",
    "-- Options for HTML output ----------------------------------------------",
    "The theme to use for HTML and HTML Help pages.  See the documentation for",
    "a list of builtin themes.",
    "",
    "Theme options are theme-specific and customize the look and feel of a theme",
    "further.  For a list of options available for each theme, see the",
    "documentation.",
    "",
    "html_theme_options = {}",
    "Add any paths that contain custom static files (such as style sheets) here,",
    "relative to this directory. They are copied after the builtin static files,",
    "so a file named \"default.css\" will overwrite the builtin \"default.css\".",
    "-- Options for HTMLHelp output ------------------------------------------",
    "Output file base name for HTML help builder.",
    "coding: utf-8",
    "coding: utf-8",
    "pylint: skip-file",
    "we don't need lib_lightgbm while building docs",
    "coding: utf-8",
    "pylint: skip-file",
    "check saved model persistence",
    "we need to check the consistency of model file here, so test for exact equal",
    "check early stopping is working. Make it stop very early, so the scores should be very close to zero",
    "scores likely to be different, but prediction should still be the same",
    "coding: utf-8",
    "pylint: skip-file",
    "Tests that `seed` is the same as `random_state`",
    "sklearn <0.19 cannot accept instance, but many tests could be passed only with min_data=1 and min_data_in_bin=1",
    "we cannot use `check_estimator` directly since there is no skip test mechanism",
    "we cannot leave default params (see https://github.com/Microsoft/LightGBM/issues/833)",
    "coding: utf-8",
    "pylint: skip-file",
    "coding: utf-8",
    "pylint: skip-file",
    "no early stopping",
    "early stopping occurs",
    "test custom eval metrics",
    "shuffle = False, override metric in params",
    "shuffle = True, callbacks",
    "self defined folds",
    "lambdarank",
    "test feature_names with whitespaces",
    "take subsets and train",
    "test sliced labels",
    "append some columns",
    "append some rows",
    "test sliced 2d matrix",
    "test sliced CSR",
    "coding: utf-8",
    "pylint: skip-file",
    "convert from one-based to  zero-based index",
    "convert from boundaries to size",
    "--- start Booster interfaces",
    "create boosting",
    "initialize the boosting",
    "create objective function",
    "initialize the objective function",
    "create training metric",
    "reset the boosting",
    "create objective function",
    "initialize the objective function",
    "some help functions used to convert data",
    "Row iterator of on column for CSC matrix",
    "return value at idx, only can access by ascent order",
    "return next non-zero pair, if index < 0, means no more data",
    "start of c_api functions",
    "sample data first",
    "sample data first",
    "sample data first",
    "no more data",
    "---- start of booster",
    "---- start of some help functions",
    "set number of threads for openmp",
    "check for alias",
    "read parameters from config file",
    "remove str after \"#\"",
    "check for alias again",
    "load configs",
    "prediction is needed if using input initial model(continued train)",
    "need to continue training",
    "sync up random seed for data partition",
    "load Training data",
    "load data for parallel training",
    "load data for single machine",
    "need save binary file",
    "create training metric",
    "only when have metrics then need to construct validation data",
    "Add validation data, if it exists",
    "add",
    "need save binary file",
    "add metric for validation data",
    "output used time on each iteration",
    "need init network",
    "create boosting",
    "create objective function",
    "load training data",
    "initialize the objective function",
    "initialize the boosting",
    "add validation data into boosting",
    "convert model to if-else statement code",
    "create predictor",
    "Free memory",
    "create predictor",
    "counts for all labels",
    "start from top label, and accumulate DCG",
    "counts for all labels",
    "calculate k Max DCG by one pass",
    "get sorted indices by score",
    "calculate dcg",
    "get sorted indices by score",
    "calculate multi dcg by one pass",
    "wait for all client start up",
    "default set to -1",
    "distance at k-th communication, distance[k] = 2^k",
    "set incoming rank at k-th commuication",
    "set outgoing rank at k-th commuication",
    "defalut set as -1",
    "construct all recursive halving map for all machines",
    "let 1 << k <= num_machines",
    "distance of each communication",
    "if num_machines = 2^k, don't need to group machines",
    "communication direction, %2 == 0 is positive",
    "neighbor at k-th communication",
    "receive data block at k-th communication",
    "send data block at k-th communication",
    "if num_machines != 2^k, need to group machines",
    "group, two machine in one group, total \"rest\" groups will have 2 machines.",
    "let left machine as group leader",
    "cache block information for groups, group with 2 machines will have double block size",
    "convert from group to node leader",
    "convert from node to group",
    "meet new group",
    "add block len for this group",
    "calculate the group block start",
    "not need to construct",
    "get receive block informations",
    "accumulate block len",
    "get send block informations",
    "accumulate block len",
    "static member definition",
    "if small package or small count , do it by all gather.(reduce the communication times.)",
    "assign the blocks to every rank.",
    "do reduce scatter",
    "do all gather",
    "assign blocks",
    "need use buffer here, since size of \"output\" is smaller than size after all gather",
    "copy back",
    "assign blocks",
    "start all gather",
    "when num_machines is small and data is large",
    "use output as receive buffer",
    "get current local block size",
    "get out rank",
    "get in rank",
    "get send information",
    "get recv information",
    "send and recv at same time",
    "rotate in-place",
    "use output as receive buffer",
    "get current local block size",
    "get send information",
    "get recv information",
    "send and recv at same time",
    "use output as receive buffer",
    "send and recv at same time",
    "send local data to neighbor first",
    "receive neighbor data first",
    "reduce",
    "get target",
    "get send information",
    "get recv information",
    "send and recv at same time",
    "reduce",
    "send result to neighbor",
    "receive result from neighbor",
    "copy result",
    "start up socket",
    "parse clients from file",
    "get ip list of local machine",
    "get local rank",
    "construct listener",
    "construct communication topo",
    "construct linkers",
    "free listener",
    "set timeout",
    "accept incoming socket",
    "receive rank",
    "add new socket",
    "save ranks that need to connect with",
    "start listener",
    "start connect",
    "let smaller rank connect to larger rank",
    "send local rank",
    "wait for listener",
    "print connected linkers",
    "Get some statistic from 2 line",
    "if only have one line on file",
    "Constructors",
    "Get type tag",
    "Comparisons",
    "This has to be separate, not in Statics, because Json() accesses statics().null.",
    "advance until next line, or end of input",
    "advance until closing tokens",
    "The usual case: non-escaped characters",
    "Handle escapes",
    "Extract 4-byte escape sequence",
    "Explicitly check length of the substring. The following loop",
    "relies on std::string returning the terminating NUL when",
    "accessing str[length]. Checking here reduces brittleness.",
    "JSON specifies that characters outside the BMP shall be encoded as a pair",
    "of 4-hex-digit \\u escapes encoding their surrogate pair components. Check",
    "whether we're in the middle of such a beast: the previous codepoint was an",
    "escaped lead (high) surrogate, and this is a trail (low) surrogate.",
    "Reassemble the two surrogate pairs into one astral-plane character, per",
    "the UTF-16 algorithm.",
    "Integer part",
    "Decimal part",
    "Exponent part",
    "Check for any trailing garbage",
    "Documented in json11.hpp",
    "Check for another object",
    "get column names",
    "load label idx first",
    "erase label column name",
    "load ignore columns",
    "load weight idx",
    "load group idx",
    "don't support query id in data file when training in parallel",
    "read data to memory",
    "sample data",
    "construct feature bin mappers",
    "initialize label",
    "extract features",
    "sample data from file",
    "construct feature bin mappers",
    "initialize label",
    "extract features",
    "load data from binary file",
    "check meta data",
    "need to check training data",
    "read data in memory",
    "initialize label",
    "extract features",
    "Get number of lines of data file",
    "initialize label",
    "extract features",
    "load data from binary file",
    "not need to check validation data",
    "check meta data",
    "buffer to read binary file",
    "check token",
    "read size of header",
    "re-allocmate space if not enough",
    "read header",
    "get header",
    "num_groups",
    "real_feature_idx_",
    "feature2group",
    "feature2subfeature",
    "group_bin_boundaries",
    "group_feature_start_",
    "group_feature_cnt_",
    "get feature names",
    "write feature names",
    "read size of meta data",
    "re-allocate space if not enough",
    "read meta data",
    "load meta data",
    "sample local used data if need to partition",
    "if not contain query file, minimal sample unit is one record",
    "if contain query file, minimal sample unit is one query",
    "if is new query",
    "read feature data",
    "read feature size",
    "re-allocate space if not enough",
    "fill feature_names_ if not header",
    "if only one machine, find bin locally",
    "if have multi-machines, need to find bin distributed",
    "different machines will find bin for different features",
    "start and len will store the process feature indices for different machines",
    "machine i will find bins for features in [ start[i], start[i] + len[i] )",
    "get size of bin mapper with max_bin size",
    "since sizes of different feature may not be same, we expand all bin mapper to type_size",
    "find local feature bins and copy to buffer",
    "free",
    "convert to binary size",
    "gather global feature bin mappers",
    "restore features bins from buffer",
    "---- private functions ----",
    "if features are ordered, not need to use hist_buf",
    "read all lines",
    "get query data",
    "if not contain query data, minimal sample unit is one record",
    "if contain query data, minimal sample unit is one query",
    "if is new query",
    "get query data",
    "if not contain query file, minimal sample unit is one record",
    "if contain query file, minimal sample unit is one query",
    "if is new query",
    "parse features",
    "-1 means doesn't use this feature",
    "check the range of label_idx, weight_idx and group_idx",
    "fill feature_names_ if not header",
    "start find bins",
    "if only one machine, find bin locally",
    "if have multi-machines, need to find bin distributed",
    "different machines will find bin for different features",
    "start and len will store the process feature indices for different machines",
    "machine i will find bins for features in [ start[i], start[i] + len[i] )",
    "get size of bin mapper with max_bin size",
    "since sizes of different feature may not be same, we expand all bin mapper to type_size",
    "find local feature bins and copy to buffer",
    "free",
    "convert to binary size",
    "gather global feature bin mappers",
    "restore features bins from buffer",
    "if doesn't need to prediction with initial model",
    "parser",
    "set label",
    "free processed line:",
    "shrink_to_fit will be very slow in linux, and seems not free memory, disable for now",
    "text_reader_->Lines()[i].shrink_to_fit();",
    "push data",
    "if is used feature",
    "if need to prediction with initial model",
    "parser",
    "set initial score",
    "set label",
    "free processed line:",
    "shrink_to_fit will be very slow in linux, and seems not free memory, disable for now",
    "text_reader_->Lines()[i].shrink_to_fit();",
    "push data",
    "if is used feature",
    "metadata_ will manage space of init_score",
    "text data can be free after loaded feature values",
    "parser",
    "set initial score",
    "set label",
    "push data",
    "if is used feature",
    "only need part of data",
    "need full data",
    "metadata_ will manage space of init_score",
    "read size of token",
    "deep copy function for BinMapper",
    "mean size for one bin",
    "need a new bin",
    "update bin upper bound",
    "last bin upper bound",
    "find distinct_values first",
    "push zero in the front",
    "use the large value",
    "push zero in the back",
    "convert to int type first",
    "sort by counts",
    "avoid first bin is zero",
    "will ignore the categorical of small counts",
    "need an additional bin for NaN",
    "use -1 to represent NaN",
    "Use MissingType::None to represent this bin contains all categoricals",
    "check trival(num_bin_ == 1) feature",
    "check useless bin",
    "calculate sparse rate",
    "sparse threshold",
    "for lambdarank, it needs query data for partition data in parallel learning",
    "need convert query_id to boundaries",
    "check weights",
    "check query boundries",
    "contain initial score file",
    "check weights",
    "get local weights",
    "check query boundries",
    "get local query boundaries",
    "contain initial score file",
    "get local initial scores",
    "re-load query weight",
    "save to nullptr",
    "save to nullptr",
    "save to nullptr",
    "default weight file name",
    "default weight file name",
    "use first line to count number class",
    "default query file name",
    "root is in the depth 0",
    "non-leaf",
    "leaf",
    "use this for the missing value conversion",
    "Predict func by Map to ifelse",
    "use this for the missing value conversion",
    "non-leaf",
    "left subtree",
    "right subtree",
    "leaf",
    "non-leaf",
    "left subtree",
    "right subtree",
    "leaf",
    "recursive computation of SHAP values for a decision tree",
    "extend the unique path",
    "leaf node",
    "internal node",
    "see if we have already split on this feature,",
    "if so we undo that split so we can redo it for this node",
    "clear old metrics",
    "to lower",
    "split",
    "remove duplicate",
    "add names of objective function if not providing metric",
    "load main config types",
    "generate seeds by seed.",
    "sub-config setup",
    "check for conflicts",
    "check if objective_type, metric_type, and num_class match",
    "Change pool size to -1 (no limit) when using data parallel to reduce communication costs",
    "Check max_depth and num_leaves",
    "label_gain = 2^i - 1, may overflow, so we use 31 here",
    "label_gain = 2^i - 1, may overflow, so we use 31 here",
    "default eval ndcg @[1-5]",
    "filter is based on sampling data, so decrease its range",
    "put dense feature first",
    "sort by non zero cnt",
    "sort by non zero cnt, bigger first",
    "take apart small sparse group, due it will not gain on speed",
    "shuffle groups",
    "get num_features",
    "get bin_mappers",
    "copy feature bin mapper data",
    "copy feature bin mapper data",
    "if not pass a filename, just append \".bin\" of original file",
    "get size of header",
    "size of feature names",
    "write header",
    "write feature names",
    "get size of meta data",
    "write meta data",
    "write feature data",
    "get size of feature",
    "write feature",
    "feature is not used",
    "construct histograms for smaller leaf",
    "if not use ordered bin",
    "used ordered bin",
    "feature is not used",
    "construct histograms for smaller leaf",
    "if not use ordered bin",
    "used ordered bin",
    "fixed hessian.",
    "feature is not used",
    "construct histograms for smaller leaf",
    "if not use ordered bin",
    "used ordered bin",
    "feature is not used",
    "construct histograms for smaller leaf",
    "if not use ordered bin",
    "used ordered bin",
    "fixed hessian.",
    "PredictRaw",
    "PredictRawByMap",
    "Predict",
    "PredictByMap",
    "PredictLeafIndex",
    "PredictLeafIndexByMap",
    "output model type",
    "output number of class",
    "output label index",
    "output max_feature_idx",
    "output objective",
    "output tree models",
    "store the importance first",
    "sort the importance",
    "use serialized string to restore this object",
    "Use first 128 chars to avoid exceed the message buffer.",
    "get number of classes",
    "get index of label",
    "get max_feature_idx first",
    "get average_output",
    "get feature names",
    "set zero",
    "predict all the trees for one iteration",
    "check early stopping",
    "set zero",
    "predict all the trees for one iteration",
    "check early stopping",
    "margin_threshold will be captured by value",
    "copy and sort",
    "margin_threshold will be captured by value",
    "load forced_splits file",
    "init tree learner",
    "push training metrics",
    "create buffer for gradients and hessians",
    "get max feature index",
    "get label index",
    "get feature names",
    "if need bagging, create buffer",
    "reset config for tree learner",
    "multi-class",
    "binary class",
    "for a validation dataset, we need its score and metric",
    "update score",
    "objective function will calculate gradients and hessians",
    "random bagging, minimal unit is one record",
    "if need bagging",
    "set bagging data to tree learner",
    "get subset",
    "output used time per iteration",
    "boosting from average label; or customized \"average\" if implemented for the current objective",
    "boosting first",
    "bagging logic",
    "need to copy gradients for bagging subset.",
    "shrinkage by learning rate",
    "update score",
    "only add default score one-time",
    "updates scores",
    "add model",
    "reset score",
    "remove model",
    "print message for metric",
    "pop last early_stopping_round_ models",
    "update training score",
    "we need to predict out-of-bag scores of data for boosting",
    "update validation score",
    "print training metric",
    "print validation metric",
    "set zero",
    "predict all the trees for one iteration",
    "check early stopping",
    "push training metrics",
    "not same training data, need reset score and others",
    "create score tracker",
    "update score",
    "create buffer for gradients and hessians",
    "if need bagging, create buffer",
    "Get the max size of pool",
    "at least need 2 leaves",
    "push split information for all leaves",
    "get ordered bin",
    "check existing for ordered bin",
    "initialize splits for leaf",
    "initialize data partition",
    "initialize ordered gradients and hessians",
    "if has ordered bin, need to allocate a buffer to fast split",
    "get ordered bin",
    "initialize splits for leaf",
    "initialize data partition",
    "initialize ordered gradients and hessians",
    "if has ordered bin, need to allocate a buffer to fast split",
    "Get the max size of pool",
    "at least need 2 leaves",
    "push split information for all leaves",
    "some initial works before training",
    "root leaf",
    "only root leaf can be splitted on first time",
    "some initial works before finding best split",
    "find best threshold for every feature",
    "Get a leaf with max split gain",
    "Get split information for best leaf",
    "cannot split, quit",
    "split tree with best leaf",
    "reset histogram pool",
    "at least use one feature",
    "initialize used features",
    "Get used feature at current tree",
    "initialize data partition",
    "reset the splits for leaves",
    "Sumup for root",
    "use all data",
    "use bagging, only use part of data",
    "if has ordered bin, need to initialize the ordered bin",
    "use all data, pass nullptr",
    "bagging, only use part of data",
    "mark used data",
    "initialize ordered bin",
    "check depth of current leaf",
    "only need to check left leaf, since right leaf is in same level of left leaf",
    "no enough data to continue",
    "only have root",
    "put parent(left) leaf's histograms into larger leaf's histograms",
    "put parent(left) leaf's histograms to larger leaf's histograms",
    "split for the ordered bin",
    "mark data that at left-leaf",
    "split the ordered bin",
    "construct smaller leaf",
    "construct larger leaf",
    "find splits",
    "only has root leaf",
    "find best threshold for larger child",
    "start at root leaf",
    "before processing next node from queue, store info for current left/right leaf",
    "store \"best split\" for left and right, even if they might be overwritten by forced split",
    "then, compute own splits",
    "split info should exist because searching in bfs fashion - should have added from parent",
    "split tree, will return right leaf",
    "left = parent",
    "split tree, will return right leaf",
    "init the leaves that used on next iteration",
    "bag_mapper[index_mapper[i]]",
    "get feature partition",
    "get local used features",
    "get best split at smaller leaf",
    "find local best split for larger leaf",
    "sync global best info",
    "update best split",
    "instantiate template classes, otherwise linker cannot find the code",
    "initialize SerialTreeLearner",
    "Get local rank and global machine size",
    "allocate buffer for communication",
    "generate feature partition for current tree",
    "get local used feature",
    "get block start and block len for reduce scatter",
    "get buffer_write_start_pos_",
    "get buffer_read_start_pos_",
    "sync global data sumup info",
    "global sumup reduce",
    "copy back",
    "set global sumup info",
    "init global data count in leaf",
    "construct local histograms",
    "copy to buffer",
    "Reduce scatter for histogram",
    "restore global histograms from buffer",
    "find best threshold for smaller child",
    "only root leaf",
    "construct histgroms for large leaf, we init larger leaf as the parent, so we can just subtract the smaller leaf's histograms",
    "find best threshold for larger child",
    "find local best split for larger leaf",
    "sync global best info",
    "set best split",
    "need update global number of data in leaf",
    "instantiate template classes, otherwise linker cannot find the code",
    "initialize SerialTreeLearner",
    "some additional variables needed for GPU trainer",
    "Initialize GPU buffers and kernels",
    "some functions used for debugging the GPU histogram construction",
    "printf(\"grad %g != %g (%d ULPs)\\n\", h1[i].sum_gradients, h2[i].sum_gradients, ulps);",
    "goto err;",
    "printf(\"hessian %g != %g (%d ULPs)\\n\", h1[i].sum_hessians, h2[i].sum_hessians, ulps);",
    "goto err;",
    "we roughly want 256 workgroups per device, and we have num_dense_feature4_ feature tuples.",
    "also guarantee that there are at least 2K examples per workgroup",
    "return 0;",
    "we have already copied ordered gradients, ordered hessians and indices to GPU",
    "decide the best number of workgroups working on one feature4 tuple",
    "set work group size based on feature size",
    "each 2^exp_workgroups_per_feature workgroups work on a feature4 tuple",
    "we need to refresh the kernel arguments after reallocating",
    "The only argument that needs to be changed later is num_data_",
    "the GPU kernel will process all features in one call, and each",
    "2^exp_workgroups_per_feature (compile time constant) workgroup will",
    "process one feature4 tuple",
    "for the root node, indices are not copied",
    "for constant hessian, hessians are not copied except for the root node",
    "there will be 2^exp_workgroups_per_feature = num_workgroups / num_dense_feature4 sub-histogram per feature4",
    "and we will launch num_feature workgroups for this kernel",
    "will launch threads for all features",
    "the queue should be asynchrounous, and we will can WaitAndGetHistograms() before we start processing dense feature groups",
    "copy the results asynchronously. Size depends on if double precision is used",
    "we will wait for this object in WaitAndGetHistograms",
    "when the output is ready, the computation is done",
    "values of this feature has been redistributed to multiple bins; need a reduction here",
    "how many feature-group tuples we have",
    "leave some safe margin for prefetching",
    "256 work-items per workgroup. Each work-item prefetches one tuple for that feature",
    "clear sparse/dense maps",
    "do nothing if no features can be processed on GPU",
    "allocate memory for all features (FIXME: 4 GB barrier on some devices, need to split to multiple buffers)",
    "unpin old buffer if necessary before destructing them",
    "make ordered_gradients and hessians larger (including extra room for prefetching), and pin them",
    "allocate space for gradients and hessians on device",
    "we will copy gradients and hessians in after ordered_gradients_ and ordered_hessians_ are constructed",
    "allocate feature mask, for disabling some feature-groups' histogram calculation",
    "copy indices to the device",
    "histogram bin entry size depends on the precision (single/double)",
    "create output buffer, each feature has a histogram with device_bin_size_ bins,",
    "each work group generates a sub-histogram of dword_features_ features.",
    "only initialize once here, as this will not need to change when ResetTrainingData() is called",
    "create atomic counters for inter-group coordination",
    "The output buffer is allocated to host directly, to overlap compute and data transfer",
    "find the dense feature-groups and group then into Feature4 data structure (several feature-groups packed into 4 bytes)",
    "looking for dword_features_ non-sparse feature-groups",
    "decide if we need to redistribute the bin",
    "multiplier must be a power of 2",
    "device_bin_mults_.push_back(1);",
    "found",
    "for data transfer time",
    "Now generate new data structure feature4, and copy data to the device",
    "preallocate arrays for all threads, and pin them",
    "building Feature4 bundles; each thread handles dword_features_ features",
    "one feature datapoint is 4 bits",
    "this guarantees that the RawGet() function is inlined, rather than using virtual function dispatching",
    "one feature datapoint is one byte",
    "this guarantees that the RawGet() function is inlined, rather than using virtual function dispatching",
    "Dense bin",
    "Dense 4-bit bin",
    "working on the remaining (less than dword_features_) feature groups",
    "fill the leftover features",
    "fill this empty feature with some \"random\" value",
    "fill this empty feature with some \"random\" value",
    "copying the last 1 to (dword_features - 1) feature-groups in the last tuple",
    "deallocate pinned space for feature copying",
    "data transfer time",
    "for other types of failure, build log might not be available; program.build_log() can crash",
    "Something bad happened. Just return \"No log available.\"",
    "build is okay, log may contain warnings",
    "destroy any old kernels",
    "create OpenCL kernels for different number of workgroups per feature",
    "currently we don't use constant memory",
    "compile the GPU kernel depending if double precision is used, constant hessian is used, etc",
    "kernel with indices in an array",
    "kernel with all features enabled, with elimited branches",
    "kernel with all data indices (for root node, and assumes that root node always uses all features)",
    "do nothing if no features can be processed on GPU",
    "The only argument that needs to be changed later is num_data_",
    "hessian is passed as a parameter, but it is not available now.",
    "hessian will be set in BeforeTrain()",
    "Get the max bin size, used for selecting best GPU kernel",
    "initialize GPU",
    "determine which kernel to use based on the max number of bins",
    "setup GPU kernel arguments after we allocating all the buffers",
    "check if we need to recompile the GPU kernel (is_constant_hessian changed)",
    "this should rarely occur",
    "GPU memory has to been reallocated because data may have been changed",
    "setup GPU kernel arguments after we allocating all the buffers",
    "Copy initial full hessians and gradients to GPU.",
    "We start copying as early as possible, instead of at ConstructHistogram().",
    "setup hessian parameters only",
    "hessian is passed as a parameter",
    "use bagging",
    "On GPU, we start copying indices, gradients and hessians now, instead at ConstructHistogram()",
    "copy used gradients and hessians to ordered buffer",
    "transfer the indices to GPU",
    "transfer hessian to GPU",
    "setup hessian parameters only",
    "hessian is passed as a parameter",
    "transfer gradients to GPU",
    "only have root",
    "Copy indices, gradients and hessians as early as possible",
    "only need to initialize for smaller leaf",
    "Get leaf boundary",
    "copy indices to the GPU:",
    "copy ordered hessians to the GPU:",
    "copy ordered gradients to the GPU:",
    "do nothing if no features can be processed on GPU",
    "copy data indices if it is not null",
    "generate and copy ordered_gradients if gradients is not null",
    "generate and copy ordered_hessians if hessians is not null",
    "converted indices in is_feature_used to feature-group indices",
    "construct the feature masks for dense feature-groups",
    "if no feature group is used, just return and do not use GPU",
    "if not all feature groups are used, we need to transfer the feature mask to GPU",
    "otherwise, we will use a specialized GPU kernel with all feature groups enabled",
    "All data have been prepared, now run the GPU kernel",
    "construct smaller leaf",
    "ConstructGPUHistogramsAsync will return true if there are availabe feature gourps dispatched to GPU",
    "then construct sparse features on CPU",
    "We set data_indices to null to avoid rebuilding ordered gradients/hessians",
    "wait for GPU to finish, only if GPU is actually used",
    "use double precision",
    "use single precision",
    "Compare GPU histogram with CPU histogram, useful for debuggin GPU code problem",
    "#define GPU_DEBUG_COMPARE",
    "construct larger leaf",
    "then construct sparse features on CPU",
    "We set data_indices to null to avoid rebuilding ordered gradients/hessians",
    "wait for GPU to finish, only if GPU is actually used",
    "use double precision",
    "use single precision",
    "do some sanity check for the GPU algorithm",
    "limit top k",
    "get max bin",
    "calculate buffer size",
    "left and right on same time, so need double size",
    "initialize histograms for global",
    "sync global data sumup info",
    "set global sumup info",
    "init global data count in leaf",
    "get local sumup",
    "get local sumup",
    "get mean number on machines",
    "weighted gain",
    "get top k",
    "Copy histogram to buffer, and Get local aggregate features",
    "copy histograms.",
    "copy smaller leaf histograms first",
    "mark local aggregated feature",
    "copy",
    "then copy larger leaf histograms",
    "mark local aggregated feature",
    "copy",
    "use local data to find local best splits",
    "find splits",
    "only has root leaf",
    "find best threshold for larger child",
    "local voting",
    "gather",
    "get all top-k from all machines",
    "global voting",
    "copy local histgrams to buffer",
    "Reduce scatter for histogram",
    "find best split from local aggregated histograms",
    "restore from buffer",
    "find best threshold",
    "restore from buffer",
    "find best threshold",
    "find local best",
    "find local best split for larger leaf",
    "sync global best info",
    "copy back",
    "set the global number of data for leaves",
    "init the global sumup info",
    "instantiate template classes, otherwise linker cannot find the code"
  ],
  "v2.1.0": [
    "coding: utf-8",
    "pylint: disable=invalid-name, exec-used, C0111",
    "coding: utf-8",
    "pylint: disable = invalid-name, W0105",
    "create predictor first",
    "check dataset",
    "reduce cost for prediction training data",
    "process callbacks",
    "Most of legacy advanced options becomes callbacks",
    "construct booster",
    "start training",
    "check evaluation result.",
    "lambdarank task, split according to groups",
    "run preprocessing on the data set if needed",
    "setup callbacks",
    "coding: utf-8",
    "pylint: disable = C0103",
    "simplejson does not support Python 3.2, it throws a SyntaxError",
    "because of u'...' Unicode literals.",
    "DeprecationWarning is not shown by default, so let's create our own with higher level",
    "coding: utf-8",
    "pylint: disable = invalid-name, W0105, C0111, C0301",
    "minor change to support `**kwargs`",
    "sklearn interface has another naming convention",
    "user can set verbose with kwargs, it has higher priority",
    "reduce cost for prediction training data",
    "free dataset",
    "Switch to using a multiclass objective in the underlying LGBM instance",
    "check group data",
    "coding: utf-8",
    "we don't need lib_lightgbm while building docs",
    "coding: utf-8",
    "pylint: disable = C0103",
    "coding: utf-8",
    "coding: utf-8",
    "pylint: disable = invalid-name, C0111, C0301",
    "pylint: disable = R0912, R0913, R0914, W0105, W0201, W0212",
    "TypeError: obj is not a string or a number",
    "ValueError: invalid literal",
    "process for args",
    "get categorical features",
    "process for reference dataset",
    "start construct data",
    "check data has header or not",
    "load init score",
    "need re group init score",
    "set feature names",
    "change non-float data to float data, need to copy",
    "create valid",
    "construct subset",
    "create train",
    "set to None",
    "we're done if self and reference share a common upstrem reference",
    "group data from LightGBM is boundaries data, need to convert to group size",
    "Training task",
    "construct booster object",
    "save reference to data",
    "buffer for inner predict",
    "set network if necessary",
    "Prediction task",
    "need reset training data",
    "need to push new valid data",
    "Get name of features",
    "avoid to predict many time in one iteration",
    "Get num of inner evals",
    "Get name of evals",
    "coding: utf-8",
    "pylint: disable = invalid-name, W0105, C0301",
    "Callback environment used by callbacks",
    "coding: utf-8",
    "pylint: disable = invalid-name, C0111",
    "load or create your dataset",
    "create dataset for lightgbm",
    "if you want to re-use data, remember to set free_raw_data=False",
    "specify your configurations as a dict",
    "generate a feature name",
    "feature_name and categorical_feature",
    "check feature name",
    "save model to file",
    "dump model to json (and save to file)",
    "feature names",
    "feature importances",
    "load model to predict",
    "can only predict with the best iteration (or the saving iteration)",
    "eval with loaded model",
    "dump model with pickle",
    "load model with pickle to predict",
    "can predict with any iteration when loaded in pickle way",
    "eval with loaded model",
    "continue training",
    "init_model accepts:",
    "1. model file name",
    "2. Booster()",
    "decay learning rates",
    "learning_rates accepts:",
    "1. list/tuple with length = num_boost_round",
    "2. function(curr_iter)",
    "change other parameters during training",
    "self-defined objective function",
    "f(preds: array, train_data: Dataset) -> grad: array, hess: array",
    "log likelihood loss",
    "self-defined eval metric",
    "f(preds: array, train_data: Dataset) -> name: string, value: array, is_higher_better: bool",
    "binary error",
    "callback",
    "coding: utf-8",
    "pylint: disable = invalid-name, C0111",
    "load or create your dataset",
    "train",
    "predict",
    "eval",
    "feature importances",
    "other scikit-learn modules",
    "coding: utf-8",
    "pylint: disable = invalid-name, C0111",
    "load or create your dataset",
    "create dataset for lightgbm",
    "specify your configurations as a dict",
    "train",
    "coding: utf-8",
    "pylint: disable = invalid-name, C0111",
    "load or create your dataset",
    "create dataset for lightgbm",
    "specify your configurations as a dict",
    "train",
    "save model to file",
    "predict",
    "eval",
    "!/usr/bin/env python3",
    "-*- coding: utf-8 -*-",
    "",
    "LightGBM documentation build configuration file, created by",
    "sphinx-quickstart on Thu May  4 14:30:58 2017.",
    "",
    "This file is execfile()d with the current directory set to its",
    "containing dir.",
    "",
    "Note that not all possible configuration values are present in this",
    "autogenerated file.",
    "",
    "All configuration values have a default; values that are commented out",
    "serve to show the default.",
    "If extensions (or modules to document with autodoc) are in another directory,",
    "add these directories to sys.path here. If the directory is relative to the",
    "documentation root, use os.path.abspath to make it absolute, like shown here.",
    "",
    "-- mock out modules",
    "-- General configuration ------------------------------------------------",
    "If your documentation needs a minimal Sphinx version, state it here.",
    "Add any Sphinx extension module names here, as strings. They can be",
    "extensions coming with Sphinx (named 'sphinx.ext.*') or your custom",
    "ones.",
    "Add any paths that contain templates here, relative to this directory.",
    "The suffix(es) of source filenames.",
    "You can specify multiple suffix as a list of string:",
    "source_suffix = ['.rst', '.md']",
    "The master toctree document.",
    "General information about the project.",
    "The version info for the project you're documenting, acts as replacement for",
    "|version| and |release|, also used in various other places throughout the",
    "built documents.",
    "",
    "The short X.Y version.",
    "The full version, including alpha/beta/rc tags.",
    "The language for content autogenerated by Sphinx. Refer to documentation",
    "for a list of supported languages.",
    "",
    "This is also used if you do content translation via gettext catalogs.",
    "Usually you set \"language\" from the command line for these cases.",
    "List of patterns, relative to source directory, that match files and",
    "directories to ignore when looking for source files.",
    "This patterns also effect to html_static_path and html_extra_path",
    "The name of the Pygments (syntax highlighting) style to use.",
    "If true, `todo` and `todoList` produce output, else they produce nothing.",
    "Both the class' and the __init__ method's docstring are concatenated and inserted.",
    "-- Options for HTML output ----------------------------------------------",
    "The theme to use for HTML and HTML Help pages.  See the documentation for",
    "a list of builtin themes.",
    "",
    "Theme options are theme-specific and customize the look and feel of a theme",
    "further.  For a list of options available for each theme, see the",
    "documentation.",
    "",
    "html_theme_options = {}",
    "Add any paths that contain custom static files (such as style sheets) here,",
    "relative to this directory. They are copied after the builtin static files,",
    "so a file named \"default.css\" will overwrite the builtin \"default.css\".",
    "-- Options for HTMLHelp output ------------------------------------------",
    "Output file base name for HTML help builder.",
    "coding: utf-8",
    "coding: utf-8",
    "pylint: skip-file",
    "we don't need lib_lightgbm while building docs",
    "coding: utf-8",
    "pylint: skip-file",
    "check saved model persistence",
    "we need to check the consistency of model file here, so test for exact equal",
    "check early stopping is working. Make it stop very early, so the scores should be very close to zero",
    "scores likely to be different, but prediction should still be the same",
    "coding: utf-8",
    "pylint: skip-file",
    "Tests that `seed` is the same as `random_state`",
    "sklearn <0.19 cannot accept instance, but many tests could be passed only with min_data=1 and min_data_in_bin=1",
    "we cannot use `check_estimator` directly since there is no skip test mechanism",
    "we cannot leave default params (see https://github.com/Microsoft/LightGBM/issues/833)",
    "coding: utf-8",
    "pylint: skip-file",
    "coding: utf-8",
    "pylint: skip-file",
    "no early stopping",
    "early stopping occurs",
    "test custom eval metrics",
    "shuffle = False, override metric in params",
    "shuffle = True, callbacks",
    "self defined folds",
    "lambdarank",
    "test feature_names with whitespaces",
    "take subsets and train",
    "test sliced labels",
    "append some columns",
    "append some rows",
    "test sliced 2d matrix",
    "test sliced CSR",
    "coding: utf-8",
    "pylint: skip-file",
    "convert from one-based to  zero-based index",
    "convert from boundaries to size",
    "--- start Booster interfaces",
    "create boosting",
    "initialize the boosting",
    "create objective function",
    "initialize the objective function",
    "create training metric",
    "reset the boosting",
    "create objective function",
    "initialize the objective function",
    "some help functions used to convert data",
    "Row iterator of on column for CSC matrix",
    "return value at idx, only can access by ascent order",
    "return next non-zero pair, if index < 0, means no more data",
    "start of c_api functions",
    "sample data first",
    "sample data first",
    "sample data first",
    "no more data",
    "---- start of booster",
    "---- start of some help functions",
    "set number of threads for openmp",
    "check for alias",
    "read parameters from config file",
    "remove str after \"#\"",
    "check for alias again",
    "load configs",
    "prediction is needed if using input initial model(continued train)",
    "need to continue training",
    "sync up random seed for data partition",
    "load Training data",
    "load data for parallel training",
    "load data for single machine",
    "need save binary file",
    "create training metric",
    "only when have metrics then need to construct validation data",
    "Add validation data, if it exists",
    "add",
    "need save binary file",
    "add metric for validation data",
    "output used time on each iteration",
    "need init network",
    "create boosting",
    "create objective function",
    "load training data",
    "initialize the objective function",
    "initialize the boosting",
    "add validation data into boosting",
    "convert model to if-else statement code",
    "create predictor",
    "Free memory",
    "create predictor",
    "counts for all labels",
    "start from top label, and accumulate DCG",
    "counts for all labels",
    "calculate k Max DCG by one pass",
    "get sorted indices by score",
    "calculate dcg",
    "get sorted indices by score",
    "calculate multi dcg by one pass",
    "wait for all client start up",
    "default set to -1",
    "distance at k-th communication, distance[k] = 2^k",
    "set incoming rank at k-th commuication",
    "set outgoing rank at k-th commuication",
    "defalut set as -1",
    "construct all recursive halving map for all machines",
    "let 1 << k <= num_machines",
    "distance of each communication",
    "if num_machines = 2^k, don't need to group machines",
    "communication direction, %2 == 0 is positive",
    "neighbor at k-th communication",
    "receive data block at k-th communication",
    "send data block at k-th communication",
    "if num_machines != 2^k, need to group machines",
    "group, two machine in one group, total \"rest\" groups will have 2 machines.",
    "let left machine as group leader",
    "cache block information for groups, group with 2 machines will have double block size",
    "convert from group to node leader",
    "convert from node to group",
    "meet new group",
    "add block len for this group",
    "calculate the group block start",
    "not need to construct",
    "get receive block informations",
    "accumulate block len",
    "get send block informations",
    "accumulate block len",
    "static member definition",
    "if small package or small count , do it by all gather.(reduce the communication times.)",
    "assign the blocks to every rank.",
    "do reduce scatter",
    "do all gather",
    "assign blocks",
    "need use buffer here, since size of \"output\" is smaller than size after all gather",
    "copy back",
    "assign blocks",
    "start all gather",
    "when num_machines is small and data is large",
    "use output as receive buffer",
    "get current local block size",
    "get out rank",
    "get in rank",
    "get send information",
    "get recv information",
    "send and recv at same time",
    "rotate in-place",
    "use output as receive buffer",
    "get current local block size",
    "get send information",
    "get recv information",
    "send and recv at same time",
    "use output as receive buffer",
    "send and recv at same time",
    "send local data to neighbor first",
    "receive neighbor data first",
    "reduce",
    "get target",
    "get send information",
    "get recv information",
    "send and recv at same time",
    "reduce",
    "send result to neighbor",
    "receive result from neighbor",
    "copy result",
    "start up socket",
    "parse clients from file",
    "get ip list of local machine",
    "get local rank",
    "construct listener",
    "construct communication topo",
    "construct linkers",
    "free listener",
    "set timeout",
    "accept incoming socket",
    "receive rank",
    "add new socket",
    "save ranks that need to connect with",
    "start listener",
    "start connect",
    "let smaller rank connect to larger rank",
    "send local rank",
    "wait for listener",
    "print connected linkers",
    "Get some statistic from 2 line",
    "if only have one line on file",
    "get column names",
    "load label idx first",
    "erase label column name",
    "load ignore columns",
    "load weight idx",
    "load group idx",
    "don't support query id in data file when training in parallel",
    "read data to memory",
    "sample data",
    "construct feature bin mappers",
    "initialize label",
    "extract features",
    "sample data from file",
    "construct feature bin mappers",
    "initialize label",
    "extract features",
    "load data from binary file",
    "check meta data",
    "need to check training data",
    "read data in memory",
    "initialize label",
    "extract features",
    "Get number of lines of data file",
    "initialize label",
    "extract features",
    "load data from binary file",
    "not need to check validation data",
    "check meta data",
    "buffer to read binary file",
    "check token",
    "read size of header",
    "re-allocmate space if not enough",
    "read header",
    "get header",
    "num_groups",
    "real_feature_idx_",
    "feature2group",
    "feature2subfeature",
    "group_bin_boundaries",
    "group_feature_start_",
    "group_feature_cnt_",
    "get feature names",
    "write feature names",
    "read size of meta data",
    "re-allocate space if not enough",
    "read meta data",
    "load meta data",
    "sample local used data if need to partition",
    "if not contain query file, minimal sample unit is one record",
    "if contain query file, minimal sample unit is one query",
    "if is new query",
    "read feature data",
    "read feature size",
    "re-allocate space if not enough",
    "fill feature_names_ if not header",
    "if only one machine, find bin locally",
    "if have multi-machines, need to find bin distributed",
    "different machines will find bin for different features",
    "start and len will store the process feature indices for different machines",
    "machine i will find bins for features in [ start[i], start[i] + len[i] )",
    "get size of bin mapper with max_bin size",
    "since sizes of different feature may not be same, we expand all bin mapper to type_size",
    "find local feature bins and copy to buffer",
    "free",
    "convert to binary size",
    "gather global feature bin mappers",
    "restore features bins from buffer",
    "---- private functions ----",
    "if features are ordered, not need to use hist_buf",
    "read all lines",
    "get query data",
    "if not contain query data, minimal sample unit is one record",
    "if contain query data, minimal sample unit is one query",
    "if is new query",
    "get query data",
    "if not contain query file, minimal sample unit is one record",
    "if contain query file, minimal sample unit is one query",
    "if is new query",
    "parse features",
    "-1 means doesn't use this feature",
    "check the range of label_idx, weight_idx and group_idx",
    "fill feature_names_ if not header",
    "start find bins",
    "if only one machine, find bin locally",
    "if have multi-machines, need to find bin distributed",
    "different machines will find bin for different features",
    "start and len will store the process feature indices for different machines",
    "machine i will find bins for features in [ start[i], start[i] + len[i] )",
    "get size of bin mapper with max_bin size",
    "since sizes of different feature may not be same, we expand all bin mapper to type_size",
    "find local feature bins and copy to buffer",
    "free",
    "convert to binary size",
    "gather global feature bin mappers",
    "restore features bins from buffer",
    "if doesn't need to prediction with initial model",
    "parser",
    "set label",
    "free processed line:",
    "shrink_to_fit will be very slow in linux, and seems not free memory, disable for now",
    "text_reader_->Lines()[i].shrink_to_fit();",
    "push data",
    "if is used feature",
    "if need to prediction with initial model",
    "parser",
    "set initial score",
    "set label",
    "free processed line:",
    "shrink_to_fit will be very slow in linux, and seems not free memory, disable for now",
    "text_reader_->Lines()[i].shrink_to_fit();",
    "push data",
    "if is used feature",
    "metadata_ will manage space of init_score",
    "text data can be free after loaded feature values",
    "parser",
    "set initial score",
    "set label",
    "push data",
    "if is used feature",
    "only need part of data",
    "need full data",
    "metadata_ will manage space of init_score",
    "read size of token",
    "deep copy function for BinMapper",
    "mean size for one bin",
    "need a new bin",
    "update bin upper bound",
    "last bin upper bound",
    "find distinct_values first",
    "push zero in the front",
    "use the large value",
    "push zero in the back",
    "convert to int type first",
    "sort by counts",
    "avoid first bin is zero",
    "will ignore the categorical of small counts",
    "need an additional bin for NaN",
    "use -1 to represent NaN",
    "Use MissingType::None to represent this bin contains all categoricals",
    "check trival(num_bin_ == 1) feature",
    "check useless bin",
    "calculate sparse rate",
    "sparse threshold",
    "for lambdarank, it needs query data for partition data in parallel learning",
    "need convert query_id to boundaries",
    "check weights",
    "check query boundries",
    "contain initial score file",
    "check weights",
    "get local weights",
    "check query boundries",
    "get local query boundaries",
    "contain initial score file",
    "get local initial scores",
    "re-load query weight",
    "save to nullptr",
    "save to nullptr",
    "save to nullptr",
    "default weight file name",
    "default weight file name",
    "use first line to count number class",
    "default query file name",
    "root is in the depth 0",
    "non-leaf",
    "leaf",
    "use this for the missing value conversion",
    "Predict func by Map to ifelse",
    "use this for the missing value conversion",
    "non-leaf",
    "left subtree",
    "right subtree",
    "leaf",
    "non-leaf",
    "left subtree",
    "right subtree",
    "leaf",
    "recursive computation of SHAP values for a decision tree",
    "extend the unique path",
    "leaf node",
    "internal node",
    "see if we have already split on this feature,",
    "if so we undo that split so we can redo it for this node",
    "clear old metrics",
    "to lower",
    "split",
    "remove duplicate",
    "add names of objective function if not providing metric",
    "load main config types",
    "generate seeds by seed.",
    "sub-config setup",
    "check for conflicts",
    "check if objective_type, metric_type, and num_class match",
    "Change pool size to -1 (no limit) when using data parallel to reduce communication costs",
    "Check max_depth and num_leaves",
    "label_gain = 2^i - 1, may overflow, so we use 31 here",
    "label_gain = 2^i - 1, may overflow, so we use 31 here",
    "default eval ndcg @[1-5]",
    "filter is based on sampling data, so decrease its range",
    "put dense feature first",
    "sort by non zero cnt",
    "sort by non zero cnt, bigger first",
    "take apart small sparse group, due it will not gain on speed",
    "shuffle groups",
    "get num_features",
    "get bin_mappers",
    "copy feature bin mapper data",
    "copy feature bin mapper data",
    "if not pass a filename, just append \".bin\" of original file",
    "get size of header",
    "size of feature names",
    "write header",
    "write feature names",
    "get size of meta data",
    "write meta data",
    "write feature data",
    "get size of feature",
    "write feature",
    "feature is not used",
    "construct histograms for smaller leaf",
    "if not use ordered bin",
    "used ordered bin",
    "feature is not used",
    "construct histograms for smaller leaf",
    "if not use ordered bin",
    "used ordered bin",
    "fixed hessian.",
    "feature is not used",
    "construct histograms for smaller leaf",
    "if not use ordered bin",
    "used ordered bin",
    "feature is not used",
    "construct histograms for smaller leaf",
    "if not use ordered bin",
    "used ordered bin",
    "fixed hessian.",
    "PredictRaw",
    "PredictRawByMap",
    "Predict",
    "PredictByMap",
    "PredictLeafIndex",
    "PredictLeafIndexByMap",
    "output model type",
    "output number of class",
    "output label index",
    "output max_feature_idx",
    "output objective",
    "output tree models",
    "store the importance first",
    "sort the importance",
    "use serialized string to restore this object",
    "Use first 128 chars to avoid exceed the message buffer.",
    "get number of classes",
    "get index of label",
    "get max_feature_idx first",
    "get average_output",
    "get feature names",
    "set zero",
    "predict all the trees for one iteration",
    "check early stopping",
    "set zero",
    "predict all the trees for one iteration",
    "check early stopping",
    "margin_threshold will be captured by value",
    "copy and sort",
    "margin_threshold will be captured by value",
    "init tree learner",
    "push training metrics",
    "create buffer for gradients and hessians",
    "get max feature index",
    "get label index",
    "get feature names",
    "if need bagging, create buffer",
    "reset config for tree learner",
    "multi-class",
    "binary class",
    "for a validation dataset, we need its score and metric",
    "update score",
    "objective function will calculate gradients and hessians",
    "random bagging, minimal unit is one record",
    "if need bagging",
    "set bagging data to tree learner",
    "get subset",
    "output used time per iteration",
    "boosting from average label; or customized \"average\" if implemented for the current objective",
    "boosting first",
    "bagging logic",
    "need to copy gradients for bagging subset.",
    "shrinkage by learning rate",
    "update score",
    "only add default score one-time",
    "updates scores",
    "add model",
    "reset score",
    "remove model",
    "print message for metric",
    "pop last early_stopping_round_ models",
    "update training score",
    "we need to predict out-of-bag scores of data for boosting",
    "update validation score",
    "print training metric",
    "print validation metric",
    "set zero",
    "predict all the trees for one iteration",
    "check early stopping",
    "push training metrics",
    "not same training data, need reset score and others",
    "create score tracker",
    "update score",
    "create buffer for gradients and hessians",
    "if need bagging, create buffer",
    "Get the max size of pool",
    "at least need 2 leaves",
    "push split information for all leaves",
    "get ordered bin",
    "check existing for ordered bin",
    "initialize splits for leaf",
    "initialize data partition",
    "initialize ordered gradients and hessians",
    "if has ordered bin, need to allocate a buffer to fast split",
    "get ordered bin",
    "initialize splits for leaf",
    "initialize data partition",
    "initialize ordered gradients and hessians",
    "if has ordered bin, need to allocate a buffer to fast split",
    "Get the max size of pool",
    "at least need 2 leaves",
    "push split information for all leaves",
    "some initial works before training",
    "root leaf",
    "only root leaf can be splitted on first time",
    "some initial works before finding best split",
    "find best threshold for every feature",
    "Get a leaf with max split gain",
    "Get split information for best leaf",
    "cannot split, quit",
    "split tree with best leaf",
    "reset histogram pool",
    "at least use one feature",
    "initialize used features",
    "Get used feature at current tree",
    "initialize data partition",
    "reset the splits for leaves",
    "Sumup for root",
    "use all data",
    "use bagging, only use part of data",
    "if has ordered bin, need to initialize the ordered bin",
    "use all data, pass nullptr",
    "bagging, only use part of data",
    "mark used data",
    "initialize ordered bin",
    "check depth of current leaf",
    "only need to check left leaf, since right leaf is in same level of left leaf",
    "no enough data to continue",
    "only have root",
    "put parent(left) leaf's histograms into larger leaf's histograms",
    "put parent(left) leaf's histograms to larger leaf's histograms",
    "split for the ordered bin",
    "mark data that at left-leaf",
    "split the ordered bin",
    "construct smaller leaf",
    "construct larger leaf",
    "find splits",
    "only has root leaf",
    "find best threshold for larger child",
    "left = parent",
    "split tree, will return right leaf",
    "init the leaves that used on next iteration",
    "bag_mapper[index_mapper[i]]",
    "get feature partition",
    "get local used features",
    "get best split at smaller leaf",
    "find local best split for larger leaf",
    "sync global best info",
    "update best split",
    "instantiate template classes, otherwise linker cannot find the code",
    "initialize SerialTreeLearner",
    "Get local rank and global machine size",
    "allocate buffer for communication",
    "generate feature partition for current tree",
    "get local used feature",
    "get block start and block len for reduce scatter",
    "get buffer_write_start_pos_",
    "get buffer_read_start_pos_",
    "sync global data sumup info",
    "global sumup reduce",
    "copy back",
    "set global sumup info",
    "init global data count in leaf",
    "construct local histograms",
    "copy to buffer",
    "Reduce scatter for histogram",
    "restore global histograms from buffer",
    "find best threshold for smaller child",
    "only root leaf",
    "construct histgroms for large leaf, we init larger leaf as the parent, so we can just subtract the smaller leaf's histograms",
    "find best threshold for larger child",
    "find local best split for larger leaf",
    "sync global best info",
    "set best split",
    "need update global number of data in leaf",
    "instantiate template classes, otherwise linker cannot find the code",
    "initialize SerialTreeLearner",
    "some additional variables needed for GPU trainer",
    "Initialize GPU buffers and kernels",
    "some functions used for debugging the GPU histogram construction",
    "printf(\"grad %g != %g (%d ULPs)\\n\", h1[i].sum_gradients, h2[i].sum_gradients, ulps);",
    "goto err;",
    "printf(\"hessian %g != %g (%d ULPs)\\n\", h1[i].sum_hessians, h2[i].sum_hessians, ulps);",
    "goto err;",
    "we roughly want 256 workgroups per device, and we have num_dense_feature4_ feature tuples.",
    "also guarantee that there are at least 2K examples per workgroup",
    "return 0;",
    "we have already copied ordered gradients, ordered hessians and indices to GPU",
    "decide the best number of workgroups working on one feature4 tuple",
    "set work group size based on feature size",
    "each 2^exp_workgroups_per_feature workgroups work on a feature4 tuple",
    "we need to refresh the kernel arguments after reallocating",
    "The only argument that needs to be changed later is num_data_",
    "the GPU kernel will process all features in one call, and each",
    "2^exp_workgroups_per_feature (compile time constant) workgroup will",
    "process one feature4 tuple",
    "for the root node, indices are not copied",
    "for constant hessian, hessians are not copied except for the root node",
    "there will be 2^exp_workgroups_per_feature = num_workgroups / num_dense_feature4 sub-histogram per feature4",
    "and we will launch num_feature workgroups for this kernel",
    "will launch threads for all features",
    "the queue should be asynchrounous, and we will can WaitAndGetHistograms() before we start processing dense feature groups",
    "copy the results asynchronously. Size depends on if double precision is used",
    "we will wait for this object in WaitAndGetHistograms",
    "when the output is ready, the computation is done",
    "values of this feature has been redistributed to multiple bins; need a reduction here",
    "how many feature-group tuples we have",
    "leave some safe margin for prefetching",
    "256 work-items per workgroup. Each work-item prefetches one tuple for that feature",
    "clear sparse/dense maps",
    "do nothing if no features can be processed on GPU",
    "allocate memory for all features (FIXME: 4 GB barrier on some devices, need to split to multiple buffers)",
    "unpin old buffer if necessary before destructing them",
    "make ordered_gradients and hessians larger (including extra room for prefetching), and pin them",
    "allocate space for gradients and hessians on device",
    "we will copy gradients and hessians in after ordered_gradients_ and ordered_hessians_ are constructed",
    "allocate feature mask, for disabling some feature-groups' histogram calculation",
    "copy indices to the device",
    "histogram bin entry size depends on the precision (single/double)",
    "create output buffer, each feature has a histogram with device_bin_size_ bins,",
    "each work group generates a sub-histogram of dword_features_ features.",
    "only initialize once here, as this will not need to change when ResetTrainingData() is called",
    "create atomic counters for inter-group coordination",
    "The output buffer is allocated to host directly, to overlap compute and data transfer",
    "find the dense feature-groups and group then into Feature4 data structure (several feature-groups packed into 4 bytes)",
    "looking for dword_features_ non-sparse feature-groups",
    "decide if we need to redistribute the bin",
    "multiplier must be a power of 2",
    "device_bin_mults_.push_back(1);",
    "found",
    "for data transfer time",
    "Now generate new data structure feature4, and copy data to the device",
    "preallocate arrays for all threads, and pin them",
    "building Feature4 bundles; each thread handles dword_features_ features",
    "one feature datapoint is 4 bits",
    "this guarantees that the RawGet() function is inlined, rather than using virtual function dispatching",
    "one feature datapoint is one byte",
    "this guarantees that the RawGet() function is inlined, rather than using virtual function dispatching",
    "Dense bin",
    "Dense 4-bit bin",
    "working on the remaining (less than dword_features_) feature groups",
    "fill the leftover features",
    "fill this empty feature with some \"random\" value",
    "fill this empty feature with some \"random\" value",
    "copying the last 1 to (dword_features - 1) feature-groups in the last tuple",
    "deallocate pinned space for feature copying",
    "data transfer time",
    "for other types of failure, build log might not be available; program.build_log() can crash",
    "Something bad happened. Just return \"No log available.\"",
    "build is okay, log may contain warnings",
    "destroy any old kernels",
    "create OpenCL kernels for different number of workgroups per feature",
    "currently we don't use constant memory",
    "compile the GPU kernel depending if double precision is used, constant hessian is used, etc",
    "kernel with indices in an array",
    "kernel with all features enabled, with elimited branches",
    "kernel with all data indices (for root node, and assumes that root node always uses all features)",
    "do nothing if no features can be processed on GPU",
    "The only argument that needs to be changed later is num_data_",
    "hessian is passed as a parameter, but it is not available now.",
    "hessian will be set in BeforeTrain()",
    "Get the max bin size, used for selecting best GPU kernel",
    "initialize GPU",
    "determine which kernel to use based on the max number of bins",
    "setup GPU kernel arguments after we allocating all the buffers",
    "check if we need to recompile the GPU kernel (is_constant_hessian changed)",
    "this should rarely occur",
    "GPU memory has to been reallocated because data may have been changed",
    "setup GPU kernel arguments after we allocating all the buffers",
    "Copy initial full hessians and gradients to GPU.",
    "We start copying as early as possible, instead of at ConstructHistogram().",
    "setup hessian parameters only",
    "hessian is passed as a parameter",
    "use bagging",
    "On GPU, we start copying indices, gradients and hessians now, instead at ConstructHistogram()",
    "copy used gradients and hessians to ordered buffer",
    "transfer the indices to GPU",
    "transfer hessian to GPU",
    "setup hessian parameters only",
    "hessian is passed as a parameter",
    "transfer gradients to GPU",
    "only have root",
    "Copy indices, gradients and hessians as early as possible",
    "only need to initialize for smaller leaf",
    "Get leaf boundary",
    "copy indices to the GPU:",
    "copy ordered hessians to the GPU:",
    "copy ordered gradients to the GPU:",
    "do nothing if no features can be processed on GPU",
    "copy data indices if it is not null",
    "generate and copy ordered_gradients if gradients is not null",
    "generate and copy ordered_hessians if hessians is not null",
    "converted indices in is_feature_used to feature-group indices",
    "construct the feature masks for dense feature-groups",
    "if no feature group is used, just return and do not use GPU",
    "if not all feature groups are used, we need to transfer the feature mask to GPU",
    "otherwise, we will use a specialized GPU kernel with all feature groups enabled",
    "All data have been prepared, now run the GPU kernel",
    "construct smaller leaf",
    "ConstructGPUHistogramsAsync will return true if there are availabe feature gourps dispatched to GPU",
    "then construct sparse features on CPU",
    "We set data_indices to null to avoid rebuilding ordered gradients/hessians",
    "wait for GPU to finish, only if GPU is actually used",
    "use double precision",
    "use single precision",
    "Compare GPU histogram with CPU histogram, useful for debuggin GPU code problem",
    "#define GPU_DEBUG_COMPARE",
    "construct larger leaf",
    "then construct sparse features on CPU",
    "We set data_indices to null to avoid rebuilding ordered gradients/hessians",
    "wait for GPU to finish, only if GPU is actually used",
    "use double precision",
    "use single precision",
    "do some sanity check for the GPU algorithm",
    "limit top k",
    "get max bin",
    "calculate buffer size",
    "left and right on same time, so need double size",
    "initialize histograms for global",
    "sync global data sumup info",
    "set global sumup info",
    "init global data count in leaf",
    "get local sumup",
    "get local sumup",
    "get mean number on machines",
    "weighted gain",
    "get top k",
    "Copy histogram to buffer, and Get local aggregate features",
    "copy histograms.",
    "copy smaller leaf histograms first",
    "mark local aggregated feature",
    "copy",
    "then copy larger leaf histograms",
    "mark local aggregated feature",
    "copy",
    "use local data to find local best splits",
    "find splits",
    "only has root leaf",
    "find best threshold for larger child",
    "local voting",
    "gather",
    "get all top-k from all machines",
    "global voting",
    "copy local histgrams to buffer",
    "Reduce scatter for histogram",
    "find best split from local aggregated histograms",
    "restore from buffer",
    "find best threshold",
    "restore from buffer",
    "find best threshold",
    "find local best",
    "find local best split for larger leaf",
    "sync global best info",
    "copy back",
    "set the global number of data for leaves",
    "init the global sumup info",
    "instantiate template classes, otherwise linker cannot find the code"
  ],
  "v2.0.12": [
    "coding: utf-8",
    "pylint: disable=invalid-name, exec-used, C0111",
    "coding: utf-8",
    "pylint: disable = invalid-name, W0105",
    "create predictor first",
    "check dataset",
    "reduce cost for prediction training data",
    "process callbacks",
    "Most of legacy advanced options becomes callbacks",
    "construct booster",
    "start training",
    "check evaluation result.",
    "lambdarank task, split according to groups",
    "run preprocessing on the data set if needed",
    "setup callbacks",
    "coding: utf-8",
    "pylint: disable = C0103",
    "simplejson does not support Python 3.2, it throws a SyntaxError",
    "because of u'...' Unicode literals.",
    "DeprecationWarning is not shown by default, so let's create our own with higher level",
    "coding: utf-8",
    "pylint: disable = invalid-name, W0105, C0111, C0301",
    "minor change to support `**kwargs`",
    "sklearn interface has another naming convention",
    "user can set verbose with kwargs, it has higher priority",
    "reduce cost for prediction training data",
    "free dataset",
    "Switch to using a multiclass objective in the underlying LGBM instance",
    "check group data",
    "coding: utf-8",
    "we don't need lib_lightgbm while building docs",
    "coding: utf-8",
    "pylint: disable = C0103",
    "coding: utf-8",
    "coding: utf-8",
    "pylint: disable = invalid-name, C0111, C0301",
    "pylint: disable = R0912, R0913, R0914, W0105, W0201, W0212",
    "TypeError: obj is not a string or a number",
    "ValueError: invalid literal",
    "process for args",
    "get categorical features",
    "process for reference dataset",
    "start construct data",
    "check data has header or not",
    "load init score",
    "need re group init score",
    "set feature names",
    "change non-float data to float data, need to copy",
    "create valid",
    "construct subset",
    "create train",
    "set to None",
    "we're done if self and reference share a common upstrem reference",
    "group data from LightGBM is boundaries data, need to convert to group size",
    "Training task",
    "construct booster object",
    "save reference to data",
    "buffer for inner predict",
    "set network if necessary",
    "Prediction task",
    "need reset training data",
    "need to push new valid data",
    "Get name of features",
    "avoid to predict many time in one iteration",
    "Get num of inner evals",
    "Get name of evals",
    "coding: utf-8",
    "pylint: disable = invalid-name, W0105, C0301",
    "Callback environment used by callbacks",
    "coding: utf-8",
    "pylint: disable = invalid-name, C0111",
    "load or create your dataset",
    "create dataset for lightgbm",
    "if you want to re-use data, remember to set free_raw_data=False",
    "specify your configurations as a dict",
    "generate a feature name",
    "feature_name and categorical_feature",
    "check feature name",
    "save model to file",
    "dump model to json (and save to file)",
    "feature names",
    "feature importances",
    "load model to predict",
    "can only predict with the best iteration (or the saving iteration)",
    "eval with loaded model",
    "dump model with pickle",
    "load model with pickle to predict",
    "can predict with any iteration when loaded in pickle way",
    "eval with loaded model",
    "continue training",
    "init_model accepts:",
    "1. model file name",
    "2. Booster()",
    "decay learning rates",
    "learning_rates accepts:",
    "1. list/tuple with length = num_boost_round",
    "2. function(curr_iter)",
    "change other parameters during training",
    "self-defined objective function",
    "f(preds: array, train_data: Dataset) -> grad: array, hess: array",
    "log likelihood loss",
    "self-defined eval metric",
    "f(preds: array, train_data: Dataset) -> name: string, value: array, is_higher_better: bool",
    "binary error",
    "callback",
    "coding: utf-8",
    "pylint: disable = invalid-name, C0111",
    "load or create your dataset",
    "train",
    "predict",
    "eval",
    "feature importances",
    "other scikit-learn modules",
    "coding: utf-8",
    "pylint: disable = invalid-name, C0111",
    "load or create your dataset",
    "create dataset for lightgbm",
    "specify your configurations as a dict",
    "train",
    "coding: utf-8",
    "pylint: disable = invalid-name, C0111",
    "load or create your dataset",
    "create dataset for lightgbm",
    "specify your configurations as a dict",
    "train",
    "save model to file",
    "predict",
    "eval",
    "!/usr/bin/env python3",
    "-*- coding: utf-8 -*-",
    "",
    "LightGBM documentation build configuration file, created by",
    "sphinx-quickstart on Thu May  4 14:30:58 2017.",
    "",
    "This file is execfile()d with the current directory set to its",
    "containing dir.",
    "",
    "Note that not all possible configuration values are present in this",
    "autogenerated file.",
    "",
    "All configuration values have a default; values that are commented out",
    "serve to show the default.",
    "If extensions (or modules to document with autodoc) are in another directory,",
    "add these directories to sys.path here. If the directory is relative to the",
    "documentation root, use os.path.abspath to make it absolute, like shown here.",
    "",
    "-- mock out modules",
    "-- General configuration ------------------------------------------------",
    "If your documentation needs a minimal Sphinx version, state it here.",
    "Add any Sphinx extension module names here, as strings. They can be",
    "extensions coming with Sphinx (named 'sphinx.ext.*') or your custom",
    "ones.",
    "Add any paths that contain templates here, relative to this directory.",
    "The suffix(es) of source filenames.",
    "You can specify multiple suffix as a list of string:",
    "source_suffix = ['.rst', '.md']",
    "The master toctree document.",
    "General information about the project.",
    "The version info for the project you're documenting, acts as replacement for",
    "|version| and |release|, also used in various other places throughout the",
    "built documents.",
    "",
    "The short X.Y version.",
    "The full version, including alpha/beta/rc tags.",
    "The language for content autogenerated by Sphinx. Refer to documentation",
    "for a list of supported languages.",
    "",
    "This is also used if you do content translation via gettext catalogs.",
    "Usually you set \"language\" from the command line for these cases.",
    "List of patterns, relative to source directory, that match files and",
    "directories to ignore when looking for source files.",
    "This patterns also effect to html_static_path and html_extra_path",
    "The name of the Pygments (syntax highlighting) style to use.",
    "If true, `todo` and `todoList` produce output, else they produce nothing.",
    "Both the class' and the __init__ method's docstring are concatenated and inserted.",
    "-- Options for HTML output ----------------------------------------------",
    "The theme to use for HTML and HTML Help pages.  See the documentation for",
    "a list of builtin themes.",
    "",
    "Theme options are theme-specific and customize the look and feel of a theme",
    "further.  For a list of options available for each theme, see the",
    "documentation.",
    "",
    "html_theme_options = {}",
    "Add any paths that contain custom static files (such as style sheets) here,",
    "relative to this directory. They are copied after the builtin static files,",
    "so a file named \"default.css\" will overwrite the builtin \"default.css\".",
    "-- Options for HTMLHelp output ------------------------------------------",
    "Output file base name for HTML help builder.",
    "coding: utf-8",
    "coding: utf-8",
    "pylint: skip-file",
    "we don't need lib_lightgbm while building docs",
    "coding: utf-8",
    "pylint: skip-file",
    "check saved model persistence",
    "we need to check the consistency of model file here, so test for exact equal",
    "check early stopping is working. Make it stop very early, so the scores should be very close to zero",
    "scores likely to be different, but prediction should still be the same",
    "coding: utf-8",
    "pylint: skip-file",
    "Tests that `seed` is the same as `random_state`",
    "sklearn <0.19 cannot accept instance, but many tests could be passed only with min_data=1 and min_data_in_bin=1",
    "we cannot use `check_estimator` directly since there is no skip test mechanism",
    "we cannot leave default params (see https://github.com/Microsoft/LightGBM/issues/833)",
    "coding: utf-8",
    "pylint: skip-file",
    "coding: utf-8",
    "pylint: skip-file",
    "no early stopping",
    "early stopping occurs",
    "test custom eval metrics",
    "shuffle = False, override metric in params",
    "shuffle = True, callbacks",
    "self defined folds",
    "lambdarank",
    "test feature_names with whitespaces",
    "take subsets and train",
    "coding: utf-8",
    "pylint: skip-file",
    "convert from one-based to  zero-based index",
    "convert from boundaries to size",
    "--- start Booster interfaces",
    "create boosting",
    "initialize the boosting",
    "create objective function",
    "initialize the objective function",
    "create training metric",
    "reset the boosting",
    "create objective function",
    "initialize the objective function",
    "some help functions used to convert data",
    "Row iterator of on column for CSC matrix",
    "return value at idx, only can access by ascent order",
    "return next non-zero pair, if index < 0, means no more data",
    "start of c_api functions",
    "sample data first",
    "sample data first",
    "sample data first",
    "no more data",
    "---- start of booster",
    "---- start of some help functions",
    "set number of threads for openmp",
    "check for alias",
    "read parameters from config file",
    "remove str after \"#\"",
    "check for alias again",
    "load configs",
    "prediction is needed if using input initial model(continued train)",
    "need to continue training",
    "sync up random seed for data partition",
    "load Training data",
    "load data for parallel training",
    "load data for single machine",
    "need save binary file",
    "create training metric",
    "only when have metrics then need to construct validation data",
    "Add validation data, if it exists",
    "add",
    "need save binary file",
    "add metric for validation data",
    "output used time on each iteration",
    "need init network",
    "create boosting",
    "create objective function",
    "load training data",
    "initialize the objective function",
    "initialize the boosting",
    "add validation data into boosting",
    "convert model to if-else statement code",
    "create predictor",
    "Free memory",
    "create predictor",
    "counts for all labels",
    "start from top label, and accumulate DCG",
    "counts for all labels",
    "calculate k Max DCG by one pass",
    "get sorted indices by score",
    "calculate dcg",
    "get sorted indices by score",
    "calculate multi dcg by one pass",
    "wait for all client start up",
    "default set to -1",
    "distance at k-th communication, distance[k] = 2^k",
    "set incoming rank at k-th commuication",
    "set outgoing rank at k-th commuication",
    "defalut set as -1",
    "construct all recursive halving map for all machines",
    "let 1 << k <= num_machines",
    "distance of each communication",
    "if num_machines = 2^k, don't need to group machines",
    "communication direction, %2 == 0 is positive",
    "neighbor at k-th communication",
    "receive data block at k-th communication",
    "send data block at k-th communication",
    "static member definition",
    "if small package or small count , do it by all gather.(reduce the communication times.)",
    "assign the blocks to every rank.",
    "do reduce scatter",
    "do all gather",
    "assign blocks",
    "need use buffer here, since size of \"output\" is smaller than size after all gather",
    "copy back",
    "assign blocks",
    "start all gather",
    "use output as receive buffer",
    "get current local block size",
    "get out rank",
    "get in rank",
    "get send information",
    "get recv information",
    "send and recv at same time",
    "rotate in-place",
    "get target",
    "get send information",
    "get recv information",
    "send and recv at same time",
    "reduce",
    "copy result",
    "start up socket",
    "parse clients from file",
    "get ip list of local machine",
    "get local rank",
    "construct listener",
    "construct communication topo",
    "construct linkers",
    "free listener",
    "set timeout",
    "accept incoming socket",
    "receive rank",
    "add new socket",
    "save ranks that need to connect with",
    "start listener",
    "start connect",
    "let smaller rank connect to larger rank",
    "send local rank",
    "wait for listener",
    "print connected linkers",
    "Get some statistic from 2 line",
    "if only have one line on file",
    "get column names",
    "load label idx first",
    "erase label column name",
    "load ignore columns",
    "load weight idx",
    "load group idx",
    "don't support query id in data file when training in parallel",
    "read data to memory",
    "sample data",
    "construct feature bin mappers",
    "initialize label",
    "extract features",
    "sample data from file",
    "construct feature bin mappers",
    "initialize label",
    "extract features",
    "load data from binary file",
    "check meta data",
    "need to check training data",
    "read data in memory",
    "initialize label",
    "extract features",
    "Get number of lines of data file",
    "initialize label",
    "extract features",
    "load data from binary file",
    "not need to check validation data",
    "check meta data",
    "buffer to read binary file",
    "check token",
    "read size of header",
    "re-allocmate space if not enough",
    "read header",
    "get header",
    "num_groups",
    "real_feature_idx_",
    "feature2group",
    "feature2subfeature",
    "group_bin_boundaries",
    "group_feature_start_",
    "group_feature_cnt_",
    "get feature names",
    "write feature names",
    "read size of meta data",
    "re-allocate space if not enough",
    "read meta data",
    "load meta data",
    "sample local used data if need to partition",
    "if not contain query file, minimal sample unit is one record",
    "if contain query file, minimal sample unit is one query",
    "if is new query",
    "read feature data",
    "read feature size",
    "re-allocate space if not enough",
    "fill feature_names_ if not header",
    "if only one machine, find bin locally",
    "if have multi-machines, need to find bin distributed",
    "different machines will find bin for different features",
    "start and len will store the process feature indices for different machines",
    "machine i will find bins for features in [ start[i], start[i] + len[i] )",
    "get size of bin mapper with max_bin size",
    "since sizes of different feature may not be same, we expand all bin mapper to type_size",
    "find local feature bins and copy to buffer",
    "free",
    "convert to binary size",
    "gather global feature bin mappers",
    "restore features bins from buffer",
    "---- private functions ----",
    "if features are ordered, not need to use hist_buf",
    "read all lines",
    "get query data",
    "if not contain query data, minimal sample unit is one record",
    "if contain query data, minimal sample unit is one query",
    "if is new query",
    "get query data",
    "if not contain query file, minimal sample unit is one record",
    "if contain query file, minimal sample unit is one query",
    "if is new query",
    "parse features",
    "-1 means doesn't use this feature",
    "check the range of label_idx, weight_idx and group_idx",
    "fill feature_names_ if not header",
    "start find bins",
    "if only one machine, find bin locally",
    "if have multi-machines, need to find bin distributed",
    "different machines will find bin for different features",
    "start and len will store the process feature indices for different machines",
    "machine i will find bins for features in [ start[i], start[i] + len[i] )",
    "get size of bin mapper with max_bin size",
    "since sizes of different feature may not be same, we expand all bin mapper to type_size",
    "find local feature bins and copy to buffer",
    "free",
    "convert to binary size",
    "gather global feature bin mappers",
    "restore features bins from buffer",
    "if doesn't need to prediction with initial model",
    "parser",
    "set label",
    "free processed line:",
    "shrink_to_fit will be very slow in linux, and seems not free memory, disable for now",
    "text_reader_->Lines()[i].shrink_to_fit();",
    "push data",
    "if is used feature",
    "if need to prediction with initial model",
    "parser",
    "set initial score",
    "set label",
    "free processed line:",
    "shrink_to_fit will be very slow in linux, and seems not free memory, disable for now",
    "text_reader_->Lines()[i].shrink_to_fit();",
    "push data",
    "if is used feature",
    "metadata_ will manage space of init_score",
    "text data can be free after loaded feature values",
    "parser",
    "set initial score",
    "set label",
    "push data",
    "if is used feature",
    "only need part of data",
    "need full data",
    "metadata_ will manage space of init_score",
    "read size of token",
    "deep copy function for BinMapper",
    "mean size for one bin",
    "need a new bin",
    "update bin upper bound",
    "last bin upper bound",
    "find distinct_values first",
    "push zero in the front",
    "use the large value",
    "push zero in the back",
    "convert to int type first",
    "sort by counts",
    "avoid first bin is zero",
    "will ignore the categorical of small counts",
    "need an additional bin for NaN",
    "use -1 to represent NaN",
    "Use MissingType::None to represent this bin contains all categoricals",
    "check trival(num_bin_ == 1) feature",
    "check useless bin",
    "calculate sparse rate",
    "sparse threshold",
    "for lambdarank, it needs query data for partition data in parallel learning",
    "need convert query_id to boundaries",
    "check weights",
    "check query boundries",
    "contain initial score file",
    "check weights",
    "get local weights",
    "check query boundries",
    "get local query boundaries",
    "contain initial score file",
    "get local initial scores",
    "re-load query weight",
    "save to nullptr",
    "save to nullptr",
    "save to nullptr",
    "default weight file name",
    "default weight file name",
    "use first line to count number class",
    "default query file name",
    "root is in the depth 0",
    "non-leaf",
    "leaf",
    "use this for the missing value conversion",
    "Predict func by Map to ifelse",
    "use this for the missing value conversion",
    "non-leaf",
    "left subtree",
    "right subtree",
    "leaf",
    "non-leaf",
    "left subtree",
    "right subtree",
    "leaf",
    "recursive computation of SHAP values for a decision tree",
    "extend the unique path",
    "leaf node",
    "internal node",
    "see if we have already split on this feature,",
    "if so we undo that split so we can redo it for this node",
    "clear old metrics",
    "to lower",
    "split",
    "remove duplicate",
    "load main config types",
    "generate seeds by seed.",
    "sub-config setup",
    "check for conflicts",
    "check if objective_type, metric_type, and num_class match",
    "Change pool size to -1 (no limit) when using data parallel to reduce communication costs",
    "Check max_depth and num_leaves",
    "label_gain = 2^i - 1, may overflow, so we use 31 here",
    "label_gain = 2^i - 1, may overflow, so we use 31 here",
    "default eval ndcg @[1-5]",
    "filter is based on sampling data, so decrease its range",
    "put dense feature first",
    "sort by non zero cnt",
    "sort by non zero cnt, bigger first",
    "take apart small sparse group, due it will not gain on speed",
    "shuffle groups",
    "get num_features",
    "get bin_mappers",
    "copy feature bin mapper data",
    "copy feature bin mapper data",
    "if not pass a filename, just append \".bin\" of original file",
    "get size of header",
    "size of feature names",
    "write header",
    "write feature names",
    "get size of meta data",
    "write meta data",
    "write feature data",
    "get size of feature",
    "write feature",
    "feature is not used",
    "construct histograms for smaller leaf",
    "if not use ordered bin",
    "used ordered bin",
    "feature is not used",
    "construct histograms for smaller leaf",
    "if not use ordered bin",
    "used ordered bin",
    "fixed hessian.",
    "feature is not used",
    "construct histograms for smaller leaf",
    "if not use ordered bin",
    "used ordered bin",
    "feature is not used",
    "construct histograms for smaller leaf",
    "if not use ordered bin",
    "used ordered bin",
    "fixed hessian.",
    "PredictRaw",
    "PredictRawByMap",
    "Predict",
    "PredictByMap",
    "PredictLeafIndex",
    "PredictLeafIndexByMap",
    "output model type",
    "output number of class",
    "output label index",
    "output max_feature_idx",
    "output objective",
    "output tree models",
    "store the importance first",
    "sort the importance",
    "use serialized string to restore this object",
    "Use first 128 chars to avoid exceed the message buffer.",
    "get number of classes",
    "get index of label",
    "get max_feature_idx first",
    "get average_output",
    "get feature names",
    "set zero",
    "predict all the trees for one iteration",
    "check early stopping",
    "set zero",
    "predict all the trees for one iteration",
    "check early stopping",
    "margin_threshold will be captured by value",
    "copy and sort",
    "margin_threshold will be captured by value",
    "init tree learner",
    "push training metrics",
    "create buffer for gradients and hessians",
    "get max feature index",
    "get label index",
    "get feature names",
    "if need bagging, create buffer",
    "reset config for tree learner",
    "multi-class",
    "binary class",
    "for a validation dataset, we need its score and metric",
    "update score",
    "objective function will calculate gradients and hessians",
    "random bagging, minimal unit is one record",
    "if need bagging",
    "set bagging data to tree learner",
    "get subset",
    "output used time per iteration",
    "boosting from average label; or customized \"average\" if implemented for the current objective",
    "boosting first",
    "bagging logic",
    "need to copy gradients for bagging subset.",
    "shrinkage by learning rate",
    "update score",
    "only add default score one-time",
    "updates scores",
    "add model",
    "reset score",
    "remove model",
    "print message for metric",
    "pop last early_stopping_round_ models",
    "update training score",
    "we need to predict out-of-bag scores of data for boosting",
    "update validation score",
    "print training metric",
    "print validation metric",
    "set zero",
    "predict all the trees for one iteration",
    "check early stopping",
    "push training metrics",
    "not same training data, need reset score and others",
    "create score tracker",
    "update score",
    "create buffer for gradients and hessians",
    "if need bagging, create buffer",
    "Get the max size of pool",
    "at least need 2 leaves",
    "push split information for all leaves",
    "get ordered bin",
    "check existing for ordered bin",
    "initialize splits for leaf",
    "initialize data partition",
    "initialize ordered gradients and hessians",
    "if has ordered bin, need to allocate a buffer to fast split",
    "get ordered bin",
    "initialize splits for leaf",
    "initialize data partition",
    "initialize ordered gradients and hessians",
    "if has ordered bin, need to allocate a buffer to fast split",
    "Get the max size of pool",
    "at least need 2 leaves",
    "push split information for all leaves",
    "some initial works before training",
    "root leaf",
    "only root leaf can be splitted on first time",
    "some initial works before finding best split",
    "find best threshold for every feature",
    "Get a leaf with max split gain",
    "Get split information for best leaf",
    "cannot split, quit",
    "split tree with best leaf",
    "reset histogram pool",
    "at least use one feature",
    "initialize used features",
    "Get used feature at current tree",
    "initialize data partition",
    "reset the splits for leaves",
    "Sumup for root",
    "use all data",
    "use bagging, only use part of data",
    "if has ordered bin, need to initialize the ordered bin",
    "use all data, pass nullptr",
    "bagging, only use part of data",
    "mark used data",
    "initialize ordered bin",
    "check depth of current leaf",
    "only need to check left leaf, since right leaf is in same level of left leaf",
    "no enough data to continue",
    "only have root",
    "put parent(left) leaf's histograms into larger leaf's histograms",
    "put parent(left) leaf's histograms to larger leaf's histograms",
    "split for the ordered bin",
    "mark data that at left-leaf",
    "split the ordered bin",
    "construct smaller leaf",
    "construct larger leaf",
    "find splits",
    "only has root leaf",
    "find best threshold for larger child",
    "left = parent",
    "split tree, will return right leaf",
    "init the leaves that used on next iteration",
    "get feature partition",
    "get local used features",
    "get best split at smaller leaf",
    "find local best split for larger leaf",
    "sync global best info",
    "update best split",
    "instantiate template classes, otherwise linker cannot find the code",
    "initialize SerialTreeLearner",
    "Get local rank and global machine size",
    "allocate buffer for communication",
    "generate feature partition for current tree",
    "get local used feature",
    "get block start and block len for reduce scatter",
    "get buffer_write_start_pos_",
    "get buffer_read_start_pos_",
    "sync global data sumup info",
    "global sumup reduce",
    "copy back",
    "set global sumup info",
    "init global data count in leaf",
    "construct local histograms",
    "copy to buffer",
    "Reduce scatter for histogram",
    "restore global histograms from buffer",
    "find best threshold for smaller child",
    "only root leaf",
    "construct histgroms for large leaf, we init larger leaf as the parent, so we can just subtract the smaller leaf's histograms",
    "find best threshold for larger child",
    "find local best split for larger leaf",
    "sync global best info",
    "set best split",
    "need update global number of data in leaf",
    "instantiate template classes, otherwise linker cannot find the code",
    "initialize SerialTreeLearner",
    "some additional variables needed for GPU trainer",
    "Initialize GPU buffers and kernels",
    "some functions used for debugging the GPU histogram construction",
    "printf(\"grad %g != %g (%d ULPs)\\n\", h1[i].sum_gradients, h2[i].sum_gradients, ulps);",
    "goto err;",
    "printf(\"hessian %g != %g (%d ULPs)\\n\", h1[i].sum_hessians, h2[i].sum_hessians, ulps);",
    "goto err;",
    "we roughly want 256 workgroups per device, and we have num_dense_feature4_ feature tuples.",
    "also guarantee that there are at least 2K examples per workgroup",
    "return 0;",
    "we have already copied ordered gradients, ordered hessians and indices to GPU",
    "decide the best number of workgroups working on one feature4 tuple",
    "set work group size based on feature size",
    "each 2^exp_workgroups_per_feature workgroups work on a feature4 tuple",
    "we need to refresh the kernel arguments after reallocating",
    "The only argument that needs to be changed later is num_data_",
    "the GPU kernel will process all features in one call, and each",
    "2^exp_workgroups_per_feature (compile time constant) workgroup will",
    "process one feature4 tuple",
    "for the root node, indices are not copied",
    "for constant hessian, hessians are not copied except for the root node",
    "there will be 2^exp_workgroups_per_feature = num_workgroups / num_dense_feature4 sub-histogram per feature4",
    "and we will launch num_feature workgroups for this kernel",
    "will launch threads for all features",
    "the queue should be asynchrounous, and we will can WaitAndGetHistograms() before we start processing dense feature groups",
    "copy the results asynchronously. Size depends on if double precision is used",
    "we will wait for this object in WaitAndGetHistograms",
    "when the output is ready, the computation is done",
    "values of this feature has been redistributed to multiple bins; need a reduction here",
    "how many feature-group tuples we have",
    "leave some safe margin for prefetching",
    "256 work-items per workgroup. Each work-item prefetches one tuple for that feature",
    "clear sparse/dense maps",
    "do nothing if no features can be processed on GPU",
    "allocate memory for all features (FIXME: 4 GB barrier on some devices, need to split to multiple buffers)",
    "unpin old buffer if necessary before destructing them",
    "make ordered_gradients and hessians larger (including extra room for prefetching), and pin them",
    "allocate space for gradients and hessians on device",
    "we will copy gradients and hessians in after ordered_gradients_ and ordered_hessians_ are constructed",
    "allocate feature mask, for disabling some feature-groups' histogram calculation",
    "copy indices to the device",
    "histogram bin entry size depends on the precision (single/double)",
    "create output buffer, each feature has a histogram with device_bin_size_ bins,",
    "each work group generates a sub-histogram of dword_features_ features.",
    "only initialize once here, as this will not need to change when ResetTrainingData() is called",
    "create atomic counters for inter-group coordination",
    "The output buffer is allocated to host directly, to overlap compute and data transfer",
    "find the dense feature-groups and group then into Feature4 data structure (several feature-groups packed into 4 bytes)",
    "looking for dword_features_ non-sparse feature-groups",
    "decide if we need to redistribute the bin",
    "multiplier must be a power of 2",
    "device_bin_mults_.push_back(1);",
    "found",
    "for data transfer time",
    "Now generate new data structure feature4, and copy data to the device",
    "preallocate arrays for all threads, and pin them",
    "building Feature4 bundles; each thread handles dword_features_ features",
    "one feature datapoint is 4 bits",
    "this guarantees that the RawGet() function is inlined, rather than using virtual function dispatching",
    "one feature datapoint is one byte",
    "this guarantees that the RawGet() function is inlined, rather than using virtual function dispatching",
    "Dense bin",
    "Dense 4-bit bin",
    "working on the remaining (less than dword_features_) feature groups",
    "fill the leftover features",
    "fill this empty feature with some \"random\" value",
    "fill this empty feature with some \"random\" value",
    "copying the last 1 to (dword_features - 1) feature-groups in the last tuple",
    "deallocate pinned space for feature copying",
    "data transfer time",
    "for other types of failure, build log might not be available; program.build_log() can crash",
    "Something bad happened. Just return \"No log available.\"",
    "build is okay, log may contain warnings",
    "destroy any old kernels",
    "create OpenCL kernels for different number of workgroups per feature",
    "currently we don't use constant memory",
    "compile the GPU kernel depending if double precision is used, constant hessian is used, etc",
    "kernel with indices in an array",
    "kernel with all features enabled, with elimited branches",
    "kernel with all data indices (for root node, and assumes that root node always uses all features)",
    "do nothing if no features can be processed on GPU",
    "The only argument that needs to be changed later is num_data_",
    "hessian is passed as a parameter, but it is not available now.",
    "hessian will be set in BeforeTrain()",
    "Get the max bin size, used for selecting best GPU kernel",
    "initialize GPU",
    "determine which kernel to use based on the max number of bins",
    "setup GPU kernel arguments after we allocating all the buffers",
    "check if we need to recompile the GPU kernel (is_constant_hessian changed)",
    "this should rarely occur",
    "GPU memory has to been reallocated because data may have been changed",
    "setup GPU kernel arguments after we allocating all the buffers",
    "Copy initial full hessians and gradients to GPU.",
    "We start copying as early as possible, instead of at ConstructHistogram().",
    "setup hessian parameters only",
    "hessian is passed as a parameter",
    "use bagging",
    "On GPU, we start copying indices, gradients and hessians now, instead at ConstructHistogram()",
    "copy used gradients and hessians to ordered buffer",
    "transfer the indices to GPU",
    "transfer hessian to GPU",
    "setup hessian parameters only",
    "hessian is passed as a parameter",
    "transfer gradients to GPU",
    "only have root",
    "Copy indices, gradients and hessians as early as possible",
    "only need to initialize for smaller leaf",
    "Get leaf boundary",
    "copy indices to the GPU:",
    "copy ordered hessians to the GPU:",
    "copy ordered gradients to the GPU:",
    "do nothing if no features can be processed on GPU",
    "copy data indices if it is not null",
    "generate and copy ordered_gradients if gradients is not null",
    "generate and copy ordered_hessians if hessians is not null",
    "converted indices in is_feature_used to feature-group indices",
    "construct the feature masks for dense feature-groups",
    "if no feature group is used, just return and do not use GPU",
    "if not all feature groups are used, we need to transfer the feature mask to GPU",
    "otherwise, we will use a specialized GPU kernel with all feature groups enabled",
    "All data have been prepared, now run the GPU kernel",
    "construct smaller leaf",
    "ConstructGPUHistogramsAsync will return true if there are availabe feature gourps dispatched to GPU",
    "then construct sparse features on CPU",
    "We set data_indices to null to avoid rebuilding ordered gradients/hessians",
    "wait for GPU to finish, only if GPU is actually used",
    "use double precision",
    "use single precision",
    "Compare GPU histogram with CPU histogram, useful for debuggin GPU code problem",
    "#define GPU_DEBUG_COMPARE",
    "construct larger leaf",
    "then construct sparse features on CPU",
    "We set data_indices to null to avoid rebuilding ordered gradients/hessians",
    "wait for GPU to finish, only if GPU is actually used",
    "use double precision",
    "use single precision",
    "do some sanity check for the GPU algorithm",
    "limit top k",
    "get max bin",
    "calculate buffer size",
    "left and right on same time, so need double size",
    "initialize histograms for global",
    "sync global data sumup info",
    "set global sumup info",
    "init global data count in leaf",
    "get local sumup",
    "get local sumup",
    "get mean number on machines",
    "weighted gain",
    "get top k",
    "Copy histogram to buffer, and Get local aggregate features",
    "copy histograms.",
    "copy smaller leaf histograms first",
    "mark local aggregated feature",
    "copy",
    "then copy larger leaf histograms",
    "mark local aggregated feature",
    "copy",
    "use local data to find local best splits",
    "find splits",
    "only has root leaf",
    "find best threshold for larger child",
    "local voting",
    "gather",
    "get all top-k from all machines",
    "global voting",
    "copy local histgrams to buffer",
    "Reduce scatter for histogram",
    "find best split from local aggregated histograms",
    "restore from buffer",
    "find best threshold",
    "restore from buffer",
    "find best threshold",
    "find local best",
    "find local best split for larger leaf",
    "sync global best info",
    "copy back",
    "set the global number of data for leaves",
    "init the global sumup info",
    "instantiate template classes, otherwise linker cannot find the code"
  ],
  "v2.0.10": [
    "coding: utf-8",
    "pylint: disable=invalid-name, exec-used, C0111",
    "coding: utf-8",
    "pylint: disable = invalid-name, W0105",
    "Most of legacy advanced options becomes callbacks",
    "check evaluation result.",
    "lambdarank task, split according to groups",
    "run preprocessing on the data set if needed",
    "setup callbacks",
    "coding: utf-8",
    "pylint: disable = C0103",
    "simplejson does not support Python 3.2, it throws a SyntaxError",
    "because of u'...' Unicode literals.",
    "LGBMDeprecated = None  Don't uncomment it because it causes error without installed sklearn",
    "coding: utf-8",
    "pylint: disable = invalid-name, W0105, C0111, C0301",
    "DeprecationWarning is not shown by default, so let's create our own with higher level",
    "minor change to support `**kwargs`",
    "sklearn interface has another naming convention",
    "user can set verbose with kwargs, it has higher priority",
    "free dataset",
    "Switch to using a multiclass objective in the underlying LGBM instance",
    "check group data",
    "coding: utf-8",
    "we don't need lib_lightgbm while building docs",
    "coding: utf-8",
    "pylint: disable = C0103",
    "coding: utf-8",
    "coding: utf-8",
    "pylint: disable = invalid-name, C0111, C0301",
    "pylint: disable = R0912, R0913, R0914, W0105, W0201, W0212",
    "TypeError: obj is not a string or a number",
    "ValueError: invalid literal",
    "load init score",
    "need re group init score",
    "set feature names",
    "we're done if self and reference share a common upstrem reference",
    "group data from LightGBM is boundaries data, need to convert to group size",
    "coding: utf-8",
    "pylint: disable = invalid-name, W0105, C0301",
    "Callback environment used by callbacks",
    "coding: utf-8",
    "pylint: disable = invalid-name, C0111",
    "load or create your dataset",
    "create dataset for lightgbm",
    "if you want to re-use data, remember to set free_raw_data=False",
    "specify your configurations as a dict",
    "generate a feature name",
    "feature_name and categorical_feature",
    "check feature name",
    "save model to file",
    "dump model to json (and save to file)",
    "feature names",
    "feature importances",
    "load model to predict",
    "can only predict with the best iteration (or the saving iteration)",
    "eval with loaded model",
    "dump model with pickle",
    "load model with pickle to predict",
    "can predict with any iteration when loaded in pickle way",
    "eval with loaded model",
    "continue training",
    "init_model accepts:",
    "1. model file name",
    "2. Booster()",
    "decay learning rates",
    "learning_rates accepts:",
    "1. list/tuple with length = num_boost_round",
    "2. function(curr_iter)",
    "change other parameters during training",
    "self-defined objective function",
    "f(preds: array, train_data: Dataset) -> grad: array, hess: array",
    "log likelihood loss",
    "self-defined eval metric",
    "f(preds: array, train_data: Dataset) -> name: string, value: array, is_higher_better: bool",
    "binary error",
    "callback",
    "coding: utf-8",
    "pylint: disable = invalid-name, C0111",
    "load or create your dataset",
    "train",
    "predict",
    "eval",
    "feature importances",
    "other scikit-learn modules",
    "coding: utf-8",
    "pylint: disable = invalid-name, C0111",
    "load or create your dataset",
    "create dataset for lightgbm",
    "specify your configurations as a dict",
    "train",
    "coding: utf-8",
    "pylint: disable = invalid-name, C0111",
    "load or create your dataset",
    "create dataset for lightgbm",
    "specify your configurations as a dict",
    "train",
    "save model to file",
    "predict",
    "eval",
    "coding: utf-8",
    "pylint: disable = C0111, C0103",
    "print out the pmml for a decision tree",
    "specify the objective as function name and binarySplit for",
    "splitCharacteristic because each node has 2 children",
    "list each feature name as a mining field, and treat all outliers as is,",
    "unless specified",
    "begin printing out the decision tree",
    "open the model file and then process it",
    "ignore first 6 and empty lines",
    "print out data dictionary entries for each column",
    "not adding any interval definition, all values are currently",
    "valid",
    "list each feature name as a mining field, and treat all outliers",
    "as is, unless specified",
    "read each array that contains pertinent information for the pmml",
    "these arrays will be used to recreate the traverse the decision tree",
    "!/usr/bin/env python3",
    "-*- coding: utf-8 -*-",
    "",
    "LightGBM documentation build configuration file, created by",
    "sphinx-quickstart on Thu May  4 14:30:58 2017.",
    "",
    "This file is execfile()d with the current directory set to its",
    "containing dir.",
    "",
    "Note that not all possible configuration values are present in this",
    "autogenerated file.",
    "",
    "All configuration values have a default; values that are commented out",
    "serve to show the default.",
    "If extensions (or modules to document with autodoc) are in another directory,",
    "add these directories to sys.path here. If the directory is relative to the",
    "documentation root, use os.path.abspath to make it absolute, like shown here.",
    "",
    "-- mock out modules",
    "-- General configuration ------------------------------------------------",
    "If your documentation needs a minimal Sphinx version, state it here.",
    "Add any Sphinx extension module names here, as strings. They can be",
    "extensions coming with Sphinx (named 'sphinx.ext.*') or your custom",
    "ones.",
    "Add any paths that contain templates here, relative to this directory.",
    "The suffix(es) of source filenames.",
    "You can specify multiple suffix as a list of string:",
    "source_suffix = ['.rst', '.md']",
    "The master toctree document.",
    "General information about the project.",
    "The version info for the project you're documenting, acts as replacement for",
    "|version| and |release|, also used in various other places throughout the",
    "built documents.",
    "",
    "The short X.Y version.",
    "The full version, including alpha/beta/rc tags.",
    "The language for content autogenerated by Sphinx. Refer to documentation",
    "for a list of supported languages.",
    "",
    "This is also used if you do content translation via gettext catalogs.",
    "Usually you set \"language\" from the command line for these cases.",
    "List of patterns, relative to source directory, that match files and",
    "directories to ignore when looking for source files.",
    "This patterns also effect to html_static_path and html_extra_path",
    "The name of the Pygments (syntax highlighting) style to use.",
    "If true, `todo` and `todoList` produce output, else they produce nothing.",
    "Both the class' and the __init__ method's docstring are concatenated and inserted.",
    "-- Options for HTML output ----------------------------------------------",
    "The theme to use for HTML and HTML Help pages.  See the documentation for",
    "a list of builtin themes.",
    "",
    "Theme options are theme-specific and customize the look and feel of a theme",
    "further.  For a list of options available for each theme, see the",
    "documentation.",
    "",
    "html_theme_options = {}",
    "Add any paths that contain custom static files (such as style sheets) here,",
    "relative to this directory. They are copied after the builtin static files,",
    "so a file named \"default.css\" will overwrite the builtin \"default.css\".",
    "-- Options for HTMLHelp output ------------------------------------------",
    "Output file base name for HTML help builder.",
    "-- Options for LaTeX output ---------------------------------------------",
    "The paper size ('letterpaper' or 'a4paper').",
    "",
    "'papersize': 'letterpaper',",
    "The font size ('10pt', '11pt' or '12pt').",
    "",
    "'pointsize': '10pt',",
    "Additional stuff for the LaTeX preamble.",
    "",
    "'preamble': '',",
    "Latex figure (float) alignment",
    "",
    "'figure_align': 'htbp',",
    "Grouping the document tree into LaTeX files. List of tuples",
    "(source start file, target name, title,",
    "author, documentclass [howto, manual, or own class]).",
    "latex_documents = [",
    "(master_doc, 'LightGBM.tex', 'LightGBM Documentation',",
    "'Microsoft Corporation', 'manual'),",
    "]",
    "-- Options for manual page output ---------------------------------------",
    "One entry per manual page. List of tuples",
    "(source start file, name, description, authors, manual section).",
    "man_pages = [",
    "(master_doc, 'lightgbm', 'LightGBM Documentation',",
    "[author], 1)",
    "]",
    "-- Options for Texinfo output -------------------------------------------",
    "Grouping the document tree into Texinfo files. List of tuples",
    "(source start file, target name, title, author,",
    "dir menu entry, description, category)",
    "texinfo_documents = [",
    "(master_doc, 'LightGBM', 'LightGBM Documentation',",
    "author, 'LightGBM', 'One line description of project.',",
    "'Miscellaneous'),",
    "]",
    "coding: utf-8",
    "coding: utf-8",
    "pylint: skip-file",
    "we don't need lib_lightgbm while building docs",
    "coding: utf-8",
    "pylint: skip-file",
    "check saved model persistence",
    "we need to check the consistency of model file here, so test for exact equal",
    "check early stopping is working. Make it stop very early, so the scores should be very close to zero",
    "scores likely to be different, but prediction should still be the same",
    "coding: utf-8",
    "pylint: skip-file",
    "Tests that `seed` is the same as `random_state`",
    "sklearn <0.19 cannot accept instance, but many tests could be passed only with min_data=1 and min_data_in_bin=1",
    "we cannot use `check_estimator` directly since there is no skip test mechanism",
    "we cannot leave default params (see https://github.com/Microsoft/LightGBM/issues/833)",
    "coding: utf-8",
    "pylint: skip-file",
    "coding: utf-8",
    "pylint: skip-file",
    "no early stopping",
    "early stopping occurs",
    "test custom eval metrics",
    "shuffle = False, override metric in params",
    "shuffle = True, callbacks",
    "self defined folds",
    "lambdarank",
    "test feature_names with whitespaces",
    "take subsets and train",
    "convert from one-based to  zero-based index",
    "convert from boundaries to size",
    "--- start Booster interfaces",
    "create boosting",
    "initialize the boosting",
    "create objective function",
    "initialize the objective function",
    "create training metric",
    "reset the boosting",
    "create objective function",
    "initialize the objective function",
    "some help functions used to convert data",
    "Row iterator of on column for CSC matrix",
    "return value at idx, only can access by ascent order",
    "return next non-zero pair, if index < 0, means no more data",
    "start of c_api functions",
    "sample data first",
    "sample data first",
    "sample data first",
    "no more data",
    "---- start of booster",
    "---- start of some help functions",
    "set number of threads for openmp",
    "check for alias",
    "read parameters from config file",
    "remove str after \"#\"",
    "check for alias again",
    "load configs",
    "prediction is needed if using input initial model(continued train)",
    "need to continue training",
    "sync up random seed for data partition",
    "load Training data",
    "load data for parallel training",
    "load data for single machine",
    "need save binary file",
    "create training metric",
    "only when have metrics then need to construct validation data",
    "Add validation data, if it exists",
    "add",
    "need save binary file",
    "add metric for validation data",
    "output used time on each iteration",
    "need init network",
    "create boosting",
    "create objective function",
    "load training data",
    "initialize the objective function",
    "initialize the boosting",
    "add validation data into boosting",
    "convert model to if-else statement code",
    "create predictor",
    "counts for all labels",
    "start from top label, and accumulate DCG",
    "counts for all labels",
    "calculate k Max DCG by one pass",
    "get sorted indices by score",
    "calculate dcg",
    "get sorted indices by score",
    "calculate multi dcg by one pass",
    "wait for all client start up",
    "default set to -1",
    "distance at k-th communication, distance[k] = 2^k",
    "set incoming rank at k-th commuication",
    "set outgoing rank at k-th commuication",
    "defalut set as -1",
    "construct all recursive halving map for all machines",
    "let 1 << k <= num_machines",
    "distance of each communication",
    "if num_machines = 2^k, don't need to group machines",
    "communication direction, %2 == 0 is positive",
    "neighbor at k-th communication",
    "receive data block at k-th communication",
    "send data block at k-th communication",
    "static member definition",
    "if small package or small count , do it by all gather.(reduce the communication times.)",
    "assign the blocks to every rank.",
    "do reduce scatter",
    "do all gather",
    "assign blocks",
    "need use buffer here, since size of \"output\" is smaller than size after all gather",
    "copy back",
    "assign blocks",
    "start all gather",
    "use output as receive buffer",
    "get current local block size",
    "get out rank",
    "get in rank",
    "get send information",
    "get recv information",
    "send and recv at same time",
    "rotate in-place",
    "get target",
    "get send information",
    "get recv information",
    "send and recv at same time",
    "reduce",
    "copy result",
    "start up socket",
    "parse clients from file",
    "get ip list of local machine",
    "get local rank",
    "construct listener",
    "construct communication topo",
    "construct linkers",
    "free listener",
    "set timeout",
    "accept incoming socket",
    "receive rank",
    "add new socket",
    "save ranks that need to connect with",
    "start listener",
    "start connect",
    "let smaller rank connect to larger rank",
    "send local rank",
    "wait for listener",
    "print connected linkers",
    "Get some statistic from 2 line",
    "if only have one line on file",
    "get column names",
    "load label idx first",
    "erase label column name",
    "load ignore columns",
    "load weight idx",
    "load group idx",
    "don't support query id in data file when training in parallel",
    "read data to memory",
    "sample data",
    "construct feature bin mappers",
    "initialize label",
    "extract features",
    "sample data from file",
    "construct feature bin mappers",
    "initialize label",
    "extract features",
    "load data from binary file",
    "check meta data",
    "need to check training data",
    "read data in memory",
    "initialize label",
    "extract features",
    "Get number of lines of data file",
    "initialize label",
    "extract features",
    "load data from binary file",
    "not need to check validation data",
    "check meta data",
    "buffer to read binary file",
    "check token",
    "read size of header",
    "re-allocmate space if not enough",
    "read header",
    "get header",
    "num_groups",
    "real_feature_idx_",
    "feature2group",
    "feature2subfeature",
    "group_bin_boundaries",
    "group_feature_start_",
    "group_feature_cnt_",
    "get feature names",
    "write feature names",
    "read size of meta data",
    "re-allocate space if not enough",
    "read meta data",
    "load meta data",
    "sample local used data if need to partition",
    "if not contain query file, minimal sample unit is one record",
    "if contain query file, minimal sample unit is one query",
    "if is new query",
    "read feature data",
    "read feature size",
    "re-allocate space if not enough",
    "fill feature_names_ if not header",
    "if only one machine, find bin locally",
    "if have multi-machines, need to find bin distributed",
    "different machines will find bin for different features",
    "start and len will store the process feature indices for different machines",
    "machine i will find bins for features in [ start[i], start[i] + len[i] )",
    "get size of bin mapper with max_bin size",
    "since sizes of different feature may not be same, we expand all bin mapper to type_size",
    "find local feature bins and copy to buffer",
    "free",
    "convert to binary size",
    "gather global feature bin mappers",
    "restore features bins from buffer",
    "---- private functions ----",
    "if features are ordered, not need to use hist_buf",
    "read all lines",
    "get query data",
    "if not contain query data, minimal sample unit is one record",
    "if contain query data, minimal sample unit is one query",
    "if is new query",
    "get query data",
    "if not contain query file, minimal sample unit is one record",
    "if contain query file, minimal sample unit is one query",
    "if is new query",
    "parse features",
    "-1 means doesn't use this feature",
    "check the range of label_idx, weight_idx and group_idx",
    "fill feature_names_ if not header",
    "start find bins",
    "if only one machine, find bin locally",
    "if have multi-machines, need to find bin distributed",
    "different machines will find bin for different features",
    "start and len will store the process feature indices for different machines",
    "machine i will find bins for features in [ start[i], start[i] + len[i] )",
    "get size of bin mapper with max_bin size",
    "since sizes of different feature may not be same, we expand all bin mapper to type_size",
    "find local feature bins and copy to buffer",
    "free",
    "convert to binary size",
    "gather global feature bin mappers",
    "restore features bins from buffer",
    "if doesn't need to prediction with initial model",
    "parser",
    "set label",
    "free processed line:",
    "shrink_to_fit will be very slow in linux, and seems not free memory, disable for now",
    "text_reader_->Lines()[i].shrink_to_fit();",
    "push data",
    "if is used feature",
    "if need to prediction with initial model",
    "parser",
    "set initial score",
    "set label",
    "free processed line:",
    "shrink_to_fit will be very slow in linux, and seems not free memory, disable for now",
    "text_reader_->Lines()[i].shrink_to_fit();",
    "push data",
    "if is used feature",
    "metadata_ will manage space of init_score",
    "text data can be free after loaded feature values",
    "parser",
    "set initial score",
    "set label",
    "push data",
    "if is used feature",
    "only need part of data",
    "need full data",
    "metadata_ will manage space of init_score",
    "read size of token",
    "deep copy function for BinMapper",
    "mean size for one bin",
    "need a new bin",
    "update bin upper bound",
    "last bin upper bound",
    "find distinct_values first",
    "push zero in the front",
    "push zero in the back",
    "convert to int type first",
    "sort by counts",
    "avoid first bin is zero",
    "will ignore the categorical of small counts",
    "need an additional bin for NaN",
    "use -1 to represent NaN",
    "Use MissingType::None to represent this bin contains all categoricals",
    "check trival(num_bin_ == 1) feature",
    "check useless bin",
    "calculate sparse rate",
    "sparse threshold",
    "for lambdarank, it needs query data for partition data in parallel learning",
    "need convert query_id to boundaries",
    "check weights",
    "check query boundries",
    "contain initial score file",
    "check weights",
    "get local weights",
    "check query boundries",
    "get local query boundaries",
    "contain initial score file",
    "get local initial scores",
    "re-load query weight",
    "save to nullptr",
    "save to nullptr",
    "save to nullptr",
    "default weight file name",
    "default weight file name",
    "use first line to count number class",
    "default query file name",
    "root is in the depth 0",
    "non-leaf",
    "leaf",
    "use this for the missing value conversion",
    "non-leaf",
    "left subtree",
    "right subtree",
    "leaf",
    "recursive computation of SHAP values for a decision tree",
    "extend the unique path",
    "leaf node",
    "internal node",
    "see if we have already split on this feature,",
    "if so we undo that split so we can redo it for this node",
    "clear old metrics",
    "to lower",
    "split",
    "remove duplicate",
    "load main config types",
    "generate seeds by seed.",
    "sub-config setup",
    "check for conflicts",
    "check if objective_type, metric_type, and num_class match",
    "Change pool size to -1 (no limit) when using data parallel to reduce communication costs",
    "Check max_depth and num_leaves",
    "label_gain = 2^i - 1, may overflow, so we use 31 here",
    "label_gain = 2^i - 1, may overflow, so we use 31 here",
    "default eval ndcg @[1-5]",
    "filter is based on sampling data, so decrease its range",
    "put dense feature first",
    "sort by non zero cnt",
    "sort by non zero cnt, bigger first",
    "take apart small sparse group, due it will not gain on speed",
    "shuffle groups",
    "get num_features",
    "get bin_mappers",
    "copy feature bin mapper data",
    "copy feature bin mapper data",
    "if not pass a filename, just append \".bin\" of original file",
    "get size of header",
    "size of feature names",
    "write header",
    "write feature names",
    "get size of meta data",
    "write meta data",
    "write feature data",
    "get size of feature",
    "write feature",
    "feature is not used",
    "construct histograms for smaller leaf",
    "if not use ordered bin",
    "used ordered bin",
    "feature is not used",
    "construct histograms for smaller leaf",
    "if not use ordered bin",
    "used ordered bin",
    "fixed hessian.",
    "feature is not used",
    "construct histograms for smaller leaf",
    "if not use ordered bin",
    "used ordered bin",
    "feature is not used",
    "construct histograms for smaller leaf",
    "if not use ordered bin",
    "used ordered bin",
    "fixed hessian.",
    "set zero",
    "predict all the trees for one iteration",
    "check early stopping",
    "PredictRaw",
    "Predict",
    "PredictLeafIndex",
    "output model type",
    "output number of class",
    "output label index",
    "output max_feature_idx",
    "output objective",
    "output tree models",
    "store the importance first",
    "sort the importance",
    "use serialized string to restore this object",
    "get number of classes",
    "get index of label",
    "get max_feature_idx first",
    "get average_output",
    "get feature names",
    "get tree models",
    "margin_threshold will be captured by value",
    "copy and sort",
    "margin_threshold will be captured by value",
    "init tree learner",
    "push training metrics",
    "create buffer for gradients and hessians",
    "get max feature index",
    "get label index",
    "get feature names",
    "if need bagging, create buffer",
    "reset config for tree learner",
    "multi-class",
    "binary class",
    "for a validation dataset, we need its score and metric",
    "update score",
    "objective function will calculate gradients and hessians",
    "random bagging, minimal unit is one record",
    "if need bagging",
    "set bagging data to tree learner",
    "get subset",
    "output used time per iteration",
    "boosting from average label; or customized \"average\" if implemented for the current objective",
    "boosting first",
    "bagging logic",
    "need to copy gradients for bagging subset.",
    "shrinkage by learning rate",
    "update score",
    "only add default score one-time",
    "updates scores",
    "add model",
    "reset score",
    "remove model",
    "print message for metric",
    "pop last early_stopping_round_ models",
    "update training score",
    "we need to predict out-of-bag scores of data for boosting",
    "update validation score",
    "print training metric",
    "print validation metric",
    "set zero",
    "predict all the trees for one iteration",
    "check early stopping",
    "push training metrics",
    "not same training data, need reset score and others",
    "create score tracker",
    "update score",
    "create buffer for gradients and hessians",
    "if need bagging, create buffer",
    "Get the max size of pool",
    "at least need 2 leaves",
    "push split information for all leaves",
    "get ordered bin",
    "check existing for ordered bin",
    "initialize splits for leaf",
    "initialize data partition",
    "initialize ordered gradients and hessians",
    "if has ordered bin, need to allocate a buffer to fast split",
    "get ordered bin",
    "initialize splits for leaf",
    "initialize data partition",
    "initialize ordered gradients and hessians",
    "if has ordered bin, need to allocate a buffer to fast split",
    "Get the max size of pool",
    "at least need 2 leaves",
    "push split information for all leaves",
    "some initial works before training",
    "root leaf",
    "only root leaf can be splitted on first time",
    "some initial works before finding best split",
    "find best threshold for every feature",
    "Get a leaf with max split gain",
    "Get split information for best leaf",
    "cannot split, quit",
    "split tree with best leaf",
    "avoid zero hessians.",
    "reset histogram pool",
    "initialize used features",
    "Get used feature at current tree",
    "initialize data partition",
    "reset the splits for leaves",
    "Sumup for root",
    "use all data",
    "use bagging, only use part of data",
    "if has ordered bin, need to initialize the ordered bin",
    "use all data, pass nullptr",
    "bagging, only use part of data",
    "mark used data",
    "initialize ordered bin",
    "check depth of current leaf",
    "only need to check left leaf, since right leaf is in same level of left leaf",
    "no enough data to continue",
    "only have root",
    "put parent(left) leaf's histograms into larger leaf's histograms",
    "put parent(left) leaf's histograms to larger leaf's histograms",
    "split for the ordered bin",
    "mark data that at left-leaf",
    "split the ordered bin",
    "construct smaller leaf",
    "construct larger leaf",
    "find splits",
    "only has root leaf",
    "find best threshold for larger child",
    "left = parent",
    "split tree, will return right leaf",
    "init the leaves that used on next iteration",
    "get feature partition",
    "get local used features",
    "get best split at smaller leaf",
    "find local best split for larger leaf",
    "sync global best info",
    "update best split",
    "instantiate template classes, otherwise linker cannot find the code",
    "initialize SerialTreeLearner",
    "Get local rank and global machine size",
    "allocate buffer for communication",
    "generate feature partition for current tree",
    "get local used feature",
    "get block start and block len for reduce scatter",
    "get buffer_write_start_pos_",
    "get buffer_read_start_pos_",
    "sync global data sumup info",
    "global sumup reduce",
    "copy back",
    "set global sumup info",
    "init global data count in leaf",
    "construct local histograms",
    "copy to buffer",
    "Reduce scatter for histogram",
    "restore global histograms from buffer",
    "find best threshold for smaller child",
    "only root leaf",
    "construct histgroms for large leaf, we init larger leaf as the parent, so we can just subtract the smaller leaf's histograms",
    "find best threshold for larger child",
    "find local best split for larger leaf",
    "sync global best info",
    "set best split",
    "need update global number of data in leaf",
    "instantiate template classes, otherwise linker cannot find the code",
    "initialize SerialTreeLearner",
    "some additional variables needed for GPU trainer",
    "Initialize GPU buffers and kernels",
    "some functions used for debugging the GPU histogram construction",
    "printf(\"grad %g != %g (%d ULPs)\\n\", h1[i].sum_gradients, h2[i].sum_gradients, ulps);",
    "goto err;",
    "printf(\"hessian %g != %g (%d ULPs)\\n\", h1[i].sum_hessians, h2[i].sum_hessians, ulps);",
    "goto err;",
    "we roughly want 256 workgroups per device, and we have num_dense_feature4_ feature tuples.",
    "also guarantee that there are at least 2K examples per workgroup",
    "return 0;",
    "we have already copied ordered gradients, ordered hessians and indices to GPU",
    "decide the best number of workgroups working on one feature4 tuple",
    "set work group size based on feature size",
    "each 2^exp_workgroups_per_feature workgroups work on a feature4 tuple",
    "we need to refresh the kernel arguments after reallocating",
    "The only argument that needs to be changed later is num_data_",
    "the GPU kernel will process all features in one call, and each",
    "2^exp_workgroups_per_feature (compile time constant) workgroup will",
    "process one feature4 tuple",
    "for the root node, indices are not copied",
    "for constant hessian, hessians are not copied except for the root node",
    "there will be 2^exp_workgroups_per_feature = num_workgroups / num_dense_feature4 sub-histogram per feature4",
    "and we will launch num_feature workgroups for this kernel",
    "will launch threads for all features",
    "the queue should be asynchrounous, and we will can WaitAndGetHistograms() before we start processing dense feature groups",
    "copy the results asynchronously. Size depends on if double precision is used",
    "we will wait for this object in WaitAndGetHistograms",
    "when the output is ready, the computation is done",
    "values of this feature has been redistributed to multiple bins; need a reduction here",
    "how many feature-group tuples we have",
    "leave some safe margin for prefetching",
    "256 work-items per workgroup. Each work-item prefetches one tuple for that feature",
    "clear sparse/dense maps",
    "do nothing if no features can be processed on GPU",
    "allocate memory for all features (FIXME: 4 GB barrier on some devices, need to split to multiple buffers)",
    "unpin old buffer if necessary before destructing them",
    "make ordered_gradients and hessians larger (including extra room for prefetching), and pin them",
    "allocate space for gradients and hessians on device",
    "we will copy gradients and hessians in after ordered_gradients_ and ordered_hessians_ are constructed",
    "allocate feature mask, for disabling some feature-groups' histogram calculation",
    "copy indices to the device",
    "histogram bin entry size depends on the precision (single/double)",
    "create output buffer, each feature has a histogram with device_bin_size_ bins,",
    "each work group generates a sub-histogram of dword_features_ features.",
    "only initialize once here, as this will not need to change when ResetTrainingData() is called",
    "create atomic counters for inter-group coordination",
    "The output buffer is allocated to host directly, to overlap compute and data transfer",
    "find the dense feature-groups and group then into Feature4 data structure (several feature-groups packed into 4 bytes)",
    "looking for dword_features_ non-sparse feature-groups",
    "decide if we need to redistribute the bin",
    "multiplier must be a power of 2",
    "device_bin_mults_.push_back(1);",
    "found",
    "for data transfer time",
    "Now generate new data structure feature4, and copy data to the device",
    "preallocate arrays for all threads, and pin them",
    "building Feature4 bundles; each thread handles dword_features_ features",
    "one feature datapoint is 4 bits",
    "this guarantees that the RawGet() function is inlined, rather than using virtual function dispatching",
    "one feature datapoint is one byte",
    "this guarantees that the RawGet() function is inlined, rather than using virtual function dispatching",
    "Dense bin",
    "Dense 4-bit bin",
    "working on the remaining (less than dword_features_) feature groups",
    "fill the leftover features",
    "fill this empty feature with some \"random\" value",
    "fill this empty feature with some \"random\" value",
    "copying the last 1 to (dword_features - 1) feature-groups in the last tuple",
    "deallocate pinned space for feature copying",
    "data transfer time",
    "for other types of failure, build log might not be available; program.build_log() can crash",
    "Something bad happened. Just return \"No log available.\"",
    "build is okay, log may contain warnings",
    "destroy any old kernels",
    "create OpenCL kernels for different number of workgroups per feature",
    "currently we don't use constant memory",
    "compile the GPU kernel depending if double precision is used, constant hessian is used, etc",
    "kernel with indices in an array",
    "kernel with all features enabled, with elimited branches",
    "kernel with all data indices (for root node, and assumes that root node always uses all features)",
    "do nothing if no features can be processed on GPU",
    "The only argument that needs to be changed later is num_data_",
    "hessian is passed as a parameter, but it is not available now.",
    "hessian will be set in BeforeTrain()",
    "Get the max bin size, used for selecting best GPU kernel",
    "initialize GPU",
    "determine which kernel to use based on the max number of bins",
    "setup GPU kernel arguments after we allocating all the buffers",
    "check if we need to recompile the GPU kernel (is_constant_hessian changed)",
    "this should rarely occur",
    "GPU memory has to been reallocated because data may have been changed",
    "setup GPU kernel arguments after we allocating all the buffers",
    "Copy initial full hessians and gradients to GPU.",
    "We start copying as early as possible, instead of at ConstructHistogram().",
    "setup hessian parameters only",
    "hessian is passed as a parameter",
    "use bagging",
    "On GPU, we start copying indices, gradients and hessians now, instead at ConstructHistogram()",
    "copy used gradients and hessians to ordered buffer",
    "transfer the indices to GPU",
    "transfer hessian to GPU",
    "setup hessian parameters only",
    "hessian is passed as a parameter",
    "transfer gradients to GPU",
    "only have root",
    "Copy indices, gradients and hessians as early as possible",
    "only need to initialize for smaller leaf",
    "Get leaf boundary",
    "copy indices to the GPU:",
    "copy ordered hessians to the GPU:",
    "copy ordered gradients to the GPU:",
    "do nothing if no features can be processed on GPU",
    "copy data indices if it is not null",
    "generate and copy ordered_gradients if gradients is not null",
    "generate and copy ordered_hessians if hessians is not null",
    "converted indices in is_feature_used to feature-group indices",
    "construct the feature masks for dense feature-groups",
    "if no feature group is used, just return and do not use GPU",
    "if not all feature groups are used, we need to transfer the feature mask to GPU",
    "otherwise, we will use a specialized GPU kernel with all feature groups enabled",
    "All data have been prepared, now run the GPU kernel",
    "construct smaller leaf",
    "ConstructGPUHistogramsAsync will return true if there are availabe feature gourps dispatched to GPU",
    "then construct sparse features on CPU",
    "We set data_indices to null to avoid rebuilding ordered gradients/hessians",
    "wait for GPU to finish, only if GPU is actually used",
    "use double precision",
    "use single precision",
    "Compare GPU histogram with CPU histogram, useful for debuggin GPU code problem",
    "#define GPU_DEBUG_COMPARE",
    "construct larger leaf",
    "then construct sparse features on CPU",
    "We set data_indices to null to avoid rebuilding ordered gradients/hessians",
    "wait for GPU to finish, only if GPU is actually used",
    "use double precision",
    "use single precision",
    "do some sanity check for the GPU algorithm",
    "limit top k",
    "get max bin",
    "calculate buffer size",
    "left and right on same time, so need double size",
    "initialize histograms for global",
    "sync global data sumup info",
    "set global sumup info",
    "init global data count in leaf",
    "get local sumup",
    "get local sumup",
    "get mean number on machines",
    "weighted gain",
    "get top k",
    "Copy histogram to buffer, and Get local aggregate features",
    "copy histograms.",
    "copy smaller leaf histograms first",
    "mark local aggregated feature",
    "copy",
    "then copy larger leaf histograms",
    "mark local aggregated feature",
    "copy",
    "use local data to find local best splits",
    "find splits",
    "only has root leaf",
    "find best threshold for larger child",
    "local voting",
    "gather",
    "get all top-k from all machines",
    "global voting",
    "copy local histgrams to buffer",
    "Reduce scatter for histogram",
    "find best split from local aggregated histograms",
    "restore from buffer",
    "find best threshold",
    "restore from buffer",
    "find best threshold",
    "find local best",
    "find local best split for larger leaf",
    "sync global best info",
    "copy back",
    "set the global number of data for leaves",
    "init the global sumup info",
    "instantiate template classes, otherwise linker cannot find the code"
  ],
  "v2.0.6": [
    "coding: utf-8",
    "pylint: disable=invalid-name, exec-used, C0111",
    "coding: utf-8",
    "pylint: disable = invalid-name, W0105",
    "Most of legacy advanced options becomes callbacks",
    "check evaluation result.",
    "lambdarank task, split according to groups",
    "run preprocessing on the data set if needed",
    "setup callbacks",
    "coding: utf-8",
    "pylint: disable = C0103",
    "simplejson does not support Python 3.2, it throws a SyntaxError",
    "because of u'...' Unicode literals.",
    "coding: utf-8",
    "pylint: disable = invalid-name, W0105, C0111, C0301",
    "minor change to support `**kwargs`",
    "user can set verbose with kwargs, it has higher priority",
    "free dataset",
    "Switch to using a multiclass objective in the underlying LGBM instance",
    "coding: utf-8",
    "we don't need lib_lightgbm while building docs",
    "coding: utf-8",
    "pylint: disable = C0103",
    "coding: utf-8",
    "coding: utf-8",
    "pylint: disable = invalid-name, C0111, C0301",
    "pylint: disable = R0912, R0913, R0914, W0105, W0201, W0212",
    "TypeError: obj is not a string or a number",
    "ValueError: invalid literal",
    "load init score",
    "need re group init score",
    "set feature names",
    "we're done if self and reference share a common upstrem reference",
    "group data from LightGBM is boundaries data, need to convert to group size",
    "coding: utf-8",
    "pylint: disable = invalid-name, W0105, C0301",
    "Callback environment used by callbacks",
    "coding: utf-8",
    "pylint: disable = invalid-name, C0111",
    "load or create your dataset",
    "create dataset for lightgbm",
    "if you want to re-use data, remember to set free_raw_data=False",
    "specify your configurations as a dict",
    "generate a feature name",
    "feature_name and categorical_feature",
    "check feature name",
    "save model to file",
    "load model to predict",
    "can only predict with the best iteration (or the saving iteration)",
    "eval with loaded model",
    "dump model with pickle",
    "load model with pickle to predict",
    "can predict with any iteration when loaded in pickle way",
    "eval with loaded model",
    "continue training",
    "init_model accepts:",
    "1. model file name",
    "2. Booster()",
    "decay learning rates",
    "learning_rates accepts:",
    "1. list/tuple with length = num_boost_round",
    "2. function(curr_iter)",
    "change other parameters during training",
    "self-defined objective function",
    "f(preds: array, train_data: Dataset) -> grad: array, hess: array",
    "log likelihood loss",
    "self-defined eval metric",
    "f(preds: array, train_data: Dataset) -> name: string, value: array, is_higher_better: bool",
    "binary error",
    "callback",
    "coding: utf-8",
    "pylint: disable = invalid-name, C0111",
    "load or create your dataset",
    "train",
    "predict",
    "eval",
    "feature importances",
    "other scikit-learn modules",
    "coding: utf-8",
    "pylint: disable = invalid-name, C0111",
    "load or create your dataset",
    "create dataset for lightgbm",
    "specify your configurations as a dict",
    "train",
    "coding: utf-8",
    "pylint: disable = invalid-name, C0111",
    "load or create your dataset",
    "create dataset for lightgbm",
    "specify your configurations as a dict",
    "train",
    "save model to file",
    "predict",
    "eval",
    "dump model to json (and save to file)",
    "feature importances",
    "coding: utf-8",
    "pylint: disable = C0111, C0103",
    "print out the pmml for a decision tree",
    "specify the objective as function name and binarySplit for",
    "splitCharacteristic because each node has 2 children",
    "list each feature name as a mining field, and treat all outliers as is,",
    "unless specified",
    "begin printing out the decision tree",
    "open the model file and then process it",
    "ignore first 6 and empty lines",
    "print out data dictionary entries for each column",
    "not adding any interval definition, all values are currently",
    "valid",
    "list each feature name as a mining field, and treat all outliers",
    "as is, unless specified",
    "read each array that contains pertinent information for the pmml",
    "these arrays will be used to recreate the traverse the decision tree",
    "!/usr/bin/env python3",
    "-*- coding: utf-8 -*-",
    "",
    "LightGBM documentation build configuration file, created by",
    "sphinx-quickstart on Thu May  4 14:30:58 2017.",
    "",
    "This file is execfile()d with the current directory set to its",
    "containing dir.",
    "",
    "Note that not all possible configuration values are present in this",
    "autogenerated file.",
    "",
    "All configuration values have a default; values that are commented out",
    "serve to show the default.",
    "If extensions (or modules to document with autodoc) are in another directory,",
    "add these directories to sys.path here. If the directory is relative to the",
    "documentation root, use os.path.abspath to make it absolute, like shown here.",
    "",
    "-- mock out modules",
    "-- General configuration ------------------------------------------------",
    "If your documentation needs a minimal Sphinx version, state it here.",
    "",
    "needs_sphinx = '1.0'",
    "Add any Sphinx extension module names here, as strings. They can be",
    "extensions coming with Sphinx (named 'sphinx.ext.*') or your custom",
    "ones.",
    "Add any paths that contain templates here, relative to this directory.",
    "The suffix(es) of source filenames.",
    "You can specify multiple suffix as a list of string:",
    "The master toctree document.",
    "General information about the project.",
    "The version info for the project you're documenting, acts as replacement for",
    "|version| and |release|, also used in various other places throughout the",
    "built documents.",
    "",
    "The short X.Y version.",
    "The full version, including alpha/beta/rc tags.",
    "The language for content autogenerated by Sphinx. Refer to documentation",
    "for a list of supported languages.",
    "",
    "This is also used if you do content translation via gettext catalogs.",
    "Usually you set \"language\" from the command line for these cases.",
    "List of patterns, relative to source directory, that match files and",
    "directories to ignore when looking for source files.",
    "This patterns also effect to html_static_path and html_extra_path",
    "The name of the Pygments (syntax highlighting) style to use.",
    "If true, `todo` and `todoList` produce output, else they produce nothing.",
    "Both the class' and the __init__ method's docstring are concatenated and inserted.",
    "-- Options for HTML output ----------------------------------------------",
    "The theme to use for HTML and HTML Help pages.  See the documentation for",
    "a list of builtin themes.",
    "",
    "Theme options are theme-specific and customize the look and feel of a theme",
    "further.  For a list of options available for each theme, see the",
    "documentation.",
    "",
    "html_theme_options = {}",
    "Add any paths that contain custom static files (such as style sheets) here,",
    "relative to this directory. They are copied after the builtin static files,",
    "so a file named \"default.css\" will overwrite the builtin \"default.css\".",
    "-- Options for HTMLHelp output ------------------------------------------",
    "Output file base name for HTML help builder.",
    "-- Options for LaTeX output ---------------------------------------------",
    "The paper size ('letterpaper' or 'a4paper').",
    "",
    "'papersize': 'letterpaper',",
    "The font size ('10pt', '11pt' or '12pt').",
    "",
    "'pointsize': '10pt',",
    "Additional stuff for the LaTeX preamble.",
    "",
    "'preamble': '',",
    "Latex figure (float) alignment",
    "",
    "'figure_align': 'htbp',",
    "Grouping the document tree into LaTeX files. List of tuples",
    "(source start file, target name, title,",
    "author, documentclass [howto, manual, or own class]).",
    "-- Options for manual page output ---------------------------------------",
    "One entry per manual page. List of tuples",
    "(source start file, name, description, authors, manual section).",
    "-- Options for Texinfo output -------------------------------------------",
    "Grouping the document tree into Texinfo files. List of tuples",
    "(source start file, target name, title, author,",
    "dir menu entry, description, category)",
    "https://recommonmark.readthedocs.io/en/latest/",
    "coding: utf-8",
    "coding: utf-8",
    "pylint: skip-file",
    "we don't need lib_lightgbm while building docs",
    "coding: utf-8",
    "pylint: skip-file",
    "check saved model persistence",
    "we need to check the consistency of model file here, so test for exact equal",
    "check early stopping is working. Make it stop very early, so the scores should be very close to zero",
    "scores likely to be different, but prediction should still be the same",
    "coding: utf-8",
    "pylint: skip-file",
    "coding: utf-8",
    "pylint: skip-file",
    "coding: utf-8",
    "pylint: skip-file",
    "no early stopping",
    "early stopping occurs",
    "test custom eval metrics",
    "test dump model",
    "shuffle = False, override metric in params",
    "shuffle = True, callbacks",
    "self defined folds",
    "lambdarank",
    "test feature_names with whitespaces",
    "take subsets and train",
    "convert from one-based to  zero-based index",
    "convert from boundaries to size",
    "--- start Booster interfaces",
    "create boosting",
    "initialize the boosting",
    "create objective function",
    "initialize the objective function",
    "create training metric",
    "reset the boosting",
    "create objective function",
    "initialize the objective function",
    "some help functions used to convert data",
    "Row iterator of on column for CSC matrix",
    "return value at idx, only can access by ascent order",
    "return next non-zero pair, if index < 0, means no more data",
    "start of c_api functions",
    "sample data first",
    "sample data first",
    "sample data first",
    "no more data",
    "---- start of booster",
    "---- start of some help functions",
    "set number of threads for openmp",
    "check for alias",
    "read parameters from config file",
    "remove str after \"#\"",
    "Command-line has higher priority",
    "check for alias again",
    "load configs",
    "prediction is needed if using input initial model(continued train)",
    "need to continue training",
    "sync up random seed for data partition",
    "load Training data",
    "load data for parallel training",
    "load data for single machine",
    "need save binary file",
    "create training metric",
    "only when have metrics then need to construct validation data",
    "Add validation data, if it exists",
    "add",
    "need save binary file",
    "add metric for validation data",
    "output used time on each iteration",
    "need init network",
    "create boosting",
    "create objective function",
    "load training data",
    "initialize the objective function",
    "initialize the boosting",
    "add validation data into boosting",
    "convert model to if-else statement code",
    "create predictor",
    "counts for all labels",
    "start from top label, and accumulate DCG",
    "counts for all labels",
    "calculate k Max DCG by one pass",
    "get sorted indices by score",
    "calculate dcg",
    "get sorted indices by score",
    "calculate multi dcg by one pass",
    "wait for all client start up",
    "default set to -1",
    "distance at k-th communication, distance[k] = 2^k",
    "set incoming rank at k-th commuication",
    "set outgoing rank at k-th commuication",
    "defalut set as -1",
    "construct all recursive halving map for all machines",
    "let 1 << k <= num_machines",
    "distance of each communication",
    "if num_machines = 2^k, don't need to group machines",
    "communication direction, %2 == 0 is positive",
    "neighbor at k-th communication",
    "receive data block at k-th communication",
    "send data block at k-th communication",
    "static member definition",
    "if small package or small count , do it by all gather.(reduce the communication times.)",
    "assign the blocks to every rank.",
    "do reduce scatter",
    "do all gather",
    "assign blocks",
    "need use buffer here, since size of \"output\" is smaller than size after all gather",
    "copy back",
    "assign blocks",
    "start all gather",
    "use output as receive buffer",
    "get current local block size",
    "get out rank",
    "get in rank",
    "get send information",
    "get recv information",
    "send and recv at same time",
    "rotate in-place",
    "get target",
    "get send information",
    "get recv information",
    "send and recv at same time",
    "reduce",
    "copy result",
    "start up socket",
    "parse clients from file",
    "get ip list of local machine",
    "get local rank",
    "construct listener",
    "construct communication topo",
    "construct linkers",
    "free listener",
    "set timeout",
    "accept incoming socket",
    "receive rank",
    "add new socket",
    "save ranks that need to connect with",
    "start listener",
    "start connect",
    "let smaller rank connect to larger rank",
    "send local rank",
    "wait for listener",
    "print connected linkers",
    "Get some statistic from 2 line",
    "if only have one line on file",
    "get column names",
    "load label idx first",
    "erase label column name",
    "load ignore columns",
    "load weight idx",
    "load group idx",
    "don't support query id in data file when training in parallel",
    "read data to memory",
    "sample data",
    "construct feature bin mappers",
    "initialize label",
    "extract features",
    "sample data from file",
    "construct feature bin mappers",
    "initialize label",
    "extract features",
    "load data from binary file",
    "check meta data",
    "need to check training data",
    "read data in memory",
    "initialize label",
    "extract features",
    "Get number of lines of data file",
    "initialize label",
    "extract features",
    "load data from binary file",
    "not need to check validation data",
    "check meta data",
    "buffer to read binary file",
    "check token",
    "read size of header",
    "re-allocmate space if not enough",
    "read header",
    "get header",
    "num_groups",
    "real_feature_idx_",
    "feature2group",
    "feature2subfeature",
    "group_bin_boundaries",
    "group_feature_start_",
    "group_feature_cnt_",
    "get feature names",
    "write feature names",
    "read size of meta data",
    "re-allocate space if not enough",
    "read meta data",
    "load meta data",
    "sample local used data if need to partition",
    "if not contain query file, minimal sample unit is one record",
    "if contain query file, minimal sample unit is one query",
    "if is new query",
    "read feature data",
    "read feature size",
    "re-allocate space if not enough",
    "fill feature_names_ if not header",
    "---- private functions ----",
    "if features are ordered, not need to use hist_buf",
    "read all lines",
    "get query data",
    "if not contain query data, minimal sample unit is one record",
    "if contain query data, minimal sample unit is one query",
    "if is new query",
    "get query data",
    "if not contain query file, minimal sample unit is one record",
    "if contain query file, minimal sample unit is one query",
    "if is new query",
    "parse features",
    "-1 means doesn't use this feature",
    "check the range of label_idx, weight_idx and group_idx",
    "fill feature_names_ if not header",
    "start find bins",
    "if only one machine, find bin locally",
    "if have multi-machines, need to find bin distributed",
    "different machines will find bin for different features",
    "start and len will store the process feature indices for different machines",
    "machine i will find bins for features in [ start[i], start[i] + len[i] )",
    "get size of bin mapper with max_bin size",
    "since sizes of different feature may not be same, we expand all bin mapper to type_size",
    "find local feature bins and copy to buffer",
    "free",
    "convert to binary size",
    "gather global feature bin mappers",
    "restore features bins from buffer",
    "if doesn't need to prediction with initial model",
    "parser",
    "set label",
    "free processed line:",
    "shrink_to_fit will be very slow in linux, and seems not free memory, disable for now",
    "text_reader_->Lines()[i].shrink_to_fit();",
    "push data",
    "if is used feature",
    "if need to prediction with initial model",
    "parser",
    "set initial score",
    "set label",
    "free processed line:",
    "shrink_to_fit will be very slow in linux, and seems not free memory, disable for now",
    "text_reader_->Lines()[i].shrink_to_fit();",
    "push data",
    "if is used feature",
    "metadata_ will manage space of init_score",
    "text data can be free after loaded feature values",
    "parser",
    "set initial score",
    "set label",
    "push data",
    "if is used feature",
    "only need part of data",
    "need full data",
    "metadata_ will manage space of init_score",
    "read size of token",
    "deep copy function for BinMapper",
    "mean size for one bin",
    "need a new bin",
    "update bin upper bound",
    "last bin upper bound",
    "find distinct_values first",
    "push zero in the front",
    "push zero in the back",
    "convert to int type first",
    "sort by counts",
    "avoid first bin is zero",
    "will ignore the categorical of small counts",
    "need an additional bin for NaN",
    "use -1 to represent NaN",
    "Use MissingType::None to represent this bin contains all categoricals",
    "check trival(num_bin_ == 1) feature",
    "check useless bin",
    "calculate sparse rate",
    "sparse threshold",
    "for lambdarank, it needs query data for partition data in parallel learning",
    "need convert query_id to boundaries",
    "check weights",
    "check query boundries",
    "contain initial score file",
    "check weights",
    "get local weights",
    "check query boundries",
    "get local query boundaries",
    "contain initial score file",
    "get local initial scores",
    "re-load query weight",
    "save to nullptr",
    "save to nullptr",
    "save to nullptr",
    "default weight file name",
    "default weight file name",
    "use first line to count number class",
    "default query file name",
    "root is in the depth 0",
    "non-leaf",
    "leaf",
    "use this for the missing value conversion",
    "non-leaf",
    "left subtree",
    "right subtree",
    "leaf",
    "clear old metrics",
    "to lower",
    "split",
    "remove duplicate",
    "load main config types",
    "generate seeds by seed.",
    "sub-config setup",
    "check for conflicts",
    "check if objective_type, metric_type, and num_class match",
    "Change pool size to -1 (no limit) when using data parallel to reduce communication costs",
    "Check max_depth and num_leaves",
    "label_gain = 2^i - 1, may overflow, so we use 31 here",
    "label_gain = 2^i - 1, may overflow, so we use 31 here",
    "default eval ndcg @[1-5]",
    "filter is based on sampling data, so decrease its range",
    "put dense feature first",
    "sort by non zero cnt",
    "sort by non zero cnt, bigger first",
    "take apart small sparse group, due it will not gain on speed",
    "shuffle groups",
    "get num_features",
    "get bin_mappers",
    "copy feature bin mapper data",
    "copy feature bin mapper data",
    "if not pass a filename, just append \".bin\" of original file",
    "get size of header",
    "size of feature names",
    "write header",
    "write feature names",
    "get size of meta data",
    "write meta data",
    "write feature data",
    "get size of feature",
    "write feature",
    "feature is not used",
    "construct histograms for smaller leaf",
    "if not use ordered bin",
    "used ordered bin",
    "feature is not used",
    "construct histograms for smaller leaf",
    "if not use ordered bin",
    "used ordered bin",
    "fixed hessian.",
    "feature is not used",
    "construct histograms for smaller leaf",
    "if not use ordered bin",
    "used ordered bin",
    "feature is not used",
    "construct histograms for smaller leaf",
    "if not use ordered bin",
    "used ordered bin",
    "fixed hessian.",
    "set zero",
    "predict all the trees for one iteration",
    "check early stopping",
    "margin_threshold will be captured by value",
    "copy and sort",
    "margin_threshold will be captured by value",
    "init tree learner",
    "push training metrics",
    "create buffer for gradients and hessians",
    "get max feature index",
    "get label index",
    "get feature names",
    "if need bagging, create buffer",
    "reset config for tree learner",
    "+ 1 here for the binary classification",
    "multi-class",
    "binary class",
    "push training metrics",
    "not same training data, need reset score and others",
    "create score tracker",
    "update score",
    "create buffer for gradients and hessians",
    "get max feature index",
    "get label index",
    "get feature names",
    "if need bagging, create buffer",
    "for a validation dataset, we need its score and metric",
    "update score",
    "random bagging, minimal unit is one record",
    "if need bagging",
    "set bagging data to tree learner",
    "get subset",
    "we need to predict out-of-bag scores of data for boosting",
    "output used time per iteration",
    "boosting from average label; or customized \"average\" if implemented for the current objective",
    "boosting first",
    "bagging logic",
    "need to use subset gradient and hessian",
    "get sub gradients",
    "cannot multi-threading here.",
    "shrinkage by learning rate",
    "update score",
    "only add default score one-time",
    "add model",
    "reset score",
    "remove model",
    "print message for metric",
    "pop last early_stopping_round_ models",
    "update training score",
    "update validation score",
    "print training metric",
    "print validation metric",
    "set zero",
    "predict all the trees for one iteration",
    "check early stopping",
    "objective function will calculate gradients and hessians",
    "PredictRaw",
    "Predict",
    "PredictLeafIndex",
    "output model type",
    "output number of class",
    "output label index",
    "output max_feature_idx",
    "output objective",
    "output tree models",
    "use serialized string to restore this object",
    "get number of classes",
    "get index of label",
    "get max_feature_idx first",
    "get boost_from_average_",
    "get average_output",
    "get feature names",
    "get tree models",
    "store the importance first",
    "sort the importance",
    "Get the max size of pool",
    "at least need 2 leaves",
    "push split information for all leaves",
    "get ordered bin",
    "check existing for ordered bin",
    "initialize splits for leaf",
    "initialize data partition",
    "initialize ordered gradients and hessians",
    "if has ordered bin, need to allocate a buffer to fast split",
    "get ordered bin",
    "initialize splits for leaf",
    "initialize data partition",
    "initialize ordered gradients and hessians",
    "if has ordered bin, need to allocate a buffer to fast split",
    "Get the max size of pool",
    "at least need 2 leaves",
    "push split information for all leaves",
    "some initial works before training",
    "root leaf",
    "only root leaf can be splitted on first time",
    "some initial works before finding best split",
    "find best threshold for every feature",
    "Get a leaf with max split gain",
    "Get split information for best leaf",
    "cannot split, quit",
    "split tree with best leaf",
    "avoid zero hessians.",
    "reset histogram pool",
    "initialize used features",
    "Get used feature at current tree",
    "initialize data partition",
    "reset the splits for leaves",
    "Sumup for root",
    "use all data",
    "use bagging, only use part of data",
    "if has ordered bin, need to initialize the ordered bin",
    "use all data, pass nullptr",
    "bagging, only use part of data",
    "mark used data",
    "initialize ordered bin",
    "check depth of current leaf",
    "only need to check left leaf, since right leaf is in same level of left leaf",
    "no enough data to continue",
    "only have root",
    "put parent(left) leaf's histograms into larger leaf's histograms",
    "put parent(left) leaf's histograms to larger leaf's histograms",
    "split for the ordered bin",
    "mark data that at left-leaf",
    "split the ordered bin",
    "construct smaller leaf",
    "construct larger leaf",
    "find splits",
    "only has root leaf",
    "find best threshold for larger child",
    "left = parent",
    "split tree, will return right leaf",
    "init the leaves that used on next iteration",
    "get feature partition",
    "get local used features",
    "get best split at smaller leaf",
    "find local best split for larger leaf",
    "sync global best info",
    "update best split",
    "instantiate template classes, otherwise linker cannot find the code",
    "initialize SerialTreeLearner",
    "Get local rank and global machine size",
    "allocate buffer for communication",
    "generate feature partition for current tree",
    "get local used feature",
    "get block start and block len for reduce scatter",
    "get buffer_write_start_pos_",
    "get buffer_read_start_pos_",
    "sync global data sumup info",
    "global sumup reduce",
    "copy back",
    "set global sumup info",
    "init global data count in leaf",
    "construct local histograms",
    "copy to buffer",
    "Reduce scatter for histogram",
    "restore global histograms from buffer",
    "find best threshold for smaller child",
    "only root leaf",
    "construct histgroms for large leaf, we init larger leaf as the parent, so we can just subtract the smaller leaf's histograms",
    "find best threshold for larger child",
    "find local best split for larger leaf",
    "sync global best info",
    "set best split",
    "need update global number of data in leaf",
    "instantiate template classes, otherwise linker cannot find the code",
    "initialize SerialTreeLearner",
    "some additional variables needed for GPU trainer",
    "Initialize GPU buffers and kernels",
    "some functions used for debugging the GPU histogram construction",
    "printf(\"grad %g != %g (%d ULPs)\\n\", h1[i].sum_gradients, h2[i].sum_gradients, ulps);",
    "goto err;",
    "printf(\"hessian %g != %g (%d ULPs)\\n\", h1[i].sum_hessians, h2[i].sum_hessians, ulps);",
    "goto err;",
    "we roughly want 256 workgroups per device, and we have num_dense_feature4_ feature tuples.",
    "also guarantee that there are at least 2K examples per workgroup",
    "return 0;",
    "we have already copied ordered gradients, ordered hessians and indices to GPU",
    "decide the best number of workgroups working on one feature4 tuple",
    "set work group size based on feature size",
    "each 2^exp_workgroups_per_feature workgroups work on a feature4 tuple",
    "we need to refresh the kernel arguments after reallocating",
    "The only argument that needs to be changed later is num_data_",
    "the GPU kernel will process all features in one call, and each",
    "2^exp_workgroups_per_feature (compile time constant) workgroup will",
    "process one feature4 tuple",
    "for the root node, indices are not copied",
    "for constant hessian, hessians are not copied except for the root node",
    "there will be 2^exp_workgroups_per_feature = num_workgroups / num_dense_feature4 sub-histogram per feature4",
    "and we will launch num_feature workgroups for this kernel",
    "will launch threads for all features",
    "the queue should be asynchrounous, and we will can WaitAndGetHistograms() before we start processing dense feature groups",
    "copy the results asynchronously. Size depends on if double precision is used",
    "we will wait for this object in WaitAndGetHistograms",
    "when the output is ready, the computation is done",
    "values of this feature has been redistributed to multiple bins; need a reduction here",
    "how many feature-group tuples we have",
    "leave some safe margin for prefetching",
    "256 work-items per workgroup. Each work-item prefetches one tuple for that feature",
    "clear sparse/dense maps",
    "do nothing if no features can be processed on GPU",
    "allocate memory for all features (FIXME: 4 GB barrier on some devices, need to split to multiple buffers)",
    "unpin old buffer if necessary before destructing them",
    "make ordered_gradients and hessians larger (including extra room for prefetching), and pin them",
    "allocate space for gradients and hessians on device",
    "we will copy gradients and hessians in after ordered_gradients_ and ordered_hessians_ are constructed",
    "allocate feature mask, for disabling some feature-groups' histogram calculation",
    "copy indices to the device",
    "histogram bin entry size depends on the precision (single/double)",
    "create output buffer, each feature has a histogram with device_bin_size_ bins,",
    "each work group generates a sub-histogram of dword_features_ features.",
    "only initialize once here, as this will not need to change when ResetTrainingData() is called",
    "create atomic counters for inter-group coordination",
    "The output buffer is allocated to host directly, to overlap compute and data transfer",
    "find the dense feature-groups and group then into Feature4 data structure (several feature-groups packed into 4 bytes)",
    "looking for dword_features_ non-sparse feature-groups",
    "decide if we need to redistribute the bin",
    "multiplier must be a power of 2",
    "device_bin_mults_.push_back(1);",
    "found",
    "for data transfer time",
    "Now generate new data structure feature4, and copy data to the device",
    "preallocate arrays for all threads, and pin them",
    "building Feature4 bundles; each thread handles dword_features_ features",
    "one feature datapoint is 4 bits",
    "this guarantees that the RawGet() function is inlined, rather than using virtual function dispatching",
    "one feature datapoint is one byte",
    "this guarantees that the RawGet() function is inlined, rather than using virtual function dispatching",
    "Dense bin",
    "Dense 4-bit bin",
    "working on the remaining (less than dword_features_) feature groups",
    "fill the leftover features",
    "fill this empty feature with some \"random\" value",
    "fill this empty feature with some \"random\" value",
    "copying the last 1 to (dword_features - 1) feature-groups in the last tuple",
    "deallocate pinned space for feature copying",
    "data transfer time",
    "for other types of failure, build log might not be available; program.build_log() can crash",
    "Something bad happened. Just return \"No log available.\"",
    "build is okay, log may contain warnings",
    "destroy any old kernels",
    "create OpenCL kernels for different number of workgroups per feature",
    "currently we don't use constant memory",
    "compile the GPU kernel depending if double precision is used, constant hessian is used, etc",
    "kernel with indices in an array",
    "kernel with all features enabled, with elimited branches",
    "kernel with all data indices (for root node, and assumes that root node always uses all features)",
    "do nothing if no features can be processed on GPU",
    "The only argument that needs to be changed later is num_data_",
    "hessian is passed as a parameter, but it is not available now.",
    "hessian will be set in BeforeTrain()",
    "Get the max bin size, used for selecting best GPU kernel",
    "initialize GPU",
    "determine which kernel to use based on the max number of bins",
    "setup GPU kernel arguments after we allocating all the buffers",
    "check if we need to recompile the GPU kernel (is_constant_hessian changed)",
    "this should rarely occur",
    "GPU memory has to been reallocated because data may have been changed",
    "setup GPU kernel arguments after we allocating all the buffers",
    "Copy initial full hessians and gradients to GPU.",
    "We start copying as early as possible, instead of at ConstructHistogram().",
    "setup hessian parameters only",
    "hessian is passed as a parameter",
    "use bagging",
    "On GPU, we start copying indices, gradients and hessians now, instead at ConstructHistogram()",
    "copy used gradients and hessians to ordered buffer",
    "transfer the indices to GPU",
    "transfer hessian to GPU",
    "setup hessian parameters only",
    "hessian is passed as a parameter",
    "transfer gradients to GPU",
    "only have root",
    "Copy indices, gradients and hessians as early as possible",
    "only need to initialize for smaller leaf",
    "Get leaf boundary",
    "copy indices to the GPU:",
    "copy ordered hessians to the GPU:",
    "copy ordered gradients to the GPU:",
    "do nothing if no features can be processed on GPU",
    "copy data indices if it is not null",
    "generate and copy ordered_gradients if gradients is not null",
    "generate and copy ordered_hessians if hessians is not null",
    "converted indices in is_feature_used to feature-group indices",
    "construct the feature masks for dense feature-groups",
    "if no feature group is used, just return and do not use GPU",
    "if not all feature groups are used, we need to transfer the feature mask to GPU",
    "otherwise, we will use a specialized GPU kernel with all feature groups enabled",
    "All data have been prepared, now run the GPU kernel",
    "construct smaller leaf",
    "ConstructGPUHistogramsAsync will return true if there are availabe feature gourps dispatched to GPU",
    "then construct sparse features on CPU",
    "We set data_indices to null to avoid rebuilding ordered gradients/hessians",
    "wait for GPU to finish, only if GPU is actually used",
    "use double precision",
    "use single precision",
    "Compare GPU histogram with CPU histogram, useful for debuggin GPU code problem",
    "#define GPU_DEBUG_COMPARE",
    "construct larger leaf",
    "then construct sparse features on CPU",
    "We set data_indices to null to avoid rebuilding ordered gradients/hessians",
    "wait for GPU to finish, only if GPU is actually used",
    "use double precision",
    "use single precision",
    "do some sanity check for the GPU algorithm",
    "limit top k",
    "get max bin",
    "calculate buffer size",
    "left and right on same time, so need double size",
    "initialize histograms for global",
    "sync global data sumup info",
    "set global sumup info",
    "init global data count in leaf",
    "get local sumup",
    "get local sumup",
    "get mean number on machines",
    "weighted gain",
    "get top k",
    "Copy histogram to buffer, and Get local aggregate features",
    "copy histograms.",
    "copy smaller leaf histograms first",
    "mark local aggregated feature",
    "copy",
    "then copy larger leaf histograms",
    "mark local aggregated feature",
    "copy",
    "use local data to find local best splits",
    "find splits",
    "only has root leaf",
    "find best threshold for larger child",
    "local voting",
    "gather",
    "get all top-k from all machines",
    "global voting",
    "copy local histgrams to buffer",
    "Reduce scatter for histogram",
    "find best split from local aggregated histograms",
    "restore from buffer",
    "find best threshold",
    "restore from buffer",
    "find best threshold",
    "find local best",
    "find local best split for larger leaf",
    "sync global best info",
    "copy back",
    "set the global number of data for leaves",
    "init the global sumup info",
    "instantiate template classes, otherwise linker cannot find the code"
  ],
  "v2.0.3": [
    "coding: utf-8",
    "pylint: disable=invalid-name, exec-used",
    "coding: utf-8",
    "pylint: disable = invalid-name, W0105",
    "Most of legacy advanced options becomes callbacks",
    "check evaluation result.",
    "lambdarank task, split according to groups",
    "run preprocessing on the data set if needed",
    "setup callbacks",
    "coding: utf-8",
    "pylint: disable = C0103",
    "simplejson does not support Python 3.2, it throws a SyntaxError",
    "because of u'...' Unicode literals.",
    "coding: utf-8",
    "pylint: disable = invalid-name, W0105, C0111, C0301",
    "minor change to support `**kwargs`",
    "user can set verbose with kwargs, it has higher priority",
    "free dataset",
    "Switch to using a multiclass objective in the underlying LGBM instance",
    "coding: utf-8",
    "we don't need lib_lightgbm while building docs",
    "coding: utf-8",
    "pylint: disable = C0103",
    "coding: utf-8",
    "coding: utf-8",
    "pylint: disable = invalid-name, C0111, C0301",
    "pylint: disable = R0912, R0913, R0914, W0105, W0201, W0212",
    "TypeError: obj is not a string or a number",
    "ValueError: invalid literal",
    "load init score",
    "need re group init score",
    "set feature names",
    "group data from LightGBM is boundaries data, need to convert to group size",
    "coding: utf-8",
    "pylint: disable = invalid-name, W0105, C0301",
    "Callback environment used by callbacks",
    "coding: utf-8",
    "pylint: disable = invalid-name, C0111",
    "load or create your dataset",
    "create dataset for lightgbm",
    "if you want to re-use data, remember to set free_raw_data=False",
    "specify your configurations as a dict",
    "generate a feature name",
    "feature_name and categorical_feature",
    "check feature name",
    "save model to file",
    "load model to predict",
    "can only predict with the best iteration (or the saving iteration)",
    "eval with loaded model",
    "dump model with pickle",
    "load model with pickle to predict",
    "can predict with any iteration when loaded in pickle way",
    "eval with loaded model",
    "continue training",
    "init_model accepts:",
    "1. model file name",
    "2. Booster()",
    "decay learning rates",
    "learning_rates accepts:",
    "1. list/tuple with length = num_boost_round",
    "2. function(curr_iter)",
    "change other parameters during training",
    "self-defined objective function",
    "f(preds: array, train_data: Dataset) -> grad: array, hess: array",
    "log likelihood loss",
    "self-defined eval metric",
    "f(preds: array, train_data: Dataset) -> name: string, value: array, is_higher_better: bool",
    "binary error",
    "callback",
    "coding: utf-8",
    "pylint: disable = invalid-name, C0111",
    "load or create your dataset",
    "train",
    "predict",
    "eval",
    "feature importances",
    "other scikit-learn modules",
    "coding: utf-8",
    "pylint: disable = invalid-name, C0111",
    "load or create your dataset",
    "create dataset for lightgbm",
    "specify your configurations as a dict",
    "train",
    "coding: utf-8",
    "pylint: disable = invalid-name, C0111",
    "load or create your dataset",
    "create dataset for lightgbm",
    "specify your configurations as a dict",
    "train",
    "save model to file",
    "predict",
    "eval",
    "dump model to json (and save to file)",
    "feature importances",
    "coding: utf-8",
    "pylint: disable = C0111, C0103",
    "print out the pmml for a decision tree",
    "specify the objective as function name and binarySplit for",
    "splitCharacteristic because each node has 2 children",
    "list each feature name as a mining field, and treat all outliers as is,",
    "unless specified",
    "begin printing out the decision tree",
    "open the model file and then process it",
    "ignore first 6 and empty lines",
    "print out data dictionary entries for each column",
    "not adding any interval definition, all values are currently",
    "valid",
    "list each feature name as a mining field, and treat all outliers",
    "as is, unless specified",
    "read each array that contains pertinent information for the pmml",
    "these arrays will be used to recreate the traverse the decision tree",
    "!/usr/bin/env python3",
    "-*- coding: utf-8 -*-",
    "",
    "LightGBM documentation build configuration file, created by",
    "sphinx-quickstart on Thu May  4 14:30:58 2017.",
    "",
    "This file is execfile()d with the current directory set to its",
    "containing dir.",
    "",
    "Note that not all possible configuration values are present in this",
    "autogenerated file.",
    "",
    "All configuration values have a default; values that are commented out",
    "serve to show the default.",
    "If extensions (or modules to document with autodoc) are in another directory,",
    "add these directories to sys.path here. If the directory is relative to the",
    "documentation root, use os.path.abspath to make it absolute, like shown here.",
    "",
    "-- mock out modules",
    "-- General configuration ------------------------------------------------",
    "If your documentation needs a minimal Sphinx version, state it here.",
    "",
    "needs_sphinx = '1.0'",
    "Add any Sphinx extension module names here, as strings. They can be",
    "extensions coming with Sphinx (named 'sphinx.ext.*') or your custom",
    "ones.",
    "Add any paths that contain templates here, relative to this directory.",
    "The suffix(es) of source filenames.",
    "You can specify multiple suffix as a list of string:",
    "The master toctree document.",
    "General information about the project.",
    "The version info for the project you're documenting, acts as replacement for",
    "|version| and |release|, also used in various other places throughout the",
    "built documents.",
    "",
    "The short X.Y version.",
    "The full version, including alpha/beta/rc tags.",
    "The language for content autogenerated by Sphinx. Refer to documentation",
    "for a list of supported languages.",
    "",
    "This is also used if you do content translation via gettext catalogs.",
    "Usually you set \"language\" from the command line for these cases.",
    "List of patterns, relative to source directory, that match files and",
    "directories to ignore when looking for source files.",
    "This patterns also effect to html_static_path and html_extra_path",
    "The name of the Pygments (syntax highlighting) style to use.",
    "If true, `todo` and `todoList` produce output, else they produce nothing.",
    "Both the class' and the __init__ method's docstring are concatenated and inserted.",
    "-- Options for HTML output ----------------------------------------------",
    "The theme to use for HTML and HTML Help pages.  See the documentation for",
    "a list of builtin themes.",
    "",
    "Theme options are theme-specific and customize the look and feel of a theme",
    "further.  For a list of options available for each theme, see the",
    "documentation.",
    "",
    "html_theme_options = {}",
    "Add any paths that contain custom static files (such as style sheets) here,",
    "relative to this directory. They are copied after the builtin static files,",
    "so a file named \"default.css\" will overwrite the builtin \"default.css\".",
    "-- Options for HTMLHelp output ------------------------------------------",
    "Output file base name for HTML help builder.",
    "-- Options for LaTeX output ---------------------------------------------",
    "The paper size ('letterpaper' or 'a4paper').",
    "",
    "'papersize': 'letterpaper',",
    "The font size ('10pt', '11pt' or '12pt').",
    "",
    "'pointsize': '10pt',",
    "Additional stuff for the LaTeX preamble.",
    "",
    "'preamble': '',",
    "Latex figure (float) alignment",
    "",
    "'figure_align': 'htbp',",
    "Grouping the document tree into LaTeX files. List of tuples",
    "(source start file, target name, title,",
    "author, documentclass [howto, manual, or own class]).",
    "-- Options for manual page output ---------------------------------------",
    "One entry per manual page. List of tuples",
    "(source start file, name, description, authors, manual section).",
    "-- Options for Texinfo output -------------------------------------------",
    "Grouping the document tree into Texinfo files. List of tuples",
    "(source start file, target name, title, author,",
    "dir menu entry, description, category)",
    "https://recommonmark.readthedocs.io/en/latest/",
    "coding: utf-8",
    "coding: utf-8",
    "pylint: skip-file",
    "we don't need lib_lightgbm while building docs",
    "coding: utf-8",
    "pylint: skip-file",
    "check saved model persistence",
    "we need to check the consistency of model file here, so test for exact equal",
    "check early stopping is working. Make it stop very early, so the scores should be very close to zero",
    "scores likely to be different, but prediction should still be the same",
    "check pmml",
    "coding: utf-8",
    "pylint: skip-file",
    "coding: utf-8",
    "pylint: skip-file",
    "coding: utf-8",
    "pylint: skip-file",
    "no early stopping",
    "early stopping occurs",
    "test custom eval metrics",
    "test dump model",
    "shuffle = False, override metric in params",
    "shuffle = True, callbacks",
    "self defined folds",
    "lambdarank",
    "test feature_names with whitespaces",
    "convert from one-based to  zero-based index",
    "convert from boundaries to size",
    "--- start Booster interfaces",
    "create boosting",
    "initialize the boosting",
    "create objective function",
    "initialize the objective function",
    "create training metric",
    "reset the boosting",
    "create objective function",
    "initialize the objective function",
    "some help functions used to convert data",
    "Row iterator of on column for CSC matrix",
    "return value at idx, only can access by ascent order",
    "return next non-zero pair, if index < 0, means no more data",
    "start of c_api functions",
    "sample data first",
    "sample data first",
    "sample data first",
    "no more data",
    "---- start of booster",
    "---- start of some help functions",
    "set number of threads for openmp",
    "check for alias",
    "read parameters from config file",
    "remove str after \"#\"",
    "Command-line has higher priority",
    "check for alias again",
    "load configs",
    "prediction is needed if using input initial model(continued train)",
    "need to continue training",
    "sync up random seed for data partition",
    "load Training data",
    "load data for parallel training",
    "load data for single machine",
    "need save binary file",
    "create training metric",
    "only when have metrics then need to construct validation data",
    "Add validation data, if it exists",
    "add",
    "need save binary file",
    "add metric for validation data",
    "output used time on each iteration",
    "need init network",
    "sync global random seed for feature patition",
    "create boosting",
    "create objective function",
    "load training data",
    "initialize the objective function",
    "initialize the boosting",
    "add validation data into boosting",
    "output used time per iteration",
    "save model to file",
    "convert model to if-else statement code",
    "create predictor",
    "no need to sync if not parallel learning",
    "counts for all labels",
    "start from top label, and accumulate DCG",
    "counts for all labels",
    "calculate k Max DCG by one pass",
    "get sorted indices by score",
    "calculate dcg",
    "get sorted indices by score",
    "calculate multi dcg by one pass",
    "wait for all client start up",
    "default set to -1",
    "distance at k-th communication, distance[k] = 2^k",
    "set incoming rank at k-th commuication",
    "set outgoing rank at k-th commuication",
    "defalut set as -1",
    "construct all recursive halving map for all machines",
    "let 1 << k <= num_machines",
    "distance of each communication",
    "if num_machines = 2^k, don't need to group machines",
    "communication direction, %2 == 0 is positive",
    "neighbor at k-th communication",
    "receive data block at k-th communication",
    "send data block at k-th communication",
    "if num_machines != 2^k, need to group machines",
    "group, two machine in one group, total \"rest\" groups will have 2 machines.",
    "let left machine as group leader",
    "cache block information for groups, group with 2 machines will have double block size",
    "convert from group to node leader",
    "convert from node to group",
    "meet new group",
    "add block len for this group",
    "calculate the group block start",
    "not need to construct",
    "get receive block informations",
    "accumulate block len",
    "get send block informations",
    "accumulate block len",
    "static member definition",
    "if small package or small count , do it by all gather.(reduce the communication times.)",
    "assign the blocks to every rank.",
    "do reduce scatter",
    "do all gather",
    "assign blocks",
    "need use buffer here, since size of \"output\" is smaller than size after all gather",
    "copy back",
    "assign blocks",
    "start all gather",
    "use output as receive buffer",
    "get current local block size",
    "get out rank",
    "get send information",
    "get in rank",
    "get recv information",
    "send and recv at same time",
    "rotate in-place",
    "send local data to neighbor first",
    "receive neighbor data first",
    "reduce",
    "start recursive halfing",
    "get target",
    "get send information",
    "get recv information",
    "send and recv at same time",
    "reduce",
    "send result to neighbor",
    "receive result from neighbor",
    "copy result",
    "start up socket",
    "parse clients from file",
    "get ip list of local machine",
    "get local rank",
    "construct listener",
    "construct communication topo",
    "construct linkers",
    "free listener",
    "set timeout",
    "accept incoming socket",
    "receive rank",
    "add new socket",
    "save ranks that need to connect with",
    "start listener",
    "start connect",
    "let smaller rank connect to larger rank",
    "send local rank",
    "wait for listener",
    "print connected linkers",
    "Get some statistic from 2 line",
    "if only have one line on file",
    "get column names",
    "load label idx first",
    "erase label column name",
    "load ignore columns",
    "load weight idx",
    "load group idx",
    "don't support query id in data file when training in parallel",
    "read data to memory",
    "sample data",
    "construct feature bin mappers",
    "initialize label",
    "extract features",
    "sample data from file",
    "construct feature bin mappers",
    "initialize label",
    "extract features",
    "load data from binary file",
    "check meta data",
    "need to check training data",
    "read data in memory",
    "initialize label",
    "extract features",
    "Get number of lines of data file",
    "initialize label",
    "extract features",
    "load data from binary file",
    "not need to check validation data",
    "check meta data",
    "buffer to read binary file",
    "check token",
    "read size of header",
    "re-allocmate space if not enough",
    "read header",
    "get header",
    "num_groups",
    "real_feature_idx_",
    "feature2group",
    "feature2subfeature",
    "group_bin_boundaries",
    "group_feature_start_",
    "group_feature_cnt_",
    "get feature names",
    "write feature names",
    "read size of meta data",
    "re-allocate space if not enough",
    "read meta data",
    "load meta data",
    "sample local used data if need to partition",
    "if not contain query file, minimal sample unit is one record",
    "if contain query file, minimal sample unit is one query",
    "if is new query",
    "read feature data",
    "read feature size",
    "re-allocate space if not enough",
    "fill feature_names_ if not header",
    "---- private functions ----",
    "if features are ordered, not need to use hist_buf",
    "read all lines",
    "get query data",
    "if not contain query data, minimal sample unit is one record",
    "if contain query data, minimal sample unit is one query",
    "if is new query",
    "get query data",
    "if not contain query file, minimal sample unit is one record",
    "if contain query file, minimal sample unit is one query",
    "if is new query",
    "parse features",
    "-1 means doesn't use this feature",
    "check the range of label_idx, weight_idx and group_idx",
    "fill feature_names_ if not header",
    "start find bins",
    "if only one machine, find bin locally",
    "if have multi-machines, need to find bin distributed",
    "different machines will find bin for different features",
    "start and len will store the process feature indices for different machines",
    "machine i will find bins for features in [ start[i], start[i] + len[i] )",
    "get max_bin",
    "sync global max_bin",
    "get size of bin mapper with max_bin size",
    "since sizes of different feature may not be same, we expand all bin mapper to type_size",
    "find local feature bins and copy to buffer",
    "free",
    "convert to binary size",
    "gather global feature bin mappers",
    "restore features bins from buffer",
    "if doesn't need to prediction with initial model",
    "parser",
    "set label",
    "free processed line:",
    "shrink_to_fit will be very slow in linux, and seems not free memory, disable for now",
    "text_reader_->Lines()[i].shrink_to_fit();",
    "push data",
    "if is used feature",
    "if need to prediction with initial model",
    "parser",
    "set initial score",
    "set label",
    "free processed line:",
    "shrink_to_fit will be very slow in linux, and seems not free memory, disable for now",
    "text_reader_->Lines()[i].shrink_to_fit();",
    "push data",
    "if is used feature",
    "metadata_ will manage space of init_score",
    "text data can be free after loaded feature values",
    "parser",
    "set initial score",
    "set label",
    "push data",
    "if is used feature",
    "only need part of data",
    "need full data",
    "metadata_ will manage space of init_score",
    "read size of token",
    "deep copy function for BinMapper",
    "mean size for one bin",
    "need a new bin",
    "update bin upper bound",
    "last bin upper bound",
    "find distinct_values first",
    "push zero in the front",
    "push zero in the back",
    "convert to int type first",
    "sort by counts",
    "will ignore the categorical of small counts",
    "check trival(num_bin_ == 1) feature",
    "check useless bin",
    "calculate sparse rate",
    "sparse threshold",
    "for lambdarank, it needs query data for partition data in parallel learning",
    "need convert query_id to boundaries",
    "check weights",
    "check query boundries",
    "contain initial score file",
    "check weights",
    "get local weights",
    "check query boundries",
    "get local query boundaries",
    "contain initial score file",
    "get local initial scores",
    "re-load query weight",
    "save to nullptr",
    "save to nullptr",
    "save to nullptr",
    "default weight file name",
    "default weight file name",
    "use first line to count number class",
    "default query file name",
    "root is in the depth 0",
    "update parent info",
    "if cur node is left child",
    "add new node",
    "add two new leaves",
    "update new leaves",
    "save current leaf value to internal node before change",
    "update leaf depth",
    "non-leaf",
    "leaf",
    "non-leaf",
    "left subtree",
    "right subtree",
    "leaf",
    "load main config types",
    "generate seeds by seed.",
    "sub-config setup",
    "check for conflicts",
    "clear old metrics",
    "to lower",
    "split",
    "remove duplicate",
    "check if objective_type, metric_type, and num_class match",
    "Change pool size to -1 (no limit) when using data parallel to reduce communication costs",
    "label_gain = 2^i - 1, may overflow, so we use 31 here",
    "label_gain = 2^i - 1, may overflow, so we use 31 here",
    "default eval ndcg @[1-5]",
    "filter is based on sampling data, so decrease its range",
    "put dense feature first",
    "sort by non zero cnt",
    "sort by non zero cnt, bigger first",
    "take apart small sparse group, due it will not gain on speed",
    "shuffle groups",
    "get num_features",
    "get bin_mappers",
    "copy feature bin mapper data",
    "copy feature bin mapper data",
    "if not pass a filename, just append \".bin\" of original file",
    "get size of header",
    "size of feature names",
    "write header",
    "write feature names",
    "get size of meta data",
    "write meta data",
    "write feature data",
    "get size of feature",
    "write feature",
    "feature is not used",
    "construct histograms for smaller leaf",
    "if not use ordered bin",
    "used ordered bin",
    "feature is not used",
    "construct histograms for smaller leaf",
    "if not use ordered bin",
    "used ordered bin",
    "fixed hessian.",
    "feature is not used",
    "construct histograms for smaller leaf",
    "if not use ordered bin",
    "used ordered bin",
    "feature is not used",
    "construct histograms for smaller leaf",
    "if not use ordered bin",
    "used ordered bin",
    "fixed hessian.",
    "set zero",
    "predict all the trees for one iteration",
    "check early stopping",
    "margin_threshold will be captured by value",
    "copy and sort",
    "margin_threshold will be captured by value",
    "init tree learner",
    "push training metrics",
    "not same training data, need reset score and others",
    "create score tracker",
    "update score",
    "create buffer for gradients and hessians",
    "get max feature index",
    "get label index",
    "get feature names",
    "if need bagging, create buffer",
    "reset config for tree learner",
    "+ 1 here for the binary classification",
    "multi-class",
    "binary class",
    "for a validation dataset, we need its score and metric",
    "update score",
    "random bagging, minimal unit is one record",
    "if need bagging",
    "set bagging data to tree learner",
    "get subset",
    "we need to predict out-of-bag scores of data for boosting",
    "boosting from average prediction. It doesn't work well for classification, remove it for now.",
    "boosting first",
    "bagging logic",
    "get sub gradients",
    "cannot multi-threading here.",
    "shrinkage by learning rate",
    "update score",
    "only add default score one-time",
    "add model",
    "reset score",
    "remove model",
    "print message for metric",
    "pop last early_stopping_round_ models",
    "update training score",
    "update validation score",
    "print training metric",
    "print validation metric",
    "objective function will calculate gradients and hessians",
    "PredictRaw",
    "Predict",
    "PredictLeafIndex",
    "output model type",
    "output number of class",
    "output label index",
    "output max_feature_idx",
    "output objective",
    "output tree models",
    "use serialized string to restore this object",
    "get number of classes",
    "get index of label",
    "get max_feature_idx first",
    "get boost_from_average_",
    "get feature names",
    "get tree models",
    "store the importance first",
    "sort the importance",
    "Get the max size of pool",
    "at least need 2 leaves",
    "push split information for all leaves",
    "get ordered bin",
    "check existing for ordered bin",
    "initialize splits for leaf",
    "initialize data partition",
    "initialize ordered gradients and hessians",
    "if has ordered bin, need to allocate a buffer to fast split",
    "get ordered bin",
    "initialize splits for leaf",
    "initialize data partition",
    "initialize ordered gradients and hessians",
    "if has ordered bin, need to allocate a buffer to fast split",
    "Get the max size of pool",
    "at least need 2 leaves",
    "push split information for all leaves",
    "some initial works before training",
    "root leaf",
    "only root leaf can be splitted on first time",
    "some initial works before finding best split",
    "find best threshold for every feature",
    "find best split from all features",
    "Get a leaf with max split gain",
    "Get split information for best leaf",
    "cannot split, quit",
    "split tree with best leaf",
    "avoid zero hessians.",
    "reset histogram pool",
    "initialize used features",
    "Get used feature at current tree",
    "initialize data partition",
    "reset the splits for leaves",
    "Sumup for root",
    "use all data",
    "use bagging, only use part of data",
    "if has ordered bin, need to initialize the ordered bin",
    "use all data, pass nullptr",
    "bagging, only use part of data",
    "mark used data",
    "initialize ordered bin",
    "check depth of current leaf",
    "only need to check left leaf, since right leaf is in same level of left leaf",
    "no enough data to continue",
    "only have root",
    "put parent(left) leaf's histograms into larger leaf's histograms",
    "put parent(left) leaf's histograms to larger leaf's histograms",
    "split for the ordered bin",
    "mark data that at left-leaf",
    "split the ordered bin",
    "construct smaller leaf",
    "construct larger leaf",
    "find splits",
    "only has root leaf",
    "find best threshold for larger child",
    "left = parent",
    "split tree, will return right leaf",
    "split data partition",
    "init the leaves that used on next iteration",
    "get feature partition",
    "get local used features",
    "get best split at smaller leaf",
    "find local best split for larger leaf",
    "sync global best info",
    "copy back",
    "update best split",
    "instantiate template classes, otherwise linker cannot find the code",
    "initialize SerialTreeLearner",
    "Get local rank and global machine size",
    "allocate buffer for communication",
    "generate feature partition for current tree",
    "get local used feature",
    "get block start and block len for reduce scatter",
    "get buffer_write_start_pos_",
    "get buffer_read_start_pos_",
    "sync global data sumup info",
    "global sumup reduce",
    "copy back",
    "set global sumup info",
    "init global data count in leaf",
    "construct local histograms",
    "copy to buffer",
    "Reduce scatter for histogram",
    "restore global histograms from buffer",
    "find best threshold for smaller child",
    "only root leaf",
    "construct histgroms for large leaf, we init larger leaf as the parent, so we can just subtract the smaller leaf's histograms",
    "find best threshold for larger child",
    "find local best split for larger leaf",
    "sync global best info",
    "set best split",
    "need update global number of data in leaf",
    "instantiate template classes, otherwise linker cannot find the code",
    "initialize SerialTreeLearner",
    "some additional variables needed for GPU trainer",
    "Initialize GPU buffers and kernels",
    "some functions used for debugging the GPU histogram construction",
    "printf(\"grad %g != %g (%d ULPs)\\n\", h1[i].sum_gradients, h2[i].sum_gradients, ulps);",
    "goto err;",
    "printf(\"hessian %g != %g (%d ULPs)\\n\", h1[i].sum_hessians, h2[i].sum_hessians, ulps);",
    "goto err;",
    "we roughly want 256 workgroups per device, and we have num_dense_feature4_ feature tuples.",
    "also guarantee that there are at least 2K examples per workgroup",
    "return 0;",
    "we have already copied ordered gradients, ordered hessians and indices to GPU",
    "decide the best number of workgroups working on one feature4 tuple",
    "set work group size based on feature size",
    "each 2^exp_workgroups_per_feature workgroups work on a feature4 tuple",
    "we need to refresh the kernel arguments after reallocating",
    "The only argument that needs to be changed later is num_data_",
    "the GPU kernel will process all features in one call, and each",
    "2^exp_workgroups_per_feature (compile time constant) workgroup will",
    "process one feature4 tuple",
    "for the root node, indices are not copied",
    "for constant hessian, hessians are not copied except for the root node",
    "there will be 2^exp_workgroups_per_feature = num_workgroups / num_dense_feature4 sub-histogram per feature4",
    "and we will launch num_feature workgroups for this kernel",
    "will launch threads for all features",
    "the queue should be asynchrounous, and we will can WaitAndGetHistograms() before we start processing dense feature groups",
    "copy the results asynchronously. Size depends on if double precision is used",
    "we will wait for this object in WaitAndGetHistograms",
    "when the output is ready, the computation is done",
    "values of this feature has been redistributed to multiple bins; need a reduction here",
    "how many feature-group tuples we have",
    "leave some safe margin for prefetching",
    "256 work-items per workgroup. Each work-item prefetches one tuple for that feature",
    "clear sparse/dense maps",
    "do nothing if no features can be processed on GPU",
    "allocate memory for all features (FIXME: 4 GB barrier on some devices, need to split to multiple buffers)",
    "unpin old buffer if necessary before destructing them",
    "make ordered_gradients and hessians larger (including extra room for prefetching), and pin them",
    "allocate space for gradients and hessians on device",
    "we will copy gradients and hessians in after ordered_gradients_ and ordered_hessians_ are constructed",
    "allocate feature mask, for disabling some feature-groups' histogram calculation",
    "copy indices to the device",
    "histogram bin entry size depends on the precision (single/double)",
    "create output buffer, each feature has a histogram with device_bin_size_ bins,",
    "each work group generates a sub-histogram of dword_features_ features.",
    "only initialize once here, as this will not need to change when ResetTrainingData() is called",
    "create atomic counters for inter-group coordination",
    "The output buffer is allocated to host directly, to overlap compute and data transfer",
    "find the dense feature-groups and group then into Feature4 data structure (several feature-groups packed into 4 bytes)",
    "looking for dword_features_ non-sparse feature-groups",
    "decide if we need to redistribute the bin",
    "multiplier must be a power of 2",
    "device_bin_mults_.push_back(1);",
    "found",
    "for data transfer time",
    "Now generate new data structure feature4, and copy data to the device",
    "preallocate arrays for all threads, and pin them",
    "building Feature4 bundles; each thread handles dword_features_ features",
    "one feature datapoint is 4 bits",
    "this guarantees that the RawGet() function is inlined, rather than using virtual function dispatching",
    "one feature datapoint is one byte",
    "this guarantees that the RawGet() function is inlined, rather than using virtual function dispatching",
    "Dense bin",
    "Dense 4-bit bin",
    "working on the remaining (less than dword_features_) feature groups",
    "fill the leftover features",
    "fill this empty feature with some \"random\" value",
    "fill this empty feature with some \"random\" value",
    "copying the last 1 to (dword_features - 1) feature-groups in the last tuple",
    "deallocate pinned space for feature copying",
    "data transfer time",
    "for other types of failure, build log might not be available; program.build_log() can crash",
    "Something bad happened. Just return \"No log available.\"",
    "build is okay, log may contain warnings",
    "destroy any old kernels",
    "create OpenCL kernels for different number of workgroups per feature",
    "currently we don't use constant memory",
    "compile the GPU kernel depending if double precision is used, constant hessian is used, etc",
    "kernel with indices in an array",
    "kernel with all features enabled, with elimited branches",
    "kernel with all data indices (for root node, and assumes that root node always uses all features)",
    "do nothing if no features can be processed on GPU",
    "The only argument that needs to be changed later is num_data_",
    "hessian is passed as a parameter, but it is not available now.",
    "hessian will be set in BeforeTrain()",
    "Get the max bin size, used for selecting best GPU kernel",
    "initialize GPU",
    "determine which kernel to use based on the max number of bins",
    "setup GPU kernel arguments after we allocating all the buffers",
    "check if we need to recompile the GPU kernel (is_constant_hessian changed)",
    "this should rarely occur",
    "GPU memory has to been reallocated because data may have been changed",
    "setup GPU kernel arguments after we allocating all the buffers",
    "Copy initial full hessians and gradients to GPU.",
    "We start copying as early as possible, instead of at ConstructHistogram().",
    "setup hessian parameters only",
    "hessian is passed as a parameter",
    "use bagging",
    "On GPU, we start copying indices, gradients and hessians now, instead at ConstructHistogram()",
    "copy used gradients and hessians to ordered buffer",
    "transfer the indices to GPU",
    "transfer hessian to GPU",
    "setup hessian parameters only",
    "hessian is passed as a parameter",
    "transfer gradients to GPU",
    "only have root",
    "Copy indices, gradients and hessians as early as possible",
    "only need to initialize for smaller leaf",
    "Get leaf boundary",
    "copy indices to the GPU:",
    "copy ordered hessians to the GPU:",
    "copy ordered gradients to the GPU:",
    "do nothing if no features can be processed on GPU",
    "copy data indices if it is not null",
    "generate and copy ordered_gradients if gradients is not null",
    "generate and copy ordered_hessians if hessians is not null",
    "converted indices in is_feature_used to feature-group indices",
    "construct the feature masks for dense feature-groups",
    "if no feature group is used, just return and do not use GPU",
    "if not all feature groups are used, we need to transfer the feature mask to GPU",
    "otherwise, we will use a specialized GPU kernel with all feature groups enabled",
    "All data have been prepared, now run the GPU kernel",
    "construct smaller leaf",
    "ConstructGPUHistogramsAsync will return true if there are availabe feature gourps dispatched to GPU",
    "then construct sparse features on CPU",
    "We set data_indices to null to avoid rebuilding ordered gradients/hessians",
    "wait for GPU to finish, only if GPU is actually used",
    "use double precision",
    "use single precision",
    "Compare GPU histogram with CPU histogram, useful for debuggin GPU code problem",
    "#define GPU_DEBUG_COMPARE",
    "construct larger leaf",
    "then construct sparse features on CPU",
    "We set data_indices to null to avoid rebuilding ordered gradients/hessians",
    "wait for GPU to finish, only if GPU is actually used",
    "use double precision",
    "use single precision",
    "do some sanity check for the GPU algorithm",
    "limit top k",
    "get max bin",
    "calculate buffer size",
    "left and right on same time, so need double size",
    "initialize histograms for global",
    "sync global data sumup info",
    "set global sumup info",
    "init global data count in leaf",
    "get local sumup",
    "get local sumup",
    "get mean number on machines",
    "weighted gain",
    "get top k",
    "Copy histogram to buffer, and Get local aggregate features",
    "copy histograms.",
    "copy smaller leaf histograms first",
    "mark local aggregated feature",
    "copy",
    "then copy larger leaf histograms",
    "mark local aggregated feature",
    "copy",
    "use local data to find local best splits",
    "find splits",
    "only has root leaf",
    "find best threshold for larger child",
    "local voting",
    "gather",
    "get all top-k from all machines",
    "global voting",
    "copy local histgrams to buffer",
    "Reduce scatter for histogram",
    "find best split from local aggregated histograms",
    "restore from buffer",
    "find best threshold",
    "restore from buffer",
    "find best threshold",
    "find local best",
    "find local best split for larger leaf",
    "sync global best info",
    "copy back",
    "set the global number of data for leaves",
    "init the global sumup info",
    "instantiate template classes, otherwise linker cannot find the code"
  ],
  "v2.0": [
    "coding: utf-8",
    "pylint: disable=invalid-name, exec-used",
    "coding: utf-8",
    "pylint: disable = invalid-name, W0105",
    "Most of legacy advanced options becomes callbacks",
    "check evaluation result.",
    "run preprocessing on the data set if needed",
    "setup callbacks",
    "coding: utf-8",
    "pylint: disable = C0103",
    "simplejson does not support Python 3.2, it throws a SyntaxError",
    "because of u'...' Unicode literals.",
    "coding: utf-8",
    "pylint: disable = invalid-name, W0105, C0111, C0301",
    "Switch to using a multiclass objective in the underlying LGBM instance",
    "coding: utf-8",
    "coding: utf-8",
    "pylint: disable = C0103",
    "coding: utf-8",
    "coding: utf-8",
    "pylint: disable = invalid-name, C0111, C0301",
    "pylint: disable = R0912, R0913, R0914, W0105, W0201, W0212",
    "TypeError: obj is not a string or a number",
    "ValueError: invalid literal",
    "load init score",
    "need re group init score",
    "set feature names",
    "group data from LightGBM is boundaries data, need to convert to group size",
    "coding: utf-8",
    "pylint: disable = invalid-name, W0105, C0301",
    "Callback environment used by callbacks",
    "coding: utf-8",
    "pylint: disable = invalid-name, C0111",
    "load or create your dataset",
    "create dataset for lightgbm",
    "if you want to re-use data, remember to set free_raw_data=False",
    "specify your configurations as a dict",
    "generate a feature name",
    "feature_name and categorical_feature",
    "check feature name",
    "save model to file",
    "load model to predict",
    "can only predict with the best iteration (or the saving iteration)",
    "eval with loaded model",
    "dump model with pickle",
    "load model with pickle to predict",
    "can predict with any iteration when loaded in pickle way",
    "eval with loaded model",
    "continue training",
    "init_model accepts:",
    "1. model file name",
    "2. Booster()",
    "decay learning rates",
    "learning_rates accepts:",
    "1. list/tuple with length = num_boost_round",
    "2. function(curr_iter)",
    "change other parameters during training",
    "self-defined objective function",
    "f(preds: array, train_data: Dataset) -> grad: array, hess: array",
    "log likelihood loss",
    "self-defined eval metric",
    "f(preds: array, train_data: Dataset) -> name: string, value: array, is_higher_better: bool",
    "binary error",
    "callback",
    "coding: utf-8",
    "pylint: disable = invalid-name, C0111",
    "load or create your dataset",
    "train",
    "predict",
    "eval",
    "feature importances",
    "other scikit-learn modules",
    "coding: utf-8",
    "pylint: disable = invalid-name, C0111",
    "load or create your dataset",
    "create dataset for lightgbm",
    "specify your configurations as a dict",
    "train",
    "coding: utf-8",
    "pylint: disable = invalid-name, C0111",
    "load or create your dataset",
    "create dataset for lightgbm",
    "specify your configurations as a dict",
    "train",
    "save model to file",
    "predict",
    "eval",
    "dump model to json (and save to file)",
    "feature importances",
    "coding: utf-8",
    "pylint: disable = C0111, C0103",
    "print out the pmml for a decision tree",
    "specify the objective as function name and binarySplit for",
    "splitCharacteristic because each node has 2 children",
    "list each feature name as a mining field, and treat all outliers as is,",
    "unless specified",
    "begin printing out the decision tree",
    "open the model file and then process it",
    "ignore first 6 and empty lines",
    "print out data dictionary entries for each column",
    "not adding any interval definition, all values are currently",
    "valid",
    "list each feature name as a mining field, and treat all outliers",
    "as is, unless specified",
    "read each array that contains pertinent information for the pmml",
    "these arrays will be used to recreate the traverse the decision tree",
    "coding: utf-8",
    "pylint: skip-file",
    "coding: utf-8",
    "pylint: skip-file",
    "check saved model persistence",
    "check pmml",
    "coding: utf-8",
    "pylint: skip-file",
    "coding: utf-8",
    "pylint: skip-file",
    "coding: utf-8",
    "pylint: skip-file",
    "no early stopping",
    "early stopping occurs",
    "application",
    "boosting",
    "io",
    "metric",
    "network",
    "objective",
    "treelearner",
    "c_api",
    "convert from one-based to  zero-based index",
    "convert from boundaries to size",
    "--- start Booster interfaces",
    "application",
    "boosting",
    "io",
    "metric",
    "network",
    "objective",
    "treelearner",
    "c_api",
    "create boosting",
    "initialize the boosting",
    "create objective function",
    "initialize the objective function",
    "create training metric",
    "reset the boosting",
    "create objective function",
    "initialize the objective function",
    "not threading safe now",
    "boosting_->SetNumIterationForPred may be set by other thread during prediction.",
    "some help functions used to convert data",
    "Row iterator of on column for CSC matrix",
    "return value at idx, only can access by ascent order",
    "return next non-zero pair, if index < 0, means no more data",
    "start of c_api functions",
    "sample data first",
    "sample data first",
    "sample data first",
    "no more data",
    "---- start of booster",
    "---- start of some help functions",
    "set number of threads for openmp",
    "check for alias",
    "read parameters from config file",
    "remove str after \"#\"",
    "Command-line has higher priority",
    "check for alias again",
    "load configs",
    "prediction is needed if using input initial model(continued train)",
    "need to continue training",
    "sync up random seed for data partition",
    "load Training data",
    "load data for parallel training",
    "load data for single machine",
    "need save binary file",
    "create training metric",
    "only when have metrics then need to construct validation data",
    "Add validation data, if it exists",
    "add",
    "need save binary file",
    "add metric for validation data",
    "output used time on each iteration",
    "need init network",
    "sync global random seed for feature patition",
    "create boosting",
    "create objective function",
    "load training data",
    "initialize the objective function",
    "initialize the boosting",
    "add validation data into boosting",
    "output used time per iteration",
    "save model to file",
    "create predictor",
    "no need to sync if not parallel learning",
    "counts for all labels",
    "start from top label, and accumulate DCG",
    "counts for all labels",
    "calculate k Max DCG by one pass",
    "get sorted indices by score",
    "calculate dcg",
    "get sorted indices by score",
    "calculate multi dcg by one pass",
    "wait for all client start up",
    "default set to -1",
    "distance at k-th communication, distance[k] = 2^k",
    "set incoming rank at k-th commuication",
    "set outgoing rank at k-th commuication",
    "defalut set as -1",
    "construct all recursive halving map for all machines",
    "let 1 << k <= num_machines",
    "distance of each communication",
    "if num_machines = 2^k, don't need to group machines",
    "communication direction, %2 == 0 is positive",
    "neighbor at k-th communication",
    "receive data block at k-th communication",
    "send data block at k-th communication",
    "if num_machines != 2^k, need to group machines",
    "group, two machine in one group, total \"rest\" groups will have 2 machines.",
    "let left machine as group leader",
    "cache block information for groups, group with 2 machines will have double block size",
    "convert from group to node leader",
    "convert from node to group",
    "meet new group",
    "add block len for this group",
    "calculate the group block start",
    "not need to construct",
    "get receive block informations",
    "accumulate block len",
    "get send block informations",
    "accumulate block len",
    "static member definition",
    "if small package or small count , do it by all gather.(reduce the communication times.)",
    "assign the blocks to every rank.",
    "do reduce scatter",
    "do all gather",
    "assign blocks",
    "need use buffer here, since size of \"output\" is smaller than size after all gather",
    "copy back",
    "assign blocks",
    "start all gather",
    "use output as receive buffer",
    "get current local block size",
    "get out rank",
    "get send information",
    "get in rank",
    "get recv information",
    "send and recv at same time",
    "rotate in-place",
    "send local data to neighbor first",
    "receive neighbor data first",
    "reduce",
    "start recursive halfing",
    "get target",
    "get send information",
    "get recv information",
    "send and recv at same time",
    "reduce",
    "send result to neighbor",
    "receive result from neighbor",
    "copy result",
    "start up socket",
    "parse clients from file",
    "get ip list of local machine",
    "get local rank",
    "construct listener",
    "construct communication topo",
    "construct linkers",
    "free listener",
    "set timeout",
    "accept incoming socket",
    "receive rank",
    "add new socket",
    "save ranks that need to connect with",
    "start listener",
    "start connect",
    "let smaller rank connect to larger rank",
    "send local rank",
    "wait for listener",
    "print connected linkers",
    "Get some statistic from 2 line",
    "if only have one line on file",
    "get column names",
    "load label idx first",
    "erase label column name",
    "load ignore columns",
    "load weight idx",
    "load group idx",
    "don't support query id in data file when training in parallel",
    "read data to memory",
    "sample data",
    "construct feature bin mappers",
    "initialize label",
    "extract features",
    "sample data from file",
    "construct feature bin mappers",
    "initialize label",
    "extract features",
    "load data from binary file",
    "check meta data",
    "need to check training data",
    "read data in memory",
    "initialize label",
    "extract features",
    "Get number of lines of data file",
    "initialize label",
    "extract features",
    "load data from binary file",
    "not need to check validation data",
    "check meta data",
    "buffer to read binary file",
    "check token",
    "read size of header",
    "re-allocmate space if not enough",
    "read header",
    "get header",
    "num_groups",
    "real_feature_idx_",
    "feature2group",
    "feature2subfeature",
    "group_bin_boundaries",
    "group_feature_start_",
    "group_feature_cnt_",
    "get feature names",
    "write feature names",
    "read size of meta data",
    "re-allocate space if not enough",
    "read meta data",
    "load meta data",
    "sample local used data if need to partition",
    "if not contain query file, minimal sample unit is one record",
    "if contain query file, minimal sample unit is one query",
    "if is new query",
    "read feature data",
    "read feature size",
    "re-allocate space if not enough",
    "fill feature_names_ if not header",
    "---- private functions ----",
    "if features are ordered, not need to use hist_buf",
    "read all lines",
    "get query data",
    "if not contain query data, minimal sample unit is one record",
    "if contain query data, minimal sample unit is one query",
    "if is new query",
    "get query data",
    "if not contain query file, minimal sample unit is one record",
    "if contain query file, minimal sample unit is one query",
    "if is new query",
    "parse features",
    "-1 means doesn't use this feature",
    "check the range of label_idx, weight_idx and group_idx",
    "fill feature_names_ if not header",
    "start find bins",
    "if only one machine, find bin locally",
    "if have multi-machines, need to find bin distributed",
    "different machines will find bin for different features",
    "start and len will store the process feature indices for different machines",
    "machine i will find bins for features in [ start[i], start[i] + len[i] )",
    "get max_bin",
    "sync global max_bin",
    "get size of bin mapper with max_bin size",
    "since sizes of different feature may not be same, we expand all bin mapper to type_size",
    "find local feature bins and copy to buffer",
    "free",
    "convert to binary size",
    "gather global feature bin mappers",
    "restore features bins from buffer",
    "if doesn't need to prediction with initial model",
    "parser",
    "set label",
    "free processed line:",
    "shrink_to_fit will be very slow in linux, and seems not free memory, disable for now",
    "text_reader_->Lines()[i].shrink_to_fit();",
    "push data",
    "if is used feature",
    "if need to prediction with initial model",
    "parser",
    "set initial score",
    "set label",
    "free processed line:",
    "shrink_to_fit will be very slow in linux, and seems not free memory, disable for now",
    "text_reader_->Lines()[i].shrink_to_fit();",
    "push data",
    "if is used feature",
    "metadata_ will manage space of init_score",
    "text data can be free after loaded feature values",
    "parser",
    "set initial score",
    "set label",
    "push data",
    "if is used feature",
    "only need part of data",
    "need full data",
    "metadata_ will manage space of init_score",
    "read size of token",
    "deep copy function for BinMapper",
    "find distinct_values first",
    "push zero in the front",
    "push zero in the back",
    "use distinct value is enough",
    "mean size for one bin",
    "need a new bin",
    "update bin upper bound",
    "last bin upper bound",
    "convert to int type first",
    "sort by counts",
    "will ignore the categorical of small counts",
    "check trival(num_bin_ == 1) feature",
    "check useless bin",
    "calculate sparse rate",
    "sparse threshold",
    "for lambdarank, it needs query data for partition data in parallel learning",
    "need convert query_id to boundaries",
    "check weights",
    "check query boundries",
    "contain initial score file",
    "check weights",
    "get local weights",
    "check query boundries",
    "get local query boundaries",
    "contain initial score file",
    "get local initial scores",
    "re-load query weight",
    "save to nullptr",
    "save to nullptr",
    "save to nullptr",
    "default weight file name",
    "default weight file name",
    "use first line to count number class",
    "default query file name",
    "root is in the depth 0",
    "update parent info",
    "if cur node is left child",
    "add new node",
    "add two new leaves",
    "update new leaves",
    "save current leaf value to internal node before change",
    "update leaf depth",
    "non-leaf",
    "leaf",
    "load main config types",
    "generate seeds by seed.",
    "sub-config setup",
    "check for conflicts",
    "clear old metrics",
    "to lower",
    "split",
    "remove duplicate",
    "check if objective_type, metric_type, and num_class match",
    "Change pool size to -1 (no limit) when using data parallel to reduce communication costs",
    "label_gain = 2^i - 1, may overflow, so we use 31 here",
    "label_gain = 2^i - 1, may overflow, so we use 31 here",
    "default eval ndcg @[1-5]",
    "get num_features",
    "get bin_mappers",
    "copy feature bin mapper data",
    "copy feature bin mapper data",
    "if not pass a filename, just append \".bin\" of original file",
    "get size of header",
    "size of feature names",
    "write header",
    "write feature names",
    "get size of meta data",
    "write meta data",
    "write feature data",
    "get size of feature",
    "write feature",
    "feature is not used",
    "construct histograms for smaller leaf",
    "if not use ordered bin",
    "used ordered bin",
    "only binary classification need sigmoid transform",
    "init tree learner",
    "push training metrics",
    "not same training data, need reset score and others",
    "create score tracker",
    "update score",
    "create buffer for gradients and hessians",
    "get max feature index",
    "get label index",
    "get feature names",
    "if need bagging, create buffer",
    "reset config for tree learner",
    "+ 1 here for the binary classification",
    "multi-class",
    "binary class",
    "for a validation dataset, we need its score and metric",
    "update score",
    "random bagging, minimal unit is one record",
    "if need bagging",
    "set bagging data to tree learner",
    "get subset",
    "we need to predict out-of-bag scores of data for boosting",
    "boosting from average prediction. It doesn't work well for classification, remove it for now.",
    "boosting first",
    "bagging logic",
    "get sub gradients",
    "cannot multi-threading here.",
    "shrinkage by learning rate",
    "update score",
    "only add default score one-time",
    "add model",
    "reset score",
    "remove model",
    "print message for metric",
    "pop last early_stopping_round_ models",
    "update training score",
    "update validation score",
    "print training metric",
    "print validation metric",
    "objective function will calculate gradients and hessians",
    "output model type",
    "output number of class",
    "output label index",
    "output max_feature_idx",
    "output objective name",
    "output sigmoid parameter",
    "output tree models",
    "use serialized string to restore this object",
    "get number of classes",
    "get index of label",
    "get max_feature_idx first",
    "get sigmoid parameter",
    "get boost_from_average_",
    "get feature names",
    "get tree models",
    "store the importance first",
    "sort the importance",
    "if need sigmoid transform",
    "Get the max size of pool",
    "at least need 2 leaves",
    "push split information for all leaves",
    "get ordered bin",
    "check existing for ordered bin",
    "initialize splits for leaf",
    "initialize data partition",
    "initialize ordered gradients and hessians",
    "if has ordered bin, need to allocate a buffer to fast split",
    "get ordered bin",
    "check existing for ordered bin",
    "initialize splits for leaf",
    "initialize data partition",
    "initialize ordered gradients and hessians",
    "if has ordered bin, need to allocate a buffer to fast split",
    "Get the max size of pool",
    "at least need 2 leaves",
    "push split information for all leaves",
    "some initial works before training",
    "root leaf",
    "only root leaf can be splitted on first time",
    "some initial works before finding best split",
    "find best threshold for every feature",
    "find best split from all features",
    "Get a leaf with max split gain",
    "Get split information for best leaf",
    "cannot split, quit",
    "split tree with best leaf",
    "avoid zero hessians.",
    "reset histogram pool",
    "initialize used features",
    "Get used feature at current tree",
    "initialize data partition",
    "reset the splits for leaves",
    "Sumup for root",
    "use all data",
    "use bagging, only use part of data",
    "if has ordered bin, need to initialize the ordered bin",
    "use all data, pass nullptr",
    "bagging, only use part of data",
    "mark used data",
    "initialize ordered bin",
    "check depth of current leaf",
    "only need to check left leaf, since right leaf is in same level of left leaf",
    "no enough data to continue",
    "only have root",
    "put parent(left) leaf's histograms into larger leaf's histograms",
    "put parent(left) leaf's histograms to larger leaf's histograms",
    "split for the ordered bin",
    "mark data that at left-leaf",
    "split the ordered bin",
    "construct smaller leaf",
    "construct larger leaf",
    "find splits",
    "only has root leaf",
    "find best threshold for larger child",
    "left = parent",
    "split tree, will return right leaf",
    "split data partition",
    "init the leaves that used on next iteration",
    "get feature partition",
    "get local used features",
    "get best split at smaller leaf",
    "find local best split for larger leaf",
    "sync global best info",
    "copy back",
    "update best split",
    "initialize SerialTreeLearner",
    "Get local rank and global machine size",
    "allocate buffer for communication",
    "generate feature partition for current tree",
    "get local used feature",
    "get block start and block len for reduce scatter",
    "get buffer_write_start_pos_",
    "get buffer_read_start_pos_",
    "sync global data sumup info",
    "global sumup reduce",
    "copy back",
    "set global sumup info",
    "init global data count in leaf",
    "construct local histograms",
    "copy to buffer",
    "Reduce scatter for histogram",
    "restore global histograms from buffer",
    "find best threshold for smaller child",
    "only root leaf",
    "construct histgroms for large leaf, we init larger leaf as the parent, so we can just subtract the smaller leaf's histograms",
    "find best threshold for larger child",
    "find local best split for larger leaf",
    "sync global best info",
    "set best split",
    "need update global number of data in leaf",
    "limit top k",
    "get max bin",
    "calculate buffer size",
    "left and right on same time, so need double size",
    "initialize histograms for global",
    "sync global data sumup info",
    "set global sumup info",
    "init global data count in leaf",
    "get local sumup",
    "get local sumup",
    "get mean number on machines",
    "weighted gain",
    "get top k",
    "Copy histogram to buffer, and Get local aggregate features",
    "copy histograms.",
    "copy smaller leaf histograms first",
    "mark local aggregated feature",
    "copy",
    "then copy larger leaf histograms",
    "mark local aggregated feature",
    "copy",
    "use local data to find local best splits",
    "construct smaller leaf",
    "construct larger leaf",
    "find splits",
    "only has root leaf",
    "find best threshold for larger child",
    "local voting",
    "gather",
    "get all top-k from all machines",
    "global voting",
    "copy local histgrams to buffer",
    "Reduce scatter for histogram",
    "find best split from local aggregated histograms",
    "restore from buffer",
    "find best threshold",
    "restore from buffer",
    "find best threshold",
    "find local best",
    "find local best split for larger leaf",
    "sync global best info",
    "copy back",
    "set the global number of data for leaves",
    "init the global sumup info"
  ],
  "v1": [
    "coding: utf-8",
    "pylint: disable=invalid-name, exec-used",
    "coding: utf-8",
    "pylint: disable = invalid-name, W0105",
    "Most of legacy advanced options becomes callbacks",
    "check evaluation result.",
    "run preprocessing on the data set if needed",
    "setup callbacks",
    "coding: utf-8",
    "pylint: disable = C0103",
    "simplejson does not support Python 3.2, it throws a SyntaxError",
    "because of u'...' Unicode literals.",
    "coding: utf-8",
    "pylint: disable = invalid-name, W0105, C0111, C0301",
    "Switch to using a multiclass objective in the underlying LGBM instance",
    "coding: utf-8",
    "coding: utf-8",
    "pylint: disable = C0103",
    "coding: utf-8",
    "coding: utf-8",
    "pylint: disable = invalid-name, C0111, C0301",
    "pylint: disable = R0912, R0913, R0914, W0105, W0201, W0212",
    "TypeError: obj is not a string or a number",
    "ValueError: invalid literal",
    "load init score",
    "need re group init score",
    "set feature names",
    "group data from LightGBM is boundaries data, need to convert to group size",
    "coding: utf-8",
    "pylint: disable = invalid-name, W0105, C0301",
    "Callback environment used by callbacks",
    "coding: utf-8",
    "pylint: disable = invalid-name, C0111",
    "load or create your dataset",
    "create dataset for lightgbm",
    "if you want to re-use data, remember to set free_raw_data=False",
    "specify your configurations as a dict",
    "generate a feature name",
    "feature_name and categorical_feature",
    "check feature name",
    "save model to file",
    "continue training",
    "init_model accepts:",
    "1. model file name",
    "2. Booster()",
    "decay learning rates",
    "learning_rates accepts:",
    "1. list/tuple with length = num_boost_round",
    "2. function(curr_iter)",
    "change other parameters during training",
    "self-defined objective function",
    "f(preds: array, train_data: Dataset) -> grad: array, hess: array",
    "log likelihood loss",
    "self-defined eval metric",
    "f(preds: array, train_data: Dataset) -> name: string, value: array, is_higher_better: bool",
    "binary error",
    "callback",
    "coding: utf-8",
    "pylint: disable = invalid-name, C0111",
    "load or create your dataset",
    "train",
    "predict",
    "eval",
    "feature importances",
    "other scikit-learn modules",
    "coding: utf-8",
    "pylint: disable = invalid-name, C0111",
    "load or create your dataset",
    "create dataset for lightgbm",
    "specify your configurations as a dict",
    "train",
    "coding: utf-8",
    "pylint: disable = invalid-name, C0111",
    "load or create your dataset",
    "create dataset for lightgbm",
    "specify your configurations as a dict",
    "train",
    "save model to file",
    "predict",
    "eval",
    "dump model to json (and save to file)",
    "feature importances",
    "coding: utf-8",
    "pylint: disable = C0111, C0103",
    "print out the pmml for a decision tree",
    "specify the objective as function name and binarySplit for",
    "splitCharacteristic because each node has 2 children",
    "list each feature name as a mining field, and treat all outliers as is,",
    "unless specified",
    "begin printing out the decision tree",
    "open the model file and then process it",
    "ignore first 6 and empty lines",
    "print out data dictionary entries for each column",
    "not adding any interval definition, all values are currently",
    "valid",
    "list each feature name as a mining field, and treat all outliers",
    "as is, unless specified",
    "read each array that contains pertinent information for the pmml",
    "these arrays will be used to recreate the traverse the decision tree",
    "coding: utf-8",
    "pylint: skip-file",
    "coding: utf-8",
    "pylint: skip-file",
    "check saved model persistence",
    "check pmml",
    "coding: utf-8",
    "pylint: skip-file",
    "coding: utf-8",
    "pylint: skip-file",
    "coding: utf-8",
    "pylint: skip-file",
    "application",
    "boosting",
    "io",
    "metric",
    "network",
    "objective",
    "treelearner",
    "c_api",
    "convert from one-based to  zero-based index",
    "convert from boundaries to size",
    "--- start Booster interfaces",
    "create boosting",
    "initialize the boosting",
    "create objective function",
    "initialize the objective function",
    "create training metric",
    "reset the boosting",
    "create objective function",
    "initialize the objective function",
    "not threading safe now",
    "boosting_->SetNumIterationForPred may be set by other thread during prediction.",
    "some help functions used to convert data",
    "Row iterator of on column for CSC matrix",
    "return value at idx, only can access by ascent order",
    "return next non-zero pair, if index < 0, means no more data",
    "start of c_api functions",
    "sample data first",
    "sample data first",
    "if need expand feature set",
    "edit the feature value",
    "sample data first",
    "no more data",
    "---- start of booster",
    "---- start of some help functions",
    "set number of threads for openmp",
    "check for alias",
    "read parameters from config file",
    "remove str after \"#\"",
    "Command-line has higher priority",
    "check for alias again",
    "load configs",
    "prediction is needed if using input initial model(continued train)",
    "need to continue training",
    "sync up random seed for data partition",
    "load Training data",
    "load data for parallel training",
    "load data for single machine",
    "need save binary file",
    "create training metric",
    "only when have metrics then need to construct validation data",
    "Add validation data, if it exists",
    "add",
    "need save binary file",
    "add metric for validation data",
    "output used time on each iteration",
    "need init network",
    "sync global random seed for feature patition",
    "create boosting",
    "create objective function",
    "load training data",
    "initialize the objective function",
    "initialize the boosting",
    "add validation data into boosting",
    "output used time per iteration",
    "save model to file",
    "create predictor",
    "no need to sync if not parallel learning",
    "only inited one time",
    "counts for all labels",
    "start from top label, and accumulate DCG",
    "counts for all labels",
    "calculate k Max DCG by one pass",
    "get sorted indices by score",
    "calculate dcg",
    "get sorted indices by score",
    "calculate multi dcg by one pass",
    "wait for all client start up",
    "default set to -1",
    "distance at k-th communication, distance[k] = 2^k",
    "set incoming rank at k-th commuication",
    "set outgoing rank at k-th commuication",
    "defalut set as -1",
    "construct all recursive halving map for all machines",
    "let 1 << k <= num_machines",
    "distance of each communication",
    "if num_machines = 2^k, don't need to group machines",
    "communication direction, %2 == 0 is positive",
    "neighbor at k-th communication",
    "receive data block at k-th communication",
    "send data block at k-th communication",
    "if num_machines != 2^k, need to group machines",
    "group, two machine in one group, total \"rest\" groups will have 2 machines.",
    "let left machine as group leader",
    "cache block information for groups, group with 2 machines will have double block size",
    "convert from group to node leader",
    "convert from node to group",
    "meet new group",
    "add block len for this group",
    "calculate the group block start",
    "not need to construct",
    "get receive block informations",
    "accumulate block len",
    "get send block informations",
    "accumulate block len",
    "static member definition",
    "if small package or small count , do it by all gather.(reduce the communication times.)",
    "assign the blocks to every rank.",
    "do reduce scatter",
    "do all gather",
    "assign blocks",
    "need use buffer here, since size of \"output\" is smaller than size after all gather",
    "copy back",
    "assign blocks",
    "start all gather",
    "use output as receive buffer",
    "get current local block size",
    "get out rank",
    "get send information",
    "get in rank",
    "get recv information",
    "send and recv at same time",
    "rotate in-place",
    "send local data to neighbor first",
    "receive neighbor data first",
    "reduce",
    "start recursive halfing",
    "get target",
    "get send information",
    "get recv information",
    "send and recv at same time",
    "reduce",
    "send result to neighbor",
    "receive result from neighbor",
    "copy result",
    "start up socket",
    "parser clients from file",
    "get ip list of local machine",
    "get local rank",
    "construct listener",
    "construct communication topo",
    "construct linkers",
    "free listener",
    "set timeout",
    "accept incoming socket",
    "receive rank",
    "add new socket",
    "save ranks that need to connect with",
    "start listener",
    "start connect",
    "let smaller rank connect to larger rank",
    "send local rank",
    "wait for listener",
    "print connected linkers",
    "Get some statistic from 2 line",
    "if only have one line on file",
    "get column names",
    "load label idx first",
    "erase label column name",
    "load ignore columns",
    "load weight idx",
    "load group idx",
    "load categorical features",
    "don't support query id in data file when training in parallel",
    "read data to memory",
    "sample data",
    "construct feature bin mappers",
    "initialize label",
    "extract features",
    "sample data from file",
    "construct feature bin mappers",
    "initialize label",
    "extract features",
    "load data from binary file",
    "check meta data",
    "need to check training data",
    "read data in memory",
    "initialize label",
    "extract features",
    "Get number of lines of data file",
    "initialize label",
    "extract features",
    "load data from binary file",
    "not need to check validation data",
    "check meta data",
    "buffer to read binary file",
    "check token",
    "read size of header",
    "re-allocmate space if not enough",
    "read header",
    "get header",
    "get feature names",
    "write feature names",
    "read size of meta data",
    "re-allocate space if not enough",
    "read meta data",
    "load meta data",
    "sample local used data if need to partition",
    "if not contain query file, minimal sample unit is one record",
    "if contain query file, minimal sample unit is one query",
    "if is new query",
    "read feature data",
    "read feature size",
    "re-allocate space if not enough",
    "fill feature_names_ if not header",
    "-1 means doesn't use this feature",
    "map real feature index to used feature index",
    "push new feature",
    "if feature is trival(only 1 bin), free spaces",
    "---- private functions ----",
    "read all lines",
    "get query data",
    "if not contain query data, minimal sample unit is one record",
    "if contain query data, minimal sample unit is one query",
    "if is new query",
    "get query data",
    "if not contain query file, minimal sample unit is one record",
    "if contain query file, minimal sample unit is one query",
    "if is new query",
    "sample_values[i][j], means the value of j-th sample on i-th feature",
    "temp buffer for one line features and label",
    "parse features",
    "if need expand feature set",
    "-1 means doesn't use this feature",
    "check the range of label_idx, weight_idx and group_idx",
    "fill feature_names_ if not header",
    "start find bins",
    "if only one machine, find bin locally",
    "map real feature index to used feature index",
    "push new feature",
    "if feature is trival(only 1 bin), free spaces",
    "if have multi-machines, need find bin distributed",
    "different machines will find bin for different features",
    "start and len will store the process feature indices for different machines",
    "machine i will find bins for features in [ strat[i], start[i] + len[i] )",
    "get size of bin mapper with max_bin_ size",
    "since sizes of different feature may not be same, we expand all bin mapper to type_size",
    "find local feature bins and copy to buffer",
    "convert to binary size",
    "gather global feature bin mappers",
    "restore features bins from buffer",
    "if doesn't need to prediction with initial model",
    "parser",
    "set label",
    "free processed line:",
    "shrink_to_fit will be very slow in linux, and seems not free memory, disable for now",
    "text_reader_->Lines()[i].shrink_to_fit();",
    "push data",
    "if is used feature",
    "if need to prediction with initial model",
    "parser",
    "set initial score",
    "set label",
    "free processed line:",
    "shrink_to_fit will be very slow in linux, and seems not free memory, disable for now",
    "text_reader_->Lines()[i].shrink_to_fit();",
    "push data",
    "if is used feature",
    "metadata_ will manage space of init_score",
    "text data can be free after loaded feature values",
    "parser",
    "set initial score",
    "set label",
    "push data",
    "if is used feature",
    "only need part of data",
    "need full data",
    "metadata_ will manage space of init_score",
    "read size of token",
    "deep copy function for BinMapper",
    "find distinct_values first",
    "push zero in the front",
    "push zero in the back",
    "use distinct value is enough",
    "mean size for one bin",
    "need a new bin",
    "update bin upper bound",
    "last bin upper bound",
    "convert to int type first",
    "sort by counts",
    "will ingore the categorical of small counts",
    "check trival(num_bin_ == 1) feature",
    "calculate sparse rate",
    "sparse threshold",
    "for lambdarank, it needs query data for partition data in parallel learning",
    "need convert query_id to boundaries",
    "check weights",
    "check query boundries",
    "contain initial score file",
    "check weights",
    "get local weights",
    "check query boundries",
    "get local query boundaries",
    "contain initial score file",
    "get local initial scores",
    "re-load query weight",
    "save to nullptr",
    "save to nullptr",
    "save to nullptr",
    "default weight file name",
    "default weight file name",
    "use first line to count number class",
    "default query file name",
    "root is in the depth 0",
    "update parent info",
    "if cur node is left child",
    "add new node",
    "add two new leaves",
    "update new leaves",
    "save current leaf value to internal node before change",
    "update leaf depth",
    "non-leaf",
    "leaf",
    "load main config types",
    "generate seeds by seed.",
    "sub-config setup",
    "check for conflicts",
    "clear old metrics",
    "to lower",
    "split",
    "remove dumplicate",
    "check if objective_type, metric_type, and num_class match",
    "Change pool size to -1 (not limit) when using data parallel to reduce communication costs",
    "label_gain = 2^i - 1, may overflow, so we use 31 here",
    "label_gain = 2^i - 1, may overflow, so we use 31 here",
    "default eval ndcg @[1-5]",
    "copy feature bin mapper data",
    "if not pass a filename, just append \".bin\" of original file",
    "get size of header",
    "size of feature names",
    "write header",
    "write feature names",
    "get size of meta data",
    "write meta data",
    "write feature data",
    "get size of feature",
    "write feature",
    "only binary classification need sigmoid transform",
    "init tree learner",
    "push training metrics",
    "not same training data, need reset score and others",
    "create score tracker",
    "update score",
    "create buffer for gradients and hessians",
    "get max feature index",
    "get label index",
    "get feature names",
    "get feature infos",
    "if need bagging, create buffer",
    "reset config for tree learner",
    "for a validation dataset, we need its score and metric",
    "update score",
    "random bagging, minimal unit is one record",
    "if need bagging",
    "set bagging data to tree learner",
    "get subset",
    "we need to predict out-of-bag socres of data for boosting",
    "boosting first",
    "bagging logic",
    "get sub gradients",
    "train a new tree",
    "if cannot learn a new tree, then stop",
    "shrinkage by learning rate",
    "update score",
    "add model",
    "reset score",
    "remove model",
    "print message for metric",
    "pop last early_stopping_round_ models",
    "update training score",
    "update validation score",
    "print training metric",
    "print validation metric",
    "objective function will calculate gradients and hessians",
    "output model type",
    "output number of class",
    "output label index",
    "output max_feature_idx",
    "output objective name",
    "output sigmoid parameter",
    "output tree models",
    "use serialized string to restore this object",
    "get number of classes",
    "get index of label",
    "get max_feature_idx first",
    "get sigmoid parameter",
    "get feature names",
    "returns offset, or lines.size() if not found.",
    "load feature information",
    "search for each feature name",
    "get tree models",
    "store the importance first",
    "sort the importance",
    "if need sigmoid transform",
    "Get the max size of pool",
    "at least need 2 leaves",
    "push split information for all leaves",
    "initialize ordered_bins_ with nullptr",
    "get ordered bin",
    "check existing for ordered bin",
    "initialize splits for leaf",
    "initialize data partition",
    "initialize ordered gradients and hessians",
    "if has ordered bin, need to allocate a buffer to fast split",
    "initialize ordered_bins_ with nullptr",
    "get ordered bin",
    "check existing for ordered bin",
    "initialize splits for leaf",
    "initialize data partition",
    "initialize ordered gradients and hessians",
    "if has ordered bin, need to allocate a buffer to fast split",
    "Get the max size of pool",
    "at least need 2 leaves",
    "push split information for all leaves",
    "some initial works before training",
    "save pointer to last trained tree",
    "root leaf",
    "only root leaf can be splitted on first time",
    "some initial works before finding best split",
    "find best threshold for every feature",
    "find best split from all features",
    "Get a leaf with max split gain",
    "Get split information for best leaf",
    "cannot split, quit",
    "split tree with best leaf",
    "reset histogram pool",
    "initialize used features",
    "Get used feature at current tree",
    "initialize data partition",
    "reset the splits for leaves",
    "Sumup for root",
    "use all data",
    "point to gradients, avoid copy",
    "use bagging, only use part of data",
    "copy used gradients and hessians to ordered buffer",
    "point to ordered_gradients_ and ordered_hessians_",
    "if has ordered bin, need to initialize the ordered bin",
    "use all data, pass nullptr",
    "bagging, only use part of data",
    "mark used data",
    "initialize ordered bin",
    "check depth of current leaf",
    "only need to check left leaf, since right leaf is in same level of left leaf",
    "no enough data to continue",
    "-1 if only has one leaf. else equal the index of smaller leaf",
    "only have root",
    "put parent(left) leaf's histograms into larger leaf's histograms",
    "put parent(left) leaf's histograms to larger leaf's histograms",
    "init for the ordered gradients, only initialize when have 2 leaves",
    "only need to initialize for smaller leaf",
    "Get leaf boundary",
    "copy",
    "assign pointer",
    "need order gradient for larger leaf",
    "copy",
    "split for the ordered bin",
    "mark data that at left-leaf",
    "split the ordered bin",
    "feature is not used",
    "if parent(larger) leaf cannot split at current feature",
    "construct histograms for smaller leaf",
    "if not use ordered bin",
    "used ordered bin",
    "find best threshold for smaller child",
    "only has root leaf",
    "construct histgroms for large leaf, we initialize larger leaf as the parent,",
    "so we can just subtract the smaller leaf's histograms",
    "if not use ordered bin",
    "used ordered bin",
    "find best threshold for larger child",
    "left = parent",
    "split tree, will return right leaf",
    "split data partition",
    "init the leaves that used on next iteration",
    "get feature partition",
    "get local used features",
    "get best split at smaller leaf",
    "get best split at larger leaf",
    "sync global best info",
    "copy back",
    "update best split",
    "initialize SerialTreeLearner",
    "Get local rank and global machine size",
    "allocate buffer for communication",
    "generate feature partition for current tree",
    "get local used feature",
    "get block start and block len for reduce scatter",
    "get buffer_write_start_pos_",
    "get buffer_read_start_pos_",
    "sync global data sumup info",
    "global sumup reduce",
    "copy back",
    "set global sumup info",
    "init global data count in leaf",
    "construct local histograms",
    "construct histograms for smaller leaf",
    "if not use ordered bin",
    "used ordered bin",
    "copy to buffer",
    "Reduce scatter for histogram",
    "restore global histograms from buffer",
    "find best threshold for smaller child",
    "only root leaf",
    "construct histgroms for large leaf, we init larger leaf as the parent, so we can just subtract the smaller leaf's histograms",
    "find best threshold for larger child",
    "find local best split for smaller leaf",
    "find local best split for larger leaf",
    "sync global best info",
    "set best split",
    "need update global number of data in leaf",
    "limit top k",
    "get max bin",
    "calculate buffer size",
    "left and right on same time, so need double size",
    "initialize histograms for global",
    "sync global data sumup info",
    "set global sumup info",
    "init global data count in leaf",
    "get local sumup",
    "get local sumup",
    "get mean number on machines",
    "weighted gain",
    "get top k",
    "Copy histogram to buffer, and Get local aggregate features",
    "copy histograms.",
    "copy smaller leaf histograms first",
    "mark local aggregated feature",
    "copy",
    "then copy larger leaf histograms",
    "mark local aggregated feature",
    "copy",
    "use local data to find local best splits",
    "local voting",
    "gather",
    "get all top-k from all machines",
    "global voting",
    "copy local histgrams to buffer",
    "Reduce scatter for histogram",
    "find best split from local aggregated histograms",
    "restore from buffer",
    "find best threshold",
    "restore from buffer",
    "find best threshold",
    "find local best",
    "sync global best info",
    "copy back",
    "set the global number of data for leaves",
    "init the global sumup info"
  ]
}