Version,Commit Message
v3.5.1,!/usr/bin/env python
v3.5.1,!/usr/bin/env python
v3.5.1,!/usr/bin/env python
v3.5.1,!/usr/bin/env python
v3.5.1,!/usr/bin/env python
v3.5.1,!/usr/bin/env python3
v3.5.1,-*- coding: utf-8 -*-
v3.5.1,
v3.5.1,"OpenNMT-py documentation build configuration file, created by"
v3.5.1,sphinx-quickstart on Sun Dec 17 12:07:14 2017.
v3.5.1,
v3.5.1,This file is execfile()d with the current directory set to its
v3.5.1,containing dir.
v3.5.1,
v3.5.1,Note that not all possible configuration values are present in this
v3.5.1,autogenerated file.
v3.5.1,
v3.5.1,All configuration values have a default; values that are commented out
v3.5.1,serve to show the default.
v3.5.1,"If extensions (or modules to document with autodoc) are in another directory,"
v3.5.1,add these directories to sys.path here. If the directory is relative to the
v3.5.1,"documentation root, use os.path.abspath to make it absolute, like shown here."
v3.5.1,
v3.5.1,import os
v3.5.1,import sys
v3.5.1,"sys.path.insert(0, os.path.abspath('.'))"
v3.5.1,-- General configuration ------------------------------------------------
v3.5.1,"If your documentation needs a minimal Sphinx version, state it here."
v3.5.1,
v3.5.1,needs_sphinx = '6.0'
v3.5.1,"Add any Sphinx extension module names here, as strings. They can be"
v3.5.1,extensions coming with Sphinx (named 'sphinx.ext.*') or your custom
v3.5.1,ones.
v3.5.1,Show base classes
v3.5.1,"Use ""variables"" section for Attributes instead of weird block things"
v3.5.1,mimicking the function style.
v3.5.1,"Add any paths that contain templates here, relative to this directory."
v3.5.1,The suffix(es) of source filenames.
v3.5.1,You can specify multiple suffix as a list of string:
v3.5.1,
v3.5.1,"source_suffix = ['.rst', '.md']"
v3.5.1,The master toctree document.
v3.5.1,General information about the project.
v3.5.1,"The version info for the project you're documenting, acts as replacement for"
v3.5.1,"|version| and |release|, also used in various other places throughout the"
v3.5.1,built documents.
v3.5.1,
v3.5.1,The short X.Y version.
v3.5.1,"The full version, including alpha/beta/rc tags."
v3.5.1,The language for content autogenerated by Sphinx. Refer to documentation
v3.5.1,for a list of supported languages.
v3.5.1,
v3.5.1,This is also used if you do content translation via gettext catalogs.
v3.5.1,"Usually you set ""language"" from the command line for these cases."
v3.5.1,"List of patterns, relative to source directory, that match files and"
v3.5.1,directories to ignore when looking for source files.
v3.5.1,This patterns also effect to html_static_path and html_extra_path
v3.5.1,The name of the Pygments (syntax highlighting) style to use.
v3.5.1,"If true, `todo` and `todoList` produce output, else they produce nothing."
v3.5.1,-- Options for HTML output ----------------------------------------------
v3.5.1,The theme to use for HTML and HTML Help pages.  See the documentation for
v3.5.1,a list of builtin themes.
v3.5.1,
v3.5.1,html_theme = 'sphinx_materialdesign_theme'
v3.5.1,html_theme_path = [sphinx_materialdesign_theme.get_path()]
v3.5.1,Theme options are theme-specific and customize the look and feel of a theme
v3.5.1,"further.  For a list of options available for each theme, see the"
v3.5.1,documentation.
v3.5.1,
v3.5.1,html_theme_options = {}
v3.5.1,"Add any paths that contain custom static files (such as style sheets) here,"
v3.5.1,"relative to this directory. They are copied after the builtin static files,"
v3.5.1,"so a file named ""default.css"" will overwrite the builtin ""default.css""."
v3.5.1,"Custom sidebar templates, must be a dictionary that maps document names"
v3.5.1,to template names.
v3.5.1,
v3.5.1,This is required for the alabaster theme
v3.5.1,refs: http://alabaster.readthedocs.io/en/latest/installation.html#sidebars
v3.5.1,-- Options for HTMLHelp output ------------------------------------------
v3.5.1,Output file base name for HTML help builder.
v3.5.1,-- Options for LaTeX output ---------------------------------------------
v3.5.1,The paper size ('letterpaper' or 'a4paper').
v3.5.1,
v3.5.1,"'papersize': 'letterpaper',"
v3.5.1,"The font size ('10pt', '11pt' or '12pt')."
v3.5.1,
v3.5.1,"'pointsize': '10pt',"
v3.5.1,Additional stuff for the LaTeX preamble.
v3.5.1,
v3.5.1,"'preamble': '',"
v3.5.1,Latex figure (float) alignment
v3.5.1,
v3.5.1,"'figure_align': 'htbp',"
v3.5.1,Grouping the document tree into LaTeX files. List of tuples
v3.5.1,"(source start file, target name, title,"
v3.5.1,"author, documentclass [howto, manual, or own class])."
v3.5.1,-- Options for manual page output ---------------------------------------
v3.5.1,One entry per manual page. List of tuples
v3.5.1,"(source start file, name, description, authors, manual section)."
v3.5.1,-- Options for Texinfo output -------------------------------------------
v3.5.1,Grouping the document tree into Texinfo files. List of tuples
v3.5.1,"(source start file, target name, title, author,"
v3.5.1,"dir menu entry, description, category)"
v3.5.1,Build the translator (along with the model)
v3.5.1,Put messages sizes in antichronological order
v3.5.1,Caluculate antichronological history sizes
v3.5.1,Prune the history from the beginning
v3.5.1,Put back indices in chronological order.
v3.5.1,Build the translator (along with the model)
v3.5.1,We need to build the Llama tokenizer to count tokens and prune the history.
v3.5.1,The hypotheses are lists of one element but we still need to take the first one.
v3.5.1,#####
v3.5.1,UI #
v3.5.1,#####
v3.5.1,What are the 3 best french cities ?
v3.5.1,Which one is better if I like outdoor activities ?
v3.5.1,Which one is better if I like cultural outings?
v3.5.1,What are the best neighborhoods in these 5 cities?
v3.5.1,!/usr/bin/env python3
v3.5.1,Usage: python3 filter_train.py in.src in.trg out.src out.trg max-tokens
v3.5.1,flake8: noqa
v3.5.1,-*- coding: utf-8 -*-
v3.5.1,Generated by the protocol buffer compiler.  DO NOT EDIT!
v3.5.1,source: sentencepiece_model.proto
v3.5.1,@@protoc_insertion_point(imports)
v3.5.1,@@protoc_insertion_point(module_scope)
v3.5.1,!/usr/bin/env python
v3.5.1,-*- coding: utf-8 -*-
v3.5.1,is this reachable?
v3.5.1,Read in embeddings
v3.5.1,Write to file
v3.5.1,!/usr/bin/env python
v3.5.1,if shard == 0:
v3.5.1,"vocab_size = onmt_safetensor[""generator.weight""].size(0)"
v3.5.1,"vocab[11] = ""</s>""  # Falcon only"
v3.5.1,converts a SentencePiece vocabulary to the format expected by dynamic data
v3.5.1,"(essentially converts float expected counts to ""fixed precision"" int pseudo"
v3.5.1,counts)
v3.5.1,from onmt.utils.misc import use_gpu
v3.5.1,"Add in default model arguments, possibly added since training."
v3.5.1,this patch is no longer needed included in converter
v3.5.1,"if hasattr(model_opt, 'rnn_size'):"
v3.5.1,model_opt.hidden_size = model_opt.rnn_size
v3.5.1,build_base_model expects updated and validated opts
v3.5.1,-*- encoding: utf-8 -*-
v3.5.1,!/usr/bin/env python
v3.5.1,Falcon stores QKV in one single tensor but it is not simply piled up Q+K+V
v3.5.1,it is heads interleaved to we need to slice first
v3.5.1,also it uses the HF rotary so we need to permute Q and K interleave
v3.5.1,!/usr/bin/env python
v3.5.1,-*- coding: utf-8 -*-
v3.5.1,Author: Rico Sennrich
v3.5.1,flake8: noqa
v3.5.1,This file is retrieved from https://github.com/rsennrich/subword-nmt
v3.5.1,hack for python2/3 compatibility
v3.5.1,check version information
v3.5.1,some hacking to deal with duplicates (only consider first instance)
v3.5.1,don't print end-of-word symbols
v3.5.1,sys.stderr.write('cannot split {0} further.\n'.format(segment))
v3.5.1,sys.stderr.write('OOV: {0}\n'.format(segment))
v3.5.1,sys.stderr.write('OOV: {0}\n'.format(segment))
v3.5.1,python 2/3 compatibility
v3.5.1,read/write files as UTF-8
v3.5.1,!/usr/bin/env python3
v3.5.1,coding: utf-8
v3.5.1,"In order to use this tool, please install comet first"
v3.5.1,https://github.com/Unbabel/COMET
v3.5.1,Let's say you have a source file with N sentences in SL - eg: source.sl
v3.5.1,and the corresponding references (N sentences) reference.tl
v3.5.1,Translate your file in TL with the -n_best nbest options nbest being
v3.5.1,then number of hypotheses and output the target to -output target.nbest.tl
v3.5.1,Then you need to duplicate source and reference sentences nbest times
v3.5.1,for this script.
v3.5.1,for instance using awk '{for(i=1; i<=n; i++) print}' n=5 reference.tl \
v3.5.1,> reference.5.tl
v3.5.1,same for source.
v3.5.1,This script can be run (for instance with nbest = 5) as follows:
v3.5.1,python oracle_bleu.py --nbest-src source.5.sl --nbest-hyp target.5.tl \
v3.5.1,--nbest-ref reference.5.tl --nbest-order 5 --output target.maxbleu.tl
v3.5.1,It will search in all hyp the best comet score
v3.5.1,when choosing a reference-less model no nbest-ref is required
v3.5.1,for nbest in nbests:
v3.5.1,!/usr/bin/env python
v3.5.1,!/usr/bin/env python3
v3.5.1,coding: utf-8
v3.5.1,Let's say you have a source file with N sentences in SL - eg: source.sl
v3.5.1,Translate your file in TL with the -n_best nbest options nbest being
v3.5.1,then number of hypotheses and output the target to -output target.nbest.tl
v3.5.1,This script can be run (for instance with nbest = 5) as follows:
v3.5.1,python mbr_bleu.py --nbest-hyp target.5.tl \
v3.5.1,--nbest-order 5 --output target.mbr.tl
v3.5.1,It will compare all hyp with eachother and output the max bleu
v3.5.1,!/usr/bin/env python
v3.5.1,-*- coding: utf-8 -*-
v3.5.1,Author: Rico Sennrich
v3.5.1,flake8: noqa
v3.5.1,This file is retrieved from https://github.com/rsennrich/subword-nmt
v3.5.1,hack for python2/3 compatibility
v3.5.1,"find all instances of pair, and update frequency/indices around it"
v3.5.1,find first symbol
v3.5.1,"if first symbol is followed by second symbol, we've found an occurrence of pair (old_word[i:i+2])"
v3.5.1,"assuming a symbol sequence ""A B C"", if ""B C"" is merged, reduce the frequency of ""A B"""
v3.5.1,"assuming a symbol sequence ""A B C B"", if ""B C"" is merged, reduce the frequency of ""C B""."
v3.5.1,"however, skip this if the sequence is A B C B C, because the frequency of ""C B"" will be reduced by the previous code block"
v3.5.1,find new pair
v3.5.1,"assuming a symbol sequence ""A BC D"", if ""B C"" is merged, increase the frequency of ""A BC"""
v3.5.1,"assuming a symbol sequence ""A BC B"", if ""B C"" is merged, increase the frequency of ""BC B"""
v3.5.1,"however, if the sequence is A BC BC, skip this step because the count of ""BC BC"" will be incremented by the previous code block"
v3.5.1,data structure of pair frequencies
v3.5.1,index from pairs to words
v3.5.1,version 0.2 changes the handling of the end-of-word token ('</w>');
v3.5.1,version numbering allows bckward compatibility
v3.5.1,"threshold is inspired by Zipfian assumption, but should only affect speed"
v3.5.1,we probably missed the best pair because of pruning; go back to full statistics
v3.5.1,"threshold is inspired by Zipfian assumption, but should only affect speed"
v3.5.1,python 2/3 compatibility
v3.5.1,read/write files as UTF-8
v3.5.1,Now we can pipe the full file through the model using the Iterator
v3.5.1,reminder a batch includes .src .tgt .indices and it is sorted
v3.5.1,print(batch)
v3.5.1,Compute and retrieve the loss for EACH sentence
v3.5.1,Now we need to rearrange the batch of ppl
v3.5.1,in the original order with indices
v3.5.1,!/usr/bin/env python
v3.5.1,-*- coding: utf-8 -*-
v3.5.1,!/usr/bin/env python
v3.5.1,!/usr/bin/env python
v3.5.1,!/usr/bin/env python
v3.5.1,!/usr/bin/env python
v3.5.1,!/usr/bin/env python
v3.5.1,!/usr/bin/env python3
v3.5.1,coding: utf-8
v3.5.1,Let's say you have a source file with N sentences in SL - eg: source.sl
v3.5.1,and the corresponding references (N sentences) reference.tl
v3.5.1,Translate your file in TL with the -n_best nbest options nbest being
v3.5.1,then number of hypotheses and output the target to -output target.nbest.tl
v3.5.1,Then you need to duplicate reference sentences nbest times for this script.
v3.5.1,for instance using awk '{for(i=1; i<=n; i++) print}' n=5 reference.tl \
v3.5.1,> reference.5.tl
v3.5.1,This script can be run (for instance with nbest = 5) as follows:
v3.5.1,python oracle_bleu.py --nbest-hyp target.5.tl --nbest-ref reference.5.tl \
v3.5.1,--nbest-order 5 --output target.maxbleu.tl
v3.5.1,It will search in all hyp the best bleu wrt reference
v3.5.1,and output the max bleu
v3.5.1,!/usr/bin/env python
v3.5.1,!/usr/bin/env python
v3.5.1,with the two module = imp.load_source() below
v3.5.1,we ghost the old torchtext.data.field and depercated
v3.5.1,onmt.inputters.text_dataset
v3.5.1,however this require some functions / classes to be
v3.5.1,monkey patched for loading the old field/vocab objects.
v3.5.1,"module3 = imp.load_source(""Vocab"", ""tools/convertv2_v3.py"")"
v3.5.1,"voc = dict(sorted(fields.vocab.__dict__['freqs'].items(),"
v3.5.1,"key=lambda x: (-x[1], x[0]))).keys()"
v3.5.1,"voc = dict(sorted(fields.vocab.__dict__['freqs'].items(),"
v3.5.1,"key=lambda x: (-x[1], x[0]))).keys()"
v3.5.1,!/usr/bin/env python
v3.5.1,redpajama stores QKV in one single tensor but it is not simply piled up Q+K+V
v3.5.1,it is heads interleaved to we need to slice first
v3.5.1,also it uses the HF rotary so we need to permute Q and K interleave
v3.5.1,Avoid functionality on inference
v3.5.1,weights are in the .pt file
v3.5.1,weights are not in the .pt checkpoint but stored in the safetensors file
v3.5.1,Build embeddings.
v3.5.1,Build encoder.
v3.5.1,Build embeddings.
v3.5.1,Build decoder.
v3.5.1,Share the embedding matrix - preprocess with share_vocab required.
v3.5.1,src/tgt vocab should be the same if `-share_vocab` is specified.
v3.5.1,Update vocabulary embeddings with checkpoint embeddings
v3.5.1,Embedding layers
v3.5.1,Just for debugging purposes
v3.5.1,Remove old vocabulary associated embeddings
v3.5.1,for back compat when attention_dropout was not defined
v3.5.1,Build Model
v3.5.1,Build Generator.
v3.5.1,If new training initialize the model params
v3.5.1,If update_vocab init also but checkpoint will overwrite old weights
v3.5.1,ONLY for legacy fusedam with amp pytorch requires NOT to half the model
v3.5.1,Update model embeddings with those from the checkpoint
v3.5.1,after initialization
v3.5.1,after this checkpoint contains no embeddings
v3.5.1,when using LoRa or updating the vocab (no more embeddings in ckpt)
v3.5.1,=> strict=False when loading state_dict
v3.5.1,weights are in the .pt file
v3.5.1,weights are not in the .pt checkpoint but stored in the safetensors file
v3.5.1,!/usr/bin/env python
v3.5.1,"maybe prepare pretrained embeddings, if any"
v3.5.1,if transform + options set in 'valid' we need to copy in main
v3.5.1,transform / options for scoring considered as inference
v3.5.1,Load checkpoint if we resume from a previous training.
v3.5.1,ensure tensorboard output is written in the directory
v3.5.1,of previous checkpoints
v3.5.1,Override checkpoint's update_embeddings as it defaults to false
v3.5.1,Override checkpoint's freezing settings as it defaults to false
v3.5.1,NOTE: It's important that ``opt`` has been validated and updated
v3.5.1,at this point.
v3.5.1,Build model.
v3.5.1,Build optimizer.
v3.5.1,Build model saver
v3.5.1,Use Tensorboard for visualization during training
v3.5.1,Options only during inference
v3.5.1,"Truncation options, for text corpus"
v3.5.1,"as for False, this will be added in _add_train_general_opts"
v3.5.1,GPU
v3.5.1,Embedding Options
v3.5.1,Model Task Options
v3.5.1,Encoder-Decoder Options
v3.5.1,Freeze Encoder and/or Decoder
v3.5.1,The following options (bridge_extra_node to n_steps) are used
v3.5.1,for training with --encoder_type ggnn (Gated Graph Neural Network).
v3.5.1,Attention options
v3.5.1,Alignement options
v3.5.1,Generator and loss options.
v3.5.1,LoRa
v3.5.1,Init options
v3.5.1,Pretrained word vectors
v3.5.1,Freeze word vectors
v3.5.1,Optimization options
v3.5.1,learning rate
v3.5.1,Alpha and Beta values for Google Length + Coverage penalty
v3.5.1,"Described here: https://arxiv.org/pdf/1609.08144.pdf, Section 7"
v3.5.1,Length penalty options
v3.5.1,Coverage penalty options
v3.5.1,Decoding Length constraint
v3.5.1,Decoding content constraint
v3.5.1,Adding options related to source and target features
v3.5.1,Adding options relate to decoding strategy
v3.5.1,Adding option for logging
v3.5.1,Adding options related to Transforms
v3.5.1,Copyright 2016 The Chromium Authors. All rights reserved.
v3.5.1,Use of this source code is governed by a BSD-style license that can be
v3.5.1,found in the LICENSE file.
v3.5.1,"Get the key 'value' in the dict, or just use 'value'"
v3.5.1,Create a thread to listen for errors in the child processes.
v3.5.1,Build translator
v3.5.1,Build vocab
v3.5.1,Build transform pipe
v3.5.1,Basic attributes.
v3.5.1,Set model in training mode.
v3.5.1,Let's clean the GPUs before training loop
v3.5.1,UPDATE DROPOUT
v3.5.1,Run patience mechanism
v3.5.1,"If the patience has reached the limit, stop training"
v3.5.1,swap model params w/ moving average
v3.5.1,(and keep the original parameters)
v3.5.1,Set model in validating mode.
v3.5.1,raw_srcs = []
v3.5.1,raw_refs = []
v3.5.1,F-prop through the model.
v3.5.1,Compute loss.
v3.5.1,Compute validation metrics (at batch.dataset level)
v3.5.1,Compute stats
v3.5.1,Update statistics.
v3.5.1,Set model back to training mode.
v3.5.1,Truncated BPTT: reminder not compatible with accum > 1
v3.5.1,1. Create truncated target.
v3.5.1,2. F-prop all but generator.
v3.5.1,3. Compute loss.
v3.5.1,The loss of the prompt will be set to zero.
v3.5.1,"If truncated, don't backprop fully."
v3.5.1,"in case of multi step gradient accumulation,"
v3.5.1,update only after accum batches
v3.5.1,For Flake
v3.5.1,we avoid padding while mean pooling
v3.5.1,incoming and outgoing edge embedding
v3.5.1,Find vocab data for tree builting
v3.5.1,Propogation Model
v3.5.1,Initialize the bridge layer
v3.5.1,Token embedding
v3.5.1,Initialize graph using formatted input sequence
v3.5.1,Number of flagged nodes defines node count for this sample
v3.5.1,"(Nodes can have no flags on them, but must be in 'flags' list)."
v3.5.1,The total number of integers in the vocab should allow
v3.5.1,for all features and edges to be defined.
v3.5.1,Use first extra node as only source for decoder init
v3.5.1,Average all nodes to get bridge input
v3.5.1,"LSTM has hidden and cell state, other only one"
v3.5.1,Total number of states
v3.5.1,Build a linear layer for each
v3.5.1,Initialize the bridge layer
v3.5.1,src lengths data is wrapped inside a Tensor.
v3.5.1,"LSTM has hidden and cell state, other only one"
v3.5.1,Total number of states
v3.5.1,Build a linear layer for each
v3.5.1,Auto import python files in this directory
v3.5.1,batch x len x dim
v3.5.1,"feed_forward applies residual, so we remove and apply residual with un-normed"
v3.5.1,Padding mask is now (batch x 1 x slen x slen)
v3.5.1,1 to be expanded to number of heads in MHA
v3.5.1,Run the forward pass of every layer of the tranformer.
v3.5.1,Dimensions and padding for constructing the word embedding matrix
v3.5.1,Dimensions and padding for feature embedding matrices
v3.5.1,(these have no effect if feat_vocab_sizes is empty)
v3.5.1,The embedding matrix look-up tables. The first look-up table
v3.5.1,"is for words. Subsequent ones are for features, if any exist."
v3.5.1,The final output size of word + feature vectors. This can vary
v3.5.1,from the word vector size if and only if features are defined.
v3.5.1,This is the attribute you should access if you need to know
v3.5.1,how big your embeddings are going to be.
v3.5.1,The sequence of operations that converts the input sequence
v3.5.1,into a sequence of embeddings. At minimum this consists of
v3.5.1,looking up the embeddings for each word and feature in the
v3.5.1,input. Model parameters may require the sequence to contain
v3.5.1,additional operations as well.
v3.5.1,features must use word_vec_size
v3.5.1,features will use feat_vec_size
v3.5.1,Some utilitary functions for pretrained embeddings
v3.5.1,is this reachable?
v3.5.1,Write to file
v3.5.1,set the opt in place
v3.5.1,set the opt in place
v3.5.1,flake8: noqa
v3.5.1,For command-line option parsing
v3.5.1,"Check pass, set the args."
v3.5.1,"This SRU version implements its own cuda-level optimization,"
v3.5.1,so it requires that:
v3.5.1,1. `cupy` and `pynvrtc` python package installed.
v3.5.1,2. pytorch is built with cuda support.
v3.5.1,3. library path set: export LD_LIBRARY_PATH=<cuda lib path>.
v3.5.1,Check 1.
v3.5.1,Check 2.
v3.5.1,Check 3.
v3.5.1,This sets up device to use.
v3.5.1,-> directions x batch x dim
v3.5.1,For DEBUG
v3.5.1,"size = (length, batch, x.size(-1)) \"
v3.5.1,"if x.dim() == 3 else (batch, x.size(-1))"
v3.5.1,grad_x = x.new(*x.size()) if k_ == 3 else x.new(*size).zero_()
v3.5.1,Normal use
v3.5.1,"An entry check here, will catch on train side and translate side"
v3.5.1,if requirements are not satisfied.
v3.5.1,RNNDecoderState wraps hidden as a tuple.
v3.5.1,fh -> (layers*directions) x batch x dim
v3.5.1,This class is mainly used by decoder.py for RNNs but also
v3.5.1,by the CNN / transformer decoder when copy attention is used
v3.5.1,CNN has its own attention mechanism ConvMultiStepAttention
v3.5.1,Transformer has its own MultiHeadedAttention
v3.5.1,mlp wants it with bias
v3.5.1,"(batch, t_len, d) x (batch, d, s_len) --> (batch, t_len, s_len)"
v3.5.1,"(batch, t_len, s_len, d)"
v3.5.1,one step input
v3.5.1,"compute attention scores, as in Luong et al."
v3.5.1,Softmax or sparsemax to normalize attention weights
v3.5.1,each context vector c_t is the weighted average
v3.5.1,over all the source hidden states
v3.5.1,concatenate
v3.5.1,clamping necessary because of numerical errors: loss should be lower
v3.5.1,"bounded by zero, but negative values near zero are possible without"
v3.5.1,the clamp
v3.5.1,Help functions for Rotary Embeddings
v3.5.1,https://arxiv.org/pdf/2104.09864.pdf
v3.5.1,too convoluted to make maxseqlen a parameter.
v3.5.1,we suppose src_seq_len at training and max_length at inference
v3.5.1,are both < 2048 tokens.
v3.5.1,"rope is now matrix [maxseqlen, dim/2]"
v3.5.1,Help functions for max_relative positions
v3.5.1,https://arxiv.org/abs/1803.02155
v3.5.1,Shift values to be >= 0
v3.5.1,"now relative_position is in the range [0, inf)"
v3.5.1,half of the buckets are for exact increments in positions
v3.5.1,The other half of the buckets are for logarithmically bigger bins in positions
v3.5.1,up to max_distance
v3.5.1,Help functions to split model dim per head
v3.5.1,https://arxiv.org/pdf/1803.02155.pdf
v3.5.1,in the paper they suggest either two embeds
v3.5.1,relative_key / relative_value or only
v3.5.1,relative_key. We implemented the same embed
v3.5.1,for both.
v3.5.1,"1) Project key, value, and query."
v3.5.1,as a reminder at training layer_cache[0] remains False
v3.5.1,Retrieve keys and values from the KV cache (decoding mode only).
v3.5.1,Resize rotary embeddings.
v3.5.1,Resize rotary embeddings.
v3.5.1,We take a margin of 32 tokens as the kv_cache
v3.5.1,is incremented by 32 tokens every 32 tokens.
v3.5.1,Increase the cached key pad mask by concatenation.
v3.5.1,For decoding only.
v3.5.1,Retrieve keys and values from linear layers (training mode).
v3.5.1,Resize rotary embeddings.
v3.5.1,expand key on heads dimension when it's less than query heads (multi-query variant)
v3.5.1,expand value on heads dimension when it's less than query heads (multi-query variant)
v3.5.1,"2) When standard pos. enc. or rotary, use flash attention"
v3.5.1,Ultimately flashv2 will be part of pytorch https://github.com/pytorch/pytorch/pull/105602
v3.5.1,"In the meantime: if vanilla tranformer or Rotary embeddings (not rel_pos, not alibi)"
v3.5.1,then use flash2 if seq len > 256 otherwise use xtransformer from pt2 uptream
v3.5.1,Apply flash2 attention.
v3.5.1,Apply scaled dot product attention.
v3.5.1,batch x num_heads x query_len x key_len
v3.5.1,1 or key_len x key_len
v3.5.1,1 or key_len x key_len x dim_per_head
v3.5.1,not 100% necessary but expand to nb of heads
v3.5.1,now mask and scores have the same shape
v3.5.1,3) Apply attention dropout and compute context vectors.
v3.5.1,We use the same embeddings for key and value
v3.5.1,--------------------------------------------------------------------------
v3.5.1,copied and adapted https://github.com/microsoft/LoRA/
v3.5.1,Copyright (c) Microsoft Corporation. All rights reserved.
v3.5.1,Licensed under the MIT License (MIT).
v3.5.1,Support bnb quantization of nderlying layers
v3.5.1,--------------------------------------------------------------------------
v3.5.1,Optional dropout
v3.5.1,Mark the weight as unmerged
v3.5.1,LoRA implemented in a dense layer
v3.5.1,Actual trainable parameters
v3.5.1,Freezing the pre-trained weight matrix
v3.5.1,initialize A the same way as the default
v3.5.1,for nn.Linear and B to zero
v3.5.1,Make sure that the weights are not merged
v3.5.1,Merge the weights and mark it
v3.5.1,Actual trainable parameters
v3.5.1,Freezing the pre-trained weight matrix
v3.5.1,we do not super().reset_parameters() save lot of time and useless when no grad.
v3.5.1,initialize A the same way as the default
v3.5.1,for nn.Linear and B to zero
v3.5.1,Make sure that the weights are not merged
v3.5.1,Merge the weights and mark it
v3.5.1,cannot merge/unmerge quantized weigts with unquantized lora_X
v3.5.1,Check if QLoraLinear has a custom __init__ method
v3.5.1,Invoke the __init__ method of QLoraLinear
v3.5.1,LoRA implemented in a dense layer
v3.5.1,At the moment this class is only used by embeddings.Embeddings look-up tables
v3.5.1,for some reason list comprehension is slower in this scenario
v3.5.1,"for silu, see: https://arxiv.org/pdf/2002.05202.pdf"
v3.5.1,-*- coding: utf-8 -*-
v3.5.1,class AverageAttention(torch.jit.ScriptModule):
v3.5.1,@torch.jit.script
v3.5.1,Code taken from bitsandbytes but modified with arg device to accept skipt_init
v3.5.1,from torch.nn.utils => makes model building way faster.
v3.5.1,out_features * in_features
v3.5.1,norm is out_features * 1
v3.5.1,batch_size * out_features
v3.5.1,out_features
v3.5.1,out_features
v3.5.1,batch_size * out_features
v3.5.1,"out_channels, in_channels // groups, * kernel_size"
v3.5.1,out_features
v3.5.1,This is used nowhere in the code at the moment (Vincent Nguyen 05/18/2018)
v3.5.1,"in_channels, out_channels, *kernel_size"
v3.5.1,"in_channels, out_channels, *kernel_size"
v3.5.1,"self.out_channels, 1"
v3.5.1,out_features
v3.5.1,out_features
v3.5.1,store roots on diagonal
v3.5.1,Original probabilities.
v3.5.1,Probability of copying p(z=1) batch.
v3.5.1,Probability of not copying: p_{word}(w) * (1 - p(z))
v3.5.1,probabilities assigned by the model to the gold targets
v3.5.1,probability of tokens copied from source
v3.5.1,Set scores for unk to 0 and add eps
v3.5.1,find the indices in which you do not use the copy mechanism
v3.5.1,Drop padding.
v3.5.1,We exclude tokenization for contractions in
v3.5.1,order to avoid inconsistencies with pyonmtok's tokenization.
v3.5.1,"(e.g. ""I ca n't"" with spacy, ""I can ' t"" with pyonmttok)"
v3.5.1,Use Spacy's stopwords to get rid of junk entries
v3.5.1,Perform tokenization with spacy for consistency.
v3.5.1,We ensure that the target lemma is present in the lemmatized
v3.5.1,"target string, that the match is an exact match (there is"
v3.5.1,whitespace before or after the term)
v3.5.1,and we perform some bound checking.
v3.5.1,Map the lemmatized string match index to
v3.5.1,the lemmatized list index
v3.5.1,We need to know if the term is multiword
v3.5.1,Join multiword target lemmas with a unique separator so
v3.5.1,we can treat them as single word and not change the indices.
v3.5.1,Construct the final source from the lemmatized list
v3.5.1,that contains the terms. We compare the tokens in the
v3.5.1,term-augmented lemma list with the tokens in the original
v3.5.1,"lemma list. If the lemma is the same, then we replace with"
v3.5.1,the token from the original tokenized source list. If they
v3.5.1,"are not the same, it means the lemma has been augemented"
v3.5.1,"with a term, so we inject this in the final list."
v3.5.1,Restore the spaces in multi-word terms
v3.5.1,Skip half examples to improve performance. This means we set
v3.5.1,"a hard limit for the `term_corpus_ratio` to 0.5, which is actually"
v3.5.1,quite high. TODO: We can add this (skipping examples) as an option
v3.5.1,Filter out very short or very long sentences
v3.5.1,from the TM for better performance
v3.5.1,We split the `batch` and perform fuzzy matching
v3.5.1,in smaller chunks of 10.000 examples in order to
v3.5.1,reduce memory usage.
v3.5.1,Perfomance is not affected.
v3.5.1,Probably redundant but let's be safe
v3.5.1,in case some examples are already fuzzied
v3.5.1,(e.g. from another pipeline or workflow)
v3.5.1,We don't want exact matches
v3.5.1,Apply a basic filtering to leave out very short or very long
v3.5.1,sentences and speed up things a bit during fuzzy matching
v3.5.1,Do nothing
v3.5.1,We set the start number of tags to a random number from 1
v3.5.1,to 12 + the number of subsequent tags that
v3.5.1,will be added. We also apply weights to this choice so tags
v3.5.1,"are more probable to start from 1, then from 2, etc."
v3.5.1,This way we cover most scenarios met in real usage and
v3.5.1,the system will learn to handle a fairly large number of
v3.5.1,numbered tags (but not an excessively large number)
v3.5.1,Make sure we only search for exact matches (we don't want
v3.5.1,to match part of words) and perform some bound checking
v3.5.1,Create all possible tag forms. We inject a special
v3.5.1,unicode char (âˆ¥) as a placeholder for whitespace in order
v3.5.1,to keep the indices unaltered. This char is replaced with
v3.5.1,spaces before we return the augmented examples.
v3.5.1,Make a weighted choice between paired tags or single tags.
v3.5.1,"We usually encounter, and thus here we favor, paired tags"
v3.5.1,with a ratio 1/3.
v3.5.1,Check if the tags include the
v3.5.1,"mandatory ""#"" number placeholder"""
v3.5.1,We split the user-defined tags in the # placeholder
v3.5.1,in order to number them
v3.5.1,Skip half examples to speed up the transform. This sets
v3.5.1,"a hard limit of 0.5 to the `tags_corpus_ratio`, which is"
v3.5.1,excessive and should be avoided anyway.
v3.5.1,normalize dict src/tgt for each dataset
v3.5.1,"print(""src empty"")"
v3.5.1,"print(""too many same char in src"")"
v3.5.1,"print(""too many same word in src"")"
v3.5.1,"print(""avg token min"", len(src_str) / len(ex['src']))"
v3.5.1,"print(""avg token max"", len(src_str) / len(ex['src']))"
v3.5.1,"print(""text does not fully belong to wanted script"")"
v3.5.1,"print(""Some text belong to unwanted scripts"")"
v3.5.1,"print(""langid does not match"", _id(src_str))"
v3.5.1,"print(""src = tgt"")"
v3.5.1,"print(""tgt empty"")"
v3.5.1,"print(""src / tgt ratio "", len(src_str) / len(tgt_str))"
v3.5.1,"print(""too many same char in tgt"")"
v3.5.1,"print(""too many same word in tgt"")"
v3.5.1,"print(""avg token min"", len(tgt_str) / len(ex['tgt']))"
v3.5.1,"print(""avg token max"", len(tgt_str) / len(ex['tgt']))"
v3.5.1,"print(""text does not fully belong to wanted script"")"
v3.5.1,"print(""Some text belong to unwanted scripts"")"
v3.5.1,"print(""langid does not match"", _id(tgt_str))"
v3.5.1,"doc break we add it, restart new doc"
v3.5.1,case 1st ex is already longer
v3.5.1,adding cur ex is too long we add cur doc
v3.5.1,and reset doc to cur ex
v3.5.1,we start the new doc with cur ex
v3.5.1,we cumulate cur ex to cur doc
v3.5.1,Auto import python files in this directory
v3.5.1,1. sample number of tokens to corrupt
v3.5.1,2. sample positions to corrput
v3.5.1,3. sample corrupted values
v3.5.1,1. sample number of tokens to corrupt
v3.5.1,2. sample positions to corrput
v3.5.1,3. Drop token on chosen position
v3.5.1,1. sample number of tokens to corrupt
v3.5.1,2. sample positions to corrput
v3.5.1,3. mask word on chosen position
v3.5.1,"Sharing options among `TokenizerTransform`s, same name conflict in"
v3.5.1,this scope will be resolved by remove previous occurrence in parser
v3.5.1,subword regularization(or BPE dropout) options:
v3.5.1,subword vocabulary restriction options:
v3.5.1,This method embeds a custom logic to correctly handle certain placeholders
v3.5.1,in case the tokenizer doesn't preserve them.
v3.5.1,Locate the end-of-sentence placeholders.
v3.5.1,Tokenize each sentence separately.
v3.5.1,Locate the mask-before placeholders
v3.5.1,(to zero-out the prompt loss during LM finetuning).
v3.5.1,Tokenize each chunk separately and insert the padding token.
v3.5.1,between each sequence of tokens.
v3.5.1,Re-insert the eos token.
v3.5.1,derterministic subwording
v3.5.1,subword sampling when nbest_size > 1 or -1
v3.5.1,alpha should be 0.0 < alpha < 1.0
v3.5.1,Load vocabulary file if provided and set threshold
v3.5.1,Load Subword Model
v3.5.1,"ugly patch to make sure ""\n\n"" is split in two items"
v3.5.1,-1: keep everything (i.e. 1 mask per token)
v3.5.1,0: replace everything (i.e. no mask)
v3.5.1,1: 1 mask per span
v3.5.1,view each subword as word start / input is word level token
v3.5.1,Pretend it ends with a full stop so last span is a sentence
v3.5.1,"Tokens that are full stops, where the previous token is not"
v3.5.1,Make sure we have enough to mask
v3.5.1,Trim to masking budget
v3.5.1,Handle 0-length mask (inserts) separately
v3.5.1,assert is_word_start[-1] == 0
v3.5.1,assert tokens_length - 1 not in indices
v3.5.1,"keep index, but replace it with [MASK]"
v3.5.1,"acts as a long length, so spans don't go over the end of doc"
v3.5.1,next position from each word_start
v3.5.1,delete token: 1 mask/remove per span
v3.5.1,"keep index, but replace it with [MASK]: 1 mask per token"
v3.5.1,A bit faster when all lengths are 1
v3.5.1,to cover whole token
v3.5.1,delete token
v3.5.1,"keep index, but replace it with [MASK]"
v3.5.1,assert tokens_length - 1 not in indices
v3.5.1,prefix src/tgt for each dataset
v3.5.1,prefix as general option for inference
v3.5.1,suffix src/tgt for each dataset
v3.5.1,suffix as general option for inference
v3.5.1,!/usr/bin/env python3
v3.5.1,-*- coding: utf-8 -*-
v3.5.1,Most code taken from: https://github.com/alvations/sacremoses
v3.5.1,Which in turn is based on the Moses punctuation normalizer.
v3.5.1,https://github.com/moses-smt/mosesdecoder/blob/master/scripts/
v3.5.1,tokenizer/normalize-punctuation.perl
v3.5.1,don't fix period at end of sentence
v3.5.1,Regex substitutions from replace-unicode-punctuation.perl
v3.5.1,https://github.com/moses-smt/mosesdecoder/blob/master/
v3.5.1,scripts/tokenizer/replace-unicode-punctuation.perl
v3.5.1,Adds the penn substitutions after extra_whitespace regexes.
v3.5.1,"Optionally, replace unicode puncts BEFORE normalization."
v3.5.1,Actual normalization.
v3.5.1,"Optionally, replace unicode puncts BEFORE normalization."
v3.5.1,normalize dict src/tgt for each dataset
v3.5.1,One source feature expected but none given and no default provided
v3.5.1,Provided default does not match required features
v3.5.1,Data not properly annotated.
v3.5.1,In this case we do not use the default as it might be an error
v3.5.1,batch 0 will always predict EOS. The other batches will predict
v3.5.1,non-eos scores.
v3.5.1,"""best"" prediction is eos - that should be blocked"
v3.5.1,include at least one prediction OTHER than EOS
v3.5.1,that is greater than -1e20
v3.5.1,now batch 0 has ended and no others have
v3.5.1,initial step
v3.5.1,batch 0 dies on step 0
v3.5.1,include at least one prediction OTHER than EOS
v3.5.1,that is greater than -1e20
v3.5.1,step 2
v3.5.1,(old) batch 8 dies on step 1
v3.5.1,step 3
v3.5.1,everything dies
v3.5.1,initial step
v3.5.1,batch 0 dies on step 0
v3.5.1,include at least one prediction OTHER than EOS
v3.5.1,that is greater than -1e20
v3.5.1,step 2
v3.5.1,(old) batch 8 dies on step 1
v3.5.1,step 3
v3.5.1,everything dies
v3.5.1,initial step
v3.5.1,finish one beam
v3.5.1,include at least one prediction OTHER than EOS
v3.5.1,that is greater than -1e20
v3.5.1,step 2
v3.5.1,finish example in last batch
v3.5.1,(old) batch 8 dies on step 1
v3.5.1,step 3
v3.5.1,everything dies
v3.5.1,initial step
v3.5.1,batch 0 dies on step 0
v3.5.1,include at least one prediction OTHER than EOS
v3.5.1,that is greater than -1e20
v3.5.1,step 2
v3.5.1,(old) batch 8 dies on step 1
v3.5.1,step 3
v3.5.1,everything dies
v3.5.1,illegal_weights_mask = torch.ByteTensor([
v3.5.1,"[0, 0, 0, 0, 0, 0, 0],"
v3.5.1,"[0, 0, 0, 1, 1, 1, 1],"
v3.5.1,"[0, 0, 0, 0, 0, 1, 1],"
v3.5.1,"[0, 0, 1, 1, 1, 1, 1]])"
v3.5.1,TODO: fix for pytorch 0.3
v3.5.1,illegal_weights = alignments.masked_select(illegal_weights_mask)
v3.5.1,"self.assertEqual(0.0, illegal_weights.data.sum())"
v3.5.1,this could be considered an integration test because it touches
v3.5.1,the filesystem for the config file (and the models)
v3.5.1,no dummy prefix
v3.5.1,no dummy prefix
v3.5.1,make sure the scalars are in the event accumulator tags
v3.5.1,required arguments
v3.5.1,transforms that require vocab will not create if not provide vocab
v3.5.1,1. Init first transform in the pipe
v3.5.1,2. Init second transform in the pipe
v3.5.1,3. Sequential combine them into a transform pipe
v3.5.1,4. apply transform pipe for example
v3.5.1,"5. example after the pipe exceed the length limit, thus filtered"
v3.5.1,6. Transform statistics registed (here for filtertoolong)
v3.5.1,"7. after report, statistics become empty as a fresh start"
v3.5.1,filter_transform.warm_up()
v3.5.1,test BPE-dropout:
v3.5.1,1. disable bpe dropout for not training example
v3.5.1,2. enable bpe dropout for training example
v3.5.1,3. (NOTE) disable dropout won't take effect if already seen
v3.5.1,this is caused by the cache mechanism in bpe:
v3.5.1,return cached subword if the original token is seen when no dropout
v3.5.1,test SP regularization:
v3.5.1,1. enable regularization for training example
v3.5.1,2. disable regularization for not training example
v3.5.1,Test mask location
v3.5.1,Test mask location
v3.5.1,Test mask location
v3.5.1,Not apply token drop for not training example
v3.5.1,apply token drop for training example
v3.5.1,Not apply token mask for not training example
v3.5.1,apply token mask for training example
v3.5.1,require vocabs to warm_up
v3.5.1,Not apply token mask for not training example
v3.5.1,apply token mask for training example
v3.5.1,"Defalt: full_stop_token=[""."", ""?"", ""!""]"
v3.5.1,"Defalt: full_stop_token=[""."", ""?"", ""!""]"
v3.5.1,random_ratio of inserted tokens are chosen in vocab
v3.5.1,others are MASK_TOK
v3.5.1,"insert_ratio=0.0,"
v3.5.1,"random_ratio=0.0,"
v3.5.1,"Defalt: full_stop_token=[""."", ""?"", ""!""]"
v3.5.1,all token are considered as an individual word
v3.5.1,1. tokens are dropped when replace_length is 0
v3.5.1,"print(f""token delete: {masked} / {tokens}"")"
v3.5.1,2. tokens are replaced by MASK when replace_length is 1
v3.5.1,"print(f""token mask: {masked} / {tokens}"")"
v3.5.1,"insert_ratio=0.0,"
v3.5.1,"random_ratio=0.0,"
v3.5.1,"Defalt: full_stop_token=[""."", ""?"", ""!""]"
v3.5.1,start token of word are identified using subword marker
v3.5.1,"1. replace_length 0: ""words"" are dropped"
v3.5.1,"print(f""word delete: {masked} / {tokens}"")"
v3.5.1,"self.assertEqual(len(masked), n_words - n_masked)"
v3.5.1,"2. replace_length 1: ""words"" are replaced with a single MASK"
v3.5.1,"print(f""whole word single mask: {masked} / {tokens}"")"
v3.5.1,len(masked) depend on number of tokens in select word
v3.5.1,"3. replace_length -1: all tokens in ""words"" are replaced with MASK"
v3.5.1,"print(f""whole word multi mask: {masked} / {tokens}"")"
v3.5.1,number of mask_tok depend on number of tokens in selected word
v3.5.1,number of MASK_TOK can be greater than n_masked
v3.5.1,"insert_ratio=0.5,"
v3.5.1,"random_ratio=0.3,"
v3.5.1,"Defalt: full_stop_token=[""."", ""?"", ""!""]"
v3.5.1,start token of word are identified using subword marker
v3.5.1,n_words = sum(token_starts)
v3.5.1,n_masked = math.ceil(n_words * bart_noise.mask_ratio)
v3.5.1,"print(f""Text Span Infilling: {infillied} / {tokens}"")"
v3.5.1,"print(n_words, n_masked)"
v3.5.1,Build the translator (along with the model)
v3.5.1,Required arguments
v3.5.1,!/usr/bin/env python
v3.5.1,-*- coding: utf-8 -*-
v3.5.1,Inject some dummy training options that may needed when build fields
v3.5.1,Remove the generated *pt files.
v3.5.1,Remove the generated data samples
v3.5.1,all beams repeat (beam >= 1 repeat dummy scores)
v3.5.1,predict repeat_idx over and over again
v3.5.1,"before repeat, scores are either 0 or -inf"
v3.5.1,"on repeat, `repeat_idx` score is BLOCKED_SCORE"
v3.5.1,"(but it's still the best score, thus we have"
v3.5.1,"[BLOCKED_SCORE, -inf, -inf, -inf, -inf]"
v3.5.1,repetitions keeps maximizing score
v3.5.1,"index 0 has been blocked, so repeating=>+0.0 score"
v3.5.1,other indexes are -inf so repeating=>BLOCKED_SCORE
v3.5.1,which is higher
v3.5.1,beam 0 and beam >=2 will repeat (beam >= 2 repeat dummy scores)
v3.5.1,non-interesting beams are going to get dummy values
v3.5.1,"on initial round, only predicted scores for beam 0"
v3.5.1,matter. Make two predictions. Top one will be repeated
v3.5.1,"in beam zero, second one will live on in beam 1."
v3.5.1,predict the same thing in beam 0
v3.5.1,continue pushing around what beam 1 predicts
v3.5.1,"now beam 0 dies (along with the others), beam 1 -> beam 0"
v3.5.1,"now beam 0 dies (along with the others), beam 1 -> beam 0"
v3.5.1,beam 0 and beam >= 2 will repeat (beam 2 repeats excluded idx)
v3.5.1,non-interesting beams are going to get dummy values
v3.5.1,predict the same thing in beam 0
v3.5.1,continue pushing around what beam 1 predicts
v3.5.1,predict the allowed-repeat again in beam 2
v3.5.1,"now beam 0 dies, beam 1 -> beam 0, beam 2 -> beam 1"
v3.5.1,and the rest die
v3.5.1,"since all preds after i=0 are 0, we can check"
v3.5.1,that the beam is the correct idx by checking that
v3.5.1,the curr score is the initial score
v3.5.1,beam 0 will always predict EOS. The other beams will predict
v3.5.1,non-eos scores.
v3.5.1,non-interesting beams are going to get dummy values
v3.5.1,"""best"" prediction is eos - that should be blocked"
v3.5.1,include at least beam_sz predictions OTHER than EOS
v3.5.1,that are greater than -1e20
v3.5.1,predict eos in beam 0
v3.5.1,provide beam_sz other good predictions
v3.5.1,now the top beam has ended and no others have
v3.5.1,"not of interest, but want to make sure it keeps running"
v3.5.1,since only beam 0 terminates and n_best = 2
v3.5.1,"this is also a test that when block_ngram_repeat=0,"
v3.5.1,repeating is acceptable
v3.5.1,non-interesting beams are going to get dummy values
v3.5.1,"""best"" prediction is eos - that should be blocked"
v3.5.1,include at least beam_sz predictions OTHER than EOS
v3.5.1,that are greater than -1e20
v3.5.1,predict eos in beam 1
v3.5.1,provide beam_sz other good predictions in other beams
v3.5.1,beam 1 dies on min_length
v3.5.1,beam 0 dies on the step after beam 1 dies
v3.5.1,"inp_lens is tiled in initialize, reassign to make attn match"
v3.5.1,non-interesting beams are going to get dummy values
v3.5.1,"""best"" prediction is eos - that should be blocked"
v3.5.1,include at least beam_sz predictions OTHER than EOS
v3.5.1,that are greater than -1e20
v3.5.1,predict eos in beam 1
v3.5.1,provide beam_sz other good predictions in other beams
v3.5.1,no top beams are finished yet
v3.5.1,beam 1 dies on min_length
v3.5.1,no top beams are finished yet
v3.5.1,beam 0 dies on the step after beam 1 dies
v3.5.1,top beam is finished now so there are attentions
v3.5.1,two beams are finished in each batch
v3.5.1,second dim is cut down to the non-padded src length
v3.5.1,first dim is equal to the time of death
v3.5.1,(beam 0 died at current step - adjust for SOS)
v3.5.1,(beam 1 died at last step - adjust for SOS)
v3.5.1,behavior gets weird when beam is already done so just stop
v3.5.1,this is just test_beam.TestBeamAgainstReferenceCase repeated
v3.5.1,in each batch.
v3.5.1,"init_preds: [4, 3, 5, 6, 7] - no EOS's"
v3.5.1,no EOS's yet
v3.5.1,"[5, 3, 2, 6, 0], so beam 2 predicts EOS!"
v3.5.1,assumes beam 2 finished on last step
v3.5.1,ended beam 2 shouldn't continue
v3.5.1,"[2, 5, 3, 6, 0] repeat self.BATCH_SZ, so beam 0 predicts EOS!"
v3.5.1,"[-2.4879, -3.8910, -4.1010, -4.2010, -4.4010] repeat self.BATCH_SZ"
v3.5.1,another beam is finished in all batches
v3.5.1,new beam 0 finished
v3.5.1,new beam 0 is old beam 3
v3.5.1,assumes beam 0 finished on last step
v3.5.1,"[5, 2, 6, 1, 0] repeat self.BATCH_SZ, so beam 1 predicts EOS!"
v3.5.1,we finish 3 hyps per example in this step
v3.5.1,new beam 1 is old beam 3
v3.5.1,this could be considered an integration test because it tests
v3.5.1,interactions between the GNMT scorer and the beam
v3.5.1,"-data option is required, but not used in this test, so dummy."
v3.5.1,len x batch x nfeat
v3.5.1,Initialize vectors to compare size with
v3.5.1,Ensure correct sizes and types
v3.5.1,Make sure that output has the correct size and type
v3.5.1,"[('encoder_type', 'transformer'),"
v3.5.1,"('word_vec_size', 16), ('hidden_size', 16)],"
v3.5.1,""""""" Only do SRU test if requirment is safisfied. """""""
v3.5.1,SRU doesn't support input_feed.
v3.5.1,first check there's nothing unexpectedly not trainable
v3.5.1,ok: word embeddings shouldn't be trainable
v3.5.1,if word vecs are freezed
v3.5.1,ok: positional encodings shouldn't be trainable
v3.5.1,then check nothing unexpectedly trainable
v3.5.1,Decoder state
v3.5.1,Build the RNN.
v3.5.1,Set up the context gate.
v3.5.1,Set up the standard attention.
v3.5.1,The encoder hidden is  (layers*directions) x batch x dim.
v3.5.1,We need to convert it to layers x batch x (directions*dim).
v3.5.1,Init the input feed.
v3.5.1,Update the state with the result.
v3.5.1,Concatenates sequence of tensors along a new dimension.
v3.5.1,NOTE: v0.3 to 0.4: dec_outs / attns[*] may not be list
v3.5.1,(in particular in case of SRU) it was not raising error in 0.3
v3.5.1,since stack(Variable) was allowed.
v3.5.1,"In 0.4, SRU returns a tensor that shouldn't be stacke"
v3.5.1,Calculate the attention.
v3.5.1,Calculate the context gate.
v3.5.1,Additional args check.
v3.5.1,Input feed concatenates hidden state with
v3.5.1,input at every time step.
v3.5.1,TODO: context gate should be employed
v3.5.1,instead of second RNN transform.
v3.5.1,Update the coverage attention.
v3.5.1,"attns[""coverage""] is actually c^(t+1) of See et al(2017)"
v3.5.1,1-index shifted
v3.5.1,Decoder State
v3.5.1,CNNDecoder has its own attention mechanism.
v3.5.1,Set up a separate copy attention layer if needed.
v3.5.1,The output of CNNEncoder.
v3.5.1,The combination of output of CNNEncoder and source embeddings.
v3.5.1,Process the result and update the attentions.
v3.5.1,Update the state.
v3.5.1,TODO change the way attns is returned dict => list or tuple (onnx)
v3.5.1,Auto import python files in this directory
v3.5.1,src_len is a single tensor shared between all models.
v3.5.1,This assumption will not hold if Translator is modified
v3.5.1,to calculate src_len as something other than the length
v3.5.1,of the input.
v3.5.1,"return _, (B, Q_len, K_len)"
v3.5.1,"layer average attention across heads, get ``(B, Q, K)``"
v3.5.1,"Case 1: no full_context, no align heads -> layer avg baseline"
v3.5.1,"Case 2: no full_context, 1 align heads -> guided align"
v3.5.1,"Case 3: full_context, 1 align heads -> full cte guided align"
v3.5.1,"Add triangular future_mask and pad_mask, result mask in (B, T, T)."
v3.5.1,Patch for scaled dot product attention.
v3.5.1,"Only mask padding, result mask in (B, 1, T)."
v3.5.1,T: could be 1 in the case of stepwise decoding or tgt_len
v3.5.1,masking is necessary when sequence length is greater than one
v3.5.1,mask now are (batch x 1 x tlen x s or t len)
v3.5.1,1 = heads to be expanded in MHA
v3.5.1,"feed_forward applies residual, so we remove and apply residual with un-normed"
v3.5.1,Decoder State
v3.5.1,"previously, there was a GlobalAttention module here for copy"
v3.5.1,"attention. But it was never actually used -- the ""copy"" attention"
v3.5.1,just reuses the context attention.
v3.5.1,"attns[""align""] = torch.stack(attn_aligns, 0).mean(0)  # All avg"
v3.5.1,TODO change the way attns is returned dict => list or tuple (onnx)
v3.5.1,first value set to True triggered by the beginning of decoding
v3.5.1,layer_cache becomes active in the MultiHeadedAttention fwd
v3.5.1,T: could be 1 in the case of stepwise decoding or tgt_len
v3.5.1,Masking is necessary when sequence length is greater than one
v3.5.1,"The decoding has not started yet,"
v3.5.1,we compute the scores on the source tokens in one shot.
v3.5.1,mask now are (batch x 1 x tlen x tlen)
v3.5.1,1 = heads to be expanded in MHA
v3.5.1,"feed_forward applies residual, so we remove and apply residual with un-normed"
v3.5.1,decoding mode.
v3.5.1,Initialize KV and key_pad_mask cache.
v3.5.1,training mode.
v3.5.1,TODO change the way attns is returned dict => list or tuple (onnx)
v3.5.1,"buffer size in bytes, determine equiv. # of elements based on data type"
v3.5.1,copy tensors into buffer_t
v3.5.1,all-reduce and rescale
v3.5.1,copy all-reduced buffer back into tensors
v3.5.1,"print(filled, sz)"
v3.5.1,"tensor is bigger than buffer, all-reduce and rescale directly"
v3.5.1,"buffer is full, all-reduce and replace buffer with grad"
v3.5.1,add tensor to buffer
v3.5.1,"propagate exception to parent process, keeping original traceback"
v3.5.1,"propagate exception to parent process, keeping original traceback"
v3.5.1,TODO: Find a better way to check for sparse gradients.
v3.5.1,we use apex.amp
v3.5.1,In this case use the old FusedAdam with
v3.5.1,FP16_optimizer wrapper
v3.5.1,Load everything from the checkpoint.
v3.5.1,Build everything from scratch.
v3.5.1,"Reset optimizer, keep options."
v3.5.1,"Reset options, keep optimizer."
v3.5.1,State can be partially restored.
v3.5.1,should be: self._optimizer.zero_grad(set_to_none)
v3.5.1,but apex.amp is not up-to-date:
v3.5.1,https://github.com/NVIDIA/apex/blob/master/apex/amp/_process_optimizer.py#L367
v3.5.1,"unscaled optimizer's gradients (already done therefore skip),"
v3.5.1,skips optimizer.step() if gradients contain infs/NaNs.
v3.5.1,Updates the scale for next iteration.
v3.5.1,Code below is an implementation of https://arxiv.org/pdf/1804.04235.pdf
v3.5.1,inspired but modified from https://github.com/DeadAt0m/adafactor-pytorch
v3.5.1,backward compatibility
v3.5.1,assuming a list/generator of parameter means single group
v3.5.1,compute combined scale factor for this group
v3.5.1,norm is in fact norm*scale
v3.5.1,note: p.grad should not ever be set for correct operation of
v3.5.1,mixed precision optimizer that sometimes sends None gradients
v3.5.1,State initialization
v3.5.1,Exponential moving average of gradient values
v3.5.1,Exponential moving average of squared gradient values
v3.5.1,-*- coding: utf-8 -*-
v3.5.1,placing this here make it easier to call logger.info
v3.5.1,"from anywhere, just 'from onmt.utils.logging import logger'"
v3.5.1,"align_head contains value in [0, 1) presenting attn prob,"
v3.5.1,0 was resulted by the context attention src_pad_mask
v3.5.1,"So, the correspand position in ref_align should also be 0"
v3.5.1,"Therefore, clip align_head to > 1e-18 should be bias free."
v3.5.1,rescale with tau (temperature) and apply the log_softmax.
v3.5.1,ct2 expects src with lengths without padding
v3.5.1,again we use raw probs to rescale with tau and apply log_softmax
v3.5.1,lm_scores are in log space so log_target=True
v3.5.1,rescale with tau (temperature) and apply the log_softmax.
v3.5.1,ct2 expects src with lengths without padding
v3.5.1,again we use raw probs to rescale with tau and apply log_softmax
v3.5.1,lm_scores are in log space so log_target=True
v3.5.1,Create a mask with zeros at prompt positions and ones at answer postions.
v3.5.1,Apply the mask on the target side.
v3.5.1,Put the padding token index at the prompt positions.
v3.5.1,take into account here the tgt_shift_index (0 / 1 = LM/NMT)
v3.5.1,Correct target copy token instead of <unk>
v3.5.1,tgt[i] = align[i] + len(tgt_vocab)
v3.5.1,for i such that tgt[i] == 0 and align[i] != 0
v3.5.1,in the case criterion reduction is None then we need
v3.5.1,to sum the loss of each sentence in the batch
v3.5.1,Check Transforms
v3.5.1,Check path
v3.5.1,tgt is src for LM task
v3.5.1,Check weight
v3.5.1,Check features
v3.5.1,validation when train:
v3.5.1,Check embeddings stuff
v3.5.1,"Backward compatibility with ""fix_word_vecs_*"" opts"
v3.5.1,encoder and decoder should be same sizes
v3.5.1,"Load default opt values, then overwrite with the opts in"
v3.5.1,"the checkpoint. That way, if there are new options added,"
v3.5.1,the defaults are used.
v3.5.1,It comes from training
v3.5.1,TODO: needs to be added as inference opt
v3.5.1,Don't do anything
v3.5.1,Update best score of each criteria
v3.5.1,Reset tolerance
v3.5.1,Update current status
v3.5.1,Decrease tolerance
v3.5.1,Log
v3.5.1,Log
v3.5.1,Get a list of world_size lists with len(stat_list) Statistics objects
v3.5.1,"this param init is overridden by model_builder, useless then."
v3.5.1,SRU doesn't support PackedSequence.
v3.5.1,-*- coding: utf-8 -*-
v3.5.1,threshold on 1 to avoid div by 0
v3.5.1,treat alignment matrix one by one as each have different lengths
v3.5.1,No alignment if not exist valid tgt token
v3.5.1,get valid alignment (sub-matrix from full paded aligment matrix)
v3.5.1,Helper functions
v3.5.1,Keeps track of the original words/subwords
v3.5.1,('prior_tokenization' option)
v3.5.1,In case there is a final case_markup when new_spacer is on
v3.5.1,########## #
v3.5.1,Translator #
v3.5.1,########## #
v3.5.1,"Set ""default"" translation options on empty cfgfile"
v3.5.1,Build translator from options
v3.5.1,################### #
v3.5.1,Validation iterator #
v3.5.1,################### #
v3.5.1,Reinstantiate the validation iterator
v3.5.1,transforms_cls = get_transforms_cls(model_opt._all_transform)
v3.5.1,Retrieve raw references and sources
v3.5.1,########### #
v3.5.1,Predictions #
v3.5.1,########### #
v3.5.1,####### #
v3.5.1,Outputs #
v3.5.1,####### #
v3.5.1,Flatten predictions
v3.5.1,Save results
v3.5.1,-*- coding: utf-8 -*-
v3.5.1,this one is needed for Random Shuffler of batches
v3.5.1,in multi gpu it ensures datasets are read in the same order
v3.5.1,some cudnn methods can be random even after fixing the seed
v3.5.1,unless you tell it to be deterministic
v3.5.1,This one is needed for various tranfroms
v3.5.1,These ensure same initialization in multi gpu mode
v3.5.1,we need to check the model path + any tokenizer path
v3.5.1,patch to log stdout spawned processes of dataloader
v3.5.1,bucket_size = batch_size
v3.5.1,For TRAIN we shuffle batches within the bucket
v3.5.1,otherwise sequential
v3.5.1,for specific case of rnn_packed need to be sorted
v3.5.1,within the batch
v3.5.1,single thread - create batch directly on GPU if device is gpu
v3.5.1,multithread faster to create batch on CPU in each thread and then move it to gpu
v3.5.1,Move tensor_batch from cpu to device
v3.5.1,Check if all tokens have features or none at all
v3.5.1,Make features part of src like
v3.5.1,"{'src': {'src': ..., 'feats': [...., ....]}}"
v3.5.1,careful below it will return a bucket sorted by corpora
v3.5.1,but we sort by length later and shuffle batches
v3.5.1,at this point an example looks like:
v3.5.1,"{'src': {'src': ..., 'feats': [....]},"
v3.5.1,"'tgt': {'tgt': ...},"
v3.5.1,"'src_original': ['tok1', ...'tokn'],"
v3.5.1,"'tgt_original': ['tok1', ...'tokm'],"
v3.5.1,'cid': corpus id
v3.5.1,'cid_line_number' : cid line number
v3.5.1,"'align': ...,"
v3.5.1,}
v3.5.1,Need to add features in last dimensions
v3.5.1,Keep it consistent with dynamic data
v3.5.1,make a small vocab containing just the tokens in the source sequence
v3.5.1,Map source tokens to indices in the dynamic dict.
v3.5.1,-*- coding: utf-8 -*-
v3.5.1,this is hack: if the special separator ï½Ÿnewlineï½ is returned because of the
v3.5.1,"""docify"" transform.get_specials we don't add it if the corresponding newline code"
v3.5.1,is already included in the sentencepiece or BPE-with-gpt2-pretok.
v3.5.1,Reached end of file
v3.5.1,'src_original' and 'tgt_original' store the
v3.5.1,original line before tokenization. These
v3.5.1,fields are used later on in the feature
v3.5.1,transforms.
v3.5.1,empty example: skip
v3.5.1,ugly patch because in_feat and out_feat are reversed in WQLinear_GEMM
v3.5.1,bitsandbytes quantize weights when .cuda() is called
v3.5.1,for huge models we need to save Ram
v3.5.1,so we load the weights  module by module and transfer them to GPU for quantization
v3.5.1,bitsandbytes quantize weights when .cuda() is called
v3.5.1,for huge models we need to save Ram
v3.5.1,so we load the weights  module by module and transfer them to GPU for quantization
v3.5.1,"No encoder in LM, seq2seq count formatting kept"
v3.5.1,_check_save_model_path
v3.5.1,This preserves backward-compat for models using customed layernorm
v3.5.1,Force add_ffnbias to True if bias found in model w_1 keys
v3.5.1,fix v2 compatibility
v3.5.1,end of patch for backward compatibility
v3.5.1,!/usr/bin/env python
v3.5.1,!/usr/bin/env python
v3.5.1,!/usr/bin/env python
v3.5.1,-*- coding: utf-8 -*-
v3.5.1,!/usr/bin/env python
v3.5.1,BPE training
v3.5.1,SentencePiece training
v3.5.1,!/usr/bin/env python
v3.5.1,!/usr/bin/env python
v3.5.1,Set sharing strategy manually instead of default based on the OS.
v3.5.1,torch.multiprocessing.set_sharing_strategy('file_system')
v3.5.1,Create a thread to listen for errors in the child processes.
v3.5.1,Train with multiprocessing.
v3.5.1,magic indices
v3.5.1,result caching
v3.5.1,Here we set the decoder to start with self.start (BOS or EOS)
v3.5.1,not 100% necessary to define those
v3.5.1,self.is_finished = torch.zeros(
v3.5.1,"[self.batch_size, self.parallel_paths], dtype=torch.bool"
v3.5.1,)
v3.5.1,fix length constraint and remove eos from count
v3.5.1,add one to account for BOS. Don't account for EOS because hitting
v3.5.1,this implies it hasn't been found.
v3.5.1,we don't block nothing if the user doesn't want it
v3.5.1,we can't block nothing beam's too short
v3.5.1,we check paths one by one
v3.5.1,we don't forbid nothing if the user doesn't want it
v3.5.1,we can't forbid nothing if beam's too short
v3.5.1,Reordering forbidden_tokens following beam selection
v3.5.1,We rebuild a dict to ensure we get the value and not the pointer
v3.5.1,Grabing the newly selected tokens and associated ngram
v3.5.1,skip the blocking if any token in current_ngram is excluded
v3.5.1,"pickups: Tensor where specified index were set to 1, others 0"
v3.5.1,"dropdowns: opposite of pickups, 1 for those shouldn't pick"
v3.5.1,Minus dropdowns to log_probs making probabilities of
v3.5.1,unspecified index close to 0
v3.5.1,"prediction step have surpass length of given target_prefix,"
v3.5.1,no need to further change this attr
v3.5.1,keep indices until overflowing p
v3.5.1,Set all logits that are not in the top-p to -10000.
v3.5.1,This puts the probabilities close to 0.
v3.5.1,Set all logits that are not in the top-k to -10000.
v3.5.1,This puts the probabilities close to 0.
v3.5.1,"For temp=0.0, take the argmax to avoid divide-by-zero errors."
v3.5.1,keep_topk=1 is also equivalent to argmax.
v3.5.1,maybe fix some prediction at this step by modifying log_probs
v3.5.1,"shape: (sum(~ self.is_finished), 1)"
v3.5.1,in LM task src_len is associated with currently generated src
v3.5.1,and therefore needs to follow the generation
v3.5.1,!/usr/bin/env python
v3.5.1,for debugging
v3.5.1,TODO: maybe add dynamic part
v3.5.1,Statistics
v3.5.1,those two should be the same except feat dim
v3.5.1,"batch['src'][perm[j], :, :])"
v3.5.1,trans.src
v3.5.1,we rebuild a small batch made of the sub-segments
v3.5.1,in the long segment.
v3.5.1,new sub-batch ready to be translated
v3.5.1,we re-insert the sub-batch in the initial translations
v3.5.1,For seq2seq when we need to force doc to spit the same number of sents
v3.5.1,In the case of length_penalty = none we report the total logprobs
v3.5.1,divided by the number of sentence to get an approximation of the
v3.5.1,per sentence logprob. We also return the corresponding ppl
v3.5.1,"When a length_penalty is used eg: ""avg"" or ""wu"" since logprobs"
v3.5.1,are normalized per token we report the per line per token logprob
v3.5.1,"and the corresponding ""per word perplexity"""
v3.5.1,Turn any copied words into UNKs.
v3.5.1,"Decoder forward, takes [batch, tgt_len, nfeats] as input"
v3.5.1,"and [batch, src_len, hidden] as enc_out"
v3.5.1,"in case of inference tgt_len = 1, batch = beam times batch_size"
v3.5.1,"in case of Gold Scoring tgt_len = actual length, batch = 1 batch"
v3.5.1,Generator forward.
v3.5.1,"returns [(batch_size x beam_size) , vocab ] when 1 step"
v3.5.1,"or [batch_size, tgt_len, vocab ] when full sentence"
v3.5.1,"here we have scores [tgt_lenxbatch, vocab] or [beamxbatch, vocab]"
v3.5.1,at this point scores is batch first (dim=0)
v3.5.1,"returns [(batch_size x beam_size) , vocab ] when 1 step"
v3.5.1,"or [batch_size, tgt_len, vocab ] when full sentence"
v3.5.1,(0) add BOS and padding to tgt prediction
v3.5.1,(1) Encoder forward.
v3.5.1,(2) Repeat src objects `n_best` times.
v3.5.1,"We use batch_size x n_best, get ``(batch * n_best, src_len, nfeat)``"
v3.5.1,Quick fix. Transformers return None as enc_states.
v3.5.1,enc_states are only used later on to init decoder's state
v3.5.1,"but are never used in Transformer decoder, so we can skip"
v3.5.1,"(3) Init decoder with n_best src,"
v3.5.1,"reshape tgt to ``(len, batch * n_best, nfeat)``"
v3.5.1,it should be done in a better way
v3.5.1,here dec_in is batch first
v3.5.1,masked_select
v3.5.1,get aligned src id for each prediction's valid tgt tokens
v3.5.1,TODO: support these blacklisted features
v3.5.1,(0) Prep the components of the search.
v3.5.1,(1) Run the encoder on the src.
v3.5.1,(2) prep decode_strategy. Possibly repeat src objects.
v3.5.1,(3) Begin decoding step by step:
v3.5.1,Reorder states.
v3.5.1,TODO: support these blacklisted features
v3.5.1,(0) Prep the components of the search.
v3.5.1,(1) split src into src and target_prefix to avoid padding.
v3.5.1,(2) init decoder
v3.5.1,(3) prep decode_strategy. Possibly repeat src objects.
v3.5.1,(4) Begin decoding step by step:
v3.5.1,beg_time = time()
v3.5.1,Reorder states.
v3.5.1,select indexes in model state/cache
v3.5.1,if step == 0:
v3.5.1,"print(""step0 time: "", time() - beg_time)"
v3.5.1,beam parameters
v3.5.1,beam state
v3.5.1,"""global state"" of the old beam"
v3.5.1,buffers for the topk scores and 'backpointer'
v3.5.1,for testing
v3.5.1,maybe fix some prediction at this step by modifying log_probs
v3.5.1,Flatten probs into a list of possibilities.
v3.5.1,after this we get topk_ids between 0 and beam_size*vocab_size
v3.5.1,topk_ids // vocab_size => indice in beam
v3.5.1,topk_ids % vocab_size => true vocab indice
v3.5.1,using lists instead of tensors for topk_scores and is_finished make things faster
v3.5.1,Store finished hypotheses for this example in the batch.
v3.5.1,End condition is the top beam finished and we can return
v3.5.1,n_best hypotheses.
v3.5.1,early stop when top beam is finished
v3.5.1,Penalize beams that finished.
v3.5.1,this is required to pursue finished beams in non finished batches
v3.5.1,"If all sentences are translated, no need to go further."
v3.5.1,reset the selection for the next step
v3.5.1,Remove finished batches for the next step.
v3.5.1,using integer division to get an integer _B without casting
v3.5.1,force the output to be longer than self.min_length
v3.5.1,Multiply probs by the beam probability.
v3.5.1,"if the sequence ends now, then the penalty is the current"
v3.5.1,"length + 1, to include the EOS token"
v3.5.1,Avoid any direction that would repeat unwanted ngrams
v3.5.1,Pick up candidate token by curr_scores
v3.5.1,Recover log probs.
v3.5.1,Length penalty is just a scalar. It doesn't matter if it's applied
v3.5.1,before or after the topk.
v3.5.1,Resolve beam origin and map to batch index flat representation.
v3.5.1,Append last prediction to reordered alive sequence
v3.5.1,update global state (step == 1)
v3.5.1,update global state (step > 1)
v3.5.1,"shape: (batch_size x beam_size, 1)"
v3.5.1,in LM task src_len is associated with currently generated src
v3.5.1,and therefore needs to follow the generation
v3.5.1,Term will be subtracted from probability
v3.5.1,Probability will be divided by this
v3.5.1,these warnings indicate that either the alpha/beta
v3.5.1,"forces a penalty to be a no-op, or a penalty is a no-op but"
v3.5.1,the alpha/beta would suggest otherwise.
v3.5.1,using some coverage penalty
v3.5.1,!/usr/bin/env python
v3.5.1,semaphore doesn't have a timeout arg in Python 2.7
v3.5.1,perform a first request to initialize everything
v3.5.1,backwards compatibility for confs
v3.5.1,every segment becomes a dict for flexibility purposes
v3.5.1,NOTE: translator returns lists of `n_best` list
v3.5.1,build back results with empty texts
v3.5.1,load can be called multiple times: modify copy
v3.5.1,output contain alignment
v3.5.1,Below are all the different penalty terms implemented so far.
v3.5.1,Subtract coverage penalty from topk log probs.
v3.5.1,Divide topk log probs by length penalty.
v3.5.1,These comp lists are costy but less than for loops
v3.5.1,Chinese segmentation
v3.5.1,Chinese simplify -> Chinese traditional standard
v3.5.1,Chinese simplify -> Chinese traditional (HongKong)
v3.5.1,Chinese simplify -> Chinese traditional (Taiwan)
v3.5.1,Chinese traditional -> Chinese simplify (v1)
v3.5.1,Chinese traditional -> Chinese simplify (v2)
v3.5.1,Auto import python files in this directory
v3.5.1,contractions
v3.5.1,number separators
v3.5.1,punctuation
v3.5.1,double brackets
v3.5.1,miscellaneous
v3.5.1,Clean and Concat the dataset
v3.5.1,"joiner = tokenizer._tokenize(""\n"")"
v3.5.1,tokens += tokenizer._tokenize([x])
v3.5.1,Tokenize the dataset.
v3.5.1,Build the translator (along with the model.
v3.5.1,Score the dataset.
v3.5.1,zero out the context tokens
v3.5.1,"def custom_stopping_criteria(input_ids, score, **kwargs):"
v3.5.1,"stop_ids = [29871, 13, 13] # \n\n"
v3.5.1,return input_ids[-len(stop_ids)]
v3.5.1,Build the translator (along with the model)
v3.5.1,get prompt and make sure it fits
v3.5.1,"def custom_stopping_criteria(input_ids, score, **kwargs):"
v3.5.1,"stop_ids = [29871, 13, 13] # \n\n"
v3.5.1,return input_ids[-len(stop_ids)]
v3.5.1,Build the translator (along with the model)
v3.5.1,get prompt and make sure it fits
v3.5.0,!/usr/bin/env python
v3.5.0,!/usr/bin/env python
v3.5.0,!/usr/bin/env python
v3.5.0,!/usr/bin/env python
v3.5.0,!/usr/bin/env python
v3.5.0,!/usr/bin/env python3
v3.5.0,-*- coding: utf-8 -*-
v3.5.0,
v3.5.0,"OpenNMT-py documentation build configuration file, created by"
v3.5.0,sphinx-quickstart on Sun Dec 17 12:07:14 2017.
v3.5.0,
v3.5.0,This file is execfile()d with the current directory set to its
v3.5.0,containing dir.
v3.5.0,
v3.5.0,Note that not all possible configuration values are present in this
v3.5.0,autogenerated file.
v3.5.0,
v3.5.0,All configuration values have a default; values that are commented out
v3.5.0,serve to show the default.
v3.5.0,"If extensions (or modules to document with autodoc) are in another directory,"
v3.5.0,add these directories to sys.path here. If the directory is relative to the
v3.5.0,"documentation root, use os.path.abspath to make it absolute, like shown here."
v3.5.0,
v3.5.0,import os
v3.5.0,import sys
v3.5.0,"sys.path.insert(0, os.path.abspath('.'))"
v3.5.0,-- General configuration ------------------------------------------------
v3.5.0,"If your documentation needs a minimal Sphinx version, state it here."
v3.5.0,
v3.5.0,needs_sphinx = '6.0'
v3.5.0,"Add any Sphinx extension module names here, as strings. They can be"
v3.5.0,extensions coming with Sphinx (named 'sphinx.ext.*') or your custom
v3.5.0,ones.
v3.5.0,Show base classes
v3.5.0,"Use ""variables"" section for Attributes instead of weird block things"
v3.5.0,mimicking the function style.
v3.5.0,"Add any paths that contain templates here, relative to this directory."
v3.5.0,The suffix(es) of source filenames.
v3.5.0,You can specify multiple suffix as a list of string:
v3.5.0,
v3.5.0,"source_suffix = ['.rst', '.md']"
v3.5.0,The master toctree document.
v3.5.0,General information about the project.
v3.5.0,"The version info for the project you're documenting, acts as replacement for"
v3.5.0,"|version| and |release|, also used in various other places throughout the"
v3.5.0,built documents.
v3.5.0,
v3.5.0,The short X.Y version.
v3.5.0,"The full version, including alpha/beta/rc tags."
v3.5.0,The language for content autogenerated by Sphinx. Refer to documentation
v3.5.0,for a list of supported languages.
v3.5.0,
v3.5.0,This is also used if you do content translation via gettext catalogs.
v3.5.0,"Usually you set ""language"" from the command line for these cases."
v3.5.0,"List of patterns, relative to source directory, that match files and"
v3.5.0,directories to ignore when looking for source files.
v3.5.0,This patterns also effect to html_static_path and html_extra_path
v3.5.0,The name of the Pygments (syntax highlighting) style to use.
v3.5.0,"If true, `todo` and `todoList` produce output, else they produce nothing."
v3.5.0,-- Options for HTML output ----------------------------------------------
v3.5.0,The theme to use for HTML and HTML Help pages.  See the documentation for
v3.5.0,a list of builtin themes.
v3.5.0,
v3.5.0,html_theme = 'sphinx_materialdesign_theme'
v3.5.0,html_theme_path = [sphinx_materialdesign_theme.get_path()]
v3.5.0,Theme options are theme-specific and customize the look and feel of a theme
v3.5.0,"further.  For a list of options available for each theme, see the"
v3.5.0,documentation.
v3.5.0,
v3.5.0,html_theme_options = {}
v3.5.0,"Add any paths that contain custom static files (such as style sheets) here,"
v3.5.0,"relative to this directory. They are copied after the builtin static files,"
v3.5.0,"so a file named ""default.css"" will overwrite the builtin ""default.css""."
v3.5.0,"Custom sidebar templates, must be a dictionary that maps document names"
v3.5.0,to template names.
v3.5.0,
v3.5.0,This is required for the alabaster theme
v3.5.0,refs: http://alabaster.readthedocs.io/en/latest/installation.html#sidebars
v3.5.0,-- Options for HTMLHelp output ------------------------------------------
v3.5.0,Output file base name for HTML help builder.
v3.5.0,-- Options for LaTeX output ---------------------------------------------
v3.5.0,The paper size ('letterpaper' or 'a4paper').
v3.5.0,
v3.5.0,"'papersize': 'letterpaper',"
v3.5.0,"The font size ('10pt', '11pt' or '12pt')."
v3.5.0,
v3.5.0,"'pointsize': '10pt',"
v3.5.0,Additional stuff for the LaTeX preamble.
v3.5.0,
v3.5.0,"'preamble': '',"
v3.5.0,Latex figure (float) alignment
v3.5.0,
v3.5.0,"'figure_align': 'htbp',"
v3.5.0,Grouping the document tree into LaTeX files. List of tuples
v3.5.0,"(source start file, target name, title,"
v3.5.0,"author, documentclass [howto, manual, or own class])."
v3.5.0,-- Options for manual page output ---------------------------------------
v3.5.0,One entry per manual page. List of tuples
v3.5.0,"(source start file, name, description, authors, manual section)."
v3.5.0,-- Options for Texinfo output -------------------------------------------
v3.5.0,Grouping the document tree into Texinfo files. List of tuples
v3.5.0,"(source start file, target name, title, author,"
v3.5.0,"dir menu entry, description, category)"
v3.5.0,Build the translator (along with the model)
v3.5.0,Put messages sizes in antichronological order
v3.5.0,Caluculate antichronological history sizes
v3.5.0,Prune the history from the beginning
v3.5.0,Put back indices in chronological order.
v3.5.0,Build the translator (along with the model)
v3.5.0,We need to build the Llama tokenizer to count tokens and prune the history.
v3.5.0,The hypotheses are lists of one element but we still need to take the first one.
v3.5.0,#####
v3.5.0,UI #
v3.5.0,#####
v3.5.0,What are the 3 best french cities ?
v3.5.0,Which one is better if I like outdoor activities ?
v3.5.0,Which one is better if I like cultural outings?
v3.5.0,What are the best neighborhoods in these 5 cities?
v3.5.0,!/usr/bin/env python3
v3.5.0,Usage: python3 filter_train.py in.src in.trg out.src out.trg max-tokens
v3.5.0,flake8: noqa
v3.5.0,-*- coding: utf-8 -*-
v3.5.0,Generated by the protocol buffer compiler.  DO NOT EDIT!
v3.5.0,source: sentencepiece_model.proto
v3.5.0,@@protoc_insertion_point(imports)
v3.5.0,@@protoc_insertion_point(module_scope)
v3.5.0,!/usr/bin/env python
v3.5.0,-*- coding: utf-8 -*-
v3.5.0,is this reachable?
v3.5.0,Read in embeddings
v3.5.0,Write to file
v3.5.0,!/usr/bin/env python
v3.5.0,if shard == 0:
v3.5.0,"vocab_size = onmt_safetensor[""generator.weight""].size(0)"
v3.5.0,"vocab[11] = ""</s>""  # Falcon only"
v3.5.0,converts a SentencePiece vocabulary to the format expected by dynamic data
v3.5.0,"(essentially converts float expected counts to ""fixed precision"" int pseudo"
v3.5.0,counts)
v3.5.0,from onmt.utils.misc import use_gpu
v3.5.0,"Add in default model arguments, possibly added since training."
v3.5.0,this patch is no longer needed included in converter
v3.5.0,"if hasattr(model_opt, 'rnn_size'):"
v3.5.0,model_opt.hidden_size = model_opt.rnn_size
v3.5.0,build_base_model expects updated and validated opts
v3.5.0,-*- encoding: utf-8 -*-
v3.5.0,!/usr/bin/env python
v3.5.0,Falcon stores QKV in one single tensor but it is not simply piled up Q+K+V
v3.5.0,it is heads interleaved to we need to slice first
v3.5.0,also it uses the HF rotary so we need to permute Q and K interleave
v3.5.0,!/usr/bin/env python
v3.5.0,-*- coding: utf-8 -*-
v3.5.0,Author: Rico Sennrich
v3.5.0,flake8: noqa
v3.5.0,This file is retrieved from https://github.com/rsennrich/subword-nmt
v3.5.0,hack for python2/3 compatibility
v3.5.0,check version information
v3.5.0,some hacking to deal with duplicates (only consider first instance)
v3.5.0,don't print end-of-word symbols
v3.5.0,sys.stderr.write('cannot split {0} further.\n'.format(segment))
v3.5.0,sys.stderr.write('OOV: {0}\n'.format(segment))
v3.5.0,sys.stderr.write('OOV: {0}\n'.format(segment))
v3.5.0,python 2/3 compatibility
v3.5.0,read/write files as UTF-8
v3.5.0,!/usr/bin/env python3
v3.5.0,coding: utf-8
v3.5.0,"In order to use this tool, please install comet first"
v3.5.0,https://github.com/Unbabel/COMET
v3.5.0,Let's say you have a source file with N sentences in SL - eg: source.sl
v3.5.0,and the corresponding references (N sentences) reference.tl
v3.5.0,Translate your file in TL with the -n_best nbest options nbest being
v3.5.0,then number of hypotheses and output the target to -output target.nbest.tl
v3.5.0,Then you need to duplicate source and reference sentences nbest times
v3.5.0,for this script.
v3.5.0,for instance using awk '{for(i=1; i<=n; i++) print}' n=5 reference.tl \
v3.5.0,> reference.5.tl
v3.5.0,same for source.
v3.5.0,This script can be run (for instance with nbest = 5) as follows:
v3.5.0,python oracle_bleu.py --nbest-src source.5.sl --nbest-hyp target.5.tl \
v3.5.0,--nbest-ref reference.5.tl --nbest-order 5 --output target.maxbleu.tl
v3.5.0,It will search in all hyp the best comet score
v3.5.0,when choosing a reference-less model no nbest-ref is required
v3.5.0,for nbest in nbests:
v3.5.0,!/usr/bin/env python
v3.5.0,!/usr/bin/env python3
v3.5.0,coding: utf-8
v3.5.0,Let's say you have a source file with N sentences in SL - eg: source.sl
v3.5.0,Translate your file in TL with the -n_best nbest options nbest being
v3.5.0,then number of hypotheses and output the target to -output target.nbest.tl
v3.5.0,This script can be run (for instance with nbest = 5) as follows:
v3.5.0,python mbr_bleu.py --nbest-hyp target.5.tl \
v3.5.0,--nbest-order 5 --output target.mbr.tl
v3.5.0,It will compare all hyp with eachother and output the max bleu
v3.5.0,!/usr/bin/env python
v3.5.0,-*- coding: utf-8 -*-
v3.5.0,Author: Rico Sennrich
v3.5.0,flake8: noqa
v3.5.0,This file is retrieved from https://github.com/rsennrich/subword-nmt
v3.5.0,hack for python2/3 compatibility
v3.5.0,"find all instances of pair, and update frequency/indices around it"
v3.5.0,find first symbol
v3.5.0,"if first symbol is followed by second symbol, we've found an occurrence of pair (old_word[i:i+2])"
v3.5.0,"assuming a symbol sequence ""A B C"", if ""B C"" is merged, reduce the frequency of ""A B"""
v3.5.0,"assuming a symbol sequence ""A B C B"", if ""B C"" is merged, reduce the frequency of ""C B""."
v3.5.0,"however, skip this if the sequence is A B C B C, because the frequency of ""C B"" will be reduced by the previous code block"
v3.5.0,find new pair
v3.5.0,"assuming a symbol sequence ""A BC D"", if ""B C"" is merged, increase the frequency of ""A BC"""
v3.5.0,"assuming a symbol sequence ""A BC B"", if ""B C"" is merged, increase the frequency of ""BC B"""
v3.5.0,"however, if the sequence is A BC BC, skip this step because the count of ""BC BC"" will be incremented by the previous code block"
v3.5.0,data structure of pair frequencies
v3.5.0,index from pairs to words
v3.5.0,version 0.2 changes the handling of the end-of-word token ('</w>');
v3.5.0,version numbering allows bckward compatibility
v3.5.0,"threshold is inspired by Zipfian assumption, but should only affect speed"
v3.5.0,we probably missed the best pair because of pruning; go back to full statistics
v3.5.0,"threshold is inspired by Zipfian assumption, but should only affect speed"
v3.5.0,python 2/3 compatibility
v3.5.0,read/write files as UTF-8
v3.5.0,Now we can pipe the full file through the model using the Iterator
v3.5.0,reminder a batch includes .src .tgt .indices and it is sorted
v3.5.0,print(batch)
v3.5.0,Compute and retrieve the loss for EACH sentence
v3.5.0,Now we need to rearrange the batch of ppl
v3.5.0,in the original order with indices
v3.5.0,!/usr/bin/env python
v3.5.0,-*- coding: utf-8 -*-
v3.5.0,!/usr/bin/env python
v3.5.0,!/usr/bin/env python
v3.5.0,!/usr/bin/env python
v3.5.0,!/usr/bin/env python
v3.5.0,!/usr/bin/env python
v3.5.0,!/usr/bin/env python3
v3.5.0,coding: utf-8
v3.5.0,Let's say you have a source file with N sentences in SL - eg: source.sl
v3.5.0,and the corresponding references (N sentences) reference.tl
v3.5.0,Translate your file in TL with the -n_best nbest options nbest being
v3.5.0,then number of hypotheses and output the target to -output target.nbest.tl
v3.5.0,Then you need to duplicate reference sentences nbest times for this script.
v3.5.0,for instance using awk '{for(i=1; i<=n; i++) print}' n=5 reference.tl \
v3.5.0,> reference.5.tl
v3.5.0,This script can be run (for instance with nbest = 5) as follows:
v3.5.0,python oracle_bleu.py --nbest-hyp target.5.tl --nbest-ref reference.5.tl \
v3.5.0,--nbest-order 5 --output target.maxbleu.tl
v3.5.0,It will search in all hyp the best bleu wrt reference
v3.5.0,and output the max bleu
v3.5.0,!/usr/bin/env python
v3.5.0,!/usr/bin/env python
v3.5.0,with the two module = imp.load_source() below
v3.5.0,we ghost the old torchtext.data.field and depercated
v3.5.0,onmt.inputters.text_dataset
v3.5.0,however this require some functions / classes to be
v3.5.0,monkey patched for loading the old field/vocab objects.
v3.5.0,"module3 = imp.load_source(""Vocab"", ""tools/convertv2_v3.py"")"
v3.5.0,"voc = dict(sorted(fields.vocab.__dict__['freqs'].items(),"
v3.5.0,"key=lambda x: (-x[1], x[0]))).keys()"
v3.5.0,"voc = dict(sorted(fields.vocab.__dict__['freqs'].items(),"
v3.5.0,"key=lambda x: (-x[1], x[0]))).keys()"
v3.5.0,!/usr/bin/env python
v3.5.0,redpajama stores QKV in one single tensor but it is not simply piled up Q+K+V
v3.5.0,it is heads interleaved to we need to slice first
v3.5.0,also it uses the HF rotary so we need to permute Q and K interleave
v3.5.0,Avoid functionality on inference
v3.5.0,weights are in the .pt file
v3.5.0,weights are not in the .pt checkpoint but stored in the safetensors file
v3.5.0,Build embeddings.
v3.5.0,Build encoder.
v3.5.0,Build embeddings.
v3.5.0,Build decoder.
v3.5.0,Share the embedding matrix - preprocess with share_vocab required.
v3.5.0,src/tgt vocab should be the same if `-share_vocab` is specified.
v3.5.0,Update vocabulary embeddings with checkpoint embeddings
v3.5.0,Embedding layers
v3.5.0,Just for debugging purposes
v3.5.0,Remove old vocabulary associated embeddings
v3.5.0,for back compat when attention_dropout was not defined
v3.5.0,Build Model
v3.5.0,Build Generator.
v3.5.0,If new training initialize the model params
v3.5.0,If update_vocab init also but checkpoint will overwrite old weights
v3.5.0,ONLY for legacy fusedam with amp pytorch requires NOT to half the model
v3.5.0,Update model embeddings with those from the checkpoint
v3.5.0,after initialization
v3.5.0,after this checkpoint contains no embeddings
v3.5.0,when using LoRa or updating the vocab (no more embeddings in ckpt)
v3.5.0,=> strict=False when loading state_dict
v3.5.0,weights are in the .pt file
v3.5.0,weights are not in the .pt checkpoint but stored in the safetensors file
v3.5.0,!/usr/bin/env python
v3.5.0,if transform + options set in 'valid' we need to copy in main
v3.5.0,transform / options for scoring considered as inference
v3.5.0,"maybe prepare pretrained embeddings, if any"
v3.5.0,Load checkpoint if we resume from a previous training.
v3.5.0,ensure tensorboard output is written in the directory
v3.5.0,of previous checkpoints
v3.5.0,Override checkpoint's update_embeddings as it defaults to false
v3.5.0,Override checkpoint's freezing settings as it defaults to false
v3.5.0,NOTE: It's important that ``opt`` has been validated and updated
v3.5.0,at this point.
v3.5.0,Build model.
v3.5.0,Build optimizer.
v3.5.0,Build model saver
v3.5.0,Use Tensorboard for visualization during training
v3.5.0,Options only during inference
v3.5.0,"Truncation options, for text corpus"
v3.5.0,"as for False, this will be added in _add_train_general_opts"
v3.5.0,GPU
v3.5.0,Embedding Options
v3.5.0,Model Task Options
v3.5.0,Encoder-Decoder Options
v3.5.0,Freeze Encoder and/or Decoder
v3.5.0,The following options (bridge_extra_node to n_steps) are used
v3.5.0,for training with --encoder_type ggnn (Gated Graph Neural Network).
v3.5.0,Attention options
v3.5.0,Alignement options
v3.5.0,Generator and loss options.
v3.5.0,LoRa
v3.5.0,Init options
v3.5.0,Pretrained word vectors
v3.5.0,Freeze word vectors
v3.5.0,Optimization options
v3.5.0,learning rate
v3.5.0,Alpha and Beta values for Google Length + Coverage penalty
v3.5.0,"Described here: https://arxiv.org/pdf/1609.08144.pdf, Section 7"
v3.5.0,Length penalty options
v3.5.0,Coverage penalty options
v3.5.0,Decoding Length constraint
v3.5.0,Decoding content constraint
v3.5.0,Adding options related to source and target features
v3.5.0,Adding options relate to decoding strategy
v3.5.0,Adding option for logging
v3.5.0,Adding options related to Transforms
v3.5.0,Copyright 2016 The Chromium Authors. All rights reserved.
v3.5.0,Use of this source code is governed by a BSD-style license that can be
v3.5.0,found in the LICENSE file.
v3.5.0,"Get the key 'value' in the dict, or just use 'value'"
v3.5.0,Create a thread to listen for errors in the child processes.
v3.5.0,Build translator
v3.5.0,Build vocab
v3.5.0,Build transform pipe
v3.5.0,Basic attributes.
v3.5.0,Set model in training mode.
v3.5.0,Let's clean the GPUs before training loop
v3.5.0,UPDATE DROPOUT
v3.5.0,Run patience mechanism
v3.5.0,"If the patience has reached the limit, stop training"
v3.5.0,swap model params w/ moving average
v3.5.0,(and keep the original parameters)
v3.5.0,Set model in validating mode.
v3.5.0,raw_srcs = []
v3.5.0,raw_refs = []
v3.5.0,F-prop through the model.
v3.5.0,Compute loss.
v3.5.0,Compute validation metrics (at batch.dataset level)
v3.5.0,Compute stats
v3.5.0,Update statistics.
v3.5.0,Set model back to training mode.
v3.5.0,Truncated BPTT: reminder not compatible with accum > 1
v3.5.0,1. Create truncated target.
v3.5.0,2. F-prop all but generator.
v3.5.0,3. Compute loss.
v3.5.0,The loss of the prompt will be set to zero.
v3.5.0,"If truncated, don't backprop fully."
v3.5.0,"in case of multi step gradient accumulation,"
v3.5.0,update only after accum batches
v3.5.0,For Flake
v3.5.0,we avoid padding while mean pooling
v3.5.0,incoming and outgoing edge embedding
v3.5.0,Find vocab data for tree builting
v3.5.0,Propogation Model
v3.5.0,Initialize the bridge layer
v3.5.0,Token embedding
v3.5.0,Initialize graph using formatted input sequence
v3.5.0,Number of flagged nodes defines node count for this sample
v3.5.0,"(Nodes can have no flags on them, but must be in 'flags' list)."
v3.5.0,The total number of integers in the vocab should allow
v3.5.0,for all features and edges to be defined.
v3.5.0,Use first extra node as only source for decoder init
v3.5.0,Average all nodes to get bridge input
v3.5.0,"LSTM has hidden and cell state, other only one"
v3.5.0,Total number of states
v3.5.0,Build a linear layer for each
v3.5.0,Initialize the bridge layer
v3.5.0,src lengths data is wrapped inside a Tensor.
v3.5.0,"LSTM has hidden and cell state, other only one"
v3.5.0,Total number of states
v3.5.0,Build a linear layer for each
v3.5.0,Auto import python files in this directory
v3.5.0,batch x len x dim
v3.5.0,"feed_forward applies residual, so we remove and apply residual with un-normed"
v3.5.0,Padding mask is now (batch x 1 x slen x slen)
v3.5.0,1 to be expanded to number of heads in MHA
v3.5.0,Run the forward pass of every layer of the tranformer.
v3.5.0,Dimensions and padding for constructing the word embedding matrix
v3.5.0,Dimensions and padding for feature embedding matrices
v3.5.0,(these have no effect if feat_vocab_sizes is empty)
v3.5.0,The embedding matrix look-up tables. The first look-up table
v3.5.0,"is for words. Subsequent ones are for features, if any exist."
v3.5.0,The final output size of word + feature vectors. This can vary
v3.5.0,from the word vector size if and only if features are defined.
v3.5.0,This is the attribute you should access if you need to know
v3.5.0,how big your embeddings are going to be.
v3.5.0,The sequence of operations that converts the input sequence
v3.5.0,into a sequence of embeddings. At minimum this consists of
v3.5.0,looking up the embeddings for each word and feature in the
v3.5.0,input. Model parameters may require the sequence to contain
v3.5.0,additional operations as well.
v3.5.0,features must use word_vec_size
v3.5.0,features will use feat_vec_size
v3.5.0,Some utilitary functions for pretrained embeddings
v3.5.0,is this reachable?
v3.5.0,Write to file
v3.5.0,set the opt in place
v3.5.0,set the opt in place
v3.5.0,flake8: noqa
v3.5.0,For command-line option parsing
v3.5.0,"Check pass, set the args."
v3.5.0,"This SRU version implements its own cuda-level optimization,"
v3.5.0,so it requires that:
v3.5.0,1. `cupy` and `pynvrtc` python package installed.
v3.5.0,2. pytorch is built with cuda support.
v3.5.0,3. library path set: export LD_LIBRARY_PATH=<cuda lib path>.
v3.5.0,Check 1.
v3.5.0,Check 2.
v3.5.0,Check 3.
v3.5.0,This sets up device to use.
v3.5.0,-> directions x batch x dim
v3.5.0,For DEBUG
v3.5.0,"size = (length, batch, x.size(-1)) \"
v3.5.0,"if x.dim() == 3 else (batch, x.size(-1))"
v3.5.0,grad_x = x.new(*x.size()) if k_ == 3 else x.new(*size).zero_()
v3.5.0,Normal use
v3.5.0,"An entry check here, will catch on train side and translate side"
v3.5.0,if requirements are not satisfied.
v3.5.0,RNNDecoderState wraps hidden as a tuple.
v3.5.0,fh -> (layers*directions) x batch x dim
v3.5.0,This class is mainly used by decoder.py for RNNs but also
v3.5.0,by the CNN / transformer decoder when copy attention is used
v3.5.0,CNN has its own attention mechanism ConvMultiStepAttention
v3.5.0,Transformer has its own MultiHeadedAttention
v3.5.0,mlp wants it with bias
v3.5.0,"(batch, t_len, d) x (batch, d, s_len) --> (batch, t_len, s_len)"
v3.5.0,"(batch, t_len, s_len, d)"
v3.5.0,one step input
v3.5.0,"compute attention scores, as in Luong et al."
v3.5.0,Softmax or sparsemax to normalize attention weights
v3.5.0,each context vector c_t is the weighted average
v3.5.0,over all the source hidden states
v3.5.0,concatenate
v3.5.0,clamping necessary because of numerical errors: loss should be lower
v3.5.0,"bounded by zero, but negative values near zero are possible without"
v3.5.0,the clamp
v3.5.0,Help functions for Rotary Embeddings
v3.5.0,https://arxiv.org/pdf/2104.09864.pdf
v3.5.0,too convoluted to make maxseqlen a parameter.
v3.5.0,we suppose src_seq_len at training and max_length at inference
v3.5.0,are both < 2048 tokens.
v3.5.0,"rope is now matrix [maxseqlen, dim/2]"
v3.5.0,Help functions for max_relative positions
v3.5.0,https://arxiv.org/abs/1803.02155
v3.5.0,Shift values to be >= 0
v3.5.0,"now relative_position is in the range [0, inf)"
v3.5.0,half of the buckets are for exact increments in positions
v3.5.0,The other half of the buckets are for logarithmically bigger bins in positions
v3.5.0,up to max_distance
v3.5.0,Help functions to split model dim per head
v3.5.0,https://arxiv.org/pdf/1803.02155.pdf
v3.5.0,in the paper they suggest either two embeds
v3.5.0,relative_key / relative_value or only
v3.5.0,relative_key. We implemented the same embed
v3.5.0,for both.
v3.5.0,"1) Project key, value, and query."
v3.5.0,as a reminder at training layer_cache[0] remains False
v3.5.0,Retrieve keys and values from the KV cache (decoding mode only).
v3.5.0,Resize rotary embeddings.
v3.5.0,Resize rotary embeddings.
v3.5.0,We take a margin of 32 tokens as the kv_cache
v3.5.0,is incremented by 32 tokens every 32 tokens.
v3.5.0,Increase the cached key pad mask by concatenation.
v3.5.0,For decoding only.
v3.5.0,Retrieve keys and values from linear layers (training mode).
v3.5.0,Resize rotary embeddings.
v3.5.0,expand key on heads dimension when it's less than query heads (multi-query variant)
v3.5.0,expand value on heads dimension when it's less than query heads (multi-query variant)
v3.5.0,"2) When standard pos. enc. or rotary, use flash attention"
v3.5.0,Ultimately flashv2 will be part of pytorch https://github.com/pytorch/pytorch/pull/105602
v3.5.0,"In the meantime: if vanilla tranformer or Rotary embeddings (not rel_pos, not alibi)"
v3.5.0,then use flash2 if seq len > 256 otherwise use xtransformer from pt2 uptream
v3.5.0,Apply flash2 attention.
v3.5.0,Apply scaled dot product attention.
v3.5.0,batch x num_heads x query_len x key_len
v3.5.0,1 or key_len x key_len
v3.5.0,1 or key_len x key_len x dim_per_head
v3.5.0,not 100% necessary but expand to nb of heads
v3.5.0,now mask and scores have the same shape
v3.5.0,3) Apply attention dropout and compute context vectors.
v3.5.0,We use the same embeddings for key and value
v3.5.0,--------------------------------------------------------------------------
v3.5.0,copied and adapted https://github.com/microsoft/LoRA/
v3.5.0,Copyright (c) Microsoft Corporation. All rights reserved.
v3.5.0,Licensed under the MIT License (MIT).
v3.5.0,Support bnb quantization of nderlying layers
v3.5.0,--------------------------------------------------------------------------
v3.5.0,Optional dropout
v3.5.0,Mark the weight as unmerged
v3.5.0,LoRA implemented in a dense layer
v3.5.0,Actual trainable parameters
v3.5.0,Freezing the pre-trained weight matrix
v3.5.0,initialize A the same way as the default
v3.5.0,for nn.Linear and B to zero
v3.5.0,Make sure that the weights are not merged
v3.5.0,Merge the weights and mark it
v3.5.0,Actual trainable parameters
v3.5.0,Freezing the pre-trained weight matrix
v3.5.0,we do not super().reset_parameters() save lot of time and useless when no grad.
v3.5.0,initialize A the same way as the default
v3.5.0,for nn.Linear and B to zero
v3.5.0,Make sure that the weights are not merged
v3.5.0,Merge the weights and mark it
v3.5.0,cannot merge/unmerge quantized weigts with unquantized lora_X
v3.5.0,Check if QLoraLinear has a custom __init__ method
v3.5.0,Invoke the __init__ method of QLoraLinear
v3.5.0,LoRA implemented in a dense layer
v3.5.0,At the moment this class is only used by embeddings.Embeddings look-up tables
v3.5.0,for some reason list comprehension is slower in this scenario
v3.5.0,"for silu, see: https://arxiv.org/pdf/2002.05202.pdf"
v3.5.0,-*- coding: utf-8 -*-
v3.5.0,class AverageAttention(torch.jit.ScriptModule):
v3.5.0,@torch.jit.script
v3.5.0,Code taken from bitsandbytes but modified with arg device to accept skipt_init
v3.5.0,from torch.nn.utils => makes model building way faster.
v3.5.0,out_features * in_features
v3.5.0,norm is out_features * 1
v3.5.0,batch_size * out_features
v3.5.0,out_features
v3.5.0,out_features
v3.5.0,batch_size * out_features
v3.5.0,"out_channels, in_channels // groups, * kernel_size"
v3.5.0,out_features
v3.5.0,This is used nowhere in the code at the moment (Vincent Nguyen 05/18/2018)
v3.5.0,"in_channels, out_channels, *kernel_size"
v3.5.0,"in_channels, out_channels, *kernel_size"
v3.5.0,"self.out_channels, 1"
v3.5.0,out_features
v3.5.0,out_features
v3.5.0,store roots on diagonal
v3.5.0,Original probabilities.
v3.5.0,Probability of copying p(z=1) batch.
v3.5.0,Probability of not copying: p_{word}(w) * (1 - p(z))
v3.5.0,probabilities assigned by the model to the gold targets
v3.5.0,probability of tokens copied from source
v3.5.0,Set scores for unk to 0 and add eps
v3.5.0,find the indices in which you do not use the copy mechanism
v3.5.0,Drop padding.
v3.5.0,We exclude tokenization for contractions in
v3.5.0,order to avoid inconsistencies with pyonmtok's tokenization.
v3.5.0,"(e.g. ""I ca n't"" with spacy, ""I can ' t"" with pyonmttok)"
v3.5.0,Use Spacy's stopwords to get rid of junk entries
v3.5.0,Perform tokenization with spacy for consistency.
v3.5.0,We ensure that the target lemma is present in the lemmatized
v3.5.0,"target string, that the match is an exact match (there is"
v3.5.0,whitespace before or after the term)
v3.5.0,and we perform some bound checking.
v3.5.0,Map the lemmatized string match index to
v3.5.0,the lemmatized list index
v3.5.0,We need to know if the term is multiword
v3.5.0,Join multiword target lemmas with a unique separator so
v3.5.0,we can treat them as single word and not change the indices.
v3.5.0,Construct the final source from the lemmatized list
v3.5.0,that contains the terms. We compare the tokens in the
v3.5.0,term-augmented lemma list with the tokens in the original
v3.5.0,"lemma list. If the lemma is the same, then we replace with"
v3.5.0,the token from the original tokenized source list. If they
v3.5.0,"are not the same, it means the lemma has been augemented"
v3.5.0,"with a term, so we inject this in the final list."
v3.5.0,Restore the spaces in multi-word terms
v3.5.0,Skip half examples to improve performance. This means we set
v3.5.0,"a hard limit for the `term_corpus_ratio` to 0.5, which is actually"
v3.5.0,quite high. TODO: We can add this (skipping examples) as an option
v3.5.0,Filter out very short or very long sentences
v3.5.0,from the TM for better performance
v3.5.0,We split the `batch` and perform fuzzy matching
v3.5.0,in smaller chunks of 10.000 examples in order to
v3.5.0,reduce memory usage.
v3.5.0,Perfomance is not affected.
v3.5.0,Probably redundant but let's be safe
v3.5.0,in case some examples are already fuzzied
v3.5.0,(e.g. from another pipeline or workflow)
v3.5.0,We don't want exact matches
v3.5.0,Apply a basic filtering to leave out very short or very long
v3.5.0,sentences and speed up things a bit during fuzzy matching
v3.5.0,Do nothing
v3.5.0,We set the start number of tags to a random number from 1
v3.5.0,to 12 + the number of subsequent tags that
v3.5.0,will be added. We also apply weights to this choice so tags
v3.5.0,"are more probable to start from 1, then from 2, etc."
v3.5.0,This way we cover most scenarios met in real usage and
v3.5.0,the system will learn to handle a fairly large number of
v3.5.0,numbered tags (but not an excessively large number)
v3.5.0,Make sure we only search for exact matches (we don't want
v3.5.0,to match part of words) and perform some bound checking
v3.5.0,Create all possible tag forms. We inject a special
v3.5.0,unicode char (âˆ¥) as a placeholder for whitespace in order
v3.5.0,to keep the indices unaltered. This char is replaced with
v3.5.0,spaces before we return the augmented examples.
v3.5.0,Make a weighted choice between paired tags or single tags.
v3.5.0,"We usually encounter, and thus here we favor, paired tags"
v3.5.0,with a ratio 1/3.
v3.5.0,Check if the tags include the
v3.5.0,"mandatory ""#"" number placeholder"""
v3.5.0,We split the user-defined tags in the # placeholder
v3.5.0,in order to number them
v3.5.0,Skip half examples to speed up the transform. This sets
v3.5.0,"a hard limit of 0.5 to the `tags_corpus_ratio`, which is"
v3.5.0,excessive and should be avoided anyway.
v3.5.0,normalize dict src/tgt for each dataset
v3.5.0,"print(""src empty"")"
v3.5.0,"print(""too many same char in src"")"
v3.5.0,"print(""too many same word in src"")"
v3.5.0,"print(""avg token min"", len(src_str) / len(ex['src']))"
v3.5.0,"print(""avg token max"", len(src_str) / len(ex['src']))"
v3.5.0,"print(""text does not fully belong to wanted script"")"
v3.5.0,"print(""Some text belong to unwanted scripts"")"
v3.5.0,"print(""langid does not match"", _id(src_str))"
v3.5.0,"print(""src = tgt"")"
v3.5.0,"print(""tgt empty"")"
v3.5.0,"print(""src / tgt ratio "", len(src_str) / len(tgt_str))"
v3.5.0,"print(""too many same char in tgt"")"
v3.5.0,"print(""too many same word in tgt"")"
v3.5.0,"print(""avg token min"", len(tgt_str) / len(ex['tgt']))"
v3.5.0,"print(""avg token max"", len(tgt_str) / len(ex['tgt']))"
v3.5.0,"print(""text does not fully belong to wanted script"")"
v3.5.0,"print(""Some text belong to unwanted scripts"")"
v3.5.0,"print(""langid does not match"", _id(tgt_str))"
v3.5.0,"doc break we add it, restart new doc"
v3.5.0,case 1st ex is already longer
v3.5.0,adding cur ex is too long we add cur doc
v3.5.0,and reset doc to cur ex
v3.5.0,we start the new doc with cur ex
v3.5.0,we cumulate cur ex to cur doc
v3.5.0,Auto import python files in this directory
v3.5.0,1. sample number of tokens to corrupt
v3.5.0,2. sample positions to corrput
v3.5.0,3. sample corrupted values
v3.5.0,1. sample number of tokens to corrupt
v3.5.0,2. sample positions to corrput
v3.5.0,3. Drop token on chosen position
v3.5.0,1. sample number of tokens to corrupt
v3.5.0,2. sample positions to corrput
v3.5.0,3. mask word on chosen position
v3.5.0,"Sharing options among `TokenizerTransform`s, same name conflict in"
v3.5.0,this scope will be resolved by remove previous occurrence in parser
v3.5.0,subword regularization(or BPE dropout) options:
v3.5.0,subword vocabulary restriction options:
v3.5.0,This method embeds a custom logic to correctly handle certain placeholders
v3.5.0,in case the tokenizer doesn't preserve them.
v3.5.0,Locate the end-of-sentence placeholders.
v3.5.0,Tokenize each sentence separately.
v3.5.0,Locate the mask-before placeholders
v3.5.0,(to zero-out the prompt loss during LM finetuning).
v3.5.0,Tokenize each chunk separately and insert the padding token.
v3.5.0,between each sequence of tokens.
v3.5.0,Re-insert the eos token.
v3.5.0,derterministic subwording
v3.5.0,subword sampling when nbest_size > 1 or -1
v3.5.0,alpha should be 0.0 < alpha < 1.0
v3.5.0,Load vocabulary file if provided and set threshold
v3.5.0,Load Subword Model
v3.5.0,"ugly patch to make sure ""\n\n"" is split in two items"
v3.5.0,-1: keep everything (i.e. 1 mask per token)
v3.5.0,0: replace everything (i.e. no mask)
v3.5.0,1: 1 mask per span
v3.5.0,view each subword as word start / input is word level token
v3.5.0,Pretend it ends with a full stop so last span is a sentence
v3.5.0,"Tokens that are full stops, where the previous token is not"
v3.5.0,Make sure we have enough to mask
v3.5.0,Trim to masking budget
v3.5.0,Handle 0-length mask (inserts) separately
v3.5.0,assert is_word_start[-1] == 0
v3.5.0,assert tokens_length - 1 not in indices
v3.5.0,"keep index, but replace it with [MASK]"
v3.5.0,"acts as a long length, so spans don't go over the end of doc"
v3.5.0,next position from each word_start
v3.5.0,delete token: 1 mask/remove per span
v3.5.0,"keep index, but replace it with [MASK]: 1 mask per token"
v3.5.0,A bit faster when all lengths are 1
v3.5.0,to cover whole token
v3.5.0,delete token
v3.5.0,"keep index, but replace it with [MASK]"
v3.5.0,assert tokens_length - 1 not in indices
v3.5.0,prefix src/tgt for each dataset
v3.5.0,prefix as general option for inference
v3.5.0,suffix src/tgt for each dataset
v3.5.0,suffix as general option for inference
v3.5.0,!/usr/bin/env python3
v3.5.0,-*- coding: utf-8 -*-
v3.5.0,Most code taken from: https://github.com/alvations/sacremoses
v3.5.0,Which in turn is based on the Moses punctuation normalizer.
v3.5.0,https://github.com/moses-smt/mosesdecoder/blob/master/scripts/
v3.5.0,tokenizer/normalize-punctuation.perl
v3.5.0,don't fix period at end of sentence
v3.5.0,Regex substitutions from replace-unicode-punctuation.perl
v3.5.0,https://github.com/moses-smt/mosesdecoder/blob/master/
v3.5.0,scripts/tokenizer/replace-unicode-punctuation.perl
v3.5.0,Adds the penn substitutions after extra_whitespace regexes.
v3.5.0,"Optionally, replace unicode puncts BEFORE normalization."
v3.5.0,Actual normalization.
v3.5.0,"Optionally, replace unicode puncts BEFORE normalization."
v3.5.0,normalize dict src/tgt for each dataset
v3.5.0,One source feature expected but none given and no default provided
v3.5.0,Provided default does not match required features
v3.5.0,Data not properly annotated.
v3.5.0,In this case we do not use the default as it might be an error
v3.5.0,batch 0 will always predict EOS. The other batches will predict
v3.5.0,non-eos scores.
v3.5.0,"""best"" prediction is eos - that should be blocked"
v3.5.0,include at least one prediction OTHER than EOS
v3.5.0,that is greater than -1e20
v3.5.0,now batch 0 has ended and no others have
v3.5.0,initial step
v3.5.0,batch 0 dies on step 0
v3.5.0,include at least one prediction OTHER than EOS
v3.5.0,that is greater than -1e20
v3.5.0,step 2
v3.5.0,(old) batch 8 dies on step 1
v3.5.0,step 3
v3.5.0,everything dies
v3.5.0,initial step
v3.5.0,batch 0 dies on step 0
v3.5.0,include at least one prediction OTHER than EOS
v3.5.0,that is greater than -1e20
v3.5.0,step 2
v3.5.0,(old) batch 8 dies on step 1
v3.5.0,step 3
v3.5.0,everything dies
v3.5.0,initial step
v3.5.0,finish one beam
v3.5.0,include at least one prediction OTHER than EOS
v3.5.0,that is greater than -1e20
v3.5.0,step 2
v3.5.0,finish example in last batch
v3.5.0,(old) batch 8 dies on step 1
v3.5.0,step 3
v3.5.0,everything dies
v3.5.0,initial step
v3.5.0,batch 0 dies on step 0
v3.5.0,include at least one prediction OTHER than EOS
v3.5.0,that is greater than -1e20
v3.5.0,step 2
v3.5.0,(old) batch 8 dies on step 1
v3.5.0,step 3
v3.5.0,everything dies
v3.5.0,illegal_weights_mask = torch.ByteTensor([
v3.5.0,"[0, 0, 0, 0, 0, 0, 0],"
v3.5.0,"[0, 0, 0, 1, 1, 1, 1],"
v3.5.0,"[0, 0, 0, 0, 0, 1, 1],"
v3.5.0,"[0, 0, 1, 1, 1, 1, 1]])"
v3.5.0,TODO: fix for pytorch 0.3
v3.5.0,illegal_weights = alignments.masked_select(illegal_weights_mask)
v3.5.0,"self.assertEqual(0.0, illegal_weights.data.sum())"
v3.5.0,this could be considered an integration test because it touches
v3.5.0,the filesystem for the config file (and the models)
v3.5.0,no dummy prefix
v3.5.0,no dummy prefix
v3.5.0,make sure the scalars are in the event accumulator tags
v3.5.0,required arguments
v3.5.0,transforms that require vocab will not create if not provide vocab
v3.5.0,1. Init first transform in the pipe
v3.5.0,2. Init second transform in the pipe
v3.5.0,3. Sequential combine them into a transform pipe
v3.5.0,4. apply transform pipe for example
v3.5.0,"5. example after the pipe exceed the length limit, thus filtered"
v3.5.0,6. Transform statistics registed (here for filtertoolong)
v3.5.0,"7. after report, statistics become empty as a fresh start"
v3.5.0,filter_transform.warm_up()
v3.5.0,test BPE-dropout:
v3.5.0,1. disable bpe dropout for not training example
v3.5.0,2. enable bpe dropout for training example
v3.5.0,3. (NOTE) disable dropout won't take effect if already seen
v3.5.0,this is caused by the cache mechanism in bpe:
v3.5.0,return cached subword if the original token is seen when no dropout
v3.5.0,test SP regularization:
v3.5.0,1. enable regularization for training example
v3.5.0,2. disable regularization for not training example
v3.5.0,Test mask location
v3.5.0,Test mask location
v3.5.0,Test mask location
v3.5.0,Not apply token drop for not training example
v3.5.0,apply token drop for training example
v3.5.0,Not apply token mask for not training example
v3.5.0,apply token mask for training example
v3.5.0,require vocabs to warm_up
v3.5.0,Not apply token mask for not training example
v3.5.0,apply token mask for training example
v3.5.0,"Defalt: full_stop_token=[""."", ""?"", ""!""]"
v3.5.0,"Defalt: full_stop_token=[""."", ""?"", ""!""]"
v3.5.0,random_ratio of inserted tokens are chosen in vocab
v3.5.0,others are MASK_TOK
v3.5.0,"insert_ratio=0.0,"
v3.5.0,"random_ratio=0.0,"
v3.5.0,"Defalt: full_stop_token=[""."", ""?"", ""!""]"
v3.5.0,all token are considered as an individual word
v3.5.0,1. tokens are dropped when replace_length is 0
v3.5.0,"print(f""token delete: {masked} / {tokens}"")"
v3.5.0,2. tokens are replaced by MASK when replace_length is 1
v3.5.0,"print(f""token mask: {masked} / {tokens}"")"
v3.5.0,"insert_ratio=0.0,"
v3.5.0,"random_ratio=0.0,"
v3.5.0,"Defalt: full_stop_token=[""."", ""?"", ""!""]"
v3.5.0,start token of word are identified using subword marker
v3.5.0,"1. replace_length 0: ""words"" are dropped"
v3.5.0,"print(f""word delete: {masked} / {tokens}"")"
v3.5.0,"self.assertEqual(len(masked), n_words - n_masked)"
v3.5.0,"2. replace_length 1: ""words"" are replaced with a single MASK"
v3.5.0,"print(f""whole word single mask: {masked} / {tokens}"")"
v3.5.0,len(masked) depend on number of tokens in select word
v3.5.0,"3. replace_length -1: all tokens in ""words"" are replaced with MASK"
v3.5.0,"print(f""whole word multi mask: {masked} / {tokens}"")"
v3.5.0,number of mask_tok depend on number of tokens in selected word
v3.5.0,number of MASK_TOK can be greater than n_masked
v3.5.0,"insert_ratio=0.5,"
v3.5.0,"random_ratio=0.3,"
v3.5.0,"Defalt: full_stop_token=[""."", ""?"", ""!""]"
v3.5.0,start token of word are identified using subword marker
v3.5.0,n_words = sum(token_starts)
v3.5.0,n_masked = math.ceil(n_words * bart_noise.mask_ratio)
v3.5.0,"print(f""Text Span Infilling: {infillied} / {tokens}"")"
v3.5.0,"print(n_words, n_masked)"
v3.5.0,Build the translator (along with the model)
v3.5.0,Required arguments
v3.5.0,!/usr/bin/env python
v3.5.0,-*- coding: utf-8 -*-
v3.5.0,Inject some dummy training options that may needed when build fields
v3.5.0,Remove the generated *pt files.
v3.5.0,Remove the generated data samples
v3.5.0,all beams repeat (beam >= 1 repeat dummy scores)
v3.5.0,predict repeat_idx over and over again
v3.5.0,"before repeat, scores are either 0 or -inf"
v3.5.0,"on repeat, `repeat_idx` score is BLOCKED_SCORE"
v3.5.0,"(but it's still the best score, thus we have"
v3.5.0,"[BLOCKED_SCORE, -inf, -inf, -inf, -inf]"
v3.5.0,repetitions keeps maximizing score
v3.5.0,"index 0 has been blocked, so repeating=>+0.0 score"
v3.5.0,other indexes are -inf so repeating=>BLOCKED_SCORE
v3.5.0,which is higher
v3.5.0,beam 0 and beam >=2 will repeat (beam >= 2 repeat dummy scores)
v3.5.0,non-interesting beams are going to get dummy values
v3.5.0,"on initial round, only predicted scores for beam 0"
v3.5.0,matter. Make two predictions. Top one will be repeated
v3.5.0,"in beam zero, second one will live on in beam 1."
v3.5.0,predict the same thing in beam 0
v3.5.0,continue pushing around what beam 1 predicts
v3.5.0,"now beam 0 dies (along with the others), beam 1 -> beam 0"
v3.5.0,"now beam 0 dies (along with the others), beam 1 -> beam 0"
v3.5.0,beam 0 and beam >= 2 will repeat (beam 2 repeats excluded idx)
v3.5.0,non-interesting beams are going to get dummy values
v3.5.0,predict the same thing in beam 0
v3.5.0,continue pushing around what beam 1 predicts
v3.5.0,predict the allowed-repeat again in beam 2
v3.5.0,"now beam 0 dies, beam 1 -> beam 0, beam 2 -> beam 1"
v3.5.0,and the rest die
v3.5.0,"since all preds after i=0 are 0, we can check"
v3.5.0,that the beam is the correct idx by checking that
v3.5.0,the curr score is the initial score
v3.5.0,beam 0 will always predict EOS. The other beams will predict
v3.5.0,non-eos scores.
v3.5.0,non-interesting beams are going to get dummy values
v3.5.0,"""best"" prediction is eos - that should be blocked"
v3.5.0,include at least beam_sz predictions OTHER than EOS
v3.5.0,that are greater than -1e20
v3.5.0,predict eos in beam 0
v3.5.0,provide beam_sz other good predictions
v3.5.0,now the top beam has ended and no others have
v3.5.0,"not of interest, but want to make sure it keeps running"
v3.5.0,since only beam 0 terminates and n_best = 2
v3.5.0,"this is also a test that when block_ngram_repeat=0,"
v3.5.0,repeating is acceptable
v3.5.0,non-interesting beams are going to get dummy values
v3.5.0,"""best"" prediction is eos - that should be blocked"
v3.5.0,include at least beam_sz predictions OTHER than EOS
v3.5.0,that are greater than -1e20
v3.5.0,predict eos in beam 1
v3.5.0,provide beam_sz other good predictions in other beams
v3.5.0,beam 1 dies on min_length
v3.5.0,beam 0 dies on the step after beam 1 dies
v3.5.0,"inp_lens is tiled in initialize, reassign to make attn match"
v3.5.0,non-interesting beams are going to get dummy values
v3.5.0,"""best"" prediction is eos - that should be blocked"
v3.5.0,include at least beam_sz predictions OTHER than EOS
v3.5.0,that are greater than -1e20
v3.5.0,predict eos in beam 1
v3.5.0,provide beam_sz other good predictions in other beams
v3.5.0,no top beams are finished yet
v3.5.0,beam 1 dies on min_length
v3.5.0,no top beams are finished yet
v3.5.0,beam 0 dies on the step after beam 1 dies
v3.5.0,top beam is finished now so there are attentions
v3.5.0,two beams are finished in each batch
v3.5.0,second dim is cut down to the non-padded src length
v3.5.0,first dim is equal to the time of death
v3.5.0,(beam 0 died at current step - adjust for SOS)
v3.5.0,(beam 1 died at last step - adjust for SOS)
v3.5.0,behavior gets weird when beam is already done so just stop
v3.5.0,this is just test_beam.TestBeamAgainstReferenceCase repeated
v3.5.0,in each batch.
v3.5.0,"init_preds: [4, 3, 5, 6, 7] - no EOS's"
v3.5.0,no EOS's yet
v3.5.0,"[5, 3, 2, 6, 0], so beam 2 predicts EOS!"
v3.5.0,assumes beam 2 finished on last step
v3.5.0,ended beam 2 shouldn't continue
v3.5.0,"[2, 5, 3, 6, 0] repeat self.BATCH_SZ, so beam 0 predicts EOS!"
v3.5.0,"[-2.4879, -3.8910, -4.1010, -4.2010, -4.4010] repeat self.BATCH_SZ"
v3.5.0,another beam is finished in all batches
v3.5.0,new beam 0 finished
v3.5.0,new beam 0 is old beam 3
v3.5.0,assumes beam 0 finished on last step
v3.5.0,"[5, 2, 6, 1, 0] repeat self.BATCH_SZ, so beam 1 predicts EOS!"
v3.5.0,we finish 3 hyps per example in this step
v3.5.0,new beam 1 is old beam 3
v3.5.0,this could be considered an integration test because it tests
v3.5.0,interactions between the GNMT scorer and the beam
v3.5.0,"-data option is required, but not used in this test, so dummy."
v3.5.0,len x batch x nfeat
v3.5.0,Initialize vectors to compare size with
v3.5.0,Ensure correct sizes and types
v3.5.0,Make sure that output has the correct size and type
v3.5.0,"[('encoder_type', 'transformer'),"
v3.5.0,"('word_vec_size', 16), ('hidden_size', 16)],"
v3.5.0,""""""" Only do SRU test if requirment is safisfied. """""""
v3.5.0,SRU doesn't support input_feed.
v3.5.0,first check there's nothing unexpectedly not trainable
v3.5.0,ok: word embeddings shouldn't be trainable
v3.5.0,if word vecs are freezed
v3.5.0,ok: positional encodings shouldn't be trainable
v3.5.0,then check nothing unexpectedly trainable
v3.5.0,Decoder state
v3.5.0,Build the RNN.
v3.5.0,Set up the context gate.
v3.5.0,Set up the standard attention.
v3.5.0,The encoder hidden is  (layers*directions) x batch x dim.
v3.5.0,We need to convert it to layers x batch x (directions*dim).
v3.5.0,Init the input feed.
v3.5.0,Update the state with the result.
v3.5.0,Concatenates sequence of tensors along a new dimension.
v3.5.0,NOTE: v0.3 to 0.4: dec_outs / attns[*] may not be list
v3.5.0,(in particular in case of SRU) it was not raising error in 0.3
v3.5.0,since stack(Variable) was allowed.
v3.5.0,"In 0.4, SRU returns a tensor that shouldn't be stacke"
v3.5.0,Calculate the attention.
v3.5.0,Calculate the context gate.
v3.5.0,Additional args check.
v3.5.0,Input feed concatenates hidden state with
v3.5.0,input at every time step.
v3.5.0,TODO: context gate should be employed
v3.5.0,instead of second RNN transform.
v3.5.0,Update the coverage attention.
v3.5.0,"attns[""coverage""] is actually c^(t+1) of See et al(2017)"
v3.5.0,1-index shifted
v3.5.0,Decoder State
v3.5.0,CNNDecoder has its own attention mechanism.
v3.5.0,Set up a separate copy attention layer if needed.
v3.5.0,The output of CNNEncoder.
v3.5.0,The combination of output of CNNEncoder and source embeddings.
v3.5.0,Process the result and update the attentions.
v3.5.0,Update the state.
v3.5.0,TODO change the way attns is returned dict => list or tuple (onnx)
v3.5.0,Auto import python files in this directory
v3.5.0,src_len is a single tensor shared between all models.
v3.5.0,This assumption will not hold if Translator is modified
v3.5.0,to calculate src_len as something other than the length
v3.5.0,of the input.
v3.5.0,"return _, (B, Q_len, K_len)"
v3.5.0,"layer average attention across heads, get ``(B, Q, K)``"
v3.5.0,"Case 1: no full_context, no align heads -> layer avg baseline"
v3.5.0,"Case 2: no full_context, 1 align heads -> guided align"
v3.5.0,"Case 3: full_context, 1 align heads -> full cte guided align"
v3.5.0,"Add triangular future_mask and pad_mask, result mask in (B, T, T)."
v3.5.0,Patch for scaled dot product attention.
v3.5.0,"Only mask padding, result mask in (B, 1, T)."
v3.5.0,T: could be 1 in the case of stepwise decoding or tgt_len
v3.5.0,masking is necessary when sequence length is greater than one
v3.5.0,mask now are (batch x 1 x tlen x s or t len)
v3.5.0,1 = heads to be expanded in MHA
v3.5.0,"feed_forward applies residual, so we remove and apply residual with un-normed"
v3.5.0,Decoder State
v3.5.0,"previously, there was a GlobalAttention module here for copy"
v3.5.0,"attention. But it was never actually used -- the ""copy"" attention"
v3.5.0,just reuses the context attention.
v3.5.0,"attns[""align""] = torch.stack(attn_aligns, 0).mean(0)  # All avg"
v3.5.0,TODO change the way attns is returned dict => list or tuple (onnx)
v3.5.0,first value set to True triggered by the beginning of decoding
v3.5.0,layer_cache becomes active in the MultiHeadedAttention fwd
v3.5.0,T: could be 1 in the case of stepwise decoding or tgt_len
v3.5.0,Masking is necessary when sequence length is greater than one
v3.5.0,"The decoding has not started yet,"
v3.5.0,we compute the scores on the source tokens in one shot.
v3.5.0,mask now are (batch x 1 x tlen x tlen)
v3.5.0,1 = heads to be expanded in MHA
v3.5.0,"feed_forward applies residual, so we remove and apply residual with un-normed"
v3.5.0,decoding mode.
v3.5.0,Initialize KV and key_pad_mask cache.
v3.5.0,training mode.
v3.5.0,TODO change the way attns is returned dict => list or tuple (onnx)
v3.5.0,"buffer size in bytes, determine equiv. # of elements based on data type"
v3.5.0,copy tensors into buffer_t
v3.5.0,all-reduce and rescale
v3.5.0,copy all-reduced buffer back into tensors
v3.5.0,"print(filled, sz)"
v3.5.0,"tensor is bigger than buffer, all-reduce and rescale directly"
v3.5.0,"buffer is full, all-reduce and replace buffer with grad"
v3.5.0,add tensor to buffer
v3.5.0,"propagate exception to parent process, keeping original traceback"
v3.5.0,"propagate exception to parent process, keeping original traceback"
v3.5.0,TODO: Find a better way to check for sparse gradients.
v3.5.0,we use apex.amp
v3.5.0,In this case use the old FusedAdam with
v3.5.0,FP16_optimizer wrapper
v3.5.0,Load everything from the checkpoint.
v3.5.0,Build everything from scratch.
v3.5.0,"Reset optimizer, keep options."
v3.5.0,"Reset options, keep optimizer."
v3.5.0,State can be partially restored.
v3.5.0,should be: self._optimizer.zero_grad(set_to_none)
v3.5.0,but apex.amp is not up-to-date:
v3.5.0,https://github.com/NVIDIA/apex/blob/master/apex/amp/_process_optimizer.py#L367
v3.5.0,"unscaled optimizer's gradients (already done therefore skip),"
v3.5.0,skips optimizer.step() if gradients contain infs/NaNs.
v3.5.0,Updates the scale for next iteration.
v3.5.0,Code below is an implementation of https://arxiv.org/pdf/1804.04235.pdf
v3.5.0,inspired but modified from https://github.com/DeadAt0m/adafactor-pytorch
v3.5.0,backward compatibility
v3.5.0,assuming a list/generator of parameter means single group
v3.5.0,compute combined scale factor for this group
v3.5.0,norm is in fact norm*scale
v3.5.0,note: p.grad should not ever be set for correct operation of
v3.5.0,mixed precision optimizer that sometimes sends None gradients
v3.5.0,State initialization
v3.5.0,Exponential moving average of gradient values
v3.5.0,Exponential moving average of squared gradient values
v3.5.0,-*- coding: utf-8 -*-
v3.5.0,placing this here make it easier to call logger.info
v3.5.0,"from anywhere, just 'from onmt.utils.logging import logger'"
v3.5.0,"align_head contains value in [0, 1) presenting attn prob,"
v3.5.0,0 was resulted by the context attention src_pad_mask
v3.5.0,"So, the correspand position in ref_align should also be 0"
v3.5.0,"Therefore, clip align_head to > 1e-18 should be bias free."
v3.5.0,rescale with tau (temperature) and apply the log_softmax.
v3.5.0,ct2 expects src with lengths without padding
v3.5.0,again we use raw probs to rescale with tau and apply log_softmax
v3.5.0,lm_scores are in log space so log_target=True
v3.5.0,rescale with tau (temperature) and apply the log_softmax.
v3.5.0,ct2 expects src with lengths without padding
v3.5.0,again we use raw probs to rescale with tau and apply log_softmax
v3.5.0,lm_scores are in log space so log_target=True
v3.5.0,Create a mask with zeros at prompt positions and ones at answer postions.
v3.5.0,Apply the mask on the target side.
v3.5.0,Put the padding token index at the prompt positions.
v3.5.0,take into account here the tgt_shift_index (0 / 1 = LM/NMT)
v3.5.0,Correct target copy token instead of <unk>
v3.5.0,tgt[i] = align[i] + len(tgt_vocab)
v3.5.0,for i such that tgt[i] == 0 and align[i] != 0
v3.5.0,in the case criterion reduction is None then we need
v3.5.0,to sum the loss of each sentence in the batch
v3.5.0,Check Transforms
v3.5.0,Check path
v3.5.0,tgt is src for LM task
v3.5.0,Check weight
v3.5.0,Check features
v3.5.0,validation when train:
v3.5.0,Check embeddings stuff
v3.5.0,"Backward compatibility with ""fix_word_vecs_*"" opts"
v3.5.0,encoder and decoder should be same sizes
v3.5.0,"Load default opt values, then overwrite with the opts in"
v3.5.0,"the checkpoint. That way, if there are new options added,"
v3.5.0,the defaults are used.
v3.5.0,It comes from training
v3.5.0,TODO: needs to be added as inference opt
v3.5.0,Don't do anything
v3.5.0,Update best score of each criteria
v3.5.0,Reset tolerance
v3.5.0,Update current status
v3.5.0,Decrease tolerance
v3.5.0,Log
v3.5.0,Log
v3.5.0,Get a list of world_size lists with len(stat_list) Statistics objects
v3.5.0,"this param init is overridden by model_builder, useless then."
v3.5.0,SRU doesn't support PackedSequence.
v3.5.0,-*- coding: utf-8 -*-
v3.5.0,threshold on 1 to avoid div by 0
v3.5.0,treat alignment matrix one by one as each have different lengths
v3.5.0,No alignment if not exist valid tgt token
v3.5.0,get valid alignment (sub-matrix from full paded aligment matrix)
v3.5.0,Helper functions
v3.5.0,Keeps track of the original words/subwords
v3.5.0,('prior_tokenization' option)
v3.5.0,In case there is a final case_markup when new_spacer is on
v3.5.0,########## #
v3.5.0,Translator #
v3.5.0,########## #
v3.5.0,"Set ""default"" translation options on empty cfgfile"
v3.5.0,Build translator from options
v3.5.0,################### #
v3.5.0,Validation iterator #
v3.5.0,################### #
v3.5.0,Reinstantiate the validation iterator
v3.5.0,Retrieve raw references and sources
v3.5.0,########### #
v3.5.0,Predictions #
v3.5.0,########### #
v3.5.0,####### #
v3.5.0,Outputs #
v3.5.0,####### #
v3.5.0,Flatten predictions
v3.5.0,Save results
v3.5.0,-*- coding: utf-8 -*-
v3.5.0,this one is needed for Random Shuffler of batches
v3.5.0,in multi gpu it ensures datasets are read in the same order
v3.5.0,some cudnn methods can be random even after fixing the seed
v3.5.0,unless you tell it to be deterministic
v3.5.0,This one is needed for various tranfroms
v3.5.0,These ensure same initialization in multi gpu mode
v3.5.0,we need to check the model path + any tokenizer path
v3.5.0,patch to log stdout spawned processes of dataloader
v3.5.0,bucket_size = batch_size
v3.5.0,For TRAIN we shuffle batches within the bucket
v3.5.0,otherwise sequential
v3.5.0,for specific case of rnn_packed need to be sorted
v3.5.0,within the batch
v3.5.0,single thread - create batch directly on GPU if device is gpu
v3.5.0,multithread faster to create batch on CPU in each thread and then move it to gpu
v3.5.0,Move tensor_batch from cpu to device
v3.5.0,Check if all tokens have features or none at all
v3.5.0,Make features part of src like
v3.5.0,"{'src': {'src': ..., 'feats': [...., ....]}}"
v3.5.0,careful below it will return a bucket sorted by corpora
v3.5.0,but we sort by length later and shuffle batches
v3.5.0,at this point an example looks like:
v3.5.0,"{'src': {'src': ..., 'feats': [....]},"
v3.5.0,"'tgt': {'tgt': ...},"
v3.5.0,"'src_original': ['tok1', ...'tokn'],"
v3.5.0,"'tgt_original': ['tok1', ...'tokm'],"
v3.5.0,'cid': corpus id
v3.5.0,'cid_line_number' : cid line number
v3.5.0,"'align': ...,"
v3.5.0,}
v3.5.0,Need to add features in last dimensions
v3.5.0,Keep it consistent with dynamic data
v3.5.0,make a small vocab containing just the tokens in the source sequence
v3.5.0,Map source tokens to indices in the dynamic dict.
v3.5.0,-*- coding: utf-8 -*-
v3.5.0,this is hack: if the special separator ï½Ÿnewlineï½ is returned because of the
v3.5.0,"""docify"" transform.get_specials we don't add it if the corresponding newline code"
v3.5.0,is already included in the sentencepiece or BPE-with-gpt2-pretok.
v3.5.0,Reached end of file
v3.5.0,'src_original' and 'tgt_original' store the
v3.5.0,original line before tokenization. These
v3.5.0,fields are used later on in the feature
v3.5.0,transforms.
v3.5.0,empty example: skip
v3.5.0,ugly patch because in_feat and out_feat are reversed in WQLinear_GEMM
v3.5.0,bitsandbytes quantize weights when .cuda() is called
v3.5.0,for huge models we need to save Ram
v3.5.0,so we load the weights  module by module and transfer them to GPU for quantization
v3.5.0,bitsandbytes quantize weights when .cuda() is called
v3.5.0,for huge models we need to save Ram
v3.5.0,so we load the weights  module by module and transfer them to GPU for quantization
v3.5.0,"No encoder in LM, seq2seq count formatting kept"
v3.5.0,_check_save_model_path
v3.5.0,This preserves backward-compat for models using customed layernorm
v3.5.0,Force add_ffnbias to True if bias found in model w_1 keys
v3.5.0,fix v2 compatibility
v3.5.0,end of patch for backward compatibility
v3.5.0,!/usr/bin/env python
v3.5.0,!/usr/bin/env python
v3.5.0,!/usr/bin/env python
v3.5.0,-*- coding: utf-8 -*-
v3.5.0,!/usr/bin/env python
v3.5.0,BPE training
v3.5.0,SentencePiece training
v3.5.0,!/usr/bin/env python
v3.5.0,!/usr/bin/env python
v3.5.0,Set sharing strategy manually instead of default based on the OS.
v3.5.0,torch.multiprocessing.set_sharing_strategy('file_system')
v3.5.0,Create a thread to listen for errors in the child processes.
v3.5.0,Train with multiprocessing.
v3.5.0,magic indices
v3.5.0,result caching
v3.5.0,Here we set the decoder to start with self.start (BOS or EOS)
v3.5.0,not 100% necessary to define those
v3.5.0,self.is_finished = torch.zeros(
v3.5.0,"[self.batch_size, self.parallel_paths], dtype=torch.bool"
v3.5.0,)
v3.5.0,fix length constraint and remove eos from count
v3.5.0,add one to account for BOS. Don't account for EOS because hitting
v3.5.0,this implies it hasn't been found.
v3.5.0,we don't block nothing if the user doesn't want it
v3.5.0,we can't block nothing beam's too short
v3.5.0,we check paths one by one
v3.5.0,we don't forbid nothing if the user doesn't want it
v3.5.0,we can't forbid nothing if beam's too short
v3.5.0,Reordering forbidden_tokens following beam selection
v3.5.0,We rebuild a dict to ensure we get the value and not the pointer
v3.5.0,Grabing the newly selected tokens and associated ngram
v3.5.0,skip the blocking if any token in current_ngram is excluded
v3.5.0,"pickups: Tensor where specified index were set to 1, others 0"
v3.5.0,"dropdowns: opposite of pickups, 1 for those shouldn't pick"
v3.5.0,Minus dropdowns to log_probs making probabilities of
v3.5.0,unspecified index close to 0
v3.5.0,"prediction step have surpass length of given target_prefix,"
v3.5.0,no need to further change this attr
v3.5.0,keep indices until overflowing p
v3.5.0,Set all logits that are not in the top-p to -10000.
v3.5.0,This puts the probabilities close to 0.
v3.5.0,Set all logits that are not in the top-k to -10000.
v3.5.0,This puts the probabilities close to 0.
v3.5.0,"For temp=0.0, take the argmax to avoid divide-by-zero errors."
v3.5.0,keep_topk=1 is also equivalent to argmax.
v3.5.0,maybe fix some prediction at this step by modifying log_probs
v3.5.0,"shape: (sum(~ self.is_finished), 1)"
v3.5.0,in LM task src_len is associated with currently generated src
v3.5.0,and therefore needs to follow the generation
v3.5.0,!/usr/bin/env python
v3.5.0,for debugging
v3.5.0,TODO: maybe add dynamic part
v3.5.0,Statistics
v3.5.0,those two should be the same except feat dim
v3.5.0,"batch['src'][perm[j], :, :])"
v3.5.0,trans.src
v3.5.0,we rebuild a small batch made of the sub-segments
v3.5.0,in the long segment.
v3.5.0,new sub-batch ready to be translated
v3.5.0,we re-insert the sub-batch in the initial translations
v3.5.0,For seq2seq when we need to force doc to spit the same number of sents
v3.5.0,In the case of length_penalty = none we report the total logprobs
v3.5.0,divided by the number of sentence to get an approximation of the
v3.5.0,per sentence logprob. We also return the corresponding ppl
v3.5.0,"When a length_penalty is used eg: ""avg"" or ""wu"" since logprobs"
v3.5.0,are normalized per token we report the per line per token logprob
v3.5.0,"and the corresponding ""per word perplexity"""
v3.5.0,Turn any copied words into UNKs.
v3.5.0,"Decoder forward, takes [batch, tgt_len, nfeats] as input"
v3.5.0,"and [batch, src_len, hidden] as enc_out"
v3.5.0,"in case of inference tgt_len = 1, batch = beam times batch_size"
v3.5.0,"in case of Gold Scoring tgt_len = actual length, batch = 1 batch"
v3.5.0,Generator forward.
v3.5.0,"returns [(batch_size x beam_size) , vocab ] when 1 step"
v3.5.0,"or [batch_size, tgt_len, vocab ] when full sentence"
v3.5.0,"here we have scores [tgt_lenxbatch, vocab] or [beamxbatch, vocab]"
v3.5.0,at this point scores is batch first (dim=0)
v3.5.0,"returns [(batch_size x beam_size) , vocab ] when 1 step"
v3.5.0,"or [batch_size, tgt_len, vocab ] when full sentence"
v3.5.0,(0) add BOS and padding to tgt prediction
v3.5.0,(1) Encoder forward.
v3.5.0,(2) Repeat src objects `n_best` times.
v3.5.0,"We use batch_size x n_best, get ``(batch * n_best, src_len, nfeat)``"
v3.5.0,Quick fix. Transformers return None as enc_states.
v3.5.0,enc_states are only used later on to init decoder's state
v3.5.0,"but are never used in Transformer decoder, so we can skip"
v3.5.0,"(3) Init decoder with n_best src,"
v3.5.0,"reshape tgt to ``(len, batch * n_best, nfeat)``"
v3.5.0,it should be done in a better way
v3.5.0,here dec_in is batch first
v3.5.0,masked_select
v3.5.0,get aligned src id for each prediction's valid tgt tokens
v3.5.0,TODO: support these blacklisted features
v3.5.0,(0) Prep the components of the search.
v3.5.0,(1) Run the encoder on the src.
v3.5.0,(2) prep decode_strategy. Possibly repeat src objects.
v3.5.0,(3) Begin decoding step by step:
v3.5.0,Reorder states.
v3.5.0,TODO: support these blacklisted features
v3.5.0,(0) Prep the components of the search.
v3.5.0,(1) split src into src and target_prefix to avoid padding.
v3.5.0,(2) init decoder
v3.5.0,(3) prep decode_strategy. Possibly repeat src objects.
v3.5.0,(4) Begin decoding step by step:
v3.5.0,beg_time = time()
v3.5.0,Reorder states.
v3.5.0,select indexes in model state/cache
v3.5.0,if step == 0:
v3.5.0,"print(""step0 time: "", time() - beg_time)"
v3.5.0,beam parameters
v3.5.0,beam state
v3.5.0,"""global state"" of the old beam"
v3.5.0,buffers for the topk scores and 'backpointer'
v3.5.0,for testing
v3.5.0,maybe fix some prediction at this step by modifying log_probs
v3.5.0,Flatten probs into a list of possibilities.
v3.5.0,after this we get topk_ids between 0 and beam_size*vocab_size
v3.5.0,topk_ids // vocab_size => indice in beam
v3.5.0,topk_ids % vocab_size => true vocab indice
v3.5.0,using lists instead of tensors for topk_scores and is_finished make things faster
v3.5.0,Store finished hypotheses for this example in the batch.
v3.5.0,End condition is the top beam finished and we can return
v3.5.0,n_best hypotheses.
v3.5.0,early stop when top beam is finished
v3.5.0,Penalize beams that finished.
v3.5.0,this is required to pursue finished beams in non finished batches
v3.5.0,"If all sentences are translated, no need to go further."
v3.5.0,reset the selection for the next step
v3.5.0,Remove finished batches for the next step.
v3.5.0,using integer division to get an integer _B without casting
v3.5.0,force the output to be longer than self.min_length
v3.5.0,Multiply probs by the beam probability.
v3.5.0,"if the sequence ends now, then the penalty is the current"
v3.5.0,"length + 1, to include the EOS token"
v3.5.0,Avoid any direction that would repeat unwanted ngrams
v3.5.0,Pick up candidate token by curr_scores
v3.5.0,Recover log probs.
v3.5.0,Length penalty is just a scalar. It doesn't matter if it's applied
v3.5.0,before or after the topk.
v3.5.0,Resolve beam origin and map to batch index flat representation.
v3.5.0,Append last prediction to reordered alive sequence
v3.5.0,update global state (step == 1)
v3.5.0,update global state (step > 1)
v3.5.0,"shape: (batch_size x beam_size, 1)"
v3.5.0,in LM task src_len is associated with currently generated src
v3.5.0,and therefore needs to follow the generation
v3.5.0,Term will be subtracted from probability
v3.5.0,Probability will be divided by this
v3.5.0,these warnings indicate that either the alpha/beta
v3.5.0,"forces a penalty to be a no-op, or a penalty is a no-op but"
v3.5.0,the alpha/beta would suggest otherwise.
v3.5.0,using some coverage penalty
v3.5.0,!/usr/bin/env python
v3.5.0,semaphore doesn't have a timeout arg in Python 2.7
v3.5.0,perform a first request to initialize everything
v3.5.0,backwards compatibility for confs
v3.5.0,every segment becomes a dict for flexibility purposes
v3.5.0,NOTE: translator returns lists of `n_best` list
v3.5.0,build back results with empty texts
v3.5.0,load can be called multiple times: modify copy
v3.5.0,output contain alignment
v3.5.0,Below are all the different penalty terms implemented so far.
v3.5.0,Subtract coverage penalty from topk log probs.
v3.5.0,Divide topk log probs by length penalty.
v3.5.0,These comp lists are costy but less than for loops
v3.5.0,Chinese segmentation
v3.5.0,Chinese simplify -> Chinese traditional standard
v3.5.0,Chinese simplify -> Chinese traditional (HongKong)
v3.5.0,Chinese simplify -> Chinese traditional (Taiwan)
v3.5.0,Chinese traditional -> Chinese simplify (v1)
v3.5.0,Chinese traditional -> Chinese simplify (v2)
v3.5.0,Auto import python files in this directory
v3.5.0,contractions
v3.5.0,number separators
v3.5.0,punctuation
v3.5.0,double brackets
v3.5.0,miscellaneous
v3.5.0,Clean and Concat the dataset
v3.5.0,"joiner = tokenizer._tokenize(""\n"")"
v3.5.0,tokens += tokenizer._tokenize([x])
v3.5.0,Tokenize the dataset.
v3.5.0,Build the translator (along with the model.
v3.5.0,Score the dataset.
v3.5.0,zero out the context tokens
v3.5.0,"def custom_stopping_criteria(input_ids, score, **kwargs):"
v3.5.0,"stop_ids = [29871, 13, 13] # \n\n"
v3.5.0,return input_ids[-len(stop_ids)]
v3.5.0,Build the translator (along with the model)
v3.5.0,get prompt and make sure it fits
v3.5.0,"def custom_stopping_criteria(input_ids, score, **kwargs):"
v3.5.0,"stop_ids = [29871, 13, 13] # \n\n"
v3.5.0,return input_ids[-len(stop_ids)]
v3.5.0,Build the translator (along with the model)
v3.5.0,get prompt and make sure it fits
v3.4.3,!/usr/bin/env python
v3.4.3,!/usr/bin/env python
v3.4.3,!/usr/bin/env python
v3.4.3,!/usr/bin/env python
v3.4.3,!/usr/bin/env python
v3.4.3,!/usr/bin/env python3
v3.4.3,-*- coding: utf-8 -*-
v3.4.3,
v3.4.3,"OpenNMT-py documentation build configuration file, created by"
v3.4.3,sphinx-quickstart on Sun Dec 17 12:07:14 2017.
v3.4.3,
v3.4.3,This file is execfile()d with the current directory set to its
v3.4.3,containing dir.
v3.4.3,
v3.4.3,Note that not all possible configuration values are present in this
v3.4.3,autogenerated file.
v3.4.3,
v3.4.3,All configuration values have a default; values that are commented out
v3.4.3,serve to show the default.
v3.4.3,"If extensions (or modules to document with autodoc) are in another directory,"
v3.4.3,add these directories to sys.path here. If the directory is relative to the
v3.4.3,"documentation root, use os.path.abspath to make it absolute, like shown here."
v3.4.3,
v3.4.3,import os
v3.4.3,import sys
v3.4.3,"sys.path.insert(0, os.path.abspath('.'))"
v3.4.3,-- General configuration ------------------------------------------------
v3.4.3,"If your documentation needs a minimal Sphinx version, state it here."
v3.4.3,
v3.4.3,needs_sphinx = '6.0'
v3.4.3,"Add any Sphinx extension module names here, as strings. They can be"
v3.4.3,extensions coming with Sphinx (named 'sphinx.ext.*') or your custom
v3.4.3,ones.
v3.4.3,Show base classes
v3.4.3,"Use ""variables"" section for Attributes instead of weird block things"
v3.4.3,mimicking the function style.
v3.4.3,"Add any paths that contain templates here, relative to this directory."
v3.4.3,The suffix(es) of source filenames.
v3.4.3,You can specify multiple suffix as a list of string:
v3.4.3,
v3.4.3,"source_suffix = ['.rst', '.md']"
v3.4.3,The master toctree document.
v3.4.3,General information about the project.
v3.4.3,"The version info for the project you're documenting, acts as replacement for"
v3.4.3,"|version| and |release|, also used in various other places throughout the"
v3.4.3,built documents.
v3.4.3,
v3.4.3,The short X.Y version.
v3.4.3,"The full version, including alpha/beta/rc tags."
v3.4.3,The language for content autogenerated by Sphinx. Refer to documentation
v3.4.3,for a list of supported languages.
v3.4.3,
v3.4.3,This is also used if you do content translation via gettext catalogs.
v3.4.3,"Usually you set ""language"" from the command line for these cases."
v3.4.3,"List of patterns, relative to source directory, that match files and"
v3.4.3,directories to ignore when looking for source files.
v3.4.3,This patterns also effect to html_static_path and html_extra_path
v3.4.3,The name of the Pygments (syntax highlighting) style to use.
v3.4.3,"If true, `todo` and `todoList` produce output, else they produce nothing."
v3.4.3,-- Options for HTML output ----------------------------------------------
v3.4.3,The theme to use for HTML and HTML Help pages.  See the documentation for
v3.4.3,a list of builtin themes.
v3.4.3,
v3.4.3,html_theme = 'sphinx_materialdesign_theme'
v3.4.3,html_theme_path = [sphinx_materialdesign_theme.get_path()]
v3.4.3,Theme options are theme-specific and customize the look and feel of a theme
v3.4.3,"further.  For a list of options available for each theme, see the"
v3.4.3,documentation.
v3.4.3,
v3.4.3,html_theme_options = {}
v3.4.3,"Add any paths that contain custom static files (such as style sheets) here,"
v3.4.3,"relative to this directory. They are copied after the builtin static files,"
v3.4.3,"so a file named ""default.css"" will overwrite the builtin ""default.css""."
v3.4.3,"Custom sidebar templates, must be a dictionary that maps document names"
v3.4.3,to template names.
v3.4.3,
v3.4.3,This is required for the alabaster theme
v3.4.3,refs: http://alabaster.readthedocs.io/en/latest/installation.html#sidebars
v3.4.3,-- Options for HTMLHelp output ------------------------------------------
v3.4.3,Output file base name for HTML help builder.
v3.4.3,-- Options for LaTeX output ---------------------------------------------
v3.4.3,The paper size ('letterpaper' or 'a4paper').
v3.4.3,
v3.4.3,"'papersize': 'letterpaper',"
v3.4.3,"The font size ('10pt', '11pt' or '12pt')."
v3.4.3,
v3.4.3,"'pointsize': '10pt',"
v3.4.3,Additional stuff for the LaTeX preamble.
v3.4.3,
v3.4.3,"'preamble': '',"
v3.4.3,Latex figure (float) alignment
v3.4.3,
v3.4.3,"'figure_align': 'htbp',"
v3.4.3,Grouping the document tree into LaTeX files. List of tuples
v3.4.3,"(source start file, target name, title,"
v3.4.3,"author, documentclass [howto, manual, or own class])."
v3.4.3,-- Options for manual page output ---------------------------------------
v3.4.3,One entry per manual page. List of tuples
v3.4.3,"(source start file, name, description, authors, manual section)."
v3.4.3,-- Options for Texinfo output -------------------------------------------
v3.4.3,Grouping the document tree into Texinfo files. List of tuples
v3.4.3,"(source start file, target name, title, author,"
v3.4.3,"dir menu entry, description, category)"
v3.4.3,Build the translator (along with the model)
v3.4.3,Put messages sizes in antichronological order
v3.4.3,Caluculate antichronological history sizes
v3.4.3,Prune the history from the beginning
v3.4.3,Put back indices in chronological order.
v3.4.3,Build the translator (along with the model)
v3.4.3,We need to build the Llama tokenizer to count tokens and prune the history.
v3.4.3,The hypotheses are lists of one element but we still need to take the first one.
v3.4.3,#####
v3.4.3,UI #
v3.4.3,#####
v3.4.3,What are the 3 best french cities ?
v3.4.3,Which one is better if I like outdoor activities ?
v3.4.3,Which one is better if I like cultural outings?
v3.4.3,What are the best neighborhoods in these 5 cities?
v3.4.3,!/usr/bin/env python3
v3.4.3,Usage: python3 filter_train.py in.src in.trg out.src out.trg max-tokens
v3.4.3,flake8: noqa
v3.4.3,-*- coding: utf-8 -*-
v3.4.3,Generated by the protocol buffer compiler.  DO NOT EDIT!
v3.4.3,source: sentencepiece_model.proto
v3.4.3,@@protoc_insertion_point(imports)
v3.4.3,@@protoc_insertion_point(module_scope)
v3.4.3,!/usr/bin/env python
v3.4.3,-*- coding: utf-8 -*-
v3.4.3,is this reachable?
v3.4.3,Read in embeddings
v3.4.3,Write to file
v3.4.3,converts a SentencePiece vocabulary to the format expected by dynamic data
v3.4.3,"(essentially converts float expected counts to ""fixed precision"" int pseudo"
v3.4.3,counts)
v3.4.3,from onmt.utils.misc import use_gpu
v3.4.3,"Add in default model arguments, possibly added since training."
v3.4.3,this patch is no longer needed included in converter
v3.4.3,"if hasattr(model_opt, 'rnn_size'):"
v3.4.3,model_opt.hidden_size = model_opt.rnn_size
v3.4.3,build_base_model expects updated and validated opts
v3.4.3,-*- encoding: utf-8 -*-
v3.4.3,!/usr/bin/env python
v3.4.3,Falcon stores QKV in one single tensor but it is not simply piled up Q+K+V
v3.4.3,it is heads interleaved to we need to slice first
v3.4.3,also it uses the HF rotary so we need to permute Q and K interleave
v3.4.3,!/usr/bin/env python
v3.4.3,-*- coding: utf-8 -*-
v3.4.3,Author: Rico Sennrich
v3.4.3,flake8: noqa
v3.4.3,This file is retrieved from https://github.com/rsennrich/subword-nmt
v3.4.3,hack for python2/3 compatibility
v3.4.3,check version information
v3.4.3,some hacking to deal with duplicates (only consider first instance)
v3.4.3,don't print end-of-word symbols
v3.4.3,sys.stderr.write('cannot split {0} further.\n'.format(segment))
v3.4.3,sys.stderr.write('OOV: {0}\n'.format(segment))
v3.4.3,sys.stderr.write('OOV: {0}\n'.format(segment))
v3.4.3,python 2/3 compatibility
v3.4.3,read/write files as UTF-8
v3.4.3,!/usr/bin/env python3
v3.4.3,coding: utf-8
v3.4.3,"In order to use this tool, please install comet first"
v3.4.3,https://github.com/Unbabel/COMET
v3.4.3,Let's say you have a source file with N sentences in SL - eg: source.sl
v3.4.3,and the corresponding references (N sentences) reference.tl
v3.4.3,Translate your file in TL with the -n_best nbest options nbest being
v3.4.3,then number of hypotheses and output the target to -output target.nbest.tl
v3.4.3,Then you need to duplicate source and reference sentences nbest times
v3.4.3,for this script.
v3.4.3,for instance using awk '{for(i=1; i<=n; i++) print}' n=5 reference.tl \
v3.4.3,> reference.5.tl
v3.4.3,same for source.
v3.4.3,This script can be run (for instance with nbest = 5) as follows:
v3.4.3,python oracle_bleu.py --nbest-src source.5.sl --nbest-hyp target.5.tl \
v3.4.3,--nbest-ref reference.5.tl --nbest-order 5 --output target.maxbleu.tl
v3.4.3,It will search in all hyp the best comet score
v3.4.3,when choosing a reference-less model no nbest-ref is required
v3.4.3,for nbest in nbests:
v3.4.3,!/usr/bin/env python
v3.4.3,!/usr/bin/env python3
v3.4.3,coding: utf-8
v3.4.3,Let's say you have a source file with N sentences in SL - eg: source.sl
v3.4.3,Translate your file in TL with the -n_best nbest options nbest being
v3.4.3,then number of hypotheses and output the target to -output target.nbest.tl
v3.4.3,This script can be run (for instance with nbest = 5) as follows:
v3.4.3,python mbr_bleu.py --nbest-hyp target.5.tl \
v3.4.3,--nbest-order 5 --output target.mbr.tl
v3.4.3,It will compare all hyp with eachother and output the max bleu
v3.4.3,!/usr/bin/env python
v3.4.3,!/usr/bin/env python
v3.4.3,-*- coding: utf-8 -*-
v3.4.3,Author: Rico Sennrich
v3.4.3,flake8: noqa
v3.4.3,This file is retrieved from https://github.com/rsennrich/subword-nmt
v3.4.3,hack for python2/3 compatibility
v3.4.3,"find all instances of pair, and update frequency/indices around it"
v3.4.3,find first symbol
v3.4.3,"if first symbol is followed by second symbol, we've found an occurrence of pair (old_word[i:i+2])"
v3.4.3,"assuming a symbol sequence ""A B C"", if ""B C"" is merged, reduce the frequency of ""A B"""
v3.4.3,"assuming a symbol sequence ""A B C B"", if ""B C"" is merged, reduce the frequency of ""C B""."
v3.4.3,"however, skip this if the sequence is A B C B C, because the frequency of ""C B"" will be reduced by the previous code block"
v3.4.3,find new pair
v3.4.3,"assuming a symbol sequence ""A BC D"", if ""B C"" is merged, increase the frequency of ""A BC"""
v3.4.3,"assuming a symbol sequence ""A BC B"", if ""B C"" is merged, increase the frequency of ""BC B"""
v3.4.3,"however, if the sequence is A BC BC, skip this step because the count of ""BC BC"" will be incremented by the previous code block"
v3.4.3,data structure of pair frequencies
v3.4.3,index from pairs to words
v3.4.3,version 0.2 changes the handling of the end-of-word token ('</w>');
v3.4.3,version numbering allows bckward compatibility
v3.4.3,"threshold is inspired by Zipfian assumption, but should only affect speed"
v3.4.3,we probably missed the best pair because of pruning; go back to full statistics
v3.4.3,"threshold is inspired by Zipfian assumption, but should only affect speed"
v3.4.3,python 2/3 compatibility
v3.4.3,read/write files as UTF-8
v3.4.3,Now we can pipe the full file through the model using the Iterator
v3.4.3,reminder a batch includes .src .tgt .indices and it is sorted
v3.4.3,Compute and retrieve the loss for EACH sentence
v3.4.3,Now we need to rearrange the batch of ppl
v3.4.3,in the original order with indices
v3.4.3,!/usr/bin/env python
v3.4.3,-*- coding: utf-8 -*-
v3.4.3,!/usr/bin/env python
v3.4.3,!/usr/bin/env python
v3.4.3,!/usr/bin/env python
v3.4.3,!/usr/bin/env python
v3.4.3,!/usr/bin/env python
v3.4.3,!/usr/bin/env python3
v3.4.3,coding: utf-8
v3.4.3,Let's say you have a source file with N sentences in SL - eg: source.sl
v3.4.3,and the corresponding references (N sentences) reference.tl
v3.4.3,Translate your file in TL with the -n_best nbest options nbest being
v3.4.3,then number of hypotheses and output the target to -output target.nbest.tl
v3.4.3,Then you need to duplicate reference sentences nbest times for this script.
v3.4.3,for instance using awk '{for(i=1; i<=n; i++) print}' n=5 reference.tl \
v3.4.3,> reference.5.tl
v3.4.3,This script can be run (for instance with nbest = 5) as follows:
v3.4.3,python oracle_bleu.py --nbest-hyp target.5.tl --nbest-ref reference.5.tl \
v3.4.3,--nbest-order 5 --output target.maxbleu.tl
v3.4.3,It will search in all hyp the best bleu wrt reference
v3.4.3,and output the max bleu
v3.4.3,!/usr/bin/env python
v3.4.3,with the two module = imp.load_source() below
v3.4.3,we ghost the old torchtext.data.field and depercated
v3.4.3,onmt.inputters.text_dataset
v3.4.3,however this require some functions / classes to be
v3.4.3,monkey patched for loading the old field/vocab objects.
v3.4.3,"module3 = imp.load_source(""Vocab"", ""tools/convertv2_v3.py"")"
v3.4.3,"voc = dict(sorted(fields.vocab.__dict__['freqs'].items(),"
v3.4.3,"key=lambda x: (-x[1], x[0]))).keys()"
v3.4.3,"voc = dict(sorted(fields.vocab.__dict__['freqs'].items(),"
v3.4.3,"key=lambda x: (-x[1], x[0]))).keys()"
v3.4.3,!/usr/bin/env python
v3.4.3,redpajama stores QKV in one single tensor but it is not simply piled up Q+K+V
v3.4.3,it is heads interleaved to we need to slice first
v3.4.3,also it uses the HF rotary so we need to permute Q and K interleave
v3.4.3,Avoid functionality on inference
v3.4.3,weights are in the .pt file
v3.4.3,weights are not in the .pt checkpoint but stored in the safetensors file
v3.4.3,Build embeddings.
v3.4.3,Build encoder.
v3.4.3,Build embeddings.
v3.4.3,Build decoder.
v3.4.3,Share the embedding matrix - preprocess with share_vocab required.
v3.4.3,src/tgt vocab should be the same if `-share_vocab` is specified.
v3.4.3,Update vocabulary embeddings with checkpoint embeddings
v3.4.3,Embedding layers
v3.4.3,Just for debugging purposes
v3.4.3,Remove old vocabulary associated embeddings
v3.4.3,for back compat when attention_dropout was not defined
v3.4.3,Build Model
v3.4.3,Build Generator.
v3.4.3,If new training initialize the model params
v3.4.3,If update_vocab init also but checkpoint will overwrite old weights
v3.4.3,ONLY for legacy fusedam with amp pytorch requires NOT to half the model
v3.4.3,Update model embeddings with those from the checkpoint
v3.4.3,after initialization
v3.4.3,after this checkpoint contains no embeddings
v3.4.3,when using LoRa or updating the vocab (no more embeddings in ckpt)
v3.4.3,=> strict=False when loading state_dict
v3.4.3,weights are in the .pt file
v3.4.3,weights are not in the .pt checkpoint but stored in the safetensors file
v3.4.3,!/usr/bin/env python
v3.4.3,if transform + options set in 'valid' we need to copy in main
v3.4.3,transform / options for scoring considered as inference
v3.4.3,"maybe prepare pretrained embeddings, if any"
v3.4.3,Load checkpoint if we resume from a previous training.
v3.4.3,ensure tensorboard output is written in the directory
v3.4.3,of previous checkpoints
v3.4.3,Override checkpoint's update_embeddings as it defaults to false
v3.4.3,Override checkpoint's freezing settings as it defaults to false
v3.4.3,NOTE: It's important that ``opt`` has been validated and updated
v3.4.3,at this point.
v3.4.3,Build model.
v3.4.3,Build optimizer.
v3.4.3,Build model saver
v3.4.3,Use Tensorboard for visualization during training
v3.4.3,Options only during inference
v3.4.3,"Truncation options, for text corpus"
v3.4.3,"as for False, this will be added in _add_train_general_opts"
v3.4.3,GPU
v3.4.3,Embedding Options
v3.4.3,Model Task Options
v3.4.3,Encoder-Decoder Options
v3.4.3,Freeze Encoder and/or Decoder
v3.4.3,The following options (bridge_extra_node to n_steps) are used
v3.4.3,for training with --encoder_type ggnn (Gated Graph Neural Network).
v3.4.3,Attention options
v3.4.3,Alignement options
v3.4.3,Generator and loss options.
v3.4.3,LoRa
v3.4.3,Init options
v3.4.3,Pretrained word vectors
v3.4.3,Freeze word vectors
v3.4.3,Optimization options
v3.4.3,learning rate
v3.4.3,options relate to data preprare
v3.4.3,options relate to train
v3.4.3,Alpha and Beta values for Google Length + Coverage penalty
v3.4.3,"Described here: https://arxiv.org/pdf/1609.08144.pdf, Section 7"
v3.4.3,Length penalty options
v3.4.3,Coverage penalty options
v3.4.3,Decoding Length constraint
v3.4.3,Decoding content constraint
v3.4.3,Adding options related to source and target features
v3.4.3,Adding options relate to decoding strategy
v3.4.3,Adding option for logging
v3.4.3,Adding options related to Transforms
v3.4.3,Copyright 2016 The Chromium Authors. All rights reserved.
v3.4.3,Use of this source code is governed by a BSD-style license that can be
v3.4.3,found in the LICENSE file.
v3.4.3,"Get the key 'value' in the dict, or just use 'value'"
v3.4.3,Create a thread to listen for errors in the child processes.
v3.4.3,Build translator
v3.4.3,Build vocab
v3.4.3,Build transform pipe
v3.4.3,Basic attributes.
v3.4.3,Set model in training mode.
v3.4.3,Let's clean the GPUs before training loop
v3.4.3,UPDATE DROPOUT
v3.4.3,Run patience mechanism
v3.4.3,"If the patience has reached the limit, stop training"
v3.4.3,swap model params w/ moving average
v3.4.3,(and keep the original parameters)
v3.4.3,Set model in validating mode.
v3.4.3,raw_srcs = []
v3.4.3,raw_refs = []
v3.4.3,F-prop through the model.
v3.4.3,Compute loss.
v3.4.3,Compute validation metrics (at batch.dataset level)
v3.4.3,Compute stats
v3.4.3,Update statistics.
v3.4.3,Set model back to training mode.
v3.4.3,Truncated BPTT: reminder not compatible with accum > 1
v3.4.3,1. Create truncated target.
v3.4.3,2. F-prop all but generator.
v3.4.3,3. Compute loss.
v3.4.3,The loss of the prompt will be set to zero.
v3.4.3,"If truncated, don't backprop fully."
v3.4.3,"in case of multi step gradient accumulation,"
v3.4.3,update only after accum batches
v3.4.3,For Flake
v3.4.3,we avoid padding while mean pooling
v3.4.3,incoming and outgoing edge embedding
v3.4.3,Find vocab data for tree builting
v3.4.3,Propogation Model
v3.4.3,Initialize the bridge layer
v3.4.3,Token embedding
v3.4.3,Initialize graph using formatted input sequence
v3.4.3,Number of flagged nodes defines node count for this sample
v3.4.3,"(Nodes can have no flags on them, but must be in 'flags' list)."
v3.4.3,The total number of integers in the vocab should allow
v3.4.3,for all features and edges to be defined.
v3.4.3,Use first extra node as only source for decoder init
v3.4.3,Average all nodes to get bridge input
v3.4.3,"LSTM has hidden and cell state, other only one"
v3.4.3,Total number of states
v3.4.3,Build a linear layer for each
v3.4.3,Initialize the bridge layer
v3.4.3,src lengths data is wrapped inside a Tensor.
v3.4.3,"LSTM has hidden and cell state, other only one"
v3.4.3,Total number of states
v3.4.3,Build a linear layer for each
v3.4.3,Auto import python files in this directory
v3.4.3,batch x len x dim
v3.4.3,"feed_forward applies residual, so we remove and apply residual with un-normed"
v3.4.3,Padding mask is now (batch x 1 x slen x slen)
v3.4.3,1 to be expanded to number of heads in MHA
v3.4.3,Run the forward pass of every layer of the tranformer.
v3.4.3,Dimensions and padding for constructing the word embedding matrix
v3.4.3,Dimensions and padding for feature embedding matrices
v3.4.3,(these have no effect if feat_vocab_sizes is empty)
v3.4.3,The embedding matrix look-up tables. The first look-up table
v3.4.3,"is for words. Subsequent ones are for features, if any exist."
v3.4.3,The final output size of word + feature vectors. This can vary
v3.4.3,from the word vector size if and only if features are defined.
v3.4.3,This is the attribute you should access if you need to know
v3.4.3,how big your embeddings are going to be.
v3.4.3,The sequence of operations that converts the input sequence
v3.4.3,into a sequence of embeddings. At minimum this consists of
v3.4.3,looking up the embeddings for each word and feature in the
v3.4.3,input. Model parameters may require the sequence to contain
v3.4.3,additional operations as well.
v3.4.3,features must use word_vec_size
v3.4.3,features will use feat_vec_size
v3.4.3,Some utilitary functions for pretrained embeddings
v3.4.3,is this reachable?
v3.4.3,Write to file
v3.4.3,set the opt in place
v3.4.3,set the opt in place
v3.4.3,flake8: noqa
v3.4.3,For command-line option parsing
v3.4.3,"Check pass, set the args."
v3.4.3,"This SRU version implements its own cuda-level optimization,"
v3.4.3,so it requires that:
v3.4.3,1. `cupy` and `pynvrtc` python package installed.
v3.4.3,2. pytorch is built with cuda support.
v3.4.3,3. library path set: export LD_LIBRARY_PATH=<cuda lib path>.
v3.4.3,Check 1.
v3.4.3,Check 2.
v3.4.3,Check 3.
v3.4.3,This sets up device to use.
v3.4.3,-> directions x batch x dim
v3.4.3,For DEBUG
v3.4.3,"size = (length, batch, x.size(-1)) \"
v3.4.3,"if x.dim() == 3 else (batch, x.size(-1))"
v3.4.3,grad_x = x.new(*x.size()) if k_ == 3 else x.new(*size).zero_()
v3.4.3,Normal use
v3.4.3,"An entry check here, will catch on train side and translate side"
v3.4.3,if requirements are not satisfied.
v3.4.3,RNNDecoderState wraps hidden as a tuple.
v3.4.3,fh -> (layers*directions) x batch x dim
v3.4.3,This class is mainly used by decoder.py for RNNs but also
v3.4.3,by the CNN / transformer decoder when copy attention is used
v3.4.3,CNN has its own attention mechanism ConvMultiStepAttention
v3.4.3,Transformer has its own MultiHeadedAttention
v3.4.3,mlp wants it with bias
v3.4.3,"(batch, t_len, d) x (batch, d, s_len) --> (batch, t_len, s_len)"
v3.4.3,"(batch, t_len, s_len, d)"
v3.4.3,one step input
v3.4.3,"compute attention scores, as in Luong et al."
v3.4.3,Softmax or sparsemax to normalize attention weights
v3.4.3,each context vector c_t is the weighted average
v3.4.3,over all the source hidden states
v3.4.3,concatenate
v3.4.3,clamping necessary because of numerical errors: loss should be lower
v3.4.3,"bounded by zero, but negative values near zero are possible without"
v3.4.3,the clamp
v3.4.3,Help functions for Rotary Embeddings
v3.4.3,https://arxiv.org/pdf/2104.09864.pdf
v3.4.3,too convoluted to make maxseqlen a parameter.
v3.4.3,we suppose src_seq_len at training and max_length at inference
v3.4.3,are both < 2048 tokens.
v3.4.3,"rope is now matrix [maxseqlen, dim/2]"
v3.4.3,Help functions for max_relative positions
v3.4.3,https://arxiv.org/abs/1803.02155
v3.4.3,Shift values to be >= 0
v3.4.3,"now relative_position is in the range [0, inf)"
v3.4.3,half of the buckets are for exact increments in positions
v3.4.3,The other half of the buckets are for logarithmically bigger bins in positions
v3.4.3,up to max_distance
v3.4.3,Help functions to split model dim per head
v3.4.3,https://arxiv.org/pdf/1803.02155.pdf
v3.4.3,in the paper they suggest either two embeds
v3.4.3,relative_key / relative_value or only
v3.4.3,relative_key. We implemented the same embed
v3.4.3,for both.
v3.4.3,"1) Project key, value, and query."
v3.4.3,as a reminder at training layer_cache[0] remains False
v3.4.3,expand key on heads dimension when it's less than query heads (multi-query variant)
v3.4.3,expand value on heads dimension when it's less than query heads (multi-query variant)
v3.4.3,"2) When standard pos. enc. or rotary, use flash attention"
v3.4.3,Ultimately flashv2 will be part of pytorch https://github.com/pytorch/pytorch/pull/105602
v3.4.3,"In the meantime: if vanilla tranformer or Rotary embeddings (not rel_pos, not alibi)"
v3.4.3,then use flash2 if seq len > 256 otherwise use xtransformer from pt2 uptream
v3.4.3,batch x num_heads x query_len x key_len
v3.4.3,1 or key_len x key_len
v3.4.3,1 or key_len x key_len x dim_per_head
v3.4.3,not 100% necessary but expand to nb of heads
v3.4.3,now mask and scores have the same shape
v3.4.3,3) Apply attention dropout and compute context vectors.
v3.4.3,We use the same embeddings for key and value
v3.4.3,--------------------------------------------------------------------------
v3.4.3,copied and adapted https://github.com/microsoft/LoRA/
v3.4.3,Copyright (c) Microsoft Corporation. All rights reserved.
v3.4.3,Licensed under the MIT License (MIT).
v3.4.3,Support bnb quantization of nderlying layers
v3.4.3,--------------------------------------------------------------------------
v3.4.3,Optional dropout
v3.4.3,Mark the weight as unmerged
v3.4.3,LoRA implemented in a dense layer
v3.4.3,Actual trainable parameters
v3.4.3,Freezing the pre-trained weight matrix
v3.4.3,initialize A the same way as the default
v3.4.3,for nn.Linear and B to zero
v3.4.3,Make sure that the weights are not merged
v3.4.3,Merge the weights and mark it
v3.4.3,Actual trainable parameters
v3.4.3,Freezing the pre-trained weight matrix
v3.4.3,we do not super().reset_parameters() save lot of time and useless when no grad.
v3.4.3,initialize A the same way as the default
v3.4.3,for nn.Linear and B to zero
v3.4.3,Make sure that the weights are not merged
v3.4.3,Merge the weights and mark it
v3.4.3,cannot merge/unmerge quantized weigts with unquantized lora_X
v3.4.3,Check if QLoraLinear has a custom __init__ method
v3.4.3,Invoke the __init__ method of QLoraLinear
v3.4.3,LoRA implemented in a dense layer
v3.4.3,At the moment this class is only used by embeddings.Embeddings look-up tables
v3.4.3,for some reason list comprehension is slower in this scenario
v3.4.3,"for silu, see: https://arxiv.org/pdf/2002.05202.pdf"
v3.4.3,-*- coding: utf-8 -*-
v3.4.3,class AverageAttention(torch.jit.ScriptModule):
v3.4.3,@torch.jit.script
v3.4.3,Code taken from bitsandbytes but modified with arg device to accept skipt_init
v3.4.3,from torch.nn.utils => makes model building way faster.
v3.4.3,"weights are cast automatically as Int8Params, but the bias has to be cast manually"
v3.4.3,reorder weight layout back from ampere/turing to row
v3.4.3,"we only need to save SCB as extra data, because CB for quantized weights"
v3.4.3,is already stored in weight.data
v3.4.3,"case 1: .cuda was called, SCB is in self.weight"
v3.4.3,"case 2: self.init_8bit_state was called, SCB is in self.state"
v3.4.3,"buffers not yet initialized, can't call them directly without"
v3.4.3,"weights are cast automatically as Int8Params, but the bias has to be cast manually"
v3.4.3,we converted 8-bit row major to turing/ampere format in the first inference pass
v3.4.3,we no longer need the row-major weight
v3.4.3,out_features * in_features
v3.4.3,norm is out_features * 1
v3.4.3,batch_size * out_features
v3.4.3,out_features
v3.4.3,out_features
v3.4.3,batch_size * out_features
v3.4.3,"out_channels, in_channels // groups, * kernel_size"
v3.4.3,out_features
v3.4.3,This is used nowhere in the code at the moment (Vincent Nguyen 05/18/2018)
v3.4.3,"in_channels, out_channels, *kernel_size"
v3.4.3,"in_channels, out_channels, *kernel_size"
v3.4.3,"self.out_channels, 1"
v3.4.3,out_features
v3.4.3,out_features
v3.4.3,store roots on diagonal
v3.4.3,Original probabilities.
v3.4.3,Probability of copying p(z=1) batch.
v3.4.3,Probability of not copying: p_{word}(w) * (1 - p(z))
v3.4.3,probabilities assigned by the model to the gold targets
v3.4.3,probability of tokens copied from source
v3.4.3,Set scores for unk to 0 and add eps
v3.4.3,find the indices in which you do not use the copy mechanism
v3.4.3,Drop padding.
v3.4.3,We exclude tokenization for contractions in
v3.4.3,order to avoid inconsistencies with pyonmtok's tokenization.
v3.4.3,"(e.g. ""I ca n't"" with spacy, ""I can ' t"" with pyonmttok)"
v3.4.3,Use Spacy's stopwords to get rid of junk entries
v3.4.3,Perform tokenization with spacy for consistency.
v3.4.3,We ensure that the target lemma is present in the lemmatized
v3.4.3,"target string, that the match is an exact match (there is"
v3.4.3,whitespace before or after the term)
v3.4.3,and we perform some bound checking.
v3.4.3,Map the lemmatized string match index to
v3.4.3,the lemmatized list index
v3.4.3,We need to know if the term is multiword
v3.4.3,Join multiword target lemmas with a unique separator so
v3.4.3,we can treat them as single word and not change the indices.
v3.4.3,Construct the final source from the lemmatized list
v3.4.3,that contains the terms. We compare the tokens in the
v3.4.3,term-augmented lemma list with the tokens in the original
v3.4.3,"lemma list. If the lemma is the same, then we replace with"
v3.4.3,the token from the original tokenized source list. If they
v3.4.3,"are not the same, it means the lemma has been augemented"
v3.4.3,"with a term, so we inject this in the final list."
v3.4.3,Restore the spaces in multi-word terms
v3.4.3,Skip half examples to improve performance. This means we set
v3.4.3,"a hard limit for the `term_corpus_ratio` to 0.5, which is actually"
v3.4.3,quite high. TODO: We can add this (skipping examples) as an option
v3.4.3,Filter out very short or very long sentences
v3.4.3,from the TM for better performance
v3.4.3,We split the `batch` and perform fuzzy matching
v3.4.3,in smaller chunks of 10.000 examples in order to
v3.4.3,reduce memory usage.
v3.4.3,Perfomance is not affected.
v3.4.3,Probably redundant but let's be safe
v3.4.3,in case some examples are already fuzzied
v3.4.3,(e.g. from another pipeline or workflow)
v3.4.3,We don't want exact matches
v3.4.3,Apply a basic filtering to leave out very short or very long
v3.4.3,sentences and speed up things a bit during fuzzy matching
v3.4.3,Do nothing
v3.4.3,We set the start number of tags to a random number from 1
v3.4.3,to 12 + the number of subsequent tags that
v3.4.3,will be added. We also apply weights to this choice so tags
v3.4.3,"are more probable to start from 1, then from 2, etc."
v3.4.3,This way we cover most scenarios met in real usage and
v3.4.3,the system will learn to handle a fairly large number of
v3.4.3,numbered tags (but not an excessively large number)
v3.4.3,Make sure we only search for exact matches (we don't want
v3.4.3,to match part of words) and perform some bound checking
v3.4.3,Create all possible tag forms. We inject a special
v3.4.3,unicode char (âˆ¥) as a placeholder for whitespace in order
v3.4.3,to keep the indices unaltered. This char is replaced with
v3.4.3,spaces before we return the augmented examples.
v3.4.3,Make a weighted choice between paired tags or single tags.
v3.4.3,"We usually encounter, and thus here we favor, paired tags"
v3.4.3,with a ratio 1/3.
v3.4.3,Check if the tags include the
v3.4.3,"mandatory ""#"" number placeholder"""
v3.4.3,We split the user-defined tags in the # placeholder
v3.4.3,in order to number them
v3.4.3,Skip half examples to speed up the transform. This sets
v3.4.3,"a hard limit of 0.5 to the `tags_corpus_ratio`, which is"
v3.4.3,excessive and should be avoided anyway.
v3.4.3,normalize dict src/tgt for each dataset
v3.4.3,"print(""src empty"")"
v3.4.3,"print(""too many same char in src"")"
v3.4.3,"print(""too many same word in src"")"
v3.4.3,"print(""avg token min"", len(src_str) / len(ex['src']))"
v3.4.3,"print(""avg token max"", len(src_str) / len(ex['src']))"
v3.4.3,"print(""text does not fully belong to wanted script"")"
v3.4.3,"print(""Some text belong to unwanted scripts"")"
v3.4.3,"print(""langid does not match"", _id(src_str))"
v3.4.3,"print(""src = tgt"")"
v3.4.3,"print(""tgt empty"")"
v3.4.3,"print(""src / tgt ratio "", len(src_str) / len(tgt_str))"
v3.4.3,"print(""too many same char in tgt"")"
v3.4.3,"print(""too many same word in tgt"")"
v3.4.3,"print(""avg token min"", len(tgt_str) / len(ex['tgt']))"
v3.4.3,"print(""avg token max"", len(tgt_str) / len(ex['tgt']))"
v3.4.3,"print(""text does not fully belong to wanted script"")"
v3.4.3,"print(""Some text belong to unwanted scripts"")"
v3.4.3,"print(""langid does not match"", _id(tgt_str))"
v3.4.3,"doc break we add it, restart new doc"
v3.4.3,case 1st ex is already longer
v3.4.3,adding cur ex is too long we add cur doc
v3.4.3,and reset doc to cur ex
v3.4.3,we start the new doc with cur ex
v3.4.3,we cumulate cur ex to cur doc
v3.4.3,Auto import python files in this directory
v3.4.3,1. sample number of tokens to corrupt
v3.4.3,2. sample positions to corrput
v3.4.3,3. sample corrupted values
v3.4.3,1. sample number of tokens to corrupt
v3.4.3,2. sample positions to corrput
v3.4.3,3. Drop token on chosen position
v3.4.3,1. sample number of tokens to corrupt
v3.4.3,2. sample positions to corrput
v3.4.3,3. mask word on chosen position
v3.4.3,"Sharing options among `TokenizerTransform`s, same name conflict in"
v3.4.3,this scope will be resolved by remove previous occurrence in parser
v3.4.3,subword regularization(or BPE dropout) options:
v3.4.3,subword vocabulary restriction options:
v3.4.3,This method embeds a custom logic to correctly handle certain placeholders
v3.4.3,in case the tokenizer doesn't preserve them.
v3.4.3,Locate the end-of-sentence placeholders.
v3.4.3,Tokenize each sentence separately.
v3.4.3,Locate the mask-before placeholders
v3.4.3,(to zero-out the prompt loss during LM finetuning).
v3.4.3,Tokenize each chunk separately and insert the padding token.
v3.4.3,between each sequence of tokens.
v3.4.3,Re-insert the eos token.
v3.4.3,derterministic subwording
v3.4.3,subword sampling when nbest_size > 1 or -1
v3.4.3,alpha should be 0.0 < alpha < 1.0
v3.4.3,Load vocabulary file if provided and set threshold
v3.4.3,Load Subword Model
v3.4.3,-1: keep everything (i.e. 1 mask per token)
v3.4.3,0: replace everything (i.e. no mask)
v3.4.3,1: 1 mask per span
v3.4.3,view each subword as word start / input is word level token
v3.4.3,Pretend it ends with a full stop so last span is a sentence
v3.4.3,"Tokens that are full stops, where the previous token is not"
v3.4.3,Make sure we have enough to mask
v3.4.3,Trim to masking budget
v3.4.3,Handle 0-length mask (inserts) separately
v3.4.3,assert is_word_start[-1] == 0
v3.4.3,assert tokens_length - 1 not in indices
v3.4.3,"keep index, but replace it with [MASK]"
v3.4.3,"acts as a long length, so spans don't go over the end of doc"
v3.4.3,next position from each word_start
v3.4.3,delete token: 1 mask/remove per span
v3.4.3,"keep index, but replace it with [MASK]: 1 mask per token"
v3.4.3,A bit faster when all lengths are 1
v3.4.3,to cover whole token
v3.4.3,delete token
v3.4.3,"keep index, but replace it with [MASK]"
v3.4.3,assert tokens_length - 1 not in indices
v3.4.3,prefix src/tgt for each dataset
v3.4.3,prefix as general option for inference
v3.4.3,suffix src/tgt for each dataset
v3.4.3,suffix as general option for inference
v3.4.3,!/usr/bin/env python3
v3.4.3,-*- coding: utf-8 -*-
v3.4.3,Most code taken from: https://github.com/alvations/sacremoses
v3.4.3,Which in turn is based on the Moses punctuation normalizer.
v3.4.3,https://github.com/moses-smt/mosesdecoder/blob/master/scripts/
v3.4.3,tokenizer/normalize-punctuation.perl
v3.4.3,don't fix period at end of sentence
v3.4.3,Regex substitutions from replace-unicode-punctuation.perl
v3.4.3,https://github.com/moses-smt/mosesdecoder/blob/master/
v3.4.3,scripts/tokenizer/replace-unicode-punctuation.perl
v3.4.3,Adds the penn substitutions after extra_whitespace regexes.
v3.4.3,"Optionally, replace unicode puncts BEFORE normalization."
v3.4.3,Actual normalization.
v3.4.3,"Optionally, replace unicode puncts BEFORE normalization."
v3.4.3,normalize dict src/tgt for each dataset
v3.4.3,One source feature expected but none given and no default provided
v3.4.3,Provided default does not match required features
v3.4.3,Data not properly annotated.
v3.4.3,In this case we do not use the default as it might be an error
v3.4.3,batch 0 will always predict EOS. The other batches will predict
v3.4.3,non-eos scores.
v3.4.3,"""best"" prediction is eos - that should be blocked"
v3.4.3,include at least one prediction OTHER than EOS
v3.4.3,that is greater than -1e20
v3.4.3,now batch 0 has ended and no others have
v3.4.3,initial step
v3.4.3,batch 0 dies on step 0
v3.4.3,include at least one prediction OTHER than EOS
v3.4.3,that is greater than -1e20
v3.4.3,step 2
v3.4.3,(old) batch 8 dies on step 1
v3.4.3,step 3
v3.4.3,everything dies
v3.4.3,initial step
v3.4.3,batch 0 dies on step 0
v3.4.3,include at least one prediction OTHER than EOS
v3.4.3,that is greater than -1e20
v3.4.3,step 2
v3.4.3,(old) batch 8 dies on step 1
v3.4.3,step 3
v3.4.3,everything dies
v3.4.3,initial step
v3.4.3,finish one beam
v3.4.3,include at least one prediction OTHER than EOS
v3.4.3,that is greater than -1e20
v3.4.3,step 2
v3.4.3,finish example in last batch
v3.4.3,(old) batch 8 dies on step 1
v3.4.3,step 3
v3.4.3,everything dies
v3.4.3,initial step
v3.4.3,batch 0 dies on step 0
v3.4.3,include at least one prediction OTHER than EOS
v3.4.3,that is greater than -1e20
v3.4.3,step 2
v3.4.3,(old) batch 8 dies on step 1
v3.4.3,step 3
v3.4.3,everything dies
v3.4.3,illegal_weights_mask = torch.ByteTensor([
v3.4.3,"[0, 0, 0, 0, 0, 0, 0],"
v3.4.3,"[0, 0, 0, 1, 1, 1, 1],"
v3.4.3,"[0, 0, 0, 0, 0, 1, 1],"
v3.4.3,"[0, 0, 1, 1, 1, 1, 1]])"
v3.4.3,TODO: fix for pytorch 0.3
v3.4.3,illegal_weights = alignments.masked_select(illegal_weights_mask)
v3.4.3,"self.assertEqual(0.0, illegal_weights.data.sum())"
v3.4.3,this could be considered an integration test because it touches
v3.4.3,the filesystem for the config file (and the models)
v3.4.3,no dummy prefix
v3.4.3,no dummy prefix
v3.4.3,make sure the scalars are in the event accumulator tags
v3.4.3,required arguments
v3.4.3,transforms that require vocab will not create if not provide vocab
v3.4.3,1. Init first transform in the pipe
v3.4.3,2. Init second transform in the pipe
v3.4.3,3. Sequential combine them into a transform pipe
v3.4.3,4. apply transform pipe for example
v3.4.3,"5. example after the pipe exceed the length limit, thus filtered"
v3.4.3,6. Transform statistics registed (here for filtertoolong)
v3.4.3,"7. after report, statistics become empty as a fresh start"
v3.4.3,filter_transform.warm_up()
v3.4.3,test BPE-dropout:
v3.4.3,1. disable bpe dropout for not training example
v3.4.3,2. enable bpe dropout for training example
v3.4.3,3. (NOTE) disable dropout won't take effect if already seen
v3.4.3,this is caused by the cache mechanism in bpe:
v3.4.3,return cached subword if the original token is seen when no dropout
v3.4.3,test SP regularization:
v3.4.3,1. enable regularization for training example
v3.4.3,2. disable regularization for not training example
v3.4.3,Test mask location
v3.4.3,Test mask location
v3.4.3,Test mask location
v3.4.3,Not apply token drop for not training example
v3.4.3,apply token drop for training example
v3.4.3,Not apply token mask for not training example
v3.4.3,apply token mask for training example
v3.4.3,require vocabs to warm_up
v3.4.3,Not apply token mask for not training example
v3.4.3,apply token mask for training example
v3.4.3,"Defalt: full_stop_token=[""."", ""?"", ""!""]"
v3.4.3,"Defalt: full_stop_token=[""."", ""?"", ""!""]"
v3.4.3,random_ratio of inserted tokens are chosen in vocab
v3.4.3,others are MASK_TOK
v3.4.3,"insert_ratio=0.0,"
v3.4.3,"random_ratio=0.0,"
v3.4.3,"Defalt: full_stop_token=[""."", ""?"", ""!""]"
v3.4.3,all token are considered as an individual word
v3.4.3,1. tokens are dropped when replace_length is 0
v3.4.3,"print(f""token delete: {masked} / {tokens}"")"
v3.4.3,2. tokens are replaced by MASK when replace_length is 1
v3.4.3,"print(f""token mask: {masked} / {tokens}"")"
v3.4.3,"insert_ratio=0.0,"
v3.4.3,"random_ratio=0.0,"
v3.4.3,"Defalt: full_stop_token=[""."", ""?"", ""!""]"
v3.4.3,start token of word are identified using subword marker
v3.4.3,"1. replace_length 0: ""words"" are dropped"
v3.4.3,"print(f""word delete: {masked} / {tokens}"")"
v3.4.3,"self.assertEqual(len(masked), n_words - n_masked)"
v3.4.3,"2. replace_length 1: ""words"" are replaced with a single MASK"
v3.4.3,"print(f""whole word single mask: {masked} / {tokens}"")"
v3.4.3,len(masked) depend on number of tokens in select word
v3.4.3,"3. replace_length -1: all tokens in ""words"" are replaced with MASK"
v3.4.3,"print(f""whole word multi mask: {masked} / {tokens}"")"
v3.4.3,number of mask_tok depend on number of tokens in selected word
v3.4.3,number of MASK_TOK can be greater than n_masked
v3.4.3,"insert_ratio=0.5,"
v3.4.3,"random_ratio=0.3,"
v3.4.3,"Defalt: full_stop_token=[""."", ""?"", ""!""]"
v3.4.3,start token of word are identified using subword marker
v3.4.3,n_words = sum(token_starts)
v3.4.3,n_masked = math.ceil(n_words * bart_noise.mask_ratio)
v3.4.3,"print(f""Text Span Infilling: {infillied} / {tokens}"")"
v3.4.3,"print(n_words, n_masked)"
v3.4.3,Build the translator (along with the model)
v3.4.3,Required arguments
v3.4.3,!/usr/bin/env python
v3.4.3,-*- coding: utf-8 -*-
v3.4.3,Inject some dummy training options that may needed when build fields
v3.4.3,Remove the generated *pt files.
v3.4.3,Remove the generated data samples
v3.4.3,all beams repeat (beam >= 1 repeat dummy scores)
v3.4.3,predict repeat_idx over and over again
v3.4.3,"before repeat, scores are either 0 or -inf"
v3.4.3,"on repeat, `repeat_idx` score is BLOCKED_SCORE"
v3.4.3,"(but it's still the best score, thus we have"
v3.4.3,"[BLOCKED_SCORE, -inf, -inf, -inf, -inf]"
v3.4.3,repetitions keeps maximizing score
v3.4.3,"index 0 has been blocked, so repeating=>+0.0 score"
v3.4.3,other indexes are -inf so repeating=>BLOCKED_SCORE
v3.4.3,which is higher
v3.4.3,beam 0 and beam >=2 will repeat (beam >= 2 repeat dummy scores)
v3.4.3,non-interesting beams are going to get dummy values
v3.4.3,"on initial round, only predicted scores for beam 0"
v3.4.3,matter. Make two predictions. Top one will be repeated
v3.4.3,"in beam zero, second one will live on in beam 1."
v3.4.3,predict the same thing in beam 0
v3.4.3,continue pushing around what beam 1 predicts
v3.4.3,"now beam 0 dies (along with the others), beam 1 -> beam 0"
v3.4.3,"now beam 0 dies (along with the others), beam 1 -> beam 0"
v3.4.3,beam 0 and beam >= 2 will repeat (beam 2 repeats excluded idx)
v3.4.3,non-interesting beams are going to get dummy values
v3.4.3,predict the same thing in beam 0
v3.4.3,continue pushing around what beam 1 predicts
v3.4.3,predict the allowed-repeat again in beam 2
v3.4.3,"now beam 0 dies, beam 1 -> beam 0, beam 2 -> beam 1"
v3.4.3,and the rest die
v3.4.3,"since all preds after i=0 are 0, we can check"
v3.4.3,that the beam is the correct idx by checking that
v3.4.3,the curr score is the initial score
v3.4.3,beam 0 will always predict EOS. The other beams will predict
v3.4.3,non-eos scores.
v3.4.3,non-interesting beams are going to get dummy values
v3.4.3,"""best"" prediction is eos - that should be blocked"
v3.4.3,include at least beam_sz predictions OTHER than EOS
v3.4.3,that are greater than -1e20
v3.4.3,predict eos in beam 0
v3.4.3,provide beam_sz other good predictions
v3.4.3,now the top beam has ended and no others have
v3.4.3,"not of interest, but want to make sure it keeps running"
v3.4.3,since only beam 0 terminates and n_best = 2
v3.4.3,"this is also a test that when block_ngram_repeat=0,"
v3.4.3,repeating is acceptable
v3.4.3,non-interesting beams are going to get dummy values
v3.4.3,"""best"" prediction is eos - that should be blocked"
v3.4.3,include at least beam_sz predictions OTHER than EOS
v3.4.3,that are greater than -1e20
v3.4.3,predict eos in beam 1
v3.4.3,provide beam_sz other good predictions in other beams
v3.4.3,beam 1 dies on min_length
v3.4.3,beam 0 dies on the step after beam 1 dies
v3.4.3,"inp_lens is tiled in initialize, reassign to make attn match"
v3.4.3,non-interesting beams are going to get dummy values
v3.4.3,"""best"" prediction is eos - that should be blocked"
v3.4.3,include at least beam_sz predictions OTHER than EOS
v3.4.3,that are greater than -1e20
v3.4.3,predict eos in beam 1
v3.4.3,provide beam_sz other good predictions in other beams
v3.4.3,no top beams are finished yet
v3.4.3,beam 1 dies on min_length
v3.4.3,no top beams are finished yet
v3.4.3,beam 0 dies on the step after beam 1 dies
v3.4.3,top beam is finished now so there are attentions
v3.4.3,two beams are finished in each batch
v3.4.3,second dim is cut down to the non-padded src length
v3.4.3,first dim is equal to the time of death
v3.4.3,(beam 0 died at current step - adjust for SOS)
v3.4.3,(beam 1 died at last step - adjust for SOS)
v3.4.3,behavior gets weird when beam is already done so just stop
v3.4.3,this is just test_beam.TestBeamAgainstReferenceCase repeated
v3.4.3,in each batch.
v3.4.3,"init_preds: [4, 3, 5, 6, 7] - no EOS's"
v3.4.3,no EOS's yet
v3.4.3,"[5, 3, 2, 6, 0], so beam 2 predicts EOS!"
v3.4.3,assumes beam 2 finished on last step
v3.4.3,ended beam 2 shouldn't continue
v3.4.3,"[2, 5, 3, 6, 0] repeat self.BATCH_SZ, so beam 0 predicts EOS!"
v3.4.3,"[-2.4879, -3.8910, -4.1010, -4.2010, -4.4010] repeat self.BATCH_SZ"
v3.4.3,another beam is finished in all batches
v3.4.3,new beam 0 finished
v3.4.3,new beam 0 is old beam 3
v3.4.3,assumes beam 0 finished on last step
v3.4.3,"[5, 2, 6, 1, 0] repeat self.BATCH_SZ, so beam 1 predicts EOS!"
v3.4.3,we finish 3 hyps per example in this step
v3.4.3,new beam 1 is old beam 3
v3.4.3,this could be considered an integration test because it tests
v3.4.3,interactions between the GNMT scorer and the beam
v3.4.3,"-data option is required, but not used in this test, so dummy."
v3.4.3,len x batch x nfeat
v3.4.3,Initialize vectors to compare size with
v3.4.3,Ensure correct sizes and types
v3.4.3,Make sure that output has the correct size and type
v3.4.3,"[('encoder_type', 'transformer'),"
v3.4.3,"('word_vec_size', 16), ('hidden_size', 16)],"
v3.4.3,""""""" Only do SRU test if requirment is safisfied. """""""
v3.4.3,SRU doesn't support input_feed.
v3.4.3,first check there's nothing unexpectedly not trainable
v3.4.3,ok: word embeddings shouldn't be trainable
v3.4.3,if word vecs are freezed
v3.4.3,ok: positional encodings shouldn't be trainable
v3.4.3,then check nothing unexpectedly trainable
v3.4.3,Decoder state
v3.4.3,Build the RNN.
v3.4.3,Set up the context gate.
v3.4.3,Set up the standard attention.
v3.4.3,The encoder hidden is  (layers*directions) x batch x dim.
v3.4.3,We need to convert it to layers x batch x (directions*dim).
v3.4.3,Init the input feed.
v3.4.3,Update the state with the result.
v3.4.3,Concatenates sequence of tensors along a new dimension.
v3.4.3,NOTE: v0.3 to 0.4: dec_outs / attns[*] may not be list
v3.4.3,(in particular in case of SRU) it was not raising error in 0.3
v3.4.3,since stack(Variable) was allowed.
v3.4.3,"In 0.4, SRU returns a tensor that shouldn't be stacke"
v3.4.3,Calculate the attention.
v3.4.3,Calculate the context gate.
v3.4.3,Additional args check.
v3.4.3,Input feed concatenates hidden state with
v3.4.3,input at every time step.
v3.4.3,TODO: context gate should be employed
v3.4.3,instead of second RNN transform.
v3.4.3,Update the coverage attention.
v3.4.3,"attns[""coverage""] is actually c^(t+1) of See et al(2017)"
v3.4.3,1-index shifted
v3.4.3,Decoder State
v3.4.3,CNNDecoder has its own attention mechanism.
v3.4.3,Set up a separate copy attention layer if needed.
v3.4.3,The output of CNNEncoder.
v3.4.3,The combination of output of CNNEncoder and source embeddings.
v3.4.3,Process the result and update the attentions.
v3.4.3,Update the state.
v3.4.3,TODO change the way attns is returned dict => list or tuple (onnx)
v3.4.3,Auto import python files in this directory
v3.4.3,src_len is a single tensor shared between all models.
v3.4.3,This assumption will not hold if Translator is modified
v3.4.3,to calculate src_len as something other than the length
v3.4.3,of the input.
v3.4.3,"return _, (B, Q_len, K_len)"
v3.4.3,"layer average attention across heads, get ``(B, Q, K)``"
v3.4.3,"Case 1: no full_context, no align heads -> layer avg baseline"
v3.4.3,"Case 2: no full_context, 1 align heads -> guided align"
v3.4.3,"Case 3: full_context, 1 align heads -> full cte guided align"
v3.4.3,T: could be 1 in the case of stepwise decoding or tgt_len
v3.4.3,masking is necessary when sequence length is greater than one
v3.4.3,mask now are (batch x 1 x tlen x s or t len)
v3.4.3,1 = heads to be expanded in MHA
v3.4.3,"feed_forward applies residual, so we remove and apply residual with un-normed"
v3.4.3,Decoder State
v3.4.3,"previously, there was a GlobalAttention module here for copy"
v3.4.3,"attention. But it was never actually used -- the ""copy"" attention"
v3.4.3,just reuses the context attention.
v3.4.3,"attns[""align""] = torch.stack(attn_aligns, 0).mean(0)  # All avg"
v3.4.3,TODO change the way attns is returned dict => list or tuple (onnx)
v3.4.3,first value set to True triggered by the beginning of decoding
v3.4.3,layer_cache becomes active in the MultiHeadedAttention fwd
v3.4.3,T: could be 1 in the case of stepwise decoding or tgt_len
v3.4.3,masking is necessary when sequence length is greater than one
v3.4.3,mask now are (batch x 1 x tlen x tlen)
v3.4.3,1 = heads to be expanded in MHA
v3.4.3,"feed_forward applies residual, so we remove and apply residual with un-normed"
v3.4.3,TODO change the way attns is returned dict => list or tuple (onnx)
v3.4.3,"buffer size in bytes, determine equiv. # of elements based on data type"
v3.4.3,copy tensors into buffer_t
v3.4.3,all-reduce and rescale
v3.4.3,copy all-reduced buffer back into tensors
v3.4.3,"print(filled, sz)"
v3.4.3,"tensor is bigger than buffer, all-reduce and rescale directly"
v3.4.3,"buffer is full, all-reduce and replace buffer with grad"
v3.4.3,add tensor to buffer
v3.4.3,"propagate exception to parent process, keeping original traceback"
v3.4.3,"propagate exception to parent process, keeping original traceback"
v3.4.3,TODO: Find a better way to check for sparse gradients.
v3.4.3,we use apex.amp
v3.4.3,In this case use the old FusedAdam with
v3.4.3,FP16_optimizer wrapper
v3.4.3,Load everything from the checkpoint.
v3.4.3,Build everything from scratch.
v3.4.3,"Reset optimizer, keep options."
v3.4.3,"Reset options, keep optimizer."
v3.4.3,State can be partially restored.
v3.4.3,should be: self._optimizer.zero_grad(set_to_none)
v3.4.3,but apex.amp is not up-to-date:
v3.4.3,https://github.com/NVIDIA/apex/blob/master/apex/amp/_process_optimizer.py#L367
v3.4.3,"unscaled optimizer's gradients (already done therefore skip),"
v3.4.3,skips optimizer.step() if gradients contain infs/NaNs.
v3.4.3,Updates the scale for next iteration.
v3.4.3,Code below is an implementation of https://arxiv.org/pdf/1804.04235.pdf
v3.4.3,inspired but modified from https://github.com/DeadAt0m/adafactor-pytorch
v3.4.3,backward compatibility
v3.4.3,assuming a list/generator of parameter means single group
v3.4.3,compute combined scale factor for this group
v3.4.3,norm is in fact norm*scale
v3.4.3,note: p.grad should not ever be set for correct operation of
v3.4.3,mixed precision optimizer that sometimes sends None gradients
v3.4.3,State initialization
v3.4.3,Exponential moving average of gradient values
v3.4.3,Exponential moving average of squared gradient values
v3.4.3,-*- coding: utf-8 -*-
v3.4.3,placing this here make it easier to call logger.info
v3.4.3,"from anywhere, just 'from onmt.utils.logging import logger'"
v3.4.3,"align_head contains value in [0, 1) presenting attn prob,"
v3.4.3,0 was resulted by the context attention src_pad_mask
v3.4.3,"So, the correspand position in ref_align should also be 0"
v3.4.3,"Therefore, clip align_head to > 1e-18 should be bias free."
v3.4.3,rescale with tau (temperature) and apply the log_softmax.
v3.4.3,ct2 expects src with lengths without padding
v3.4.3,again we use raw probs to rescale with tau and apply log_softmax
v3.4.3,lm_scores are in log space so log_target=True
v3.4.3,rescale with tau (temperature) and apply the log_softmax.
v3.4.3,ct2 expects src with lengths without padding
v3.4.3,again we use raw probs to rescale with tau and apply log_softmax
v3.4.3,lm_scores are in log space so log_target=True
v3.4.3,Create a mask with zeros at prompt positions and ones at answer postions.
v3.4.3,Apply the mask on the target side.
v3.4.3,Put the padding token index at the prompt positions.
v3.4.3,take into account here the tgt_shift_index (0 / 1 = LM/NMT)
v3.4.3,Correct target copy token instead of <unk>
v3.4.3,tgt[i] = align[i] + len(tgt_vocab)
v3.4.3,for i such that tgt[i] == 0 and align[i] != 0
v3.4.3,in the case criterion reduction is None then we need
v3.4.3,to sum the loss of each sentence in the batch
v3.4.3,Check Transforms
v3.4.3,Check path
v3.4.3,tgt is src for LM task
v3.4.3,Check weight
v3.4.3,Check features
v3.4.3,validation when train:
v3.4.3,Check embeddings stuff
v3.4.3,"Backward compatibility with ""fix_word_vecs_*"" opts"
v3.4.3,encoder and decoder should be same sizes
v3.4.3,"Load default opt values, then overwrite with the opts in"
v3.4.3,"the checkpoint. That way, if there are new options added,"
v3.4.3,the defaults are used.
v3.4.3,It comes from training
v3.4.3,TODO: needs to be added as inference opt
v3.4.3,Don't do anything
v3.4.3,Update best score of each criteria
v3.4.3,Reset tolerance
v3.4.3,Update current status
v3.4.3,Decrease tolerance
v3.4.3,Log
v3.4.3,Log
v3.4.3,Get a list of world_size lists with len(stat_list) Statistics objects
v3.4.3,"this param init is overridden by model_builder, useless then."
v3.4.3,SRU doesn't support PackedSequence.
v3.4.3,-*- coding: utf-8 -*-
v3.4.3,threshold on 1 to avoid div by 0
v3.4.3,treat alignment matrix one by one as each have different lengths
v3.4.3,No alignment if not exist valid tgt token
v3.4.3,get valid alignment (sub-matrix from full paded aligment matrix)
v3.4.3,Helper functions
v3.4.3,Keeps track of the original words/subwords
v3.4.3,('prior_tokenization' option)
v3.4.3,In case there is a final case_markup when new_spacer is on
v3.4.3,########## #
v3.4.3,Translator #
v3.4.3,########## #
v3.4.3,Set translation options
v3.4.3,Build translator from options
v3.4.3,################### #
v3.4.3,Validation iterator #
v3.4.3,################### #
v3.4.3,Reinstantiate the validation iterator
v3.4.3,Retrieve raw references and sources
v3.4.3,########### #
v3.4.3,Predictions #
v3.4.3,########### #
v3.4.3,####### #
v3.4.3,Outputs #
v3.4.3,####### #
v3.4.3,Flatten predictions
v3.4.3,Save results
v3.4.3,-*- coding: utf-8 -*-
v3.4.3,this one is needed for Random Shuffler of batches
v3.4.3,in multi gpu it ensures datasets are read in the same order
v3.4.3,some cudnn methods can be random even after fixing the seed
v3.4.3,unless you tell it to be deterministic
v3.4.3,This one is needed for various tranfroms
v3.4.3,These ensure same initialization in multi gpu mode
v3.4.3,we need to check the model path + any tokenizer path
v3.4.3,patch to log stdout spawned processes of dataloader
v3.4.3,bucket_size = batch_size
v3.4.3,For TRAIN we shuffle batches within the bucket
v3.4.3,otherwise sequential
v3.4.3,for specific case of rnn_packed need to be sorted
v3.4.3,within the batch
v3.4.3,single thread - create batch directly on GPU if device is gpu
v3.4.3,multithread faster to create batch on CPU in each thread and then move it to gpu
v3.4.3,Move tensor_batch from cpu to device
v3.4.3,Check if all tokens have features or none at all
v3.4.3,Make features part of src like
v3.4.3,"{'src': {'src': ..., 'feats': [...., ....]}}"
v3.4.3,careful below it will return a bucket sorted by corpora
v3.4.3,but we sort by length later and shuffle batches
v3.4.3,at this point an example looks like:
v3.4.3,"{'src': {'src': ..., 'feats': [....]},"
v3.4.3,"'tgt': {'tgt': ...},"
v3.4.3,"'src_original': ['tok1', ...'tokn'],"
v3.4.3,"'tgt_original': ['tok1', ...'tokm'],"
v3.4.3,'cid': corpus id
v3.4.3,'cid_line_number' : cid line number
v3.4.3,"'align': ...,"
v3.4.3,}
v3.4.3,Need to add features in last dimensions
v3.4.3,Keep it consistent with dynamic data
v3.4.3,make a small vocab containing just the tokens in the source sequence
v3.4.3,Map source tokens to indices in the dynamic dict.
v3.4.3,-*- coding: utf-8 -*-
v3.4.3,this is hack: if the special separator ï½Ÿnewlineï½ is returned because of the
v3.4.3,"""docify"" transform.get_specials we don't add it if the corresponding newline code"
v3.4.3,is already included in the sentencepiece or BPE-with-gpt2-pretok.
v3.4.3,'src_original' and 'tgt_original' store the
v3.4.3,original line before tokenization. These
v3.4.3,fields are used later on in the feature
v3.4.3,transforms.
v3.4.3,empty example: skip
v3.4.3,bitsandbytes quantize weights when .cuda() is called
v3.4.3,for huge models we need to save Ram
v3.4.3,so we load the weights  module by module and transfer them to GPU for quantization
v3.4.3,bitsandbytes quantize weights when .cuda() is called
v3.4.3,for huge models we need to save Ram
v3.4.3,so we load the weights  module by module and transfer them to GPU for quantization
v3.4.3,"No encoder in LM, seq2seq count formatting kept"
v3.4.3,_check_save_model_path
v3.4.3,This preserves backward-compat for models using customed layernorm
v3.4.3,Force add_ffnbias to True if bias found in model w_1 keys
v3.4.3,fix v2 compatibility
v3.4.3,end of patch for backward compatibility
v3.4.3,!/usr/bin/env python
v3.4.3,!/usr/bin/env python
v3.4.3,!/usr/bin/env python
v3.4.3,-*- coding: utf-8 -*-
v3.4.3,!/usr/bin/env python
v3.4.3,BPE training
v3.4.3,SentencePiece training
v3.4.3,!/usr/bin/env python
v3.4.3,!/usr/bin/env python
v3.4.3,Set sharing strategy manually instead of default based on the OS.
v3.4.3,torch.multiprocessing.set_sharing_strategy('file_system')
v3.4.3,Create a thread to listen for errors in the child processes.
v3.4.3,Train with multiprocessing.
v3.4.3,magic indices
v3.4.3,result caching
v3.4.3,Here we set the decoder to start with self.start (BOS or EOS)
v3.4.3,not 100% necessary to define those
v3.4.3,self.is_finished = torch.zeros(
v3.4.3,"[self.batch_size, self.parallel_paths], dtype=torch.bool"
v3.4.3,)
v3.4.3,fix length constraint and remove eos from count
v3.4.3,add one to account for BOS. Don't account for EOS because hitting
v3.4.3,this implies it hasn't been found.
v3.4.3,we don't block nothing if the user doesn't want it
v3.4.3,we can't block nothing beam's too short
v3.4.3,we check paths one by one
v3.4.3,we don't forbid nothing if the user doesn't want it
v3.4.3,we can't forbid nothing if beam's too short
v3.4.3,Reordering forbidden_tokens following beam selection
v3.4.3,We rebuild a dict to ensure we get the value and not the pointer
v3.4.3,Grabing the newly selected tokens and associated ngram
v3.4.3,skip the blocking if any token in current_ngram is excluded
v3.4.3,"pickups: Tensor where specified index were set to 1, others 0"
v3.4.3,"dropdowns: opposite of pickups, 1 for those shouldn't pick"
v3.4.3,Minus dropdowns to log_probs making probabilities of
v3.4.3,unspecified index close to 0
v3.4.3,"prediction step have surpass length of given target_prefix,"
v3.4.3,no need to further change this attr
v3.4.3,keep indices until overflowing p
v3.4.3,Set all logits that are not in the top-p to -10000.
v3.4.3,This puts the probabilities close to 0.
v3.4.3,Set all logits that are not in the top-k to -10000.
v3.4.3,This puts the probabilities close to 0.
v3.4.3,"For temp=0.0, take the argmax to avoid divide-by-zero errors."
v3.4.3,keep_topk=1 is also equivalent to argmax.
v3.4.3,maybe fix some prediction at this step by modifying log_probs
v3.4.3,"shape: (sum(~ self.is_finished), 1)"
v3.4.3,in LM task src_len is associated with currently generated src
v3.4.3,and therefore needs to follow the generation
v3.4.3,!/usr/bin/env python
v3.4.3,for debugging
v3.4.3,TODO: maybe add dynamic part
v3.4.3,Statistics
v3.4.3,those two should be the same except feat dim
v3.4.3,"batch['src'][perm[j], :, :])"
v3.4.3,trans.src
v3.4.3,we rebuild a small batch made of the sub-segments
v3.4.3,in the long segment.
v3.4.3,new sub-batch ready to be translated
v3.4.3,we re-insert the sub-batch in the initial translations
v3.4.3,For seq2seq when we need to force doc to spit the same number of sents
v3.4.3,In the case of length_penalty = none we report the total logprobs
v3.4.3,divided by the number of sentence to get an approximation of the
v3.4.3,per sentence logprob. We also return the corresponding ppl
v3.4.3,"When a length_penalty is used eg: ""avg"" or ""wu"" since logprobs"
v3.4.3,are normalized per token we report the per line per token logprob
v3.4.3,"and the corresponding ""per word perplexity"""
v3.4.3,Turn any copied words into UNKs.
v3.4.3,"Decoder forward, takes [batch, tgt_len, nfeats] as input"
v3.4.3,"and [batch, src_len, hidden] as enc_out"
v3.4.3,"in case of inference tgt_len = 1, batch = beam times batch_size"
v3.4.3,"in case of Gold Scoring tgt_len = actual length, batch = 1 batch"
v3.4.3,Generator forward.
v3.4.3,"returns [(batch_size x beam_size) , vocab ] when 1 step"
v3.4.3,"or [batch_size, tgt_len, vocab ] when full sentence"
v3.4.3,"here we have scores [tgt_lenxbatch, vocab] or [beamxbatch, vocab]"
v3.4.3,at this point scores is batch first (dim=0)
v3.4.3,"returns [(batch_size x beam_size) , vocab ] when 1 step"
v3.4.3,"or [batch_size, tgt_len, vocab ] when full sentence"
v3.4.3,(0) add BOS and padding to tgt prediction
v3.4.3,(1) Encoder forward.
v3.4.3,(2) Repeat src objects `n_best` times.
v3.4.3,"We use batch_size x n_best, get ``(batch * n_best, src_len, nfeat)``"
v3.4.3,Quick fix. Transformers return None as enc_states.
v3.4.3,enc_states are only used later on to init decoder's state
v3.4.3,"but are never used in Transformer decoder, so we can skip"
v3.4.3,"(3) Init decoder with n_best src,"
v3.4.3,"reshape tgt to ``(len, batch * n_best, nfeat)``"
v3.4.3,it should be done in a better way
v3.4.3,here dec_in is batch first
v3.4.3,masked_select
v3.4.3,get aligned src id for each prediction's valid tgt tokens
v3.4.3,TODO: support these blacklisted features
v3.4.3,(0) Prep the components of the search.
v3.4.3,(1) Run the encoder on the src.
v3.4.3,(2) prep decode_strategy. Possibly repeat src objects.
v3.4.3,(3) Begin decoding step by step:
v3.4.3,Reorder states.
v3.4.3,TODO: support these blacklisted features
v3.4.3,(0) Prep the components of the search.
v3.4.3,(1) split src into src and target_prefix to avoid padding.
v3.4.3,(2) init decoder
v3.4.3,(3) prep decode_strategy. Possibly repeat src objects.
v3.4.3,(4) Begin decoding step by step:
v3.4.3,Reorder states.
v3.4.3,select indexes in model state/cache
v3.4.3,beam parameters
v3.4.3,beam state
v3.4.3,"""global state"" of the old beam"
v3.4.3,buffers for the topk scores and 'backpointer'
v3.4.3,for testing
v3.4.3,maybe fix some prediction at this step by modifying log_probs
v3.4.3,Flatten probs into a list of possibilities.
v3.4.3,after this we get topk_ids between 0 and beam_size*vocab_size
v3.4.3,topk_ids // vocab_size => indice in beam
v3.4.3,topk_ids % vocab_size => true vocab indice
v3.4.3,Store finished hypotheses for this example in the batch.
v3.4.3,End condition is the top beam finished and we can return
v3.4.3,n_best hypotheses.
v3.4.3,early stop when top beam is finished
v3.4.3,Penalize beams that finished.
v3.4.3,this is required to pursue finished beams in non finished batches
v3.4.3,"If all sentences are translated, no need to go further."
v3.4.3,reset the selection for the next step
v3.4.3,assert torch.equal(
v3.4.3,"self.src_len[self.select_indices],"
v3.4.3,"self.src_len.view(_B_old, self.beam_size)[non_finished].view("
v3.4.3,_B_new * self.beam_size
v3.4.3,"),"
v3.4.3,)
v3.4.3,Remove finished batches for the next step.
v3.4.3,here we combine two slections in one
v3.4.3,self.topk_log_probs = self.topk_log_probs[non_finished]
v3.4.3,"self._batch_index = self._batch_index.index_select(0, non_finished)"
v3.4.3,using integer division to get an integer _B without casting
v3.4.3,force the output to be longer than self.min_length
v3.4.3,Multiply probs by the beam probability.
v3.4.3,"if the sequence ends now, then the penalty is the current"
v3.4.3,"length + 1, to include the EOS token"
v3.4.3,Avoid any direction that would repeat unwanted ngrams
v3.4.3,Pick up candidate token by curr_scores
v3.4.3,Recover log probs.
v3.4.3,Length penalty is just a scalar. It doesn't matter if it's applied
v3.4.3,before or after the topk.
v3.4.3,Resolve beam origin and map to batch index flat representation.
v3.4.3,Append last prediction to reordered alive sequence
v3.4.3,update global state (step == 1)
v3.4.3,update global state (step > 1)
v3.4.3,"shape: (batch_size x beam_size, 1)"
v3.4.3,in LM task src_len is associated with currently generated src
v3.4.3,and therefore needs to follow the generation
v3.4.3,Term will be subtracted from probability
v3.4.3,Probability will be divided by this
v3.4.3,these warnings indicate that either the alpha/beta
v3.4.3,"forces a penalty to be a no-op, or a penalty is a no-op but"
v3.4.3,the alpha/beta would suggest otherwise.
v3.4.3,using some coverage penalty
v3.4.3,!/usr/bin/env python
v3.4.3,semaphore doesn't have a timeout arg in Python 2.7
v3.4.3,perform a first request to initialize everything
v3.4.3,backwards compatibility for confs
v3.4.3,every segment becomes a dict for flexibility purposes
v3.4.3,NOTE: translator returns lists of `n_best` list
v3.4.3,build back results with empty texts
v3.4.3,load can be called multiple times: modify copy
v3.4.3,output contain alignment
v3.4.3,Below are all the different penalty terms implemented so far.
v3.4.3,Subtract coverage penalty from topk log probs.
v3.4.3,Divide topk log probs by length penalty.
v3.4.3,These comp lists are costy but less than for loops
v3.4.3,Chinese segmentation
v3.4.3,Chinese simplify -> Chinese traditional standard
v3.4.3,Chinese simplify -> Chinese traditional (HongKong)
v3.4.3,Chinese simplify -> Chinese traditional (Taiwan)
v3.4.3,Chinese traditional -> Chinese simplify (v1)
v3.4.3,Chinese traditional -> Chinese simplify (v2)
v3.4.3,Auto import python files in this directory
v3.4.3,"def custom_stopping_criteria(input_ids, score, **kwargs):"
v3.4.3,"stop_ids = [29871, 13, 13] # \n\n"
v3.4.3,return input_ids[-len(stop_ids)]
v3.4.3,Build the translator (along with the model)
v3.4.3,get prompt and make sure it fits
v3.4.3,"def custom_stopping_criteria(input_ids, score, **kwargs):"
v3.4.3,"stop_ids = [29871, 13, 13] # \n\n"
v3.4.3,return input_ids[-len(stop_ids)]
v3.4.3,Build the translator (along with the model)
v3.4.3,get prompt and make sure it fits
v3.4.2,!/usr/bin/env python
v3.4.2,!/usr/bin/env python
v3.4.2,!/usr/bin/env python
v3.4.2,!/usr/bin/env python
v3.4.2,!/usr/bin/env python
v3.4.2,!/usr/bin/env python3
v3.4.2,-*- coding: utf-8 -*-
v3.4.2,
v3.4.2,"OpenNMT-py documentation build configuration file, created by"
v3.4.2,sphinx-quickstart on Sun Dec 17 12:07:14 2017.
v3.4.2,
v3.4.2,This file is execfile()d with the current directory set to its
v3.4.2,containing dir.
v3.4.2,
v3.4.2,Note that not all possible configuration values are present in this
v3.4.2,autogenerated file.
v3.4.2,
v3.4.2,All configuration values have a default; values that are commented out
v3.4.2,serve to show the default.
v3.4.2,"If extensions (or modules to document with autodoc) are in another directory,"
v3.4.2,add these directories to sys.path here. If the directory is relative to the
v3.4.2,"documentation root, use os.path.abspath to make it absolute, like shown here."
v3.4.2,
v3.4.2,import os
v3.4.2,import sys
v3.4.2,"sys.path.insert(0, os.path.abspath('.'))"
v3.4.2,-- General configuration ------------------------------------------------
v3.4.2,"If your documentation needs a minimal Sphinx version, state it here."
v3.4.2,
v3.4.2,needs_sphinx = '6.0'
v3.4.2,"Add any Sphinx extension module names here, as strings. They can be"
v3.4.2,extensions coming with Sphinx (named 'sphinx.ext.*') or your custom
v3.4.2,ones.
v3.4.2,Show base classes
v3.4.2,"Use ""variables"" section for Attributes instead of weird block things"
v3.4.2,mimicking the function style.
v3.4.2,"Add any paths that contain templates here, relative to this directory."
v3.4.2,The suffix(es) of source filenames.
v3.4.2,You can specify multiple suffix as a list of string:
v3.4.2,
v3.4.2,"source_suffix = ['.rst', '.md']"
v3.4.2,The master toctree document.
v3.4.2,General information about the project.
v3.4.2,"The version info for the project you're documenting, acts as replacement for"
v3.4.2,"|version| and |release|, also used in various other places throughout the"
v3.4.2,built documents.
v3.4.2,
v3.4.2,The short X.Y version.
v3.4.2,"The full version, including alpha/beta/rc tags."
v3.4.2,The language for content autogenerated by Sphinx. Refer to documentation
v3.4.2,for a list of supported languages.
v3.4.2,
v3.4.2,This is also used if you do content translation via gettext catalogs.
v3.4.2,"Usually you set ""language"" from the command line for these cases."
v3.4.2,"List of patterns, relative to source directory, that match files and"
v3.4.2,directories to ignore when looking for source files.
v3.4.2,This patterns also effect to html_static_path and html_extra_path
v3.4.2,The name of the Pygments (syntax highlighting) style to use.
v3.4.2,"If true, `todo` and `todoList` produce output, else they produce nothing."
v3.4.2,-- Options for HTML output ----------------------------------------------
v3.4.2,The theme to use for HTML and HTML Help pages.  See the documentation for
v3.4.2,a list of builtin themes.
v3.4.2,
v3.4.2,html_theme = 'sphinx_materialdesign_theme'
v3.4.2,html_theme_path = [sphinx_materialdesign_theme.get_path()]
v3.4.2,Theme options are theme-specific and customize the look and feel of a theme
v3.4.2,"further.  For a list of options available for each theme, see the"
v3.4.2,documentation.
v3.4.2,
v3.4.2,html_theme_options = {}
v3.4.2,"Add any paths that contain custom static files (such as style sheets) here,"
v3.4.2,"relative to this directory. They are copied after the builtin static files,"
v3.4.2,"so a file named ""default.css"" will overwrite the builtin ""default.css""."
v3.4.2,"Custom sidebar templates, must be a dictionary that maps document names"
v3.4.2,to template names.
v3.4.2,
v3.4.2,This is required for the alabaster theme
v3.4.2,refs: http://alabaster.readthedocs.io/en/latest/installation.html#sidebars
v3.4.2,-- Options for HTMLHelp output ------------------------------------------
v3.4.2,Output file base name for HTML help builder.
v3.4.2,-- Options for LaTeX output ---------------------------------------------
v3.4.2,The paper size ('letterpaper' or 'a4paper').
v3.4.2,
v3.4.2,"'papersize': 'letterpaper',"
v3.4.2,"The font size ('10pt', '11pt' or '12pt')."
v3.4.2,
v3.4.2,"'pointsize': '10pt',"
v3.4.2,Additional stuff for the LaTeX preamble.
v3.4.2,
v3.4.2,"'preamble': '',"
v3.4.2,Latex figure (float) alignment
v3.4.2,
v3.4.2,"'figure_align': 'htbp',"
v3.4.2,Grouping the document tree into LaTeX files. List of tuples
v3.4.2,"(source start file, target name, title,"
v3.4.2,"author, documentclass [howto, manual, or own class])."
v3.4.2,-- Options for manual page output ---------------------------------------
v3.4.2,One entry per manual page. List of tuples
v3.4.2,"(source start file, name, description, authors, manual section)."
v3.4.2,-- Options for Texinfo output -------------------------------------------
v3.4.2,Grouping the document tree into Texinfo files. List of tuples
v3.4.2,"(source start file, target name, title, author,"
v3.4.2,"dir menu entry, description, category)"
v3.4.2,Build the translator (along with the model)
v3.4.2,Put messages sizes in antichronological order
v3.4.2,Caluculate antichronological history sizes
v3.4.2,Prune the history from the beginning
v3.4.2,Put back indices in chronological order.
v3.4.2,Build the translator (along with the model)
v3.4.2,We need to build the Llama tokenizer to count tokens and prune the history.
v3.4.2,The hypotheses are lists of one element but we still need to take the first one.
v3.4.2,#####
v3.4.2,UI #
v3.4.2,#####
v3.4.2,What are the 3 best french cities ?
v3.4.2,Which one is better if I like outdoor activities ?
v3.4.2,Which one is better if I like cultural outings?
v3.4.2,What are the best neighborhoods in these 5 cities?
v3.4.2,!/usr/bin/env python3
v3.4.2,Usage: python3 filter_train.py in.src in.trg out.src out.trg max-tokens
v3.4.2,flake8: noqa
v3.4.2,-*- coding: utf-8 -*-
v3.4.2,Generated by the protocol buffer compiler.  DO NOT EDIT!
v3.4.2,source: sentencepiece_model.proto
v3.4.2,@@protoc_insertion_point(imports)
v3.4.2,@@protoc_insertion_point(module_scope)
v3.4.2,!/usr/bin/env python
v3.4.2,-*- coding: utf-8 -*-
v3.4.2,is this reachable?
v3.4.2,Read in embeddings
v3.4.2,Write to file
v3.4.2,converts a SentencePiece vocabulary to the format expected by dynamic data
v3.4.2,"(essentially converts float expected counts to ""fixed precision"" int pseudo"
v3.4.2,counts)
v3.4.2,from onmt.utils.misc import use_gpu
v3.4.2,"Add in default model arguments, possibly added since training."
v3.4.2,this patch is no longer needed included in converter
v3.4.2,"if hasattr(model_opt, 'rnn_size'):"
v3.4.2,model_opt.hidden_size = model_opt.rnn_size
v3.4.2,build_base_model expects updated and validated opts
v3.4.2,-*- encoding: utf-8 -*-
v3.4.2,!/usr/bin/env python
v3.4.2,Falcon stores QKV in one single tensor but it is not simply piled up Q+K+V
v3.4.2,it is heads interleaved to we need to slice first
v3.4.2,also it uses the HF rotary so we need to permute Q and K interleave
v3.4.2,!/usr/bin/env python
v3.4.2,-*- coding: utf-8 -*-
v3.4.2,Author: Rico Sennrich
v3.4.2,flake8: noqa
v3.4.2,This file is retrieved from https://github.com/rsennrich/subword-nmt
v3.4.2,hack for python2/3 compatibility
v3.4.2,check version information
v3.4.2,some hacking to deal with duplicates (only consider first instance)
v3.4.2,don't print end-of-word symbols
v3.4.2,sys.stderr.write('cannot split {0} further.\n'.format(segment))
v3.4.2,sys.stderr.write('OOV: {0}\n'.format(segment))
v3.4.2,sys.stderr.write('OOV: {0}\n'.format(segment))
v3.4.2,python 2/3 compatibility
v3.4.2,read/write files as UTF-8
v3.4.2,!/usr/bin/env python3
v3.4.2,coding: utf-8
v3.4.2,"In order to use this tool, please install comet first"
v3.4.2,https://github.com/Unbabel/COMET
v3.4.2,Let's say you have a source file with N sentences in SL - eg: source.sl
v3.4.2,and the corresponding references (N sentences) reference.tl
v3.4.2,Translate your file in TL with the -n_best nbest options nbest being
v3.4.2,then number of hypotheses and output the target to -output target.nbest.tl
v3.4.2,Then you need to duplicate source and reference sentences nbest times
v3.4.2,for this script.
v3.4.2,for instance using awk '{for(i=1; i<=n; i++) print}' n=5 reference.tl \
v3.4.2,> reference.5.tl
v3.4.2,same for source.
v3.4.2,This script can be run (for instance with nbest = 5) as follows:
v3.4.2,python oracle_bleu.py --nbest-src source.5.sl --nbest-hyp target.5.tl \
v3.4.2,--nbest-ref reference.5.tl --nbest-order 5 --output target.maxbleu.tl
v3.4.2,It will search in all hyp the best comet score
v3.4.2,when choosing a reference-less model no nbest-ref is required
v3.4.2,for nbest in nbests:
v3.4.2,!/usr/bin/env python
v3.4.2,!/usr/bin/env python3
v3.4.2,coding: utf-8
v3.4.2,Let's say you have a source file with N sentences in SL - eg: source.sl
v3.4.2,Translate your file in TL with the -n_best nbest options nbest being
v3.4.2,then number of hypotheses and output the target to -output target.nbest.tl
v3.4.2,This script can be run (for instance with nbest = 5) as follows:
v3.4.2,python mbr_bleu.py --nbest-hyp target.5.tl \
v3.4.2,--nbest-order 5 --output target.mbr.tl
v3.4.2,It will compare all hyp with eachother and output the max bleu
v3.4.2,!/usr/bin/env python
v3.4.2,!/usr/bin/env python
v3.4.2,-*- coding: utf-8 -*-
v3.4.2,Author: Rico Sennrich
v3.4.2,flake8: noqa
v3.4.2,This file is retrieved from https://github.com/rsennrich/subword-nmt
v3.4.2,hack for python2/3 compatibility
v3.4.2,"find all instances of pair, and update frequency/indices around it"
v3.4.2,find first symbol
v3.4.2,"if first symbol is followed by second symbol, we've found an occurrence of pair (old_word[i:i+2])"
v3.4.2,"assuming a symbol sequence ""A B C"", if ""B C"" is merged, reduce the frequency of ""A B"""
v3.4.2,"assuming a symbol sequence ""A B C B"", if ""B C"" is merged, reduce the frequency of ""C B""."
v3.4.2,"however, skip this if the sequence is A B C B C, because the frequency of ""C B"" will be reduced by the previous code block"
v3.4.2,find new pair
v3.4.2,"assuming a symbol sequence ""A BC D"", if ""B C"" is merged, increase the frequency of ""A BC"""
v3.4.2,"assuming a symbol sequence ""A BC B"", if ""B C"" is merged, increase the frequency of ""BC B"""
v3.4.2,"however, if the sequence is A BC BC, skip this step because the count of ""BC BC"" will be incremented by the previous code block"
v3.4.2,data structure of pair frequencies
v3.4.2,index from pairs to words
v3.4.2,version 0.2 changes the handling of the end-of-word token ('</w>');
v3.4.2,version numbering allows bckward compatibility
v3.4.2,"threshold is inspired by Zipfian assumption, but should only affect speed"
v3.4.2,we probably missed the best pair because of pruning; go back to full statistics
v3.4.2,"threshold is inspired by Zipfian assumption, but should only affect speed"
v3.4.2,python 2/3 compatibility
v3.4.2,read/write files as UTF-8
v3.4.2,Now we can pipe the full file through the model using the Iterator
v3.4.2,reminder a batch includes .src .tgt .indices and it is sorted
v3.4.2,Compute and retrieve the loss for EACH sentence
v3.4.2,Now we need to rearrange the batch of ppl
v3.4.2,in the original order with indices
v3.4.2,!/usr/bin/env python
v3.4.2,-*- coding: utf-8 -*-
v3.4.2,!/usr/bin/env python
v3.4.2,!/usr/bin/env python
v3.4.2,!/usr/bin/env python
v3.4.2,!/usr/bin/env python
v3.4.2,!/usr/bin/env python
v3.4.2,!/usr/bin/env python3
v3.4.2,coding: utf-8
v3.4.2,Let's say you have a source file with N sentences in SL - eg: source.sl
v3.4.2,and the corresponding references (N sentences) reference.tl
v3.4.2,Translate your file in TL with the -n_best nbest options nbest being
v3.4.2,then number of hypotheses and output the target to -output target.nbest.tl
v3.4.2,Then you need to duplicate reference sentences nbest times for this script.
v3.4.2,for instance using awk '{for(i=1; i<=n; i++) print}' n=5 reference.tl \
v3.4.2,> reference.5.tl
v3.4.2,This script can be run (for instance with nbest = 5) as follows:
v3.4.2,python oracle_bleu.py --nbest-hyp target.5.tl --nbest-ref reference.5.tl \
v3.4.2,--nbest-order 5 --output target.maxbleu.tl
v3.4.2,It will search in all hyp the best bleu wrt reference
v3.4.2,and output the max bleu
v3.4.2,!/usr/bin/env python
v3.4.2,with the two module = imp.load_source() below
v3.4.2,we ghost the old torchtext.data.field and depercated
v3.4.2,onmt.inputters.text_dataset
v3.4.2,however this require some functions / classes to be
v3.4.2,monkey patched for loading the old field/vocab objects.
v3.4.2,"module3 = imp.load_source(""Vocab"", ""tools/convertv2_v3.py"")"
v3.4.2,"voc = dict(sorted(fields.vocab.__dict__['freqs'].items(),"
v3.4.2,"key=lambda x: (-x[1], x[0]))).keys()"
v3.4.2,"voc = dict(sorted(fields.vocab.__dict__['freqs'].items(),"
v3.4.2,"key=lambda x: (-x[1], x[0]))).keys()"
v3.4.2,!/usr/bin/env python
v3.4.2,redpajama stores QKV in one single tensor but it is not simply piled up Q+K+V
v3.4.2,it is heads interleaved to we need to slice first
v3.4.2,also it uses the HF rotary so we need to permute Q and K interleave
v3.4.2,Avoid functionality on inference
v3.4.2,weights are in the .pt file
v3.4.2,weights are not in the .pt checkpoint but stored in the safetensors file
v3.4.2,Build embeddings.
v3.4.2,Build encoder.
v3.4.2,Build embeddings.
v3.4.2,Build decoder.
v3.4.2,Share the embedding matrix - preprocess with share_vocab required.
v3.4.2,src/tgt vocab should be the same if `-share_vocab` is specified.
v3.4.2,Update vocabulary embeddings with checkpoint embeddings
v3.4.2,Embedding layers
v3.4.2,Just for debugging purposes
v3.4.2,Remove old vocabulary associated embeddings
v3.4.2,for back compat when attention_dropout was not defined
v3.4.2,Build Model
v3.4.2,Build Generator.
v3.4.2,If new training initialize the model params
v3.4.2,If update_vocab init also but checkpoint will overwrite old weights
v3.4.2,ONLY for legacy fusedam with amp pytorch requires NOT to half the model
v3.4.2,Update model embeddings with those from the checkpoint
v3.4.2,after initialization
v3.4.2,after this checkpoint contains no embeddings
v3.4.2,when using LoRa or updating the vocab (no more embeddings in ckpt)
v3.4.2,=> strict=False when loading state_dict
v3.4.2,weights are in the .pt file
v3.4.2,weights are not in the .pt checkpoint but stored in the safetensors file
v3.4.2,!/usr/bin/env python
v3.4.2,if transform + options set in 'valid' we need to copy in main
v3.4.2,transform / options for scoring considered as inference
v3.4.2,"maybe prepare pretrained embeddings, if any"
v3.4.2,Load checkpoint if we resume from a previous training.
v3.4.2,ensure tensorboard output is written in the directory
v3.4.2,of previous checkpoints
v3.4.2,Override checkpoint's update_embeddings as it defaults to false
v3.4.2,Override checkpoint's freezing settings as it defaults to false
v3.4.2,NOTE: It's important that ``opt`` has been validated and updated
v3.4.2,at this point.
v3.4.2,Build model.
v3.4.2,Build optimizer.
v3.4.2,Build model saver
v3.4.2,Use Tensorboard for visualization during training
v3.4.2,Options only during inference
v3.4.2,"Truncation options, for text corpus"
v3.4.2,"as for False, this will be added in _add_train_general_opts"
v3.4.2,GPU
v3.4.2,Embedding Options
v3.4.2,Model Task Options
v3.4.2,Encoder-Decoder Options
v3.4.2,Freeze Encoder and/or Decoder
v3.4.2,The following options (bridge_extra_node to n_steps) are used
v3.4.2,for training with --encoder_type ggnn (Gated Graph Neural Network).
v3.4.2,Attention options
v3.4.2,Alignement options
v3.4.2,Generator and loss options.
v3.4.2,LoRa
v3.4.2,Init options
v3.4.2,Pretrained word vectors
v3.4.2,Freeze word vectors
v3.4.2,Optimization options
v3.4.2,learning rate
v3.4.2,options relate to data preprare
v3.4.2,options relate to train
v3.4.2,Alpha and Beta values for Google Length + Coverage penalty
v3.4.2,"Described here: https://arxiv.org/pdf/1609.08144.pdf, Section 7"
v3.4.2,Length penalty options
v3.4.2,Coverage penalty options
v3.4.2,Decoding Length constraint
v3.4.2,Decoding content constraint
v3.4.2,Adding options related to source and target features
v3.4.2,Adding options relate to decoding strategy
v3.4.2,Adding option for logging
v3.4.2,Adding options related to Transforms
v3.4.2,Copyright 2016 The Chromium Authors. All rights reserved.
v3.4.2,Use of this source code is governed by a BSD-style license that can be
v3.4.2,found in the LICENSE file.
v3.4.2,"Get the key 'value' in the dict, or just use 'value'"
v3.4.2,Create a thread to listen for errors in the child processes.
v3.4.2,Build translator
v3.4.2,Build vocab
v3.4.2,Build transform pipe
v3.4.2,Basic attributes.
v3.4.2,Set model in training mode.
v3.4.2,Let's clean the GPUs before training loop
v3.4.2,UPDATE DROPOUT
v3.4.2,Run patience mechanism
v3.4.2,"If the patience has reached the limit, stop training"
v3.4.2,swap model params w/ moving average
v3.4.2,(and keep the original parameters)
v3.4.2,Set model in validating mode.
v3.4.2,raw_srcs = []
v3.4.2,raw_refs = []
v3.4.2,F-prop through the model.
v3.4.2,Compute loss.
v3.4.2,Compute validation metrics (at batch.dataset level)
v3.4.2,Compute stats
v3.4.2,Update statistics.
v3.4.2,Set model back to training mode.
v3.4.2,Truncated BPTT: reminder not compatible with accum > 1
v3.4.2,1. Create truncated target.
v3.4.2,2. F-prop all but generator.
v3.4.2,3. Compute loss.
v3.4.2,The loss of the prompt will be set to zero.
v3.4.2,"If truncated, don't backprop fully."
v3.4.2,"in case of multi step gradient accumulation,"
v3.4.2,update only after accum batches
v3.4.2,For Flake
v3.4.2,we avoid padding while mean pooling
v3.4.2,incoming and outgoing edge embedding
v3.4.2,Find vocab data for tree builting
v3.4.2,Propogation Model
v3.4.2,Initialize the bridge layer
v3.4.2,Token embedding
v3.4.2,Initialize graph using formatted input sequence
v3.4.2,Number of flagged nodes defines node count for this sample
v3.4.2,"(Nodes can have no flags on them, but must be in 'flags' list)."
v3.4.2,The total number of integers in the vocab should allow
v3.4.2,for all features and edges to be defined.
v3.4.2,Use first extra node as only source for decoder init
v3.4.2,Average all nodes to get bridge input
v3.4.2,"LSTM has hidden and cell state, other only one"
v3.4.2,Total number of states
v3.4.2,Build a linear layer for each
v3.4.2,Initialize the bridge layer
v3.4.2,src lengths data is wrapped inside a Tensor.
v3.4.2,"LSTM has hidden and cell state, other only one"
v3.4.2,Total number of states
v3.4.2,Build a linear layer for each
v3.4.2,Auto import python files in this directory
v3.4.2,batch x len x dim
v3.4.2,"feed_forward applies residual, so we remove and apply residual with un-normed"
v3.4.2,mask is now (batch x 1 x slen x slen)
v3.4.2,1 to be expanded to number of heads in MHA
v3.4.2,Run the forward pass of every layer of the tranformer.
v3.4.2,Dimensions and padding for constructing the word embedding matrix
v3.4.2,Dimensions and padding for feature embedding matrices
v3.4.2,(these have no effect if feat_vocab_sizes is empty)
v3.4.2,The embedding matrix look-up tables. The first look-up table
v3.4.2,"is for words. Subsequent ones are for features, if any exist."
v3.4.2,The final output size of word + feature vectors. This can vary
v3.4.2,from the word vector size if and only if features are defined.
v3.4.2,This is the attribute you should access if you need to know
v3.4.2,how big your embeddings are going to be.
v3.4.2,The sequence of operations that converts the input sequence
v3.4.2,into a sequence of embeddings. At minimum this consists of
v3.4.2,looking up the embeddings for each word and feature in the
v3.4.2,input. Model parameters may require the sequence to contain
v3.4.2,additional operations as well.
v3.4.2,features must use word_vec_size
v3.4.2,features will use feat_vec_size
v3.4.2,Some utilitary functions for pretrained embeddings
v3.4.2,is this reachable?
v3.4.2,Write to file
v3.4.2,set the opt in place
v3.4.2,set the opt in place
v3.4.2,flake8: noqa
v3.4.2,For command-line option parsing
v3.4.2,"Check pass, set the args."
v3.4.2,"This SRU version implements its own cuda-level optimization,"
v3.4.2,so it requires that:
v3.4.2,1. `cupy` and `pynvrtc` python package installed.
v3.4.2,2. pytorch is built with cuda support.
v3.4.2,3. library path set: export LD_LIBRARY_PATH=<cuda lib path>.
v3.4.2,Check 1.
v3.4.2,Check 2.
v3.4.2,Check 3.
v3.4.2,This sets up device to use.
v3.4.2,-> directions x batch x dim
v3.4.2,For DEBUG
v3.4.2,"size = (length, batch, x.size(-1)) \"
v3.4.2,"if x.dim() == 3 else (batch, x.size(-1))"
v3.4.2,grad_x = x.new(*x.size()) if k_ == 3 else x.new(*size).zero_()
v3.4.2,Normal use
v3.4.2,"An entry check here, will catch on train side and translate side"
v3.4.2,if requirements are not satisfied.
v3.4.2,RNNDecoderState wraps hidden as a tuple.
v3.4.2,fh -> (layers*directions) x batch x dim
v3.4.2,This class is mainly used by decoder.py for RNNs but also
v3.4.2,by the CNN / transformer decoder when copy attention is used
v3.4.2,CNN has its own attention mechanism ConvMultiStepAttention
v3.4.2,Transformer has its own MultiHeadedAttention
v3.4.2,mlp wants it with bias
v3.4.2,"(batch, t_len, d) x (batch, d, s_len) --> (batch, t_len, s_len)"
v3.4.2,"(batch, t_len, s_len, d)"
v3.4.2,one step input
v3.4.2,"compute attention scores, as in Luong et al."
v3.4.2,Softmax or sparsemax to normalize attention weights
v3.4.2,each context vector c_t is the weighted average
v3.4.2,over all the source hidden states
v3.4.2,concatenate
v3.4.2,clamping necessary because of numerical errors: loss should be lower
v3.4.2,"bounded by zero, but negative values near zero are possible without"
v3.4.2,the clamp
v3.4.2,Help functions for Rotary Embeddings
v3.4.2,https://arxiv.org/pdf/2104.09864.pdf
v3.4.2,too convoluted to make maxseqlen a parameter.
v3.4.2,we suppose src_seq_len at training and max_length at inference
v3.4.2,are both < 2048 tokens.
v3.4.2,"rope is now matrix [maxseqlen, dim/2]"
v3.4.2,Help functions for max_relative positions
v3.4.2,https://arxiv.org/abs/1803.02155
v3.4.2,Shift values to be >= 0
v3.4.2,"now relative_position is in the range [0, inf)"
v3.4.2,half of the buckets are for exact increments in positions
v3.4.2,The other half of the buckets are for logarithmically bigger bins in positions
v3.4.2,up to max_distance
v3.4.2,Help functions to split model dim per head
v3.4.2,https://arxiv.org/pdf/1803.02155.pdf
v3.4.2,in the paper they suggest either two embeds
v3.4.2,relative_key / relative_value or only
v3.4.2,relative_key. We implemented the same embed
v3.4.2,for both.
v3.4.2,"1) Project key, value, and query."
v3.4.2,as a reminder at training layer_cache[0] remains False
v3.4.2,expand key on heads dimension when it's less than query heads (multi-query variant)
v3.4.2,expand value on heads dimension when it's less than query heads (multi-query variant)
v3.4.2,"2) When standard pos. enc. or rotary, use flash attention"
v3.4.2,Ultimately flashv2 will be part of pytorch https://github.com/pytorch/pytorch/pull/105602
v3.4.2,"In the meantime: if vanilla tranformer or Rotary embeddings (not rel_pos, not alibi)"
v3.4.2,then use flash2 if seq len > 256 otherwise use xtransformer from pt2 uptream
v3.4.2,batch x num_heads x query_len x key_len
v3.4.2,1 or key_len x key_len
v3.4.2,1 or key_len x key_len x dim_per_head
v3.4.2,not 100% necessary but expand to nb of heads
v3.4.2,now mask and scores have the same shape
v3.4.2,3) Apply attention dropout and compute context vectors.
v3.4.2,We use the same embeddings for key and value
v3.4.2,--------------------------------------------------------------------------
v3.4.2,copied and adapted https://github.com/microsoft/LoRA/
v3.4.2,Copyright (c) Microsoft Corporation. All rights reserved.
v3.4.2,Licensed under the MIT License (MIT).
v3.4.2,Support bnb quantization of nderlying layers
v3.4.2,--------------------------------------------------------------------------
v3.4.2,Optional dropout
v3.4.2,Mark the weight as unmerged
v3.4.2,LoRA implemented in a dense layer
v3.4.2,Actual trainable parameters
v3.4.2,Freezing the pre-trained weight matrix
v3.4.2,initialize A the same way as the default
v3.4.2,for nn.Linear and B to zero
v3.4.2,Make sure that the weights are not merged
v3.4.2,Merge the weights and mark it
v3.4.2,Actual trainable parameters
v3.4.2,Freezing the pre-trained weight matrix
v3.4.2,we do not super().reset_parameters() save lot of time and useless when no grad.
v3.4.2,initialize A the same way as the default
v3.4.2,for nn.Linear and B to zero
v3.4.2,Make sure that the weights are not merged
v3.4.2,Merge the weights and mark it
v3.4.2,cannot merge/unmerge quantized weigts with unquantized lora_X
v3.4.2,Check if QLoraLinear has a custom __init__ method
v3.4.2,Invoke the __init__ method of QLoraLinear
v3.4.2,LoRA implemented in a dense layer
v3.4.2,At the moment this class is only used by embeddings.Embeddings look-up tables
v3.4.2,for some reason list comprehension is slower in this scenario
v3.4.2,"for silu, see: https://arxiv.org/pdf/2002.05202.pdf"
v3.4.2,-*- coding: utf-8 -*-
v3.4.2,class AverageAttention(torch.jit.ScriptModule):
v3.4.2,@torch.jit.script
v3.4.2,Code taken from bitsandbytes but modified with arg device to accept skipt_init
v3.4.2,from torch.nn.utils => makes model building way faster.
v3.4.2,"weights are cast automatically as Int8Params, but the bias has to be cast manually"
v3.4.2,reorder weight layout back from ampere/turing to row
v3.4.2,"we only need to save SCB as extra data, because CB for quantized weights"
v3.4.2,is already stored in weight.data
v3.4.2,"case 1: .cuda was called, SCB is in self.weight"
v3.4.2,"case 2: self.init_8bit_state was called, SCB is in self.state"
v3.4.2,"buffers not yet initialized, can't call them directly without"
v3.4.2,"weights are cast automatically as Int8Params, but the bias has to be cast manually"
v3.4.2,we converted 8-bit row major to turing/ampere format in the first inference pass
v3.4.2,we no longer need the row-major weight
v3.4.2,out_features * in_features
v3.4.2,norm is out_features * 1
v3.4.2,batch_size * out_features
v3.4.2,out_features
v3.4.2,out_features
v3.4.2,batch_size * out_features
v3.4.2,"out_channels, in_channels // groups, * kernel_size"
v3.4.2,out_features
v3.4.2,This is used nowhere in the code at the moment (Vincent Nguyen 05/18/2018)
v3.4.2,"in_channels, out_channels, *kernel_size"
v3.4.2,"in_channels, out_channels, *kernel_size"
v3.4.2,"self.out_channels, 1"
v3.4.2,out_features
v3.4.2,out_features
v3.4.2,store roots on diagonal
v3.4.2,Original probabilities.
v3.4.2,Probability of copying p(z=1) batch.
v3.4.2,Probability of not copying: p_{word}(w) * (1 - p(z))
v3.4.2,probabilities assigned by the model to the gold targets
v3.4.2,probability of tokens copied from source
v3.4.2,Set scores for unk to 0 and add eps
v3.4.2,find the indices in which you do not use the copy mechanism
v3.4.2,Drop padding.
v3.4.2,We exclude tokenization for contractions in
v3.4.2,order to avoid inconsistencies with pyonmtok's tokenization.
v3.4.2,"(e.g. ""I ca n't"" with spacy, ""I can ' t"" with pyonmttok)"
v3.4.2,Use Spacy's stopwords to get rid of junk entries
v3.4.2,Perform tokenization with spacy for consistency.
v3.4.2,We ensure that the target lemma is present in the lemmatized
v3.4.2,"target string, that the match is an exact match (there is"
v3.4.2,whitespace before or after the term)
v3.4.2,and we perform some bound checking.
v3.4.2,Map the lemmatized string match index to
v3.4.2,the lemmatized list index
v3.4.2,We need to know if the term is multiword
v3.4.2,Join multiword target lemmas with a unique separator so
v3.4.2,we can treat them as single word and not change the indices.
v3.4.2,Construct the final source from the lemmatized list
v3.4.2,that contains the terms. We compare the tokens in the
v3.4.2,term-augmented lemma list with the tokens in the original
v3.4.2,"lemma list. If the lemma is the same, then we replace with"
v3.4.2,the token from the original tokenized source list. If they
v3.4.2,"are not the same, it means the lemma has been augemented"
v3.4.2,"with a term, so we inject this in the final list."
v3.4.2,Restore the spaces in multi-word terms
v3.4.2,Skip half examples to improve performance. This means we set
v3.4.2,"a hard limit for the `term_corpus_ratio` to 0.5, which is actually"
v3.4.2,quite high. TODO: We can add this (skipping examples) as an option
v3.4.2,Filter out very short or very long sentences
v3.4.2,from the TM for better performance
v3.4.2,We split the `batch` and perform fuzzy matching
v3.4.2,in smaller chunks of 10.000 examples in order to
v3.4.2,reduce memory usage.
v3.4.2,Perfomance is not affected.
v3.4.2,Probably redundant but let's be safe
v3.4.2,in case some examples are already fuzzied
v3.4.2,(e.g. from another pipeline or workflow)
v3.4.2,We don't want exact matches
v3.4.2,Apply a basic filtering to leave out very short or very long
v3.4.2,sentences and speed up things a bit during fuzzy matching
v3.4.2,Do nothing
v3.4.2,We set the start number of tags to a random number from 1
v3.4.2,to 12 + the number of subsequent tags that
v3.4.2,will be added. We also apply weights to this choice so tags
v3.4.2,"are more probable to start from 1, then from 2, etc."
v3.4.2,This way we cover most scenarios met in real usage and
v3.4.2,the system will learn to handle a fairly large number of
v3.4.2,numbered tags (but not an excessively large number)
v3.4.2,Make sure we only search for exact matches (we don't want
v3.4.2,to match part of words) and perform some bound checking
v3.4.2,Create all possible tag forms. We inject a special
v3.4.2,unicode char (âˆ¥) as a placeholder for whitespace in order
v3.4.2,to keep the indices unaltered. This char is replaced with
v3.4.2,spaces before we return the augmented examples.
v3.4.2,Make a weighted choice between paired tags or single tags.
v3.4.2,"We usually encounter, and thus here we favor, paired tags"
v3.4.2,with a ratio 1/3.
v3.4.2,Check if the tags include the
v3.4.2,"mandatory ""#"" number placeholder"""
v3.4.2,We split the user-defined tags in the # placeholder
v3.4.2,in order to number them
v3.4.2,Skip half examples to speed up the transform. This sets
v3.4.2,"a hard limit of 0.5 to the `tags_corpus_ratio`, which is"
v3.4.2,excessive and should be avoided anyway.
v3.4.2,normalize dict src/tgt for each dataset
v3.4.2,"print(""src empty"")"
v3.4.2,"print(""too many same char in src"")"
v3.4.2,"print(""too many same word in src"")"
v3.4.2,"print(""avg token min"", len(src_str) / len(ex['src']))"
v3.4.2,"print(""avg token max"", len(src_str) / len(ex['src']))"
v3.4.2,"print(""text does not fully belong to wanted script"")"
v3.4.2,"print(""Some text belong to unwanted scripts"")"
v3.4.2,"print(""langid does not match"", _id(src_str))"
v3.4.2,"print(""src = tgt"")"
v3.4.2,"print(""tgt empty"")"
v3.4.2,"print(""src / tgt ratio "", len(src_str) / len(tgt_str))"
v3.4.2,"print(""too many same char in tgt"")"
v3.4.2,"print(""too many same word in tgt"")"
v3.4.2,"print(""avg token min"", len(tgt_str) / len(ex['tgt']))"
v3.4.2,"print(""avg token max"", len(tgt_str) / len(ex['tgt']))"
v3.4.2,"print(""text does not fully belong to wanted script"")"
v3.4.2,"print(""Some text belong to unwanted scripts"")"
v3.4.2,"print(""langid does not match"", _id(tgt_str))"
v3.4.2,"doc break we add it, restart new doc"
v3.4.2,case 1st ex is already longer
v3.4.2,adding cur ex is too long we add cur doc
v3.4.2,and reset doc to cur ex
v3.4.2,we start the new doc with cur ex
v3.4.2,we cumulate cur ex to cur doc
v3.4.2,Auto import python files in this directory
v3.4.2,1. sample number of tokens to corrupt
v3.4.2,2. sample positions to corrput
v3.4.2,3. sample corrupted values
v3.4.2,1. sample number of tokens to corrupt
v3.4.2,2. sample positions to corrput
v3.4.2,3. Drop token on chosen position
v3.4.2,1. sample number of tokens to corrupt
v3.4.2,2. sample positions to corrput
v3.4.2,3. mask word on chosen position
v3.4.2,"Sharing options among `TokenizerTransform`s, same name conflict in"
v3.4.2,this scope will be resolved by remove previous occurrence in parser
v3.4.2,subword regularization(or BPE dropout) options:
v3.4.2,subword vocabulary restriction options:
v3.4.2,This method embeds a custom logic to correctly handle certain placeholders
v3.4.2,in case the tokenizer doesn't preserve them.
v3.4.2,Locate the end-of-sentence placeholders.
v3.4.2,Tokenize each sentence separately.
v3.4.2,Locate the mask-before placeholders
v3.4.2,(to zero-out the prompt loss during LM finetuning).
v3.4.2,Tokenize each chunk separately and insert the padding token.
v3.4.2,between each sequence of tokens.
v3.4.2,Re-insert the eos token.
v3.4.2,derterministic subwording
v3.4.2,subword sampling when nbest_size > 1 or -1
v3.4.2,alpha should be 0.0 < alpha < 1.0
v3.4.2,Load vocabulary file if provided and set threshold
v3.4.2,Load Subword Model
v3.4.2,-1: keep everything (i.e. 1 mask per token)
v3.4.2,0: replace everything (i.e. no mask)
v3.4.2,1: 1 mask per span
v3.4.2,view each subword as word start / input is word level token
v3.4.2,Pretend it ends with a full stop so last span is a sentence
v3.4.2,"Tokens that are full stops, where the previous token is not"
v3.4.2,Make sure we have enough to mask
v3.4.2,Trim to masking budget
v3.4.2,Handle 0-length mask (inserts) separately
v3.4.2,assert is_word_start[-1] == 0
v3.4.2,assert tokens_length - 1 not in indices
v3.4.2,"keep index, but replace it with [MASK]"
v3.4.2,"acts as a long length, so spans don't go over the end of doc"
v3.4.2,next position from each word_start
v3.4.2,delete token: 1 mask/remove per span
v3.4.2,"keep index, but replace it with [MASK]: 1 mask per token"
v3.4.2,A bit faster when all lengths are 1
v3.4.2,to cover whole token
v3.4.2,delete token
v3.4.2,"keep index, but replace it with [MASK]"
v3.4.2,assert tokens_length - 1 not in indices
v3.4.2,prefix src/tgt for each dataset
v3.4.2,prefix as general option for inference
v3.4.2,suffix src/tgt for each dataset
v3.4.2,suffix as general option for inference
v3.4.2,!/usr/bin/env python3
v3.4.2,-*- coding: utf-8 -*-
v3.4.2,Most code taken from: https://github.com/alvations/sacremoses
v3.4.2,Which in turn is based on the Moses punctuation normalizer.
v3.4.2,https://github.com/moses-smt/mosesdecoder/blob/master/scripts/
v3.4.2,tokenizer/normalize-punctuation.perl
v3.4.2,don't fix period at end of sentence
v3.4.2,Regex substitutions from replace-unicode-punctuation.perl
v3.4.2,https://github.com/moses-smt/mosesdecoder/blob/master/
v3.4.2,scripts/tokenizer/replace-unicode-punctuation.perl
v3.4.2,Adds the penn substitutions after extra_whitespace regexes.
v3.4.2,"Optionally, replace unicode puncts BEFORE normalization."
v3.4.2,Actual normalization.
v3.4.2,"Optionally, replace unicode puncts BEFORE normalization."
v3.4.2,normalize dict src/tgt for each dataset
v3.4.2,One source feature expected but none given and no default provided
v3.4.2,Provided default does not match required features
v3.4.2,Data not properly annotated.
v3.4.2,In this case we do not use the default as it might be an error
v3.4.2,batch 0 will always predict EOS. The other batches will predict
v3.4.2,non-eos scores.
v3.4.2,"""best"" prediction is eos - that should be blocked"
v3.4.2,include at least one prediction OTHER than EOS
v3.4.2,that is greater than -1e20
v3.4.2,now batch 0 has ended and no others have
v3.4.2,initial step
v3.4.2,batch 0 dies on step 0
v3.4.2,include at least one prediction OTHER than EOS
v3.4.2,that is greater than -1e20
v3.4.2,step 2
v3.4.2,(old) batch 8 dies on step 1
v3.4.2,step 3
v3.4.2,everything dies
v3.4.2,initial step
v3.4.2,batch 0 dies on step 0
v3.4.2,include at least one prediction OTHER than EOS
v3.4.2,that is greater than -1e20
v3.4.2,step 2
v3.4.2,(old) batch 8 dies on step 1
v3.4.2,step 3
v3.4.2,everything dies
v3.4.2,initial step
v3.4.2,finish one beam
v3.4.2,include at least one prediction OTHER than EOS
v3.4.2,that is greater than -1e20
v3.4.2,step 2
v3.4.2,finish example in last batch
v3.4.2,(old) batch 8 dies on step 1
v3.4.2,step 3
v3.4.2,everything dies
v3.4.2,initial step
v3.4.2,batch 0 dies on step 0
v3.4.2,include at least one prediction OTHER than EOS
v3.4.2,that is greater than -1e20
v3.4.2,step 2
v3.4.2,(old) batch 8 dies on step 1
v3.4.2,step 3
v3.4.2,everything dies
v3.4.2,illegal_weights_mask = torch.ByteTensor([
v3.4.2,"[0, 0, 0, 0, 0, 0, 0],"
v3.4.2,"[0, 0, 0, 1, 1, 1, 1],"
v3.4.2,"[0, 0, 0, 0, 0, 1, 1],"
v3.4.2,"[0, 0, 1, 1, 1, 1, 1]])"
v3.4.2,TODO: fix for pytorch 0.3
v3.4.2,illegal_weights = alignments.masked_select(illegal_weights_mask)
v3.4.2,"self.assertEqual(0.0, illegal_weights.data.sum())"
v3.4.2,this could be considered an integration test because it touches
v3.4.2,the filesystem for the config file (and the models)
v3.4.2,no dummy prefix
v3.4.2,no dummy prefix
v3.4.2,make sure the scalars are in the event accumulator tags
v3.4.2,required arguments
v3.4.2,transforms that require vocab will not create if not provide vocab
v3.4.2,1. Init first transform in the pipe
v3.4.2,2. Init second transform in the pipe
v3.4.2,3. Sequential combine them into a transform pipe
v3.4.2,4. apply transform pipe for example
v3.4.2,"5. example after the pipe exceed the length limit, thus filtered"
v3.4.2,6. Transform statistics registed (here for filtertoolong)
v3.4.2,"7. after report, statistics become empty as a fresh start"
v3.4.2,filter_transform.warm_up()
v3.4.2,test BPE-dropout:
v3.4.2,1. disable bpe dropout for not training example
v3.4.2,2. enable bpe dropout for training example
v3.4.2,3. (NOTE) disable dropout won't take effect if already seen
v3.4.2,this is caused by the cache mechanism in bpe:
v3.4.2,return cached subword if the original token is seen when no dropout
v3.4.2,test SP regularization:
v3.4.2,1. enable regularization for training example
v3.4.2,2. disable regularization for not training example
v3.4.2,Test mask location
v3.4.2,Test mask location
v3.4.2,Test mask location
v3.4.2,Not apply token drop for not training example
v3.4.2,apply token drop for training example
v3.4.2,Not apply token mask for not training example
v3.4.2,apply token mask for training example
v3.4.2,require vocabs to warm_up
v3.4.2,Not apply token mask for not training example
v3.4.2,apply token mask for training example
v3.4.2,"Defalt: full_stop_token=[""."", ""?"", ""!""]"
v3.4.2,"Defalt: full_stop_token=[""."", ""?"", ""!""]"
v3.4.2,random_ratio of inserted tokens are chosen in vocab
v3.4.2,others are MASK_TOK
v3.4.2,"insert_ratio=0.0,"
v3.4.2,"random_ratio=0.0,"
v3.4.2,"Defalt: full_stop_token=[""."", ""?"", ""!""]"
v3.4.2,all token are considered as an individual word
v3.4.2,1. tokens are dropped when replace_length is 0
v3.4.2,"print(f""token delete: {masked} / {tokens}"")"
v3.4.2,2. tokens are replaced by MASK when replace_length is 1
v3.4.2,"print(f""token mask: {masked} / {tokens}"")"
v3.4.2,"insert_ratio=0.0,"
v3.4.2,"random_ratio=0.0,"
v3.4.2,"Defalt: full_stop_token=[""."", ""?"", ""!""]"
v3.4.2,start token of word are identified using subword marker
v3.4.2,"1. replace_length 0: ""words"" are dropped"
v3.4.2,"print(f""word delete: {masked} / {tokens}"")"
v3.4.2,"self.assertEqual(len(masked), n_words - n_masked)"
v3.4.2,"2. replace_length 1: ""words"" are replaced with a single MASK"
v3.4.2,"print(f""whole word single mask: {masked} / {tokens}"")"
v3.4.2,len(masked) depend on number of tokens in select word
v3.4.2,"3. replace_length -1: all tokens in ""words"" are replaced with MASK"
v3.4.2,"print(f""whole word multi mask: {masked} / {tokens}"")"
v3.4.2,number of mask_tok depend on number of tokens in selected word
v3.4.2,number of MASK_TOK can be greater than n_masked
v3.4.2,"insert_ratio=0.5,"
v3.4.2,"random_ratio=0.3,"
v3.4.2,"Defalt: full_stop_token=[""."", ""?"", ""!""]"
v3.4.2,start token of word are identified using subword marker
v3.4.2,n_words = sum(token_starts)
v3.4.2,n_masked = math.ceil(n_words * bart_noise.mask_ratio)
v3.4.2,"print(f""Text Span Infilling: {infillied} / {tokens}"")"
v3.4.2,"print(n_words, n_masked)"
v3.4.2,Build the translator (along with the model)
v3.4.2,Required arguments
v3.4.2,!/usr/bin/env python
v3.4.2,-*- coding: utf-8 -*-
v3.4.2,Inject some dummy training options that may needed when build fields
v3.4.2,Remove the generated *pt files.
v3.4.2,Remove the generated data samples
v3.4.2,all beams repeat (beam >= 1 repeat dummy scores)
v3.4.2,predict repeat_idx over and over again
v3.4.2,"before repeat, scores are either 0 or -inf"
v3.4.2,"on repeat, `repeat_idx` score is BLOCKED_SCORE"
v3.4.2,"(but it's still the best score, thus we have"
v3.4.2,"[BLOCKED_SCORE, -inf, -inf, -inf, -inf]"
v3.4.2,repetitions keeps maximizing score
v3.4.2,"index 0 has been blocked, so repeating=>+0.0 score"
v3.4.2,other indexes are -inf so repeating=>BLOCKED_SCORE
v3.4.2,which is higher
v3.4.2,beam 0 and beam >=2 will repeat (beam >= 2 repeat dummy scores)
v3.4.2,non-interesting beams are going to get dummy values
v3.4.2,"on initial round, only predicted scores for beam 0"
v3.4.2,matter. Make two predictions. Top one will be repeated
v3.4.2,"in beam zero, second one will live on in beam 1."
v3.4.2,predict the same thing in beam 0
v3.4.2,continue pushing around what beam 1 predicts
v3.4.2,"now beam 0 dies (along with the others), beam 1 -> beam 0"
v3.4.2,"now beam 0 dies (along with the others), beam 1 -> beam 0"
v3.4.2,beam 0 and beam >= 2 will repeat (beam 2 repeats excluded idx)
v3.4.2,non-interesting beams are going to get dummy values
v3.4.2,predict the same thing in beam 0
v3.4.2,continue pushing around what beam 1 predicts
v3.4.2,predict the allowed-repeat again in beam 2
v3.4.2,"now beam 0 dies, beam 1 -> beam 0, beam 2 -> beam 1"
v3.4.2,and the rest die
v3.4.2,"since all preds after i=0 are 0, we can check"
v3.4.2,that the beam is the correct idx by checking that
v3.4.2,the curr score is the initial score
v3.4.2,beam 0 will always predict EOS. The other beams will predict
v3.4.2,non-eos scores.
v3.4.2,non-interesting beams are going to get dummy values
v3.4.2,"""best"" prediction is eos - that should be blocked"
v3.4.2,include at least beam_sz predictions OTHER than EOS
v3.4.2,that are greater than -1e20
v3.4.2,predict eos in beam 0
v3.4.2,provide beam_sz other good predictions
v3.4.2,now the top beam has ended and no others have
v3.4.2,"not of interest, but want to make sure it keeps running"
v3.4.2,since only beam 0 terminates and n_best = 2
v3.4.2,"this is also a test that when block_ngram_repeat=0,"
v3.4.2,repeating is acceptable
v3.4.2,non-interesting beams are going to get dummy values
v3.4.2,"""best"" prediction is eos - that should be blocked"
v3.4.2,include at least beam_sz predictions OTHER than EOS
v3.4.2,that are greater than -1e20
v3.4.2,predict eos in beam 1
v3.4.2,provide beam_sz other good predictions in other beams
v3.4.2,beam 1 dies on min_length
v3.4.2,beam 0 dies on the step after beam 1 dies
v3.4.2,"inp_lens is tiled in initialize, reassign to make attn match"
v3.4.2,non-interesting beams are going to get dummy values
v3.4.2,"""best"" prediction is eos - that should be blocked"
v3.4.2,include at least beam_sz predictions OTHER than EOS
v3.4.2,that are greater than -1e20
v3.4.2,predict eos in beam 1
v3.4.2,provide beam_sz other good predictions in other beams
v3.4.2,no top beams are finished yet
v3.4.2,beam 1 dies on min_length
v3.4.2,no top beams are finished yet
v3.4.2,beam 0 dies on the step after beam 1 dies
v3.4.2,top beam is finished now so there are attentions
v3.4.2,two beams are finished in each batch
v3.4.2,second dim is cut down to the non-padded src length
v3.4.2,first dim is equal to the time of death
v3.4.2,(beam 0 died at current step - adjust for SOS)
v3.4.2,(beam 1 died at last step - adjust for SOS)
v3.4.2,behavior gets weird when beam is already done so just stop
v3.4.2,this is just test_beam.TestBeamAgainstReferenceCase repeated
v3.4.2,in each batch.
v3.4.2,"init_preds: [4, 3, 5, 6, 7] - no EOS's"
v3.4.2,no EOS's yet
v3.4.2,"[5, 3, 2, 6, 0], so beam 2 predicts EOS!"
v3.4.2,assumes beam 2 finished on last step
v3.4.2,ended beam 2 shouldn't continue
v3.4.2,"[2, 5, 3, 6, 0] repeat self.BATCH_SZ, so beam 0 predicts EOS!"
v3.4.2,"[-2.4879, -3.8910, -4.1010, -4.2010, -4.4010] repeat self.BATCH_SZ"
v3.4.2,another beam is finished in all batches
v3.4.2,new beam 0 finished
v3.4.2,new beam 0 is old beam 3
v3.4.2,assumes beam 0 finished on last step
v3.4.2,"[5, 2, 6, 1, 0] repeat self.BATCH_SZ, so beam 1 predicts EOS!"
v3.4.2,we finish 3 hyps per example in this step
v3.4.2,new beam 1 is old beam 3
v3.4.2,this could be considered an integration test because it tests
v3.4.2,interactions between the GNMT scorer and the beam
v3.4.2,"-data option is required, but not used in this test, so dummy."
v3.4.2,len x batch x nfeat
v3.4.2,Initialize vectors to compare size with
v3.4.2,Ensure correct sizes and types
v3.4.2,Make sure that output has the correct size and type
v3.4.2,"[('encoder_type', 'transformer'),"
v3.4.2,"('word_vec_size', 16), ('hidden_size', 16)],"
v3.4.2,""""""" Only do SRU test if requirment is safisfied. """""""
v3.4.2,SRU doesn't support input_feed.
v3.4.2,first check there's nothing unexpectedly not trainable
v3.4.2,ok: word embeddings shouldn't be trainable
v3.4.2,if word vecs are freezed
v3.4.2,ok: positional encodings shouldn't be trainable
v3.4.2,then check nothing unexpectedly trainable
v3.4.2,Decoder state
v3.4.2,Build the RNN.
v3.4.2,Set up the context gate.
v3.4.2,Set up the standard attention.
v3.4.2,The encoder hidden is  (layers*directions) x batch x dim.
v3.4.2,We need to convert it to layers x batch x (directions*dim).
v3.4.2,Init the input feed.
v3.4.2,Update the state with the result.
v3.4.2,Concatenates sequence of tensors along a new dimension.
v3.4.2,NOTE: v0.3 to 0.4: dec_outs / attns[*] may not be list
v3.4.2,(in particular in case of SRU) it was not raising error in 0.3
v3.4.2,since stack(Variable) was allowed.
v3.4.2,"In 0.4, SRU returns a tensor that shouldn't be stacke"
v3.4.2,Calculate the attention.
v3.4.2,Calculate the context gate.
v3.4.2,Additional args check.
v3.4.2,Input feed concatenates hidden state with
v3.4.2,input at every time step.
v3.4.2,TODO: context gate should be employed
v3.4.2,instead of second RNN transform.
v3.4.2,Update the coverage attention.
v3.4.2,"attns[""coverage""] is actually c^(t+1) of See et al(2017)"
v3.4.2,1-index shifted
v3.4.2,Decoder State
v3.4.2,CNNDecoder has its own attention mechanism.
v3.4.2,Set up a separate copy attention layer if needed.
v3.4.2,The output of CNNEncoder.
v3.4.2,The combination of output of CNNEncoder and source embeddings.
v3.4.2,Process the result and update the attentions.
v3.4.2,Update the state.
v3.4.2,TODO change the way attns is returned dict => list or tuple (onnx)
v3.4.2,Auto import python files in this directory
v3.4.2,src_len is a single tensor shared between all models.
v3.4.2,This assumption will not hold if Translator is modified
v3.4.2,to calculate src_len as something other than the length
v3.4.2,of the input.
v3.4.2,"return _, (B, Q_len, K_len)"
v3.4.2,"layer average attention across heads, get ``(B, Q, K)``"
v3.4.2,"Case 1: no full_context, no align heads -> layer avg baseline"
v3.4.2,"Case 2: no full_context, 1 align heads -> guided align"
v3.4.2,"Case 3: full_context, 1 align heads -> full cte guided align"
v3.4.2,T: could be 1 in the case of stepwise decoding or tgt_len
v3.4.2,masking is necessary when sequence length is greater than one
v3.4.2,mask now are (batch x 1 x tlen x s or t len)
v3.4.2,1 = heads to be expanded in MHA
v3.4.2,"feed_forward applies residual, so we remove and apply residual with un-normed"
v3.4.2,Decoder State
v3.4.2,"previously, there was a GlobalAttention module here for copy"
v3.4.2,"attention. But it was never actually used -- the ""copy"" attention"
v3.4.2,just reuses the context attention.
v3.4.2,"attns[""align""] = torch.stack(attn_aligns, 0).mean(0)  # All avg"
v3.4.2,TODO change the way attns is returned dict => list or tuple (onnx)
v3.4.2,first value set to True triggered by the beginning of decoding
v3.4.2,layer_cache becomes active in the MultiHeadedAttention fwd
v3.4.2,T: could be 1 in the case of stepwise decoding or tgt_len
v3.4.2,masking is necessary when sequence length is greater than one
v3.4.2,mask now are (batch x 1 x tlen x tlen)
v3.4.2,1 = heads to be expanded in MHA
v3.4.2,"feed_forward applies residual, so we remove and apply residual with un-normed"
v3.4.2,TODO change the way attns is returned dict => list or tuple (onnx)
v3.4.2,"buffer size in bytes, determine equiv. # of elements based on data type"
v3.4.2,copy tensors into buffer_t
v3.4.2,all-reduce and rescale
v3.4.2,copy all-reduced buffer back into tensors
v3.4.2,"print(filled, sz)"
v3.4.2,"tensor is bigger than buffer, all-reduce and rescale directly"
v3.4.2,"buffer is full, all-reduce and replace buffer with grad"
v3.4.2,add tensor to buffer
v3.4.2,"propagate exception to parent process, keeping original traceback"
v3.4.2,"propagate exception to parent process, keeping original traceback"
v3.4.2,TODO: Find a better way to check for sparse gradients.
v3.4.2,we use apex.amp
v3.4.2,In this case use the old FusedAdam with
v3.4.2,FP16_optimizer wrapper
v3.4.2,Load everything from the checkpoint.
v3.4.2,Build everything from scratch.
v3.4.2,"Reset optimizer, keep options."
v3.4.2,"Reset options, keep optimizer."
v3.4.2,State can be partially restored.
v3.4.2,should be: self._optimizer.zero_grad(set_to_none)
v3.4.2,but apex.amp is not up-to-date:
v3.4.2,https://github.com/NVIDIA/apex/blob/master/apex/amp/_process_optimizer.py#L367
v3.4.2,"unscaled optimizer's gradients (already done therefore skip),"
v3.4.2,skips optimizer.step() if gradients contain infs/NaNs.
v3.4.2,Updates the scale for next iteration.
v3.4.2,Code below is an implementation of https://arxiv.org/pdf/1804.04235.pdf
v3.4.2,inspired but modified from https://github.com/DeadAt0m/adafactor-pytorch
v3.4.2,backward compatibility
v3.4.2,assuming a list/generator of parameter means single group
v3.4.2,compute combined scale factor for this group
v3.4.2,norm is in fact norm*scale
v3.4.2,note: p.grad should not ever be set for correct operation of
v3.4.2,mixed precision optimizer that sometimes sends None gradients
v3.4.2,State initialization
v3.4.2,Exponential moving average of gradient values
v3.4.2,Exponential moving average of squared gradient values
v3.4.2,-*- coding: utf-8 -*-
v3.4.2,placing this here make it easier to call logger.info
v3.4.2,"from anywhere, just 'from onmt.utils.logging import logger'"
v3.4.2,"align_head contains value in [0, 1) presenting attn prob,"
v3.4.2,0 was resulted by the context attention src_pad_mask
v3.4.2,"So, the correspand position in ref_align should also be 0"
v3.4.2,"Therefore, clip align_head to > 1e-18 should be bias free."
v3.4.2,rescale with tau (temperature) and apply the log_softmax.
v3.4.2,ct2 expects src with lengths without padding
v3.4.2,again we use raw probs to rescale with tau and apply log_softmax
v3.4.2,lm_scores are in log space so log_target=True
v3.4.2,rescale with tau (temperature) and apply the log_softmax.
v3.4.2,ct2 expects src with lengths without padding
v3.4.2,again we use raw probs to rescale with tau and apply log_softmax
v3.4.2,lm_scores are in log space so log_target=True
v3.4.2,Create a mask with zeros at prompt positions and ones at answer postions.
v3.4.2,Apply the mask on the target side.
v3.4.2,Put the padding token index at the prompt positions.
v3.4.2,take into account here the tgt_shift_index (0 / 1 = LM/NMT)
v3.4.2,Correct target copy token instead of <unk>
v3.4.2,tgt[i] = align[i] + len(tgt_vocab)
v3.4.2,for i such that tgt[i] == 0 and align[i] != 0
v3.4.2,in the case criterion reduction is None then we need
v3.4.2,to sum the loss of each sentence in the batch
v3.4.2,Check Transforms
v3.4.2,Check path
v3.4.2,tgt is src for LM task
v3.4.2,Check weight
v3.4.2,Check features
v3.4.2,validation when train:
v3.4.2,Check embeddings stuff
v3.4.2,"Backward compatibility with ""fix_word_vecs_*"" opts"
v3.4.2,encoder and decoder should be same sizes
v3.4.2,"Load default opt values, then overwrite with the opts in"
v3.4.2,"the checkpoint. That way, if there are new options added,"
v3.4.2,the defaults are used.
v3.4.2,It comes from training
v3.4.2,TODO: needs to be added as inference opt
v3.4.2,Don't do anything
v3.4.2,Update best score of each criteria
v3.4.2,Reset tolerance
v3.4.2,Update current status
v3.4.2,Decrease tolerance
v3.4.2,Log
v3.4.2,Log
v3.4.2,Get a list of world_size lists with len(stat_list) Statistics objects
v3.4.2,"this param init is overridden by model_builder, useless then."
v3.4.2,SRU doesn't support PackedSequence.
v3.4.2,-*- coding: utf-8 -*-
v3.4.2,threshold on 1 to avoid div by 0
v3.4.2,treat alignment matrix one by one as each have different lengths
v3.4.2,No alignment if not exist valid tgt token
v3.4.2,get valid alignment (sub-matrix from full paded aligment matrix)
v3.4.2,Helper functions
v3.4.2,Keeps track of the original words/subwords
v3.4.2,('prior_tokenization' option)
v3.4.2,In case there is a final case_markup when new_spacer is on
v3.4.2,########## #
v3.4.2,Translator #
v3.4.2,########## #
v3.4.2,Set translation options
v3.4.2,Build translator from options
v3.4.2,################### #
v3.4.2,Validation iterator #
v3.4.2,################### #
v3.4.2,Reinstantiate the validation iterator
v3.4.2,Retrieve raw references and sources
v3.4.2,########### #
v3.4.2,Predictions #
v3.4.2,########### #
v3.4.2,####### #
v3.4.2,Outputs #
v3.4.2,####### #
v3.4.2,Flatten predictions
v3.4.2,Save results
v3.4.2,-*- coding: utf-8 -*-
v3.4.2,this one is needed for Random Shuffler of batches
v3.4.2,in multi gpu it ensures datasets are read in the same order
v3.4.2,some cudnn methods can be random even after fixing the seed
v3.4.2,unless you tell it to be deterministic
v3.4.2,This one is needed for various tranfroms
v3.4.2,These ensure same initialization in multi gpu mode
v3.4.2,we need to check the model path + any tokenizer path
v3.4.2,patch to log stdout spawned processes of dataloader
v3.4.2,bucket_size = batch_size
v3.4.2,For TRAIN we need to group examples by length
v3.4.2,"for faster performance, but otherwise, sequential."
v3.4.2,For TRAIN we shuffle batches within the bucket
v3.4.2,otherwise sequential
v3.4.2,for specific case of rnn_packed need to be sorted
v3.4.2,within the batch
v3.4.2,single thread - create batch directly on GPU if device is gpu
v3.4.2,multithread faster to create batch on CPU in each thread and then move it to gpu
v3.4.2,Move tensor_batch from cpu to device
v3.4.2,Check if all tokens have features or none at all
v3.4.2,Make features part of src like
v3.4.2,"{'src': {'src': ..., 'feats': [...., ....]}}"
v3.4.2,at this point an example looks like:
v3.4.2,"{'src': {'src': ..., 'feats': [....]},"
v3.4.2,"'tgt': {'tgt': ...},"
v3.4.2,"'src_original': ['tok1', ...'tokn'],"
v3.4.2,"'tgt_original': ['tok1', ...'tokm'],"
v3.4.2,'indices' : seq in bucket
v3.4.2,"'align': ...,"
v3.4.2,}
v3.4.2,Need to add features in last dimensions
v3.4.2,Keep it consistent with dynamic data
v3.4.2,make a small vocab containing just the tokens in the source sequence
v3.4.2,Map source tokens to indices in the dynamic dict.
v3.4.2,-*- coding: utf-8 -*-
v3.4.2,this is hack: if the special separator ï½Ÿnewlineï½ is returned because of the
v3.4.2,"""docify"" transform.get_specials we don't add it if the corresponding newline code"
v3.4.2,is already included in the sentencepiece or BPE-with-gpt2-pretok.
v3.4.2,'src_original' and 'tgt_original' store the
v3.4.2,original line before tokenization. These
v3.4.2,fields are used later on in the feature
v3.4.2,transforms.
v3.4.2,NOTE: moved to dynamic_iterator.py cf process()
v3.4.2,item = self.transform.apply(
v3.4.2,"example, is_train=self.infinitely, corpus_name=self.cid)"
v3.4.2,empty example: skip
v3.4.2,bitsandbytes quantize weights when .cuda() is called
v3.4.2,for huge models we need to save Ram
v3.4.2,so we load the weights  module by module and transfer them to GPU for quantization
v3.4.2,bitsandbytes quantize weights when .cuda() is called
v3.4.2,for huge models we need to save Ram
v3.4.2,so we load the weights  module by module and transfer them to GPU for quantization
v3.4.2,"No encoder in LM, seq2seq count formatting kept"
v3.4.2,_check_save_model_path
v3.4.2,This preserves backward-compat for models using customed layernorm
v3.4.2,Force add_ffnbias to True if bias found in model w_1 keys
v3.4.2,fix v2 compatibility
v3.4.2,end of patch for backward compatibility
v3.4.2,!/usr/bin/env python
v3.4.2,!/usr/bin/env python
v3.4.2,!/usr/bin/env python
v3.4.2,-*- coding: utf-8 -*-
v3.4.2,!/usr/bin/env python
v3.4.2,BPE training
v3.4.2,SentencePiece training
v3.4.2,!/usr/bin/env python
v3.4.2,!/usr/bin/env python
v3.4.2,Set sharing strategy manually instead of default based on the OS.
v3.4.2,torch.multiprocessing.set_sharing_strategy('file_system')
v3.4.2,Create a thread to listen for errors in the child processes.
v3.4.2,Train with multiprocessing.
v3.4.2,magic indices
v3.4.2,result caching
v3.4.2,Here we set the decoder to start with self.start (BOS or EOS)
v3.4.2,fix length constraint and remove eos from count
v3.4.2,add one to account for BOS. Don't account for EOS because hitting
v3.4.2,this implies it hasn't been found.
v3.4.2,we don't block nothing if the user doesn't want it
v3.4.2,we can't block nothing beam's too short
v3.4.2,we check paths one by one
v3.4.2,we don't forbid nothing if the user doesn't want it
v3.4.2,we can't forbid nothing if beam's too short
v3.4.2,Reordering forbidden_tokens following beam selection
v3.4.2,We rebuild a dict to ensure we get the value and not the pointer
v3.4.2,Grabing the newly selected tokens and associated ngram
v3.4.2,skip the blocking if any token in current_ngram is excluded
v3.4.2,"pickups: Tensor where specified index were set to 1, others 0"
v3.4.2,"dropdowns: opposite of pickups, 1 for those shouldn't pick"
v3.4.2,Minus dropdowns to log_probs making probabilities of
v3.4.2,unspecified index close to 0
v3.4.2,"prediction step have surpass length of given target_prefix,"
v3.4.2,no need to further change this attr
v3.4.2,keep indices until overflowing p
v3.4.2,Set all logits that are not in the top-p to -10000.
v3.4.2,This puts the probabilities close to 0.
v3.4.2,Set all logits that are not in the top-k to -10000.
v3.4.2,This puts the probabilities close to 0.
v3.4.2,"For temp=0.0, take the argmax to avoid divide-by-zero errors."
v3.4.2,keep_topk=1 is also equivalent to argmax.
v3.4.2,maybe fix some prediction at this step by modifying log_probs
v3.4.2,"shape: (sum(~ self.is_finished), 1)"
v3.4.2,in LM task src_len is associated with currently generated src
v3.4.2,and therefore needs to follow the generation
v3.4.2,!/usr/bin/env python
v3.4.2,for debugging
v3.4.2,TODO: maybe add dynamic part
v3.4.2,Statistics
v3.4.2,those two should be the same except feat dim
v3.4.2,"batch['src'][perm[j], :, :])"
v3.4.2,trans.src
v3.4.2,we rebuild a small batch made of the sub-segments
v3.4.2,in the long segment.
v3.4.2,new sub-batch ready to be translated
v3.4.2,we re-insert the sub-batch in the initial translations
v3.4.2,For seq2seq when we need to force doc to spit the same number of sents
v3.4.2,In the case of length_penalty = none we report the total logprobs
v3.4.2,divided by the number of sentence to get an approximation of the
v3.4.2,per sentence logprob. We also return the corresponding ppl
v3.4.2,"When a length_penalty is used eg: ""avg"" or ""wu"" since logprobs"
v3.4.2,are normalized per token we report the per line per token logprob
v3.4.2,"and the corresponding ""per word perplexity"""
v3.4.2,Turn any copied words into UNKs.
v3.4.2,"Decoder forward, takes [batch, tgt_len, nfeats] as input"
v3.4.2,"and [batch, src_len, hidden] as enc_out"
v3.4.2,"in case of inference tgt_len = 1, batch = beam times batch_size"
v3.4.2,"in case of Gold Scoring tgt_len = actual length, batch = 1 batch"
v3.4.2,Generator forward.
v3.4.2,"returns [(batch_size x beam_size) , vocab ] when 1 step"
v3.4.2,"or [batch_size, tgt_len, vocab ] when full sentence"
v3.4.2,"here we have scores [tgt_lenxbatch, vocab] or [beamxbatch, vocab]"
v3.4.2,at this point scores is batch first (dim=0)
v3.4.2,"returns [(batch_size x beam_size) , vocab ] when 1 step"
v3.4.2,"or [batch_size, tgt_len, vocab ] when full sentence"
v3.4.2,(0) add BOS and padding to tgt prediction
v3.4.2,(1) Encoder forward.
v3.4.2,(2) Repeat src objects `n_best` times.
v3.4.2,"We use batch_size x n_best, get ``(batch * n_best, src_len, nfeat)``"
v3.4.2,Quick fix. Transformers return None as enc_states.
v3.4.2,enc_states are only used later on to init decoder's state
v3.4.2,"but are never used in Transformer decoder, so we can skip"
v3.4.2,"(3) Init decoder with n_best src,"
v3.4.2,"reshape tgt to ``(len, batch * n_best, nfeat)``"
v3.4.2,it should be done in a better way
v3.4.2,here dec_in is batch first
v3.4.2,masked_select
v3.4.2,get aligned src id for each prediction's valid tgt tokens
v3.4.2,TODO: support these blacklisted features
v3.4.2,(0) Prep the components of the search.
v3.4.2,(1) Run the encoder on the src.
v3.4.2,(2) prep decode_strategy. Possibly repeat src objects.
v3.4.2,(3) Begin decoding step by step:
v3.4.2,"decoder_input = decode_strategy.current_predictions.view(1, -1,"
v3.4.2,1)
v3.4.2,Reorder states.
v3.4.2,TODO: support these blacklisted features
v3.4.2,(0) Prep the components of the search.
v3.4.2,(1) split src into src and target_prefix to avoid padding.
v3.4.2,(2) init decoder
v3.4.2,(3) prep decode_strategy. Possibly repeat src objects.
v3.4.2,(4) Begin decoding step by step:
v3.4.2,Reorder states.
v3.4.2,select indexes in model state/cache
v3.4.2,beam parameters
v3.4.2,beam state
v3.4.2,"""global state"" of the old beam"
v3.4.2,buffers for the topk scores and 'backpointer'
v3.4.2,for testing
v3.4.2,maybe fix some prediction at this step by modifying log_probs
v3.4.2,Flatten probs into a list of possibilities.
v3.4.2,Penalize beams that finished.
v3.4.2,"on real data (newstest2017) with the pretrained transformer,"
v3.4.2,it's faster to not move this back to the original device
v3.4.2,Store finished hypotheses for this batch.
v3.4.2,End condition is the top beam finished and we can return
v3.4.2,n_best hypotheses.
v3.4.2,"If all sentences are translated, no need to go further."
v3.4.2,Remove finished batches for the next step.
v3.4.2,using integer division to get an integer _B without casting
v3.4.2,force the output to be longer than self.min_length
v3.4.2,Multiply probs by the beam probability.
v3.4.2,"if the sequence ends now, then the penalty is the current"
v3.4.2,"length + 1, to include the EOS token"
v3.4.2,Avoid any direction that would repeat unwanted ngrams
v3.4.2,Pick up candidate token by curr_scores
v3.4.2,Recover log probs.
v3.4.2,Length penalty is just a scalar. It doesn't matter if it's applied
v3.4.2,before or after the topk.
v3.4.2,Resolve beam origin and map to batch index flat representation.
v3.4.2,Append last prediction.
v3.4.2,update global state (step == 1)
v3.4.2,update global state (step > 1)
v3.4.2,"shape: (batch_size x beam_size, 1)"
v3.4.2,in LM task src_len is associated with currently generated src
v3.4.2,and therefore needs to follow the generation
v3.4.2,in LM task src_len is associated with currently generated src
v3.4.2,and therefore needs to follow the generation
v3.4.2,Term will be subtracted from probability
v3.4.2,Probability will be divided by this
v3.4.2,these warnings indicate that either the alpha/beta
v3.4.2,"forces a penalty to be a no-op, or a penalty is a no-op but"
v3.4.2,the alpha/beta would suggest otherwise.
v3.4.2,using some coverage penalty
v3.4.2,!/usr/bin/env python
v3.4.2,semaphore doesn't have a timeout arg in Python 2.7
v3.4.2,perform a first request to initialize everything
v3.4.2,backwards compatibility for confs
v3.4.2,every segment becomes a dict for flexibility purposes
v3.4.2,NOTE: translator returns lists of `n_best` list
v3.4.2,build back results with empty texts
v3.4.2,load can be called multiple times: modify copy
v3.4.2,output contain alignment
v3.4.2,Below are all the different penalty terms implemented so far.
v3.4.2,Subtract coverage penalty from topk log probs.
v3.4.2,Divide topk log probs by length penalty.
v3.4.2,These comp lists are costy but less than for loops
v3.4.2,Chinese segmentation
v3.4.2,Chinese simplify -> Chinese traditional standard
v3.4.2,Chinese simplify -> Chinese traditional (HongKong)
v3.4.2,Chinese simplify -> Chinese traditional (Taiwan)
v3.4.2,Chinese traditional -> Chinese simplify (v1)
v3.4.2,Chinese traditional -> Chinese simplify (v2)
v3.4.2,Auto import python files in this directory
v3.4.2,"def custom_stopping_criteria(input_ids, score, **kwargs):"
v3.4.2,"stop_ids = [29871, 13, 13] # \n\n"
v3.4.2,return input_ids[-len(stop_ids)]
v3.4.2,Build the translator (along with the model)
v3.4.2,get prompt and make sure it fits
v3.4.2,"def custom_stopping_criteria(input_ids, score, **kwargs):"
v3.4.2,"stop_ids = [29871, 13, 13] # \n\n"
v3.4.2,return input_ids[-len(stop_ids)]
v3.4.2,Build the translator (along with the model)
v3.4.2,get prompt and make sure it fits
v3.4.1,!/usr/bin/env python
v3.4.1,!/usr/bin/env python
v3.4.1,!/usr/bin/env python
v3.4.1,!/usr/bin/env python
v3.4.1,!/usr/bin/env python
v3.4.1,!/usr/bin/env python3
v3.4.1,-*- coding: utf-8 -*-
v3.4.1,
v3.4.1,"OpenNMT-py documentation build configuration file, created by"
v3.4.1,sphinx-quickstart on Sun Dec 17 12:07:14 2017.
v3.4.1,
v3.4.1,This file is execfile()d with the current directory set to its
v3.4.1,containing dir.
v3.4.1,
v3.4.1,Note that not all possible configuration values are present in this
v3.4.1,autogenerated file.
v3.4.1,
v3.4.1,All configuration values have a default; values that are commented out
v3.4.1,serve to show the default.
v3.4.1,"If extensions (or modules to document with autodoc) are in another directory,"
v3.4.1,add these directories to sys.path here. If the directory is relative to the
v3.4.1,"documentation root, use os.path.abspath to make it absolute, like shown here."
v3.4.1,
v3.4.1,import os
v3.4.1,import sys
v3.4.1,"sys.path.insert(0, os.path.abspath('.'))"
v3.4.1,-- General configuration ------------------------------------------------
v3.4.1,"If your documentation needs a minimal Sphinx version, state it here."
v3.4.1,
v3.4.1,needs_sphinx = '6.0'
v3.4.1,"Add any Sphinx extension module names here, as strings. They can be"
v3.4.1,extensions coming with Sphinx (named 'sphinx.ext.*') or your custom
v3.4.1,ones.
v3.4.1,Show base classes
v3.4.1,"Use ""variables"" section for Attributes instead of weird block things"
v3.4.1,mimicking the function style.
v3.4.1,"Add any paths that contain templates here, relative to this directory."
v3.4.1,The suffix(es) of source filenames.
v3.4.1,You can specify multiple suffix as a list of string:
v3.4.1,
v3.4.1,"source_suffix = ['.rst', '.md']"
v3.4.1,The master toctree document.
v3.4.1,General information about the project.
v3.4.1,"The version info for the project you're documenting, acts as replacement for"
v3.4.1,"|version| and |release|, also used in various other places throughout the"
v3.4.1,built documents.
v3.4.1,
v3.4.1,The short X.Y version.
v3.4.1,"The full version, including alpha/beta/rc tags."
v3.4.1,The language for content autogenerated by Sphinx. Refer to documentation
v3.4.1,for a list of supported languages.
v3.4.1,
v3.4.1,This is also used if you do content translation via gettext catalogs.
v3.4.1,"Usually you set ""language"" from the command line for these cases."
v3.4.1,"List of patterns, relative to source directory, that match files and"
v3.4.1,directories to ignore when looking for source files.
v3.4.1,This patterns also effect to html_static_path and html_extra_path
v3.4.1,The name of the Pygments (syntax highlighting) style to use.
v3.4.1,"If true, `todo` and `todoList` produce output, else they produce nothing."
v3.4.1,-- Options for HTML output ----------------------------------------------
v3.4.1,The theme to use for HTML and HTML Help pages.  See the documentation for
v3.4.1,a list of builtin themes.
v3.4.1,
v3.4.1,html_theme = 'sphinx_materialdesign_theme'
v3.4.1,html_theme_path = [sphinx_materialdesign_theme.get_path()]
v3.4.1,Theme options are theme-specific and customize the look and feel of a theme
v3.4.1,"further.  For a list of options available for each theme, see the"
v3.4.1,documentation.
v3.4.1,
v3.4.1,html_theme_options = {}
v3.4.1,"Add any paths that contain custom static files (such as style sheets) here,"
v3.4.1,"relative to this directory. They are copied after the builtin static files,"
v3.4.1,"so a file named ""default.css"" will overwrite the builtin ""default.css""."
v3.4.1,"Custom sidebar templates, must be a dictionary that maps document names"
v3.4.1,to template names.
v3.4.1,
v3.4.1,This is required for the alabaster theme
v3.4.1,refs: http://alabaster.readthedocs.io/en/latest/installation.html#sidebars
v3.4.1,-- Options for HTMLHelp output ------------------------------------------
v3.4.1,Output file base name for HTML help builder.
v3.4.1,-- Options for LaTeX output ---------------------------------------------
v3.4.1,The paper size ('letterpaper' or 'a4paper').
v3.4.1,
v3.4.1,"'papersize': 'letterpaper',"
v3.4.1,"The font size ('10pt', '11pt' or '12pt')."
v3.4.1,
v3.4.1,"'pointsize': '10pt',"
v3.4.1,Additional stuff for the LaTeX preamble.
v3.4.1,
v3.4.1,"'preamble': '',"
v3.4.1,Latex figure (float) alignment
v3.4.1,
v3.4.1,"'figure_align': 'htbp',"
v3.4.1,Grouping the document tree into LaTeX files. List of tuples
v3.4.1,"(source start file, target name, title,"
v3.4.1,"author, documentclass [howto, manual, or own class])."
v3.4.1,-- Options for manual page output ---------------------------------------
v3.4.1,One entry per manual page. List of tuples
v3.4.1,"(source start file, name, description, authors, manual section)."
v3.4.1,-- Options for Texinfo output -------------------------------------------
v3.4.1,Grouping the document tree into Texinfo files. List of tuples
v3.4.1,"(source start file, target name, title, author,"
v3.4.1,"dir menu entry, description, category)"
v3.4.1,"inf_type = ""ct2"""
v3.4.1,#####################
v3.4.1,Inference with CT2 #
v3.4.1,#####################
v3.4.1,#####################
v3.4.1,Inference with -py #
v3.4.1,#####################
v3.4.1,"ckpt_path = ""finetuned_llama7B/llama7B-vicuna-onmt_step_4000.pt"""
v3.4.1,we receive a text box content
v3.4.1,might be good to split also based on full period (later)
v3.4.1,we reformat the transformed batch to be numericalized / tensorified
v3.4.1,#####
v3.4.1,UI #
v3.4.1,#####
v3.4.1,What are the 3 best french cities ?
v3.4.1,Which one is better if I like outdoor activities ?
v3.4.1,Which one is better if I like cultural outings?
v3.4.1,What are the best neighborhoods in these 5 cities?
v3.4.1,!/usr/bin/env python3
v3.4.1,Usage: python3 filter_train.py in.src in.trg out.src out.trg max-tokens
v3.4.1,flake8: noqa
v3.4.1,-*- coding: utf-8 -*-
v3.4.1,Generated by the protocol buffer compiler.  DO NOT EDIT!
v3.4.1,source: sentencepiece_model.proto
v3.4.1,@@protoc_insertion_point(imports)
v3.4.1,@@protoc_insertion_point(module_scope)
v3.4.1,!/usr/bin/env python
v3.4.1,-*- coding: utf-8 -*-
v3.4.1,is this reachable?
v3.4.1,Read in embeddings
v3.4.1,Write to file
v3.4.1,converts a SentencePiece vocabulary to the format expected by dynamic data
v3.4.1,"(essentially converts float expected counts to ""fixed precision"" int pseudo"
v3.4.1,counts)
v3.4.1,from onmt.utils.misc import use_gpu
v3.4.1,"Add in default model arguments, possibly added since training."
v3.4.1,this patch is no longer needed included in converter
v3.4.1,"if hasattr(model_opt, 'rnn_size'):"
v3.4.1,model_opt.hidden_size = model_opt.rnn_size
v3.4.1,build_base_model expects updated and validated opts
v3.4.1,-*- encoding: utf-8 -*-
v3.4.1,!/usr/bin/env python
v3.4.1,Falcon stores QKV in one single tensor but it is not simply piled up Q+K+V
v3.4.1,it is heads interleaved to we need to slice first
v3.4.1,also it uses the HF rotary so we need to permute Q and K interleave
v3.4.1,!/usr/bin/env python
v3.4.1,-*- coding: utf-8 -*-
v3.4.1,Author: Rico Sennrich
v3.4.1,flake8: noqa
v3.4.1,This file is retrieved from https://github.com/rsennrich/subword-nmt
v3.4.1,hack for python2/3 compatibility
v3.4.1,check version information
v3.4.1,some hacking to deal with duplicates (only consider first instance)
v3.4.1,don't print end-of-word symbols
v3.4.1,sys.stderr.write('cannot split {0} further.\n'.format(segment))
v3.4.1,sys.stderr.write('OOV: {0}\n'.format(segment))
v3.4.1,sys.stderr.write('OOV: {0}\n'.format(segment))
v3.4.1,python 2/3 compatibility
v3.4.1,read/write files as UTF-8
v3.4.1,!/usr/bin/env python3
v3.4.1,coding: utf-8
v3.4.1,"In order to use this tool, please install comet first"
v3.4.1,https://github.com/Unbabel/COMET
v3.4.1,Let's say you have a source file with N sentences in SL - eg: source.sl
v3.4.1,and the corresponding references (N sentences) reference.tl
v3.4.1,Translate your file in TL with the -n_best nbest options nbest being
v3.4.1,then number of hypotheses and output the target to -output target.nbest.tl
v3.4.1,Then you need to duplicate source and reference sentences nbest times
v3.4.1,for this script.
v3.4.1,for instance using awk '{for(i=1; i<=n; i++) print}' n=5 reference.tl \
v3.4.1,> reference.5.tl
v3.4.1,same for source.
v3.4.1,This script can be run (for instance with nbest = 5) as follows:
v3.4.1,python oracle_bleu.py --nbest-src source.5.sl --nbest-hyp target.5.tl \
v3.4.1,--nbest-ref reference.5.tl --nbest-order 5 --output target.maxbleu.tl
v3.4.1,It will search in all hyp the best comet score
v3.4.1,when choosing a reference-less model no nbest-ref is required
v3.4.1,for nbest in nbests:
v3.4.1,!/usr/bin/env python
v3.4.1,!/usr/bin/env python3
v3.4.1,coding: utf-8
v3.4.1,Let's say you have a source file with N sentences in SL - eg: source.sl
v3.4.1,Translate your file in TL with the -n_best nbest options nbest being
v3.4.1,then number of hypotheses and output the target to -output target.nbest.tl
v3.4.1,This script can be run (for instance with nbest = 5) as follows:
v3.4.1,python mbr_bleu.py --nbest-hyp target.5.tl \
v3.4.1,--nbest-order 5 --output target.mbr.tl
v3.4.1,It will compare all hyp with eachother and output the max bleu
v3.4.1,!/usr/bin/env python
v3.4.1,!/usr/bin/env python
v3.4.1,-*- coding: utf-8 -*-
v3.4.1,Author: Rico Sennrich
v3.4.1,flake8: noqa
v3.4.1,This file is retrieved from https://github.com/rsennrich/subword-nmt
v3.4.1,hack for python2/3 compatibility
v3.4.1,"find all instances of pair, and update frequency/indices around it"
v3.4.1,find first symbol
v3.4.1,"if first symbol is followed by second symbol, we've found an occurrence of pair (old_word[i:i+2])"
v3.4.1,"assuming a symbol sequence ""A B C"", if ""B C"" is merged, reduce the frequency of ""A B"""
v3.4.1,"assuming a symbol sequence ""A B C B"", if ""B C"" is merged, reduce the frequency of ""C B""."
v3.4.1,"however, skip this if the sequence is A B C B C, because the frequency of ""C B"" will be reduced by the previous code block"
v3.4.1,find new pair
v3.4.1,"assuming a symbol sequence ""A BC D"", if ""B C"" is merged, increase the frequency of ""A BC"""
v3.4.1,"assuming a symbol sequence ""A BC B"", if ""B C"" is merged, increase the frequency of ""BC B"""
v3.4.1,"however, if the sequence is A BC BC, skip this step because the count of ""BC BC"" will be incremented by the previous code block"
v3.4.1,data structure of pair frequencies
v3.4.1,index from pairs to words
v3.4.1,version 0.2 changes the handling of the end-of-word token ('</w>');
v3.4.1,version numbering allows bckward compatibility
v3.4.1,"threshold is inspired by Zipfian assumption, but should only affect speed"
v3.4.1,we probably missed the best pair because of pruning; go back to full statistics
v3.4.1,"threshold is inspired by Zipfian assumption, but should only affect speed"
v3.4.1,python 2/3 compatibility
v3.4.1,read/write files as UTF-8
v3.4.1,Now we can pipe the full file through the model using the Iterator
v3.4.1,reminder a batch includes .src .tgt .indices and it is sorted
v3.4.1,Compute and retrieve the loss for EACH sentence
v3.4.1,Now we need to rearrange the batch of ppl
v3.4.1,in the original order with indices
v3.4.1,!/usr/bin/env python
v3.4.1,-*- coding: utf-8 -*-
v3.4.1,!/usr/bin/env python
v3.4.1,!/usr/bin/env python
v3.4.1,!/usr/bin/env python
v3.4.1,!/usr/bin/env python
v3.4.1,!/usr/bin/env python
v3.4.1,!/usr/bin/env python3
v3.4.1,coding: utf-8
v3.4.1,Let's say you have a source file with N sentences in SL - eg: source.sl
v3.4.1,and the corresponding references (N sentences) reference.tl
v3.4.1,Translate your file in TL with the -n_best nbest options nbest being
v3.4.1,then number of hypotheses and output the target to -output target.nbest.tl
v3.4.1,Then you need to duplicate reference sentences nbest times for this script.
v3.4.1,for instance using awk '{for(i=1; i<=n; i++) print}' n=5 reference.tl \
v3.4.1,> reference.5.tl
v3.4.1,This script can be run (for instance with nbest = 5) as follows:
v3.4.1,python oracle_bleu.py --nbest-hyp target.5.tl --nbest-ref reference.5.tl \
v3.4.1,--nbest-order 5 --output target.maxbleu.tl
v3.4.1,It will search in all hyp the best bleu wrt reference
v3.4.1,and output the max bleu
v3.4.1,!/usr/bin/env python
v3.4.1,with the two module = imp.load_source() below
v3.4.1,we ghost the old torchtext.data.field and depercated
v3.4.1,onmt.inputters.text_dataset
v3.4.1,however this require some functions / classes to be
v3.4.1,monkey patched for loading the old field/vocab objects.
v3.4.1,"module3 = imp.load_source(""Vocab"", ""tools/convertv2_v3.py"")"
v3.4.1,"voc = dict(sorted(fields.vocab.__dict__['freqs'].items(),"
v3.4.1,"key=lambda x: (-x[1], x[0]))).keys()"
v3.4.1,"voc = dict(sorted(fields.vocab.__dict__['freqs'].items(),"
v3.4.1,"key=lambda x: (-x[1], x[0]))).keys()"
v3.4.1,!/usr/bin/env python
v3.4.1,redpajama stores QKV in one single tensor but it is not simply piled up Q+K+V
v3.4.1,it is heads interleaved to we need to slice first
v3.4.1,also it uses the HF rotary so we need to permute Q and K interleave
v3.4.1,Avoid functionality on inference
v3.4.1,weights are in the .pt file
v3.4.1,weights are not in the .pt checkpoint but stored in the safetensors file
v3.4.1,Build embeddings.
v3.4.1,Build encoder.
v3.4.1,Build embeddings.
v3.4.1,Build decoder.
v3.4.1,Share the embedding matrix - preprocess with share_vocab required.
v3.4.1,src/tgt vocab should be the same if `-share_vocab` is specified.
v3.4.1,Update vocabulary embeddings with checkpoint embeddings
v3.4.1,Embedding layers
v3.4.1,Just for debugging purposes
v3.4.1,Remove old vocabulary associated embeddings
v3.4.1,for back compat when attention_dropout was not defined
v3.4.1,Build Model
v3.4.1,Build Generator.
v3.4.1,If new training initialize the model params
v3.4.1,If update_vocab init also but checkpoint will overwrite old weights
v3.4.1,ONLY for legacy fusedam with amp pytorch requires NOT to half the model
v3.4.1,Update model embeddings with those from the checkpoint
v3.4.1,after initialization
v3.4.1,after this checkpoint contains no embeddings
v3.4.1,when using LoRa or updating the vocab (no more embeddings in ckpt)
v3.4.1,=> strict=False when loading state_dict
v3.4.1,weights are in the .pt file
v3.4.1,weights are not in the .pt checkpoint but stored in the safetensors file
v3.4.1,!/usr/bin/env python
v3.4.1,if transform + options set in 'valid' we need to copy in main
v3.4.1,transform / options for scoring considered as inference
v3.4.1,"maybe prepare pretrained embeddings, if any"
v3.4.1,Load checkpoint if we resume from a previous training.
v3.4.1,ensure tensorboard output is written in the directory
v3.4.1,of previous checkpoints
v3.4.1,Override checkpoint's update_embeddings as it defaults to false
v3.4.1,Override checkpoint's freezing settings as it defaults to false
v3.4.1,NOTE: It's important that ``opt`` has been validated and updated
v3.4.1,at this point.
v3.4.1,Build model.
v3.4.1,Build optimizer.
v3.4.1,Build model saver
v3.4.1,Use Tensorboard for visualization during training
v3.4.1,Options only during inference
v3.4.1,"Truncation options, for text corpus"
v3.4.1,"as for False, this will be added in _add_train_general_opts"
v3.4.1,GPU
v3.4.1,Embedding Options
v3.4.1,Model Task Options
v3.4.1,Encoder-Decoder Options
v3.4.1,Freeze Encoder and/or Decoder
v3.4.1,The following options (bridge_extra_node to n_steps) are used
v3.4.1,for training with --encoder_type ggnn (Gated Graph Neural Network).
v3.4.1,Attention options
v3.4.1,Alignement options
v3.4.1,Generator and loss options.
v3.4.1,LoRa
v3.4.1,Init options
v3.4.1,Pretrained word vectors
v3.4.1,Freeze word vectors
v3.4.1,Optimization options
v3.4.1,learning rate
v3.4.1,options relate to data preprare
v3.4.1,options relate to train
v3.4.1,Alpha and Beta values for Google Length + Coverage penalty
v3.4.1,"Described here: https://arxiv.org/pdf/1609.08144.pdf, Section 7"
v3.4.1,Length penalty options
v3.4.1,Coverage penalty options
v3.4.1,Decoding Length constraint
v3.4.1,Decoding content constraint
v3.4.1,Adding options related to source and target features
v3.4.1,Adding options relate to decoding strategy
v3.4.1,Adding option for logging
v3.4.1,Adding options related to Transforms
v3.4.1,Copyright 2016 The Chromium Authors. All rights reserved.
v3.4.1,Use of this source code is governed by a BSD-style license that can be
v3.4.1,found in the LICENSE file.
v3.4.1,"Get the key 'value' in the dict, or just use 'value'"
v3.4.1,Create a thread to listen for errors in the child processes.
v3.4.1,Basic attributes.
v3.4.1,Set model in training mode.
v3.4.1,Let's clean the GPUs before training loop
v3.4.1,UPDATE DROPOUT
v3.4.1,Run patience mechanism
v3.4.1,"If the patience has reached the limit, stop training"
v3.4.1,swap model params w/ moving average
v3.4.1,(and keep the original parameters)
v3.4.1,Set model in validating mode.
v3.4.1,raw_srcs = []
v3.4.1,raw_refs = []
v3.4.1,F-prop through the model.
v3.4.1,Compute loss.
v3.4.1,Compute validation metrics (at batch.dataset level)
v3.4.1,Compute stats
v3.4.1,Update statistics.
v3.4.1,Set model back to training mode.
v3.4.1,Truncated BPTT: reminder not compatible with accum > 1
v3.4.1,1. Create truncated target.
v3.4.1,2. F-prop all but generator.
v3.4.1,3. Compute loss.
v3.4.1,The loss of the prompt will be set to zero.
v3.4.1,"If truncated, don't backprop fully."
v3.4.1,"in case of multi step gradient accumulation,"
v3.4.1,update only after accum batches
v3.4.1,For Flake
v3.4.1,we avoid padding while mean pooling
v3.4.1,incoming and outgoing edge embedding
v3.4.1,Find vocab data for tree builting
v3.4.1,Propogation Model
v3.4.1,Initialize the bridge layer
v3.4.1,Token embedding
v3.4.1,Initialize graph using formatted input sequence
v3.4.1,Number of flagged nodes defines node count for this sample
v3.4.1,"(Nodes can have no flags on them, but must be in 'flags' list)."
v3.4.1,The total number of integers in the vocab should allow
v3.4.1,for all features and edges to be defined.
v3.4.1,Use first extra node as only source for decoder init
v3.4.1,Average all nodes to get bridge input
v3.4.1,"LSTM has hidden and cell state, other only one"
v3.4.1,Total number of states
v3.4.1,Build a linear layer for each
v3.4.1,Initialize the bridge layer
v3.4.1,src lengths data is wrapped inside a Tensor.
v3.4.1,"LSTM has hidden and cell state, other only one"
v3.4.1,Total number of states
v3.4.1,Build a linear layer for each
v3.4.1,Auto import python files in this directory
v3.4.1,batch x len x dim
v3.4.1,"feed_forward applies residual, so we remove and apply residual with un-normed"
v3.4.1,mask is now (batch x 1 x slen x slen)
v3.4.1,1 to be expanded to number of heads in MHA
v3.4.1,Run the forward pass of every layer of the tranformer.
v3.4.1,Dimensions and padding for constructing the word embedding matrix
v3.4.1,Dimensions and padding for feature embedding matrices
v3.4.1,(these have no effect if feat_vocab_sizes is empty)
v3.4.1,The embedding matrix look-up tables. The first look-up table
v3.4.1,"is for words. Subsequent ones are for features, if any exist."
v3.4.1,The final output size of word + feature vectors. This can vary
v3.4.1,from the word vector size if and only if features are defined.
v3.4.1,This is the attribute you should access if you need to know
v3.4.1,how big your embeddings are going to be.
v3.4.1,The sequence of operations that converts the input sequence
v3.4.1,into a sequence of embeddings. At minimum this consists of
v3.4.1,looking up the embeddings for each word and feature in the
v3.4.1,input. Model parameters may require the sequence to contain
v3.4.1,additional operations as well.
v3.4.1,features must use word_vec_size
v3.4.1,features will use feat_vec_size
v3.4.1,Some utilitary functions for pretrained embeddings
v3.4.1,is this reachable?
v3.4.1,Write to file
v3.4.1,set the opt in place
v3.4.1,set the opt in place
v3.4.1,flake8: noqa
v3.4.1,For command-line option parsing
v3.4.1,"Check pass, set the args."
v3.4.1,"This SRU version implements its own cuda-level optimization,"
v3.4.1,so it requires that:
v3.4.1,1. `cupy` and `pynvrtc` python package installed.
v3.4.1,2. pytorch is built with cuda support.
v3.4.1,3. library path set: export LD_LIBRARY_PATH=<cuda lib path>.
v3.4.1,Check 1.
v3.4.1,Check 2.
v3.4.1,Check 3.
v3.4.1,This sets up device to use.
v3.4.1,-> directions x batch x dim
v3.4.1,For DEBUG
v3.4.1,"size = (length, batch, x.size(-1)) \"
v3.4.1,"if x.dim() == 3 else (batch, x.size(-1))"
v3.4.1,grad_x = x.new(*x.size()) if k_ == 3 else x.new(*size).zero_()
v3.4.1,Normal use
v3.4.1,"An entry check here, will catch on train side and translate side"
v3.4.1,if requirements are not satisfied.
v3.4.1,RNNDecoderState wraps hidden as a tuple.
v3.4.1,fh -> (layers*directions) x batch x dim
v3.4.1,This class is mainly used by decoder.py for RNNs but also
v3.4.1,by the CNN / transformer decoder when copy attention is used
v3.4.1,CNN has its own attention mechanism ConvMultiStepAttention
v3.4.1,Transformer has its own MultiHeadedAttention
v3.4.1,mlp wants it with bias
v3.4.1,"(batch, t_len, d) x (batch, d, s_len) --> (batch, t_len, s_len)"
v3.4.1,"(batch, t_len, s_len, d)"
v3.4.1,one step input
v3.4.1,"compute attention scores, as in Luong et al."
v3.4.1,Softmax or sparsemax to normalize attention weights
v3.4.1,each context vector c_t is the weighted average
v3.4.1,over all the source hidden states
v3.4.1,concatenate
v3.4.1,clamping necessary because of numerical errors: loss should be lower
v3.4.1,"bounded by zero, but negative values near zero are possible without"
v3.4.1,the clamp
v3.4.1,Help functions for Rotary Embeddings
v3.4.1,https://arxiv.org/pdf/2104.09864.pdf
v3.4.1,too convoluted to make maxseqlen a parameter.
v3.4.1,we suppose src_seq_len at training and max_length at inference
v3.4.1,are both < 2048 tokens.
v3.4.1,"rope is now matrix [maxseqlen, dim/2]"
v3.4.1,Help functions for max_relative positions
v3.4.1,https://arxiv.org/abs/1803.02155
v3.4.1,Shift values to be >= 0
v3.4.1,"now relative_position is in the range [0, inf)"
v3.4.1,half of the buckets are for exact increments in positions
v3.4.1,The other half of the buckets are for logarithmically bigger bins in positions
v3.4.1,up to max_distance
v3.4.1,Help functions to split model dim per head
v3.4.1,class MultiHeadedAttention(torch.jit.ScriptModule):
v3.4.1,https://arxiv.org/pdf/1803.02155.pdf
v3.4.1,in the paper they suggest either two embeds
v3.4.1,relative_key / relative_value or only
v3.4.1,relative_key. We implemented the same embed
v3.4.1,for both.
v3.4.1,@torch.jit.script_method
v3.4.1,"1) Project key, value, and query."
v3.4.1,as a reminder at training layer_cache[0] remains False
v3.4.1,expand key on heads dimension when it's less than query heads (multi-query variant)
v3.4.1,expand value on heads dimension when it's less than query heads (multi-query variant)
v3.4.1,"2) When standard pos. enc. or rotary, use flash attention"
v3.4.1,batch x num_heads x query_len x key_len
v3.4.1,1 or key_len x key_len
v3.4.1,1 or key_len x key_len x dim_per_head
v3.4.1,not 100% necessary but expand to nb of heads
v3.4.1,now mask and scores have the same shape
v3.4.1,3) Apply attention dropout and compute context vectors.
v3.4.1,We use the same embeddings for key and value
v3.4.1,--------------------------------------------------------------------------
v3.4.1,copied and adapted https://github.com/microsoft/LoRA/
v3.4.1,Copyright (c) Microsoft Corporation. All rights reserved.
v3.4.1,Licensed under the MIT License (MIT).
v3.4.1,Support bnb quantization of nderlying layers
v3.4.1,--------------------------------------------------------------------------
v3.4.1,Optional dropout
v3.4.1,Mark the weight as unmerged
v3.4.1,LoRA implemented in a dense layer
v3.4.1,Actual trainable parameters
v3.4.1,Freezing the pre-trained weight matrix
v3.4.1,initialize A the same way as the default
v3.4.1,for nn.Linear and B to zero
v3.4.1,Make sure that the weights are not merged
v3.4.1,Merge the weights and mark it
v3.4.1,Actual trainable parameters
v3.4.1,Freezing the pre-trained weight matrix
v3.4.1,we do not super().reset_parameters() save lot of time and useless when no grad.
v3.4.1,initialize A the same way as the default
v3.4.1,for nn.Linear and B to zero
v3.4.1,Make sure that the weights are not merged
v3.4.1,Merge the weights and mark it
v3.4.1,cannot merge/unmerge quantized weigts with unquantized lora_X
v3.4.1,Check if QLoraLinear has a custom __init__ method
v3.4.1,Invoke the __init__ method of QLoraLinear
v3.4.1,LoRA implemented in a dense layer
v3.4.1,At the moment this class is only used by embeddings.Embeddings look-up tables
v3.4.1,"for silu, see: https://arxiv.org/pdf/2002.05202.pdf"
v3.4.1,-*- coding: utf-8 -*-
v3.4.1,class AverageAttention(torch.jit.ScriptModule):
v3.4.1,@torch.jit.script
v3.4.1,Code taken from bitsandbytes but modified with arg device to accept skipt_init
v3.4.1,from torch.nn.utils => makes model building way faster.
v3.4.1,"weights are cast automatically as Int8Params, but the bias has to be cast manually"
v3.4.1,reorder weight layout back from ampere/turing to row
v3.4.1,"we only need to save SCB as extra data, because CB for quantized weights"
v3.4.1,is already stored in weight.data
v3.4.1,"case 1: .cuda was called, SCB is in self.weight"
v3.4.1,"case 2: self.init_8bit_state was called, SCB is in self.state"
v3.4.1,"buffers not yet initialized, can't call them directly without"
v3.4.1,"weights are cast automatically as Int8Params, but the bias has to be cast manually"
v3.4.1,we converted 8-bit row major to turing/ampere format in the first inference pass
v3.4.1,we no longer need the row-major weight
v3.4.1,out_features * in_features
v3.4.1,norm is out_features * 1
v3.4.1,batch_size * out_features
v3.4.1,out_features
v3.4.1,out_features
v3.4.1,batch_size * out_features
v3.4.1,"out_channels, in_channels // groups, * kernel_size"
v3.4.1,out_features
v3.4.1,This is used nowhere in the code at the moment (Vincent Nguyen 05/18/2018)
v3.4.1,"in_channels, out_channels, *kernel_size"
v3.4.1,"in_channels, out_channels, *kernel_size"
v3.4.1,"self.out_channels, 1"
v3.4.1,out_features
v3.4.1,out_features
v3.4.1,store roots on diagonal
v3.4.1,Original probabilities.
v3.4.1,Probability of copying p(z=1) batch.
v3.4.1,Probability of not copying: p_{word}(w) * (1 - p(z))
v3.4.1,probabilities assigned by the model to the gold targets
v3.4.1,probability of tokens copied from source
v3.4.1,Set scores for unk to 0 and add eps
v3.4.1,find the indices in which you do not use the copy mechanism
v3.4.1,Drop padding.
v3.4.1,We exclude tokenization for contractions in
v3.4.1,order to avoid inconsistencies with pyonmtok's tokenization.
v3.4.1,"(e.g. ""I ca n't"" with spacy, ""I can ' t"" with pyonmttok)"
v3.4.1,Use Spacy's stopwords to get rid of junk entries
v3.4.1,Perform tokenization with spacy for consistency.
v3.4.1,We ensure that the target lemma is present in the lemmatized
v3.4.1,"target string, that the match is an exact match (there is"
v3.4.1,whitespace before or after the term)
v3.4.1,and we perform some bound checking.
v3.4.1,Map the lemmatized string match index to
v3.4.1,the lemmatized list index
v3.4.1,We need to know if the term is multiword
v3.4.1,Join multiword target lemmas with a unique separator so
v3.4.1,we can treat them as single word and not change the indices.
v3.4.1,Construct the final source from the lemmatized list
v3.4.1,that contains the terms. We compare the tokens in the
v3.4.1,term-augmented lemma list with the tokens in the original
v3.4.1,"lemma list. If the lemma is the same, then we replace with"
v3.4.1,the token from the original tokenized source list. If they
v3.4.1,"are not the same, it means the lemma has been augemented"
v3.4.1,"with a term, so we inject this in the final list."
v3.4.1,Restore the spaces in multi-word terms
v3.4.1,Skip half examples to improve performance. This means we set
v3.4.1,"a hard limit for the `term_corpus_ratio` to 0.5, which is actually"
v3.4.1,quite high. TODO: We can add this (skipping examples) as an option
v3.4.1,Filter out very short or very long sentences
v3.4.1,from the TM for better performance
v3.4.1,We split the `batch` and perform fuzzy matching
v3.4.1,in smaller chunks of 10.000 examples in order to
v3.4.1,reduce memory usage.
v3.4.1,Perfomance is not affected.
v3.4.1,Probably redundant but let's be safe
v3.4.1,in case some examples are already fuzzied
v3.4.1,(e.g. from another pipeline or workflow)
v3.4.1,We don't want exact matches
v3.4.1,Apply a basic filtering to leave out very short or very long
v3.4.1,sentences and speed up things a bit during fuzzy matching
v3.4.1,Do nothing
v3.4.1,We set the start number of tags to a random number from 1
v3.4.1,to 12 + the number of subsequent tags that
v3.4.1,will be added. We also apply weights to this choice so tags
v3.4.1,"are more probable to start from 1, then from 2, etc."
v3.4.1,This way we cover most scenarios met in real usage and
v3.4.1,the system will learn to handle a fairly large number of
v3.4.1,numbered tags (but not an excessively large number)
v3.4.1,Make sure we only search for exact matches (we don't want
v3.4.1,to match part of words) and perform some bound checking
v3.4.1,Create all possible tag forms. We inject a special
v3.4.1,unicode char (âˆ¥) as a placeholder for whitespace in order
v3.4.1,to keep the indices unaltered. This char is replaced with
v3.4.1,spaces before we return the augmented examples.
v3.4.1,Make a weighted choice between paired tags or single tags.
v3.4.1,"We usually encounter, and thus here we favor, paired tags"
v3.4.1,with a ratio 1/3.
v3.4.1,Check if the tags include the
v3.4.1,"mandatory ""#"" number placeholder"""
v3.4.1,We split the user-defined tags in the # placeholder
v3.4.1,in order to number them
v3.4.1,Skip half examples to speed up the transform. This sets
v3.4.1,"a hard limit of 0.5 to the `tags_corpus_ratio`, which is"
v3.4.1,excessive and should be avoided anyway.
v3.4.1,normalize dict src/tgt for each dataset
v3.4.1,"print(""src empty"")"
v3.4.1,"print(""too many same char in src"")"
v3.4.1,"print(""too many same word in src"")"
v3.4.1,"print(""avg token min"", len(src_str) / len(ex['src']))"
v3.4.1,"print(""avg token max"", len(src_str) / len(ex['src']))"
v3.4.1,"print(""text does not fully belong to wanted script"")"
v3.4.1,"print(""Some text belong to unwanted scripts"")"
v3.4.1,"print(""langid does not match"", _id(src_str))"
v3.4.1,"print(""src = tgt"")"
v3.4.1,"print(""tgt empty"")"
v3.4.1,"print(""src / tgt ratio "", len(src_str) / len(tgt_str))"
v3.4.1,"print(""too many same char in tgt"")"
v3.4.1,"print(""too many same word in tgt"")"
v3.4.1,"print(""avg token min"", len(tgt_str) / len(ex['tgt']))"
v3.4.1,"print(""avg token max"", len(tgt_str) / len(ex['tgt']))"
v3.4.1,"print(""text does not fully belong to wanted script"")"
v3.4.1,"print(""Some text belong to unwanted scripts"")"
v3.4.1,"print(""langid does not match"", _id(tgt_str))"
v3.4.1,"doc break we add it, restart new doc"
v3.4.1,case 1st ex is already longer
v3.4.1,adding cur ex is too long we add cur doc
v3.4.1,and reset doc to cur ex
v3.4.1,we start the new doc with cur ex
v3.4.1,we cumulate cur ex to cur doc
v3.4.1,Auto import python files in this directory
v3.4.1,1. sample number of tokens to corrupt
v3.4.1,2. sample positions to corrput
v3.4.1,3. sample corrupted values
v3.4.1,1. sample number of tokens to corrupt
v3.4.1,2. sample positions to corrput
v3.4.1,3. Drop token on chosen position
v3.4.1,1. sample number of tokens to corrupt
v3.4.1,2. sample positions to corrput
v3.4.1,3. mask word on chosen position
v3.4.1,"Sharing options among `TokenizerTransform`s, same name conflict in"
v3.4.1,this scope will be resolved by remove previous occurrence in parser
v3.4.1,subword regularization(or BPE dropout) options:
v3.4.1,subword vocabulary restriction options:
v3.4.1,This method embeds a custom logic to correctly handle certain placeholders
v3.4.1,in case the tokenizer doesn't preserve them.
v3.4.1,Locate the end-of-sentence placeholders.
v3.4.1,Tokenize each sentence separately.
v3.4.1,Locate the mask-before placeholders
v3.4.1,(to zero-out the prompt loss during LM finetuning).
v3.4.1,Tokenize each chunk separately and insert the padding token.
v3.4.1,between each sequence of tokens.
v3.4.1,Re-insert the eos token.
v3.4.1,derterministic subwording
v3.4.1,subword sampling when nbest_size > 1 or -1
v3.4.1,alpha should be 0.0 < alpha < 1.0
v3.4.1,Load vocabulary file if provided and set threshold
v3.4.1,Load Subword Model
v3.4.1,-1: keep everything (i.e. 1 mask per token)
v3.4.1,0: replace everything (i.e. no mask)
v3.4.1,1: 1 mask per span
v3.4.1,view each subword as word start / input is word level token
v3.4.1,Pretend it ends with a full stop so last span is a sentence
v3.4.1,"Tokens that are full stops, where the previous token is not"
v3.4.1,Make sure we have enough to mask
v3.4.1,Trim to masking budget
v3.4.1,Handle 0-length mask (inserts) separately
v3.4.1,assert is_word_start[-1] == 0
v3.4.1,assert tokens_length - 1 not in indices
v3.4.1,"keep index, but replace it with [MASK]"
v3.4.1,"acts as a long length, so spans don't go over the end of doc"
v3.4.1,next position from each word_start
v3.4.1,delete token: 1 mask/remove per span
v3.4.1,"keep index, but replace it with [MASK]: 1 mask per token"
v3.4.1,A bit faster when all lengths are 1
v3.4.1,to cover whole token
v3.4.1,delete token
v3.4.1,"keep index, but replace it with [MASK]"
v3.4.1,assert tokens_length - 1 not in indices
v3.4.1,prefix src/tgt for each dataset
v3.4.1,prefix as general option for inference
v3.4.1,suffix src/tgt for each dataset
v3.4.1,suffix as general option for inference
v3.4.1,!/usr/bin/env python3
v3.4.1,-*- coding: utf-8 -*-
v3.4.1,Most code taken from: https://github.com/alvations/sacremoses
v3.4.1,Which in turn is based on the Moses punctuation normalizer.
v3.4.1,https://github.com/moses-smt/mosesdecoder/blob/master/scripts/
v3.4.1,tokenizer/normalize-punctuation.perl
v3.4.1,don't fix period at end of sentence
v3.4.1,Regex substitutions from replace-unicode-punctuation.perl
v3.4.1,https://github.com/moses-smt/mosesdecoder/blob/master/
v3.4.1,scripts/tokenizer/replace-unicode-punctuation.perl
v3.4.1,Adds the penn substitutions after extra_whitespace regexes.
v3.4.1,"Optionally, replace unicode puncts BEFORE normalization."
v3.4.1,Actual normalization.
v3.4.1,"Optionally, replace unicode puncts BEFORE normalization."
v3.4.1,normalize dict src/tgt for each dataset
v3.4.1,One source feature expected but none given and no default provided
v3.4.1,Provided default does not match required features
v3.4.1,Data not properly annotated.
v3.4.1,In this case we do not use the default as it might be an error
v3.4.1,batch 0 will always predict EOS. The other batches will predict
v3.4.1,non-eos scores.
v3.4.1,"""best"" prediction is eos - that should be blocked"
v3.4.1,include at least one prediction OTHER than EOS
v3.4.1,that is greater than -1e20
v3.4.1,now batch 0 has ended and no others have
v3.4.1,initial step
v3.4.1,batch 0 dies on step 0
v3.4.1,include at least one prediction OTHER than EOS
v3.4.1,that is greater than -1e20
v3.4.1,step 2
v3.4.1,(old) batch 8 dies on step 1
v3.4.1,step 3
v3.4.1,everything dies
v3.4.1,initial step
v3.4.1,batch 0 dies on step 0
v3.4.1,include at least one prediction OTHER than EOS
v3.4.1,that is greater than -1e20
v3.4.1,step 2
v3.4.1,(old) batch 8 dies on step 1
v3.4.1,step 3
v3.4.1,everything dies
v3.4.1,initial step
v3.4.1,finish one beam
v3.4.1,include at least one prediction OTHER than EOS
v3.4.1,that is greater than -1e20
v3.4.1,step 2
v3.4.1,finish example in last batch
v3.4.1,(old) batch 8 dies on step 1
v3.4.1,step 3
v3.4.1,everything dies
v3.4.1,initial step
v3.4.1,batch 0 dies on step 0
v3.4.1,include at least one prediction OTHER than EOS
v3.4.1,that is greater than -1e20
v3.4.1,step 2
v3.4.1,(old) batch 8 dies on step 1
v3.4.1,step 3
v3.4.1,everything dies
v3.4.1,illegal_weights_mask = torch.ByteTensor([
v3.4.1,"[0, 0, 0, 0, 0, 0, 0],"
v3.4.1,"[0, 0, 0, 1, 1, 1, 1],"
v3.4.1,"[0, 0, 0, 0, 0, 1, 1],"
v3.4.1,"[0, 0, 1, 1, 1, 1, 1]])"
v3.4.1,TODO: fix for pytorch 0.3
v3.4.1,illegal_weights = alignments.masked_select(illegal_weights_mask)
v3.4.1,"self.assertEqual(0.0, illegal_weights.data.sum())"
v3.4.1,this could be considered an integration test because it touches
v3.4.1,the filesystem for the config file (and the models)
v3.4.1,no dummy prefix
v3.4.1,no dummy prefix
v3.4.1,make sure the scalars are in the event accumulator tags
v3.4.1,required arguments
v3.4.1,transforms that require vocab will not create if not provide vocab
v3.4.1,1. Init first transform in the pipe
v3.4.1,2. Init second transform in the pipe
v3.4.1,3. Sequential combine them into a transform pipe
v3.4.1,4. apply transform pipe for example
v3.4.1,"5. example after the pipe exceed the length limit, thus filtered"
v3.4.1,6. Transform statistics registed (here for filtertoolong)
v3.4.1,"7. after report, statistics become empty as a fresh start"
v3.4.1,filter_transform.warm_up()
v3.4.1,test BPE-dropout:
v3.4.1,1. disable bpe dropout for not training example
v3.4.1,2. enable bpe dropout for training example
v3.4.1,3. (NOTE) disable dropout won't take effect if already seen
v3.4.1,this is caused by the cache mechanism in bpe:
v3.4.1,return cached subword if the original token is seen when no dropout
v3.4.1,test SP regularization:
v3.4.1,1. enable regularization for training example
v3.4.1,2. disable regularization for not training example
v3.4.1,Test mask location
v3.4.1,Test mask location
v3.4.1,Test mask location
v3.4.1,Not apply token drop for not training example
v3.4.1,apply token drop for training example
v3.4.1,Not apply token mask for not training example
v3.4.1,apply token mask for training example
v3.4.1,require vocabs to warm_up
v3.4.1,Not apply token mask for not training example
v3.4.1,apply token mask for training example
v3.4.1,"Defalt: full_stop_token=[""."", ""?"", ""!""]"
v3.4.1,"Defalt: full_stop_token=[""."", ""?"", ""!""]"
v3.4.1,random_ratio of inserted tokens are chosen in vocab
v3.4.1,others are MASK_TOK
v3.4.1,"insert_ratio=0.0,"
v3.4.1,"random_ratio=0.0,"
v3.4.1,"Defalt: full_stop_token=[""."", ""?"", ""!""]"
v3.4.1,all token are considered as an individual word
v3.4.1,1. tokens are dropped when replace_length is 0
v3.4.1,"print(f""token delete: {masked} / {tokens}"")"
v3.4.1,2. tokens are replaced by MASK when replace_length is 1
v3.4.1,"print(f""token mask: {masked} / {tokens}"")"
v3.4.1,"insert_ratio=0.0,"
v3.4.1,"random_ratio=0.0,"
v3.4.1,"Defalt: full_stop_token=[""."", ""?"", ""!""]"
v3.4.1,start token of word are identified using subword marker
v3.4.1,"1. replace_length 0: ""words"" are dropped"
v3.4.1,"print(f""word delete: {masked} / {tokens}"")"
v3.4.1,"self.assertEqual(len(masked), n_words - n_masked)"
v3.4.1,"2. replace_length 1: ""words"" are replaced with a single MASK"
v3.4.1,"print(f""whole word single mask: {masked} / {tokens}"")"
v3.4.1,len(masked) depend on number of tokens in select word
v3.4.1,"3. replace_length -1: all tokens in ""words"" are replaced with MASK"
v3.4.1,"print(f""whole word multi mask: {masked} / {tokens}"")"
v3.4.1,number of mask_tok depend on number of tokens in selected word
v3.4.1,number of MASK_TOK can be greater than n_masked
v3.4.1,"insert_ratio=0.5,"
v3.4.1,"random_ratio=0.3,"
v3.4.1,"Defalt: full_stop_token=[""."", ""?"", ""!""]"
v3.4.1,start token of word are identified using subword marker
v3.4.1,n_words = sum(token_starts)
v3.4.1,n_masked = math.ceil(n_words * bart_noise.mask_ratio)
v3.4.1,"print(f""Text Span Infilling: {infillied} / {tokens}"")"
v3.4.1,"print(n_words, n_masked)"
v3.4.1,!/usr/bin/env python
v3.4.1,-*- coding: utf-8 -*-
v3.4.1,Inject some dummy training options that may needed when build fields
v3.4.1,Remove the generated *pt files.
v3.4.1,Remove the generated data samples
v3.4.1,all beams repeat (beam >= 1 repeat dummy scores)
v3.4.1,predict repeat_idx over and over again
v3.4.1,"before repeat, scores are either 0 or -inf"
v3.4.1,"on repeat, `repeat_idx` score is BLOCKED_SCORE"
v3.4.1,"(but it's still the best score, thus we have"
v3.4.1,"[BLOCKED_SCORE, -inf, -inf, -inf, -inf]"
v3.4.1,repetitions keeps maximizing score
v3.4.1,"index 0 has been blocked, so repeating=>+0.0 score"
v3.4.1,other indexes are -inf so repeating=>BLOCKED_SCORE
v3.4.1,which is higher
v3.4.1,beam 0 and beam >=2 will repeat (beam >= 2 repeat dummy scores)
v3.4.1,non-interesting beams are going to get dummy values
v3.4.1,"on initial round, only predicted scores for beam 0"
v3.4.1,matter. Make two predictions. Top one will be repeated
v3.4.1,"in beam zero, second one will live on in beam 1."
v3.4.1,predict the same thing in beam 0
v3.4.1,continue pushing around what beam 1 predicts
v3.4.1,"now beam 0 dies (along with the others), beam 1 -> beam 0"
v3.4.1,"now beam 0 dies (along with the others), beam 1 -> beam 0"
v3.4.1,beam 0 and beam >= 2 will repeat (beam 2 repeats excluded idx)
v3.4.1,non-interesting beams are going to get dummy values
v3.4.1,predict the same thing in beam 0
v3.4.1,continue pushing around what beam 1 predicts
v3.4.1,predict the allowed-repeat again in beam 2
v3.4.1,"now beam 0 dies, beam 1 -> beam 0, beam 2 -> beam 1"
v3.4.1,and the rest die
v3.4.1,"since all preds after i=0 are 0, we can check"
v3.4.1,that the beam is the correct idx by checking that
v3.4.1,the curr score is the initial score
v3.4.1,beam 0 will always predict EOS. The other beams will predict
v3.4.1,non-eos scores.
v3.4.1,non-interesting beams are going to get dummy values
v3.4.1,"""best"" prediction is eos - that should be blocked"
v3.4.1,include at least beam_sz predictions OTHER than EOS
v3.4.1,that are greater than -1e20
v3.4.1,predict eos in beam 0
v3.4.1,provide beam_sz other good predictions
v3.4.1,now the top beam has ended and no others have
v3.4.1,"not of interest, but want to make sure it keeps running"
v3.4.1,since only beam 0 terminates and n_best = 2
v3.4.1,"this is also a test that when block_ngram_repeat=0,"
v3.4.1,repeating is acceptable
v3.4.1,non-interesting beams are going to get dummy values
v3.4.1,"""best"" prediction is eos - that should be blocked"
v3.4.1,include at least beam_sz predictions OTHER than EOS
v3.4.1,that are greater than -1e20
v3.4.1,predict eos in beam 1
v3.4.1,provide beam_sz other good predictions in other beams
v3.4.1,beam 1 dies on min_length
v3.4.1,beam 0 dies on the step after beam 1 dies
v3.4.1,"inp_lens is tiled in initialize, reassign to make attn match"
v3.4.1,non-interesting beams are going to get dummy values
v3.4.1,"""best"" prediction is eos - that should be blocked"
v3.4.1,include at least beam_sz predictions OTHER than EOS
v3.4.1,that are greater than -1e20
v3.4.1,predict eos in beam 1
v3.4.1,provide beam_sz other good predictions in other beams
v3.4.1,no top beams are finished yet
v3.4.1,beam 1 dies on min_length
v3.4.1,no top beams are finished yet
v3.4.1,beam 0 dies on the step after beam 1 dies
v3.4.1,top beam is finished now so there are attentions
v3.4.1,two beams are finished in each batch
v3.4.1,second dim is cut down to the non-padded src length
v3.4.1,first dim is equal to the time of death
v3.4.1,(beam 0 died at current step - adjust for SOS)
v3.4.1,(beam 1 died at last step - adjust for SOS)
v3.4.1,behavior gets weird when beam is already done so just stop
v3.4.1,this is just test_beam.TestBeamAgainstReferenceCase repeated
v3.4.1,in each batch.
v3.4.1,"init_preds: [4, 3, 5, 6, 7] - no EOS's"
v3.4.1,no EOS's yet
v3.4.1,"[5, 3, 2, 6, 0], so beam 2 predicts EOS!"
v3.4.1,assumes beam 2 finished on last step
v3.4.1,ended beam 2 shouldn't continue
v3.4.1,"[2, 5, 3, 6, 0] repeat self.BATCH_SZ, so beam 0 predicts EOS!"
v3.4.1,"[-2.4879, -3.8910, -4.1010, -4.2010, -4.4010] repeat self.BATCH_SZ"
v3.4.1,another beam is finished in all batches
v3.4.1,new beam 0 finished
v3.4.1,new beam 0 is old beam 3
v3.4.1,assumes beam 0 finished on last step
v3.4.1,"[5, 2, 6, 1, 0] repeat self.BATCH_SZ, so beam 1 predicts EOS!"
v3.4.1,we finish 3 hyps per example in this step
v3.4.1,new beam 1 is old beam 3
v3.4.1,this could be considered an integration test because it tests
v3.4.1,interactions between the GNMT scorer and the beam
v3.4.1,"-data option is required, but not used in this test, so dummy."
v3.4.1,len x batch x nfeat
v3.4.1,Initialize vectors to compare size with
v3.4.1,Ensure correct sizes and types
v3.4.1,Make sure that output has the correct size and type
v3.4.1,"[('encoder_type', 'transformer'),"
v3.4.1,"('word_vec_size', 16), ('hidden_size', 16)],"
v3.4.1,""""""" Only do SRU test if requirment is safisfied. """""""
v3.4.1,SRU doesn't support input_feed.
v3.4.1,first check there's nothing unexpectedly not trainable
v3.4.1,ok: word embeddings shouldn't be trainable
v3.4.1,if word vecs are freezed
v3.4.1,ok: positional encodings shouldn't be trainable
v3.4.1,then check nothing unexpectedly trainable
v3.4.1,Decoder state
v3.4.1,Build the RNN.
v3.4.1,Set up the context gate.
v3.4.1,Set up the standard attention.
v3.4.1,The encoder hidden is  (layers*directions) x batch x dim.
v3.4.1,We need to convert it to layers x batch x (directions*dim).
v3.4.1,Init the input feed.
v3.4.1,Update the state with the result.
v3.4.1,Concatenates sequence of tensors along a new dimension.
v3.4.1,NOTE: v0.3 to 0.4: dec_outs / attns[*] may not be list
v3.4.1,(in particular in case of SRU) it was not raising error in 0.3
v3.4.1,since stack(Variable) was allowed.
v3.4.1,"In 0.4, SRU returns a tensor that shouldn't be stacke"
v3.4.1,Calculate the attention.
v3.4.1,Calculate the context gate.
v3.4.1,Additional args check.
v3.4.1,Input feed concatenates hidden state with
v3.4.1,input at every time step.
v3.4.1,TODO: context gate should be employed
v3.4.1,instead of second RNN transform.
v3.4.1,Update the coverage attention.
v3.4.1,"attns[""coverage""] is actually c^(t+1) of See et al(2017)"
v3.4.1,1-index shifted
v3.4.1,Decoder State
v3.4.1,CNNDecoder has its own attention mechanism.
v3.4.1,Set up a separate copy attention layer if needed.
v3.4.1,The output of CNNEncoder.
v3.4.1,The combination of output of CNNEncoder and source embeddings.
v3.4.1,Process the result and update the attentions.
v3.4.1,Update the state.
v3.4.1,TODO change the way attns is returned dict => list or tuple (onnx)
v3.4.1,Auto import python files in this directory
v3.4.1,src_len is a single tensor shared between all models.
v3.4.1,This assumption will not hold if Translator is modified
v3.4.1,to calculate src_len as something other than the length
v3.4.1,of the input.
v3.4.1,"return _, (B, Q_len, K_len)"
v3.4.1,"layer average attention across heads, get ``(B, Q, K)``"
v3.4.1,"Case 1: no full_context, no align heads -> layer avg baseline"
v3.4.1,"Case 2: no full_context, 1 align heads -> guided align"
v3.4.1,"Case 3: full_context, 1 align heads -> full cte guided align"
v3.4.1,BoolTensor was introduced in pytorch 1.2
v3.4.1,T: could be 1 in the case of stepwise decoding or tgt_len
v3.4.1,masking is necessary when sequence length is greater than one
v3.4.1,mask now are (batch x 1 x tlen x s or t len)
v3.4.1,1 = heads to be expanded in MHA
v3.4.1,"feed_forward applies residual, so we remove and apply residual with un-normed"
v3.4.1,Decoder State
v3.4.1,"previously, there was a GlobalAttention module here for copy"
v3.4.1,"attention. But it was never actually used -- the ""copy"" attention"
v3.4.1,just reuses the context attention.
v3.4.1,"attns[""align""] = torch.stack(attn_aligns, 0).mean(0)  # All avg"
v3.4.1,TODO change the way attns is returned dict => list or tuple (onnx)
v3.4.1,first value set to True triggered by the beginning of decoding
v3.4.1,layer_cache becomes active in the MultiHeadedAttention fwd
v3.4.1,T: could be 1 in the case of stepwise decoding or tgt_len
v3.4.1,masking is necessary when sequence length is greater than one
v3.4.1,mask now are (batch x 1 x tlen x tlen)
v3.4.1,1 = heads to be expanded in MHA
v3.4.1,"feed_forward applies residual, so we remove and apply residual with un-normed"
v3.4.1,TODO change the way attns is returned dict => list or tuple (onnx)
v3.4.1,"buffer size in bytes, determine equiv. # of elements based on data type"
v3.4.1,copy tensors into buffer_t
v3.4.1,all-reduce and rescale
v3.4.1,copy all-reduced buffer back into tensors
v3.4.1,"print(filled, sz)"
v3.4.1,"tensor is bigger than buffer, all-reduce and rescale directly"
v3.4.1,"buffer is full, all-reduce and replace buffer with grad"
v3.4.1,add tensor to buffer
v3.4.1,"propagate exception to parent process, keeping original traceback"
v3.4.1,"propagate exception to parent process, keeping original traceback"
v3.4.1,TODO: Find a better way to check for sparse gradients.
v3.4.1,we use apex.amp
v3.4.1,In this case use the old FusedAdam with
v3.4.1,FP16_optimizer wrapper
v3.4.1,Load everything from the checkpoint.
v3.4.1,Build everything from scratch.
v3.4.1,"Reset optimizer, keep options."
v3.4.1,"Reset options, keep optimizer."
v3.4.1,State can be partially restored.
v3.4.1,should be: self._optimizer.zero_grad(set_to_none)
v3.4.1,but apex.amp is not up-to-date:
v3.4.1,https://github.com/NVIDIA/apex/blob/master/apex/amp/_process_optimizer.py#L367
v3.4.1,"unscaled optimizer's gradients (already done therefore skip),"
v3.4.1,skips optimizer.step() if gradients contain infs/NaNs.
v3.4.1,Updates the scale for next iteration.
v3.4.1,Code below is an implementation of https://arxiv.org/pdf/1804.04235.pdf
v3.4.1,inspired but modified from https://github.com/DeadAt0m/adafactor-pytorch
v3.4.1,backward compatibility
v3.4.1,assuming a list/generator of parameter means single group
v3.4.1,compute combined scale factor for this group
v3.4.1,norm is in fact norm*scale
v3.4.1,note: p.grad should not ever be set for correct operation of
v3.4.1,mixed precision optimizer that sometimes sends None gradients
v3.4.1,State initialization
v3.4.1,Exponential moving average of gradient values
v3.4.1,Exponential moving average of squared gradient values
v3.4.1,-*- coding: utf-8 -*-
v3.4.1,placing this here make it easier to call logger.info
v3.4.1,"from anywhere, just 'from onmt.utils.logging import logger'"
v3.4.1,"align_head contains value in [0, 1) presenting attn prob,"
v3.4.1,0 was resulted by the context attention src_pad_mask
v3.4.1,"So, the correspand position in ref_align should also be 0"
v3.4.1,"Therefore, clip align_head to > 1e-18 should be bias free."
v3.4.1,rescale with tau (temperature) and apply the log_softmax.
v3.4.1,ct2 expects src with lengths without padding
v3.4.1,again we use raw probs to rescale with tau and apply log_softmax
v3.4.1,lm_scores are in log space so log_target=True
v3.4.1,rescale with tau (temperature) and apply the log_softmax.
v3.4.1,ct2 expects src with lengths without padding
v3.4.1,again we use raw probs to rescale with tau and apply log_softmax
v3.4.1,lm_scores are in log space so log_target=True
v3.4.1,Create a mask with zeros at prompt positions and ones at answer postions.
v3.4.1,Apply the mask on the target side.
v3.4.1,Put the padding token index at the prompt positions.
v3.4.1,take into account here the tgt_shift_index (0 / 1 = LM/NMT)
v3.4.1,Correct target copy token instead of <unk>
v3.4.1,tgt[i] = align[i] + len(tgt_vocab)
v3.4.1,for i such that tgt[i] == 0 and align[i] != 0
v3.4.1,in the case criterion reduction is None then we need
v3.4.1,to sum the loss of each sentence in the batch
v3.4.1,Check Transforms
v3.4.1,Check path
v3.4.1,tgt is src for LM task
v3.4.1,Check weight
v3.4.1,Check features
v3.4.1,validation when train:
v3.4.1,Check embeddings stuff
v3.4.1,"Backward compatibility with ""fix_word_vecs_*"" opts"
v3.4.1,encoder and decoder should be same sizes
v3.4.1,"Load default opt values, then overwrite with the opts in"
v3.4.1,"the checkpoint. That way, if there are new options added,"
v3.4.1,the defaults are used.
v3.4.1,It comes from training
v3.4.1,TODO: needs to be added as inference opt
v3.4.1,Don't do anything
v3.4.1,Update best score of each criteria
v3.4.1,Reset tolerance
v3.4.1,Update current status
v3.4.1,Decrease tolerance
v3.4.1,Log
v3.4.1,Log
v3.4.1,Get a list of world_size lists with len(stat_list) Statistics objects
v3.4.1,"this param init is overridden by model_builder, useless then."
v3.4.1,SRU doesn't support PackedSequence.
v3.4.1,-*- coding: utf-8 -*-
v3.4.1,threshold on 1 to avoid div by 0
v3.4.1,treat alignment matrix one by one as each have different lengths
v3.4.1,No alignment if not exist valid tgt token
v3.4.1,get valid alignment (sub-matrix from full paded aligment matrix)
v3.4.1,Helper functions
v3.4.1,Keeps track of the original words/subwords
v3.4.1,('prior_tokenization' option)
v3.4.1,In case there is a final case_markup when new_spacer is on
v3.4.1,########## #
v3.4.1,Translator #
v3.4.1,########## #
v3.4.1,Set translation options
v3.4.1,Build translator from options
v3.4.1,################### #
v3.4.1,Validation iterator #
v3.4.1,################### #
v3.4.1,Reinstantiate the validation iterator
v3.4.1,Retrieve raw references and sources
v3.4.1,########### #
v3.4.1,Predictions #
v3.4.1,########### #
v3.4.1,####### #
v3.4.1,Outputs #
v3.4.1,####### #
v3.4.1,Flatten predictions
v3.4.1,Save results
v3.4.1,-*- coding: utf-8 -*-
v3.4.1,this one is needed for Random Shuffler of batches
v3.4.1,in multi gpu it ensures datasets are read in the same order
v3.4.1,some cudnn methods can be random even after fixing the seed
v3.4.1,unless you tell it to be deterministic
v3.4.1,This one is needed for various tranfroms
v3.4.1,These ensure same initialization in multi gpu mode
v3.4.1,we need to check the model path + any tokenizer path
v3.4.1,patch to log stdout spawned processes of dataloader
v3.4.1,bucket_size = batch_size
v3.4.1,For TRAIN we need to group examples by length
v3.4.1,"for faster performance, but otherwise, sequential."
v3.4.1,For TRAIN we shuffle batches within the bucket
v3.4.1,otherwise sequential
v3.4.1,for specific case of rnn_packed need to be sorted
v3.4.1,within the batch
v3.4.1,Check if all tokens have features or none at all
v3.4.1,Make features part of src like
v3.4.1,"{'src': {'src': ..., 'feats': [...., ....]}}"
v3.4.1,at this point an example looks like:
v3.4.1,"{'src': {'src': ..., 'feats': [....]},"
v3.4.1,"'tgt': {'tgt': ...},"
v3.4.1,"'src_original': ['tok1', ...'tokn'],"
v3.4.1,"'tgt_original': ['tok1', ...'tokm'],"
v3.4.1,'indices' : seq in bucket
v3.4.1,"'align': ...,"
v3.4.1,}
v3.4.1,Need to add features in last dimensions
v3.4.1,Keep it consistent with dynamic data
v3.4.1,make a small vocab containing just the tokens in the source sequence
v3.4.1,Map source tokens to indices in the dynamic dict.
v3.4.1,-*- coding: utf-8 -*-
v3.4.1,temporary as long as translation_server and scoring_preparator still use lists
v3.4.1,this is hack: if the special separator ï½Ÿnewlineï½ is returned because of the
v3.4.1,"""docify"" transform.get_specials we don't add it if the corresponding newline code"
v3.4.1,is already included in the sentencepiece or BPE-with-gpt2-pretok.
v3.4.1,'src_original' and 'tgt_original' store the
v3.4.1,original line before tokenization. These
v3.4.1,fields are used later on in the feature
v3.4.1,transforms.
v3.4.1,NOTE: moved to dynamic_iterator.py cf process()
v3.4.1,item = self.transform.apply(
v3.4.1,"example, is_train=self.infinitely, corpus_name=self.cid)"
v3.4.1,empty example: skip
v3.4.1,bitsandbytes quantize weights when .cuda() is called
v3.4.1,for huge models we need to save Ram
v3.4.1,so we load the weights  module by module and transfer them to GPU for quantization
v3.4.1,bitsandbytes quantize weights when .cuda() is called
v3.4.1,for huge models we need to save Ram
v3.4.1,so we load the weights  module by module and transfer them to GPU for quantization
v3.4.1,"No encoder in LM, seq2seq count formatting kept"
v3.4.1,_check_save_model_path
v3.4.1,This preserves backward-compat for models using customed layernorm
v3.4.1,Force add_ffnbias to True if bias found in model w_1 keys
v3.4.1,fix v2 compatibility
v3.4.1,end of patch for backward compatibility
v3.4.1,!/usr/bin/env python
v3.4.1,!/usr/bin/env python
v3.4.1,!/usr/bin/env python
v3.4.1,-*- coding: utf-8 -*-
v3.4.1,!/usr/bin/env python
v3.4.1,BPE training
v3.4.1,SentencePiece training
v3.4.1,!/usr/bin/env python
v3.4.1,!/usr/bin/env python
v3.4.1,Set sharing strategy manually instead of default based on the OS.
v3.4.1,torch.multiprocessing.set_sharing_strategy('file_system')
v3.4.1,Create a thread to listen for errors in the child processes.
v3.4.1,Train with multiprocessing.
v3.4.1,magic indices
v3.4.1,result caching
v3.4.1,Here we set the decoder to start with self.start (BOS or EOS)
v3.4.1,fix length constraint and remove eos from count
v3.4.1,add one to account for BOS. Don't account for EOS because hitting
v3.4.1,this implies it hasn't been found.
v3.4.1,we don't block nothing if the user doesn't want it
v3.4.1,we can't block nothing beam's too short
v3.4.1,we check paths one by one
v3.4.1,we don't forbid nothing if the user doesn't want it
v3.4.1,we can't forbid nothing if beam's too short
v3.4.1,Reordering forbidden_tokens following beam selection
v3.4.1,We rebuild a dict to ensure we get the value and not the pointer
v3.4.1,Grabing the newly selected tokens and associated ngram
v3.4.1,skip the blocking if any token in current_ngram is excluded
v3.4.1,"pickups: Tensor where specified index were set to 1, others 0"
v3.4.1,"dropdowns: opposite of pickups, 1 for those shouldn't pick"
v3.4.1,Minus dropdowns to log_probs making probabilities of
v3.4.1,unspecified index close to 0
v3.4.1,"prediction step have surpass length of given target_prefix,"
v3.4.1,no need to further change this attr
v3.4.1,keep indices until overflowing p
v3.4.1,Set all logits that are not in the top-p to -10000.
v3.4.1,This puts the probabilities close to 0.
v3.4.1,Set all logits that are not in the top-k to -10000.
v3.4.1,This puts the probabilities close to 0.
v3.4.1,"For temp=0.0, take the argmax to avoid divide-by-zero errors."
v3.4.1,keep_topk=1 is also equivalent to argmax.
v3.4.1,maybe fix some prediction at this step by modifying log_probs
v3.4.1,"shape: (sum(~ self.is_finished), 1)"
v3.4.1,in LM task src_len is associated with currently generated src
v3.4.1,and therefore needs to follow the generation
v3.4.1,!/usr/bin/env python
v3.4.1,for debugging
v3.4.1,TODO: maybe add dynamic part
v3.4.1,Statistics
v3.4.1,those two should be the same except feat dim
v3.4.1,"batch['src'][perm[j], :, :])"
v3.4.1,trans.src
v3.4.1,we rebuild a small batch made of the sub-segments
v3.4.1,in the long segment.
v3.4.1,new sub-batch ready to be translated
v3.4.1,we re-insert the sub-batch in the initial translations
v3.4.1,In the case of length_penalty = none we report the total logprobs
v3.4.1,divided by the number of sentence to get an approximation of the
v3.4.1,per sentence logprob. We also return the corresponding ppl
v3.4.1,"When a length_penalty is used eg: ""avg"" or ""wu"" since logprobs"
v3.4.1,are normalized per token we report the per line per token logprob
v3.4.1,"and the corresponding ""per word perplexity"""
v3.4.1,Turn any copied words into UNKs.
v3.4.1,"Decoder forward, takes [batch, tgt_len, nfeats] as input"
v3.4.1,"and [batch, src_len, hidden] as enc_out"
v3.4.1,"in case of inference tgt_len = 1, batch = beam times batch_size"
v3.4.1,"in case of Gold Scoring tgt_len = actual length, batch = 1 batch"
v3.4.1,Generator forward.
v3.4.1,"returns [(batch_size x beam_size) , vocab ] when 1 step"
v3.4.1,"or [batch_size, tgt_len, vocab ] when full sentence"
v3.4.1,"here we have scores [tgt_lenxbatch, vocab] or [beamxbatch, vocab]"
v3.4.1,at this point scores is batch first (dim=0)
v3.4.1,"returns [(batch_size x beam_size) , vocab ] when 1 step"
v3.4.1,"or [batch_size, tgt_len, vocab ] when full sentence"
v3.4.1,(0) add BOS and padding to tgt prediction
v3.4.1,(1) Encoder forward.
v3.4.1,(2) Repeat src objects `n_best` times.
v3.4.1,"We use batch_size x n_best, get ``(batch * n_best, src_len, nfeat)``"
v3.4.1,Quick fix. Transformers return None as enc_states.
v3.4.1,enc_states are only used later on to init decoder's state
v3.4.1,"but are never used in Transformer decoder, so we can skip"
v3.4.1,"(3) Init decoder with n_best src,"
v3.4.1,"reshape tgt to ``(len, batch * n_best, nfeat)``"
v3.4.1,it should be done in a better way
v3.4.1,here dec_in is batch first
v3.4.1,masked_select
v3.4.1,get aligned src id for each prediction's valid tgt tokens
v3.4.1,TODO: support these blacklisted features
v3.4.1,(0) Prep the components of the search.
v3.4.1,(1) Run the encoder on the src.
v3.4.1,(2) prep decode_strategy. Possibly repeat src objects.
v3.4.1,(3) Begin decoding step by step:
v3.4.1,"decoder_input = decode_strategy.current_predictions.view(1, -1,"
v3.4.1,1)
v3.4.1,Reorder states.
v3.4.1,TODO: support these blacklisted features
v3.4.1,(0) Prep the components of the search.
v3.4.1,(1) split src into src and target_prefix to avoid padding.
v3.4.1,(2) init decoder
v3.4.1,(3) prep decode_strategy. Possibly repeat src objects.
v3.4.1,(4) Begin decoding step by step:
v3.4.1,Reorder states.
v3.4.1,select indexes in model state/cache
v3.4.1,beam parameters
v3.4.1,beam state
v3.4.1,BoolTensor was introduced in pytorch 1.2
v3.4.1,"""global state"" of the old beam"
v3.4.1,buffers for the topk scores and 'backpointer'
v3.4.1,for testing
v3.4.1,maybe fix some prediction at this step by modifying log_probs
v3.4.1,Flatten probs into a list of possibilities.
v3.4.1,Penalize beams that finished.
v3.4.1,"on real data (newstest2017) with the pretrained transformer,"
v3.4.1,it's faster to not move this back to the original device
v3.4.1,Store finished hypotheses for this batch.
v3.4.1,End condition is the top beam finished and we can return
v3.4.1,n_best hypotheses.
v3.4.1,"If all sentences are translated, no need to go further."
v3.4.1,Remove finished batches for the next step.
v3.4.1,using integer division to get an integer _B without casting
v3.4.1,force the output to be longer than self.min_length
v3.4.1,Multiply probs by the beam probability.
v3.4.1,"if the sequence ends now, then the penalty is the current"
v3.4.1,"length + 1, to include the EOS token"
v3.4.1,Avoid any direction that would repeat unwanted ngrams
v3.4.1,Pick up candidate token by curr_scores
v3.4.1,Recover log probs.
v3.4.1,Length penalty is just a scalar. It doesn't matter if it's applied
v3.4.1,before or after the topk.
v3.4.1,Resolve beam origin and map to batch index flat representation.
v3.4.1,Append last prediction.
v3.4.1,update global state (step == 1)
v3.4.1,update global state (step > 1)
v3.4.1,"shape: (batch_size x beam_size, 1)"
v3.4.1,in LM task src_len is associated with currently generated src
v3.4.1,and therefore needs to follow the generation
v3.4.1,in LM task src_len is associated with currently generated src
v3.4.1,and therefore needs to follow the generation
v3.4.1,Term will be subtracted from probability
v3.4.1,Probability will be divided by this
v3.4.1,these warnings indicate that either the alpha/beta
v3.4.1,"forces a penalty to be a no-op, or a penalty is a no-op but"
v3.4.1,the alpha/beta would suggest otherwise.
v3.4.1,using some coverage penalty
v3.4.1,!/usr/bin/env python
v3.4.1,semaphore doesn't have a timeout arg in Python 2.7
v3.4.1,perform a first request to initialize everything
v3.4.1,backwards compatibility for confs
v3.4.1,every segment becomes a dict for flexibility purposes
v3.4.1,NOTE: translator returns lists of `n_best` list
v3.4.1,build back results with empty texts
v3.4.1,load can be called multiple times: modify copy
v3.4.1,output contain alignment
v3.4.1,Below are all the different penalty terms implemented so far.
v3.4.1,Subtract coverage penalty from topk log probs.
v3.4.1,Divide topk log probs by length penalty.
v3.4.1,Sorting
v3.4.1,Chinese segmentation
v3.4.1,Chinese simplify -> Chinese traditional standard
v3.4.1,Chinese simplify -> Chinese traditional (HongKong)
v3.4.1,Chinese simplify -> Chinese traditional (Taiwan)
v3.4.1,Chinese traditional -> Chinese simplify (v1)
v3.4.1,Chinese traditional -> Chinese simplify (v2)
v3.4.1,Auto import python files in this directory
v3.4.1,"def custom_stopping_criteria(input_ids, score, **kwargs):"
v3.4.1,"stop_ids = [29871, 13, 13] # \n\n"
v3.4.1,return input_ids[-len(stop_ids)]
v3.4.1,Build the translator (along with the model)
v3.4.1,get prompt and make sure it fits
v3.4.1,"def custom_stopping_criteria(input_ids, score, **kwargs):"
v3.4.1,"stop_ids = [29871, 13, 13] # \n\n"
v3.4.1,return input_ids[-len(stop_ids)]
v3.4.1,Build the translator (along with the model)
v3.4.1,Build the transforms (along with the tokenizer)
v3.4.1,get prompt and make sure it fits
v3.4.0,!/usr/bin/env python
v3.4.0,!/usr/bin/env python
v3.4.0,!/usr/bin/env python
v3.4.0,!/usr/bin/env python
v3.4.0,!/usr/bin/env python
v3.4.0,!/usr/bin/env python3
v3.4.0,-*- coding: utf-8 -*-
v3.4.0,
v3.4.0,"OpenNMT-py documentation build configuration file, created by"
v3.4.0,sphinx-quickstart on Sun Dec 17 12:07:14 2017.
v3.4.0,
v3.4.0,This file is execfile()d with the current directory set to its
v3.4.0,containing dir.
v3.4.0,
v3.4.0,Note that not all possible configuration values are present in this
v3.4.0,autogenerated file.
v3.4.0,
v3.4.0,All configuration values have a default; values that are commented out
v3.4.0,serve to show the default.
v3.4.0,"If extensions (or modules to document with autodoc) are in another directory,"
v3.4.0,add these directories to sys.path here. If the directory is relative to the
v3.4.0,"documentation root, use os.path.abspath to make it absolute, like shown here."
v3.4.0,
v3.4.0,import os
v3.4.0,import sys
v3.4.0,"sys.path.insert(0, os.path.abspath('.'))"
v3.4.0,-- General configuration ------------------------------------------------
v3.4.0,"If your documentation needs a minimal Sphinx version, state it here."
v3.4.0,
v3.4.0,needs_sphinx = '6.0'
v3.4.0,"Add any Sphinx extension module names here, as strings. They can be"
v3.4.0,extensions coming with Sphinx (named 'sphinx.ext.*') or your custom
v3.4.0,ones.
v3.4.0,Show base classes
v3.4.0,"Use ""variables"" section for Attributes instead of weird block things"
v3.4.0,mimicking the function style.
v3.4.0,"Add any paths that contain templates here, relative to this directory."
v3.4.0,The suffix(es) of source filenames.
v3.4.0,You can specify multiple suffix as a list of string:
v3.4.0,
v3.4.0,"source_suffix = ['.rst', '.md']"
v3.4.0,The master toctree document.
v3.4.0,General information about the project.
v3.4.0,"The version info for the project you're documenting, acts as replacement for"
v3.4.0,"|version| and |release|, also used in various other places throughout the"
v3.4.0,built documents.
v3.4.0,
v3.4.0,The short X.Y version.
v3.4.0,"The full version, including alpha/beta/rc tags."
v3.4.0,The language for content autogenerated by Sphinx. Refer to documentation
v3.4.0,for a list of supported languages.
v3.4.0,
v3.4.0,This is also used if you do content translation via gettext catalogs.
v3.4.0,"Usually you set ""language"" from the command line for these cases."
v3.4.0,"List of patterns, relative to source directory, that match files and"
v3.4.0,directories to ignore when looking for source files.
v3.4.0,This patterns also effect to html_static_path and html_extra_path
v3.4.0,The name of the Pygments (syntax highlighting) style to use.
v3.4.0,"If true, `todo` and `todoList` produce output, else they produce nothing."
v3.4.0,-- Options for HTML output ----------------------------------------------
v3.4.0,The theme to use for HTML and HTML Help pages.  See the documentation for
v3.4.0,a list of builtin themes.
v3.4.0,
v3.4.0,html_theme = 'sphinx_materialdesign_theme'
v3.4.0,html_theme_path = [sphinx_materialdesign_theme.get_path()]
v3.4.0,Theme options are theme-specific and customize the look and feel of a theme
v3.4.0,"further.  For a list of options available for each theme, see the"
v3.4.0,documentation.
v3.4.0,
v3.4.0,html_theme_options = {}
v3.4.0,"Add any paths that contain custom static files (such as style sheets) here,"
v3.4.0,"relative to this directory. They are copied after the builtin static files,"
v3.4.0,"so a file named ""default.css"" will overwrite the builtin ""default.css""."
v3.4.0,"Custom sidebar templates, must be a dictionary that maps document names"
v3.4.0,to template names.
v3.4.0,
v3.4.0,This is required for the alabaster theme
v3.4.0,refs: http://alabaster.readthedocs.io/en/latest/installation.html#sidebars
v3.4.0,-- Options for HTMLHelp output ------------------------------------------
v3.4.0,Output file base name for HTML help builder.
v3.4.0,-- Options for LaTeX output ---------------------------------------------
v3.4.0,The paper size ('letterpaper' or 'a4paper').
v3.4.0,
v3.4.0,"'papersize': 'letterpaper',"
v3.4.0,"The font size ('10pt', '11pt' or '12pt')."
v3.4.0,
v3.4.0,"'pointsize': '10pt',"
v3.4.0,Additional stuff for the LaTeX preamble.
v3.4.0,
v3.4.0,"'preamble': '',"
v3.4.0,Latex figure (float) alignment
v3.4.0,
v3.4.0,"'figure_align': 'htbp',"
v3.4.0,Grouping the document tree into LaTeX files. List of tuples
v3.4.0,"(source start file, target name, title,"
v3.4.0,"author, documentclass [howto, manual, or own class])."
v3.4.0,-- Options for manual page output ---------------------------------------
v3.4.0,One entry per manual page. List of tuples
v3.4.0,"(source start file, name, description, authors, manual section)."
v3.4.0,-- Options for Texinfo output -------------------------------------------
v3.4.0,Grouping the document tree into Texinfo files. List of tuples
v3.4.0,"(source start file, target name, title, author,"
v3.4.0,"dir menu entry, description, category)"
v3.4.0,"inf_type = ""ct2"""
v3.4.0,#####################
v3.4.0,Inference with CT2 #
v3.4.0,#####################
v3.4.0,#####################
v3.4.0,Inference with -py #
v3.4.0,#####################
v3.4.0,"ckpt_path = ""finetuned_llama7B/llama7B-vicuna-onmt_step_4000.pt"""
v3.4.0,we receive a text box content
v3.4.0,might be good to split also based on full period (later)
v3.4.0,we reformat the transformed batch to be numericalized / tensorified
v3.4.0,#####
v3.4.0,UI #
v3.4.0,#####
v3.4.0,What are the 3 best french cities ?
v3.4.0,Which one is better if I like outdoor activities ?
v3.4.0,Which one is better if I like cultural outings?
v3.4.0,What are the best neighborhoods in these 5 cities?
v3.4.0,!/usr/bin/env python3
v3.4.0,Usage: python3 filter_train.py in.src in.trg out.src out.trg max-tokens
v3.4.0,flake8: noqa
v3.4.0,-*- coding: utf-8 -*-
v3.4.0,Generated by the protocol buffer compiler.  DO NOT EDIT!
v3.4.0,source: sentencepiece_model.proto
v3.4.0,@@protoc_insertion_point(imports)
v3.4.0,@@protoc_insertion_point(module_scope)
v3.4.0,!/usr/bin/env python
v3.4.0,-*- coding: utf-8 -*-
v3.4.0,is this reachable?
v3.4.0,Read in embeddings
v3.4.0,Write to file
v3.4.0,converts a SentencePiece vocabulary to the format expected by dynamic data
v3.4.0,"(essentially converts float expected counts to ""fixed precision"" int pseudo"
v3.4.0,counts)
v3.4.0,from onmt.utils.misc import use_gpu
v3.4.0,"Add in default model arguments, possibly added since training."
v3.4.0,this patch is no longer needed included in converter
v3.4.0,"if hasattr(model_opt, 'rnn_size'):"
v3.4.0,model_opt.hidden_size = model_opt.rnn_size
v3.4.0,build_base_model expects updated and validated opts
v3.4.0,-*- encoding: utf-8 -*-
v3.4.0,!/usr/bin/env python
v3.4.0,Falcon stores QKV in one single tensor but it is not simply piled up Q+K+V
v3.4.0,it is heads interleaved to we need to slice first
v3.4.0,also it uses the HF rotary so we need to permute Q and K interleave
v3.4.0,!/usr/bin/env python
v3.4.0,-*- coding: utf-8 -*-
v3.4.0,Author: Rico Sennrich
v3.4.0,flake8: noqa
v3.4.0,This file is retrieved from https://github.com/rsennrich/subword-nmt
v3.4.0,hack for python2/3 compatibility
v3.4.0,check version information
v3.4.0,some hacking to deal with duplicates (only consider first instance)
v3.4.0,don't print end-of-word symbols
v3.4.0,sys.stderr.write('cannot split {0} further.\n'.format(segment))
v3.4.0,sys.stderr.write('OOV: {0}\n'.format(segment))
v3.4.0,sys.stderr.write('OOV: {0}\n'.format(segment))
v3.4.0,python 2/3 compatibility
v3.4.0,read/write files as UTF-8
v3.4.0,!/usr/bin/env python3
v3.4.0,coding: utf-8
v3.4.0,"In order to use this tool, please install comet first"
v3.4.0,https://github.com/Unbabel/COMET
v3.4.0,Let's say you have a source file with N sentences in SL - eg: source.sl
v3.4.0,and the corresponding references (N sentences) reference.tl
v3.4.0,Translate your file in TL with the -n_best nbest options nbest being
v3.4.0,then number of hypotheses and output the target to -output target.nbest.tl
v3.4.0,Then you need to duplicate source and reference sentences nbest times
v3.4.0,for this script.
v3.4.0,for instance using awk '{for(i=1; i<=n; i++) print}' n=5 reference.tl \
v3.4.0,> reference.5.tl
v3.4.0,same for source.
v3.4.0,This script can be run (for instance with nbest = 5) as follows:
v3.4.0,python oracle_bleu.py --nbest-src source.5.sl --nbest-hyp target.5.tl \
v3.4.0,--nbest-ref reference.5.tl --nbest-order 5 --output target.maxbleu.tl
v3.4.0,It will search in all hyp the best comet score
v3.4.0,when choosing a reference-less model no nbest-ref is required
v3.4.0,for nbest in nbests:
v3.4.0,!/usr/bin/env python
v3.4.0,!/usr/bin/env python3
v3.4.0,coding: utf-8
v3.4.0,Let's say you have a source file with N sentences in SL - eg: source.sl
v3.4.0,Translate your file in TL with the -n_best nbest options nbest being
v3.4.0,then number of hypotheses and output the target to -output target.nbest.tl
v3.4.0,This script can be run (for instance with nbest = 5) as follows:
v3.4.0,python mbr_bleu.py --nbest-hyp target.5.tl \
v3.4.0,--nbest-order 5 --output target.mbr.tl
v3.4.0,It will compare all hyp with eachother and output the max bleu
v3.4.0,!/usr/bin/env python
v3.4.0,!/usr/bin/env python
v3.4.0,-*- coding: utf-8 -*-
v3.4.0,Author: Rico Sennrich
v3.4.0,flake8: noqa
v3.4.0,This file is retrieved from https://github.com/rsennrich/subword-nmt
v3.4.0,hack for python2/3 compatibility
v3.4.0,"find all instances of pair, and update frequency/indices around it"
v3.4.0,find first symbol
v3.4.0,"if first symbol is followed by second symbol, we've found an occurrence of pair (old_word[i:i+2])"
v3.4.0,"assuming a symbol sequence ""A B C"", if ""B C"" is merged, reduce the frequency of ""A B"""
v3.4.0,"assuming a symbol sequence ""A B C B"", if ""B C"" is merged, reduce the frequency of ""C B""."
v3.4.0,"however, skip this if the sequence is A B C B C, because the frequency of ""C B"" will be reduced by the previous code block"
v3.4.0,find new pair
v3.4.0,"assuming a symbol sequence ""A BC D"", if ""B C"" is merged, increase the frequency of ""A BC"""
v3.4.0,"assuming a symbol sequence ""A BC B"", if ""B C"" is merged, increase the frequency of ""BC B"""
v3.4.0,"however, if the sequence is A BC BC, skip this step because the count of ""BC BC"" will be incremented by the previous code block"
v3.4.0,data structure of pair frequencies
v3.4.0,index from pairs to words
v3.4.0,version 0.2 changes the handling of the end-of-word token ('</w>');
v3.4.0,version numbering allows bckward compatibility
v3.4.0,"threshold is inspired by Zipfian assumption, but should only affect speed"
v3.4.0,we probably missed the best pair because of pruning; go back to full statistics
v3.4.0,"threshold is inspired by Zipfian assumption, but should only affect speed"
v3.4.0,python 2/3 compatibility
v3.4.0,read/write files as UTF-8
v3.4.0,Now we can pipe the full file through the model using the Iterator
v3.4.0,reminder a batch includes .src .tgt .indices and it is sorted
v3.4.0,Compute and retrieve the loss for EACH sentence
v3.4.0,Now we need to rearrange the batch of ppl
v3.4.0,in the original order with indices
v3.4.0,!/usr/bin/env python
v3.4.0,-*- coding: utf-8 -*-
v3.4.0,!/usr/bin/env python
v3.4.0,!/usr/bin/env python
v3.4.0,!/usr/bin/env python
v3.4.0,!/usr/bin/env python
v3.4.0,!/usr/bin/env python
v3.4.0,!/usr/bin/env python3
v3.4.0,coding: utf-8
v3.4.0,Let's say you have a source file with N sentences in SL - eg: source.sl
v3.4.0,and the corresponding references (N sentences) reference.tl
v3.4.0,Translate your file in TL with the -n_best nbest options nbest being
v3.4.0,then number of hypotheses and output the target to -output target.nbest.tl
v3.4.0,Then you need to duplicate reference sentences nbest times for this script.
v3.4.0,for instance using awk '{for(i=1; i<=n; i++) print}' n=5 reference.tl \
v3.4.0,> reference.5.tl
v3.4.0,This script can be run (for instance with nbest = 5) as follows:
v3.4.0,python oracle_bleu.py --nbest-hyp target.5.tl --nbest-ref reference.5.tl \
v3.4.0,--nbest-order 5 --output target.maxbleu.tl
v3.4.0,It will search in all hyp the best bleu wrt reference
v3.4.0,and output the max bleu
v3.4.0,!/usr/bin/env python
v3.4.0,with the two module = imp.load_source() below
v3.4.0,we ghost the old torchtext.data.field and depercated
v3.4.0,onmt.inputters.text_dataset
v3.4.0,however this require some functions / classes to be
v3.4.0,monkey patched for loading the old field/vocab objects.
v3.4.0,"module3 = imp.load_source(""Vocab"", ""tools/convertv2_v3.py"")"
v3.4.0,"voc = dict(sorted(fields.vocab.__dict__['freqs'].items(),"
v3.4.0,"key=lambda x: (-x[1], x[0]))).keys()"
v3.4.0,"voc = dict(sorted(fields.vocab.__dict__['freqs'].items(),"
v3.4.0,"key=lambda x: (-x[1], x[0]))).keys()"
v3.4.0,!/usr/bin/env python
v3.4.0,redpajama stores QKV in one single tensor but it is not simply piled up Q+K+V
v3.4.0,it is heads interleaved to we need to slice first
v3.4.0,also it uses the HF rotary so we need to permute Q and K interleave
v3.4.0,Avoid functionality on inference
v3.4.0,weights are in the .pt file
v3.4.0,weights are not in the .pt checkpoint but stored in the safetensors file
v3.4.0,Build embeddings.
v3.4.0,Build encoder.
v3.4.0,Build embeddings.
v3.4.0,Build decoder.
v3.4.0,Share the embedding matrix - preprocess with share_vocab required.
v3.4.0,src/tgt vocab should be the same if `-share_vocab` is specified.
v3.4.0,Update vocabulary embeddings with checkpoint embeddings
v3.4.0,Embedding layers
v3.4.0,Just for debugging purposes
v3.4.0,Remove old vocabulary associated embeddings
v3.4.0,for back compat when attention_dropout was not defined
v3.4.0,Build Model
v3.4.0,Build Generator.
v3.4.0,If new training initialize the model params
v3.4.0,If update_vocab init also but checkpoint will overwrite old weights
v3.4.0,ONLY for legacy fusedam with amp pytorch requires NOT to half the model
v3.4.0,Update model embeddings with those from the checkpoint
v3.4.0,after initialization
v3.4.0,after this checkpoint contains no embeddings
v3.4.0,when using LoRa or updating the vocab (no more embeddings in ckpt)
v3.4.0,=> strict=False when loading state_dict
v3.4.0,weights are in the .pt file
v3.4.0,weights are not in the .pt checkpoint but stored in the safetensors file
v3.4.0,!/usr/bin/env python
v3.4.0,if transform + options set in 'valid' we need to copy in main
v3.4.0,transform / options for scoring considered as inference
v3.4.0,"maybe prepare pretrained embeddings, if any"
v3.4.0,Load checkpoint if we resume from a previous training.
v3.4.0,ensure tensorboard output is written in the directory
v3.4.0,of previous checkpoints
v3.4.0,Override checkpoint's update_embeddings as it defaults to false
v3.4.0,Override checkpoint's freezing settings as it defaults to false
v3.4.0,NOTE: It's important that ``opt`` has been validated and updated
v3.4.0,at this point.
v3.4.0,Build model.
v3.4.0,Build optimizer.
v3.4.0,Build model saver
v3.4.0,Use Tensorboard for visualization during training
v3.4.0,Options only during inference
v3.4.0,"Truncation options, for text corpus"
v3.4.0,"as for False, this will be added in _add_train_general_opts"
v3.4.0,GPU
v3.4.0,Embedding Options
v3.4.0,Model Task Options
v3.4.0,Encoder-Decoder Options
v3.4.0,Freeze Encoder and/or Decoder
v3.4.0,The following options (bridge_extra_node to n_steps) are used
v3.4.0,for training with --encoder_type ggnn (Gated Graph Neural Network).
v3.4.0,Attention options
v3.4.0,Alignement options
v3.4.0,Generator and loss options.
v3.4.0,LoRa
v3.4.0,Init options
v3.4.0,Pretrained word vectors
v3.4.0,Freeze word vectors
v3.4.0,Optimization options
v3.4.0,learning rate
v3.4.0,options relate to data preprare
v3.4.0,options relate to train
v3.4.0,Alpha and Beta values for Google Length + Coverage penalty
v3.4.0,"Described here: https://arxiv.org/pdf/1609.08144.pdf, Section 7"
v3.4.0,Length penalty options
v3.4.0,Coverage penalty options
v3.4.0,Decoding Length constraint
v3.4.0,Decoding content constraint
v3.4.0,Adding options related to source and target features
v3.4.0,Adding options relate to decoding strategy
v3.4.0,Adding option for logging
v3.4.0,Adding options related to Transforms
v3.4.0,Copyright 2016 The Chromium Authors. All rights reserved.
v3.4.0,Use of this source code is governed by a BSD-style license that can be
v3.4.0,found in the LICENSE file.
v3.4.0,"Get the key 'value' in the dict, or just use 'value'"
v3.4.0,Create a thread to listen for errors in the child processes.
v3.4.0,Basic attributes.
v3.4.0,Set model in training mode.
v3.4.0,Let's clean the GPUs before training loop
v3.4.0,UPDATE DROPOUT
v3.4.0,Run patience mechanism
v3.4.0,"If the patience has reached the limit, stop training"
v3.4.0,swap model params w/ moving average
v3.4.0,(and keep the original parameters)
v3.4.0,Set model in validating mode.
v3.4.0,raw_srcs = []
v3.4.0,raw_refs = []
v3.4.0,F-prop through the model.
v3.4.0,Compute loss.
v3.4.0,Compute validation metrics (at batch.dataset level)
v3.4.0,Compute stats
v3.4.0,Update statistics.
v3.4.0,Set model back to training mode.
v3.4.0,Truncated BPTT: reminder not compatible with accum > 1
v3.4.0,1. Create truncated target.
v3.4.0,2. F-prop all but generator.
v3.4.0,3. Compute loss.
v3.4.0,"If truncated, don't backprop fully."
v3.4.0,"in case of multi step gradient accumulation,"
v3.4.0,update only after accum batches
v3.4.0,For Flake
v3.4.0,we avoid padding while mean pooling
v3.4.0,incoming and outgoing edge embedding
v3.4.0,Find vocab data for tree builting
v3.4.0,Propogation Model
v3.4.0,Initialize the bridge layer
v3.4.0,Token embedding
v3.4.0,Initialize graph using formatted input sequence
v3.4.0,Number of flagged nodes defines node count for this sample
v3.4.0,"(Nodes can have no flags on them, but must be in 'flags' list)."
v3.4.0,The total number of integers in the vocab should allow
v3.4.0,for all features and edges to be defined.
v3.4.0,Use first extra node as only source for decoder init
v3.4.0,Average all nodes to get bridge input
v3.4.0,"LSTM has hidden and cell state, other only one"
v3.4.0,Total number of states
v3.4.0,Build a linear layer for each
v3.4.0,Initialize the bridge layer
v3.4.0,src lengths data is wrapped inside a Tensor.
v3.4.0,"LSTM has hidden and cell state, other only one"
v3.4.0,Total number of states
v3.4.0,Build a linear layer for each
v3.4.0,Auto import python files in this directory
v3.4.0,batch x len x dim
v3.4.0,"feed_forward applies residual, so we remove and apply residual with un-normed"
v3.4.0,mask is now (batch x 1 x slen x slen)
v3.4.0,1 to be expanded to number of heads in MHA
v3.4.0,Run the forward pass of every layer of the tranformer.
v3.4.0,Dimensions and padding for constructing the word embedding matrix
v3.4.0,Dimensions and padding for feature embedding matrices
v3.4.0,(these have no effect if feat_vocab_sizes is empty)
v3.4.0,The embedding matrix look-up tables. The first look-up table
v3.4.0,"is for words. Subsequent ones are for features, if any exist."
v3.4.0,The final output size of word + feature vectors. This can vary
v3.4.0,from the word vector size if and only if features are defined.
v3.4.0,This is the attribute you should access if you need to know
v3.4.0,how big your embeddings are going to be.
v3.4.0,The sequence of operations that converts the input sequence
v3.4.0,into a sequence of embeddings. At minimum this consists of
v3.4.0,looking up the embeddings for each word and feature in the
v3.4.0,input. Model parameters may require the sequence to contain
v3.4.0,additional operations as well.
v3.4.0,features must use word_vec_size
v3.4.0,features will use feat_vec_size
v3.4.0,Some utilitary functions for pretrained embeddings
v3.4.0,is this reachable?
v3.4.0,Write to file
v3.4.0,set the opt in place
v3.4.0,set the opt in place
v3.4.0,flake8: noqa
v3.4.0,For command-line option parsing
v3.4.0,"Check pass, set the args."
v3.4.0,"This SRU version implements its own cuda-level optimization,"
v3.4.0,so it requires that:
v3.4.0,1. `cupy` and `pynvrtc` python package installed.
v3.4.0,2. pytorch is built with cuda support.
v3.4.0,3. library path set: export LD_LIBRARY_PATH=<cuda lib path>.
v3.4.0,Check 1.
v3.4.0,Check 2.
v3.4.0,Check 3.
v3.4.0,This sets up device to use.
v3.4.0,-> directions x batch x dim
v3.4.0,For DEBUG
v3.4.0,"size = (length, batch, x.size(-1)) \"
v3.4.0,"if x.dim() == 3 else (batch, x.size(-1))"
v3.4.0,grad_x = x.new(*x.size()) if k_ == 3 else x.new(*size).zero_()
v3.4.0,Normal use
v3.4.0,"An entry check here, will catch on train side and translate side"
v3.4.0,if requirements are not satisfied.
v3.4.0,RNNDecoderState wraps hidden as a tuple.
v3.4.0,fh -> (layers*directions) x batch x dim
v3.4.0,This class is mainly used by decoder.py for RNNs but also
v3.4.0,by the CNN / transformer decoder when copy attention is used
v3.4.0,CNN has its own attention mechanism ConvMultiStepAttention
v3.4.0,Transformer has its own MultiHeadedAttention
v3.4.0,mlp wants it with bias
v3.4.0,"(batch, t_len, d) x (batch, d, s_len) --> (batch, t_len, s_len)"
v3.4.0,"(batch, t_len, s_len, d)"
v3.4.0,one step input
v3.4.0,"compute attention scores, as in Luong et al."
v3.4.0,Softmax or sparsemax to normalize attention weights
v3.4.0,each context vector c_t is the weighted average
v3.4.0,over all the source hidden states
v3.4.0,concatenate
v3.4.0,clamping necessary because of numerical errors: loss should be lower
v3.4.0,"bounded by zero, but negative values near zero are possible without"
v3.4.0,the clamp
v3.4.0,Help functions for Rotary Embeddings
v3.4.0,https://arxiv.org/pdf/2104.09864.pdf
v3.4.0,too convoluted to make maxseqlen a parameter.
v3.4.0,we suppose src_seq_len at training and max_length at inference
v3.4.0,are both < 2048 tokens.
v3.4.0,"rope is now matrix [maxseqlen, dim/2]"
v3.4.0,Help functions for max_relative positions
v3.4.0,https://arxiv.org/abs/1803.02155
v3.4.0,Shift values to be >= 0
v3.4.0,"now relative_position is in the range [0, inf)"
v3.4.0,half of the buckets are for exact increments in positions
v3.4.0,The other half of the buckets are for logarithmically bigger bins in positions
v3.4.0,up to max_distance
v3.4.0,Help functions to split model dim per head
v3.4.0,class MultiHeadedAttention(torch.jit.ScriptModule):
v3.4.0,https://arxiv.org/pdf/1803.02155.pdf
v3.4.0,in the paper they suggest either two embeds
v3.4.0,relative_key / relative_value or only
v3.4.0,relative_key. We implemented the same embed
v3.4.0,for both.
v3.4.0,@torch.jit.script_method
v3.4.0,"1) Project key, value, and query."
v3.4.0,as a reminder at training layer_cache[0] remains False
v3.4.0,expand key on heads dimension when it's less than query heads (multi-query variant)
v3.4.0,expand value on heads dimension when it's less than query heads (multi-query variant)
v3.4.0,"2) When standard pos. enc. or rotary, use flash attention"
v3.4.0,batch x num_heads x query_len x key_len
v3.4.0,1 or key_len x key_len
v3.4.0,1 or key_len x key_len x dim_per_head
v3.4.0,not 100% necessary but expand to nb of heads
v3.4.0,now mask and scores have the same shape
v3.4.0,3) Apply attention dropout and compute context vectors.
v3.4.0,We use the same embeddings for key and value
v3.4.0,--------------------------------------------------------------------------
v3.4.0,copied and adapted https://github.com/microsoft/LoRA/
v3.4.0,Copyright (c) Microsoft Corporation. All rights reserved.
v3.4.0,Licensed under the MIT License (MIT).
v3.4.0,Support bnb quantization of nderlying layers
v3.4.0,--------------------------------------------------------------------------
v3.4.0,Optional dropout
v3.4.0,Mark the weight as unmerged
v3.4.0,LoRA implemented in a dense layer
v3.4.0,Actual trainable parameters
v3.4.0,Freezing the pre-trained weight matrix
v3.4.0,initialize A the same way as the default
v3.4.0,for nn.Linear and B to zero
v3.4.0,Make sure that the weights are not merged
v3.4.0,Merge the weights and mark it
v3.4.0,Actual trainable parameters
v3.4.0,Freezing the pre-trained weight matrix
v3.4.0,we do not super().reset_parameters() save lot of time and useless when no grad.
v3.4.0,initialize A the same way as the default
v3.4.0,for nn.Linear and B to zero
v3.4.0,Make sure that the weights are not merged
v3.4.0,Merge the weights and mark it
v3.4.0,cannot merge/unmerge quantized weigts with unquantized lora_X
v3.4.0,Check if QLoraLinear has a custom __init__ method
v3.4.0,Invoke the __init__ method of QLoraLinear
v3.4.0,LoRA implemented in a dense layer
v3.4.0,At the moment this class is only used by embeddings.Embeddings look-up tables
v3.4.0,"for silu, see: https://arxiv.org/pdf/2002.05202.pdf"
v3.4.0,-*- coding: utf-8 -*-
v3.4.0,class AverageAttention(torch.jit.ScriptModule):
v3.4.0,@torch.jit.script
v3.4.0,Code taken from bitsandbytes but modified with arg device to accept skipt_init
v3.4.0,from torch.nn.utils => makes model building way faster.
v3.4.0,"weights are cast automatically as Int8Params, but the bias has to be cast manually"
v3.4.0,reorder weight layout back from ampere/turing to row
v3.4.0,"we only need to save SCB as extra data, because CB for quantized weights"
v3.4.0,is already stored in weight.data
v3.4.0,"case 1: .cuda was called, SCB is in self.weight"
v3.4.0,"case 2: self.init_8bit_state was called, SCB is in self.state"
v3.4.0,"buffers not yet initialized, can't call them directly without"
v3.4.0,"weights are cast automatically as Int8Params, but the bias has to be cast manually"
v3.4.0,we converted 8-bit row major to turing/ampere format in the first inference pass
v3.4.0,we no longer need the row-major weight
v3.4.0,out_features * in_features
v3.4.0,norm is out_features * 1
v3.4.0,batch_size * out_features
v3.4.0,out_features
v3.4.0,out_features
v3.4.0,batch_size * out_features
v3.4.0,"out_channels, in_channels // groups, * kernel_size"
v3.4.0,out_features
v3.4.0,This is used nowhere in the code at the moment (Vincent Nguyen 05/18/2018)
v3.4.0,"in_channels, out_channels, *kernel_size"
v3.4.0,"in_channels, out_channels, *kernel_size"
v3.4.0,"self.out_channels, 1"
v3.4.0,out_features
v3.4.0,out_features
v3.4.0,store roots on diagonal
v3.4.0,Original probabilities.
v3.4.0,Probability of copying p(z=1) batch.
v3.4.0,Probability of not copying: p_{word}(w) * (1 - p(z))
v3.4.0,probabilities assigned by the model to the gold targets
v3.4.0,probability of tokens copied from source
v3.4.0,Set scores for unk to 0 and add eps
v3.4.0,find the indices in which you do not use the copy mechanism
v3.4.0,Drop padding.
v3.4.0,We exclude tokenization for contractions in
v3.4.0,order to avoid inconsistencies with pyonmtok's tokenization.
v3.4.0,"(e.g. ""I ca n't"" with spacy, ""I can ' t"" with pyonmttok)"
v3.4.0,Use Spacy's stopwords to get rid of junk entries
v3.4.0,Perform tokenization with spacy for consistency.
v3.4.0,We ensure that the target lemma is present in the lemmatized
v3.4.0,"target string, that the match is an exact match (there is"
v3.4.0,whitespace before or after the term)
v3.4.0,and we perform some bound checking.
v3.4.0,Map the lemmatized string match index to
v3.4.0,the lemmatized list index
v3.4.0,We need to know if the term is multiword
v3.4.0,Join multiword target lemmas with a unique separator so
v3.4.0,we can treat them as single word and not change the indices.
v3.4.0,Construct the final source from the lemmatized list
v3.4.0,that contains the terms. We compare the tokens in the
v3.4.0,term-augmented lemma list with the tokens in the original
v3.4.0,"lemma list. If the lemma is the same, then we replace with"
v3.4.0,the token from the original tokenized source list. If they
v3.4.0,"are not the same, it means the lemma has been augemented"
v3.4.0,"with a term, so we inject this in the final list."
v3.4.0,Restore the spaces in multi-word terms
v3.4.0,Skip half examples to improve performance. This means we set
v3.4.0,"a hard limit for the `term_corpus_ratio` to 0.5, which is actually"
v3.4.0,quite high. TODO: We can add this (skipping examples) as an option
v3.4.0,Filter out very short or very long sentences
v3.4.0,from the TM for better performance
v3.4.0,We split the `batch` and perform fuzzy matching
v3.4.0,in smaller chunks of 10.000 examples in order to
v3.4.0,reduce memory usage.
v3.4.0,Perfomance is not affected.
v3.4.0,Probably redundant but let's be safe
v3.4.0,in case some examples are already fuzzied
v3.4.0,(e.g. from another pipeline or workflow)
v3.4.0,We don't want exact matches
v3.4.0,Apply a basic filtering to leave out very short or very long
v3.4.0,sentences and speed up things a bit during fuzzy matching
v3.4.0,Do nothing
v3.4.0,We set the start number of tags to a random number from 1
v3.4.0,to 12 + the number of subsequent tags that
v3.4.0,will be added. We also apply weights to this choice so tags
v3.4.0,"are more probable to start from 1, then from 2, etc."
v3.4.0,This way we cover most scenarios met in real usage and
v3.4.0,the system will learn to handle a fairly large number of
v3.4.0,numbered tags (but not an excessively large number)
v3.4.0,Make sure we only search for exact matches (we don't want
v3.4.0,to match part of words) and perform some bound checking
v3.4.0,Create all possible tag forms. We inject a special
v3.4.0,unicode char (âˆ¥) as a placeholder for whitespace in order
v3.4.0,to keep the indices unaltered. This char is replaced with
v3.4.0,spaces before we return the augmented examples.
v3.4.0,Make a weighted choice between paired tags or single tags.
v3.4.0,"We usually encounter, and thus here we favor, paired tags"
v3.4.0,with a ratio 1/3.
v3.4.0,Check if the tags include the
v3.4.0,"mandatory ""#"" number placeholder"""
v3.4.0,We split the user-defined tags in the # placeholder
v3.4.0,in order to number them
v3.4.0,Skip half examples to speed up the transform. This sets
v3.4.0,"a hard limit of 0.5 to the `tags_corpus_ratio`, which is"
v3.4.0,excessive and should be avoided anyway.
v3.4.0,normalize dict src/tgt for each dataset
v3.4.0,"print(""src empty"")"
v3.4.0,"print(""too many same char in src"")"
v3.4.0,"print(""too many same word in src"")"
v3.4.0,"print(""avg token min"", len(src_str) / len(ex['src']))"
v3.4.0,"print(""avg token max"", len(src_str) / len(ex['src']))"
v3.4.0,"print(""text does not fully belong to wanted script"")"
v3.4.0,"print(""Some text belong to unwanted scripts"")"
v3.4.0,"print(""langid does not match"", _id(src_str))"
v3.4.0,"print(""src = tgt"")"
v3.4.0,"print(""tgt empty"")"
v3.4.0,"print(""src / tgt ratio "", len(src_str) / len(tgt_str))"
v3.4.0,"print(""too many same char in tgt"")"
v3.4.0,"print(""too many same word in tgt"")"
v3.4.0,"print(""avg token min"", len(tgt_str) / len(ex['tgt']))"
v3.4.0,"print(""avg token max"", len(tgt_str) / len(ex['tgt']))"
v3.4.0,"print(""text does not fully belong to wanted script"")"
v3.4.0,"print(""Some text belong to unwanted scripts"")"
v3.4.0,"print(""langid does not match"", _id(tgt_str))"
v3.4.0,"doc break we add it, restart new doc"
v3.4.0,case 1st ex is already longer
v3.4.0,adding cur ex is too long we add cur doc
v3.4.0,and reset doc to cur ex
v3.4.0,we start the new doc with cur ex
v3.4.0,we cumulate cur ex to cur doc
v3.4.0,Auto import python files in this directory
v3.4.0,1. sample number of tokens to corrupt
v3.4.0,2. sample positions to corrput
v3.4.0,3. sample corrupted values
v3.4.0,1. sample number of tokens to corrupt
v3.4.0,2. sample positions to corrput
v3.4.0,3. Drop token on chosen position
v3.4.0,1. sample number of tokens to corrupt
v3.4.0,2. sample positions to corrput
v3.4.0,3. mask word on chosen position
v3.4.0,"Sharing options among `TokenizerTransform`s, same name conflict in"
v3.4.0,this scope will be resolved by remove previous occurrence in parser
v3.4.0,subword regularization(or BPE dropout) options:
v3.4.0,subword vocabulary restriction options:
v3.4.0,derterministic subwording
v3.4.0,subword sampling when nbest_size > 1 or -1
v3.4.0,alpha should be 0.0 < alpha < 1.0
v3.4.0,Load vocabulary file if provided and set threshold
v3.4.0,Load Subword Model
v3.4.0,-1: keep everything (i.e. 1 mask per token)
v3.4.0,0: replace everything (i.e. no mask)
v3.4.0,1: 1 mask per span
v3.4.0,view each subword as word start / input is word level token
v3.4.0,Pretend it ends with a full stop so last span is a sentence
v3.4.0,"Tokens that are full stops, where the previous token is not"
v3.4.0,Make sure we have enough to mask
v3.4.0,Trim to masking budget
v3.4.0,Handle 0-length mask (inserts) separately
v3.4.0,assert is_word_start[-1] == 0
v3.4.0,assert tokens_length - 1 not in indices
v3.4.0,"keep index, but replace it with [MASK]"
v3.4.0,"acts as a long length, so spans don't go over the end of doc"
v3.4.0,next position from each word_start
v3.4.0,delete token: 1 mask/remove per span
v3.4.0,"keep index, but replace it with [MASK]: 1 mask per token"
v3.4.0,A bit faster when all lengths are 1
v3.4.0,to cover whole token
v3.4.0,delete token
v3.4.0,"keep index, but replace it with [MASK]"
v3.4.0,assert tokens_length - 1 not in indices
v3.4.0,prefix src/tgt for each dataset
v3.4.0,prefix as general option for inference
v3.4.0,suffix src/tgt for each dataset
v3.4.0,suffix as general option for inference
v3.4.0,!/usr/bin/env python3
v3.4.0,-*- coding: utf-8 -*-
v3.4.0,Most code taken from: https://github.com/alvations/sacremoses
v3.4.0,Which in turn is based on the Moses punctuation normalizer.
v3.4.0,https://github.com/moses-smt/mosesdecoder/blob/master/scripts/
v3.4.0,tokenizer/normalize-punctuation.perl
v3.4.0,don't fix period at end of sentence
v3.4.0,Regex substitutions from replace-unicode-punctuation.perl
v3.4.0,https://github.com/moses-smt/mosesdecoder/blob/master/
v3.4.0,scripts/tokenizer/replace-unicode-punctuation.perl
v3.4.0,Adds the penn substitutions after extra_whitespace regexes.
v3.4.0,"Optionally, replace unicode puncts BEFORE normalization."
v3.4.0,Actual normalization.
v3.4.0,"Optionally, replace unicode puncts BEFORE normalization."
v3.4.0,normalize dict src/tgt for each dataset
v3.4.0,One source feature expected but none given and no default provided
v3.4.0,Provided default does not match required features
v3.4.0,Data not properly annotated.
v3.4.0,In this case we do not use the default as it might be an error
v3.4.0,batch 0 will always predict EOS. The other batches will predict
v3.4.0,non-eos scores.
v3.4.0,"""best"" prediction is eos - that should be blocked"
v3.4.0,include at least one prediction OTHER than EOS
v3.4.0,that is greater than -1e20
v3.4.0,now batch 0 has ended and no others have
v3.4.0,initial step
v3.4.0,batch 0 dies on step 0
v3.4.0,include at least one prediction OTHER than EOS
v3.4.0,that is greater than -1e20
v3.4.0,step 2
v3.4.0,(old) batch 8 dies on step 1
v3.4.0,step 3
v3.4.0,everything dies
v3.4.0,initial step
v3.4.0,batch 0 dies on step 0
v3.4.0,include at least one prediction OTHER than EOS
v3.4.0,that is greater than -1e20
v3.4.0,step 2
v3.4.0,(old) batch 8 dies on step 1
v3.4.0,step 3
v3.4.0,everything dies
v3.4.0,initial step
v3.4.0,finish one beam
v3.4.0,include at least one prediction OTHER than EOS
v3.4.0,that is greater than -1e20
v3.4.0,step 2
v3.4.0,finish example in last batch
v3.4.0,(old) batch 8 dies on step 1
v3.4.0,step 3
v3.4.0,everything dies
v3.4.0,initial step
v3.4.0,batch 0 dies on step 0
v3.4.0,include at least one prediction OTHER than EOS
v3.4.0,that is greater than -1e20
v3.4.0,step 2
v3.4.0,(old) batch 8 dies on step 1
v3.4.0,step 3
v3.4.0,everything dies
v3.4.0,illegal_weights_mask = torch.ByteTensor([
v3.4.0,"[0, 0, 0, 0, 0, 0, 0],"
v3.4.0,"[0, 0, 0, 1, 1, 1, 1],"
v3.4.0,"[0, 0, 0, 0, 0, 1, 1],"
v3.4.0,"[0, 0, 1, 1, 1, 1, 1]])"
v3.4.0,TODO: fix for pytorch 0.3
v3.4.0,illegal_weights = alignments.masked_select(illegal_weights_mask)
v3.4.0,"self.assertEqual(0.0, illegal_weights.data.sum())"
v3.4.0,this could be considered an integration test because it touches
v3.4.0,the filesystem for the config file (and the models)
v3.4.0,no dummy prefix
v3.4.0,no dummy prefix
v3.4.0,make sure the scalars are in the event accumulator tags
v3.4.0,required arguments
v3.4.0,transforms that require vocab will not create if not provide vocab
v3.4.0,1. Init first transform in the pipe
v3.4.0,2. Init second transform in the pipe
v3.4.0,3. Sequential combine them into a transform pipe
v3.4.0,4. apply transform pipe for example
v3.4.0,"5. example after the pipe exceed the length limit, thus filtered"
v3.4.0,6. Transform statistics registed (here for filtertoolong)
v3.4.0,"7. after report, statistics become empty as a fresh start"
v3.4.0,filter_transform.warm_up()
v3.4.0,test BPE-dropout:
v3.4.0,1. disable bpe dropout for not training example
v3.4.0,2. enable bpe dropout for training example
v3.4.0,3. (NOTE) disable dropout won't take effect if already seen
v3.4.0,this is caused by the cache mechanism in bpe:
v3.4.0,return cached subword if the original token is seen when no dropout
v3.4.0,test SP regularization:
v3.4.0,1. enable regularization for training example
v3.4.0,2. disable regularization for not training example
v3.4.0,Not apply token drop for not training example
v3.4.0,apply token drop for training example
v3.4.0,Not apply token mask for not training example
v3.4.0,apply token mask for training example
v3.4.0,require vocabs to warm_up
v3.4.0,Not apply token mask for not training example
v3.4.0,apply token mask for training example
v3.4.0,"Defalt: full_stop_token=[""."", ""?"", ""!""]"
v3.4.0,"Defalt: full_stop_token=[""."", ""?"", ""!""]"
v3.4.0,random_ratio of inserted tokens are chosen in vocab
v3.4.0,others are MASK_TOK
v3.4.0,"insert_ratio=0.0,"
v3.4.0,"random_ratio=0.0,"
v3.4.0,"Defalt: full_stop_token=[""."", ""?"", ""!""]"
v3.4.0,all token are considered as an individual word
v3.4.0,1. tokens are dropped when replace_length is 0
v3.4.0,"print(f""token delete: {masked} / {tokens}"")"
v3.4.0,2. tokens are replaced by MASK when replace_length is 1
v3.4.0,"print(f""token mask: {masked} / {tokens}"")"
v3.4.0,"insert_ratio=0.0,"
v3.4.0,"random_ratio=0.0,"
v3.4.0,"Defalt: full_stop_token=[""."", ""?"", ""!""]"
v3.4.0,start token of word are identified using subword marker
v3.4.0,"1. replace_length 0: ""words"" are dropped"
v3.4.0,"print(f""word delete: {masked} / {tokens}"")"
v3.4.0,"self.assertEqual(len(masked), n_words - n_masked)"
v3.4.0,"2. replace_length 1: ""words"" are replaced with a single MASK"
v3.4.0,"print(f""whole word single mask: {masked} / {tokens}"")"
v3.4.0,len(masked) depend on number of tokens in select word
v3.4.0,"3. replace_length -1: all tokens in ""words"" are replaced with MASK"
v3.4.0,"print(f""whole word multi mask: {masked} / {tokens}"")"
v3.4.0,number of mask_tok depend on number of tokens in selected word
v3.4.0,number of MASK_TOK can be greater than n_masked
v3.4.0,"insert_ratio=0.5,"
v3.4.0,"random_ratio=0.3,"
v3.4.0,"Defalt: full_stop_token=[""."", ""?"", ""!""]"
v3.4.0,start token of word are identified using subword marker
v3.4.0,n_words = sum(token_starts)
v3.4.0,n_masked = math.ceil(n_words * bart_noise.mask_ratio)
v3.4.0,"print(f""Text Span Infilling: {infillied} / {tokens}"")"
v3.4.0,"print(n_words, n_masked)"
v3.4.0,!/usr/bin/env python
v3.4.0,-*- coding: utf-8 -*-
v3.4.0,Inject some dummy training options that may needed when build fields
v3.4.0,Remove the generated *pt files.
v3.4.0,Remove the generated data samples
v3.4.0,all beams repeat (beam >= 1 repeat dummy scores)
v3.4.0,predict repeat_idx over and over again
v3.4.0,"before repeat, scores are either 0 or -inf"
v3.4.0,"on repeat, `repeat_idx` score is BLOCKED_SCORE"
v3.4.0,"(but it's still the best score, thus we have"
v3.4.0,"[BLOCKED_SCORE, -inf, -inf, -inf, -inf]"
v3.4.0,repetitions keeps maximizing score
v3.4.0,"index 0 has been blocked, so repeating=>+0.0 score"
v3.4.0,other indexes are -inf so repeating=>BLOCKED_SCORE
v3.4.0,which is higher
v3.4.0,beam 0 and beam >=2 will repeat (beam >= 2 repeat dummy scores)
v3.4.0,non-interesting beams are going to get dummy values
v3.4.0,"on initial round, only predicted scores for beam 0"
v3.4.0,matter. Make two predictions. Top one will be repeated
v3.4.0,"in beam zero, second one will live on in beam 1."
v3.4.0,predict the same thing in beam 0
v3.4.0,continue pushing around what beam 1 predicts
v3.4.0,"now beam 0 dies (along with the others), beam 1 -> beam 0"
v3.4.0,"now beam 0 dies (along with the others), beam 1 -> beam 0"
v3.4.0,beam 0 and beam >= 2 will repeat (beam 2 repeats excluded idx)
v3.4.0,non-interesting beams are going to get dummy values
v3.4.0,predict the same thing in beam 0
v3.4.0,continue pushing around what beam 1 predicts
v3.4.0,predict the allowed-repeat again in beam 2
v3.4.0,"now beam 0 dies, beam 1 -> beam 0, beam 2 -> beam 1"
v3.4.0,and the rest die
v3.4.0,"since all preds after i=0 are 0, we can check"
v3.4.0,that the beam is the correct idx by checking that
v3.4.0,the curr score is the initial score
v3.4.0,beam 0 will always predict EOS. The other beams will predict
v3.4.0,non-eos scores.
v3.4.0,non-interesting beams are going to get dummy values
v3.4.0,"""best"" prediction is eos - that should be blocked"
v3.4.0,include at least beam_sz predictions OTHER than EOS
v3.4.0,that are greater than -1e20
v3.4.0,predict eos in beam 0
v3.4.0,provide beam_sz other good predictions
v3.4.0,now the top beam has ended and no others have
v3.4.0,"not of interest, but want to make sure it keeps running"
v3.4.0,since only beam 0 terminates and n_best = 2
v3.4.0,"this is also a test that when block_ngram_repeat=0,"
v3.4.0,repeating is acceptable
v3.4.0,non-interesting beams are going to get dummy values
v3.4.0,"""best"" prediction is eos - that should be blocked"
v3.4.0,include at least beam_sz predictions OTHER than EOS
v3.4.0,that are greater than -1e20
v3.4.0,predict eos in beam 1
v3.4.0,provide beam_sz other good predictions in other beams
v3.4.0,beam 1 dies on min_length
v3.4.0,beam 0 dies on the step after beam 1 dies
v3.4.0,"inp_lens is tiled in initialize, reassign to make attn match"
v3.4.0,non-interesting beams are going to get dummy values
v3.4.0,"""best"" prediction is eos - that should be blocked"
v3.4.0,include at least beam_sz predictions OTHER than EOS
v3.4.0,that are greater than -1e20
v3.4.0,predict eos in beam 1
v3.4.0,provide beam_sz other good predictions in other beams
v3.4.0,no top beams are finished yet
v3.4.0,beam 1 dies on min_length
v3.4.0,no top beams are finished yet
v3.4.0,beam 0 dies on the step after beam 1 dies
v3.4.0,top beam is finished now so there are attentions
v3.4.0,two beams are finished in each batch
v3.4.0,second dim is cut down to the non-padded src length
v3.4.0,first dim is equal to the time of death
v3.4.0,(beam 0 died at current step - adjust for SOS)
v3.4.0,(beam 1 died at last step - adjust for SOS)
v3.4.0,behavior gets weird when beam is already done so just stop
v3.4.0,this is just test_beam.TestBeamAgainstReferenceCase repeated
v3.4.0,in each batch.
v3.4.0,"init_preds: [4, 3, 5, 6, 7] - no EOS's"
v3.4.0,no EOS's yet
v3.4.0,"[5, 3, 2, 6, 0], so beam 2 predicts EOS!"
v3.4.0,assumes beam 2 finished on last step
v3.4.0,ended beam 2 shouldn't continue
v3.4.0,"[2, 5, 3, 6, 0] repeat self.BATCH_SZ, so beam 0 predicts EOS!"
v3.4.0,"[-2.4879, -3.8910, -4.1010, -4.2010, -4.4010] repeat self.BATCH_SZ"
v3.4.0,another beam is finished in all batches
v3.4.0,new beam 0 finished
v3.4.0,new beam 0 is old beam 3
v3.4.0,assumes beam 0 finished on last step
v3.4.0,"[5, 2, 6, 1, 0] repeat self.BATCH_SZ, so beam 1 predicts EOS!"
v3.4.0,we finish 3 hyps per example in this step
v3.4.0,new beam 1 is old beam 3
v3.4.0,this could be considered an integration test because it tests
v3.4.0,interactions between the GNMT scorer and the beam
v3.4.0,"-data option is required, but not used in this test, so dummy."
v3.4.0,len x batch x nfeat
v3.4.0,Initialize vectors to compare size with
v3.4.0,Ensure correct sizes and types
v3.4.0,Make sure that output has the correct size and type
v3.4.0,"[('encoder_type', 'transformer'),"
v3.4.0,"('word_vec_size', 16), ('hidden_size', 16)],"
v3.4.0,""""""" Only do SRU test if requirment is safisfied. """""""
v3.4.0,SRU doesn't support input_feed.
v3.4.0,first check there's nothing unexpectedly not trainable
v3.4.0,ok: word embeddings shouldn't be trainable
v3.4.0,if word vecs are freezed
v3.4.0,ok: positional encodings shouldn't be trainable
v3.4.0,then check nothing unexpectedly trainable
v3.4.0,Decoder state
v3.4.0,Build the RNN.
v3.4.0,Set up the context gate.
v3.4.0,Set up the standard attention.
v3.4.0,The encoder hidden is  (layers*directions) x batch x dim.
v3.4.0,We need to convert it to layers x batch x (directions*dim).
v3.4.0,Init the input feed.
v3.4.0,Update the state with the result.
v3.4.0,Concatenates sequence of tensors along a new dimension.
v3.4.0,NOTE: v0.3 to 0.4: dec_outs / attns[*] may not be list
v3.4.0,(in particular in case of SRU) it was not raising error in 0.3
v3.4.0,since stack(Variable) was allowed.
v3.4.0,"In 0.4, SRU returns a tensor that shouldn't be stacke"
v3.4.0,Calculate the attention.
v3.4.0,Calculate the context gate.
v3.4.0,Additional args check.
v3.4.0,Input feed concatenates hidden state with
v3.4.0,input at every time step.
v3.4.0,TODO: context gate should be employed
v3.4.0,instead of second RNN transform.
v3.4.0,Update the coverage attention.
v3.4.0,"attns[""coverage""] is actually c^(t+1) of See et al(2017)"
v3.4.0,1-index shifted
v3.4.0,Decoder State
v3.4.0,CNNDecoder has its own attention mechanism.
v3.4.0,Set up a separate copy attention layer if needed.
v3.4.0,The output of CNNEncoder.
v3.4.0,The combination of output of CNNEncoder and source embeddings.
v3.4.0,Process the result and update the attentions.
v3.4.0,Update the state.
v3.4.0,TODO change the way attns is returned dict => list or tuple (onnx)
v3.4.0,Auto import python files in this directory
v3.4.0,src_len is a single tensor shared between all models.
v3.4.0,This assumption will not hold if Translator is modified
v3.4.0,to calculate src_len as something other than the length
v3.4.0,of the input.
v3.4.0,"return _, (B, Q_len, K_len)"
v3.4.0,"layer average attention across heads, get ``(B, Q, K)``"
v3.4.0,"Case 1: no full_context, no align heads -> layer avg baseline"
v3.4.0,"Case 2: no full_context, 1 align heads -> guided align"
v3.4.0,"Case 3: full_context, 1 align heads -> full cte guided align"
v3.4.0,BoolTensor was introduced in pytorch 1.2
v3.4.0,T: could be 1 in the case of stepwise decoding or tgt_len
v3.4.0,masking is necessary when sequence length is greater than one
v3.4.0,mask now are (batch x 1 x tlen x s or t len)
v3.4.0,1 = heads to be expanded in MHA
v3.4.0,"feed_forward applies residual, so we remove and apply residual with un-normed"
v3.4.0,Decoder State
v3.4.0,"previously, there was a GlobalAttention module here for copy"
v3.4.0,"attention. But it was never actually used -- the ""copy"" attention"
v3.4.0,just reuses the context attention.
v3.4.0,"attns[""align""] = torch.stack(attn_aligns, 0).mean(0)  # All avg"
v3.4.0,TODO change the way attns is returned dict => list or tuple (onnx)
v3.4.0,first value set to True triggered by the beginning of decoding
v3.4.0,layer_cache becomes active in the MultiHeadedAttention fwd
v3.4.0,T: could be 1 in the case of stepwise decoding or tgt_len
v3.4.0,masking is necessary when sequence length is greater than one
v3.4.0,mask now are (batch x 1 x tlen x tlen)
v3.4.0,1 = heads to be expanded in MHA
v3.4.0,"feed_forward applies residual, so we remove and apply residual with un-normed"
v3.4.0,TODO change the way attns is returned dict => list or tuple (onnx)
v3.4.0,"buffer size in bytes, determine equiv. # of elements based on data type"
v3.4.0,copy tensors into buffer_t
v3.4.0,all-reduce and rescale
v3.4.0,copy all-reduced buffer back into tensors
v3.4.0,"print(filled, sz)"
v3.4.0,"tensor is bigger than buffer, all-reduce and rescale directly"
v3.4.0,"buffer is full, all-reduce and replace buffer with grad"
v3.4.0,add tensor to buffer
v3.4.0,"propagate exception to parent process, keeping original traceback"
v3.4.0,"propagate exception to parent process, keeping original traceback"
v3.4.0,TODO: Find a better way to check for sparse gradients.
v3.4.0,we use apex.amp
v3.4.0,In this case use the old FusedAdam with
v3.4.0,FP16_optimizer wrapper
v3.4.0,Load everything from the checkpoint.
v3.4.0,Build everything from scratch.
v3.4.0,"Reset optimizer, keep options."
v3.4.0,"Reset options, keep optimizer."
v3.4.0,State can be partially restored.
v3.4.0,should be: self._optimizer.zero_grad(set_to_none)
v3.4.0,but apex.amp is not up-to-date:
v3.4.0,https://github.com/NVIDIA/apex/blob/master/apex/amp/_process_optimizer.py#L367
v3.4.0,"unscaled optimizer's gradients (already done therefore skip),"
v3.4.0,skips optimizer.step() if gradients contain infs/NaNs.
v3.4.0,Updates the scale for next iteration.
v3.4.0,Code below is an implementation of https://arxiv.org/pdf/1804.04235.pdf
v3.4.0,inspired but modified from https://github.com/DeadAt0m/adafactor-pytorch
v3.4.0,backward compatibility
v3.4.0,assuming a list/generator of parameter means single group
v3.4.0,compute combined scale factor for this group
v3.4.0,norm is in fact norm*scale
v3.4.0,note: p.grad should not ever be set for correct operation of
v3.4.0,mixed precision optimizer that sometimes sends None gradients
v3.4.0,State initialization
v3.4.0,Exponential moving average of gradient values
v3.4.0,Exponential moving average of squared gradient values
v3.4.0,-*- coding: utf-8 -*-
v3.4.0,placing this here make it easier to call logger.info
v3.4.0,"from anywhere, just 'from onmt.utils.logging import logger'"
v3.4.0,"align_head contains value in [0, 1) presenting attn prob,"
v3.4.0,0 was resulted by the context attention src_pad_mask
v3.4.0,"So, the correspand position in ref_align should also be 0"
v3.4.0,"Therefore, clip align_head to > 1e-18 should be bias free."
v3.4.0,rescale with tau (temperature) and apply the log_softmax.
v3.4.0,ct2 expects src with lengths without padding
v3.4.0,again we use raw probs to rescale with tau and apply log_softmax
v3.4.0,lm_scores are in log space so log_target=True
v3.4.0,rescale with tau (temperature) and apply the log_softmax.
v3.4.0,ct2 expects src with lengths without padding
v3.4.0,again we use raw probs to rescale with tau and apply log_softmax
v3.4.0,lm_scores are in log space so log_target=True
v3.4.0,take into account here the tgt_shift_index (0 / 1 = LM/NMT)
v3.4.0,Correct target copy token instead of <unk>
v3.4.0,tgt[i] = align[i] + len(tgt_vocab)
v3.4.0,for i such that tgt[i] == 0 and align[i] != 0
v3.4.0,in the case criterion reduction is None then we need
v3.4.0,to sum the loss of each sentence in the batch
v3.4.0,Check Transforms
v3.4.0,Check path
v3.4.0,tgt is src for LM task
v3.4.0,Check weight
v3.4.0,Check features
v3.4.0,validation when train:
v3.4.0,Check embeddings stuff
v3.4.0,"Backward compatibility with ""fix_word_vecs_*"" opts"
v3.4.0,encoder and decoder should be same sizes
v3.4.0,"Load default opt values, then overwrite with the opts in"
v3.4.0,"the checkpoint. That way, if there are new options added,"
v3.4.0,the defaults are used.
v3.4.0,It comes from training
v3.4.0,TODO: needs to be added as inference opt
v3.4.0,Don't do anything
v3.4.0,Update best score of each criteria
v3.4.0,Reset tolerance
v3.4.0,Update current status
v3.4.0,Decrease tolerance
v3.4.0,Log
v3.4.0,Log
v3.4.0,Get a list of world_size lists with len(stat_list) Statistics objects
v3.4.0,"this param init is overridden by model_builder, useless then."
v3.4.0,SRU doesn't support PackedSequence.
v3.4.0,-*- coding: utf-8 -*-
v3.4.0,threshold on 1 to avoid div by 0
v3.4.0,treat alignment matrix one by one as each have different lengths
v3.4.0,No alignment if not exist valid tgt token
v3.4.0,get valid alignment (sub-matrix from full paded aligment matrix)
v3.4.0,Helper functions
v3.4.0,Keeps track of the original words/subwords
v3.4.0,('prior_tokenization' option)
v3.4.0,In case there is a final case_markup when new_spacer is on
v3.4.0,########## #
v3.4.0,Translator #
v3.4.0,########## #
v3.4.0,Set translation options
v3.4.0,Build translator from options
v3.4.0,################### #
v3.4.0,Validation iterator #
v3.4.0,################### #
v3.4.0,Reinstantiate the validation iterator
v3.4.0,Retrieve raw references and sources
v3.4.0,########### #
v3.4.0,Predictions #
v3.4.0,########### #
v3.4.0,####### #
v3.4.0,Outputs #
v3.4.0,####### #
v3.4.0,Flatten predictions
v3.4.0,Save results
v3.4.0,-*- coding: utf-8 -*-
v3.4.0,this one is needed for Random Shuffler of batches
v3.4.0,in multi gpu it ensures datasets are read in the same order
v3.4.0,some cudnn methods can be random even after fixing the seed
v3.4.0,unless you tell it to be deterministic
v3.4.0,This one is needed for various tranfroms
v3.4.0,These ensure same initialization in multi gpu mode
v3.4.0,we need to check the model path + any tokenizer path
v3.4.0,patch to log stdout spawned processes of dataloader
v3.4.0,bucket_size = batch_size
v3.4.0,For TRAIN we need to group examples by length
v3.4.0,"for faster performance, but otherwise, sequential."
v3.4.0,For TRAIN we shuffle batches within the bucket
v3.4.0,otherwise sequential
v3.4.0,for specific case of rnn_packed need to be sorted
v3.4.0,within the batch
v3.4.0,Check if all tokens have features or none at all
v3.4.0,Make features part of src like
v3.4.0,"{'src': {'src': ..., 'feats': [...., ....]}}"
v3.4.0,at this point an example looks like:
v3.4.0,"{'src': {'src': ..., 'feats': [....]},"
v3.4.0,"'tgt': {'tgt': ...},"
v3.4.0,"'src_original': ['tok1', ...'tokn'],"
v3.4.0,"'tgt_original': ['tok1', ...'tokm'],"
v3.4.0,'indices' : seq in bucket
v3.4.0,"'align': ...,"
v3.4.0,}
v3.4.0,Need to add features in last dimensions
v3.4.0,Keep it consistent with dynamic data
v3.4.0,make a small vocab containing just the tokens in the source sequence
v3.4.0,Map source tokens to indices in the dynamic dict.
v3.4.0,-*- coding: utf-8 -*-
v3.4.0,temporary as long as translation_server and scoring_preparator still use lists
v3.4.0,this is hack: if the special separator ï½Ÿnewlineï½ is returned because of the
v3.4.0,"""docify"" transform.get_specials we don't add it if the corresponding newline code"
v3.4.0,is already included in the sentencepiece or BPE-with-gpt2-pretok.
v3.4.0,'src_original' and 'tgt_original' store the
v3.4.0,original line before tokenization. These
v3.4.0,fields are used later on in the feature
v3.4.0,transforms.
v3.4.0,NOTE: moved to dynamic_iterator.py cf process()
v3.4.0,item = self.transform.apply(
v3.4.0,"example, is_train=self.infinitely, corpus_name=self.cid)"
v3.4.0,empty example: skip
v3.4.0,bitsandbytes quantize weights when .cuda() is called
v3.4.0,for huge models we need to save Ram
v3.4.0,so we load the weights  module by module and transfer them to GPU for quantization
v3.4.0,bitsandbytes quantize weights when .cuda() is called
v3.4.0,for huge models we need to save Ram
v3.4.0,so we load the weights  module by module and transfer them to GPU for quantization
v3.4.0,"No encoder in LM, seq2seq count formatting kept"
v3.4.0,_check_save_model_path
v3.4.0,This preserves backward-compat for models using customed layernorm
v3.4.0,Force add_ffnbias to True if bias found in model w_1 keys
v3.4.0,fix v2 compatibility
v3.4.0,end of patch for backward compatibility
v3.4.0,!/usr/bin/env python
v3.4.0,!/usr/bin/env python
v3.4.0,!/usr/bin/env python
v3.4.0,-*- coding: utf-8 -*-
v3.4.0,!/usr/bin/env python
v3.4.0,BPE training
v3.4.0,SentencePiece training
v3.4.0,!/usr/bin/env python
v3.4.0,!/usr/bin/env python
v3.4.0,Set sharing strategy manually instead of default based on the OS.
v3.4.0,torch.multiprocessing.set_sharing_strategy('file_system')
v3.4.0,Create a thread to listen for errors in the child processes.
v3.4.0,Train with multiprocessing.
v3.4.0,magic indices
v3.4.0,result caching
v3.4.0,Here we set the decoder to start with self.start (BOS or EOS)
v3.4.0,fix length constraint and remove eos from count
v3.4.0,add one to account for BOS. Don't account for EOS because hitting
v3.4.0,this implies it hasn't been found.
v3.4.0,we don't block nothing if the user doesn't want it
v3.4.0,we can't block nothing beam's too short
v3.4.0,we check paths one by one
v3.4.0,we don't forbid nothing if the user doesn't want it
v3.4.0,we can't forbid nothing if beam's too short
v3.4.0,Reordering forbidden_tokens following beam selection
v3.4.0,We rebuild a dict to ensure we get the value and not the pointer
v3.4.0,Grabing the newly selected tokens and associated ngram
v3.4.0,skip the blocking if any token in current_ngram is excluded
v3.4.0,"pickups: Tensor where specified index were set to 1, others 0"
v3.4.0,"dropdowns: opposite of pickups, 1 for those shouldn't pick"
v3.4.0,Minus dropdowns to log_probs making probabilities of
v3.4.0,unspecified index close to 0
v3.4.0,"prediction step have surpass length of given target_prefix,"
v3.4.0,no need to further change this attr
v3.4.0,keep indices until overflowing p
v3.4.0,Set all logits that are not in the top-p to -10000.
v3.4.0,This puts the probabilities close to 0.
v3.4.0,Set all logits that are not in the top-k to -10000.
v3.4.0,This puts the probabilities close to 0.
v3.4.0,"For temp=0.0, take the argmax to avoid divide-by-zero errors."
v3.4.0,keep_topk=1 is also equivalent to argmax.
v3.4.0,maybe fix some prediction at this step by modifying log_probs
v3.4.0,"shape: (sum(~ self.is_finished), 1)"
v3.4.0,in LM task src_len is associated with currently generated src
v3.4.0,and therefore needs to follow the generation
v3.4.0,!/usr/bin/env python
v3.4.0,for debugging
v3.4.0,TODO: maybe add dynamic part
v3.4.0,Statistics
v3.4.0,those two should be the same except feat dim
v3.4.0,"batch['src'][perm[j], :, :])"
v3.4.0,trans.src
v3.4.0,we rebuild a small batch made of the sub-segments
v3.4.0,in the long segment.
v3.4.0,new sub-batch ready to be translated
v3.4.0,we re-insert the sub-batch in the initial translations
v3.4.0,In the case of length_penalty = none we report the total logprobs
v3.4.0,divided by the number of sentence to get an approximation of the
v3.4.0,per sentence logprob. We also return the corresponding ppl
v3.4.0,"When a length_penalty is used eg: ""avg"" or ""wu"" since logprobs"
v3.4.0,are normalized per token we report the per line per token logprob
v3.4.0,"and the corresponding ""per word perplexity"""
v3.4.0,Turn any copied words into UNKs.
v3.4.0,"Decoder forward, takes [batch, tgt_len, nfeats] as input"
v3.4.0,"and [batch, src_len, hidden] as enc_out"
v3.4.0,"in case of inference tgt_len = 1, batch = beam times batch_size"
v3.4.0,"in case of Gold Scoring tgt_len = actual length, batch = 1 batch"
v3.4.0,Generator forward.
v3.4.0,"returns [(batch_size x beam_size) , vocab ] when 1 step"
v3.4.0,"or [batch_size, tgt_len, vocab ] when full sentence"
v3.4.0,"here we have scores [tgt_lenxbatch, vocab] or [beamxbatch, vocab]"
v3.4.0,at this point scores is batch first (dim=0)
v3.4.0,"returns [(batch_size x beam_size) , vocab ] when 1 step"
v3.4.0,"or [batch_size, tgt_len, vocab ] when full sentence"
v3.4.0,(0) add BOS and padding to tgt prediction
v3.4.0,(1) Encoder forward.
v3.4.0,(2) Repeat src objects `n_best` times.
v3.4.0,"We use batch_size x n_best, get ``(batch * n_best, src_len, nfeat)``"
v3.4.0,Quick fix. Transformers return None as enc_states.
v3.4.0,enc_states are only used later on to init decoder's state
v3.4.0,"but are never used in Transformer decoder, so we can skip"
v3.4.0,"(3) Init decoder with n_best src,"
v3.4.0,"reshape tgt to ``(len, batch * n_best, nfeat)``"
v3.4.0,it should be done in a better way
v3.4.0,here dec_in is batch first
v3.4.0,masked_select
v3.4.0,get aligned src id for each prediction's valid tgt tokens
v3.4.0,TODO: support these blacklisted features
v3.4.0,(0) Prep the components of the search.
v3.4.0,(1) Run the encoder on the src.
v3.4.0,(2) prep decode_strategy. Possibly repeat src objects.
v3.4.0,(3) Begin decoding step by step:
v3.4.0,"decoder_input = decode_strategy.current_predictions.view(1, -1,"
v3.4.0,1)
v3.4.0,Reorder states.
v3.4.0,TODO: support these blacklisted features
v3.4.0,(0) Prep the components of the search.
v3.4.0,(1) split src into src and target_prefix to avoid padding.
v3.4.0,(2) init decoder
v3.4.0,(3) prep decode_strategy. Possibly repeat src objects.
v3.4.0,(4) Begin decoding step by step:
v3.4.0,Reorder states.
v3.4.0,select indexes in model state/cache
v3.4.0,beam parameters
v3.4.0,beam state
v3.4.0,BoolTensor was introduced in pytorch 1.2
v3.4.0,"""global state"" of the old beam"
v3.4.0,buffers for the topk scores and 'backpointer'
v3.4.0,for testing
v3.4.0,maybe fix some prediction at this step by modifying log_probs
v3.4.0,Flatten probs into a list of possibilities.
v3.4.0,Penalize beams that finished.
v3.4.0,"on real data (newstest2017) with the pretrained transformer,"
v3.4.0,it's faster to not move this back to the original device
v3.4.0,Store finished hypotheses for this batch.
v3.4.0,End condition is the top beam finished and we can return
v3.4.0,n_best hypotheses.
v3.4.0,"If all sentences are translated, no need to go further."
v3.4.0,Remove finished batches for the next step.
v3.4.0,using integer division to get an integer _B without casting
v3.4.0,force the output to be longer than self.min_length
v3.4.0,Multiply probs by the beam probability.
v3.4.0,"if the sequence ends now, then the penalty is the current"
v3.4.0,"length + 1, to include the EOS token"
v3.4.0,Avoid any direction that would repeat unwanted ngrams
v3.4.0,Pick up candidate token by curr_scores
v3.4.0,Recover log probs.
v3.4.0,Length penalty is just a scalar. It doesn't matter if it's applied
v3.4.0,before or after the topk.
v3.4.0,Resolve beam origin and map to batch index flat representation.
v3.4.0,Append last prediction.
v3.4.0,update global state (step == 1)
v3.4.0,update global state (step > 1)
v3.4.0,"shape: (batch_size x beam_size, 1)"
v3.4.0,in LM task src_len is associated with currently generated src
v3.4.0,and therefore needs to follow the generation
v3.4.0,in LM task src_len is associated with currently generated src
v3.4.0,and therefore needs to follow the generation
v3.4.0,Term will be subtracted from probability
v3.4.0,Probability will be divided by this
v3.4.0,these warnings indicate that either the alpha/beta
v3.4.0,"forces a penalty to be a no-op, or a penalty is a no-op but"
v3.4.0,the alpha/beta would suggest otherwise.
v3.4.0,using some coverage penalty
v3.4.0,!/usr/bin/env python
v3.4.0,semaphore doesn't have a timeout arg in Python 2.7
v3.4.0,perform a first request to initialize everything
v3.4.0,backwards compatibility for confs
v3.4.0,every segment becomes a dict for flexibility purposes
v3.4.0,NOTE: translator returns lists of `n_best` list
v3.4.0,build back results with empty texts
v3.4.0,load can be called multiple times: modify copy
v3.4.0,output contain alignment
v3.4.0,Below are all the different penalty terms implemented so far.
v3.4.0,Subtract coverage penalty from topk log probs.
v3.4.0,Divide topk log probs by length penalty.
v3.4.0,Sorting
v3.4.0,Chinese segmentation
v3.4.0,Chinese simplify -> Chinese traditional standard
v3.4.0,Chinese simplify -> Chinese traditional (HongKong)
v3.4.0,Chinese simplify -> Chinese traditional (Taiwan)
v3.4.0,Chinese traditional -> Chinese simplify (v1)
v3.4.0,Chinese traditional -> Chinese simplify (v2)
v3.4.0,Auto import python files in this directory
v3.4.0,"def custom_stopping_criteria(input_ids, score, **kwargs):"
v3.4.0,"stop_ids = [29871, 13, 13] # \n\n"
v3.4.0,return input_ids[-len(stop_ids)]
v3.4.0,Build the translator (along with the model)
v3.4.0,get prompt and make sure it fits
v3.4.0,"def custom_stopping_criteria(input_ids, score, **kwargs):"
v3.4.0,"stop_ids = [29871, 13, 13] # \n\n"
v3.4.0,return input_ids[-len(stop_ids)]
v3.4.0,Build the translator (along with the model)
v3.4.0,Build the transforms (along with the tokenizer)
v3.4.0,get prompt and make sure it fits
v3.3.0,!/usr/bin/env python
v3.3.0,!/usr/bin/env python
v3.3.0,!/usr/bin/env python
v3.3.0,!/usr/bin/env python
v3.3.0,!/usr/bin/env python
v3.3.0,!/usr/bin/env python3
v3.3.0,-*- coding: utf-8 -*-
v3.3.0,
v3.3.0,"OpenNMT-py documentation build configuration file, created by"
v3.3.0,sphinx-quickstart on Sun Dec 17 12:07:14 2017.
v3.3.0,
v3.3.0,This file is execfile()d with the current directory set to its
v3.3.0,containing dir.
v3.3.0,
v3.3.0,Note that not all possible configuration values are present in this
v3.3.0,autogenerated file.
v3.3.0,
v3.3.0,All configuration values have a default; values that are commented out
v3.3.0,serve to show the default.
v3.3.0,"If extensions (or modules to document with autodoc) are in another directory,"
v3.3.0,add these directories to sys.path here. If the directory is relative to the
v3.3.0,"documentation root, use os.path.abspath to make it absolute, like shown here."
v3.3.0,
v3.3.0,import os
v3.3.0,import sys
v3.3.0,"sys.path.insert(0, os.path.abspath('.'))"
v3.3.0,-- General configuration ------------------------------------------------
v3.3.0,"If your documentation needs a minimal Sphinx version, state it here."
v3.3.0,
v3.3.0,needs_sphinx = '6.0'
v3.3.0,"Add any Sphinx extension module names here, as strings. They can be"
v3.3.0,extensions coming with Sphinx (named 'sphinx.ext.*') or your custom
v3.3.0,ones.
v3.3.0,Show base classes
v3.3.0,"Use ""variables"" section for Attributes instead of weird block things"
v3.3.0,mimicking the function style.
v3.3.0,"Add any paths that contain templates here, relative to this directory."
v3.3.0,The suffix(es) of source filenames.
v3.3.0,You can specify multiple suffix as a list of string:
v3.3.0,
v3.3.0,"source_suffix = ['.rst', '.md']"
v3.3.0,The master toctree document.
v3.3.0,General information about the project.
v3.3.0,"The version info for the project you're documenting, acts as replacement for"
v3.3.0,"|version| and |release|, also used in various other places throughout the"
v3.3.0,built documents.
v3.3.0,
v3.3.0,The short X.Y version.
v3.3.0,"The full version, including alpha/beta/rc tags."
v3.3.0,The language for content autogenerated by Sphinx. Refer to documentation
v3.3.0,for a list of supported languages.
v3.3.0,
v3.3.0,This is also used if you do content translation via gettext catalogs.
v3.3.0,"Usually you set ""language"" from the command line for these cases."
v3.3.0,"List of patterns, relative to source directory, that match files and"
v3.3.0,directories to ignore when looking for source files.
v3.3.0,This patterns also effect to html_static_path and html_extra_path
v3.3.0,The name of the Pygments (syntax highlighting) style to use.
v3.3.0,"If true, `todo` and `todoList` produce output, else they produce nothing."
v3.3.0,-- Options for HTML output ----------------------------------------------
v3.3.0,The theme to use for HTML and HTML Help pages.  See the documentation for
v3.3.0,a list of builtin themes.
v3.3.0,
v3.3.0,html_theme = 'sphinx_materialdesign_theme'
v3.3.0,html_theme_path = [sphinx_materialdesign_theme.get_path()]
v3.3.0,Theme options are theme-specific and customize the look and feel of a theme
v3.3.0,"further.  For a list of options available for each theme, see the"
v3.3.0,documentation.
v3.3.0,
v3.3.0,html_theme_options = {}
v3.3.0,"Add any paths that contain custom static files (such as style sheets) here,"
v3.3.0,"relative to this directory. They are copied after the builtin static files,"
v3.3.0,"so a file named ""default.css"" will overwrite the builtin ""default.css""."
v3.3.0,"Custom sidebar templates, must be a dictionary that maps document names"
v3.3.0,to template names.
v3.3.0,
v3.3.0,This is required for the alabaster theme
v3.3.0,refs: http://alabaster.readthedocs.io/en/latest/installation.html#sidebars
v3.3.0,-- Options for HTMLHelp output ------------------------------------------
v3.3.0,Output file base name for HTML help builder.
v3.3.0,-- Options for LaTeX output ---------------------------------------------
v3.3.0,The paper size ('letterpaper' or 'a4paper').
v3.3.0,
v3.3.0,"'papersize': 'letterpaper',"
v3.3.0,"The font size ('10pt', '11pt' or '12pt')."
v3.3.0,
v3.3.0,"'pointsize': '10pt',"
v3.3.0,Additional stuff for the LaTeX preamble.
v3.3.0,
v3.3.0,"'preamble': '',"
v3.3.0,Latex figure (float) alignment
v3.3.0,
v3.3.0,"'figure_align': 'htbp',"
v3.3.0,Grouping the document tree into LaTeX files. List of tuples
v3.3.0,"(source start file, target name, title,"
v3.3.0,"author, documentclass [howto, manual, or own class])."
v3.3.0,-- Options for manual page output ---------------------------------------
v3.3.0,One entry per manual page. List of tuples
v3.3.0,"(source start file, name, description, authors, manual section)."
v3.3.0,-- Options for Texinfo output -------------------------------------------
v3.3.0,Grouping the document tree into Texinfo files. List of tuples
v3.3.0,"(source start file, target name, title, author,"
v3.3.0,"dir menu entry, description, category)"
v3.3.0,"inf_type = ""ct2"""
v3.3.0,#####################
v3.3.0,Inference with CT2 #
v3.3.0,#####################
v3.3.0,#####################
v3.3.0,Inference with -py #
v3.3.0,#####################
v3.3.0,"ckpt_path = ""finetuned_llama7B/llama7B-vicuna-onmt_step_4000.pt"""
v3.3.0,we receive a text box content
v3.3.0,might be good to split also based on full period (later)
v3.3.0,we reformat the transformed batch to be numericalized / tensorified
v3.3.0,#####
v3.3.0,UI #
v3.3.0,#####
v3.3.0,What are the 3 best french cities ?
v3.3.0,Which one is better if I like outdoor activities ?
v3.3.0,Which one is better if I like cultural outings?
v3.3.0,What are the best neighborhoods in these 5 cities?
v3.3.0,!/usr/bin/env python3
v3.3.0,Usage: python3 filter_train.py in.src in.trg out.src out.trg max-tokens
v3.3.0,!/usr/bin/env python
v3.3.0,-*- coding: utf-8 -*-
v3.3.0,is this reachable?
v3.3.0,Read in embeddings
v3.3.0,Write to file
v3.3.0,converts a SentencePiece vocabulary to the format expected by dynamic data
v3.3.0,"(essentially converts float expected counts to ""fixed precision"" int pseudo"
v3.3.0,counts)
v3.3.0,from onmt.utils.misc import use_gpu
v3.3.0,"Add in default model arguments, possibly added since training."
v3.3.0,this patch is no longer needed included in converter
v3.3.0,"if hasattr(model_opt, 'rnn_size'):"
v3.3.0,model_opt.hidden_size = model_opt.rnn_size
v3.3.0,build_base_model expects updated and validated opts
v3.3.0,-*- encoding: utf-8 -*-
v3.3.0,!/usr/bin/env python
v3.3.0,Falcon stores QKV in one single tensor but it is not simply piled up Q+K+V
v3.3.0,it is heads interleaved to we need to slice first
v3.3.0,also it uses the HF rotary so we need to permute Q and K interleave
v3.3.0,!/usr/bin/env python
v3.3.0,-*- coding: utf-8 -*-
v3.3.0,Author: Rico Sennrich
v3.3.0,flake8: noqa
v3.3.0,This file is retrieved from https://github.com/rsennrich/subword-nmt
v3.3.0,hack for python2/3 compatibility
v3.3.0,check version information
v3.3.0,some hacking to deal with duplicates (only consider first instance)
v3.3.0,don't print end-of-word symbols
v3.3.0,sys.stderr.write('cannot split {0} further.\n'.format(segment))
v3.3.0,sys.stderr.write('OOV: {0}\n'.format(segment))
v3.3.0,sys.stderr.write('OOV: {0}\n'.format(segment))
v3.3.0,python 2/3 compatibility
v3.3.0,read/write files as UTF-8
v3.3.0,!/usr/bin/env python3
v3.3.0,coding: utf-8
v3.3.0,"In order to use this tool, please install comet first"
v3.3.0,https://github.com/Unbabel/COMET
v3.3.0,Let's say you have a source file with N sentences in SL - eg: source.sl
v3.3.0,and the corresponding references (N sentences) reference.tl
v3.3.0,Translate your file in TL with the -n_best nbest options nbest being
v3.3.0,then number of hypotheses and output the target to -output target.nbest.tl
v3.3.0,Then you need to duplicate source and reference sentences nbest times
v3.3.0,for this script.
v3.3.0,for instance using awk '{for(i=1; i<=n; i++) print}' n=5 reference.tl \
v3.3.0,> reference.5.tl
v3.3.0,same for source.
v3.3.0,This script can be run (for instance with nbest = 5) as follows:
v3.3.0,python oracle_bleu.py --nbest-src source.5.sl --nbest-hyp target.5.tl \
v3.3.0,--nbest-ref reference.5.tl --nbest-order 5 --output target.maxbleu.tl
v3.3.0,It will search in all hyp the best comet score
v3.3.0,when choosing a reference-less model no nbest-ref is required
v3.3.0,for nbest in nbests:
v3.3.0,!/usr/bin/env python
v3.3.0,!/usr/bin/env python3
v3.3.0,coding: utf-8
v3.3.0,Let's say you have a source file with N sentences in SL - eg: source.sl
v3.3.0,Translate your file in TL with the -n_best nbest options nbest being
v3.3.0,then number of hypotheses and output the target to -output target.nbest.tl
v3.3.0,This script can be run (for instance with nbest = 5) as follows:
v3.3.0,python mbr_bleu.py --nbest-hyp target.5.tl \
v3.3.0,--nbest-order 5 --output target.mbr.tl
v3.3.0,It will compare all hyp with eachother and output the max bleu
v3.3.0,!/usr/bin/env python
v3.3.0,!/usr/bin/env python
v3.3.0,-*- coding: utf-8 -*-
v3.3.0,Author: Rico Sennrich
v3.3.0,flake8: noqa
v3.3.0,This file is retrieved from https://github.com/rsennrich/subword-nmt
v3.3.0,hack for python2/3 compatibility
v3.3.0,"find all instances of pair, and update frequency/indices around it"
v3.3.0,find first symbol
v3.3.0,"if first symbol is followed by second symbol, we've found an occurrence of pair (old_word[i:i+2])"
v3.3.0,"assuming a symbol sequence ""A B C"", if ""B C"" is merged, reduce the frequency of ""A B"""
v3.3.0,"assuming a symbol sequence ""A B C B"", if ""B C"" is merged, reduce the frequency of ""C B""."
v3.3.0,"however, skip this if the sequence is A B C B C, because the frequency of ""C B"" will be reduced by the previous code block"
v3.3.0,find new pair
v3.3.0,"assuming a symbol sequence ""A BC D"", if ""B C"" is merged, increase the frequency of ""A BC"""
v3.3.0,"assuming a symbol sequence ""A BC B"", if ""B C"" is merged, increase the frequency of ""BC B"""
v3.3.0,"however, if the sequence is A BC BC, skip this step because the count of ""BC BC"" will be incremented by the previous code block"
v3.3.0,data structure of pair frequencies
v3.3.0,index from pairs to words
v3.3.0,version 0.2 changes the handling of the end-of-word token ('</w>');
v3.3.0,version numbering allows bckward compatibility
v3.3.0,"threshold is inspired by Zipfian assumption, but should only affect speed"
v3.3.0,we probably missed the best pair because of pruning; go back to full statistics
v3.3.0,"threshold is inspired by Zipfian assumption, but should only affect speed"
v3.3.0,python 2/3 compatibility
v3.3.0,read/write files as UTF-8
v3.3.0,Now we can pipe the full file through the model using the Iterator
v3.3.0,reminder a batch includes .src .tgt .indices and it is sorted
v3.3.0,Compute and retrieve the loss for EACH sentence
v3.3.0,Now we need to rearrange the batch of ppl
v3.3.0,in the original order with indices
v3.3.0,!/usr/bin/env python
v3.3.0,-*- coding: utf-8 -*-
v3.3.0,!/usr/bin/env python
v3.3.0,!/usr/bin/env python
v3.3.0,!/usr/bin/env python
v3.3.0,!/usr/bin/env python3
v3.3.0,coding: utf-8
v3.3.0,Let's say you have a source file with N sentences in SL - eg: source.sl
v3.3.0,and the corresponding references (N sentences) reference.tl
v3.3.0,Translate your file in TL with the -n_best nbest options nbest being
v3.3.0,then number of hypotheses and output the target to -output target.nbest.tl
v3.3.0,Then you need to duplicate reference sentences nbest times for this script.
v3.3.0,for instance using awk '{for(i=1; i<=n; i++) print}' n=5 reference.tl \
v3.3.0,> reference.5.tl
v3.3.0,This script can be run (for instance with nbest = 5) as follows:
v3.3.0,python oracle_bleu.py --nbest-hyp target.5.tl --nbest-ref reference.5.tl \
v3.3.0,--nbest-order 5 --output target.maxbleu.tl
v3.3.0,It will search in all hyp the best bleu wrt reference
v3.3.0,and output the max bleu
v3.3.0,!/usr/bin/env python
v3.3.0,with the two module = imp.load_source() below
v3.3.0,we ghost the old torchtext.data.field and depercated
v3.3.0,onmt.inputters.text_dataset
v3.3.0,however this require some functions / classes to be
v3.3.0,monkey patched for loading the old field/vocab objects.
v3.3.0,"module3 = imp.load_source(""Vocab"", ""tools/convertv2_v3.py"")"
v3.3.0,"voc = dict(sorted(fields.vocab.__dict__['freqs'].items(),"
v3.3.0,"key=lambda x: (-x[1], x[0]))).keys()"
v3.3.0,"voc = dict(sorted(fields.vocab.__dict__['freqs'].items(),"
v3.3.0,"key=lambda x: (-x[1], x[0]))).keys()"
v3.3.0,!/usr/bin/env python
v3.3.0,redpajama stores QKV in one single tensor but it is not simply piled up Q+K+V
v3.3.0,it is heads interleaved to we need to slice first
v3.3.0,also it uses the HF rotary so we need to permute Q and K interleave
v3.3.0,Avoid functionality on inference
v3.3.0,weights are in the .pt file
v3.3.0,weights are not in the .pt checkpoint but stored in the safetensors file
v3.3.0,Build embeddings.
v3.3.0,Build encoder.
v3.3.0,Build embeddings.
v3.3.0,Build decoder.
v3.3.0,Share the embedding matrix - preprocess with share_vocab required.
v3.3.0,src/tgt vocab should be the same if `-share_vocab` is specified.
v3.3.0,Update vocabulary embeddings with checkpoint embeddings
v3.3.0,Embedding layers
v3.3.0,Just for debugging purposes
v3.3.0,Remove old vocabulary associated embeddings
v3.3.0,for back compat when attention_dropout was not defined
v3.3.0,Build Model
v3.3.0,Build Generator.
v3.3.0,If new training initialize the model params
v3.3.0,If update_vocab init also but checkpoint will overwrite old weights
v3.3.0,ONLY for legacy fusedam with amp pytorch requires NOT to half the model
v3.3.0,Update model embeddings with those from the checkpoint
v3.3.0,after initialization
v3.3.0,after this checkpoint contains no embeddings
v3.3.0,when using LoRa or updating the vocab (no more embeddings in ckpt)
v3.3.0,=> strict=False when loading state_dict
v3.3.0,weights are in the .pt file
v3.3.0,weights are not in the .pt checkpoint but stored in the safetensors file
v3.3.0,!/usr/bin/env python
v3.3.0,if transform + options set in 'valid' we need to copy in main
v3.3.0,transform / options for scoring considered as inference
v3.3.0,"maybe prepare pretrained embeddings, if any"
v3.3.0,Load checkpoint if we resume from a previous training.
v3.3.0,ensure tensorboard output is written in the directory
v3.3.0,of previous checkpoints
v3.3.0,Override checkpoint's update_embeddings as it defaults to false
v3.3.0,Override checkpoint's freezing settings as it defaults to false
v3.3.0,NOTE: It's important that ``opt`` has been validated and updated
v3.3.0,at this point.
v3.3.0,Build model.
v3.3.0,Build optimizer.
v3.3.0,Build model saver
v3.3.0,Use Tensorboard for visualization during training
v3.3.0,Options only during inference
v3.3.0,"Truncation options, for text corpus"
v3.3.0,"as for False, this will be added in _add_train_general_opts"
v3.3.0,Embedding Options
v3.3.0,Model Task Options
v3.3.0,Encoder-Decoder Options
v3.3.0,Freeze Encoder and/or Decoder
v3.3.0,The following options (bridge_extra_node to n_steps) are used
v3.3.0,for training with --encoder_type ggnn (Gated Graph Neural Network).
v3.3.0,Attention options
v3.3.0,Alignement options
v3.3.0,Generator and loss options.
v3.3.0,GPU
v3.3.0,LoRa
v3.3.0,Init options
v3.3.0,Pretrained word vectors
v3.3.0,Freeze word vectors
v3.3.0,Optimization options
v3.3.0,learning rate
v3.3.0,options relate to data preprare
v3.3.0,options relate to train
v3.3.0,Alpha and Beta values for Google Length + Coverage penalty
v3.3.0,"Described here: https://arxiv.org/pdf/1609.08144.pdf, Section 7"
v3.3.0,Length penalty options
v3.3.0,Coverage penalty options
v3.3.0,Decoding Length constraint
v3.3.0,Decoding content constraint
v3.3.0,Adding options related to source and target features
v3.3.0,Adding options relate to decoding strategy
v3.3.0,Adding option for logging
v3.3.0,Adding options related to Transforms
v3.3.0,Copyright 2016 The Chromium Authors. All rights reserved.
v3.3.0,Use of this source code is governed by a BSD-style license that can be
v3.3.0,found in the LICENSE file.
v3.3.0,"Get the key 'value' in the dict, or just use 'value'"
v3.3.0,Basic attributes.
v3.3.0,Set model in training mode.
v3.3.0,Let's clean the GPUs before training loop
v3.3.0,UPDATE DROPOUT
v3.3.0,Run patience mechanism
v3.3.0,"If the patience has reached the limit, stop training"
v3.3.0,swap model params w/ moving average
v3.3.0,(and keep the original parameters)
v3.3.0,Set model in validating mode.
v3.3.0,F-prop through the model.
v3.3.0,Compute loss.
v3.3.0,Compute validation metrics (at batch.dataset level)
v3.3.0,Compute stats
v3.3.0,Update statistics.
v3.3.0,Set model back to training mode.
v3.3.0,Truncated BPTT: reminder not compatible with accum > 1
v3.3.0,1. Create truncated target.
v3.3.0,2. F-prop all but generator.
v3.3.0,3. Compute loss.
v3.3.0,Compute and save stats
v3.3.0,"If truncated, don't backprop fully."
v3.3.0,"in case of multi step gradient accumulation,"
v3.3.0,update only after accum batches
v3.3.0,For Flake
v3.3.0,we avoid padding while mean pooling
v3.3.0,incoming and outgoing edge embedding
v3.3.0,Find vocab data for tree builting
v3.3.0,Propogation Model
v3.3.0,Initialize the bridge layer
v3.3.0,Token embedding
v3.3.0,Initialize graph using formatted input sequence
v3.3.0,Number of flagged nodes defines node count for this sample
v3.3.0,"(Nodes can have no flags on them, but must be in 'flags' list)."
v3.3.0,The total number of integers in the vocab should allow
v3.3.0,for all features and edges to be defined.
v3.3.0,Use first extra node as only source for decoder init
v3.3.0,Average all nodes to get bridge input
v3.3.0,"LSTM has hidden and cell state, other only one"
v3.3.0,Total number of states
v3.3.0,Build a linear layer for each
v3.3.0,Initialize the bridge layer
v3.3.0,src lengths data is wrapped inside a Tensor.
v3.3.0,"LSTM has hidden and cell state, other only one"
v3.3.0,Total number of states
v3.3.0,Build a linear layer for each
v3.3.0,Auto import python files in this directory
v3.3.0,batch x len x dim
v3.3.0,"feed_forward applies residual, so we remove and apply residual with un-normed"
v3.3.0,mask is now (batch x 1 x slen x slen)
v3.3.0,1 to be expanded to number of heads in MHA
v3.3.0,Run the forward pass of every layer of the tranformer.
v3.3.0,Dimensions and padding for constructing the word embedding matrix
v3.3.0,Dimensions and padding for feature embedding matrices
v3.3.0,(these have no effect if feat_vocab_sizes is empty)
v3.3.0,The embedding matrix look-up tables. The first look-up table
v3.3.0,"is for words. Subsequent ones are for features, if any exist."
v3.3.0,The final output size of word + feature vectors. This can vary
v3.3.0,from the word vector size if and only if features are defined.
v3.3.0,This is the attribute you should access if you need to know
v3.3.0,how big your embeddings are going to be.
v3.3.0,The sequence of operations that converts the input sequence
v3.3.0,into a sequence of embeddings. At minimum this consists of
v3.3.0,looking up the embeddings for each word and feature in the
v3.3.0,input. Model parameters may require the sequence to contain
v3.3.0,additional operations as well.
v3.3.0,features must use word_vec_size
v3.3.0,features will use feat_vec_size
v3.3.0,Some utilitary functions for pretrained embeddings
v3.3.0,is this reachable?
v3.3.0,Write to file
v3.3.0,set the opt in place
v3.3.0,set the opt in place
v3.3.0,flake8: noqa
v3.3.0,For command-line option parsing
v3.3.0,"Check pass, set the args."
v3.3.0,"This SRU version implements its own cuda-level optimization,"
v3.3.0,so it requires that:
v3.3.0,1. `cupy` and `pynvrtc` python package installed.
v3.3.0,2. pytorch is built with cuda support.
v3.3.0,3. library path set: export LD_LIBRARY_PATH=<cuda lib path>.
v3.3.0,Check 1.
v3.3.0,Check 2.
v3.3.0,Check 3.
v3.3.0,This sets up device to use.
v3.3.0,-> directions x batch x dim
v3.3.0,For DEBUG
v3.3.0,"size = (length, batch, x.size(-1)) \"
v3.3.0,"if x.dim() == 3 else (batch, x.size(-1))"
v3.3.0,grad_x = x.new(*x.size()) if k_ == 3 else x.new(*size).zero_()
v3.3.0,Normal use
v3.3.0,"An entry check here, will catch on train side and translate side"
v3.3.0,if requirements are not satisfied.
v3.3.0,RNNDecoderState wraps hidden as a tuple.
v3.3.0,fh -> (layers*directions) x batch x dim
v3.3.0,This class is mainly used by decoder.py for RNNs but also
v3.3.0,by the CNN / transformer decoder when copy attention is used
v3.3.0,CNN has its own attention mechanism ConvMultiStepAttention
v3.3.0,Transformer has its own MultiHeadedAttention
v3.3.0,mlp wants it with bias
v3.3.0,"(batch, t_len, d) x (batch, d, s_len) --> (batch, t_len, s_len)"
v3.3.0,"(batch, t_len, s_len, d)"
v3.3.0,one step input
v3.3.0,"compute attention scores, as in Luong et al."
v3.3.0,Softmax or sparsemax to normalize attention weights
v3.3.0,each context vector c_t is the weighted average
v3.3.0,over all the source hidden states
v3.3.0,concatenate
v3.3.0,clamping necessary because of numerical errors: loss should be lower
v3.3.0,"bounded by zero, but negative values near zero are possible without"
v3.3.0,the clamp
v3.3.0,Help functions for Rotary Embeddings
v3.3.0,https://arxiv.org/pdf/2104.09864.pdf
v3.3.0,too convoluted to make maxseqlen a parameter.
v3.3.0,we suppose src_seq_len at training and max_length at inference
v3.3.0,are both < 2048 tokens.
v3.3.0,"rope is now matrix [maxseqlen, dim/2]"
v3.3.0,Help functions for max_relative positions
v3.3.0,https://arxiv.org/abs/1803.02155
v3.3.0,Shift values to be >= 0
v3.3.0,Help functions to split model dim per head
v3.3.0,class MultiHeadedAttention(torch.jit.ScriptModule):
v3.3.0,https://arxiv.org/pdf/1803.02155.pdf
v3.3.0,in the paper they suggest either two embeds
v3.3.0,relative_key / relative_value or only
v3.3.0,relative_key. We implemented the same embed
v3.3.0,for both.
v3.3.0,@torch.jit.script_method
v3.3.0,"1) Project key, value, and query."
v3.3.0,as a reminder at training layer_cache[0] remains False
v3.3.0,2) Calculate and scale scores.
v3.3.0,expand key on heads dimension when it's less than query heads (multi-query variant)
v3.3.0,batch x num_heads x query_len x key_len
v3.3.0,1 or key_len x key_len
v3.3.0,1 or key_len x key_len x dim_per_head
v3.3.0,not 100% necessary but expand to nb of heads
v3.3.0,now mask and scores have the same shape
v3.3.0,3) Apply attention dropout and compute context vectors.
v3.3.0,expand value on heads dimension when it's less than query heads (multi-query variant)
v3.3.0,We use the same embeddings for key and value
v3.3.0,--------------------------------------------------------------------------
v3.3.0,copied and adapted https://github.com/microsoft/LoRA/
v3.3.0,Copyright (c) Microsoft Corporation. All rights reserved.
v3.3.0,Licensed under the MIT License (MIT).
v3.3.0,Support bnb quantization of nderlying layers
v3.3.0,--------------------------------------------------------------------------
v3.3.0,Optional dropout
v3.3.0,Mark the weight as unmerged
v3.3.0,LoRA implemented in a dense layer
v3.3.0,Actual trainable parameters
v3.3.0,Freezing the pre-trained weight matrix
v3.3.0,initialize A the same way as the default
v3.3.0,for nn.Linear and B to zero
v3.3.0,Make sure that the weights are not merged
v3.3.0,Merge the weights and mark it
v3.3.0,Actual trainable parameters
v3.3.0,Freezing the pre-trained weight matrix
v3.3.0,we do not super().reset_parameters() save lot of time and useless when no grad.
v3.3.0,initialize A the same way as the default
v3.3.0,for nn.Linear and B to zero
v3.3.0,Make sure that the weights are not merged
v3.3.0,Merge the weights and mark it
v3.3.0,cannot merge/unmerge quantized weigts with unquantized lora_X
v3.3.0,Check if QLoraLinear has a custom __init__ method
v3.3.0,Invoke the __init__ method of QLoraLinear
v3.3.0,LoRA implemented in a dense layer
v3.3.0,At the moment this class is only used by embeddings.Embeddings look-up tables
v3.3.0,"for silu, see: https://arxiv.org/pdf/2002.05202.pdf"
v3.3.0,-*- coding: utf-8 -*-
v3.3.0,class AverageAttention(torch.jit.ScriptModule):
v3.3.0,@torch.jit.script
v3.3.0,Code taken from bitsandbytes but modified with arg device to accept skipt_init
v3.3.0,from torch.nn.utils => makes model building way faster.
v3.3.0,"weights are cast automatically as Int8Params, but the bias has to be cast manually"
v3.3.0,reorder weight layout back from ampere/turing to row
v3.3.0,"we only need to save SCB as extra data, because CB for quantized weights"
v3.3.0,is already stored in weight.data
v3.3.0,"case 1: .cuda was called, SCB is in self.weight"
v3.3.0,"case 2: self.init_8bit_state was called, SCB is in self.state"
v3.3.0,"buffers not yet initialized, can't call them directly without"
v3.3.0,"weights are cast automatically as Int8Params, but the bias has to be cast manually"
v3.3.0,we converted 8-bit row major to turing/ampere format in the first inference pass
v3.3.0,we no longer need the row-major weight
v3.3.0,out_features * in_features
v3.3.0,norm is out_features * 1
v3.3.0,batch_size * out_features
v3.3.0,out_features
v3.3.0,out_features
v3.3.0,batch_size * out_features
v3.3.0,"out_channels, in_channels // groups, * kernel_size"
v3.3.0,out_features
v3.3.0,This is used nowhere in the code at the moment (Vincent Nguyen 05/18/2018)
v3.3.0,"in_channels, out_channels, *kernel_size"
v3.3.0,"in_channels, out_channels, *kernel_size"
v3.3.0,"self.out_channels, 1"
v3.3.0,out_features
v3.3.0,out_features
v3.3.0,store roots on diagonal
v3.3.0,Original probabilities.
v3.3.0,Probability of copying p(z=1) batch.
v3.3.0,Probability of not copying: p_{word}(w) * (1 - p(z))
v3.3.0,probabilities assigned by the model to the gold targets
v3.3.0,probability of tokens copied from source
v3.3.0,Set scores for unk to 0 and add eps
v3.3.0,find the indices in which you do not use the copy mechanism
v3.3.0,Drop padding.
v3.3.0,Filter out very short or very long sentences
v3.3.0,from the TM for better performance
v3.3.0,We split the `batch` and perform fuzzy matching
v3.3.0,in smaller chunks of 10.000 examples in order to
v3.3.0,reduce memory usage.
v3.3.0,Perfomance is not affected.
v3.3.0,Probably redundant but let's be safe
v3.3.0,in case some examples are already fuzzied
v3.3.0,(e.g. from another pipeline or workflow)
v3.3.0,We don't want exact matches
v3.3.0,Apply a basic filtering to leave out very short or very long
v3.3.0,sentences and speed up things a bit during fuzzy matching
v3.3.0,Do nothing
v3.3.0,We set the start number of tags to a random number from 1
v3.3.0,to 12 + the number of subsequent tags that
v3.3.0,will be added. We also apply weights to this choice so tags
v3.3.0,"are more probable to start from 1, then from 2, etc."
v3.3.0,This way we cover most scenarios met in real usage and
v3.3.0,the system will learn to handle a fairly large number of
v3.3.0,numbered tags (but not an excessively large number)
v3.3.0,Make sure we only search for exact matches (we don't want
v3.3.0,to match part of words) and perform some bound checking
v3.3.0,Create all possible tag forms. We inject a special
v3.3.0,unicode char (âˆ¥) as a placeholder for whitespace in order
v3.3.0,to keep the indices unaltered. This char is replaced with
v3.3.0,spaces before we return the augmented examples.
v3.3.0,Make a weighted choice between paired tags or single tags.
v3.3.0,"We usually encounter, and thus here we favor, paired tags"
v3.3.0,with a ratio 1/3.
v3.3.0,Check if the tags include the
v3.3.0,"mandatory ""#"" number placeholder"""
v3.3.0,We split the user-defined tags in the # placeholder
v3.3.0,in order to number them
v3.3.0,normalize dict src/tgt for each dataset
v3.3.0,"print(""src empty"")"
v3.3.0,"print(""too many same char in src"")"
v3.3.0,"print(""too many same word in src"")"
v3.3.0,"print(""avg token min"", len(src_str) / len(ex['src']))"
v3.3.0,"print(""avg token max"", len(src_str) / len(ex['src']))"
v3.3.0,"print(""text does not fully belong to wanted script"")"
v3.3.0,"print(""Some text belong to unwanted scripts"")"
v3.3.0,"print(""langid does not match"", _id(src_str))"
v3.3.0,"print(""src = tgt"")"
v3.3.0,"print(""tgt empty"")"
v3.3.0,"print(""src / tgt ratio "", len(src_str) / len(tgt_str))"
v3.3.0,"print(""too many same char in tgt"")"
v3.3.0,"print(""too many same word in tgt"")"
v3.3.0,"print(""avg token min"", len(tgt_str) / len(ex['tgt']))"
v3.3.0,"print(""avg token max"", len(tgt_str) / len(ex['tgt']))"
v3.3.0,"print(""text does not fully belong to wanted script"")"
v3.3.0,"print(""Some text belong to unwanted scripts"")"
v3.3.0,"print(""langid does not match"", _id(tgt_str))"
v3.3.0,"doc break we add it, restart new doc"
v3.3.0,case 1st ex is already longer
v3.3.0,adding cur ex is too long we add cur doc
v3.3.0,and reset doc to cur ex
v3.3.0,we start the new doc with cur ex
v3.3.0,we cumulate cur ex to cur doc
v3.3.0,Auto import python files in this directory
v3.3.0,1. sample number of tokens to corrupt
v3.3.0,2. sample positions to corrput
v3.3.0,3. sample corrupted values
v3.3.0,1. sample number of tokens to corrupt
v3.3.0,2. sample positions to corrput
v3.3.0,3. Drop token on chosen position
v3.3.0,1. sample number of tokens to corrupt
v3.3.0,2. sample positions to corrput
v3.3.0,3. mask word on chosen position
v3.3.0,"Sharing options among `TokenizerTransform`s, same name conflict in"
v3.3.0,this scope will be resolved by remove previous occurrence in parser
v3.3.0,subword regularization(or BPE dropout) options:
v3.3.0,subword vocabulary restriction options:
v3.3.0,derterministic subwording
v3.3.0,subword sampling when nbest_size > 1 or -1
v3.3.0,alpha should be 0.0 < alpha < 1.0
v3.3.0,Load vocabulary file if provided and set threshold
v3.3.0,Load Subword Model
v3.3.0,-1: keep everything (i.e. 1 mask per token)
v3.3.0,0: replace everything (i.e. no mask)
v3.3.0,1: 1 mask per span
v3.3.0,view each subword as word start / input is word level token
v3.3.0,Pretend it ends with a full stop so last span is a sentence
v3.3.0,"Tokens that are full stops, where the previous token is not"
v3.3.0,Make sure we have enough to mask
v3.3.0,Trim to masking budget
v3.3.0,Handle 0-length mask (inserts) separately
v3.3.0,assert is_word_start[-1] == 0
v3.3.0,assert tokens_length - 1 not in indices
v3.3.0,"keep index, but replace it with [MASK]"
v3.3.0,"acts as a long length, so spans don't go over the end of doc"
v3.3.0,next position from each word_start
v3.3.0,delete token: 1 mask/remove per span
v3.3.0,"keep index, but replace it with [MASK]: 1 mask per token"
v3.3.0,A bit faster when all lengths are 1
v3.3.0,to cover whole token
v3.3.0,delete token
v3.3.0,"keep index, but replace it with [MASK]"
v3.3.0,assert tokens_length - 1 not in indices
v3.3.0,prefix src/tgt for each dataset
v3.3.0,prefix as general option for inference
v3.3.0,suffix src/tgt for each dataset
v3.3.0,suffix as general option for inference
v3.3.0,!/usr/bin/env python3
v3.3.0,-*- coding: utf-8 -*-
v3.3.0,Most code taken from: https://github.com/alvations/sacremoses
v3.3.0,Which in turn is based on the Moses punctuation normalizer.
v3.3.0,https://github.com/moses-smt/mosesdecoder/blob/master/scripts/
v3.3.0,tokenizer/normalize-punctuation.perl
v3.3.0,don't fix period at end of sentence
v3.3.0,Regex substitutions from replace-unicode-punctuation.perl
v3.3.0,https://github.com/moses-smt/mosesdecoder/blob/master/
v3.3.0,scripts/tokenizer/replace-unicode-punctuation.perl
v3.3.0,Adds the penn substitutions after extra_whitespace regexes.
v3.3.0,"Optionally, replace unicode puncts BEFORE normalization."
v3.3.0,Actual normalization.
v3.3.0,"Optionally, replace unicode puncts BEFORE normalization."
v3.3.0,normalize dict src/tgt for each dataset
v3.3.0,One source feature expected but none given and no default provided
v3.3.0,Provided default does not match required features
v3.3.0,Data not properly annotated.
v3.3.0,In this case we do not use the default as it might be an error
v3.3.0,batch 0 will always predict EOS. The other batches will predict
v3.3.0,non-eos scores.
v3.3.0,"""best"" prediction is eos - that should be blocked"
v3.3.0,include at least one prediction OTHER than EOS
v3.3.0,that is greater than -1e20
v3.3.0,now batch 0 has ended and no others have
v3.3.0,initial step
v3.3.0,batch 0 dies on step 0
v3.3.0,include at least one prediction OTHER than EOS
v3.3.0,that is greater than -1e20
v3.3.0,step 2
v3.3.0,(old) batch 8 dies on step 1
v3.3.0,step 3
v3.3.0,everything dies
v3.3.0,initial step
v3.3.0,batch 0 dies on step 0
v3.3.0,include at least one prediction OTHER than EOS
v3.3.0,that is greater than -1e20
v3.3.0,step 2
v3.3.0,(old) batch 8 dies on step 1
v3.3.0,step 3
v3.3.0,everything dies
v3.3.0,initial step
v3.3.0,finish one beam
v3.3.0,include at least one prediction OTHER than EOS
v3.3.0,that is greater than -1e20
v3.3.0,step 2
v3.3.0,finish example in last batch
v3.3.0,(old) batch 8 dies on step 1
v3.3.0,step 3
v3.3.0,everything dies
v3.3.0,initial step
v3.3.0,batch 0 dies on step 0
v3.3.0,include at least one prediction OTHER than EOS
v3.3.0,that is greater than -1e20
v3.3.0,step 2
v3.3.0,(old) batch 8 dies on step 1
v3.3.0,step 3
v3.3.0,everything dies
v3.3.0,illegal_weights_mask = torch.ByteTensor([
v3.3.0,"[0, 0, 0, 0, 0, 0, 0],"
v3.3.0,"[0, 0, 0, 1, 1, 1, 1],"
v3.3.0,"[0, 0, 0, 0, 0, 1, 1],"
v3.3.0,"[0, 0, 1, 1, 1, 1, 1]])"
v3.3.0,TODO: fix for pytorch 0.3
v3.3.0,illegal_weights = alignments.masked_select(illegal_weights_mask)
v3.3.0,"self.assertEqual(0.0, illegal_weights.data.sum())"
v3.3.0,this could be considered an integration test because it touches
v3.3.0,the filesystem for the config file (and the models)
v3.3.0,no dummy prefix
v3.3.0,no dummy prefix
v3.3.0,make sure the scalars are in the event accumulator tags
v3.3.0,required arguments
v3.3.0,transforms that require vocab will not create if not provide vocab
v3.3.0,1. Init first transform in the pipe
v3.3.0,2. Init second transform in the pipe
v3.3.0,3. Sequential combine them into a transform pipe
v3.3.0,4. apply transform pipe for example
v3.3.0,"5. example after the pipe exceed the length limit, thus filtered"
v3.3.0,6. Transform statistics registed (here for filtertoolong)
v3.3.0,"7. after report, statistics become empty as a fresh start"
v3.3.0,filter_transform.warm_up()
v3.3.0,test BPE-dropout:
v3.3.0,1. disable bpe dropout for not training example
v3.3.0,2. enable bpe dropout for training example
v3.3.0,3. (NOTE) disable dropout won't take effect if already seen
v3.3.0,this is caused by the cache mechanism in bpe:
v3.3.0,return cached subword if the original token is seen when no dropout
v3.3.0,test SP regularization:
v3.3.0,1. enable regularization for training example
v3.3.0,2. disable regularization for not training example
v3.3.0,Not apply token drop for not training example
v3.3.0,apply token drop for training example
v3.3.0,Not apply token mask for not training example
v3.3.0,apply token mask for training example
v3.3.0,require vocabs to warm_up
v3.3.0,Not apply token mask for not training example
v3.3.0,apply token mask for training example
v3.3.0,"Defalt: full_stop_token=[""."", ""?"", ""!""]"
v3.3.0,"Defalt: full_stop_token=[""."", ""?"", ""!""]"
v3.3.0,random_ratio of inserted tokens are chosen in vocab
v3.3.0,others are MASK_TOK
v3.3.0,"insert_ratio=0.0,"
v3.3.0,"random_ratio=0.0,"
v3.3.0,"Defalt: full_stop_token=[""."", ""?"", ""!""]"
v3.3.0,all token are considered as an individual word
v3.3.0,1. tokens are dropped when replace_length is 0
v3.3.0,"print(f""token delete: {masked} / {tokens}"")"
v3.3.0,2. tokens are replaced by MASK when replace_length is 1
v3.3.0,"print(f""token mask: {masked} / {tokens}"")"
v3.3.0,"insert_ratio=0.0,"
v3.3.0,"random_ratio=0.0,"
v3.3.0,"Defalt: full_stop_token=[""."", ""?"", ""!""]"
v3.3.0,start token of word are identified using subword marker
v3.3.0,"1. replace_length 0: ""words"" are dropped"
v3.3.0,"print(f""word delete: {masked} / {tokens}"")"
v3.3.0,"self.assertEqual(len(masked), n_words - n_masked)"
v3.3.0,"2. replace_length 1: ""words"" are replaced with a single MASK"
v3.3.0,"print(f""whole word single mask: {masked} / {tokens}"")"
v3.3.0,len(masked) depend on number of tokens in select word
v3.3.0,"3. replace_length -1: all tokens in ""words"" are replaced with MASK"
v3.3.0,"print(f""whole word multi mask: {masked} / {tokens}"")"
v3.3.0,number of mask_tok depend on number of tokens in selected word
v3.3.0,number of MASK_TOK can be greater than n_masked
v3.3.0,"insert_ratio=0.5,"
v3.3.0,"random_ratio=0.3,"
v3.3.0,"Defalt: full_stop_token=[""."", ""?"", ""!""]"
v3.3.0,start token of word are identified using subword marker
v3.3.0,n_words = sum(token_starts)
v3.3.0,n_masked = math.ceil(n_words * bart_noise.mask_ratio)
v3.3.0,"print(f""Text Span Infilling: {infillied} / {tokens}"")"
v3.3.0,"print(n_words, n_masked)"
v3.3.0,!/usr/bin/env python
v3.3.0,-*- coding: utf-8 -*-
v3.3.0,Inject some dummy training options that may needed when build fields
v3.3.0,Remove the generated *pt files.
v3.3.0,Remove the generated data samples
v3.3.0,all beams repeat (beam >= 1 repeat dummy scores)
v3.3.0,predict repeat_idx over and over again
v3.3.0,"before repeat, scores are either 0 or -inf"
v3.3.0,"on repeat, `repeat_idx` score is BLOCKED_SCORE"
v3.3.0,"(but it's still the best score, thus we have"
v3.3.0,"[BLOCKED_SCORE, -inf, -inf, -inf, -inf]"
v3.3.0,repetitions keeps maximizing score
v3.3.0,"index 0 has been blocked, so repeating=>+0.0 score"
v3.3.0,other indexes are -inf so repeating=>BLOCKED_SCORE
v3.3.0,which is higher
v3.3.0,beam 0 and beam >=2 will repeat (beam >= 2 repeat dummy scores)
v3.3.0,non-interesting beams are going to get dummy values
v3.3.0,"on initial round, only predicted scores for beam 0"
v3.3.0,matter. Make two predictions. Top one will be repeated
v3.3.0,"in beam zero, second one will live on in beam 1."
v3.3.0,predict the same thing in beam 0
v3.3.0,continue pushing around what beam 1 predicts
v3.3.0,"now beam 0 dies (along with the others), beam 1 -> beam 0"
v3.3.0,"now beam 0 dies (along with the others), beam 1 -> beam 0"
v3.3.0,beam 0 and beam >= 2 will repeat (beam 2 repeats excluded idx)
v3.3.0,non-interesting beams are going to get dummy values
v3.3.0,predict the same thing in beam 0
v3.3.0,continue pushing around what beam 1 predicts
v3.3.0,predict the allowed-repeat again in beam 2
v3.3.0,"now beam 0 dies, beam 1 -> beam 0, beam 2 -> beam 1"
v3.3.0,and the rest die
v3.3.0,"since all preds after i=0 are 0, we can check"
v3.3.0,that the beam is the correct idx by checking that
v3.3.0,the curr score is the initial score
v3.3.0,beam 0 will always predict EOS. The other beams will predict
v3.3.0,non-eos scores.
v3.3.0,non-interesting beams are going to get dummy values
v3.3.0,"""best"" prediction is eos - that should be blocked"
v3.3.0,include at least beam_sz predictions OTHER than EOS
v3.3.0,that are greater than -1e20
v3.3.0,predict eos in beam 0
v3.3.0,provide beam_sz other good predictions
v3.3.0,now the top beam has ended and no others have
v3.3.0,"not of interest, but want to make sure it keeps running"
v3.3.0,since only beam 0 terminates and n_best = 2
v3.3.0,"this is also a test that when block_ngram_repeat=0,"
v3.3.0,repeating is acceptable
v3.3.0,non-interesting beams are going to get dummy values
v3.3.0,"""best"" prediction is eos - that should be blocked"
v3.3.0,include at least beam_sz predictions OTHER than EOS
v3.3.0,that are greater than -1e20
v3.3.0,predict eos in beam 1
v3.3.0,provide beam_sz other good predictions in other beams
v3.3.0,beam 1 dies on min_length
v3.3.0,beam 0 dies on the step after beam 1 dies
v3.3.0,"inp_lens is tiled in initialize, reassign to make attn match"
v3.3.0,non-interesting beams are going to get dummy values
v3.3.0,"""best"" prediction is eos - that should be blocked"
v3.3.0,include at least beam_sz predictions OTHER than EOS
v3.3.0,that are greater than -1e20
v3.3.0,predict eos in beam 1
v3.3.0,provide beam_sz other good predictions in other beams
v3.3.0,no top beams are finished yet
v3.3.0,beam 1 dies on min_length
v3.3.0,no top beams are finished yet
v3.3.0,beam 0 dies on the step after beam 1 dies
v3.3.0,top beam is finished now so there are attentions
v3.3.0,two beams are finished in each batch
v3.3.0,second dim is cut down to the non-padded src length
v3.3.0,first dim is equal to the time of death
v3.3.0,(beam 0 died at current step - adjust for SOS)
v3.3.0,(beam 1 died at last step - adjust for SOS)
v3.3.0,behavior gets weird when beam is already done so just stop
v3.3.0,this is just test_beam.TestBeamAgainstReferenceCase repeated
v3.3.0,in each batch.
v3.3.0,"init_preds: [4, 3, 5, 6, 7] - no EOS's"
v3.3.0,no EOS's yet
v3.3.0,"[5, 3, 2, 6, 0], so beam 2 predicts EOS!"
v3.3.0,assumes beam 2 finished on last step
v3.3.0,ended beam 2 shouldn't continue
v3.3.0,"[2, 5, 3, 6, 0] repeat self.BATCH_SZ, so beam 0 predicts EOS!"
v3.3.0,"[-2.4879, -3.8910, -4.1010, -4.2010, -4.4010] repeat self.BATCH_SZ"
v3.3.0,another beam is finished in all batches
v3.3.0,new beam 0 finished
v3.3.0,new beam 0 is old beam 3
v3.3.0,assumes beam 0 finished on last step
v3.3.0,"[5, 2, 6, 1, 0] repeat self.BATCH_SZ, so beam 1 predicts EOS!"
v3.3.0,we finish 3 hyps per example in this step
v3.3.0,new beam 1 is old beam 3
v3.3.0,this could be considered an integration test because it tests
v3.3.0,interactions between the GNMT scorer and the beam
v3.3.0,"-data option is required, but not used in this test, so dummy."
v3.3.0,len x batch x nfeat
v3.3.0,Initialize vectors to compare size with
v3.3.0,Ensure correct sizes and types
v3.3.0,Make sure that output has the correct size and type
v3.3.0,"[('encoder_type', 'transformer'),"
v3.3.0,"('word_vec_size', 16), ('hidden_size', 16)],"
v3.3.0,""""""" Only do SRU test if requirment is safisfied. """""""
v3.3.0,SRU doesn't support input_feed.
v3.3.0,first check there's nothing unexpectedly not trainable
v3.3.0,ok: word embeddings shouldn't be trainable
v3.3.0,if word vecs are freezed
v3.3.0,ok: positional encodings shouldn't be trainable
v3.3.0,then check nothing unexpectedly trainable
v3.3.0,Decoder state
v3.3.0,Build the RNN.
v3.3.0,Set up the context gate.
v3.3.0,Set up the standard attention.
v3.3.0,The encoder hidden is  (layers*directions) x batch x dim.
v3.3.0,We need to convert it to layers x batch x (directions*dim).
v3.3.0,Init the input feed.
v3.3.0,Update the state with the result.
v3.3.0,Concatenates sequence of tensors along a new dimension.
v3.3.0,NOTE: v0.3 to 0.4: dec_outs / attns[*] may not be list
v3.3.0,(in particular in case of SRU) it was not raising error in 0.3
v3.3.0,since stack(Variable) was allowed.
v3.3.0,"In 0.4, SRU returns a tensor that shouldn't be stacke"
v3.3.0,Calculate the attention.
v3.3.0,Calculate the context gate.
v3.3.0,Additional args check.
v3.3.0,Input feed concatenates hidden state with
v3.3.0,input at every time step.
v3.3.0,TODO: context gate should be employed
v3.3.0,instead of second RNN transform.
v3.3.0,Update the coverage attention.
v3.3.0,"attns[""coverage""] is actually c^(t+1) of See et al(2017)"
v3.3.0,1-index shifted
v3.3.0,Decoder State
v3.3.0,CNNDecoder has its own attention mechanism.
v3.3.0,Set up a separate copy attention layer if needed.
v3.3.0,The output of CNNEncoder.
v3.3.0,The combination of output of CNNEncoder and source embeddings.
v3.3.0,Process the result and update the attentions.
v3.3.0,Update the state.
v3.3.0,TODO change the way attns is returned dict => list or tuple (onnx)
v3.3.0,Auto import python files in this directory
v3.3.0,src_len is a single tensor shared between all models.
v3.3.0,This assumption will not hold if Translator is modified
v3.3.0,to calculate src_len as something other than the length
v3.3.0,of the input.
v3.3.0,"return _, (B, Q_len, K_len)"
v3.3.0,"layer average attention across heads, get ``(B, Q, K)``"
v3.3.0,"Case 1: no full_context, no align heads -> layer avg baseline"
v3.3.0,"Case 2: no full_context, 1 align heads -> guided align"
v3.3.0,"Case 3: full_context, 1 align heads -> full cte guided align"
v3.3.0,BoolTensor was introduced in pytorch 1.2
v3.3.0,T: could be 1 in the case of stepwise decoding or tgt_len
v3.3.0,masking is necessary when sequence length is greater than one
v3.3.0,mask now are (batch x 1 x tlen x s or t len)
v3.3.0,1 = heads to be expanded in MHA
v3.3.0,"feed_forward applies residual, so we remove and apply residual with un-normed"
v3.3.0,Decoder State
v3.3.0,"previously, there was a GlobalAttention module here for copy"
v3.3.0,"attention. But it was never actually used -- the ""copy"" attention"
v3.3.0,just reuses the context attention.
v3.3.0,"attns[""align""] = torch.stack(attn_aligns, 0).mean(0)  # All avg"
v3.3.0,TODO change the way attns is returned dict => list or tuple (onnx)
v3.3.0,first value set to True triggered by the beginning of decoding
v3.3.0,layer_cache becomes active in the MultiHeadedAttention fwd
v3.3.0,T: could be 1 in the case of stepwise decoding or tgt_len
v3.3.0,masking is necessary when sequence length is greater than one
v3.3.0,mask now are (batch x 1 x tlen x tlen)
v3.3.0,1 = heads to be expanded in MHA
v3.3.0,"feed_forward applies residual, so we remove and apply residual with un-normed"
v3.3.0,TODO change the way attns is returned dict => list or tuple (onnx)
v3.3.0,"buffer size in bytes, determine equiv. # of elements based on data type"
v3.3.0,copy tensors into buffer_t
v3.3.0,all-reduce and rescale
v3.3.0,copy all-reduced buffer back into tensors
v3.3.0,"print(filled, sz)"
v3.3.0,"tensor is bigger than buffer, all-reduce and rescale directly"
v3.3.0,"buffer is full, all-reduce and replace buffer with grad"
v3.3.0,add tensor to buffer
v3.3.0,"propagate exception to parent process, keeping original traceback"
v3.3.0,TODO: Find a better way to check for sparse gradients.
v3.3.0,we use apex.amp
v3.3.0,In this case use the old FusedAdam with
v3.3.0,FP16_optimizer wrapper
v3.3.0,Load everything from the checkpoint.
v3.3.0,Build everything from scratch.
v3.3.0,"Reset optimizer, keep options."
v3.3.0,"Reset options, keep optimizer."
v3.3.0,State can be partially restored.
v3.3.0,should be: self._optimizer.zero_grad(set_to_none)
v3.3.0,but apex.amp is not up-to-date:
v3.3.0,https://github.com/NVIDIA/apex/blob/master/apex/amp/_process_optimizer.py#L367
v3.3.0,"unscaled optimizer's gradients (already done therefore skip),"
v3.3.0,skips optimizer.step() if gradients contain infs/NaNs.
v3.3.0,Updates the scale for next iteration.
v3.3.0,Code below is an implementation of https://arxiv.org/pdf/1804.04235.pdf
v3.3.0,inspired but modified from https://github.com/DeadAt0m/adafactor-pytorch
v3.3.0,backward compatibility
v3.3.0,assuming a list/generator of parameter means single group
v3.3.0,compute combined scale factor for this group
v3.3.0,norm is in fact norm*scale
v3.3.0,note: p.grad should not ever be set for correct operation of
v3.3.0,mixed precision optimizer that sometimes sends None gradients
v3.3.0,State initialization
v3.3.0,Exponential moving average of gradient values
v3.3.0,Exponential moving average of squared gradient values
v3.3.0,-*- coding: utf-8 -*-
v3.3.0,placing this here make it easier to call logger.info
v3.3.0,"from anywhere, just 'from onmt.utils.logging import logger'"
v3.3.0,"align_head contains value in [0, 1) presenting attn prob,"
v3.3.0,0 was resulted by the context attention src_pad_mask
v3.3.0,"So, the correspand position in ref_align should also be 0"
v3.3.0,"Therefore, clip align_head to > 1e-18 should be bias free."
v3.3.0,rescale with tau (temperature) and apply the log_softmax.
v3.3.0,ct2 expects src with lengths without padding
v3.3.0,again we use raw probs to rescale with tau and apply log_softmax
v3.3.0,lm_scores are in log space so log_target=True
v3.3.0,rescale with tau (temperature) and apply the log_softmax.
v3.3.0,ct2 expects src with lengths without padding
v3.3.0,again we use raw probs to rescale with tau and apply log_softmax
v3.3.0,lm_scores are in log space so log_target=True
v3.3.0,take into account here the tgt_shift_index (0 / 1 = LM/NMT)
v3.3.0,Correct target copy token instead of <unk>
v3.3.0,tgt[i] = align[i] + len(tgt_vocab)
v3.3.0,for i such that tgt[i] == 0 and align[i] != 0
v3.3.0,in the case criterion reduction is None then we need
v3.3.0,to sum the loss of each sentence in the batch
v3.3.0,Check Transforms
v3.3.0,Check path
v3.3.0,tgt is src for LM task
v3.3.0,Check weight
v3.3.0,Check features
v3.3.0,validation when train:
v3.3.0,Check embeddings stuff
v3.3.0,"Backward compatibility with ""fix_word_vecs_*"" opts"
v3.3.0,encoder and decoder should be same sizes
v3.3.0,"Load default opt values, then overwrite with the opts in"
v3.3.0,"the checkpoint. That way, if there are new options added,"
v3.3.0,the defaults are used.
v3.3.0,It comes from training
v3.3.0,TODO: needs to be added as inference opt
v3.3.0,Don't do anything
v3.3.0,Update best score of each criteria
v3.3.0,Reset tolerance
v3.3.0,Update current status
v3.3.0,Decrease tolerance
v3.3.0,Log
v3.3.0,Log
v3.3.0,Get a list of world_size lists with len(stat_list) Statistics objects
v3.3.0,"this param init is overridden by model_builder, useless then."
v3.3.0,SRU doesn't support PackedSequence.
v3.3.0,-*- coding: utf-8 -*-
v3.3.0,threshold on 1 to avoid div by 0
v3.3.0,treat alignment matrix one by one as each have different lengths
v3.3.0,No alignment if not exist valid tgt token
v3.3.0,get valid alignment (sub-matrix from full paded aligment matrix)
v3.3.0,Helper functions
v3.3.0,Keeps track of the original words/subwords
v3.3.0,('prior_tokenization' option)
v3.3.0,In case there is a final case_markup when new_spacer is on
v3.3.0,translate
v3.3.0,for validation we build an infer_iter per batch
v3.3.0,in order to avoid oom issues because there is no
v3.3.0,batching strategy in `textbatch_to_tensor`
v3.3.0,apply_reverse refs
v3.3.0,flatten preds
v3.3.0,save results
v3.3.0,-*- coding: utf-8 -*-
v3.3.0,this one is needed for Random Shuffler of batches
v3.3.0,in multi gpu it ensures datasets are read in the same order
v3.3.0,some cudnn methods can be random even after fixing the seed
v3.3.0,unless you tell it to be deterministic
v3.3.0,This one is needed for various tranfroms
v3.3.0,These ensure same initialization in multi gpu mode
v3.3.0,we need to check the model path + any tokenizer path
v3.3.0,patch to log stdout spawned processes of dataloader
v3.3.0,bucket_size = batch_size
v3.3.0,For TRAIN we need to group examples by length
v3.3.0,"for faster performance, but otherwise, sequential."
v3.3.0,For TRAIN we shuffle batches within the bucket
v3.3.0,otherwise sequential
v3.3.0,for specific case of rnn_packed need to be sorted
v3.3.0,within the batch
v3.3.0,Check if all tokens have features or none at all
v3.3.0,Make features part of src like
v3.3.0,"{'src': {'src': ..., 'feats': [...., ....]}}"
v3.3.0,at this point an example looks like:
v3.3.0,"{'src': {'src': ..., 'feats': [....]},"
v3.3.0,"'tgt': {'tgt': ...},"
v3.3.0,"'src_original': ['tok1', ...'tokn'],"
v3.3.0,"'tgt_original': ['tok1', ...'tokm'],"
v3.3.0,'indices' : seq in bucket
v3.3.0,"'align': ...,"
v3.3.0,}
v3.3.0,Need to add features in last dimensions
v3.3.0,Keep it consistent with dynamic data
v3.3.0,make a small vocab containing just the tokens in the source sequence
v3.3.0,Map source tokens to indices in the dynamic dict.
v3.3.0,-*- coding: utf-8 -*-
v3.3.0,temporary as long as translation_server and scoring_preparator still use lists
v3.3.0,this is hack: if the special separator ï½Ÿnewlineï½ is returned because of the
v3.3.0,"""docify"" transform.get_specials we don't add it if the corresponding newline code"
v3.3.0,is already included in the sentencepiece or BPE-with-gpt2-pretok.
v3.3.0,'src_original' and 'tgt_original' store the
v3.3.0,original line before tokenization. These
v3.3.0,fields are used later on in the feature
v3.3.0,transforms.
v3.3.0,NOTE: moved to dynamic_iterator.py cf process()
v3.3.0,item = self.transform.apply(
v3.3.0,"example, is_train=self.infinitely, corpus_name=self.cid)"
v3.3.0,empty example: skip
v3.3.0,bitsandbytes quantize weights when .cuda() is called
v3.3.0,for huge models we need to save Ram
v3.3.0,so we load the weights  module by module and transfer them to GPU for quantization
v3.3.0,bitsandbytes quantize weights when .cuda() is called
v3.3.0,for huge models we need to save Ram
v3.3.0,so we load the weights  module by module and transfer them to GPU for quantization
v3.3.0,"No encoder in LM, seq2seq count formatting kept"
v3.3.0,_check_save_model_path
v3.3.0,This preserves backward-compat for models using customed layernorm
v3.3.0,Force add_ffnbias to True if bias found in model w_1 keys
v3.3.0,fix v2 compatibility
v3.3.0,end of patch for backward compatibility
v3.3.0,!/usr/bin/env python
v3.3.0,!/usr/bin/env python
v3.3.0,!/usr/bin/env python
v3.3.0,-*- coding: utf-8 -*-
v3.3.0,!/usr/bin/env python
v3.3.0,BPE training
v3.3.0,SentencePiece training
v3.3.0,!/usr/bin/env python
v3.3.0,!/usr/bin/env python
v3.3.0,Set sharing strategy manually instead of default based on the OS.
v3.3.0,torch.multiprocessing.set_sharing_strategy('file_system')
v3.3.0,Create a thread to listen for errors in the child processes.
v3.3.0,Train with multiprocessing.
v3.3.0,magic indices
v3.3.0,result caching
v3.3.0,Here we set the decoder to start with self.start (BOS or EOS)
v3.3.0,fix length constraint and remove eos from count
v3.3.0,add one to account for BOS. Don't account for EOS because hitting
v3.3.0,this implies it hasn't been found.
v3.3.0,we don't block nothing if the user doesn't want it
v3.3.0,we can't block nothing beam's too short
v3.3.0,we check paths one by one
v3.3.0,we don't forbid nothing if the user doesn't want it
v3.3.0,we can't forbid nothing if beam's too short
v3.3.0,Reordering forbidden_tokens following beam selection
v3.3.0,We rebuild a dict to ensure we get the value and not the pointer
v3.3.0,Grabing the newly selected tokens and associated ngram
v3.3.0,skip the blocking if any token in current_ngram is excluded
v3.3.0,"pickups: Tensor where specified index were set to 1, others 0"
v3.3.0,"dropdowns: opposite of pickups, 1 for those shouldn't pick"
v3.3.0,Minus dropdowns to log_probs making probabilities of
v3.3.0,unspecified index close to 0
v3.3.0,"prediction step have surpass length of given target_prefix,"
v3.3.0,no need to further change this attr
v3.3.0,keep indices until overflowing p
v3.3.0,Set all logits that are not in the top-p to -10000.
v3.3.0,This puts the probabilities close to 0.
v3.3.0,Set all logits that are not in the top-k to -10000.
v3.3.0,This puts the probabilities close to 0.
v3.3.0,"For temp=0.0, take the argmax to avoid divide-by-zero errors."
v3.3.0,keep_topk=1 is also equivalent to argmax.
v3.3.0,maybe fix some prediction at this step by modifying log_probs
v3.3.0,"shape: (sum(~ self.is_finished), 1)"
v3.3.0,in LM task src_len is associated with currently generated src
v3.3.0,and therefore needs to follow the generation
v3.3.0,!/usr/bin/env python
v3.3.0,for debugging
v3.3.0,TODO: maybe add dynamic part
v3.3.0,Statistics
v3.3.0,those two should be the same except feat dim
v3.3.0,"batch['src'][perm[j], :, :])"
v3.3.0,trans.src
v3.3.0,we rebuild a small batch made of the sub-segments
v3.3.0,in the long segment.
v3.3.0,new sub-batch ready to be translated
v3.3.0,we re-insert the sub-batch in the initial translations
v3.3.0,In the case of length_penalty = none we report the total logprobs
v3.3.0,divided by the number of sentence to get an approximation of the
v3.3.0,per sentence logprob. We also return the corresponding ppl
v3.3.0,"When a length_penalty is used eg: ""avg"" or ""wu"" since logprobs"
v3.3.0,are normalized per token we report the per line per token logprob
v3.3.0,"and the corresponding ""per word perplexity"""
v3.3.0,Turn any copied words into UNKs.
v3.3.0,"Decoder forward, takes [batch, tgt_len, nfeats] as input"
v3.3.0,"and [batch, src_len, hidden] as enc_out"
v3.3.0,"in case of inference tgt_len = 1, batch = beam times batch_size"
v3.3.0,"in case of Gold Scoring tgt_len = actual length, batch = 1 batch"
v3.3.0,Generator forward.
v3.3.0,"returns [(batch_size x beam_size) , vocab ] when 1 step"
v3.3.0,"or [batch_size, tgt_len, vocab ] when full sentence"
v3.3.0,"here we have scores [tgt_lenxbatch, vocab] or [beamxbatch, vocab]"
v3.3.0,at this point scores is batch first (dim=0)
v3.3.0,"returns [(batch_size x beam_size) , vocab ] when 1 step"
v3.3.0,"or [batch_size, tgt_len, vocab ] when full sentence"
v3.3.0,(0) add BOS and padding to tgt prediction
v3.3.0,(1) Encoder forward.
v3.3.0,(2) Repeat src objects `n_best` times.
v3.3.0,"We use batch_size x n_best, get ``(batch * n_best, src_len, nfeat)``"
v3.3.0,Quick fix. Transformers return None as enc_states.
v3.3.0,enc_states are only used later on to init decoder's state
v3.3.0,"but are never used in Transformer decoder, so we can skip"
v3.3.0,"(3) Init decoder with n_best src,"
v3.3.0,"reshape tgt to ``(len, batch * n_best, nfeat)``"
v3.3.0,it should be done in a better way
v3.3.0,here dec_in is batch first
v3.3.0,masked_select
v3.3.0,get aligned src id for each prediction's valid tgt tokens
v3.3.0,TODO: support these blacklisted features
v3.3.0,(0) Prep the components of the search.
v3.3.0,(1) Run the encoder on the src.
v3.3.0,(2) prep decode_strategy. Possibly repeat src objects.
v3.3.0,(3) Begin decoding step by step:
v3.3.0,"decoder_input = decode_strategy.current_predictions.view(1, -1,"
v3.3.0,1)
v3.3.0,Reorder states.
v3.3.0,TODO: support these blacklisted features
v3.3.0,(0) Prep the components of the search.
v3.3.0,(1) split src into src and target_prefix to avoid padding.
v3.3.0,(2) init decoder
v3.3.0,(3) prep decode_strategy. Possibly repeat src objects.
v3.3.0,(4) Begin decoding step by step:
v3.3.0,Reorder states.
v3.3.0,select indexes in model state/cache
v3.3.0,beam parameters
v3.3.0,beam state
v3.3.0,BoolTensor was introduced in pytorch 1.2
v3.3.0,"""global state"" of the old beam"
v3.3.0,buffers for the topk scores and 'backpointer'
v3.3.0,for testing
v3.3.0,maybe fix some prediction at this step by modifying log_probs
v3.3.0,Flatten probs into a list of possibilities.
v3.3.0,Penalize beams that finished.
v3.3.0,"on real data (newstest2017) with the pretrained transformer,"
v3.3.0,it's faster to not move this back to the original device
v3.3.0,Store finished hypotheses for this batch.
v3.3.0,End condition is the top beam finished and we can return
v3.3.0,n_best hypotheses.
v3.3.0,"If all sentences are translated, no need to go further."
v3.3.0,Remove finished batches for the next step.
v3.3.0,using integer division to get an integer _B without casting
v3.3.0,force the output to be longer than self.min_length
v3.3.0,Multiply probs by the beam probability.
v3.3.0,"if the sequence ends now, then the penalty is the current"
v3.3.0,"length + 1, to include the EOS token"
v3.3.0,Avoid any direction that would repeat unwanted ngrams
v3.3.0,Pick up candidate token by curr_scores
v3.3.0,Recover log probs.
v3.3.0,Length penalty is just a scalar. It doesn't matter if it's applied
v3.3.0,before or after the topk.
v3.3.0,Resolve beam origin and map to batch index flat representation.
v3.3.0,Append last prediction.
v3.3.0,update global state (step == 1)
v3.3.0,update global state (step > 1)
v3.3.0,"shape: (batch_size x beam_size, 1)"
v3.3.0,in LM task src_len is associated with currently generated src
v3.3.0,and therefore needs to follow the generation
v3.3.0,in LM task src_len is associated with currently generated src
v3.3.0,and therefore needs to follow the generation
v3.3.0,Term will be subtracted from probability
v3.3.0,Probability will be divided by this
v3.3.0,these warnings indicate that either the alpha/beta
v3.3.0,"forces a penalty to be a no-op, or a penalty is a no-op but"
v3.3.0,the alpha/beta would suggest otherwise.
v3.3.0,using some coverage penalty
v3.3.0,!/usr/bin/env python
v3.3.0,semaphore doesn't have a timeout arg in Python 2.7
v3.3.0,perform a first request to initialize everything
v3.3.0,backwards compatibility for confs
v3.3.0,every segment becomes a dict for flexibility purposes
v3.3.0,NOTE: translator returns lists of `n_best` list
v3.3.0,build back results with empty texts
v3.3.0,load can be called multiple times: modify copy
v3.3.0,output contain alignment
v3.3.0,Below are all the different penalty terms implemented so far.
v3.3.0,Subtract coverage penalty from topk log probs.
v3.3.0,Divide topk log probs by length penalty.
v3.3.0,Sorting
v3.3.0,Chinese segmentation
v3.3.0,Chinese simplify -> Chinese traditional standard
v3.3.0,Chinese simplify -> Chinese traditional (HongKong)
v3.3.0,Chinese simplify -> Chinese traditional (Taiwan)
v3.3.0,Chinese traditional -> Chinese simplify (v1)
v3.3.0,Chinese traditional -> Chinese simplify (v2)
v3.3.0,Auto import python files in this directory
v3.3.0,"def custom_stopping_criteria(input_ids, score, **kwargs):"
v3.3.0,"stop_ids = [29871, 13, 13] # \n\n"
v3.3.0,return input_ids[-len(stop_ids)]
v3.3.0,Build the translator (along with the model)
v3.3.0,Build the transforms (along with the tokenizer)
v3.3.0,get prompt and make sure it fits
v3.2.0,!/usr/bin/env python
v3.2.0,!/usr/bin/env python
v3.2.0,!/usr/bin/env python
v3.2.0,!/usr/bin/env python
v3.2.0,!/usr/bin/env python
v3.2.0,!/usr/bin/env python3
v3.2.0,-*- coding: utf-8 -*-
v3.2.0,
v3.2.0,"OpenNMT-py documentation build configuration file, created by"
v3.2.0,sphinx-quickstart on Sun Dec 17 12:07:14 2017.
v3.2.0,
v3.2.0,This file is execfile()d with the current directory set to its
v3.2.0,containing dir.
v3.2.0,
v3.2.0,Note that not all possible configuration values are present in this
v3.2.0,autogenerated file.
v3.2.0,
v3.2.0,All configuration values have a default; values that are commented out
v3.2.0,serve to show the default.
v3.2.0,"If extensions (or modules to document with autodoc) are in another directory,"
v3.2.0,add these directories to sys.path here. If the directory is relative to the
v3.2.0,"documentation root, use os.path.abspath to make it absolute, like shown here."
v3.2.0,
v3.2.0,import os
v3.2.0,import sys
v3.2.0,"sys.path.insert(0, os.path.abspath('.'))"
v3.2.0,-- General configuration ------------------------------------------------
v3.2.0,"If your documentation needs a minimal Sphinx version, state it here."
v3.2.0,
v3.2.0,needs_sphinx = '6.0'
v3.2.0,"Add any Sphinx extension module names here, as strings. They can be"
v3.2.0,extensions coming with Sphinx (named 'sphinx.ext.*') or your custom
v3.2.0,ones.
v3.2.0,Show base classes
v3.2.0,"Use ""variables"" section for Attributes instead of weird block things"
v3.2.0,mimicking the function style.
v3.2.0,"Add any paths that contain templates here, relative to this directory."
v3.2.0,The suffix(es) of source filenames.
v3.2.0,You can specify multiple suffix as a list of string:
v3.2.0,
v3.2.0,"source_suffix = ['.rst', '.md']"
v3.2.0,The master toctree document.
v3.2.0,General information about the project.
v3.2.0,"The version info for the project you're documenting, acts as replacement for"
v3.2.0,"|version| and |release|, also used in various other places throughout the"
v3.2.0,built documents.
v3.2.0,
v3.2.0,The short X.Y version.
v3.2.0,"The full version, including alpha/beta/rc tags."
v3.2.0,The language for content autogenerated by Sphinx. Refer to documentation
v3.2.0,for a list of supported languages.
v3.2.0,
v3.2.0,This is also used if you do content translation via gettext catalogs.
v3.2.0,"Usually you set ""language"" from the command line for these cases."
v3.2.0,"List of patterns, relative to source directory, that match files and"
v3.2.0,directories to ignore when looking for source files.
v3.2.0,This patterns also effect to html_static_path and html_extra_path
v3.2.0,The name of the Pygments (syntax highlighting) style to use.
v3.2.0,"If true, `todo` and `todoList` produce output, else they produce nothing."
v3.2.0,-- Options for HTML output ----------------------------------------------
v3.2.0,The theme to use for HTML and HTML Help pages.  See the documentation for
v3.2.0,a list of builtin themes.
v3.2.0,
v3.2.0,html_theme = 'sphinx_materialdesign_theme'
v3.2.0,html_theme_path = [sphinx_materialdesign_theme.get_path()]
v3.2.0,Theme options are theme-specific and customize the look and feel of a theme
v3.2.0,"further.  For a list of options available for each theme, see the"
v3.2.0,documentation.
v3.2.0,
v3.2.0,html_theme_options = {}
v3.2.0,"Add any paths that contain custom static files (such as style sheets) here,"
v3.2.0,"relative to this directory. They are copied after the builtin static files,"
v3.2.0,"so a file named ""default.css"" will overwrite the builtin ""default.css""."
v3.2.0,"Custom sidebar templates, must be a dictionary that maps document names"
v3.2.0,to template names.
v3.2.0,
v3.2.0,This is required for the alabaster theme
v3.2.0,refs: http://alabaster.readthedocs.io/en/latest/installation.html#sidebars
v3.2.0,-- Options for HTMLHelp output ------------------------------------------
v3.2.0,Output file base name for HTML help builder.
v3.2.0,-- Options for LaTeX output ---------------------------------------------
v3.2.0,The paper size ('letterpaper' or 'a4paper').
v3.2.0,
v3.2.0,"'papersize': 'letterpaper',"
v3.2.0,"The font size ('10pt', '11pt' or '12pt')."
v3.2.0,
v3.2.0,"'pointsize': '10pt',"
v3.2.0,Additional stuff for the LaTeX preamble.
v3.2.0,
v3.2.0,"'preamble': '',"
v3.2.0,Latex figure (float) alignment
v3.2.0,
v3.2.0,"'figure_align': 'htbp',"
v3.2.0,Grouping the document tree into LaTeX files. List of tuples
v3.2.0,"(source start file, target name, title,"
v3.2.0,"author, documentclass [howto, manual, or own class])."
v3.2.0,-- Options for manual page output ---------------------------------------
v3.2.0,One entry per manual page. List of tuples
v3.2.0,"(source start file, name, description, authors, manual section)."
v3.2.0,-- Options for Texinfo output -------------------------------------------
v3.2.0,Grouping the document tree into Texinfo files. List of tuples
v3.2.0,"(source start file, target name, title, author,"
v3.2.0,"dir menu entry, description, category)"
v3.2.0,"inf_type = ""ct2"""
v3.2.0,#####################
v3.2.0,Inference with CT2 #
v3.2.0,#####################
v3.2.0,#####################
v3.2.0,Inference with -py #
v3.2.0,#####################
v3.2.0,"ckpt_path = ""finetuned_llama7B/llama7B-vicuna-onmt_step_4000.pt"""
v3.2.0,we receive a text box content
v3.2.0,might be good to split also based on full period (later)
v3.2.0,we reformat the transformed batch to be numericalized / tensorified
v3.2.0,#####
v3.2.0,UI #
v3.2.0,#####
v3.2.0,What are the 3 best french cities ?
v3.2.0,Which one is better if I like outdoor activities ?
v3.2.0,Which one is better if I like cultural outings?
v3.2.0,What are the best neighborhoods in these 5 cities?
v3.2.0,!/usr/bin/env python3
v3.2.0,Usage: python3 filter_train.py in.src in.trg out.src out.trg max-tokens
v3.2.0,!/usr/bin/env python
v3.2.0,-*- coding: utf-8 -*-
v3.2.0,is this reachable?
v3.2.0,Read in embeddings
v3.2.0,Write to file
v3.2.0,converts a SentencePiece vocabulary to the format expected by dynamic data
v3.2.0,"(essentially converts float expected counts to ""fixed precision"" int pseudo"
v3.2.0,counts)
v3.2.0,from onmt.utils.misc import use_gpu
v3.2.0,"Add in default model arguments, possibly added since training."
v3.2.0,this patch is no longer needed included in converter
v3.2.0,"if hasattr(model_opt, 'rnn_size'):"
v3.2.0,model_opt.hidden_size = model_opt.rnn_size
v3.2.0,build_base_model expects updated and validated opts
v3.2.0,-*- encoding: utf-8 -*-
v3.2.0,!/usr/bin/env python
v3.2.0,flake8: noqa
v3.2.0,Falcon stores QKV in one single tensor but it is not simply piled up Q+K+V
v3.2.0,it is heads interleaved to we need to slice first
v3.2.0,also it uses the HF rotary so we need to permute Q and K interleave
v3.2.0,!/usr/bin/env python
v3.2.0,-*- coding: utf-8 -*-
v3.2.0,Author: Rico Sennrich
v3.2.0,flake8: noqa
v3.2.0,This file is retrieved from https://github.com/rsennrich/subword-nmt
v3.2.0,hack for python2/3 compatibility
v3.2.0,check version information
v3.2.0,some hacking to deal with duplicates (only consider first instance)
v3.2.0,don't print end-of-word symbols
v3.2.0,sys.stderr.write('cannot split {0} further.\n'.format(segment))
v3.2.0,sys.stderr.write('OOV: {0}\n'.format(segment))
v3.2.0,sys.stderr.write('OOV: {0}\n'.format(segment))
v3.2.0,python 2/3 compatibility
v3.2.0,read/write files as UTF-8
v3.2.0,!/usr/bin/env python3
v3.2.0,coding: utf-8
v3.2.0,"In order to use this tool, please install comet first"
v3.2.0,https://github.com/Unbabel/COMET
v3.2.0,Let's say you have a source file with N sentences in SL - eg: source.sl
v3.2.0,and the corresponding references (N sentences) reference.tl
v3.2.0,Translate your file in TL with the -n_best nbest options nbest being
v3.2.0,then number of hypotheses and output the target to -output target.nbest.tl
v3.2.0,Then you need to duplicate source and reference sentences nbest times
v3.2.0,for this script.
v3.2.0,for instance using awk '{for(i=1; i<=n; i++) print}' n=5 reference.tl \
v3.2.0,> reference.5.tl
v3.2.0,same for source.
v3.2.0,This script can be run (for instance with nbest = 5) as follows:
v3.2.0,python oracle_bleu.py --nbest-src source.5.sl --nbest-hyp target.5.tl \
v3.2.0,--nbest-ref reference.5.tl --nbest-order 5 --output target.maxbleu.tl
v3.2.0,It will search in all hyp the best comet score
v3.2.0,when choosing a reference-less model no nbest-ref is required
v3.2.0,for nbest in nbests:
v3.2.0,!/usr/bin/env python
v3.2.0,!/usr/bin/env python3
v3.2.0,coding: utf-8
v3.2.0,Let's say you have a source file with N sentences in SL - eg: source.sl
v3.2.0,Translate your file in TL with the -n_best nbest options nbest being
v3.2.0,then number of hypotheses and output the target to -output target.nbest.tl
v3.2.0,This script can be run (for instance with nbest = 5) as follows:
v3.2.0,python mbr_bleu.py --nbest-hyp target.5.tl \
v3.2.0,--nbest-order 5 --output target.mbr.tl
v3.2.0,It will compare all hyp with eachother and output the max bleu
v3.2.0,!/usr/bin/env python
v3.2.0,flake8: noqa
v3.2.0,!/usr/bin/env python
v3.2.0,-*- coding: utf-8 -*-
v3.2.0,Author: Rico Sennrich
v3.2.0,flake8: noqa
v3.2.0,This file is retrieved from https://github.com/rsennrich/subword-nmt
v3.2.0,hack for python2/3 compatibility
v3.2.0,"find all instances of pair, and update frequency/indices around it"
v3.2.0,find first symbol
v3.2.0,"if first symbol is followed by second symbol, we've found an occurrence of pair (old_word[i:i+2])"
v3.2.0,"assuming a symbol sequence ""A B C"", if ""B C"" is merged, reduce the frequency of ""A B"""
v3.2.0,"assuming a symbol sequence ""A B C B"", if ""B C"" is merged, reduce the frequency of ""C B""."
v3.2.0,"however, skip this if the sequence is A B C B C, because the frequency of ""C B"" will be reduced by the previous code block"
v3.2.0,find new pair
v3.2.0,"assuming a symbol sequence ""A BC D"", if ""B C"" is merged, increase the frequency of ""A BC"""
v3.2.0,"assuming a symbol sequence ""A BC B"", if ""B C"" is merged, increase the frequency of ""BC B"""
v3.2.0,"however, if the sequence is A BC BC, skip this step because the count of ""BC BC"" will be incremented by the previous code block"
v3.2.0,data structure of pair frequencies
v3.2.0,index from pairs to words
v3.2.0,version 0.2 changes the handling of the end-of-word token ('</w>');
v3.2.0,version numbering allows bckward compatibility
v3.2.0,"threshold is inspired by Zipfian assumption, but should only affect speed"
v3.2.0,we probably missed the best pair because of pruning; go back to full statistics
v3.2.0,"threshold is inspired by Zipfian assumption, but should only affect speed"
v3.2.0,python 2/3 compatibility
v3.2.0,read/write files as UTF-8
v3.2.0,Now we can pipe the full file through the model using the Iterator
v3.2.0,reminder a batch includes .src .tgt .indices and it is sorted
v3.2.0,Compute and retrieve the loss for EACH sentence
v3.2.0,Now we need to rearrange the batch of ppl
v3.2.0,in the original order with indices
v3.2.0,!/usr/bin/env python
v3.2.0,-*- coding: utf-8 -*-
v3.2.0,!/usr/bin/env python
v3.2.0,flake8: noqa
v3.2.0,!/usr/bin/env python
v3.2.0,!/usr/bin/env python
v3.2.0,flake8: noqa
v3.2.0,!/usr/bin/env python3
v3.2.0,coding: utf-8
v3.2.0,Let's say you have a source file with N sentences in SL - eg: source.sl
v3.2.0,and the corresponding references (N sentences) reference.tl
v3.2.0,Translate your file in TL with the -n_best nbest options nbest being
v3.2.0,then number of hypotheses and output the target to -output target.nbest.tl
v3.2.0,Then you need to duplicate reference sentences nbest times for this script.
v3.2.0,for instance using awk '{for(i=1; i<=n; i++) print}' n=5 reference.tl \
v3.2.0,> reference.5.tl
v3.2.0,This script can be run (for instance with nbest = 5) as follows:
v3.2.0,python oracle_bleu.py --nbest-hyp target.5.tl --nbest-ref reference.5.tl \
v3.2.0,--nbest-order 5 --output target.maxbleu.tl
v3.2.0,It will search in all hyp the best bleu wrt reference
v3.2.0,and output the max bleu
v3.2.0,!/usr/bin/env python
v3.2.0,with the two module = imp.load_source() below
v3.2.0,we ghost the old torchtext.data.field and depercated
v3.2.0,onmt.inputters.text_dataset
v3.2.0,however this require some functions / classes to be
v3.2.0,monkey patched for loading the old field/vocab objects.
v3.2.0,"module3 = imp.load_source(""Vocab"", ""tools/convertv2_v3.py"")"
v3.2.0,"voc = dict(sorted(fields.vocab.__dict__['freqs'].items(),"
v3.2.0,"key=lambda x: (-x[1], x[0]))).keys()"
v3.2.0,"voc = dict(sorted(fields.vocab.__dict__['freqs'].items(),"
v3.2.0,"key=lambda x: (-x[1], x[0]))).keys()"
v3.2.0,!/usr/bin/env python
v3.2.0,flake8: noqa
v3.2.0,redpajama stores QKV in one single tensor but it is not simply piled up Q+K+V
v3.2.0,it is heads interleaved to we need to slice first
v3.2.0,also it uses the HF rotary so we need to permute Q and K interleave
v3.2.0,Avoid functionality on inference
v3.2.0,Build embeddings.
v3.2.0,Build encoder.
v3.2.0,Build embeddings.
v3.2.0,Build decoder.
v3.2.0,Share the embedding matrix - preprocess with share_vocab required.
v3.2.0,src/tgt vocab should be the same if `-share_vocab` is specified.
v3.2.0,Update vocabulary embeddings with checkpoint embeddings
v3.2.0,Embedding layers
v3.2.0,Just for debugging purposes
v3.2.0,Remove old vocabulary associated embeddings
v3.2.0,for back compat when attention_dropout was not defined
v3.2.0,Build Model
v3.2.0,Build Generator.
v3.2.0,If new training initialize the model params
v3.2.0,If update_vocab init also but checkpoint will overwrite old weights
v3.2.0,ONLY for legacy fusedam with amp pytorch requires NOT to half the model
v3.2.0,Update model embeddings with those from the checkpoint
v3.2.0,after initialization
v3.2.0,after this checkpoint contains no embeddings
v3.2.0,when using LoRa or updating the vocab (no more embeddings in ckpt)
v3.2.0,=> strict=False when loading state_dict
v3.2.0,!/usr/bin/env python
v3.2.0,if transform + options set in 'valid' we need to copy in main
v3.2.0,transform / options for scoring considered as inference
v3.2.0,"maybe prepare pretrained embeddings, if any"
v3.2.0,Load checkpoint if we resume from a previous training.
v3.2.0,ensure tensorboard output is written in the directory
v3.2.0,of previous checkpoints
v3.2.0,Override checkpoint's update_embeddings as it defaults to false
v3.2.0,Override checkpoint's freezing settings as it defaults to false
v3.2.0,NOTE: It's important that ``opt`` has been validated and updated
v3.2.0,at this point.
v3.2.0,Build model.
v3.2.0,Build optimizer.
v3.2.0,Build model saver
v3.2.0,Use Tensorboard for visualization during training
v3.2.0,Options only during inference
v3.2.0,"Truncation options, for text corpus"
v3.2.0,"as for False, this will be added in _add_train_general_opts"
v3.2.0,Embedding Options
v3.2.0,Model Task Options
v3.2.0,Encoder-Decoder Options
v3.2.0,Freeze Encoder and/or Decoder
v3.2.0,The following options (bridge_extra_node to n_steps) are used
v3.2.0,for training with --encoder_type ggnn (Gated Graph Neural Network).
v3.2.0,Attention options
v3.2.0,Alignement options
v3.2.0,Generator and loss options.
v3.2.0,GPU
v3.2.0,LoRa
v3.2.0,Init options
v3.2.0,Pretrained word vectors
v3.2.0,Freeze word vectors
v3.2.0,Optimization options
v3.2.0,learning rate
v3.2.0,options relate to data preprare
v3.2.0,options relate to train
v3.2.0,Alpha and Beta values for Google Length + Coverage penalty
v3.2.0,"Described here: https://arxiv.org/pdf/1609.08144.pdf, Section 7"
v3.2.0,Length penalty options
v3.2.0,Coverage penalty options
v3.2.0,Decoding Length constraint
v3.2.0,Decoding content constraint
v3.2.0,Adding options related to source and target features
v3.2.0,Adding options relate to decoding strategy
v3.2.0,Adding option for logging
v3.2.0,Adding options related to Transforms
v3.2.0,Copyright 2016 The Chromium Authors. All rights reserved.
v3.2.0,Use of this source code is governed by a BSD-style license that can be
v3.2.0,found in the LICENSE file.
v3.2.0,"Get the key 'value' in the dict, or just use 'value'"
v3.2.0,Basic attributes.
v3.2.0,Set model in training mode.
v3.2.0,Let's clean the GPUs before training loop
v3.2.0,UPDATE DROPOUT
v3.2.0,Run patience mechanism
v3.2.0,"If the patience has reached the limit, stop training"
v3.2.0,swap model params w/ moving average
v3.2.0,(and keep the original parameters)
v3.2.0,Set model in validating mode.
v3.2.0,F-prop through the model.
v3.2.0,Compute loss.
v3.2.0,Compute validation metrics (at batch.dataset level)
v3.2.0,Compute stats
v3.2.0,Update statistics.
v3.2.0,Set model back to training mode.
v3.2.0,Truncated BPTT: reminder not compatible with accum > 1
v3.2.0,1. Create truncated target.
v3.2.0,2. F-prop all but generator.
v3.2.0,3. Compute loss.
v3.2.0,Compute and save stats
v3.2.0,"If truncated, don't backprop fully."
v3.2.0,"in case of multi step gradient accumulation,"
v3.2.0,update only after accum batches
v3.2.0,For Flake
v3.2.0,we avoid padding while mean pooling
v3.2.0,incoming and outgoing edge embedding
v3.2.0,Find vocab data for tree builting
v3.2.0,Propogation Model
v3.2.0,Initialize the bridge layer
v3.2.0,Token embedding
v3.2.0,Initialize graph using formatted input sequence
v3.2.0,Number of flagged nodes defines node count for this sample
v3.2.0,"(Nodes can have no flags on them, but must be in 'flags' list)."
v3.2.0,The total number of integers in the vocab should allow
v3.2.0,for all features and edges to be defined.
v3.2.0,Use first extra node as only source for decoder init
v3.2.0,Average all nodes to get bridge input
v3.2.0,"LSTM has hidden and cell state, other only one"
v3.2.0,Total number of states
v3.2.0,Build a linear layer for each
v3.2.0,Initialize the bridge layer
v3.2.0,src lengths data is wrapped inside a Tensor.
v3.2.0,"LSTM has hidden and cell state, other only one"
v3.2.0,Total number of states
v3.2.0,Build a linear layer for each
v3.2.0,batch x len x dim
v3.2.0,"feed_forward applies residual, so we remove and apply residual with un-normed"
v3.2.0,mask is now (batch x 1 x slen x slen)
v3.2.0,1 to be expanded to number of heads in MHA
v3.2.0,Run the forward pass of every layer of the tranformer.
v3.2.0,Dimensions and padding for constructing the word embedding matrix
v3.2.0,Dimensions and padding for feature embedding matrices
v3.2.0,(these have no effect if feat_vocab_sizes is empty)
v3.2.0,The embedding matrix look-up tables. The first look-up table
v3.2.0,"is for words. Subsequent ones are for features, if any exist."
v3.2.0,The final output size of word + feature vectors. This can vary
v3.2.0,from the word vector size if and only if features are defined.
v3.2.0,This is the attribute you should access if you need to know
v3.2.0,how big your embeddings are going to be.
v3.2.0,The sequence of operations that converts the input sequence
v3.2.0,into a sequence of embeddings. At minimum this consists of
v3.2.0,looking up the embeddings for each word and feature in the
v3.2.0,input. Model parameters may require the sequence to contain
v3.2.0,additional operations as well.
v3.2.0,features must use word_vec_size
v3.2.0,features will use feat_vec_size
v3.2.0,Some utilitary functions for pretrained embeddings
v3.2.0,is this reachable?
v3.2.0,Write to file
v3.2.0,set the opt in place
v3.2.0,set the opt in place
v3.2.0,flake8: noqa
v3.2.0,For command-line option parsing
v3.2.0,"Check pass, set the args."
v3.2.0,"This SRU version implements its own cuda-level optimization,"
v3.2.0,so it requires that:
v3.2.0,1. `cupy` and `pynvrtc` python package installed.
v3.2.0,2. pytorch is built with cuda support.
v3.2.0,3. library path set: export LD_LIBRARY_PATH=<cuda lib path>.
v3.2.0,Check 1.
v3.2.0,Check 2.
v3.2.0,Check 3.
v3.2.0,This sets up device to use.
v3.2.0,-> directions x batch x dim
v3.2.0,For DEBUG
v3.2.0,"size = (length, batch, x.size(-1)) \"
v3.2.0,"if x.dim() == 3 else (batch, x.size(-1))"
v3.2.0,grad_x = x.new(*x.size()) if k_ == 3 else x.new(*size).zero_()
v3.2.0,Normal use
v3.2.0,"An entry check here, will catch on train side and translate side"
v3.2.0,if requirements are not satisfied.
v3.2.0,RNNDecoderState wraps hidden as a tuple.
v3.2.0,fh -> (layers*directions) x batch x dim
v3.2.0,This class is mainly used by decoder.py for RNNs but also
v3.2.0,by the CNN / transformer decoder when copy attention is used
v3.2.0,CNN has its own attention mechanism ConvMultiStepAttention
v3.2.0,Transformer has its own MultiHeadedAttention
v3.2.0,mlp wants it with bias
v3.2.0,"(batch, t_len, d) x (batch, d, s_len) --> (batch, t_len, s_len)"
v3.2.0,"(batch, t_len, s_len, d)"
v3.2.0,one step input
v3.2.0,"compute attention scores, as in Luong et al."
v3.2.0,Softmax or sparsemax to normalize attention weights
v3.2.0,each context vector c_t is the weighted average
v3.2.0,over all the source hidden states
v3.2.0,concatenate
v3.2.0,clamping necessary because of numerical errors: loss should be lower
v3.2.0,"bounded by zero, but negative values near zero are possible without"
v3.2.0,the clamp
v3.2.0,Help functions for Rotary Embeddings
v3.2.0,https://arxiv.org/pdf/2104.09864.pdf
v3.2.0,too convoluted to make maxseqlen a parameter.
v3.2.0,we suppose src_seq_len at training and max_length at inference
v3.2.0,are both < 2048 tokens.
v3.2.0,"rope is now matrix [maxseqlen, dim/2]"
v3.2.0,Help functions for max_relative positions
v3.2.0,https://arxiv.org/abs/1803.02155
v3.2.0,Shift values to be >= 0
v3.2.0,Help functions to split model dim per head
v3.2.0,class MultiHeadedAttention(torch.jit.ScriptModule):
v3.2.0,https://arxiv.org/pdf/1803.02155.pdf
v3.2.0,in the paper they suggest either two embeds
v3.2.0,relative_key / relative_value or only
v3.2.0,relative_key. We implemented the same embed
v3.2.0,for both.
v3.2.0,@torch.jit.script_method
v3.2.0,"1) Project key, value, and query."
v3.2.0,as a reminder at training layer_cache[0] remains False
v3.2.0,2) Calculate and scale scores.
v3.2.0,batch x num_heads x query_len x key_len
v3.2.0,1 or key_len x key_len
v3.2.0,1 or key_len x key_len x dim_per_head
v3.2.0,not 100% necessary but expand to nb of heads
v3.2.0,now mask and scores have the same shape
v3.2.0,3) Apply attention dropout and compute context vectors.
v3.2.0,We use the same embeddings for key and value
v3.2.0,--------------------------------------------------------------------------
v3.2.0,copied and adapted https://github.com/microsoft/LoRA/
v3.2.0,Copyright (c) Microsoft Corporation. All rights reserved.
v3.2.0,Licensed under the MIT License (MIT).
v3.2.0,Support bnb quantization of nderlying layers
v3.2.0,--------------------------------------------------------------------------
v3.2.0,Optional dropout
v3.2.0,Mark the weight as unmerged
v3.2.0,LoRA implemented in a dense layer
v3.2.0,Actual trainable parameters
v3.2.0,Freezing the pre-trained weight matrix
v3.2.0,initialize A the same way as the default
v3.2.0,for nn.Linear and B to zero
v3.2.0,Make sure that the weights are not merged
v3.2.0,Merge the weights and mark it
v3.2.0,Actual trainable parameters
v3.2.0,Freezing the pre-trained weight matrix
v3.2.0,we do not super().reset_parameters() save lot of time and useless when no grad.
v3.2.0,initialize A the same way as the default
v3.2.0,for nn.Linear and B to zero
v3.2.0,Make sure that the weights are not merged
v3.2.0,Merge the weights and mark it
v3.2.0,cannot merge/unmerge quantized weigts with unquantized lora_X
v3.2.0,Check if QLoraLinear has a custom __init__ method
v3.2.0,Invoke the __init__ method of QLoraLinear
v3.2.0,LoRA implemented in a dense layer
v3.2.0,At the moment this class is only used by embeddings.Embeddings look-up tables
v3.2.0,"for silu, see: https://arxiv.org/pdf/2002.05202.pdf"
v3.2.0,-*- coding: utf-8 -*-
v3.2.0,class AverageAttention(torch.jit.ScriptModule):
v3.2.0,@torch.jit.script
v3.2.0,Code taken from bitsandbytes but modified with arg device to accept skipt_init
v3.2.0,from torch.nn.utils => makes model building way faster.
v3.2.0,"weights are cast automatically as Int8Params, but the bias has to be cast manually"
v3.2.0,reorder weight layout back from ampere/turing to row
v3.2.0,"we only need to save SCB as extra data, because CB for quantized weights"
v3.2.0,is already stored in weight.data
v3.2.0,"case 1: .cuda was called, SCB is in self.weight"
v3.2.0,"case 2: self.init_8bit_state was called, SCB is in self.state"
v3.2.0,"buffers not yet initialized, can't call them directly without"
v3.2.0,"weights are cast automatically as Int8Params, but the bias has to be cast manually"
v3.2.0,we converted 8-bit row major to turing/ampere format in the first inference pass
v3.2.0,we no longer need the row-major weight
v3.2.0,out_features * in_features
v3.2.0,norm is out_features * 1
v3.2.0,batch_size * out_features
v3.2.0,out_features
v3.2.0,out_features
v3.2.0,batch_size * out_features
v3.2.0,"out_channels, in_channels // groups, * kernel_size"
v3.2.0,out_features
v3.2.0,This is used nowhere in the code at the moment (Vincent Nguyen 05/18/2018)
v3.2.0,"in_channels, out_channels, *kernel_size"
v3.2.0,"in_channels, out_channels, *kernel_size"
v3.2.0,"self.out_channels, 1"
v3.2.0,out_features
v3.2.0,out_features
v3.2.0,store roots on diagonal
v3.2.0,Original probabilities.
v3.2.0,Probability of copying p(z=1) batch.
v3.2.0,Probability of not copying: p_{word}(w) * (1 - p(z))
v3.2.0,probabilities assigned by the model to the gold targets
v3.2.0,probability of tokens copied from source
v3.2.0,Set scores for unk to 0 and add eps
v3.2.0,find the indices in which you do not use the copy mechanism
v3.2.0,Drop padding.
v3.2.0,Filter out very short or very long sentences
v3.2.0,from the TM for better performance
v3.2.0,We split the `batch` and perform fuzzy matching
v3.2.0,in smaller chunks of 10.000 examples in order to
v3.2.0,reduce memory usage.
v3.2.0,Perfomance is not affected.
v3.2.0,Probably redundant but let's be safe
v3.2.0,in case some examples are already fuzzied
v3.2.0,(e.g. from another pipeline or workflow)
v3.2.0,We don't want exact matches
v3.2.0,Apply a basic filtering to leave out very short or very long
v3.2.0,sentences and speed up things a bit during fuzzy matching
v3.2.0,Do nothing
v3.2.0,We set the start number of tags to a random number from 1
v3.2.0,to 12 + the number of subsequent tags that
v3.2.0,will be added. We also apply weights to this choice so tags
v3.2.0,"are more probable to start from 1, then from 2, etc."
v3.2.0,This way we cover most scenarios met in real usage and
v3.2.0,the system will learn to handle a fairly large number of
v3.2.0,numbered tags (but not an excessively large number)
v3.2.0,Make sure we only search for exact matches (we don't want
v3.2.0,to match part of words) and perform some bound checking
v3.2.0,Create all possible tag forms. We inject a special
v3.2.0,unicode char (âˆ¥) as a placeholder for whitespace in order
v3.2.0,to keep the indices unaltered. This char is replaced with
v3.2.0,spaces before we return the augmented examples.
v3.2.0,Make a weighted choice between paired tags or single tags.
v3.2.0,"We usually encounter, and thus here we favor, paired tags"
v3.2.0,with a ratio 1/3.
v3.2.0,Check if the tags include the
v3.2.0,"mandatory ""#"" number placeholder"""
v3.2.0,We split the user-defined tags in the # placeholder
v3.2.0,in order to number them
v3.2.0,normalize dict src/tgt for each dataset
v3.2.0,"print(""src empty"")"
v3.2.0,"print(""too many same char in src"")"
v3.2.0,"print(""too many same word in src"")"
v3.2.0,"print(""avg token min"", len(src_str) / len(ex['src']))"
v3.2.0,"print(""avg token max"", len(src_str) / len(ex['src']))"
v3.2.0,"print(""text does not fully belong to wanted script"")"
v3.2.0,"print(""Some text belong to unwanted scripts"")"
v3.2.0,"print(""langid does not match"", _id(src_str))"
v3.2.0,"print(""src = tgt"")"
v3.2.0,"print(""tgt empty"")"
v3.2.0,"print(""src / tgt ratio "", len(src_str) / len(tgt_str))"
v3.2.0,"print(""too many same char in tgt"")"
v3.2.0,"print(""too many same word in tgt"")"
v3.2.0,"print(""avg token min"", len(tgt_str) / len(ex['tgt']))"
v3.2.0,"print(""avg token max"", len(tgt_str) / len(ex['tgt']))"
v3.2.0,"print(""text does not fully belong to wanted script"")"
v3.2.0,"print(""Some text belong to unwanted scripts"")"
v3.2.0,"print(""langid does not match"", _id(tgt_str))"
v3.2.0,"doc break we add it, restart new doc"
v3.2.0,case 1st ex is already longer
v3.2.0,adding cur ex is too long we add cur doc
v3.2.0,and reset doc to cur ex
v3.2.0,we start the new doc with cur ex
v3.2.0,we cumulate cur ex to cur doc
v3.2.0,Auto import python files in this directory
v3.2.0,1. sample number of tokens to corrupt
v3.2.0,2. sample positions to corrput
v3.2.0,3. sample corrupted values
v3.2.0,1. sample number of tokens to corrupt
v3.2.0,2. sample positions to corrput
v3.2.0,3. Drop token on chosen position
v3.2.0,1. sample number of tokens to corrupt
v3.2.0,2. sample positions to corrput
v3.2.0,3. mask word on chosen position
v3.2.0,"Sharing options among `TokenizerTransform`s, same name conflict in"
v3.2.0,this scope will be resolved by remove previous occurrence in parser
v3.2.0,subword regularization(or BPE dropout) options:
v3.2.0,subword vocabulary restriction options:
v3.2.0,derterministic subwording
v3.2.0,subword sampling when nbest_size > 1 or -1
v3.2.0,alpha should be 0.0 < alpha < 1.0
v3.2.0,Load vocabulary file if provided and set threshold
v3.2.0,Load Subword Model
v3.2.0,-1: keep everything (i.e. 1 mask per token)
v3.2.0,0: replace everything (i.e. no mask)
v3.2.0,1: 1 mask per span
v3.2.0,view each subword as word start / input is word level token
v3.2.0,Pretend it ends with a full stop so last span is a sentence
v3.2.0,"Tokens that are full stops, where the previous token is not"
v3.2.0,Make sure we have enough to mask
v3.2.0,Trim to masking budget
v3.2.0,Handle 0-length mask (inserts) separately
v3.2.0,assert is_word_start[-1] == 0
v3.2.0,assert tokens_length - 1 not in indices
v3.2.0,"keep index, but replace it with [MASK]"
v3.2.0,"acts as a long length, so spans don't go over the end of doc"
v3.2.0,next position from each word_start
v3.2.0,delete token: 1 mask/remove per span
v3.2.0,"keep index, but replace it with [MASK]: 1 mask per token"
v3.2.0,A bit faster when all lengths are 1
v3.2.0,to cover whole token
v3.2.0,delete token
v3.2.0,"keep index, but replace it with [MASK]"
v3.2.0,assert tokens_length - 1 not in indices
v3.2.0,prefix src/tgt for each dataset
v3.2.0,prefix as general option for inference
v3.2.0,suffix src/tgt for each dataset
v3.2.0,suffix as general option for inference
v3.2.0,!/usr/bin/env python3
v3.2.0,-*- coding: utf-8 -*-
v3.2.0,Most code taken from: https://github.com/alvations/sacremoses
v3.2.0,Which in turn is based on the Moses punctuation normalizer.
v3.2.0,https://github.com/moses-smt/mosesdecoder/blob/master/scripts/
v3.2.0,tokenizer/normalize-punctuation.perl
v3.2.0,don't fix period at end of sentence
v3.2.0,Regex substitutions from replace-unicode-punctuation.perl
v3.2.0,https://github.com/moses-smt/mosesdecoder/blob/master/
v3.2.0,scripts/tokenizer/replace-unicode-punctuation.perl
v3.2.0,Adds the penn substitutions after extra_whitespace regexes.
v3.2.0,"Optionally, replace unicode puncts BEFORE normalization."
v3.2.0,Actual normalization.
v3.2.0,"Optionally, replace unicode puncts BEFORE normalization."
v3.2.0,normalize dict src/tgt for each dataset
v3.2.0,One source feature expected but none given and no default provided
v3.2.0,Provided default does not match required features
v3.2.0,Data not properly annotated.
v3.2.0,In this case we do not use the default as it might be an error
v3.2.0,batch 0 will always predict EOS. The other batches will predict
v3.2.0,non-eos scores.
v3.2.0,"""best"" prediction is eos - that should be blocked"
v3.2.0,include at least one prediction OTHER than EOS
v3.2.0,that is greater than -1e20
v3.2.0,now batch 0 has ended and no others have
v3.2.0,initial step
v3.2.0,batch 0 dies on step 0
v3.2.0,include at least one prediction OTHER than EOS
v3.2.0,that is greater than -1e20
v3.2.0,step 2
v3.2.0,(old) batch 8 dies on step 1
v3.2.0,step 3
v3.2.0,everything dies
v3.2.0,initial step
v3.2.0,batch 0 dies on step 0
v3.2.0,include at least one prediction OTHER than EOS
v3.2.0,that is greater than -1e20
v3.2.0,step 2
v3.2.0,(old) batch 8 dies on step 1
v3.2.0,step 3
v3.2.0,everything dies
v3.2.0,initial step
v3.2.0,finish one beam
v3.2.0,include at least one prediction OTHER than EOS
v3.2.0,that is greater than -1e20
v3.2.0,step 2
v3.2.0,finish example in last batch
v3.2.0,(old) batch 8 dies on step 1
v3.2.0,step 3
v3.2.0,everything dies
v3.2.0,initial step
v3.2.0,batch 0 dies on step 0
v3.2.0,include at least one prediction OTHER than EOS
v3.2.0,that is greater than -1e20
v3.2.0,step 2
v3.2.0,(old) batch 8 dies on step 1
v3.2.0,step 3
v3.2.0,everything dies
v3.2.0,illegal_weights_mask = torch.ByteTensor([
v3.2.0,"[0, 0, 0, 0, 0, 0, 0],"
v3.2.0,"[0, 0, 0, 1, 1, 1, 1],"
v3.2.0,"[0, 0, 0, 0, 0, 1, 1],"
v3.2.0,"[0, 0, 1, 1, 1, 1, 1]])"
v3.2.0,TODO: fix for pytorch 0.3
v3.2.0,illegal_weights = alignments.masked_select(illegal_weights_mask)
v3.2.0,"self.assertEqual(0.0, illegal_weights.data.sum())"
v3.2.0,this could be considered an integration test because it touches
v3.2.0,the filesystem for the config file (and the models)
v3.2.0,no dummy prefix
v3.2.0,no dummy prefix
v3.2.0,make sure the scalars are in the event accumulator tags
v3.2.0,required arguments
v3.2.0,transforms that require vocab will not create if not provide vocab
v3.2.0,1. Init first transform in the pipe
v3.2.0,2. Init second transform in the pipe
v3.2.0,3. Sequential combine them into a transform pipe
v3.2.0,4. apply transform pipe for example
v3.2.0,"5. example after the pipe exceed the length limit, thus filtered"
v3.2.0,6. Transform statistics registed (here for filtertoolong)
v3.2.0,"7. after report, statistics become empty as a fresh start"
v3.2.0,filter_transform.warm_up()
v3.2.0,test BPE-dropout:
v3.2.0,1. disable bpe dropout for not training example
v3.2.0,2. enable bpe dropout for training example
v3.2.0,3. (NOTE) disable dropout won't take effect if already seen
v3.2.0,this is caused by the cache mechanism in bpe:
v3.2.0,return cached subword if the original token is seen when no dropout
v3.2.0,test SP regularization:
v3.2.0,1. enable regularization for training example
v3.2.0,2. disable regularization for not training example
v3.2.0,Not apply token drop for not training example
v3.2.0,apply token drop for training example
v3.2.0,Not apply token mask for not training example
v3.2.0,apply token mask for training example
v3.2.0,require vocabs to warm_up
v3.2.0,Not apply token mask for not training example
v3.2.0,apply token mask for training example
v3.2.0,"Defalt: full_stop_token=[""."", ""?"", ""!""]"
v3.2.0,"Defalt: full_stop_token=[""."", ""?"", ""!""]"
v3.2.0,random_ratio of inserted tokens are chosen in vocab
v3.2.0,others are MASK_TOK
v3.2.0,"insert_ratio=0.0,"
v3.2.0,"random_ratio=0.0,"
v3.2.0,"Defalt: full_stop_token=[""."", ""?"", ""!""]"
v3.2.0,all token are considered as an individual word
v3.2.0,1. tokens are dropped when replace_length is 0
v3.2.0,"print(f""token delete: {masked} / {tokens}"")"
v3.2.0,2. tokens are replaced by MASK when replace_length is 1
v3.2.0,"print(f""token mask: {masked} / {tokens}"")"
v3.2.0,"insert_ratio=0.0,"
v3.2.0,"random_ratio=0.0,"
v3.2.0,"Defalt: full_stop_token=[""."", ""?"", ""!""]"
v3.2.0,start token of word are identified using subword marker
v3.2.0,"1. replace_length 0: ""words"" are dropped"
v3.2.0,"print(f""word delete: {masked} / {tokens}"")"
v3.2.0,"self.assertEqual(len(masked), n_words - n_masked)"
v3.2.0,"2. replace_length 1: ""words"" are replaced with a single MASK"
v3.2.0,"print(f""whole word single mask: {masked} / {tokens}"")"
v3.2.0,len(masked) depend on number of tokens in select word
v3.2.0,"3. replace_length -1: all tokens in ""words"" are replaced with MASK"
v3.2.0,"print(f""whole word multi mask: {masked} / {tokens}"")"
v3.2.0,number of mask_tok depend on number of tokens in selected word
v3.2.0,number of MASK_TOK can be greater than n_masked
v3.2.0,"insert_ratio=0.5,"
v3.2.0,"random_ratio=0.3,"
v3.2.0,"Defalt: full_stop_token=[""."", ""?"", ""!""]"
v3.2.0,start token of word are identified using subword marker
v3.2.0,n_words = sum(token_starts)
v3.2.0,n_masked = math.ceil(n_words * bart_noise.mask_ratio)
v3.2.0,"print(f""Text Span Infilling: {infillied} / {tokens}"")"
v3.2.0,"print(n_words, n_masked)"
v3.2.0,!/usr/bin/env python
v3.2.0,-*- coding: utf-8 -*-
v3.2.0,Inject some dummy training options that may needed when build fields
v3.2.0,Remove the generated *pt files.
v3.2.0,Remove the generated data samples
v3.2.0,all beams repeat (beam >= 1 repeat dummy scores)
v3.2.0,predict repeat_idx over and over again
v3.2.0,"before repeat, scores are either 0 or -inf"
v3.2.0,"on repeat, `repeat_idx` score is BLOCKED_SCORE"
v3.2.0,"(but it's still the best score, thus we have"
v3.2.0,"[BLOCKED_SCORE, -inf, -inf, -inf, -inf]"
v3.2.0,repetitions keeps maximizing score
v3.2.0,"index 0 has been blocked, so repeating=>+0.0 score"
v3.2.0,other indexes are -inf so repeating=>BLOCKED_SCORE
v3.2.0,which is higher
v3.2.0,beam 0 and beam >=2 will repeat (beam >= 2 repeat dummy scores)
v3.2.0,non-interesting beams are going to get dummy values
v3.2.0,"on initial round, only predicted scores for beam 0"
v3.2.0,matter. Make two predictions. Top one will be repeated
v3.2.0,"in beam zero, second one will live on in beam 1."
v3.2.0,predict the same thing in beam 0
v3.2.0,continue pushing around what beam 1 predicts
v3.2.0,"now beam 0 dies (along with the others), beam 1 -> beam 0"
v3.2.0,"now beam 0 dies (along with the others), beam 1 -> beam 0"
v3.2.0,beam 0 and beam >= 2 will repeat (beam 2 repeats excluded idx)
v3.2.0,non-interesting beams are going to get dummy values
v3.2.0,predict the same thing in beam 0
v3.2.0,continue pushing around what beam 1 predicts
v3.2.0,predict the allowed-repeat again in beam 2
v3.2.0,"now beam 0 dies, beam 1 -> beam 0, beam 2 -> beam 1"
v3.2.0,and the rest die
v3.2.0,"since all preds after i=0 are 0, we can check"
v3.2.0,that the beam is the correct idx by checking that
v3.2.0,the curr score is the initial score
v3.2.0,beam 0 will always predict EOS. The other beams will predict
v3.2.0,non-eos scores.
v3.2.0,non-interesting beams are going to get dummy values
v3.2.0,"""best"" prediction is eos - that should be blocked"
v3.2.0,include at least beam_sz predictions OTHER than EOS
v3.2.0,that are greater than -1e20
v3.2.0,predict eos in beam 0
v3.2.0,provide beam_sz other good predictions
v3.2.0,now the top beam has ended and no others have
v3.2.0,"not of interest, but want to make sure it keeps running"
v3.2.0,since only beam 0 terminates and n_best = 2
v3.2.0,"this is also a test that when block_ngram_repeat=0,"
v3.2.0,repeating is acceptable
v3.2.0,non-interesting beams are going to get dummy values
v3.2.0,"""best"" prediction is eos - that should be blocked"
v3.2.0,include at least beam_sz predictions OTHER than EOS
v3.2.0,that are greater than -1e20
v3.2.0,predict eos in beam 1
v3.2.0,provide beam_sz other good predictions in other beams
v3.2.0,beam 1 dies on min_length
v3.2.0,beam 0 dies on the step after beam 1 dies
v3.2.0,"inp_lens is tiled in initialize, reassign to make attn match"
v3.2.0,non-interesting beams are going to get dummy values
v3.2.0,"""best"" prediction is eos - that should be blocked"
v3.2.0,include at least beam_sz predictions OTHER than EOS
v3.2.0,that are greater than -1e20
v3.2.0,predict eos in beam 1
v3.2.0,provide beam_sz other good predictions in other beams
v3.2.0,no top beams are finished yet
v3.2.0,beam 1 dies on min_length
v3.2.0,no top beams are finished yet
v3.2.0,beam 0 dies on the step after beam 1 dies
v3.2.0,top beam is finished now so there are attentions
v3.2.0,two beams are finished in each batch
v3.2.0,second dim is cut down to the non-padded src length
v3.2.0,first dim is equal to the time of death
v3.2.0,(beam 0 died at current step - adjust for SOS)
v3.2.0,(beam 1 died at last step - adjust for SOS)
v3.2.0,behavior gets weird when beam is already done so just stop
v3.2.0,this is just test_beam.TestBeamAgainstReferenceCase repeated
v3.2.0,in each batch.
v3.2.0,"init_preds: [4, 3, 5, 6, 7] - no EOS's"
v3.2.0,no EOS's yet
v3.2.0,"[5, 3, 2, 6, 0], so beam 2 predicts EOS!"
v3.2.0,assumes beam 2 finished on last step
v3.2.0,ended beam 2 shouldn't continue
v3.2.0,"[2, 5, 3, 6, 0] repeat self.BATCH_SZ, so beam 0 predicts EOS!"
v3.2.0,"[-2.4879, -3.8910, -4.1010, -4.2010, -4.4010] repeat self.BATCH_SZ"
v3.2.0,another beam is finished in all batches
v3.2.0,new beam 0 finished
v3.2.0,new beam 0 is old beam 3
v3.2.0,assumes beam 0 finished on last step
v3.2.0,"[5, 2, 6, 1, 0] repeat self.BATCH_SZ, so beam 1 predicts EOS!"
v3.2.0,we finish 3 hyps per example in this step
v3.2.0,new beam 1 is old beam 3
v3.2.0,this could be considered an integration test because it tests
v3.2.0,interactions between the GNMT scorer and the beam
v3.2.0,"-data option is required, but not used in this test, so dummy."
v3.2.0,len x batch x nfeat
v3.2.0,Initialize vectors to compare size with
v3.2.0,Ensure correct sizes and types
v3.2.0,Make sure that output has the correct size and type
v3.2.0,"[('encoder_type', 'transformer'),"
v3.2.0,"('word_vec_size', 16), ('hidden_size', 16)],"
v3.2.0,""""""" Only do SRU test if requirment is safisfied. """""""
v3.2.0,SRU doesn't support input_feed.
v3.2.0,first check there's nothing unexpectedly not trainable
v3.2.0,ok: word embeddings shouldn't be trainable
v3.2.0,if word vecs are freezed
v3.2.0,ok: positional encodings shouldn't be trainable
v3.2.0,then check nothing unexpectedly trainable
v3.2.0,Decoder state
v3.2.0,Build the RNN.
v3.2.0,Set up the context gate.
v3.2.0,Set up the standard attention.
v3.2.0,The encoder hidden is  (layers*directions) x batch x dim.
v3.2.0,We need to convert it to layers x batch x (directions*dim).
v3.2.0,Init the input feed.
v3.2.0,Update the state with the result.
v3.2.0,Concatenates sequence of tensors along a new dimension.
v3.2.0,NOTE: v0.3 to 0.4: dec_outs / attns[*] may not be list
v3.2.0,(in particular in case of SRU) it was not raising error in 0.3
v3.2.0,since stack(Variable) was allowed.
v3.2.0,"In 0.4, SRU returns a tensor that shouldn't be stacke"
v3.2.0,Calculate the attention.
v3.2.0,Calculate the context gate.
v3.2.0,Additional args check.
v3.2.0,Input feed concatenates hidden state with
v3.2.0,input at every time step.
v3.2.0,TODO: context gate should be employed
v3.2.0,instead of second RNN transform.
v3.2.0,Update the coverage attention.
v3.2.0,"attns[""coverage""] is actually c^(t+1) of See et al(2017)"
v3.2.0,1-index shifted
v3.2.0,Decoder State
v3.2.0,CNNDecoder has its own attention mechanism.
v3.2.0,Set up a separate copy attention layer if needed.
v3.2.0,The output of CNNEncoder.
v3.2.0,The combination of output of CNNEncoder and source embeddings.
v3.2.0,Process the result and update the attentions.
v3.2.0,Update the state.
v3.2.0,TODO change the way attns is returned dict => list or tuple (onnx)
v3.2.0,src_len is a single tensor shared between all models.
v3.2.0,This assumption will not hold if Translator is modified
v3.2.0,to calculate src_len as something other than the length
v3.2.0,of the input.
v3.2.0,"return _, (B, Q_len, K_len)"
v3.2.0,"layer average attention across heads, get ``(B, Q, K)``"
v3.2.0,"Case 1: no full_context, no align heads -> layer avg baseline"
v3.2.0,"Case 2: no full_context, 1 align heads -> guided align"
v3.2.0,"Case 3: full_context, 1 align heads -> full cte guided align"
v3.2.0,BoolTensor was introduced in pytorch 1.2
v3.2.0,T: could be 1 in the case of stepwise decoding or tgt_len
v3.2.0,masking is necessary when sequence length is greater than one
v3.2.0,mask now are (batch x 1 x tlen x s or t len)
v3.2.0,1 = heads to be expanded in MHA
v3.2.0,"feed_forward applies residual, so we remove and apply residual with un-normed"
v3.2.0,Decoder State
v3.2.0,"previously, there was a GlobalAttention module here for copy"
v3.2.0,"attention. But it was never actually used -- the ""copy"" attention"
v3.2.0,just reuses the context attention.
v3.2.0,"attns[""align""] = torch.stack(attn_aligns, 0).mean(0)  # All avg"
v3.2.0,TODO change the way attns is returned dict => list or tuple (onnx)
v3.2.0,first value set to True triggered by the beginning of decoding
v3.2.0,layer_cache becomes active in the MultiHeadedAttention fwd
v3.2.0,T: could be 1 in the case of stepwise decoding or tgt_len
v3.2.0,masking is necessary when sequence length is greater than one
v3.2.0,mask now are (batch x 1 x tlen x tlen)
v3.2.0,1 = heads to be expanded in MHA
v3.2.0,"feed_forward applies residual, so we remove and apply residual with un-normed"
v3.2.0,TODO change the way attns is returned dict => list or tuple (onnx)
v3.2.0,"buffer size in bytes, determine equiv. # of elements based on data type"
v3.2.0,copy tensors into buffer_t
v3.2.0,all-reduce and rescale
v3.2.0,copy all-reduced buffer back into tensors
v3.2.0,"print(filled, sz)"
v3.2.0,"tensor is bigger than buffer, all-reduce and rescale directly"
v3.2.0,"buffer is full, all-reduce and replace buffer with grad"
v3.2.0,add tensor to buffer
v3.2.0,"propagate exception to parent process, keeping original traceback"
v3.2.0,TODO: Find a better way to check for sparse gradients.
v3.2.0,we use apex.amp
v3.2.0,In this case use the old FusedAdam with
v3.2.0,FP16_optimizer wrapper
v3.2.0,Load everything from the checkpoint.
v3.2.0,Build everything from scratch.
v3.2.0,"Reset optimizer, keep options."
v3.2.0,"Reset options, keep optimizer."
v3.2.0,State can be partially restored.
v3.2.0,should be: self._optimizer.zero_grad(set_to_none)
v3.2.0,but apex.amp is not up-to-date:
v3.2.0,https://github.com/NVIDIA/apex/blob/master/apex/amp/_process_optimizer.py#L367
v3.2.0,"unscaled optimizer's gradients (already done therefore skip),"
v3.2.0,skips optimizer.step() if gradients contain infs/NaNs.
v3.2.0,Updates the scale for next iteration.
v3.2.0,Code below is an implementation of https://arxiv.org/pdf/1804.04235.pdf
v3.2.0,inspired but modified from https://github.com/DeadAt0m/adafactor-pytorch
v3.2.0,backward compatibility
v3.2.0,assuming a list/generator of parameter means single group
v3.2.0,compute combined scale factor for this group
v3.2.0,norm is in fact norm*scale
v3.2.0,note: p.grad should not ever be set for correct operation of
v3.2.0,mixed precision optimizer that sometimes sends None gradients
v3.2.0,State initialization
v3.2.0,Exponential moving average of gradient values
v3.2.0,Exponential moving average of squared gradient values
v3.2.0,-*- coding: utf-8 -*-
v3.2.0,placing this here make it easier to call logger.info
v3.2.0,"from anywhere, just 'from onmt.utils.logging import logger'"
v3.2.0,"align_head contains value in [0, 1) presenting attn prob,"
v3.2.0,0 was resulted by the context attention src_pad_mask
v3.2.0,"So, the correspand position in ref_align should also be 0"
v3.2.0,"Therefore, clip align_head to > 1e-18 should be bias free."
v3.2.0,rescale with tau (temperature) and apply the log_softmax.
v3.2.0,ct2 expects src with lengths without padding
v3.2.0,again we use raw probs to rescale with tau and apply log_softmax
v3.2.0,lm_scores are in log space so log_target=True
v3.2.0,rescale with tau (temperature) and apply the log_softmax.
v3.2.0,ct2 expects src with lengths without padding
v3.2.0,again we use raw probs to rescale with tau and apply log_softmax
v3.2.0,lm_scores are in log space so log_target=True
v3.2.0,take into account here the tgt_shift_index (0 / 1 = LM/NMT)
v3.2.0,Correct target copy token instead of <unk>
v3.2.0,tgt[i] = align[i] + len(tgt_vocab)
v3.2.0,for i such that tgt[i] == 0 and align[i] != 0
v3.2.0,in the case criterion reduction is None then we need
v3.2.0,to sum the loss of each sentence in the batch
v3.2.0,Check Transforms
v3.2.0,Check path
v3.2.0,tgt is src for LM task
v3.2.0,Check weight
v3.2.0,Check features
v3.2.0,validation when train:
v3.2.0,Check embeddings stuff
v3.2.0,"Backward compatibility with ""fix_word_vecs_*"" opts"
v3.2.0,encoder and decoder should be same sizes
v3.2.0,"Load default opt values, then overwrite with the opts in"
v3.2.0,"the checkpoint. That way, if there are new options added,"
v3.2.0,the defaults are used.
v3.2.0,It comes from training
v3.2.0,TODO: needs to be added as inference opt
v3.2.0,Don't do anything
v3.2.0,Update best score of each criteria
v3.2.0,Reset tolerance
v3.2.0,Update current status
v3.2.0,Decrease tolerance
v3.2.0,Log
v3.2.0,Log
v3.2.0,Get a list of world_size lists with len(stat_list) Statistics objects
v3.2.0,"this param init is overridden by model_builder, useless then."
v3.2.0,SRU doesn't support PackedSequence.
v3.2.0,-*- coding: utf-8 -*-
v3.2.0,threshold on 1 to avoid div by 0
v3.2.0,treat alignment matrix one by one as each have different lengths
v3.2.0,No alignment if not exist valid tgt token
v3.2.0,get valid alignment (sub-matrix from full paded aligment matrix)
v3.2.0,Helper functions
v3.2.0,Keeps track of the original words/subwords
v3.2.0,('prior_tokenization' option)
v3.2.0,In case there is a final case_markup when new_spacer is on
v3.2.0,translate
v3.2.0,for validation we build an infer_iter per batch
v3.2.0,in order to avoid oom issues because there is no
v3.2.0,batching strategy in `textbatch_to_tensor`
v3.2.0,apply_reverse refs
v3.2.0,flatten preds
v3.2.0,save results
v3.2.0,-*- coding: utf-8 -*-
v3.2.0,this one is needed for Random Shuffler of batches
v3.2.0,in multi gpu it ensures datasets are read in the same order
v3.2.0,some cudnn methods can be random even after fixing the seed
v3.2.0,unless you tell it to be deterministic
v3.2.0,This one is needed for various tranfroms
v3.2.0,These ensure same initialization in multi gpu mode
v3.2.0,we need to check the model path + any tokenizer path
v3.2.0,patch to log stdout spawned processes of dataloader
v3.2.0,bucket_size = batch_size
v3.2.0,For TRAIN we need to group examples by length
v3.2.0,"for faster performance, but otherwise, sequential."
v3.2.0,For TRAIN we shuffle batches within the bucket
v3.2.0,otherwise sequential
v3.2.0,for specific case of rnn_packed need to be sorted
v3.2.0,within the batch
v3.2.0,Check if all tokens have features or none at all
v3.2.0,Make features part of src like
v3.2.0,"{'src': {'src': ..., 'feats': [...., ....]}}"
v3.2.0,at this point an example looks like:
v3.2.0,"{'src': {'src': ..., 'feats': [....]},"
v3.2.0,"'tgt': {'tgt': ...},"
v3.2.0,"'src_original': ['tok1', ...'tokn'],"
v3.2.0,"'tgt_original': ['tok1', ...'tokm'],"
v3.2.0,'indices' : seq in bucket
v3.2.0,"'align': ...,"
v3.2.0,}
v3.2.0,Need to add features in last dimensions
v3.2.0,Keep it consistent with dynamic data
v3.2.0,make a small vocab containing just the tokens in the source sequence
v3.2.0,Map source tokens to indices in the dynamic dict.
v3.2.0,-*- coding: utf-8 -*-
v3.2.0,this is hack: if the special separator ï½Ÿnewlineï½ is returned because of the
v3.2.0,"""docify"" transform.get_specials we don't add it if the corresponding newline code"
v3.2.0,is already included in the sentencepiece or BPE-with-gpt2-pretok.
v3.2.0,'src_original' and 'tgt_original' store the
v3.2.0,original line before tokenization. These
v3.2.0,fields are used later on in the feature
v3.2.0,transforms.
v3.2.0,NOTE: moved to dynamic_iterator.py cf process()
v3.2.0,item = self.transform.apply(
v3.2.0,"example, is_train=self.infinitely, corpus_name=self.cid)"
v3.2.0,empty example: skip
v3.2.0,bitsandbytes quantize weights when .cuda() is called
v3.2.0,for huge models we need to save Ram
v3.2.0,so we load the weights  module by module and transfer them to GPU for quantization
v3.2.0,"No encoder in LM, seq2seq count formatting kept"
v3.2.0,_check_save_model_path
v3.2.0,This preserves backward-compat for models using customed layernorm
v3.2.0,Force add_ffnbias to True if bias found in model w_1 keys
v3.2.0,fix v2 compatibility
v3.2.0,end of patch for backward compatibility
v3.2.0,!/usr/bin/env python
v3.2.0,!/usr/bin/env python
v3.2.0,!/usr/bin/env python
v3.2.0,-*- coding: utf-8 -*-
v3.2.0,!/usr/bin/env python
v3.2.0,BPE training
v3.2.0,SentencePiece training
v3.2.0,!/usr/bin/env python
v3.2.0,!/usr/bin/env python
v3.2.0,Set sharing strategy manually instead of default based on the OS.
v3.2.0,torch.multiprocessing.set_sharing_strategy('file_system')
v3.2.0,Create a thread to listen for errors in the child processes.
v3.2.0,Train with multiprocessing.
v3.2.0,magic indices
v3.2.0,result caching
v3.2.0,Here we set the decoder to start with self.start (BOS or EOS)
v3.2.0,fix length constraint and remove eos from count
v3.2.0,add one to account for BOS. Don't account for EOS because hitting
v3.2.0,this implies it hasn't been found.
v3.2.0,we don't block nothing if the user doesn't want it
v3.2.0,we can't block nothing beam's too short
v3.2.0,we check paths one by one
v3.2.0,we don't forbid nothing if the user doesn't want it
v3.2.0,we can't forbid nothing if beam's too short
v3.2.0,Reordering forbidden_tokens following beam selection
v3.2.0,We rebuild a dict to ensure we get the value and not the pointer
v3.2.0,Grabing the newly selected tokens and associated ngram
v3.2.0,skip the blocking if any token in current_ngram is excluded
v3.2.0,"pickups: Tensor where specified index were set to 1, others 0"
v3.2.0,"dropdowns: opposite of pickups, 1 for those shouldn't pick"
v3.2.0,Minus dropdowns to log_probs making probabilities of
v3.2.0,unspecified index close to 0
v3.2.0,"prediction step have surpass length of given target_prefix,"
v3.2.0,no need to further change this attr
v3.2.0,keep indices until overflowing p
v3.2.0,Set all logits that are not in the top-p to -10000.
v3.2.0,This puts the probabilities close to 0.
v3.2.0,Set all logits that are not in the top-k to -10000.
v3.2.0,This puts the probabilities close to 0.
v3.2.0,"For temp=0.0, take the argmax to avoid divide-by-zero errors."
v3.2.0,keep_topk=1 is also equivalent to argmax.
v3.2.0,maybe fix some prediction at this step by modifying log_probs
v3.2.0,"shape: (sum(~ self.is_finished), 1)"
v3.2.0,in LM task src_len is associated with currently generated src
v3.2.0,and therefore needs to follow the generation
v3.2.0,!/usr/bin/env python
v3.2.0,for debugging
v3.2.0,TODO: maybe add dynamic part
v3.2.0,Statistics
v3.2.0,those two should be the same except feat dim
v3.2.0,"batch['src'][perm[j], :, :])"
v3.2.0,trans.src
v3.2.0,we rebuild a small batch made of the sub-segments
v3.2.0,in the long segment.
v3.2.0,new sub-batch ready to be translated
v3.2.0,we re-insert the sub-batch in the initial translations
v3.2.0,In the case of length_penalty = none we report the total logprobs
v3.2.0,divided by the number of sentence to get an approximation of the
v3.2.0,per sentence logprob. We also return the corresponding ppl
v3.2.0,"When a length_penalty is used eg: ""avg"" or ""wu"" since logprobs"
v3.2.0,are normalized per token we report the per line per token logprob
v3.2.0,"and the corresponding ""per word perplexity"""
v3.2.0,Turn any copied words into UNKs.
v3.2.0,"Decoder forward, takes [batch, tgt_len, nfeats] as input"
v3.2.0,"and [batch, src_len, hidden] as enc_out"
v3.2.0,"in case of inference tgt_len = 1, batch = beam times batch_size"
v3.2.0,"in case of Gold Scoring tgt_len = actual length, batch = 1 batch"
v3.2.0,Generator forward.
v3.2.0,"returns [(batch_size x beam_size) , vocab ] when 1 step"
v3.2.0,"or [batch_size, tgt_len, vocab ] when full sentence"
v3.2.0,"here we have scores [tgt_lenxbatch, vocab] or [beamxbatch, vocab]"
v3.2.0,at this point scores is batch first (dim=0)
v3.2.0,"returns [(batch_size x beam_size) , vocab ] when 1 step"
v3.2.0,"or [batch_size, tgt_len, vocab ] when full sentence"
v3.2.0,(0) add BOS and padding to tgt prediction
v3.2.0,(1) Encoder forward.
v3.2.0,(2) Repeat src objects `n_best` times.
v3.2.0,"We use batch_size x n_best, get ``(batch * n_best, src_len, nfeat)``"
v3.2.0,Quick fix. Transformers return None as enc_states.
v3.2.0,enc_states are only used later on to init decoder's state
v3.2.0,"but are never used in Transformer decoder, so we can skip"
v3.2.0,"(3) Init decoder with n_best src,"
v3.2.0,"reshape tgt to ``(len, batch * n_best, nfeat)``"
v3.2.0,it should be done in a better way
v3.2.0,here dec_in is batch first
v3.2.0,masked_select
v3.2.0,get aligned src id for each prediction's valid tgt tokens
v3.2.0,TODO: support these blacklisted features
v3.2.0,(0) Prep the components of the search.
v3.2.0,(1) Run the encoder on the src.
v3.2.0,(2) prep decode_strategy. Possibly repeat src objects.
v3.2.0,(3) Begin decoding step by step:
v3.2.0,"decoder_input = decode_strategy.current_predictions.view(1, -1,"
v3.2.0,1)
v3.2.0,Reorder states.
v3.2.0,TODO: support these blacklisted features
v3.2.0,(0) Prep the components of the search.
v3.2.0,(1) split src into src and target_prefix to avoid padding.
v3.2.0,(2) init decoder
v3.2.0,(3) prep decode_strategy. Possibly repeat src objects.
v3.2.0,(4) Begin decoding step by step:
v3.2.0,Reorder states.
v3.2.0,select indexes in model state/cache
v3.2.0,beam parameters
v3.2.0,beam state
v3.2.0,BoolTensor was introduced in pytorch 1.2
v3.2.0,"""global state"" of the old beam"
v3.2.0,buffers for the topk scores and 'backpointer'
v3.2.0,for testing
v3.2.0,maybe fix some prediction at this step by modifying log_probs
v3.2.0,Flatten probs into a list of possibilities.
v3.2.0,Penalize beams that finished.
v3.2.0,"on real data (newstest2017) with the pretrained transformer,"
v3.2.0,it's faster to not move this back to the original device
v3.2.0,Store finished hypotheses for this batch.
v3.2.0,End condition is the top beam finished and we can return
v3.2.0,n_best hypotheses.
v3.2.0,"If all sentences are translated, no need to go further."
v3.2.0,Remove finished batches for the next step.
v3.2.0,using integer division to get an integer _B without casting
v3.2.0,force the output to be longer than self.min_length
v3.2.0,Multiply probs by the beam probability.
v3.2.0,"if the sequence ends now, then the penalty is the current"
v3.2.0,"length + 1, to include the EOS token"
v3.2.0,Avoid any direction that would repeat unwanted ngrams
v3.2.0,Pick up candidate token by curr_scores
v3.2.0,Recover log probs.
v3.2.0,Length penalty is just a scalar. It doesn't matter if it's applied
v3.2.0,before or after the topk.
v3.2.0,Resolve beam origin and map to batch index flat representation.
v3.2.0,Append last prediction.
v3.2.0,update global state (step == 1)
v3.2.0,update global state (step > 1)
v3.2.0,"shape: (batch_size x beam_size, 1)"
v3.2.0,in LM task src_len is associated with currently generated src
v3.2.0,and therefore needs to follow the generation
v3.2.0,in LM task src_len is associated with currently generated src
v3.2.0,and therefore needs to follow the generation
v3.2.0,Term will be subtracted from probability
v3.2.0,Probability will be divided by this
v3.2.0,these warnings indicate that either the alpha/beta
v3.2.0,"forces a penalty to be a no-op, or a penalty is a no-op but"
v3.2.0,the alpha/beta would suggest otherwise.
v3.2.0,using some coverage penalty
v3.2.0,!/usr/bin/env python
v3.2.0,semaphore doesn't have a timeout arg in Python 2.7
v3.2.0,perform a first request to initialize everything
v3.2.0,backwards compatibility for confs
v3.2.0,every segment becomes a dict for flexibility purposes
v3.2.0,NOTE: translator returns lists of `n_best` list
v3.2.0,build back results with empty texts
v3.2.0,load can be called multiple times: modify copy
v3.2.0,output contain alignment
v3.2.0,Below are all the different penalty terms implemented so far.
v3.2.0,Subtract coverage penalty from topk log probs.
v3.2.0,Divide topk log probs by length penalty.
v3.2.0,Sorting
v3.2.0,Chinese segmentation
v3.2.0,Chinese simplify -> Chinese traditional standard
v3.2.0,Chinese simplify -> Chinese traditional (HongKong)
v3.2.0,Chinese simplify -> Chinese traditional (Taiwan)
v3.2.0,Chinese traditional -> Chinese simplify (v1)
v3.2.0,Chinese traditional -> Chinese simplify (v2)
v3.2.0,Auto import python files in this directory
v3.1.3,!/usr/bin/env python
v3.1.3,!/usr/bin/env python
v3.1.3,!/usr/bin/env python
v3.1.3,!/usr/bin/env python
v3.1.3,!/usr/bin/env python
v3.1.3,!/usr/bin/env python3
v3.1.3,-*- coding: utf-8 -*-
v3.1.3,
v3.1.3,"OpenNMT-py documentation build configuration file, created by"
v3.1.3,sphinx-quickstart on Sun Dec 17 12:07:14 2017.
v3.1.3,
v3.1.3,This file is execfile()d with the current directory set to its
v3.1.3,containing dir.
v3.1.3,
v3.1.3,Note that not all possible configuration values are present in this
v3.1.3,autogenerated file.
v3.1.3,
v3.1.3,All configuration values have a default; values that are commented out
v3.1.3,serve to show the default.
v3.1.3,"If extensions (or modules to document with autodoc) are in another directory,"
v3.1.3,add these directories to sys.path here. If the directory is relative to the
v3.1.3,"documentation root, use os.path.abspath to make it absolute, like shown here."
v3.1.3,
v3.1.3,import os
v3.1.3,import sys
v3.1.3,"sys.path.insert(0, os.path.abspath('.'))"
v3.1.3,-- General configuration ------------------------------------------------
v3.1.3,"If your documentation needs a minimal Sphinx version, state it here."
v3.1.3,
v3.1.3,needs_sphinx = '6.0'
v3.1.3,"Add any Sphinx extension module names here, as strings. They can be"
v3.1.3,extensions coming with Sphinx (named 'sphinx.ext.*') or your custom
v3.1.3,ones.
v3.1.3,Show base classes
v3.1.3,"Use ""variables"" section for Attributes instead of weird block things"
v3.1.3,mimicking the function style.
v3.1.3,"Add any paths that contain templates here, relative to this directory."
v3.1.3,The suffix(es) of source filenames.
v3.1.3,You can specify multiple suffix as a list of string:
v3.1.3,
v3.1.3,"source_suffix = ['.rst', '.md']"
v3.1.3,The master toctree document.
v3.1.3,General information about the project.
v3.1.3,"The version info for the project you're documenting, acts as replacement for"
v3.1.3,"|version| and |release|, also used in various other places throughout the"
v3.1.3,built documents.
v3.1.3,
v3.1.3,The short X.Y version.
v3.1.3,"The full version, including alpha/beta/rc tags."
v3.1.3,The language for content autogenerated by Sphinx. Refer to documentation
v3.1.3,for a list of supported languages.
v3.1.3,
v3.1.3,This is also used if you do content translation via gettext catalogs.
v3.1.3,"Usually you set ""language"" from the command line for these cases."
v3.1.3,"List of patterns, relative to source directory, that match files and"
v3.1.3,directories to ignore when looking for source files.
v3.1.3,This patterns also effect to html_static_path and html_extra_path
v3.1.3,The name of the Pygments (syntax highlighting) style to use.
v3.1.3,"If true, `todo` and `todoList` produce output, else they produce nothing."
v3.1.3,-- Options for HTML output ----------------------------------------------
v3.1.3,The theme to use for HTML and HTML Help pages.  See the documentation for
v3.1.3,a list of builtin themes.
v3.1.3,
v3.1.3,html_theme = 'sphinx_materialdesign_theme'
v3.1.3,html_theme_path = [sphinx_materialdesign_theme.get_path()]
v3.1.3,Theme options are theme-specific and customize the look and feel of a theme
v3.1.3,"further.  For a list of options available for each theme, see the"
v3.1.3,documentation.
v3.1.3,
v3.1.3,html_theme_options = {}
v3.1.3,"Add any paths that contain custom static files (such as style sheets) here,"
v3.1.3,"relative to this directory. They are copied after the builtin static files,"
v3.1.3,"so a file named ""default.css"" will overwrite the builtin ""default.css""."
v3.1.3,"Custom sidebar templates, must be a dictionary that maps document names"
v3.1.3,to template names.
v3.1.3,
v3.1.3,This is required for the alabaster theme
v3.1.3,refs: http://alabaster.readthedocs.io/en/latest/installation.html#sidebars
v3.1.3,-- Options for HTMLHelp output ------------------------------------------
v3.1.3,Output file base name for HTML help builder.
v3.1.3,-- Options for LaTeX output ---------------------------------------------
v3.1.3,The paper size ('letterpaper' or 'a4paper').
v3.1.3,
v3.1.3,"'papersize': 'letterpaper',"
v3.1.3,"The font size ('10pt', '11pt' or '12pt')."
v3.1.3,
v3.1.3,"'pointsize': '10pt',"
v3.1.3,Additional stuff for the LaTeX preamble.
v3.1.3,
v3.1.3,"'preamble': '',"
v3.1.3,Latex figure (float) alignment
v3.1.3,
v3.1.3,"'figure_align': 'htbp',"
v3.1.3,Grouping the document tree into LaTeX files. List of tuples
v3.1.3,"(source start file, target name, title,"
v3.1.3,"author, documentclass [howto, manual, or own class])."
v3.1.3,-- Options for manual page output ---------------------------------------
v3.1.3,One entry per manual page. List of tuples
v3.1.3,"(source start file, name, description, authors, manual section)."
v3.1.3,-- Options for Texinfo output -------------------------------------------
v3.1.3,Grouping the document tree into Texinfo files. List of tuples
v3.1.3,"(source start file, target name, title, author,"
v3.1.3,"dir menu entry, description, category)"
v3.1.3,What are the 3 best french cities ?
v3.1.3,Which one is better if I like outdoor activities ?
v3.1.3,Which one is better if I like cultural outings?
v3.1.3,What are the best neighborhoods in these 5 cities?
v3.1.3,!/usr/bin/env python3
v3.1.3,Usage: python3 filter_train.py in.src in.trg out.src out.trg max-tokens
v3.1.3,!/usr/bin/env python
v3.1.3,-*- coding: utf-8 -*-
v3.1.3,is this reachable?
v3.1.3,Read in embeddings
v3.1.3,Write to file
v3.1.3,converts a SentencePiece vocabulary to the format expected by dynamic data
v3.1.3,"(essentially converts float expected counts to ""fixed precision"" int pseudo"
v3.1.3,counts)
v3.1.3,from onmt.utils.misc import use_gpu
v3.1.3,"Add in default model arguments, possibly added since training."
v3.1.3,this patch is no longer needed included in converter
v3.1.3,"if hasattr(model_opt, 'rnn_size'):"
v3.1.3,model_opt.hidden_size = model_opt.rnn_size
v3.1.3,build_base_model expects updated and validated opts
v3.1.3,-*- encoding: utf-8 -*-
v3.1.3,!/usr/bin/env python
v3.1.3,-*- coding: utf-8 -*-
v3.1.3,Author: Rico Sennrich
v3.1.3,flake8: noqa
v3.1.3,This file is retrieved from https://github.com/rsennrich/subword-nmt
v3.1.3,hack for python2/3 compatibility
v3.1.3,check version information
v3.1.3,some hacking to deal with duplicates (only consider first instance)
v3.1.3,don't print end-of-word symbols
v3.1.3,sys.stderr.write('cannot split {0} further.\n'.format(segment))
v3.1.3,sys.stderr.write('OOV: {0}\n'.format(segment))
v3.1.3,sys.stderr.write('OOV: {0}\n'.format(segment))
v3.1.3,python 2/3 compatibility
v3.1.3,read/write files as UTF-8
v3.1.3,!/usr/bin/env python3
v3.1.3,coding: utf-8
v3.1.3,"In order to use this tool, please install comet first"
v3.1.3,https://github.com/Unbabel/COMET
v3.1.3,Let's say you have a source file with N sentences in SL - eg: source.sl
v3.1.3,and the corresponding references (N sentences) reference.tl
v3.1.3,Translate your file in TL with the -n_best nbest options nbest being
v3.1.3,then number of hypotheses and output the target to -output target.nbest.tl
v3.1.3,Then you need to duplicate source and reference sentences nbest times
v3.1.3,for this script.
v3.1.3,for instance using awk '{for(i=1; i<=n; i++) print}' n=5 reference.tl \
v3.1.3,> reference.5.tl
v3.1.3,same for source.
v3.1.3,This script can be run (for instance with nbest = 5) as follows:
v3.1.3,python oracle_bleu.py --nbest-src source.5.sl --nbest-hyp target.5.tl \
v3.1.3,--nbest-ref reference.5.tl --nbest-order 5 --output target.maxbleu.tl
v3.1.3,It will search in all hyp the best comet score
v3.1.3,when choosing a reference-less model no nbest-ref is required
v3.1.3,for nbest in nbests:
v3.1.3,!/usr/bin/env python
v3.1.3,!/usr/bin/env python3
v3.1.3,coding: utf-8
v3.1.3,Let's say you have a source file with N sentences in SL - eg: source.sl
v3.1.3,Translate your file in TL with the -n_best nbest options nbest being
v3.1.3,then number of hypotheses and output the target to -output target.nbest.tl
v3.1.3,This script can be run (for instance with nbest = 5) as follows:
v3.1.3,python mbr_bleu.py --nbest-hyp target.5.tl \
v3.1.3,--nbest-order 5 --output target.mbr.tl
v3.1.3,It will compare all hyp with eachother and output the max bleu
v3.1.3,!/usr/bin/env python
v3.1.3,flake8: noqa
v3.1.3,!/usr/bin/env python
v3.1.3,-*- coding: utf-8 -*-
v3.1.3,Author: Rico Sennrich
v3.1.3,flake8: noqa
v3.1.3,This file is retrieved from https://github.com/rsennrich/subword-nmt
v3.1.3,hack for python2/3 compatibility
v3.1.3,"find all instances of pair, and update frequency/indices around it"
v3.1.3,find first symbol
v3.1.3,"if first symbol is followed by second symbol, we've found an occurrence of pair (old_word[i:i+2])"
v3.1.3,"assuming a symbol sequence ""A B C"", if ""B C"" is merged, reduce the frequency of ""A B"""
v3.1.3,"assuming a symbol sequence ""A B C B"", if ""B C"" is merged, reduce the frequency of ""C B""."
v3.1.3,"however, skip this if the sequence is A B C B C, because the frequency of ""C B"" will be reduced by the previous code block"
v3.1.3,find new pair
v3.1.3,"assuming a symbol sequence ""A BC D"", if ""B C"" is merged, increase the frequency of ""A BC"""
v3.1.3,"assuming a symbol sequence ""A BC B"", if ""B C"" is merged, increase the frequency of ""BC B"""
v3.1.3,"however, if the sequence is A BC BC, skip this step because the count of ""BC BC"" will be incremented by the previous code block"
v3.1.3,data structure of pair frequencies
v3.1.3,index from pairs to words
v3.1.3,version 0.2 changes the handling of the end-of-word token ('</w>');
v3.1.3,version numbering allows bckward compatibility
v3.1.3,"threshold is inspired by Zipfian assumption, but should only affect speed"
v3.1.3,we probably missed the best pair because of pruning; go back to full statistics
v3.1.3,"threshold is inspired by Zipfian assumption, but should only affect speed"
v3.1.3,python 2/3 compatibility
v3.1.3,read/write files as UTF-8
v3.1.3,Now we can pipe the full file through the model using the Iterator
v3.1.3,reminder a batch includes .src .tgt .indices and it is sorted
v3.1.3,Compute and retrieve the loss for EACH sentence
v3.1.3,Now we need to rearrange the batch of ppl
v3.1.3,in the original order with indices
v3.1.3,!/usr/bin/env python
v3.1.3,-*- coding: utf-8 -*-
v3.1.3,!/usr/bin/env python
v3.1.3,flake8: noqa
v3.1.3,!/usr/bin/env python
v3.1.3,!/usr/bin/env python
v3.1.3,flake8: noqa
v3.1.3,!/usr/bin/env python3
v3.1.3,coding: utf-8
v3.1.3,Let's say you have a source file with N sentences in SL - eg: source.sl
v3.1.3,and the corresponding references (N sentences) reference.tl
v3.1.3,Translate your file in TL with the -n_best nbest options nbest being
v3.1.3,then number of hypotheses and output the target to -output target.nbest.tl
v3.1.3,Then you need to duplicate reference sentences nbest times for this script.
v3.1.3,for instance using awk '{for(i=1; i<=n; i++) print}' n=5 reference.tl \
v3.1.3,> reference.5.tl
v3.1.3,This script can be run (for instance with nbest = 5) as follows:
v3.1.3,python oracle_bleu.py --nbest-hyp target.5.tl --nbest-ref reference.5.tl \
v3.1.3,--nbest-order 5 --output target.maxbleu.tl
v3.1.3,It will search in all hyp the best bleu wrt reference
v3.1.3,and output the max bleu
v3.1.3,!/usr/bin/env python
v3.1.3,with the two module = imp.load_source() below
v3.1.3,we ghost the old torchtext.data.field and depercated
v3.1.3,onmt.inputters.text_dataset
v3.1.3,however this require some functions / classes to be
v3.1.3,monkey patched for loading the old field/vocab objects.
v3.1.3,"module3 = imp.load_source(""Vocab"", ""tools/convertv2_v3.py"")"
v3.1.3,"voc = dict(sorted(fields.vocab.__dict__['freqs'].items(),"
v3.1.3,"key=lambda x: (-x[1], x[0]))).keys()"
v3.1.3,"voc = dict(sorted(fields.vocab.__dict__['freqs'].items(),"
v3.1.3,"key=lambda x: (-x[1], x[0]))).keys()"
v3.1.3,!/usr/bin/env python
v3.1.3,flake8: noqa
v3.1.3,redpajama stores QKV in one single tensor but it is not simply piled up Q+K+V
v3.1.3,it is heads interleaved to we need to slice first
v3.1.3,also it uses the HF rotary so we need to permute Q and K interleave
v3.1.3,Avoid functionality on inference
v3.1.3,Build embeddings.
v3.1.3,Build encoder.
v3.1.3,Build embeddings.
v3.1.3,Build decoder.
v3.1.3,Share the embedding matrix - preprocess with share_vocab required.
v3.1.3,src/tgt vocab should be the same if `-share_vocab` is specified.
v3.1.3,Update vocabulary embeddings with checkpoint embeddings
v3.1.3,Embedding layers
v3.1.3,Just for debugging purposes
v3.1.3,Remove old vocabulary associated embeddings
v3.1.3,for back compat when attention_dropout was not defined
v3.1.3,Build Model
v3.1.3,Build Generator.
v3.1.3,Load the model states from checkpoint or initialize them.
v3.1.3,This preserves backward-compat for models using customed layernorm
v3.1.3,end of patch for backward compatibility
v3.1.3,Update model embeddings with those from the checkpoint
v3.1.3,after initialization
v3.1.3,when using LoRa or updating the vocab (no more embeddings in ckpt)
v3.1.3,=> strict=False when loading state_dict
v3.1.3,!/usr/bin/env python
v3.1.3,if transform + options set in 'valid' we need to copy in main
v3.1.3,transform / options for scoring considered as inference
v3.1.3,"maybe prepare pretrained embeddings, if any"
v3.1.3,Load checkpoint if we resume from a previous training.
v3.1.3,ensure tensorboard output is written in the directory
v3.1.3,of previous checkpoints
v3.1.3,Override checkpoint's update_embeddings as it defaults to false
v3.1.3,Override checkpoint's freezing settings as it defaults to false
v3.1.3,NOTE: It's important that ``opt`` has been validated and updated
v3.1.3,at this point.
v3.1.3,Build model.
v3.1.3,Build optimizer.
v3.1.3,Build model saver
v3.1.3,Use Tensorboard for visualization during training
v3.1.3,Options only during inference
v3.1.3,"Truncation options, for text corpus"
v3.1.3,"as for False, this will be added in _add_train_general_opts"
v3.1.3,Embedding Options
v3.1.3,Model Task Options
v3.1.3,Encoder-Decoder Options
v3.1.3,Freeze Encoder and/or Decoder
v3.1.3,The following options (bridge_extra_node to n_steps) are used
v3.1.3,for training with --encoder_type ggnn (Gated Graph Neural Network).
v3.1.3,Attention options
v3.1.3,Alignement options
v3.1.3,Generator and loss options.
v3.1.3,GPU
v3.1.3,LoRa
v3.1.3,Init options
v3.1.3,Pretrained word vectors
v3.1.3,Freeze word vectors
v3.1.3,Optimization options
v3.1.3,learning rate
v3.1.3,options relate to data preprare
v3.1.3,options relate to train
v3.1.3,Alpha and Beta values for Google Length + Coverage penalty
v3.1.3,"Described here: https://arxiv.org/pdf/1609.08144.pdf, Section 7"
v3.1.3,Length penalty options
v3.1.3,Coverage penalty options
v3.1.3,Decoding Length constraint
v3.1.3,Decoding content constraint
v3.1.3,Adding options related to source and target features
v3.1.3,Adding options relate to decoding strategy
v3.1.3,Adding option for logging
v3.1.3,Adding options related to Transforms
v3.1.3,Copyright 2016 The Chromium Authors. All rights reserved.
v3.1.3,Use of this source code is governed by a BSD-style license that can be
v3.1.3,found in the LICENSE file.
v3.1.3,"Get the key 'value' in the dict, or just use 'value'"
v3.1.3,Basic attributes.
v3.1.3,Set model in training mode.
v3.1.3,Let's clean the GPUs before training loop
v3.1.3,UPDATE DROPOUT
v3.1.3,Run patience mechanism
v3.1.3,"If the patience has reached the limit, stop training"
v3.1.3,swap model params w/ moving average
v3.1.3,(and keep the original parameters)
v3.1.3,Set model in validating mode.
v3.1.3,F-prop through the model.
v3.1.3,Compute loss.
v3.1.3,Compute validation metrics (at batch.dataset level)
v3.1.3,Compute stats
v3.1.3,Update statistics.
v3.1.3,Set model back to training mode.
v3.1.3,Truncated BPTT: reminder not compatible with accum > 1
v3.1.3,1. Create truncated target.
v3.1.3,2. F-prop all but generator.
v3.1.3,3. Compute loss.
v3.1.3,Compute and save stats
v3.1.3,"If truncated, don't backprop fully."
v3.1.3,"in case of multi step gradient accumulation,"
v3.1.3,update only after accum batches
v3.1.3,For Flake
v3.1.3,we avoid padding while mean pooling
v3.1.3,incoming and outgoing edge embedding
v3.1.3,Find vocab data for tree builting
v3.1.3,Propogation Model
v3.1.3,Initialize the bridge layer
v3.1.3,Token embedding
v3.1.3,Initialize graph using formatted input sequence
v3.1.3,Number of flagged nodes defines node count for this sample
v3.1.3,"(Nodes can have no flags on them, but must be in 'flags' list)."
v3.1.3,The total number of integers in the vocab should allow
v3.1.3,for all features and edges to be defined.
v3.1.3,Use first extra node as only source for decoder init
v3.1.3,Average all nodes to get bridge input
v3.1.3,"LSTM has hidden and cell state, other only one"
v3.1.3,Total number of states
v3.1.3,Build a linear layer for each
v3.1.3,Initialize the bridge layer
v3.1.3,src lengths data is wrapped inside a Tensor.
v3.1.3,"LSTM has hidden and cell state, other only one"
v3.1.3,Total number of states
v3.1.3,Build a linear layer for each
v3.1.3,batch x len x dim
v3.1.3,mask is now (batch x 1 x slen x slen)
v3.1.3,1 to be expanded to number of heads in MHA
v3.1.3,Run the forward pass of every layer of the tranformer.
v3.1.3,Dimensions and padding for constructing the word embedding matrix
v3.1.3,Dimensions and padding for feature embedding matrices
v3.1.3,(these have no effect if feat_vocab_sizes is empty)
v3.1.3,The embedding matrix look-up tables. The first look-up table
v3.1.3,"is for words. Subsequent ones are for features, if any exist."
v3.1.3,The final output size of word + feature vectors. This can vary
v3.1.3,from the word vector size if and only if features are defined.
v3.1.3,This is the attribute you should access if you need to know
v3.1.3,how big your embeddings are going to be.
v3.1.3,The sequence of operations that converts the input sequence
v3.1.3,into a sequence of embeddings. At minimum this consists of
v3.1.3,looking up the embeddings for each word and feature in the
v3.1.3,input. Model parameters may require the sequence to contain
v3.1.3,additional operations as well.
v3.1.3,features must use word_vec_size
v3.1.3,features will use feat_vec_size
v3.1.3,Some utilitary functions for pretrained embeddings
v3.1.3,is this reachable?
v3.1.3,Write to file
v3.1.3,set the opt in place
v3.1.3,set the opt in place
v3.1.3,flake8: noqa
v3.1.3,For command-line option parsing
v3.1.3,"Check pass, set the args."
v3.1.3,"This SRU version implements its own cuda-level optimization,"
v3.1.3,so it requires that:
v3.1.3,1. `cupy` and `pynvrtc` python package installed.
v3.1.3,2. pytorch is built with cuda support.
v3.1.3,3. library path set: export LD_LIBRARY_PATH=<cuda lib path>.
v3.1.3,Check 1.
v3.1.3,Check 2.
v3.1.3,Check 3.
v3.1.3,This sets up device to use.
v3.1.3,-> directions x batch x dim
v3.1.3,For DEBUG
v3.1.3,"size = (length, batch, x.size(-1)) \"
v3.1.3,"if x.dim() == 3 else (batch, x.size(-1))"
v3.1.3,grad_x = x.new(*x.size()) if k_ == 3 else x.new(*size).zero_()
v3.1.3,Normal use
v3.1.3,"An entry check here, will catch on train side and translate side"
v3.1.3,if requirements are not satisfied.
v3.1.3,RNNDecoderState wraps hidden as a tuple.
v3.1.3,fh -> (layers*directions) x batch x dim
v3.1.3,This class is mainly used by decoder.py for RNNs but also
v3.1.3,by the CNN / transformer decoder when copy attention is used
v3.1.3,CNN has its own attention mechanism ConvMultiStepAttention
v3.1.3,Transformer has its own MultiHeadedAttention
v3.1.3,mlp wants it with bias
v3.1.3,"(batch, t_len, d) x (batch, d, s_len) --> (batch, t_len, s_len)"
v3.1.3,"(batch, t_len, s_len, d)"
v3.1.3,one step input
v3.1.3,"compute attention scores, as in Luong et al."
v3.1.3,Softmax or sparsemax to normalize attention weights
v3.1.3,each context vector c_t is the weighted average
v3.1.3,over all the source hidden states
v3.1.3,concatenate
v3.1.3,clamping necessary because of numerical errors: loss should be lower
v3.1.3,"bounded by zero, but negative values near zero are possible without"
v3.1.3,the clamp
v3.1.3,Help functions for Rotary Embeddings
v3.1.3,https://arxiv.org/pdf/2104.09864.pdf
v3.1.3,too convoluted to make maxseqlen a parameter.
v3.1.3,we suppose src_seq_len at training and max_length at inference
v3.1.3,are both < 2048 tokens.
v3.1.3,"rope is now matrix [maxseqlen, dim/2]"
v3.1.3,Help functions for max_relative positions
v3.1.3,https://arxiv.org/abs/1803.02155
v3.1.3,Shift values to be >= 0
v3.1.3,Help functions to split model dim per head
v3.1.3,class MultiHeadedAttention(torch.jit.ScriptModule):
v3.1.3,https://arxiv.org/pdf/1803.02155.pdf
v3.1.3,in the paper they suggest either two embeds
v3.1.3,relative_key / relative_value or only
v3.1.3,relative_key. We implemented the same embed
v3.1.3,for both.
v3.1.3,@torch.jit.script_method
v3.1.3,"1) Project key, value, and query."
v3.1.3,as a reminder at training layer_cache[0] remains False
v3.1.3,2) Calculate and scale scores.
v3.1.3,batch x num_heads x query_len x key_len
v3.1.3,1 or key_len x key_len
v3.1.3,1 or key_len x key_len x dim_per_head
v3.1.3,not 100% necessary but expand to nb of heads
v3.1.3,now mask and scores have the same shape
v3.1.3,3) Apply attention dropout and compute context vectors.
v3.1.3,We use the same embeddings for key and value
v3.1.3,--------------------------------------------------------------------------
v3.1.3,Mostly copied from https://github.com/microsoft/LoRA/
v3.1.3,Copyright (c) Microsoft Corporation. All rights reserved.
v3.1.3,Licensed under the MIT License (MIT).
v3.1.3,
v3.1.3,--------------------------------------------------------------------------
v3.1.3,Optional dropout
v3.1.3,Mark the weight as unmerged
v3.1.3,LoRA implemented in a dense layer
v3.1.3,Actual trainable parameters
v3.1.3,Freezing the pre-trained weight matrix
v3.1.3,initialize A the same way as the default
v3.1.3,for nn.Linear and B to zero
v3.1.3,Make sure that the weights are not merged
v3.1.3,Merge the weights and mark it
v3.1.3,LoRA implemented in a dense layer
v3.1.3,Set this to True if the layer to replace stores
v3.1.3,"weight like (fan_in, fan_out)"
v3.1.3,Actual trainable parameters
v3.1.3,Freezing the pre-trained weight matrix
v3.1.3,initialize A the same way as the default
v3.1.3,for nn.Linear and B to zero
v3.1.3,Make sure that the weights are not merged
v3.1.3,Merge the weights and mark it
v3.1.3,LoRA implemented in a dense layer
v3.1.3,Actual trainable parameters
v3.1.3,Freezing the pre-trained weight matrix
v3.1.3,Compute the indices
v3.1.3,initialize A the same way as the default
v3.1.3,for nn.Linear and B to zero
v3.1.3,Make sure that the weights are not merged
v3.1.3,Merge the weights and mark it
v3.1.3,At the moment this class is only used by embeddings.Embeddings look-up tables
v3.1.3,"for silu, see: https://arxiv.org/pdf/2002.05202.pdf"
v3.1.3,-*- coding: utf-8 -*-
v3.1.3,class AverageAttention(torch.jit.ScriptModule):
v3.1.3,@torch.jit.script
v3.1.3,out_features * in_features
v3.1.3,norm is out_features * 1
v3.1.3,batch_size * out_features
v3.1.3,out_features
v3.1.3,out_features
v3.1.3,batch_size * out_features
v3.1.3,"out_channels, in_channels // groups, * kernel_size"
v3.1.3,out_features
v3.1.3,This is used nowhere in the code at the moment (Vincent Nguyen 05/18/2018)
v3.1.3,"in_channels, out_channels, *kernel_size"
v3.1.3,"in_channels, out_channels, *kernel_size"
v3.1.3,"self.out_channels, 1"
v3.1.3,out_features
v3.1.3,out_features
v3.1.3,store roots on diagonal
v3.1.3,Original probabilities.
v3.1.3,Probability of copying p(z=1) batch.
v3.1.3,Probability of not copying: p_{word}(w) * (1 - p(z))
v3.1.3,probabilities assigned by the model to the gold targets
v3.1.3,probability of tokens copied from source
v3.1.3,Set scores for unk to 0 and add eps
v3.1.3,find the indices in which you do not use the copy mechanism
v3.1.3,Drop padding.
v3.1.3,Filter out very short or very long sentences
v3.1.3,from the TM for better performance
v3.1.3,We split the `batch` and perform fuzzy matching
v3.1.3,in smaller chunks of 10.000 examples in order to
v3.1.3,reduce memory usage.
v3.1.3,Perfomance is not affected.
v3.1.3,Probably redundant but let's be safe
v3.1.3,in case some examples are already fuzzied
v3.1.3,(e.g. from another pipeline or workflow)
v3.1.3,We don't want exact matches
v3.1.3,Apply a basic filtering to leave out very short or very long
v3.1.3,sentences and speed up things a bit during fuzzy matching
v3.1.3,Do nothing
v3.1.3,We set the start number of tags to a random number from 1
v3.1.3,to 12 + the number of subsequent tags that
v3.1.3,will be added. We also apply weights to this choice so tags
v3.1.3,"are more probable to start from 1, then from 2, etc."
v3.1.3,This way we cover most scenarios met in real usage and
v3.1.3,the system will learn to handle a fairly large number of
v3.1.3,numbered tags (but not an excessively large number)
v3.1.3,Make sure we only search for exact matches (we don't want
v3.1.3,to match part of words) and perform some bound checking
v3.1.3,Create all possible tag forms. We inject a special
v3.1.3,unicode char (âˆ¥) as a placeholder for whitespace in order
v3.1.3,to keep the indices unaltered. This char is replaced with
v3.1.3,spaces before we return the augmented examples.
v3.1.3,Make a weighted choice between paired tags or single tags.
v3.1.3,"We usually encounter, and thus here we favor, paired tags"
v3.1.3,with a ratio 1/3.
v3.1.3,Check if the tags include the
v3.1.3,"mandatory ""#"" number placeholder"""
v3.1.3,We split the user-defined tags in the # placeholder
v3.1.3,in order to number them
v3.1.3,normalize dict src/tgt for each dataset
v3.1.3,"print(""src empty"")"
v3.1.3,"print(""too many same char in src"")"
v3.1.3,"print(""too many same word in src"")"
v3.1.3,"print(""avg token min"", len(src_str) / len(ex['src']))"
v3.1.3,"print(""avg token max"", len(src_str) / len(ex['src']))"
v3.1.3,"print(""text does not fully belong to wanted script"")"
v3.1.3,"print(""Some text belong to unwanted scripts"")"
v3.1.3,"print(""langid does not match"", _id(src_str))"
v3.1.3,"print(""src = tgt"")"
v3.1.3,"print(""tgt empty"")"
v3.1.3,"print(""src / tgt ratio "", len(src_str) / len(tgt_str))"
v3.1.3,"print(""too many same char in tgt"")"
v3.1.3,"print(""too many same word in tgt"")"
v3.1.3,"print(""avg token min"", len(tgt_str) / len(ex['tgt']))"
v3.1.3,"print(""avg token max"", len(tgt_str) / len(ex['tgt']))"
v3.1.3,"print(""text does not fully belong to wanted script"")"
v3.1.3,"print(""Some text belong to unwanted scripts"")"
v3.1.3,"print(""langid does not match"", _id(tgt_str))"
v3.1.3,"doc break we add it, restart new doc"
v3.1.3,case 1st ex is already longer
v3.1.3,adding cur ex is too long we add cur doc
v3.1.3,and reset doc to cur ex
v3.1.3,we start the new doc with cur ex
v3.1.3,we cumulate cur ex to cur doc
v3.1.3,Auto import python files in this directory
v3.1.3,1. sample number of tokens to corrupt
v3.1.3,2. sample positions to corrput
v3.1.3,3. sample corrupted values
v3.1.3,1. sample number of tokens to corrupt
v3.1.3,2. sample positions to corrput
v3.1.3,3. Drop token on chosen position
v3.1.3,1. sample number of tokens to corrupt
v3.1.3,2. sample positions to corrput
v3.1.3,3. mask word on chosen position
v3.1.3,"Sharing options among `TokenizerTransform`s, same name conflict in"
v3.1.3,this scope will be resolved by remove previous occurrence in parser
v3.1.3,subword regularization(or BPE dropout) options:
v3.1.3,subword vocabulary restriction options:
v3.1.3,derterministic subwording
v3.1.3,subword sampling when nbest_size > 1 or -1
v3.1.3,alpha should be 0.0 < alpha < 1.0
v3.1.3,Load vocabulary file if provided and set threshold
v3.1.3,Load Subword Model
v3.1.3,-1: keep everything (i.e. 1 mask per token)
v3.1.3,0: replace everything (i.e. no mask)
v3.1.3,1: 1 mask per span
v3.1.3,view each subword as word start / input is word level token
v3.1.3,Pretend it ends with a full stop so last span is a sentence
v3.1.3,"Tokens that are full stops, where the previous token is not"
v3.1.3,Make sure we have enough to mask
v3.1.3,Trim to masking budget
v3.1.3,Handle 0-length mask (inserts) separately
v3.1.3,assert is_word_start[-1] == 0
v3.1.3,assert tokens_length - 1 not in indices
v3.1.3,"keep index, but replace it with [MASK]"
v3.1.3,"acts as a long length, so spans don't go over the end of doc"
v3.1.3,next position from each word_start
v3.1.3,delete token: 1 mask/remove per span
v3.1.3,"keep index, but replace it with [MASK]: 1 mask per token"
v3.1.3,A bit faster when all lengths are 1
v3.1.3,to cover whole token
v3.1.3,delete token
v3.1.3,"keep index, but replace it with [MASK]"
v3.1.3,assert tokens_length - 1 not in indices
v3.1.3,prefix src/tgt for each dataset
v3.1.3,prefix as general option for inference
v3.1.3,suffix src/tgt for each dataset
v3.1.3,suffix as general option for inference
v3.1.3,!/usr/bin/env python3
v3.1.3,-*- coding: utf-8 -*-
v3.1.3,Most code taken from: https://github.com/alvations/sacremoses
v3.1.3,Which in turn is based on the Moses punctuation normalizer.
v3.1.3,https://github.com/moses-smt/mosesdecoder/blob/master/scripts/
v3.1.3,tokenizer/normalize-punctuation.perl
v3.1.3,don't fix period at end of sentence
v3.1.3,Regex substitutions from replace-unicode-punctuation.perl
v3.1.3,https://github.com/moses-smt/mosesdecoder/blob/master/
v3.1.3,scripts/tokenizer/replace-unicode-punctuation.perl
v3.1.3,Adds the penn substitutions after extra_whitespace regexes.
v3.1.3,"Optionally, replace unicode puncts BEFORE normalization."
v3.1.3,Actual normalization.
v3.1.3,"Optionally, replace unicode puncts BEFORE normalization."
v3.1.3,normalize dict src/tgt for each dataset
v3.1.3,One source feature expected but none given and no default provided
v3.1.3,Provided default does not match required features
v3.1.3,Data not properly annotated.
v3.1.3,In this case we do not use the default as it might be an error
v3.1.3,batch 0 will always predict EOS. The other batches will predict
v3.1.3,non-eos scores.
v3.1.3,"""best"" prediction is eos - that should be blocked"
v3.1.3,include at least one prediction OTHER than EOS
v3.1.3,that is greater than -1e20
v3.1.3,now batch 0 has ended and no others have
v3.1.3,initial step
v3.1.3,batch 0 dies on step 0
v3.1.3,include at least one prediction OTHER than EOS
v3.1.3,that is greater than -1e20
v3.1.3,step 2
v3.1.3,(old) batch 8 dies on step 1
v3.1.3,step 3
v3.1.3,everything dies
v3.1.3,initial step
v3.1.3,batch 0 dies on step 0
v3.1.3,include at least one prediction OTHER than EOS
v3.1.3,that is greater than -1e20
v3.1.3,step 2
v3.1.3,(old) batch 8 dies on step 1
v3.1.3,step 3
v3.1.3,everything dies
v3.1.3,initial step
v3.1.3,finish one beam
v3.1.3,include at least one prediction OTHER than EOS
v3.1.3,that is greater than -1e20
v3.1.3,step 2
v3.1.3,finish example in last batch
v3.1.3,(old) batch 8 dies on step 1
v3.1.3,step 3
v3.1.3,everything dies
v3.1.3,initial step
v3.1.3,batch 0 dies on step 0
v3.1.3,include at least one prediction OTHER than EOS
v3.1.3,that is greater than -1e20
v3.1.3,step 2
v3.1.3,(old) batch 8 dies on step 1
v3.1.3,step 3
v3.1.3,everything dies
v3.1.3,illegal_weights_mask = torch.ByteTensor([
v3.1.3,"[0, 0, 0, 0, 0, 0, 0],"
v3.1.3,"[0, 0, 0, 1, 1, 1, 1],"
v3.1.3,"[0, 0, 0, 0, 0, 1, 1],"
v3.1.3,"[0, 0, 1, 1, 1, 1, 1]])"
v3.1.3,TODO: fix for pytorch 0.3
v3.1.3,illegal_weights = alignments.masked_select(illegal_weights_mask)
v3.1.3,"self.assertEqual(0.0, illegal_weights.data.sum())"
v3.1.3,this could be considered an integration test because it touches
v3.1.3,the filesystem for the config file (and the models)
v3.1.3,no dummy prefix
v3.1.3,no dummy prefix
v3.1.3,make sure the scalars are in the event accumulator tags
v3.1.3,required arguments
v3.1.3,transforms that require vocab will not create if not provide vocab
v3.1.3,1. Init first transform in the pipe
v3.1.3,2. Init second transform in the pipe
v3.1.3,3. Sequential combine them into a transform pipe
v3.1.3,4. apply transform pipe for example
v3.1.3,"5. example after the pipe exceed the length limit, thus filtered"
v3.1.3,6. Transform statistics registed (here for filtertoolong)
v3.1.3,"7. after report, statistics become empty as a fresh start"
v3.1.3,filter_transform.warm_up()
v3.1.3,test BPE-dropout:
v3.1.3,1. disable bpe dropout for not training example
v3.1.3,2. enable bpe dropout for training example
v3.1.3,3. (NOTE) disable dropout won't take effect if already seen
v3.1.3,this is caused by the cache mechanism in bpe:
v3.1.3,return cached subword if the original token is seen when no dropout
v3.1.3,test SP regularization:
v3.1.3,1. enable regularization for training example
v3.1.3,2. disable regularization for not training example
v3.1.3,Not apply token drop for not training example
v3.1.3,apply token drop for training example
v3.1.3,Not apply token mask for not training example
v3.1.3,apply token mask for training example
v3.1.3,require vocabs to warm_up
v3.1.3,Not apply token mask for not training example
v3.1.3,apply token mask for training example
v3.1.3,"Defalt: full_stop_token=[""."", ""?"", ""!""]"
v3.1.3,"Defalt: full_stop_token=[""."", ""?"", ""!""]"
v3.1.3,random_ratio of inserted tokens are chosen in vocab
v3.1.3,others are MASK_TOK
v3.1.3,"insert_ratio=0.0,"
v3.1.3,"random_ratio=0.0,"
v3.1.3,"Defalt: full_stop_token=[""."", ""?"", ""!""]"
v3.1.3,all token are considered as an individual word
v3.1.3,1. tokens are dropped when replace_length is 0
v3.1.3,"print(f""token delete: {masked} / {tokens}"")"
v3.1.3,2. tokens are replaced by MASK when replace_length is 1
v3.1.3,"print(f""token mask: {masked} / {tokens}"")"
v3.1.3,"insert_ratio=0.0,"
v3.1.3,"random_ratio=0.0,"
v3.1.3,"Defalt: full_stop_token=[""."", ""?"", ""!""]"
v3.1.3,start token of word are identified using subword marker
v3.1.3,"1. replace_length 0: ""words"" are dropped"
v3.1.3,"print(f""word delete: {masked} / {tokens}"")"
v3.1.3,"self.assertEqual(len(masked), n_words - n_masked)"
v3.1.3,"2. replace_length 1: ""words"" are replaced with a single MASK"
v3.1.3,"print(f""whole word single mask: {masked} / {tokens}"")"
v3.1.3,len(masked) depend on number of tokens in select word
v3.1.3,"3. replace_length -1: all tokens in ""words"" are replaced with MASK"
v3.1.3,"print(f""whole word multi mask: {masked} / {tokens}"")"
v3.1.3,number of mask_tok depend on number of tokens in selected word
v3.1.3,number of MASK_TOK can be greater than n_masked
v3.1.3,"insert_ratio=0.5,"
v3.1.3,"random_ratio=0.3,"
v3.1.3,"Defalt: full_stop_token=[""."", ""?"", ""!""]"
v3.1.3,start token of word are identified using subword marker
v3.1.3,n_words = sum(token_starts)
v3.1.3,n_masked = math.ceil(n_words * bart_noise.mask_ratio)
v3.1.3,"print(f""Text Span Infilling: {infillied} / {tokens}"")"
v3.1.3,"print(n_words, n_masked)"
v3.1.3,!/usr/bin/env python
v3.1.3,-*- coding: utf-8 -*-
v3.1.3,Inject some dummy training options that may needed when build fields
v3.1.3,Remove the generated *pt files.
v3.1.3,Remove the generated data samples
v3.1.3,all beams repeat (beam >= 1 repeat dummy scores)
v3.1.3,predict repeat_idx over and over again
v3.1.3,"before repeat, scores are either 0 or -inf"
v3.1.3,"on repeat, `repeat_idx` score is BLOCKED_SCORE"
v3.1.3,"(but it's still the best score, thus we have"
v3.1.3,"[BLOCKED_SCORE, -inf, -inf, -inf, -inf]"
v3.1.3,repetitions keeps maximizing score
v3.1.3,"index 0 has been blocked, so repeating=>+0.0 score"
v3.1.3,other indexes are -inf so repeating=>BLOCKED_SCORE
v3.1.3,which is higher
v3.1.3,beam 0 and beam >=2 will repeat (beam >= 2 repeat dummy scores)
v3.1.3,non-interesting beams are going to get dummy values
v3.1.3,"on initial round, only predicted scores for beam 0"
v3.1.3,matter. Make two predictions. Top one will be repeated
v3.1.3,"in beam zero, second one will live on in beam 1."
v3.1.3,predict the same thing in beam 0
v3.1.3,continue pushing around what beam 1 predicts
v3.1.3,"now beam 0 dies (along with the others), beam 1 -> beam 0"
v3.1.3,"now beam 0 dies (along with the others), beam 1 -> beam 0"
v3.1.3,beam 0 and beam >= 2 will repeat (beam 2 repeats excluded idx)
v3.1.3,non-interesting beams are going to get dummy values
v3.1.3,predict the same thing in beam 0
v3.1.3,continue pushing around what beam 1 predicts
v3.1.3,predict the allowed-repeat again in beam 2
v3.1.3,"now beam 0 dies, beam 1 -> beam 0, beam 2 -> beam 1"
v3.1.3,and the rest die
v3.1.3,"since all preds after i=0 are 0, we can check"
v3.1.3,that the beam is the correct idx by checking that
v3.1.3,the curr score is the initial score
v3.1.3,beam 0 will always predict EOS. The other beams will predict
v3.1.3,non-eos scores.
v3.1.3,non-interesting beams are going to get dummy values
v3.1.3,"""best"" prediction is eos - that should be blocked"
v3.1.3,include at least beam_sz predictions OTHER than EOS
v3.1.3,that are greater than -1e20
v3.1.3,predict eos in beam 0
v3.1.3,provide beam_sz other good predictions
v3.1.3,now the top beam has ended and no others have
v3.1.3,"not of interest, but want to make sure it keeps running"
v3.1.3,since only beam 0 terminates and n_best = 2
v3.1.3,"this is also a test that when block_ngram_repeat=0,"
v3.1.3,repeating is acceptable
v3.1.3,non-interesting beams are going to get dummy values
v3.1.3,"""best"" prediction is eos - that should be blocked"
v3.1.3,include at least beam_sz predictions OTHER than EOS
v3.1.3,that are greater than -1e20
v3.1.3,predict eos in beam 1
v3.1.3,provide beam_sz other good predictions in other beams
v3.1.3,beam 1 dies on min_length
v3.1.3,beam 0 dies on the step after beam 1 dies
v3.1.3,"inp_lens is tiled in initialize, reassign to make attn match"
v3.1.3,non-interesting beams are going to get dummy values
v3.1.3,"""best"" prediction is eos - that should be blocked"
v3.1.3,include at least beam_sz predictions OTHER than EOS
v3.1.3,that are greater than -1e20
v3.1.3,predict eos in beam 1
v3.1.3,provide beam_sz other good predictions in other beams
v3.1.3,no top beams are finished yet
v3.1.3,beam 1 dies on min_length
v3.1.3,no top beams are finished yet
v3.1.3,beam 0 dies on the step after beam 1 dies
v3.1.3,top beam is finished now so there are attentions
v3.1.3,two beams are finished in each batch
v3.1.3,second dim is cut down to the non-padded src length
v3.1.3,first dim is equal to the time of death
v3.1.3,(beam 0 died at current step - adjust for SOS)
v3.1.3,(beam 1 died at last step - adjust for SOS)
v3.1.3,behavior gets weird when beam is already done so just stop
v3.1.3,this is just test_beam.TestBeamAgainstReferenceCase repeated
v3.1.3,in each batch.
v3.1.3,"init_preds: [4, 3, 5, 6, 7] - no EOS's"
v3.1.3,no EOS's yet
v3.1.3,"[5, 3, 2, 6, 0], so beam 2 predicts EOS!"
v3.1.3,assumes beam 2 finished on last step
v3.1.3,ended beam 2 shouldn't continue
v3.1.3,"[2, 5, 3, 6, 0] repeat self.BATCH_SZ, so beam 0 predicts EOS!"
v3.1.3,"[-2.4879, -3.8910, -4.1010, -4.2010, -4.4010] repeat self.BATCH_SZ"
v3.1.3,another beam is finished in all batches
v3.1.3,new beam 0 finished
v3.1.3,new beam 0 is old beam 3
v3.1.3,assumes beam 0 finished on last step
v3.1.3,"[5, 2, 6, 1, 0] repeat self.BATCH_SZ, so beam 1 predicts EOS!"
v3.1.3,we finish 3 hyps per example in this step
v3.1.3,new beam 1 is old beam 3
v3.1.3,this could be considered an integration test because it tests
v3.1.3,interactions between the GNMT scorer and the beam
v3.1.3,"-data option is required, but not used in this test, so dummy."
v3.1.3,len x batch x nfeat
v3.1.3,Initialize vectors to compare size with
v3.1.3,Ensure correct sizes and types
v3.1.3,Make sure that output has the correct size and type
v3.1.3,"[('encoder_type', 'transformer'),"
v3.1.3,"('word_vec_size', 16), ('hidden_size', 16)],"
v3.1.3,""""""" Only do SRU test if requirment is safisfied. """""""
v3.1.3,SRU doesn't support input_feed.
v3.1.3,first check there's nothing unexpectedly not trainable
v3.1.3,ok: word embeddings shouldn't be trainable
v3.1.3,if word vecs are freezed
v3.1.3,ok: positional encodings shouldn't be trainable
v3.1.3,then check nothing unexpectedly trainable
v3.1.3,Decoder state
v3.1.3,Build the RNN.
v3.1.3,Set up the context gate.
v3.1.3,Set up the standard attention.
v3.1.3,The encoder hidden is  (layers*directions) x batch x dim.
v3.1.3,We need to convert it to layers x batch x (directions*dim).
v3.1.3,Init the input feed.
v3.1.3,Update the state with the result.
v3.1.3,Concatenates sequence of tensors along a new dimension.
v3.1.3,NOTE: v0.3 to 0.4: dec_outs / attns[*] may not be list
v3.1.3,(in particular in case of SRU) it was not raising error in 0.3
v3.1.3,since stack(Variable) was allowed.
v3.1.3,"In 0.4, SRU returns a tensor that shouldn't be stacke"
v3.1.3,Calculate the attention.
v3.1.3,Calculate the context gate.
v3.1.3,Additional args check.
v3.1.3,Input feed concatenates hidden state with
v3.1.3,input at every time step.
v3.1.3,TODO: context gate should be employed
v3.1.3,instead of second RNN transform.
v3.1.3,Update the coverage attention.
v3.1.3,"attns[""coverage""] is actually c^(t+1) of See et al(2017)"
v3.1.3,1-index shifted
v3.1.3,Decoder State
v3.1.3,CNNDecoder has its own attention mechanism.
v3.1.3,Set up a separate copy attention layer if needed.
v3.1.3,The output of CNNEncoder.
v3.1.3,The combination of output of CNNEncoder and source embeddings.
v3.1.3,Process the result and update the attentions.
v3.1.3,Update the state.
v3.1.3,TODO change the way attns is returned dict => list or tuple (onnx)
v3.1.3,src_len is a single tensor shared between all models.
v3.1.3,This assumption will not hold if Translator is modified
v3.1.3,to calculate src_len as something other than the length
v3.1.3,of the input.
v3.1.3,"return _, (B, Q_len, K_len)"
v3.1.3,"layer average attention across heads, get ``(B, Q, K)``"
v3.1.3,"Case 1: no full_context, no align heads -> layer avg baseline"
v3.1.3,"Case 2: no full_context, 1 align heads -> guided align"
v3.1.3,"Case 3: full_context, 1 align heads -> full cte guided align"
v3.1.3,BoolTensor was introduced in pytorch 1.2
v3.1.3,T: could be 1 in the case of stepwise decoding or tgt_len
v3.1.3,masking is necessary when sequence length is greater than one
v3.1.3,mask now are (batch x 1 x tlen x s or t len)
v3.1.3,1 = heads to be expanded in MHA
v3.1.3,Decoder State
v3.1.3,"previously, there was a GlobalAttention module here for copy"
v3.1.3,"attention. But it was never actually used -- the ""copy"" attention"
v3.1.3,just reuses the context attention.
v3.1.3,"attns[""align""] = torch.stack(attn_aligns, 0).mean(0)  # All avg"
v3.1.3,TODO change the way attns is returned dict => list or tuple (onnx)
v3.1.3,first value set to True triggered by the beginning of decoding
v3.1.3,layer_cache becomes active in the MultiHeadedAttention fwd
v3.1.3,T: could be 1 in the case of stepwise decoding or tgt_len
v3.1.3,masking is necessary when sequence length is greater than one
v3.1.3,mask now are (batch x 1 x tlen x tlen)
v3.1.3,1 = heads to be expanded in MHA
v3.1.3,TODO change the way attns is returned dict => list or tuple (onnx)
v3.1.3,"buffer size in bytes, determine equiv. # of elements based on data type"
v3.1.3,copy tensors into buffer_t
v3.1.3,all-reduce and rescale
v3.1.3,copy all-reduced buffer back into tensors
v3.1.3,"print(filled, sz)"
v3.1.3,"tensor is bigger than buffer, all-reduce and rescale directly"
v3.1.3,"buffer is full, all-reduce and replace buffer with grad"
v3.1.3,add tensor to buffer
v3.1.3,"propagate exception to parent process, keeping original traceback"
v3.1.3,TODO: Find a better way to check for sparse gradients.
v3.1.3,we use apex.amp
v3.1.3,In this case use the old FusedAdam with
v3.1.3,FP16_optimizer wrapper
v3.1.3,Load everything from the checkpoint.
v3.1.3,Build everything from scratch.
v3.1.3,"Reset optimizer, keep options."
v3.1.3,"Reset options, keep optimizer."
v3.1.3,State can be partially restored.
v3.1.3,should be: self._optimizer.zero_grad(set_to_none)
v3.1.3,but apex.amp is not up-to-date:
v3.1.3,https://github.com/NVIDIA/apex/blob/master/apex/amp/_process_optimizer.py#L367
v3.1.3,"unscaled optimizer's gradients (already done therefore skip),"
v3.1.3,skips optimizer.step() if gradients contain infs/NaNs.
v3.1.3,Updates the scale for next iteration.
v3.1.3,Code below is an implementation of https://arxiv.org/pdf/1804.04235.pdf
v3.1.3,inspired but modified from https://github.com/DeadAt0m/adafactor-pytorch
v3.1.3,backward compatibility
v3.1.3,assuming a list/generator of parameter means single group
v3.1.3,compute combined scale factor for this group
v3.1.3,norm is in fact norm*scale
v3.1.3,note: p.grad should not ever be set for correct operation of
v3.1.3,mixed precision optimizer that sometimes sends None gradients
v3.1.3,State initialization
v3.1.3,Exponential moving average of gradient values
v3.1.3,Exponential moving average of squared gradient values
v3.1.3,-*- coding: utf-8 -*-
v3.1.3,placing this here make it easier to call logger.info
v3.1.3,"from anywhere, just 'from onmt.utils.logging import logger'"
v3.1.3,"align_head contains value in [0, 1) presenting attn prob,"
v3.1.3,0 was resulted by the context attention src_pad_mask
v3.1.3,"So, the correspand position in ref_align should also be 0"
v3.1.3,"Therefore, clip align_head to > 1e-18 should be bias free."
v3.1.3,rescale with tau (temperature) and apply the log_softmax.
v3.1.3,ct2 expects src with lengths without padding
v3.1.3,again we use raw probs to rescale with tau and apply log_softmax
v3.1.3,lm_scores are in log space so log_target=True
v3.1.3,rescale with tau (temperature) and apply the log_softmax.
v3.1.3,ct2 expects src with lengths without padding
v3.1.3,again we use raw probs to rescale with tau and apply log_softmax
v3.1.3,lm_scores are in log space so log_target=True
v3.1.3,take into account here the tgt_shift_index (0 / 1 = LM/NMT)
v3.1.3,Correct target copy token instead of <unk>
v3.1.3,tgt[i] = align[i] + len(tgt_vocab)
v3.1.3,for i such that tgt[i] == 0 and align[i] != 0
v3.1.3,in the case criterion reduction is None then we need
v3.1.3,to sum the loss of each sentence in the batch
v3.1.3,Check Transforms
v3.1.3,Check path
v3.1.3,tgt is src for LM task
v3.1.3,Check weight
v3.1.3,Check features
v3.1.3,validation when train:
v3.1.3,Check embeddings stuff
v3.1.3,"Backward compatibility with ""fix_word_vecs_*"" opts"
v3.1.3,encoder and decoder should be same sizes
v3.1.3,"Load default opt values, then overwrite with the opts in"
v3.1.3,"the checkpoint. That way, if there are new options added,"
v3.1.3,the defaults are used.
v3.1.3,It comes from training
v3.1.3,TODO: needs to be added as inference opt
v3.1.3,Don't do anything
v3.1.3,Update best score of each criteria
v3.1.3,Reset tolerance
v3.1.3,Update current status
v3.1.3,Decrease tolerance
v3.1.3,Log
v3.1.3,Log
v3.1.3,Get a list of world_size lists with len(stat_list) Statistics objects
v3.1.3,"this param init is overridden by model_builder, useless then."
v3.1.3,SRU doesn't support PackedSequence.
v3.1.3,-*- coding: utf-8 -*-
v3.1.3,threshold on 1 to avoid div by 0
v3.1.3,treat alignment matrix one by one as each have different lengths
v3.1.3,No alignment if not exist valid tgt token
v3.1.3,get valid alignment (sub-matrix from full paded aligment matrix)
v3.1.3,Helper functions
v3.1.3,Keeps track of the original words/subwords
v3.1.3,('prior_tokenization' option)
v3.1.3,In case there is a final case_markup when new_spacer is on
v3.1.3,translate
v3.1.3,for validation we build an infer_iter per batch
v3.1.3,in order to avoid oom issues because there is no
v3.1.3,batching strategy in `textbatch_to_tensor`
v3.1.3,apply_reverse refs
v3.1.3,flatten preds
v3.1.3,save results
v3.1.3,-*- coding: utf-8 -*-
v3.1.3,this one is needed for Random Shuffler of batches
v3.1.3,in multi gpu it ensures datasets are read in the same order
v3.1.3,some cudnn methods can be random even after fixing the seed
v3.1.3,unless you tell it to be deterministic
v3.1.3,This one is needed for various tranfroms
v3.1.3,These ensure same initialization in multi gpu mode
v3.1.3,we need to check the model path + any tokenizer path
v3.1.3,patch to log stdout spawned processes of dataloader
v3.1.3,bucket_size = batch_size
v3.1.3,For TRAIN we need to group examples by length
v3.1.3,"for faster performance, but otherwise, sequential."
v3.1.3,For TRAIN we shuffle batches within the bucket
v3.1.3,otherwise sequential
v3.1.3,for specific case of rnn_packed need to be sorted
v3.1.3,within the batch
v3.1.3,Check if all tokens have features or none at all
v3.1.3,Make features part of src like
v3.1.3,"{'src': {'src': ..., 'feats': [...., ....]}}"
v3.1.3,at this point an example looks like:
v3.1.3,"{'src': {'src': ..., 'feats': [....]},"
v3.1.3,"'tgt': {'tgt': ...},"
v3.1.3,"'src_original': ['tok1', ...'tokn'],"
v3.1.3,"'tgt_original': ['tok1', ...'tokm'],"
v3.1.3,'indices' : seq in bucket
v3.1.3,"'align': ...,"
v3.1.3,}
v3.1.3,Need to add features in last dimensions
v3.1.3,Keep it consistent with dynamic data
v3.1.3,make a small vocab containing just the tokens in the source sequence
v3.1.3,Map source tokens to indices in the dynamic dict.
v3.1.3,-*- coding: utf-8 -*-
v3.1.3,'src_original' and 'tgt_original' store the
v3.1.3,original line before tokenization. These
v3.1.3,fields are used later on in the feature
v3.1.3,transforms.
v3.1.3,NOTE: moved to dynamic_iterator.py cf process()
v3.1.3,item = self.transform.apply(
v3.1.3,"example, is_train=self.infinitely, corpus_name=self.cid)"
v3.1.3,empty example: skip
v3.1.3,"No encoder in LM, seq2seq count formatting kept"
v3.1.3,_check_save_model_path
v3.1.3,!/usr/bin/env python
v3.1.3,!/usr/bin/env python
v3.1.3,!/usr/bin/env python
v3.1.3,-*- coding: utf-8 -*-
v3.1.3,!/usr/bin/env python
v3.1.3,BPE training
v3.1.3,SentencePiece training
v3.1.3,!/usr/bin/env python
v3.1.3,!/usr/bin/env python
v3.1.3,Set sharing strategy manually instead of default based on the OS.
v3.1.3,torch.multiprocessing.set_sharing_strategy('file_system')
v3.1.3,Create a thread to listen for errors in the child processes.
v3.1.3,Train with multiprocessing.
v3.1.3,magic indices
v3.1.3,result caching
v3.1.3,Here we set the decoder to start with self.start (BOS or EOS)
v3.1.3,fix length constraint and remove eos from count
v3.1.3,add one to account for BOS. Don't account for EOS because hitting
v3.1.3,this implies it hasn't been found.
v3.1.3,we don't block nothing if the user doesn't want it
v3.1.3,we can't block nothing beam's too short
v3.1.3,we check paths one by one
v3.1.3,we don't forbid nothing if the user doesn't want it
v3.1.3,we can't forbid nothing if beam's too short
v3.1.3,Reordering forbidden_tokens following beam selection
v3.1.3,We rebuild a dict to ensure we get the value and not the pointer
v3.1.3,Grabing the newly selected tokens and associated ngram
v3.1.3,skip the blocking if any token in current_ngram is excluded
v3.1.3,"pickups: Tensor where specified index were set to 1, others 0"
v3.1.3,"dropdowns: opposite of pickups, 1 for those shouldn't pick"
v3.1.3,Minus dropdowns to log_probs making probabilities of
v3.1.3,unspecified index close to 0
v3.1.3,"prediction step have surpass length of given target_prefix,"
v3.1.3,no need to further change this attr
v3.1.3,keep indices until overflowing p
v3.1.3,Set all logits that are not in the top-p to -10000.
v3.1.3,This puts the probabilities close to 0.
v3.1.3,Set all logits that are not in the top-k to -10000.
v3.1.3,This puts the probabilities close to 0.
v3.1.3,"For temp=0.0, take the argmax to avoid divide-by-zero errors."
v3.1.3,keep_topk=1 is also equivalent to argmax.
v3.1.3,maybe fix some prediction at this step by modifying log_probs
v3.1.3,"shape: (sum(~ self.is_finished), 1)"
v3.1.3,in LM task src_len is associated with currently generated src
v3.1.3,and therefore needs to follow the generation
v3.1.3,!/usr/bin/env python
v3.1.3,for debugging
v3.1.3,TODO: maybe add dynamic part
v3.1.3,Statistics
v3.1.3,those two should be the same except feat dim
v3.1.3,"batch['src'][perm[j], :, :])"
v3.1.3,trans.src
v3.1.3,we rebuild a small batch made of the sub-segments
v3.1.3,in the long segment.
v3.1.3,new sub-batch ready to be translated
v3.1.3,we re-insert the sub-batch in the initial translations
v3.1.3,In the case of length_penalty = none we report the total logprobs
v3.1.3,divided by the number of sentence to get an approximation of the
v3.1.3,per sentence logprob. We also return the corresponding ppl
v3.1.3,"When a length_penalty is used eg: ""avg"" or ""wu"" since logprobs"
v3.1.3,are normalized per token we report the per line per token logprob
v3.1.3,"and the corresponding ""per word perplexity"""
v3.1.3,Turn any copied words into UNKs.
v3.1.3,"Decoder forward, takes [batch, tgt_len, nfeats] as input"
v3.1.3,"and [batch, src_len, hidden] as enc_out"
v3.1.3,"in case of inference tgt_len = 1, batch = beam times batch_size"
v3.1.3,"in case of Gold Scoring tgt_len = actual length, batch = 1 batch"
v3.1.3,Generator forward.
v3.1.3,"returns [(batch_size x beam_size) , vocab ] when 1 step"
v3.1.3,"or [batch_size, tgt_len, vocab ] when full sentence"
v3.1.3,"here we have scores [tgt_lenxbatch, vocab] or [beamxbatch, vocab]"
v3.1.3,at this point scores is batch first (dim=0)
v3.1.3,"returns [(batch_size x beam_size) , vocab ] when 1 step"
v3.1.3,"or [batch_size, tgt_len, vocab ] when full sentence"
v3.1.3,(0) add BOS and padding to tgt prediction
v3.1.3,(1) Encoder forward.
v3.1.3,(2) Repeat src objects `n_best` times.
v3.1.3,"We use batch_size x n_best, get ``(batch * n_best, src_len, nfeat)``"
v3.1.3,Quick fix. Transformers return None as enc_states.
v3.1.3,enc_states are only used later on to init decoder's state
v3.1.3,"but are never used in Transformer decoder, so we can skip"
v3.1.3,"(3) Init decoder with n_best src,"
v3.1.3,"reshape tgt to ``(len, batch * n_best, nfeat)``"
v3.1.3,it should be done in a better way
v3.1.3,here dec_in is batch first
v3.1.3,masked_select
v3.1.3,get aligned src id for each prediction's valid tgt tokens
v3.1.3,TODO: support these blacklisted features
v3.1.3,(0) Prep the components of the search.
v3.1.3,(1) Run the encoder on the src.
v3.1.3,(2) prep decode_strategy. Possibly repeat src objects.
v3.1.3,(3) Begin decoding step by step:
v3.1.3,"decoder_input = decode_strategy.current_predictions.view(1, -1,"
v3.1.3,1)
v3.1.3,Reorder states.
v3.1.3,TODO: support these blacklisted features
v3.1.3,(0) Prep the components of the search.
v3.1.3,(1) split src into src and target_prefix to avoid padding.
v3.1.3,(2) init decoder
v3.1.3,(3) prep decode_strategy. Possibly repeat src objects.
v3.1.3,(4) Begin decoding step by step:
v3.1.3,Reorder states.
v3.1.3,select indexes in model state/cache
v3.1.3,beam parameters
v3.1.3,beam state
v3.1.3,BoolTensor was introduced in pytorch 1.2
v3.1.3,"""global state"" of the old beam"
v3.1.3,buffers for the topk scores and 'backpointer'
v3.1.3,for testing
v3.1.3,maybe fix some prediction at this step by modifying log_probs
v3.1.3,Flatten probs into a list of possibilities.
v3.1.3,Penalize beams that finished.
v3.1.3,"on real data (newstest2017) with the pretrained transformer,"
v3.1.3,it's faster to not move this back to the original device
v3.1.3,Store finished hypotheses for this batch.
v3.1.3,End condition is the top beam finished and we can return
v3.1.3,n_best hypotheses.
v3.1.3,"If all sentences are translated, no need to go further."
v3.1.3,Remove finished batches for the next step.
v3.1.3,using integer division to get an integer _B without casting
v3.1.3,force the output to be longer than self.min_length
v3.1.3,Multiply probs by the beam probability.
v3.1.3,"if the sequence ends now, then the penalty is the current"
v3.1.3,"length + 1, to include the EOS token"
v3.1.3,Avoid any direction that would repeat unwanted ngrams
v3.1.3,Pick up candidate token by curr_scores
v3.1.3,Recover log probs.
v3.1.3,Length penalty is just a scalar. It doesn't matter if it's applied
v3.1.3,before or after the topk.
v3.1.3,Resolve beam origin and map to batch index flat representation.
v3.1.3,Append last prediction.
v3.1.3,update global state (step == 1)
v3.1.3,update global state (step > 1)
v3.1.3,"shape: (batch_size x beam_size, 1)"
v3.1.3,in LM task src_len is associated with currently generated src
v3.1.3,and therefore needs to follow the generation
v3.1.3,in LM task src_len is associated with currently generated src
v3.1.3,and therefore needs to follow the generation
v3.1.3,Term will be subtracted from probability
v3.1.3,Probability will be divided by this
v3.1.3,these warnings indicate that either the alpha/beta
v3.1.3,"forces a penalty to be a no-op, or a penalty is a no-op but"
v3.1.3,the alpha/beta would suggest otherwise.
v3.1.3,using some coverage penalty
v3.1.3,!/usr/bin/env python
v3.1.3,semaphore doesn't have a timeout arg in Python 2.7
v3.1.3,perform a first request to initialize everything
v3.1.3,backwards compatibility for confs
v3.1.3,every segment becomes a dict for flexibility purposes
v3.1.3,NOTE: translator returns lists of `n_best` list
v3.1.3,build back results with empty texts
v3.1.3,load can be called multiple times: modify copy
v3.1.3,output contain alignment
v3.1.3,Below are all the different penalty terms implemented so far.
v3.1.3,Subtract coverage penalty from topk log probs.
v3.1.3,Divide topk log probs by length penalty.
v3.1.3,Sorting
v3.1.3,Chinese segmentation
v3.1.3,Chinese simplify -> Chinese traditional standard
v3.1.3,Chinese simplify -> Chinese traditional (HongKong)
v3.1.3,Chinese simplify -> Chinese traditional (Taiwan)
v3.1.3,Chinese traditional -> Chinese simplify (v1)
v3.1.3,Chinese traditional -> Chinese simplify (v2)
v3.1.3,Auto import python files in this directory
v3.1.2,!/usr/bin/env python
v3.1.2,!/usr/bin/env python
v3.1.2,!/usr/bin/env python
v3.1.2,!/usr/bin/env python
v3.1.2,!/usr/bin/env python
v3.1.2,!/usr/bin/env python3
v3.1.2,-*- coding: utf-8 -*-
v3.1.2,
v3.1.2,"OpenNMT-py documentation build configuration file, created by"
v3.1.2,sphinx-quickstart on Sun Dec 17 12:07:14 2017.
v3.1.2,
v3.1.2,This file is execfile()d with the current directory set to its
v3.1.2,containing dir.
v3.1.2,
v3.1.2,Note that not all possible configuration values are present in this
v3.1.2,autogenerated file.
v3.1.2,
v3.1.2,All configuration values have a default; values that are commented out
v3.1.2,serve to show the default.
v3.1.2,"If extensions (or modules to document with autodoc) are in another directory,"
v3.1.2,add these directories to sys.path here. If the directory is relative to the
v3.1.2,"documentation root, use os.path.abspath to make it absolute, like shown here."
v3.1.2,
v3.1.2,import os
v3.1.2,import sys
v3.1.2,"sys.path.insert(0, os.path.abspath('.'))"
v3.1.2,-- General configuration ------------------------------------------------
v3.1.2,"If your documentation needs a minimal Sphinx version, state it here."
v3.1.2,
v3.1.2,needs_sphinx = '6.0'
v3.1.2,"Add any Sphinx extension module names here, as strings. They can be"
v3.1.2,extensions coming with Sphinx (named 'sphinx.ext.*') or your custom
v3.1.2,ones.
v3.1.2,Show base classes
v3.1.2,"Use ""variables"" section for Attributes instead of weird block things"
v3.1.2,mimicking the function style.
v3.1.2,"Add any paths that contain templates here, relative to this directory."
v3.1.2,The suffix(es) of source filenames.
v3.1.2,You can specify multiple suffix as a list of string:
v3.1.2,
v3.1.2,"source_suffix = ['.rst', '.md']"
v3.1.2,The master toctree document.
v3.1.2,General information about the project.
v3.1.2,"The version info for the project you're documenting, acts as replacement for"
v3.1.2,"|version| and |release|, also used in various other places throughout the"
v3.1.2,built documents.
v3.1.2,
v3.1.2,The short X.Y version.
v3.1.2,"The full version, including alpha/beta/rc tags."
v3.1.2,The language for content autogenerated by Sphinx. Refer to documentation
v3.1.2,for a list of supported languages.
v3.1.2,
v3.1.2,This is also used if you do content translation via gettext catalogs.
v3.1.2,"Usually you set ""language"" from the command line for these cases."
v3.1.2,"List of patterns, relative to source directory, that match files and"
v3.1.2,directories to ignore when looking for source files.
v3.1.2,This patterns also effect to html_static_path and html_extra_path
v3.1.2,The name of the Pygments (syntax highlighting) style to use.
v3.1.2,"If true, `todo` and `todoList` produce output, else they produce nothing."
v3.1.2,-- Options for HTML output ----------------------------------------------
v3.1.2,The theme to use for HTML and HTML Help pages.  See the documentation for
v3.1.2,a list of builtin themes.
v3.1.2,
v3.1.2,html_theme = 'sphinx_materialdesign_theme'
v3.1.2,html_theme_path = [sphinx_materialdesign_theme.get_path()]
v3.1.2,Theme options are theme-specific and customize the look and feel of a theme
v3.1.2,"further.  For a list of options available for each theme, see the"
v3.1.2,documentation.
v3.1.2,
v3.1.2,html_theme_options = {}
v3.1.2,"Add any paths that contain custom static files (such as style sheets) here,"
v3.1.2,"relative to this directory. They are copied after the builtin static files,"
v3.1.2,"so a file named ""default.css"" will overwrite the builtin ""default.css""."
v3.1.2,"Custom sidebar templates, must be a dictionary that maps document names"
v3.1.2,to template names.
v3.1.2,
v3.1.2,This is required for the alabaster theme
v3.1.2,refs: http://alabaster.readthedocs.io/en/latest/installation.html#sidebars
v3.1.2,-- Options for HTMLHelp output ------------------------------------------
v3.1.2,Output file base name for HTML help builder.
v3.1.2,-- Options for LaTeX output ---------------------------------------------
v3.1.2,The paper size ('letterpaper' or 'a4paper').
v3.1.2,
v3.1.2,"'papersize': 'letterpaper',"
v3.1.2,"The font size ('10pt', '11pt' or '12pt')."
v3.1.2,
v3.1.2,"'pointsize': '10pt',"
v3.1.2,Additional stuff for the LaTeX preamble.
v3.1.2,
v3.1.2,"'preamble': '',"
v3.1.2,Latex figure (float) alignment
v3.1.2,
v3.1.2,"'figure_align': 'htbp',"
v3.1.2,Grouping the document tree into LaTeX files. List of tuples
v3.1.2,"(source start file, target name, title,"
v3.1.2,"author, documentclass [howto, manual, or own class])."
v3.1.2,-- Options for manual page output ---------------------------------------
v3.1.2,One entry per manual page. List of tuples
v3.1.2,"(source start file, name, description, authors, manual section)."
v3.1.2,-- Options for Texinfo output -------------------------------------------
v3.1.2,Grouping the document tree into Texinfo files. List of tuples
v3.1.2,"(source start file, target name, title, author,"
v3.1.2,"dir menu entry, description, category)"
v3.1.2,!/usr/bin/env python3
v3.1.2,Usage: python3 filter_train.py in.src in.trg out.src out.trg max-tokens
v3.1.2,!/usr/bin/env python
v3.1.2,-*- coding: utf-8 -*-
v3.1.2,is this reachable?
v3.1.2,Read in embeddings
v3.1.2,Write to file
v3.1.2,converts a SentencePiece vocabulary to the format expected by dynamic data
v3.1.2,"(essentially converts float expected counts to ""fixed precision"" int pseudo"
v3.1.2,counts)
v3.1.2,from onmt.utils.misc import use_gpu
v3.1.2,"Add in default model arguments, possibly added since training."
v3.1.2,this patch is no longer needed included in converter
v3.1.2,"if hasattr(model_opt, 'rnn_size'):"
v3.1.2,model_opt.hidden_size = model_opt.rnn_size
v3.1.2,build_base_model expects updated and validated opts
v3.1.2,-*- encoding: utf-8 -*-
v3.1.2,!/usr/bin/env python
v3.1.2,-*- coding: utf-8 -*-
v3.1.2,Author: Rico Sennrich
v3.1.2,flake8: noqa
v3.1.2,This file is retrieved from https://github.com/rsennrich/subword-nmt
v3.1.2,hack for python2/3 compatibility
v3.1.2,check version information
v3.1.2,some hacking to deal with duplicates (only consider first instance)
v3.1.2,don't print end-of-word symbols
v3.1.2,sys.stderr.write('cannot split {0} further.\n'.format(segment))
v3.1.2,sys.stderr.write('OOV: {0}\n'.format(segment))
v3.1.2,sys.stderr.write('OOV: {0}\n'.format(segment))
v3.1.2,python 2/3 compatibility
v3.1.2,read/write files as UTF-8
v3.1.2,!/usr/bin/env python3
v3.1.2,coding: utf-8
v3.1.2,"In order to use this tool, please install comet first"
v3.1.2,https://github.com/Unbabel/COMET
v3.1.2,Let's say you have a source file with N sentences in SL - eg: source.sl
v3.1.2,and the corresponding references (N sentences) reference.tl
v3.1.2,Translate your file in TL with the -n_best nbest options nbest being
v3.1.2,then number of hypotheses and output the target to -output target.nbest.tl
v3.1.2,Then you need to duplicate source and reference sentences nbest times
v3.1.2,for this script.
v3.1.2,for instance using awk '{for(i=1; i<=n; i++) print}' n=5 reference.tl \
v3.1.2,> reference.5.tl
v3.1.2,same for source.
v3.1.2,This script can be run (for instance with nbest = 5) as follows:
v3.1.2,python oracle_bleu.py --nbest-src source.5.sl --nbest-hyp target.5.tl \
v3.1.2,--nbest-ref reference.5.tl --nbest-order 5 --output target.maxbleu.tl
v3.1.2,It will search in all hyp the best comet score
v3.1.2,when choosing a reference-less model no nbest-ref is required
v3.1.2,for nbest in nbests:
v3.1.2,!/usr/bin/env python
v3.1.2,!/usr/bin/env python3
v3.1.2,coding: utf-8
v3.1.2,Let's say you have a source file with N sentences in SL - eg: source.sl
v3.1.2,Translate your file in TL with the -n_best nbest options nbest being
v3.1.2,then number of hypotheses and output the target to -output target.nbest.tl
v3.1.2,This script can be run (for instance with nbest = 5) as follows:
v3.1.2,python mbr_bleu.py --nbest-hyp target.5.tl \
v3.1.2,--nbest-order 5 --output target.mbr.tl
v3.1.2,It will compare all hyp with eachother and output the max bleu
v3.1.2,!/usr/bin/env python
v3.1.2,-*- coding: utf-8 -*-
v3.1.2,Author: Rico Sennrich
v3.1.2,flake8: noqa
v3.1.2,This file is retrieved from https://github.com/rsennrich/subword-nmt
v3.1.2,hack for python2/3 compatibility
v3.1.2,"find all instances of pair, and update frequency/indices around it"
v3.1.2,find first symbol
v3.1.2,"if first symbol is followed by second symbol, we've found an occurrence of pair (old_word[i:i+2])"
v3.1.2,"assuming a symbol sequence ""A B C"", if ""B C"" is merged, reduce the frequency of ""A B"""
v3.1.2,"assuming a symbol sequence ""A B C B"", if ""B C"" is merged, reduce the frequency of ""C B""."
v3.1.2,"however, skip this if the sequence is A B C B C, because the frequency of ""C B"" will be reduced by the previous code block"
v3.1.2,find new pair
v3.1.2,"assuming a symbol sequence ""A BC D"", if ""B C"" is merged, increase the frequency of ""A BC"""
v3.1.2,"assuming a symbol sequence ""A BC B"", if ""B C"" is merged, increase the frequency of ""BC B"""
v3.1.2,"however, if the sequence is A BC BC, skip this step because the count of ""BC BC"" will be incremented by the previous code block"
v3.1.2,data structure of pair frequencies
v3.1.2,index from pairs to words
v3.1.2,version 0.2 changes the handling of the end-of-word token ('</w>');
v3.1.2,version numbering allows bckward compatibility
v3.1.2,"threshold is inspired by Zipfian assumption, but should only affect speed"
v3.1.2,we probably missed the best pair because of pruning; go back to full statistics
v3.1.2,"threshold is inspired by Zipfian assumption, but should only affect speed"
v3.1.2,python 2/3 compatibility
v3.1.2,read/write files as UTF-8
v3.1.2,Now we can pipe the full file through the model using the Iterator
v3.1.2,reminder a batch includes .src .tgt .indices and it is sorted
v3.1.2,Compute and retrieve the loss for EACH sentence
v3.1.2,Now we need to rearrange the batch of ppl
v3.1.2,in the original order with indices
v3.1.2,!/usr/bin/env python
v3.1.2,-*- coding: utf-8 -*-
v3.1.2,!/usr/bin/env python
v3.1.2,flake8: noqa
v3.1.2,!/usr/bin/env python
v3.1.2,!/usr/bin/env python3
v3.1.2,coding: utf-8
v3.1.2,Let's say you have a source file with N sentences in SL - eg: source.sl
v3.1.2,and the corresponding references (N sentences) reference.tl
v3.1.2,Translate your file in TL with the -n_best nbest options nbest being
v3.1.2,then number of hypotheses and output the target to -output target.nbest.tl
v3.1.2,Then you need to duplicate reference sentences nbest times for this script.
v3.1.2,for instance using awk '{for(i=1; i<=n; i++) print}' n=5 reference.tl \
v3.1.2,> reference.5.tl
v3.1.2,This script can be run (for instance with nbest = 5) as follows:
v3.1.2,python oracle_bleu.py --nbest-hyp target.5.tl --nbest-ref reference.5.tl \
v3.1.2,--nbest-order 5 --output target.maxbleu.tl
v3.1.2,It will search in all hyp the best bleu wrt reference
v3.1.2,and output the max bleu
v3.1.2,!/usr/bin/env python
v3.1.2,with the two module = imp.load_source() below
v3.1.2,we ghost the old torchtext.data.field and depercated
v3.1.2,onmt.inputters.text_dataset
v3.1.2,however this require some functions / classes to be
v3.1.2,monkey patched for loading the old field/vocab objects.
v3.1.2,"module3 = imp.load_source(""Vocab"", ""tools/convertv2_v3.py"")"
v3.1.2,"voc = dict(sorted(fields.vocab.__dict__['freqs'].items(),"
v3.1.2,"key=lambda x: (-x[1], x[0]))).keys()"
v3.1.2,"voc = dict(sorted(fields.vocab.__dict__['freqs'].items(),"
v3.1.2,"key=lambda x: (-x[1], x[0]))).keys()"
v3.1.2,Avoid functionality on inference
v3.1.2,Build embeddings.
v3.1.2,Build encoder.
v3.1.2,Build embeddings.
v3.1.2,Build decoder.
v3.1.2,Share the embedding matrix - preprocess with share_vocab required.
v3.1.2,src/tgt vocab should be the same if `-share_vocab` is specified.
v3.1.2,Update vocabulary embeddings with checkpoint embeddings
v3.1.2,Embedding layers
v3.1.2,Just for debugging purposes
v3.1.2,Remove old vocabulary associated embeddings
v3.1.2,for back compat when attention_dropout was not defined
v3.1.2,Build Model
v3.1.2,Build Generator.
v3.1.2,Load the model states from checkpoint or initialize them.
v3.1.2,This preserves backward-compat for models using customed layernorm
v3.1.2,end of patch for backward compatibility
v3.1.2,Update model embeddings with those from the checkpoint
v3.1.2,after initialization
v3.1.2,when using LoRa or updating the vocab (no more embeddings in ckpt)
v3.1.2,=> strict=False when loading state_dict
v3.1.2,!/usr/bin/env python
v3.1.2,if transform + options set in 'valid' we need to copy in main
v3.1.2,transform / options for scoring considered as inference
v3.1.2,"maybe prepare pretrained embeddings, if any"
v3.1.2,Load checkpoint if we resume from a previous training.
v3.1.2,ensure tensorboard output is written in the directory
v3.1.2,of previous checkpoints
v3.1.2,Override checkpoint's update_embeddings as it defaults to false
v3.1.2,Override checkpoint's freezing settings as it defaults to false
v3.1.2,NOTE: It's important that ``opt`` has been validated and updated
v3.1.2,at this point.
v3.1.2,Build model.
v3.1.2,Build optimizer.
v3.1.2,Build model saver
v3.1.2,Use Tensorboard for visualization during training
v3.1.2,Options only during inference
v3.1.2,"Truncation options, for text corpus"
v3.1.2,"as for False, this will be added in _add_train_general_opts"
v3.1.2,Embedding Options
v3.1.2,Model Task Options
v3.1.2,Encoder-Decoder Options
v3.1.2,Freeze Encoder and/or Decoder
v3.1.2,The following options (bridge_extra_node to n_steps) are used
v3.1.2,for training with --encoder_type ggnn (Gated Graph Neural Network).
v3.1.2,Attention options
v3.1.2,Alignement options
v3.1.2,Generator and loss options.
v3.1.2,GPU
v3.1.2,LoRa
v3.1.2,Init options
v3.1.2,Pretrained word vectors
v3.1.2,Freeze word vectors
v3.1.2,Optimization options
v3.1.2,learning rate
v3.1.2,options relate to data preprare
v3.1.2,options relate to train
v3.1.2,Alpha and Beta values for Google Length + Coverage penalty
v3.1.2,"Described here: https://arxiv.org/pdf/1609.08144.pdf, Section 7"
v3.1.2,Length penalty options
v3.1.2,Coverage penalty options
v3.1.2,Decoding Length constraint
v3.1.2,Decoding content constraint
v3.1.2,Adding options related to source and target features
v3.1.2,Adding options relate to decoding strategy
v3.1.2,Adding option for logging
v3.1.2,Adding options related to Transforms
v3.1.2,Copyright 2016 The Chromium Authors. All rights reserved.
v3.1.2,Use of this source code is governed by a BSD-style license that can be
v3.1.2,found in the LICENSE file.
v3.1.2,"Get the key 'value' in the dict, or just use 'value'"
v3.1.2,Basic attributes.
v3.1.2,Set model in training mode.
v3.1.2,Let's clean the GPUs before training loop
v3.1.2,UPDATE DROPOUT
v3.1.2,Run patience mechanism
v3.1.2,"If the patience has reached the limit, stop training"
v3.1.2,swap model params w/ moving average
v3.1.2,(and keep the original parameters)
v3.1.2,Set model in validating mode.
v3.1.2,F-prop through the model.
v3.1.2,Compute loss.
v3.1.2,Compute validation metrics (at batch.dataset level)
v3.1.2,Compute stats
v3.1.2,Update statistics.
v3.1.2,Set model back to training mode.
v3.1.2,Truncated BPTT: reminder not compatible with accum > 1
v3.1.2,1. Create truncated target.
v3.1.2,2. F-prop all but generator.
v3.1.2,3. Compute loss.
v3.1.2,Compute and save stats
v3.1.2,"If truncated, don't backprop fully."
v3.1.2,"in case of multi step gradient accumulation,"
v3.1.2,update only after accum batches
v3.1.2,For Flake
v3.1.2,we avoid padding while mean pooling
v3.1.2,incoming and outgoing edge embedding
v3.1.2,Find vocab data for tree builting
v3.1.2,Propogation Model
v3.1.2,Initialize the bridge layer
v3.1.2,Token embedding
v3.1.2,Initialize graph using formatted input sequence
v3.1.2,Number of flagged nodes defines node count for this sample
v3.1.2,"(Nodes can have no flags on them, but must be in 'flags' list)."
v3.1.2,The total number of integers in the vocab should allow
v3.1.2,for all features and edges to be defined.
v3.1.2,Use first extra node as only source for decoder init
v3.1.2,Average all nodes to get bridge input
v3.1.2,"LSTM has hidden and cell state, other only one"
v3.1.2,Total number of states
v3.1.2,Build a linear layer for each
v3.1.2,Initialize the bridge layer
v3.1.2,src lengths data is wrapped inside a Tensor.
v3.1.2,"LSTM has hidden and cell state, other only one"
v3.1.2,Total number of states
v3.1.2,Build a linear layer for each
v3.1.2,batch x len x dim
v3.1.2,mask is now (batch x 1 x slen x slen)
v3.1.2,1 to be expanded to number of heads in MHA
v3.1.2,Run the forward pass of every layer of the tranformer.
v3.1.2,Dimensions and padding for constructing the word embedding matrix
v3.1.2,Dimensions and padding for feature embedding matrices
v3.1.2,(these have no effect if feat_vocab_sizes is empty)
v3.1.2,The embedding matrix look-up tables. The first look-up table
v3.1.2,"is for words. Subsequent ones are for features, if any exist."
v3.1.2,The final output size of word + feature vectors. This can vary
v3.1.2,from the word vector size if and only if features are defined.
v3.1.2,This is the attribute you should access if you need to know
v3.1.2,how big your embeddings are going to be.
v3.1.2,The sequence of operations that converts the input sequence
v3.1.2,into a sequence of embeddings. At minimum this consists of
v3.1.2,looking up the embeddings for each word and feature in the
v3.1.2,input. Model parameters may require the sequence to contain
v3.1.2,additional operations as well.
v3.1.2,features must use word_vec_size
v3.1.2,features will use feat_vec_size
v3.1.2,Some utilitary functions for pretrained embeddings
v3.1.2,is this reachable?
v3.1.2,Write to file
v3.1.2,set the opt in place
v3.1.2,set the opt in place
v3.1.2,flake8: noqa
v3.1.2,For command-line option parsing
v3.1.2,"Check pass, set the args."
v3.1.2,"This SRU version implements its own cuda-level optimization,"
v3.1.2,so it requires that:
v3.1.2,1. `cupy` and `pynvrtc` python package installed.
v3.1.2,2. pytorch is built with cuda support.
v3.1.2,3. library path set: export LD_LIBRARY_PATH=<cuda lib path>.
v3.1.2,Check 1.
v3.1.2,Check 2.
v3.1.2,Check 3.
v3.1.2,This sets up device to use.
v3.1.2,-> directions x batch x dim
v3.1.2,For DEBUG
v3.1.2,"size = (length, batch, x.size(-1)) \"
v3.1.2,"if x.dim() == 3 else (batch, x.size(-1))"
v3.1.2,grad_x = x.new(*x.size()) if k_ == 3 else x.new(*size).zero_()
v3.1.2,Normal use
v3.1.2,"An entry check here, will catch on train side and translate side"
v3.1.2,if requirements are not satisfied.
v3.1.2,RNNDecoderState wraps hidden as a tuple.
v3.1.2,fh -> (layers*directions) x batch x dim
v3.1.2,This class is mainly used by decoder.py for RNNs but also
v3.1.2,by the CNN / transformer decoder when copy attention is used
v3.1.2,CNN has its own attention mechanism ConvMultiStepAttention
v3.1.2,Transformer has its own MultiHeadedAttention
v3.1.2,mlp wants it with bias
v3.1.2,"(batch, t_len, d) x (batch, d, s_len) --> (batch, t_len, s_len)"
v3.1.2,"(batch, t_len, s_len, d)"
v3.1.2,one step input
v3.1.2,"compute attention scores, as in Luong et al."
v3.1.2,Softmax or sparsemax to normalize attention weights
v3.1.2,each context vector c_t is the weighted average
v3.1.2,over all the source hidden states
v3.1.2,concatenate
v3.1.2,clamping necessary because of numerical errors: loss should be lower
v3.1.2,"bounded by zero, but negative values near zero are possible without"
v3.1.2,the clamp
v3.1.2,Help functions for Rotary Embeddings
v3.1.2,https://arxiv.org/pdf/2104.09864.pdf
v3.1.2,too convoluted to make maxseqlen a parameter.
v3.1.2,we suppose src_seq_len at training and max_length at inference
v3.1.2,are both < 2048 tokens.
v3.1.2,"rope is now matrix [maxseqlen, dim/2]"
v3.1.2,Help functions for max_relative positions
v3.1.2,https://arxiv.org/abs/1803.02155
v3.1.2,Shift values to be >= 0
v3.1.2,Help functions to split model dim per head
v3.1.2,class MultiHeadedAttention(torch.jit.ScriptModule):
v3.1.2,https://arxiv.org/pdf/1803.02155.pdf
v3.1.2,in the paper they suggest either two embeds
v3.1.2,relative_key / relative_value or only
v3.1.2,relative_key. We implemented the same embed
v3.1.2,for both.
v3.1.2,@torch.jit.script_method
v3.1.2,"1) Project key, value, and query."
v3.1.2,as a reminder at training layer_cache[0] remains False
v3.1.2,2) Calculate and scale scores.
v3.1.2,batch x num_heads x query_len x key_len
v3.1.2,1 or key_len x key_len
v3.1.2,1 or key_len x key_len x dim_per_head
v3.1.2,not 100% necessary but expand to nb of heads
v3.1.2,now mask and scores have the same shape
v3.1.2,3) Apply attention dropout and compute context vectors.
v3.1.2,We use the same embeddings for key and value
v3.1.2,--------------------------------------------------------------------------
v3.1.2,Mostly copied from https://github.com/microsoft/LoRA/
v3.1.2,Copyright (c) Microsoft Corporation. All rights reserved.
v3.1.2,Licensed under the MIT License (MIT).
v3.1.2,
v3.1.2,--------------------------------------------------------------------------
v3.1.2,Optional dropout
v3.1.2,Mark the weight as unmerged
v3.1.2,LoRA implemented in a dense layer
v3.1.2,Actual trainable parameters
v3.1.2,Freezing the pre-trained weight matrix
v3.1.2,initialize A the same way as the default
v3.1.2,for nn.Linear and B to zero
v3.1.2,Make sure that the weights are not merged
v3.1.2,Merge the weights and mark it
v3.1.2,LoRA implemented in a dense layer
v3.1.2,Set this to True if the layer to replace stores
v3.1.2,"weight like (fan_in, fan_out)"
v3.1.2,Actual trainable parameters
v3.1.2,Freezing the pre-trained weight matrix
v3.1.2,initialize A the same way as the default
v3.1.2,for nn.Linear and B to zero
v3.1.2,Make sure that the weights are not merged
v3.1.2,Merge the weights and mark it
v3.1.2,LoRA implemented in a dense layer
v3.1.2,Actual trainable parameters
v3.1.2,Freezing the pre-trained weight matrix
v3.1.2,Compute the indices
v3.1.2,initialize A the same way as the default
v3.1.2,for nn.Linear and B to zero
v3.1.2,Make sure that the weights are not merged
v3.1.2,Merge the weights and mark it
v3.1.2,At the moment this class is only used by embeddings.Embeddings look-up tables
v3.1.2,"for silu, see: https://arxiv.org/pdf/2002.05202.pdf"
v3.1.2,-*- coding: utf-8 -*-
v3.1.2,class AverageAttention(torch.jit.ScriptModule):
v3.1.2,@torch.jit.script
v3.1.2,out_features * in_features
v3.1.2,norm is out_features * 1
v3.1.2,batch_size * out_features
v3.1.2,out_features
v3.1.2,out_features
v3.1.2,batch_size * out_features
v3.1.2,"out_channels, in_channels // groups, * kernel_size"
v3.1.2,out_features
v3.1.2,This is used nowhere in the code at the moment (Vincent Nguyen 05/18/2018)
v3.1.2,"in_channels, out_channels, *kernel_size"
v3.1.2,"in_channels, out_channels, *kernel_size"
v3.1.2,"self.out_channels, 1"
v3.1.2,out_features
v3.1.2,out_features
v3.1.2,store roots on diagonal
v3.1.2,Original probabilities.
v3.1.2,Probability of copying p(z=1) batch.
v3.1.2,Probability of not copying: p_{word}(w) * (1 - p(z))
v3.1.2,probabilities assigned by the model to the gold targets
v3.1.2,probability of tokens copied from source
v3.1.2,Set scores for unk to 0 and add eps
v3.1.2,find the indices in which you do not use the copy mechanism
v3.1.2,Drop padding.
v3.1.2,Filter out very short or very long sentences
v3.1.2,from the TM for better performance
v3.1.2,We split the `batch` and perform fuzzy matching
v3.1.2,in smaller chunks of 10.000 examples in order to
v3.1.2,reduce memory usage.
v3.1.2,Perfomance is not affected.
v3.1.2,Probably redundant but let's be safe
v3.1.2,in case some examples are already fuzzied
v3.1.2,(e.g. from another pipeline or workflow)
v3.1.2,We don't want exact matches
v3.1.2,Apply a basic filtering to leave out very short or very long
v3.1.2,sentences and speed up things a bit during fuzzy matching
v3.1.2,Do nothing
v3.1.2,We set the start number of tags to a random number from 1
v3.1.2,to 12 + the number of subsequent tags that
v3.1.2,will be added. We also apply weights to this choice so tags
v3.1.2,"are more probable to start from 1, then from 2, etc."
v3.1.2,This way we cover most scenarios met in real usage and
v3.1.2,the system will learn to handle a fairly large number of
v3.1.2,numbered tags (but not an excessively large number)
v3.1.2,Make sure we only search for exact matches (we don't want
v3.1.2,to match part of words) and perform some bound checking
v3.1.2,Create all possible tag forms. We inject a special
v3.1.2,unicode char (âˆ¥) as a placeholder for whitespace in order
v3.1.2,to keep the indices unaltered. This char is replaced with
v3.1.2,spaces before we return the augmented examples.
v3.1.2,Make a weighted choice between paired tags or single tags.
v3.1.2,"We usually encounter, and thus here we favor, paired tags"
v3.1.2,with a ratio 1/3.
v3.1.2,Check if the tags include the
v3.1.2,"mandatory ""#"" number placeholder"""
v3.1.2,We split the user-defined tags in the # placeholder
v3.1.2,in order to number them
v3.1.2,normalize dict src/tgt for each dataset
v3.1.2,"print(""src empty"")"
v3.1.2,"print(""too many same char in src"")"
v3.1.2,"print(""too many same word in src"")"
v3.1.2,"print(""avg token min"", len(src_str) / len(ex['src']))"
v3.1.2,"print(""avg token max"", len(src_str) / len(ex['src']))"
v3.1.2,"print(""text does not fully belong to wanted script"")"
v3.1.2,"print(""Some text belong to unwanted scripts"")"
v3.1.2,"print(""langid does not match"", _id(src_str))"
v3.1.2,"print(""src = tgt"")"
v3.1.2,"print(""tgt empty"")"
v3.1.2,"print(""src / tgt ratio "", len(src_str) / len(tgt_str))"
v3.1.2,"print(""too many same char in tgt"")"
v3.1.2,"print(""too many same word in tgt"")"
v3.1.2,"print(""avg token min"", len(tgt_str) / len(ex['tgt']))"
v3.1.2,"print(""avg token max"", len(tgt_str) / len(ex['tgt']))"
v3.1.2,"print(""text does not fully belong to wanted script"")"
v3.1.2,"print(""Some text belong to unwanted scripts"")"
v3.1.2,"print(""langid does not match"", _id(tgt_str))"
v3.1.2,"doc break we add it, restart new doc"
v3.1.2,case 1st ex is already longer
v3.1.2,adding cur ex is too long we add cur doc
v3.1.2,and reset doc to cur ex
v3.1.2,we start the new doc with cur ex
v3.1.2,we cumulate cur ex to cur doc
v3.1.2,Auto import python files in this directory
v3.1.2,1. sample number of tokens to corrupt
v3.1.2,2. sample positions to corrput
v3.1.2,3. sample corrupted values
v3.1.2,1. sample number of tokens to corrupt
v3.1.2,2. sample positions to corrput
v3.1.2,3. Drop token on chosen position
v3.1.2,1. sample number of tokens to corrupt
v3.1.2,2. sample positions to corrput
v3.1.2,3. mask word on chosen position
v3.1.2,"Sharing options among `TokenizerTransform`s, same name conflict in"
v3.1.2,this scope will be resolved by remove previous occurrence in parser
v3.1.2,subword regularization(or BPE dropout) options:
v3.1.2,subword vocabulary restriction options:
v3.1.2,derterministic subwording
v3.1.2,subword sampling when nbest_size > 1 or -1
v3.1.2,alpha should be 0.0 < alpha < 1.0
v3.1.2,Load vocabulary file if provided and set threshold
v3.1.2,Load Subword Model
v3.1.2,-1: keep everything (i.e. 1 mask per token)
v3.1.2,0: replace everything (i.e. no mask)
v3.1.2,1: 1 mask per span
v3.1.2,view each subword as word start / input is word level token
v3.1.2,Pretend it ends with a full stop so last span is a sentence
v3.1.2,"Tokens that are full stops, where the previous token is not"
v3.1.2,Make sure we have enough to mask
v3.1.2,Trim to masking budget
v3.1.2,Handle 0-length mask (inserts) separately
v3.1.2,assert is_word_start[-1] == 0
v3.1.2,assert tokens_length - 1 not in indices
v3.1.2,"keep index, but replace it with [MASK]"
v3.1.2,"acts as a long length, so spans don't go over the end of doc"
v3.1.2,next position from each word_start
v3.1.2,delete token: 1 mask/remove per span
v3.1.2,"keep index, but replace it with [MASK]: 1 mask per token"
v3.1.2,A bit faster when all lengths are 1
v3.1.2,to cover whole token
v3.1.2,delete token
v3.1.2,"keep index, but replace it with [MASK]"
v3.1.2,assert tokens_length - 1 not in indices
v3.1.2,prefix src/tgt for each dataset
v3.1.2,prefix as general option for inference
v3.1.2,suffix src/tgt for each dataset
v3.1.2,suffix as general option for inference
v3.1.2,!/usr/bin/env python3
v3.1.2,-*- coding: utf-8 -*-
v3.1.2,Most code taken from: https://github.com/alvations/sacremoses
v3.1.2,Which in turn is based on the Moses punctuation normalizer.
v3.1.2,https://github.com/moses-smt/mosesdecoder/blob/master/scripts/
v3.1.2,tokenizer/normalize-punctuation.perl
v3.1.2,don't fix period at end of sentence
v3.1.2,Regex substitutions from replace-unicode-punctuation.perl
v3.1.2,https://github.com/moses-smt/mosesdecoder/blob/master/
v3.1.2,scripts/tokenizer/replace-unicode-punctuation.perl
v3.1.2,Adds the penn substitutions after extra_whitespace regexes.
v3.1.2,"Optionally, replace unicode puncts BEFORE normalization."
v3.1.2,Actual normalization.
v3.1.2,"Optionally, replace unicode puncts BEFORE normalization."
v3.1.2,normalize dict src/tgt for each dataset
v3.1.2,One source feature expected but none given and no default provided
v3.1.2,Provided default does not match required features
v3.1.2,Data not properly annotated.
v3.1.2,In this case we do not use the default as it might be an error
v3.1.2,batch 0 will always predict EOS. The other batches will predict
v3.1.2,non-eos scores.
v3.1.2,"""best"" prediction is eos - that should be blocked"
v3.1.2,include at least one prediction OTHER than EOS
v3.1.2,that is greater than -1e20
v3.1.2,now batch 0 has ended and no others have
v3.1.2,initial step
v3.1.2,batch 0 dies on step 0
v3.1.2,include at least one prediction OTHER than EOS
v3.1.2,that is greater than -1e20
v3.1.2,step 2
v3.1.2,(old) batch 8 dies on step 1
v3.1.2,step 3
v3.1.2,everything dies
v3.1.2,initial step
v3.1.2,batch 0 dies on step 0
v3.1.2,include at least one prediction OTHER than EOS
v3.1.2,that is greater than -1e20
v3.1.2,step 2
v3.1.2,(old) batch 8 dies on step 1
v3.1.2,step 3
v3.1.2,everything dies
v3.1.2,initial step
v3.1.2,finish one beam
v3.1.2,include at least one prediction OTHER than EOS
v3.1.2,that is greater than -1e20
v3.1.2,step 2
v3.1.2,finish example in last batch
v3.1.2,(old) batch 8 dies on step 1
v3.1.2,step 3
v3.1.2,everything dies
v3.1.2,initial step
v3.1.2,batch 0 dies on step 0
v3.1.2,include at least one prediction OTHER than EOS
v3.1.2,that is greater than -1e20
v3.1.2,step 2
v3.1.2,(old) batch 8 dies on step 1
v3.1.2,step 3
v3.1.2,everything dies
v3.1.2,illegal_weights_mask = torch.ByteTensor([
v3.1.2,"[0, 0, 0, 0, 0, 0, 0],"
v3.1.2,"[0, 0, 0, 1, 1, 1, 1],"
v3.1.2,"[0, 0, 0, 0, 0, 1, 1],"
v3.1.2,"[0, 0, 1, 1, 1, 1, 1]])"
v3.1.2,TODO: fix for pytorch 0.3
v3.1.2,illegal_weights = alignments.masked_select(illegal_weights_mask)
v3.1.2,"self.assertEqual(0.0, illegal_weights.data.sum())"
v3.1.2,this could be considered an integration test because it touches
v3.1.2,the filesystem for the config file (and the models)
v3.1.2,no dummy prefix
v3.1.2,no dummy prefix
v3.1.2,make sure the scalars are in the event accumulator tags
v3.1.2,required arguments
v3.1.2,transforms that require vocab will not create if not provide vocab
v3.1.2,1. Init first transform in the pipe
v3.1.2,2. Init second transform in the pipe
v3.1.2,3. Sequential combine them into a transform pipe
v3.1.2,4. apply transform pipe for example
v3.1.2,"5. example after the pipe exceed the length limit, thus filtered"
v3.1.2,6. Transform statistics registed (here for filtertoolong)
v3.1.2,"7. after report, statistics become empty as a fresh start"
v3.1.2,filter_transform.warm_up()
v3.1.2,test BPE-dropout:
v3.1.2,1. disable bpe dropout for not training example
v3.1.2,2. enable bpe dropout for training example
v3.1.2,3. (NOTE) disable dropout won't take effect if already seen
v3.1.2,this is caused by the cache mechanism in bpe:
v3.1.2,return cached subword if the original token is seen when no dropout
v3.1.2,test SP regularization:
v3.1.2,1. enable regularization for training example
v3.1.2,2. disable regularization for not training example
v3.1.2,Not apply token drop for not training example
v3.1.2,apply token drop for training example
v3.1.2,Not apply token mask for not training example
v3.1.2,apply token mask for training example
v3.1.2,require vocabs to warm_up
v3.1.2,Not apply token mask for not training example
v3.1.2,apply token mask for training example
v3.1.2,"Defalt: full_stop_token=[""."", ""?"", ""!""]"
v3.1.2,"Defalt: full_stop_token=[""."", ""?"", ""!""]"
v3.1.2,random_ratio of inserted tokens are chosen in vocab
v3.1.2,others are MASK_TOK
v3.1.2,"insert_ratio=0.0,"
v3.1.2,"random_ratio=0.0,"
v3.1.2,"Defalt: full_stop_token=[""."", ""?"", ""!""]"
v3.1.2,all token are considered as an individual word
v3.1.2,1. tokens are dropped when replace_length is 0
v3.1.2,"print(f""token delete: {masked} / {tokens}"")"
v3.1.2,2. tokens are replaced by MASK when replace_length is 1
v3.1.2,"print(f""token mask: {masked} / {tokens}"")"
v3.1.2,"insert_ratio=0.0,"
v3.1.2,"random_ratio=0.0,"
v3.1.2,"Defalt: full_stop_token=[""."", ""?"", ""!""]"
v3.1.2,start token of word are identified using subword marker
v3.1.2,"1. replace_length 0: ""words"" are dropped"
v3.1.2,"print(f""word delete: {masked} / {tokens}"")"
v3.1.2,"self.assertEqual(len(masked), n_words - n_masked)"
v3.1.2,"2. replace_length 1: ""words"" are replaced with a single MASK"
v3.1.2,"print(f""whole word single mask: {masked} / {tokens}"")"
v3.1.2,len(masked) depend on number of tokens in select word
v3.1.2,"3. replace_length -1: all tokens in ""words"" are replaced with MASK"
v3.1.2,"print(f""whole word multi mask: {masked} / {tokens}"")"
v3.1.2,number of mask_tok depend on number of tokens in selected word
v3.1.2,number of MASK_TOK can be greater than n_masked
v3.1.2,"insert_ratio=0.5,"
v3.1.2,"random_ratio=0.3,"
v3.1.2,"Defalt: full_stop_token=[""."", ""?"", ""!""]"
v3.1.2,start token of word are identified using subword marker
v3.1.2,n_words = sum(token_starts)
v3.1.2,n_masked = math.ceil(n_words * bart_noise.mask_ratio)
v3.1.2,"print(f""Text Span Infilling: {infillied} / {tokens}"")"
v3.1.2,"print(n_words, n_masked)"
v3.1.2,!/usr/bin/env python
v3.1.2,-*- coding: utf-8 -*-
v3.1.2,Inject some dummy training options that may needed when build fields
v3.1.2,Remove the generated *pt files.
v3.1.2,Remove the generated data samples
v3.1.2,all beams repeat (beam >= 1 repeat dummy scores)
v3.1.2,predict repeat_idx over and over again
v3.1.2,"before repeat, scores are either 0 or -inf"
v3.1.2,"on repeat, `repeat_idx` score is BLOCKED_SCORE"
v3.1.2,"(but it's still the best score, thus we have"
v3.1.2,"[BLOCKED_SCORE, -inf, -inf, -inf, -inf]"
v3.1.2,repetitions keeps maximizing score
v3.1.2,"index 0 has been blocked, so repeating=>+0.0 score"
v3.1.2,other indexes are -inf so repeating=>BLOCKED_SCORE
v3.1.2,which is higher
v3.1.2,beam 0 and beam >=2 will repeat (beam >= 2 repeat dummy scores)
v3.1.2,non-interesting beams are going to get dummy values
v3.1.2,"on initial round, only predicted scores for beam 0"
v3.1.2,matter. Make two predictions. Top one will be repeated
v3.1.2,"in beam zero, second one will live on in beam 1."
v3.1.2,predict the same thing in beam 0
v3.1.2,continue pushing around what beam 1 predicts
v3.1.2,"now beam 0 dies (along with the others), beam 1 -> beam 0"
v3.1.2,"now beam 0 dies (along with the others), beam 1 -> beam 0"
v3.1.2,beam 0 and beam >= 2 will repeat (beam 2 repeats excluded idx)
v3.1.2,non-interesting beams are going to get dummy values
v3.1.2,predict the same thing in beam 0
v3.1.2,continue pushing around what beam 1 predicts
v3.1.2,predict the allowed-repeat again in beam 2
v3.1.2,"now beam 0 dies, beam 1 -> beam 0, beam 2 -> beam 1"
v3.1.2,and the rest die
v3.1.2,"since all preds after i=0 are 0, we can check"
v3.1.2,that the beam is the correct idx by checking that
v3.1.2,the curr score is the initial score
v3.1.2,beam 0 will always predict EOS. The other beams will predict
v3.1.2,non-eos scores.
v3.1.2,non-interesting beams are going to get dummy values
v3.1.2,"""best"" prediction is eos - that should be blocked"
v3.1.2,include at least beam_sz predictions OTHER than EOS
v3.1.2,that are greater than -1e20
v3.1.2,predict eos in beam 0
v3.1.2,provide beam_sz other good predictions
v3.1.2,now the top beam has ended and no others have
v3.1.2,"not of interest, but want to make sure it keeps running"
v3.1.2,since only beam 0 terminates and n_best = 2
v3.1.2,"this is also a test that when block_ngram_repeat=0,"
v3.1.2,repeating is acceptable
v3.1.2,non-interesting beams are going to get dummy values
v3.1.2,"""best"" prediction is eos - that should be blocked"
v3.1.2,include at least beam_sz predictions OTHER than EOS
v3.1.2,that are greater than -1e20
v3.1.2,predict eos in beam 1
v3.1.2,provide beam_sz other good predictions in other beams
v3.1.2,beam 1 dies on min_length
v3.1.2,beam 0 dies on the step after beam 1 dies
v3.1.2,"inp_lens is tiled in initialize, reassign to make attn match"
v3.1.2,non-interesting beams are going to get dummy values
v3.1.2,"""best"" prediction is eos - that should be blocked"
v3.1.2,include at least beam_sz predictions OTHER than EOS
v3.1.2,that are greater than -1e20
v3.1.2,predict eos in beam 1
v3.1.2,provide beam_sz other good predictions in other beams
v3.1.2,no top beams are finished yet
v3.1.2,beam 1 dies on min_length
v3.1.2,no top beams are finished yet
v3.1.2,beam 0 dies on the step after beam 1 dies
v3.1.2,top beam is finished now so there are attentions
v3.1.2,two beams are finished in each batch
v3.1.2,second dim is cut down to the non-padded src length
v3.1.2,first dim is equal to the time of death
v3.1.2,(beam 0 died at current step - adjust for SOS)
v3.1.2,(beam 1 died at last step - adjust for SOS)
v3.1.2,behavior gets weird when beam is already done so just stop
v3.1.2,this is just test_beam.TestBeamAgainstReferenceCase repeated
v3.1.2,in each batch.
v3.1.2,"init_preds: [4, 3, 5, 6, 7] - no EOS's"
v3.1.2,no EOS's yet
v3.1.2,"[5, 3, 2, 6, 0], so beam 2 predicts EOS!"
v3.1.2,assumes beam 2 finished on last step
v3.1.2,ended beam 2 shouldn't continue
v3.1.2,"[2, 5, 3, 6, 0] repeat self.BATCH_SZ, so beam 0 predicts EOS!"
v3.1.2,"[-2.4879, -3.8910, -4.1010, -4.2010, -4.4010] repeat self.BATCH_SZ"
v3.1.2,another beam is finished in all batches
v3.1.2,new beam 0 finished
v3.1.2,new beam 0 is old beam 3
v3.1.2,assumes beam 0 finished on last step
v3.1.2,"[5, 2, 6, 1, 0] repeat self.BATCH_SZ, so beam 1 predicts EOS!"
v3.1.2,we finish 3 hyps per example in this step
v3.1.2,new beam 1 is old beam 3
v3.1.2,this could be considered an integration test because it tests
v3.1.2,interactions between the GNMT scorer and the beam
v3.1.2,"-data option is required, but not used in this test, so dummy."
v3.1.2,len x batch x nfeat
v3.1.2,Initialize vectors to compare size with
v3.1.2,Ensure correct sizes and types
v3.1.2,Make sure that output has the correct size and type
v3.1.2,"[('encoder_type', 'transformer'),"
v3.1.2,"('word_vec_size', 16), ('hidden_size', 16)],"
v3.1.2,""""""" Only do SRU test if requirment is safisfied. """""""
v3.1.2,SRU doesn't support input_feed.
v3.1.2,first check there's nothing unexpectedly not trainable
v3.1.2,ok: word embeddings shouldn't be trainable
v3.1.2,if word vecs are freezed
v3.1.2,ok: positional encodings shouldn't be trainable
v3.1.2,then check nothing unexpectedly trainable
v3.1.2,Decoder state
v3.1.2,Build the RNN.
v3.1.2,Set up the context gate.
v3.1.2,Set up the standard attention.
v3.1.2,The encoder hidden is  (layers*directions) x batch x dim.
v3.1.2,We need to convert it to layers x batch x (directions*dim).
v3.1.2,Init the input feed.
v3.1.2,Update the state with the result.
v3.1.2,Concatenates sequence of tensors along a new dimension.
v3.1.2,NOTE: v0.3 to 0.4: dec_outs / attns[*] may not be list
v3.1.2,(in particular in case of SRU) it was not raising error in 0.3
v3.1.2,since stack(Variable) was allowed.
v3.1.2,"In 0.4, SRU returns a tensor that shouldn't be stacke"
v3.1.2,Calculate the attention.
v3.1.2,Calculate the context gate.
v3.1.2,Additional args check.
v3.1.2,Input feed concatenates hidden state with
v3.1.2,input at every time step.
v3.1.2,TODO: context gate should be employed
v3.1.2,instead of second RNN transform.
v3.1.2,Update the coverage attention.
v3.1.2,"attns[""coverage""] is actually c^(t+1) of See et al(2017)"
v3.1.2,1-index shifted
v3.1.2,Decoder State
v3.1.2,CNNDecoder has its own attention mechanism.
v3.1.2,Set up a separate copy attention layer if needed.
v3.1.2,The output of CNNEncoder.
v3.1.2,The combination of output of CNNEncoder and source embeddings.
v3.1.2,Process the result and update the attentions.
v3.1.2,Update the state.
v3.1.2,TODO change the way attns is returned dict => list or tuple (onnx)
v3.1.2,src_len is a single tensor shared between all models.
v3.1.2,This assumption will not hold if Translator is modified
v3.1.2,to calculate src_len as something other than the length
v3.1.2,of the input.
v3.1.2,"return _, (B, Q_len, K_len)"
v3.1.2,"layer average attention across heads, get ``(B, Q, K)``"
v3.1.2,"Case 1: no full_context, no align heads -> layer avg baseline"
v3.1.2,"Case 2: no full_context, 1 align heads -> guided align"
v3.1.2,"Case 3: full_context, 1 align heads -> full cte guided align"
v3.1.2,BoolTensor was introduced in pytorch 1.2
v3.1.2,T: could be 1 in the case of stepwise decoding or tgt_len
v3.1.2,masking is necessary when sequence length is greater than one
v3.1.2,mask now are (batch x 1 x tlen x s or t len)
v3.1.2,1 = heads to be expanded in MHA
v3.1.2,Decoder State
v3.1.2,"previously, there was a GlobalAttention module here for copy"
v3.1.2,"attention. But it was never actually used -- the ""copy"" attention"
v3.1.2,just reuses the context attention.
v3.1.2,"attns[""align""] = torch.stack(attn_aligns, 0).mean(0)  # All avg"
v3.1.2,TODO change the way attns is returned dict => list or tuple (onnx)
v3.1.2,first value set to True triggered by the beginning of decoding
v3.1.2,layer_cache becomes active in the MultiHeadedAttention fwd
v3.1.2,T: could be 1 in the case of stepwise decoding or tgt_len
v3.1.2,masking is necessary when sequence length is greater than one
v3.1.2,mask now are (batch x 1 x tlen x tlen)
v3.1.2,1 = heads to be expanded in MHA
v3.1.2,TODO change the way attns is returned dict => list or tuple (onnx)
v3.1.2,"buffer size in bytes, determine equiv. # of elements based on data type"
v3.1.2,copy tensors into buffer_t
v3.1.2,all-reduce and rescale
v3.1.2,copy all-reduced buffer back into tensors
v3.1.2,"print(filled, sz)"
v3.1.2,"tensor is bigger than buffer, all-reduce and rescale directly"
v3.1.2,"buffer is full, all-reduce and replace buffer with grad"
v3.1.2,add tensor to buffer
v3.1.2,"propagate exception to parent process, keeping original traceback"
v3.1.2,TODO: Find a better way to check for sparse gradients.
v3.1.2,we use apex.amp
v3.1.2,In this case use the old FusedAdam with
v3.1.2,FP16_optimizer wrapper
v3.1.2,Load everything from the checkpoint.
v3.1.2,Build everything from scratch.
v3.1.2,"Reset optimizer, keep options."
v3.1.2,"Reset options, keep optimizer."
v3.1.2,State can be partially restored.
v3.1.2,should be: self._optimizer.zero_grad(set_to_none)
v3.1.2,but apex.amp is not up-to-date:
v3.1.2,https://github.com/NVIDIA/apex/blob/master/apex/amp/_process_optimizer.py#L367
v3.1.2,"unscaled optimizer's gradients (already done therefore skip),"
v3.1.2,skips optimizer.step() if gradients contain infs/NaNs.
v3.1.2,Updates the scale for next iteration.
v3.1.2,Code below is an implementation of https://arxiv.org/pdf/1804.04235.pdf
v3.1.2,inspired but modified from https://github.com/DeadAt0m/adafactor-pytorch
v3.1.2,backward compatibility
v3.1.2,assuming a list/generator of parameter means single group
v3.1.2,compute combined scale factor for this group
v3.1.2,norm is in fact norm*scale
v3.1.2,note: p.grad should not ever be set for correct operation of
v3.1.2,mixed precision optimizer that sometimes sends None gradients
v3.1.2,State initialization
v3.1.2,Exponential moving average of gradient values
v3.1.2,Exponential moving average of squared gradient values
v3.1.2,-*- coding: utf-8 -*-
v3.1.2,placing this here make it easier to call logger.info
v3.1.2,"from anywhere, just 'from onmt.utils.logging import logger'"
v3.1.2,"align_head contains value in [0, 1) presenting attn prob,"
v3.1.2,0 was resulted by the context attention src_pad_mask
v3.1.2,"So, the correspand position in ref_align should also be 0"
v3.1.2,"Therefore, clip align_head to > 1e-18 should be bias free."
v3.1.2,rescale with tau (temperature) and apply the log_softmax.
v3.1.2,ct2 expects src with lengths without padding
v3.1.2,again we use raw probs to rescale with tau and apply log_softmax
v3.1.2,lm_scores are in log space so log_target=True
v3.1.2,rescale with tau (temperature) and apply the log_softmax.
v3.1.2,ct2 expects src with lengths without padding
v3.1.2,again we use raw probs to rescale with tau and apply log_softmax
v3.1.2,lm_scores are in log space so log_target=True
v3.1.2,take into account here the tgt_shift_index (0 / 1 = LM/NMT)
v3.1.2,Correct target copy token instead of <unk>
v3.1.2,tgt[i] = align[i] + len(tgt_vocab)
v3.1.2,for i such that tgt[i] == 0 and align[i] != 0
v3.1.2,in the case criterion reduction is None then we need
v3.1.2,to sum the loss of each sentence in the batch
v3.1.2,Check Transforms
v3.1.2,Check path
v3.1.2,tgt is src for LM task
v3.1.2,Check weight
v3.1.2,Check features
v3.1.2,validation when train:
v3.1.2,Check embeddings stuff
v3.1.2,"Backward compatibility with ""fix_word_vecs_*"" opts"
v3.1.2,encoder and decoder should be same sizes
v3.1.2,"Load default opt values, then overwrite with the opts in"
v3.1.2,"the checkpoint. That way, if there are new options added,"
v3.1.2,the defaults are used.
v3.1.2,It comes from training
v3.1.2,TODO: needs to be added as inference opt
v3.1.2,Don't do anything
v3.1.2,Update best score of each criteria
v3.1.2,Reset tolerance
v3.1.2,Update current status
v3.1.2,Decrease tolerance
v3.1.2,Log
v3.1.2,Log
v3.1.2,Get a list of world_size lists with len(stat_list) Statistics objects
v3.1.2,"this param init is overridden by model_builder, useless then."
v3.1.2,SRU doesn't support PackedSequence.
v3.1.2,-*- coding: utf-8 -*-
v3.1.2,threshold on 1 to avoid div by 0
v3.1.2,treat alignment matrix one by one as each have different lengths
v3.1.2,No alignment if not exist valid tgt token
v3.1.2,get valid alignment (sub-matrix from full paded aligment matrix)
v3.1.2,Helper functions
v3.1.2,Keeps track of the original words/subwords
v3.1.2,('prior_tokenization' option)
v3.1.2,In case there is a final case_markup when new_spacer is on
v3.1.2,translate
v3.1.2,for validation we build an infer_iter per batch
v3.1.2,in order to avoid oom issues because there is no
v3.1.2,batching strategy in `textbatch_to_tensor`
v3.1.2,apply_reverse refs
v3.1.2,flatten preds
v3.1.2,save results
v3.1.2,-*- coding: utf-8 -*-
v3.1.2,this one is needed for Random Shuffler of batches
v3.1.2,in multi gpu it ensures datasets are read in the same order
v3.1.2,some cudnn methods can be random even after fixing the seed
v3.1.2,unless you tell it to be deterministic
v3.1.2,This one is needed for various tranfroms
v3.1.2,These ensure same initialization in multi gpu mode
v3.1.2,we need to check the model path + any tokenizer path
v3.1.2,patch to log stdout spawned processes of dataloader
v3.1.2,bucket_size = batch_size
v3.1.2,For TRAIN we need to group examples by length
v3.1.2,"for faster performance, but otherwise, sequential."
v3.1.2,For TRAIN we shuffle batches within the bucket
v3.1.2,otherwise sequential
v3.1.2,for specific case of rnn_packed need to be sorted
v3.1.2,within the batch
v3.1.2,Check if all tokens have features or none at all
v3.1.2,Make features part of src like
v3.1.2,"{'src': {'src': ..., 'feats': [...., ....]}}"
v3.1.2,at this point an example looks like:
v3.1.2,"{'src': {'src': ..., 'feats': [....]},"
v3.1.2,"'tgt': {'tgt': ...},"
v3.1.2,"'src_original': ['tok1', ...'tokn'],"
v3.1.2,"'tgt_original': ['tok1', ...'tokm'],"
v3.1.2,'indices' : seq in bucket
v3.1.2,"'align': ...,"
v3.1.2,}
v3.1.2,Need to add features in last dimensions
v3.1.2,Keep it consistent with dynamic data
v3.1.2,make a small vocab containing just the tokens in the source sequence
v3.1.2,Map source tokens to indices in the dynamic dict.
v3.1.2,-*- coding: utf-8 -*-
v3.1.2,'src_original' and 'tgt_original' store the
v3.1.2,original line before tokenization. These
v3.1.2,fields are used later on in the feature
v3.1.2,transforms.
v3.1.2,NOTE: moved to dynamic_iterator.py cf process()
v3.1.2,item = self.transform.apply(
v3.1.2,"example, is_train=self.infinitely, corpus_name=self.cid)"
v3.1.2,empty example: skip
v3.1.2,"No encoder in LM, seq2seq count formatting kept"
v3.1.2,_check_save_model_path
v3.1.2,!/usr/bin/env python
v3.1.2,!/usr/bin/env python
v3.1.2,!/usr/bin/env python
v3.1.2,-*- coding: utf-8 -*-
v3.1.2,!/usr/bin/env python
v3.1.2,BPE training
v3.1.2,SentencePiece training
v3.1.2,!/usr/bin/env python
v3.1.2,!/usr/bin/env python
v3.1.2,Set sharing strategy manually instead of default based on the OS.
v3.1.2,torch.multiprocessing.set_sharing_strategy('file_system')
v3.1.2,Create a thread to listen for errors in the child processes.
v3.1.2,Train with multiprocessing.
v3.1.2,magic indices
v3.1.2,result caching
v3.1.2,Here we set the decoder to start with self.start (BOS or EOS)
v3.1.2,fix length constraint and remove eos from count
v3.1.2,add one to account for BOS. Don't account for EOS because hitting
v3.1.2,this implies it hasn't been found.
v3.1.2,we don't block nothing if the user doesn't want it
v3.1.2,we can't block nothing beam's too short
v3.1.2,we check paths one by one
v3.1.2,we don't forbid nothing if the user doesn't want it
v3.1.2,we can't forbid nothing if beam's too short
v3.1.2,Reordering forbidden_tokens following beam selection
v3.1.2,We rebuild a dict to ensure we get the value and not the pointer
v3.1.2,Grabing the newly selected tokens and associated ngram
v3.1.2,skip the blocking if any token in current_ngram is excluded
v3.1.2,"pickups: Tensor where specified index were set to 1, others 0"
v3.1.2,"dropdowns: opposite of pickups, 1 for those shouldn't pick"
v3.1.2,Minus dropdowns to log_probs making probabilities of
v3.1.2,unspecified index close to 0
v3.1.2,"prediction step have surpass length of given target_prefix,"
v3.1.2,no need to further change this attr
v3.1.2,keep indices until overflowing p
v3.1.2,Set all logits that are not in the top-p to -10000.
v3.1.2,This puts the probabilities close to 0.
v3.1.2,Set all logits that are not in the top-k to -10000.
v3.1.2,This puts the probabilities close to 0.
v3.1.2,"For temp=0.0, take the argmax to avoid divide-by-zero errors."
v3.1.2,keep_topk=1 is also equivalent to argmax.
v3.1.2,maybe fix some prediction at this step by modifying log_probs
v3.1.2,"shape: (sum(~ self.is_finished), 1)"
v3.1.2,in LM task src_len is associated with currently generated src
v3.1.2,and therefore needs to follow the generation
v3.1.2,!/usr/bin/env python
v3.1.2,for debugging
v3.1.2,TODO: maybe add dynamic part
v3.1.2,Statistics
v3.1.2,those two should be the same except feat dim
v3.1.2,"batch['src'][perm[j], :, :])"
v3.1.2,trans.src
v3.1.2,we rebuild a small batch made of the sub-segments
v3.1.2,in the long segment.
v3.1.2,new sub-batch ready to be translated
v3.1.2,we re-insert the sub-batch in the initial translations
v3.1.2,In the case of length_penalty = none we report the total logprobs
v3.1.2,divided by the number of sentence to get an approximation of the
v3.1.2,per sentence logprob. We also return the corresponding ppl
v3.1.2,"When a length_penalty is used eg: ""avg"" or ""wu"" since logprobs"
v3.1.2,are normalized per token we report the per line per token logprob
v3.1.2,"and the corresponding ""per word perplexity"""
v3.1.2,Turn any copied words into UNKs.
v3.1.2,"Decoder forward, takes [batch, tgt_len, nfeats] as input"
v3.1.2,"and [batch, src_len, hidden] as enc_out"
v3.1.2,"in case of inference tgt_len = 1, batch = beam times batch_size"
v3.1.2,"in case of Gold Scoring tgt_len = actual length, batch = 1 batch"
v3.1.2,Generator forward.
v3.1.2,"returns [(batch_size x beam_size) , vocab ] when 1 step"
v3.1.2,"or [batch_size, tgt_len, vocab ] when full sentence"
v3.1.2,"here we have scores [tgt_lenxbatch, vocab] or [beamxbatch, vocab]"
v3.1.2,at this point scores is batch first (dim=0)
v3.1.2,"returns [(batch_size x beam_size) , vocab ] when 1 step"
v3.1.2,"or [batch_size, tgt_len, vocab ] when full sentence"
v3.1.2,(0) add BOS and padding to tgt prediction
v3.1.2,(1) Encoder forward.
v3.1.2,(2) Repeat src objects `n_best` times.
v3.1.2,"We use batch_size x n_best, get ``(batch * n_best, src_len, nfeat)``"
v3.1.2,Quick fix. Transformers return None as enc_states.
v3.1.2,enc_states are only used later on to init decoder's state
v3.1.2,"but are never used in Transformer decoder, so we can skip"
v3.1.2,"(3) Init decoder with n_best src,"
v3.1.2,"reshape tgt to ``(len, batch * n_best, nfeat)``"
v3.1.2,it should be done in a better way
v3.1.2,here dec_in is batch first
v3.1.2,masked_select
v3.1.2,get aligned src id for each prediction's valid tgt tokens
v3.1.2,TODO: support these blacklisted features
v3.1.2,(0) Prep the components of the search.
v3.1.2,(1) Run the encoder on the src.
v3.1.2,(2) prep decode_strategy. Possibly repeat src objects.
v3.1.2,(3) Begin decoding step by step:
v3.1.2,"decoder_input = decode_strategy.current_predictions.view(1, -1,"
v3.1.2,1)
v3.1.2,Reorder states.
v3.1.2,TODO: support these blacklisted features
v3.1.2,(0) Prep the components of the search.
v3.1.2,(1) split src into src and target_prefix to avoid padding.
v3.1.2,(2) init decoder
v3.1.2,(3) prep decode_strategy. Possibly repeat src objects.
v3.1.2,(4) Begin decoding step by step:
v3.1.2,Reorder states.
v3.1.2,select indexes in model state/cache
v3.1.2,beam parameters
v3.1.2,beam state
v3.1.2,BoolTensor was introduced in pytorch 1.2
v3.1.2,"""global state"" of the old beam"
v3.1.2,buffers for the topk scores and 'backpointer'
v3.1.2,for testing
v3.1.2,maybe fix some prediction at this step by modifying log_probs
v3.1.2,Flatten probs into a list of possibilities.
v3.1.2,Penalize beams that finished.
v3.1.2,"on real data (newstest2017) with the pretrained transformer,"
v3.1.2,it's faster to not move this back to the original device
v3.1.2,Store finished hypotheses for this batch.
v3.1.2,End condition is the top beam finished and we can return
v3.1.2,n_best hypotheses.
v3.1.2,"If all sentences are translated, no need to go further."
v3.1.2,Remove finished batches for the next step.
v3.1.2,using integer division to get an integer _B without casting
v3.1.2,force the output to be longer than self.min_length
v3.1.2,Multiply probs by the beam probability.
v3.1.2,"if the sequence ends now, then the penalty is the current"
v3.1.2,"length + 1, to include the EOS token"
v3.1.2,Avoid any direction that would repeat unwanted ngrams
v3.1.2,Pick up candidate token by curr_scores
v3.1.2,Recover log probs.
v3.1.2,Length penalty is just a scalar. It doesn't matter if it's applied
v3.1.2,before or after the topk.
v3.1.2,Resolve beam origin and map to batch index flat representation.
v3.1.2,Append last prediction.
v3.1.2,update global state (step == 1)
v3.1.2,update global state (step > 1)
v3.1.2,"shape: (batch_size x beam_size, 1)"
v3.1.2,in LM task src_len is associated with currently generated src
v3.1.2,and therefore needs to follow the generation
v3.1.2,in LM task src_len is associated with currently generated src
v3.1.2,and therefore needs to follow the generation
v3.1.2,Term will be subtracted from probability
v3.1.2,Probability will be divided by this
v3.1.2,these warnings indicate that either the alpha/beta
v3.1.2,"forces a penalty to be a no-op, or a penalty is a no-op but"
v3.1.2,the alpha/beta would suggest otherwise.
v3.1.2,using some coverage penalty
v3.1.2,!/usr/bin/env python
v3.1.2,semaphore doesn't have a timeout arg in Python 2.7
v3.1.2,perform a first request to initialize everything
v3.1.2,backwards compatibility for confs
v3.1.2,every segment becomes a dict for flexibility purposes
v3.1.2,NOTE: translator returns lists of `n_best` list
v3.1.2,build back results with empty texts
v3.1.2,load can be called multiple times: modify copy
v3.1.2,output contain alignment
v3.1.2,Below are all the different penalty terms implemented so far.
v3.1.2,Subtract coverage penalty from topk log probs.
v3.1.2,Divide topk log probs by length penalty.
v3.1.2,Sorting
v3.1.2,Chinese segmentation
v3.1.2,Chinese simplify -> Chinese traditional standard
v3.1.2,Chinese simplify -> Chinese traditional (HongKong)
v3.1.2,Chinese simplify -> Chinese traditional (Taiwan)
v3.1.2,Chinese traditional -> Chinese simplify (v1)
v3.1.2,Chinese traditional -> Chinese simplify (v2)
v3.1.2,Auto import python files in this directory
v3.1.1,!/usr/bin/env python
v3.1.1,!/usr/bin/env python
v3.1.1,!/usr/bin/env python
v3.1.1,!/usr/bin/env python
v3.1.1,!/usr/bin/env python
v3.1.1,!/usr/bin/env python3
v3.1.1,-*- coding: utf-8 -*-
v3.1.1,
v3.1.1,"OpenNMT-py documentation build configuration file, created by"
v3.1.1,sphinx-quickstart on Sun Dec 17 12:07:14 2017.
v3.1.1,
v3.1.1,This file is execfile()d with the current directory set to its
v3.1.1,containing dir.
v3.1.1,
v3.1.1,Note that not all possible configuration values are present in this
v3.1.1,autogenerated file.
v3.1.1,
v3.1.1,All configuration values have a default; values that are commented out
v3.1.1,serve to show the default.
v3.1.1,"If extensions (or modules to document with autodoc) are in another directory,"
v3.1.1,add these directories to sys.path here. If the directory is relative to the
v3.1.1,"documentation root, use os.path.abspath to make it absolute, like shown here."
v3.1.1,
v3.1.1,import os
v3.1.1,import sys
v3.1.1,"sys.path.insert(0, os.path.abspath('.'))"
v3.1.1,-- General configuration ------------------------------------------------
v3.1.1,"If your documentation needs a minimal Sphinx version, state it here."
v3.1.1,
v3.1.1,needs_sphinx = '6.0'
v3.1.1,"Add any Sphinx extension module names here, as strings. They can be"
v3.1.1,extensions coming with Sphinx (named 'sphinx.ext.*') or your custom
v3.1.1,ones.
v3.1.1,Show base classes
v3.1.1,"Use ""variables"" section for Attributes instead of weird block things"
v3.1.1,mimicking the function style.
v3.1.1,"Add any paths that contain templates here, relative to this directory."
v3.1.1,The suffix(es) of source filenames.
v3.1.1,You can specify multiple suffix as a list of string:
v3.1.1,
v3.1.1,"source_suffix = ['.rst', '.md']"
v3.1.1,The master toctree document.
v3.1.1,General information about the project.
v3.1.1,"The version info for the project you're documenting, acts as replacement for"
v3.1.1,"|version| and |release|, also used in various other places throughout the"
v3.1.1,built documents.
v3.1.1,
v3.1.1,The short X.Y version.
v3.1.1,"The full version, including alpha/beta/rc tags."
v3.1.1,The language for content autogenerated by Sphinx. Refer to documentation
v3.1.1,for a list of supported languages.
v3.1.1,
v3.1.1,This is also used if you do content translation via gettext catalogs.
v3.1.1,"Usually you set ""language"" from the command line for these cases."
v3.1.1,"List of patterns, relative to source directory, that match files and"
v3.1.1,directories to ignore when looking for source files.
v3.1.1,This patterns also effect to html_static_path and html_extra_path
v3.1.1,The name of the Pygments (syntax highlighting) style to use.
v3.1.1,"If true, `todo` and `todoList` produce output, else they produce nothing."
v3.1.1,-- Options for HTML output ----------------------------------------------
v3.1.1,The theme to use for HTML and HTML Help pages.  See the documentation for
v3.1.1,a list of builtin themes.
v3.1.1,
v3.1.1,html_theme = 'sphinx_materialdesign_theme'
v3.1.1,html_theme_path = [sphinx_materialdesign_theme.get_path()]
v3.1.1,Theme options are theme-specific and customize the look and feel of a theme
v3.1.1,"further.  For a list of options available for each theme, see the"
v3.1.1,documentation.
v3.1.1,
v3.1.1,html_theme_options = {}
v3.1.1,"Add any paths that contain custom static files (such as style sheets) here,"
v3.1.1,"relative to this directory. They are copied after the builtin static files,"
v3.1.1,"so a file named ""default.css"" will overwrite the builtin ""default.css""."
v3.1.1,"Custom sidebar templates, must be a dictionary that maps document names"
v3.1.1,to template names.
v3.1.1,
v3.1.1,This is required for the alabaster theme
v3.1.1,refs: http://alabaster.readthedocs.io/en/latest/installation.html#sidebars
v3.1.1,-- Options for HTMLHelp output ------------------------------------------
v3.1.1,Output file base name for HTML help builder.
v3.1.1,-- Options for LaTeX output ---------------------------------------------
v3.1.1,The paper size ('letterpaper' or 'a4paper').
v3.1.1,
v3.1.1,"'papersize': 'letterpaper',"
v3.1.1,"The font size ('10pt', '11pt' or '12pt')."
v3.1.1,
v3.1.1,"'pointsize': '10pt',"
v3.1.1,Additional stuff for the LaTeX preamble.
v3.1.1,
v3.1.1,"'preamble': '',"
v3.1.1,Latex figure (float) alignment
v3.1.1,
v3.1.1,"'figure_align': 'htbp',"
v3.1.1,Grouping the document tree into LaTeX files. List of tuples
v3.1.1,"(source start file, target name, title,"
v3.1.1,"author, documentclass [howto, manual, or own class])."
v3.1.1,-- Options for manual page output ---------------------------------------
v3.1.1,One entry per manual page. List of tuples
v3.1.1,"(source start file, name, description, authors, manual section)."
v3.1.1,-- Options for Texinfo output -------------------------------------------
v3.1.1,Grouping the document tree into Texinfo files. List of tuples
v3.1.1,"(source start file, target name, title, author,"
v3.1.1,"dir menu entry, description, category)"
v3.1.1,!/usr/bin/env python3
v3.1.1,Usage: python3 filter_train.py in.src in.trg out.src out.trg max-tokens
v3.1.1,!/usr/bin/env python
v3.1.1,-*- coding: utf-8 -*-
v3.1.1,is this reachable?
v3.1.1,Read in embeddings
v3.1.1,Write to file
v3.1.1,converts a SentencePiece vocabulary to the format expected by dynamic data
v3.1.1,"(essentially converts float expected counts to ""fixed precision"" int pseudo"
v3.1.1,counts)
v3.1.1,"Add in default model arguments, possibly added since training."
v3.1.1,this patch is no longer needed included in converter
v3.1.1,"if hasattr(model_opt, 'rnn_size'):"
v3.1.1,model_opt.hidden_size = model_opt.rnn_size
v3.1.1,build_base_model expects updated and validated opts
v3.1.1,-*- encoding: utf-8 -*-
v3.1.1,!/usr/bin/env python
v3.1.1,-*- coding: utf-8 -*-
v3.1.1,Author: Rico Sennrich
v3.1.1,flake8: noqa
v3.1.1,This file is retrieved from https://github.com/rsennrich/subword-nmt
v3.1.1,hack for python2/3 compatibility
v3.1.1,check version information
v3.1.1,some hacking to deal with duplicates (only consider first instance)
v3.1.1,don't print end-of-word symbols
v3.1.1,sys.stderr.write('cannot split {0} further.\n'.format(segment))
v3.1.1,sys.stderr.write('OOV: {0}\n'.format(segment))
v3.1.1,sys.stderr.write('OOV: {0}\n'.format(segment))
v3.1.1,python 2/3 compatibility
v3.1.1,read/write files as UTF-8
v3.1.1,!/usr/bin/env python3
v3.1.1,coding: utf-8
v3.1.1,"In order to use this tool, please install comet first"
v3.1.1,https://github.com/Unbabel/COMET
v3.1.1,Let's say you have a source file with N sentences in SL - eg: source.sl
v3.1.1,and the corresponding references (N sentences) reference.tl
v3.1.1,Translate your file in TL with the -n_best nbest options nbest being
v3.1.1,then number of hypotheses and output the target to -output target.nbest.tl
v3.1.1,Then you need to duplicate source and reference sentences nbest times
v3.1.1,for this script.
v3.1.1,for instance using awk '{for(i=1; i<=n; i++) print}' n=5 reference.tl \
v3.1.1,> reference.5.tl
v3.1.1,same for source.
v3.1.1,This script can be run (for instance with nbest = 5) as follows:
v3.1.1,python oracle_bleu.py --nbest-src source.5.sl --nbest-hyp target.5.tl \
v3.1.1,--nbest-ref reference.5.tl --nbest-order 5 --output target.maxbleu.tl
v3.1.1,It will search in all hyp the best comet score
v3.1.1,when choosing a reference-less model no nbest-ref is required
v3.1.1,for nbest in nbests:
v3.1.1,!/usr/bin/env python
v3.1.1,!/usr/bin/env python3
v3.1.1,coding: utf-8
v3.1.1,Let's say you have a source file with N sentences in SL - eg: source.sl
v3.1.1,Translate your file in TL with the -n_best nbest options nbest being
v3.1.1,then number of hypotheses and output the target to -output target.nbest.tl
v3.1.1,This script can be run (for instance with nbest = 5) as follows:
v3.1.1,python mbr_bleu.py --nbest-hyp target.5.tl \
v3.1.1,--nbest-order 5 --output target.mbr.tl
v3.1.1,It will compare all hyp with eachother and output the max bleu
v3.1.1,!/usr/bin/env python
v3.1.1,-*- coding: utf-8 -*-
v3.1.1,Author: Rico Sennrich
v3.1.1,flake8: noqa
v3.1.1,This file is retrieved from https://github.com/rsennrich/subword-nmt
v3.1.1,hack for python2/3 compatibility
v3.1.1,"find all instances of pair, and update frequency/indices around it"
v3.1.1,find first symbol
v3.1.1,"if first symbol is followed by second symbol, we've found an occurrence of pair (old_word[i:i+2])"
v3.1.1,"assuming a symbol sequence ""A B C"", if ""B C"" is merged, reduce the frequency of ""A B"""
v3.1.1,"assuming a symbol sequence ""A B C B"", if ""B C"" is merged, reduce the frequency of ""C B""."
v3.1.1,"however, skip this if the sequence is A B C B C, because the frequency of ""C B"" will be reduced by the previous code block"
v3.1.1,find new pair
v3.1.1,"assuming a symbol sequence ""A BC D"", if ""B C"" is merged, increase the frequency of ""A BC"""
v3.1.1,"assuming a symbol sequence ""A BC B"", if ""B C"" is merged, increase the frequency of ""BC B"""
v3.1.1,"however, if the sequence is A BC BC, skip this step because the count of ""BC BC"" will be incremented by the previous code block"
v3.1.1,data structure of pair frequencies
v3.1.1,index from pairs to words
v3.1.1,version 0.2 changes the handling of the end-of-word token ('</w>');
v3.1.1,version numbering allows bckward compatibility
v3.1.1,"threshold is inspired by Zipfian assumption, but should only affect speed"
v3.1.1,we probably missed the best pair because of pruning; go back to full statistics
v3.1.1,"threshold is inspired by Zipfian assumption, but should only affect speed"
v3.1.1,python 2/3 compatibility
v3.1.1,read/write files as UTF-8
v3.1.1,Now we can pipe the full file through the model using the Iterator
v3.1.1,reminder a batch includes .src .tgt .indices and it is sorted
v3.1.1,Compute and retrieve the loss for EACH sentence
v3.1.1,Now we need to rearrange the batch of ppl
v3.1.1,in the original order with indices
v3.1.1,!/usr/bin/env python
v3.1.1,-*- coding: utf-8 -*-
v3.1.1,!/usr/bin/env python
v3.1.1,!/usr/bin/env python3
v3.1.1,coding: utf-8
v3.1.1,Let's say you have a source file with N sentences in SL - eg: source.sl
v3.1.1,and the corresponding references (N sentences) reference.tl
v3.1.1,Translate your file in TL with the -n_best nbest options nbest being
v3.1.1,then number of hypotheses and output the target to -output target.nbest.tl
v3.1.1,Then you need to duplicate reference sentences nbest times for this script.
v3.1.1,for instance using awk '{for(i=1; i<=n; i++) print}' n=5 reference.tl \
v3.1.1,> reference.5.tl
v3.1.1,This script can be run (for instance with nbest = 5) as follows:
v3.1.1,python oracle_bleu.py --nbest-hyp target.5.tl --nbest-ref reference.5.tl \
v3.1.1,--nbest-order 5 --output target.maxbleu.tl
v3.1.1,It will search in all hyp the best bleu wrt reference
v3.1.1,and output the max bleu
v3.1.1,!/usr/bin/env python
v3.1.1,with the two module = imp.load_source() below
v3.1.1,we ghost the old torchtext.data.field and depercated
v3.1.1,onmt.inputters.text_dataset
v3.1.1,however this require some functions / classes to be
v3.1.1,monkey patched for loading the old field/vocab objects.
v3.1.1,"module3 = imp.load_source(""Vocab"", ""tools/convertv2_v3.py"")"
v3.1.1,"voc = dict(sorted(fields.vocab.__dict__['freqs'].items(),"
v3.1.1,"key=lambda x: (-x[1], x[0]))).keys()"
v3.1.1,"voc = dict(sorted(fields.vocab.__dict__['freqs'].items(),"
v3.1.1,"key=lambda x: (-x[1], x[0]))).keys()"
v3.1.1,Patch for NLLB200 model loading
v3.1.1,Avoid functionality on inference
v3.1.1,Build embeddings.
v3.1.1,Build encoder.
v3.1.1,Build embeddings.
v3.1.1,Build decoder.
v3.1.1,Share the embedding matrix - preprocess with share_vocab required.
v3.1.1,src/tgt vocab should be the same if `-share_vocab` is specified.
v3.1.1,Update vocabulary embeddings with checkpoint embeddings
v3.1.1,Embedding layers
v3.1.1,Just for debugging purposes
v3.1.1,Remove old vocabulary associated embeddings
v3.1.1,for back compat when attention_dropout was not defined
v3.1.1,Build Model
v3.1.1,Build Generator.
v3.1.1,Load the model states from checkpoint or initialize them.
v3.1.1,This preserves backward-compat for models using customed layernorm
v3.1.1,end of patch for backward compatibility
v3.1.1,Update model embeddings with those from the checkpoint
v3.1.1,after initialization
v3.1.1,!/usr/bin/env python
v3.1.1,if transform + options set in 'valid' we need to copy in main
v3.1.1,transform / options for scoring considered as inference
v3.1.1,"maybe prepare pretrained embeddings, if any"
v3.1.1,Load checkpoint if we resume from a previous training.
v3.1.1,ensure tensorboard output is written in the directory
v3.1.1,of previous checkpoints
v3.1.1,Override checkpoint's update_embeddings as it defaults to false
v3.1.1,Override checkpoint's freezing settings as it defaults to false
v3.1.1,NOTE: It's important that ``opt`` has been validated and updated
v3.1.1,at this point.
v3.1.1,Build model.
v3.1.1,Build optimizer.
v3.1.1,Build model saver
v3.1.1,Use Tensorboard for visualization during training
v3.1.1,Options only during inference
v3.1.1,"Truncation options, for text corpus"
v3.1.1,"as for False, this will be added in _add_train_general_opts"
v3.1.1,Embedding Options
v3.1.1,Model Task Options
v3.1.1,Encoder-Decoder Options
v3.1.1,Freeze Encoder and/or Decoder
v3.1.1,The following options (bridge_extra_node to n_steps) are used
v3.1.1,for training with --encoder_type ggnn (Gated Graph Neural Network).
v3.1.1,Attention options
v3.1.1,Alignement options
v3.1.1,Generator and loss options.
v3.1.1,GPU
v3.1.1,LoRa
v3.1.1,Init options
v3.1.1,Pretrained word vectors
v3.1.1,Freeze word vectors
v3.1.1,Optimization options
v3.1.1,learning rate
v3.1.1,options relate to data preprare
v3.1.1,options relate to train
v3.1.1,Alpha and Beta values for Google Length + Coverage penalty
v3.1.1,"Described here: https://arxiv.org/pdf/1609.08144.pdf, Section 7"
v3.1.1,Length penalty options
v3.1.1,Coverage penalty options
v3.1.1,Decoding Length constraint
v3.1.1,Decoding content constraint
v3.1.1,Adding options related to source and target features
v3.1.1,Adding options relate to decoding strategy
v3.1.1,Adding option for logging
v3.1.1,Adding options related to Transforms
v3.1.1,Copyright 2016 The Chromium Authors. All rights reserved.
v3.1.1,Use of this source code is governed by a BSD-style license that can be
v3.1.1,found in the LICENSE file.
v3.1.1,"Get the key 'value' in the dict, or just use 'value'"
v3.1.1,Basic attributes.
v3.1.1,Set model in training mode.
v3.1.1,Let's clean the GPUs before training loop
v3.1.1,UPDATE DROPOUT
v3.1.1,Run patience mechanism
v3.1.1,"If the patience has reached the limit, stop training"
v3.1.1,swap model params w/ moving average
v3.1.1,(and keep the original parameters)
v3.1.1,Set model in validating mode.
v3.1.1,F-prop through the model.
v3.1.1,Compute loss.
v3.1.1,Compute validation metrics (at batch.dataset level)
v3.1.1,Compute stats
v3.1.1,Update statistics.
v3.1.1,Set model back to training mode.
v3.1.1,Truncated BPTT: reminder not compatible with accum > 1
v3.1.1,1. Create truncated target.
v3.1.1,2. F-prop all but generator.
v3.1.1,3. Compute loss.
v3.1.1,Compute and save stats
v3.1.1,in theory we should divide by accum_count and bptt
v3.1.1,to rescale for each sub batch
v3.1.1,4. Update the parameters and statistics.
v3.1.1,Multi GPU gradient gather
v3.1.1,"If truncated, don't backprop fully."
v3.1.1,"in case of multi step gradient accumulation,"
v3.1.1,update only after accum batches
v3.1.1,For Flake
v3.1.1,we avoid padding while mean pooling
v3.1.1,incoming and outgoing edge embedding
v3.1.1,Find vocab data for tree builting
v3.1.1,Propogation Model
v3.1.1,Initialize the bridge layer
v3.1.1,Token embedding
v3.1.1,Initialize graph using formatted input sequence
v3.1.1,Number of flagged nodes defines node count for this sample
v3.1.1,"(Nodes can have no flags on them, but must be in 'flags' list)."
v3.1.1,The total number of integers in the vocab should allow
v3.1.1,for all features and edges to be defined.
v3.1.1,Use first extra node as only source for decoder init
v3.1.1,Average all nodes to get bridge input
v3.1.1,"LSTM has hidden and cell state, other only one"
v3.1.1,Total number of states
v3.1.1,Build a linear layer for each
v3.1.1,Initialize the bridge layer
v3.1.1,src lengths data is wrapped inside a Tensor.
v3.1.1,"LSTM has hidden and cell state, other only one"
v3.1.1,Total number of states
v3.1.1,Build a linear layer for each
v3.1.1,batch x len x dim
v3.1.1,mask is now (batch x 1 x slen x slen)
v3.1.1,1 to be expanded to number of heads in MHA
v3.1.1,Run the forward pass of every layer of the tranformer.
v3.1.1,Dimensions and padding for constructing the word embedding matrix
v3.1.1,Dimensions and padding for feature embedding matrices
v3.1.1,(these have no effect if feat_vocab_sizes is empty)
v3.1.1,The embedding matrix look-up tables. The first look-up table
v3.1.1,"is for words. Subsequent ones are for features, if any exist."
v3.1.1,The final output size of word + feature vectors. This can vary
v3.1.1,from the word vector size if and only if features are defined.
v3.1.1,This is the attribute you should access if you need to know
v3.1.1,how big your embeddings are going to be.
v3.1.1,The sequence of operations that converts the input sequence
v3.1.1,into a sequence of embeddings. At minimum this consists of
v3.1.1,looking up the embeddings for each word and feature in the
v3.1.1,input. Model parameters may require the sequence to contain
v3.1.1,additional operations as well.
v3.1.1,features must use word_vec_size
v3.1.1,features will use feat_vec_size
v3.1.1,Some utilitary functions for pretrained embeddings
v3.1.1,is this reachable?
v3.1.1,Write to file
v3.1.1,set the opt in place
v3.1.1,set the opt in place
v3.1.1,flake8: noqa
v3.1.1,For command-line option parsing
v3.1.1,"Check pass, set the args."
v3.1.1,"This SRU version implements its own cuda-level optimization,"
v3.1.1,so it requires that:
v3.1.1,1. `cupy` and `pynvrtc` python package installed.
v3.1.1,2. pytorch is built with cuda support.
v3.1.1,3. library path set: export LD_LIBRARY_PATH=<cuda lib path>.
v3.1.1,Check 1.
v3.1.1,Check 2.
v3.1.1,Check 3.
v3.1.1,This sets up device to use.
v3.1.1,-> directions x batch x dim
v3.1.1,For DEBUG
v3.1.1,"size = (length, batch, x.size(-1)) \"
v3.1.1,"if x.dim() == 3 else (batch, x.size(-1))"
v3.1.1,grad_x = x.new(*x.size()) if k_ == 3 else x.new(*size).zero_()
v3.1.1,Normal use
v3.1.1,"An entry check here, will catch on train side and translate side"
v3.1.1,if requirements are not satisfied.
v3.1.1,RNNDecoderState wraps hidden as a tuple.
v3.1.1,fh -> (layers*directions) x batch x dim
v3.1.1,This class is mainly used by decoder.py for RNNs but also
v3.1.1,by the CNN / transformer decoder when copy attention is used
v3.1.1,CNN has its own attention mechanism ConvMultiStepAttention
v3.1.1,Transformer has its own MultiHeadedAttention
v3.1.1,mlp wants it with bias
v3.1.1,"(batch, t_len, d) x (batch, d, s_len) --> (batch, t_len, s_len)"
v3.1.1,"(batch, t_len, s_len, d)"
v3.1.1,one step input
v3.1.1,"compute attention scores, as in Luong et al."
v3.1.1,Softmax or sparsemax to normalize attention weights
v3.1.1,each context vector c_t is the weighted average
v3.1.1,over all the source hidden states
v3.1.1,concatenate
v3.1.1,clamping necessary because of numerical errors: loss should be lower
v3.1.1,"bounded by zero, but negative values near zero are possible without"
v3.1.1,the clamp
v3.1.1,Shift values to be >= 0
v3.1.1,class MultiHeadedAttention(torch.jit.ScriptModule):
v3.1.1,https://arxiv.org/pdf/1803.02155.pdf
v3.1.1,in the paper they suggest either two embeds
v3.1.1,relative_key / relative_value or only
v3.1.1,relative_key. We implemented the same embed
v3.1.1,for both.
v3.1.1,@torch.jit.script_method
v3.1.1,"1) Project key, value, and query."
v3.1.1,as a reminder at training layer_cache[0] remains False
v3.1.1,2) Calculate and scale scores.
v3.1.1,batch x num_heads x query_len x key_len
v3.1.1,1 or key_len x key_len
v3.1.1,1 or key_len x key_len x dim_per_head
v3.1.1,not 100% necessary but expand to nb of heads
v3.1.1,now mask and scores have the same shape
v3.1.1,3) Apply attention dropout and compute context vectors.
v3.1.1,We use the same embeddings for key and value
v3.1.1,--------------------------------------------------------------------------
v3.1.1,MOstly copied from https://github.com/microsoft/LoRA/
v3.1.1,Copyright (c) Microsoft Corporation. All rights reserved.
v3.1.1,Licensed under the MIT License (MIT).
v3.1.1,--------------------------------------------------------------------------
v3.1.1,Optional dropout
v3.1.1,Mark the weight as unmerged
v3.1.1,LoRA implemented in a dense layer
v3.1.1,Actual trainable parameters
v3.1.1,Freezing the pre-trained weight matrix
v3.1.1,initialize A the same way as the default
v3.1.1,for nn.Linear and B to zero
v3.1.1,Make sure that the weights are not merged
v3.1.1,Merge the weights and mark it
v3.1.1,LoRA implemented in a dense layer
v3.1.1,Set this to True if the layer to replace stores
v3.1.1,"weight like (fan_in, fan_out)"
v3.1.1,Actual trainable parameters
v3.1.1,Freezing the pre-trained weight matrix
v3.1.1,initialize A the same way as the default
v3.1.1,for nn.Linear and B to zero
v3.1.1,Make sure that the weights are not merged
v3.1.1,Merge the weights and mark it
v3.1.1,LoRA implemented in a dense layer
v3.1.1,Actual trainable parameters
v3.1.1,Freezing the pre-trained weight matrix
v3.1.1,Compute the indices
v3.1.1,initialize A the same way as the default
v3.1.1,for nn.Linear and B to zero
v3.1.1,Make sure that the weights are not merged
v3.1.1,Merge the weights and mark it
v3.1.1,At the moment this class is only used by embeddings.Embeddings look-up tables
v3.1.1,-*- coding: utf-8 -*-
v3.1.1,class AverageAttention(torch.jit.ScriptModule):
v3.1.1,@torch.jit.script
v3.1.1,out_features * in_features
v3.1.1,norm is out_features * 1
v3.1.1,batch_size * out_features
v3.1.1,out_features
v3.1.1,out_features
v3.1.1,batch_size * out_features
v3.1.1,"out_channels, in_channels // groups, * kernel_size"
v3.1.1,out_features
v3.1.1,This is used nowhere in the code at the moment (Vincent Nguyen 05/18/2018)
v3.1.1,"in_channels, out_channels, *kernel_size"
v3.1.1,"in_channels, out_channels, *kernel_size"
v3.1.1,"self.out_channels, 1"
v3.1.1,out_features
v3.1.1,out_features
v3.1.1,store roots on diagonal
v3.1.1,Original probabilities.
v3.1.1,Probability of copying p(z=1) batch.
v3.1.1,Probability of not copying: p_{word}(w) * (1 - p(z))
v3.1.1,probabilities assigned by the model to the gold targets
v3.1.1,probability of tokens copied from source
v3.1.1,Set scores for unk to 0 and add eps
v3.1.1,find the indices in which you do not use the copy mechanism
v3.1.1,Drop padding.
v3.1.1,Filter out very short or very long sentences
v3.1.1,from the TM for better performance
v3.1.1,We split the `batch` and perform fuzzy matching
v3.1.1,in smaller chunks of 10.000 examples in order to
v3.1.1,reduce memory usage.
v3.1.1,Perfomance is not affected.
v3.1.1,Probably redundant but let's be safe
v3.1.1,in case some examples are already fuzzied
v3.1.1,(e.g. from another pipeline or workflow)
v3.1.1,We don't want exact matches
v3.1.1,Apply a basic filtering to leave out very short or very long
v3.1.1,sentences and speed up things a bit during fuzzy matching
v3.1.1,Do nothing
v3.1.1,We set the start number of tags to a random number from 1
v3.1.1,to 12 + the number of subsequent tags that
v3.1.1,will be added. We also apply weights to this choice so tags
v3.1.1,"are more probable to start from 1, then from 2, etc."
v3.1.1,This way we cover most scenarios met in real usage and
v3.1.1,the system will learn to handle a fairly large number of
v3.1.1,numbered tags (but not an excessively large number)
v3.1.1,Make sure we only search for exact matches (we don't want
v3.1.1,to match part of words) and perform some bound checking
v3.1.1,Make a weighted choice between paired tags or single tags.
v3.1.1,"We usually encounter, and thus here we favor, paired tags"
v3.1.1,with a ratio 1/3.
v3.1.1,Check if the tags include the
v3.1.1,"mandatory ""#"" number placeholder"""
v3.1.1,We split the user-defined tags in the # placeholder
v3.1.1,in order to number them
v3.1.1,"doc break we add it, restart new doc"
v3.1.1,case 1st ex is already longer
v3.1.1,adding cur ex is too long we add cur doc
v3.1.1,and reset doc to cur ex
v3.1.1,we start the new doc with cur ex
v3.1.1,we cumulate cur ex to cur doc
v3.1.1,Auto import python files in this directory
v3.1.1,1. sample number of tokens to corrupt
v3.1.1,2. sample positions to corrput
v3.1.1,3. sample corrupted values
v3.1.1,1. sample number of tokens to corrupt
v3.1.1,2. sample positions to corrput
v3.1.1,3. Drop token on chosen position
v3.1.1,1. sample number of tokens to corrupt
v3.1.1,2. sample positions to corrput
v3.1.1,3. mask word on chosen position
v3.1.1,"Sharing options among `TokenizerTransform`s, same name conflict in"
v3.1.1,this scope will be resolved by remove previous occurrence in parser
v3.1.1,subword regularization(or BPE dropout) options:
v3.1.1,subword vocabulary restriction options:
v3.1.1,derterministic subwording
v3.1.1,subword sampling when nbest_size > 1 or -1
v3.1.1,alpha should be 0.0 < alpha < 1.0
v3.1.1,Load vocabulary file if provided and set threshold
v3.1.1,Load Subword Model
v3.1.1,-1: keep everything (i.e. 1 mask per token)
v3.1.1,0: replace everything (i.e. no mask)
v3.1.1,1: 1 mask per span
v3.1.1,view each subword as word start / input is word level token
v3.1.1,Pretend it ends with a full stop so last span is a sentence
v3.1.1,"Tokens that are full stops, where the previous token is not"
v3.1.1,Make sure we have enough to mask
v3.1.1,Trim to masking budget
v3.1.1,Handle 0-length mask (inserts) separately
v3.1.1,assert is_word_start[-1] == 0
v3.1.1,assert tokens_length - 1 not in indices
v3.1.1,"keep index, but replace it with [MASK]"
v3.1.1,"acts as a long length, so spans don't go over the end of doc"
v3.1.1,next position from each word_start
v3.1.1,delete token: 1 mask/remove per span
v3.1.1,"keep index, but replace it with [MASK]: 1 mask per token"
v3.1.1,A bit faster when all lengths are 1
v3.1.1,to cover whole token
v3.1.1,delete token
v3.1.1,"keep index, but replace it with [MASK]"
v3.1.1,assert tokens_length - 1 not in indices
v3.1.1,prefix src/tgt for each dataset
v3.1.1,prefix as general option for inference
v3.1.1,suffix src/tgt for each dataset
v3.1.1,suffix as general option for inference
v3.1.1,!/usr/bin/env python3
v3.1.1,-*- coding: utf-8 -*-
v3.1.1,Most code taken from: https://github.com/alvations/sacremoses
v3.1.1,Which in turn is based on the Moses punctuation normalizer.
v3.1.1,https://github.com/moses-smt/mosesdecoder/blob/master/scripts/
v3.1.1,tokenizer/normalize-punctuation.perl
v3.1.1,don't fix period at end of sentence
v3.1.1,Regex substitutions from replace-unicode-punctuation.perl
v3.1.1,https://github.com/moses-smt/mosesdecoder/blob/master/
v3.1.1,scripts/tokenizer/replace-unicode-punctuation.perl
v3.1.1,Adds the penn substitutions after extra_whitespace regexes.
v3.1.1,"Optionally, replace unicode puncts BEFORE normalization."
v3.1.1,Actual normalization.
v3.1.1,"print(regexp, substitution)"
v3.1.1,print(text)
v3.1.1,"Optionally, replace unicode puncts BEFORE normalization."
v3.1.1,One source feature expected but none given and no default provided
v3.1.1,Provided default does not match required features
v3.1.1,Data not properly annotated.
v3.1.1,In this case we do not use the default as it might be an error
v3.1.1,batch 0 will always predict EOS. The other batches will predict
v3.1.1,non-eos scores.
v3.1.1,"""best"" prediction is eos - that should be blocked"
v3.1.1,include at least one prediction OTHER than EOS
v3.1.1,that is greater than -1e20
v3.1.1,now batch 0 has ended and no others have
v3.1.1,initial step
v3.1.1,batch 0 dies on step 0
v3.1.1,include at least one prediction OTHER than EOS
v3.1.1,that is greater than -1e20
v3.1.1,step 2
v3.1.1,(old) batch 8 dies on step 1
v3.1.1,step 3
v3.1.1,everything dies
v3.1.1,initial step
v3.1.1,batch 0 dies on step 0
v3.1.1,include at least one prediction OTHER than EOS
v3.1.1,that is greater than -1e20
v3.1.1,step 2
v3.1.1,(old) batch 8 dies on step 1
v3.1.1,step 3
v3.1.1,everything dies
v3.1.1,initial step
v3.1.1,finish one beam
v3.1.1,include at least one prediction OTHER than EOS
v3.1.1,that is greater than -1e20
v3.1.1,step 2
v3.1.1,finish example in last batch
v3.1.1,(old) batch 8 dies on step 1
v3.1.1,step 3
v3.1.1,everything dies
v3.1.1,initial step
v3.1.1,batch 0 dies on step 0
v3.1.1,include at least one prediction OTHER than EOS
v3.1.1,that is greater than -1e20
v3.1.1,step 2
v3.1.1,(old) batch 8 dies on step 1
v3.1.1,step 3
v3.1.1,everything dies
v3.1.1,illegal_weights_mask = torch.ByteTensor([
v3.1.1,"[0, 0, 0, 0, 0, 0, 0],"
v3.1.1,"[0, 0, 0, 1, 1, 1, 1],"
v3.1.1,"[0, 0, 0, 0, 0, 1, 1],"
v3.1.1,"[0, 0, 1, 1, 1, 1, 1]])"
v3.1.1,TODO: fix for pytorch 0.3
v3.1.1,illegal_weights = alignments.masked_select(illegal_weights_mask)
v3.1.1,"self.assertEqual(0.0, illegal_weights.data.sum())"
v3.1.1,this could be considered an integration test because it touches
v3.1.1,the filesystem for the config file (and the models)
v3.1.1,no dummy prefix
v3.1.1,no dummy prefix
v3.1.1,make sure the scalars are in the event accumulator tags
v3.1.1,required arguments
v3.1.1,transforms that require vocab will not create if not provide vocab
v3.1.1,1. Init first transform in the pipe
v3.1.1,2. Init second transform in the pipe
v3.1.1,3. Sequential combine them into a transform pipe
v3.1.1,4. apply transform pipe for example
v3.1.1,"5. example after the pipe exceed the length limit, thus filtered"
v3.1.1,6. Transform statistics registed (here for filtertoolong)
v3.1.1,"7. after report, statistics become empty as a fresh start"
v3.1.1,filter_transform.warm_up()
v3.1.1,test BPE-dropout:
v3.1.1,1. disable bpe dropout for not training example
v3.1.1,2. enable bpe dropout for training example
v3.1.1,3. (NOTE) disable dropout won't take effect if already seen
v3.1.1,this is caused by the cache mechanism in bpe:
v3.1.1,return cached subword if the original token is seen when no dropout
v3.1.1,test SP regularization:
v3.1.1,1. enable regularization for training example
v3.1.1,2. disable regularization for not training example
v3.1.1,Not apply token drop for not training example
v3.1.1,apply token drop for training example
v3.1.1,Not apply token mask for not training example
v3.1.1,apply token mask for training example
v3.1.1,require vocabs to warm_up
v3.1.1,Not apply token mask for not training example
v3.1.1,apply token mask for training example
v3.1.1,"Defalt: full_stop_token=[""."", ""?"", ""!""]"
v3.1.1,"Defalt: full_stop_token=[""."", ""?"", ""!""]"
v3.1.1,random_ratio of inserted tokens are chosen in vocab
v3.1.1,others are MASK_TOK
v3.1.1,"insert_ratio=0.0,"
v3.1.1,"random_ratio=0.0,"
v3.1.1,"Defalt: full_stop_token=[""."", ""?"", ""!""]"
v3.1.1,all token are considered as an individual word
v3.1.1,1. tokens are dropped when replace_length is 0
v3.1.1,"print(f""token delete: {masked} / {tokens}"")"
v3.1.1,2. tokens are replaced by MASK when replace_length is 1
v3.1.1,"print(f""token mask: {masked} / {tokens}"")"
v3.1.1,"insert_ratio=0.0,"
v3.1.1,"random_ratio=0.0,"
v3.1.1,"Defalt: full_stop_token=[""."", ""?"", ""!""]"
v3.1.1,start token of word are identified using subword marker
v3.1.1,"1. replace_length 0: ""words"" are dropped"
v3.1.1,"print(f""word delete: {masked} / {tokens}"")"
v3.1.1,"self.assertEqual(len(masked), n_words - n_masked)"
v3.1.1,"2. replace_length 1: ""words"" are replaced with a single MASK"
v3.1.1,"print(f""whole word single mask: {masked} / {tokens}"")"
v3.1.1,len(masked) depend on number of tokens in select word
v3.1.1,"3. replace_length -1: all tokens in ""words"" are replaced with MASK"
v3.1.1,"print(f""whole word multi mask: {masked} / {tokens}"")"
v3.1.1,number of mask_tok depend on number of tokens in selected word
v3.1.1,number of MASK_TOK can be greater than n_masked
v3.1.1,"insert_ratio=0.5,"
v3.1.1,"random_ratio=0.3,"
v3.1.1,"Defalt: full_stop_token=[""."", ""?"", ""!""]"
v3.1.1,start token of word are identified using subword marker
v3.1.1,n_words = sum(token_starts)
v3.1.1,n_masked = math.ceil(n_words * bart_noise.mask_ratio)
v3.1.1,"print(f""Text Span Infilling: {infillied} / {tokens}"")"
v3.1.1,"print(n_words, n_masked)"
v3.1.1,!/usr/bin/env python
v3.1.1,-*- coding: utf-8 -*-
v3.1.1,Inject some dummy training options that may needed when build fields
v3.1.1,Remove the generated *pt files.
v3.1.1,Remove the generated data samples
v3.1.1,all beams repeat (beam >= 1 repeat dummy scores)
v3.1.1,predict repeat_idx over and over again
v3.1.1,"before repeat, scores are either 0 or -inf"
v3.1.1,"on repeat, `repeat_idx` score is BLOCKED_SCORE"
v3.1.1,"(but it's still the best score, thus we have"
v3.1.1,"[BLOCKED_SCORE, -inf, -inf, -inf, -inf]"
v3.1.1,repetitions keeps maximizing score
v3.1.1,"index 0 has been blocked, so repeating=>+0.0 score"
v3.1.1,other indexes are -inf so repeating=>BLOCKED_SCORE
v3.1.1,which is higher
v3.1.1,beam 0 and beam >=2 will repeat (beam >= 2 repeat dummy scores)
v3.1.1,non-interesting beams are going to get dummy values
v3.1.1,"on initial round, only predicted scores for beam 0"
v3.1.1,matter. Make two predictions. Top one will be repeated
v3.1.1,"in beam zero, second one will live on in beam 1."
v3.1.1,predict the same thing in beam 0
v3.1.1,continue pushing around what beam 1 predicts
v3.1.1,"now beam 0 dies (along with the others), beam 1 -> beam 0"
v3.1.1,"now beam 0 dies (along with the others), beam 1 -> beam 0"
v3.1.1,beam 0 and beam >= 2 will repeat (beam 2 repeats excluded idx)
v3.1.1,non-interesting beams are going to get dummy values
v3.1.1,predict the same thing in beam 0
v3.1.1,continue pushing around what beam 1 predicts
v3.1.1,predict the allowed-repeat again in beam 2
v3.1.1,"now beam 0 dies, beam 1 -> beam 0, beam 2 -> beam 1"
v3.1.1,and the rest die
v3.1.1,"since all preds after i=0 are 0, we can check"
v3.1.1,that the beam is the correct idx by checking that
v3.1.1,the curr score is the initial score
v3.1.1,beam 0 will always predict EOS. The other beams will predict
v3.1.1,non-eos scores.
v3.1.1,non-interesting beams are going to get dummy values
v3.1.1,"""best"" prediction is eos - that should be blocked"
v3.1.1,include at least beam_sz predictions OTHER than EOS
v3.1.1,that are greater than -1e20
v3.1.1,predict eos in beam 0
v3.1.1,provide beam_sz other good predictions
v3.1.1,now the top beam has ended and no others have
v3.1.1,"not of interest, but want to make sure it keeps running"
v3.1.1,since only beam 0 terminates and n_best = 2
v3.1.1,"this is also a test that when block_ngram_repeat=0,"
v3.1.1,repeating is acceptable
v3.1.1,non-interesting beams are going to get dummy values
v3.1.1,"""best"" prediction is eos - that should be blocked"
v3.1.1,include at least beam_sz predictions OTHER than EOS
v3.1.1,that are greater than -1e20
v3.1.1,predict eos in beam 1
v3.1.1,provide beam_sz other good predictions in other beams
v3.1.1,beam 1 dies on min_length
v3.1.1,beam 0 dies on the step after beam 1 dies
v3.1.1,"inp_lens is tiled in initialize, reassign to make attn match"
v3.1.1,non-interesting beams are going to get dummy values
v3.1.1,"""best"" prediction is eos - that should be blocked"
v3.1.1,include at least beam_sz predictions OTHER than EOS
v3.1.1,that are greater than -1e20
v3.1.1,predict eos in beam 1
v3.1.1,provide beam_sz other good predictions in other beams
v3.1.1,no top beams are finished yet
v3.1.1,beam 1 dies on min_length
v3.1.1,no top beams are finished yet
v3.1.1,beam 0 dies on the step after beam 1 dies
v3.1.1,top beam is finished now so there are attentions
v3.1.1,two beams are finished in each batch
v3.1.1,second dim is cut down to the non-padded src length
v3.1.1,first dim is equal to the time of death
v3.1.1,(beam 0 died at current step - adjust for SOS)
v3.1.1,(beam 1 died at last step - adjust for SOS)
v3.1.1,behavior gets weird when beam is already done so just stop
v3.1.1,this is just test_beam.TestBeamAgainstReferenceCase repeated
v3.1.1,in each batch.
v3.1.1,"init_preds: [4, 3, 5, 6, 7] - no EOS's"
v3.1.1,no EOS's yet
v3.1.1,"[5, 3, 2, 6, 0], so beam 2 predicts EOS!"
v3.1.1,assumes beam 2 finished on last step
v3.1.1,ended beam 2 shouldn't continue
v3.1.1,"[2, 5, 3, 6, 0] repeat self.BATCH_SZ, so beam 0 predicts EOS!"
v3.1.1,"[-2.4879, -3.8910, -4.1010, -4.2010, -4.4010] repeat self.BATCH_SZ"
v3.1.1,another beam is finished in all batches
v3.1.1,new beam 0 finished
v3.1.1,new beam 0 is old beam 3
v3.1.1,assumes beam 0 finished on last step
v3.1.1,"[5, 2, 6, 1, 0] repeat self.BATCH_SZ, so beam 1 predicts EOS!"
v3.1.1,we finish 3 hyps per example in this step
v3.1.1,new beam 1 is old beam 3
v3.1.1,this could be considered an integration test because it tests
v3.1.1,interactions between the GNMT scorer and the beam
v3.1.1,"-data option is required, but not used in this test, so dummy."
v3.1.1,len x batch x nfeat
v3.1.1,Initialize vectors to compare size with
v3.1.1,Ensure correct sizes and types
v3.1.1,Make sure that output has the correct size and type
v3.1.1,"[('encoder_type', 'transformer'),"
v3.1.1,"('word_vec_size', 16), ('hidden_size', 16)],"
v3.1.1,""""""" Only do SRU test if requirment is safisfied. """""""
v3.1.1,SRU doesn't support input_feed.
v3.1.1,first check there's nothing unexpectedly not trainable
v3.1.1,ok: word embeddings shouldn't be trainable
v3.1.1,if word vecs are freezed
v3.1.1,ok: positional encodings shouldn't be trainable
v3.1.1,then check nothing unexpectedly trainable
v3.1.1,Decoder state
v3.1.1,Build the RNN.
v3.1.1,Set up the context gate.
v3.1.1,Set up the standard attention.
v3.1.1,The encoder hidden is  (layers*directions) x batch x dim.
v3.1.1,We need to convert it to layers x batch x (directions*dim).
v3.1.1,Init the input feed.
v3.1.1,Update the state with the result.
v3.1.1,Concatenates sequence of tensors along a new dimension.
v3.1.1,NOTE: v0.3 to 0.4: dec_outs / attns[*] may not be list
v3.1.1,(in particular in case of SRU) it was not raising error in 0.3
v3.1.1,since stack(Variable) was allowed.
v3.1.1,"In 0.4, SRU returns a tensor that shouldn't be stacke"
v3.1.1,Calculate the attention.
v3.1.1,Calculate the context gate.
v3.1.1,Additional args check.
v3.1.1,Input feed concatenates hidden state with
v3.1.1,input at every time step.
v3.1.1,TODO: context gate should be employed
v3.1.1,instead of second RNN transform.
v3.1.1,Update the coverage attention.
v3.1.1,"attns[""coverage""] is actually c^(t+1) of See et al(2017)"
v3.1.1,1-index shifted
v3.1.1,Decoder State
v3.1.1,CNNDecoder has its own attention mechanism.
v3.1.1,Set up a separate copy attention layer if needed.
v3.1.1,The output of CNNEncoder.
v3.1.1,The combination of output of CNNEncoder and source embeddings.
v3.1.1,Process the result and update the attentions.
v3.1.1,Update the state.
v3.1.1,TODO change the way attns is returned dict => list or tuple (onnx)
v3.1.1,src_len is a single tensor shared between all models.
v3.1.1,This assumption will not hold if Translator is modified
v3.1.1,to calculate src_len as something other than the length
v3.1.1,of the input.
v3.1.1,"return _, (B, Q_len, K_len)"
v3.1.1,"layer average attention across heads, get ``(B, Q, K)``"
v3.1.1,"Case 1: no full_context, no align heads -> layer avg baseline"
v3.1.1,"Case 2: no full_context, 1 align heads -> guided align"
v3.1.1,"Case 3: full_context, 1 align heads -> full cte guided align"
v3.1.1,BoolTensor was introduced in pytorch 1.2
v3.1.1,T: could be 1 in the case of stepwise decoding or tgt_len
v3.1.1,masking is necessary when sequence length is greater than one
v3.1.1,mask now are (batch x 1 x tlen x s or t len)
v3.1.1,1 = heads to be expanded in MHA
v3.1.1,Decoder State
v3.1.1,"previously, there was a GlobalAttention module here for copy"
v3.1.1,"attention. But it was never actually used -- the ""copy"" attention"
v3.1.1,just reuses the context attention.
v3.1.1,"attns[""align""] = torch.stack(attn_aligns, 0).mean(0)  # All avg"
v3.1.1,TODO change the way attns is returned dict => list or tuple (onnx)
v3.1.1,first value set to True triggered by the beginning of decoding
v3.1.1,layer_cache becomes active in the MultiHeadedAttention fwd
v3.1.1,T: could be 1 in the case of stepwise decoding or tgt_len
v3.1.1,masking is necessary when sequence length is greater than one
v3.1.1,mask now are (batch x 1 x tlen x tlen)
v3.1.1,1 = heads to be expanded in MHA
v3.1.1,TODO change the way attns is returned dict => list or tuple (onnx)
v3.1.1,"buffer size in bytes, determine equiv. # of elements based on data type"
v3.1.1,copy tensors into buffer_t
v3.1.1,all-reduce and rescale
v3.1.1,copy all-reduced buffer back into tensors
v3.1.1,"print(filled, sz)"
v3.1.1,"tensor is bigger than buffer, all-reduce and rescale directly"
v3.1.1,"buffer is full, all-reduce and replace buffer with grad"
v3.1.1,add tensor to buffer
v3.1.1,"propagate exception to parent process, keeping original traceback"
v3.1.1,TODO: Find a better way to check for sparse gradients.
v3.1.1,we use apex.amp
v3.1.1,In this case use the old FusedAdam with
v3.1.1,FP16_optimizer wrapper
v3.1.1,Load everything from the checkpoint.
v3.1.1,Build everything from scratch.
v3.1.1,"Reset optimizer, keep options."
v3.1.1,"Reset options, keep optimizer."
v3.1.1,State can be partially restored.
v3.1.1,should be: self._optimizer.zero_grad(set_to_none)
v3.1.1,but apex.amp is not up-to-date:
v3.1.1,https://github.com/NVIDIA/apex/blob/master/apex/amp/_process_optimizer.py#L367
v3.1.1,"unscaled optimizer's gradients (already done therefore skip),"
v3.1.1,skips optimizer.step() if gradients contain infs/NaNs.
v3.1.1,Updates the scale for next iteration.
v3.1.1,Code below is an implementation of https://arxiv.org/pdf/1804.04235.pdf
v3.1.1,inspired but modified from https://github.com/DeadAt0m/adafactor-pytorch
v3.1.1,backward compatibility
v3.1.1,assuming a list/generator of parameter means single group
v3.1.1,compute combined scale factor for this group
v3.1.1,norm is in fact norm*scale
v3.1.1,note: p.grad should not ever be set for correct operation of
v3.1.1,mixed precision optimizer that sometimes sends None gradients
v3.1.1,State initialization
v3.1.1,Exponential moving average of gradient values
v3.1.1,Exponential moving average of squared gradient values
v3.1.1,-*- coding: utf-8 -*-
v3.1.1,placing this here make it easier to call logger.info
v3.1.1,"from anywhere, just 'from onmt.utils.logging import logger'"
v3.1.1,"align_head contains value in [0, 1) presenting attn prob,"
v3.1.1,0 was resulted by the context attention src_pad_mask
v3.1.1,"So, the correspand position in ref_align should also be 0"
v3.1.1,"Therefore, clip align_head to > 1e-18 should be bias free."
v3.1.1,rescale with tau (temperature) and apply the log_softmax.
v3.1.1,ct2 expects src with lengths without padding
v3.1.1,again we use raw probs to rescale with tau and apply log_softmax
v3.1.1,lm_scores are in log space so log_target=True
v3.1.1,rescale with tau (temperature) and apply the log_softmax.
v3.1.1,ct2 expects src with lengths without padding
v3.1.1,again we use raw probs to rescale with tau and apply log_softmax
v3.1.1,lm_scores are in log space so log_target=True
v3.1.1,take into account here the tgt_shift_index (0 / 1 = LM/NMT)
v3.1.1,Correct target copy token instead of <unk>
v3.1.1,tgt[i] = align[i] + len(tgt_vocab)
v3.1.1,for i such that tgt[i] == 0 and align[i] != 0
v3.1.1,in the case criterion reduction is None then we need
v3.1.1,to sum the loss of each sentence in the batch
v3.1.1,Check Transforms
v3.1.1,Check path
v3.1.1,tgt is src for LM task
v3.1.1,Check weight
v3.1.1,Check features
v3.1.1,validation when train:
v3.1.1,Check embeddings stuff
v3.1.1,"Backward compatibility with ""fix_word_vecs_*"" opts"
v3.1.1,encoder and decoder should be same sizes
v3.1.1,"Load default opt values, then overwrite with the opts in"
v3.1.1,"the checkpoint. That way, if there are new options added,"
v3.1.1,the defaults are used.
v3.1.1,It comes from training
v3.1.1,TODO: needs to be added as inference opt
v3.1.1,Don't do anything
v3.1.1,Update best score of each criteria
v3.1.1,Reset tolerance
v3.1.1,Update current status
v3.1.1,Decrease tolerance
v3.1.1,Log
v3.1.1,Log
v3.1.1,Get a list of world_size lists with len(stat_list) Statistics objects
v3.1.1,"this param init is overridden by model_builder, useless then."
v3.1.1,SRU doesn't support PackedSequence.
v3.1.1,-*- coding: utf-8 -*-
v3.1.1,threshold on 1 to avoid div by 0
v3.1.1,treat alignment matrix one by one as each have different lengths
v3.1.1,No alignment if not exist valid tgt token
v3.1.1,get valid alignment (sub-matrix from full paded aligment matrix)
v3.1.1,Helper functions
v3.1.1,Keeps track of the original words/subwords
v3.1.1,('prior_tokenization' option)
v3.1.1,In case there is a final case_markup when new_spacer is on
v3.1.1,translate
v3.1.1,for validation we build an infer_iter per batch
v3.1.1,in order to avoid oom issues because there is no
v3.1.1,batching strategy in `textbatch_to_tensor`
v3.1.1,apply_reverse refs
v3.1.1,flatten preds
v3.1.1,save results
v3.1.1,-*- coding: utf-8 -*-
v3.1.1,this one is needed for Random Shuffler of batches
v3.1.1,in multi gpu it ensures datasets are read in the same order
v3.1.1,some cudnn methods can be random even after fixing the seed
v3.1.1,unless you tell it to be deterministic
v3.1.1,This one is needed for various tranfroms
v3.1.1,These ensure same initialization in multi gpu mode
v3.1.1,we need to check the model path + any tokenizer path
v3.1.1,patch to log stdout spawned processes of dataloader
v3.1.1,bucket_size = batch_size
v3.1.1,For TRAIN we need to group examples by length
v3.1.1,"for faster performance, but otherwise, sequential."
v3.1.1,For TRAIN we shuffle batches within the bucket
v3.1.1,otherwise sequential
v3.1.1,for specific case of rnn_packed need to be sorted
v3.1.1,within the batch
v3.1.1,Check if all tokens have features or none at all
v3.1.1,Make features part of src like
v3.1.1,"{'src': {'src': ..., 'feats': [...., ....]}}"
v3.1.1,at this point an example looks like:
v3.1.1,"{'src': {'src': ..., 'feats': [....]},"
v3.1.1,"'tgt': {'tgt': ...},"
v3.1.1,"'src_original': ['tok1', ...'tokn'],"
v3.1.1,"'tgt_original': ['tok1', ...'tokm'],"
v3.1.1,'indices' : seq in bucket
v3.1.1,"'align': ...,"
v3.1.1,}
v3.1.1,Need to add features in last dimensions
v3.1.1,Keep it consistent with dynamic data
v3.1.1,make a small vocab containing just the tokens in the source sequence
v3.1.1,Map source tokens to indices in the dynamic dict.
v3.1.1,-*- coding: utf-8 -*-
v3.1.1,'src_original' and 'tgt_original' store the
v3.1.1,original line before tokenization. These
v3.1.1,fields are used later on in the feature
v3.1.1,transforms.
v3.1.1,NOTE: moved to dynamic_iterator.py cf process()
v3.1.1,item = self.transform.apply(
v3.1.1,"example, is_train=self.infinitely, corpus_name=self.cid)"
v3.1.1,empty example: skip
v3.1.1,"No encoder in LM, seq2seq count formatting kept"
v3.1.1,_check_save_model_path
v3.1.1,for future use?
v3.1.1,if we want to save the LoRa model state_dict only
v3.1.1,"model_state_dict = lora_state_dict(model, bias='lora_only')"
v3.1.1,and comment the line below
v3.1.1,NOTE: We need to trim the vocab to remove any unk tokens that
v3.1.1,were not originally here.
v3.1.1,"for side in [""src"", ""tgt""]:"
v3.1.1,keys_to_pop = []
v3.1.1,"if hasattr(vocab[side], ""fields""):"
v3.1.1,unk_token = vocab[side].fields[0][1].vocab.itos[0]
v3.1.1,"for key, value in vocab[side].fields[0][1].vocab.stoi.items():"
v3.1.1,if value == 0 and key != unk_token:
v3.1.1,keys_to_pop.append(key)
v3.1.1,for key in keys_to_pop:
v3.1.1,"vocab[side].fields[0][1].vocab.stoi.pop(key, None)"
v3.1.1,!/usr/bin/env python
v3.1.1,!/usr/bin/env python
v3.1.1,!/usr/bin/env python
v3.1.1,-*- coding: utf-8 -*-
v3.1.1,!/usr/bin/env python
v3.1.1,!/usr/bin/env python
v3.1.1,!/usr/bin/env python
v3.1.1,Set sharing strategy manually instead of default based on the OS.
v3.1.1,torch.multiprocessing.set_sharing_strategy('file_system')
v3.1.1,Create a thread to listen for errors in the child processes.
v3.1.1,Train with multiprocessing.
v3.1.1,magic indices
v3.1.1,result caching
v3.1.1,Here we set the decoder to start with self.start (BOS or EOS)
v3.1.1,fix length constraint and remove eos from count
v3.1.1,add one to account for BOS. Don't account for EOS because hitting
v3.1.1,this implies it hasn't been found.
v3.1.1,we don't block nothing if the user doesn't want it
v3.1.1,we can't block nothing beam's too short
v3.1.1,we check paths one by one
v3.1.1,we don't forbid nothing if the user doesn't want it
v3.1.1,we can't forbid nothing if beam's too short
v3.1.1,Reordering forbidden_tokens following beam selection
v3.1.1,We rebuild a dict to ensure we get the value and not the pointer
v3.1.1,Grabing the newly selected tokens and associated ngram
v3.1.1,skip the blocking if any token in current_ngram is excluded
v3.1.1,"pickups: Tensor where specified index were set to 1, others 0"
v3.1.1,"dropdowns: opposite of pickups, 1 for those shouldn't pick"
v3.1.1,Minus dropdowns to log_probs making probabilities of
v3.1.1,unspecified index close to 0
v3.1.1,"prediction step have surpass length of given target_prefix,"
v3.1.1,no need to further change this attr
v3.1.1,keep indices until overflowing p
v3.1.1,Set all logits that are not in the top-p to -10000.
v3.1.1,This puts the probabilities close to 0.
v3.1.1,Set all logits that are not in the top-k to -10000.
v3.1.1,This puts the probabilities close to 0.
v3.1.1,"For temp=0.0, take the argmax to avoid divide-by-zero errors."
v3.1.1,keep_topk=1 is also equivalent to argmax.
v3.1.1,maybe fix some prediction at this step by modifying log_probs
v3.1.1,"shape: (sum(~ self.is_finished), 1)"
v3.1.1,in LM task src_len is associated with currently generated src
v3.1.1,and therefore needs to follow the generation
v3.1.1,!/usr/bin/env python
v3.1.1,for debugging
v3.1.1,TODO: maybe add dynamic part
v3.1.1,Statistics
v3.1.1,Here we handle the cases of mismatch in number of segments
v3.1.1,between source and target. We re-translate seg by seg.
v3.1.1,those two should be the same except feat dim
v3.1.1,"batch['src'][perm[j], :, :])"
v3.1.1,trans.src
v3.1.1,we rebuild a small batch made of the sub-segments
v3.1.1,in the long segment.
v3.1.1,new sub-batch ready to be translated
v3.1.1,we re-insert the sub-batch in the initial translations
v3.1.1,In the case of length_penalty = none we report the total logprobs
v3.1.1,divided by the number of sentence to get an approximation of the
v3.1.1,per sentence logprob. We also return the corresponding ppl
v3.1.1,"When a length_penalty is used eg: ""avg"" or ""wu"" since logprobs"
v3.1.1,are normalized per token we report the per line per token logprob
v3.1.1,"and the corresponding ""per word perplexity"""
v3.1.1,Turn any copied words into UNKs.
v3.1.1,"Decoder forward, takes [batch, tgt_len, nfeats] as input"
v3.1.1,"and [batch, src_len, hidden] as enc_out"
v3.1.1,"in case of inference tgt_len = 1, batch = beam times batch_size"
v3.1.1,"in case of Gold Scoring tgt_len = actual length, batch = 1 batch"
v3.1.1,Generator forward.
v3.1.1,"returns [(batch_size x beam_size) , vocab ] when 1 step"
v3.1.1,"or [batch_size, tgt_len, vocab ] when full sentence"
v3.1.1,"here we have scores [tgt_lenxbatch, vocab] or [beamxbatch, vocab]"
v3.1.1,at this point scores is batch first (dim=0)
v3.1.1,"returns [(batch_size x beam_size) , vocab ] when 1 step"
v3.1.1,"or [batch_size, tgt_len, vocab ] when full sentence"
v3.1.1,(0) add BOS and padding to tgt prediction
v3.1.1,(1) Encoder forward.
v3.1.1,(2) Repeat src objects `n_best` times.
v3.1.1,"We use batch_size x n_best, get ``(batch * n_best, src_len, nfeat)``"
v3.1.1,Quick fix. Transformers return None as enc_states.
v3.1.1,enc_states are only used later on to init decoder's state
v3.1.1,"but are never used in Transformer decoder, so we can skip"
v3.1.1,"(3) Init decoder with n_best src,"
v3.1.1,"reshape tgt to ``(len, batch * n_best, nfeat)``"
v3.1.1,it should be done in a better way
v3.1.1,here dec_in is batch first
v3.1.1,masked_select
v3.1.1,get aligned src id for each prediction's valid tgt tokens
v3.1.1,TODO: support these blacklisted features
v3.1.1,(0) Prep the components of the search.
v3.1.1,(1) Run the encoder on the src.
v3.1.1,(2) prep decode_strategy. Possibly repeat src objects.
v3.1.1,(3) Begin decoding step by step:
v3.1.1,"decoder_input = decode_strategy.current_predictions.view(1, -1,"
v3.1.1,1)
v3.1.1,Reorder states.
v3.1.1,TODO: support these blacklisted features
v3.1.1,(0) Prep the components of the search.
v3.1.1,(1) split src into src and target_prefix to avoid padding.
v3.1.1,(2) init decoder
v3.1.1,(3) prep decode_strategy. Possibly repeat src objects.
v3.1.1,(4) Begin decoding step by step:
v3.1.1,Reorder states.
v3.1.1,select indexes in model state/cache
v3.1.1,beam parameters
v3.1.1,beam state
v3.1.1,BoolTensor was introduced in pytorch 1.2
v3.1.1,"""global state"" of the old beam"
v3.1.1,buffers for the topk scores and 'backpointer'
v3.1.1,for testing
v3.1.1,maybe fix some prediction at this step by modifying log_probs
v3.1.1,Flatten probs into a list of possibilities.
v3.1.1,Penalize beams that finished.
v3.1.1,"on real data (newstest2017) with the pretrained transformer,"
v3.1.1,it's faster to not move this back to the original device
v3.1.1,Store finished hypotheses for this batch.
v3.1.1,End condition is the top beam finished and we can return
v3.1.1,n_best hypotheses.
v3.1.1,"If all sentences are translated, no need to go further."
v3.1.1,Remove finished batches for the next step.
v3.1.1,using integer division to get an integer _B without casting
v3.1.1,force the output to be longer than self.min_length
v3.1.1,Multiply probs by the beam probability.
v3.1.1,"if the sequence ends now, then the penalty is the current"
v3.1.1,"length + 1, to include the EOS token"
v3.1.1,Avoid any direction that would repeat unwanted ngrams
v3.1.1,Pick up candidate token by curr_scores
v3.1.1,Recover log probs.
v3.1.1,Length penalty is just a scalar. It doesn't matter if it's applied
v3.1.1,before or after the topk.
v3.1.1,Resolve beam origin and map to batch index flat representation.
v3.1.1,Append last prediction.
v3.1.1,update global state (step == 1)
v3.1.1,update global state (step > 1)
v3.1.1,"shape: (batch_size x beam_size, 1)"
v3.1.1,in LM task src_len is associated with currently generated src
v3.1.1,and therefore needs to follow the generation
v3.1.1,in LM task src_len is associated with currently generated src
v3.1.1,and therefore needs to follow the generation
v3.1.1,Term will be subtracted from probability
v3.1.1,Probability will be divided by this
v3.1.1,these warnings indicate that either the alpha/beta
v3.1.1,"forces a penalty to be a no-op, or a penalty is a no-op but"
v3.1.1,the alpha/beta would suggest otherwise.
v3.1.1,using some coverage penalty
v3.1.1,!/usr/bin/env python
v3.1.1,semaphore doesn't have a timeout arg in Python 2.7
v3.1.1,perform a first request to initialize everything
v3.1.1,backwards compatibility for confs
v3.1.1,every segment becomes a dict for flexibility purposes
v3.1.1,NOTE: translator returns lists of `n_best` list
v3.1.1,build back results with empty texts
v3.1.1,load can be called multiple times: modify copy
v3.1.1,output contain alignment
v3.1.1,Below are all the different penalty terms implemented so far.
v3.1.1,Subtract coverage penalty from topk log probs.
v3.1.1,Divide topk log probs by length penalty.
v3.1.1,Sorting
v3.1.1,Chinese segmentation
v3.1.1,Chinese simplify -> Chinese traditional standard
v3.1.1,Chinese simplify -> Chinese traditional (HongKong)
v3.1.1,Chinese simplify -> Chinese traditional (Taiwan)
v3.1.1,Chinese traditional -> Chinese simplify (v1)
v3.1.1,Chinese traditional -> Chinese simplify (v2)
v3.1.1,Auto import python files in this directory
v3.1.0,!/usr/bin/env python
v3.1.0,!/usr/bin/env python
v3.1.0,!/usr/bin/env python
v3.1.0,!/usr/bin/env python
v3.1.0,!/usr/bin/env python
v3.1.0,!/usr/bin/env python3
v3.1.0,-*- coding: utf-8 -*-
v3.1.0,
v3.1.0,"OpenNMT-py documentation build configuration file, created by"
v3.1.0,sphinx-quickstart on Sun Dec 17 12:07:14 2017.
v3.1.0,
v3.1.0,This file is execfile()d with the current directory set to its
v3.1.0,containing dir.
v3.1.0,
v3.1.0,Note that not all possible configuration values are present in this
v3.1.0,autogenerated file.
v3.1.0,
v3.1.0,All configuration values have a default; values that are commented out
v3.1.0,serve to show the default.
v3.1.0,"If extensions (or modules to document with autodoc) are in another directory,"
v3.1.0,add these directories to sys.path here. If the directory is relative to the
v3.1.0,"documentation root, use os.path.abspath to make it absolute, like shown here."
v3.1.0,
v3.1.0,import os
v3.1.0,import sys
v3.1.0,"sys.path.insert(0, os.path.abspath('.'))"
v3.1.0,-- General configuration ------------------------------------------------
v3.1.0,"If your documentation needs a minimal Sphinx version, state it here."
v3.1.0,
v3.1.0,needs_sphinx = '6.0'
v3.1.0,"Add any Sphinx extension module names here, as strings. They can be"
v3.1.0,extensions coming with Sphinx (named 'sphinx.ext.*') or your custom
v3.1.0,ones.
v3.1.0,Show base classes
v3.1.0,"Use ""variables"" section for Attributes instead of weird block things"
v3.1.0,mimicking the function style.
v3.1.0,"Add any paths that contain templates here, relative to this directory."
v3.1.0,The suffix(es) of source filenames.
v3.1.0,You can specify multiple suffix as a list of string:
v3.1.0,
v3.1.0,"source_suffix = ['.rst', '.md']"
v3.1.0,The master toctree document.
v3.1.0,General information about the project.
v3.1.0,"The version info for the project you're documenting, acts as replacement for"
v3.1.0,"|version| and |release|, also used in various other places throughout the"
v3.1.0,built documents.
v3.1.0,
v3.1.0,The short X.Y version.
v3.1.0,"The full version, including alpha/beta/rc tags."
v3.1.0,The language for content autogenerated by Sphinx. Refer to documentation
v3.1.0,for a list of supported languages.
v3.1.0,
v3.1.0,This is also used if you do content translation via gettext catalogs.
v3.1.0,"Usually you set ""language"" from the command line for these cases."
v3.1.0,"List of patterns, relative to source directory, that match files and"
v3.1.0,directories to ignore when looking for source files.
v3.1.0,This patterns also effect to html_static_path and html_extra_path
v3.1.0,The name of the Pygments (syntax highlighting) style to use.
v3.1.0,"If true, `todo` and `todoList` produce output, else they produce nothing."
v3.1.0,-- Options for HTML output ----------------------------------------------
v3.1.0,The theme to use for HTML and HTML Help pages.  See the documentation for
v3.1.0,a list of builtin themes.
v3.1.0,
v3.1.0,html_theme = 'sphinx_materialdesign_theme'
v3.1.0,html_theme_path = [sphinx_materialdesign_theme.get_path()]
v3.1.0,Theme options are theme-specific and customize the look and feel of a theme
v3.1.0,"further.  For a list of options available for each theme, see the"
v3.1.0,documentation.
v3.1.0,
v3.1.0,html_theme_options = {}
v3.1.0,"Add any paths that contain custom static files (such as style sheets) here,"
v3.1.0,"relative to this directory. They are copied after the builtin static files,"
v3.1.0,"so a file named ""default.css"" will overwrite the builtin ""default.css""."
v3.1.0,"Custom sidebar templates, must be a dictionary that maps document names"
v3.1.0,to template names.
v3.1.0,
v3.1.0,This is required for the alabaster theme
v3.1.0,refs: http://alabaster.readthedocs.io/en/latest/installation.html#sidebars
v3.1.0,-- Options for HTMLHelp output ------------------------------------------
v3.1.0,Output file base name for HTML help builder.
v3.1.0,-- Options for LaTeX output ---------------------------------------------
v3.1.0,The paper size ('letterpaper' or 'a4paper').
v3.1.0,
v3.1.0,"'papersize': 'letterpaper',"
v3.1.0,"The font size ('10pt', '11pt' or '12pt')."
v3.1.0,
v3.1.0,"'pointsize': '10pt',"
v3.1.0,Additional stuff for the LaTeX preamble.
v3.1.0,
v3.1.0,"'preamble': '',"
v3.1.0,Latex figure (float) alignment
v3.1.0,
v3.1.0,"'figure_align': 'htbp',"
v3.1.0,Grouping the document tree into LaTeX files. List of tuples
v3.1.0,"(source start file, target name, title,"
v3.1.0,"author, documentclass [howto, manual, or own class])."
v3.1.0,-- Options for manual page output ---------------------------------------
v3.1.0,One entry per manual page. List of tuples
v3.1.0,"(source start file, name, description, authors, manual section)."
v3.1.0,-- Options for Texinfo output -------------------------------------------
v3.1.0,Grouping the document tree into Texinfo files. List of tuples
v3.1.0,"(source start file, target name, title, author,"
v3.1.0,"dir menu entry, description, category)"
v3.1.0,!/usr/bin/env python3
v3.1.0,Usage: python3 filter_train.py in.src in.trg out.src out.trg max-tokens
v3.1.0,!/usr/bin/env python
v3.1.0,-*- coding: utf-8 -*-
v3.1.0,is this reachable?
v3.1.0,Read in embeddings
v3.1.0,Write to file
v3.1.0,converts a SentencePiece vocabulary to the format expected by dynamic data
v3.1.0,"(essentially converts float expected counts to ""fixed precision"" int pseudo"
v3.1.0,counts)
v3.1.0,"Add in default model arguments, possibly added since training."
v3.1.0,this patch is no longer needed included in converter
v3.1.0,"if hasattr(model_opt, 'rnn_size'):"
v3.1.0,model_opt.hidden_size = model_opt.rnn_size
v3.1.0,build_base_model expects updated and validated opts
v3.1.0,-*- encoding: utf-8 -*-
v3.1.0,!/usr/bin/env python
v3.1.0,-*- coding: utf-8 -*-
v3.1.0,Author: Rico Sennrich
v3.1.0,flake8: noqa
v3.1.0,This file is retrieved from https://github.com/rsennrich/subword-nmt
v3.1.0,hack for python2/3 compatibility
v3.1.0,check version information
v3.1.0,some hacking to deal with duplicates (only consider first instance)
v3.1.0,don't print end-of-word symbols
v3.1.0,sys.stderr.write('cannot split {0} further.\n'.format(segment))
v3.1.0,sys.stderr.write('OOV: {0}\n'.format(segment))
v3.1.0,sys.stderr.write('OOV: {0}\n'.format(segment))
v3.1.0,python 2/3 compatibility
v3.1.0,read/write files as UTF-8
v3.1.0,!/usr/bin/env python3
v3.1.0,coding: utf-8
v3.1.0,"In order to use this tool, please install comet first"
v3.1.0,https://github.com/Unbabel/COMET
v3.1.0,Let's say you have a source file with N sentences in SL - eg: source.sl
v3.1.0,and the corresponding references (N sentences) reference.tl
v3.1.0,Translate your file in TL with the -n_best nbest options nbest being
v3.1.0,then number of hypotheses and output the target to -output target.nbest.tl
v3.1.0,Then you need to duplicate source and reference sentences nbest times
v3.1.0,for this script.
v3.1.0,for instance using awk '{for(i=1; i<=n; i++) print}' n=5 reference.tl \
v3.1.0,> reference.5.tl
v3.1.0,same for source.
v3.1.0,This script can be run (for instance with nbest = 5) as follows:
v3.1.0,python oracle_bleu.py --nbest-src source.5.sl --nbest-hyp target.5.tl \
v3.1.0,--nbest-ref reference.5.tl --nbest-order 5 --output target.maxbleu.tl
v3.1.0,It will search in all hyp the best comet score
v3.1.0,when choosing a reference-less model no nbest-ref is required
v3.1.0,for nbest in nbests:
v3.1.0,!/usr/bin/env python
v3.1.0,!/usr/bin/env python3
v3.1.0,coding: utf-8
v3.1.0,Let's say you have a source file with N sentences in SL - eg: source.sl
v3.1.0,Translate your file in TL with the -n_best nbest options nbest being
v3.1.0,then number of hypotheses and output the target to -output target.nbest.tl
v3.1.0,This script can be run (for instance with nbest = 5) as follows:
v3.1.0,python mbr_bleu.py --nbest-hyp target.5.tl \
v3.1.0,--nbest-order 5 --output target.mbr.tl
v3.1.0,It will compare all hyp with eachother and output the max bleu
v3.1.0,!/usr/bin/env python
v3.1.0,-*- coding: utf-8 -*-
v3.1.0,Author: Rico Sennrich
v3.1.0,flake8: noqa
v3.1.0,This file is retrieved from https://github.com/rsennrich/subword-nmt
v3.1.0,hack for python2/3 compatibility
v3.1.0,"find all instances of pair, and update frequency/indices around it"
v3.1.0,find first symbol
v3.1.0,"if first symbol is followed by second symbol, we've found an occurrence of pair (old_word[i:i+2])"
v3.1.0,"assuming a symbol sequence ""A B C"", if ""B C"" is merged, reduce the frequency of ""A B"""
v3.1.0,"assuming a symbol sequence ""A B C B"", if ""B C"" is merged, reduce the frequency of ""C B""."
v3.1.0,"however, skip this if the sequence is A B C B C, because the frequency of ""C B"" will be reduced by the previous code block"
v3.1.0,find new pair
v3.1.0,"assuming a symbol sequence ""A BC D"", if ""B C"" is merged, increase the frequency of ""A BC"""
v3.1.0,"assuming a symbol sequence ""A BC B"", if ""B C"" is merged, increase the frequency of ""BC B"""
v3.1.0,"however, if the sequence is A BC BC, skip this step because the count of ""BC BC"" will be incremented by the previous code block"
v3.1.0,data structure of pair frequencies
v3.1.0,index from pairs to words
v3.1.0,version 0.2 changes the handling of the end-of-word token ('</w>');
v3.1.0,version numbering allows bckward compatibility
v3.1.0,"threshold is inspired by Zipfian assumption, but should only affect speed"
v3.1.0,we probably missed the best pair because of pruning; go back to full statistics
v3.1.0,"threshold is inspired by Zipfian assumption, but should only affect speed"
v3.1.0,python 2/3 compatibility
v3.1.0,read/write files as UTF-8
v3.1.0,Now we can pipe the full file through the model using the Iterator
v3.1.0,reminder a batch includes .src .tgt .indices and it is sorted
v3.1.0,Compute and retrieve the loss for EACH sentence
v3.1.0,Now we need to rearrange the batch of ppl
v3.1.0,in the original order with indices
v3.1.0,!/usr/bin/env python
v3.1.0,-*- coding: utf-8 -*-
v3.1.0,!/usr/bin/env python
v3.1.0,!/usr/bin/env python3
v3.1.0,coding: utf-8
v3.1.0,Let's say you have a source file with N sentences in SL - eg: source.sl
v3.1.0,and the corresponding references (N sentences) reference.tl
v3.1.0,Translate your file in TL with the -n_best nbest options nbest being
v3.1.0,then number of hypotheses and output the target to -output target.nbest.tl
v3.1.0,Then you need to duplicate reference sentences nbest times for this script.
v3.1.0,for instance using awk '{for(i=1; i<=n; i++) print}' n=5 reference.tl \
v3.1.0,> reference.5.tl
v3.1.0,This script can be run (for instance with nbest = 5) as follows:
v3.1.0,python oracle_bleu.py --nbest-hyp target.5.tl --nbest-ref reference.5.tl \
v3.1.0,--nbest-order 5 --output target.maxbleu.tl
v3.1.0,It will search in all hyp the best bleu wrt reference
v3.1.0,and output the max bleu
v3.1.0,!/usr/bin/env python
v3.1.0,with the two module = imp.load_source() below
v3.1.0,we ghost the old torchtext.data.field and depercated
v3.1.0,onmt.inputters.text_dataset
v3.1.0,however this require some functions / classes to be
v3.1.0,monkey patched for loading the old field/vocab objects.
v3.1.0,"module3 = imp.load_source(""Vocab"", ""tools/convertv2_v3.py"")"
v3.1.0,"voc = dict(sorted(fields.vocab.__dict__['freqs'].items(),"
v3.1.0,"key=lambda x: (-x[1], x[0]))).keys()"
v3.1.0,"voc = dict(sorted(fields.vocab.__dict__['freqs'].items(),"
v3.1.0,"key=lambda x: (-x[1], x[0]))).keys()"
v3.1.0,Patch for NLLB200 model loading
v3.1.0,Avoid functionality on inference
v3.1.0,Build embeddings.
v3.1.0,Build encoder.
v3.1.0,Build embeddings.
v3.1.0,Build decoder.
v3.1.0,Share the embedding matrix - preprocess with share_vocab required.
v3.1.0,src/tgt vocab should be the same if `-share_vocab` is specified.
v3.1.0,Update vocabulary embeddings with checkpoint embeddings
v3.1.0,Embedding layers
v3.1.0,Just for debugging purposes
v3.1.0,Remove old vocabulary associated embeddings
v3.1.0,for back compat when attention_dropout was not defined
v3.1.0,Build Model
v3.1.0,Build Generator.
v3.1.0,Load the model states from checkpoint or initialize them.
v3.1.0,This preserves backward-compat for models using customed layernorm
v3.1.0,end of patch for backward compatibility
v3.1.0,Update model embeddings with those from the checkpoint
v3.1.0,after initialization
v3.1.0,!/usr/bin/env python
v3.1.0,if transform + options set in 'valid' we need to copy in main
v3.1.0,transform / options for scoring considered as inference
v3.1.0,"maybe prepare pretrained embeddings, if any"
v3.1.0,Load checkpoint if we resume from a previous training.
v3.1.0,ensure tensorboard output is written in the directory
v3.1.0,of previous checkpoints
v3.1.0,Override checkpoint's update_embeddings as it defaults to false
v3.1.0,Override checkpoint's freezing settings as it defaults to false
v3.1.0,NOTE: It's important that ``opt`` has been validated and updated
v3.1.0,at this point.
v3.1.0,Build model.
v3.1.0,Build optimizer.
v3.1.0,Build model saver
v3.1.0,Use Tensorboard for visualization during training
v3.1.0,Options only during inference
v3.1.0,"Truncation options, for text corpus"
v3.1.0,"as for False, this will be added in _add_train_general_opts"
v3.1.0,Embedding Options
v3.1.0,Model Task Options
v3.1.0,Encoder-Decoder Options
v3.1.0,Freeze Encoder and/or Decoder
v3.1.0,The following options (bridge_extra_node to n_steps) are used
v3.1.0,for training with --encoder_type ggnn (Gated Graph Neural Network).
v3.1.0,Attention options
v3.1.0,Alignement options
v3.1.0,Generator and loss options.
v3.1.0,GPU
v3.1.0,LoRa
v3.1.0,Init options
v3.1.0,Pretrained word vectors
v3.1.0,Freeze word vectors
v3.1.0,Optimization options
v3.1.0,learning rate
v3.1.0,options relate to data preprare
v3.1.0,options relate to train
v3.1.0,Alpha and Beta values for Google Length + Coverage penalty
v3.1.0,"Described here: https://arxiv.org/pdf/1609.08144.pdf, Section 7"
v3.1.0,Length penalty options
v3.1.0,Coverage penalty options
v3.1.0,Decoding Length constraint
v3.1.0,Decoding content constraint
v3.1.0,Adding options related to source and target features
v3.1.0,Adding options relate to decoding strategy
v3.1.0,Adding option for logging
v3.1.0,Adding options related to Transforms
v3.1.0,Copyright 2016 The Chromium Authors. All rights reserved.
v3.1.0,Use of this source code is governed by a BSD-style license that can be
v3.1.0,found in the LICENSE file.
v3.1.0,"Get the key 'value' in the dict, or just use 'value'"
v3.1.0,Basic attributes.
v3.1.0,Set model in training mode.
v3.1.0,Let's clean the GPUs before training loop
v3.1.0,UPDATE DROPOUT
v3.1.0,Run patience mechanism
v3.1.0,"If the patience has reached the limit, stop training"
v3.1.0,swap model params w/ moving average
v3.1.0,(and keep the original parameters)
v3.1.0,Set model in validating mode.
v3.1.0,F-prop through the model.
v3.1.0,Compute loss.
v3.1.0,Compute validation metrics (at batch.dataset level)
v3.1.0,Compute stats
v3.1.0,Update statistics.
v3.1.0,Set model back to training mode.
v3.1.0,Truncated BPTT: reminder not compatible with accum > 1
v3.1.0,1. Create truncated target.
v3.1.0,2. F-prop all but generator.
v3.1.0,3. Compute loss.
v3.1.0,Compute and save stats
v3.1.0,in theory we should divide by accum_count and bptt
v3.1.0,to rescale for each sub batch
v3.1.0,4. Update the parameters and statistics.
v3.1.0,Multi GPU gradient gather
v3.1.0,"If truncated, don't backprop fully."
v3.1.0,"in case of multi step gradient accumulation,"
v3.1.0,update only after accum batches
v3.1.0,For Flake
v3.1.0,we avoid padding while mean pooling
v3.1.0,incoming and outgoing edge embedding
v3.1.0,Find vocab data for tree builting
v3.1.0,Propogation Model
v3.1.0,Initialize the bridge layer
v3.1.0,Token embedding
v3.1.0,Initialize graph using formatted input sequence
v3.1.0,Number of flagged nodes defines node count for this sample
v3.1.0,"(Nodes can have no flags on them, but must be in 'flags' list)."
v3.1.0,The total number of integers in the vocab should allow
v3.1.0,for all features and edges to be defined.
v3.1.0,Use first extra node as only source for decoder init
v3.1.0,Average all nodes to get bridge input
v3.1.0,"LSTM has hidden and cell state, other only one"
v3.1.0,Total number of states
v3.1.0,Build a linear layer for each
v3.1.0,Initialize the bridge layer
v3.1.0,src lengths data is wrapped inside a Tensor.
v3.1.0,"LSTM has hidden and cell state, other only one"
v3.1.0,Total number of states
v3.1.0,Build a linear layer for each
v3.1.0,batch x len x dim
v3.1.0,mask is now (batch x 1 x slen x slen)
v3.1.0,1 to be expanded to number of heads in MHA
v3.1.0,Run the forward pass of every layer of the tranformer.
v3.1.0,Dimensions and padding for constructing the word embedding matrix
v3.1.0,Dimensions and padding for feature embedding matrices
v3.1.0,(these have no effect if feat_vocab_sizes is empty)
v3.1.0,The embedding matrix look-up tables. The first look-up table
v3.1.0,"is for words. Subsequent ones are for features, if any exist."
v3.1.0,The final output size of word + feature vectors. This can vary
v3.1.0,from the word vector size if and only if features are defined.
v3.1.0,This is the attribute you should access if you need to know
v3.1.0,how big your embeddings are going to be.
v3.1.0,The sequence of operations that converts the input sequence
v3.1.0,into a sequence of embeddings. At minimum this consists of
v3.1.0,looking up the embeddings for each word and feature in the
v3.1.0,input. Model parameters may require the sequence to contain
v3.1.0,additional operations as well.
v3.1.0,features must use word_vec_size
v3.1.0,features will use feat_vec_size
v3.1.0,Some utilitary functions for pretrained embeddings
v3.1.0,is this reachable?
v3.1.0,Write to file
v3.1.0,set the opt in place
v3.1.0,set the opt in place
v3.1.0,flake8: noqa
v3.1.0,For command-line option parsing
v3.1.0,"Check pass, set the args."
v3.1.0,"This SRU version implements its own cuda-level optimization,"
v3.1.0,so it requires that:
v3.1.0,1. `cupy` and `pynvrtc` python package installed.
v3.1.0,2. pytorch is built with cuda support.
v3.1.0,3. library path set: export LD_LIBRARY_PATH=<cuda lib path>.
v3.1.0,Check 1.
v3.1.0,Check 2.
v3.1.0,Check 3.
v3.1.0,This sets up device to use.
v3.1.0,-> directions x batch x dim
v3.1.0,For DEBUG
v3.1.0,"size = (length, batch, x.size(-1)) \"
v3.1.0,"if x.dim() == 3 else (batch, x.size(-1))"
v3.1.0,grad_x = x.new(*x.size()) if k_ == 3 else x.new(*size).zero_()
v3.1.0,Normal use
v3.1.0,"An entry check here, will catch on train side and translate side"
v3.1.0,if requirements are not satisfied.
v3.1.0,RNNDecoderState wraps hidden as a tuple.
v3.1.0,fh -> (layers*directions) x batch x dim
v3.1.0,This class is mainly used by decoder.py for RNNs but also
v3.1.0,by the CNN / transformer decoder when copy attention is used
v3.1.0,CNN has its own attention mechanism ConvMultiStepAttention
v3.1.0,Transformer has its own MultiHeadedAttention
v3.1.0,mlp wants it with bias
v3.1.0,"(batch, t_len, d) x (batch, d, s_len) --> (batch, t_len, s_len)"
v3.1.0,"(batch, t_len, s_len, d)"
v3.1.0,one step input
v3.1.0,"compute attention scores, as in Luong et al."
v3.1.0,Softmax or sparsemax to normalize attention weights
v3.1.0,each context vector c_t is the weighted average
v3.1.0,over all the source hidden states
v3.1.0,concatenate
v3.1.0,clamping necessary because of numerical errors: loss should be lower
v3.1.0,"bounded by zero, but negative values near zero are possible without"
v3.1.0,the clamp
v3.1.0,Shift values to be >= 0
v3.1.0,class MultiHeadedAttention(torch.jit.ScriptModule):
v3.1.0,https://arxiv.org/pdf/1803.02155.pdf
v3.1.0,in the paper they suggest either two embeds
v3.1.0,relative_key / relative_value or only
v3.1.0,relative_key. We implemented the same embed
v3.1.0,for both.
v3.1.0,@torch.jit.script_method
v3.1.0,"1) Project key, value, and query."
v3.1.0,as a reminder at training layer_cache[0] remains False
v3.1.0,2) Calculate and scale scores.
v3.1.0,batch x num_heads x query_len x key_len
v3.1.0,1 or key_len x key_len
v3.1.0,1 or key_len x key_len x dim_per_head
v3.1.0,not 100% necessary but expand to nb of heads
v3.1.0,now mask and scores have the same shape
v3.1.0,3) Apply attention dropout and compute context vectors.
v3.1.0,We use the same embeddings for key and value
v3.1.0,--------------------------------------------------------------------------
v3.1.0,MOstly copied from https://github.com/microsoft/LoRA/
v3.1.0,Copyright (c) Microsoft Corporation. All rights reserved.
v3.1.0,Licensed under the MIT License (MIT).
v3.1.0,--------------------------------------------------------------------------
v3.1.0,Optional dropout
v3.1.0,Mark the weight as unmerged
v3.1.0,LoRA implemented in a dense layer
v3.1.0,Actual trainable parameters
v3.1.0,Freezing the pre-trained weight matrix
v3.1.0,initialize A the same way as the default
v3.1.0,for nn.Linear and B to zero
v3.1.0,Make sure that the weights are not merged
v3.1.0,Merge the weights and mark it
v3.1.0,LoRA implemented in a dense layer
v3.1.0,Set this to True if the layer to replace stores
v3.1.0,"weight like (fan_in, fan_out)"
v3.1.0,Actual trainable parameters
v3.1.0,Freezing the pre-trained weight matrix
v3.1.0,initialize A the same way as the default
v3.1.0,for nn.Linear and B to zero
v3.1.0,Make sure that the weights are not merged
v3.1.0,Merge the weights and mark it
v3.1.0,LoRA implemented in a dense layer
v3.1.0,Actual trainable parameters
v3.1.0,Freezing the pre-trained weight matrix
v3.1.0,Compute the indices
v3.1.0,initialize A the same way as the default
v3.1.0,for nn.Linear and B to zero
v3.1.0,Make sure that the weights are not merged
v3.1.0,Merge the weights and mark it
v3.1.0,At the moment this class is only used by embeddings.Embeddings look-up tables
v3.1.0,-*- coding: utf-8 -*-
v3.1.0,class AverageAttention(torch.jit.ScriptModule):
v3.1.0,@torch.jit.script
v3.1.0,out_features * in_features
v3.1.0,norm is out_features * 1
v3.1.0,batch_size * out_features
v3.1.0,out_features
v3.1.0,out_features
v3.1.0,batch_size * out_features
v3.1.0,"out_channels, in_channels // groups, * kernel_size"
v3.1.0,out_features
v3.1.0,This is used nowhere in the code at the moment (Vincent Nguyen 05/18/2018)
v3.1.0,"in_channels, out_channels, *kernel_size"
v3.1.0,"in_channels, out_channels, *kernel_size"
v3.1.0,"self.out_channels, 1"
v3.1.0,out_features
v3.1.0,out_features
v3.1.0,store roots on diagonal
v3.1.0,Original probabilities.
v3.1.0,Probability of copying p(z=1) batch.
v3.1.0,Probability of not copying: p_{word}(w) * (1 - p(z))
v3.1.0,probabilities assigned by the model to the gold targets
v3.1.0,probability of tokens copied from source
v3.1.0,Set scores for unk to 0 and add eps
v3.1.0,find the indices in which you do not use the copy mechanism
v3.1.0,Drop padding.
v3.1.0,Filter out very short or very long sentences
v3.1.0,from the TM for better performance
v3.1.0,We split the `batch` and perform fuzzy matching
v3.1.0,in smaller chunks of 10.000 examples in order to
v3.1.0,reduce memory usage.
v3.1.0,Perfomance is not affected.
v3.1.0,Probably redundant but let's be safe
v3.1.0,in case some examples are already fuzzied
v3.1.0,(e.g. from another pipeline or workflow)
v3.1.0,We don't want exact matches
v3.1.0,Apply a basic filtering to leave out very short or very long
v3.1.0,sentences and speed up things a bit during fuzzy matching
v3.1.0,Do nothing
v3.1.0,We set the start number of tags to a random number from 1
v3.1.0,to 12 + the number of subsequent tags that
v3.1.0,will be added. We also apply weights to this choice so tags
v3.1.0,"are more probable to start from 1, then from 2, etc."
v3.1.0,This way we cover most scenarios met in real usage and
v3.1.0,the system will learn to handle a fairly large number of
v3.1.0,numbered tags (but not an excessively large number)
v3.1.0,Make sure we only search for exact matches (we don't want
v3.1.0,to match part of words) and perform some bound checking
v3.1.0,Make a weighted choice between paired tags or single tags.
v3.1.0,"We usually encounter, and thus here we favor, paired tags"
v3.1.0,with a ratio 1/3.
v3.1.0,Check if the tags include the
v3.1.0,"mandatory ""#"" number placeholder"""
v3.1.0,We split the user-defined tags in the # placeholder
v3.1.0,in order to number them
v3.1.0,"doc break we add it, restart new doc"
v3.1.0,case 1st ex is already longer
v3.1.0,adding cur ex is too long we add cur doc
v3.1.0,and reset doc to cur ex
v3.1.0,we start the new doc with cur ex
v3.1.0,we cumulate cur ex to cur doc
v3.1.0,Auto import python files in this directory
v3.1.0,1. sample number of tokens to corrupt
v3.1.0,2. sample positions to corrput
v3.1.0,3. sample corrupted values
v3.1.0,1. sample number of tokens to corrupt
v3.1.0,2. sample positions to corrput
v3.1.0,3. Drop token on chosen position
v3.1.0,1. sample number of tokens to corrupt
v3.1.0,2. sample positions to corrput
v3.1.0,3. mask word on chosen position
v3.1.0,"Sharing options among `TokenizerTransform`s, same name conflict in"
v3.1.0,this scope will be resolved by remove previous occurrence in parser
v3.1.0,subword regularization(or BPE dropout) options:
v3.1.0,subword vocabulary restriction options:
v3.1.0,derterministic subwording
v3.1.0,subword sampling when nbest_size > 1 or -1
v3.1.0,alpha should be 0.0 < alpha < 1.0
v3.1.0,Load vocabulary file if provided and set threshold
v3.1.0,Load Subword Model
v3.1.0,-1: keep everything (i.e. 1 mask per token)
v3.1.0,0: replace everything (i.e. no mask)
v3.1.0,1: 1 mask per span
v3.1.0,view each subword as word start / input is word level token
v3.1.0,Pretend it ends with a full stop so last span is a sentence
v3.1.0,"Tokens that are full stops, where the previous token is not"
v3.1.0,Make sure we have enough to mask
v3.1.0,Trim to masking budget
v3.1.0,Handle 0-length mask (inserts) separately
v3.1.0,assert is_word_start[-1] == 0
v3.1.0,assert tokens_length - 1 not in indices
v3.1.0,"keep index, but replace it with [MASK]"
v3.1.0,"acts as a long length, so spans don't go over the end of doc"
v3.1.0,next position from each word_start
v3.1.0,delete token: 1 mask/remove per span
v3.1.0,"keep index, but replace it with [MASK]: 1 mask per token"
v3.1.0,A bit faster when all lengths are 1
v3.1.0,to cover whole token
v3.1.0,delete token
v3.1.0,"keep index, but replace it with [MASK]"
v3.1.0,assert tokens_length - 1 not in indices
v3.1.0,prefix src/tgt for each dataset
v3.1.0,prefix as general option for inference
v3.1.0,suffix src/tgt for each dataset
v3.1.0,suffix as general option for inference
v3.1.0,!/usr/bin/env python3
v3.1.0,-*- coding: utf-8 -*-
v3.1.0,Most code taken from: https://github.com/alvations/sacremoses
v3.1.0,Which in turn is based on the Moses punctuation normalizer.
v3.1.0,https://github.com/moses-smt/mosesdecoder/blob/master/scripts/
v3.1.0,tokenizer/normalize-punctuation.perl
v3.1.0,don't fix period at end of sentence
v3.1.0,Regex substitutions from replace-unicode-punctuation.perl
v3.1.0,https://github.com/moses-smt/mosesdecoder/blob/master/
v3.1.0,scripts/tokenizer/replace-unicode-punctuation.perl
v3.1.0,Adds the penn substitutions after extra_whitespace regexes.
v3.1.0,"Optionally, replace unicode puncts BEFORE normalization."
v3.1.0,Actual normalization.
v3.1.0,"print(regexp, substitution)"
v3.1.0,print(text)
v3.1.0,"Optionally, replace unicode puncts BEFORE normalization."
v3.1.0,One source feature expected but none given and no default provided
v3.1.0,Provided default does not match required features
v3.1.0,Data not properly annotated.
v3.1.0,In this case we do not use the default as it might be an error
v3.1.0,batch 0 will always predict EOS. The other batches will predict
v3.1.0,non-eos scores.
v3.1.0,"""best"" prediction is eos - that should be blocked"
v3.1.0,include at least one prediction OTHER than EOS
v3.1.0,that is greater than -1e20
v3.1.0,now batch 0 has ended and no others have
v3.1.0,initial step
v3.1.0,batch 0 dies on step 0
v3.1.0,include at least one prediction OTHER than EOS
v3.1.0,that is greater than -1e20
v3.1.0,step 2
v3.1.0,(old) batch 8 dies on step 1
v3.1.0,step 3
v3.1.0,everything dies
v3.1.0,initial step
v3.1.0,batch 0 dies on step 0
v3.1.0,include at least one prediction OTHER than EOS
v3.1.0,that is greater than -1e20
v3.1.0,step 2
v3.1.0,(old) batch 8 dies on step 1
v3.1.0,step 3
v3.1.0,everything dies
v3.1.0,initial step
v3.1.0,finish one beam
v3.1.0,include at least one prediction OTHER than EOS
v3.1.0,that is greater than -1e20
v3.1.0,step 2
v3.1.0,finish example in last batch
v3.1.0,(old) batch 8 dies on step 1
v3.1.0,step 3
v3.1.0,everything dies
v3.1.0,initial step
v3.1.0,batch 0 dies on step 0
v3.1.0,include at least one prediction OTHER than EOS
v3.1.0,that is greater than -1e20
v3.1.0,step 2
v3.1.0,(old) batch 8 dies on step 1
v3.1.0,step 3
v3.1.0,everything dies
v3.1.0,illegal_weights_mask = torch.ByteTensor([
v3.1.0,"[0, 0, 0, 0, 0, 0, 0],"
v3.1.0,"[0, 0, 0, 1, 1, 1, 1],"
v3.1.0,"[0, 0, 0, 0, 0, 1, 1],"
v3.1.0,"[0, 0, 1, 1, 1, 1, 1]])"
v3.1.0,TODO: fix for pytorch 0.3
v3.1.0,illegal_weights = alignments.masked_select(illegal_weights_mask)
v3.1.0,"self.assertEqual(0.0, illegal_weights.data.sum())"
v3.1.0,this could be considered an integration test because it touches
v3.1.0,the filesystem for the config file (and the models)
v3.1.0,no dummy prefix
v3.1.0,no dummy prefix
v3.1.0,make sure the scalars are in the event accumulator tags
v3.1.0,required arguments
v3.1.0,transforms that require vocab will not create if not provide vocab
v3.1.0,1. Init first transform in the pipe
v3.1.0,2. Init second transform in the pipe
v3.1.0,3. Sequential combine them into a transform pipe
v3.1.0,4. apply transform pipe for example
v3.1.0,"5. example after the pipe exceed the length limit, thus filtered"
v3.1.0,6. Transform statistics registed (here for filtertoolong)
v3.1.0,"7. after report, statistics become empty as a fresh start"
v3.1.0,filter_transform.warm_up()
v3.1.0,test BPE-dropout:
v3.1.0,1. disable bpe dropout for not training example
v3.1.0,2. enable bpe dropout for training example
v3.1.0,3. (NOTE) disable dropout won't take effect if already seen
v3.1.0,this is caused by the cache mechanism in bpe:
v3.1.0,return cached subword if the original token is seen when no dropout
v3.1.0,test SP regularization:
v3.1.0,1. enable regularization for training example
v3.1.0,2. disable regularization for not training example
v3.1.0,Not apply token drop for not training example
v3.1.0,apply token drop for training example
v3.1.0,Not apply token mask for not training example
v3.1.0,apply token mask for training example
v3.1.0,require vocabs to warm_up
v3.1.0,Not apply token mask for not training example
v3.1.0,apply token mask for training example
v3.1.0,"Defalt: full_stop_token=[""."", ""?"", ""!""]"
v3.1.0,"Defalt: full_stop_token=[""."", ""?"", ""!""]"
v3.1.0,random_ratio of inserted tokens are chosen in vocab
v3.1.0,others are MASK_TOK
v3.1.0,"insert_ratio=0.0,"
v3.1.0,"random_ratio=0.0,"
v3.1.0,"Defalt: full_stop_token=[""."", ""?"", ""!""]"
v3.1.0,all token are considered as an individual word
v3.1.0,1. tokens are dropped when replace_length is 0
v3.1.0,"print(f""token delete: {masked} / {tokens}"")"
v3.1.0,2. tokens are replaced by MASK when replace_length is 1
v3.1.0,"print(f""token mask: {masked} / {tokens}"")"
v3.1.0,"insert_ratio=0.0,"
v3.1.0,"random_ratio=0.0,"
v3.1.0,"Defalt: full_stop_token=[""."", ""?"", ""!""]"
v3.1.0,start token of word are identified using subword marker
v3.1.0,"1. replace_length 0: ""words"" are dropped"
v3.1.0,"print(f""word delete: {masked} / {tokens}"")"
v3.1.0,"self.assertEqual(len(masked), n_words - n_masked)"
v3.1.0,"2. replace_length 1: ""words"" are replaced with a single MASK"
v3.1.0,"print(f""whole word single mask: {masked} / {tokens}"")"
v3.1.0,len(masked) depend on number of tokens in select word
v3.1.0,"3. replace_length -1: all tokens in ""words"" are replaced with MASK"
v3.1.0,"print(f""whole word multi mask: {masked} / {tokens}"")"
v3.1.0,number of mask_tok depend on number of tokens in selected word
v3.1.0,number of MASK_TOK can be greater than n_masked
v3.1.0,"insert_ratio=0.5,"
v3.1.0,"random_ratio=0.3,"
v3.1.0,"Defalt: full_stop_token=[""."", ""?"", ""!""]"
v3.1.0,start token of word are identified using subword marker
v3.1.0,n_words = sum(token_starts)
v3.1.0,n_masked = math.ceil(n_words * bart_noise.mask_ratio)
v3.1.0,"print(f""Text Span Infilling: {infillied} / {tokens}"")"
v3.1.0,"print(n_words, n_masked)"
v3.1.0,!/usr/bin/env python
v3.1.0,-*- coding: utf-8 -*-
v3.1.0,Inject some dummy training options that may needed when build fields
v3.1.0,Remove the generated *pt files.
v3.1.0,Remove the generated data samples
v3.1.0,all beams repeat (beam >= 1 repeat dummy scores)
v3.1.0,predict repeat_idx over and over again
v3.1.0,"before repeat, scores are either 0 or -inf"
v3.1.0,"on repeat, `repeat_idx` score is BLOCKED_SCORE"
v3.1.0,"(but it's still the best score, thus we have"
v3.1.0,"[BLOCKED_SCORE, -inf, -inf, -inf, -inf]"
v3.1.0,repetitions keeps maximizing score
v3.1.0,"index 0 has been blocked, so repeating=>+0.0 score"
v3.1.0,other indexes are -inf so repeating=>BLOCKED_SCORE
v3.1.0,which is higher
v3.1.0,beam 0 and beam >=2 will repeat (beam >= 2 repeat dummy scores)
v3.1.0,non-interesting beams are going to get dummy values
v3.1.0,"on initial round, only predicted scores for beam 0"
v3.1.0,matter. Make two predictions. Top one will be repeated
v3.1.0,"in beam zero, second one will live on in beam 1."
v3.1.0,predict the same thing in beam 0
v3.1.0,continue pushing around what beam 1 predicts
v3.1.0,"now beam 0 dies (along with the others), beam 1 -> beam 0"
v3.1.0,"now beam 0 dies (along with the others), beam 1 -> beam 0"
v3.1.0,beam 0 and beam >= 2 will repeat (beam 2 repeats excluded idx)
v3.1.0,non-interesting beams are going to get dummy values
v3.1.0,predict the same thing in beam 0
v3.1.0,continue pushing around what beam 1 predicts
v3.1.0,predict the allowed-repeat again in beam 2
v3.1.0,"now beam 0 dies, beam 1 -> beam 0, beam 2 -> beam 1"
v3.1.0,and the rest die
v3.1.0,"since all preds after i=0 are 0, we can check"
v3.1.0,that the beam is the correct idx by checking that
v3.1.0,the curr score is the initial score
v3.1.0,beam 0 will always predict EOS. The other beams will predict
v3.1.0,non-eos scores.
v3.1.0,non-interesting beams are going to get dummy values
v3.1.0,"""best"" prediction is eos - that should be blocked"
v3.1.0,include at least beam_sz predictions OTHER than EOS
v3.1.0,that are greater than -1e20
v3.1.0,predict eos in beam 0
v3.1.0,provide beam_sz other good predictions
v3.1.0,now the top beam has ended and no others have
v3.1.0,"not of interest, but want to make sure it keeps running"
v3.1.0,since only beam 0 terminates and n_best = 2
v3.1.0,"this is also a test that when block_ngram_repeat=0,"
v3.1.0,repeating is acceptable
v3.1.0,non-interesting beams are going to get dummy values
v3.1.0,"""best"" prediction is eos - that should be blocked"
v3.1.0,include at least beam_sz predictions OTHER than EOS
v3.1.0,that are greater than -1e20
v3.1.0,predict eos in beam 1
v3.1.0,provide beam_sz other good predictions in other beams
v3.1.0,beam 1 dies on min_length
v3.1.0,beam 0 dies on the step after beam 1 dies
v3.1.0,"inp_lens is tiled in initialize, reassign to make attn match"
v3.1.0,non-interesting beams are going to get dummy values
v3.1.0,"""best"" prediction is eos - that should be blocked"
v3.1.0,include at least beam_sz predictions OTHER than EOS
v3.1.0,that are greater than -1e20
v3.1.0,predict eos in beam 1
v3.1.0,provide beam_sz other good predictions in other beams
v3.1.0,no top beams are finished yet
v3.1.0,beam 1 dies on min_length
v3.1.0,no top beams are finished yet
v3.1.0,beam 0 dies on the step after beam 1 dies
v3.1.0,top beam is finished now so there are attentions
v3.1.0,two beams are finished in each batch
v3.1.0,second dim is cut down to the non-padded src length
v3.1.0,first dim is equal to the time of death
v3.1.0,(beam 0 died at current step - adjust for SOS)
v3.1.0,(beam 1 died at last step - adjust for SOS)
v3.1.0,behavior gets weird when beam is already done so just stop
v3.1.0,this is just test_beam.TestBeamAgainstReferenceCase repeated
v3.1.0,in each batch.
v3.1.0,"init_preds: [4, 3, 5, 6, 7] - no EOS's"
v3.1.0,no EOS's yet
v3.1.0,"[5, 3, 2, 6, 0], so beam 2 predicts EOS!"
v3.1.0,assumes beam 2 finished on last step
v3.1.0,ended beam 2 shouldn't continue
v3.1.0,"[2, 5, 3, 6, 0] repeat self.BATCH_SZ, so beam 0 predicts EOS!"
v3.1.0,"[-2.4879, -3.8910, -4.1010, -4.2010, -4.4010] repeat self.BATCH_SZ"
v3.1.0,another beam is finished in all batches
v3.1.0,new beam 0 finished
v3.1.0,new beam 0 is old beam 3
v3.1.0,assumes beam 0 finished on last step
v3.1.0,"[5, 2, 6, 1, 0] repeat self.BATCH_SZ, so beam 1 predicts EOS!"
v3.1.0,we finish 3 hyps per example in this step
v3.1.0,new beam 1 is old beam 3
v3.1.0,this could be considered an integration test because it tests
v3.1.0,interactions between the GNMT scorer and the beam
v3.1.0,"-data option is required, but not used in this test, so dummy."
v3.1.0,len x batch x nfeat
v3.1.0,Initialize vectors to compare size with
v3.1.0,Ensure correct sizes and types
v3.1.0,Make sure that output has the correct size and type
v3.1.0,"[('encoder_type', 'transformer'),"
v3.1.0,"('word_vec_size', 16), ('hidden_size', 16)],"
v3.1.0,""""""" Only do SRU test if requirment is safisfied. """""""
v3.1.0,SRU doesn't support input_feed.
v3.1.0,first check there's nothing unexpectedly not trainable
v3.1.0,ok: word embeddings shouldn't be trainable
v3.1.0,if word vecs are freezed
v3.1.0,ok: positional encodings shouldn't be trainable
v3.1.0,then check nothing unexpectedly trainable
v3.1.0,Decoder state
v3.1.0,Build the RNN.
v3.1.0,Set up the context gate.
v3.1.0,Set up the standard attention.
v3.1.0,The encoder hidden is  (layers*directions) x batch x dim.
v3.1.0,We need to convert it to layers x batch x (directions*dim).
v3.1.0,Init the input feed.
v3.1.0,Update the state with the result.
v3.1.0,Concatenates sequence of tensors along a new dimension.
v3.1.0,NOTE: v0.3 to 0.4: dec_outs / attns[*] may not be list
v3.1.0,(in particular in case of SRU) it was not raising error in 0.3
v3.1.0,since stack(Variable) was allowed.
v3.1.0,"In 0.4, SRU returns a tensor that shouldn't be stacke"
v3.1.0,Calculate the attention.
v3.1.0,Calculate the context gate.
v3.1.0,Additional args check.
v3.1.0,Input feed concatenates hidden state with
v3.1.0,input at every time step.
v3.1.0,TODO: context gate should be employed
v3.1.0,instead of second RNN transform.
v3.1.0,Update the coverage attention.
v3.1.0,"attns[""coverage""] is actually c^(t+1) of See et al(2017)"
v3.1.0,1-index shifted
v3.1.0,Decoder State
v3.1.0,CNNDecoder has its own attention mechanism.
v3.1.0,Set up a separate copy attention layer if needed.
v3.1.0,The output of CNNEncoder.
v3.1.0,The combination of output of CNNEncoder and source embeddings.
v3.1.0,Process the result and update the attentions.
v3.1.0,Update the state.
v3.1.0,TODO change the way attns is returned dict => list or tuple (onnx)
v3.1.0,src_len is a single tensor shared between all models.
v3.1.0,This assumption will not hold if Translator is modified
v3.1.0,to calculate src_len as something other than the length
v3.1.0,of the input.
v3.1.0,"return _, (B, Q_len, K_len)"
v3.1.0,"layer average attention across heads, get ``(B, Q, K)``"
v3.1.0,"Case 1: no full_context, no align heads -> layer avg baseline"
v3.1.0,"Case 2: no full_context, 1 align heads -> guided align"
v3.1.0,"Case 3: full_context, 1 align heads -> full cte guided align"
v3.1.0,BoolTensor was introduced in pytorch 1.2
v3.1.0,T: could be 1 in the case of stepwise decoding or tgt_len
v3.1.0,masking is necessary when sequence length is greater than one
v3.1.0,mask now are (batch x 1 x tlen x s or t len)
v3.1.0,1 = heads to be expanded in MHA
v3.1.0,Decoder State
v3.1.0,"previously, there was a GlobalAttention module here for copy"
v3.1.0,"attention. But it was never actually used -- the ""copy"" attention"
v3.1.0,just reuses the context attention.
v3.1.0,"attns[""align""] = torch.stack(attn_aligns, 0).mean(0)  # All avg"
v3.1.0,TODO change the way attns is returned dict => list or tuple (onnx)
v3.1.0,first value set to True triggered by the beginning of decoding
v3.1.0,layer_cache becomes active in the MultiHeadedAttention fwd
v3.1.0,T: could be 1 in the case of stepwise decoding or tgt_len
v3.1.0,masking is necessary when sequence length is greater than one
v3.1.0,mask now are (batch x 1 x tlen x tlen)
v3.1.0,1 = heads to be expanded in MHA
v3.1.0,TODO change the way attns is returned dict => list or tuple (onnx)
v3.1.0,"buffer size in bytes, determine equiv. # of elements based on data type"
v3.1.0,copy tensors into buffer_t
v3.1.0,all-reduce and rescale
v3.1.0,copy all-reduced buffer back into tensors
v3.1.0,"print(filled, sz)"
v3.1.0,"tensor is bigger than buffer, all-reduce and rescale directly"
v3.1.0,"buffer is full, all-reduce and replace buffer with grad"
v3.1.0,add tensor to buffer
v3.1.0,"propagate exception to parent process, keeping original traceback"
v3.1.0,TODO: Find a better way to check for sparse gradients.
v3.1.0,we use apex.amp
v3.1.0,In this case use the old FusedAdam with
v3.1.0,FP16_optimizer wrapper
v3.1.0,Load everything from the checkpoint.
v3.1.0,Build everything from scratch.
v3.1.0,"Reset optimizer, keep options."
v3.1.0,"Reset options, keep optimizer."
v3.1.0,State can be partially restored.
v3.1.0,should be: self._optimizer.zero_grad(set_to_none)
v3.1.0,but apex.amp is not up-to-date:
v3.1.0,https://github.com/NVIDIA/apex/blob/master/apex/amp/_process_optimizer.py#L367
v3.1.0,"unscaled optimizer's gradients (already done therefore skip),"
v3.1.0,skips optimizer.step() if gradients contain infs/NaNs.
v3.1.0,Updates the scale for next iteration.
v3.1.0,Code below is an implementation of https://arxiv.org/pdf/1804.04235.pdf
v3.1.0,inspired but modified from https://github.com/DeadAt0m/adafactor-pytorch
v3.1.0,backward compatibility
v3.1.0,assuming a list/generator of parameter means single group
v3.1.0,compute combined scale factor for this group
v3.1.0,norm is in fact norm*scale
v3.1.0,note: p.grad should not ever be set for correct operation of
v3.1.0,mixed precision optimizer that sometimes sends None gradients
v3.1.0,State initialization
v3.1.0,Exponential moving average of gradient values
v3.1.0,Exponential moving average of squared gradient values
v3.1.0,-*- coding: utf-8 -*-
v3.1.0,placing this here make it easier to call logger.info
v3.1.0,"from anywhere, just 'from onmt.utils.logging import logger'"
v3.1.0,"align_head contains value in [0, 1) presenting attn prob,"
v3.1.0,0 was resulted by the context attention src_pad_mask
v3.1.0,"So, the correspand position in ref_align should also be 0"
v3.1.0,"Therefore, clip align_head to > 1e-18 should be bias free."
v3.1.0,rescale with tau (temperature) and apply the log_softmax.
v3.1.0,ct2 expects src with lengths without padding
v3.1.0,again we use raw probs to rescale with tau and apply log_softmax
v3.1.0,lm_scores are in log space so log_target=True
v3.1.0,rescale with tau (temperature) and apply the log_softmax.
v3.1.0,ct2 expects src with lengths without padding
v3.1.0,again we use raw probs to rescale with tau and apply log_softmax
v3.1.0,lm_scores are in log space so log_target=True
v3.1.0,take into account here the tgt_shift_index (0 / 1 = LM/NMT)
v3.1.0,Correct target copy token instead of <unk>
v3.1.0,tgt[i] = align[i] + len(tgt_vocab)
v3.1.0,for i such that tgt[i] == 0 and align[i] != 0
v3.1.0,in the case criterion reduction is None then we need
v3.1.0,to sum the loss of each sentence in the batch
v3.1.0,Check Transforms
v3.1.0,Check path
v3.1.0,tgt is src for LM task
v3.1.0,Check weight
v3.1.0,Check features
v3.1.0,validation when train:
v3.1.0,Check embeddings stuff
v3.1.0,"Backward compatibility with ""fix_word_vecs_*"" opts"
v3.1.0,encoder and decoder should be same sizes
v3.1.0,"Load default opt values, then overwrite with the opts in"
v3.1.0,"the checkpoint. That way, if there are new options added,"
v3.1.0,the defaults are used.
v3.1.0,It comes from training
v3.1.0,TODO: needs to be added as inference opt
v3.1.0,Don't do anything
v3.1.0,Update best score of each criteria
v3.1.0,Reset tolerance
v3.1.0,Update current status
v3.1.0,Decrease tolerance
v3.1.0,Log
v3.1.0,Log
v3.1.0,Get a list of world_size lists with len(stat_list) Statistics objects
v3.1.0,"this param init is overridden by model_builder, useless then."
v3.1.0,SRU doesn't support PackedSequence.
v3.1.0,-*- coding: utf-8 -*-
v3.1.0,threshold on 1 to avoid div by 0
v3.1.0,treat alignment matrix one by one as each have different lengths
v3.1.0,No alignment if not exist valid tgt token
v3.1.0,get valid alignment (sub-matrix from full paded aligment matrix)
v3.1.0,Helper functions
v3.1.0,Keeps track of the original words/subwords
v3.1.0,('prior_tokenization' option)
v3.1.0,In case there is a final case_markup when new_spacer is on
v3.1.0,translate
v3.1.0,for validation we build an infer_iter per batch
v3.1.0,in order to avoid oom issues because there is no
v3.1.0,batching strategy in `textbatch_to_tensor`
v3.1.0,apply_reverse refs
v3.1.0,flatten preds
v3.1.0,save results
v3.1.0,-*- coding: utf-8 -*-
v3.1.0,this one is needed for Random Shuffler of batches
v3.1.0,in multi gpu it ensures datasets are read in the same order
v3.1.0,some cudnn methods can be random even after fixing the seed
v3.1.0,unless you tell it to be deterministic
v3.1.0,This one is needed for various tranfroms
v3.1.0,These ensure same initialization in multi gpu mode
v3.1.0,we need to check the model path + any tokenizer path
v3.1.0,patch to log stdout spawned processes of dataloader
v3.1.0,bucket_size = batch_size
v3.1.0,For TRAIN we need to group examples by length
v3.1.0,"for faster performance, but otherwise, sequential."
v3.1.0,For TRAIN we shuffle batches within the bucket
v3.1.0,otherwise sequential
v3.1.0,for specific case of rnn_packed need to be sorted
v3.1.0,within the batch
v3.1.0,Check if all tokens have features or none at all
v3.1.0,Make features part of src like
v3.1.0,"{'src': {'src': ..., 'feats': [...., ....]}}"
v3.1.0,at this point an example looks like:
v3.1.0,"{'src': {'src': ..., 'feats': [....]},"
v3.1.0,"'tgt': {'tgt': ...},"
v3.1.0,"'src_original': ['tok1', ...'tokn'],"
v3.1.0,"'tgt_original': ['tok1', ...'tokm'],"
v3.1.0,'indices' : seq in bucket
v3.1.0,"'align': ...,"
v3.1.0,}
v3.1.0,Need to add features in last dimensions
v3.1.0,Keep it consistent with dynamic data
v3.1.0,make a small vocab containing just the tokens in the source sequence
v3.1.0,Map source tokens to indices in the dynamic dict.
v3.1.0,-*- coding: utf-8 -*-
v3.1.0,'src_original' and 'tgt_original' store the
v3.1.0,original line before tokenization. These
v3.1.0,fields are used later on in the feature
v3.1.0,transforms.
v3.1.0,NOTE: moved to dynamic_iterator.py cf process()
v3.1.0,item = self.transform.apply(
v3.1.0,"example, is_train=self.infinitely, corpus_name=self.cid)"
v3.1.0,empty example: skip
v3.1.0,"No encoder in LM, seq2seq count formatting kept"
v3.1.0,_check_save_model_path
v3.1.0,for future use?
v3.1.0,if we want to save the LoRa model state_dict only
v3.1.0,"model_state_dict = lora_state_dict(model, bias='lora_only')"
v3.1.0,and comment the line below
v3.1.0,NOTE: We need to trim the vocab to remove any unk tokens that
v3.1.0,were not originally here.
v3.1.0,"for side in [""src"", ""tgt""]:"
v3.1.0,keys_to_pop = []
v3.1.0,"if hasattr(vocab[side], ""fields""):"
v3.1.0,unk_token = vocab[side].fields[0][1].vocab.itos[0]
v3.1.0,"for key, value in vocab[side].fields[0][1].vocab.stoi.items():"
v3.1.0,if value == 0 and key != unk_token:
v3.1.0,keys_to_pop.append(key)
v3.1.0,for key in keys_to_pop:
v3.1.0,"vocab[side].fields[0][1].vocab.stoi.pop(key, None)"
v3.1.0,!/usr/bin/env python
v3.1.0,!/usr/bin/env python
v3.1.0,!/usr/bin/env python
v3.1.0,-*- coding: utf-8 -*-
v3.1.0,!/usr/bin/env python
v3.1.0,!/usr/bin/env python
v3.1.0,!/usr/bin/env python
v3.1.0,Set sharing strategy manually instead of default based on the OS.
v3.1.0,torch.multiprocessing.set_sharing_strategy('file_system')
v3.1.0,Create a thread to listen for errors in the child processes.
v3.1.0,Train with multiprocessing.
v3.1.0,magic indices
v3.1.0,result caching
v3.1.0,Here we set the decoder to start with self.start (BOS or EOS)
v3.1.0,fix length constraint and remove eos from count
v3.1.0,add one to account for BOS. Don't account for EOS because hitting
v3.1.0,this implies it hasn't been found.
v3.1.0,we don't block nothing if the user doesn't want it
v3.1.0,we can't block nothing beam's too short
v3.1.0,we check paths one by one
v3.1.0,we don't forbid nothing if the user doesn't want it
v3.1.0,we can't forbid nothing if beam's too short
v3.1.0,Reordering forbidden_tokens following beam selection
v3.1.0,We rebuild a dict to ensure we get the value and not the pointer
v3.1.0,Grabing the newly selected tokens and associated ngram
v3.1.0,skip the blocking if any token in current_ngram is excluded
v3.1.0,"pickups: Tensor where specified index were set to 1, others 0"
v3.1.0,"dropdowns: opposite of pickups, 1 for those shouldn't pick"
v3.1.0,Minus dropdowns to log_probs making probabilities of
v3.1.0,unspecified index close to 0
v3.1.0,"prediction step have surpass length of given target_prefix,"
v3.1.0,no need to further change this attr
v3.1.0,keep indices until overflowing p
v3.1.0,Set all logits that are not in the top-p to -10000.
v3.1.0,This puts the probabilities close to 0.
v3.1.0,Set all logits that are not in the top-k to -10000.
v3.1.0,This puts the probabilities close to 0.
v3.1.0,"For temp=0.0, take the argmax to avoid divide-by-zero errors."
v3.1.0,keep_topk=1 is also equivalent to argmax.
v3.1.0,maybe fix some prediction at this step by modifying log_probs
v3.1.0,"shape: (sum(~ self.is_finished), 1)"
v3.1.0,in LM task src_len is associated with currently generated src
v3.1.0,and therefore needs to follow the generation
v3.1.0,!/usr/bin/env python
v3.1.0,for debugging
v3.1.0,TODO: maybe add dynamic part
v3.1.0,Statistics
v3.1.0,Here we handle the cases of mismatch in number of segments
v3.1.0,between source and target. We re-translate seg by seg.
v3.1.0,those two should be the same except feat dim
v3.1.0,"batch['src'][perm[j], :, :])"
v3.1.0,trans.src
v3.1.0,we rebuild a small batch made of the sub-segments
v3.1.0,in the long segment.
v3.1.0,new sub-batch ready to be translated
v3.1.0,we re-insert the sub-batch in the initial translations
v3.1.0,In the case of length_penalty = none we report the total logprobs
v3.1.0,divided by the number of sentence to get an approximation of the
v3.1.0,per sentence logprob. We also return the corresponding ppl
v3.1.0,"When a length_penalty is used eg: ""avg"" or ""wu"" since logprobs"
v3.1.0,are normalized per token we report the per line per token logprob
v3.1.0,"and the corresponding ""per word perplexity"""
v3.1.0,Turn any copied words into UNKs.
v3.1.0,"Decoder forward, takes [batch, tgt_len, nfeats] as input"
v3.1.0,"and [batch, src_len, hidden] as enc_out"
v3.1.0,"in case of inference tgt_len = 1, batch = beam times batch_size"
v3.1.0,"in case of Gold Scoring tgt_len = actual length, batch = 1 batch"
v3.1.0,Generator forward.
v3.1.0,"returns [(batch_size x beam_size) , vocab ] when 1 step"
v3.1.0,"or [batch_size, tgt_len, vocab ] when full sentence"
v3.1.0,"here we have scores [tgt_lenxbatch, vocab] or [beamxbatch, vocab]"
v3.1.0,at this point scores is batch first (dim=0)
v3.1.0,"returns [(batch_size x beam_size) , vocab ] when 1 step"
v3.1.0,"or [batch_size, tgt_len, vocab ] when full sentence"
v3.1.0,(0) add BOS and padding to tgt prediction
v3.1.0,(1) Encoder forward.
v3.1.0,(2) Repeat src objects `n_best` times.
v3.1.0,"We use batch_size x n_best, get ``(batch * n_best, src_len, nfeat)``"
v3.1.0,Quick fix. Transformers return None as enc_states.
v3.1.0,enc_states are only used later on to init decoder's state
v3.1.0,"but are never used in Transformer decoder, so we can skip"
v3.1.0,"(3) Init decoder with n_best src,"
v3.1.0,"reshape tgt to ``(len, batch * n_best, nfeat)``"
v3.1.0,it should be done in a better way
v3.1.0,here dec_in is batch first
v3.1.0,masked_select
v3.1.0,get aligned src id for each prediction's valid tgt tokens
v3.1.0,TODO: support these blacklisted features
v3.1.0,(0) Prep the components of the search.
v3.1.0,(1) Run the encoder on the src.
v3.1.0,(2) prep decode_strategy. Possibly repeat src objects.
v3.1.0,(3) Begin decoding step by step:
v3.1.0,"decoder_input = decode_strategy.current_predictions.view(1, -1,"
v3.1.0,1)
v3.1.0,Reorder states.
v3.1.0,TODO: support these blacklisted features
v3.1.0,(0) Prep the components of the search.
v3.1.0,(1) split src into src and target_prefix to avoid padding.
v3.1.0,(2) init decoder
v3.1.0,(3) prep decode_strategy. Possibly repeat src objects.
v3.1.0,(4) Begin decoding step by step:
v3.1.0,Reorder states.
v3.1.0,select indexes in model state/cache
v3.1.0,beam parameters
v3.1.0,beam state
v3.1.0,BoolTensor was introduced in pytorch 1.2
v3.1.0,"""global state"" of the old beam"
v3.1.0,buffers for the topk scores and 'backpointer'
v3.1.0,for testing
v3.1.0,maybe fix some prediction at this step by modifying log_probs
v3.1.0,Flatten probs into a list of possibilities.
v3.1.0,Penalize beams that finished.
v3.1.0,"on real data (newstest2017) with the pretrained transformer,"
v3.1.0,it's faster to not move this back to the original device
v3.1.0,Store finished hypotheses for this batch.
v3.1.0,End condition is the top beam finished and we can return
v3.1.0,n_best hypotheses.
v3.1.0,"If all sentences are translated, no need to go further."
v3.1.0,Remove finished batches for the next step.
v3.1.0,using integer division to get an integer _B without casting
v3.1.0,force the output to be longer than self.min_length
v3.1.0,Multiply probs by the beam probability.
v3.1.0,"if the sequence ends now, then the penalty is the current"
v3.1.0,"length + 1, to include the EOS token"
v3.1.0,Avoid any direction that would repeat unwanted ngrams
v3.1.0,Pick up candidate token by curr_scores
v3.1.0,Recover log probs.
v3.1.0,Length penalty is just a scalar. It doesn't matter if it's applied
v3.1.0,before or after the topk.
v3.1.0,Resolve beam origin and map to batch index flat representation.
v3.1.0,Append last prediction.
v3.1.0,update global state (step == 1)
v3.1.0,update global state (step > 1)
v3.1.0,"shape: (batch_size x beam_size, 1)"
v3.1.0,in LM task src_len is associated with currently generated src
v3.1.0,and therefore needs to follow the generation
v3.1.0,in LM task src_len is associated with currently generated src
v3.1.0,and therefore needs to follow the generation
v3.1.0,Term will be subtracted from probability
v3.1.0,Probability will be divided by this
v3.1.0,these warnings indicate that either the alpha/beta
v3.1.0,"forces a penalty to be a no-op, or a penalty is a no-op but"
v3.1.0,the alpha/beta would suggest otherwise.
v3.1.0,using some coverage penalty
v3.1.0,!/usr/bin/env python
v3.1.0,semaphore doesn't have a timeout arg in Python 2.7
v3.1.0,perform a first request to initialize everything
v3.1.0,backwards compatibility for confs
v3.1.0,every segment becomes a dict for flexibility purposes
v3.1.0,NOTE: translator returns lists of `n_best` list
v3.1.0,build back results with empty texts
v3.1.0,load can be called multiple times: modify copy
v3.1.0,output contain alignment
v3.1.0,Below are all the different penalty terms implemented so far.
v3.1.0,Subtract coverage penalty from topk log probs.
v3.1.0,Divide topk log probs by length penalty.
v3.1.0,Sorting
v3.1.0,Chinese segmentation
v3.1.0,Chinese simplify -> Chinese traditional standard
v3.1.0,Chinese simplify -> Chinese traditional (HongKong)
v3.1.0,Chinese simplify -> Chinese traditional (Taiwan)
v3.1.0,Chinese traditional -> Chinese simplify (v1)
v3.1.0,Chinese traditional -> Chinese simplify (v2)
v3.1.0,Auto import python files in this directory
v3.0.4,!/usr/bin/env python
v3.0.4,!/usr/bin/env python
v3.0.4,!/usr/bin/env python
v3.0.4,!/usr/bin/env python
v3.0.4,!/usr/bin/env python
v3.0.4,!/usr/bin/env python3
v3.0.4,Usage: python3 filter_train.py in.src in.trg out.src out.trg max-tokens
v3.0.4,!/usr/bin/env python3
v3.0.4,-*- coding: utf-8 -*-
v3.0.4,
v3.0.4,"OpenNMT-py documentation build configuration file, created by"
v3.0.4,sphinx-quickstart on Sun Dec 17 12:07:14 2017.
v3.0.4,
v3.0.4,This file is execfile()d with the current directory set to its
v3.0.4,containing dir.
v3.0.4,
v3.0.4,Note that not all possible configuration values are present in this
v3.0.4,autogenerated file.
v3.0.4,
v3.0.4,All configuration values have a default; values that are commented out
v3.0.4,serve to show the default.
v3.0.4,"If extensions (or modules to document with autodoc) are in another directory,"
v3.0.4,add these directories to sys.path here. If the directory is relative to the
v3.0.4,"documentation root, use os.path.abspath to make it absolute, like shown here."
v3.0.4,
v3.0.4,import os
v3.0.4,import sys
v3.0.4,"sys.path.insert(0, os.path.abspath('.'))"
v3.0.4,-- General configuration ------------------------------------------------
v3.0.4,"If your documentation needs a minimal Sphinx version, state it here."
v3.0.4,
v3.0.4,needs_sphinx = '1.0'
v3.0.4,"Add any Sphinx extension module names here, as strings. They can be"
v3.0.4,extensions coming with Sphinx (named 'sphinx.ext.*') or your custom
v3.0.4,ones.
v3.0.4,Show base classes
v3.0.4,"Use ""variables"" section for Attributes instead of weird block things"
v3.0.4,mimicking the function style.
v3.0.4,"Add any paths that contain templates here, relative to this directory."
v3.0.4,The suffix(es) of source filenames.
v3.0.4,You can specify multiple suffix as a list of string:
v3.0.4,
v3.0.4,"source_suffix = ['.rst', '.md']"
v3.0.4,The master toctree document.
v3.0.4,General information about the project.
v3.0.4,"The version info for the project you're documenting, acts as replacement for"
v3.0.4,"|version| and |release|, also used in various other places throughout the"
v3.0.4,built documents.
v3.0.4,
v3.0.4,The short X.Y version.
v3.0.4,"The full version, including alpha/beta/rc tags."
v3.0.4,The language for content autogenerated by Sphinx. Refer to documentation
v3.0.4,for a list of supported languages.
v3.0.4,
v3.0.4,This is also used if you do content translation via gettext catalogs.
v3.0.4,"Usually you set ""language"" from the command line for these cases."
v3.0.4,"List of patterns, relative to source directory, that match files and"
v3.0.4,directories to ignore when looking for source files.
v3.0.4,This patterns also effect to html_static_path and html_extra_path
v3.0.4,The name of the Pygments (syntax highlighting) style to use.
v3.0.4,"If true, `todo` and `todoList` produce output, else they produce nothing."
v3.0.4,-- Options for HTML output ----------------------------------------------
v3.0.4,The theme to use for HTML and HTML Help pages.  See the documentation for
v3.0.4,a list of builtin themes.
v3.0.4,
v3.0.4,html_theme = 'sphinx_materialdesign_theme'
v3.0.4,html_theme_path = [sphinx_materialdesign_theme.get_path()]
v3.0.4,Theme options are theme-specific and customize the look and feel of a theme
v3.0.4,"further.  For a list of options available for each theme, see the"
v3.0.4,documentation.
v3.0.4,
v3.0.4,html_theme_options = {}
v3.0.4,"Add any paths that contain custom static files (such as style sheets) here,"
v3.0.4,"relative to this directory. They are copied after the builtin static files,"
v3.0.4,"so a file named ""default.css"" will overwrite the builtin ""default.css""."
v3.0.4,"Custom sidebar templates, must be a dictionary that maps document names"
v3.0.4,to template names.
v3.0.4,
v3.0.4,This is required for the alabaster theme
v3.0.4,refs: http://alabaster.readthedocs.io/en/latest/installation.html#sidebars
v3.0.4,-- Options for HTMLHelp output ------------------------------------------
v3.0.4,Output file base name for HTML help builder.
v3.0.4,-- Options for LaTeX output ---------------------------------------------
v3.0.4,The paper size ('letterpaper' or 'a4paper').
v3.0.4,
v3.0.4,"'papersize': 'letterpaper',"
v3.0.4,"The font size ('10pt', '11pt' or '12pt')."
v3.0.4,
v3.0.4,"'pointsize': '10pt',"
v3.0.4,Additional stuff for the LaTeX preamble.
v3.0.4,
v3.0.4,"'preamble': '',"
v3.0.4,Latex figure (float) alignment
v3.0.4,
v3.0.4,"'figure_align': 'htbp',"
v3.0.4,Grouping the document tree into LaTeX files. List of tuples
v3.0.4,"(source start file, target name, title,"
v3.0.4,"author, documentclass [howto, manual, or own class])."
v3.0.4,-- Options for manual page output ---------------------------------------
v3.0.4,One entry per manual page. List of tuples
v3.0.4,"(source start file, name, description, authors, manual section)."
v3.0.4,-- Options for Texinfo output -------------------------------------------
v3.0.4,Grouping the document tree into Texinfo files. List of tuples
v3.0.4,"(source start file, target name, title, author,"
v3.0.4,"dir menu entry, description, category)"
v3.0.4,!/usr/bin/env python
v3.0.4,-*- coding: utf-8 -*-
v3.0.4,is this reachable?
v3.0.4,Read in embeddings
v3.0.4,Write to file
v3.0.4,converts a SentencePiece vocabulary to the format expected by dynamic data
v3.0.4,"(essentially converts float expected counts to ""fixed precision"" int pseudo"
v3.0.4,counts)
v3.0.4,"Add in default model arguments, possibly added since training."
v3.0.4,this patch is no longer needed included in converter
v3.0.4,"if hasattr(model_opt, 'rnn_size'):"
v3.0.4,model_opt.hidden_size = model_opt.rnn_size
v3.0.4,build_base_model expects updated and validated opts
v3.0.4,-*- encoding: utf-8 -*-
v3.0.4,!/usr/bin/env python
v3.0.4,-*- coding: utf-8 -*-
v3.0.4,Author: Rico Sennrich
v3.0.4,flake8: noqa
v3.0.4,This file is retrieved from https://github.com/rsennrich/subword-nmt
v3.0.4,hack for python2/3 compatibility
v3.0.4,check version information
v3.0.4,some hacking to deal with duplicates (only consider first instance)
v3.0.4,don't print end-of-word symbols
v3.0.4,sys.stderr.write('cannot split {0} further.\n'.format(segment))
v3.0.4,sys.stderr.write('OOV: {0}\n'.format(segment))
v3.0.4,sys.stderr.write('OOV: {0}\n'.format(segment))
v3.0.4,python 2/3 compatibility
v3.0.4,read/write files as UTF-8
v3.0.4,!/usr/bin/env python3
v3.0.4,coding: utf-8
v3.0.4,"In order to use this tool, please install comet first"
v3.0.4,https://github.com/Unbabel/COMET
v3.0.4,Let's say you have a source file with N sentences in SL - eg: source.sl
v3.0.4,and the corresponding references (N sentences) reference.tl
v3.0.4,Translate your file in TL with the -n_best nbest options nbest being
v3.0.4,then number of hypotheses and output the target to -output target.nbest.tl
v3.0.4,Then you need to duplicate source and reference sentences nbest times
v3.0.4,for this script.
v3.0.4,for instance using awk '{for(i=1; i<=n; i++) print}' n=5 reference.tl \
v3.0.4,> reference.5.tl
v3.0.4,same for source.
v3.0.4,This script can be run (for instance with nbest = 5) as follows:
v3.0.4,python oracle_bleu.py --nbest-src source.5.sl --nbest-hyp target.5.tl \
v3.0.4,--nbest-ref reference.5.tl --nbest-order 5 --output target.maxbleu.tl
v3.0.4,It will search in all hyp the best comet score
v3.0.4,when choosing a reference-less model no nbest-ref is required
v3.0.4,for nbest in nbests:
v3.0.4,!/usr/bin/env python
v3.0.4,!/usr/bin/env python3
v3.0.4,coding: utf-8
v3.0.4,Let's say you have a source file with N sentences in SL - eg: source.sl
v3.0.4,Translate your file in TL with the -n_best nbest options nbest being
v3.0.4,then number of hypotheses and output the target to -output target.nbest.tl
v3.0.4,This script can be run (for instance with nbest = 5) as follows:
v3.0.4,python mbr_bleu.py --nbest-hyp target.5.tl \
v3.0.4,--nbest-order 5 --output target.mbr.tl
v3.0.4,It will compare all hyp with eachother and output the max bleu
v3.0.4,!/usr/bin/env python
v3.0.4,-*- coding: utf-8 -*-
v3.0.4,Author: Rico Sennrich
v3.0.4,flake8: noqa
v3.0.4,This file is retrieved from https://github.com/rsennrich/subword-nmt
v3.0.4,hack for python2/3 compatibility
v3.0.4,"find all instances of pair, and update frequency/indices around it"
v3.0.4,find first symbol
v3.0.4,"if first symbol is followed by second symbol, we've found an occurrence of pair (old_word[i:i+2])"
v3.0.4,"assuming a symbol sequence ""A B C"", if ""B C"" is merged, reduce the frequency of ""A B"""
v3.0.4,"assuming a symbol sequence ""A B C B"", if ""B C"" is merged, reduce the frequency of ""C B""."
v3.0.4,"however, skip this if the sequence is A B C B C, because the frequency of ""C B"" will be reduced by the previous code block"
v3.0.4,find new pair
v3.0.4,"assuming a symbol sequence ""A BC D"", if ""B C"" is merged, increase the frequency of ""A BC"""
v3.0.4,"assuming a symbol sequence ""A BC B"", if ""B C"" is merged, increase the frequency of ""BC B"""
v3.0.4,"however, if the sequence is A BC BC, skip this step because the count of ""BC BC"" will be incremented by the previous code block"
v3.0.4,data structure of pair frequencies
v3.0.4,index from pairs to words
v3.0.4,version 0.2 changes the handling of the end-of-word token ('</w>');
v3.0.4,version numbering allows bckward compatibility
v3.0.4,"threshold is inspired by Zipfian assumption, but should only affect speed"
v3.0.4,we probably missed the best pair because of pruning; go back to full statistics
v3.0.4,"threshold is inspired by Zipfian assumption, but should only affect speed"
v3.0.4,python 2/3 compatibility
v3.0.4,read/write files as UTF-8
v3.0.4,Now we can pipe the full file through the model using the Iterator
v3.0.4,reminder a batch includes .src .tgt .indices and it is sorted
v3.0.4,Compute and retrieve the loss for EACH sentence
v3.0.4,Now we need to rearrange the batch of ppl
v3.0.4,in the original order with indices
v3.0.4,!/usr/bin/env python
v3.0.4,-*- coding: utf-8 -*-
v3.0.4,!/usr/bin/env python
v3.0.4,!/usr/bin/env python3
v3.0.4,coding: utf-8
v3.0.4,Let's say you have a source file with N sentences in SL - eg: source.sl
v3.0.4,and the corresponding references (N sentences) reference.tl
v3.0.4,Translate your file in TL with the -n_best nbest options nbest being
v3.0.4,then number of hypotheses and output the target to -output target.nbest.tl
v3.0.4,Then you need to duplicate reference sentences nbest times for this script.
v3.0.4,for instance using awk '{for(i=1; i<=n; i++) print}' n=5 reference.tl \
v3.0.4,> reference.5.tl
v3.0.4,This script can be run (for instance with nbest = 5) as follows:
v3.0.4,python oracle_bleu.py --nbest-hyp target.5.tl --nbest-ref reference.5.tl \
v3.0.4,--nbest-order 5 --output target.maxbleu.tl
v3.0.4,It will search in all hyp the best bleu wrt reference
v3.0.4,and output the max bleu
v3.0.4,!/usr/bin/env python
v3.0.4,with the two module = imp.load_source() below
v3.0.4,we ghost the old torchtext.data.field and depercated
v3.0.4,onmt.inputters.text_dataset
v3.0.4,however this require some functions / classes to be
v3.0.4,monkey patched for loading the old field/vocab objects.
v3.0.4,"module3 = imp.load_source(""Vocab"", ""tools/convertv2_v3.py"")"
v3.0.4,"voc = dict(sorted(fields.vocab.__dict__['freqs'].items(),"
v3.0.4,"key=lambda x: (-x[1], x[0]))).keys()"
v3.0.4,"voc = dict(sorted(fields.vocab.__dict__['freqs'].items(),"
v3.0.4,"key=lambda x: (-x[1], x[0]))).keys()"
v3.0.4,Patch for NLLB200 model loading
v3.0.4,Avoid functionality on inference
v3.0.4,Build embeddings.
v3.0.4,Build encoder.
v3.0.4,Build embeddings.
v3.0.4,Build decoder.
v3.0.4,Share the embedding matrix - preprocess with share_vocab required.
v3.0.4,src/tgt vocab should be the same if `-share_vocab` is specified.
v3.0.4,Update vocabulary embeddings with checkpoint embeddings
v3.0.4,Embedding layers
v3.0.4,Just for debugging purposes
v3.0.4,Remove old vocabulary associated embeddings
v3.0.4,for back compat when attention_dropout was not defined
v3.0.4,Build Model
v3.0.4,Build Generator.
v3.0.4,Load the model states from checkpoint or initialize them.
v3.0.4,This preserves backward-compat for models using customed layernorm
v3.0.4,end of patch for backward compatibility
v3.0.4,Update model embeddings with those from the checkpoint
v3.0.4,after initialization
v3.0.4,!/usr/bin/env python
v3.0.4,if transform + options set in 'valid' we need to copy in main
v3.0.4,transform / options for scoring considered as inference
v3.0.4,"maybe prepare pretrained embeddings, if any"
v3.0.4,Load checkpoint if we resume from a previous training.
v3.0.4,ensure tensorboard output is written in the directory
v3.0.4,of previous checkpoints
v3.0.4,Override checkpoint's update_embeddings as it defaults to false
v3.0.4,Override checkpoint's freezing settings as it defaults to false
v3.0.4,NOTE: It's important that ``opt`` has been validated and updated
v3.0.4,at this point.
v3.0.4,Build model.
v3.0.4,Build optimizer.
v3.0.4,Build model saver
v3.0.4,Use Tensorboard for visualization during training
v3.0.4,Options only during inference
v3.0.4,"Truncation options, for text corpus"
v3.0.4,"as for False, this will be added in _add_train_general_opts"
v3.0.4,Embedding Options
v3.0.4,Model Task Options
v3.0.4,Encoder-Decoder Options
v3.0.4,Freeze Encoder and/or Decoder
v3.0.4,The following options (bridge_extra_node to n_steps) are used
v3.0.4,for training with --encoder_type ggnn (Gated Graph Neural Network).
v3.0.4,Attention options
v3.0.4,Alignement options
v3.0.4,Generator and loss options.
v3.0.4,GPU
v3.0.4,Init options
v3.0.4,Pretrained word vectors
v3.0.4,Freeze word vectors
v3.0.4,Optimization options
v3.0.4,learning rate
v3.0.4,options relate to data preprare
v3.0.4,options relate to train
v3.0.4,Alpha and Beta values for Google Length + Coverage penalty
v3.0.4,"Described here: https://arxiv.org/pdf/1609.08144.pdf, Section 7"
v3.0.4,Length penalty options
v3.0.4,Coverage penalty options
v3.0.4,Decoding Length constraint
v3.0.4,Decoding content constraint
v3.0.4,Adding options relate to decoding strategy
v3.0.4,Adding option for logging
v3.0.4,Adding options related to Transforms
v3.0.4,Copyright 2016 The Chromium Authors. All rights reserved.
v3.0.4,Use of this source code is governed by a BSD-style license that can be
v3.0.4,found in the LICENSE file.
v3.0.4,"Get the key 'value' in the dict, or just use 'value'"
v3.0.4,Basic attributes.
v3.0.4,Set model in training mode.
v3.0.4,Let's clean the GPUs before training loop
v3.0.4,UPDATE DROPOUT
v3.0.4,Run patience mechanism
v3.0.4,"If the patience has reached the limit, stop training"
v3.0.4,swap model params w/ moving average
v3.0.4,(and keep the original parameters)
v3.0.4,Set model in validating mode.
v3.0.4,F-prop through the model.
v3.0.4,Compute loss.
v3.0.4,Compute validation metrics (at batch.dataset level)
v3.0.4,Compute stats
v3.0.4,Update statistics.
v3.0.4,Set model back to training mode.
v3.0.4,Truncated BPTT: reminder not compatible with accum > 1
v3.0.4,1. Create truncated target.
v3.0.4,2. F-prop all but generator.
v3.0.4,3. Compute loss.
v3.0.4,Compute and save stats
v3.0.4,in theory we should divide by accum_count and bptt
v3.0.4,to rescale for each sub batch
v3.0.4,4. Update the parameters and statistics.
v3.0.4,Multi GPU gradient gather
v3.0.4,"If truncated, don't backprop fully."
v3.0.4,"in case of multi step gradient accumulation,"
v3.0.4,update only after accum batches
v3.0.4,For Flake
v3.0.4,we avoid padding while mean pooling
v3.0.4,incoming and outgoing edge embedding
v3.0.4,Find vocab data for tree builting
v3.0.4,Propogation Model
v3.0.4,Initialize the bridge layer
v3.0.4,Token embedding
v3.0.4,Initialize graph using formatted input sequence
v3.0.4,Number of flagged nodes defines node count for this sample
v3.0.4,"(Nodes can have no flags on them, but must be in 'flags' list)."
v3.0.4,The total number of integers in the vocab should allow
v3.0.4,for all features and edges to be defined.
v3.0.4,Use first extra node as only source for decoder init
v3.0.4,Average all nodes to get bridge input
v3.0.4,"LSTM has hidden and cell state, other only one"
v3.0.4,Total number of states
v3.0.4,Build a linear layer for each
v3.0.4,Initialize the bridge layer
v3.0.4,src lengths data is wrapped inside a Tensor.
v3.0.4,"LSTM has hidden and cell state, other only one"
v3.0.4,Total number of states
v3.0.4,Build a linear layer for each
v3.0.4,batch x len x dim
v3.0.4,mask is now (batch x 1 x slen x slen)
v3.0.4,1 to be expanded to number of heads in MHA
v3.0.4,Run the forward pass of every layer of the tranformer.
v3.0.4,Dimensions and padding for constructing the word embedding matrix
v3.0.4,Dimensions and padding for feature embedding matrices
v3.0.4,(these have no effect if feat_vocab_sizes is empty)
v3.0.4,The embedding matrix look-up tables. The first look-up table
v3.0.4,"is for words. Subsequent ones are for features, if any exist."
v3.0.4,The final output size of word + feature vectors. This can vary
v3.0.4,from the word vector size if and only if features are defined.
v3.0.4,This is the attribute you should access if you need to know
v3.0.4,how big your embeddings are going to be.
v3.0.4,The sequence of operations that converts the input sequence
v3.0.4,into a sequence of embeddings. At minimum this consists of
v3.0.4,looking up the embeddings for each word and feature in the
v3.0.4,input. Model parameters may require the sequence to contain
v3.0.4,additional operations as well.
v3.0.4,features must use word_vec_size
v3.0.4,features will use feat_vec_size
v3.0.4,Some utilitary functions for pretrained embeddings
v3.0.4,is this reachable?
v3.0.4,Write to file
v3.0.4,set the opt in place
v3.0.4,set the opt in place
v3.0.4,flake8: noqa
v3.0.4,For command-line option parsing
v3.0.4,"Check pass, set the args."
v3.0.4,"This SRU version implements its own cuda-level optimization,"
v3.0.4,so it requires that:
v3.0.4,1. `cupy` and `pynvrtc` python package installed.
v3.0.4,2. pytorch is built with cuda support.
v3.0.4,3. library path set: export LD_LIBRARY_PATH=<cuda lib path>.
v3.0.4,Check 1.
v3.0.4,Check 2.
v3.0.4,Check 3.
v3.0.4,This sets up device to use.
v3.0.4,-> directions x batch x dim
v3.0.4,For DEBUG
v3.0.4,"size = (length, batch, x.size(-1)) \"
v3.0.4,"if x.dim() == 3 else (batch, x.size(-1))"
v3.0.4,grad_x = x.new(*x.size()) if k_ == 3 else x.new(*size).zero_()
v3.0.4,Normal use
v3.0.4,"An entry check here, will catch on train side and translate side"
v3.0.4,if requirements are not satisfied.
v3.0.4,RNNDecoderState wraps hidden as a tuple.
v3.0.4,fh -> (layers*directions) x batch x dim
v3.0.4,This class is mainly used by decoder.py for RNNs but also
v3.0.4,by the CNN / transformer decoder when copy attention is used
v3.0.4,CNN has its own attention mechanism ConvMultiStepAttention
v3.0.4,Transformer has its own MultiHeadedAttention
v3.0.4,mlp wants it with bias
v3.0.4,"(batch, t_len, d) x (batch, d, s_len) --> (batch, t_len, s_len)"
v3.0.4,"(batch, t_len, s_len, d)"
v3.0.4,one step input
v3.0.4,"compute attention scores, as in Luong et al."
v3.0.4,Softmax or sparsemax to normalize attention weights
v3.0.4,each context vector c_t is the weighted average
v3.0.4,over all the source hidden states
v3.0.4,concatenate
v3.0.4,clamping necessary because of numerical errors: loss should be lower
v3.0.4,"bounded by zero, but negative values near zero are possible without"
v3.0.4,the clamp
v3.0.4,Shift values to be >= 0
v3.0.4,class MultiHeadedAttention(torch.jit.ScriptModule):
v3.0.4,https://arxiv.org/pdf/1803.02155.pdf
v3.0.4,in the paper they suggest either two embeds
v3.0.4,relative_key / relative_value or only
v3.0.4,relative_key. We implemented the same embed
v3.0.4,for both.
v3.0.4,@torch.jit.script_method
v3.0.4,"1) Project key, value, and query."
v3.0.4,as a reminder at training layer_cache[0] remains False
v3.0.4,2) Calculate and scale scores.
v3.0.4,batch x num_heads x query_len x key_len
v3.0.4,1 or key_len x key_len
v3.0.4,1 or key_len x key_len x dim_per_head
v3.0.4,not 100% necessary but expand to nb of heads
v3.0.4,now mask and scores have the same shape
v3.0.4,3) Apply attention dropout and compute context vectors.
v3.0.4,We use the same embeddings for key and value
v3.0.4,At the moment this class is only used by embeddings.Embeddings look-up tables
v3.0.4,-*- coding: utf-8 -*-
v3.0.4,class AverageAttention(torch.jit.ScriptModule):
v3.0.4,@torch.jit.script
v3.0.4,out_features * in_features
v3.0.4,norm is out_features * 1
v3.0.4,batch_size * out_features
v3.0.4,out_features
v3.0.4,out_features
v3.0.4,batch_size * out_features
v3.0.4,"out_channels, in_channels // groups, * kernel_size"
v3.0.4,out_features
v3.0.4,This is used nowhere in the code at the moment (Vincent Nguyen 05/18/2018)
v3.0.4,"in_channels, out_channels, *kernel_size"
v3.0.4,"in_channels, out_channels, *kernel_size"
v3.0.4,"self.out_channels, 1"
v3.0.4,out_features
v3.0.4,out_features
v3.0.4,store roots on diagonal
v3.0.4,Original probabilities.
v3.0.4,Probability of copying p(z=1) batch.
v3.0.4,Probability of not copying: p_{word}(w) * (1 - p(z))
v3.0.4,probabilities assigned by the model to the gold targets
v3.0.4,probability of tokens copied from source
v3.0.4,Set scores for unk to 0 and add eps
v3.0.4,find the indices in which you do not use the copy mechanism
v3.0.4,Drop padding.
v3.0.4,Filter out very short or very long sentences
v3.0.4,from the TM for better performance
v3.0.4,We split the `batch` and perform fuzzy matching
v3.0.4,in smaller chunks of 10.000 examples in order to
v3.0.4,reduce memory usage.
v3.0.4,Perfomance is not affected.
v3.0.4,Probably redundant but let's be safe
v3.0.4,in case some examples are already fuzzied
v3.0.4,(e.g. from another pipeline or workflow)
v3.0.4,We don't want exact matches
v3.0.4,Apply a basic filtering to leave out very short or very long
v3.0.4,sentences and speed up things a bit during fuzzy matching
v3.0.4,Do nothing
v3.0.4,Do nothing
v3.0.4,Punctuation only
v3.0.4,Auto import python files in this directory
v3.0.4,1. sample number of tokens to corrupt
v3.0.4,2. sample positions to corrput
v3.0.4,3. sample corrupted values
v3.0.4,1. sample number of tokens to corrupt
v3.0.4,2. sample positions to corrput
v3.0.4,3. Drop token on chosen position
v3.0.4,1. sample number of tokens to corrupt
v3.0.4,2. sample positions to corrput
v3.0.4,3. mask word on chosen position
v3.0.4,"Sharing options among `TokenizerTransform`s, same name conflict in"
v3.0.4,this scope will be resolved by remove previous occurrence in parser
v3.0.4,subword regularization(or BPE dropout) options:
v3.0.4,subword vocabulary restriction options:
v3.0.4,derterministic subwording
v3.0.4,subword sampling when nbest_size > 1 or -1
v3.0.4,alpha should be 0.0 < alpha < 1.0
v3.0.4,Load vocabulary file if provided and set threshold
v3.0.4,Load Subword Model
v3.0.4,-1: keep everything (i.e. 1 mask per token)
v3.0.4,0: replace everything (i.e. no mask)
v3.0.4,1: 1 mask per span
v3.0.4,view each subword as word start / input is word level token
v3.0.4,Pretend it ends with a full stop so last span is a sentence
v3.0.4,"Tokens that are full stops, where the previous token is not"
v3.0.4,Make sure we have enough to mask
v3.0.4,Trim to masking budget
v3.0.4,Handle 0-length mask (inserts) separately
v3.0.4,assert is_word_start[-1] == 0
v3.0.4,assert tokens_length - 1 not in indices
v3.0.4,"keep index, but replace it with [MASK]"
v3.0.4,"acts as a long length, so spans don't go over the end of doc"
v3.0.4,next position from each word_start
v3.0.4,delete token: 1 mask/remove per span
v3.0.4,"keep index, but replace it with [MASK]: 1 mask per token"
v3.0.4,A bit faster when all lengths are 1
v3.0.4,to cover whole token
v3.0.4,delete token
v3.0.4,"keep index, but replace it with [MASK]"
v3.0.4,assert tokens_length - 1 not in indices
v3.0.4,!/usr/bin/env python3
v3.0.4,-*- coding: utf-8 -*-
v3.0.4,Most code taken from: https://github.com/alvations/sacremoses
v3.0.4,Which in turn is based on the Moses punctuation normalizer.
v3.0.4,https://github.com/moses-smt/mosesdecoder/blob/master/scripts/
v3.0.4,tokenizer/normalize-punctuation.perl
v3.0.4,don't fix period at end of sentence
v3.0.4,Regex substitutions from replace-unicode-punctuation.perl
v3.0.4,https://github.com/moses-smt/mosesdecoder/blob/master/
v3.0.4,scripts/tokenizer/replace-unicode-punctuation.perl
v3.0.4,Adds the penn substitutions after extra_whitespace regexes.
v3.0.4,"Optionally, replace unicode puncts BEFORE normalization."
v3.0.4,Actual normalization.
v3.0.4,"print(regexp, substitution)"
v3.0.4,print(text)
v3.0.4,"Optionally, replace unicode puncts BEFORE normalization."
v3.0.4,batch 0 will always predict EOS. The other batches will predict
v3.0.4,non-eos scores.
v3.0.4,"""best"" prediction is eos - that should be blocked"
v3.0.4,include at least one prediction OTHER than EOS
v3.0.4,that is greater than -1e20
v3.0.4,now batch 0 has ended and no others have
v3.0.4,initial step
v3.0.4,batch 0 dies on step 0
v3.0.4,include at least one prediction OTHER than EOS
v3.0.4,that is greater than -1e20
v3.0.4,step 2
v3.0.4,(old) batch 8 dies on step 1
v3.0.4,step 3
v3.0.4,everything dies
v3.0.4,initial step
v3.0.4,batch 0 dies on step 0
v3.0.4,include at least one prediction OTHER than EOS
v3.0.4,that is greater than -1e20
v3.0.4,step 2
v3.0.4,(old) batch 8 dies on step 1
v3.0.4,step 3
v3.0.4,everything dies
v3.0.4,initial step
v3.0.4,finish one beam
v3.0.4,include at least one prediction OTHER than EOS
v3.0.4,that is greater than -1e20
v3.0.4,step 2
v3.0.4,finish example in last batch
v3.0.4,(old) batch 8 dies on step 1
v3.0.4,step 3
v3.0.4,everything dies
v3.0.4,initial step
v3.0.4,batch 0 dies on step 0
v3.0.4,include at least one prediction OTHER than EOS
v3.0.4,that is greater than -1e20
v3.0.4,step 2
v3.0.4,(old) batch 8 dies on step 1
v3.0.4,step 3
v3.0.4,everything dies
v3.0.4,illegal_weights_mask = torch.ByteTensor([
v3.0.4,"[0, 0, 0, 0, 0, 0, 0],"
v3.0.4,"[0, 0, 0, 1, 1, 1, 1],"
v3.0.4,"[0, 0, 0, 0, 0, 1, 1],"
v3.0.4,"[0, 0, 1, 1, 1, 1, 1]])"
v3.0.4,TODO: fix for pytorch 0.3
v3.0.4,illegal_weights = alignments.masked_select(illegal_weights_mask)
v3.0.4,"self.assertEqual(0.0, illegal_weights.data.sum())"
v3.0.4,this could be considered an integration test because it touches
v3.0.4,the filesystem for the config file (and the models)
v3.0.4,no dummy prefix
v3.0.4,no dummy prefix
v3.0.4,make sure the scalars are in the event accumulator tags
v3.0.4,required arguments
v3.0.4,transforms that require vocab will not create if not provide vocab
v3.0.4,1. Init first transform in the pipe
v3.0.4,2. Init second transform in the pipe
v3.0.4,3. Sequential combine them into a transform pipe
v3.0.4,4. apply transform pipe for example
v3.0.4,"5. example after the pipe exceed the length limit, thus filtered"
v3.0.4,6. Transform statistics registed (here for filtertoolong)
v3.0.4,"7. after report, statistics become empty as a fresh start"
v3.0.4,filter_transform.warm_up()
v3.0.4,test BPE-dropout:
v3.0.4,1. disable bpe dropout for not training example
v3.0.4,2. enable bpe dropout for training example
v3.0.4,3. (NOTE) disable dropout won't take effect if already seen
v3.0.4,this is caused by the cache mechanism in bpe:
v3.0.4,return cached subword if the original token is seen when no dropout
v3.0.4,test SP regularization:
v3.0.4,1. enable regularization for training example
v3.0.4,2. disable regularization for not training example
v3.0.4,Not apply token drop for not training example
v3.0.4,apply token drop for training example
v3.0.4,Not apply token mask for not training example
v3.0.4,apply token mask for training example
v3.0.4,require vocabs to warm_up
v3.0.4,Not apply token mask for not training example
v3.0.4,apply token mask for training example
v3.0.4,"Defalt: full_stop_token=[""."", ""?"", ""!""]"
v3.0.4,"Defalt: full_stop_token=[""."", ""?"", ""!""]"
v3.0.4,random_ratio of inserted tokens are chosen in vocab
v3.0.4,others are MASK_TOK
v3.0.4,"insert_ratio=0.0,"
v3.0.4,"random_ratio=0.0,"
v3.0.4,"Defalt: full_stop_token=[""."", ""?"", ""!""]"
v3.0.4,all token are considered as an individual word
v3.0.4,1. tokens are dropped when replace_length is 0
v3.0.4,"print(f""token delete: {masked} / {tokens}"")"
v3.0.4,2. tokens are replaced by MASK when replace_length is 1
v3.0.4,"print(f""token mask: {masked} / {tokens}"")"
v3.0.4,"insert_ratio=0.0,"
v3.0.4,"random_ratio=0.0,"
v3.0.4,"Defalt: full_stop_token=[""."", ""?"", ""!""]"
v3.0.4,start token of word are identified using subword marker
v3.0.4,"1. replace_length 0: ""words"" are dropped"
v3.0.4,"print(f""word delete: {masked} / {tokens}"")"
v3.0.4,"self.assertEqual(len(masked), n_words - n_masked)"
v3.0.4,"2. replace_length 1: ""words"" are replaced with a single MASK"
v3.0.4,"print(f""whole word single mask: {masked} / {tokens}"")"
v3.0.4,len(masked) depend on number of tokens in select word
v3.0.4,"3. replace_length -1: all tokens in ""words"" are replaced with MASK"
v3.0.4,"print(f""whole word multi mask: {masked} / {tokens}"")"
v3.0.4,number of mask_tok depend on number of tokens in selected word
v3.0.4,number of MASK_TOK can be greater than n_masked
v3.0.4,"insert_ratio=0.5,"
v3.0.4,"random_ratio=0.3,"
v3.0.4,"Defalt: full_stop_token=[""."", ""?"", ""!""]"
v3.0.4,start token of word are identified using subword marker
v3.0.4,n_words = sum(token_starts)
v3.0.4,n_masked = math.ceil(n_words * bart_noise.mask_ratio)
v3.0.4,"print(f""Text Span Infilling: {infillied} / {tokens}"")"
v3.0.4,"print(n_words, n_masked)"
v3.0.4,!/usr/bin/env python
v3.0.4,-*- coding: utf-8 -*-
v3.0.4,Inject some dummy training options that may needed when build fields
v3.0.4,Remove the generated *pt files.
v3.0.4,Remove the generated data samples
v3.0.4,all beams repeat (beam >= 1 repeat dummy scores)
v3.0.4,predict repeat_idx over and over again
v3.0.4,"before repeat, scores are either 0 or -inf"
v3.0.4,"on repeat, `repeat_idx` score is BLOCKED_SCORE"
v3.0.4,"(but it's still the best score, thus we have"
v3.0.4,"[BLOCKED_SCORE, -inf, -inf, -inf, -inf]"
v3.0.4,repetitions keeps maximizing score
v3.0.4,"index 0 has been blocked, so repeating=>+0.0 score"
v3.0.4,other indexes are -inf so repeating=>BLOCKED_SCORE
v3.0.4,which is higher
v3.0.4,beam 0 and beam >=2 will repeat (beam >= 2 repeat dummy scores)
v3.0.4,non-interesting beams are going to get dummy values
v3.0.4,"on initial round, only predicted scores for beam 0"
v3.0.4,matter. Make two predictions. Top one will be repeated
v3.0.4,"in beam zero, second one will live on in beam 1."
v3.0.4,predict the same thing in beam 0
v3.0.4,continue pushing around what beam 1 predicts
v3.0.4,"now beam 0 dies (along with the others), beam 1 -> beam 0"
v3.0.4,"now beam 0 dies (along with the others), beam 1 -> beam 0"
v3.0.4,beam 0 and beam >= 2 will repeat (beam 2 repeats excluded idx)
v3.0.4,non-interesting beams are going to get dummy values
v3.0.4,predict the same thing in beam 0
v3.0.4,continue pushing around what beam 1 predicts
v3.0.4,predict the allowed-repeat again in beam 2
v3.0.4,"now beam 0 dies, beam 1 -> beam 0, beam 2 -> beam 1"
v3.0.4,and the rest die
v3.0.4,"since all preds after i=0 are 0, we can check"
v3.0.4,that the beam is the correct idx by checking that
v3.0.4,the curr score is the initial score
v3.0.4,beam 0 will always predict EOS. The other beams will predict
v3.0.4,non-eos scores.
v3.0.4,non-interesting beams are going to get dummy values
v3.0.4,"""best"" prediction is eos - that should be blocked"
v3.0.4,include at least beam_sz predictions OTHER than EOS
v3.0.4,that are greater than -1e20
v3.0.4,predict eos in beam 0
v3.0.4,provide beam_sz other good predictions
v3.0.4,now the top beam has ended and no others have
v3.0.4,"not of interest, but want to make sure it keeps running"
v3.0.4,since only beam 0 terminates and n_best = 2
v3.0.4,"this is also a test that when block_ngram_repeat=0,"
v3.0.4,repeating is acceptable
v3.0.4,non-interesting beams are going to get dummy values
v3.0.4,"""best"" prediction is eos - that should be blocked"
v3.0.4,include at least beam_sz predictions OTHER than EOS
v3.0.4,that are greater than -1e20
v3.0.4,predict eos in beam 1
v3.0.4,provide beam_sz other good predictions in other beams
v3.0.4,beam 1 dies on min_length
v3.0.4,beam 0 dies on the step after beam 1 dies
v3.0.4,"inp_lens is tiled in initialize, reassign to make attn match"
v3.0.4,non-interesting beams are going to get dummy values
v3.0.4,"""best"" prediction is eos - that should be blocked"
v3.0.4,include at least beam_sz predictions OTHER than EOS
v3.0.4,that are greater than -1e20
v3.0.4,predict eos in beam 1
v3.0.4,provide beam_sz other good predictions in other beams
v3.0.4,no top beams are finished yet
v3.0.4,beam 1 dies on min_length
v3.0.4,no top beams are finished yet
v3.0.4,beam 0 dies on the step after beam 1 dies
v3.0.4,top beam is finished now so there are attentions
v3.0.4,two beams are finished in each batch
v3.0.4,second dim is cut down to the non-padded src length
v3.0.4,first dim is equal to the time of death
v3.0.4,(beam 0 died at current step - adjust for SOS)
v3.0.4,(beam 1 died at last step - adjust for SOS)
v3.0.4,behavior gets weird when beam is already done so just stop
v3.0.4,this is just test_beam.TestBeamAgainstReferenceCase repeated
v3.0.4,in each batch.
v3.0.4,"init_preds: [4, 3, 5, 6, 7] - no EOS's"
v3.0.4,no EOS's yet
v3.0.4,"[5, 3, 2, 6, 0], so beam 2 predicts EOS!"
v3.0.4,assumes beam 2 finished on last step
v3.0.4,ended beam 2 shouldn't continue
v3.0.4,"[2, 5, 3, 6, 0] repeat self.BATCH_SZ, so beam 0 predicts EOS!"
v3.0.4,"[-2.4879, -3.8910, -4.1010, -4.2010, -4.4010] repeat self.BATCH_SZ"
v3.0.4,another beam is finished in all batches
v3.0.4,new beam 0 finished
v3.0.4,new beam 0 is old beam 3
v3.0.4,assumes beam 0 finished on last step
v3.0.4,"[5, 2, 6, 1, 0] repeat self.BATCH_SZ, so beam 1 predicts EOS!"
v3.0.4,we finish 3 hyps per example in this step
v3.0.4,new beam 1 is old beam 3
v3.0.4,this could be considered an integration test because it tests
v3.0.4,interactions between the GNMT scorer and the beam
v3.0.4,"-data option is required, but not used in this test, so dummy."
v3.0.4,len x batch x nfeat
v3.0.4,Initialize vectors to compare size with
v3.0.4,Ensure correct sizes and types
v3.0.4,Make sure that output has the correct size and type
v3.0.4,"[('encoder_type', 'transformer'),"
v3.0.4,"('word_vec_size', 16), ('hidden_size', 16)],"
v3.0.4,""""""" Only do SRU test if requirment is safisfied. """""""
v3.0.4,SRU doesn't support input_feed.
v3.0.4,first check there's nothing unexpectedly not trainable
v3.0.4,ok: word embeddings shouldn't be trainable
v3.0.4,if word vecs are freezed
v3.0.4,ok: positional encodings shouldn't be trainable
v3.0.4,then check nothing unexpectedly trainable
v3.0.4,Decoder state
v3.0.4,Build the RNN.
v3.0.4,Set up the context gate.
v3.0.4,Set up the standard attention.
v3.0.4,The encoder hidden is  (layers*directions) x batch x dim.
v3.0.4,We need to convert it to layers x batch x (directions*dim).
v3.0.4,Init the input feed.
v3.0.4,Update the state with the result.
v3.0.4,Concatenates sequence of tensors along a new dimension.
v3.0.4,NOTE: v0.3 to 0.4: dec_outs / attns[*] may not be list
v3.0.4,(in particular in case of SRU) it was not raising error in 0.3
v3.0.4,since stack(Variable) was allowed.
v3.0.4,"In 0.4, SRU returns a tensor that shouldn't be stacke"
v3.0.4,Calculate the attention.
v3.0.4,Calculate the context gate.
v3.0.4,Additional args check.
v3.0.4,Input feed concatenates hidden state with
v3.0.4,input at every time step.
v3.0.4,TODO: context gate should be employed
v3.0.4,instead of second RNN transform.
v3.0.4,Update the coverage attention.
v3.0.4,"attns[""coverage""] is actually c^(t+1) of See et al(2017)"
v3.0.4,1-index shifted
v3.0.4,Decoder State
v3.0.4,CNNDecoder has its own attention mechanism.
v3.0.4,Set up a separate copy attention layer if needed.
v3.0.4,The output of CNNEncoder.
v3.0.4,The combination of output of CNNEncoder and source embeddings.
v3.0.4,Process the result and update the attentions.
v3.0.4,Update the state.
v3.0.4,TODO change the way attns is returned dict => list or tuple (onnx)
v3.0.4,src_len is a single tensor shared between all models.
v3.0.4,This assumption will not hold if Translator is modified
v3.0.4,to calculate src_len as something other than the length
v3.0.4,of the input.
v3.0.4,"return _, (B, Q_len, K_len)"
v3.0.4,"layer average attention across heads, get ``(B, Q, K)``"
v3.0.4,"Case 1: no full_context, no align heads -> layer avg baseline"
v3.0.4,"Case 2: no full_context, 1 align heads -> guided align"
v3.0.4,"Case 3: full_context, 1 align heads -> full cte guided align"
v3.0.4,BoolTensor was introduced in pytorch 1.2
v3.0.4,T: could be 1 in the case of stepwise decoding or tgt_len
v3.0.4,masking is necessary when sequence length is greater than one
v3.0.4,mask now are (batch x 1 x tlen x s or t len)
v3.0.4,1 = heads to be expanded in MHA
v3.0.4,Decoder State
v3.0.4,"previously, there was a GlobalAttention module here for copy"
v3.0.4,"attention. But it was never actually used -- the ""copy"" attention"
v3.0.4,just reuses the context attention.
v3.0.4,"attns[""align""] = torch.stack(attn_aligns, 0).mean(0)  # All avg"
v3.0.4,TODO change the way attns is returned dict => list or tuple (onnx)
v3.0.4,first value set to True triggered by the beginning of decoding
v3.0.4,layer_cache becomes active in the MultiHeadedAttention fwd
v3.0.4,T: could be 1 in the case of stepwise decoding or tgt_len
v3.0.4,masking is necessary when sequence length is greater than one
v3.0.4,mask now are (batch x 1 x tlen x tlen)
v3.0.4,1 = heads to be expanded in MHA
v3.0.4,TODO change the way attns is returned dict => list or tuple (onnx)
v3.0.4,"buffer size in bytes, determine equiv. # of elements based on data type"
v3.0.4,copy tensors into buffer_t
v3.0.4,all-reduce and rescale
v3.0.4,copy all-reduced buffer back into tensors
v3.0.4,"print(filled, sz)"
v3.0.4,"tensor is bigger than buffer, all-reduce and rescale directly"
v3.0.4,"buffer is full, all-reduce and replace buffer with grad"
v3.0.4,add tensor to buffer
v3.0.4,"propagate exception to parent process, keeping original traceback"
v3.0.4,TODO: Find a better way to check for sparse gradients.
v3.0.4,we use apex.amp
v3.0.4,In this case use the old FusedAdam with
v3.0.4,FP16_optimizer wrapper
v3.0.4,Load everything from the checkpoint.
v3.0.4,Build everything from scratch.
v3.0.4,"Reset optimizer, keep options."
v3.0.4,"Reset options, keep optimizer."
v3.0.4,State can be partially restored.
v3.0.4,should be: self._optimizer.zero_grad(set_to_none)
v3.0.4,but apex.amp is not up-to-date:
v3.0.4,https://github.com/NVIDIA/apex/blob/master/apex/amp/_process_optimizer.py#L367
v3.0.4,"unscaled optimizer's gradients (already done therefore skip),"
v3.0.4,skips optimizer.step() if gradients contain infs/NaNs.
v3.0.4,Updates the scale for next iteration.
v3.0.4,Code below is an implementation of https://arxiv.org/pdf/1804.04235.pdf
v3.0.4,inspired but modified from https://github.com/DeadAt0m/adafactor-pytorch
v3.0.4,backward compatibility
v3.0.4,assuming a list/generator of parameter means single group
v3.0.4,compute combined scale factor for this group
v3.0.4,norm is in fact norm*scale
v3.0.4,note: p.grad should not ever be set for correct operation of
v3.0.4,mixed precision optimizer that sometimes sends None gradients
v3.0.4,State initialization
v3.0.4,Exponential moving average of gradient values
v3.0.4,Exponential moving average of squared gradient values
v3.0.4,-*- coding: utf-8 -*-
v3.0.4,placing this here make it easier to call logger.info
v3.0.4,"from anywhere, just 'from onmt.utils.logging import logger'"
v3.0.4,"align_head contains value in [0, 1) presenting attn prob,"
v3.0.4,0 was resulted by the context attention src_pad_mask
v3.0.4,"So, the correspand position in ref_align should also be 0"
v3.0.4,"Therefore, clip align_head to > 1e-18 should be bias free."
v3.0.4,rescale with tau (temperature) and apply the log_softmax.
v3.0.4,ct2 expects src with lengths without padding
v3.0.4,again we use raw probs to rescale with tau and apply log_softmax
v3.0.4,lm_scores are in log space so log_target=True
v3.0.4,rescale with tau (temperature) and apply the log_softmax.
v3.0.4,ct2 expects src with lengths without padding
v3.0.4,again we use raw probs to rescale with tau and apply log_softmax
v3.0.4,lm_scores are in log space so log_target=True
v3.0.4,take into account here the tgt_shift_index (0 / 1 = LM/NMT)
v3.0.4,Correct target copy token instead of <unk>
v3.0.4,tgt[i] = align[i] + len(tgt_vocab)
v3.0.4,for i such that tgt[i] == 0 and align[i] != 0
v3.0.4,in the case criterion reduction is None then we need
v3.0.4,to sum the loss of each sentence in the batch
v3.0.4,Check Transforms
v3.0.4,Check path
v3.0.4,tgt is src for LM task
v3.0.4,Check prefix: will be used when use prefix transform
v3.0.4,Check weight
v3.0.4,Check features
v3.0.4,validation when train:
v3.0.4,Check embeddings stuff
v3.0.4,"Backward compatibility with ""fix_word_vecs_*"" opts"
v3.0.4,encoder and decoder should be same sizes
v3.0.4,"Load default opt values, then overwrite with the opts in"
v3.0.4,"the checkpoint. That way, if there are new options added,"
v3.0.4,the defaults are used.
v3.0.4,It comes from training
v3.0.4,TODO: needs to be added as inference opt
v3.0.4,Don't do anything
v3.0.4,Update best score of each criteria
v3.0.4,Reset tolerance
v3.0.4,Update current status
v3.0.4,Decrease tolerance
v3.0.4,Log
v3.0.4,Log
v3.0.4,Get a list of world_size lists with len(stat_list) Statistics objects
v3.0.4,"this param init is overridden by model_builder, useless then."
v3.0.4,SRU doesn't support PackedSequence.
v3.0.4,-*- coding: utf-8 -*-
v3.0.4,threshold on 1 to avoid div by 0
v3.0.4,treat alignment matrix one by one as each have different lengths
v3.0.4,No alignment if not exist valid tgt token
v3.0.4,get valid alignment (sub-matrix from full paded aligment matrix)
v3.0.4,Helper functions
v3.0.4,Keeps track of the original words/subwords
v3.0.4,('prior_tokenization' option)
v3.0.4,In case there is a final case_markup when new_spacer is on
v3.0.4,translate
v3.0.4,for validation we build an infer_iter per batch
v3.0.4,in order to avoid oom issues because there is no
v3.0.4,batching strategy in `textbatch_to_tensor`
v3.0.4,apply_reverse refs
v3.0.4,flatten preds
v3.0.4,save results
v3.0.4,-*- coding: utf-8 -*-
v3.0.4,this one is needed for Random Shuffler of batches
v3.0.4,in multi gpu it ensures datasets are read in the same order
v3.0.4,some cudnn methods can be random even after fixing the seed
v3.0.4,unless you tell it to be deterministic
v3.0.4,This one is needed for various tranfroms
v3.0.4,These ensure same initialization in multi gpu mode
v3.0.4,we need to check the model path + any tokenizer path
v3.0.4,patch to log stdout spawned processes of dataloader
v3.0.4,bucket_size = batch_size
v3.0.4,For TRAIN we need to group examples by length
v3.0.4,"for faster performance, but otherwise, sequential."
v3.0.4,For TRAIN we shuffle batches within the bucket
v3.0.4,otherwise sequential
v3.0.4,for specific case of rnn_packed need to be sorted
v3.0.4,within the batch
v3.0.4,Make features part of src like
v3.0.4,"{'src': {'src': ..., 'feat1': ...., 'feat2': ....}}"
v3.0.4,We apply the same TransformPipe to all the bucket
v3.0.4,at this point an example looks like:
v3.0.4,"{'src': {'src': ..., 'feat1': ...., 'feat2': ....},"
v3.0.4,"'tgt': {'tgt': ...},"
v3.0.4,"'src_original': ['tok1', ...'tokn'],"
v3.0.4,"'tgt_original': ['tok1', ...'tokm'],"
v3.0.4,'indices' : seq in bucket
v3.0.4,"'align': ...,"
v3.0.4,}
v3.0.4,we'll need to change this if we introduce tgt feat
v3.0.4,Need to add features in last dimensions
v3.0.4,Need to add features also in 'src'
v3.0.4,make a small vocab containing just the tokens in the source sequence
v3.0.4,Map source tokens to indices in the dynamic dict.
v3.0.4,-*- coding: utf-8 -*-
v3.0.4,'src_original' and 'tgt_original' store the
v3.0.4,original line before tokenization. These
v3.0.4,fields are used later on in the feature
v3.0.4,transforms.
v3.0.4,NOTE: moved to dynamic_iterator.py cf process()
v3.0.4,item = self.transform.apply(
v3.0.4,"example, is_train=self.infinitely, corpus_name=self.cid)"
v3.0.4,empty example: skip
v3.0.4,"No encoder in LM, seq2seq count formatting kept"
v3.0.4,_check_save_model_path
v3.0.4,NOTE: We need to trim the vocab to remove any unk tokens that
v3.0.4,were not originally here.
v3.0.4,"for side in [""src"", ""tgt""]:"
v3.0.4,keys_to_pop = []
v3.0.4,"if hasattr(vocab[side], ""fields""):"
v3.0.4,unk_token = vocab[side].fields[0][1].vocab.itos[0]
v3.0.4,"for key, value in vocab[side].fields[0][1].vocab.stoi.items():"
v3.0.4,if value == 0 and key != unk_token:
v3.0.4,keys_to_pop.append(key)
v3.0.4,for key in keys_to_pop:
v3.0.4,"vocab[side].fields[0][1].vocab.stoi.pop(key, None)"
v3.0.4,!/usr/bin/env python
v3.0.4,!/usr/bin/env python
v3.0.4,!/usr/bin/env python
v3.0.4,-*- coding: utf-8 -*-
v3.0.4,!/usr/bin/env python
v3.0.4,Just for debugging purposes
v3.0.4,It appends features to subwords when dumping to file
v3.0.4,!/usr/bin/env python
v3.0.4,!/usr/bin/env python
v3.0.4,Set sharing strategy manually instead of default based on the OS.
v3.0.4,torch.multiprocessing.set_sharing_strategy('file_system')
v3.0.4,Create a thread to listen for errors in the child processes.
v3.0.4,Train with multiprocessing.
v3.0.4,magic indices
v3.0.4,result caching
v3.0.4,Here we set the decoder to start with self.start (BOS or EOS)
v3.0.4,fix length constraint and remove eos from count
v3.0.4,add one to account for BOS. Don't account for EOS because hitting
v3.0.4,this implies it hasn't been found.
v3.0.4,we don't block nothing if the user doesn't want it
v3.0.4,we can't block nothing beam's too short
v3.0.4,we check paths one by one
v3.0.4,we don't forbid nothing if the user doesn't want it
v3.0.4,we can't forbid nothing if beam's too short
v3.0.4,Reordering forbidden_tokens following beam selection
v3.0.4,We rebuild a dict to ensure we get the value and not the pointer
v3.0.4,Grabing the newly selected tokens and associated ngram
v3.0.4,skip the blocking if any token in current_ngram is excluded
v3.0.4,"pickups: Tensor where specified index were set to 1, others 0"
v3.0.4,"dropdowns: opposite of pickups, 1 for those shouldn't pick"
v3.0.4,Minus dropdowns to log_probs making probabilities of
v3.0.4,unspecified index close to 0
v3.0.4,"prediction step have surpass length of given target_prefix,"
v3.0.4,no need to further change this attr
v3.0.4,keep indices until overflowing p
v3.0.4,Set all logits that are not in the top-p to -10000.
v3.0.4,This puts the probabilities close to 0.
v3.0.4,Set all logits that are not in the top-k to -10000.
v3.0.4,This puts the probabilities close to 0.
v3.0.4,"For temp=0.0, take the argmax to avoid divide-by-zero errors."
v3.0.4,keep_topk=1 is also equivalent to argmax.
v3.0.4,maybe fix some prediction at this step by modifying log_probs
v3.0.4,"shape: (sum(~ self.is_finished), 1)"
v3.0.4,in LM task src_len is associated with currently generated src
v3.0.4,and therefore needs to follow the generation
v3.0.4,!/usr/bin/env python
v3.0.4,for debugging
v3.0.4,TODO: maybe add dynamic part
v3.0.4,Statistics
v3.0.4,In the case of length_penalty = none we report the total logprobs
v3.0.4,divided by the number of sentence to get an approximation of the
v3.0.4,per sentence logprob. We also return the corresponding ppl
v3.0.4,"When a length_penalty is used eg: ""avg"" or ""wu"" since logprobs"
v3.0.4,are normalized per token we report the per line per token logprob
v3.0.4,"and the corresponding ""per word perplexity"""
v3.0.4,Turn any copied words into UNKs.
v3.0.4,"Decoder forward, takes [batch, tgt_len, nfeats] as input"
v3.0.4,"and [batch, src_len, hidden] as enc_out"
v3.0.4,"in case of inference tgt_len = 1, batch = beam times batch_size"
v3.0.4,"in case of Gold Scoring tgt_len = actual length, batch = 1 batch"
v3.0.4,Generator forward.
v3.0.4,"returns [(batch_size x beam_size) , vocab ] when 1 step"
v3.0.4,"or [batch_size, tgt_len, vocab ] when full sentence"
v3.0.4,"here we have scores [tgt_lenxbatch, vocab] or [beamxbatch, vocab]"
v3.0.4,at this point scores is batch first (dim=0)
v3.0.4,"returns [(batch_size x beam_size) , vocab ] when 1 step"
v3.0.4,"or [batch_size, tgt_len, vocab ] when full sentence"
v3.0.4,(0) add BOS and padding to tgt prediction
v3.0.4,(1) Encoder forward.
v3.0.4,(2) Repeat src objects `n_best` times.
v3.0.4,"We use batch_size x n_best, get ``(batch * n_best, src_len, nfeat)``"
v3.0.4,"(3) Init decoder with n_best src,"
v3.0.4,"reshape tgt to ``(len, batch * n_best, nfeat)``"
v3.0.4,it should be done in a better way
v3.0.4,here dec_in is batch first
v3.0.4,masked_select
v3.0.4,get aligned src id for each prediction's valid tgt tokens
v3.0.4,TODO: support these blacklisted features
v3.0.4,(0) Prep the components of the search.
v3.0.4,(1) Run the encoder on the src.
v3.0.4,(2) prep decode_strategy. Possibly repeat src objects.
v3.0.4,(3) Begin decoding step by step:
v3.0.4,"decoder_input = decode_strategy.current_predictions.view(1, -1,"
v3.0.4,1)
v3.0.4,Reorder states.
v3.0.4,TODO: support these blacklisted features
v3.0.4,(0) Prep the components of the search.
v3.0.4,(1) split src into src and target_prefix to avoid padding.
v3.0.4,(2) init decoder
v3.0.4,(3) prep decode_strategy. Possibly repeat src objects.
v3.0.4,(4) Begin decoding step by step:
v3.0.4,Reorder states.
v3.0.4,select indexes in model state/cache
v3.0.4,beam parameters
v3.0.4,beam state
v3.0.4,BoolTensor was introduced in pytorch 1.2
v3.0.4,"""global state"" of the old beam"
v3.0.4,buffers for the topk scores and 'backpointer'
v3.0.4,for testing
v3.0.4,maybe fix some prediction at this step by modifying log_probs
v3.0.4,Flatten probs into a list of possibilities.
v3.0.4,Penalize beams that finished.
v3.0.4,"on real data (newstest2017) with the pretrained transformer,"
v3.0.4,it's faster to not move this back to the original device
v3.0.4,Store finished hypotheses for this batch.
v3.0.4,End condition is the top beam finished and we can return
v3.0.4,n_best hypotheses.
v3.0.4,"If all sentences are translated, no need to go further."
v3.0.4,Remove finished batches for the next step.
v3.0.4,using integer division to get an integer _B without casting
v3.0.4,force the output to be longer than self.min_length
v3.0.4,Multiply probs by the beam probability.
v3.0.4,"if the sequence ends now, then the penalty is the current"
v3.0.4,"length + 1, to include the EOS token"
v3.0.4,Avoid any direction that would repeat unwanted ngrams
v3.0.4,Pick up candidate token by curr_scores
v3.0.4,Recover log probs.
v3.0.4,Length penalty is just a scalar. It doesn't matter if it's applied
v3.0.4,before or after the topk.
v3.0.4,Resolve beam origin and map to batch index flat representation.
v3.0.4,Append last prediction.
v3.0.4,update global state (step == 1)
v3.0.4,update global state (step > 1)
v3.0.4,"shape: (batch_size x beam_size, 1)"
v3.0.4,in LM task src_len is associated with currently generated src
v3.0.4,and therefore needs to follow the generation
v3.0.4,in LM task src_len is associated with currently generated src
v3.0.4,and therefore needs to follow the generation
v3.0.4,Term will be subtracted from probability
v3.0.4,Probability will be divided by this
v3.0.4,these warnings indicate that either the alpha/beta
v3.0.4,"forces a penalty to be a no-op, or a penalty is a no-op but"
v3.0.4,the alpha/beta would suggest otherwise.
v3.0.4,using some coverage penalty
v3.0.4,!/usr/bin/env python
v3.0.4,semaphore doesn't have a timeout arg in Python 2.7
v3.0.4,perform a first request to initialize everything
v3.0.4,backwards compatibility for confs
v3.0.4,every segment becomes a dict for flexibility purposes
v3.0.4,NOTE: translator returns lists of `n_best` list
v3.0.4,build back results with empty texts
v3.0.4,load can be called multiple times: modify copy
v3.0.4,output contain alignment
v3.0.4,Below are all the different penalty terms implemented so far.
v3.0.4,Subtract coverage penalty from topk log probs.
v3.0.4,Divide topk log probs by length penalty.
v3.0.4,Sorting
v3.0.4,Chinese segmentation
v3.0.4,Chinese simplify -> Chinese traditional standard
v3.0.4,Chinese simplify -> Chinese traditional (HongKong)
v3.0.4,Chinese simplify -> Chinese traditional (Taiwan)
v3.0.4,Chinese traditional -> Chinese simplify (v1)
v3.0.4,Chinese traditional -> Chinese simplify (v2)
v3.0.4,Auto import python files in this directory
v3.0.3,!/usr/bin/env python
v3.0.3,!/usr/bin/env python
v3.0.3,!/usr/bin/env python
v3.0.3,!/usr/bin/env python
v3.0.3,!/usr/bin/env python
v3.0.3,!/usr/bin/env python3
v3.0.3,-*- coding: utf-8 -*-
v3.0.3,
v3.0.3,"OpenNMT-py documentation build configuration file, created by"
v3.0.3,sphinx-quickstart on Sun Dec 17 12:07:14 2017.
v3.0.3,
v3.0.3,This file is execfile()d with the current directory set to its
v3.0.3,containing dir.
v3.0.3,
v3.0.3,Note that not all possible configuration values are present in this
v3.0.3,autogenerated file.
v3.0.3,
v3.0.3,All configuration values have a default; values that are commented out
v3.0.3,serve to show the default.
v3.0.3,"If extensions (or modules to document with autodoc) are in another directory,"
v3.0.3,add these directories to sys.path here. If the directory is relative to the
v3.0.3,"documentation root, use os.path.abspath to make it absolute, like shown here."
v3.0.3,
v3.0.3,import os
v3.0.3,import sys
v3.0.3,"sys.path.insert(0, os.path.abspath('.'))"
v3.0.3,-- General configuration ------------------------------------------------
v3.0.3,"If your documentation needs a minimal Sphinx version, state it here."
v3.0.3,
v3.0.3,needs_sphinx = '1.0'
v3.0.3,"Add any Sphinx extension module names here, as strings. They can be"
v3.0.3,extensions coming with Sphinx (named 'sphinx.ext.*') or your custom
v3.0.3,ones.
v3.0.3,Show base classes
v3.0.3,"Use ""variables"" section for Attributes instead of weird block things"
v3.0.3,mimicking the function style.
v3.0.3,"Add any paths that contain templates here, relative to this directory."
v3.0.3,The suffix(es) of source filenames.
v3.0.3,You can specify multiple suffix as a list of string:
v3.0.3,
v3.0.3,"source_suffix = ['.rst', '.md']"
v3.0.3,The master toctree document.
v3.0.3,General information about the project.
v3.0.3,"The version info for the project you're documenting, acts as replacement for"
v3.0.3,"|version| and |release|, also used in various other places throughout the"
v3.0.3,built documents.
v3.0.3,
v3.0.3,The short X.Y version.
v3.0.3,"The full version, including alpha/beta/rc tags."
v3.0.3,The language for content autogenerated by Sphinx. Refer to documentation
v3.0.3,for a list of supported languages.
v3.0.3,
v3.0.3,This is also used if you do content translation via gettext catalogs.
v3.0.3,"Usually you set ""language"" from the command line for these cases."
v3.0.3,"List of patterns, relative to source directory, that match files and"
v3.0.3,directories to ignore when looking for source files.
v3.0.3,This patterns also effect to html_static_path and html_extra_path
v3.0.3,The name of the Pygments (syntax highlighting) style to use.
v3.0.3,"If true, `todo` and `todoList` produce output, else they produce nothing."
v3.0.3,-- Options for HTML output ----------------------------------------------
v3.0.3,The theme to use for HTML and HTML Help pages.  See the documentation for
v3.0.3,a list of builtin themes.
v3.0.3,
v3.0.3,html_theme = 'sphinx_materialdesign_theme'
v3.0.3,html_theme_path = [sphinx_materialdesign_theme.get_path()]
v3.0.3,Theme options are theme-specific and customize the look and feel of a theme
v3.0.3,"further.  For a list of options available for each theme, see the"
v3.0.3,documentation.
v3.0.3,
v3.0.3,html_theme_options = {}
v3.0.3,"Add any paths that contain custom static files (such as style sheets) here,"
v3.0.3,"relative to this directory. They are copied after the builtin static files,"
v3.0.3,"so a file named ""default.css"" will overwrite the builtin ""default.css""."
v3.0.3,"Custom sidebar templates, must be a dictionary that maps document names"
v3.0.3,to template names.
v3.0.3,
v3.0.3,This is required for the alabaster theme
v3.0.3,refs: http://alabaster.readthedocs.io/en/latest/installation.html#sidebars
v3.0.3,-- Options for HTMLHelp output ------------------------------------------
v3.0.3,Output file base name for HTML help builder.
v3.0.3,-- Options for LaTeX output ---------------------------------------------
v3.0.3,The paper size ('letterpaper' or 'a4paper').
v3.0.3,
v3.0.3,"'papersize': 'letterpaper',"
v3.0.3,"The font size ('10pt', '11pt' or '12pt')."
v3.0.3,
v3.0.3,"'pointsize': '10pt',"
v3.0.3,Additional stuff for the LaTeX preamble.
v3.0.3,
v3.0.3,"'preamble': '',"
v3.0.3,Latex figure (float) alignment
v3.0.3,
v3.0.3,"'figure_align': 'htbp',"
v3.0.3,Grouping the document tree into LaTeX files. List of tuples
v3.0.3,"(source start file, target name, title,"
v3.0.3,"author, documentclass [howto, manual, or own class])."
v3.0.3,-- Options for manual page output ---------------------------------------
v3.0.3,One entry per manual page. List of tuples
v3.0.3,"(source start file, name, description, authors, manual section)."
v3.0.3,-- Options for Texinfo output -------------------------------------------
v3.0.3,Grouping the document tree into Texinfo files. List of tuples
v3.0.3,"(source start file, target name, title, author,"
v3.0.3,"dir menu entry, description, category)"
v3.0.3,!/usr/bin/env python
v3.0.3,-*- coding: utf-8 -*-
v3.0.3,is this reachable?
v3.0.3,Read in embeddings
v3.0.3,Write to file
v3.0.3,converts a SentencePiece vocabulary to the format expected by dynamic data
v3.0.3,"(essentially converts float expected counts to ""fixed precision"" int pseudo"
v3.0.3,counts)
v3.0.3,"Add in default model arguments, possibly added since training."
v3.0.3,this patch is no longer needed included in converter
v3.0.3,"if hasattr(model_opt, 'rnn_size'):"
v3.0.3,model_opt.hidden_size = model_opt.rnn_size
v3.0.3,build_base_model expects updated and validated opts
v3.0.3,-*- encoding: utf-8 -*-
v3.0.3,!/usr/bin/env python
v3.0.3,-*- coding: utf-8 -*-
v3.0.3,Author: Rico Sennrich
v3.0.3,flake8: noqa
v3.0.3,This file is retrieved from https://github.com/rsennrich/subword-nmt
v3.0.3,hack for python2/3 compatibility
v3.0.3,check version information
v3.0.3,some hacking to deal with duplicates (only consider first instance)
v3.0.3,don't print end-of-word symbols
v3.0.3,sys.stderr.write('cannot split {0} further.\n'.format(segment))
v3.0.3,sys.stderr.write('OOV: {0}\n'.format(segment))
v3.0.3,sys.stderr.write('OOV: {0}\n'.format(segment))
v3.0.3,python 2/3 compatibility
v3.0.3,read/write files as UTF-8
v3.0.3,!/usr/bin/env python3
v3.0.3,coding: utf-8
v3.0.3,"In order to use this tool, please install comet first"
v3.0.3,https://github.com/Unbabel/COMET
v3.0.3,Let's say you have a source file with N sentences in SL - eg: source.sl
v3.0.3,and the corresponding references (N sentences) reference.tl
v3.0.3,Translate your file in TL with the -n_best nbest options nbest being
v3.0.3,then number of hypotheses and output the target to -output target.nbest.tl
v3.0.3,Then you need to duplicate source and reference sentences nbest times
v3.0.3,for this script.
v3.0.3,for instance using awk '{for(i=1; i<=n; i++) print}' n=5 reference.tl \
v3.0.3,> reference.5.tl
v3.0.3,same for source.
v3.0.3,This script can be run (for instance with nbest = 5) as follows:
v3.0.3,python oracle_bleu.py --nbest-src source.5.sl --nbest-hyp target.5.tl \
v3.0.3,--nbest-ref reference.5.tl --nbest-order 5 --output target.maxbleu.tl
v3.0.3,It will search in all hyp the best comet score
v3.0.3,when choosing a reference-less model no nbest-ref is required
v3.0.3,for nbest in nbests:
v3.0.3,!/usr/bin/env python
v3.0.3,!/usr/bin/env python3
v3.0.3,coding: utf-8
v3.0.3,Let's say you have a source file with N sentences in SL - eg: source.sl
v3.0.3,Translate your file in TL with the -n_best nbest options nbest being
v3.0.3,then number of hypotheses and output the target to -output target.nbest.tl
v3.0.3,This script can be run (for instance with nbest = 5) as follows:
v3.0.3,python mbr_bleu.py --nbest-hyp target.5.tl \
v3.0.3,--nbest-order 5 --output target.mbr.tl
v3.0.3,It will compare all hyp with eachother and output the max bleu
v3.0.3,!/usr/bin/env python
v3.0.3,-*- coding: utf-8 -*-
v3.0.3,Author: Rico Sennrich
v3.0.3,flake8: noqa
v3.0.3,This file is retrieved from https://github.com/rsennrich/subword-nmt
v3.0.3,hack for python2/3 compatibility
v3.0.3,"find all instances of pair, and update frequency/indices around it"
v3.0.3,find first symbol
v3.0.3,"if first symbol is followed by second symbol, we've found an occurrence of pair (old_word[i:i+2])"
v3.0.3,"assuming a symbol sequence ""A B C"", if ""B C"" is merged, reduce the frequency of ""A B"""
v3.0.3,"assuming a symbol sequence ""A B C B"", if ""B C"" is merged, reduce the frequency of ""C B""."
v3.0.3,"however, skip this if the sequence is A B C B C, because the frequency of ""C B"" will be reduced by the previous code block"
v3.0.3,find new pair
v3.0.3,"assuming a symbol sequence ""A BC D"", if ""B C"" is merged, increase the frequency of ""A BC"""
v3.0.3,"assuming a symbol sequence ""A BC B"", if ""B C"" is merged, increase the frequency of ""BC B"""
v3.0.3,"however, if the sequence is A BC BC, skip this step because the count of ""BC BC"" will be incremented by the previous code block"
v3.0.3,data structure of pair frequencies
v3.0.3,index from pairs to words
v3.0.3,version 0.2 changes the handling of the end-of-word token ('</w>');
v3.0.3,version numbering allows bckward compatibility
v3.0.3,"threshold is inspired by Zipfian assumption, but should only affect speed"
v3.0.3,we probably missed the best pair because of pruning; go back to full statistics
v3.0.3,"threshold is inspired by Zipfian assumption, but should only affect speed"
v3.0.3,python 2/3 compatibility
v3.0.3,read/write files as UTF-8
v3.0.3,Now we can pipe the full file through the model using the Iterator
v3.0.3,reminder a batch includes .src .tgt .indices and it is sorted
v3.0.3,Compute and retrieve the loss for EACH sentence
v3.0.3,loss is returned normalized by tokens
v3.0.3,we unnormalize to cumulate at doc level
v3.0.3,Now we need to rearrange the batch of ppl
v3.0.3,in the original order with indices
v3.0.3,!/usr/bin/env python
v3.0.3,-*- coding: utf-8 -*-
v3.0.3,!/usr/bin/env python
v3.0.3,!/usr/bin/env python3
v3.0.3,coding: utf-8
v3.0.3,Let's say you have a source file with N sentences in SL - eg: source.sl
v3.0.3,and the corresponding references (N sentences) reference.tl
v3.0.3,Translate your file in TL with the -n_best nbest options nbest being
v3.0.3,then number of hypotheses and output the target to -output target.nbest.tl
v3.0.3,Then you need to duplicate reference sentences nbest times for this script.
v3.0.3,for instance using awk '{for(i=1; i<=n; i++) print}' n=5 reference.tl \
v3.0.3,> reference.5.tl
v3.0.3,This script can be run (for instance with nbest = 5) as follows:
v3.0.3,python oracle_bleu.py --nbest-hyp target.5.tl --nbest-ref reference.5.tl \
v3.0.3,--nbest-order 5 --output target.maxbleu.tl
v3.0.3,It will search in all hyp the best bleu wrt reference
v3.0.3,and output the max bleu
v3.0.3,!/usr/bin/env python
v3.0.3,with the two module = imp.load_source() below
v3.0.3,we ghost the old torchtext.data.field and depercated
v3.0.3,onmt.inputters.text_dataset
v3.0.3,however this require some functions / classes to be
v3.0.3,monkey patched for loading the old field/vocab objects.
v3.0.3,"module3 = imp.load_source(""Vocab"", ""tools/convertv2_v3.py"")"
v3.0.3,"voc = dict(sorted(fields.vocab.__dict__['freqs'].items(),"
v3.0.3,"key=lambda x: (-x[1], x[0]))).keys()"
v3.0.3,"voc = dict(sorted(fields.vocab.__dict__['freqs'].items(),"
v3.0.3,"key=lambda x: (-x[1], x[0]))).keys()"
v3.0.3,"this patch is no longer needed, included in converter"
v3.0.3,"if hasattr(model_opt, 'rnn_size'):"
v3.0.3,model_opt.hidden_size = model_opt.rnn_size
v3.0.3,Avoid functionality on inference
v3.0.3,Build embeddings.
v3.0.3,Build encoder.
v3.0.3,Build embeddings.
v3.0.3,Build decoder.
v3.0.3,Share the embedding matrix - preprocess with share_vocab required.
v3.0.3,src/tgt vocab should be the same if `-share_vocab` is specified.
v3.0.3,Update vocabulary embeddings with checkpoint embeddings
v3.0.3,Embedding layers
v3.0.3,Just for debugging purposes
v3.0.3,Remove old vocabulary associated embeddings
v3.0.3,for back compat when attention_dropout was not defined
v3.0.3,Build Model
v3.0.3,Build Generator.
v3.0.3,Load the model states from checkpoint or initialize them.
v3.0.3,This preserves backward-compat for models using customed layernorm
v3.0.3,end of patch for backward compatibility
v3.0.3,Update model embeddings with those from the checkpoint
v3.0.3,after initialization
v3.0.3,!/usr/bin/env python
v3.0.3,"maybe prepare pretrained embeddings, if any"
v3.0.3,Load checkpoint if we resume from a previous training.
v3.0.3,ensure tensorboard output is written in the directory
v3.0.3,of previous checkpoints
v3.0.3,Override checkpoint's update_embeddings as it defaults to false
v3.0.3,Override checkpoint's freezing settings as it defaults to false
v3.0.3,NOTE: It's important that ``opt`` has been validated and updated
v3.0.3,at this point.
v3.0.3,Build model.
v3.0.3,Build optimizer.
v3.0.3,Build model saver
v3.0.3,Use Tensorboard for visualization during training
v3.0.3,Options only during inference
v3.0.3,"Truncation options, for text corpus"
v3.0.3,"as for False, this will be added in _add_train_general_opts"
v3.0.3,Embedding Options
v3.0.3,Model Task Options
v3.0.3,Encoder-Decoder Options
v3.0.3,Freeze Encoder and/or Decoder
v3.0.3,The following options (bridge_extra_node to n_steps) are used
v3.0.3,for training with --encoder_type ggnn (Gated Graph Neural Network).
v3.0.3,Attention options
v3.0.3,Alignement options
v3.0.3,Generator and loss options.
v3.0.3,GPU
v3.0.3,Init options
v3.0.3,Pretrained word vectors
v3.0.3,Freeze word vectors
v3.0.3,Optimization options
v3.0.3,learning rate
v3.0.3,options relate to data preprare
v3.0.3,options relate to train
v3.0.3,Alpha and Beta values for Google Length + Coverage penalty
v3.0.3,"Described here: https://arxiv.org/pdf/1609.08144.pdf, Section 7"
v3.0.3,Length penalty options
v3.0.3,Coverage penalty options
v3.0.3,Decoding Length constraint
v3.0.3,Decoding content constraint
v3.0.3,Adding options relate to decoding strategy
v3.0.3,Adding option for logging
v3.0.3,Adding options related to Transforms
v3.0.3,Copyright 2016 The Chromium Authors. All rights reserved.
v3.0.3,Use of this source code is governed by a BSD-style license that can be
v3.0.3,found in the LICENSE file.
v3.0.3,"Get the key 'value' in the dict, or just use 'value'"
v3.0.3,Basic attributes.
v3.0.3,Set model in training mode.
v3.0.3,Let's clean the GPUs before training loop
v3.0.3,UPDATE DROPOUT
v3.0.3,Run patience mechanism
v3.0.3,"If the patience has reached the limit, stop training"
v3.0.3,swap model params w/ moving average
v3.0.3,(and keep the original parameters)
v3.0.3,Set model in validating mode.
v3.0.3,F-prop through the model.
v3.0.3,Compute loss.
v3.0.3,Compute validation metrics (at batch.dataset level)
v3.0.3,Compute stats
v3.0.3,Update statistics.
v3.0.3,Set model back to training mode.
v3.0.3,Truncated BPTT: reminder not compatible with accum > 1
v3.0.3,1. Create truncated target.
v3.0.3,2. F-prop all but generator.
v3.0.3,3. Compute loss.
v3.0.3,Compute and save stats
v3.0.3,in theory we should divide by accum_count and bptt
v3.0.3,to rescale for each sub batch
v3.0.3,4. Update the parameters and statistics.
v3.0.3,Multi GPU gradient gather
v3.0.3,"If truncated, don't backprop fully."
v3.0.3,"in case of multi step gradient accumulation,"
v3.0.3,update only after accum batches
v3.0.3,For Flake
v3.0.3,we avoid padding while mean pooling
v3.0.3,incoming and outgoing edge embedding
v3.0.3,Find vocab data for tree builting
v3.0.3,Propogation Model
v3.0.3,Initialize the bridge layer
v3.0.3,Token embedding
v3.0.3,Initialize graph using formatted input sequence
v3.0.3,Number of flagged nodes defines node count for this sample
v3.0.3,"(Nodes can have no flags on them, but must be in 'flags' list)."
v3.0.3,The total number of integers in the vocab should allow
v3.0.3,for all features and edges to be defined.
v3.0.3,Use first extra node as only source for decoder init
v3.0.3,Average all nodes to get bridge input
v3.0.3,"LSTM has hidden and cell state, other only one"
v3.0.3,Total number of states
v3.0.3,Build a linear layer for each
v3.0.3,Initialize the bridge layer
v3.0.3,src lengths data is wrapped inside a Tensor.
v3.0.3,"LSTM has hidden and cell state, other only one"
v3.0.3,Total number of states
v3.0.3,Build a linear layer for each
v3.0.3,batch x len x dim
v3.0.3,mask is now (batch x 1 x slen x slen)
v3.0.3,1 to be expanded to number of heads in MHA
v3.0.3,Run the forward pass of every layer of the tranformer.
v3.0.3,Dimensions and padding for constructing the word embedding matrix
v3.0.3,Dimensions and padding for feature embedding matrices
v3.0.3,(these have no effect if feat_vocab_sizes is empty)
v3.0.3,The embedding matrix look-up tables. The first look-up table
v3.0.3,"is for words. Subsequent ones are for features, if any exist."
v3.0.3,The final output size of word + feature vectors. This can vary
v3.0.3,from the word vector size if and only if features are defined.
v3.0.3,This is the attribute you should access if you need to know
v3.0.3,how big your embeddings are going to be.
v3.0.3,The sequence of operations that converts the input sequence
v3.0.3,into a sequence of embeddings. At minimum this consists of
v3.0.3,looking up the embeddings for each word and feature in the
v3.0.3,input. Model parameters may require the sequence to contain
v3.0.3,additional operations as well.
v3.0.3,features must use word_vec_size
v3.0.3,features will use feat_vec_size
v3.0.3,Some utilitary functions for pretrained embeddings
v3.0.3,is this reachable?
v3.0.3,Write to file
v3.0.3,set the opt in place
v3.0.3,set the opt in place
v3.0.3,flake8: noqa
v3.0.3,For command-line option parsing
v3.0.3,"Check pass, set the args."
v3.0.3,"This SRU version implements its own cuda-level optimization,"
v3.0.3,so it requires that:
v3.0.3,1. `cupy` and `pynvrtc` python package installed.
v3.0.3,2. pytorch is built with cuda support.
v3.0.3,3. library path set: export LD_LIBRARY_PATH=<cuda lib path>.
v3.0.3,Check 1.
v3.0.3,Check 2.
v3.0.3,Check 3.
v3.0.3,This sets up device to use.
v3.0.3,-> directions x batch x dim
v3.0.3,For DEBUG
v3.0.3,"size = (length, batch, x.size(-1)) \"
v3.0.3,"if x.dim() == 3 else (batch, x.size(-1))"
v3.0.3,grad_x = x.new(*x.size()) if k_ == 3 else x.new(*size).zero_()
v3.0.3,Normal use
v3.0.3,"An entry check here, will catch on train side and translate side"
v3.0.3,if requirements are not satisfied.
v3.0.3,RNNDecoderState wraps hidden as a tuple.
v3.0.3,fh -> (layers*directions) x batch x dim
v3.0.3,This class is mainly used by decoder.py for RNNs but also
v3.0.3,by the CNN / transformer decoder when copy attention is used
v3.0.3,CNN has its own attention mechanism ConvMultiStepAttention
v3.0.3,Transformer has its own MultiHeadedAttention
v3.0.3,mlp wants it with bias
v3.0.3,"(batch, t_len, d) x (batch, d, s_len) --> (batch, t_len, s_len)"
v3.0.3,"(batch, t_len, s_len, d)"
v3.0.3,one step input
v3.0.3,"compute attention scores, as in Luong et al."
v3.0.3,Softmax or sparsemax to normalize attention weights
v3.0.3,each context vector c_t is the weighted average
v3.0.3,over all the source hidden states
v3.0.3,concatenate
v3.0.3,clamping necessary because of numerical errors: loss should be lower
v3.0.3,"bounded by zero, but negative values near zero are possible without"
v3.0.3,the clamp
v3.0.3,Shift values to be >= 0
v3.0.3,class MultiHeadedAttention(torch.jit.ScriptModule):
v3.0.3,https://arxiv.org/pdf/1803.02155.pdf
v3.0.3,in the paper they suggest either two embeds
v3.0.3,relative_key / relative_value or only
v3.0.3,relative_key. We implemented the same embed
v3.0.3,for both.
v3.0.3,@torch.jit.script_method
v3.0.3,"1) Project key, value, and query."
v3.0.3,as a reminder at training layer_cache[0] remains False
v3.0.3,2) Calculate and scale scores.
v3.0.3,batch x num_heads x query_len x key_len
v3.0.3,1 or key_len x key_len
v3.0.3,1 or key_len x key_len x dim_per_head
v3.0.3,not 100% necessary but expand to nb of heads
v3.0.3,now mask and scores have the same shape
v3.0.3,3) Apply attention dropout and compute context vectors.
v3.0.3,We use the same embeddings for key and value
v3.0.3,At the moment this class is only used by embeddings.Embeddings look-up tables
v3.0.3,-*- coding: utf-8 -*-
v3.0.3,class AverageAttention(torch.jit.ScriptModule):
v3.0.3,@torch.jit.script
v3.0.3,out_features * in_features
v3.0.3,norm is out_features * 1
v3.0.3,batch_size * out_features
v3.0.3,out_features
v3.0.3,out_features
v3.0.3,batch_size * out_features
v3.0.3,"out_channels, in_channels // groups, * kernel_size"
v3.0.3,out_features
v3.0.3,This is used nowhere in the code at the moment (Vincent Nguyen 05/18/2018)
v3.0.3,"in_channels, out_channels, *kernel_size"
v3.0.3,"in_channels, out_channels, *kernel_size"
v3.0.3,"self.out_channels, 1"
v3.0.3,out_features
v3.0.3,out_features
v3.0.3,store roots on diagonal
v3.0.3,Original probabilities.
v3.0.3,Probability of copying p(z=1) batch.
v3.0.3,Probability of not copying: p_{word}(w) * (1 - p(z))
v3.0.3,probabilities assigned by the model to the gold targets
v3.0.3,probability of tokens copied from source
v3.0.3,Set scores for unk to 0 and add eps
v3.0.3,find the indices in which you do not use the copy mechanism
v3.0.3,Drop padding.
v3.0.3,Do nothing
v3.0.3,Do nothing
v3.0.3,Punctuation only
v3.0.3,Auto import python files in this directory
v3.0.3,1. sample number of tokens to corrupt
v3.0.3,2. sample positions to corrput
v3.0.3,3. sample corrupted values
v3.0.3,1. sample number of tokens to corrupt
v3.0.3,2. sample positions to corrput
v3.0.3,3. Drop token on chosen position
v3.0.3,1. sample number of tokens to corrupt
v3.0.3,2. sample positions to corrput
v3.0.3,3. mask word on chosen position
v3.0.3,"Sharing options among `TokenizerTransform`s, same name conflict in"
v3.0.3,this scope will be resolved by remove previous occurrence in parser
v3.0.3,subword regularization(or BPE dropout) options:
v3.0.3,subword vocabulary restriction options:
v3.0.3,derterministic subwording
v3.0.3,subword sampling when nbest_size > 1 or -1
v3.0.3,alpha should be 0.0 < alpha < 1.0
v3.0.3,Load vocabulary file if provided and set threshold
v3.0.3,Load Subword Model
v3.0.3,-1: keep everything (i.e. 1 mask per token)
v3.0.3,0: replace everything (i.e. no mask)
v3.0.3,1: 1 mask per span
v3.0.3,view each subword as word start / input is word level token
v3.0.3,Pretend it ends with a full stop so last span is a sentence
v3.0.3,"Tokens that are full stops, where the previous token is not"
v3.0.3,Make sure we have enough to mask
v3.0.3,Trim to masking budget
v3.0.3,Handle 0-length mask (inserts) separately
v3.0.3,assert is_word_start[-1] == 0
v3.0.3,assert tokens_length - 1 not in indices
v3.0.3,"keep index, but replace it with [MASK]"
v3.0.3,"acts as a long length, so spans don't go over the end of doc"
v3.0.3,next position from each word_start
v3.0.3,delete token: 1 mask/remove per span
v3.0.3,"keep index, but replace it with [MASK]: 1 mask per token"
v3.0.3,A bit faster when all lengths are 1
v3.0.3,to cover whole token
v3.0.3,delete token
v3.0.3,"keep index, but replace it with [MASK]"
v3.0.3,assert tokens_length - 1 not in indices
v3.0.3,batch 0 will always predict EOS. The other batches will predict
v3.0.3,non-eos scores.
v3.0.3,"""best"" prediction is eos - that should be blocked"
v3.0.3,include at least one prediction OTHER than EOS
v3.0.3,that is greater than -1e20
v3.0.3,now batch 0 has ended and no others have
v3.0.3,initial step
v3.0.3,batch 0 dies on step 0
v3.0.3,include at least one prediction OTHER than EOS
v3.0.3,that is greater than -1e20
v3.0.3,step 2
v3.0.3,(old) batch 8 dies on step 1
v3.0.3,step 3
v3.0.3,everything dies
v3.0.3,initial step
v3.0.3,batch 0 dies on step 0
v3.0.3,include at least one prediction OTHER than EOS
v3.0.3,that is greater than -1e20
v3.0.3,step 2
v3.0.3,(old) batch 8 dies on step 1
v3.0.3,step 3
v3.0.3,everything dies
v3.0.3,initial step
v3.0.3,finish one beam
v3.0.3,include at least one prediction OTHER than EOS
v3.0.3,that is greater than -1e20
v3.0.3,step 2
v3.0.3,finish example in last batch
v3.0.3,(old) batch 8 dies on step 1
v3.0.3,step 3
v3.0.3,everything dies
v3.0.3,initial step
v3.0.3,batch 0 dies on step 0
v3.0.3,include at least one prediction OTHER than EOS
v3.0.3,that is greater than -1e20
v3.0.3,step 2
v3.0.3,(old) batch 8 dies on step 1
v3.0.3,step 3
v3.0.3,everything dies
v3.0.3,illegal_weights_mask = torch.ByteTensor([
v3.0.3,"[0, 0, 0, 0, 0, 0, 0],"
v3.0.3,"[0, 0, 0, 1, 1, 1, 1],"
v3.0.3,"[0, 0, 0, 0, 0, 1, 1],"
v3.0.3,"[0, 0, 1, 1, 1, 1, 1]])"
v3.0.3,TODO: fix for pytorch 0.3
v3.0.3,illegal_weights = alignments.masked_select(illegal_weights_mask)
v3.0.3,"self.assertEqual(0.0, illegal_weights.data.sum())"
v3.0.3,this could be considered an integration test because it touches
v3.0.3,the filesystem for the config file (and the models)
v3.0.3,no dummy prefix
v3.0.3,no dummy prefix
v3.0.3,make sure the scalars are in the event accumulator tags
v3.0.3,required arguments
v3.0.3,transforms that require vocab will not create if not provide vocab
v3.0.3,1. Init first transform in the pipe
v3.0.3,2. Init second transform in the pipe
v3.0.3,3. Sequential combine them into a transform pipe
v3.0.3,4. apply transform pipe for example
v3.0.3,"5. example after the pipe exceed the length limit, thus filtered"
v3.0.3,6. Transform statistics registed (here for filtertoolong)
v3.0.3,"7. after report, statistics become empty as a fresh start"
v3.0.3,filter_transform.warm_up()
v3.0.3,test BPE-dropout:
v3.0.3,1. disable bpe dropout for not training example
v3.0.3,2. enable bpe dropout for training example
v3.0.3,3. (NOTE) disable dropout won't take effect if already seen
v3.0.3,this is caused by the cache mechanism in bpe:
v3.0.3,return cached subword if the original token is seen when no dropout
v3.0.3,test SP regularization:
v3.0.3,1. enable regularization for training example
v3.0.3,2. disable regularization for not training example
v3.0.3,Not apply token drop for not training example
v3.0.3,apply token drop for training example
v3.0.3,Not apply token mask for not training example
v3.0.3,apply token mask for training example
v3.0.3,require vocabs to warm_up
v3.0.3,Not apply token mask for not training example
v3.0.3,apply token mask for training example
v3.0.3,"Defalt: full_stop_token=[""."", ""?"", ""!""]"
v3.0.3,"Defalt: full_stop_token=[""."", ""?"", ""!""]"
v3.0.3,random_ratio of inserted tokens are chosen in vocab
v3.0.3,others are MASK_TOK
v3.0.3,"insert_ratio=0.0,"
v3.0.3,"random_ratio=0.0,"
v3.0.3,"Defalt: full_stop_token=[""."", ""?"", ""!""]"
v3.0.3,all token are considered as an individual word
v3.0.3,1. tokens are dropped when replace_length is 0
v3.0.3,"print(f""token delete: {masked} / {tokens}"")"
v3.0.3,2. tokens are replaced by MASK when replace_length is 1
v3.0.3,"print(f""token mask: {masked} / {tokens}"")"
v3.0.3,"insert_ratio=0.0,"
v3.0.3,"random_ratio=0.0,"
v3.0.3,"Defalt: full_stop_token=[""."", ""?"", ""!""]"
v3.0.3,start token of word are identified using subword marker
v3.0.3,"1. replace_length 0: ""words"" are dropped"
v3.0.3,"print(f""word delete: {masked} / {tokens}"")"
v3.0.3,"self.assertEqual(len(masked), n_words - n_masked)"
v3.0.3,"2. replace_length 1: ""words"" are replaced with a single MASK"
v3.0.3,"print(f""whole word single mask: {masked} / {tokens}"")"
v3.0.3,len(masked) depend on number of tokens in select word
v3.0.3,"3. replace_length -1: all tokens in ""words"" are replaced with MASK"
v3.0.3,"print(f""whole word multi mask: {masked} / {tokens}"")"
v3.0.3,number of mask_tok depend on number of tokens in selected word
v3.0.3,number of MASK_TOK can be greater than n_masked
v3.0.3,"insert_ratio=0.5,"
v3.0.3,"random_ratio=0.3,"
v3.0.3,"Defalt: full_stop_token=[""."", ""?"", ""!""]"
v3.0.3,start token of word are identified using subword marker
v3.0.3,n_words = sum(token_starts)
v3.0.3,n_masked = math.ceil(n_words * bart_noise.mask_ratio)
v3.0.3,"print(f""Text Span Infilling: {infillied} / {tokens}"")"
v3.0.3,"print(n_words, n_masked)"
v3.0.3,!/usr/bin/env python
v3.0.3,-*- coding: utf-8 -*-
v3.0.3,Inject some dummy training options that may needed when build fields
v3.0.3,Remove the generated *pt files.
v3.0.3,Remove the generated data samples
v3.0.3,all beams repeat (beam >= 1 repeat dummy scores)
v3.0.3,predict repeat_idx over and over again
v3.0.3,"before repeat, scores are either 0 or -inf"
v3.0.3,"on repeat, `repeat_idx` score is BLOCKED_SCORE"
v3.0.3,"(but it's still the best score, thus we have"
v3.0.3,"[BLOCKED_SCORE, -inf, -inf, -inf, -inf]"
v3.0.3,repetitions keeps maximizing score
v3.0.3,"index 0 has been blocked, so repeating=>+0.0 score"
v3.0.3,other indexes are -inf so repeating=>BLOCKED_SCORE
v3.0.3,which is higher
v3.0.3,beam 0 and beam >=2 will repeat (beam >= 2 repeat dummy scores)
v3.0.3,non-interesting beams are going to get dummy values
v3.0.3,"on initial round, only predicted scores for beam 0"
v3.0.3,matter. Make two predictions. Top one will be repeated
v3.0.3,"in beam zero, second one will live on in beam 1."
v3.0.3,predict the same thing in beam 0
v3.0.3,continue pushing around what beam 1 predicts
v3.0.3,"now beam 0 dies (along with the others), beam 1 -> beam 0"
v3.0.3,"now beam 0 dies (along with the others), beam 1 -> beam 0"
v3.0.3,beam 0 and beam >= 2 will repeat (beam 2 repeats excluded idx)
v3.0.3,non-interesting beams are going to get dummy values
v3.0.3,predict the same thing in beam 0
v3.0.3,continue pushing around what beam 1 predicts
v3.0.3,predict the allowed-repeat again in beam 2
v3.0.3,"now beam 0 dies, beam 1 -> beam 0, beam 2 -> beam 1"
v3.0.3,and the rest die
v3.0.3,"since all preds after i=0 are 0, we can check"
v3.0.3,that the beam is the correct idx by checking that
v3.0.3,the curr score is the initial score
v3.0.3,beam 0 will always predict EOS. The other beams will predict
v3.0.3,non-eos scores.
v3.0.3,non-interesting beams are going to get dummy values
v3.0.3,"""best"" prediction is eos - that should be blocked"
v3.0.3,include at least beam_sz predictions OTHER than EOS
v3.0.3,that are greater than -1e20
v3.0.3,predict eos in beam 0
v3.0.3,provide beam_sz other good predictions
v3.0.3,now the top beam has ended and no others have
v3.0.3,"not of interest, but want to make sure it keeps running"
v3.0.3,since only beam 0 terminates and n_best = 2
v3.0.3,"this is also a test that when block_ngram_repeat=0,"
v3.0.3,repeating is acceptable
v3.0.3,non-interesting beams are going to get dummy values
v3.0.3,"""best"" prediction is eos - that should be blocked"
v3.0.3,include at least beam_sz predictions OTHER than EOS
v3.0.3,that are greater than -1e20
v3.0.3,predict eos in beam 1
v3.0.3,provide beam_sz other good predictions in other beams
v3.0.3,beam 1 dies on min_length
v3.0.3,beam 0 dies on the step after beam 1 dies
v3.0.3,"inp_lens is tiled in initialize, reassign to make attn match"
v3.0.3,non-interesting beams are going to get dummy values
v3.0.3,"""best"" prediction is eos - that should be blocked"
v3.0.3,include at least beam_sz predictions OTHER than EOS
v3.0.3,that are greater than -1e20
v3.0.3,predict eos in beam 1
v3.0.3,provide beam_sz other good predictions in other beams
v3.0.3,no top beams are finished yet
v3.0.3,beam 1 dies on min_length
v3.0.3,no top beams are finished yet
v3.0.3,beam 0 dies on the step after beam 1 dies
v3.0.3,top beam is finished now so there are attentions
v3.0.3,two beams are finished in each batch
v3.0.3,second dim is cut down to the non-padded src length
v3.0.3,first dim is equal to the time of death
v3.0.3,(beam 0 died at current step - adjust for SOS)
v3.0.3,(beam 1 died at last step - adjust for SOS)
v3.0.3,behavior gets weird when beam is already done so just stop
v3.0.3,this is just test_beam.TestBeamAgainstReferenceCase repeated
v3.0.3,in each batch.
v3.0.3,"init_preds: [4, 3, 5, 6, 7] - no EOS's"
v3.0.3,no EOS's yet
v3.0.3,"[5, 3, 2, 6, 0], so beam 2 predicts EOS!"
v3.0.3,assumes beam 2 finished on last step
v3.0.3,ended beam 2 shouldn't continue
v3.0.3,"[2, 5, 3, 6, 0] repeat self.BATCH_SZ, so beam 0 predicts EOS!"
v3.0.3,"[-2.4879, -3.8910, -4.1010, -4.2010, -4.4010] repeat self.BATCH_SZ"
v3.0.3,another beam is finished in all batches
v3.0.3,new beam 0 finished
v3.0.3,new beam 0 is old beam 3
v3.0.3,assumes beam 0 finished on last step
v3.0.3,"[5, 2, 6, 1, 0] repeat self.BATCH_SZ, so beam 1 predicts EOS!"
v3.0.3,we finish 3 hyps per example in this step
v3.0.3,new beam 1 is old beam 3
v3.0.3,this could be considered an integration test because it tests
v3.0.3,interactions between the GNMT scorer and the beam
v3.0.3,"-data option is required, but not used in this test, so dummy."
v3.0.3,len x batch x nfeat
v3.0.3,Initialize vectors to compare size with
v3.0.3,Ensure correct sizes and types
v3.0.3,Make sure that output has the correct size and type
v3.0.3,"[('encoder_type', 'transformer'),"
v3.0.3,"('word_vec_size', 16), ('hidden_size', 16)],"
v3.0.3,""""""" Only do SRU test if requirment is safisfied. """""""
v3.0.3,SRU doesn't support input_feed.
v3.0.3,first check there's nothing unexpectedly not trainable
v3.0.3,ok: word embeddings shouldn't be trainable
v3.0.3,if word vecs are freezed
v3.0.3,ok: positional encodings shouldn't be trainable
v3.0.3,then check nothing unexpectedly trainable
v3.0.3,Decoder state
v3.0.3,Build the RNN.
v3.0.3,Set up the context gate.
v3.0.3,Set up the standard attention.
v3.0.3,The encoder hidden is  (layers*directions) x batch x dim.
v3.0.3,We need to convert it to layers x batch x (directions*dim).
v3.0.3,Init the input feed.
v3.0.3,Update the state with the result.
v3.0.3,Concatenates sequence of tensors along a new dimension.
v3.0.3,NOTE: v0.3 to 0.4: dec_outs / attns[*] may not be list
v3.0.3,(in particular in case of SRU) it was not raising error in 0.3
v3.0.3,since stack(Variable) was allowed.
v3.0.3,"In 0.4, SRU returns a tensor that shouldn't be stacke"
v3.0.3,Calculate the attention.
v3.0.3,Calculate the context gate.
v3.0.3,Additional args check.
v3.0.3,Input feed concatenates hidden state with
v3.0.3,input at every time step.
v3.0.3,TODO: context gate should be employed
v3.0.3,instead of second RNN transform.
v3.0.3,Update the coverage attention.
v3.0.3,"attns[""coverage""] is actually c^(t+1) of See et al(2017)"
v3.0.3,1-index shifted
v3.0.3,Decoder State
v3.0.3,CNNDecoder has its own attention mechanism.
v3.0.3,Set up a separate copy attention layer if needed.
v3.0.3,The output of CNNEncoder.
v3.0.3,The combination of output of CNNEncoder and source embeddings.
v3.0.3,Process the result and update the attentions.
v3.0.3,Update the state.
v3.0.3,TODO change the way attns is returned dict => list or tuple (onnx)
v3.0.3,src_len is a single tensor shared between all models.
v3.0.3,This assumption will not hold if Translator is modified
v3.0.3,to calculate src_len as something other than the length
v3.0.3,of the input.
v3.0.3,"return _, (B, Q_len, K_len)"
v3.0.3,"layer average attention across heads, get ``(B, Q, K)``"
v3.0.3,"Case 1: no full_context, no align heads -> layer avg baseline"
v3.0.3,"Case 2: no full_context, 1 align heads -> guided align"
v3.0.3,"Case 3: full_context, 1 align heads -> full cte guided align"
v3.0.3,BoolTensor was introduced in pytorch 1.2
v3.0.3,T: could be 1 in the case of stepwise decoding or tgt_len
v3.0.3,masking is necessary when sequence length is greater than one
v3.0.3,mask now are (batch x 1 x tlen x s or t len)
v3.0.3,1 = heads to be expanded in MHA
v3.0.3,Decoder State
v3.0.3,"previously, there was a GlobalAttention module here for copy"
v3.0.3,"attention. But it was never actually used -- the ""copy"" attention"
v3.0.3,just reuses the context attention.
v3.0.3,"attns[""align""] = torch.stack(attn_aligns, 0).mean(0)  # All avg"
v3.0.3,TODO change the way attns is returned dict => list or tuple (onnx)
v3.0.3,first value set to True triggered by the beginning of decoding
v3.0.3,layer_cache becomes active in the MultiHeadedAttention fwd
v3.0.3,T: could be 1 in the case of stepwise decoding or tgt_len
v3.0.3,masking is necessary when sequence length is greater than one
v3.0.3,mask now are (batch x 1 x tlen x tlen)
v3.0.3,1 = heads to be expanded in MHA
v3.0.3,TODO change the way attns is returned dict => list or tuple (onnx)
v3.0.3,"buffer size in bytes, determine equiv. # of elements based on data type"
v3.0.3,copy tensors into buffer_t
v3.0.3,all-reduce and rescale
v3.0.3,copy all-reduced buffer back into tensors
v3.0.3,"print(filled, sz)"
v3.0.3,"tensor is bigger than buffer, all-reduce and rescale directly"
v3.0.3,"buffer is full, all-reduce and replace buffer with grad"
v3.0.3,add tensor to buffer
v3.0.3,"propagate exception to parent process, keeping original traceback"
v3.0.3,TODO: Find a better way to check for sparse gradients.
v3.0.3,we use apex.amp
v3.0.3,In this case use the old FusedAdam with
v3.0.3,FP16_optimizer wrapper
v3.0.3,Load everything from the checkpoint.
v3.0.3,Build everything from scratch.
v3.0.3,"Reset optimizer, keep options."
v3.0.3,"Reset options, keep optimizer."
v3.0.3,State can be partially restored.
v3.0.3,should be: self._optimizer.zero_grad(set_to_none)
v3.0.3,but apex.amp is not up-to-date:
v3.0.3,https://github.com/NVIDIA/apex/blob/master/apex/amp/_process_optimizer.py#L367
v3.0.3,"unscaled optimizer's gradients (already done therefore skip),"
v3.0.3,skips optimizer.step() if gradients contain infs/NaNs.
v3.0.3,Updates the scale for next iteration.
v3.0.3,Code below is an implementation of https://arxiv.org/pdf/1804.04235.pdf
v3.0.3,inspired but modified from https://github.com/DeadAt0m/adafactor-pytorch
v3.0.3,backward compatibility
v3.0.3,assuming a list/generator of parameter means single group
v3.0.3,compute combined scale factor for this group
v3.0.3,norm is in fact norm*scale
v3.0.3,note: p.grad should not ever be set for correct operation of
v3.0.3,mixed precision optimizer that sometimes sends None gradients
v3.0.3,State initialization
v3.0.3,Exponential moving average of gradient values
v3.0.3,Exponential moving average of squared gradient values
v3.0.3,-*- coding: utf-8 -*-
v3.0.3,"align_head contains value in [0, 1) presenting attn prob,"
v3.0.3,0 was resulted by the context attention src_pad_mask
v3.0.3,"So, the correspand position in ref_align should also be 0"
v3.0.3,"Therefore, clip align_head to > 1e-18 should be bias free."
v3.0.3,rescale with tau (temperature) and apply the log_softmax.
v3.0.3,ct2 expects src with lengths without padding
v3.0.3,again we use raw probs to rescale with tau and apply log_softmax
v3.0.3,lm_scores are in log space so log_target=True
v3.0.3,rescale with tau (temperature) and apply the log_softmax.
v3.0.3,ct2 expects src with lengths without padding
v3.0.3,again we use raw probs to rescale with tau and apply log_softmax
v3.0.3,lm_scores are in log space so log_target=True
v3.0.3,take into account here the tgt_shift_index (0 / 1 = LM/NMT)
v3.0.3,Correct target copy token instead of <unk>
v3.0.3,tgt[i] = align[i] + len(tgt_vocab)
v3.0.3,for i such that tgt[i] == 0 and align[i] != 0
v3.0.3,in the case criterion reduction is None then we need
v3.0.3,to sum the loss of each sentence in the batch
v3.0.3,Check Transforms
v3.0.3,Check path
v3.0.3,tgt is src for LM task
v3.0.3,Check prefix: will be used when use prefix transform
v3.0.3,Check weight
v3.0.3,Check features
v3.0.3,validation when train:
v3.0.3,Check embeddings stuff
v3.0.3,"Backward compatibility with ""fix_word_vecs_*"" opts"
v3.0.3,encoder and decoder should be same sizes
v3.0.3,"Load default opt values, then overwrite with the opts in"
v3.0.3,"the checkpoint. That way, if there are new options added,"
v3.0.3,the defaults are used.
v3.0.3,It comes from training
v3.0.3,TODO: needs to be added as inference opt
v3.0.3,Don't do anything
v3.0.3,Update best score of each criteria
v3.0.3,Reset tolerance
v3.0.3,Update current status
v3.0.3,Decrease tolerance
v3.0.3,Log
v3.0.3,Log
v3.0.3,Get a list of world_size lists with len(stat_list) Statistics objects
v3.0.3,"this param init is overridden by model_builder, useless then."
v3.0.3,SRU doesn't support PackedSequence.
v3.0.3,-*- coding: utf-8 -*-
v3.0.3,threshold on 1 to avoid div by 0
v3.0.3,treat alignment matrix one by one as each have different lengths
v3.0.3,No alignment if not exist valid tgt token
v3.0.3,get valid alignment (sub-matrix from full paded aligment matrix)
v3.0.3,Helper functions
v3.0.3,Keeps track of the original words/subwords
v3.0.3,('prior_tokenization' option)
v3.0.3,In case there is a final case_markup when new_spacer is on
v3.0.3,translate
v3.0.3,for validation we build an infer_iter per batch
v3.0.3,in order to avoid oom issues because there is no
v3.0.3,batching strategy in `textbatch_to_tensor`
v3.0.3,apply_reverse refs
v3.0.3,flatten preds
v3.0.3,save results
v3.0.3,We deactivate the decoder's cache
v3.0.3,as we use teacher forcing at training time.
v3.0.3,-*- coding: utf-8 -*-
v3.0.3,this one is needed for Random Shuffler of batches
v3.0.3,in multi gpu it ensures datasets are read in the same order
v3.0.3,some cudnn methods can be random even after fixing the seed
v3.0.3,unless you tell it to be deterministic
v3.0.3,This one is needed for various tranfroms
v3.0.3,These ensure same initialization in multi gpu mode
v3.0.3,we need to check the model path + any tokenizer path
v3.0.3,bucket_size = batch_size
v3.0.3,We only support
v3.0.3,For TRAIN we need to group examples by length
v3.0.3,"for faster performance, but otherwise, sequential."
v3.0.3,For TRAIN we shuffle batches within the bucket
v3.0.3,otherwise sequential
v3.0.3,for specific case of rnn_packed need to be sorted
v3.0.3,within the batch
v3.0.3,Maintains the longest src and tgt length in the current batch
v3.0.3,Reset current longest length at a new batch (count=1)
v3.0.3,Src: [<bos> w1 ... wN <eos>]
v3.0.3,Tgt: [w1 ... wM <eos>]
v3.0.3,Make features part of src like
v3.0.3,"{'src': {'src': ..., 'feat1': ...., 'feat2': ....}}"
v3.0.3,We apply the same TransformPipe to all the bucket
v3.0.3,at this point an example looks like:
v3.0.3,"{'src': {'src': ..., 'feat1': ...., 'feat2': ....},"
v3.0.3,"'tgt': {'tgt': ...},"
v3.0.3,"'src_original': ['tok1', ...'tokn'],"
v3.0.3,"'tgt_original': ['tok1', ...'tokm'],"
v3.0.3,'indices' : seq in bucket
v3.0.3,"'align': ...,"
v3.0.3,}
v3.0.3,we'll need to change this if we introduce tgt feat
v3.0.3,Need to add features in last dimensions
v3.0.3,Need to add features also in 'src'
v3.0.3,make a small vocab containing just the tokens in the source sequence
v3.0.3,Map source tokens to indices in the dynamic dict.
v3.0.3,-*- coding: utf-8 -*-
v3.0.3,'src_original' and 'tgt_original' store the
v3.0.3,original line before tokenization. These
v3.0.3,fields are used later on in the feature
v3.0.3,transforms.
v3.0.3,NOTE: moved to dynamic_iterator.py cf process()
v3.0.3,item = self.transform.apply(
v3.0.3,"example, is_train=self.infinitely, corpus_name=self.cid)"
v3.0.3,empty example: skip
v3.0.3,"No encoder in LM, seq2seq count formatting kept"
v3.0.3,_check_save_model_path
v3.0.3,NOTE: We need to trim the vocab to remove any unk tokens that
v3.0.3,were not originally here.
v3.0.3,"for side in [""src"", ""tgt""]:"
v3.0.3,keys_to_pop = []
v3.0.3,"if hasattr(vocab[side], ""fields""):"
v3.0.3,unk_token = vocab[side].fields[0][1].vocab.itos[0]
v3.0.3,"for key, value in vocab[side].fields[0][1].vocab.stoi.items():"
v3.0.3,if value == 0 and key != unk_token:
v3.0.3,keys_to_pop.append(key)
v3.0.3,for key in keys_to_pop:
v3.0.3,"vocab[side].fields[0][1].vocab.stoi.pop(key, None)"
v3.0.3,!/usr/bin/env python
v3.0.3,!/usr/bin/env python
v3.0.3,!/usr/bin/env python
v3.0.3,-*- coding: utf-8 -*-
v3.0.3,!/usr/bin/env python
v3.0.3,Just for debugging purposes
v3.0.3,It appends features to subwords when dumping to file
v3.0.3,!/usr/bin/env python
v3.0.3,!/usr/bin/env python
v3.0.3,Set sharing strategy manually instead of default based on the OS.
v3.0.3,torch.multiprocessing.set_sharing_strategy('file_system')
v3.0.3,Create a thread to listen for errors in the child processes.
v3.0.3,Train with multiprocessing.
v3.0.3,magic indices
v3.0.3,result caching
v3.0.3,fix length constraint and remove eos from count
v3.0.3,add one to account for BOS. Don't account for EOS because hitting
v3.0.3,this implies it hasn't been found.
v3.0.3,we don't block nothing if the user doesn't want it
v3.0.3,we can't block nothing beam's too short
v3.0.3,we check paths one by one
v3.0.3,we don't forbid nothing if the user doesn't want it
v3.0.3,we can't forbid nothing if beam's too short
v3.0.3,Reordering forbidden_tokens following beam selection
v3.0.3,We rebuild a dict to ensure we get the value and not the pointer
v3.0.3,Grabing the newly selected tokens and associated ngram
v3.0.3,skip the blocking if any token in current_ngram is excluded
v3.0.3,"pickups: Tensor where specified index were set to 1, others 0"
v3.0.3,"dropdowns: opposite of pickups, 1 for those shouldn't pick"
v3.0.3,Minus dropdowns to log_probs making probabilities of
v3.0.3,unspecified index close to 0
v3.0.3,"prediction step have surpass length of given target_prefix,"
v3.0.3,no need to further change this attr
v3.0.3,keep indices until overflowing p
v3.0.3,Set all logits that are not in the top-p to -10000.
v3.0.3,This puts the probabilities close to 0.
v3.0.3,Set all logits that are not in the top-k to -10000.
v3.0.3,This puts the probabilities close to 0.
v3.0.3,"For temp=0.0, take the argmax to avoid divide-by-zero errors."
v3.0.3,keep_topk=1 is also equivalent to argmax.
v3.0.3,maybe fix some prediction at this step by modifying log_probs
v3.0.3,"shape: (sum(~ self.is_finished), 1)"
v3.0.3,in LM task src_len is associated with currently generated src
v3.0.3,and therefore needs to follow the generation
v3.0.3,!/usr/bin/env python
v3.0.3,for debugging
v3.0.3,TODO: maybe add dynamic part
v3.0.3,Statistics
v3.0.3,In the case of length_penalty = none we report the total logprobs
v3.0.3,divided by the number of sentence to get an approximation of the
v3.0.3,per sentence logprob. We also return the corresponding ppl
v3.0.3,"When a length_penalty is used eg: ""avg"" or ""wu"" since logprobs"
v3.0.3,are normalized per token we report the per line per token logprob
v3.0.3,"and the corresponding ""per word perplexity"""
v3.0.3,Turn any copied words into UNKs.
v3.0.3,"Decoder forward, takes [batch, tgt_len, nfeats] as input"
v3.0.3,"and [batch, src_len, hidden] as enc_out"
v3.0.3,"in case of inference tgt_len = 1, batch = beam times batch_size"
v3.0.3,"in case of Gold Scoring tgt_len = actual length, batch = 1 batch"
v3.0.3,Generator forward.
v3.0.3,"returns [(batch_size x beam_size) , vocab ] when 1 step"
v3.0.3,"or [batch_size, tgt_len, vocab ] when full sentence"
v3.0.3,"here we have scores [tgt_lenxbatch, vocab] or [beamxbatch, vocab]"
v3.0.3,at this point scores is batch first (dim=0)
v3.0.3,"returns [(batch_size x beam_size) , vocab ] when 1 step"
v3.0.3,"or [batch_size, tgt_len, vocab ] when full sentence"
v3.0.3,(0) add BOS and padding to tgt prediction
v3.0.3,(1) Encoder forward.
v3.0.3,(2) Repeat src objects `n_best` times.
v3.0.3,"We use batch_size x n_best, get ``(batch * n_best, src_len, nfeat)``"
v3.0.3,"(3) Init decoder with n_best src,"
v3.0.3,"reshape tgt to ``(len, batch * n_best, nfeat)``"
v3.0.3,it should be done in a better way
v3.0.3,here dec_in is batch first
v3.0.3,masked_select
v3.0.3,get aligned src id for each prediction's valid tgt tokens
v3.0.3,TODO: support these blacklisted features
v3.0.3,(0) Prep the components of the search.
v3.0.3,(1) Run the encoder on the src.
v3.0.3,(2) prep decode_strategy. Possibly repeat src objects.
v3.0.3,(3) Begin decoding step by step:
v3.0.3,"decoder_input = decode_strategy.current_predictions.view(1, -1,"
v3.0.3,1)
v3.0.3,Reorder states.
v3.0.3,TODO: support these blacklisted features
v3.0.3,(0) Prep the components of the search.
v3.0.3,(1) split src into src and target_prefix to avoid padding.
v3.0.3,(2) init decoder
v3.0.3,(3) prep decode_strategy. Possibly repeat src objects.
v3.0.3,(4) Begin decoding step by step:
v3.0.3,Reorder states.
v3.0.3,select indexes in model state/cache
v3.0.3,beam parameters
v3.0.3,beam state
v3.0.3,BoolTensor was introduced in pytorch 1.2
v3.0.3,"""global state"" of the old beam"
v3.0.3,buffers for the topk scores and 'backpointer'
v3.0.3,for testing
v3.0.3,maybe fix some prediction at this step by modifying log_probs
v3.0.3,Flatten probs into a list of possibilities.
v3.0.3,Penalize beams that finished.
v3.0.3,"on real data (newstest2017) with the pretrained transformer,"
v3.0.3,it's faster to not move this back to the original device
v3.0.3,Store finished hypotheses for this batch.
v3.0.3,End condition is the top beam finished and we can return
v3.0.3,n_best hypotheses.
v3.0.3,"If all sentences are translated, no need to go further."
v3.0.3,Remove finished batches for the next step.
v3.0.3,using integer division to get an integer _B without casting
v3.0.3,force the output to be longer than self.min_length
v3.0.3,Multiply probs by the beam probability.
v3.0.3,"if the sequence ends now, then the penalty is the current"
v3.0.3,"length + 1, to include the EOS token"
v3.0.3,Avoid any direction that would repeat unwanted ngrams
v3.0.3,Pick up candidate token by curr_scores
v3.0.3,Recover log probs.
v3.0.3,Length penalty is just a scalar. It doesn't matter if it's applied
v3.0.3,before or after the topk.
v3.0.3,Resolve beam origin and map to batch index flat representation.
v3.0.3,Append last prediction.
v3.0.3,update global state (step == 1)
v3.0.3,update global state (step > 1)
v3.0.3,"shape: (batch_size x beam_size, 1)"
v3.0.3,in LM task src_len is associated with currently generated src
v3.0.3,and therefore needs to follow the generation
v3.0.3,in LM task src_len is associated with currently generated src
v3.0.3,and therefore needs to follow the generation
v3.0.3,Term will be subtracted from probability
v3.0.3,Probability will be divided by this
v3.0.3,these warnings indicate that either the alpha/beta
v3.0.3,"forces a penalty to be a no-op, or a penalty is a no-op but"
v3.0.3,the alpha/beta would suggest otherwise.
v3.0.3,using some coverage penalty
v3.0.3,!/usr/bin/env python
v3.0.3,semaphore doesn't have a timeout arg in Python 2.7
v3.0.3,perform a first request to initialize everything
v3.0.3,backwards compatibility for confs
v3.0.3,every segment becomes a dict for flexibility purposes
v3.0.3,NOTE: translator returns lists of `n_best` list
v3.0.3,build back results with empty texts
v3.0.3,load can be called multiple times: modify copy
v3.0.3,output contain alignment
v3.0.3,Below are all the different penalty terms implemented so far.
v3.0.3,Subtract coverage penalty from topk log probs.
v3.0.3,Divide topk log probs by length penalty.
v3.0.3,Sorting
v3.0.3,Chinese segmentation
v3.0.3,Chinese simplify -> Chinese traditional standard
v3.0.3,Chinese simplify -> Chinese traditional (HongKong)
v3.0.3,Chinese simplify -> Chinese traditional (Taiwan)
v3.0.3,Chinese traditional -> Chinese simplify (v1)
v3.0.3,Chinese traditional -> Chinese simplify (v2)
v3.0.3,Auto import python files in this directory
v3.0.2,!/usr/bin/env python
v3.0.2,!/usr/bin/env python
v3.0.2,!/usr/bin/env python
v3.0.2,!/usr/bin/env python
v3.0.2,!/usr/bin/env python
v3.0.2,!/usr/bin/env python3
v3.0.2,-*- coding: utf-8 -*-
v3.0.2,
v3.0.2,"OpenNMT-py documentation build configuration file, created by"
v3.0.2,sphinx-quickstart on Sun Dec 17 12:07:14 2017.
v3.0.2,
v3.0.2,This file is execfile()d with the current directory set to its
v3.0.2,containing dir.
v3.0.2,
v3.0.2,Note that not all possible configuration values are present in this
v3.0.2,autogenerated file.
v3.0.2,
v3.0.2,All configuration values have a default; values that are commented out
v3.0.2,serve to show the default.
v3.0.2,"If extensions (or modules to document with autodoc) are in another directory,"
v3.0.2,add these directories to sys.path here. If the directory is relative to the
v3.0.2,"documentation root, use os.path.abspath to make it absolute, like shown here."
v3.0.2,
v3.0.2,import os
v3.0.2,import sys
v3.0.2,"sys.path.insert(0, os.path.abspath('.'))"
v3.0.2,-- General configuration ------------------------------------------------
v3.0.2,"If your documentation needs a minimal Sphinx version, state it here."
v3.0.2,
v3.0.2,needs_sphinx = '1.0'
v3.0.2,"Add any Sphinx extension module names here, as strings. They can be"
v3.0.2,extensions coming with Sphinx (named 'sphinx.ext.*') or your custom
v3.0.2,ones.
v3.0.2,Show base classes
v3.0.2,"Use ""variables"" section for Attributes instead of weird block things"
v3.0.2,mimicking the function style.
v3.0.2,"Add any paths that contain templates here, relative to this directory."
v3.0.2,The suffix(es) of source filenames.
v3.0.2,You can specify multiple suffix as a list of string:
v3.0.2,
v3.0.2,"source_suffix = ['.rst', '.md']"
v3.0.2,The master toctree document.
v3.0.2,General information about the project.
v3.0.2,"The version info for the project you're documenting, acts as replacement for"
v3.0.2,"|version| and |release|, also used in various other places throughout the"
v3.0.2,built documents.
v3.0.2,
v3.0.2,The short X.Y version.
v3.0.2,"The full version, including alpha/beta/rc tags."
v3.0.2,The language for content autogenerated by Sphinx. Refer to documentation
v3.0.2,for a list of supported languages.
v3.0.2,
v3.0.2,This is also used if you do content translation via gettext catalogs.
v3.0.2,"Usually you set ""language"" from the command line for these cases."
v3.0.2,"List of patterns, relative to source directory, that match files and"
v3.0.2,directories to ignore when looking for source files.
v3.0.2,This patterns also effect to html_static_path and html_extra_path
v3.0.2,The name of the Pygments (syntax highlighting) style to use.
v3.0.2,"If true, `todo` and `todoList` produce output, else they produce nothing."
v3.0.2,-- Options for HTML output ----------------------------------------------
v3.0.2,The theme to use for HTML and HTML Help pages.  See the documentation for
v3.0.2,a list of builtin themes.
v3.0.2,
v3.0.2,html_theme = 'sphinx_materialdesign_theme'
v3.0.2,html_theme_path = [sphinx_materialdesign_theme.get_path()]
v3.0.2,Theme options are theme-specific and customize the look and feel of a theme
v3.0.2,"further.  For a list of options available for each theme, see the"
v3.0.2,documentation.
v3.0.2,
v3.0.2,html_theme_options = {}
v3.0.2,"Add any paths that contain custom static files (such as style sheets) here,"
v3.0.2,"relative to this directory. They are copied after the builtin static files,"
v3.0.2,"so a file named ""default.css"" will overwrite the builtin ""default.css""."
v3.0.2,"Custom sidebar templates, must be a dictionary that maps document names"
v3.0.2,to template names.
v3.0.2,
v3.0.2,This is required for the alabaster theme
v3.0.2,refs: http://alabaster.readthedocs.io/en/latest/installation.html#sidebars
v3.0.2,-- Options for HTMLHelp output ------------------------------------------
v3.0.2,Output file base name for HTML help builder.
v3.0.2,-- Options for LaTeX output ---------------------------------------------
v3.0.2,The paper size ('letterpaper' or 'a4paper').
v3.0.2,
v3.0.2,"'papersize': 'letterpaper',"
v3.0.2,"The font size ('10pt', '11pt' or '12pt')."
v3.0.2,
v3.0.2,"'pointsize': '10pt',"
v3.0.2,Additional stuff for the LaTeX preamble.
v3.0.2,
v3.0.2,"'preamble': '',"
v3.0.2,Latex figure (float) alignment
v3.0.2,
v3.0.2,"'figure_align': 'htbp',"
v3.0.2,Grouping the document tree into LaTeX files. List of tuples
v3.0.2,"(source start file, target name, title,"
v3.0.2,"author, documentclass [howto, manual, or own class])."
v3.0.2,-- Options for manual page output ---------------------------------------
v3.0.2,One entry per manual page. List of tuples
v3.0.2,"(source start file, name, description, authors, manual section)."
v3.0.2,-- Options for Texinfo output -------------------------------------------
v3.0.2,Grouping the document tree into Texinfo files. List of tuples
v3.0.2,"(source start file, target name, title, author,"
v3.0.2,"dir menu entry, description, category)"
v3.0.2,!/usr/bin/env python
v3.0.2,-*- coding: utf-8 -*-
v3.0.2,is this reachable?
v3.0.2,Read in embeddings
v3.0.2,Write to file
v3.0.2,converts a SentencePiece vocabulary to the format expected by dynamic data
v3.0.2,"(essentially converts float expected counts to ""fixed precision"" int pseudo"
v3.0.2,counts)
v3.0.2,"Add in default model arguments, possibly added since training."
v3.0.2,this patch is no longer needed included in converter
v3.0.2,"if hasattr(model_opt, 'rnn_size'):"
v3.0.2,model_opt.hidden_size = model_opt.rnn_size
v3.0.2,build_base_model expects updated and validated opts
v3.0.2,-*- encoding: utf-8 -*-
v3.0.2,!/usr/bin/env python
v3.0.2,-*- coding: utf-8 -*-
v3.0.2,Author: Rico Sennrich
v3.0.2,flake8: noqa
v3.0.2,This file is retrieved from https://github.com/rsennrich/subword-nmt
v3.0.2,hack for python2/3 compatibility
v3.0.2,check version information
v3.0.2,some hacking to deal with duplicates (only consider first instance)
v3.0.2,don't print end-of-word symbols
v3.0.2,sys.stderr.write('cannot split {0} further.\n'.format(segment))
v3.0.2,sys.stderr.write('OOV: {0}\n'.format(segment))
v3.0.2,sys.stderr.write('OOV: {0}\n'.format(segment))
v3.0.2,python 2/3 compatibility
v3.0.2,read/write files as UTF-8
v3.0.2,!/usr/bin/env python3
v3.0.2,coding: utf-8
v3.0.2,"In order to use this tool, please install comet first"
v3.0.2,https://github.com/Unbabel/COMET
v3.0.2,Let's say you have a source file with N sentences in SL - eg: source.sl
v3.0.2,and the corresponding references (N sentences) reference.tl
v3.0.2,Translate your file in TL with the -n_best nbest options nbest being
v3.0.2,then number of hypotheses and output the target to -output target.nbest.tl
v3.0.2,Then you need to duplicate source and reference sentences nbest times
v3.0.2,for this script.
v3.0.2,for instance using awk '{for(i=1; i<=n; i++) print}' n=5 reference.tl \
v3.0.2,> reference.5.tl
v3.0.2,same for source.
v3.0.2,This script can be run (for instance with nbest = 5) as follows:
v3.0.2,python oracle_bleu.py --nbest-src source.5.sl --nbest-hyp target.5.tl \
v3.0.2,--nbest-ref reference.5.tl --nbest-order 5 --output target.maxbleu.tl
v3.0.2,It will search in all hyp the best comet score
v3.0.2,when choosing a reference-less model no nbest-ref is required
v3.0.2,for nbest in nbests:
v3.0.2,!/usr/bin/env python
v3.0.2,!/usr/bin/env python3
v3.0.2,coding: utf-8
v3.0.2,Let's say you have a source file with N sentences in SL - eg: source.sl
v3.0.2,Translate your file in TL with the -n_best nbest options nbest being
v3.0.2,then number of hypotheses and output the target to -output target.nbest.tl
v3.0.2,This script can be run (for instance with nbest = 5) as follows:
v3.0.2,python mbr_bleu.py --nbest-hyp target.5.tl \
v3.0.2,--nbest-order 5 --output target.mbr.tl
v3.0.2,It will compare all hyp with eachother and output the max bleu
v3.0.2,!/usr/bin/env python
v3.0.2,-*- coding: utf-8 -*-
v3.0.2,Author: Rico Sennrich
v3.0.2,flake8: noqa
v3.0.2,This file is retrieved from https://github.com/rsennrich/subword-nmt
v3.0.2,hack for python2/3 compatibility
v3.0.2,"find all instances of pair, and update frequency/indices around it"
v3.0.2,find first symbol
v3.0.2,"if first symbol is followed by second symbol, we've found an occurrence of pair (old_word[i:i+2])"
v3.0.2,"assuming a symbol sequence ""A B C"", if ""B C"" is merged, reduce the frequency of ""A B"""
v3.0.2,"assuming a symbol sequence ""A B C B"", if ""B C"" is merged, reduce the frequency of ""C B""."
v3.0.2,"however, skip this if the sequence is A B C B C, because the frequency of ""C B"" will be reduced by the previous code block"
v3.0.2,find new pair
v3.0.2,"assuming a symbol sequence ""A BC D"", if ""B C"" is merged, increase the frequency of ""A BC"""
v3.0.2,"assuming a symbol sequence ""A BC B"", if ""B C"" is merged, increase the frequency of ""BC B"""
v3.0.2,"however, if the sequence is A BC BC, skip this step because the count of ""BC BC"" will be incremented by the previous code block"
v3.0.2,data structure of pair frequencies
v3.0.2,index from pairs to words
v3.0.2,version 0.2 changes the handling of the end-of-word token ('</w>');
v3.0.2,version numbering allows bckward compatibility
v3.0.2,"threshold is inspired by Zipfian assumption, but should only affect speed"
v3.0.2,we probably missed the best pair because of pruning; go back to full statistics
v3.0.2,"threshold is inspired by Zipfian assumption, but should only affect speed"
v3.0.2,python 2/3 compatibility
v3.0.2,read/write files as UTF-8
v3.0.2,Now we can pipe the full file through the model using the Iterator
v3.0.2,reminder a batch includes .src .tgt .indices and it is sorted
v3.0.2,Compute and retrieve the loss for EACH sentence
v3.0.2,loss is returned normalized by tokens
v3.0.2,we unnormalize to cumulate at doc level
v3.0.2,Now we need to rearrange the batch of ppl
v3.0.2,in the original order with indices
v3.0.2,!/usr/bin/env python
v3.0.2,-*- coding: utf-8 -*-
v3.0.2,!/usr/bin/env python
v3.0.2,!/usr/bin/env python3
v3.0.2,coding: utf-8
v3.0.2,Let's say you have a source file with N sentences in SL - eg: source.sl
v3.0.2,and the corresponding references (N sentences) reference.tl
v3.0.2,Translate your file in TL with the -n_best nbest options nbest being
v3.0.2,then number of hypotheses and output the target to -output target.nbest.tl
v3.0.2,Then you need to duplicate reference sentences nbest times for this script.
v3.0.2,for instance using awk '{for(i=1; i<=n; i++) print}' n=5 reference.tl \
v3.0.2,> reference.5.tl
v3.0.2,This script can be run (for instance with nbest = 5) as follows:
v3.0.2,python oracle_bleu.py --nbest-hyp target.5.tl --nbest-ref reference.5.tl \
v3.0.2,--nbest-order 5 --output target.maxbleu.tl
v3.0.2,It will search in all hyp the best bleu wrt reference
v3.0.2,and output the max bleu
v3.0.2,!/usr/bin/env python
v3.0.2,with the two module = imp.load_source() below
v3.0.2,we ghost the old torchtext.data.field and depercated
v3.0.2,onmt.inputters.text_dataset
v3.0.2,however this require some functions / classes to be
v3.0.2,monkey patched for loading the old field/vocab objects.
v3.0.2,"module3 = imp.load_source(""Vocab"", ""tools/convertv2_v3.py"")"
v3.0.2,"voc = dict(sorted(fields.vocab.__dict__['freqs'].items(),"
v3.0.2,"key=lambda x: (-x[1], x[0]))).keys()"
v3.0.2,"voc = dict(sorted(fields.vocab.__dict__['freqs'].items(),"
v3.0.2,"key=lambda x: (-x[1], x[0]))).keys()"
v3.0.2,"this patch is no longer needed, included in converter"
v3.0.2,"if hasattr(model_opt, 'rnn_size'):"
v3.0.2,model_opt.hidden_size = model_opt.rnn_size
v3.0.2,Avoid functionality on inference
v3.0.2,Build embeddings.
v3.0.2,Build encoder.
v3.0.2,Build embeddings.
v3.0.2,Build decoder.
v3.0.2,Share the embedding matrix - preprocess with share_vocab required.
v3.0.2,src/tgt vocab should be the same if `-share_vocab` is specified.
v3.0.2,Update vocabulary embeddings with checkpoint embeddings
v3.0.2,Embedding layers
v3.0.2,Just for debugging purposes
v3.0.2,Remove old vocabulary associated embeddings
v3.0.2,for back compat when attention_dropout was not defined
v3.0.2,Build Model
v3.0.2,Build Generator.
v3.0.2,Load the model states from checkpoint or initialize them.
v3.0.2,This preserves backward-compat for models using customed layernorm
v3.0.2,end of patch for backward compatibility
v3.0.2,Update model embeddings with those from the checkpoint
v3.0.2,after initialization
v3.0.2,!/usr/bin/env python
v3.0.2,"maybe prepare pretrained embeddings, if any"
v3.0.2,Load checkpoint if we resume from a previous training.
v3.0.2,ensure tensorboard output is written in the directory
v3.0.2,of previous checkpoints
v3.0.2,Override checkpoint's update_embeddings as it defaults to false
v3.0.2,Override checkpoint's freezing settings as it defaults to false
v3.0.2,NOTE: It's important that ``opt`` has been validated and updated
v3.0.2,at this point.
v3.0.2,Build model.
v3.0.2,Build optimizer.
v3.0.2,Build model saver
v3.0.2,Use Tensorboard for visualization during training
v3.0.2,Options only during inference
v3.0.2,"Truncation options, for text corpus"
v3.0.2,"as for False, this will be added in _add_train_general_opts"
v3.0.2,Embedding Options
v3.0.2,Model Task Options
v3.0.2,Encoder-Decoder Options
v3.0.2,Freeze Encoder and/or Decoder
v3.0.2,The following options (bridge_extra_node to n_steps) are used
v3.0.2,for training with --encoder_type ggnn (Gated Graph Neural Network).
v3.0.2,Attention options
v3.0.2,Alignement options
v3.0.2,Generator and loss options.
v3.0.2,GPU
v3.0.2,Init options
v3.0.2,Pretrained word vectors
v3.0.2,Freeze word vectors
v3.0.2,Optimization options
v3.0.2,learning rate
v3.0.2,options relate to data preprare
v3.0.2,options relate to train
v3.0.2,Alpha and Beta values for Google Length + Coverage penalty
v3.0.2,"Described here: https://arxiv.org/pdf/1609.08144.pdf, Section 7"
v3.0.2,Length penalty options
v3.0.2,Coverage penalty options
v3.0.2,Decoding Length constraint
v3.0.2,Decoding content constraint
v3.0.2,Adding options relate to decoding strategy
v3.0.2,Adding option for logging
v3.0.2,Adding options related to Transforms
v3.0.2,Copyright 2016 The Chromium Authors. All rights reserved.
v3.0.2,Use of this source code is governed by a BSD-style license that can be
v3.0.2,found in the LICENSE file.
v3.0.2,"Get the key 'value' in the dict, or just use 'value'"
v3.0.2,Basic attributes.
v3.0.2,Set model in training mode.
v3.0.2,UPDATE DROPOUT
v3.0.2,Run patience mechanism
v3.0.2,"If the patience has reached the limit, stop training"
v3.0.2,swap model params w/ moving average
v3.0.2,(and keep the original parameters)
v3.0.2,Set model in validating mode.
v3.0.2,F-prop through the model.
v3.0.2,Compute loss.
v3.0.2,Compute validation metrics (at batch.dataset level)
v3.0.2,Compute stats
v3.0.2,Update statistics.
v3.0.2,Set model back to training mode.
v3.0.2,Truncated BPTT: reminder not compatible with accum > 1
v3.0.2,1. Create truncated target.
v3.0.2,2. F-prop all but generator.
v3.0.2,3. Compute loss.
v3.0.2,Compute and save stats
v3.0.2,in theory we should divide by accum_count and bptt
v3.0.2,to rescale for each sub batch
v3.0.2,4. Update the parameters and statistics.
v3.0.2,Multi GPU gradient gather
v3.0.2,"If truncated, don't backprop fully."
v3.0.2,"in case of multi step gradient accumulation,"
v3.0.2,update only after accum batches
v3.0.2,For Flake
v3.0.2,we avoid padding while mean pooling
v3.0.2,incoming and outgoing edge embedding
v3.0.2,Find vocab data for tree builting
v3.0.2,Propogation Model
v3.0.2,Initialize the bridge layer
v3.0.2,Token embedding
v3.0.2,Initialize graph using formatted input sequence
v3.0.2,Number of flagged nodes defines node count for this sample
v3.0.2,"(Nodes can have no flags on them, but must be in 'flags' list)."
v3.0.2,The total number of integers in the vocab should allow
v3.0.2,for all features and edges to be defined.
v3.0.2,Use first extra node as only source for decoder init
v3.0.2,Average all nodes to get bridge input
v3.0.2,"LSTM has hidden and cell state, other only one"
v3.0.2,Total number of states
v3.0.2,Build a linear layer for each
v3.0.2,Initialize the bridge layer
v3.0.2,src lengths data is wrapped inside a Tensor.
v3.0.2,"LSTM has hidden and cell state, other only one"
v3.0.2,Total number of states
v3.0.2,Build a linear layer for each
v3.0.2,batch x len x dim
v3.0.2,mask is now (batch x 1 x slen x slen)
v3.0.2,1 to be expanded to number of heads in MHA
v3.0.2,Run the forward pass of every layer of the tranformer.
v3.0.2,Dimensions and padding for constructing the word embedding matrix
v3.0.2,Dimensions and padding for feature embedding matrices
v3.0.2,(these have no effect if feat_vocab_sizes is empty)
v3.0.2,The embedding matrix look-up tables. The first look-up table
v3.0.2,"is for words. Subsequent ones are for features, if any exist."
v3.0.2,The final output size of word + feature vectors. This can vary
v3.0.2,from the word vector size if and only if features are defined.
v3.0.2,This is the attribute you should access if you need to know
v3.0.2,how big your embeddings are going to be.
v3.0.2,The sequence of operations that converts the input sequence
v3.0.2,into a sequence of embeddings. At minimum this consists of
v3.0.2,looking up the embeddings for each word and feature in the
v3.0.2,input. Model parameters may require the sequence to contain
v3.0.2,additional operations as well.
v3.0.2,features must use word_vec_size
v3.0.2,features will use feat_vec_size
v3.0.2,Some utilitary functions for pretrained embeddings
v3.0.2,is this reachable?
v3.0.2,Write to file
v3.0.2,set the opt in place
v3.0.2,set the opt in place
v3.0.2,flake8: noqa
v3.0.2,For command-line option parsing
v3.0.2,"Check pass, set the args."
v3.0.2,"This SRU version implements its own cuda-level optimization,"
v3.0.2,so it requires that:
v3.0.2,1. `cupy` and `pynvrtc` python package installed.
v3.0.2,2. pytorch is built with cuda support.
v3.0.2,3. library path set: export LD_LIBRARY_PATH=<cuda lib path>.
v3.0.2,Check 1.
v3.0.2,Check 2.
v3.0.2,Check 3.
v3.0.2,This sets up device to use.
v3.0.2,-> directions x batch x dim
v3.0.2,For DEBUG
v3.0.2,"size = (length, batch, x.size(-1)) \"
v3.0.2,"if x.dim() == 3 else (batch, x.size(-1))"
v3.0.2,grad_x = x.new(*x.size()) if k_ == 3 else x.new(*size).zero_()
v3.0.2,Normal use
v3.0.2,"An entry check here, will catch on train side and translate side"
v3.0.2,if requirements are not satisfied.
v3.0.2,RNNDecoderState wraps hidden as a tuple.
v3.0.2,fh -> (layers*directions) x batch x dim
v3.0.2,This class is mainly used by decoder.py for RNNs but also
v3.0.2,by the CNN / transformer decoder when copy attention is used
v3.0.2,CNN has its own attention mechanism ConvMultiStepAttention
v3.0.2,Transformer has its own MultiHeadedAttention
v3.0.2,mlp wants it with bias
v3.0.2,"(batch, t_len, d) x (batch, d, s_len) --> (batch, t_len, s_len)"
v3.0.2,"(batch, t_len, s_len, d)"
v3.0.2,one step input
v3.0.2,"compute attention scores, as in Luong et al."
v3.0.2,Softmax or sparsemax to normalize attention weights
v3.0.2,each context vector c_t is the weighted average
v3.0.2,over all the source hidden states
v3.0.2,concatenate
v3.0.2,clamping necessary because of numerical errors: loss should be lower
v3.0.2,"bounded by zero, but negative values near zero are possible without"
v3.0.2,the clamp
v3.0.2,Shift values to be >= 0
v3.0.2,class MultiHeadedAttention(torch.jit.ScriptModule):
v3.0.2,https://arxiv.org/pdf/1803.02155.pdf
v3.0.2,in the paper they suggest either two embeds
v3.0.2,relative_key / relative_value or only
v3.0.2,relative_key. We implemented the same embed
v3.0.2,for both.
v3.0.2,@torch.jit.script_method
v3.0.2,"1) Project key, value, and query."
v3.0.2,as a reminder at training layer_cache[0] remains False
v3.0.2,2) Calculate and scale scores.
v3.0.2,batch x num_heads x query_len x key_len
v3.0.2,1 or key_len x key_len
v3.0.2,1 or key_len x key_len x dim_per_head
v3.0.2,not 100% necessary but expand to nb of heads
v3.0.2,now mask and scores have the same shape
v3.0.2,3) Apply attention dropout and compute context vectors.
v3.0.2,We use the same embeddings for key and value
v3.0.2,At the moment this class is only used by embeddings.Embeddings look-up tables
v3.0.2,-*- coding: utf-8 -*-
v3.0.2,class AverageAttention(torch.jit.ScriptModule):
v3.0.2,@torch.jit.script
v3.0.2,out_features * in_features
v3.0.2,norm is out_features * 1
v3.0.2,batch_size * out_features
v3.0.2,out_features
v3.0.2,out_features
v3.0.2,batch_size * out_features
v3.0.2,"out_channels, in_channels // groups, * kernel_size"
v3.0.2,out_features
v3.0.2,This is used nowhere in the code at the moment (Vincent Nguyen 05/18/2018)
v3.0.2,"in_channels, out_channels, *kernel_size"
v3.0.2,"in_channels, out_channels, *kernel_size"
v3.0.2,"self.out_channels, 1"
v3.0.2,out_features
v3.0.2,out_features
v3.0.2,store roots on diagonal
v3.0.2,Original probabilities.
v3.0.2,Probability of copying p(z=1) batch.
v3.0.2,Probability of not copying: p_{word}(w) * (1 - p(z))
v3.0.2,probabilities assigned by the model to the gold targets
v3.0.2,probability of tokens copied from source
v3.0.2,Set scores for unk to 0 and add eps
v3.0.2,find the indices in which you do not use the copy mechanism
v3.0.2,Drop padding.
v3.0.2,Do nothing
v3.0.2,Do nothing
v3.0.2,Punctuation only
v3.0.2,Auto import python files in this directory
v3.0.2,1. sample number of tokens to corrupt
v3.0.2,2. sample positions to corrput
v3.0.2,3. sample corrupted values
v3.0.2,1. sample number of tokens to corrupt
v3.0.2,2. sample positions to corrput
v3.0.2,3. Drop token on chosen position
v3.0.2,1. sample number of tokens to corrupt
v3.0.2,2. sample positions to corrput
v3.0.2,3. mask word on chosen position
v3.0.2,"Sharing options among `TokenizerTransform`s, same name conflict in"
v3.0.2,this scope will be resolved by remove previous occurrence in parser
v3.0.2,subword regularization(or BPE dropout) options:
v3.0.2,subword vocabulary restriction options:
v3.0.2,derterministic subwording
v3.0.2,subword sampling when nbest_size > 1 or -1
v3.0.2,alpha should be 0.0 < alpha < 1.0
v3.0.2,Load vocabulary file if provided and set threshold
v3.0.2,Load Subword Model
v3.0.2,-1: keep everything (i.e. 1 mask per token)
v3.0.2,0: replace everything (i.e. no mask)
v3.0.2,1: 1 mask per span
v3.0.2,view each subword as word start / input is word level token
v3.0.2,Pretend it ends with a full stop so last span is a sentence
v3.0.2,"Tokens that are full stops, where the previous token is not"
v3.0.2,Make sure we have enough to mask
v3.0.2,Trim to masking budget
v3.0.2,Handle 0-length mask (inserts) separately
v3.0.2,assert is_word_start[-1] == 0
v3.0.2,assert tokens_length - 1 not in indices
v3.0.2,"keep index, but replace it with [MASK]"
v3.0.2,"acts as a long length, so spans don't go over the end of doc"
v3.0.2,next position from each word_start
v3.0.2,delete token: 1 mask/remove per span
v3.0.2,"keep index, but replace it with [MASK]: 1 mask per token"
v3.0.2,A bit faster when all lengths are 1
v3.0.2,to cover whole token
v3.0.2,delete token
v3.0.2,"keep index, but replace it with [MASK]"
v3.0.2,assert tokens_length - 1 not in indices
v3.0.2,batch 0 will always predict EOS. The other batches will predict
v3.0.2,non-eos scores.
v3.0.2,"""best"" prediction is eos - that should be blocked"
v3.0.2,include at least one prediction OTHER than EOS
v3.0.2,that is greater than -1e20
v3.0.2,now batch 0 has ended and no others have
v3.0.2,initial step
v3.0.2,batch 0 dies on step 0
v3.0.2,include at least one prediction OTHER than EOS
v3.0.2,that is greater than -1e20
v3.0.2,step 2
v3.0.2,(old) batch 8 dies on step 1
v3.0.2,step 3
v3.0.2,everything dies
v3.0.2,initial step
v3.0.2,batch 0 dies on step 0
v3.0.2,include at least one prediction OTHER than EOS
v3.0.2,that is greater than -1e20
v3.0.2,step 2
v3.0.2,(old) batch 8 dies on step 1
v3.0.2,step 3
v3.0.2,everything dies
v3.0.2,initial step
v3.0.2,finish one beam
v3.0.2,include at least one prediction OTHER than EOS
v3.0.2,that is greater than -1e20
v3.0.2,step 2
v3.0.2,finish example in last batch
v3.0.2,(old) batch 8 dies on step 1
v3.0.2,step 3
v3.0.2,everything dies
v3.0.2,initial step
v3.0.2,batch 0 dies on step 0
v3.0.2,include at least one prediction OTHER than EOS
v3.0.2,that is greater than -1e20
v3.0.2,step 2
v3.0.2,(old) batch 8 dies on step 1
v3.0.2,step 3
v3.0.2,everything dies
v3.0.2,illegal_weights_mask = torch.ByteTensor([
v3.0.2,"[0, 0, 0, 0, 0, 0, 0],"
v3.0.2,"[0, 0, 0, 1, 1, 1, 1],"
v3.0.2,"[0, 0, 0, 0, 0, 1, 1],"
v3.0.2,"[0, 0, 1, 1, 1, 1, 1]])"
v3.0.2,TODO: fix for pytorch 0.3
v3.0.2,illegal_weights = alignments.masked_select(illegal_weights_mask)
v3.0.2,"self.assertEqual(0.0, illegal_weights.data.sum())"
v3.0.2,this could be considered an integration test because it touches
v3.0.2,the filesystem for the config file (and the models)
v3.0.2,no dummy prefix
v3.0.2,no dummy prefix
v3.0.2,make sure the scalars are in the event accumulator tags
v3.0.2,required arguments
v3.0.2,transforms that require vocab will not create if not provide vocab
v3.0.2,1. Init first transform in the pipe
v3.0.2,2. Init second transform in the pipe
v3.0.2,3. Sequential combine them into a transform pipe
v3.0.2,4. apply transform pipe for example
v3.0.2,"5. example after the pipe exceed the length limit, thus filtered"
v3.0.2,6. Transform statistics registed (here for filtertoolong)
v3.0.2,"7. after report, statistics become empty as a fresh start"
v3.0.2,filter_transform.warm_up()
v3.0.2,test BPE-dropout:
v3.0.2,1. disable bpe dropout for not training example
v3.0.2,2. enable bpe dropout for training example
v3.0.2,3. (NOTE) disable dropout won't take effect if already seen
v3.0.2,this is caused by the cache mechanism in bpe:
v3.0.2,return cached subword if the original token is seen when no dropout
v3.0.2,test SP regularization:
v3.0.2,1. enable regularization for training example
v3.0.2,2. disable regularization for not training example
v3.0.2,Not apply token drop for not training example
v3.0.2,apply token drop for training example
v3.0.2,Not apply token mask for not training example
v3.0.2,apply token mask for training example
v3.0.2,require vocabs to warm_up
v3.0.2,Not apply token mask for not training example
v3.0.2,apply token mask for training example
v3.0.2,"Defalt: full_stop_token=[""."", ""?"", ""!""]"
v3.0.2,"Defalt: full_stop_token=[""."", ""?"", ""!""]"
v3.0.2,random_ratio of inserted tokens are chosen in vocab
v3.0.2,others are MASK_TOK
v3.0.2,"insert_ratio=0.0,"
v3.0.2,"random_ratio=0.0,"
v3.0.2,"Defalt: full_stop_token=[""."", ""?"", ""!""]"
v3.0.2,all token are considered as an individual word
v3.0.2,1. tokens are dropped when replace_length is 0
v3.0.2,"print(f""token delete: {masked} / {tokens}"")"
v3.0.2,2. tokens are replaced by MASK when replace_length is 1
v3.0.2,"print(f""token mask: {masked} / {tokens}"")"
v3.0.2,"insert_ratio=0.0,"
v3.0.2,"random_ratio=0.0,"
v3.0.2,"Defalt: full_stop_token=[""."", ""?"", ""!""]"
v3.0.2,start token of word are identified using subword marker
v3.0.2,"1. replace_length 0: ""words"" are dropped"
v3.0.2,"print(f""word delete: {masked} / {tokens}"")"
v3.0.2,"self.assertEqual(len(masked), n_words - n_masked)"
v3.0.2,"2. replace_length 1: ""words"" are replaced with a single MASK"
v3.0.2,"print(f""whole word single mask: {masked} / {tokens}"")"
v3.0.2,len(masked) depend on number of tokens in select word
v3.0.2,"3. replace_length -1: all tokens in ""words"" are replaced with MASK"
v3.0.2,"print(f""whole word multi mask: {masked} / {tokens}"")"
v3.0.2,number of mask_tok depend on number of tokens in selected word
v3.0.2,number of MASK_TOK can be greater than n_masked
v3.0.2,"insert_ratio=0.5,"
v3.0.2,"random_ratio=0.3,"
v3.0.2,"Defalt: full_stop_token=[""."", ""?"", ""!""]"
v3.0.2,start token of word are identified using subword marker
v3.0.2,n_words = sum(token_starts)
v3.0.2,n_masked = math.ceil(n_words * bart_noise.mask_ratio)
v3.0.2,"print(f""Text Span Infilling: {infillied} / {tokens}"")"
v3.0.2,"print(n_words, n_masked)"
v3.0.2,!/usr/bin/env python
v3.0.2,-*- coding: utf-8 -*-
v3.0.2,Inject some dummy training options that may needed when build fields
v3.0.2,Remove the generated *pt files.
v3.0.2,Remove the generated data samples
v3.0.2,all beams repeat (beam >= 1 repeat dummy scores)
v3.0.2,predict repeat_idx over and over again
v3.0.2,"before repeat, scores are either 0 or -inf"
v3.0.2,"on repeat, `repeat_idx` score is BLOCKED_SCORE"
v3.0.2,"(but it's still the best score, thus we have"
v3.0.2,"[BLOCKED_SCORE, -inf, -inf, -inf, -inf]"
v3.0.2,repetitions keeps maximizing score
v3.0.2,"index 0 has been blocked, so repeating=>+0.0 score"
v3.0.2,other indexes are -inf so repeating=>BLOCKED_SCORE
v3.0.2,which is higher
v3.0.2,beam 0 and beam >=2 will repeat (beam >= 2 repeat dummy scores)
v3.0.2,non-interesting beams are going to get dummy values
v3.0.2,"on initial round, only predicted scores for beam 0"
v3.0.2,matter. Make two predictions. Top one will be repeated
v3.0.2,"in beam zero, second one will live on in beam 1."
v3.0.2,predict the same thing in beam 0
v3.0.2,continue pushing around what beam 1 predicts
v3.0.2,"now beam 0 dies (along with the others), beam 1 -> beam 0"
v3.0.2,"now beam 0 dies (along with the others), beam 1 -> beam 0"
v3.0.2,beam 0 and beam >= 2 will repeat (beam 2 repeats excluded idx)
v3.0.2,non-interesting beams are going to get dummy values
v3.0.2,predict the same thing in beam 0
v3.0.2,continue pushing around what beam 1 predicts
v3.0.2,predict the allowed-repeat again in beam 2
v3.0.2,"now beam 0 dies, beam 1 -> beam 0, beam 2 -> beam 1"
v3.0.2,and the rest die
v3.0.2,"since all preds after i=0 are 0, we can check"
v3.0.2,that the beam is the correct idx by checking that
v3.0.2,the curr score is the initial score
v3.0.2,beam 0 will always predict EOS. The other beams will predict
v3.0.2,non-eos scores.
v3.0.2,non-interesting beams are going to get dummy values
v3.0.2,"""best"" prediction is eos - that should be blocked"
v3.0.2,include at least beam_sz predictions OTHER than EOS
v3.0.2,that are greater than -1e20
v3.0.2,predict eos in beam 0
v3.0.2,provide beam_sz other good predictions
v3.0.2,now the top beam has ended and no others have
v3.0.2,"not of interest, but want to make sure it keeps running"
v3.0.2,since only beam 0 terminates and n_best = 2
v3.0.2,"this is also a test that when block_ngram_repeat=0,"
v3.0.2,repeating is acceptable
v3.0.2,non-interesting beams are going to get dummy values
v3.0.2,"""best"" prediction is eos - that should be blocked"
v3.0.2,include at least beam_sz predictions OTHER than EOS
v3.0.2,that are greater than -1e20
v3.0.2,predict eos in beam 1
v3.0.2,provide beam_sz other good predictions in other beams
v3.0.2,beam 1 dies on min_length
v3.0.2,beam 0 dies on the step after beam 1 dies
v3.0.2,"inp_lens is tiled in initialize, reassign to make attn match"
v3.0.2,non-interesting beams are going to get dummy values
v3.0.2,"""best"" prediction is eos - that should be blocked"
v3.0.2,include at least beam_sz predictions OTHER than EOS
v3.0.2,that are greater than -1e20
v3.0.2,predict eos in beam 1
v3.0.2,provide beam_sz other good predictions in other beams
v3.0.2,no top beams are finished yet
v3.0.2,beam 1 dies on min_length
v3.0.2,no top beams are finished yet
v3.0.2,beam 0 dies on the step after beam 1 dies
v3.0.2,top beam is finished now so there are attentions
v3.0.2,two beams are finished in each batch
v3.0.2,second dim is cut down to the non-padded src length
v3.0.2,first dim is equal to the time of death
v3.0.2,(beam 0 died at current step - adjust for SOS)
v3.0.2,(beam 1 died at last step - adjust for SOS)
v3.0.2,behavior gets weird when beam is already done so just stop
v3.0.2,this is just test_beam.TestBeamAgainstReferenceCase repeated
v3.0.2,in each batch.
v3.0.2,"init_preds: [4, 3, 5, 6, 7] - no EOS's"
v3.0.2,no EOS's yet
v3.0.2,"[5, 3, 2, 6, 0], so beam 2 predicts EOS!"
v3.0.2,assumes beam 2 finished on last step
v3.0.2,ended beam 2 shouldn't continue
v3.0.2,"[2, 5, 3, 6, 0] repeat self.BATCH_SZ, so beam 0 predicts EOS!"
v3.0.2,"[-2.4879, -3.8910, -4.1010, -4.2010, -4.4010] repeat self.BATCH_SZ"
v3.0.2,another beam is finished in all batches
v3.0.2,new beam 0 finished
v3.0.2,new beam 0 is old beam 3
v3.0.2,assumes beam 0 finished on last step
v3.0.2,"[5, 2, 6, 1, 0] repeat self.BATCH_SZ, so beam 1 predicts EOS!"
v3.0.2,we finish 3 hyps per example in this step
v3.0.2,new beam 1 is old beam 3
v3.0.2,this could be considered an integration test because it tests
v3.0.2,interactions between the GNMT scorer and the beam
v3.0.2,"-data option is required, but not used in this test, so dummy."
v3.0.2,len x batch x nfeat
v3.0.2,Initialize vectors to compare size with
v3.0.2,Ensure correct sizes and types
v3.0.2,Make sure that output has the correct size and type
v3.0.2,"[('encoder_type', 'transformer'),"
v3.0.2,"('word_vec_size', 16), ('hidden_size', 16)],"
v3.0.2,""""""" Only do SRU test if requirment is safisfied. """""""
v3.0.2,SRU doesn't support input_feed.
v3.0.2,first check there's nothing unexpectedly not trainable
v3.0.2,ok: word embeddings shouldn't be trainable
v3.0.2,if word vecs are freezed
v3.0.2,ok: positional encodings shouldn't be trainable
v3.0.2,then check nothing unexpectedly trainable
v3.0.2,Decoder state
v3.0.2,Build the RNN.
v3.0.2,Set up the context gate.
v3.0.2,Set up the standard attention.
v3.0.2,The encoder hidden is  (layers*directions) x batch x dim.
v3.0.2,We need to convert it to layers x batch x (directions*dim).
v3.0.2,Init the input feed.
v3.0.2,Update the state with the result.
v3.0.2,Concatenates sequence of tensors along a new dimension.
v3.0.2,NOTE: v0.3 to 0.4: dec_outs / attns[*] may not be list
v3.0.2,(in particular in case of SRU) it was not raising error in 0.3
v3.0.2,since stack(Variable) was allowed.
v3.0.2,"In 0.4, SRU returns a tensor that shouldn't be stacke"
v3.0.2,Calculate the attention.
v3.0.2,Calculate the context gate.
v3.0.2,Additional args check.
v3.0.2,Input feed concatenates hidden state with
v3.0.2,input at every time step.
v3.0.2,TODO: context gate should be employed
v3.0.2,instead of second RNN transform.
v3.0.2,Update the coverage attention.
v3.0.2,Decoder State
v3.0.2,CNNDecoder has its own attention mechanism.
v3.0.2,Set up a separate copy attention layer if needed.
v3.0.2,The output of CNNEncoder.
v3.0.2,The combination of output of CNNEncoder and source embeddings.
v3.0.2,Process the result and update the attentions.
v3.0.2,Update the state.
v3.0.2,TODO change the way attns is returned dict => list or tuple (onnx)
v3.0.2,src_len is a single tensor shared between all models.
v3.0.2,This assumption will not hold if Translator is modified
v3.0.2,to calculate src_len as something other than the length
v3.0.2,of the input.
v3.0.2,"return _, (B, Q_len, K_len)"
v3.0.2,"layer average attention across heads, get ``(B, Q, K)``"
v3.0.2,"Case 1: no full_context, no align heads -> layer avg baseline"
v3.0.2,"Case 2: no full_context, 1 align heads -> guided align"
v3.0.2,"Case 3: full_context, 1 align heads -> full cte guided align"
v3.0.2,BoolTensor was introduced in pytorch 1.2
v3.0.2,T: could be 1 in the case of stepwise decoding or tgt_len
v3.0.2,masking is necessary when sequence length is greater than one
v3.0.2,mask now are (batch x 1 x tlen x s or t len)
v3.0.2,1 = heads to be expanded in MHA
v3.0.2,Decoder State
v3.0.2,"previously, there was a GlobalAttention module here for copy"
v3.0.2,"attention. But it was never actually used -- the ""copy"" attention"
v3.0.2,just reuses the context attention.
v3.0.2,"attns[""align""] = torch.stack(attn_aligns, 0).mean(0)  # All avg"
v3.0.2,TODO change the way attns is returned dict => list or tuple (onnx)
v3.0.2,first value set to True triggered by the beginning of decoding
v3.0.2,layer_cache becomes active in the MultiHeadedAttention fwd
v3.0.2,T: could be 1 in the case of stepwise decoding or tgt_len
v3.0.2,masking is necessary when sequence length is greater than one
v3.0.2,mask now are (batch x 1 x tlen x tlen)
v3.0.2,1 = heads to be expanded in MHA
v3.0.2,TODO change the way attns is returned dict => list or tuple (onnx)
v3.0.2,"buffer size in bytes, determine equiv. # of elements based on data type"
v3.0.2,copy tensors into buffer_t
v3.0.2,all-reduce and rescale
v3.0.2,copy all-reduced buffer back into tensors
v3.0.2,"print(filled, sz)"
v3.0.2,"tensor is bigger than buffer, all-reduce and rescale directly"
v3.0.2,"buffer is full, all-reduce and replace buffer with grad"
v3.0.2,add tensor to buffer
v3.0.2,"propagate exception to parent process, keeping original traceback"
v3.0.2,TODO: Find a better way to check for sparse gradients.
v3.0.2,we use apex.amp
v3.0.2,In this case use the old FusedAdam with
v3.0.2,FP16_optimizer wrapper
v3.0.2,Load everything from the checkpoint.
v3.0.2,Build everything from scratch.
v3.0.2,"Reset optimizer, keep options."
v3.0.2,"Reset options, keep optimizer."
v3.0.2,State can be partially restored.
v3.0.2,should be: self._optimizer.zero_grad(set_to_none)
v3.0.2,but apex.amp is not up-to-date:
v3.0.2,https://github.com/NVIDIA/apex/blob/master/apex/amp/_process_optimizer.py#L367
v3.0.2,"unscaled optimizer's gradients (already done therefore skip),"
v3.0.2,skips optimizer.step() if gradients contain infs/NaNs.
v3.0.2,Updates the scale for next iteration.
v3.0.2,Code below is an implementation of https://arxiv.org/pdf/1804.04235.pdf
v3.0.2,inspired but modified from https://github.com/DeadAt0m/adafactor-pytorch
v3.0.2,backward compatibility
v3.0.2,assuming a list/generator of parameter means single group
v3.0.2,compute combined scale factor for this group
v3.0.2,norm is in fact norm*scale
v3.0.2,note: p.grad should not ever be set for correct operation of
v3.0.2,mixed precision optimizer that sometimes sends None gradients
v3.0.2,State initialization
v3.0.2,Exponential moving average of gradient values
v3.0.2,Exponential moving average of squared gradient values
v3.0.2,-*- coding: utf-8 -*-
v3.0.2,if the loss function operates on vectors of raw logits instead
v3.0.2,"of probabilities, only the first part of the generator needs to"
v3.0.2,"be passed to the NMTLossCompute. At the moment, the only"
v3.0.2,supported loss function of this kind is the sparsemax loss.
v3.0.2,"align_head contains value in [0, 1) presenting attn prob,"
v3.0.2,0 was resulted by the context attention src_pad_mask
v3.0.2,"So, the correspand position in ref_align should also be 0"
v3.0.2,"Therefore, clip align_head to > 1e-18 should be bias free."
v3.0.2,"we use the raw logits, rescale with tau (temperature) and"
v3.0.2,apply the log_softmax. reminder generator[0] is just the nn.Linear
v3.0.2,ct2 expects src with lengths without padding
v3.0.2,again we use raw probs to rescale with tau and apply log_softmax
v3.0.2,lm_scores are in log space so log_target=True
v3.0.2,"we use the raw logits, rescale with tau (temperature) and"
v3.0.2,apply the log_softmax. reminder generator[0] is just the nn.Linear
v3.0.2,ct2 expects src with lengths without padding
v3.0.2,again we use raw probs to rescale with tau and apply log_softmax
v3.0.2,lm_scores are in log space so log_target=True
v3.0.2,take into account here the tgt_shift_index (0 / 1 = LM/NMT)
v3.0.2,Correct target copy token instead of <unk>
v3.0.2,tgt[i] = align[i] + len(tgt_vocab)
v3.0.2,for i such that tgt[i] == 0 and align[i] != 0
v3.0.2,in the case criterion reduction is None then we need
v3.0.2,to sum the loss of each sentence in the batch
v3.0.2,Check Transforms
v3.0.2,Check path
v3.0.2,tgt is src for LM task
v3.0.2,Check prefix: will be used when use prefix transform
v3.0.2,Check weight
v3.0.2,Check features
v3.0.2,validation when train:
v3.0.2,Check embeddings stuff
v3.0.2,"Backward compatibility with ""fix_word_vecs_*"" opts"
v3.0.2,encoder and decoder should be same sizes
v3.0.2,"Load default opt values, then overwrite with the opts in"
v3.0.2,"the checkpoint. That way, if there are new options added,"
v3.0.2,the defaults are used.
v3.0.2,It comes from training
v3.0.2,TODO: needs to be added as inference opt
v3.0.2,Don't do anything
v3.0.2,Update best score of each criteria
v3.0.2,Reset tolerance
v3.0.2,Update current status
v3.0.2,Decrease tolerance
v3.0.2,Log
v3.0.2,Log
v3.0.2,Get a list of world_size lists with len(stat_list) Statistics objects
v3.0.2,"this param init is overridden by model_builder, useless then."
v3.0.2,SRU doesn't support PackedSequence.
v3.0.2,-*- coding: utf-8 -*-
v3.0.2,threshold on 1 to avoid div by 0
v3.0.2,treat alignment matrix one by one as each have different lengths
v3.0.2,No alignment if not exist valid tgt token
v3.0.2,get valid alignment (sub-matrix from full paded aligment matrix)
v3.0.2,Helper functions
v3.0.2,Keeps track of the original words/subwords
v3.0.2,('prior_tokenization' option)
v3.0.2,In case there is a final case_markup when new_spacer is on
v3.0.2,for validation we build an infer_iter per batch
v3.0.2,in order to avoid oom issues because there is no
v3.0.2,batching strategy in `textbatch_to_tensor`
v3.0.2,flatten sources (for validation)
v3.0.2,we deactivate the decoder's cache
v3.0.2,as we use teacher forcing at training time.
v3.0.2,-*- coding: utf-8 -*-
v3.0.2,this one is needed for Random Shuffler of batches
v3.0.2,in multi gpu it ensures datasets are read in the same order
v3.0.2,some cudnn methods can be random even after fixing the seed
v3.0.2,unless you tell it to be deterministic
v3.0.2,This one is needed for various tranfroms
v3.0.2,These ensure same initialization in multi gpu mode
v3.0.2,we need to check the model path + any tokenizer path
v3.0.2,bucket_size = batch_size
v3.0.2,We only support
v3.0.2,For TRAIN we need to group examples by length
v3.0.2,"for faster performance, but otherwise, sequential."
v3.0.2,For TRAIN we shuffle batches within the bucket
v3.0.2,otherwise sequential
v3.0.2,for specific case of rnn_packed need to be sorted
v3.0.2,within the batch
v3.0.2,Maintains the longest src and tgt length in the current batch
v3.0.2,Reset current longest length at a new batch (count=1)
v3.0.2,Src: [<bos> w1 ... wN <eos>]
v3.0.2,Tgt: [w1 ... wM <eos>]
v3.0.2,Make features part of src like
v3.0.2,"{'src': {'src': ..., 'feat1': ...., 'feat2': ....}}"
v3.0.2,We apply the same TransformPipe to all the bucket
v3.0.2,at this point an example looks like:
v3.0.2,"{'src': {'src': ..., 'feat1': ...., 'feat2': ....},"
v3.0.2,"'tgt': {'tgt': ...},"
v3.0.2,"'src_original': ['tok1', ...'tokn'],"
v3.0.2,"'tgt_original': ['tok1', ...'tokm'],"
v3.0.2,'indices' : seq in bucket
v3.0.2,"'align': ...,"
v3.0.2,}
v3.0.2,we'll need to change this if we introduce tgt feat
v3.0.2,Need to add features in last dimensions
v3.0.2,Need to add features also in 'src'
v3.0.2,make a small vocab containing just the tokens in the source sequence
v3.0.2,Map source tokens to indices in the dynamic dict.
v3.0.2,-*- coding: utf-8 -*-
v3.0.2,'src_original' and 'tgt_original' store the
v3.0.2,original line before tokenization. These
v3.0.2,fields are used later on in the feature
v3.0.2,transforms.
v3.0.2,NOTE: moved to dynamic_iterator.py cf process()
v3.0.2,item = self.transform.apply(
v3.0.2,"example, is_train=self.infinitely, corpus_name=self.cid)"
v3.0.2,empty example: skip
v3.0.2,"No encoder in LM, seq2seq count formatting kept"
v3.0.2,_check_save_model_path
v3.0.2,NOTE: We need to trim the vocab to remove any unk tokens that
v3.0.2,were not originally here.
v3.0.2,"for side in [""src"", ""tgt""]:"
v3.0.2,keys_to_pop = []
v3.0.2,"if hasattr(vocab[side], ""fields""):"
v3.0.2,unk_token = vocab[side].fields[0][1].vocab.itos[0]
v3.0.2,"for key, value in vocab[side].fields[0][1].vocab.stoi.items():"
v3.0.2,if value == 0 and key != unk_token:
v3.0.2,keys_to_pop.append(key)
v3.0.2,for key in keys_to_pop:
v3.0.2,"vocab[side].fields[0][1].vocab.stoi.pop(key, None)"
v3.0.2,!/usr/bin/env python
v3.0.2,!/usr/bin/env python
v3.0.2,!/usr/bin/env python
v3.0.2,-*- coding: utf-8 -*-
v3.0.2,!/usr/bin/env python
v3.0.2,Just for debugging purposes
v3.0.2,It appends features to subwords when dumping to file
v3.0.2,!/usr/bin/env python
v3.0.2,!/usr/bin/env python
v3.0.2,Set sharing strategy manually instead of default based on the OS.
v3.0.2,torch.multiprocessing.set_sharing_strategy('file_system')
v3.0.2,Create a thread to listen for errors in the child processes.
v3.0.2,Train with multiprocessing.
v3.0.2,magic indices
v3.0.2,result caching
v3.0.2,fix length constraint and remove eos from count
v3.0.2,add one to account for BOS. Don't account for EOS because hitting
v3.0.2,this implies it hasn't been found.
v3.0.2,we don't block nothing if the user doesn't want it
v3.0.2,we can't block nothing beam's too short
v3.0.2,we check paths one by one
v3.0.2,we don't forbid nothing if the user doesn't want it
v3.0.2,we can't forbid nothing if beam's too short
v3.0.2,Reordering forbidden_tokens following beam selection
v3.0.2,We rebuild a dict to ensure we get the value and not the pointer
v3.0.2,Grabing the newly selected tokens and associated ngram
v3.0.2,skip the blocking if any token in current_ngram is excluded
v3.0.2,"pickups: Tensor where specified index were set to 1, others 0"
v3.0.2,"dropdowns: opposite of pickups, 1 for those shouldn't pick"
v3.0.2,Minus dropdowns to log_probs making probabilities of
v3.0.2,unspecified index close to 0
v3.0.2,"prediction step have surpass length of given target_prefix,"
v3.0.2,no need to further change this attr
v3.0.2,keep indices until overflowing p
v3.0.2,Set all logits that are not in the top-p to -10000.
v3.0.2,This puts the probabilities close to 0.
v3.0.2,Set all logits that are not in the top-k to -10000.
v3.0.2,This puts the probabilities close to 0.
v3.0.2,"For temp=0.0, take the argmax to avoid divide-by-zero errors."
v3.0.2,keep_topk=1 is also equivalent to argmax.
v3.0.2,maybe fix some prediction at this step by modifying log_probs
v3.0.2,"shape: (sum(~ self.is_finished), 1)"
v3.0.2,in LM task src_len is associated with currently generated src
v3.0.2,and therefore needs to follow the generation
v3.0.2,!/usr/bin/env python
v3.0.2,for debugging
v3.0.2,TODO: maybe add dynamic part
v3.0.2,Statistics
v3.0.2,In the case of length_penalty = none we report the total logprobs
v3.0.2,divided by the number of sentence to get an approximation of the
v3.0.2,per sentence logprob. We also return the corresponding ppl
v3.0.2,"When a length_penalty is used eg: ""avg"" or ""wu"" since logprobs"
v3.0.2,are normalized per token we report the per line per token logprob
v3.0.2,"and the corresponding ""per word perplexity"""
v3.0.2,Turn any copied words into UNKs.
v3.0.2,"Decoder forward, takes [batch, tgt_len, nfeats] as input"
v3.0.2,"and [batch, src_len, hidden] as enc_out"
v3.0.2,"in case of inference tgt_len = 1, batch = beam times batch_size"
v3.0.2,"in case of Gold Scoring tgt_len = actual length, batch = 1 batch"
v3.0.2,Generator forward.
v3.0.2,"returns [(batch_size x beam_size) , vocab ] when 1 step"
v3.0.2,"or [batch_size, tgt_len, vocab ] when full sentence"
v3.0.2,"here we have scores [tgt_lenxbatch, vocab] or [beamxbatch, vocab]"
v3.0.2,at this point scores is batch first (dim=0)
v3.0.2,"returns [(batch_size x beam_size) , vocab ] when 1 step"
v3.0.2,"or [batch_size, tgt_len, vocab ] when full sentence"
v3.0.2,(0) add BOS and padding to tgt prediction
v3.0.2,(1) Encoder forward.
v3.0.2,(2) Repeat src objects `n_best` times.
v3.0.2,"We use batch_size x n_best, get ``(batch * n_best, src_len, nfeat)``"
v3.0.2,"(3) Init decoder with n_best src,"
v3.0.2,"reshape tgt to ``(len, batch * n_best, nfeat)``"
v3.0.2,it should be done in a better way
v3.0.2,here dec_in is batch first
v3.0.2,masked_select
v3.0.2,get aligned src id for each prediction's valid tgt tokens
v3.0.2,TODO: support these blacklisted features
v3.0.2,(0) Prep the components of the search.
v3.0.2,(1) Run the encoder on the src.
v3.0.2,(2) prep decode_strategy. Possibly repeat src objects.
v3.0.2,(3) Begin decoding step by step:
v3.0.2,"decoder_input = decode_strategy.current_predictions.view(1, -1,"
v3.0.2,1)
v3.0.2,Reorder states.
v3.0.2,TODO: support these blacklisted features
v3.0.2,(0) Prep the components of the search.
v3.0.2,(1) split src into src and target_prefix to avoid padding.
v3.0.2,(2) init decoder
v3.0.2,(3) prep decode_strategy. Possibly repeat src objects.
v3.0.2,(4) Begin decoding step by step:
v3.0.2,Reorder states.
v3.0.2,select indexes in model state/cache
v3.0.2,beam parameters
v3.0.2,beam state
v3.0.2,BoolTensor was introduced in pytorch 1.2
v3.0.2,"""global state"" of the old beam"
v3.0.2,buffers for the topk scores and 'backpointer'
v3.0.2,for testing
v3.0.2,maybe fix some prediction at this step by modifying log_probs
v3.0.2,Flatten probs into a list of possibilities.
v3.0.2,Penalize beams that finished.
v3.0.2,"on real data (newstest2017) with the pretrained transformer,"
v3.0.2,it's faster to not move this back to the original device
v3.0.2,Store finished hypotheses for this batch.
v3.0.2,End condition is the top beam finished and we can return
v3.0.2,n_best hypotheses.
v3.0.2,"If all sentences are translated, no need to go further."
v3.0.2,Remove finished batches for the next step.
v3.0.2,using integer division to get an integer _B without casting
v3.0.2,force the output to be longer than self.min_length
v3.0.2,Multiply probs by the beam probability.
v3.0.2,"if the sequence ends now, then the penalty is the current"
v3.0.2,"length + 1, to include the EOS token"
v3.0.2,Avoid any direction that would repeat unwanted ngrams
v3.0.2,Pick up candidate token by curr_scores
v3.0.2,Recover log probs.
v3.0.2,Length penalty is just a scalar. It doesn't matter if it's applied
v3.0.2,before or after the topk.
v3.0.2,Resolve beam origin and map to batch index flat representation.
v3.0.2,Append last prediction.
v3.0.2,update global state (step == 1)
v3.0.2,update global state (step > 1)
v3.0.2,"shape: (batch_size x beam_size, 1)"
v3.0.2,in LM task src_len is associated with currently generated src
v3.0.2,and therefore needs to follow the generation
v3.0.2,in LM task src_len is associated with currently generated src
v3.0.2,and therefore needs to follow the generation
v3.0.2,Term will be subtracted from probability
v3.0.2,Probability will be divided by this
v3.0.2,these warnings indicate that either the alpha/beta
v3.0.2,"forces a penalty to be a no-op, or a penalty is a no-op but"
v3.0.2,the alpha/beta would suggest otherwise.
v3.0.2,using some coverage penalty
v3.0.2,!/usr/bin/env python
v3.0.2,semaphore doesn't have a timeout arg in Python 2.7
v3.0.2,perform a first request to initialize everything
v3.0.2,backwards compatibility for confs
v3.0.2,every segment becomes a dict for flexibility purposes
v3.0.2,NOTE: translator returns lists of `n_best` list
v3.0.2,build back results with empty texts
v3.0.2,load can be called multiple times: modify copy
v3.0.2,output contain alignment
v3.0.2,Below are all the different penalty terms implemented so far.
v3.0.2,Subtract coverage penalty from topk log probs.
v3.0.2,Divide topk log probs by length penalty.
v3.0.2,Sorting
v3.0.2,src_raw = self.data.examples[inds[b]].src[0]
v3.0.2,Chinese segmentation
v3.0.2,Chinese simplify -> Chinese traditional standard
v3.0.2,Chinese simplify -> Chinese traditional (HongKong)
v3.0.2,Chinese simplify -> Chinese traditional (Taiwan)
v3.0.2,Chinese traditional -> Chinese simplify (v1)
v3.0.2,Chinese traditional -> Chinese simplify (v2)
v3.0.2,Auto import python files in this directory
v3.0.1,!/usr/bin/env python
v3.0.1,!/usr/bin/env python
v3.0.1,!/usr/bin/env python
v3.0.1,!/usr/bin/env python
v3.0.1,!/usr/bin/env python
v3.0.1,!/usr/bin/env python3
v3.0.1,-*- coding: utf-8 -*-
v3.0.1,
v3.0.1,"OpenNMT-py documentation build configuration file, created by"
v3.0.1,sphinx-quickstart on Sun Dec 17 12:07:14 2017.
v3.0.1,
v3.0.1,This file is execfile()d with the current directory set to its
v3.0.1,containing dir.
v3.0.1,
v3.0.1,Note that not all possible configuration values are present in this
v3.0.1,autogenerated file.
v3.0.1,
v3.0.1,All configuration values have a default; values that are commented out
v3.0.1,serve to show the default.
v3.0.1,"If extensions (or modules to document with autodoc) are in another directory,"
v3.0.1,add these directories to sys.path here. If the directory is relative to the
v3.0.1,"documentation root, use os.path.abspath to make it absolute, like shown here."
v3.0.1,
v3.0.1,import os
v3.0.1,import sys
v3.0.1,"sys.path.insert(0, os.path.abspath('.'))"
v3.0.1,-- General configuration ------------------------------------------------
v3.0.1,"If your documentation needs a minimal Sphinx version, state it here."
v3.0.1,
v3.0.1,needs_sphinx = '1.0'
v3.0.1,"Add any Sphinx extension module names here, as strings. They can be"
v3.0.1,extensions coming with Sphinx (named 'sphinx.ext.*') or your custom
v3.0.1,ones.
v3.0.1,Show base classes
v3.0.1,"Use ""variables"" section for Attributes instead of weird block things"
v3.0.1,mimicking the function style.
v3.0.1,"Add any paths that contain templates here, relative to this directory."
v3.0.1,The suffix(es) of source filenames.
v3.0.1,You can specify multiple suffix as a list of string:
v3.0.1,
v3.0.1,"source_suffix = ['.rst', '.md']"
v3.0.1,The master toctree document.
v3.0.1,General information about the project.
v3.0.1,"The version info for the project you're documenting, acts as replacement for"
v3.0.1,"|version| and |release|, also used in various other places throughout the"
v3.0.1,built documents.
v3.0.1,
v3.0.1,The short X.Y version.
v3.0.1,"The full version, including alpha/beta/rc tags."
v3.0.1,The language for content autogenerated by Sphinx. Refer to documentation
v3.0.1,for a list of supported languages.
v3.0.1,
v3.0.1,This is also used if you do content translation via gettext catalogs.
v3.0.1,"Usually you set ""language"" from the command line for these cases."
v3.0.1,"List of patterns, relative to source directory, that match files and"
v3.0.1,directories to ignore when looking for source files.
v3.0.1,This patterns also effect to html_static_path and html_extra_path
v3.0.1,The name of the Pygments (syntax highlighting) style to use.
v3.0.1,"If true, `todo` and `todoList` produce output, else they produce nothing."
v3.0.1,-- Options for HTML output ----------------------------------------------
v3.0.1,The theme to use for HTML and HTML Help pages.  See the documentation for
v3.0.1,a list of builtin themes.
v3.0.1,
v3.0.1,html_theme = 'sphinx_materialdesign_theme'
v3.0.1,html_theme_path = [sphinx_materialdesign_theme.get_path()]
v3.0.1,Theme options are theme-specific and customize the look and feel of a theme
v3.0.1,"further.  For a list of options available for each theme, see the"
v3.0.1,documentation.
v3.0.1,
v3.0.1,html_theme_options = {}
v3.0.1,"Add any paths that contain custom static files (such as style sheets) here,"
v3.0.1,"relative to this directory. They are copied after the builtin static files,"
v3.0.1,"so a file named ""default.css"" will overwrite the builtin ""default.css""."
v3.0.1,"Custom sidebar templates, must be a dictionary that maps document names"
v3.0.1,to template names.
v3.0.1,
v3.0.1,This is required for the alabaster theme
v3.0.1,refs: http://alabaster.readthedocs.io/en/latest/installation.html#sidebars
v3.0.1,-- Options for HTMLHelp output ------------------------------------------
v3.0.1,Output file base name for HTML help builder.
v3.0.1,-- Options for LaTeX output ---------------------------------------------
v3.0.1,The paper size ('letterpaper' or 'a4paper').
v3.0.1,
v3.0.1,"'papersize': 'letterpaper',"
v3.0.1,"The font size ('10pt', '11pt' or '12pt')."
v3.0.1,
v3.0.1,"'pointsize': '10pt',"
v3.0.1,Additional stuff for the LaTeX preamble.
v3.0.1,
v3.0.1,"'preamble': '',"
v3.0.1,Latex figure (float) alignment
v3.0.1,
v3.0.1,"'figure_align': 'htbp',"
v3.0.1,Grouping the document tree into LaTeX files. List of tuples
v3.0.1,"(source start file, target name, title,"
v3.0.1,"author, documentclass [howto, manual, or own class])."
v3.0.1,-- Options for manual page output ---------------------------------------
v3.0.1,One entry per manual page. List of tuples
v3.0.1,"(source start file, name, description, authors, manual section)."
v3.0.1,-- Options for Texinfo output -------------------------------------------
v3.0.1,Grouping the document tree into Texinfo files. List of tuples
v3.0.1,"(source start file, target name, title, author,"
v3.0.1,"dir menu entry, description, category)"
v3.0.1,!/usr/bin/env python
v3.0.1,-*- coding: utf-8 -*-
v3.0.1,is this reachable?
v3.0.1,Read in embeddings
v3.0.1,Write to file
v3.0.1,converts a SentencePiece vocabulary to the format expected by dynamic data
v3.0.1,"(essentially converts float expected counts to ""fixed precision"" int pseudo"
v3.0.1,counts)
v3.0.1,"Add in default model arguments, possibly added since training."
v3.0.1,this patch is no longer needed included in converter
v3.0.1,"if hasattr(model_opt, 'rnn_size'):"
v3.0.1,model_opt.hidden_size = model_opt.rnn_size
v3.0.1,build_base_model expects updated and validated opts
v3.0.1,-*- encoding: utf-8 -*-
v3.0.1,!/usr/bin/env python
v3.0.1,-*- coding: utf-8 -*-
v3.0.1,Author: Rico Sennrich
v3.0.1,flake8: noqa
v3.0.1,This file is retrieved from https://github.com/rsennrich/subword-nmt
v3.0.1,hack for python2/3 compatibility
v3.0.1,check version information
v3.0.1,some hacking to deal with duplicates (only consider first instance)
v3.0.1,don't print end-of-word symbols
v3.0.1,sys.stderr.write('cannot split {0} further.\n'.format(segment))
v3.0.1,sys.stderr.write('OOV: {0}\n'.format(segment))
v3.0.1,sys.stderr.write('OOV: {0}\n'.format(segment))
v3.0.1,python 2/3 compatibility
v3.0.1,read/write files as UTF-8
v3.0.1,!/usr/bin/env python3
v3.0.1,coding: utf-8
v3.0.1,"In order to use this tool, please install comet first"
v3.0.1,https://github.com/Unbabel/COMET
v3.0.1,Let's say you have a source file with N sentences in SL - eg: source.sl
v3.0.1,and the corresponding references (N sentences) reference.tl
v3.0.1,Translate your file in TL with the -n_best nbest options nbest being
v3.0.1,then number of hypotheses and output the target to -output target.nbest.tl
v3.0.1,Then you need to duplicate source and reference sentences nbest times
v3.0.1,for this script.
v3.0.1,for instance using awk '{for(i=1; i<=n; i++) print}' n=5 reference.tl \
v3.0.1,> reference.5.tl
v3.0.1,same for source.
v3.0.1,This script can be run (for instance with nbest = 5) as follows:
v3.0.1,python oracle_bleu.py --nbest-src source.5.sl --nbest-hyp target.5.tl \
v3.0.1,--nbest-ref reference.5.tl --nbest-order 5 --output target.maxbleu.tl
v3.0.1,It will search in all hyp the best comet score
v3.0.1,when choosing a reference-less model no nbest-ref is required
v3.0.1,for nbest in nbests:
v3.0.1,!/usr/bin/env python
v3.0.1,!/usr/bin/env python3
v3.0.1,coding: utf-8
v3.0.1,Let's say you have a source file with N sentences in SL - eg: source.sl
v3.0.1,Translate your file in TL with the -n_best nbest options nbest being
v3.0.1,then number of hypotheses and output the target to -output target.nbest.tl
v3.0.1,This script can be run (for instance with nbest = 5) as follows:
v3.0.1,python mbr_bleu.py --nbest-hyp target.5.tl \
v3.0.1,--nbest-order 5 --output target.mbr.tl
v3.0.1,It will compare all hyp with eachother and output the max bleu
v3.0.1,!/usr/bin/env python
v3.0.1,-*- coding: utf-8 -*-
v3.0.1,Author: Rico Sennrich
v3.0.1,flake8: noqa
v3.0.1,This file is retrieved from https://github.com/rsennrich/subword-nmt
v3.0.1,hack for python2/3 compatibility
v3.0.1,"find all instances of pair, and update frequency/indices around it"
v3.0.1,find first symbol
v3.0.1,"if first symbol is followed by second symbol, we've found an occurrence of pair (old_word[i:i+2])"
v3.0.1,"assuming a symbol sequence ""A B C"", if ""B C"" is merged, reduce the frequency of ""A B"""
v3.0.1,"assuming a symbol sequence ""A B C B"", if ""B C"" is merged, reduce the frequency of ""C B""."
v3.0.1,"however, skip this if the sequence is A B C B C, because the frequency of ""C B"" will be reduced by the previous code block"
v3.0.1,find new pair
v3.0.1,"assuming a symbol sequence ""A BC D"", if ""B C"" is merged, increase the frequency of ""A BC"""
v3.0.1,"assuming a symbol sequence ""A BC B"", if ""B C"" is merged, increase the frequency of ""BC B"""
v3.0.1,"however, if the sequence is A BC BC, skip this step because the count of ""BC BC"" will be incremented by the previous code block"
v3.0.1,data structure of pair frequencies
v3.0.1,index from pairs to words
v3.0.1,version 0.2 changes the handling of the end-of-word token ('</w>');
v3.0.1,version numbering allows bckward compatibility
v3.0.1,"threshold is inspired by Zipfian assumption, but should only affect speed"
v3.0.1,we probably missed the best pair because of pruning; go back to full statistics
v3.0.1,"threshold is inspired by Zipfian assumption, but should only affect speed"
v3.0.1,python 2/3 compatibility
v3.0.1,read/write files as UTF-8
v3.0.1,Now we can pipe the full file through the model using the Iterator
v3.0.1,reminder a batch includes .src .tgt .indices and it is sorted
v3.0.1,Compute and retrieve the loss for EACH sentence
v3.0.1,loss is returned normalized by tokens
v3.0.1,we unnormalize to cumulate at doc level
v3.0.1,Now we need to rearrange the batch of ppl
v3.0.1,in the original order with indices
v3.0.1,!/usr/bin/env python
v3.0.1,-*- coding: utf-8 -*-
v3.0.1,!/usr/bin/env python
v3.0.1,!/usr/bin/env python3
v3.0.1,coding: utf-8
v3.0.1,Let's say you have a source file with N sentences in SL - eg: source.sl
v3.0.1,and the corresponding references (N sentences) reference.tl
v3.0.1,Translate your file in TL with the -n_best nbest options nbest being
v3.0.1,then number of hypotheses and output the target to -output target.nbest.tl
v3.0.1,Then you need to duplicate reference sentences nbest times for this script.
v3.0.1,for instance using awk '{for(i=1; i<=n; i++) print}' n=5 reference.tl \
v3.0.1,> reference.5.tl
v3.0.1,This script can be run (for instance with nbest = 5) as follows:
v3.0.1,python oracle_bleu.py --nbest-hyp target.5.tl --nbest-ref reference.5.tl \
v3.0.1,--nbest-order 5 --output target.maxbleu.tl
v3.0.1,It will search in all hyp the best bleu wrt reference
v3.0.1,and output the max bleu
v3.0.1,!/usr/bin/env python
v3.0.1,with the two module = imp.load_source() below
v3.0.1,we ghost the old torchtext.data.field and depercated
v3.0.1,onmt.inputters.text_dataset
v3.0.1,however this require some functions / classes to be
v3.0.1,monkey patched for loading the old field/vocab objects.
v3.0.1,"module3 = imp.load_source(""Vocab"", ""tools/convertv2_v3.py"")"
v3.0.1,"voc = dict(sorted(fields.vocab.__dict__['freqs'].items(),"
v3.0.1,"key=lambda x: (-x[1], x[0]))).keys()"
v3.0.1,"voc = dict(sorted(fields.vocab.__dict__['freqs'].items(),"
v3.0.1,"key=lambda x: (-x[1], x[0]))).keys()"
v3.0.1,"this patch is no longer needed, included in converter"
v3.0.1,"if hasattr(model_opt, 'rnn_size'):"
v3.0.1,model_opt.hidden_size = model_opt.rnn_size
v3.0.1,Avoid functionality on inference
v3.0.1,Build embeddings.
v3.0.1,Build encoder.
v3.0.1,Build embeddings.
v3.0.1,Build decoder.
v3.0.1,Share the embedding matrix - preprocess with share_vocab required.
v3.0.1,src/tgt vocab should be the same if `-share_vocab` is specified.
v3.0.1,Update vocabulary embeddings with checkpoint embeddings
v3.0.1,Embedding layers
v3.0.1,Just for debugging purposes
v3.0.1,Remove old vocabulary associated embeddings
v3.0.1,for back compat when attention_dropout was not defined
v3.0.1,Build Model
v3.0.1,Build Generator.
v3.0.1,Load the model states from checkpoint or initialize them.
v3.0.1,This preserves backward-compat for models using customed layernorm
v3.0.1,end of patch for backward compatibility
v3.0.1,Update model embeddings with those from the checkpoint
v3.0.1,after initialization
v3.0.1,!/usr/bin/env python
v3.0.1,"maybe prepare pretrained embeddings, if any"
v3.0.1,Load checkpoint if we resume from a previous training.
v3.0.1,ensure tensorboard output is written in the directory
v3.0.1,of previous checkpoints
v3.0.1,Override checkpoint's update_embeddings as it defaults to false
v3.0.1,Override checkpoint's freezing settings as it defaults to false
v3.0.1,NOTE: It's important that ``opt`` has been validated and updated
v3.0.1,at this point.
v3.0.1,Build model.
v3.0.1,Build optimizer.
v3.0.1,Build model saver
v3.0.1,Use Tensorboard for visualization during training
v3.0.1,Options only during inference
v3.0.1,"Truncation options, for text corpus"
v3.0.1,"as for False, this will be added in _add_train_general_opts"
v3.0.1,Embedding Options
v3.0.1,Model Task Options
v3.0.1,Encoder-Decoder Options
v3.0.1,Freeze Encoder and/or Decoder
v3.0.1,The following options (bridge_extra_node to n_steps) are used
v3.0.1,for training with --encoder_type ggnn (Gated Graph Neural Network).
v3.0.1,Attention options
v3.0.1,Alignement options
v3.0.1,Generator and loss options.
v3.0.1,GPU
v3.0.1,Init options
v3.0.1,Pretrained word vectors
v3.0.1,Freeze word vectors
v3.0.1,Optimization options
v3.0.1,learning rate
v3.0.1,options relate to data preprare
v3.0.1,options relate to train
v3.0.1,Alpha and Beta values for Google Length + Coverage penalty
v3.0.1,"Described here: https://arxiv.org/pdf/1609.08144.pdf, Section 7"
v3.0.1,Length penalty options
v3.0.1,Coverage penalty options
v3.0.1,Decoding Length constraint
v3.0.1,Decoding content constraint
v3.0.1,Adding options relate to decoding strategy
v3.0.1,Adding option for logging
v3.0.1,Adding options related to Transforms
v3.0.1,Copyright 2016 The Chromium Authors. All rights reserved.
v3.0.1,Use of this source code is governed by a BSD-style license that can be
v3.0.1,found in the LICENSE file.
v3.0.1,"Get the key 'value' in the dict, or just use 'value'"
v3.0.1,Basic attributes.
v3.0.1,Set model in training mode.
v3.0.1,UPDATE DROPOUT
v3.0.1,Run patience mechanism
v3.0.1,"If the patience has reached the limit, stop training"
v3.0.1,swap model params w/ moving average
v3.0.1,(and keep the original parameters)
v3.0.1,Set model in validating mode.
v3.0.1,F-prop through the model.
v3.0.1,Compute loss.
v3.0.1,Compute validation metrics (at batch.dataset level)
v3.0.1,Compute stats
v3.0.1,Update statistics.
v3.0.1,Set model back to training mode.
v3.0.1,Truncated BPTT: reminder not compatible with accum > 1
v3.0.1,1. Create truncated target.
v3.0.1,2. F-prop all but generator.
v3.0.1,3. Compute loss.
v3.0.1,Compute and save stats
v3.0.1,in theory we should divide by accum_count and bptt
v3.0.1,to rescale for each sub batch
v3.0.1,4. Update the parameters and statistics.
v3.0.1,Multi GPU gradient gather
v3.0.1,"If truncated, don't backprop fully."
v3.0.1,"in case of multi step gradient accumulation,"
v3.0.1,update only after accum batches
v3.0.1,For Flake
v3.0.1,we avoid padding while mean pooling
v3.0.1,incoming and outgoing edge embedding
v3.0.1,Find vocab data for tree builting
v3.0.1,Propogation Model
v3.0.1,Initialize the bridge layer
v3.0.1,Token embedding
v3.0.1,Initialize graph using formatted input sequence
v3.0.1,Number of flagged nodes defines node count for this sample
v3.0.1,"(Nodes can have no flags on them, but must be in 'flags' list)."
v3.0.1,The total number of integers in the vocab should allow
v3.0.1,for all features and edges to be defined.
v3.0.1,Use first extra node as only source for decoder init
v3.0.1,Average all nodes to get bridge input
v3.0.1,"LSTM has hidden and cell state, other only one"
v3.0.1,Total number of states
v3.0.1,Build a linear layer for each
v3.0.1,Initialize the bridge layer
v3.0.1,src lengths data is wrapped inside a Tensor.
v3.0.1,"LSTM has hidden and cell state, other only one"
v3.0.1,Total number of states
v3.0.1,Build a linear layer for each
v3.0.1,batch x len x dim
v3.0.1,mask is now (batch x 1 x slen x slen)
v3.0.1,1 to be expanded to number of heads in MHA
v3.0.1,Run the forward pass of every layer of the tranformer.
v3.0.1,Dimensions and padding for constructing the word embedding matrix
v3.0.1,Dimensions and padding for feature embedding matrices
v3.0.1,(these have no effect if feat_vocab_sizes is empty)
v3.0.1,The embedding matrix look-up tables. The first look-up table
v3.0.1,"is for words. Subsequent ones are for features, if any exist."
v3.0.1,The final output size of word + feature vectors. This can vary
v3.0.1,from the word vector size if and only if features are defined.
v3.0.1,This is the attribute you should access if you need to know
v3.0.1,how big your embeddings are going to be.
v3.0.1,The sequence of operations that converts the input sequence
v3.0.1,into a sequence of embeddings. At minimum this consists of
v3.0.1,looking up the embeddings for each word and feature in the
v3.0.1,input. Model parameters may require the sequence to contain
v3.0.1,additional operations as well.
v3.0.1,features must use word_vec_size
v3.0.1,features will use feat_vec_size
v3.0.1,Some utilitary functions for pretrained embeddings
v3.0.1,is this reachable?
v3.0.1,Write to file
v3.0.1,set the opt in place
v3.0.1,set the opt in place
v3.0.1,flake8: noqa
v3.0.1,For command-line option parsing
v3.0.1,"Check pass, set the args."
v3.0.1,"This SRU version implements its own cuda-level optimization,"
v3.0.1,so it requires that:
v3.0.1,1. `cupy` and `pynvrtc` python package installed.
v3.0.1,2. pytorch is built with cuda support.
v3.0.1,3. library path set: export LD_LIBRARY_PATH=<cuda lib path>.
v3.0.1,Check 1.
v3.0.1,Check 2.
v3.0.1,Check 3.
v3.0.1,This sets up device to use.
v3.0.1,-> directions x batch x dim
v3.0.1,For DEBUG
v3.0.1,"size = (length, batch, x.size(-1)) \"
v3.0.1,"if x.dim() == 3 else (batch, x.size(-1))"
v3.0.1,grad_x = x.new(*x.size()) if k_ == 3 else x.new(*size).zero_()
v3.0.1,Normal use
v3.0.1,"An entry check here, will catch on train side and translate side"
v3.0.1,if requirements are not satisfied.
v3.0.1,RNNDecoderState wraps hidden as a tuple.
v3.0.1,fh -> (layers*directions) x batch x dim
v3.0.1,This class is mainly used by decoder.py for RNNs but also
v3.0.1,by the CNN / transformer decoder when copy attention is used
v3.0.1,CNN has its own attention mechanism ConvMultiStepAttention
v3.0.1,Transformer has its own MultiHeadedAttention
v3.0.1,mlp wants it with bias
v3.0.1,"(batch, t_len, d) x (batch, d, s_len) --> (batch, t_len, s_len)"
v3.0.1,"(batch, t_len, s_len, d)"
v3.0.1,one step input
v3.0.1,"compute attention scores, as in Luong et al."
v3.0.1,Softmax or sparsemax to normalize attention weights
v3.0.1,each context vector c_t is the weighted average
v3.0.1,over all the source hidden states
v3.0.1,concatenate
v3.0.1,clamping necessary because of numerical errors: loss should be lower
v3.0.1,"bounded by zero, but negative values near zero are possible without"
v3.0.1,the clamp
v3.0.1,Shift values to be >= 0
v3.0.1,class MultiHeadedAttention(torch.jit.ScriptModule):
v3.0.1,https://arxiv.org/pdf/1803.02155.pdf
v3.0.1,in the paper they suggest either two embeds
v3.0.1,relative_key / relative_value or only
v3.0.1,relative_key. We implemented the same embed
v3.0.1,for both.
v3.0.1,@torch.jit.script_method
v3.0.1,"1) Project key, value, and query."
v3.0.1,as a reminder at training layer_cache[0] remains False
v3.0.1,2) Calculate and scale scores.
v3.0.1,batch x num_heads x query_len x key_len
v3.0.1,1 or key_len x key_len
v3.0.1,1 or key_len x key_len x dim_per_head
v3.0.1,not 100% necessary but expand to nb of heads
v3.0.1,now mask and scores have the same shape
v3.0.1,3) Apply attention dropout and compute context vectors.
v3.0.1,We use the same embeddings for key and value
v3.0.1,At the moment this class is only used by embeddings.Embeddings look-up tables
v3.0.1,-*- coding: utf-8 -*-
v3.0.1,class AverageAttention(torch.jit.ScriptModule):
v3.0.1,@torch.jit.script
v3.0.1,out_features * in_features
v3.0.1,norm is out_features * 1
v3.0.1,batch_size * out_features
v3.0.1,out_features
v3.0.1,out_features
v3.0.1,batch_size * out_features
v3.0.1,"out_channels, in_channels // groups, * kernel_size"
v3.0.1,out_features
v3.0.1,This is used nowhere in the code at the moment (Vincent Nguyen 05/18/2018)
v3.0.1,"in_channels, out_channels, *kernel_size"
v3.0.1,"in_channels, out_channels, *kernel_size"
v3.0.1,"self.out_channels, 1"
v3.0.1,out_features
v3.0.1,out_features
v3.0.1,store roots on diagonal
v3.0.1,Original probabilities.
v3.0.1,Probability of copying p(z=1) batch.
v3.0.1,Probability of not copying: p_{word}(w) * (1 - p(z))
v3.0.1,probabilities assigned by the model to the gold targets
v3.0.1,probability of tokens copied from source
v3.0.1,Set scores for unk to 0 and add eps
v3.0.1,find the indices in which you do not use the copy mechanism
v3.0.1,Drop padding.
v3.0.1,Do nothing
v3.0.1,Do nothing
v3.0.1,Punctuation only
v3.0.1,Auto import python files in this directory
v3.0.1,1. sample number of tokens to corrupt
v3.0.1,2. sample positions to corrput
v3.0.1,3. sample corrupted values
v3.0.1,1. sample number of tokens to corrupt
v3.0.1,2. sample positions to corrput
v3.0.1,3. Drop token on chosen position
v3.0.1,1. sample number of tokens to corrupt
v3.0.1,2. sample positions to corrput
v3.0.1,3. mask word on chosen position
v3.0.1,"Sharing options among `TokenizerTransform`s, same name conflict in"
v3.0.1,this scope will be resolved by remove previous occurrence in parser
v3.0.1,subword regularization(or BPE dropout) options:
v3.0.1,subword vocabulary restriction options:
v3.0.1,derterministic subwording
v3.0.1,subword sampling when nbest_size > 1 or -1
v3.0.1,alpha should be 0.0 < alpha < 1.0
v3.0.1,Load vocabulary file if provided and set threshold
v3.0.1,Load Subword Model
v3.0.1,-1: keep everything (i.e. 1 mask per token)
v3.0.1,0: replace everything (i.e. no mask)
v3.0.1,1: 1 mask per span
v3.0.1,view each subword as word start / input is word level token
v3.0.1,Pretend it ends with a full stop so last span is a sentence
v3.0.1,"Tokens that are full stops, where the previous token is not"
v3.0.1,Make sure we have enough to mask
v3.0.1,Trim to masking budget
v3.0.1,Handle 0-length mask (inserts) separately
v3.0.1,assert is_word_start[-1] == 0
v3.0.1,assert tokens_length - 1 not in indices
v3.0.1,"keep index, but replace it with [MASK]"
v3.0.1,"acts as a long length, so spans don't go over the end of doc"
v3.0.1,next position from each word_start
v3.0.1,delete token: 1 mask/remove per span
v3.0.1,"keep index, but replace it with [MASK]: 1 mask per token"
v3.0.1,A bit faster when all lengths are 1
v3.0.1,to cover whole token
v3.0.1,delete token
v3.0.1,"keep index, but replace it with [MASK]"
v3.0.1,assert tokens_length - 1 not in indices
v3.0.1,batch 0 will always predict EOS. The other batches will predict
v3.0.1,non-eos scores.
v3.0.1,"""best"" prediction is eos - that should be blocked"
v3.0.1,include at least one prediction OTHER than EOS
v3.0.1,that is greater than -1e20
v3.0.1,now batch 0 has ended and no others have
v3.0.1,initial step
v3.0.1,batch 0 dies on step 0
v3.0.1,include at least one prediction OTHER than EOS
v3.0.1,that is greater than -1e20
v3.0.1,step 2
v3.0.1,(old) batch 8 dies on step 1
v3.0.1,step 3
v3.0.1,everything dies
v3.0.1,initial step
v3.0.1,batch 0 dies on step 0
v3.0.1,include at least one prediction OTHER than EOS
v3.0.1,that is greater than -1e20
v3.0.1,step 2
v3.0.1,(old) batch 8 dies on step 1
v3.0.1,step 3
v3.0.1,everything dies
v3.0.1,initial step
v3.0.1,finish one beam
v3.0.1,include at least one prediction OTHER than EOS
v3.0.1,that is greater than -1e20
v3.0.1,step 2
v3.0.1,finish example in last batch
v3.0.1,(old) batch 8 dies on step 1
v3.0.1,step 3
v3.0.1,everything dies
v3.0.1,initial step
v3.0.1,batch 0 dies on step 0
v3.0.1,include at least one prediction OTHER than EOS
v3.0.1,that is greater than -1e20
v3.0.1,step 2
v3.0.1,(old) batch 8 dies on step 1
v3.0.1,step 3
v3.0.1,everything dies
v3.0.1,illegal_weights_mask = torch.ByteTensor([
v3.0.1,"[0, 0, 0, 0, 0, 0, 0],"
v3.0.1,"[0, 0, 0, 1, 1, 1, 1],"
v3.0.1,"[0, 0, 0, 0, 0, 1, 1],"
v3.0.1,"[0, 0, 1, 1, 1, 1, 1]])"
v3.0.1,TODO: fix for pytorch 0.3
v3.0.1,illegal_weights = alignments.masked_select(illegal_weights_mask)
v3.0.1,"self.assertEqual(0.0, illegal_weights.data.sum())"
v3.0.1,this could be considered an integration test because it touches
v3.0.1,the filesystem for the config file (and the models)
v3.0.1,no dummy prefix
v3.0.1,no dummy prefix
v3.0.1,transforms that require vocab will not create if not provide vocab
v3.0.1,1. Init first transform in the pipe
v3.0.1,2. Init second transform in the pipe
v3.0.1,3. Sequential combine them into a transform pipe
v3.0.1,4. apply transform pipe for example
v3.0.1,"5. example after the pipe exceed the length limit, thus filtered"
v3.0.1,6. Transform statistics registed (here for filtertoolong)
v3.0.1,"7. after report, statistics become empty as a fresh start"
v3.0.1,filter_transform.warm_up()
v3.0.1,test BPE-dropout:
v3.0.1,1. disable bpe dropout for not training example
v3.0.1,2. enable bpe dropout for training example
v3.0.1,3. (NOTE) disable dropout won't take effect if already seen
v3.0.1,this is caused by the cache mechanism in bpe:
v3.0.1,return cached subword if the original token is seen when no dropout
v3.0.1,test SP regularization:
v3.0.1,1. enable regularization for training example
v3.0.1,2. disable regularization for not training example
v3.0.1,Not apply token drop for not training example
v3.0.1,apply token drop for training example
v3.0.1,Not apply token mask for not training example
v3.0.1,apply token mask for training example
v3.0.1,require vocabs to warm_up
v3.0.1,Not apply token mask for not training example
v3.0.1,apply token mask for training example
v3.0.1,"Defalt: full_stop_token=[""."", ""?"", ""!""]"
v3.0.1,"Defalt: full_stop_token=[""."", ""?"", ""!""]"
v3.0.1,random_ratio of inserted tokens are chosen in vocab
v3.0.1,others are MASK_TOK
v3.0.1,"insert_ratio=0.0,"
v3.0.1,"random_ratio=0.0,"
v3.0.1,"Defalt: full_stop_token=[""."", ""?"", ""!""]"
v3.0.1,all token are considered as an individual word
v3.0.1,1. tokens are dropped when replace_length is 0
v3.0.1,"print(f""token delete: {masked} / {tokens}"")"
v3.0.1,2. tokens are replaced by MASK when replace_length is 1
v3.0.1,"print(f""token mask: {masked} / {tokens}"")"
v3.0.1,"insert_ratio=0.0,"
v3.0.1,"random_ratio=0.0,"
v3.0.1,"Defalt: full_stop_token=[""."", ""?"", ""!""]"
v3.0.1,start token of word are identified using subword marker
v3.0.1,"1. replace_length 0: ""words"" are dropped"
v3.0.1,"print(f""word delete: {masked} / {tokens}"")"
v3.0.1,"self.assertEqual(len(masked), n_words - n_masked)"
v3.0.1,"2. replace_length 1: ""words"" are replaced with a single MASK"
v3.0.1,"print(f""whole word single mask: {masked} / {tokens}"")"
v3.0.1,len(masked) depend on number of tokens in select word
v3.0.1,"3. replace_length -1: all tokens in ""words"" are replaced with MASK"
v3.0.1,"print(f""whole word multi mask: {masked} / {tokens}"")"
v3.0.1,number of mask_tok depend on number of tokens in selected word
v3.0.1,number of MASK_TOK can be greater than n_masked
v3.0.1,"insert_ratio=0.5,"
v3.0.1,"random_ratio=0.3,"
v3.0.1,"Defalt: full_stop_token=[""."", ""?"", ""!""]"
v3.0.1,start token of word are identified using subword marker
v3.0.1,n_words = sum(token_starts)
v3.0.1,n_masked = math.ceil(n_words * bart_noise.mask_ratio)
v3.0.1,"print(f""Text Span Infilling: {infillied} / {tokens}"")"
v3.0.1,"print(n_words, n_masked)"
v3.0.1,!/usr/bin/env python
v3.0.1,-*- coding: utf-8 -*-
v3.0.1,Inject some dummy training options that may needed when build fields
v3.0.1,Remove the generated *pt files.
v3.0.1,Remove the generated data samples
v3.0.1,all beams repeat (beam >= 1 repeat dummy scores)
v3.0.1,predict repeat_idx over and over again
v3.0.1,"before repeat, scores are either 0 or -inf"
v3.0.1,"on repeat, `repeat_idx` score is BLOCKED_SCORE"
v3.0.1,"(but it's still the best score, thus we have"
v3.0.1,"[BLOCKED_SCORE, -inf, -inf, -inf, -inf]"
v3.0.1,repetitions keeps maximizing score
v3.0.1,"index 0 has been blocked, so repeating=>+0.0 score"
v3.0.1,other indexes are -inf so repeating=>BLOCKED_SCORE
v3.0.1,which is higher
v3.0.1,beam 0 and beam >=2 will repeat (beam >= 2 repeat dummy scores)
v3.0.1,non-interesting beams are going to get dummy values
v3.0.1,"on initial round, only predicted scores for beam 0"
v3.0.1,matter. Make two predictions. Top one will be repeated
v3.0.1,"in beam zero, second one will live on in beam 1."
v3.0.1,predict the same thing in beam 0
v3.0.1,continue pushing around what beam 1 predicts
v3.0.1,"now beam 0 dies (along with the others), beam 1 -> beam 0"
v3.0.1,"now beam 0 dies (along with the others), beam 1 -> beam 0"
v3.0.1,beam 0 and beam >= 2 will repeat (beam 2 repeats excluded idx)
v3.0.1,non-interesting beams are going to get dummy values
v3.0.1,predict the same thing in beam 0
v3.0.1,continue pushing around what beam 1 predicts
v3.0.1,predict the allowed-repeat again in beam 2
v3.0.1,"now beam 0 dies, beam 1 -> beam 0, beam 2 -> beam 1"
v3.0.1,and the rest die
v3.0.1,"since all preds after i=0 are 0, we can check"
v3.0.1,that the beam is the correct idx by checking that
v3.0.1,the curr score is the initial score
v3.0.1,beam 0 will always predict EOS. The other beams will predict
v3.0.1,non-eos scores.
v3.0.1,non-interesting beams are going to get dummy values
v3.0.1,"""best"" prediction is eos - that should be blocked"
v3.0.1,include at least beam_sz predictions OTHER than EOS
v3.0.1,that are greater than -1e20
v3.0.1,predict eos in beam 0
v3.0.1,provide beam_sz other good predictions
v3.0.1,now the top beam has ended and no others have
v3.0.1,"not of interest, but want to make sure it keeps running"
v3.0.1,since only beam 0 terminates and n_best = 2
v3.0.1,"this is also a test that when block_ngram_repeat=0,"
v3.0.1,repeating is acceptable
v3.0.1,non-interesting beams are going to get dummy values
v3.0.1,"""best"" prediction is eos - that should be blocked"
v3.0.1,include at least beam_sz predictions OTHER than EOS
v3.0.1,that are greater than -1e20
v3.0.1,predict eos in beam 1
v3.0.1,provide beam_sz other good predictions in other beams
v3.0.1,beam 1 dies on min_length
v3.0.1,beam 0 dies on the step after beam 1 dies
v3.0.1,"inp_lens is tiled in initialize, reassign to make attn match"
v3.0.1,non-interesting beams are going to get dummy values
v3.0.1,"""best"" prediction is eos - that should be blocked"
v3.0.1,include at least beam_sz predictions OTHER than EOS
v3.0.1,that are greater than -1e20
v3.0.1,predict eos in beam 1
v3.0.1,provide beam_sz other good predictions in other beams
v3.0.1,no top beams are finished yet
v3.0.1,beam 1 dies on min_length
v3.0.1,no top beams are finished yet
v3.0.1,beam 0 dies on the step after beam 1 dies
v3.0.1,top beam is finished now so there are attentions
v3.0.1,two beams are finished in each batch
v3.0.1,second dim is cut down to the non-padded src length
v3.0.1,first dim is equal to the time of death
v3.0.1,(beam 0 died at current step - adjust for SOS)
v3.0.1,(beam 1 died at last step - adjust for SOS)
v3.0.1,behavior gets weird when beam is already done so just stop
v3.0.1,this is just test_beam.TestBeamAgainstReferenceCase repeated
v3.0.1,in each batch.
v3.0.1,"init_preds: [4, 3, 5, 6, 7] - no EOS's"
v3.0.1,no EOS's yet
v3.0.1,"[5, 3, 2, 6, 0], so beam 2 predicts EOS!"
v3.0.1,assumes beam 2 finished on last step
v3.0.1,ended beam 2 shouldn't continue
v3.0.1,"[2, 5, 3, 6, 0] repeat self.BATCH_SZ, so beam 0 predicts EOS!"
v3.0.1,"[-2.4879, -3.8910, -4.1010, -4.2010, -4.4010] repeat self.BATCH_SZ"
v3.0.1,another beam is finished in all batches
v3.0.1,new beam 0 finished
v3.0.1,new beam 0 is old beam 3
v3.0.1,assumes beam 0 finished on last step
v3.0.1,"[5, 2, 6, 1, 0] repeat self.BATCH_SZ, so beam 1 predicts EOS!"
v3.0.1,we finish 3 hyps per example in this step
v3.0.1,new beam 1 is old beam 3
v3.0.1,this could be considered an integration test because it tests
v3.0.1,interactions between the GNMT scorer and the beam
v3.0.1,"-data option is required, but not used in this test, so dummy."
v3.0.1,len x batch x nfeat
v3.0.1,Initialize vectors to compare size with
v3.0.1,Ensure correct sizes and types
v3.0.1,Make sure that output has the correct size and type
v3.0.1,"[('encoder_type', 'transformer'),"
v3.0.1,"('word_vec_size', 16), ('hidden_size', 16)],"
v3.0.1,""""""" Only do SRU test if requirment is safisfied. """""""
v3.0.1,SRU doesn't support input_feed.
v3.0.1,first check there's nothing unexpectedly not trainable
v3.0.1,ok: word embeddings shouldn't be trainable
v3.0.1,if word vecs are freezed
v3.0.1,ok: positional encodings shouldn't be trainable
v3.0.1,then check nothing unexpectedly trainable
v3.0.1,Decoder state
v3.0.1,Build the RNN.
v3.0.1,Set up the context gate.
v3.0.1,Set up the standard attention.
v3.0.1,The encoder hidden is  (layers*directions) x batch x dim.
v3.0.1,We need to convert it to layers x batch x (directions*dim).
v3.0.1,Init the input feed.
v3.0.1,Update the state with the result.
v3.0.1,Concatenates sequence of tensors along a new dimension.
v3.0.1,NOTE: v0.3 to 0.4: dec_outs / attns[*] may not be list
v3.0.1,(in particular in case of SRU) it was not raising error in 0.3
v3.0.1,since stack(Variable) was allowed.
v3.0.1,"In 0.4, SRU returns a tensor that shouldn't be stacke"
v3.0.1,Calculate the attention.
v3.0.1,Calculate the context gate.
v3.0.1,Additional args check.
v3.0.1,Input feed concatenates hidden state with
v3.0.1,input at every time step.
v3.0.1,TODO: context gate should be employed
v3.0.1,instead of second RNN transform.
v3.0.1,Update the coverage attention.
v3.0.1,Decoder State
v3.0.1,CNNDecoder has its own attention mechanism.
v3.0.1,Set up a separate copy attention layer if needed.
v3.0.1,The output of CNNEncoder.
v3.0.1,The combination of output of CNNEncoder and source embeddings.
v3.0.1,Process the result and update the attentions.
v3.0.1,Update the state.
v3.0.1,TODO change the way attns is returned dict => list or tuple (onnx)
v3.0.1,src_len is a single tensor shared between all models.
v3.0.1,This assumption will not hold if Translator is modified
v3.0.1,to calculate src_len as something other than the length
v3.0.1,of the input.
v3.0.1,"return _, (B, Q_len, K_len)"
v3.0.1,"layer average attention across heads, get ``(B, Q, K)``"
v3.0.1,"Case 1: no full_context, no align heads -> layer avg baseline"
v3.0.1,"Case 2: no full_context, 1 align heads -> guided align"
v3.0.1,"Case 3: full_context, 1 align heads -> full cte guided align"
v3.0.1,BoolTensor was introduced in pytorch 1.2
v3.0.1,T: could be 1 in the case of stepwise decoding or tgt_len
v3.0.1,masking is necessary when sequence length is greater than one
v3.0.1,mask now are (batch x 1 x tlen x s or t len)
v3.0.1,1 = heads to be expanded in MHA
v3.0.1,Decoder State
v3.0.1,"previously, there was a GlobalAttention module here for copy"
v3.0.1,"attention. But it was never actually used -- the ""copy"" attention"
v3.0.1,just reuses the context attention.
v3.0.1,"attns[""align""] = torch.stack(attn_aligns, 0).mean(0)  # All avg"
v3.0.1,TODO change the way attns is returned dict => list or tuple (onnx)
v3.0.1,first value set to True triggered by the beginning of decoding
v3.0.1,layer_cache becomes active in the MultiHeadedAttention fwd
v3.0.1,T: could be 1 in the case of stepwise decoding or tgt_len
v3.0.1,masking is necessary when sequence length is greater than one
v3.0.1,mask now are (batch x 1 x tlen x tlen)
v3.0.1,1 = heads to be expanded in MHA
v3.0.1,TODO change the way attns is returned dict => list or tuple (onnx)
v3.0.1,"buffer size in bytes, determine equiv. # of elements based on data type"
v3.0.1,copy tensors into buffer_t
v3.0.1,all-reduce and rescale
v3.0.1,copy all-reduced buffer back into tensors
v3.0.1,"print(filled, sz)"
v3.0.1,"tensor is bigger than buffer, all-reduce and rescale directly"
v3.0.1,"buffer is full, all-reduce and replace buffer with grad"
v3.0.1,add tensor to buffer
v3.0.1,"propagate exception to parent process, keeping original traceback"
v3.0.1,TODO: Find a better way to check for sparse gradients.
v3.0.1,we use apex.amp
v3.0.1,In this case use the old FusedAdam with
v3.0.1,FP16_optimizer wrapper
v3.0.1,Load everything from the checkpoint.
v3.0.1,Build everything from scratch.
v3.0.1,"Reset optimizer, keep options."
v3.0.1,"Reset options, keep optimizer."
v3.0.1,State can be partially restored.
v3.0.1,should be: self._optimizer.zero_grad(set_to_none)
v3.0.1,but apex.amp is not up-to-date:
v3.0.1,https://github.com/NVIDIA/apex/blob/master/apex/amp/_process_optimizer.py#L367
v3.0.1,"unscaled optimizer's gradients (already done therefore skip),"
v3.0.1,skips optimizer.step() if gradients contain infs/NaNs.
v3.0.1,Updates the scale for next iteration.
v3.0.1,Code below is an implementation of https://arxiv.org/pdf/1804.04235.pdf
v3.0.1,inspired but modified from https://github.com/DeadAt0m/adafactor-pytorch
v3.0.1,backward compatibility
v3.0.1,assuming a list/generator of parameter means single group
v3.0.1,compute combined scale factor for this group
v3.0.1,norm is in fact norm*scale
v3.0.1,note: p.grad should not ever be set for correct operation of
v3.0.1,mixed precision optimizer that sometimes sends None gradients
v3.0.1,State initialization
v3.0.1,Exponential moving average of gradient values
v3.0.1,Exponential moving average of squared gradient values
v3.0.1,-*- coding: utf-8 -*-
v3.0.1,if the loss function operates on vectors of raw logits instead
v3.0.1,"of probabilities, only the first part of the generator needs to"
v3.0.1,"be passed to the NMTLossCompute. At the moment, the only"
v3.0.1,supported loss function of this kind is the sparsemax loss.
v3.0.1,"align_head contains value in [0, 1) presenting attn prob,"
v3.0.1,0 was resulted by the context attention src_pad_mask
v3.0.1,"So, the correspand position in ref_align should also be 0"
v3.0.1,"Therefore, clip align_head to > 1e-18 should be bias free."
v3.0.1,"we use the raw logits, rescale with tau (temperature) and"
v3.0.1,apply the log_softmax. reminder generator[0] is just the nn.Linear
v3.0.1,ct2 expects src with lengths without padding
v3.0.1,again we use raw probs to rescale with tau and apply log_softmax
v3.0.1,lm_scores are in log space so log_target=True
v3.0.1,"we use the raw logits, rescale with tau (temperature) and"
v3.0.1,apply the log_softmax. reminder generator[0] is just the nn.Linear
v3.0.1,ct2 expects src with lengths without padding
v3.0.1,again we use raw probs to rescale with tau and apply log_softmax
v3.0.1,lm_scores are in log space so log_target=True
v3.0.1,take into account here the tgt_shift_index (0 / 1 = LM/NMT)
v3.0.1,Correct target copy token instead of <unk>
v3.0.1,tgt[i] = align[i] + len(tgt_vocab)
v3.0.1,for i such that tgt[i] == 0 and align[i] != 0
v3.0.1,in the case criterion reduction is None then we need
v3.0.1,to sum the loss of each sentence in the batch
v3.0.1,Check Transforms
v3.0.1,Check path
v3.0.1,tgt is src for LM task
v3.0.1,Check prefix: will be used when use prefix transform
v3.0.1,Check weight
v3.0.1,Check features
v3.0.1,validation when train:
v3.0.1,Check embeddings stuff
v3.0.1,"Backward compatibility with ""fix_word_vecs_*"" opts"
v3.0.1,encoder and decoder should be same sizes
v3.0.1,"Load default opt values, then overwrite with the opts in"
v3.0.1,"the checkpoint. That way, if there are new options added,"
v3.0.1,the defaults are used.
v3.0.1,It comes from training
v3.0.1,TODO: needs to be added as inference opt
v3.0.1,Don't do anything
v3.0.1,Update best score of each criteria
v3.0.1,Reset tolerance
v3.0.1,Update current status
v3.0.1,Decrease tolerance
v3.0.1,Log
v3.0.1,Log
v3.0.1,Get a list of world_size lists with len(stat_list) Statistics objects
v3.0.1,"this param init is overridden by model_builder, useless then."
v3.0.1,SRU doesn't support PackedSequence.
v3.0.1,-*- coding: utf-8 -*-
v3.0.1,threshold on 1 to avoid div by 0
v3.0.1,treat alignment matrix one by one as each have different lengths
v3.0.1,No alignment if not exist valid tgt token
v3.0.1,get valid alignment (sub-matrix from full paded aligment matrix)
v3.0.1,Helper functions
v3.0.1,Keeps track of the original words/subwords
v3.0.1,('prior_tokenization' option)
v3.0.1,In case there is a final case_markup when new_spacer is on
v3.0.1,batch_side.shape[0] sentences to rebuild
v3.0.1,batch_side.shape[1] tokens per sentence
v3.0.1,we deactivate the decoder's cache
v3.0.1,as we use teacher forcing at training time.
v3.0.1,-*- coding: utf-8 -*-
v3.0.1,this one is needed for Random Shuffler of batches
v3.0.1,in multi gpu it ensures datasets are read in the same order
v3.0.1,some cudnn methods can be random even after fixing the seed
v3.0.1,unless you tell it to be deterministic
v3.0.1,This one is needed for various tranfroms
v3.0.1,These ensure same initialization in multi gpu mode
v3.0.1,we need to check the model path + any tokenizer path
v3.0.1,bucket_size = batch_size
v3.0.1,We only support
v3.0.1,For TRAIN we need to group examples by length
v3.0.1,"for faster performance, but otherwise, sequential."
v3.0.1,For TRAIN we shuffle batches within the bucket
v3.0.1,otherwise sequential
v3.0.1,for specific case of rnn_packed need to be sorted
v3.0.1,within the batch
v3.0.1,Maintains the longest src and tgt length in the current batch
v3.0.1,Reset current longest length at a new batch (count=1)
v3.0.1,Src: [<bos> w1 ... wN <eos>]
v3.0.1,Tgt: [w1 ... wM <eos>]
v3.0.1,this is a hack: appears quicker to apply it here
v3.0.1,than in the ParallelCorpusIterator
v3.0.1,Make features part of src like
v3.0.1,"{'src': {'src': ..., 'feat1': ...., 'feat2': ....}}"
v3.0.1,at this point an example looks like:
v3.0.1,"{'src': {'src': ..., 'feat1': ...., 'feat2': ....},"
v3.0.1,"'tgt': {'tgt': ...},"
v3.0.1,"'src_original': ['tok1', ...'tokn'],"
v3.0.1,"'tgt_original': ['tok1', ...'tokm'],"
v3.0.1,'indices' : seq in bucket
v3.0.1,"'align': ...,"
v3.0.1,}
v3.0.1,we'll need to change this if we introduce tgt feat
v3.0.1,Need to add features in last dimensions
v3.0.1,Need to add features also in 'src'
v3.0.1,make a small vocab containing just the tokens in the source sequence
v3.0.1,Map source tokens to indices in the dynamic dict.
v3.0.1,-*- coding: utf-8 -*-
v3.0.1,'src_original' and 'tgt_original' store the
v3.0.1,original line before tokenization. These
v3.0.1,fields are used later on in the feature
v3.0.1,transforms.
v3.0.1,NOTE: moved to dynamic_iterator.py cf process()
v3.0.1,item = self.transform.apply(
v3.0.1,"example, is_train=self.infinitely, corpus_name=self.cid)"
v3.0.1,empty example: skip
v3.0.1,"No encoder in LM, seq2seq count formatting kept"
v3.0.1,_check_save_model_path
v3.0.1,NOTE: We need to trim the vocab to remove any unk tokens that
v3.0.1,were not originally here.
v3.0.1,"for side in [""src"", ""tgt""]:"
v3.0.1,keys_to_pop = []
v3.0.1,"if hasattr(vocab[side], ""fields""):"
v3.0.1,unk_token = vocab[side].fields[0][1].vocab.itos[0]
v3.0.1,"for key, value in vocab[side].fields[0][1].vocab.stoi.items():"
v3.0.1,if value == 0 and key != unk_token:
v3.0.1,keys_to_pop.append(key)
v3.0.1,for key in keys_to_pop:
v3.0.1,"vocab[side].fields[0][1].vocab.stoi.pop(key, None)"
v3.0.1,!/usr/bin/env python
v3.0.1,!/usr/bin/env python
v3.0.1,!/usr/bin/env python
v3.0.1,-*- coding: utf-8 -*-
v3.0.1,!/usr/bin/env python
v3.0.1,Just for debugging purposes
v3.0.1,It appends features to subwords when dumping to file
v3.0.1,!/usr/bin/env python
v3.0.1,!/usr/bin/env python
v3.0.1,Set sharing strategy manually instead of default based on the OS.
v3.0.1,torch.multiprocessing.set_sharing_strategy('file_system')
v3.0.1,Create a thread to listen for errors in the child processes.
v3.0.1,Train with multiprocessing.
v3.0.1,magic indices
v3.0.1,result caching
v3.0.1,fix length constraint and remove eos from count
v3.0.1,add one to account for BOS. Don't account for EOS because hitting
v3.0.1,this implies it hasn't been found.
v3.0.1,we don't block nothing if the user doesn't want it
v3.0.1,we can't block nothing beam's too short
v3.0.1,we check paths one by one
v3.0.1,we don't forbid nothing if the user doesn't want it
v3.0.1,we can't forbid nothing if beam's too short
v3.0.1,Reordering forbidden_tokens following beam selection
v3.0.1,We rebuild a dict to ensure we get the value and not the pointer
v3.0.1,Grabing the newly selected tokens and associated ngram
v3.0.1,skip the blocking if any token in current_ngram is excluded
v3.0.1,"pickups: Tensor where specified index were set to 1, others 0"
v3.0.1,"dropdowns: opposite of pickups, 1 for those shouldn't pick"
v3.0.1,Minus dropdowns to log_probs making probabilities of
v3.0.1,unspecified index close to 0
v3.0.1,"prediction step have surpass length of given target_prefix,"
v3.0.1,no need to further change this attr
v3.0.1,keep indices until overflowing p
v3.0.1,Set all logits that are not in the top-p to -10000.
v3.0.1,This puts the probabilities close to 0.
v3.0.1,Set all logits that are not in the top-k to -10000.
v3.0.1,This puts the probabilities close to 0.
v3.0.1,"For temp=0.0, take the argmax to avoid divide-by-zero errors."
v3.0.1,keep_topk=1 is also equivalent to argmax.
v3.0.1,maybe fix some prediction at this step by modifying log_probs
v3.0.1,"shape: (sum(~ self.is_finished), 1)"
v3.0.1,in LM task src_len is associated with currently generated src
v3.0.1,and therefore needs to follow the generation
v3.0.1,!/usr/bin/env python
v3.0.1,for debugging
v3.0.1,TODO: maybe add dynamic part
v3.0.1,Statistics
v3.0.1,In the case of length_penalty = none we report the total logprobs
v3.0.1,divided by the number of sentence to get an approximation of the
v3.0.1,per sentence logprob. We also return the corresponding ppl
v3.0.1,"When a length_penalty is used eg: ""avg"" or ""wu"" since logprobs"
v3.0.1,are normalized per token we report the per line per token logprob
v3.0.1,"and the corresponding ""per word perplexity"""
v3.0.1,Turn any copied words into UNKs.
v3.0.1,"Decoder forward, takes [batch, tgt_len, nfeats] as input"
v3.0.1,"and [batch, src_len, hidden] as enc_out"
v3.0.1,"in case of inference tgt_len = 1, batch = beam times batch_size"
v3.0.1,"in case of Gold Scoring tgt_len = actual length, batch = 1 batch"
v3.0.1,Generator forward.
v3.0.1,"returns [(batch_size x beam_size) , vocab ] when 1 step"
v3.0.1,"or [batch_size, tgt_len, vocab ] when full sentence"
v3.0.1,"here we have scores [tgt_lenxbatch, vocab] or [beamxbatch, vocab]"
v3.0.1,at this point scores is batch first (dim=0)
v3.0.1,"returns [(batch_size x beam_size) , vocab ] when 1 step"
v3.0.1,"or [batch_size, tgt_len, vocab ] when full sentence"
v3.0.1,(0) add BOS and padding to tgt prediction
v3.0.1,(1) Encoder forward.
v3.0.1,(2) Repeat src objects `n_best` times.
v3.0.1,"We use batch_size x n_best, get ``(batch * n_best, src_len, nfeat)``"
v3.0.1,"(3) Init decoder with n_best src,"
v3.0.1,"reshape tgt to ``(len, batch * n_best, nfeat)``"
v3.0.1,it should be done in a better way
v3.0.1,here dec_in is batch first
v3.0.1,masked_select
v3.0.1,get aligned src id for each prediction's valid tgt tokens
v3.0.1,TODO: support these blacklisted features
v3.0.1,(0) Prep the components of the search.
v3.0.1,(1) Run the encoder on the src.
v3.0.1,(2) prep decode_strategy. Possibly repeat src objects.
v3.0.1,(3) Begin decoding step by step:
v3.0.1,"decoder_input = decode_strategy.current_predictions.view(1, -1,"
v3.0.1,1)
v3.0.1,Reorder states.
v3.0.1,TODO: support these blacklisted features
v3.0.1,(0) Prep the components of the search.
v3.0.1,(1) split src into src and target_prefix to avoid padding.
v3.0.1,(2) init decoder
v3.0.1,(3) prep decode_strategy. Possibly repeat src objects.
v3.0.1,(4) Begin decoding step by step:
v3.0.1,Reorder states.
v3.0.1,select indexes in model state/cache
v3.0.1,beam parameters
v3.0.1,beam state
v3.0.1,BoolTensor was introduced in pytorch 1.2
v3.0.1,"""global state"" of the old beam"
v3.0.1,buffers for the topk scores and 'backpointer'
v3.0.1,for testing
v3.0.1,maybe fix some prediction at this step by modifying log_probs
v3.0.1,Flatten probs into a list of possibilities.
v3.0.1,Penalize beams that finished.
v3.0.1,"on real data (newstest2017) with the pretrained transformer,"
v3.0.1,it's faster to not move this back to the original device
v3.0.1,Store finished hypotheses for this batch.
v3.0.1,End condition is the top beam finished and we can return
v3.0.1,n_best hypotheses.
v3.0.1,"If all sentences are translated, no need to go further."
v3.0.1,Remove finished batches for the next step.
v3.0.1,using integer division to get an integer _B without casting
v3.0.1,force the output to be longer than self.min_length
v3.0.1,Multiply probs by the beam probability.
v3.0.1,"if the sequence ends now, then the penalty is the current"
v3.0.1,"length + 1, to include the EOS token"
v3.0.1,Avoid any direction that would repeat unwanted ngrams
v3.0.1,Pick up candidate token by curr_scores
v3.0.1,Recover log probs.
v3.0.1,Length penalty is just a scalar. It doesn't matter if it's applied
v3.0.1,before or after the topk.
v3.0.1,Resolve beam origin and map to batch index flat representation.
v3.0.1,Append last prediction.
v3.0.1,update global state (step == 1)
v3.0.1,update global state (step > 1)
v3.0.1,"shape: (batch_size x beam_size, 1)"
v3.0.1,in LM task src_len is associated with currently generated src
v3.0.1,and therefore needs to follow the generation
v3.0.1,in LM task src_len is associated with currently generated src
v3.0.1,and therefore needs to follow the generation
v3.0.1,Term will be subtracted from probability
v3.0.1,Probability will be divided by this
v3.0.1,these warnings indicate that either the alpha/beta
v3.0.1,"forces a penalty to be a no-op, or a penalty is a no-op but"
v3.0.1,the alpha/beta would suggest otherwise.
v3.0.1,using some coverage penalty
v3.0.1,!/usr/bin/env python
v3.0.1,semaphore doesn't have a timeout arg in Python 2.7
v3.0.1,perform a first request to initialize everything
v3.0.1,backwards compatibility for confs
v3.0.1,every segment becomes a dict for flexibility purposes
v3.0.1,NOTE: translator returns lists of `n_best` list
v3.0.1,build back results with empty texts
v3.0.1,load can be called multiple times: modify copy
v3.0.1,output contain alignment
v3.0.1,Below are all the different penalty terms implemented so far.
v3.0.1,Subtract coverage penalty from topk log probs.
v3.0.1,Divide topk log probs by length penalty.
v3.0.1,Sorting
v3.0.1,src_raw = self.data.examples[inds[b]].src[0]
v3.0.1,Chinese segmentation
v3.0.1,Chinese simplify -> Chinese traditional standard
v3.0.1,Chinese simplify -> Chinese traditional (HongKong)
v3.0.1,Chinese simplify -> Chinese traditional (Taiwan)
v3.0.1,Chinese traditional -> Chinese simplify (v1)
v3.0.1,Chinese traditional -> Chinese simplify (v2)
v3.0.1,Auto import python files in this directory
v3.0.0,!/usr/bin/env python
v3.0.0,!/usr/bin/env python
v3.0.0,!/usr/bin/env python
v3.0.0,!/usr/bin/env python
v3.0.0,!/usr/bin/env python
v3.0.0,!/usr/bin/env python3
v3.0.0,-*- coding: utf-8 -*-
v3.0.0,
v3.0.0,"OpenNMT-py documentation build configuration file, created by"
v3.0.0,sphinx-quickstart on Sun Dec 17 12:07:14 2017.
v3.0.0,
v3.0.0,This file is execfile()d with the current directory set to its
v3.0.0,containing dir.
v3.0.0,
v3.0.0,Note that not all possible configuration values are present in this
v3.0.0,autogenerated file.
v3.0.0,
v3.0.0,All configuration values have a default; values that are commented out
v3.0.0,serve to show the default.
v3.0.0,"If extensions (or modules to document with autodoc) are in another directory,"
v3.0.0,add these directories to sys.path here. If the directory is relative to the
v3.0.0,"documentation root, use os.path.abspath to make it absolute, like shown here."
v3.0.0,
v3.0.0,import os
v3.0.0,import sys
v3.0.0,"sys.path.insert(0, os.path.abspath('.'))"
v3.0.0,-- General configuration ------------------------------------------------
v3.0.0,"If your documentation needs a minimal Sphinx version, state it here."
v3.0.0,
v3.0.0,needs_sphinx = '1.0'
v3.0.0,"Add any Sphinx extension module names here, as strings. They can be"
v3.0.0,extensions coming with Sphinx (named 'sphinx.ext.*') or your custom
v3.0.0,ones.
v3.0.0,Show base classes
v3.0.0,"Use ""variables"" section for Attributes instead of weird block things"
v3.0.0,mimicking the function style.
v3.0.0,"Add any paths that contain templates here, relative to this directory."
v3.0.0,The suffix(es) of source filenames.
v3.0.0,You can specify multiple suffix as a list of string:
v3.0.0,
v3.0.0,"source_suffix = ['.rst', '.md']"
v3.0.0,The master toctree document.
v3.0.0,General information about the project.
v3.0.0,"The version info for the project you're documenting, acts as replacement for"
v3.0.0,"|version| and |release|, also used in various other places throughout the"
v3.0.0,built documents.
v3.0.0,
v3.0.0,The short X.Y version.
v3.0.0,"The full version, including alpha/beta/rc tags."
v3.0.0,The language for content autogenerated by Sphinx. Refer to documentation
v3.0.0,for a list of supported languages.
v3.0.0,
v3.0.0,This is also used if you do content translation via gettext catalogs.
v3.0.0,"Usually you set ""language"" from the command line for these cases."
v3.0.0,"List of patterns, relative to source directory, that match files and"
v3.0.0,directories to ignore when looking for source files.
v3.0.0,This patterns also effect to html_static_path and html_extra_path
v3.0.0,The name of the Pygments (syntax highlighting) style to use.
v3.0.0,"If true, `todo` and `todoList` produce output, else they produce nothing."
v3.0.0,-- Options for HTML output ----------------------------------------------
v3.0.0,The theme to use for HTML and HTML Help pages.  See the documentation for
v3.0.0,a list of builtin themes.
v3.0.0,
v3.0.0,html_theme = 'sphinx_materialdesign_theme'
v3.0.0,html_theme_path = [sphinx_materialdesign_theme.get_path()]
v3.0.0,Theme options are theme-specific and customize the look and feel of a theme
v3.0.0,"further.  For a list of options available for each theme, see the"
v3.0.0,documentation.
v3.0.0,
v3.0.0,html_theme_options = {}
v3.0.0,"Add any paths that contain custom static files (such as style sheets) here,"
v3.0.0,"relative to this directory. They are copied after the builtin static files,"
v3.0.0,"so a file named ""default.css"" will overwrite the builtin ""default.css""."
v3.0.0,"Custom sidebar templates, must be a dictionary that maps document names"
v3.0.0,to template names.
v3.0.0,
v3.0.0,This is required for the alabaster theme
v3.0.0,refs: http://alabaster.readthedocs.io/en/latest/installation.html#sidebars
v3.0.0,-- Options for HTMLHelp output ------------------------------------------
v3.0.0,Output file base name for HTML help builder.
v3.0.0,-- Options for LaTeX output ---------------------------------------------
v3.0.0,The paper size ('letterpaper' or 'a4paper').
v3.0.0,
v3.0.0,"'papersize': 'letterpaper',"
v3.0.0,"The font size ('10pt', '11pt' or '12pt')."
v3.0.0,
v3.0.0,"'pointsize': '10pt',"
v3.0.0,Additional stuff for the LaTeX preamble.
v3.0.0,
v3.0.0,"'preamble': '',"
v3.0.0,Latex figure (float) alignment
v3.0.0,
v3.0.0,"'figure_align': 'htbp',"
v3.0.0,Grouping the document tree into LaTeX files. List of tuples
v3.0.0,"(source start file, target name, title,"
v3.0.0,"author, documentclass [howto, manual, or own class])."
v3.0.0,-- Options for manual page output ---------------------------------------
v3.0.0,One entry per manual page. List of tuples
v3.0.0,"(source start file, name, description, authors, manual section)."
v3.0.0,-- Options for Texinfo output -------------------------------------------
v3.0.0,Grouping the document tree into Texinfo files. List of tuples
v3.0.0,"(source start file, target name, title, author,"
v3.0.0,"dir menu entry, description, category)"
v3.0.0,!/usr/bin/env python
v3.0.0,-*- coding: utf-8 -*-
v3.0.0,is this reachable?
v3.0.0,Read in embeddings
v3.0.0,Write to file
v3.0.0,converts a SentencePiece vocabulary to the format expected by dynamic data
v3.0.0,"(essentially converts float expected counts to ""fixed precision"" int pseudo"
v3.0.0,counts)
v3.0.0,"Add in default model arguments, possibly added since training."
v3.0.0,this patch is no longer needed included in converter
v3.0.0,"if hasattr(model_opt, 'rnn_size'):"
v3.0.0,model_opt.hidden_size = model_opt.rnn_size
v3.0.0,build_base_model expects updated and validated opts
v3.0.0,-*- encoding: utf-8 -*-
v3.0.0,!/usr/bin/env python
v3.0.0,-*- coding: utf-8 -*-
v3.0.0,Author: Rico Sennrich
v3.0.0,flake8: noqa
v3.0.0,This file is retrieved from https://github.com/rsennrich/subword-nmt
v3.0.0,hack for python2/3 compatibility
v3.0.0,check version information
v3.0.0,some hacking to deal with duplicates (only consider first instance)
v3.0.0,don't print end-of-word symbols
v3.0.0,sys.stderr.write('cannot split {0} further.\n'.format(segment))
v3.0.0,sys.stderr.write('OOV: {0}\n'.format(segment))
v3.0.0,sys.stderr.write('OOV: {0}\n'.format(segment))
v3.0.0,python 2/3 compatibility
v3.0.0,read/write files as UTF-8
v3.0.0,!/usr/bin/env python3
v3.0.0,coding: utf-8
v3.0.0,"In order to use this tool, please install comet first"
v3.0.0,https://github.com/Unbabel/COMET
v3.0.0,Let's say you have a source file with N sentences in SL - eg: source.sl
v3.0.0,and the corresponding references (N sentences) reference.tl
v3.0.0,Translate your file in TL with the -n_best nbest options nbest being
v3.0.0,then number of hypotheses and output the target to -output target.nbest.tl
v3.0.0,Then you need to duplicate source and reference sentences nbest times
v3.0.0,for this script.
v3.0.0,for instance using awk '{for(i=1; i<=n; i++) print}' n=5 reference.tl \
v3.0.0,> reference.5.tl
v3.0.0,same for source.
v3.0.0,This script can be run (for instance with nbest = 5) as follows:
v3.0.0,python oracle_bleu.py --nbest-src source.5.sl --nbest-hyp target.5.tl \
v3.0.0,--nbest-ref reference.5.tl --nbest-order 5 --output target.maxbleu.tl
v3.0.0,It will search in all hyp the best comet score
v3.0.0,when choosing a reference-less model no nbest-ref is required
v3.0.0,for nbest in nbests:
v3.0.0,!/usr/bin/env python
v3.0.0,!/usr/bin/env python3
v3.0.0,coding: utf-8
v3.0.0,Let's say you have a source file with N sentences in SL - eg: source.sl
v3.0.0,Translate your file in TL with the -n_best nbest options nbest being
v3.0.0,then number of hypotheses and output the target to -output target.nbest.tl
v3.0.0,This script can be run (for instance with nbest = 5) as follows:
v3.0.0,python mbr_bleu.py --nbest-hyp target.5.tl \
v3.0.0,--nbest-order 5 --output target.mbr.tl
v3.0.0,It will compare all hyp with eachother and output the max bleu
v3.0.0,!/usr/bin/env python
v3.0.0,-*- coding: utf-8 -*-
v3.0.0,Author: Rico Sennrich
v3.0.0,flake8: noqa
v3.0.0,This file is retrieved from https://github.com/rsennrich/subword-nmt
v3.0.0,hack for python2/3 compatibility
v3.0.0,"find all instances of pair, and update frequency/indices around it"
v3.0.0,find first symbol
v3.0.0,"if first symbol is followed by second symbol, we've found an occurrence of pair (old_word[i:i+2])"
v3.0.0,"assuming a symbol sequence ""A B C"", if ""B C"" is merged, reduce the frequency of ""A B"""
v3.0.0,"assuming a symbol sequence ""A B C B"", if ""B C"" is merged, reduce the frequency of ""C B""."
v3.0.0,"however, skip this if the sequence is A B C B C, because the frequency of ""C B"" will be reduced by the previous code block"
v3.0.0,find new pair
v3.0.0,"assuming a symbol sequence ""A BC D"", if ""B C"" is merged, increase the frequency of ""A BC"""
v3.0.0,"assuming a symbol sequence ""A BC B"", if ""B C"" is merged, increase the frequency of ""BC B"""
v3.0.0,"however, if the sequence is A BC BC, skip this step because the count of ""BC BC"" will be incremented by the previous code block"
v3.0.0,data structure of pair frequencies
v3.0.0,index from pairs to words
v3.0.0,version 0.2 changes the handling of the end-of-word token ('</w>');
v3.0.0,version numbering allows bckward compatibility
v3.0.0,"threshold is inspired by Zipfian assumption, but should only affect speed"
v3.0.0,we probably missed the best pair because of pruning; go back to full statistics
v3.0.0,"threshold is inspired by Zipfian assumption, but should only affect speed"
v3.0.0,python 2/3 compatibility
v3.0.0,read/write files as UTF-8
v3.0.0,Now we can pipe the full file through the model using the Iterator
v3.0.0,reminder a batch includes .src .tgt .indices and it is sorted
v3.0.0,Compute and retrieve the loss for EACH sentence
v3.0.0,Now we need to rearrange the batch of ppl
v3.0.0,in the original order with indices
v3.0.0,!/usr/bin/env python
v3.0.0,-*- coding: utf-8 -*-
v3.0.0,!/usr/bin/env python
v3.0.0,!/usr/bin/env python3
v3.0.0,coding: utf-8
v3.0.0,Let's say you have a source file with N sentences in SL - eg: source.sl
v3.0.0,and the corresponding references (N sentences) reference.tl
v3.0.0,Translate your file in TL with the -n_best nbest options nbest being
v3.0.0,then number of hypotheses and output the target to -output target.nbest.tl
v3.0.0,Then you need to duplicate reference sentences nbest times for this script.
v3.0.0,for instance using awk '{for(i=1; i<=n; i++) print}' n=5 reference.tl \
v3.0.0,> reference.5.tl
v3.0.0,This script can be run (for instance with nbest = 5) as follows:
v3.0.0,python oracle_bleu.py --nbest-hyp target.5.tl --nbest-ref reference.5.tl \
v3.0.0,--nbest-order 5 --output target.maxbleu.tl
v3.0.0,It will search in all hyp the best bleu wrt reference
v3.0.0,and output the max bleu
v3.0.0,!/usr/bin/env python
v3.0.0,with the two module = imp.load_source() below
v3.0.0,we ghost the old torchtext.data.field and depercated
v3.0.0,onmt.inputters.text_dataset
v3.0.0,however this require some functions / classes to be
v3.0.0,monkey patched for loading the old field/vocab objects.
v3.0.0,"module3 = imp.load_source(""Vocab"", ""tools/convertv2_v3.py"")"
v3.0.0,"voc = dict(sorted(fields.vocab.__dict__['freqs'].items(),"
v3.0.0,"key=lambda x: (-x[1], x[0]))).keys()"
v3.0.0,"voc = dict(sorted(fields.vocab.__dict__['freqs'].items(),"
v3.0.0,"key=lambda x: (-x[1], x[0]))).keys()"
v3.0.0,"this patch is no longer needed, included in converter"
v3.0.0,"if hasattr(model_opt, 'rnn_size'):"
v3.0.0,model_opt.hidden_size = model_opt.rnn_size
v3.0.0,Avoid functionality on inference
v3.0.0,Build embeddings.
v3.0.0,Build encoder.
v3.0.0,Build embeddings.
v3.0.0,Build decoder.
v3.0.0,Share the embedding matrix - preprocess with share_vocab required.
v3.0.0,src/tgt vocab should be the same if `-share_vocab` is specified.
v3.0.0,Update vocabulary embeddings with checkpoint embeddings
v3.0.0,Embedding layers
v3.0.0,Just for debugging purposes
v3.0.0,Remove old vocabulary associated embeddings
v3.0.0,for back compat when attention_dropout was not defined
v3.0.0,Build Model
v3.0.0,Build Generator.
v3.0.0,Load the model states from checkpoint or initialize them.
v3.0.0,This preserves backward-compat for models using customed layernorm
v3.0.0,end of patch for backward compatibility
v3.0.0,Update model embeddings with those from the checkpoint
v3.0.0,after initialization
v3.0.0,!/usr/bin/env python
v3.0.0,"maybe prepare pretrained embeddings, if any"
v3.0.0,Load checkpoint if we resume from a previous training.
v3.0.0,ensure tensorboard output is written in the directory
v3.0.0,of previous checkpoints
v3.0.0,Override checkpoint's update_embeddings as it defaults to false
v3.0.0,Override checkpoint's freezing settings as it defaults to false
v3.0.0,NOTE: It's important that ``opt`` has been validated and updated
v3.0.0,at this point.
v3.0.0,Build model.
v3.0.0,Build optimizer.
v3.0.0,Build model saver
v3.0.0,Use Tensorboard for visualization during training
v3.0.0,Options only during inference
v3.0.0,"Truncation options, for text corpus"
v3.0.0,"as for False, this will be added in _add_train_general_opts"
v3.0.0,Embedding Options
v3.0.0,Model Task Options
v3.0.0,Encoder-Decoder Options
v3.0.0,Freeze Encoder and/or Decoder
v3.0.0,The following options (bridge_extra_node to n_steps) are used
v3.0.0,for training with --encoder_type ggnn (Gated Graph Neural Network).
v3.0.0,Attention options
v3.0.0,Alignement options
v3.0.0,Generator and loss options.
v3.0.0,GPU
v3.0.0,Init options
v3.0.0,Pretrained word vectors
v3.0.0,Freeze word vectors
v3.0.0,Optimization options
v3.0.0,learning rate
v3.0.0,options relate to data preprare
v3.0.0,options relate to train
v3.0.0,Alpha and Beta values for Google Length + Coverage penalty
v3.0.0,"Described here: https://arxiv.org/pdf/1609.08144.pdf, Section 7"
v3.0.0,Length penalty options
v3.0.0,Coverage penalty options
v3.0.0,Decoding Length constraint
v3.0.0,Decoding content constraint
v3.0.0,Adding options relate to decoding strategy
v3.0.0,Adding option for logging
v3.0.0,Adding options related to Transforms
v3.0.0,Copyright 2016 The Chromium Authors. All rights reserved.
v3.0.0,Use of this source code is governed by a BSD-style license that can be
v3.0.0,found in the LICENSE file.
v3.0.0,"Get the key 'value' in the dict, or just use 'value'"
v3.0.0,Basic attributes.
v3.0.0,Set model in training mode.
v3.0.0,UPDATE DROPOUT
v3.0.0,Run patience mechanism
v3.0.0,"If the patience has reached the limit, stop training"
v3.0.0,swap model params w/ moving average
v3.0.0,(and keep the original parameters)
v3.0.0,Set model in validating mode.
v3.0.0,F-prop through the model.
v3.0.0,Compute loss.
v3.0.0,Compute validation metrics (at batch.dataset level)
v3.0.0,Compute stats
v3.0.0,Update statistics.
v3.0.0,Set model back to training mode.
v3.0.0,Truncated BPTT: reminder not compatible with accum > 1
v3.0.0,1. Create truncated target.
v3.0.0,2. F-prop all but generator.
v3.0.0,3. Compute loss.
v3.0.0,Compute and save stats
v3.0.0,in theory we should divide by accum_count and bptt
v3.0.0,to rescale for each sub batch
v3.0.0,4. Update the parameters and statistics.
v3.0.0,Multi GPU gradient gather
v3.0.0,"If truncated, don't backprop fully."
v3.0.0,"in case of multi step gradient accumulation,"
v3.0.0,update only after accum batches
v3.0.0,For Flake
v3.0.0,we avoid padding while mean pooling
v3.0.0,incoming and outgoing edge embedding
v3.0.0,Find vocab data for tree builting
v3.0.0,Propogation Model
v3.0.0,Initialize the bridge layer
v3.0.0,Token embedding
v3.0.0,Initialize graph using formatted input sequence
v3.0.0,Number of flagged nodes defines node count for this sample
v3.0.0,"(Nodes can have no flags on them, but must be in 'flags' list)."
v3.0.0,The total number of integers in the vocab should allow
v3.0.0,for all features and edges to be defined.
v3.0.0,Use first extra node as only source for decoder init
v3.0.0,Average all nodes to get bridge input
v3.0.0,"LSTM has hidden and cell state, other only one"
v3.0.0,Total number of states
v3.0.0,Build a linear layer for each
v3.0.0,Initialize the bridge layer
v3.0.0,src lengths data is wrapped inside a Tensor.
v3.0.0,"LSTM has hidden and cell state, other only one"
v3.0.0,Total number of states
v3.0.0,Build a linear layer for each
v3.0.0,batch x len x dim
v3.0.0,mask is now (batch x 1 x slen x slen)
v3.0.0,1 to be expanded to number of heads in MHA
v3.0.0,Run the forward pass of every layer of the tranformer.
v3.0.0,Dimensions and padding for constructing the word embedding matrix
v3.0.0,Dimensions and padding for feature embedding matrices
v3.0.0,(these have no effect if feat_vocab_sizes is empty)
v3.0.0,The embedding matrix look-up tables. The first look-up table
v3.0.0,"is for words. Subsequent ones are for features, if any exist."
v3.0.0,The final output size of word + feature vectors. This can vary
v3.0.0,from the word vector size if and only if features are defined.
v3.0.0,This is the attribute you should access if you need to know
v3.0.0,how big your embeddings are going to be.
v3.0.0,The sequence of operations that converts the input sequence
v3.0.0,into a sequence of embeddings. At minimum this consists of
v3.0.0,looking up the embeddings for each word and feature in the
v3.0.0,input. Model parameters may require the sequence to contain
v3.0.0,additional operations as well.
v3.0.0,features must use word_vec_size
v3.0.0,features will use feat_vec_size
v3.0.0,Some utilitary functions for pretrained embeddings
v3.0.0,is this reachable?
v3.0.0,Write to file
v3.0.0,set the opt in place
v3.0.0,set the opt in place
v3.0.0,This class is mainly used by decoder.py for RNNs but also
v3.0.0,by the CNN / transformer decoder when copy attention is used
v3.0.0,CNN has its own attention mechanism ConvMultiStepAttention
v3.0.0,Transformer has its own MultiHeadedAttention
v3.0.0,mlp wants it with bias
v3.0.0,"(batch, t_len, d) x (batch, d, s_len) --> (batch, t_len, s_len)"
v3.0.0,"(batch, t_len, s_len, d)"
v3.0.0,one step input
v3.0.0,"compute attention scores, as in Luong et al."
v3.0.0,Softmax or sparsemax to normalize attention weights
v3.0.0,each context vector c_t is the weighted average
v3.0.0,over all the source hidden states
v3.0.0,concatenate
v3.0.0,clamping necessary because of numerical errors: loss should be lower
v3.0.0,"bounded by zero, but negative values near zero are possible without"
v3.0.0,the clamp
v3.0.0,Shift values to be >= 0
v3.0.0,class MultiHeadedAttention(torch.jit.ScriptModule):
v3.0.0,https://arxiv.org/pdf/1803.02155.pdf
v3.0.0,in the paper they suggest either two embeds
v3.0.0,relative_key / relative_value or only
v3.0.0,relative_key. We implemented the same embed
v3.0.0,for both.
v3.0.0,@torch.jit.script_method
v3.0.0,"1) Project key, value, and query."
v3.0.0,as a reminder at training layer_cache[0] remains False
v3.0.0,2) Calculate and scale scores.
v3.0.0,batch x num_heads x query_len x key_len
v3.0.0,1 or key_len x key_len
v3.0.0,1 or key_len x key_len x dim_per_head
v3.0.0,not 100% necessary but expand to nb of heads
v3.0.0,now mask and scores have the same shape
v3.0.0,3) Apply attention dropout and compute context vectors.
v3.0.0,We use the same embeddings for key and value
v3.0.0,At the moment this class is only used by embeddings.Embeddings look-up tables
v3.0.0,-*- coding: utf-8 -*-
v3.0.0,class AverageAttention(torch.jit.ScriptModule):
v3.0.0,@torch.jit.script
v3.0.0,out_features * in_features
v3.0.0,norm is out_features * 1
v3.0.0,batch_size * out_features
v3.0.0,out_features
v3.0.0,out_features
v3.0.0,batch_size * out_features
v3.0.0,"out_channels, in_channels // groups, * kernel_size"
v3.0.0,out_features
v3.0.0,This is used nowhere in the code at the moment (Vincent Nguyen 05/18/2018)
v3.0.0,"in_channels, out_channels, *kernel_size"
v3.0.0,"in_channels, out_channels, *kernel_size"
v3.0.0,"self.out_channels, 1"
v3.0.0,out_features
v3.0.0,out_features
v3.0.0,store roots on diagonal
v3.0.0,Original probabilities.
v3.0.0,Probability of copying p(z=1) batch.
v3.0.0,Probability of not copying: p_{word}(w) * (1 - p(z))
v3.0.0,probabilities assigned by the model to the gold targets
v3.0.0,probability of tokens copied from source
v3.0.0,Set scores for unk to 0 and add eps
v3.0.0,find the indices in which you do not use the copy mechanism
v3.0.0,Drop padding.
v3.0.0,Do nothing
v3.0.0,Do nothing
v3.0.0,Punctuation only
v3.0.0,Auto import python files in this directory
v3.0.0,1. sample number of tokens to corrupt
v3.0.0,2. sample positions to corrput
v3.0.0,3. sample corrupted values
v3.0.0,1. sample number of tokens to corrupt
v3.0.0,2. sample positions to corrput
v3.0.0,3. Drop token on chosen position
v3.0.0,1. sample number of tokens to corrupt
v3.0.0,2. sample positions to corrput
v3.0.0,3. mask word on chosen position
v3.0.0,"Sharing options among `TokenizerTransform`s, same name conflict in"
v3.0.0,this scope will be resolved by remove previous occurrence in parser
v3.0.0,subword regularization(or BPE dropout) options:
v3.0.0,subword vocabulary restriction options:
v3.0.0,derterministic subwording
v3.0.0,subword sampling when nbest_size > 1 or -1
v3.0.0,alpha should be 0.0 < alpha < 1.0
v3.0.0,Load vocabulary file if provided and set threshold
v3.0.0,Load Subword Model
v3.0.0,-1: keep everything (i.e. 1 mask per token)
v3.0.0,0: replace everything (i.e. no mask)
v3.0.0,1: 1 mask per span
v3.0.0,view each subword as word start / input is word level token
v3.0.0,Pretend it ends with a full stop so last span is a sentence
v3.0.0,"Tokens that are full stops, where the previous token is not"
v3.0.0,Make sure we have enough to mask
v3.0.0,Trim to masking budget
v3.0.0,Handle 0-length mask (inserts) separately
v3.0.0,assert is_word_start[-1] == 0
v3.0.0,assert tokens_length - 1 not in indices
v3.0.0,"keep index, but replace it with [MASK]"
v3.0.0,"acts as a long length, so spans don't go over the end of doc"
v3.0.0,next position from each word_start
v3.0.0,delete token: 1 mask/remove per span
v3.0.0,"keep index, but replace it with [MASK]: 1 mask per token"
v3.0.0,A bit faster when all lengths are 1
v3.0.0,to cover whole token
v3.0.0,delete token
v3.0.0,"keep index, but replace it with [MASK]"
v3.0.0,assert tokens_length - 1 not in indices
v3.0.0,batch 0 will always predict EOS. The other batches will predict
v3.0.0,non-eos scores.
v3.0.0,"""best"" prediction is eos - that should be blocked"
v3.0.0,include at least one prediction OTHER than EOS
v3.0.0,that is greater than -1e20
v3.0.0,now batch 0 has ended and no others have
v3.0.0,initial step
v3.0.0,batch 0 dies on step 0
v3.0.0,include at least one prediction OTHER than EOS
v3.0.0,that is greater than -1e20
v3.0.0,step 2
v3.0.0,(old) batch 8 dies on step 1
v3.0.0,step 3
v3.0.0,everything dies
v3.0.0,initial step
v3.0.0,batch 0 dies on step 0
v3.0.0,include at least one prediction OTHER than EOS
v3.0.0,that is greater than -1e20
v3.0.0,step 2
v3.0.0,(old) batch 8 dies on step 1
v3.0.0,step 3
v3.0.0,everything dies
v3.0.0,initial step
v3.0.0,finish one beam
v3.0.0,include at least one prediction OTHER than EOS
v3.0.0,that is greater than -1e20
v3.0.0,step 2
v3.0.0,finish example in last batch
v3.0.0,(old) batch 8 dies on step 1
v3.0.0,step 3
v3.0.0,everything dies
v3.0.0,initial step
v3.0.0,batch 0 dies on step 0
v3.0.0,include at least one prediction OTHER than EOS
v3.0.0,that is greater than -1e20
v3.0.0,step 2
v3.0.0,(old) batch 8 dies on step 1
v3.0.0,step 3
v3.0.0,everything dies
v3.0.0,illegal_weights_mask = torch.ByteTensor([
v3.0.0,"[0, 0, 0, 0, 0, 0, 0],"
v3.0.0,"[0, 0, 0, 1, 1, 1, 1],"
v3.0.0,"[0, 0, 0, 0, 0, 1, 1],"
v3.0.0,"[0, 0, 1, 1, 1, 1, 1]])"
v3.0.0,TODO: fix for pytorch 0.3
v3.0.0,illegal_weights = alignments.masked_select(illegal_weights_mask)
v3.0.0,"self.assertEqual(0.0, illegal_weights.data.sum())"
v3.0.0,this could be considered an integration test because it touches
v3.0.0,the filesystem for the config file (and the models)
v3.0.0,no dummy prefix
v3.0.0,no dummy prefix
v3.0.0,transforms that require vocab will not create if not provide vocab
v3.0.0,1. Init first transform in the pipe
v3.0.0,2. Init second transform in the pipe
v3.0.0,3. Sequential combine them into a transform pipe
v3.0.0,4. apply transform pipe for example
v3.0.0,"5. example after the pipe exceed the length limit, thus filtered"
v3.0.0,6. Transform statistics registed (here for filtertoolong)
v3.0.0,"7. after report, statistics become empty as a fresh start"
v3.0.0,filter_transform.warm_up()
v3.0.0,test BPE-dropout:
v3.0.0,1. disable bpe dropout for not training example
v3.0.0,2. enable bpe dropout for training example
v3.0.0,3. (NOTE) disable dropout won't take effect if already seen
v3.0.0,this is caused by the cache mechanism in bpe:
v3.0.0,return cached subword if the original token is seen when no dropout
v3.0.0,test SP regularization:
v3.0.0,1. enable regularization for training example
v3.0.0,2. disable regularization for not training example
v3.0.0,Not apply token drop for not training example
v3.0.0,apply token drop for training example
v3.0.0,Not apply token mask for not training example
v3.0.0,apply token mask for training example
v3.0.0,require vocabs to warm_up
v3.0.0,Not apply token mask for not training example
v3.0.0,apply token mask for training example
v3.0.0,"Defalt: full_stop_token=[""."", ""?"", ""!""]"
v3.0.0,"Defalt: full_stop_token=[""."", ""?"", ""!""]"
v3.0.0,random_ratio of inserted tokens are chosen in vocab
v3.0.0,others are MASK_TOK
v3.0.0,"insert_ratio=0.0,"
v3.0.0,"random_ratio=0.0,"
v3.0.0,"Defalt: full_stop_token=[""."", ""?"", ""!""]"
v3.0.0,all token are considered as an individual word
v3.0.0,1. tokens are dropped when replace_length is 0
v3.0.0,"print(f""token delete: {masked} / {tokens}"")"
v3.0.0,2. tokens are replaced by MASK when replace_length is 1
v3.0.0,"print(f""token mask: {masked} / {tokens}"")"
v3.0.0,"insert_ratio=0.0,"
v3.0.0,"random_ratio=0.0,"
v3.0.0,"Defalt: full_stop_token=[""."", ""?"", ""!""]"
v3.0.0,start token of word are identified using subword marker
v3.0.0,"1. replace_length 0: ""words"" are dropped"
v3.0.0,"print(f""word delete: {masked} / {tokens}"")"
v3.0.0,"self.assertEqual(len(masked), n_words - n_masked)"
v3.0.0,"2. replace_length 1: ""words"" are replaced with a single MASK"
v3.0.0,"print(f""whole word single mask: {masked} / {tokens}"")"
v3.0.0,len(masked) depend on number of tokens in select word
v3.0.0,"3. replace_length -1: all tokens in ""words"" are replaced with MASK"
v3.0.0,"print(f""whole word multi mask: {masked} / {tokens}"")"
v3.0.0,number of mask_tok depend on number of tokens in selected word
v3.0.0,number of MASK_TOK can be greater than n_masked
v3.0.0,"insert_ratio=0.5,"
v3.0.0,"random_ratio=0.3,"
v3.0.0,"Defalt: full_stop_token=[""."", ""?"", ""!""]"
v3.0.0,start token of word are identified using subword marker
v3.0.0,n_words = sum(token_starts)
v3.0.0,n_masked = math.ceil(n_words * bart_noise.mask_ratio)
v3.0.0,"print(f""Text Span Infilling: {infillied} / {tokens}"")"
v3.0.0,"print(n_words, n_masked)"
v3.0.0,!/usr/bin/env python
v3.0.0,-*- coding: utf-8 -*-
v3.0.0,Inject some dummy training options that may needed when build fields
v3.0.0,Remove the generated *pt files.
v3.0.0,Remove the generated data samples
v3.0.0,all beams repeat (beam >= 1 repeat dummy scores)
v3.0.0,predict repeat_idx over and over again
v3.0.0,"before repeat, scores are either 0 or -inf"
v3.0.0,"on repeat, `repeat_idx` score is BLOCKED_SCORE"
v3.0.0,"(but it's still the best score, thus we have"
v3.0.0,"[BLOCKED_SCORE, -inf, -inf, -inf, -inf]"
v3.0.0,repetitions keeps maximizing score
v3.0.0,"index 0 has been blocked, so repeating=>+0.0 score"
v3.0.0,other indexes are -inf so repeating=>BLOCKED_SCORE
v3.0.0,which is higher
v3.0.0,beam 0 and beam >=2 will repeat (beam >= 2 repeat dummy scores)
v3.0.0,non-interesting beams are going to get dummy values
v3.0.0,"on initial round, only predicted scores for beam 0"
v3.0.0,matter. Make two predictions. Top one will be repeated
v3.0.0,"in beam zero, second one will live on in beam 1."
v3.0.0,predict the same thing in beam 0
v3.0.0,continue pushing around what beam 1 predicts
v3.0.0,"now beam 0 dies (along with the others), beam 1 -> beam 0"
v3.0.0,"now beam 0 dies (along with the others), beam 1 -> beam 0"
v3.0.0,beam 0 and beam >= 2 will repeat (beam 2 repeats excluded idx)
v3.0.0,non-interesting beams are going to get dummy values
v3.0.0,predict the same thing in beam 0
v3.0.0,continue pushing around what beam 1 predicts
v3.0.0,predict the allowed-repeat again in beam 2
v3.0.0,"now beam 0 dies, beam 1 -> beam 0, beam 2 -> beam 1"
v3.0.0,and the rest die
v3.0.0,"since all preds after i=0 are 0, we can check"
v3.0.0,that the beam is the correct idx by checking that
v3.0.0,the curr score is the initial score
v3.0.0,beam 0 will always predict EOS. The other beams will predict
v3.0.0,non-eos scores.
v3.0.0,non-interesting beams are going to get dummy values
v3.0.0,"""best"" prediction is eos - that should be blocked"
v3.0.0,include at least beam_sz predictions OTHER than EOS
v3.0.0,that are greater than -1e20
v3.0.0,predict eos in beam 0
v3.0.0,provide beam_sz other good predictions
v3.0.0,now the top beam has ended and no others have
v3.0.0,"not of interest, but want to make sure it keeps running"
v3.0.0,since only beam 0 terminates and n_best = 2
v3.0.0,"this is also a test that when block_ngram_repeat=0,"
v3.0.0,repeating is acceptable
v3.0.0,non-interesting beams are going to get dummy values
v3.0.0,"""best"" prediction is eos - that should be blocked"
v3.0.0,include at least beam_sz predictions OTHER than EOS
v3.0.0,that are greater than -1e20
v3.0.0,predict eos in beam 1
v3.0.0,provide beam_sz other good predictions in other beams
v3.0.0,beam 1 dies on min_length
v3.0.0,beam 0 dies on the step after beam 1 dies
v3.0.0,"inp_lens is tiled in initialize, reassign to make attn match"
v3.0.0,non-interesting beams are going to get dummy values
v3.0.0,"""best"" prediction is eos - that should be blocked"
v3.0.0,include at least beam_sz predictions OTHER than EOS
v3.0.0,that are greater than -1e20
v3.0.0,predict eos in beam 1
v3.0.0,provide beam_sz other good predictions in other beams
v3.0.0,no top beams are finished yet
v3.0.0,beam 1 dies on min_length
v3.0.0,no top beams are finished yet
v3.0.0,beam 0 dies on the step after beam 1 dies
v3.0.0,top beam is finished now so there are attentions
v3.0.0,two beams are finished in each batch
v3.0.0,second dim is cut down to the non-padded src length
v3.0.0,first dim is equal to the time of death
v3.0.0,(beam 0 died at current step - adjust for SOS)
v3.0.0,(beam 1 died at last step - adjust for SOS)
v3.0.0,behavior gets weird when beam is already done so just stop
v3.0.0,this is just test_beam.TestBeamAgainstReferenceCase repeated
v3.0.0,in each batch.
v3.0.0,"init_preds: [4, 3, 5, 6, 7] - no EOS's"
v3.0.0,no EOS's yet
v3.0.0,"[5, 3, 2, 6, 0], so beam 2 predicts EOS!"
v3.0.0,assumes beam 2 finished on last step
v3.0.0,ended beam 2 shouldn't continue
v3.0.0,"[2, 5, 3, 6, 0] repeat self.BATCH_SZ, so beam 0 predicts EOS!"
v3.0.0,"[-2.4879, -3.8910, -4.1010, -4.2010, -4.4010] repeat self.BATCH_SZ"
v3.0.0,another beam is finished in all batches
v3.0.0,new beam 0 finished
v3.0.0,new beam 0 is old beam 3
v3.0.0,assumes beam 0 finished on last step
v3.0.0,"[5, 2, 6, 1, 0] repeat self.BATCH_SZ, so beam 1 predicts EOS!"
v3.0.0,we finish 3 hyps per example in this step
v3.0.0,new beam 1 is old beam 3
v3.0.0,this could be considered an integration test because it tests
v3.0.0,interactions between the GNMT scorer and the beam
v3.0.0,"-data option is required, but not used in this test, so dummy."
v3.0.0,len x batch x nfeat
v3.0.0,Initialize vectors to compare size with
v3.0.0,Ensure correct sizes and types
v3.0.0,Make sure that output has the correct size and type
v3.0.0,"[('encoder_type', 'transformer'),"
v3.0.0,"('word_vec_size', 16), ('hidden_size', 16)],"
v3.0.0,""""""" Only do SRU test if requirment is safisfied. """""""
v3.0.0,SRU doesn't support input_feed.
v3.0.0,first check there's nothing unexpectedly not trainable
v3.0.0,ok: word embeddings shouldn't be trainable
v3.0.0,if word vecs are freezed
v3.0.0,ok: positional encodings shouldn't be trainable
v3.0.0,then check nothing unexpectedly trainable
v3.0.0,Decoder state
v3.0.0,Build the RNN.
v3.0.0,Set up the context gate.
v3.0.0,Set up the standard attention.
v3.0.0,The encoder hidden is  (layers*directions) x batch x dim.
v3.0.0,We need to convert it to layers x batch x (directions*dim).
v3.0.0,Init the input feed.
v3.0.0,Update the state with the result.
v3.0.0,Concatenates sequence of tensors along a new dimension.
v3.0.0,NOTE: v0.3 to 0.4: dec_outs / attns[*] may not be list
v3.0.0,(in particular in case of SRU) it was not raising error in 0.3
v3.0.0,since stack(Variable) was allowed.
v3.0.0,"In 0.4, SRU returns a tensor that shouldn't be stacke"
v3.0.0,Calculate the attention.
v3.0.0,Calculate the context gate.
v3.0.0,Additional args check.
v3.0.0,Input feed concatenates hidden state with
v3.0.0,input at every time step.
v3.0.0,TODO: context gate should be employed
v3.0.0,instead of second RNN transform.
v3.0.0,Update the coverage attention.
v3.0.0,Decoder State
v3.0.0,CNNDecoder has its own attention mechanism.
v3.0.0,Set up a separate copy attention layer if needed.
v3.0.0,The output of CNNEncoder.
v3.0.0,The combination of output of CNNEncoder and source embeddings.
v3.0.0,Process the result and update the attentions.
v3.0.0,Update the state.
v3.0.0,TODO change the way attns is returned dict => list or tuple (onnx)
v3.0.0,src_len is a single tensor shared between all models.
v3.0.0,This assumption will not hold if Translator is modified
v3.0.0,to calculate src_len as something other than the length
v3.0.0,of the input.
v3.0.0,"return _, (B, Q_len, K_len)"
v3.0.0,"layer average attention across heads, get ``(B, Q, K)``"
v3.0.0,"Case 1: no full_context, no align heads -> layer avg baseline"
v3.0.0,"Case 2: no full_context, 1 align heads -> guided align"
v3.0.0,"Case 3: full_context, 1 align heads -> full cte guided align"
v3.0.0,BoolTensor was introduced in pytorch 1.2
v3.0.0,T: could be 1 in the case of stepwise decoding or tgt_len
v3.0.0,masking is necessary when sequence length is greater than one
v3.0.0,mask now are (batch x 1 x tlen x s or t len)
v3.0.0,1 = heads to be expanded in MHA
v3.0.0,Decoder State
v3.0.0,"previously, there was a GlobalAttention module here for copy"
v3.0.0,"attention. But it was never actually used -- the ""copy"" attention"
v3.0.0,just reuses the context attention.
v3.0.0,"attns[""align""] = torch.stack(attn_aligns, 0).mean(0)  # All avg"
v3.0.0,TODO change the way attns is returned dict => list or tuple (onnx)
v3.0.0,first value set to True triggered by the beginning of decoding
v3.0.0,layer_cache becomes active in the MultiHeadedAttention fwd
v3.0.0,T: could be 1 in the case of stepwise decoding or tgt_len
v3.0.0,masking is necessary when sequence length is greater than one
v3.0.0,mask now are (batch x 1 x tlen x tlen)
v3.0.0,1 = heads to be expanded in MHA
v3.0.0,TODO change the way attns is returned dict => list or tuple (onnx)
v3.0.0,"buffer size in bytes, determine equiv. # of elements based on data type"
v3.0.0,copy tensors into buffer_t
v3.0.0,all-reduce and rescale
v3.0.0,copy all-reduced buffer back into tensors
v3.0.0,"print(filled, sz)"
v3.0.0,"tensor is bigger than buffer, all-reduce and rescale directly"
v3.0.0,"buffer is full, all-reduce and replace buffer with grad"
v3.0.0,add tensor to buffer
v3.0.0,"propagate exception to parent process, keeping original traceback"
v3.0.0,TODO: Find a better way to check for sparse gradients.
v3.0.0,we use here a FusedAdam() copy of an old Apex repo
v3.0.0,In this case use the old FusedAdam with FP16_optimizer wrapper
v3.0.0,Load everything from the checkpoint.
v3.0.0,Build everything from scratch.
v3.0.0,"Reset optimizer, keep options."
v3.0.0,"Reset options, keep optimizer."
v3.0.0,State can be partially restored.
v3.0.0,"unscaled optimizer's gradients (already done therefore skip),"
v3.0.0,skips optimizer.step() if gradients contain infs/NaNs.
v3.0.0,Updates the scale for next iteration.
v3.0.0,Code below is an implementation of https://arxiv.org/pdf/1804.04235.pdf
v3.0.0,inspired but modified from https://github.com/DeadAt0m/adafactor-pytorch
v3.0.0,backward compatibility
v3.0.0,assuming a list/generator of parameter means single group
v3.0.0,compute combined scale factor for this group
v3.0.0,norm is in fact norm*scale
v3.0.0,note: p.grad should not ever be set for correct operation of
v3.0.0,mixed precision optimizer that sometimes sends None gradients
v3.0.0,State initialization
v3.0.0,Exponential moving average of gradient values
v3.0.0,Exponential moving average of squared gradient values
v3.0.0,-*- coding: utf-8 -*-
v3.0.0,if the loss function operates on vectors of raw logits instead
v3.0.0,"of probabilities, only the first part of the generator needs to"
v3.0.0,"be passed to the NMTLossCompute. At the moment, the only"
v3.0.0,supported loss function of this kind is the sparsemax loss.
v3.0.0,"align_head contains value in [0, 1) presenting attn prob,"
v3.0.0,0 was resulted by the context attention src_pad_mask
v3.0.0,"So, the correspand position in ref_align should also be 0"
v3.0.0,"Therefore, clip align_head to > 1e-18 should be bias free."
v3.0.0,this block does not depend on the loss value computed above
v3.0.0,and is used only for stats
v3.0.0,Correct target copy token instead of <unk>
v3.0.0,tgt[i] = align[i] + len(tgt_vocab)
v3.0.0,for i such that tgt[i] == 0 and align[i] != 0
v3.0.0,Compute sum of perplexities for stats
v3.0.0,take into account here the tgt_shift_index (0 / 1 = LM/NMT)
v3.0.0,in the case criterion reduction is None then we need
v3.0.0,to sum the loss of each sentence in the batch
v3.0.0,Check Transforms
v3.0.0,Check path
v3.0.0,tgt is src for LM task
v3.0.0,Check prefix: will be used when use prefix transform
v3.0.0,Check weight
v3.0.0,Check features
v3.0.0,validation when train:
v3.0.0,Check embeddings stuff
v3.0.0,"Backward compatibility with ""fix_word_vecs_*"" opts"
v3.0.0,encoder and decoder should be same sizes
v3.0.0,"Load default opt values, then overwrite with the opts in"
v3.0.0,"the checkpoint. That way, if there are new options added,"
v3.0.0,the defaults are used.
v3.0.0,It comes from training
v3.0.0,TODO: needs to be added as inference opt
v3.0.0,Don't do anything
v3.0.0,Update best score of each criteria
v3.0.0,Reset tolerance
v3.0.0,Update current status
v3.0.0,Decrease tolerance
v3.0.0,Log
v3.0.0,Log
v3.0.0,Get a list of world_size lists with len(stat_list) Statistics objects
v3.0.0,"this param init is overridden by model_builder, useless then."
v3.0.0,SRU doesn't support PackedSequence.
v3.0.0,-*- coding: utf-8 -*-
v3.0.0,threshold on 1 to avoid div by 0
v3.0.0,treat alignment matrix one by one as each have different lengths
v3.0.0,No alignment if not exist valid tgt token
v3.0.0,get valid alignment (sub-matrix from full paded aligment matrix)
v3.0.0,Helper functions
v3.0.0,Keeps track of the original words/subwords
v3.0.0,('prior_tokenization' option)
v3.0.0,In case there is a final case_markup when new_spacer is on
v3.0.0,-*- coding: utf-8 -*-
v3.0.0,this one is needed for Random Shuffler of batches
v3.0.0,in multi gpu it ensures datasets are read in the same order
v3.0.0,some cudnn methods can be random even after fixing the seed
v3.0.0,unless you tell it to be deterministic
v3.0.0,This one is needed for various tranfroms
v3.0.0,These ensure same initialization in multi gpu mode
v3.0.0,we need to check the model path + any tokenizer path
v3.0.0,bucket_size = batch_size
v3.0.0,We only support
v3.0.0,For TRAIN we need to group examples by length
v3.0.0,"for faster performance, but otherwise, sequential."
v3.0.0,For TRAIN we shuffle batches within the bucket
v3.0.0,otherwise sequential
v3.0.0,for specific case of rnn_packed need to be sorted
v3.0.0,within the batch
v3.0.0,Maintains the longest src and tgt length in the current batch
v3.0.0,Reset current longest length at a new batch (count=1)
v3.0.0,Src: [<bos> w1 ... wN <eos>]
v3.0.0,Tgt: [w1 ... wM <eos>]
v3.0.0,this is a hack: appears quicker to apply it here
v3.0.0,than in the ParallelCorpusIterator
v3.0.0,Make features part of src like
v3.0.0,"{'src': {'src': ..., 'feat1': ...., 'feat2': ....}}"
v3.0.0,at this point an example looks like:
v3.0.0,"{'src': {'src': ..., 'feat1': ...., 'feat2': ....},"
v3.0.0,"'tgt': {'tgt': ...},"
v3.0.0,"'src_original': ['tok1', ...'tokn'],"
v3.0.0,"'tgt_original': ['tok1', ...'tokm'],"
v3.0.0,'indices' : seq in bucket
v3.0.0,"'align': ...,"
v3.0.0,}
v3.0.0,we'll need to change this if we introduce tgt feat
v3.0.0,Need to add features in last dimensions
v3.0.0,Need to add features also in 'src'
v3.0.0,make a small vocab containing just the tokens in the source sequence
v3.0.0,Map source tokens to indices in the dynamic dict.
v3.0.0,-*- coding: utf-8 -*-
v3.0.0,'src_original' and 'tgt_original' store the
v3.0.0,original line before tokenization. These
v3.0.0,fields are used later on in the feature
v3.0.0,transforms.
v3.0.0,NOTE: moved to dynamic_iterator.py cf process()
v3.0.0,item = self.transform.apply(
v3.0.0,"example, is_train=self.infinitely, corpus_name=self.cid)"
v3.0.0,empty example: skip
v3.0.0,flake8: noqa
v3.0.0,For command-line option parsing
v3.0.0,"Check pass, set the args."
v3.0.0,"This SRU version implements its own cuda-level optimization,"
v3.0.0,so it requires that:
v3.0.0,1. `cupy` and `pynvrtc` python package installed.
v3.0.0,2. pytorch is built with cuda support.
v3.0.0,3. library path set: export LD_LIBRARY_PATH=<cuda lib path>.
v3.0.0,Check 1.
v3.0.0,Check 2.
v3.0.0,Check 3.
v3.0.0,This sets up device to use.
v3.0.0,-> directions x batch x dim
v3.0.0,For DEBUG
v3.0.0,"size = (length, batch, x.size(-1)) \"
v3.0.0,"if x.dim() == 3 else (batch, x.size(-1))"
v3.0.0,grad_x = x.new(*x.size()) if k_ == 3 else x.new(*size).zero_()
v3.0.0,Normal use
v3.0.0,"An entry check here, will catch on train side and translate side"
v3.0.0,if requirements are not satisfied.
v3.0.0,RNNDecoderState wraps hidden as a tuple.
v3.0.0,fh -> (layers*directions) x batch x dim
v3.0.0,RNN uses enc_final_hs
v3.0.0,CNN uses enc_out and enc_final_hs
v3.0.0,transformer uses src
v3.0.0,"No encoder in LM, seq2seq count formatting kept"
v3.0.0,_check_save_model_path
v3.0.0,NOTE: We need to trim the vocab to remove any unk tokens that
v3.0.0,were not originally here.
v3.0.0,"for side in [""src"", ""tgt""]:"
v3.0.0,keys_to_pop = []
v3.0.0,"if hasattr(vocab[side], ""fields""):"
v3.0.0,unk_token = vocab[side].fields[0][1].vocab.itos[0]
v3.0.0,"for key, value in vocab[side].fields[0][1].vocab.stoi.items():"
v3.0.0,if value == 0 and key != unk_token:
v3.0.0,keys_to_pop.append(key)
v3.0.0,for key in keys_to_pop:
v3.0.0,"vocab[side].fields[0][1].vocab.stoi.pop(key, None)"
v3.0.0,!/usr/bin/env python
v3.0.0,!/usr/bin/env python
v3.0.0,!/usr/bin/env python
v3.0.0,-*- coding: utf-8 -*-
v3.0.0,!/usr/bin/env python
v3.0.0,Just for debugging purposes
v3.0.0,It appends features to subwords when dumping to file
v3.0.0,!/usr/bin/env python
v3.0.0,!/usr/bin/env python
v3.0.0,Set sharing strategy manually instead of default based on the OS.
v3.0.0,torch.multiprocessing.set_sharing_strategy('file_system')
v3.0.0,Create a thread to listen for errors in the child processes.
v3.0.0,Train with multiprocessing.
v3.0.0,magic indices
v3.0.0,result caching
v3.0.0,fix length constraint and remove eos from count
v3.0.0,add one to account for BOS. Don't account for EOS because hitting
v3.0.0,this implies it hasn't been found.
v3.0.0,we don't block nothing if the user doesn't want it
v3.0.0,we can't block nothing beam's too short
v3.0.0,we check paths one by one
v3.0.0,we don't forbid nothing if the user doesn't want it
v3.0.0,we can't forbid nothing if beam's too short
v3.0.0,Reordering forbidden_tokens following beam selection
v3.0.0,We rebuild a dict to ensure we get the value and not the pointer
v3.0.0,Grabing the newly selected tokens and associated ngram
v3.0.0,skip the blocking if any token in current_ngram is excluded
v3.0.0,"pickups: Tensor where specified index were set to 1, others 0"
v3.0.0,"dropdowns: opposite of pickups, 1 for those shouldn't pick"
v3.0.0,Minus dropdowns to log_probs making probabilities of
v3.0.0,unspecified index close to 0
v3.0.0,"prediction step have surpass length of given target_prefix,"
v3.0.0,no need to further change this attr
v3.0.0,keep indices until overflowing p
v3.0.0,Set all logits that are not in the top-p to -10000.
v3.0.0,This puts the probabilities close to 0.
v3.0.0,Set all logits that are not in the top-k to -10000.
v3.0.0,This puts the probabilities close to 0.
v3.0.0,"For temp=0.0, take the argmax to avoid divide-by-zero errors."
v3.0.0,keep_topk=1 is also equivalent to argmax.
v3.0.0,maybe fix some prediction at this step by modifying log_probs
v3.0.0,"shape: (sum(~ self.is_finished), 1)"
v3.0.0,in LM task src_len is associated with currently generated src
v3.0.0,and therefore needs to follow the generation
v3.0.0,!/usr/bin/env python
v3.0.0,for debugging
v3.0.0,TODO: maybe add dynamic part
v3.0.0,Statistics
v3.0.0,In the case of length_penalty = none we report the total logprobs
v3.0.0,divided by the number of sentence to get an approximation of the
v3.0.0,per sentence logprob. We also return the corresponding ppl
v3.0.0,"When a length_penalty is used eg: ""avg"" or ""wu"" since logprobs"
v3.0.0,are normalized per token we report the per line per token logprob
v3.0.0,"and the corresponding ""per word perplexity"""
v3.0.0,Turn any copied words into UNKs.
v3.0.0,"Decoder forward, takes [batch, tgt_len, nfeats] as input"
v3.0.0,"and [batch, src_len, hidden] as enc_out"
v3.0.0,"in case of inference tgt_len = 1, batch = beam times batch_size"
v3.0.0,"in case of Gold Scoring tgt_len = actual length, batch = 1 batch"
v3.0.0,Generator forward.
v3.0.0,"returns [(batch_size x beam_size) , vocab ] when 1 step"
v3.0.0,"or [batch_size, tgt_len, vocab ] when full sentence"
v3.0.0,"here we have scores [tgt_lenxbatch, vocab] or [beamxbatch, vocab]"
v3.0.0,at this point scores is batch first (dim=0)
v3.0.0,"returns [(batch_size x beam_size) , vocab ] when 1 step"
v3.0.0,"or [batch_size, tgt_len, vocab ] when full sentence"
v3.0.0,(0) add BOS and padding to tgt prediction
v3.0.0,(1) Encoder forward.
v3.0.0,(2) Repeat src objects `n_best` times.
v3.0.0,"We use batch_size x n_best, get ``(batch * n_best, src_len, nfeat)``"
v3.0.0,"(3) Init decoder with n_best src,"
v3.0.0,"reshape tgt to ``(len, batch * n_best, nfeat)``"
v3.0.0,it should be done in a better way
v3.0.0,here dec_in is batch first
v3.0.0,masked_select
v3.0.0,get aligned src id for each prediction's valid tgt tokens
v3.0.0,TODO: support these blacklisted features
v3.0.0,(0) Prep the components of the search.
v3.0.0,(1) Run the encoder on the src.
v3.0.0,(2) prep decode_strategy. Possibly repeat src objects.
v3.0.0,(3) Begin decoding step by step:
v3.0.0,"decoder_input = decode_strategy.current_predictions.view(1, -1,"
v3.0.0,1)
v3.0.0,Reorder states.
v3.0.0,TODO: support these blacklisted features
v3.0.0,(0) Prep the components of the search.
v3.0.0,(1) split src into src and target_prefix to avoid padding.
v3.0.0,(2) init decoder
v3.0.0,(3) prep decode_strategy. Possibly repeat src objects.
v3.0.0,(4) Begin decoding step by step:
v3.0.0,Reorder states.
v3.0.0,select indexes in model state/cache
v3.0.0,beam parameters
v3.0.0,beam state
v3.0.0,BoolTensor was introduced in pytorch 1.2
v3.0.0,"""global state"" of the old beam"
v3.0.0,buffers for the topk scores and 'backpointer'
v3.0.0,for testing
v3.0.0,maybe fix some prediction at this step by modifying log_probs
v3.0.0,Flatten probs into a list of possibilities.
v3.0.0,Penalize beams that finished.
v3.0.0,"on real data (newstest2017) with the pretrained transformer,"
v3.0.0,it's faster to not move this back to the original device
v3.0.0,Store finished hypotheses for this batch.
v3.0.0,End condition is the top beam finished and we can return
v3.0.0,n_best hypotheses.
v3.0.0,"If all sentences are translated, no need to go further."
v3.0.0,Remove finished batches for the next step.
v3.0.0,using integer division to get an integer _B without casting
v3.0.0,force the output to be longer than self.min_length
v3.0.0,Multiply probs by the beam probability.
v3.0.0,"if the sequence ends now, then the penalty is the current"
v3.0.0,"length + 1, to include the EOS token"
v3.0.0,Avoid any direction that would repeat unwanted ngrams
v3.0.0,Pick up candidate token by curr_scores
v3.0.0,Recover log probs.
v3.0.0,Length penalty is just a scalar. It doesn't matter if it's applied
v3.0.0,before or after the topk.
v3.0.0,Resolve beam origin and map to batch index flat representation.
v3.0.0,Append last prediction.
v3.0.0,update global state (step == 1)
v3.0.0,update global state (step > 1)
v3.0.0,"shape: (batch_size x beam_size, 1)"
v3.0.0,in LM task src_len is associated with currently generated src
v3.0.0,and therefore needs to follow the generation
v3.0.0,in LM task src_len is associated with currently generated src
v3.0.0,and therefore needs to follow the generation
v3.0.0,Term will be subtracted from probability
v3.0.0,Probability will be divided by this
v3.0.0,these warnings indicate that either the alpha/beta
v3.0.0,"forces a penalty to be a no-op, or a penalty is a no-op but"
v3.0.0,the alpha/beta would suggest otherwise.
v3.0.0,using some coverage penalty
v3.0.0,!/usr/bin/env python
v3.0.0,semaphore doesn't have a timeout arg in Python 2.7
v3.0.0,perform a first request to initialize everything
v3.0.0,backwards compatibility for confs
v3.0.0,every segment becomes a dict for flexibility purposes
v3.0.0,NOTE: translator returns lists of `n_best` list
v3.0.0,build back results with empty texts
v3.0.0,load can be called multiple times: modify copy
v3.0.0,output contain alignment
v3.0.0,Below are all the different penalty terms implemented so far.
v3.0.0,Subtract coverage penalty from topk log probs.
v3.0.0,Divide topk log probs by length penalty.
v3.0.0,Sorting
v3.0.0,src_raw = self.data.examples[inds[b]].src[0]
v3.0.0,Chinese segmentation
v3.0.0,Chinese simplify -> Chinese traditional standard
v3.0.0,Chinese simplify -> Chinese traditional (HongKong)
v3.0.0,Chinese simplify -> Chinese traditional (Taiwan)
v3.0.0,Chinese traditional -> Chinese simplify (v1)
v3.0.0,Chinese traditional -> Chinese simplify (v2)
v3.0.0,Auto import python files in this directory
2.3.0,!/usr/bin/env python
2.3.0,!/usr/bin/env python
2.3.0,!/usr/bin/env python
2.3.0,!/usr/bin/env python
2.3.0,!/usr/bin/env python
2.3.0,!/usr/bin/env python
2.3.0,!/usr/bin/env python3
2.3.0,-*- coding: utf-8 -*-
2.3.0,
2.3.0,"OpenNMT-py documentation build configuration file, created by"
2.3.0,sphinx-quickstart on Sun Dec 17 12:07:14 2017.
2.3.0,
2.3.0,This file is execfile()d with the current directory set to its
2.3.0,containing dir.
2.3.0,
2.3.0,Note that not all possible configuration values are present in this
2.3.0,autogenerated file.
2.3.0,
2.3.0,All configuration values have a default; values that are commented out
2.3.0,serve to show the default.
2.3.0,"If extensions (or modules to document with autodoc) are in another directory,"
2.3.0,add these directories to sys.path here. If the directory is relative to the
2.3.0,"documentation root, use os.path.abspath to make it absolute, like shown here."
2.3.0,
2.3.0,import os
2.3.0,import sys
2.3.0,"sys.path.insert(0, os.path.abspath('.'))"
2.3.0,-- General configuration ------------------------------------------------
2.3.0,"If your documentation needs a minimal Sphinx version, state it here."
2.3.0,
2.3.0,needs_sphinx = '1.0'
2.3.0,"Add any Sphinx extension module names here, as strings. They can be"
2.3.0,extensions coming with Sphinx (named 'sphinx.ext.*') or your custom
2.3.0,ones.
2.3.0,Show base classes
2.3.0,"Use ""variables"" section for Attributes instead of weird block things"
2.3.0,mimicking the function style.
2.3.0,"Add any paths that contain templates here, relative to this directory."
2.3.0,The suffix(es) of source filenames.
2.3.0,You can specify multiple suffix as a list of string:
2.3.0,
2.3.0,"source_suffix = ['.rst', '.md']"
2.3.0,The master toctree document.
2.3.0,General information about the project.
2.3.0,"The version info for the project you're documenting, acts as replacement for"
2.3.0,"|version| and |release|, also used in various other places throughout the"
2.3.0,built documents.
2.3.0,
2.3.0,The short X.Y version.
2.3.0,"The full version, including alpha/beta/rc tags."
2.3.0,The language for content autogenerated by Sphinx. Refer to documentation
2.3.0,for a list of supported languages.
2.3.0,
2.3.0,This is also used if you do content translation via gettext catalogs.
2.3.0,"Usually you set ""language"" from the command line for these cases."
2.3.0,"List of patterns, relative to source directory, that match files and"
2.3.0,directories to ignore when looking for source files.
2.3.0,This patterns also effect to html_static_path and html_extra_path
2.3.0,The name of the Pygments (syntax highlighting) style to use.
2.3.0,"If true, `todo` and `todoList` produce output, else they produce nothing."
2.3.0,-- Options for HTML output ----------------------------------------------
2.3.0,The theme to use for HTML and HTML Help pages.  See the documentation for
2.3.0,a list of builtin themes.
2.3.0,
2.3.0,html_theme = 'sphinx_materialdesign_theme'
2.3.0,html_theme_path = [sphinx_materialdesign_theme.get_path()]
2.3.0,Theme options are theme-specific and customize the look and feel of a theme
2.3.0,"further.  For a list of options available for each theme, see the"
2.3.0,documentation.
2.3.0,
2.3.0,html_theme_options = {}
2.3.0,"Add any paths that contain custom static files (such as style sheets) here,"
2.3.0,"relative to this directory. They are copied after the builtin static files,"
2.3.0,"so a file named ""default.css"" will overwrite the builtin ""default.css""."
2.3.0,"Custom sidebar templates, must be a dictionary that maps document names"
2.3.0,to template names.
2.3.0,
2.3.0,This is required for the alabaster theme
2.3.0,refs: http://alabaster.readthedocs.io/en/latest/installation.html#sidebars
2.3.0,-- Options for HTMLHelp output ------------------------------------------
2.3.0,Output file base name for HTML help builder.
2.3.0,-- Options for LaTeX output ---------------------------------------------
2.3.0,The paper size ('letterpaper' or 'a4paper').
2.3.0,
2.3.0,"'papersize': 'letterpaper',"
2.3.0,"The font size ('10pt', '11pt' or '12pt')."
2.3.0,
2.3.0,"'pointsize': '10pt',"
2.3.0,Additional stuff for the LaTeX preamble.
2.3.0,
2.3.0,"'preamble': '',"
2.3.0,Latex figure (float) alignment
2.3.0,
2.3.0,"'figure_align': 'htbp',"
2.3.0,Grouping the document tree into LaTeX files. List of tuples
2.3.0,"(source start file, target name, title,"
2.3.0,"author, documentclass [howto, manual, or own class])."
2.3.0,-- Options for manual page output ---------------------------------------
2.3.0,One entry per manual page. List of tuples
2.3.0,"(source start file, name, description, authors, manual section)."
2.3.0,-- Options for Texinfo output -------------------------------------------
2.3.0,Grouping the document tree into Texinfo files. List of tuples
2.3.0,"(source start file, target name, title, author,"
2.3.0,"dir menu entry, description, category)"
2.3.0,!/usr/bin/env python
2.3.0,-*- coding: utf-8 -*-
2.3.0,is this reachable?
2.3.0,Read in embeddings
2.3.0,Write to file
2.3.0,converts a SentencePiece vocabulary to the format expected by dynamic data
2.3.0,"(essentially converts float expected counts to ""fixed precision"" int pseudo"
2.3.0,counts)
2.3.0,"Add in default model arguments, possibly added since training."
2.3.0,build_base_model expects updated and validated opts
2.3.0,-*- encoding: utf-8 -*-
2.3.0,!/usr/bin/env python
2.3.0,-*- coding: utf-8 -*-
2.3.0,Author: Rico Sennrich
2.3.0,flake8: noqa
2.3.0,This file is retrieved from https://github.com/rsennrich/subword-nmt
2.3.0,hack for python2/3 compatibility
2.3.0,check version information
2.3.0,some hacking to deal with duplicates (only consider first instance)
2.3.0,don't print end-of-word symbols
2.3.0,sys.stderr.write('cannot split {0} further.\n'.format(segment))
2.3.0,sys.stderr.write('OOV: {0}\n'.format(segment))
2.3.0,sys.stderr.write('OOV: {0}\n'.format(segment))
2.3.0,python 2/3 compatibility
2.3.0,read/write files as UTF-8
2.3.0,!/usr/bin/env python3
2.3.0,coding: utf-8
2.3.0,"In order to use this tool, please install comet first"
2.3.0,https://github.com/Unbabel/COMET
2.3.0,Let's say you have a source file with N sentences in SL - eg: source.sl
2.3.0,and the corresponding references (N sentences) reference.tl
2.3.0,Translate your file in TL with the -n_best nbest options nbest being
2.3.0,then number of hypotheses and output the target to -output target.nbest.tl
2.3.0,Then you need to duplicate source and reference sentences nbest times
2.3.0,for this script.
2.3.0,for instance using awk '{for(i=1; i<=n; i++) print}' n=5 reference.tl \
2.3.0,> reference.5.tl
2.3.0,same for source.
2.3.0,This script can be run (for instance with nbest = 5) as follows:
2.3.0,python oracle_bleu.py --nbest-src source.5.sl --nbest-hyp target.5.tl \
2.3.0,--nbest-ref reference.5.tl --nbest-order 5 --output target.maxbleu.tl
2.3.0,It will search in all hyp the best comet score
2.3.0,when choosing a reference-less model no nbest-ref is required
2.3.0,for nbest in nbests:
2.3.0,!/usr/bin/env python
2.3.0,!/usr/bin/env python3
2.3.0,coding: utf-8
2.3.0,Let's say you have a source file with N sentences in SL - eg: source.sl
2.3.0,Translate your file in TL with the -n_best nbest options nbest being
2.3.0,then number of hypotheses and output the target to -output target.nbest.tl
2.3.0,This script can be run (for instance with nbest = 5) as follows:
2.3.0,python mbr_bleu.py --nbest-hyp target.5.tl \
2.3.0,--nbest-order 5 --output target.mbr.tl
2.3.0,It will compare all hyp with eachother and output the max bleu
2.3.0,!/usr/bin/env python
2.3.0,-*- coding: utf-8 -*-
2.3.0,Author: Rico Sennrich
2.3.0,flake8: noqa
2.3.0,This file is retrieved from https://github.com/rsennrich/subword-nmt
2.3.0,hack for python2/3 compatibility
2.3.0,"find all instances of pair, and update frequency/indices around it"
2.3.0,find first symbol
2.3.0,"if first symbol is followed by second symbol, we've found an occurrence of pair (old_word[i:i+2])"
2.3.0,"assuming a symbol sequence ""A B C"", if ""B C"" is merged, reduce the frequency of ""A B"""
2.3.0,"assuming a symbol sequence ""A B C B"", if ""B C"" is merged, reduce the frequency of ""C B""."
2.3.0,"however, skip this if the sequence is A B C B C, because the frequency of ""C B"" will be reduced by the previous code block"
2.3.0,find new pair
2.3.0,"assuming a symbol sequence ""A BC D"", if ""B C"" is merged, increase the frequency of ""A BC"""
2.3.0,"assuming a symbol sequence ""A BC B"", if ""B C"" is merged, increase the frequency of ""BC B"""
2.3.0,"however, if the sequence is A BC BC, skip this step because the count of ""BC BC"" will be incremented by the previous code block"
2.3.0,data structure of pair frequencies
2.3.0,index from pairs to words
2.3.0,version 0.2 changes the handling of the end-of-word token ('</w>');
2.3.0,version numbering allows bckward compatibility
2.3.0,"threshold is inspired by Zipfian assumption, but should only affect speed"
2.3.0,we probably missed the best pair because of pruning; go back to full statistics
2.3.0,"threshold is inspired by Zipfian assumption, but should only affect speed"
2.3.0,python 2/3 compatibility
2.3.0,read/write files as UTF-8
2.3.0,Load model for inference
2.3.0,Build transforms
2.3.0,Build datareader based on src AND tgt (should be equal)
2.3.0,Cannot use build_loss_compute() we need reduction 'none' in the criterion
2.3.0,to get the loss of each sentence instead of the loss of the full batch
2.3.0,Now we can pipe the full file through the model using the Iterator
2.3.0,reminder a batch includes .src .tgt .indices and it is sorted
2.3.0,Compute and retrieve the loss for EACH sentence
2.3.0,Now we need to rearrange the batch of ppl
2.3.0,in the original order with indices
2.3.0,!/usr/bin/env python
2.3.0,-*- coding: utf-8 -*-
2.3.0,!/usr/bin/env python
2.3.0,!/usr/bin/env python3
2.3.0,coding: utf-8
2.3.0,Let's say you have a source file with N sentences in SL - eg: source.sl
2.3.0,and the corresponding references (N sentences) reference.tl
2.3.0,Translate your file in TL with the -n_best nbest options nbest being
2.3.0,then number of hypotheses and output the target to -output target.nbest.tl
2.3.0,Then you need to duplicate reference sentences nbest times for this script.
2.3.0,for instance using awk '{for(i=1; i<=n; i++) print}' n=5 reference.tl \
2.3.0,> reference.5.tl
2.3.0,This script can be run (for instance with nbest = 5) as follows:
2.3.0,python oracle_bleu.py --nbest-hyp target.5.tl --nbest-ref reference.5.tl \
2.3.0,--nbest-order 5 --output target.maxbleu.tl
2.3.0,It will search in all hyp the best bleu wrt reference
2.3.0,and output the max bleu
2.3.0,Avoid functionality on inference
2.3.0,Build embeddings.
2.3.0,Build encoder.
2.3.0,Build embeddings.
2.3.0,Build decoder.
2.3.0,Share the embedding matrix - preprocess with share_vocab required.
2.3.0,src/tgt vocab should be the same if `-share_vocab` is specified.
2.3.0,Update vocabulary embeddings with checkpoint embeddings
2.3.0,Embedding layers
2.3.0,Just for debugging purposes
2.3.0,Remove old vocabulary associated embeddings
2.3.0,for back compat when attention_dropout was not defined
2.3.0,Build Model
2.3.0,Build Generator.
2.3.0,Load the model states from checkpoint or initialize them.
2.3.0,This preserves backward-compat for models using customed layernorm
2.3.0,end of patch for backward compatibility
2.3.0,Update model embeddings with those from the checkpoint
2.3.0,after initialization
2.3.0,!/usr/bin/env python
2.3.0,ensure tensorboard output is written in the directory
2.3.0,of previous checkpoints
2.3.0,Override checkpoint's update_embeddings as it defaults to false
2.3.0,Override checkpoint's freezing settings as it defaults to false
2.3.0,NOTE: It's important that ``opt`` has been validated and updated
2.3.0,at this point.
2.3.0,Build model.
2.3.0,Build optimizer.
2.3.0,Build model saver
2.3.0,Move batch to specified device
2.3.0,Use Tensorboard for visualization during training
2.3.0,Options only during inference
2.3.0,"Truncation options, for text corpus"
2.3.0,"as for False, this will be added in _add_train_general_opts"
2.3.0,Embedding Options
2.3.0,Model Task Options
2.3.0,Encoder-Decoder Options
2.3.0,Freeze Encoder and/or Decoder
2.3.0,"group.add('--residual', '-residual',   action=""store_true"","
2.3.0,"help=""Add residual connections between RNN layers."")"
2.3.0,The following options (bridge_extra_node to n_steps) are used
2.3.0,for training with --encoder_type ggnn (Gated Graph Neural Network).
2.3.0,Attention options
2.3.0,Alignement options
2.3.0,Generator and loss options.
2.3.0,GPU
2.3.0,Init options
2.3.0,Pretrained word vectors
2.3.0,Freeze word vectors
2.3.0,Optimization options
2.3.0,learning rate
2.3.0,options relate to data preprare
2.3.0,options relate to train
2.3.0,Alpha and Beta values for Google Length + Coverage penalty
2.3.0,"Described here: https://arxiv.org/pdf/1609.08144.pdf, Section 7"
2.3.0,Length penalty options
2.3.0,Coverage penalty options
2.3.0,Decoding Length constraint
2.3.0,Decoding content constraint
2.3.0,Adding options relate to decoding strategy
2.3.0,Adding option for logging
2.3.0,Adding options related to Transforms
2.3.0,Copyright 2016 The Chromium Authors. All rights reserved.
2.3.0,Use of this source code is governed by a BSD-style license that can be
2.3.0,found in the LICENSE file.
2.3.0,"Get the key 'value' in the dict, or just use 'value'"
2.3.0,Basic attributes.
2.3.0,Set model in training mode.
2.3.0,UPDATE DROPOUT
2.3.0,gather stats unuseful on a single gpu
2.3.0,valid_stats = self._maybe_gather_stats(valid_stats)
2.3.0,Run patience mechanism
2.3.0,"If the patience has reached the limit, stop training"
2.3.0,swap model params w/ moving average
2.3.0,(and keep the original parameters)
2.3.0,Set model in validating mode.
2.3.0,F-prop through the model.
2.3.0,Compute loss.
2.3.0,Compute validation metrics (at batch.dataset level)
2.3.0,Compute stats
2.3.0,Update statistics.
2.3.0,Set model back to training mode.
2.3.0,Truncated BPTT: reminder not compatible with accum > 1
2.3.0,1. Create truncated target.
2.3.0,2. F-prop all but generator.
2.3.0,3. Compute loss.
2.3.0,Compute and save stats
2.3.0,4. Update the parameters and statistics.
2.3.0,Multi GPU gradient gather
2.3.0,"If truncated, don't backprop fully."
2.3.0,TO CHECK
2.3.0,if dec_state is not None:
2.3.0,dec_state.detach()
2.3.0,"in case of multi step gradient accumulation,"
2.3.0,update only after accum batches
2.3.0,For Flake
2.3.0,we avoid padding while mean pooling
2.3.0,incoming and outgoing edge embedding
2.3.0,Find vocab data for tree builting
2.3.0,Propogation Model
2.3.0,Initialize the bridge layer
2.3.0,Token embedding
2.3.0,Initialize graph using formatted input sequence
2.3.0,Number of flagged nodes defines node count for this sample
2.3.0,"(Nodes can have no flags on them, but must be in 'flags' list)."
2.3.0,The total number of integers in the vocab should allow
2.3.0,for all features and edges to be defined.
2.3.0,Use first extra node as only source for decoder init
2.3.0,Average all nodes to get bridge input
2.3.0,"LSTM has hidden and cell state, other only one"
2.3.0,Total number of states
2.3.0,Build a linear layer for each
2.3.0,Initialize the bridge layer
2.3.0,"s_len, batch, emb_dim = emb.size()"
2.3.0,Lengths data is wrapped inside a Tensor.
2.3.0,"LSTM has hidden and cell state, other only one"
2.3.0,Total number of states
2.3.0,Build a linear layer for each
2.3.0,"s_len, batch, emb_dim = emb.size()"
2.3.0,Run the forward pass of every layer of the tranformer.
2.3.0,Dimensions and padding for constructing the word embedding matrix
2.3.0,Dimensions and padding for feature embedding matrices
2.3.0,(these have no effect if feat_vocab_sizes is empty)
2.3.0,The embedding matrix look-up tables. The first look-up table
2.3.0,"is for words. Subsequent ones are for features, if any exist."
2.3.0,The final output size of word + feature vectors. This can vary
2.3.0,from the word vector size if and only if features are defined.
2.3.0,This is the attribute you should access if you need to know
2.3.0,how big your embeddings are going to be.
2.3.0,The sequence of operations that converts the input sequence
2.3.0,into a sequence of embeddings. At minimum this consists of
2.3.0,looking up the embeddings for each word and feature in the
2.3.0,input. Model parameters may require the sequence to contain
2.3.0,additional operations as well.
2.3.0,features must use word_vec_size
2.3.0,features will use feat_vec_size
2.3.0,Some utilitary functions for pretrained embeddings
2.3.0,is this reachable?
2.3.0,Write to file
2.3.0,set the opt in place
2.3.0,set the opt in place
2.3.0,This class is mainly used by decoder.py for RNNs but also
2.3.0,by the CNN / transformer decoder when copy attention is used
2.3.0,CNN has its own attention mechanism ConvMultiStepAttention
2.3.0,Transformer has its own MultiHeadedAttention
2.3.0,mlp wants it with bias
2.3.0,Check input sizes
2.3.0,"(batch, t_len, d) x (batch, d, s_len) --> (batch, t_len, s_len)"
2.3.0,"(batch, t_len, s_len, d)"
2.3.0,one step input
2.3.0,"compute attention scores, as in Luong et al."
2.3.0,Softmax or sparsemax to normalize attention weights
2.3.0,each context vector c_t is the weighted average
2.3.0,over all the source hidden states
2.3.0,concatenate
2.3.0,Check output sizes
2.3.0,Check output sizes
2.3.0,clamping necessary because of numerical errors: loss should be lower
2.3.0,"bounded by zero, but negative values near zero are possible without"
2.3.0,the clamp
2.3.0,from onmt.utils.misc import aeq
2.3.0,CHECKS
2.3.0,"batch, k_len, d = key.size()"
2.3.0,"batch_, k_len_, d_ = value.size()"
2.3.0,"aeq(batch, batch_)"
2.3.0,"aeq(k_len, k_len_)"
2.3.0,"aeq(d, d_)"
2.3.0,"batch_, q_len, d_ = query.size()"
2.3.0,"aeq(batch, batch_)"
2.3.0,"aeq(d, d_)"
2.3.0,"aeq(self.model_dim % 8, 0)"
2.3.0,if mask is not None:
2.3.0,"batch_, q_len_, k_len_ = mask.size()"
2.3.0,"aeq(batch_, batch)"
2.3.0,"aeq(k_len_, k_len)"
2.3.0,aeq(q_len_ == q_len)
2.3.0,END CHECKS
2.3.0,"1) Project key, value, and query."
2.3.0,1 or key_len x key_len
2.3.0,1 or key_len x key_len x dim_per_head
2.3.0,1 or key_len x key_len x dim_per_head
2.3.0,2) Calculate and scale scores.
2.3.0,batch x num_heads x query_len x key_len
2.3.0,3) Apply attention dropout and compute context vectors.
2.3.0,CHECK
2.3.0,"batch_, q_len_, d_ = output.size()"
2.3.0,"aeq(q_len, q_len_)"
2.3.0,"aeq(batch, batch_)"
2.3.0,"aeq(d, d_)"
2.3.0,Return multi-head attn
2.3.0,At the moment this class is only used by embeddings.Embeddings look-up tables
2.3.0,-*- coding: utf-8 -*-
2.3.0,checks
2.3.0,"batch, channel, height, width = base_target_emb.size()"
2.3.0,"batch_, channel_, height_, width_ = input_from_dec.size()"
2.3.0,"enc_batch, enc_channel, enc_height = encoder_out_top.size()"
2.3.0,"enc_batch_, enc_channel_, enc_height_ = encoder_out_combine.size()"
2.3.0,out_features * in_features
2.3.0,norm is out_features * 1
2.3.0,batch_size * out_features
2.3.0,out_features
2.3.0,out_features
2.3.0,batch_size * out_features
2.3.0,"out_channels, in_channels // groups, * kernel_size"
2.3.0,out_features
2.3.0,This is used nowhere in the code at the moment (Vincent Nguyen 05/18/2018)
2.3.0,"in_channels, out_channels, *kernel_size"
2.3.0,"in_channels, out_channels, *kernel_size"
2.3.0,"self.out_channels, 1"
2.3.0,out_features
2.3.0,out_features
2.3.0,store roots on diagonal
2.3.0,CHECKS
2.3.0,Original probabilities.
2.3.0,Probability of copying p(z=1) batch.
2.3.0,Probability of not copying: p_{word}(w) * (1 - p(z))
2.3.0,probabilities assigned by the model to the gold targets
2.3.0,probability of tokens copied from source
2.3.0,Set scores for unk to 0 and add eps
2.3.0,find the indices in which you do not use the copy mechanism
2.3.0,Drop padding.
2.3.0,this block does not depend on the loss value computed above
2.3.0,and is used only for stats
2.3.0,this block does not depend on the loss value computed above
2.3.0,and is used only for stats
2.3.0,Correct target copy token instead of <unk>
2.3.0,tgt[i] = align[i] + len(tgt_vocab)
2.3.0,for i such that tgt[i] == 0 and align[i] != 0
2.3.0,Compute sum of perplexities for stats
2.3.0,this part looks like it belongs in CopyGeneratorLoss
2.3.0,Compute Loss as NLL divided by seq length
2.3.0,Compute Total Loss per sequence in batch
2.3.0,Divide by length of each sequence and sum
2.3.0,Do nothing
2.3.0,Do nothing
2.3.0,Punctuation only
2.3.0,Auto import python files in this directory
2.3.0,1. sample number of tokens to corrupt
2.3.0,2. sample positions to corrput
2.3.0,3. sample corrupted values
2.3.0,1. sample number of tokens to corrupt
2.3.0,2. sample positions to corrput
2.3.0,3. Drop token on chosen position
2.3.0,1. sample number of tokens to corrupt
2.3.0,2. sample positions to corrput
2.3.0,3. mask word on chosen position
2.3.0,"Sharing options among `TokenizerTransform`s, same name conflict in"
2.3.0,this scope will be resolved by remove previous occurrence in parser
2.3.0,subword regularization(or BPE dropout) options:
2.3.0,subword vocabulary restriction options:
2.3.0,derterministic subwording
2.3.0,subword sampling when nbest_size > 1 or -1
2.3.0,alpha should be 0.0 < alpha < 1.0
2.3.0,Load vocabulary file if provided and set threshold
2.3.0,Load Subword Model
2.3.0,-1: keep everything (i.e. 1 mask per token)
2.3.0,0: replace everything (i.e. no mask)
2.3.0,1: 1 mask per span
2.3.0,view each subword as word start / input is word level token
2.3.0,Pretend it ends with a full stop so last span is a sentence
2.3.0,"Tokens that are full stops, where the previous token is not"
2.3.0,Make sure we have enough to mask
2.3.0,Trim to masking budget
2.3.0,Handle 0-length mask (inserts) separately
2.3.0,assert is_word_start[-1] == 0
2.3.0,assert tokens_length - 1 not in indices
2.3.0,"keep index, but replace it with [MASK]"
2.3.0,"acts as a long length, so spans don't go over the end of doc"
2.3.0,next position from each word_start
2.3.0,delete token: 1 mask/remove per span
2.3.0,"keep index, but replace it with [MASK]: 1 mask per token"
2.3.0,A bit faster when all lengths are 1
2.3.0,to cover whole token
2.3.0,delete token
2.3.0,"keep index, but replace it with [MASK]"
2.3.0,assert tokens_length - 1 not in indices
2.3.0,initialize fields at the top of each unit test to prevent
2.3.0,any undesired stateful effects
2.3.0,"this test touches the file system, so it could be considered an"
2.3.0,integration test
2.3.0,write utf-8 bytes
2.3.0,batch 0 will always predict EOS. The other batches will predict
2.3.0,non-eos scores.
2.3.0,"""best"" prediction is eos - that should be blocked"
2.3.0,include at least one prediction OTHER than EOS
2.3.0,that is greater than -1e20
2.3.0,now batch 0 has ended and no others have
2.3.0,initial step
2.3.0,batch 0 dies on step 0
2.3.0,include at least one prediction OTHER than EOS
2.3.0,that is greater than -1e20
2.3.0,step 2
2.3.0,(old) batch 8 dies on step 1
2.3.0,step 3
2.3.0,everything dies
2.3.0,initial step
2.3.0,batch 0 dies on step 0
2.3.0,include at least one prediction OTHER than EOS
2.3.0,that is greater than -1e20
2.3.0,step 2
2.3.0,(old) batch 8 dies on step 1
2.3.0,step 3
2.3.0,everything dies
2.3.0,initial step
2.3.0,finish one beam
2.3.0,include at least one prediction OTHER than EOS
2.3.0,that is greater than -1e20
2.3.0,step 2
2.3.0,finish example in last batch
2.3.0,(old) batch 8 dies on step 1
2.3.0,step 3
2.3.0,everything dies
2.3.0,initial step
2.3.0,batch 0 dies on step 0
2.3.0,include at least one prediction OTHER than EOS
2.3.0,that is greater than -1e20
2.3.0,step 2
2.3.0,(old) batch 8 dies on step 1
2.3.0,step 3
2.3.0,everything dies
2.3.0,illegal_weights_mask = torch.ByteTensor([
2.3.0,"[0, 0, 0, 0, 0, 0, 0],"
2.3.0,"[0, 0, 0, 1, 1, 1, 1],"
2.3.0,"[0, 0, 0, 0, 0, 1, 1],"
2.3.0,"[0, 0, 1, 1, 1, 1, 1]])"
2.3.0,TODO: fix for pytorch 0.3
2.3.0,illegal_weights = alignments.masked_select(illegal_weights_mask)
2.3.0,"self.assertEqual(0.0, illegal_weights.data.sum())"
2.3.0,this could be considered an integration test because it touches
2.3.0,the filesystem for the config file (and the models)
2.3.0,no dummy prefix
2.3.0,no dummy prefix
2.3.0,transforms that require vocab will not create if not provide vocab
2.3.0,1. Init first transform in the pipe
2.3.0,2. Init second transform in the pipe
2.3.0,3. Sequential combine them into a transform pipe
2.3.0,4. apply transform pipe for example
2.3.0,"5. example after the pipe exceed the length limit, thus filtered"
2.3.0,6. Transform statistics registed (here for filtertoolong)
2.3.0,"7. after report, statistics become empty as a fresh start"
2.3.0,filter_transform.warm_up()
2.3.0,test BPE-dropout:
2.3.0,1. disable bpe dropout for not training example
2.3.0,2. enable bpe dropout for training example
2.3.0,3. (NOTE) disable dropout won't take effect if already seen
2.3.0,this is caused by the cache mechanism in bpe:
2.3.0,return cached subword if the original token is seen when no dropout
2.3.0,test SP regularization:
2.3.0,1. enable regularization for training example
2.3.0,2. disable regularization for not training example
2.3.0,Not apply token drop for not training example
2.3.0,apply token drop for training example
2.3.0,Not apply token mask for not training example
2.3.0,apply token mask for training example
2.3.0,require vocabs to warm_up
2.3.0,Not apply token mask for not training example
2.3.0,apply token mask for training example
2.3.0,"Defalt: full_stop_token=[""."", ""?"", ""!""]"
2.3.0,"Defalt: full_stop_token=[""."", ""?"", ""!""]"
2.3.0,random_ratio of inserted tokens are chosen in vocab
2.3.0,others are MASK_TOK
2.3.0,"insert_ratio=0.0,"
2.3.0,"random_ratio=0.0,"
2.3.0,"Defalt: full_stop_token=[""."", ""?"", ""!""]"
2.3.0,all token are considered as an individual word
2.3.0,1. tokens are dropped when replace_length is 0
2.3.0,"print(f""token delete: {masked} / {tokens}"")"
2.3.0,2. tokens are replaced by MASK when replace_length is 1
2.3.0,"print(f""token mask: {masked} / {tokens}"")"
2.3.0,"insert_ratio=0.0,"
2.3.0,"random_ratio=0.0,"
2.3.0,"Defalt: full_stop_token=[""."", ""?"", ""!""]"
2.3.0,start token of word are identified using subword marker
2.3.0,"1. replace_length 0: ""words"" are dropped"
2.3.0,"print(f""word delete: {masked} / {tokens}"")"
2.3.0,"self.assertEqual(len(masked), n_words - n_masked)"
2.3.0,"2. replace_length 1: ""words"" are replaced with a single MASK"
2.3.0,"print(f""whole word single mask: {masked} / {tokens}"")"
2.3.0,len(masked) depend on number of tokens in select word
2.3.0,"3. replace_length -1: all tokens in ""words"" are replaced with MASK"
2.3.0,"print(f""whole word multi mask: {masked} / {tokens}"")"
2.3.0,number of mask_tok depend on number of tokens in selected word
2.3.0,number of MASK_TOK can be greater than n_masked
2.3.0,"insert_ratio=0.5,"
2.3.0,"random_ratio=0.3,"
2.3.0,"Defalt: full_stop_token=[""."", ""?"", ""!""]"
2.3.0,start token of word are identified using subword marker
2.3.0,n_words = sum(token_starts)
2.3.0,n_masked = math.ceil(n_words * bart_noise.mask_ratio)
2.3.0,"print(f""Text Span Infilling: {infillied} / {tokens}"")"
2.3.0,"print(n_words, n_masked)"
2.3.0,!/usr/bin/env python
2.3.0,-*- coding: utf-8 -*-
2.3.0,Inject some dummy training options that may needed when build fields
2.3.0,Remove the generated *pt files.
2.3.0,Remove the generated data samples
2.3.0,all beams repeat (beam >= 1 repeat dummy scores)
2.3.0,predict repeat_idx over and over again
2.3.0,"before repeat, scores are either 0 or -inf"
2.3.0,"on repeat, `repeat_idx` score is BLOCKED_SCORE"
2.3.0,"(but it's still the best score, thus we have"
2.3.0,"[BLOCKED_SCORE, -inf, -inf, -inf, -inf]"
2.3.0,repetitions keeps maximizing score
2.3.0,"index 0 has been blocked, so repeating=>+0.0 score"
2.3.0,other indexes are -inf so repeating=>BLOCKED_SCORE
2.3.0,which is higher
2.3.0,beam 0 and beam >=2 will repeat (beam >= 2 repeat dummy scores)
2.3.0,non-interesting beams are going to get dummy values
2.3.0,"on initial round, only predicted scores for beam 0"
2.3.0,matter. Make two predictions. Top one will be repeated
2.3.0,"in beam zero, second one will live on in beam 1."
2.3.0,predict the same thing in beam 0
2.3.0,continue pushing around what beam 1 predicts
2.3.0,"now beam 0 dies (along with the others), beam 1 -> beam 0"
2.3.0,"now beam 0 dies (along with the others), beam 1 -> beam 0"
2.3.0,beam 0 and beam >= 2 will repeat (beam 2 repeats excluded idx)
2.3.0,non-interesting beams are going to get dummy values
2.3.0,predict the same thing in beam 0
2.3.0,continue pushing around what beam 1 predicts
2.3.0,predict the allowed-repeat again in beam 2
2.3.0,"now beam 0 dies, beam 1 -> beam 0, beam 2 -> beam 1"
2.3.0,and the rest die
2.3.0,"since all preds after i=0 are 0, we can check"
2.3.0,that the beam is the correct idx by checking that
2.3.0,the curr score is the initial score
2.3.0,beam 0 will always predict EOS. The other beams will predict
2.3.0,non-eos scores.
2.3.0,non-interesting beams are going to get dummy values
2.3.0,"""best"" prediction is eos - that should be blocked"
2.3.0,include at least beam_sz predictions OTHER than EOS
2.3.0,that are greater than -1e20
2.3.0,predict eos in beam 0
2.3.0,provide beam_sz other good predictions
2.3.0,now the top beam has ended and no others have
2.3.0,"not of interest, but want to make sure it keeps running"
2.3.0,since only beam 0 terminates and n_best = 2
2.3.0,"this is also a test that when block_ngram_repeat=0,"
2.3.0,repeating is acceptable
2.3.0,non-interesting beams are going to get dummy values
2.3.0,"""best"" prediction is eos - that should be blocked"
2.3.0,include at least beam_sz predictions OTHER than EOS
2.3.0,that are greater than -1e20
2.3.0,predict eos in beam 1
2.3.0,provide beam_sz other good predictions in other beams
2.3.0,beam 1 dies on min_length
2.3.0,beam 0 dies on the step after beam 1 dies
2.3.0,"inp_lens is tiled in initialize, reassign to make attn match"
2.3.0,non-interesting beams are going to get dummy values
2.3.0,"""best"" prediction is eos - that should be blocked"
2.3.0,include at least beam_sz predictions OTHER than EOS
2.3.0,that are greater than -1e20
2.3.0,predict eos in beam 1
2.3.0,provide beam_sz other good predictions in other beams
2.3.0,no top beams are finished yet
2.3.0,beam 1 dies on min_length
2.3.0,no top beams are finished yet
2.3.0,beam 0 dies on the step after beam 1 dies
2.3.0,top beam is finished now so there are attentions
2.3.0,two beams are finished in each batch
2.3.0,second dim is cut down to the non-padded src length
2.3.0,first dim is equal to the time of death
2.3.0,(beam 0 died at current step - adjust for SOS)
2.3.0,(beam 1 died at last step - adjust for SOS)
2.3.0,behavior gets weird when beam is already done so just stop
2.3.0,this is just test_beam.TestBeamAgainstReferenceCase repeated
2.3.0,in each batch.
2.3.0,"init_preds: [4, 3, 5, 6, 7] - no EOS's"
2.3.0,no EOS's yet
2.3.0,"[5, 3, 2, 6, 0], so beam 2 predicts EOS!"
2.3.0,assumes beam 2 finished on last step
2.3.0,ended beam 2 shouldn't continue
2.3.0,"[2, 5, 3, 6, 0] repeat self.BATCH_SZ, so beam 0 predicts EOS!"
2.3.0,"[-2.4879, -3.8910, -4.1010, -4.2010, -4.4010] repeat self.BATCH_SZ"
2.3.0,another beam is finished in all batches
2.3.0,new beam 0 finished
2.3.0,new beam 0 is old beam 3
2.3.0,assumes beam 0 finished on last step
2.3.0,"[5, 2, 6, 1, 0] repeat self.BATCH_SZ, so beam 1 predicts EOS!"
2.3.0,we finish 3 hyps per example in this step
2.3.0,new beam 1 is old beam 3
2.3.0,this could be considered an integration test because it tests
2.3.0,interactions between the GNMT scorer and the beam
2.3.0,"-data option is required, but not used in this test, so dummy."
2.3.0,len x batch x nfeat
2.3.0,Initialize vectors to compare size with
2.3.0,Ensure correct sizes and types
2.3.0,Make sure that output has the correct size and type
2.3.0,"[('encoder_type', 'transformer'),"
2.3.0,"('word_vec_size', 16), ('rnn_size', 16)],"
2.3.0,""""""" Only do SRU test if requirment is safisfied. """""""
2.3.0,SRU doesn't support input_feed.
2.3.0,first check there's nothing unexpectedly not trainable
2.3.0,ok: word embeddings shouldn't be trainable
2.3.0,if word vecs are freezed
2.3.0,ok: positional encodings shouldn't be trainable
2.3.0,then check nothing unexpectedly trainable
2.3.0,Decoder state
2.3.0,Build the RNN.
2.3.0,Set up the context gate.
2.3.0,Set up the standard attention.
2.3.0,The encoder hidden is  (layers*directions) x batch x dim.
2.3.0,We need to convert it to layers x batch x (directions*dim).
2.3.0,Init the input feed.
2.3.0,Update the state with the result.
2.3.0,Concatenates sequence of tensors along a new dimension.
2.3.0,NOTE: v0.3 to 0.4: dec_outs / attns[*] may not be list
2.3.0,(in particular in case of SRU) it was not raising error in 0.3
2.3.0,since stack(Variable) was allowed.
2.3.0,"In 0.4, SRU returns a tensor that shouldn't be stacke"
2.3.0,Check
2.3.0,Calculate the attention.
2.3.0,Calculate the context gate.
2.3.0,Additional args check.
2.3.0,END Additional args check.
2.3.0,Input feed concatenates hidden state with
2.3.0,input at every time step.
2.3.0,TODO: context gate should be employed
2.3.0,instead of second RNN transform.
2.3.0,Update the coverage attention.
2.3.0,Decoder State
2.3.0,CNNDecoder has its own attention mechanism.
2.3.0,Set up a separate copy attention layer if needed.
2.3.0,The output of CNNEncoder.
2.3.0,The combination of output of CNNEncoder and source embeddings.
2.3.0,Process the result and update the attentions.
2.3.0,Update the state.
2.3.0,TODO change the way attns is returned dict => list or tuple (onnx)
2.3.0,Memory_lengths is a single tensor shared between all models.
2.3.0,This assumption will not hold if Translator is modified
2.3.0,to calculate memory_lengths as something other than the length
2.3.0,of the input.
2.3.0,"return _, (B, Q_len, K_len)"
2.3.0,"layer average attention across heads, get ``(B, Q, K)``"
2.3.0,"Case 1: no full_context, no align heads -> layer avg baseline"
2.3.0,"Case 2: no full_context, 1 align heads -> guided align"
2.3.0,"Case 3: full_context, 1 align heads -> full cte guided align"
2.3.0,BoolTensor was introduced in pytorch 1.2
2.3.0,T: could be 1 in the case of stepwise decoding or tgt_len
2.3.0,masking is necessary when sequence length is greater than one
2.3.0,Decoder State
2.3.0,"previously, there was a GlobalAttention module here for copy"
2.3.0,"attention. But it was never actually used -- the ""copy"" attention"
2.3.0,just reuses the context attention.
2.3.0,"attns[""align""] = torch.stack(attn_aligns, 0).mean(0)  # All avg"
2.3.0,TODO change the way attns is returned dict => list or tuple (onnx)
2.3.0,T: could be 1 in the case of stepwise decoding or tgt_len
2.3.0,masking is necessary when sequence length is greater than one
2.3.0,TODO change the way attns is returned dict => list or tuple (onnx)
2.3.0,"buffer size in bytes, determine equiv. # of elements based on data type"
2.3.0,copy tensors into buffer_t
2.3.0,all-reduce and rescale
2.3.0,copy all-reduced buffer back into tensors
2.3.0,"tensor is bigger than buffer, all-reduce and rescale directly"
2.3.0,"buffer is full, all-reduce and replace buffer with grad"
2.3.0,add tensor to buffer
2.3.0,NOTE: stride (if needed) is handled at the
2.3.0,generator (train_iter) level
2.3.0,Move batch to correspond device_id when consumer iterate
2.3.0,hack to dodge unpicklable `dict_keys`
2.3.0,"propagate exception to parent process, keeping original traceback"
2.3.0,TODO: Find a better way to check for sparse gradients.
2.3.0,we use here a FusedAdam() copy of an old Apex repo
2.3.0,In this case use the old FusedAdam with FP16_optimizer wrapper
2.3.0,Load everything from the checkpoint.
2.3.0,Build everything from scratch.
2.3.0,"Reset optimizer, keep options."
2.3.0,"Reset options, keep optimizer."
2.3.0,State can be partially restored.
2.3.0,"unscaled optimizer's gradients (already done therefore skip),"
2.3.0,skips optimizer.step() if gradients contain infs/NaNs.
2.3.0,Updates the scale for next iteration.
2.3.0,Code below is an implementation of https://arxiv.org/pdf/1804.04235.pdf
2.3.0,inspired but modified from https://github.com/DeadAt0m/adafactor-pytorch
2.3.0,backward compatibility
2.3.0,assuming a list/generator of parameter means single group
2.3.0,compute combined scale factor for this group
2.3.0,norm is in fact norm*scale
2.3.0,note: p.grad should not ever be set for correct operation of
2.3.0,mixed precision optimizer that sometimes sends None gradients
2.3.0,State initialization
2.3.0,Exponential moving average of gradient values
2.3.0,Exponential moving average of squared gradient values
2.3.0,-*- coding: utf-8 -*-
2.3.0,if the loss function operates on vectors of raw logits instead of
2.3.0,"probabilities, only the first part of the generator needs to be"
2.3.0,"passed to the NMTLossCompute. At the moment, the only supported"
2.3.0,loss function of this kind is the sparsemax loss.
2.3.0,in the case criterion reduction is None then we need
2.3.0,to sum the loss of each sentence in the batch
2.3.0,"attn_align should be in (batch_size, pad_tgt_size, pad_src_size)"
2.3.0,"align_idx should be a Tensor in size([N, 3]), N is total number"
2.3.0,"of align src-tgt pair in current batch, each as"
2.3.0,"['sent_NÂ°_in_batch', 'tgt_id+1', 'src_id'] (check AlignField)"
2.3.0,NOTE: tgt-src ref alignement that in range_ of shard
2.3.0,(coherent with batch.tgt)
2.3.0,"align_head contains value in [0, 1) presenting attn prob,"
2.3.0,0 was resulted by the context attention src_pad_mask
2.3.0,"So, the correspand position in ref_align should also be 0"
2.3.0,"Therefore, clip align_head to > 1e-18 should be bias free."
2.3.0,non_none: the subdict of the state dictionary where the values
2.3.0,are not None.
2.3.0,"Now, the iteration:"
2.3.0,state is a dictionary of sequences of tensor-like but we
2.3.0,want a sequence of dictionaries of tensors.
2.3.0,"First, unzip the dictionary into a sequence of keys and a"
2.3.0,sequence of tensor-like sequences.
2.3.0,"Now, yield a dictionary for each shard. The keys are always"
2.3.0,the same. values is a sequence of length #keys where each
2.3.0,element is a sequence of length #shards. We want to iterate
2.3.0,"over the shards, not over the keys: therefore, the values need"
2.3.0,to be re-zipped by shard and then each shard can be paired
2.3.0,with the keys.
2.3.0,Assumed backprop'd
2.3.0,Check Transforms
2.3.0,Check path
2.3.0,tgt is src for LM task
2.3.0,Check prefix: will be used when use prefix transform
2.3.0,Check weight
2.3.0,Check features
2.3.0,validation when train:
2.3.0,Check embeddings stuff
2.3.0,"Backward compatibility with ""fix_word_vecs_*"" opts"
2.3.0,encoder and decoder should be same sizes
2.3.0,"Load default opt values, then overwrite with the opts in"
2.3.0,"the checkpoint. That way, if there are new options added,"
2.3.0,the defaults are used.
2.3.0,It comes from training
2.3.0,TODO: needs to be added as inference opt
2.3.0,Don't do anything
2.3.0,Update best score of each criteria
2.3.0,Reset tolerance
2.3.0,Update current status
2.3.0,Decrease tolerance
2.3.0,Log
2.3.0,Log
2.3.0,Get a list of world_size lists with len(stat_list) Statistics objects
2.3.0,SRU doesn't support PackedSequence.
2.3.0,-*- coding: utf-8 -*-
2.3.0,threshold on 1 to avoid div by 0
2.3.0,treat alignment matrix one by one as each have different lengths
2.3.0,No alignment if not exist valid tgt token
2.3.0,get valid alignment (sub-matrix from full paded aligment matrix)
2.3.0,Helper functions
2.3.0,Keeps track of the original words/subwords
2.3.0,('prior_tokenization' option)
2.3.0,In case there is a final case_markup when new_spacer is on
2.3.0,-*- coding: utf-8 -*-
2.3.0,this one is needed for torchtext random call (shuffled iterator)
2.3.0,in multi gpu it ensures datasets are read in the same order
2.3.0,some cudnn methods can be random even after fixing the seed
2.3.0,unless you tell it to be deterministic
2.3.0,This one is needed for various tranfroms
2.3.0,These ensure same initialization in multi gpu mode
2.3.0,Shift values to be >= 0
2.3.0,we need to check the model path + any tokenizer path
2.3.0,fast-forward if loaded from state
2.3.0,NOTE: `rnn.pack_padded_sequence` requires that a
2.3.0,"minibatch be sorted by decreasing order, which"
2.3.0,requires reversing relative to typical sort keys
2.3.0,Maintains the longest src and tgt length in the current batch
2.3.0,Reset current longest length at a new batch (count=1)
2.3.0,Src: [<bos> w1 ... wN <eos>]
2.3.0,Tgt: [w1 ... wM <eos>]
2.3.0,coding: utf-8
2.3.0,make a small vocab containing just the tokens in the source sequence
2.3.0,add init_token and eos_token according to src construction
2.3.0,Map source tokens to indices in the dynamic dict.
2.3.0,self.src_vocabs is used in collapse_copy_scores and Translator.py
2.3.0,this assumes src_field and tgt_field are both text
2.3.0,fields needs to have only keys that examples have as attrs
2.3.0,avoid infinite recursion when fields isn't defined
2.3.0,self.src_vocabs is used in collapse_copy_scores and Translator.py
2.3.0,this assumes src_field and tgt_field are both text
2.3.0,fields needs to have only keys that examples have as attrs
2.3.0,avoid infinite recursion when fields isn't defined
2.3.0,this is a hack: appears quicker to apply it here
2.3.0,than in the ParallelCorpusIterator
2.3.0,Make features part of src as in TextMultiField
2.3.0,"{'src': {'src': ..., 'feat1': ...., 'feat2': ....}}"
2.3.0,'src_original' and 'tgt_original' store the
2.3.0,original line before tokenization. These
2.3.0,fields are used later on in the feature
2.3.0,transforms.
2.3.0,NOTE: moved to DatasetAdapter._process method in iterator.py
2.3.0,item = self.transform.apply(
2.3.0,"example, is_train=self.infinitely, corpus_name=self.cid)"
2.3.0,empty example: skip
2.3.0,Just for debugging purposes
2.3.0,It appends features to subwords when dumping to file
2.3.0,-*- coding: utf-8 -*-
2.3.0,backwards compatibility
2.3.0,monkey-patch to make torchtext Vocab's pickleable
2.3.0,"+1 for tgt side to keep coherent after ""bos"" padding,"
2.3.0,"register ['NÂ°_in_batch', 'tgt_id+1', 'src_id']"
2.3.0,this is basically copy-pasted from torchtext.
2.3.0,counters changes in place
2.3.0,keep the order of tokens specified in the vocab file by
2.3.0,adding them to the counter with decreasing counting values
2.3.0,`tgt_vocab_size` is ignored when sharing vocabularies
2.3.0,return vocab to dump with standard name
2.3.0,empty train_dataset_files so that vocab is only loaded from
2.3.0,"given paths in src_vocab_path, tgt_vocab_path"
2.3.0,Load vocabulary
2.3.0,Drop the none-using from memory but keep the last
2.3.0,"in the long run, shouldn't it be possible to do this by calling"
2.3.0,build_vocab with both the src and tgt data?
2.3.0,coding: utf-8
2.3.0,several data readers need optional dependencies. There's no
2.3.0,appropriate builtin exception
2.3.0,NOTE: not support tgt feats yet
2.3.0,-*- coding: utf-8 -*-
2.3.0,Make features part of src as in TextMultiField
2.3.0,"{'src': {'src': ..., 'feat1': ...., 'feat2': ....}}"
2.3.0,Cleanup
2.3.0,Legacy function. Currently it only truncates input if truncate is set.
2.3.0,mix this with partial
2.3.0,batch (list(list(list))): batch_size x len(self.fields) x seq_len
2.3.0,lengths: batch_size
2.3.0,data: seq_len x batch_size x len(self.fields)
2.3.0,Base field
2.3.0,Feats fields
2.3.0,"Legacy function, it is not really necessary"
2.3.0,flake8: noqa
2.3.0,For command-line option parsing
2.3.0,"Check pass, set the args."
2.3.0,"This SRU version implements its own cuda-level optimization,"
2.3.0,so it requires that:
2.3.0,1. `cupy` and `pynvrtc` python package installed.
2.3.0,2. pytorch is built with cuda support.
2.3.0,3. library path set: export LD_LIBRARY_PATH=<cuda lib path>.
2.3.0,Check 1.
2.3.0,Check 2.
2.3.0,Check 3.
2.3.0,This sets up device to use.
2.3.0,-> directions x batch x dim
2.3.0,For DEBUG
2.3.0,"size = (length, batch, x.size(-1)) \"
2.3.0,"if x.dim() == 3 else (batch, x.size(-1))"
2.3.0,grad_x = x.new(*x.size()) if k_ == 3 else x.new(*size).zero_()
2.3.0,Normal use
2.3.0,"An entry check here, will catch on train side and translate side"
2.3.0,if requirements are not satisfied.
2.3.0,RNNDecoderState wraps hidden as a tuple.
2.3.0,fh -> (layers*directions) x batch x dim
2.3.0,"No encoder in LM, seq2seq count formatting kept"
2.3.0,_check_save_model_path
2.3.0,NOTE: We need to trim the vocab to remove any unk tokens that
2.3.0,were not originally here.
2.3.0,!/usr/bin/env python
2.3.0,-*- coding: utf-8 -*-
2.3.0,Build transforms
2.3.0,!/usr/bin/env python
2.3.0,!/usr/bin/env python
2.3.0,!/usr/bin/env python
2.3.0,-*- coding: utf-8 -*-
2.3.0,!/usr/bin/env python
2.3.0,!/usr/bin/env python
2.3.0,!/usr/bin/env python
2.3.0,import onmt.opts as opts
2.3.0,Set sharing strategy manually instead of default based on the OS.
2.3.0,"maybe prepare pretrained embeddings, if any"
2.3.0,Load checkpoint if we resume from a previous training.
2.3.0,Report src and tgt vocab sizes
2.3.0,Create a thread to listen for errors in the child processes.
2.3.0,Train with multiprocessing.
2.3.0,"This does not work if we merge with the first loop, not sure why"
2.3.0,Get the iterator to generate from
2.3.0,"Once training is done, we can terminate the producers"
2.3.0,magic indices
2.3.0,result caching
2.3.0,fix length constraint and remove eos from count
2.3.0,add one to account for BOS. Don't account for EOS because hitting
2.3.0,this implies it hasn't been found.
2.3.0,we don't block nothing if the user doesn't want it
2.3.0,we can't block nothing beam's too short
2.3.0,we check paths one by one
2.3.0,we don't forbid nothing if the user doesn't want it
2.3.0,we can't forbid nothing if beam's too short
2.3.0,Reordering forbidden_tokens following beam selection
2.3.0,We rebuild a dict to ensure we get the value and not the pointer
2.3.0,Grabing the newly selected tokens and associated ngram
2.3.0,skip the blocking if any token in current_ngram is excluded
2.3.0,"pickups: Tensor where specified index were set to 1, others 0"
2.3.0,"dropdowns: opposite of pickups, 1 for those shouldn't pick"
2.3.0,Minus dropdowns to log_probs making probabilities of
2.3.0,unspecified index close to 0
2.3.0,"prediction step have surpass length of given target_prefix,"
2.3.0,no need to further change this attr
2.3.0,keep indices until overflowing p
2.3.0,Set all logits that are not in the top-p to -10000.
2.3.0,This puts the probabilities close to 0.
2.3.0,Set all logits that are not in the top-k to -10000.
2.3.0,This puts the probabilities close to 0.
2.3.0,"For temp=0.0, take the argmax to avoid divide-by-zero errors."
2.3.0,keep_topk=1 is also equivalent to argmax.
2.3.0,maybe fix some prediction at this step by modifying log_probs
2.3.0,"shape: (sum(~ self.is_finished), 1)"
2.3.0,in LM task memory_lengths is associated with currently generated src
2.3.0,and therefore needs to follow the generation
2.3.0,!/usr/bin/env python
2.3.0,Maintains the longest src and tgt length in the current batch
2.3.0,Reset current longest length at a new batch (count=1)
2.3.0,max_tgt_in_batch = 0
2.3.0,Src: [<bos> w1 ... wN <eos>]
2.3.0,Tgt: [w1 ... wM <eos>]
2.3.0,for debugging
2.3.0,TODO: maybe add dynamic part
2.3.0,Statistics
2.3.0,In the case of length_penalty = none we report the total logprobs
2.3.0,divided by the number of sentence to get an approximation of the
2.3.0,per sentence logprob. We also return the corresponding ppl
2.3.0,"When a length_penalty is used eg: ""avg"" or ""wu"" since logprobs"
2.3.0,are normalized per token we report the per line per token logprob
2.3.0,"and the corresponding ""per word perplexity"""
2.3.0,Turn any copied words into UNKs.
2.3.0,"Decoder forward, takes [tgt_len, batch, nfeats] as input"
2.3.0,"and [src_len, batch, hidden] as memory_bank"
2.3.0,"in case of inference tgt_len = 1, batch = beam times batch_size"
2.3.0,"in case of Gold Scoring tgt_len = actual length, batch = 1 batch"
2.3.0,Generator forward.
2.3.0,"returns [(batch_size x beam_size) , vocab ] when 1 step"
2.3.0,"or [ tgt_len, batch_size, vocab ] when full sentence"
2.3.0,"here we have scores [tgt_lenxbatch, vocab] or [beamxbatch, vocab]"
2.3.0,"returns [(batch_size x beam_size) , vocab ] when 1 step"
2.3.0,"or [ tgt_len, batch_size, vocab ] when full sentence"
2.3.0,(0) add BOS and padding to tgt prediction
2.3.0,(1) Encoder forward.
2.3.0,(2) Repeat src objects `n_best` times.
2.3.0,"We use batch_size x n_best, get ``(src_len, batch * n_best, nfeat)``"
2.3.0,"(3) Init decoder with n_best src,"
2.3.0,"reshape tgt to ``(len, batch * n_best, nfeat)``"
2.3.0,masked_select
2.3.0,get aligned src id for each prediction's valid tgt tokens
2.3.0,TODO: support these blacklisted features
2.3.0,(0) Prep the components of the search.
2.3.0,(1) Run the encoder on the src.
2.3.0,(2) prep decode_strategy. Possibly repeat src objects.
2.3.0,(3) Begin decoding step by step:
2.3.0,Reorder states.
2.3.0,TODO: support these blacklisted features
2.3.0,(0) Prep the components of the search.
2.3.0,(1) split src into src and target_prefix to avoid padding.
2.3.0,(2) init decoder
2.3.0,(3) prep decode_strategy. Possibly repeat src objects.
2.3.0,(4) Begin decoding step by step:
2.3.0,Reorder states.
2.3.0,select indexes in model state/cache
2.3.0,beam parameters
2.3.0,beam state
2.3.0,BoolTensor was introduced in pytorch 1.2
2.3.0,"""global state"" of the old beam"
2.3.0,buffers for the topk scores and 'backpointer'
2.3.0,for testing
2.3.0,maybe fix some prediction at this step by modifying log_probs
2.3.0,Flatten probs into a list of possibilities.
2.3.0,Penalize beams that finished.
2.3.0,"on real data (newstest2017) with the pretrained transformer,"
2.3.0,it's faster to not move this back to the original device
2.3.0,Store finished hypotheses for this batch.
2.3.0,End condition is the top beam finished and we can return
2.3.0,n_best hypotheses.
2.3.0,"If all sentences are translated, no need to go further."
2.3.0,Remove finished batches for the next step.
2.3.0,using integer division to get an integer _B without casting
2.3.0,force the output to be longer than self.min_length
2.3.0,Multiply probs by the beam probability.
2.3.0,"if the sequence ends now, then the penalty is the current"
2.3.0,"length + 1, to include the EOS token"
2.3.0,Avoid any direction that would repeat unwanted ngrams
2.3.0,Pick up candidate token by curr_scores
2.3.0,Recover log probs.
2.3.0,Length penalty is just a scalar. It doesn't matter if it's applied
2.3.0,before or after the topk.
2.3.0,Resolve beam origin and map to batch index flat representation.
2.3.0,Append last prediction.
2.3.0,update global state (step == 1)
2.3.0,update global state (step > 1)
2.3.0,"shape: (batch_size x beam_size, 1)"
2.3.0,in LM task memory_lengths is associated with currently generated src
2.3.0,and therefore needs to follow the generation
2.3.0,in LM task memory_lengths is associated with currently generated src
2.3.0,and therefore needs to follow the generation
2.3.0,Term will be subtracted from probability
2.3.0,Probability will be divided by this
2.3.0,these warnings indicate that either the alpha/beta
2.3.0,"forces a penalty to be a no-op, or a penalty is a no-op but"
2.3.0,the alpha/beta would suggest otherwise.
2.3.0,using some length penalty
2.3.0,using some coverage penalty
2.3.0,!/usr/bin/env python
2.3.0,semaphore doesn't have a timeout arg in Python 2.7
2.3.0,perform a first request to initialize everything
2.3.0,backwards compatibility for confs
2.3.0,every segment becomes a dict for flexibility purposes
2.3.0,NOTE: translator returns lists of `n_best` list
2.3.0,build back results with empty texts
2.3.0,load can be called multiple times: modify copy
2.3.0,output contain alignment
2.3.0,Below are all the different penalty terms implemented so far.
2.3.0,Subtract coverage penalty from topk log probs.
2.3.0,Divide topk log probs by length penalty.
2.3.0,Sorting
2.3.0,Chinese segmentation
2.3.0,Chinese simplify -> Chinese traditional standard
2.3.0,Chinese simplify -> Chinese traditional (HongKong)
2.3.0,Chinese simplify -> Chinese traditional (Taiwan)
2.3.0,Chinese traditional -> Chinese simplify (v1)
2.3.0,Chinese traditional -> Chinese simplify (v2)
2.3.0,Auto import python files in this directory
2.2.0,!/usr/bin/env python
2.2.0,!/usr/bin/env python
2.2.0,!/usr/bin/env python
2.2.0,!/usr/bin/env python
2.2.0,!/usr/bin/env python
2.2.0,!/usr/bin/env python3
2.2.0,-*- coding: utf-8 -*-
2.2.0,
2.2.0,"OpenNMT-py documentation build configuration file, created by"
2.2.0,sphinx-quickstart on Sun Dec 17 12:07:14 2017.
2.2.0,
2.2.0,This file is execfile()d with the current directory set to its
2.2.0,containing dir.
2.2.0,
2.2.0,Note that not all possible configuration values are present in this
2.2.0,autogenerated file.
2.2.0,
2.2.0,All configuration values have a default; values that are commented out
2.2.0,serve to show the default.
2.2.0,"If extensions (or modules to document with autodoc) are in another directory,"
2.2.0,add these directories to sys.path here. If the directory is relative to the
2.2.0,"documentation root, use os.path.abspath to make it absolute, like shown here."
2.2.0,
2.2.0,import os
2.2.0,import sys
2.2.0,"sys.path.insert(0, os.path.abspath('.'))"
2.2.0,-- General configuration ------------------------------------------------
2.2.0,"If your documentation needs a minimal Sphinx version, state it here."
2.2.0,
2.2.0,needs_sphinx = '1.0'
2.2.0,"Add any Sphinx extension module names here, as strings. They can be"
2.2.0,extensions coming with Sphinx (named 'sphinx.ext.*') or your custom
2.2.0,ones.
2.2.0,Show base classes
2.2.0,"Use ""variables"" section for Attributes instead of weird block things"
2.2.0,mimicking the function style.
2.2.0,"Add any paths that contain templates here, relative to this directory."
2.2.0,The suffix(es) of source filenames.
2.2.0,You can specify multiple suffix as a list of string:
2.2.0,
2.2.0,"source_suffix = ['.rst', '.md']"
2.2.0,The master toctree document.
2.2.0,General information about the project.
2.2.0,"The version info for the project you're documenting, acts as replacement for"
2.2.0,"|version| and |release|, also used in various other places throughout the"
2.2.0,built documents.
2.2.0,
2.2.0,The short X.Y version.
2.2.0,"The full version, including alpha/beta/rc tags."
2.2.0,The language for content autogenerated by Sphinx. Refer to documentation
2.2.0,for a list of supported languages.
2.2.0,
2.2.0,This is also used if you do content translation via gettext catalogs.
2.2.0,"Usually you set ""language"" from the command line for these cases."
2.2.0,"List of patterns, relative to source directory, that match files and"
2.2.0,directories to ignore when looking for source files.
2.2.0,This patterns also effect to html_static_path and html_extra_path
2.2.0,The name of the Pygments (syntax highlighting) style to use.
2.2.0,"If true, `todo` and `todoList` produce output, else they produce nothing."
2.2.0,-- Options for HTML output ----------------------------------------------
2.2.0,The theme to use for HTML and HTML Help pages.  See the documentation for
2.2.0,a list of builtin themes.
2.2.0,
2.2.0,html_theme = 'sphinx_materialdesign_theme'
2.2.0,html_theme_path = [sphinx_materialdesign_theme.get_path()]
2.2.0,Theme options are theme-specific and customize the look and feel of a theme
2.2.0,"further.  For a list of options available for each theme, see the"
2.2.0,documentation.
2.2.0,
2.2.0,html_theme_options = {}
2.2.0,"Add any paths that contain custom static files (such as style sheets) here,"
2.2.0,"relative to this directory. They are copied after the builtin static files,"
2.2.0,"so a file named ""default.css"" will overwrite the builtin ""default.css""."
2.2.0,"Custom sidebar templates, must be a dictionary that maps document names"
2.2.0,to template names.
2.2.0,
2.2.0,This is required for the alabaster theme
2.2.0,refs: http://alabaster.readthedocs.io/en/latest/installation.html#sidebars
2.2.0,-- Options for HTMLHelp output ------------------------------------------
2.2.0,Output file base name for HTML help builder.
2.2.0,-- Options for LaTeX output ---------------------------------------------
2.2.0,The paper size ('letterpaper' or 'a4paper').
2.2.0,
2.2.0,"'papersize': 'letterpaper',"
2.2.0,"The font size ('10pt', '11pt' or '12pt')."
2.2.0,
2.2.0,"'pointsize': '10pt',"
2.2.0,Additional stuff for the LaTeX preamble.
2.2.0,
2.2.0,"'preamble': '',"
2.2.0,Latex figure (float) alignment
2.2.0,
2.2.0,"'figure_align': 'htbp',"
2.2.0,Grouping the document tree into LaTeX files. List of tuples
2.2.0,"(source start file, target name, title,"
2.2.0,"author, documentclass [howto, manual, or own class])."
2.2.0,-- Options for manual page output ---------------------------------------
2.2.0,One entry per manual page. List of tuples
2.2.0,"(source start file, name, description, authors, manual section)."
2.2.0,-- Options for Texinfo output -------------------------------------------
2.2.0,Grouping the document tree into Texinfo files. List of tuples
2.2.0,"(source start file, target name, title, author,"
2.2.0,"dir menu entry, description, category)"
2.2.0,!/usr/bin/env python
2.2.0,-*- coding: utf-8 -*-
2.2.0,is this reachable?
2.2.0,Read in embeddings
2.2.0,Write to file
2.2.0,converts a SentencePiece vocabulary to the format expected by dynamic data
2.2.0,"(essentially converts float expected counts to ""fixed precision"" int pseudo"
2.2.0,counts)
2.2.0,"Add in default model arguments, possibly added since training."
2.2.0,build_base_model expects updated and validated opts
2.2.0,-*- encoding: utf-8 -*-
2.2.0,!/usr/bin/env python
2.2.0,-*- coding: utf-8 -*-
2.2.0,Author: Rico Sennrich
2.2.0,flake8: noqa
2.2.0,This file is retrieved from https://github.com/rsennrich/subword-nmt
2.2.0,hack for python2/3 compatibility
2.2.0,check version information
2.2.0,some hacking to deal with duplicates (only consider first instance)
2.2.0,don't print end-of-word symbols
2.2.0,sys.stderr.write('cannot split {0} further.\n'.format(segment))
2.2.0,sys.stderr.write('OOV: {0}\n'.format(segment))
2.2.0,sys.stderr.write('OOV: {0}\n'.format(segment))
2.2.0,python 2/3 compatibility
2.2.0,read/write files as UTF-8
2.2.0,!/usr/bin/env python
2.2.0,!/usr/bin/env python
2.2.0,-*- coding: utf-8 -*-
2.2.0,Author: Rico Sennrich
2.2.0,flake8: noqa
2.2.0,This file is retrieved from https://github.com/rsennrich/subword-nmt
2.2.0,hack for python2/3 compatibility
2.2.0,"find all instances of pair, and update frequency/indices around it"
2.2.0,find first symbol
2.2.0,"if first symbol is followed by second symbol, we've found an occurrence of pair (old_word[i:i+2])"
2.2.0,"assuming a symbol sequence ""A B C"", if ""B C"" is merged, reduce the frequency of ""A B"""
2.2.0,"assuming a symbol sequence ""A B C B"", if ""B C"" is merged, reduce the frequency of ""C B""."
2.2.0,"however, skip this if the sequence is A B C B C, because the frequency of ""C B"" will be reduced by the previous code block"
2.2.0,find new pair
2.2.0,"assuming a symbol sequence ""A BC D"", if ""B C"" is merged, increase the frequency of ""A BC"""
2.2.0,"assuming a symbol sequence ""A BC B"", if ""B C"" is merged, increase the frequency of ""BC B"""
2.2.0,"however, if the sequence is A BC BC, skip this step because the count of ""BC BC"" will be incremented by the previous code block"
2.2.0,data structure of pair frequencies
2.2.0,index from pairs to words
2.2.0,version 0.2 changes the handling of the end-of-word token ('</w>');
2.2.0,version numbering allows bckward compatibility
2.2.0,"threshold is inspired by Zipfian assumption, but should only affect speed"
2.2.0,we probably missed the best pair because of pruning; go back to full statistics
2.2.0,"threshold is inspired by Zipfian assumption, but should only affect speed"
2.2.0,python 2/3 compatibility
2.2.0,read/write files as UTF-8
2.2.0,!/usr/bin/env python
2.2.0,-*- coding: utf-8 -*-
2.2.0,!/usr/bin/env python
2.2.0,Avoid functionality on inference
2.2.0,Build embeddings.
2.2.0,Build encoder.
2.2.0,Build embeddings.
2.2.0,Build decoder.
2.2.0,Share the embedding matrix - preprocess with share_vocab required.
2.2.0,src/tgt vocab should be the same if `-share_vocab` is specified.
2.2.0,Update vocabulary embeddings with checkpoint embeddings
2.2.0,Embedding layers
2.2.0,Just for debugging purposes
2.2.0,Remove old vocabulary associated embeddings
2.2.0,for back compat when attention_dropout was not defined
2.2.0,Build Model
2.2.0,Build Generator.
2.2.0,Load the model states from checkpoint or initialize them.
2.2.0,This preserves backward-compat for models using customed layernorm
2.2.0,end of patch for backward compatibility
2.2.0,Update model embeddings with those from the checkpoint
2.2.0,after initialization
2.2.0,!/usr/bin/env python
2.2.0,ensure tensorboard output is written in the directory
2.2.0,of previous checkpoints
2.2.0,Override checkpoint's update_embeddings as it defaults to false
2.2.0,NOTE: It's important that ``opt`` has been validated and updated
2.2.0,at this point.
2.2.0,Build model.
2.2.0,Build optimizer.
2.2.0,Build model saver
2.2.0,Move batch to specified device
2.2.0,Use Tensorboard for visualization during training
2.2.0,Options only during inference
2.2.0,"Truncation options, for text corpus"
2.2.0,"as for False, this will be added in _add_train_general_opts"
2.2.0,Embedding Options
2.2.0,Model Task Options
2.2.0,Encoder-Decoder Options
2.2.0,"group.add('--residual', '-residual',   action=""store_true"","
2.2.0,"help=""Add residual connections between RNN layers."")"
2.2.0,The following options (bridge_extra_node to n_steps) are used
2.2.0,for training with --encoder_type ggnn (Gated Graph Neural Network).
2.2.0,Attention options
2.2.0,Alignement options
2.2.0,Generator and loss options.
2.2.0,GPU
2.2.0,Init options
2.2.0,Pretrained word vectors
2.2.0,Freeze word vectors
2.2.0,Optimization options
2.2.0,learning rate
2.2.0,options relate to data preprare
2.2.0,options relate to train
2.2.0,Alpha and Beta values for Google Length + Coverage penalty
2.2.0,"Described here: https://arxiv.org/pdf/1609.08144.pdf, Section 7"
2.2.0,Length penalty options
2.2.0,Coverage penalty options
2.2.0,Decoding Length constraint
2.2.0,Decoding content constraint
2.2.0,Adding options relate to decoding strategy
2.2.0,Adding option for logging
2.2.0,Copyright 2016 The Chromium Authors. All rights reserved.
2.2.0,Use of this source code is governed by a BSD-style license that can be
2.2.0,found in the LICENSE file.
2.2.0,"Get the key 'value' in the dict, or just use 'value'"
2.2.0,Basic attributes.
2.2.0,Set model in training mode.
2.2.0,UPDATE DROPOUT
2.2.0,Run patience mechanism
2.2.0,"If the patience has reached the limit, stop training"
2.2.0,swap model params w/ moving average
2.2.0,(and keep the original parameters)
2.2.0,Set model in validating mode.
2.2.0,F-prop through the model.
2.2.0,Compute loss.
2.2.0,Update statistics.
2.2.0,Set model back to training mode.
2.2.0,Truncated BPTT: reminder not compatible with accum > 1
2.2.0,1. Create truncated target.
2.2.0,2. F-prop all but generator.
2.2.0,3. Compute loss.
2.2.0,4. Update the parameters and statistics.
2.2.0,Multi GPU gradient gather
2.2.0,"If truncated, don't backprop fully."
2.2.0,TO CHECK
2.2.0,if dec_state is not None:
2.2.0,dec_state.detach()
2.2.0,"in case of multi step gradient accumulation,"
2.2.0,update only after accum batches
2.2.0,For Flake
2.2.0,we avoid padding while mean pooling
2.2.0,incoming and outgoing edge embedding
2.2.0,Find vocab data for tree builting
2.2.0,Propogation Model
2.2.0,Initialize the bridge layer
2.2.0,Token embedding
2.2.0,Initialize graph using formatted input sequence
2.2.0,Number of flagged nodes defines node count for this sample
2.2.0,"(Nodes can have no flags on them, but must be in 'flags' list)."
2.2.0,The total number of integers in the vocab should allow
2.2.0,for all features and edges to be defined.
2.2.0,Use first extra node as only source for decoder init
2.2.0,Average all nodes to get bridge input
2.2.0,"LSTM has hidden and cell state, other only one"
2.2.0,Total number of states
2.2.0,Build a linear layer for each
2.2.0,Initialize the bridge layer
2.2.0,"s_len, batch, emb_dim = emb.size()"
2.2.0,Lengths data is wrapped inside a Tensor.
2.2.0,"LSTM has hidden and cell state, other only one"
2.2.0,Total number of states
2.2.0,Build a linear layer for each
2.2.0,"s_len, batch, emb_dim = emb.size()"
2.2.0,Run the forward pass of every layer of the tranformer.
2.2.0,Dimensions and padding for constructing the word embedding matrix
2.2.0,Dimensions and padding for feature embedding matrices
2.2.0,(these have no effect if feat_vocab_sizes is empty)
2.2.0,The embedding matrix look-up tables. The first look-up table
2.2.0,"is for words. Subsequent ones are for features, if any exist."
2.2.0,The final output size of word + feature vectors. This can vary
2.2.0,from the word vector size if and only if features are defined.
2.2.0,This is the attribute you should access if you need to know
2.2.0,how big your embeddings are going to be.
2.2.0,The sequence of operations that converts the input sequence
2.2.0,into a sequence of embeddings. At minimum this consists of
2.2.0,looking up the embeddings for each word and feature in the
2.2.0,input. Model parameters may require the sequence to contain
2.2.0,additional operations as well.
2.2.0,features must use word_vec_size
2.2.0,features will use feat_vec_size
2.2.0,Some utilitary functions for pretrained embeddings
2.2.0,is this reachable?
2.2.0,Write to file
2.2.0,set the opt in place
2.2.0,set the opt in place
2.2.0,This class is mainly used by decoder.py for RNNs but also
2.2.0,by the CNN / transformer decoder when copy attention is used
2.2.0,CNN has its own attention mechanism ConvMultiStepAttention
2.2.0,Transformer has its own MultiHeadedAttention
2.2.0,mlp wants it with bias
2.2.0,Check input sizes
2.2.0,"(batch, t_len, d) x (batch, d, s_len) --> (batch, t_len, s_len)"
2.2.0,"(batch, t_len, s_len, d)"
2.2.0,one step input
2.2.0,"compute attention scores, as in Luong et al."
2.2.0,Softmax or sparsemax to normalize attention weights
2.2.0,each context vector c_t is the weighted average
2.2.0,over all the source hidden states
2.2.0,concatenate
2.2.0,Check output sizes
2.2.0,Check output sizes
2.2.0,clamping necessary because of numerical errors: loss should be lower
2.2.0,"bounded by zero, but negative values near zero are possible without"
2.2.0,the clamp
2.2.0,from onmt.utils.misc import aeq
2.2.0,CHECKS
2.2.0,"batch, k_len, d = key.size()"
2.2.0,"batch_, k_len_, d_ = value.size()"
2.2.0,"aeq(batch, batch_)"
2.2.0,"aeq(k_len, k_len_)"
2.2.0,"aeq(d, d_)"
2.2.0,"batch_, q_len, d_ = query.size()"
2.2.0,"aeq(batch, batch_)"
2.2.0,"aeq(d, d_)"
2.2.0,"aeq(self.model_dim % 8, 0)"
2.2.0,if mask is not None:
2.2.0,"batch_, q_len_, k_len_ = mask.size()"
2.2.0,"aeq(batch_, batch)"
2.2.0,"aeq(k_len_, k_len)"
2.2.0,aeq(q_len_ == q_len)
2.2.0,END CHECKS
2.2.0,"1) Project key, value, and query."
2.2.0,1 or key_len x key_len
2.2.0,1 or key_len x key_len x dim_per_head
2.2.0,1 or key_len x key_len x dim_per_head
2.2.0,2) Calculate and scale scores.
2.2.0,batch x num_heads x query_len x key_len
2.2.0,3) Apply attention dropout and compute context vectors.
2.2.0,CHECK
2.2.0,"batch_, q_len_, d_ = output.size()"
2.2.0,"aeq(q_len, q_len_)"
2.2.0,"aeq(batch, batch_)"
2.2.0,"aeq(d, d_)"
2.2.0,Return multi-head attn
2.2.0,At the moment this class is only used by embeddings.Embeddings look-up tables
2.2.0,-*- coding: utf-8 -*-
2.2.0,checks
2.2.0,"batch, channel, height, width = base_target_emb.size()"
2.2.0,"batch_, channel_, height_, width_ = input_from_dec.size()"
2.2.0,"enc_batch, enc_channel, enc_height = encoder_out_top.size()"
2.2.0,"enc_batch_, enc_channel_, enc_height_ = encoder_out_combine.size()"
2.2.0,out_features * in_features
2.2.0,norm is out_features * 1
2.2.0,batch_size * out_features
2.2.0,out_features
2.2.0,out_features
2.2.0,batch_size * out_features
2.2.0,"out_channels, in_channels // groups, * kernel_size"
2.2.0,out_features
2.2.0,This is used nowhere in the code at the moment (Vincent Nguyen 05/18/2018)
2.2.0,"in_channels, out_channels, *kernel_size"
2.2.0,"in_channels, out_channels, *kernel_size"
2.2.0,"self.out_channels, 1"
2.2.0,out_features
2.2.0,out_features
2.2.0,store roots on diagonal
2.2.0,CHECKS
2.2.0,Original probabilities.
2.2.0,Probability of copying p(z=1) batch.
2.2.0,Probability of not copying: p_{word}(w) * (1 - p(z))
2.2.0,probabilities assigned by the model to the gold targets
2.2.0,probability of tokens copied from source
2.2.0,Set scores for unk to 0 and add eps
2.2.0,find the indices in which you do not use the copy mechanism
2.2.0,Drop padding.
2.2.0,this block does not depend on the loss value computed above
2.2.0,and is used only for stats
2.2.0,this block does not depend on the loss value computed above
2.2.0,and is used only for stats
2.2.0,Correct target copy token instead of <unk>
2.2.0,tgt[i] = align[i] + len(tgt_vocab)
2.2.0,for i such that tgt[i] == 0 and align[i] != 0
2.2.0,Compute sum of perplexities for stats
2.2.0,this part looks like it belongs in CopyGeneratorLoss
2.2.0,Compute Loss as NLL divided by seq length
2.2.0,Compute Total Loss per sequence in batch
2.2.0,Divide by length of each sequence and sum
2.2.0,Do nothing
2.2.0,Do nothing
2.2.0,If case markup placeholder
2.2.0,Punctuation only (assumes joiner is also some punctuation token)
2.2.0,Auto import python files in this directory
2.2.0,1. sample number of tokens to corrupt
2.2.0,2. sample positions to corrput
2.2.0,3. sample corrupted values
2.2.0,1. sample number of tokens to corrupt
2.2.0,2. sample positions to corrput
2.2.0,3. Drop token on chosen position
2.2.0,1. sample number of tokens to corrupt
2.2.0,2. sample positions to corrput
2.2.0,3. mask word on chosen position
2.2.0,"Sharing options among `TokenizerTransform`s, same name conflict in"
2.2.0,this scope will be resolved by remove previous occurrence in parser
2.2.0,subword regularization(or BPE dropout) options:
2.2.0,subword vocabulary restriction options:
2.2.0,derterministic subwording
2.2.0,subword sampling when nbest_size > 1 or -1
2.2.0,alpha should be 0.0 < alpha < 1.0
2.2.0,Load vocabulary file if provided and set threshold
2.2.0,Load Subword Model
2.2.0,-1: keep everything (i.e. 1 mask per token)
2.2.0,0: replace everything (i.e. no mask)
2.2.0,1: 1 mask per span
2.2.0,view each subword as word start / input is word level token
2.2.0,Pretend it ends with a full stop so last span is a sentence
2.2.0,"Tokens that are full stops, where the previous token is not"
2.2.0,Make sure we have enough to mask
2.2.0,Trim to masking budget
2.2.0,Handle 0-length mask (inserts) separately
2.2.0,assert is_word_start[-1] == 0
2.2.0,assert tokens_length - 1 not in indices
2.2.0,"keep index, but replace it with [MASK]"
2.2.0,"acts as a long length, so spans don't go over the end of doc"
2.2.0,next position from each word_start
2.2.0,delete token: 1 mask/remove per span
2.2.0,"keep index, but replace it with [MASK]: 1 mask per token"
2.2.0,A bit faster when all lengths are 1
2.2.0,to cover whole token
2.2.0,delete token
2.2.0,"keep index, but replace it with [MASK]"
2.2.0,assert tokens_length - 1 not in indices
2.2.0,initialize fields at the top of each unit test to prevent
2.2.0,any undesired stateful effects
2.2.0,"this test touches the file system, so it could be considered an"
2.2.0,integration test
2.2.0,write utf-8 bytes
2.2.0,batch 0 will always predict EOS. The other batches will predict
2.2.0,non-eos scores.
2.2.0,"""best"" prediction is eos - that should be blocked"
2.2.0,include at least one prediction OTHER than EOS
2.2.0,that is greater than -1e20
2.2.0,now batch 0 has ended and no others have
2.2.0,initial step
2.2.0,batch 0 dies on step 0
2.2.0,include at least one prediction OTHER than EOS
2.2.0,that is greater than -1e20
2.2.0,step 2
2.2.0,(old) batch 8 dies on step 1
2.2.0,step 3
2.2.0,everything dies
2.2.0,initial step
2.2.0,batch 0 dies on step 0
2.2.0,include at least one prediction OTHER than EOS
2.2.0,that is greater than -1e20
2.2.0,step 2
2.2.0,(old) batch 8 dies on step 1
2.2.0,step 3
2.2.0,everything dies
2.2.0,initial step
2.2.0,finish one beam
2.2.0,include at least one prediction OTHER than EOS
2.2.0,that is greater than -1e20
2.2.0,step 2
2.2.0,finish example in last batch
2.2.0,(old) batch 8 dies on step 1
2.2.0,step 3
2.2.0,everything dies
2.2.0,initial step
2.2.0,batch 0 dies on step 0
2.2.0,include at least one prediction OTHER than EOS
2.2.0,that is greater than -1e20
2.2.0,step 2
2.2.0,(old) batch 8 dies on step 1
2.2.0,step 3
2.2.0,everything dies
2.2.0,illegal_weights_mask = torch.ByteTensor([
2.2.0,"[0, 0, 0, 0, 0, 0, 0],"
2.2.0,"[0, 0, 0, 1, 1, 1, 1],"
2.2.0,"[0, 0, 0, 0, 0, 1, 1],"
2.2.0,"[0, 0, 1, 1, 1, 1, 1]])"
2.2.0,TODO: fix for pytorch 0.3
2.2.0,illegal_weights = alignments.masked_select(illegal_weights_mask)
2.2.0,"self.assertEqual(0.0, illegal_weights.data.sum())"
2.2.0,this could be considered an integration test because it touches
2.2.0,the filesystem for the config file (and the models)
2.2.0,no dummy prefix
2.2.0,no dummy prefix
2.2.0,transforms that require vocab will not create if not provide vocab
2.2.0,1. Init first transform in the pipe
2.2.0,2. Init second transform in the pipe
2.2.0,3. Sequential combine them into a transform pipe
2.2.0,4. apply transform pipe for example
2.2.0,"5. example after the pipe exceed the length limit, thus filtered"
2.2.0,6. Transform statistics registed (here for filtertoolong)
2.2.0,"7. after report, statistics become empty as a fresh start"
2.2.0,filter_transform.warm_up()
2.2.0,test BPE-dropout:
2.2.0,1. disable bpe dropout for not training example
2.2.0,2. enable bpe dropout for training example
2.2.0,3. (NOTE) disable dropout won't take effect if already seen
2.2.0,this is caused by the cache mechanism in bpe:
2.2.0,return cached subword if the original token is seen when no dropout
2.2.0,test SP regularization:
2.2.0,1. enable regularization for training example
2.2.0,2. disable regularization for not training example
2.2.0,Not apply token drop for not training example
2.2.0,apply token drop for training example
2.2.0,Not apply token mask for not training example
2.2.0,apply token mask for training example
2.2.0,require vocabs to warm_up
2.2.0,Not apply token mask for not training example
2.2.0,apply token mask for training example
2.2.0,"Defalt: full_stop_token=[""."", ""?"", ""!""]"
2.2.0,"Defalt: full_stop_token=[""."", ""?"", ""!""]"
2.2.0,random_ratio of inserted tokens are chosen in vocab
2.2.0,others are MASK_TOK
2.2.0,"insert_ratio=0.0,"
2.2.0,"random_ratio=0.0,"
2.2.0,"Defalt: full_stop_token=[""."", ""?"", ""!""]"
2.2.0,all token are considered as an individual word
2.2.0,1. tokens are dropped when replace_length is 0
2.2.0,"print(f""token delete: {masked} / {tokens}"")"
2.2.0,2. tokens are replaced by MASK when replace_length is 1
2.2.0,"print(f""token mask: {masked} / {tokens}"")"
2.2.0,"insert_ratio=0.0,"
2.2.0,"random_ratio=0.0,"
2.2.0,"Defalt: full_stop_token=[""."", ""?"", ""!""]"
2.2.0,start token of word are identified using subword marker
2.2.0,"1. replace_length 0: ""words"" are dropped"
2.2.0,"print(f""word delete: {masked} / {tokens}"")"
2.2.0,"self.assertEqual(len(masked), n_words - n_masked)"
2.2.0,"2. replace_length 1: ""words"" are replaced with a single MASK"
2.2.0,"print(f""whole word single mask: {masked} / {tokens}"")"
2.2.0,len(masked) depend on number of tokens in select word
2.2.0,"3. replace_length -1: all tokens in ""words"" are replaced with MASK"
2.2.0,"print(f""whole word multi mask: {masked} / {tokens}"")"
2.2.0,number of mask_tok depend on number of tokens in selected word
2.2.0,number of MASK_TOK can be greater than n_masked
2.2.0,"insert_ratio=0.5,"
2.2.0,"random_ratio=0.3,"
2.2.0,"Defalt: full_stop_token=[""."", ""?"", ""!""]"
2.2.0,start token of word are identified using subword marker
2.2.0,n_words = sum(token_starts)
2.2.0,n_masked = math.ceil(n_words * bart_noise.mask_ratio)
2.2.0,"print(f""Text Span Infilling: {infillied} / {tokens}"")"
2.2.0,"print(n_words, n_masked)"
2.2.0,!/usr/bin/env python
2.2.0,-*- coding: utf-8 -*-
2.2.0,Inject some dummy training options that may needed when build fields
2.2.0,Remove the generated *pt files.
2.2.0,Remove the generated data samples
2.2.0,all beams repeat (beam >= 1 repeat dummy scores)
2.2.0,predict repeat_idx over and over again
2.2.0,"before repeat, scores are either 0 or -inf"
2.2.0,"on repeat, `repeat_idx` score is BLOCKED_SCORE"
2.2.0,"(but it's still the best score, thus we have"
2.2.0,"[BLOCKED_SCORE, -inf, -inf, -inf, -inf]"
2.2.0,repetitions keeps maximizing score
2.2.0,"index 0 has been blocked, so repeating=>+0.0 score"
2.2.0,other indexes are -inf so repeating=>BLOCKED_SCORE
2.2.0,which is higher
2.2.0,beam 0 and beam >=2 will repeat (beam >= 2 repeat dummy scores)
2.2.0,non-interesting beams are going to get dummy values
2.2.0,"on initial round, only predicted scores for beam 0"
2.2.0,matter. Make two predictions. Top one will be repeated
2.2.0,"in beam zero, second one will live on in beam 1."
2.2.0,predict the same thing in beam 0
2.2.0,continue pushing around what beam 1 predicts
2.2.0,"now beam 0 dies (along with the others), beam 1 -> beam 0"
2.2.0,"now beam 0 dies (along with the others), beam 1 -> beam 0"
2.2.0,beam 0 and beam >= 2 will repeat (beam 2 repeats excluded idx)
2.2.0,non-interesting beams are going to get dummy values
2.2.0,predict the same thing in beam 0
2.2.0,continue pushing around what beam 1 predicts
2.2.0,predict the allowed-repeat again in beam 2
2.2.0,"now beam 0 dies, beam 1 -> beam 0, beam 2 -> beam 1"
2.2.0,and the rest die
2.2.0,"since all preds after i=0 are 0, we can check"
2.2.0,that the beam is the correct idx by checking that
2.2.0,the curr score is the initial score
2.2.0,beam 0 will always predict EOS. The other beams will predict
2.2.0,non-eos scores.
2.2.0,non-interesting beams are going to get dummy values
2.2.0,"""best"" prediction is eos - that should be blocked"
2.2.0,include at least beam_sz predictions OTHER than EOS
2.2.0,that are greater than -1e20
2.2.0,predict eos in beam 0
2.2.0,provide beam_sz other good predictions
2.2.0,now the top beam has ended and no others have
2.2.0,"not of interest, but want to make sure it keeps running"
2.2.0,since only beam 0 terminates and n_best = 2
2.2.0,"this is also a test that when block_ngram_repeat=0,"
2.2.0,repeating is acceptable
2.2.0,non-interesting beams are going to get dummy values
2.2.0,"""best"" prediction is eos - that should be blocked"
2.2.0,include at least beam_sz predictions OTHER than EOS
2.2.0,that are greater than -1e20
2.2.0,predict eos in beam 1
2.2.0,provide beam_sz other good predictions in other beams
2.2.0,provide beam_sz other good predictions in other beams
2.2.0,beam 1 dies on min_length
2.2.0,beam 0 dies on the step after beam 1 dies
2.2.0,"inp_lens is tiled in initialize, reassign to make attn match"
2.2.0,non-interesting beams are going to get dummy values
2.2.0,"""best"" prediction is eos - that should be blocked"
2.2.0,include at least beam_sz predictions OTHER than EOS
2.2.0,that are greater than -1e20
2.2.0,predict eos in beam 1
2.2.0,provide beam_sz other good predictions in other beams
2.2.0,provide beam_sz other good predictions in other beams
2.2.0,no top beams are finished yet
2.2.0,beam 1 dies on min_length
2.2.0,no top beams are finished yet
2.2.0,beam 0 dies on the step after beam 1 dies
2.2.0,top beam is finished now so there are attentions
2.2.0,two beams are finished in each batch
2.2.0,second dim is cut down to the non-padded src length
2.2.0,first dim is equal to the time of death
2.2.0,(beam 0 died at current step - adjust for SOS)
2.2.0,(beam 1 died at last step - adjust for SOS)
2.2.0,behavior gets weird when beam is already done so just stop
2.2.0,this is just test_beam.TestBeamAgainstReferenceCase repeated
2.2.0,in each batch.
2.2.0,"init_preds: [4, 3, 5, 6, 7] - no EOS's"
2.2.0,no EOS's yet
2.2.0,"[5, 3, 2, 6, 0], so beam 2 predicts EOS!"
2.2.0,assumes beam 2 finished on last step
2.2.0,ended beam 2 shouldn't continue
2.2.0,"[2, 5, 3, 6, 0] repeat self.BATCH_SZ, so beam 0 predicts EOS!"
2.2.0,"[-2.4879, -3.8910, -4.1010, -4.2010, -4.4010] repeat self.BATCH_SZ"
2.2.0,another beam is finished in all batches
2.2.0,new beam 0 finished
2.2.0,new beam 0 is old beam 3
2.2.0,assumes beam 0 finished on last step
2.2.0,"[5, 2, 6, 1, 0] repeat self.BATCH_SZ, so beam 1 predicts EOS!"
2.2.0,new beam 1 finished
2.2.0,new beam 1 is old beam 4
2.2.0,this could be considered an integration test because it tests
2.2.0,interactions between the GNMT scorer and the beam
2.2.0,"-data option is required, but not used in this test, so dummy."
2.2.0,len x batch x nfeat
2.2.0,Initialize vectors to compare size with
2.2.0,Ensure correct sizes and types
2.2.0,Make sure that output has the correct size and type
2.2.0,"[('encoder_type', 'transformer'),"
2.2.0,"('word_vec_size', 16), ('rnn_size', 16)],"
2.2.0,""""""" Only do SRU test if requirment is safisfied. """""""
2.2.0,SRU doesn't support input_feed.
2.2.0,first check there's nothing unexpectedly not trainable
2.2.0,ok: word embeddings shouldn't be trainable
2.2.0,if word vecs are freezed
2.2.0,ok: positional encodings shouldn't be trainable
2.2.0,then check nothing unexpectedly trainable
2.2.0,Decoder state
2.2.0,Build the RNN.
2.2.0,Set up the context gate.
2.2.0,Set up the standard attention.
2.2.0,The encoder hidden is  (layers*directions) x batch x dim.
2.2.0,We need to convert it to layers x batch x (directions*dim).
2.2.0,Init the input feed.
2.2.0,Update the state with the result.
2.2.0,Concatenates sequence of tensors along a new dimension.
2.2.0,NOTE: v0.3 to 0.4: dec_outs / attns[*] may not be list
2.2.0,(in particular in case of SRU) it was not raising error in 0.3
2.2.0,since stack(Variable) was allowed.
2.2.0,"In 0.4, SRU returns a tensor that shouldn't be stacke"
2.2.0,Check
2.2.0,Calculate the attention.
2.2.0,Calculate the context gate.
2.2.0,Additional args check.
2.2.0,END Additional args check.
2.2.0,Input feed concatenates hidden state with
2.2.0,input at every time step.
2.2.0,TODO: context gate should be employed
2.2.0,instead of second RNN transform.
2.2.0,Update the coverage attention.
2.2.0,Decoder State
2.2.0,CNNDecoder has its own attention mechanism.
2.2.0,Set up a separate copy attention layer if needed.
2.2.0,The output of CNNEncoder.
2.2.0,The combination of output of CNNEncoder and source embeddings.
2.2.0,Process the result and update the attentions.
2.2.0,Update the state.
2.2.0,TODO change the way attns is returned dict => list or tuple (onnx)
2.2.0,Memory_lengths is a single tensor shared between all models.
2.2.0,This assumption will not hold if Translator is modified
2.2.0,to calculate memory_lengths as something other than the length
2.2.0,of the input.
2.2.0,"return _, (B, Q_len, K_len)"
2.2.0,"layer average attention across heads, get ``(B, Q, K)``"
2.2.0,"Case 1: no full_context, no align heads -> layer avg baseline"
2.2.0,"Case 2: no full_context, 1 align heads -> guided align"
2.2.0,"Case 3: full_context, 1 align heads -> full cte guided align"
2.2.0,BoolTensor was introduced in pytorch 1.2
2.2.0,T: could be 1 in the case of stepwise decoding or tgt_len
2.2.0,masking is necessary when sequence length is greater than one
2.2.0,Decoder State
2.2.0,"previously, there was a GlobalAttention module here for copy"
2.2.0,"attention. But it was never actually used -- the ""copy"" attention"
2.2.0,just reuses the context attention.
2.2.0,"attns[""align""] = torch.stack(attn_aligns, 0).mean(0)  # All avg"
2.2.0,TODO change the way attns is returned dict => list or tuple (onnx)
2.2.0,T: could be 1 in the case of stepwise decoding or tgt_len
2.2.0,masking is necessary when sequence length is greater than one
2.2.0,TODO change the way attns is returned dict => list or tuple (onnx)
2.2.0,"buffer size in bytes, determine equiv. # of elements based on data type"
2.2.0,copy tensors into buffer_t
2.2.0,all-reduce and rescale
2.2.0,copy all-reduced buffer back into tensors
2.2.0,"tensor is bigger than buffer, all-reduce and rescale directly"
2.2.0,"buffer is full, all-reduce and replace buffer with grad"
2.2.0,add tensor to buffer
2.2.0,NOTE: stride (if needed) is handled at the
2.2.0,generator (train_iter) level
2.2.0,Move batch to correspond device_id when consumer iterate
2.2.0,hack to dodge unpicklable `dict_keys`
2.2.0,"propagate exception to parent process, keeping original traceback"
2.2.0,TODO: Find a better way to check for sparse gradients.
2.2.0,we use here a FusedAdam() copy of an old Apex repo
2.2.0,In this case use the old FusedAdam with FP16_optimizer wrapper
2.2.0,Load everything from the checkpoint.
2.2.0,Build everything from scratch.
2.2.0,"Reset optimizer, keep options."
2.2.0,"Reset options, keep optimizer."
2.2.0,State can be partially restored.
2.2.0,"unscaled optimizer's gradients (already done therefore skip),"
2.2.0,skips optimizer.step() if gradients contain infs/NaNs.
2.2.0,Updates the scale for next iteration.
2.2.0,Code below is an implementation of https://arxiv.org/pdf/1804.04235.pdf
2.2.0,inspired but modified from https://github.com/DeadAt0m/adafactor-pytorch
2.2.0,backward compatibility
2.2.0,assuming a list/generator of parameter means single group
2.2.0,compute combined scale factor for this group
2.2.0,norm is in fact norm*scale
2.2.0,note: p.grad should not ever be set for correct operation of
2.2.0,mixed precision optimizer that sometimes sends None gradients
2.2.0,State initialization
2.2.0,Exponential moving average of gradient values
2.2.0,Exponential moving average of squared gradient values
2.2.0,-*- coding: utf-8 -*-
2.2.0,if the loss function operates on vectors of raw logits instead of
2.2.0,"probabilities, only the first part of the generator needs to be"
2.2.0,"passed to the NMTLossCompute. At the moment, the only supported"
2.2.0,loss function of this kind is the sparsemax loss.
2.2.0,"attn_align should be in (batch_size, pad_tgt_size, pad_src_size)"
2.2.0,"align_idx should be a Tensor in size([N, 3]), N is total number"
2.2.0,"of align src-tgt pair in current batch, each as"
2.2.0,"['sent_NÂ°_in_batch', 'tgt_id+1', 'src_id'] (check AlignField)"
2.2.0,NOTE: tgt-src ref alignement that in range_ of shard
2.2.0,(coherent with batch.tgt)
2.2.0,"align_head contains value in [0, 1) presenting attn prob,"
2.2.0,0 was resulted by the context attention src_pad_mask
2.2.0,"So, the correspand position in ref_align should also be 0"
2.2.0,"Therefore, clip align_head to > 1e-18 should be bias free."
2.2.0,non_none: the subdict of the state dictionary where the values
2.2.0,are not None.
2.2.0,"Now, the iteration:"
2.2.0,state is a dictionary of sequences of tensor-like but we
2.2.0,want a sequence of dictionaries of tensors.
2.2.0,"First, unzip the dictionary into a sequence of keys and a"
2.2.0,sequence of tensor-like sequences.
2.2.0,"Now, yield a dictionary for each shard. The keys are always"
2.2.0,the same. values is a sequence of length #keys where each
2.2.0,element is a sequence of length #shards. We want to iterate
2.2.0,"over the shards, not over the keys: therefore, the values need"
2.2.0,to be re-zipped by shard and then each shard can be paired
2.2.0,with the keys.
2.2.0,Assumed backprop'd
2.2.0,Check Transforms
2.2.0,Check path
2.2.0,tgt is src for LM task
2.2.0,Check prefix: will be used when use prefix transform
2.2.0,Check weight
2.2.0,Check features
2.2.0,validation when train:
2.2.0,Check embeddings stuff
2.2.0,"Backward compatibility with ""fix_word_vecs_*"" opts"
2.2.0,encoder and decoder should be same sizes
2.2.0,"Load default opt values, then overwrite with the opts in"
2.2.0,"the checkpoint. That way, if there are new options added,"
2.2.0,the defaults are used.
2.2.0,Don't do anything
2.2.0,Update best score of each criteria
2.2.0,Reset tolerance
2.2.0,Update current status
2.2.0,Decrease tolerance
2.2.0,Log
2.2.0,Log
2.2.0,Get a list of world_size lists with len(stat_list) Statistics objects
2.2.0,SRU doesn't support PackedSequence.
2.2.0,-*- coding: utf-8 -*-
2.2.0,threshold on 1 to avoid div by 0
2.2.0,treat alignment matrix one by one as each have different lengths
2.2.0,No alignment if not exist valid tgt token
2.2.0,get valid alignment (sub-matrix from full paded aligment matrix)
2.2.0,In case there is a final case_markup when new_spacer is on
2.2.0,-*- coding: utf-8 -*-
2.2.0,this one is needed for torchtext random call (shuffled iterator)
2.2.0,in multi gpu it ensures datasets are read in the same order
2.2.0,some cudnn methods can be random even after fixing the seed
2.2.0,unless you tell it to be deterministic
2.2.0,This one is needed for various tranfroms
2.2.0,These ensure same initialization in multi gpu mode
2.2.0,Shift values to be >= 0
2.2.0,we need to check the model path + any tokenizer path
2.2.0,fast-forward if loaded from state
2.2.0,NOTE: `rnn.pack_padded_sequence` requires that a
2.2.0,"minibatch be sorted by decreasing order, which"
2.2.0,requires reversing relative to typical sort keys
2.2.0,Maintains the longest src and tgt length in the current batch
2.2.0,Reset current longest length at a new batch (count=1)
2.2.0,Src: [<bos> w1 ... wN <eos>]
2.2.0,Tgt: [w1 ... wM <eos>]
2.2.0,coding: utf-8
2.2.0,make a small vocab containing just the tokens in the source sequence
2.2.0,add init_token and eos_token according to src construction
2.2.0,Map source tokens to indices in the dynamic dict.
2.2.0,self.src_vocabs is used in collapse_copy_scores and Translator.py
2.2.0,this assumes src_field and tgt_field are both text
2.2.0,fields needs to have only keys that examples have as attrs
2.2.0,avoid infinite recursion when fields isn't defined
2.2.0,this is a hack: appears quicker to apply it here
2.2.0,than in the ParallelCorpusIterator
2.2.0,Make features part of src as in TextMultiField
2.2.0,"{'src': {'src': ..., 'feat1': ...., 'feat2': ....}}"
2.2.0,NOTE: moved to DatasetAdapter._process method in iterator.py
2.2.0,item = self.transform.apply(
2.2.0,"example, is_train=self.infinitely, corpus_name=self.cid)"
2.2.0,empty example: skip
2.2.0,-*- coding: utf-8 -*-
2.2.0,backwards compatibility
2.2.0,monkey-patch to make torchtext Vocab's pickleable
2.2.0,"+1 for tgt side to keep coherent after ""bos"" padding,"
2.2.0,"register ['NÂ°_in_batch', 'tgt_id+1', 'src_id']"
2.2.0,this is basically copy-pasted from torchtext.
2.2.0,counters changes in place
2.2.0,keep the order of tokens specified in the vocab file by
2.2.0,adding them to the counter with decreasing counting values
2.2.0,`tgt_vocab_size` is ignored when sharing vocabularies
2.2.0,return vocab to dump with standard name
2.2.0,empty train_dataset_files so that vocab is only loaded from
2.2.0,"given paths in src_vocab_path, tgt_vocab_path"
2.2.0,Load vocabulary
2.2.0,Drop the none-using from memory but keep the last
2.2.0,"in the long run, shouldn't it be possible to do this by calling"
2.2.0,build_vocab with both the src and tgt data?
2.2.0,coding: utf-8
2.2.0,several data readers need optional dependencies. There's no
2.2.0,appropriate builtin exception
2.2.0,NOTE: not support tgt feats yet
2.2.0,-*- coding: utf-8 -*-
2.2.0,Legacy function. Currently it only truncates input if truncate is set.
2.2.0,mix this with partial
2.2.0,batch (list(list(list))): batch_size x len(self.fields) x seq_len
2.2.0,lengths: batch_size
2.2.0,data: seq_len x batch_size x len(self.fields)
2.2.0,Base field
2.2.0,Feats fields
2.2.0,"Legacy function, it is not really necessary"
2.2.0,flake8: noqa
2.2.0,For command-line option parsing
2.2.0,"Check pass, set the args."
2.2.0,"This SRU version implements its own cuda-level optimization,"
2.2.0,so it requires that:
2.2.0,1. `cupy` and `pynvrtc` python package installed.
2.2.0,2. pytorch is built with cuda support.
2.2.0,3. library path set: export LD_LIBRARY_PATH=<cuda lib path>.
2.2.0,Check 1.
2.2.0,Check 2.
2.2.0,Check 3.
2.2.0,This sets up device to use.
2.2.0,-> directions x batch x dim
2.2.0,For DEBUG
2.2.0,"size = (length, batch, x.size(-1)) \"
2.2.0,"if x.dim() == 3 else (batch, x.size(-1))"
2.2.0,grad_x = x.new(*x.size()) if k_ == 3 else x.new(*size).zero_()
2.2.0,Normal use
2.2.0,"An entry check here, will catch on train side and translate side"
2.2.0,if requirements are not satisfied.
2.2.0,RNNDecoderState wraps hidden as a tuple.
2.2.0,fh -> (layers*directions) x batch x dim
2.2.0,"No encoder in LM, seq2seq count formatting kept"
2.2.0,_check_save_model_path
2.2.0,NOTE: We need to trim the vocab to remove any unk tokens that
2.2.0,were not originally here.
2.2.0,!/usr/bin/env python
2.2.0,!/usr/bin/env python
2.2.0,!/usr/bin/env python
2.2.0,-*- coding: utf-8 -*-
2.2.0,!/usr/bin/env python
2.2.0,!/usr/bin/env python
2.2.0,!/usr/bin/env python
2.2.0,import onmt.opts as opts
2.2.0,Set sharing strategy manually instead of default based on the OS.
2.2.0,"maybe prepare pretrained embeddings, if any"
2.2.0,Load checkpoint if we resume from a previous training.
2.2.0,Report src and tgt vocab sizes
2.2.0,Create a thread to listen for errors in the child processes.
2.2.0,Train with multiprocessing.
2.2.0,"This does not work if we merge with the first loop, not sure why"
2.2.0,Get the iterator to generate from
2.2.0,"Once training is done, we can terminate the producers"
2.2.0,magic indices
2.2.0,result caching
2.2.0,fix length constraint and remove eos from count
2.2.0,add one to account for BOS. Don't account for EOS because hitting
2.2.0,this implies it hasn't been found.
2.2.0,we don't block nothing if the user doesn't want it
2.2.0,we can't block nothing beam's too short
2.2.0,we check paths one by one
2.2.0,we don't forbid nothing if the user doesn't want it
2.2.0,we can't forbid nothing if beam's too short
2.2.0,Reordering forbidden_tokens following beam selection
2.2.0,We rebuild a dict to ensure we get the value and not the pointer
2.2.0,Grabing the newly selected tokens and associated ngram
2.2.0,skip the blocking if any token in current_ngram is excluded
2.2.0,"pickups: Tensor where specified index were set to 1, others 0"
2.2.0,"dropdowns: opposite of pickups, 1 for those shouldn't pick"
2.2.0,Minus dropdowns to log_probs making probabilities of
2.2.0,unspecified index close to 0
2.2.0,"prediction step have surpass length of given target_prefix,"
2.2.0,no need to further change this attr
2.2.0,keep indices until overflowing p
2.2.0,Set all logits that are not in the top-p to -10000.
2.2.0,This puts the probabilities close to 0.
2.2.0,Set all logits that are not in the top-k to -10000.
2.2.0,This puts the probabilities close to 0.
2.2.0,"For temp=0.0, take the argmax to avoid divide-by-zero errors."
2.2.0,keep_topk=1 is also equivalent to argmax.
2.2.0,maybe fix some prediction at this step by modifying log_probs
2.2.0,"shape: (sum(~ self.is_finished), 1)"
2.2.0,in LM task memory_lengths is associated with currently generated src
2.2.0,and therefore needs to follow the generation
2.2.0,!/usr/bin/env python
2.2.0,Maintains the longest src and tgt length in the current batch
2.2.0,Reset current longest length at a new batch (count=1)
2.2.0,max_tgt_in_batch = 0
2.2.0,Src: [<bos> w1 ... wN <eos>]
2.2.0,Tgt: [w1 ... wM <eos>]
2.2.0,for debugging
2.2.0,TODO: maybe add dynamic part
2.2.0,Statistics
2.2.0,Turn any copied words into UNKs.
2.2.0,"Decoder forward, takes [tgt_len, batch, nfeats] as input"
2.2.0,"and [src_len, batch, hidden] as memory_bank"
2.2.0,"in case of inference tgt_len = 1, batch = beam times batch_size"
2.2.0,"in case of Gold Scoring tgt_len = actual length, batch = 1 batch"
2.2.0,Generator forward.
2.2.0,"returns [(batch_size x beam_size) , vocab ] when 1 step"
2.2.0,"or [ tgt_len, batch_size, vocab ] when full sentence"
2.2.0,"here we have scores [tgt_lenxbatch, vocab] or [beamxbatch, vocab]"
2.2.0,"returns [(batch_size x beam_size) , vocab ] when 1 step"
2.2.0,"or [ tgt_len, batch_size, vocab ] when full sentence"
2.2.0,(0) add BOS and padding to tgt prediction
2.2.0,(1) Encoder forward.
2.2.0,(2) Repeat src objects `n_best` times.
2.2.0,"We use batch_size x n_best, get ``(src_len, batch * n_best, nfeat)``"
2.2.0,"(3) Init decoder with n_best src,"
2.2.0,"reshape tgt to ``(len, batch * n_best, nfeat)``"
2.2.0,masked_select
2.2.0,get aligned src id for each prediction's valid tgt tokens
2.2.0,TODO: support these blacklisted features
2.2.0,(0) Prep the components of the search.
2.2.0,(1) Run the encoder on the src.
2.2.0,(2) prep decode_strategy. Possibly repeat src objects.
2.2.0,(3) Begin decoding step by step:
2.2.0,Reorder states.
2.2.0,TODO: support these blacklisted features
2.2.0,hack [min_len_batch-1:] because expect <bos>
2.2.0,(0) Prep the components of the search.
2.2.0,(1) split src into src and target_prefix to avoid padding.
2.2.0,(2) init decoder
2.2.0,(3) prep decode_strategy. Possibly repeat src objects.
2.2.0,(4) Begin decoding step by step:
2.2.0,Reorder states.
2.2.0,select indexes in model state/cache
2.2.0,beam parameters
2.2.0,beam state
2.2.0,BoolTensor was introduced in pytorch 1.2
2.2.0,"""global state"" of the old beam"
2.2.0,buffers for the topk scores and 'backpointer'
2.2.0,for testing
2.2.0,maybe fix some prediction at this step by modifying log_probs
2.2.0,Flatten probs into a list of possibilities.
2.2.0,Penalize beams that finished.
2.2.0,"on real data (newstest2017) with the pretrained transformer,"
2.2.0,it's faster to not move this back to the original device
2.2.0,Store finished hypotheses for this batch.
2.2.0,End condition is the top beam finished and we can return
2.2.0,n_best hypotheses.
2.2.0,"If all sentences are translated, no need to go further."
2.2.0,Remove finished batches for the next step.
2.2.0,using integer division to get an integer _B without casting
2.2.0,force the output to be longer than self.min_length
2.2.0,Multiply probs by the beam probability.
2.2.0,"if the sequence ends now, then the penalty is the current"
2.2.0,"length + 1, to include the EOS token"
2.2.0,Avoid any direction that would repeat unwanted ngrams
2.2.0,Pick up candidate token by curr_scores
2.2.0,Recover log probs.
2.2.0,Length penalty is just a scalar. It doesn't matter if it's applied
2.2.0,before or after the topk.
2.2.0,Resolve beam origin and map to batch index flat representation.
2.2.0,Append last prediction.
2.2.0,update global state (step == 1)
2.2.0,update global state (step > 1)
2.2.0,"shape: (batch_size x beam_size, 1)"
2.2.0,in LM task memory_lengths is associated with currently generated src
2.2.0,and therefore needs to follow the generation
2.2.0,in LM task memory_lengths is associated with currently generated src
2.2.0,and therefore needs to follow the generation
2.2.0,Term will be subtracted from probability
2.2.0,Probability will be divided by this
2.2.0,these warnings indicate that either the alpha/beta
2.2.0,"forces a penalty to be a no-op, or a penalty is a no-op but"
2.2.0,the alpha/beta would suggest otherwise.
2.2.0,using some length penalty
2.2.0,using some coverage penalty
2.2.0,!/usr/bin/env python
2.2.0,semaphore doesn't have a timeout arg in Python 2.7
2.2.0,perform a first request to initialize everything
2.2.0,backwards compatibility for confs
2.2.0,every segment becomes a dict for flexibility purposes
2.2.0,NOTE: translator returns lists of `n_best` list
2.2.0,build back results with empty texts
2.2.0,load can be called multiple times: modify copy
2.2.0,output contain alignment
2.2.0,Below are all the different penalty terms implemented so far.
2.2.0,Subtract coverage penalty from topk log probs.
2.2.0,Divide topk log probs by length penalty.
2.2.0,Sorting
2.2.0,Chinese segmentation
2.2.0,Chinese simplify -> Chinese traditional standard
2.2.0,Chinese simplify -> Chinese traditional (HongKong)
2.2.0,Chinese simplify -> Chinese traditional (Taiwan)
2.2.0,Chinese traditional -> Chinese simplify (v1)
2.2.0,Chinese traditional -> Chinese simplify (v2)
2.1.2,!/usr/bin/env python
2.1.2,!/usr/bin/env python
2.1.2,!/usr/bin/env python
2.1.2,!/usr/bin/env python
2.1.2,!/usr/bin/env python
2.1.2,!/usr/bin/env python3
2.1.2,-*- coding: utf-8 -*-
2.1.2,
2.1.2,"OpenNMT-py documentation build configuration file, created by"
2.1.2,sphinx-quickstart on Sun Dec 17 12:07:14 2017.
2.1.2,
2.1.2,This file is execfile()d with the current directory set to its
2.1.2,containing dir.
2.1.2,
2.1.2,Note that not all possible configuration values are present in this
2.1.2,autogenerated file.
2.1.2,
2.1.2,All configuration values have a default; values that are commented out
2.1.2,serve to show the default.
2.1.2,"If extensions (or modules to document with autodoc) are in another directory,"
2.1.2,add these directories to sys.path here. If the directory is relative to the
2.1.2,"documentation root, use os.path.abspath to make it absolute, like shown here."
2.1.2,
2.1.2,import os
2.1.2,import sys
2.1.2,"sys.path.insert(0, os.path.abspath('.'))"
2.1.2,-- General configuration ------------------------------------------------
2.1.2,"If your documentation needs a minimal Sphinx version, state it here."
2.1.2,
2.1.2,needs_sphinx = '1.0'
2.1.2,"Add any Sphinx extension module names here, as strings. They can be"
2.1.2,extensions coming with Sphinx (named 'sphinx.ext.*') or your custom
2.1.2,ones.
2.1.2,Show base classes
2.1.2,"Use ""variables"" section for Attributes instead of weird block things"
2.1.2,mimicking the function style.
2.1.2,"Add any paths that contain templates here, relative to this directory."
2.1.2,The suffix(es) of source filenames.
2.1.2,You can specify multiple suffix as a list of string:
2.1.2,
2.1.2,"source_suffix = ['.rst', '.md']"
2.1.2,The master toctree document.
2.1.2,General information about the project.
2.1.2,"The version info for the project you're documenting, acts as replacement for"
2.1.2,"|version| and |release|, also used in various other places throughout the"
2.1.2,built documents.
2.1.2,
2.1.2,The short X.Y version.
2.1.2,"The full version, including alpha/beta/rc tags."
2.1.2,The language for content autogenerated by Sphinx. Refer to documentation
2.1.2,for a list of supported languages.
2.1.2,
2.1.2,This is also used if you do content translation via gettext catalogs.
2.1.2,"Usually you set ""language"" from the command line for these cases."
2.1.2,"List of patterns, relative to source directory, that match files and"
2.1.2,directories to ignore when looking for source files.
2.1.2,This patterns also effect to html_static_path and html_extra_path
2.1.2,The name of the Pygments (syntax highlighting) style to use.
2.1.2,"If true, `todo` and `todoList` produce output, else they produce nothing."
2.1.2,-- Options for HTML output ----------------------------------------------
2.1.2,The theme to use for HTML and HTML Help pages.  See the documentation for
2.1.2,a list of builtin themes.
2.1.2,
2.1.2,html_theme = 'sphinx_materialdesign_theme'
2.1.2,html_theme_path = [sphinx_materialdesign_theme.get_path()]
2.1.2,Theme options are theme-specific and customize the look and feel of a theme
2.1.2,"further.  For a list of options available for each theme, see the"
2.1.2,documentation.
2.1.2,
2.1.2,html_theme_options = {}
2.1.2,"Add any paths that contain custom static files (such as style sheets) here,"
2.1.2,"relative to this directory. They are copied after the builtin static files,"
2.1.2,"so a file named ""default.css"" will overwrite the builtin ""default.css""."
2.1.2,"Custom sidebar templates, must be a dictionary that maps document names"
2.1.2,to template names.
2.1.2,
2.1.2,This is required for the alabaster theme
2.1.2,refs: http://alabaster.readthedocs.io/en/latest/installation.html#sidebars
2.1.2,-- Options for HTMLHelp output ------------------------------------------
2.1.2,Output file base name for HTML help builder.
2.1.2,-- Options for LaTeX output ---------------------------------------------
2.1.2,The paper size ('letterpaper' or 'a4paper').
2.1.2,
2.1.2,"'papersize': 'letterpaper',"
2.1.2,"The font size ('10pt', '11pt' or '12pt')."
2.1.2,
2.1.2,"'pointsize': '10pt',"
2.1.2,Additional stuff for the LaTeX preamble.
2.1.2,
2.1.2,"'preamble': '',"
2.1.2,Latex figure (float) alignment
2.1.2,
2.1.2,"'figure_align': 'htbp',"
2.1.2,Grouping the document tree into LaTeX files. List of tuples
2.1.2,"(source start file, target name, title,"
2.1.2,"author, documentclass [howto, manual, or own class])."
2.1.2,-- Options for manual page output ---------------------------------------
2.1.2,One entry per manual page. List of tuples
2.1.2,"(source start file, name, description, authors, manual section)."
2.1.2,-- Options for Texinfo output -------------------------------------------
2.1.2,Grouping the document tree into Texinfo files. List of tuples
2.1.2,"(source start file, target name, title, author,"
2.1.2,"dir menu entry, description, category)"
2.1.2,!/usr/bin/env python
2.1.2,-*- coding: utf-8 -*-
2.1.2,is this reachable?
2.1.2,Read in embeddings
2.1.2,Write to file
2.1.2,converts a SentencePiece vocabulary to the format expected by dynamic data
2.1.2,"(essentially converts float expected counts to ""fixed precision"" int pseudo"
2.1.2,counts)
2.1.2,"Add in default model arguments, possibly added since training."
2.1.2,build_base_model expects updated and validated opts
2.1.2,-*- encoding: utf-8 -*-
2.1.2,!/usr/bin/env python
2.1.2,-*- coding: utf-8 -*-
2.1.2,Author: Rico Sennrich
2.1.2,flake8: noqa
2.1.2,This file is retrieved from https://github.com/rsennrich/subword-nmt
2.1.2,hack for python2/3 compatibility
2.1.2,check version information
2.1.2,some hacking to deal with duplicates (only consider first instance)
2.1.2,don't print end-of-word symbols
2.1.2,sys.stderr.write('cannot split {0} further.\n'.format(segment))
2.1.2,sys.stderr.write('OOV: {0}\n'.format(segment))
2.1.2,sys.stderr.write('OOV: {0}\n'.format(segment))
2.1.2,python 2/3 compatibility
2.1.2,read/write files as UTF-8
2.1.2,!/usr/bin/env python
2.1.2,!/usr/bin/env python
2.1.2,-*- coding: utf-8 -*-
2.1.2,Author: Rico Sennrich
2.1.2,flake8: noqa
2.1.2,This file is retrieved from https://github.com/rsennrich/subword-nmt
2.1.2,hack for python2/3 compatibility
2.1.2,"find all instances of pair, and update frequency/indices around it"
2.1.2,find first symbol
2.1.2,"if first symbol is followed by second symbol, we've found an occurrence of pair (old_word[i:i+2])"
2.1.2,"assuming a symbol sequence ""A B C"", if ""B C"" is merged, reduce the frequency of ""A B"""
2.1.2,"assuming a symbol sequence ""A B C B"", if ""B C"" is merged, reduce the frequency of ""C B""."
2.1.2,"however, skip this if the sequence is A B C B C, because the frequency of ""C B"" will be reduced by the previous code block"
2.1.2,find new pair
2.1.2,"assuming a symbol sequence ""A BC D"", if ""B C"" is merged, increase the frequency of ""A BC"""
2.1.2,"assuming a symbol sequence ""A BC B"", if ""B C"" is merged, increase the frequency of ""BC B"""
2.1.2,"however, if the sequence is A BC BC, skip this step because the count of ""BC BC"" will be incremented by the previous code block"
2.1.2,data structure of pair frequencies
2.1.2,index from pairs to words
2.1.2,version 0.2 changes the handling of the end-of-word token ('</w>');
2.1.2,version numbering allows bckward compatibility
2.1.2,"threshold is inspired by Zipfian assumption, but should only affect speed"
2.1.2,we probably missed the best pair because of pruning; go back to full statistics
2.1.2,"threshold is inspired by Zipfian assumption, but should only affect speed"
2.1.2,python 2/3 compatibility
2.1.2,read/write files as UTF-8
2.1.2,!/usr/bin/env python
2.1.2,-*- coding: utf-8 -*-
2.1.2,!/usr/bin/env python
2.1.2,Avoid functionality on inference
2.1.2,Build embeddings.
2.1.2,Build encoder.
2.1.2,Build embeddings.
2.1.2,Build decoder.
2.1.2,Share the embedding matrix - preprocess with share_vocab required.
2.1.2,src/tgt vocab should be the same if `-share_vocab` is specified.
2.1.2,Update vocabulary embeddings with checkpoint embeddings
2.1.2,Embedding layers
2.1.2,Just for debugging purposes
2.1.2,Remove old vocabulary associated embeddings
2.1.2,for back compat when attention_dropout was not defined
2.1.2,Build Model
2.1.2,Build Generator.
2.1.2,Load the model states from checkpoint or initialize them.
2.1.2,This preserves backward-compat for models using customed layernorm
2.1.2,end of patch for backward compatibility
2.1.2,Update model embeddings with those from the checkpoint
2.1.2,after initialization
2.1.2,!/usr/bin/env python
2.1.2,ensure tensorboard output is written in the directory
2.1.2,of previous checkpoints
2.1.2,Override checkpoint's update_embeddings as it defaults to false
2.1.2,NOTE: It's important that ``opt`` has been validated and updated
2.1.2,at this point.
2.1.2,Build model.
2.1.2,Build optimizer.
2.1.2,Build model saver
2.1.2,Move batch to specified device
2.1.2,Use Tensorboard for visualization during training
2.1.2,Options only during inference
2.1.2,"Truncation options, for text corpus"
2.1.2,"as for False, this will be added in _add_train_general_opts"
2.1.2,Embedding Options
2.1.2,Model Task Options
2.1.2,Encoder-Decoder Options
2.1.2,"group.add('--residual', '-residual',   action=""store_true"","
2.1.2,"help=""Add residual connections between RNN layers."")"
2.1.2,The following options (bridge_extra_node to n_steps) are used
2.1.2,for training with --encoder_type ggnn (Gated Graph Neural Network).
2.1.2,Attention options
2.1.2,Alignement options
2.1.2,Generator and loss options.
2.1.2,GPU
2.1.2,Init options
2.1.2,Pretrained word vectors
2.1.2,Freeze word vectors
2.1.2,Optimization options
2.1.2,learning rate
2.1.2,options relate to data preprare
2.1.2,options relate to train
2.1.2,Alpha and Beta values for Google Length + Coverage penalty
2.1.2,"Described here: https://arxiv.org/pdf/1609.08144.pdf, Section 7"
2.1.2,Length penalty options
2.1.2,Coverage penalty options
2.1.2,Decoding Length constraint
2.1.2,Decoding content constraint
2.1.2,Adding options relate to decoding strategy
2.1.2,Adding option for logging
2.1.2,Copyright 2016 The Chromium Authors. All rights reserved.
2.1.2,Use of this source code is governed by a BSD-style license that can be
2.1.2,found in the LICENSE file.
2.1.2,"Get the key 'value' in the dict, or just use 'value'"
2.1.2,Basic attributes.
2.1.2,Set model in training mode.
2.1.2,UPDATE DROPOUT
2.1.2,Run patience mechanism
2.1.2,"If the patience has reached the limit, stop training"
2.1.2,swap model params w/ moving average
2.1.2,(and keep the original parameters)
2.1.2,Set model in validating mode.
2.1.2,F-prop through the model.
2.1.2,Compute loss.
2.1.2,Update statistics.
2.1.2,Set model back to training mode.
2.1.2,Truncated BPTT: reminder not compatible with accum > 1
2.1.2,1. Create truncated target.
2.1.2,2. F-prop all but generator.
2.1.2,3. Compute loss.
2.1.2,4. Update the parameters and statistics.
2.1.2,Multi GPU gradient gather
2.1.2,"If truncated, don't backprop fully."
2.1.2,TO CHECK
2.1.2,if dec_state is not None:
2.1.2,dec_state.detach()
2.1.2,"in case of multi step gradient accumulation,"
2.1.2,update only after accum batches
2.1.2,For Flake
2.1.2,we avoid padding while mean pooling
2.1.2,incoming and outgoing edge embedding
2.1.2,Find vocab data for tree builting
2.1.2,Propogation Model
2.1.2,Initialize the bridge layer
2.1.2,Token embedding
2.1.2,Initialize graph using formatted input sequence
2.1.2,Number of flagged nodes defines node count for this sample
2.1.2,"(Nodes can have no flags on them, but must be in 'flags' list)."
2.1.2,The total number of integers in the vocab should allow
2.1.2,for all features and edges to be defined.
2.1.2,Use first extra node as only source for decoder init
2.1.2,Average all nodes to get bridge input
2.1.2,"LSTM has hidden and cell state, other only one"
2.1.2,Total number of states
2.1.2,Build a linear layer for each
2.1.2,Initialize the bridge layer
2.1.2,"s_len, batch, emb_dim = emb.size()"
2.1.2,Lengths data is wrapped inside a Tensor.
2.1.2,"LSTM has hidden and cell state, other only one"
2.1.2,Total number of states
2.1.2,Build a linear layer for each
2.1.2,"s_len, batch, emb_dim = emb.size()"
2.1.2,Run the forward pass of every layer of the tranformer.
2.1.2,Dimensions and padding for constructing the word embedding matrix
2.1.2,Dimensions and padding for feature embedding matrices
2.1.2,(these have no effect if feat_vocab_sizes is empty)
2.1.2,The embedding matrix look-up tables. The first look-up table
2.1.2,"is for words. Subsequent ones are for features, if any exist."
2.1.2,The final output size of word + feature vectors. This can vary
2.1.2,from the word vector size if and only if features are defined.
2.1.2,This is the attribute you should access if you need to know
2.1.2,how big your embeddings are going to be.
2.1.2,The sequence of operations that converts the input sequence
2.1.2,into a sequence of embeddings. At minimum this consists of
2.1.2,looking up the embeddings for each word and feature in the
2.1.2,input. Model parameters may require the sequence to contain
2.1.2,additional operations as well.
2.1.2,features must use word_vec_size
2.1.2,features will use feat_vec_size
2.1.2,Some utilitary functions for pretrained embeddings
2.1.2,is this reachable?
2.1.2,Write to file
2.1.2,set the opt in place
2.1.2,set the opt in place
2.1.2,This class is mainly used by decoder.py for RNNs but also
2.1.2,by the CNN / transformer decoder when copy attention is used
2.1.2,CNN has its own attention mechanism ConvMultiStepAttention
2.1.2,Transformer has its own MultiHeadedAttention
2.1.2,mlp wants it with bias
2.1.2,Check input sizes
2.1.2,"(batch, t_len, d) x (batch, d, s_len) --> (batch, t_len, s_len)"
2.1.2,"(batch, t_len, s_len, d)"
2.1.2,one step input
2.1.2,"compute attention scores, as in Luong et al."
2.1.2,Softmax or sparsemax to normalize attention weights
2.1.2,each context vector c_t is the weighted average
2.1.2,over all the source hidden states
2.1.2,concatenate
2.1.2,Check output sizes
2.1.2,Check output sizes
2.1.2,clamping necessary because of numerical errors: loss should be lower
2.1.2,"bounded by zero, but negative values near zero are possible without"
2.1.2,the clamp
2.1.2,from onmt.utils.misc import aeq
2.1.2,CHECKS
2.1.2,"batch, k_len, d = key.size()"
2.1.2,"batch_, k_len_, d_ = value.size()"
2.1.2,"aeq(batch, batch_)"
2.1.2,"aeq(k_len, k_len_)"
2.1.2,"aeq(d, d_)"
2.1.2,"batch_, q_len, d_ = query.size()"
2.1.2,"aeq(batch, batch_)"
2.1.2,"aeq(d, d_)"
2.1.2,"aeq(self.model_dim % 8, 0)"
2.1.2,if mask is not None:
2.1.2,"batch_, q_len_, k_len_ = mask.size()"
2.1.2,"aeq(batch_, batch)"
2.1.2,"aeq(k_len_, k_len)"
2.1.2,aeq(q_len_ == q_len)
2.1.2,END CHECKS
2.1.2,"1) Project key, value, and query."
2.1.2,1 or key_len x key_len
2.1.2,1 or key_len x key_len x dim_per_head
2.1.2,1 or key_len x key_len x dim_per_head
2.1.2,2) Calculate and scale scores.
2.1.2,batch x num_heads x query_len x key_len
2.1.2,3) Apply attention dropout and compute context vectors.
2.1.2,CHECK
2.1.2,"batch_, q_len_, d_ = output.size()"
2.1.2,"aeq(q_len, q_len_)"
2.1.2,"aeq(batch, batch_)"
2.1.2,"aeq(d, d_)"
2.1.2,Return multi-head attn
2.1.2,At the moment this class is only used by embeddings.Embeddings look-up tables
2.1.2,-*- coding: utf-8 -*-
2.1.2,checks
2.1.2,"batch, channel, height, width = base_target_emb.size()"
2.1.2,"batch_, channel_, height_, width_ = input_from_dec.size()"
2.1.2,"enc_batch, enc_channel, enc_height = encoder_out_top.size()"
2.1.2,"enc_batch_, enc_channel_, enc_height_ = encoder_out_combine.size()"
2.1.2,out_features * in_features
2.1.2,norm is out_features * 1
2.1.2,batch_size * out_features
2.1.2,out_features
2.1.2,out_features
2.1.2,batch_size * out_features
2.1.2,"out_channels, in_channels // groups, * kernel_size"
2.1.2,out_features
2.1.2,This is used nowhere in the code at the moment (Vincent Nguyen 05/18/2018)
2.1.2,"in_channels, out_channels, *kernel_size"
2.1.2,"in_channels, out_channels, *kernel_size"
2.1.2,"self.out_channels, 1"
2.1.2,out_features
2.1.2,out_features
2.1.2,store roots on diagonal
2.1.2,CHECKS
2.1.2,Original probabilities.
2.1.2,Probability of copying p(z=1) batch.
2.1.2,Probability of not copying: p_{word}(w) * (1 - p(z))
2.1.2,probabilities assigned by the model to the gold targets
2.1.2,probability of tokens copied from source
2.1.2,Set scores for unk to 0 and add eps
2.1.2,find the indices in which you do not use the copy mechanism
2.1.2,Drop padding.
2.1.2,this block does not depend on the loss value computed above
2.1.2,and is used only for stats
2.1.2,this block does not depend on the loss value computed above
2.1.2,and is used only for stats
2.1.2,Correct target copy token instead of <unk>
2.1.2,tgt[i] = align[i] + len(tgt_vocab)
2.1.2,for i such that tgt[i] == 0 and align[i] != 0
2.1.2,Compute sum of perplexities for stats
2.1.2,this part looks like it belongs in CopyGeneratorLoss
2.1.2,Compute Loss as NLL divided by seq length
2.1.2,Compute Total Loss per sequence in batch
2.1.2,Divide by length of each sequence and sum
2.1.2,Auto import python files in this directory
2.1.2,1. sample number of tokens to corrupt
2.1.2,2. sample positions to corrput
2.1.2,3. sample corrupted values
2.1.2,1. sample number of tokens to corrupt
2.1.2,2. sample positions to corrput
2.1.2,3. Drop token on chosen position
2.1.2,1. sample number of tokens to corrupt
2.1.2,2. sample positions to corrput
2.1.2,3. mask word on chosen position
2.1.2,"Sharing options among `TokenizerTransform`s, same name conflict in"
2.1.2,this scope will be resolved by remove previous occurrence in parser
2.1.2,subword regularization(or BPE dropout) options:
2.1.2,subword vocabulary restriction options:
2.1.2,derterministic subwording
2.1.2,subword sampling when nbest_size > 1 or -1
2.1.2,alpha should be 0.0 < alpha < 1.0
2.1.2,Load vocabulary file if provided and set threshold
2.1.2,Load Subword Model
2.1.2,-1: keep everything (i.e. 1 mask per token)
2.1.2,0: replace everything (i.e. no mask)
2.1.2,1: 1 mask per span
2.1.2,view each subword as word start / input is word level token
2.1.2,Pretend it ends with a full stop so last span is a sentence
2.1.2,"Tokens that are full stops, where the previous token is not"
2.1.2,Make sure we have enough to mask
2.1.2,Trim to masking budget
2.1.2,Handle 0-length mask (inserts) separately
2.1.2,assert is_word_start[-1] == 0
2.1.2,assert tokens_length - 1 not in indices
2.1.2,"keep index, but replace it with [MASK]"
2.1.2,"acts as a long length, so spans don't go over the end of doc"
2.1.2,next position from each word_start
2.1.2,delete token: 1 mask/remove per span
2.1.2,"keep index, but replace it with [MASK]: 1 mask per token"
2.1.2,A bit faster when all lengths are 1
2.1.2,to cover whole token
2.1.2,delete token
2.1.2,"keep index, but replace it with [MASK]"
2.1.2,assert tokens_length - 1 not in indices
2.1.2,initialize fields at the top of each unit test to prevent
2.1.2,any undesired stateful effects
2.1.2,"this test touches the file system, so it could be considered an"
2.1.2,integration test
2.1.2,write utf-8 bytes
2.1.2,batch 0 will always predict EOS. The other batches will predict
2.1.2,non-eos scores.
2.1.2,"""best"" prediction is eos - that should be blocked"
2.1.2,include at least one prediction OTHER than EOS
2.1.2,that is greater than -1e20
2.1.2,now batch 0 has ended and no others have
2.1.2,initial step
2.1.2,batch 0 dies on step 0
2.1.2,include at least one prediction OTHER than EOS
2.1.2,that is greater than -1e20
2.1.2,step 2
2.1.2,(old) batch 8 dies on step 1
2.1.2,step 3
2.1.2,everything dies
2.1.2,initial step
2.1.2,batch 0 dies on step 0
2.1.2,include at least one prediction OTHER than EOS
2.1.2,that is greater than -1e20
2.1.2,step 2
2.1.2,(old) batch 8 dies on step 1
2.1.2,step 3
2.1.2,everything dies
2.1.2,initial step
2.1.2,finish one beam
2.1.2,include at least one prediction OTHER than EOS
2.1.2,that is greater than -1e20
2.1.2,step 2
2.1.2,finish example in last batch
2.1.2,(old) batch 8 dies on step 1
2.1.2,step 3
2.1.2,everything dies
2.1.2,initial step
2.1.2,batch 0 dies on step 0
2.1.2,include at least one prediction OTHER than EOS
2.1.2,that is greater than -1e20
2.1.2,step 2
2.1.2,(old) batch 8 dies on step 1
2.1.2,step 3
2.1.2,everything dies
2.1.2,illegal_weights_mask = torch.ByteTensor([
2.1.2,"[0, 0, 0, 0, 0, 0, 0],"
2.1.2,"[0, 0, 0, 1, 1, 1, 1],"
2.1.2,"[0, 0, 0, 0, 0, 1, 1],"
2.1.2,"[0, 0, 1, 1, 1, 1, 1]])"
2.1.2,TODO: fix for pytorch 0.3
2.1.2,illegal_weights = alignments.masked_select(illegal_weights_mask)
2.1.2,"self.assertEqual(0.0, illegal_weights.data.sum())"
2.1.2,this could be considered an integration test because it touches
2.1.2,the filesystem for the config file (and the models)
2.1.2,no dummy prefix
2.1.2,no dummy prefix
2.1.2,transforms that require vocab will not create if not provide vocab
2.1.2,filter_transform.warm_up()
2.1.2,test BPE-dropout:
2.1.2,1. disable bpe dropout for not training example
2.1.2,2. enable bpe dropout for training example
2.1.2,3. (NOTE) disable dropout won't take effect if already seen
2.1.2,this is caused by the cache mechanism in bpe:
2.1.2,return cached subword if the original token is seen when no dropout
2.1.2,test SP regularization:
2.1.2,1. enable regularization for training example
2.1.2,2. disable regularization for not training example
2.1.2,Not apply token drop for not training example
2.1.2,apply token drop for training example
2.1.2,Not apply token mask for not training example
2.1.2,apply token mask for training example
2.1.2,require vocabs to warm_up
2.1.2,Not apply token mask for not training example
2.1.2,apply token mask for training example
2.1.2,"Defalt: full_stop_token=[""."", ""?"", ""!""]"
2.1.2,"Defalt: full_stop_token=[""."", ""?"", ""!""]"
2.1.2,random_ratio of inserted tokens are chosen in vocab
2.1.2,others are MASK_TOK
2.1.2,"insert_ratio=0.0,"
2.1.2,"random_ratio=0.0,"
2.1.2,"Defalt: full_stop_token=[""."", ""?"", ""!""]"
2.1.2,all token are considered as an individual word
2.1.2,1. tokens are dropped when replace_length is 0
2.1.2,"print(f""token delete: {masked} / {tokens}"")"
2.1.2,2. tokens are replaced by MASK when replace_length is 1
2.1.2,"print(f""token mask: {masked} / {tokens}"")"
2.1.2,"insert_ratio=0.0,"
2.1.2,"random_ratio=0.0,"
2.1.2,"Defalt: full_stop_token=[""."", ""?"", ""!""]"
2.1.2,start token of word are identified using subword marker
2.1.2,"1. replace_length 0: ""words"" are dropped"
2.1.2,"print(f""word delete: {masked} / {tokens}"")"
2.1.2,"self.assertEqual(len(masked), n_words - n_masked)"
2.1.2,"2. replace_length 1: ""words"" are replaced with a single MASK"
2.1.2,"print(f""whole word single mask: {masked} / {tokens}"")"
2.1.2,len(masked) depend on number of tokens in select word
2.1.2,"3. replace_length -1: all tokens in ""words"" are replaced with MASK"
2.1.2,"print(f""whole word multi mask: {masked} / {tokens}"")"
2.1.2,number of mask_tok depend on number of tokens in selected word
2.1.2,number of MASK_TOK can be greater than n_masked
2.1.2,"insert_ratio=0.5,"
2.1.2,"random_ratio=0.3,"
2.1.2,"Defalt: full_stop_token=[""."", ""?"", ""!""]"
2.1.2,start token of word are identified using subword marker
2.1.2,n_words = sum(token_starts)
2.1.2,n_masked = math.ceil(n_words * bart_noise.mask_ratio)
2.1.2,"print(f""Text Span Infilling: {infillied} / {tokens}"")"
2.1.2,"print(n_words, n_masked)"
2.1.2,!/usr/bin/env python
2.1.2,-*- coding: utf-8 -*-
2.1.2,Inject some dummy training options that may needed when build fields
2.1.2,Remove the generated *pt files.
2.1.2,Remove the generated data samples
2.1.2,all beams repeat (beam >= 1 repeat dummy scores)
2.1.2,predict repeat_idx over and over again
2.1.2,"before repeat, scores are either 0 or -inf"
2.1.2,"on repeat, `repeat_idx` score is BLOCKED_SCORE"
2.1.2,"(but it's still the best score, thus we have"
2.1.2,"[BLOCKED_SCORE, -inf, -inf, -inf, -inf]"
2.1.2,repetitions keeps maximizing score
2.1.2,"index 0 has been blocked, so repeating=>+0.0 score"
2.1.2,other indexes are -inf so repeating=>BLOCKED_SCORE
2.1.2,which is higher
2.1.2,beam 0 and beam >=2 will repeat (beam >= 2 repeat dummy scores)
2.1.2,non-interesting beams are going to get dummy values
2.1.2,"on initial round, only predicted scores for beam 0"
2.1.2,matter. Make two predictions. Top one will be repeated
2.1.2,"in beam zero, second one will live on in beam 1."
2.1.2,predict the same thing in beam 0
2.1.2,continue pushing around what beam 1 predicts
2.1.2,"now beam 0 dies (along with the others), beam 1 -> beam 0"
2.1.2,"now beam 0 dies (along with the others), beam 1 -> beam 0"
2.1.2,beam 0 and beam >= 2 will repeat (beam 2 repeats excluded idx)
2.1.2,non-interesting beams are going to get dummy values
2.1.2,predict the same thing in beam 0
2.1.2,continue pushing around what beam 1 predicts
2.1.2,predict the allowed-repeat again in beam 2
2.1.2,"now beam 0 dies, beam 1 -> beam 0, beam 2 -> beam 1"
2.1.2,and the rest die
2.1.2,"since all preds after i=0 are 0, we can check"
2.1.2,that the beam is the correct idx by checking that
2.1.2,the curr score is the initial score
2.1.2,beam 0 will always predict EOS. The other beams will predict
2.1.2,non-eos scores.
2.1.2,non-interesting beams are going to get dummy values
2.1.2,"""best"" prediction is eos - that should be blocked"
2.1.2,include at least beam_sz predictions OTHER than EOS
2.1.2,that are greater than -1e20
2.1.2,predict eos in beam 0
2.1.2,provide beam_sz other good predictions
2.1.2,now the top beam has ended and no others have
2.1.2,"not of interest, but want to make sure it keeps running"
2.1.2,since only beam 0 terminates and n_best = 2
2.1.2,"this is also a test that when block_ngram_repeat=0,"
2.1.2,repeating is acceptable
2.1.2,non-interesting beams are going to get dummy values
2.1.2,"""best"" prediction is eos - that should be blocked"
2.1.2,include at least beam_sz predictions OTHER than EOS
2.1.2,that are greater than -1e20
2.1.2,predict eos in beam 1
2.1.2,provide beam_sz other good predictions in other beams
2.1.2,provide beam_sz other good predictions in other beams
2.1.2,beam 1 dies on min_length
2.1.2,beam 0 dies on the step after beam 1 dies
2.1.2,"inp_lens is tiled in initialize, reassign to make attn match"
2.1.2,non-interesting beams are going to get dummy values
2.1.2,"""best"" prediction is eos - that should be blocked"
2.1.2,include at least beam_sz predictions OTHER than EOS
2.1.2,that are greater than -1e20
2.1.2,predict eos in beam 1
2.1.2,provide beam_sz other good predictions in other beams
2.1.2,provide beam_sz other good predictions in other beams
2.1.2,no top beams are finished yet
2.1.2,beam 1 dies on min_length
2.1.2,no top beams are finished yet
2.1.2,beam 0 dies on the step after beam 1 dies
2.1.2,top beam is finished now so there are attentions
2.1.2,two beams are finished in each batch
2.1.2,second dim is cut down to the non-padded src length
2.1.2,first dim is equal to the time of death
2.1.2,(beam 0 died at current step - adjust for SOS)
2.1.2,(beam 1 died at last step - adjust for SOS)
2.1.2,behavior gets weird when beam is already done so just stop
2.1.2,this is just test_beam.TestBeamAgainstReferenceCase repeated
2.1.2,in each batch.
2.1.2,"init_preds: [4, 3, 5, 6, 7] - no EOS's"
2.1.2,no EOS's yet
2.1.2,"[5, 3, 2, 6, 0], so beam 2 predicts EOS!"
2.1.2,assumes beam 2 finished on last step
2.1.2,ended beam 2 shouldn't continue
2.1.2,"[2, 5, 3, 6, 0] repeat self.BATCH_SZ, so beam 0 predicts EOS!"
2.1.2,"[-2.4879, -3.8910, -4.1010, -4.2010, -4.4010] repeat self.BATCH_SZ"
2.1.2,another beam is finished in all batches
2.1.2,new beam 0 finished
2.1.2,new beam 0 is old beam 3
2.1.2,assumes beam 0 finished on last step
2.1.2,"[5, 2, 6, 1, 0] repeat self.BATCH_SZ, so beam 1 predicts EOS!"
2.1.2,new beam 1 finished
2.1.2,new beam 1 is old beam 4
2.1.2,this could be considered an integration test because it tests
2.1.2,interactions between the GNMT scorer and the beam
2.1.2,"-data option is required, but not used in this test, so dummy."
2.1.2,len x batch x nfeat
2.1.2,Initialize vectors to compare size with
2.1.2,Ensure correct sizes and types
2.1.2,Make sure that output has the correct size and type
2.1.2,"[('encoder_type', 'transformer'),"
2.1.2,"('word_vec_size', 16), ('rnn_size', 16)],"
2.1.2,""""""" Only do SRU test if requirment is safisfied. """""""
2.1.2,SRU doesn't support input_feed.
2.1.2,first check there's nothing unexpectedly not trainable
2.1.2,ok: word embeddings shouldn't be trainable
2.1.2,if word vecs are freezed
2.1.2,ok: positional encodings shouldn't be trainable
2.1.2,then check nothing unexpectedly trainable
2.1.2,Decoder state
2.1.2,Build the RNN.
2.1.2,Set up the context gate.
2.1.2,Set up the standard attention.
2.1.2,The encoder hidden is  (layers*directions) x batch x dim.
2.1.2,We need to convert it to layers x batch x (directions*dim).
2.1.2,Init the input feed.
2.1.2,Update the state with the result.
2.1.2,Concatenates sequence of tensors along a new dimension.
2.1.2,NOTE: v0.3 to 0.4: dec_outs / attns[*] may not be list
2.1.2,(in particular in case of SRU) it was not raising error in 0.3
2.1.2,since stack(Variable) was allowed.
2.1.2,"In 0.4, SRU returns a tensor that shouldn't be stacke"
2.1.2,Check
2.1.2,Calculate the attention.
2.1.2,Calculate the context gate.
2.1.2,Additional args check.
2.1.2,END Additional args check.
2.1.2,Input feed concatenates hidden state with
2.1.2,input at every time step.
2.1.2,TODO: context gate should be employed
2.1.2,instead of second RNN transform.
2.1.2,Update the coverage attention.
2.1.2,Decoder State
2.1.2,CNNDecoder has its own attention mechanism.
2.1.2,Set up a separate copy attention layer if needed.
2.1.2,The output of CNNEncoder.
2.1.2,The combination of output of CNNEncoder and source embeddings.
2.1.2,Process the result and update the attentions.
2.1.2,Update the state.
2.1.2,TODO change the way attns is returned dict => list or tuple (onnx)
2.1.2,Memory_lengths is a single tensor shared between all models.
2.1.2,This assumption will not hold if Translator is modified
2.1.2,to calculate memory_lengths as something other than the length
2.1.2,of the input.
2.1.2,"return _, (B, Q_len, K_len)"
2.1.2,"layer average attention across heads, get ``(B, Q, K)``"
2.1.2,"Case 1: no full_context, no align heads -> layer avg baseline"
2.1.2,"Case 2: no full_context, 1 align heads -> guided align"
2.1.2,"Case 3: full_context, 1 align heads -> full cte guided align"
2.1.2,BoolTensor was introduced in pytorch 1.2
2.1.2,T: could be 1 in the case of stepwise decoding or tgt_len
2.1.2,masking is necessary when sequence length is greater than one
2.1.2,Decoder State
2.1.2,"previously, there was a GlobalAttention module here for copy"
2.1.2,"attention. But it was never actually used -- the ""copy"" attention"
2.1.2,just reuses the context attention.
2.1.2,"attns[""align""] = torch.stack(attn_aligns, 0).mean(0)  # All avg"
2.1.2,TODO change the way attns is returned dict => list or tuple (onnx)
2.1.2,T: could be 1 in the case of stepwise decoding or tgt_len
2.1.2,masking is necessary when sequence length is greater than one
2.1.2,TODO change the way attns is returned dict => list or tuple (onnx)
2.1.2,"buffer size in bytes, determine equiv. # of elements based on data type"
2.1.2,copy tensors into buffer_t
2.1.2,all-reduce and rescale
2.1.2,copy all-reduced buffer back into tensors
2.1.2,"tensor is bigger than buffer, all-reduce and rescale directly"
2.1.2,"buffer is full, all-reduce and replace buffer with grad"
2.1.2,add tensor to buffer
2.1.2,NOTE: stride (if needed) is handled at the
2.1.2,generator (train_iter) level
2.1.2,Move batch to correspond device_id when consumer iterate
2.1.2,hack to dodge unpicklable `dict_keys`
2.1.2,"propagate exception to parent process, keeping original traceback"
2.1.2,TODO: Find a better way to check for sparse gradients.
2.1.2,we use here a FusedAdam() copy of an old Apex repo
2.1.2,In this case use the old FusedAdam with FP16_optimizer wrapper
2.1.2,Load everything from the checkpoint.
2.1.2,Build everything from scratch.
2.1.2,"Reset optimizer, keep options."
2.1.2,"Reset options, keep optimizer."
2.1.2,State can be partially restored.
2.1.2,"unscaled optimizer's gradients (already done therefore skip),"
2.1.2,skips optimizer.step() if gradients contain infs/NaNs.
2.1.2,Updates the scale for next iteration.
2.1.2,Code below is an implementation of https://arxiv.org/pdf/1804.04235.pdf
2.1.2,inspired but modified from https://github.com/DeadAt0m/adafactor-pytorch
2.1.2,backward compatibility
2.1.2,assuming a list/generator of parameter means single group
2.1.2,compute combined scale factor for this group
2.1.2,norm is in fact norm*scale
2.1.2,note: p.grad should not ever be set for correct operation of
2.1.2,mixed precision optimizer that sometimes sends None gradients
2.1.2,State initialization
2.1.2,Exponential moving average of gradient values
2.1.2,Exponential moving average of squared gradient values
2.1.2,-*- coding: utf-8 -*-
2.1.2,if the loss function operates on vectors of raw logits instead of
2.1.2,"probabilities, only the first part of the generator needs to be"
2.1.2,"passed to the NMTLossCompute. At the moment, the only supported"
2.1.2,loss function of this kind is the sparsemax loss.
2.1.2,"attn_align should be in (batch_size, pad_tgt_size, pad_src_size)"
2.1.2,"align_idx should be a Tensor in size([N, 3]), N is total number"
2.1.2,"of align src-tgt pair in current batch, each as"
2.1.2,"['sent_NÂ°_in_batch', 'tgt_id+1', 'src_id'] (check AlignField)"
2.1.2,NOTE: tgt-src ref alignement that in range_ of shard
2.1.2,(coherent with batch.tgt)
2.1.2,"align_head contains value in [0, 1) presenting attn prob,"
2.1.2,0 was resulted by the context attention src_pad_mask
2.1.2,"So, the correspand position in ref_align should also be 0"
2.1.2,"Therefore, clip align_head to > 1e-18 should be bias free."
2.1.2,non_none: the subdict of the state dictionary where the values
2.1.2,are not None.
2.1.2,"Now, the iteration:"
2.1.2,state is a dictionary of sequences of tensor-like but we
2.1.2,want a sequence of dictionaries of tensors.
2.1.2,"First, unzip the dictionary into a sequence of keys and a"
2.1.2,sequence of tensor-like sequences.
2.1.2,"Now, yield a dictionary for each shard. The keys are always"
2.1.2,the same. values is a sequence of length #keys where each
2.1.2,element is a sequence of length #shards. We want to iterate
2.1.2,"over the shards, not over the keys: therefore, the values need"
2.1.2,to be re-zipped by shard and then each shard can be paired
2.1.2,with the keys.
2.1.2,Assumed backprop'd
2.1.2,Check Transforms
2.1.2,Check path
2.1.2,tgt is src for LM task
2.1.2,Check prefix: will be used when use prefix transform
2.1.2,Check weight
2.1.2,validation when train:
2.1.2,Check embeddings stuff
2.1.2,"Backward compatibility with ""fix_word_vecs_*"" opts"
2.1.2,encoder and decoder should be same sizes
2.1.2,"Load default opt values, then overwrite with the opts in"
2.1.2,"the checkpoint. That way, if there are new options added,"
2.1.2,the defaults are used.
2.1.2,Don't do anything
2.1.2,Update best score of each criteria
2.1.2,Reset tolerance
2.1.2,Update current status
2.1.2,Decrease tolerance
2.1.2,Log
2.1.2,Log
2.1.2,Get a list of world_size lists with len(stat_list) Statistics objects
2.1.2,SRU doesn't support PackedSequence.
2.1.2,-*- coding: utf-8 -*-
2.1.2,threshold on 1 to avoid div by 0
2.1.2,treat alignment matrix one by one as each have different lengths
2.1.2,No alignment if not exist valid tgt token
2.1.2,get valid alignment (sub-matrix from full paded aligment matrix)
2.1.2,-*- coding: utf-8 -*-
2.1.2,this one is needed for torchtext random call (shuffled iterator)
2.1.2,in multi gpu it ensures datasets are read in the same order
2.1.2,some cudnn methods can be random even after fixing the seed
2.1.2,unless you tell it to be deterministic
2.1.2,This one is needed for various tranfroms
2.1.2,These ensure same initialization in multi gpu mode
2.1.2,Shift values to be >= 0
2.1.2,we need to check the model path + any tokenizer path
2.1.2,fast-forward if loaded from state
2.1.2,NOTE: `rnn.pack_padded_sequence` requires that a
2.1.2,"minibatch be sorted by decreasing order, which"
2.1.2,requires reversing relative to typical sort keys
2.1.2,Maintains the longest src and tgt length in the current batch
2.1.2,Reset current longest length at a new batch (count=1)
2.1.2,Src: [<bos> w1 ... wN <eos>]
2.1.2,Tgt: [w1 ... wM <eos>]
2.1.2,coding: utf-8
2.1.2,make a small vocab containing just the tokens in the source sequence
2.1.2,add init_token and eos_token according to src construction
2.1.2,Map source tokens to indices in the dynamic dict.
2.1.2,self.src_vocabs is used in collapse_copy_scores and Translator.py
2.1.2,this assumes src_field and tgt_field are both text
2.1.2,fields needs to have only keys that examples have as attrs
2.1.2,avoid infinite recursion when fields isn't defined
2.1.2,this is a hack: appears quicker to apply it here
2.1.2,than in the ParallelCorpusIterator
2.1.2,NOTE: moved to DatasetAdapter._process method in iterator.py
2.1.2,item = self.transform.apply(
2.1.2,"example, is_train=self.infinitely, corpus_name=self.cid)"
2.1.2,empty example: skip
2.1.2,-*- coding: utf-8 -*-
2.1.2,backwards compatibility
2.1.2,monkey-patch to make torchtext Vocab's pickleable
2.1.2,"+1 for tgt side to keep coherent after ""bos"" padding,"
2.1.2,"register ['NÂ°_in_batch', 'tgt_id+1', 'src_id']"
2.1.2,this is basically copy-pasted from torchtext.
2.1.2,counters changes in place
2.1.2,keep the order of tokens specified in the vocab file by
2.1.2,adding them to the counter with decreasing counting values
2.1.2,`tgt_vocab_size` is ignored when sharing vocabularies
2.1.2,return vocab to dump with standard name
2.1.2,empty train_dataset_files so that vocab is only loaded from
2.1.2,"given paths in src_vocab_path, tgt_vocab_path"
2.1.2,Load vocabulary
2.1.2,Drop the none-using from memory but keep the last
2.1.2,"in the long run, shouldn't it be possible to do this by calling"
2.1.2,build_vocab with both the src and tgt data?
2.1.2,coding: utf-8
2.1.2,several data readers need optional dependencies. There's no
2.1.2,appropriate builtin exception
2.1.2,NOTE: not support nfeats > 0 yet
2.1.2,-*- coding: utf-8 -*-
2.1.2,mix this with partial
2.1.2,batch (list(list(list))): batch_size x len(self.fields) x seq_len
2.1.2,lengths: batch_size
2.1.2,data: seq_len x batch_size x len(self.fields)
2.1.2,flake8: noqa
2.1.2,For command-line option parsing
2.1.2,"Check pass, set the args."
2.1.2,"This SRU version implements its own cuda-level optimization,"
2.1.2,so it requires that:
2.1.2,1. `cupy` and `pynvrtc` python package installed.
2.1.2,2. pytorch is built with cuda support.
2.1.2,3. library path set: export LD_LIBRARY_PATH=<cuda lib path>.
2.1.2,Check 1.
2.1.2,Check 2.
2.1.2,Check 3.
2.1.2,This sets up device to use.
2.1.2,-> directions x batch x dim
2.1.2,For DEBUG
2.1.2,"size = (length, batch, x.size(-1)) \"
2.1.2,"if x.dim() == 3 else (batch, x.size(-1))"
2.1.2,grad_x = x.new(*x.size()) if k_ == 3 else x.new(*size).zero_()
2.1.2,Normal use
2.1.2,"An entry check here, will catch on train side and translate side"
2.1.2,if requirements are not satisfied.
2.1.2,RNNDecoderState wraps hidden as a tuple.
2.1.2,fh -> (layers*directions) x batch x dim
2.1.2,"No encoder in LM, seq2seq count formatting kept"
2.1.2,_check_save_model_path
2.1.2,NOTE: We need to trim the vocab to remove any unk tokens that
2.1.2,were not originally here.
2.1.2,!/usr/bin/env python
2.1.2,!/usr/bin/env python
2.1.2,!/usr/bin/env python
2.1.2,-*- coding: utf-8 -*-
2.1.2,!/usr/bin/env python
2.1.2,!/usr/bin/env python
2.1.2,!/usr/bin/env python
2.1.2,import onmt.opts as opts
2.1.2,Set sharing strategy manually instead of default based on the OS.
2.1.2,"maybe prepare pretrained embeddings, if any"
2.1.2,Load checkpoint if we resume from a previous training.
2.1.2,Report src and tgt vocab sizes
2.1.2,Create a thread to listen for errors in the child processes.
2.1.2,Train with multiprocessing.
2.1.2,"This does not work if we merge with the first loop, not sure why"
2.1.2,Get the iterator to generate from
2.1.2,"Once training is done, we can terminate the producers"
2.1.2,magic indices
2.1.2,result caching
2.1.2,fix length constraint and remove eos from count
2.1.2,add one to account for BOS. Don't account for EOS because hitting
2.1.2,this implies it hasn't been found.
2.1.2,we don't block nothing if the user doesn't want it
2.1.2,we can't block nothing beam's too short
2.1.2,we check paths one by one
2.1.2,we don't forbid nothing if the user doesn't want it
2.1.2,we can't forbid nothing if beam's too short
2.1.2,Reordering forbidden_tokens following beam selection
2.1.2,We rebuild a dict to ensure we get the value and not the pointer
2.1.2,Grabing the newly selected tokens and associated ngram
2.1.2,skip the blocking if any token in current_ngram is excluded
2.1.2,"pickups: Tensor where specified index were set to 1, others 0"
2.1.2,"dropdowns: opposite of pickups, 1 for those shouldn't pick"
2.1.2,Minus dropdowns to log_probs making probabilities of
2.1.2,unspecified index close to 0
2.1.2,"prediction step have surpass length of given target_prefix,"
2.1.2,no need to further change this attr
2.1.2,keep indices until overflowing p
2.1.2,Set all logits that are not in the top-p to -10000.
2.1.2,This puts the probabilities close to 0.
2.1.2,Set all logits that are not in the top-k to -10000.
2.1.2,This puts the probabilities close to 0.
2.1.2,"For temp=0.0, take the argmax to avoid divide-by-zero errors."
2.1.2,keep_topk=1 is also equivalent to argmax.
2.1.2,maybe fix some prediction at this step by modifying log_probs
2.1.2,"shape: (sum(~ self.is_finished), 1)"
2.1.2,in LM task memory_lengths is associated with currently generated src
2.1.2,and therefore needs to follow the generation
2.1.2,!/usr/bin/env python
2.1.2,Maintains the longest src and tgt length in the current batch
2.1.2,Reset current longest length at a new batch (count=1)
2.1.2,max_tgt_in_batch = 0
2.1.2,Src: [<bos> w1 ... wN <eos>]
2.1.2,Tgt: [w1 ... wM <eos>]
2.1.2,for debugging
2.1.2,TODO: maybe add dynamic part
2.1.2,Statistics
2.1.2,Turn any copied words into UNKs.
2.1.2,"Decoder forward, takes [tgt_len, batch, nfeats] as input"
2.1.2,"and [src_len, batch, hidden] as memory_bank"
2.1.2,"in case of inference tgt_len = 1, batch = beam times batch_size"
2.1.2,"in case of Gold Scoring tgt_len = actual length, batch = 1 batch"
2.1.2,Generator forward.
2.1.2,"returns [(batch_size x beam_size) , vocab ] when 1 step"
2.1.2,"or [ tgt_len, batch_size, vocab ] when full sentence"
2.1.2,"here we have scores [tgt_lenxbatch, vocab] or [beamxbatch, vocab]"
2.1.2,"returns [(batch_size x beam_size) , vocab ] when 1 step"
2.1.2,"or [ tgt_len, batch_size, vocab ] when full sentence"
2.1.2,(0) add BOS and padding to tgt prediction
2.1.2,(1) Encoder forward.
2.1.2,(2) Repeat src objects `n_best` times.
2.1.2,"We use batch_size x n_best, get ``(src_len, batch * n_best, nfeat)``"
2.1.2,"(3) Init decoder with n_best src,"
2.1.2,"reshape tgt to ``(len, batch * n_best, nfeat)``"
2.1.2,masked_select
2.1.2,get aligned src id for each prediction's valid tgt tokens
2.1.2,TODO: support these blacklisted features
2.1.2,(0) Prep the components of the search.
2.1.2,(1) Run the encoder on the src.
2.1.2,(2) prep decode_strategy. Possibly repeat src objects.
2.1.2,(3) Begin decoding step by step:
2.1.2,Reorder states.
2.1.2,TODO: support these blacklisted features
2.1.2,hack [min_len_batch-1:] because expect <bos>
2.1.2,(0) Prep the components of the search.
2.1.2,(1) split src into src and target_prefix to avoid padding.
2.1.2,(2) init decoder
2.1.2,(3) prep decode_strategy. Possibly repeat src objects.
2.1.2,(4) Begin decoding step by step:
2.1.2,Reorder states.
2.1.2,select indexes in model state/cache
2.1.2,beam parameters
2.1.2,beam state
2.1.2,BoolTensor was introduced in pytorch 1.2
2.1.2,"""global state"" of the old beam"
2.1.2,buffers for the topk scores and 'backpointer'
2.1.2,for testing
2.1.2,maybe fix some prediction at this step by modifying log_probs
2.1.2,Flatten probs into a list of possibilities.
2.1.2,Penalize beams that finished.
2.1.2,"on real data (newstest2017) with the pretrained transformer,"
2.1.2,it's faster to not move this back to the original device
2.1.2,Store finished hypotheses for this batch.
2.1.2,End condition is the top beam finished and we can return
2.1.2,n_best hypotheses.
2.1.2,"If all sentences are translated, no need to go further."
2.1.2,Remove finished batches for the next step.
2.1.2,using integer division to get an integer _B without casting
2.1.2,force the output to be longer than self.min_length
2.1.2,Multiply probs by the beam probability.
2.1.2,"if the sequence ends now, then the penalty is the current"
2.1.2,"length + 1, to include the EOS token"
2.1.2,Avoid any direction that would repeat unwanted ngrams
2.1.2,Pick up candidate token by curr_scores
2.1.2,Recover log probs.
2.1.2,Length penalty is just a scalar. It doesn't matter if it's applied
2.1.2,before or after the topk.
2.1.2,Resolve beam origin and map to batch index flat representation.
2.1.2,Append last prediction.
2.1.2,update global state (step == 1)
2.1.2,update global state (step > 1)
2.1.2,"shape: (batch_size x beam_size, 1)"
2.1.2,in LM task memory_lengths is associated with currently generated src
2.1.2,and therefore needs to follow the generation
2.1.2,in LM task memory_lengths is associated with currently generated src
2.1.2,and therefore needs to follow the generation
2.1.2,Term will be subtracted from probability
2.1.2,Probability will be divided by this
2.1.2,these warnings indicate that either the alpha/beta
2.1.2,"forces a penalty to be a no-op, or a penalty is a no-op but"
2.1.2,the alpha/beta would suggest otherwise.
2.1.2,using some length penalty
2.1.2,using some coverage penalty
2.1.2,!/usr/bin/env python
2.1.2,semaphore doesn't have a timeout arg in Python 2.7
2.1.2,perform a first request to initialize everything
2.1.2,backwards compatibility for confs
2.1.2,every segment becomes a dict for flexibility purposes
2.1.2,NOTE: translator returns lists of `n_best` list
2.1.2,build back results with empty texts
2.1.2,load can be called multiple times: modify copy
2.1.2,output contain alignment
2.1.2,Below are all the different penalty terms implemented so far.
2.1.2,Subtract coverage penalty from topk log probs.
2.1.2,Divide topk log probs by length penalty.
2.1.2,Sorting
2.1.2,Chinese segmentation
2.1.2,Chinese simplify -> Chinese traditional standard
2.1.2,Chinese simplify -> Chinese traditional (HongKong)
2.1.2,Chinese simplify -> Chinese traditional (Taiwan)
2.1.2,Chinese traditional -> Chinese simplify (v1)
2.1.2,Chinese traditional -> Chinese simplify (v2)
2.1.1,!/usr/bin/env python
2.1.1,!/usr/bin/env python
2.1.1,!/usr/bin/env python
2.1.1,!/usr/bin/env python
2.1.1,!/usr/bin/env python
2.1.1,!/usr/bin/env python3
2.1.1,-*- coding: utf-8 -*-
2.1.1,
2.1.1,"OpenNMT-py documentation build configuration file, created by"
2.1.1,sphinx-quickstart on Sun Dec 17 12:07:14 2017.
2.1.1,
2.1.1,This file is execfile()d with the current directory set to its
2.1.1,containing dir.
2.1.1,
2.1.1,Note that not all possible configuration values are present in this
2.1.1,autogenerated file.
2.1.1,
2.1.1,All configuration values have a default; values that are commented out
2.1.1,serve to show the default.
2.1.1,"If extensions (or modules to document with autodoc) are in another directory,"
2.1.1,add these directories to sys.path here. If the directory is relative to the
2.1.1,"documentation root, use os.path.abspath to make it absolute, like shown here."
2.1.1,
2.1.1,import os
2.1.1,import sys
2.1.1,"sys.path.insert(0, os.path.abspath('.'))"
2.1.1,-- General configuration ------------------------------------------------
2.1.1,"If your documentation needs a minimal Sphinx version, state it here."
2.1.1,
2.1.1,needs_sphinx = '1.0'
2.1.1,"Add any Sphinx extension module names here, as strings. They can be"
2.1.1,extensions coming with Sphinx (named 'sphinx.ext.*') or your custom
2.1.1,ones.
2.1.1,Show base classes
2.1.1,"Use ""variables"" section for Attributes instead of weird block things"
2.1.1,mimicking the function style.
2.1.1,"Add any paths that contain templates here, relative to this directory."
2.1.1,The suffix(es) of source filenames.
2.1.1,You can specify multiple suffix as a list of string:
2.1.1,
2.1.1,"source_suffix = ['.rst', '.md']"
2.1.1,The master toctree document.
2.1.1,General information about the project.
2.1.1,"The version info for the project you're documenting, acts as replacement for"
2.1.1,"|version| and |release|, also used in various other places throughout the"
2.1.1,built documents.
2.1.1,
2.1.1,The short X.Y version.
2.1.1,"The full version, including alpha/beta/rc tags."
2.1.1,The language for content autogenerated by Sphinx. Refer to documentation
2.1.1,for a list of supported languages.
2.1.1,
2.1.1,This is also used if you do content translation via gettext catalogs.
2.1.1,"Usually you set ""language"" from the command line for these cases."
2.1.1,"List of patterns, relative to source directory, that match files and"
2.1.1,directories to ignore when looking for source files.
2.1.1,This patterns also effect to html_static_path and html_extra_path
2.1.1,The name of the Pygments (syntax highlighting) style to use.
2.1.1,"If true, `todo` and `todoList` produce output, else they produce nothing."
2.1.1,-- Options for HTML output ----------------------------------------------
2.1.1,The theme to use for HTML and HTML Help pages.  See the documentation for
2.1.1,a list of builtin themes.
2.1.1,
2.1.1,html_theme = 'sphinx_materialdesign_theme'
2.1.1,html_theme_path = [sphinx_materialdesign_theme.get_path()]
2.1.1,Theme options are theme-specific and customize the look and feel of a theme
2.1.1,"further.  For a list of options available for each theme, see the"
2.1.1,documentation.
2.1.1,
2.1.1,html_theme_options = {}
2.1.1,"Add any paths that contain custom static files (such as style sheets) here,"
2.1.1,"relative to this directory. They are copied after the builtin static files,"
2.1.1,"so a file named ""default.css"" will overwrite the builtin ""default.css""."
2.1.1,"Custom sidebar templates, must be a dictionary that maps document names"
2.1.1,to template names.
2.1.1,
2.1.1,This is required for the alabaster theme
2.1.1,refs: http://alabaster.readthedocs.io/en/latest/installation.html#sidebars
2.1.1,-- Options for HTMLHelp output ------------------------------------------
2.1.1,Output file base name for HTML help builder.
2.1.1,-- Options for LaTeX output ---------------------------------------------
2.1.1,The paper size ('letterpaper' or 'a4paper').
2.1.1,
2.1.1,"'papersize': 'letterpaper',"
2.1.1,"The font size ('10pt', '11pt' or '12pt')."
2.1.1,
2.1.1,"'pointsize': '10pt',"
2.1.1,Additional stuff for the LaTeX preamble.
2.1.1,
2.1.1,"'preamble': '',"
2.1.1,Latex figure (float) alignment
2.1.1,
2.1.1,"'figure_align': 'htbp',"
2.1.1,Grouping the document tree into LaTeX files. List of tuples
2.1.1,"(source start file, target name, title,"
2.1.1,"author, documentclass [howto, manual, or own class])."
2.1.1,-- Options for manual page output ---------------------------------------
2.1.1,One entry per manual page. List of tuples
2.1.1,"(source start file, name, description, authors, manual section)."
2.1.1,-- Options for Texinfo output -------------------------------------------
2.1.1,Grouping the document tree into Texinfo files. List of tuples
2.1.1,"(source start file, target name, title, author,"
2.1.1,"dir menu entry, description, category)"
2.1.1,!/usr/bin/env python
2.1.1,-*- coding: utf-8 -*-
2.1.1,is this reachable?
2.1.1,Read in embeddings
2.1.1,Write to file
2.1.1,converts a SentencePiece vocabulary to the format expected by dynamic data
2.1.1,"(essentially converts float expected counts to ""fixed precision"" int pseudo"
2.1.1,counts)
2.1.1,"Add in default model arguments, possibly added since training."
2.1.1,build_base_model expects updated and validated opts
2.1.1,-*- encoding: utf-8 -*-
2.1.1,!/usr/bin/env python
2.1.1,-*- coding: utf-8 -*-
2.1.1,Author: Rico Sennrich
2.1.1,flake8: noqa
2.1.1,This file is retrieved from https://github.com/rsennrich/subword-nmt
2.1.1,hack for python2/3 compatibility
2.1.1,check version information
2.1.1,some hacking to deal with duplicates (only consider first instance)
2.1.1,don't print end-of-word symbols
2.1.1,sys.stderr.write('cannot split {0} further.\n'.format(segment))
2.1.1,sys.stderr.write('OOV: {0}\n'.format(segment))
2.1.1,sys.stderr.write('OOV: {0}\n'.format(segment))
2.1.1,python 2/3 compatibility
2.1.1,read/write files as UTF-8
2.1.1,!/usr/bin/env python
2.1.1,!/usr/bin/env python
2.1.1,-*- coding: utf-8 -*-
2.1.1,Author: Rico Sennrich
2.1.1,flake8: noqa
2.1.1,This file is retrieved from https://github.com/rsennrich/subword-nmt
2.1.1,hack for python2/3 compatibility
2.1.1,"find all instances of pair, and update frequency/indices around it"
2.1.1,find first symbol
2.1.1,"if first symbol is followed by second symbol, we've found an occurrence of pair (old_word[i:i+2])"
2.1.1,"assuming a symbol sequence ""A B C"", if ""B C"" is merged, reduce the frequency of ""A B"""
2.1.1,"assuming a symbol sequence ""A B C B"", if ""B C"" is merged, reduce the frequency of ""C B""."
2.1.1,"however, skip this if the sequence is A B C B C, because the frequency of ""C B"" will be reduced by the previous code block"
2.1.1,find new pair
2.1.1,"assuming a symbol sequence ""A BC D"", if ""B C"" is merged, increase the frequency of ""A BC"""
2.1.1,"assuming a symbol sequence ""A BC B"", if ""B C"" is merged, increase the frequency of ""BC B"""
2.1.1,"however, if the sequence is A BC BC, skip this step because the count of ""BC BC"" will be incremented by the previous code block"
2.1.1,data structure of pair frequencies
2.1.1,index from pairs to words
2.1.1,version 0.2 changes the handling of the end-of-word token ('</w>');
2.1.1,version numbering allows bckward compatibility
2.1.1,"threshold is inspired by Zipfian assumption, but should only affect speed"
2.1.1,we probably missed the best pair because of pruning; go back to full statistics
2.1.1,"threshold is inspired by Zipfian assumption, but should only affect speed"
2.1.1,python 2/3 compatibility
2.1.1,read/write files as UTF-8
2.1.1,!/usr/bin/env python
2.1.1,-*- coding: utf-8 -*-
2.1.1,!/usr/bin/env python
2.1.1,Avoid functionality on inference
2.1.1,Build embeddings.
2.1.1,Build encoder.
2.1.1,Build embeddings.
2.1.1,Build decoder.
2.1.1,Share the embedding matrix - preprocess with share_vocab required.
2.1.1,src/tgt vocab should be the same if `-share_vocab` is specified.
2.1.1,Update vocabulary embeddings with checkpoint embeddings
2.1.1,Embedding layers
2.1.1,Just for debugging purposes
2.1.1,for back compat when attention_dropout was not defined
2.1.1,Build Model
2.1.1,Build Generator.
2.1.1,Load the model states from checkpoint or initialize them.
2.1.1,This preserves backward-compat for models using customed layernorm
2.1.1,end of patch for backward compatibility
2.1.1,Update model embeddings with those from the checkpoint after initialization
2.1.1,Remove old vocabulary associated embeddings
2.1.1,Embedding layers
2.1.1,!/usr/bin/env python
2.1.1,ensure tensorboard output is written in the directory
2.1.1,of previous checkpoints
2.1.1,Override checkpoint's update_embeddings as it defaults to false
2.1.1,NOTE: It's important that ``opt`` has been validated and updated
2.1.1,at this point.
2.1.1,Build model.
2.1.1,Build optimizer.
2.1.1,Build model saver
2.1.1,Move batch to specified device
2.1.1,Use Tensorboard for visualization during training
2.1.1,Options only during inference
2.1.1,"Truncation options, for text corpus"
2.1.1,"as for False, this will be added in _add_train_general_opts"
2.1.1,Embedding Options
2.1.1,Model Task Options
2.1.1,Encoder-Decoder Options
2.1.1,"group.add('--residual', '-residual',   action=""store_true"","
2.1.1,"help=""Add residual connections between RNN layers."")"
2.1.1,The following options (bridge_extra_node to n_steps) are used
2.1.1,for training with --encoder_type ggnn (Gated Graph Neural Network).
2.1.1,Attention options
2.1.1,Alignement options
2.1.1,Generator and loss options.
2.1.1,GPU
2.1.1,Init options
2.1.1,Pretrained word vectors
2.1.1,Freeze word vectors
2.1.1,Optimization options
2.1.1,learning rate
2.1.1,options relate to data preprare
2.1.1,options relate to train
2.1.1,Alpha and Beta values for Google Length + Coverage penalty
2.1.1,"Described here: https://arxiv.org/pdf/1609.08144.pdf, Section 7"
2.1.1,Length penalty options
2.1.1,Coverage penalty options
2.1.1,Decoding Length constraint
2.1.1,Decoding content constraint
2.1.1,Adding options relate to decoding strategy
2.1.1,Adding option for logging
2.1.1,Copyright 2016 The Chromium Authors. All rights reserved.
2.1.1,Use of this source code is governed by a BSD-style license that can be
2.1.1,found in the LICENSE file.
2.1.1,"Get the key 'value' in the dict, or just use 'value'"
2.1.1,Basic attributes.
2.1.1,Set model in training mode.
2.1.1,UPDATE DROPOUT
2.1.1,Run patience mechanism
2.1.1,"If the patience has reached the limit, stop training"
2.1.1,swap model params w/ moving average
2.1.1,(and keep the original parameters)
2.1.1,Set model in validating mode.
2.1.1,F-prop through the model.
2.1.1,Compute loss.
2.1.1,Update statistics.
2.1.1,Set model back to training mode.
2.1.1,Truncated BPTT: reminder not compatible with accum > 1
2.1.1,1. Create truncated target.
2.1.1,2. F-prop all but generator.
2.1.1,3. Compute loss.
2.1.1,4. Update the parameters and statistics.
2.1.1,Multi GPU gradient gather
2.1.1,"If truncated, don't backprop fully."
2.1.1,TO CHECK
2.1.1,if dec_state is not None:
2.1.1,dec_state.detach()
2.1.1,"in case of multi step gradient accumulation,"
2.1.1,update only after accum batches
2.1.1,For Flake
2.1.1,we avoid padding while mean pooling
2.1.1,incoming and outgoing edge embedding
2.1.1,Find vocab data for tree builting
2.1.1,Propogation Model
2.1.1,Initialize the bridge layer
2.1.1,Token embedding
2.1.1,Initialize graph using formatted input sequence
2.1.1,Number of flagged nodes defines node count for this sample
2.1.1,"(Nodes can have no flags on them, but must be in 'flags' list)."
2.1.1,The total number of integers in the vocab should allow
2.1.1,for all features and edges to be defined.
2.1.1,Use first extra node as only source for decoder init
2.1.1,Average all nodes to get bridge input
2.1.1,"LSTM has hidden and cell state, other only one"
2.1.1,Total number of states
2.1.1,Build a linear layer for each
2.1.1,Initialize the bridge layer
2.1.1,"s_len, batch, emb_dim = emb.size()"
2.1.1,Lengths data is wrapped inside a Tensor.
2.1.1,"LSTM has hidden and cell state, other only one"
2.1.1,Total number of states
2.1.1,Build a linear layer for each
2.1.1,"s_len, batch, emb_dim = emb.size()"
2.1.1,Run the forward pass of every layer of the tranformer.
2.1.1,Dimensions and padding for constructing the word embedding matrix
2.1.1,Dimensions and padding for feature embedding matrices
2.1.1,(these have no effect if feat_vocab_sizes is empty)
2.1.1,The embedding matrix look-up tables. The first look-up table
2.1.1,"is for words. Subsequent ones are for features, if any exist."
2.1.1,The final output size of word + feature vectors. This can vary
2.1.1,from the word vector size if and only if features are defined.
2.1.1,This is the attribute you should access if you need to know
2.1.1,how big your embeddings are going to be.
2.1.1,The sequence of operations that converts the input sequence
2.1.1,into a sequence of embeddings. At minimum this consists of
2.1.1,looking up the embeddings for each word and feature in the
2.1.1,input. Model parameters may require the sequence to contain
2.1.1,additional operations as well.
2.1.1,features must use word_vec_size
2.1.1,features will use feat_vec_size
2.1.1,Some utilitary functions for pretrained embeddings
2.1.1,is this reachable?
2.1.1,Write to file
2.1.1,set the opt in place
2.1.1,set the opt in place
2.1.1,This class is mainly used by decoder.py for RNNs but also
2.1.1,by the CNN / transformer decoder when copy attention is used
2.1.1,CNN has its own attention mechanism ConvMultiStepAttention
2.1.1,Transformer has its own MultiHeadedAttention
2.1.1,mlp wants it with bias
2.1.1,Check input sizes
2.1.1,"(batch, t_len, d) x (batch, d, s_len) --> (batch, t_len, s_len)"
2.1.1,"(batch, t_len, s_len, d)"
2.1.1,one step input
2.1.1,"compute attention scores, as in Luong et al."
2.1.1,Softmax or sparsemax to normalize attention weights
2.1.1,each context vector c_t is the weighted average
2.1.1,over all the source hidden states
2.1.1,concatenate
2.1.1,Check output sizes
2.1.1,Check output sizes
2.1.1,clamping necessary because of numerical errors: loss should be lower
2.1.1,"bounded by zero, but negative values near zero are possible without"
2.1.1,the clamp
2.1.1,from onmt.utils.misc import aeq
2.1.1,CHECKS
2.1.1,"batch, k_len, d = key.size()"
2.1.1,"batch_, k_len_, d_ = value.size()"
2.1.1,"aeq(batch, batch_)"
2.1.1,"aeq(k_len, k_len_)"
2.1.1,"aeq(d, d_)"
2.1.1,"batch_, q_len, d_ = query.size()"
2.1.1,"aeq(batch, batch_)"
2.1.1,"aeq(d, d_)"
2.1.1,"aeq(self.model_dim % 8, 0)"
2.1.1,if mask is not None:
2.1.1,"batch_, q_len_, k_len_ = mask.size()"
2.1.1,"aeq(batch_, batch)"
2.1.1,"aeq(k_len_, k_len)"
2.1.1,aeq(q_len_ == q_len)
2.1.1,END CHECKS
2.1.1,"1) Project key, value, and query."
2.1.1,1 or key_len x key_len
2.1.1,1 or key_len x key_len x dim_per_head
2.1.1,1 or key_len x key_len x dim_per_head
2.1.1,2) Calculate and scale scores.
2.1.1,batch x num_heads x query_len x key_len
2.1.1,3) Apply attention dropout and compute context vectors.
2.1.1,CHECK
2.1.1,"batch_, q_len_, d_ = output.size()"
2.1.1,"aeq(q_len, q_len_)"
2.1.1,"aeq(batch, batch_)"
2.1.1,"aeq(d, d_)"
2.1.1,Return multi-head attn
2.1.1,At the moment this class is only used by embeddings.Embeddings look-up tables
2.1.1,-*- coding: utf-8 -*-
2.1.1,checks
2.1.1,"batch, channel, height, width = base_target_emb.size()"
2.1.1,"batch_, channel_, height_, width_ = input_from_dec.size()"
2.1.1,"enc_batch, enc_channel, enc_height = encoder_out_top.size()"
2.1.1,"enc_batch_, enc_channel_, enc_height_ = encoder_out_combine.size()"
2.1.1,out_features * in_features
2.1.1,norm is out_features * 1
2.1.1,batch_size * out_features
2.1.1,out_features
2.1.1,out_features
2.1.1,batch_size * out_features
2.1.1,"out_channels, in_channels // groups, * kernel_size"
2.1.1,out_features
2.1.1,This is used nowhere in the code at the moment (Vincent Nguyen 05/18/2018)
2.1.1,"in_channels, out_channels, *kernel_size"
2.1.1,"in_channels, out_channels, *kernel_size"
2.1.1,"self.out_channels, 1"
2.1.1,out_features
2.1.1,out_features
2.1.1,store roots on diagonal
2.1.1,CHECKS
2.1.1,Original probabilities.
2.1.1,Probability of copying p(z=1) batch.
2.1.1,Probability of not copying: p_{word}(w) * (1 - p(z))
2.1.1,probabilities assigned by the model to the gold targets
2.1.1,probability of tokens copied from source
2.1.1,Set scores for unk to 0 and add eps
2.1.1,find the indices in which you do not use the copy mechanism
2.1.1,Drop padding.
2.1.1,this block does not depend on the loss value computed above
2.1.1,and is used only for stats
2.1.1,this block does not depend on the loss value computed above
2.1.1,and is used only for stats
2.1.1,Correct target copy token instead of <unk>
2.1.1,tgt[i] = align[i] + len(tgt_vocab)
2.1.1,for i such that tgt[i] == 0 and align[i] != 0
2.1.1,Compute sum of perplexities for stats
2.1.1,this part looks like it belongs in CopyGeneratorLoss
2.1.1,Compute Loss as NLL divided by seq length
2.1.1,Compute Total Loss per sequence in batch
2.1.1,Divide by length of each sequence and sum
2.1.1,Auto import python files in this directory
2.1.1,1. sample number of tokens to corrupt
2.1.1,2. sample positions to corrput
2.1.1,3. sample corrupted values
2.1.1,1. sample number of tokens to corrupt
2.1.1,2. sample positions to corrput
2.1.1,3. Drop token on chosen position
2.1.1,1. sample number of tokens to corrupt
2.1.1,2. sample positions to corrput
2.1.1,3. mask word on chosen position
2.1.1,"Sharing options among `TokenizerTransform`s, same name conflict in"
2.1.1,this scope will be resolved by remove previous occurrence in parser
2.1.1,subword regularization(or BPE dropout) options:
2.1.1,subword vocabulary restriction options:
2.1.1,derterministic subwording
2.1.1,subword sampling when nbest_size > 1 or -1
2.1.1,alpha should be 0.0 < alpha < 1.0
2.1.1,Load vocabulary file if provided and set threshold
2.1.1,Load Subword Model
2.1.1,-1: keep everything (i.e. 1 mask per token)
2.1.1,0: replace everything (i.e. no mask)
2.1.1,1: 1 mask per span
2.1.1,view each subword as word start / input is word level token
2.1.1,Pretend it ends with a full stop so last span is a sentence
2.1.1,"Tokens that are full stops, where the previous token is not"
2.1.1,Make sure we have enough to mask
2.1.1,Trim to masking budget
2.1.1,Handle 0-length mask (inserts) separately
2.1.1,assert is_word_start[-1] == 0
2.1.1,assert tokens_length - 1 not in indices
2.1.1,"keep index, but replace it with [MASK]"
2.1.1,"acts as a long length, so spans don't go over the end of doc"
2.1.1,next position from each word_start
2.1.1,delete token: 1 mask/remove per span
2.1.1,"keep index, but replace it with [MASK]: 1 mask per token"
2.1.1,A bit faster when all lengths are 1
2.1.1,to cover whole token
2.1.1,delete token
2.1.1,"keep index, but replace it with [MASK]"
2.1.1,assert tokens_length - 1 not in indices
2.1.1,initialize fields at the top of each unit test to prevent
2.1.1,any undesired stateful effects
2.1.1,"this test touches the file system, so it could be considered an"
2.1.1,integration test
2.1.1,write utf-8 bytes
2.1.1,batch 0 will always predict EOS. The other batches will predict
2.1.1,non-eos scores.
2.1.1,"""best"" prediction is eos - that should be blocked"
2.1.1,include at least one prediction OTHER than EOS
2.1.1,that is greater than -1e20
2.1.1,now batch 0 has ended and no others have
2.1.1,initial step
2.1.1,batch 0 dies on step 0
2.1.1,include at least one prediction OTHER than EOS
2.1.1,that is greater than -1e20
2.1.1,step 2
2.1.1,(old) batch 8 dies on step 1
2.1.1,step 3
2.1.1,everything dies
2.1.1,initial step
2.1.1,batch 0 dies on step 0
2.1.1,include at least one prediction OTHER than EOS
2.1.1,that is greater than -1e20
2.1.1,step 2
2.1.1,(old) batch 8 dies on step 1
2.1.1,step 3
2.1.1,everything dies
2.1.1,initial step
2.1.1,finish one beam
2.1.1,include at least one prediction OTHER than EOS
2.1.1,that is greater than -1e20
2.1.1,step 2
2.1.1,finish example in last batch
2.1.1,(old) batch 8 dies on step 1
2.1.1,step 3
2.1.1,everything dies
2.1.1,initial step
2.1.1,batch 0 dies on step 0
2.1.1,include at least one prediction OTHER than EOS
2.1.1,that is greater than -1e20
2.1.1,step 2
2.1.1,(old) batch 8 dies on step 1
2.1.1,step 3
2.1.1,everything dies
2.1.1,illegal_weights_mask = torch.ByteTensor([
2.1.1,"[0, 0, 0, 0, 0, 0, 0],"
2.1.1,"[0, 0, 0, 1, 1, 1, 1],"
2.1.1,"[0, 0, 0, 0, 0, 1, 1],"
2.1.1,"[0, 0, 1, 1, 1, 1, 1]])"
2.1.1,TODO: fix for pytorch 0.3
2.1.1,illegal_weights = alignments.masked_select(illegal_weights_mask)
2.1.1,"self.assertEqual(0.0, illegal_weights.data.sum())"
2.1.1,this could be considered an integration test because it touches
2.1.1,the filesystem for the config file (and the models)
2.1.1,no dummy prefix
2.1.1,no dummy prefix
2.1.1,transforms that require vocab will not create if not provide vocab
2.1.1,filter_transform.warm_up()
2.1.1,test BPE-dropout:
2.1.1,1. disable bpe dropout for not training example
2.1.1,2. enable bpe dropout for training example
2.1.1,3. (NOTE) disable dropout won't take effect if already seen
2.1.1,this is caused by the cache mechanism in bpe:
2.1.1,return cached subword if the original token is seen when no dropout
2.1.1,test SP regularization:
2.1.1,1. enable regularization for training example
2.1.1,2. disable regularization for not training example
2.1.1,Not apply token drop for not training example
2.1.1,apply token drop for training example
2.1.1,Not apply token mask for not training example
2.1.1,apply token mask for training example
2.1.1,require vocabs to warm_up
2.1.1,Not apply token mask for not training example
2.1.1,apply token mask for training example
2.1.1,"Defalt: full_stop_token=[""."", ""?"", ""!""]"
2.1.1,"Defalt: full_stop_token=[""."", ""?"", ""!""]"
2.1.1,random_ratio of inserted tokens are chosen in vocab
2.1.1,others are MASK_TOK
2.1.1,"insert_ratio=0.0,"
2.1.1,"random_ratio=0.0,"
2.1.1,"Defalt: full_stop_token=[""."", ""?"", ""!""]"
2.1.1,all token are considered as an individual word
2.1.1,1. tokens are dropped when replace_length is 0
2.1.1,"print(f""token delete: {masked} / {tokens}"")"
2.1.1,2. tokens are replaced by MASK when replace_length is 1
2.1.1,"print(f""token mask: {masked} / {tokens}"")"
2.1.1,"insert_ratio=0.0,"
2.1.1,"random_ratio=0.0,"
2.1.1,"Defalt: full_stop_token=[""."", ""?"", ""!""]"
2.1.1,start token of word are identified using subword marker
2.1.1,"1. replace_length 0: ""words"" are dropped"
2.1.1,"print(f""word delete: {masked} / {tokens}"")"
2.1.1,"self.assertEqual(len(masked), n_words - n_masked)"
2.1.1,"2. replace_length 1: ""words"" are replaced with a single MASK"
2.1.1,"print(f""whole word single mask: {masked} / {tokens}"")"
2.1.1,len(masked) depend on number of tokens in select word
2.1.1,"3. replace_length -1: all tokens in ""words"" are replaced with MASK"
2.1.1,"print(f""whole word multi mask: {masked} / {tokens}"")"
2.1.1,number of mask_tok depend on number of tokens in selected word
2.1.1,number of MASK_TOK can be greater than n_masked
2.1.1,"insert_ratio=0.5,"
2.1.1,"random_ratio=0.3,"
2.1.1,"Defalt: full_stop_token=[""."", ""?"", ""!""]"
2.1.1,start token of word are identified using subword marker
2.1.1,n_words = sum(token_starts)
2.1.1,n_masked = math.ceil(n_words * bart_noise.mask_ratio)
2.1.1,"print(f""Text Span Infilling: {infillied} / {tokens}"")"
2.1.1,"print(n_words, n_masked)"
2.1.1,!/usr/bin/env python
2.1.1,-*- coding: utf-8 -*-
2.1.1,Inject some dummy training options that may needed when build fields
2.1.1,Remove the generated *pt files.
2.1.1,Remove the generated data samples
2.1.1,all beams repeat (beam >= 1 repeat dummy scores)
2.1.1,predict repeat_idx over and over again
2.1.1,"before repeat, scores are either 0 or -inf"
2.1.1,"on repeat, `repeat_idx` score is BLOCKED_SCORE"
2.1.1,"(but it's still the best score, thus we have"
2.1.1,"[BLOCKED_SCORE, -inf, -inf, -inf, -inf]"
2.1.1,repetitions keeps maximizing score
2.1.1,"index 0 has been blocked, so repeating=>+0.0 score"
2.1.1,other indexes are -inf so repeating=>BLOCKED_SCORE
2.1.1,which is higher
2.1.1,beam 0 and beam >=2 will repeat (beam >= 2 repeat dummy scores)
2.1.1,non-interesting beams are going to get dummy values
2.1.1,"on initial round, only predicted scores for beam 0"
2.1.1,matter. Make two predictions. Top one will be repeated
2.1.1,"in beam zero, second one will live on in beam 1."
2.1.1,predict the same thing in beam 0
2.1.1,continue pushing around what beam 1 predicts
2.1.1,"now beam 0 dies (along with the others), beam 1 -> beam 0"
2.1.1,"now beam 0 dies (along with the others), beam 1 -> beam 0"
2.1.1,beam 0 and beam >= 2 will repeat (beam 2 repeats excluded idx)
2.1.1,non-interesting beams are going to get dummy values
2.1.1,predict the same thing in beam 0
2.1.1,continue pushing around what beam 1 predicts
2.1.1,predict the allowed-repeat again in beam 2
2.1.1,"now beam 0 dies, beam 1 -> beam 0, beam 2 -> beam 1"
2.1.1,and the rest die
2.1.1,"since all preds after i=0 are 0, we can check"
2.1.1,that the beam is the correct idx by checking that
2.1.1,the curr score is the initial score
2.1.1,beam 0 will always predict EOS. The other beams will predict
2.1.1,non-eos scores.
2.1.1,non-interesting beams are going to get dummy values
2.1.1,"""best"" prediction is eos - that should be blocked"
2.1.1,include at least beam_sz predictions OTHER than EOS
2.1.1,that are greater than -1e20
2.1.1,predict eos in beam 0
2.1.1,provide beam_sz other good predictions
2.1.1,now the top beam has ended and no others have
2.1.1,"not of interest, but want to make sure it keeps running"
2.1.1,since only beam 0 terminates and n_best = 2
2.1.1,"this is also a test that when block_ngram_repeat=0,"
2.1.1,repeating is acceptable
2.1.1,non-interesting beams are going to get dummy values
2.1.1,"""best"" prediction is eos - that should be blocked"
2.1.1,include at least beam_sz predictions OTHER than EOS
2.1.1,that are greater than -1e20
2.1.1,predict eos in beam 1
2.1.1,provide beam_sz other good predictions in other beams
2.1.1,provide beam_sz other good predictions in other beams
2.1.1,beam 1 dies on min_length
2.1.1,beam 0 dies on the step after beam 1 dies
2.1.1,"inp_lens is tiled in initialize, reassign to make attn match"
2.1.1,non-interesting beams are going to get dummy values
2.1.1,"""best"" prediction is eos - that should be blocked"
2.1.1,include at least beam_sz predictions OTHER than EOS
2.1.1,that are greater than -1e20
2.1.1,predict eos in beam 1
2.1.1,provide beam_sz other good predictions in other beams
2.1.1,provide beam_sz other good predictions in other beams
2.1.1,no top beams are finished yet
2.1.1,beam 1 dies on min_length
2.1.1,no top beams are finished yet
2.1.1,beam 0 dies on the step after beam 1 dies
2.1.1,top beam is finished now so there are attentions
2.1.1,two beams are finished in each batch
2.1.1,second dim is cut down to the non-padded src length
2.1.1,first dim is equal to the time of death
2.1.1,(beam 0 died at current step - adjust for SOS)
2.1.1,(beam 1 died at last step - adjust for SOS)
2.1.1,behavior gets weird when beam is already done so just stop
2.1.1,this is just test_beam.TestBeamAgainstReferenceCase repeated
2.1.1,in each batch.
2.1.1,"init_preds: [4, 3, 5, 6, 7] - no EOS's"
2.1.1,no EOS's yet
2.1.1,"[5, 3, 2, 6, 0], so beam 2 predicts EOS!"
2.1.1,assumes beam 2 finished on last step
2.1.1,ended beam 2 shouldn't continue
2.1.1,"[2, 5, 3, 6, 0] repeat self.BATCH_SZ, so beam 0 predicts EOS!"
2.1.1,"[-2.4879, -3.8910, -4.1010, -4.2010, -4.4010] repeat self.BATCH_SZ"
2.1.1,another beam is finished in all batches
2.1.1,new beam 0 finished
2.1.1,new beam 0 is old beam 3
2.1.1,assumes beam 0 finished on last step
2.1.1,"[5, 2, 6, 1, 0] repeat self.BATCH_SZ, so beam 1 predicts EOS!"
2.1.1,new beam 1 finished
2.1.1,new beam 1 is old beam 4
2.1.1,this could be considered an integration test because it tests
2.1.1,interactions between the GNMT scorer and the beam
2.1.1,"-data option is required, but not used in this test, so dummy."
2.1.1,len x batch x nfeat
2.1.1,Initialize vectors to compare size with
2.1.1,Ensure correct sizes and types
2.1.1,Make sure that output has the correct size and type
2.1.1,"[('encoder_type', 'transformer'),"
2.1.1,"('word_vec_size', 16), ('rnn_size', 16)],"
2.1.1,""""""" Only do SRU test if requirment is safisfied. """""""
2.1.1,SRU doesn't support input_feed.
2.1.1,first check there's nothing unexpectedly not trainable
2.1.1,ok: word embeddings shouldn't be trainable
2.1.1,if word vecs are freezed
2.1.1,ok: positional encodings shouldn't be trainable
2.1.1,then check nothing unexpectedly trainable
2.1.1,Decoder state
2.1.1,Build the RNN.
2.1.1,Set up the context gate.
2.1.1,Set up the standard attention.
2.1.1,The encoder hidden is  (layers*directions) x batch x dim.
2.1.1,We need to convert it to layers x batch x (directions*dim).
2.1.1,Init the input feed.
2.1.1,Update the state with the result.
2.1.1,Concatenates sequence of tensors along a new dimension.
2.1.1,NOTE: v0.3 to 0.4: dec_outs / attns[*] may not be list
2.1.1,(in particular in case of SRU) it was not raising error in 0.3
2.1.1,since stack(Variable) was allowed.
2.1.1,"In 0.4, SRU returns a tensor that shouldn't be stacke"
2.1.1,Check
2.1.1,Calculate the attention.
2.1.1,Calculate the context gate.
2.1.1,Additional args check.
2.1.1,END Additional args check.
2.1.1,Input feed concatenates hidden state with
2.1.1,input at every time step.
2.1.1,TODO: context gate should be employed
2.1.1,instead of second RNN transform.
2.1.1,Update the coverage attention.
2.1.1,Decoder State
2.1.1,CNNDecoder has its own attention mechanism.
2.1.1,Set up a separate copy attention layer if needed.
2.1.1,The output of CNNEncoder.
2.1.1,The combination of output of CNNEncoder and source embeddings.
2.1.1,Process the result and update the attentions.
2.1.1,Update the state.
2.1.1,TODO change the way attns is returned dict => list or tuple (onnx)
2.1.1,Memory_lengths is a single tensor shared between all models.
2.1.1,This assumption will not hold if Translator is modified
2.1.1,to calculate memory_lengths as something other than the length
2.1.1,of the input.
2.1.1,"return _, (B, Q_len, K_len)"
2.1.1,"layer average attention across heads, get ``(B, Q, K)``"
2.1.1,"Case 1: no full_context, no align heads -> layer avg baseline"
2.1.1,"Case 2: no full_context, 1 align heads -> guided align"
2.1.1,"Case 3: full_context, 1 align heads -> full cte guided align"
2.1.1,BoolTensor was introduced in pytorch 1.2
2.1.1,T: could be 1 in the case of stepwise decoding or tgt_len
2.1.1,masking is necessary when sequence length is greater than one
2.1.1,Decoder State
2.1.1,"previously, there was a GlobalAttention module here for copy"
2.1.1,"attention. But it was never actually used -- the ""copy"" attention"
2.1.1,just reuses the context attention.
2.1.1,"attns[""align""] = torch.stack(attn_aligns, 0).mean(0)  # All avg"
2.1.1,TODO change the way attns is returned dict => list or tuple (onnx)
2.1.1,T: could be 1 in the case of stepwise decoding or tgt_len
2.1.1,masking is necessary when sequence length is greater than one
2.1.1,TODO change the way attns is returned dict => list or tuple (onnx)
2.1.1,"buffer size in bytes, determine equiv. # of elements based on data type"
2.1.1,copy tensors into buffer_t
2.1.1,all-reduce and rescale
2.1.1,copy all-reduced buffer back into tensors
2.1.1,"tensor is bigger than buffer, all-reduce and rescale directly"
2.1.1,"buffer is full, all-reduce and replace buffer with grad"
2.1.1,add tensor to buffer
2.1.1,NOTE: stride (if needed) is handled at the
2.1.1,generator (train_iter) level
2.1.1,Move batch to correspond device_id when consumer iterate
2.1.1,hack to dodge unpicklable `dict_keys`
2.1.1,"propagate exception to parent process, keeping original traceback"
2.1.1,TODO: Find a better way to check for sparse gradients.
2.1.1,we use here a FusedAdam() copy of an old Apex repo
2.1.1,In this case use the old FusedAdam with FP16_optimizer wrapper
2.1.1,Load everything from the checkpoint.
2.1.1,Build everything from scratch.
2.1.1,"Reset optimizer, keep options."
2.1.1,"Reset options, keep optimizer."
2.1.1,State can be partially restored.
2.1.1,"unscaled optimizer's gradients (already done therefore skip),"
2.1.1,skips optimizer.step() if gradients contain infs/NaNs.
2.1.1,Updates the scale for next iteration.
2.1.1,Code below is an implementation of https://arxiv.org/pdf/1804.04235.pdf
2.1.1,inspired but modified from https://github.com/DeadAt0m/adafactor-pytorch
2.1.1,backward compatibility
2.1.1,assuming a list/generator of parameter means single group
2.1.1,compute combined scale factor for this group
2.1.1,norm is in fact norm*scale
2.1.1,note: p.grad should not ever be set for correct operation of
2.1.1,mixed precision optimizer that sometimes sends None gradients
2.1.1,State initialization
2.1.1,Exponential moving average of gradient values
2.1.1,Exponential moving average of squared gradient values
2.1.1,-*- coding: utf-8 -*-
2.1.1,if the loss function operates on vectors of raw logits instead of
2.1.1,"probabilities, only the first part of the generator needs to be"
2.1.1,"passed to the NMTLossCompute. At the moment, the only supported"
2.1.1,loss function of this kind is the sparsemax loss.
2.1.1,"attn_align should be in (batch_size, pad_tgt_size, pad_src_size)"
2.1.1,"align_idx should be a Tensor in size([N, 3]), N is total number"
2.1.1,"of align src-tgt pair in current batch, each as"
2.1.1,"['sent_NÂ°_in_batch', 'tgt_id+1', 'src_id'] (check AlignField)"
2.1.1,NOTE: tgt-src ref alignement that in range_ of shard
2.1.1,(coherent with batch.tgt)
2.1.1,"align_head contains value in [0, 1) presenting attn prob,"
2.1.1,0 was resulted by the context attention src_pad_mask
2.1.1,"So, the correspand position in ref_align should also be 0"
2.1.1,"Therefore, clip align_head to > 1e-18 should be bias free."
2.1.1,non_none: the subdict of the state dictionary where the values
2.1.1,are not None.
2.1.1,"Now, the iteration:"
2.1.1,state is a dictionary of sequences of tensor-like but we
2.1.1,want a sequence of dictionaries of tensors.
2.1.1,"First, unzip the dictionary into a sequence of keys and a"
2.1.1,sequence of tensor-like sequences.
2.1.1,"Now, yield a dictionary for each shard. The keys are always"
2.1.1,the same. values is a sequence of length #keys where each
2.1.1,element is a sequence of length #shards. We want to iterate
2.1.1,"over the shards, not over the keys: therefore, the values need"
2.1.1,to be re-zipped by shard and then each shard can be paired
2.1.1,with the keys.
2.1.1,Assumed backprop'd
2.1.1,Check Transforms
2.1.1,Check path
2.1.1,tgt is src for LM task
2.1.1,Check prefix: will be used when use prefix transform
2.1.1,Check weight
2.1.1,validation when train:
2.1.1,Check embeddings stuff
2.1.1,"Backward compatibility with ""fix_word_vecs_*"" opts"
2.1.1,encoder and decoder should be same sizes
2.1.1,"Load default opt values, then overwrite with the opts in"
2.1.1,"the checkpoint. That way, if there are new options added,"
2.1.1,the defaults are used.
2.1.1,Don't do anything
2.1.1,Update best score of each criteria
2.1.1,Reset tolerance
2.1.1,Update current status
2.1.1,Decrease tolerance
2.1.1,Log
2.1.1,Log
2.1.1,Get a list of world_size lists with len(stat_list) Statistics objects
2.1.1,SRU doesn't support PackedSequence.
2.1.1,-*- coding: utf-8 -*-
2.1.1,threshold on 1 to avoid div by 0
2.1.1,treat alignment matrix one by one as each have different lengths
2.1.1,No alignment if not exist valid tgt token
2.1.1,get valid alignment (sub-matrix from full paded aligment matrix)
2.1.1,-*- coding: utf-8 -*-
2.1.1,this one is needed for torchtext random call (shuffled iterator)
2.1.1,in multi gpu it ensures datasets are read in the same order
2.1.1,some cudnn methods can be random even after fixing the seed
2.1.1,unless you tell it to be deterministic
2.1.1,This one is needed for various tranfroms
2.1.1,These ensure same initialization in multi gpu mode
2.1.1,Shift values to be >= 0
2.1.1,we need to check the model path + any tokenizer path
2.1.1,fast-forward if loaded from state
2.1.1,NOTE: `rnn.pack_padded_sequence` requires that a
2.1.1,"minibatch be sorted by decreasing order, which"
2.1.1,requires reversing relative to typical sort keys
2.1.1,Maintains the longest src and tgt length in the current batch
2.1.1,Reset current longest length at a new batch (count=1)
2.1.1,Src: [<bos> w1 ... wN <eos>]
2.1.1,Tgt: [w1 ... wM <eos>]
2.1.1,coding: utf-8
2.1.1,make a small vocab containing just the tokens in the source sequence
2.1.1,add init_token and eos_token according to src construction
2.1.1,Map source tokens to indices in the dynamic dict.
2.1.1,self.src_vocabs is used in collapse_copy_scores and Translator.py
2.1.1,this assumes src_field and tgt_field are both text
2.1.1,fields needs to have only keys that examples have as attrs
2.1.1,avoid infinite recursion when fields isn't defined
2.1.1,this is a hack: appears quicker to apply it here
2.1.1,than in the ParallelCorpusIterator
2.1.1,NOTE: moved to DatasetAdapter._process method in iterator.py
2.1.1,item = self.transform.apply(
2.1.1,"example, is_train=self.infinitely, corpus_name=self.cid)"
2.1.1,empty example: skip
2.1.1,-*- coding: utf-8 -*-
2.1.1,backwards compatibility
2.1.1,monkey-patch to make torchtext Vocab's pickleable
2.1.1,"+1 for tgt side to keep coherent after ""bos"" padding,"
2.1.1,"register ['NÂ°_in_batch', 'tgt_id+1', 'src_id']"
2.1.1,this is basically copy-pasted from torchtext.
2.1.1,counters changes in place
2.1.1,keep the order of tokens specified in the vocab file by
2.1.1,adding them to the counter with decreasing counting values
2.1.1,`tgt_vocab_size` is ignored when sharing vocabularies
2.1.1,return vocab to dump with standard name
2.1.1,empty train_dataset_files so that vocab is only loaded from
2.1.1,"given paths in src_vocab_path, tgt_vocab_path"
2.1.1,Load vocabulary
2.1.1,Drop the none-using from memory but keep the last
2.1.1,"in the long run, shouldn't it be possible to do this by calling"
2.1.1,build_vocab with both the src and tgt data?
2.1.1,coding: utf-8
2.1.1,several data readers need optional dependencies. There's no
2.1.1,appropriate builtin exception
2.1.1,NOTE: not support nfeats > 0 yet
2.1.1,-*- coding: utf-8 -*-
2.1.1,mix this with partial
2.1.1,batch (list(list(list))): batch_size x len(self.fields) x seq_len
2.1.1,lengths: batch_size
2.1.1,data: seq_len x batch_size x len(self.fields)
2.1.1,flake8: noqa
2.1.1,For command-line option parsing
2.1.1,"Check pass, set the args."
2.1.1,"This SRU version implements its own cuda-level optimization,"
2.1.1,so it requires that:
2.1.1,1. `cupy` and `pynvrtc` python package installed.
2.1.1,2. pytorch is built with cuda support.
2.1.1,3. library path set: export LD_LIBRARY_PATH=<cuda lib path>.
2.1.1,Check 1.
2.1.1,Check 2.
2.1.1,Check 3.
2.1.1,This sets up device to use.
2.1.1,-> directions x batch x dim
2.1.1,For DEBUG
2.1.1,"size = (length, batch, x.size(-1)) \"
2.1.1,"if x.dim() == 3 else (batch, x.size(-1))"
2.1.1,grad_x = x.new(*x.size()) if k_ == 3 else x.new(*size).zero_()
2.1.1,Normal use
2.1.1,"An entry check here, will catch on train side and translate side"
2.1.1,if requirements are not satisfied.
2.1.1,RNNDecoderState wraps hidden as a tuple.
2.1.1,fh -> (layers*directions) x batch x dim
2.1.1,"No encoder in LM, seq2seq count formatting kept"
2.1.1,_check_save_model_path
2.1.1,NOTE: We need to trim the vocab to remove any unk tokens that
2.1.1,were not originally here.
2.1.1,!/usr/bin/env python
2.1.1,!/usr/bin/env python
2.1.1,!/usr/bin/env python
2.1.1,-*- coding: utf-8 -*-
2.1.1,!/usr/bin/env python
2.1.1,!/usr/bin/env python
2.1.1,!/usr/bin/env python
2.1.1,import onmt.opts as opts
2.1.1,Set sharing strategy manually instead of default based on the OS.
2.1.1,"maybe prepare pretrained embeddings, if any"
2.1.1,Load checkpoint if we resume from a previous training.
2.1.1,Report src and tgt vocab sizes
2.1.1,Create a thread to listen for errors in the child processes.
2.1.1,Train with multiprocessing.
2.1.1,"This does not work if we merge with the first loop, not sure why"
2.1.1,Get the iterator to generate from
2.1.1,"Once training is done, we can terminate the producers"
2.1.1,magic indices
2.1.1,result caching
2.1.1,fix length constraint and remove eos from count
2.1.1,add one to account for BOS. Don't account for EOS because hitting
2.1.1,this implies it hasn't been found.
2.1.1,we don't block nothing if the user doesn't want it
2.1.1,we can't block nothing beam's too short
2.1.1,we check paths one by one
2.1.1,we don't forbid nothing if the user doesn't want it
2.1.1,we can't forbid nothing if beam's too short
2.1.1,Reordering forbidden_tokens following beam selection
2.1.1,We rebuild a dict to ensure we get the value and not the pointer
2.1.1,Grabing the newly selected tokens and associated ngram
2.1.1,skip the blocking if any token in current_ngram is excluded
2.1.1,"pickups: Tensor where specified index were set to 1, others 0"
2.1.1,"dropdowns: opposite of pickups, 1 for those shouldn't pick"
2.1.1,Minus dropdowns to log_probs making probabilities of
2.1.1,unspecified index close to 0
2.1.1,"prediction step have surpass length of given target_prefix,"
2.1.1,no need to further change this attr
2.1.1,keep indices until overflowing p
2.1.1,Set all logits that are not in the top-p to -10000.
2.1.1,This puts the probabilities close to 0.
2.1.1,Set all logits that are not in the top-k to -10000.
2.1.1,This puts the probabilities close to 0.
2.1.1,"For temp=0.0, take the argmax to avoid divide-by-zero errors."
2.1.1,keep_topk=1 is also equivalent to argmax.
2.1.1,maybe fix some prediction at this step by modifying log_probs
2.1.1,"shape: (sum(~ self.is_finished), 1)"
2.1.1,in LM task memory_lengths is associated with currently generated src
2.1.1,and therefore needs to follow the generation
2.1.1,!/usr/bin/env python
2.1.1,Maintains the longest src and tgt length in the current batch
2.1.1,Reset current longest length at a new batch (count=1)
2.1.1,max_tgt_in_batch = 0
2.1.1,Src: [<bos> w1 ... wN <eos>]
2.1.1,Tgt: [w1 ... wM <eos>]
2.1.1,for debugging
2.1.1,TODO: maybe add dynamic part
2.1.1,Statistics
2.1.1,Turn any copied words into UNKs.
2.1.1,"Decoder forward, takes [tgt_len, batch, nfeats] as input"
2.1.1,"and [src_len, batch, hidden] as memory_bank"
2.1.1,"in case of inference tgt_len = 1, batch = beam times batch_size"
2.1.1,"in case of Gold Scoring tgt_len = actual length, batch = 1 batch"
2.1.1,Generator forward.
2.1.1,"returns [(batch_size x beam_size) , vocab ] when 1 step"
2.1.1,"or [ tgt_len, batch_size, vocab ] when full sentence"
2.1.1,"here we have scores [tgt_lenxbatch, vocab] or [beamxbatch, vocab]"
2.1.1,"returns [(batch_size x beam_size) , vocab ] when 1 step"
2.1.1,"or [ tgt_len, batch_size, vocab ] when full sentence"
2.1.1,(0) add BOS and padding to tgt prediction
2.1.1,(1) Encoder forward.
2.1.1,(2) Repeat src objects `n_best` times.
2.1.1,"We use batch_size x n_best, get ``(src_len, batch * n_best, nfeat)``"
2.1.1,"(3) Init decoder with n_best src,"
2.1.1,"reshape tgt to ``(len, batch * n_best, nfeat)``"
2.1.1,masked_select
2.1.1,get aligned src id for each prediction's valid tgt tokens
2.1.1,TODO: support these blacklisted features
2.1.1,(0) Prep the components of the search.
2.1.1,(1) Run the encoder on the src.
2.1.1,(2) prep decode_strategy. Possibly repeat src objects.
2.1.1,(3) Begin decoding step by step:
2.1.1,Reorder states.
2.1.1,TODO: support these blacklisted features
2.1.1,hack [min_len_batch-1:] because expect <bos>
2.1.1,(0) Prep the components of the search.
2.1.1,(1) split src into src and target_prefix to avoid padding.
2.1.1,(2) init decoder
2.1.1,(3) prep decode_strategy. Possibly repeat src objects.
2.1.1,(4) Begin decoding step by step:
2.1.1,Reorder states.
2.1.1,select indexes in model state/cache
2.1.1,beam parameters
2.1.1,beam state
2.1.1,BoolTensor was introduced in pytorch 1.2
2.1.1,"""global state"" of the old beam"
2.1.1,buffers for the topk scores and 'backpointer'
2.1.1,for testing
2.1.1,maybe fix some prediction at this step by modifying log_probs
2.1.1,Flatten probs into a list of possibilities.
2.1.1,Penalize beams that finished.
2.1.1,"on real data (newstest2017) with the pretrained transformer,"
2.1.1,it's faster to not move this back to the original device
2.1.1,Store finished hypotheses for this batch.
2.1.1,End condition is the top beam finished and we can return
2.1.1,n_best hypotheses.
2.1.1,"If all sentences are translated, no need to go further."
2.1.1,Remove finished batches for the next step.
2.1.1,using integer division to get an integer _B without casting
2.1.1,force the output to be longer than self.min_length
2.1.1,Multiply probs by the beam probability.
2.1.1,"if the sequence ends now, then the penalty is the current"
2.1.1,"length + 1, to include the EOS token"
2.1.1,Avoid any direction that would repeat unwanted ngrams
2.1.1,Pick up candidate token by curr_scores
2.1.1,Recover log probs.
2.1.1,Length penalty is just a scalar. It doesn't matter if it's applied
2.1.1,before or after the topk.
2.1.1,Resolve beam origin and map to batch index flat representation.
2.1.1,Append last prediction.
2.1.1,update global state (step == 1)
2.1.1,update global state (step > 1)
2.1.1,"shape: (batch_size x beam_size, 1)"
2.1.1,in LM task memory_lengths is associated with currently generated src
2.1.1,and therefore needs to follow the generation
2.1.1,in LM task memory_lengths is associated with currently generated src
2.1.1,and therefore needs to follow the generation
2.1.1,Term will be subtracted from probability
2.1.1,Probability will be divided by this
2.1.1,these warnings indicate that either the alpha/beta
2.1.1,"forces a penalty to be a no-op, or a penalty is a no-op but"
2.1.1,the alpha/beta would suggest otherwise.
2.1.1,using some length penalty
2.1.1,using some coverage penalty
2.1.1,!/usr/bin/env python
2.1.1,semaphore doesn't have a timeout arg in Python 2.7
2.1.1,perform a first request to initialize everything
2.1.1,backwards compatibility for confs
2.1.1,every segment becomes a dict for flexibility purposes
2.1.1,NOTE: translator returns lists of `n_best` list
2.1.1,build back results with empty texts
2.1.1,load can be called multiple times: modify copy
2.1.1,output contain alignment
2.1.1,Below are all the different penalty terms implemented so far.
2.1.1,Subtract coverage penalty from topk log probs.
2.1.1,Divide topk log probs by length penalty.
2.1.1,Sorting
2.1.1,Chinese segmentation
2.1.1,Chinese simplify -> Chinese traditional standard
2.1.1,Chinese simplify -> Chinese traditional (HongKong)
2.1.1,Chinese simplify -> Chinese traditional (Taiwan)
2.1.1,Chinese traditional -> Chinese simplify (v1)
2.1.1,Chinese traditional -> Chinese simplify (v2)
2.1.0,!/usr/bin/env python
2.1.0,!/usr/bin/env python
2.1.0,!/usr/bin/env python
2.1.0,!/usr/bin/env python
2.1.0,!/usr/bin/env python
2.1.0,!/usr/bin/env python3
2.1.0,-*- coding: utf-8 -*-
2.1.0,
2.1.0,"OpenNMT-py documentation build configuration file, created by"
2.1.0,sphinx-quickstart on Sun Dec 17 12:07:14 2017.
2.1.0,
2.1.0,This file is execfile()d with the current directory set to its
2.1.0,containing dir.
2.1.0,
2.1.0,Note that not all possible configuration values are present in this
2.1.0,autogenerated file.
2.1.0,
2.1.0,All configuration values have a default; values that are commented out
2.1.0,serve to show the default.
2.1.0,"If extensions (or modules to document with autodoc) are in another directory,"
2.1.0,add these directories to sys.path here. If the directory is relative to the
2.1.0,"documentation root, use os.path.abspath to make it absolute, like shown here."
2.1.0,
2.1.0,import os
2.1.0,import sys
2.1.0,"sys.path.insert(0, os.path.abspath('.'))"
2.1.0,-- General configuration ------------------------------------------------
2.1.0,"If your documentation needs a minimal Sphinx version, state it here."
2.1.0,
2.1.0,needs_sphinx = '1.0'
2.1.0,"Add any Sphinx extension module names here, as strings. They can be"
2.1.0,extensions coming with Sphinx (named 'sphinx.ext.*') or your custom
2.1.0,ones.
2.1.0,Show base classes
2.1.0,"Use ""variables"" section for Attributes instead of weird block things"
2.1.0,mimicking the function style.
2.1.0,"Add any paths that contain templates here, relative to this directory."
2.1.0,The suffix(es) of source filenames.
2.1.0,You can specify multiple suffix as a list of string:
2.1.0,
2.1.0,"source_suffix = ['.rst', '.md']"
2.1.0,The master toctree document.
2.1.0,General information about the project.
2.1.0,"The version info for the project you're documenting, acts as replacement for"
2.1.0,"|version| and |release|, also used in various other places throughout the"
2.1.0,built documents.
2.1.0,
2.1.0,The short X.Y version.
2.1.0,"The full version, including alpha/beta/rc tags."
2.1.0,The language for content autogenerated by Sphinx. Refer to documentation
2.1.0,for a list of supported languages.
2.1.0,
2.1.0,This is also used if you do content translation via gettext catalogs.
2.1.0,"Usually you set ""language"" from the command line for these cases."
2.1.0,"List of patterns, relative to source directory, that match files and"
2.1.0,directories to ignore when looking for source files.
2.1.0,This patterns also effect to html_static_path and html_extra_path
2.1.0,The name of the Pygments (syntax highlighting) style to use.
2.1.0,"If true, `todo` and `todoList` produce output, else they produce nothing."
2.1.0,-- Options for HTML output ----------------------------------------------
2.1.0,The theme to use for HTML and HTML Help pages.  See the documentation for
2.1.0,a list of builtin themes.
2.1.0,
2.1.0,html_theme = 'sphinx_materialdesign_theme'
2.1.0,html_theme_path = [sphinx_materialdesign_theme.get_path()]
2.1.0,Theme options are theme-specific and customize the look and feel of a theme
2.1.0,"further.  For a list of options available for each theme, see the"
2.1.0,documentation.
2.1.0,
2.1.0,html_theme_options = {}
2.1.0,"Add any paths that contain custom static files (such as style sheets) here,"
2.1.0,"relative to this directory. They are copied after the builtin static files,"
2.1.0,"so a file named ""default.css"" will overwrite the builtin ""default.css""."
2.1.0,"Custom sidebar templates, must be a dictionary that maps document names"
2.1.0,to template names.
2.1.0,
2.1.0,This is required for the alabaster theme
2.1.0,refs: http://alabaster.readthedocs.io/en/latest/installation.html#sidebars
2.1.0,-- Options for HTMLHelp output ------------------------------------------
2.1.0,Output file base name for HTML help builder.
2.1.0,-- Options for LaTeX output ---------------------------------------------
2.1.0,The paper size ('letterpaper' or 'a4paper').
2.1.0,
2.1.0,"'papersize': 'letterpaper',"
2.1.0,"The font size ('10pt', '11pt' or '12pt')."
2.1.0,
2.1.0,"'pointsize': '10pt',"
2.1.0,Additional stuff for the LaTeX preamble.
2.1.0,
2.1.0,"'preamble': '',"
2.1.0,Latex figure (float) alignment
2.1.0,
2.1.0,"'figure_align': 'htbp',"
2.1.0,Grouping the document tree into LaTeX files. List of tuples
2.1.0,"(source start file, target name, title,"
2.1.0,"author, documentclass [howto, manual, or own class])."
2.1.0,-- Options for manual page output ---------------------------------------
2.1.0,One entry per manual page. List of tuples
2.1.0,"(source start file, name, description, authors, manual section)."
2.1.0,-- Options for Texinfo output -------------------------------------------
2.1.0,Grouping the document tree into Texinfo files. List of tuples
2.1.0,"(source start file, target name, title, author,"
2.1.0,"dir menu entry, description, category)"
2.1.0,!/usr/bin/env python
2.1.0,-*- coding: utf-8 -*-
2.1.0,is this reachable?
2.1.0,Read in embeddings
2.1.0,Write to file
2.1.0,converts a SentencePiece vocabulary to the format expected by dynamic data
2.1.0,"(essentially converts float expected counts to ""fixed precision"" int pseudo"
2.1.0,counts)
2.1.0,"Add in default model arguments, possibly added since training."
2.1.0,build_base_model expects updated and validated opts
2.1.0,-*- encoding: utf-8 -*-
2.1.0,!/usr/bin/env python
2.1.0,-*- coding: utf-8 -*-
2.1.0,Author: Rico Sennrich
2.1.0,flake8: noqa
2.1.0,This file is retrieved from https://github.com/rsennrich/subword-nmt
2.1.0,hack for python2/3 compatibility
2.1.0,check version information
2.1.0,some hacking to deal with duplicates (only consider first instance)
2.1.0,don't print end-of-word symbols
2.1.0,sys.stderr.write('cannot split {0} further.\n'.format(segment))
2.1.0,sys.stderr.write('OOV: {0}\n'.format(segment))
2.1.0,sys.stderr.write('OOV: {0}\n'.format(segment))
2.1.0,python 2/3 compatibility
2.1.0,read/write files as UTF-8
2.1.0,!/usr/bin/env python
2.1.0,!/usr/bin/env python
2.1.0,-*- coding: utf-8 -*-
2.1.0,Author: Rico Sennrich
2.1.0,flake8: noqa
2.1.0,This file is retrieved from https://github.com/rsennrich/subword-nmt
2.1.0,hack for python2/3 compatibility
2.1.0,"find all instances of pair, and update frequency/indices around it"
2.1.0,find first symbol
2.1.0,"if first symbol is followed by second symbol, we've found an occurrence of pair (old_word[i:i+2])"
2.1.0,"assuming a symbol sequence ""A B C"", if ""B C"" is merged, reduce the frequency of ""A B"""
2.1.0,"assuming a symbol sequence ""A B C B"", if ""B C"" is merged, reduce the frequency of ""C B""."
2.1.0,"however, skip this if the sequence is A B C B C, because the frequency of ""C B"" will be reduced by the previous code block"
2.1.0,find new pair
2.1.0,"assuming a symbol sequence ""A BC D"", if ""B C"" is merged, increase the frequency of ""A BC"""
2.1.0,"assuming a symbol sequence ""A BC B"", if ""B C"" is merged, increase the frequency of ""BC B"""
2.1.0,"however, if the sequence is A BC BC, skip this step because the count of ""BC BC"" will be incremented by the previous code block"
2.1.0,data structure of pair frequencies
2.1.0,index from pairs to words
2.1.0,version 0.2 changes the handling of the end-of-word token ('</w>');
2.1.0,version numbering allows bckward compatibility
2.1.0,"threshold is inspired by Zipfian assumption, but should only affect speed"
2.1.0,we probably missed the best pair because of pruning; go back to full statistics
2.1.0,"threshold is inspired by Zipfian assumption, but should only affect speed"
2.1.0,python 2/3 compatibility
2.1.0,read/write files as UTF-8
2.1.0,!/usr/bin/env python
2.1.0,-*- coding: utf-8 -*-
2.1.0,!/usr/bin/env python
2.1.0,Avoid functionality on inference
2.1.0,Build embeddings.
2.1.0,Build encoder.
2.1.0,Build embeddings.
2.1.0,Build decoder.
2.1.0,Share the embedding matrix - preprocess with share_vocab required.
2.1.0,src/tgt vocab should be the same if `-share_vocab` is specified.
2.1.0,Update vocabulary embeddings with checkpoint embeddings
2.1.0,Embedding layers
2.1.0,Just for debugging purposes
2.1.0,for back compat when attention_dropout was not defined
2.1.0,Build Model
2.1.0,Build Generator.
2.1.0,Load the model states from checkpoint or initialize them.
2.1.0,This preserves backward-compat for models using customed layernorm
2.1.0,end of patch for backward compatibility
2.1.0,Update model embeddings with those from the checkpoint after initialization
2.1.0,Remove old vocabulary associated embeddings
2.1.0,Embedding layers
2.1.0,!/usr/bin/env python
2.1.0,ensure tensorboard output is written in the directory
2.1.0,of previous checkpoints
2.1.0,Override checkpoint's update_embeddings as it defaults to false
2.1.0,NOTE: It's important that ``opt`` has been validated and updated
2.1.0,at this point.
2.1.0,Build model.
2.1.0,Build optimizer.
2.1.0,Build model saver
2.1.0,Move batch to specified device
2.1.0,Use Tensorboard for visualization during training
2.1.0,Options only during inference
2.1.0,"Truncation options, for text corpus"
2.1.0,"as for False, this will be added in _add_train_general_opts"
2.1.0,Embedding Options
2.1.0,Model Task Options
2.1.0,Encoder-Decoder Options
2.1.0,"group.add('--residual', '-residual',   action=""store_true"","
2.1.0,"help=""Add residual connections between RNN layers."")"
2.1.0,The following options (bridge_extra_node to n_steps) are used
2.1.0,for training with --encoder_type ggnn (Gated Graph Neural Network).
2.1.0,Attention options
2.1.0,Alignement options
2.1.0,Generator and loss options.
2.1.0,GPU
2.1.0,Init options
2.1.0,Pretrained word vectors
2.1.0,Freeze word vectors
2.1.0,Optimization options
2.1.0,learning rate
2.1.0,options relate to data preprare
2.1.0,options relate to train
2.1.0,Alpha and Beta values for Google Length + Coverage penalty
2.1.0,"Described here: https://arxiv.org/pdf/1609.08144.pdf, Section 7"
2.1.0,Length penalty options
2.1.0,Coverage penalty options
2.1.0,Decoding Length constraint
2.1.0,Decoding content constraint
2.1.0,Adding options relate to decoding strategy
2.1.0,Adding option for logging
2.1.0,Copyright 2016 The Chromium Authors. All rights reserved.
2.1.0,Use of this source code is governed by a BSD-style license that can be
2.1.0,found in the LICENSE file.
2.1.0,"Get the key 'value' in the dict, or just use 'value'"
2.1.0,Basic attributes.
2.1.0,Set model in training mode.
2.1.0,UPDATE DROPOUT
2.1.0,Run patience mechanism
2.1.0,"If the patience has reached the limit, stop training"
2.1.0,swap model params w/ moving average
2.1.0,(and keep the original parameters)
2.1.0,Set model in validating mode.
2.1.0,F-prop through the model.
2.1.0,Compute loss.
2.1.0,Update statistics.
2.1.0,Set model back to training mode.
2.1.0,Truncated BPTT: reminder not compatible with accum > 1
2.1.0,1. Create truncated target.
2.1.0,2. F-prop all but generator.
2.1.0,3. Compute loss.
2.1.0,4. Update the parameters and statistics.
2.1.0,Multi GPU gradient gather
2.1.0,"If truncated, don't backprop fully."
2.1.0,TO CHECK
2.1.0,if dec_state is not None:
2.1.0,dec_state.detach()
2.1.0,"in case of multi step gradient accumulation,"
2.1.0,update only after accum batches
2.1.0,For Flake
2.1.0,we avoid padding while mean pooling
2.1.0,incoming and outgoing edge embedding
2.1.0,Find vocab data for tree builting
2.1.0,Propogation Model
2.1.0,Initialize the bridge layer
2.1.0,Token embedding
2.1.0,Initialize graph using formatted input sequence
2.1.0,Number of flagged nodes defines node count for this sample
2.1.0,"(Nodes can have no flags on them, but must be in 'flags' list)."
2.1.0,The total number of integers in the vocab should allow
2.1.0,for all features and edges to be defined.
2.1.0,Use first extra node as only source for decoder init
2.1.0,Average all nodes to get bridge input
2.1.0,"LSTM has hidden and cell state, other only one"
2.1.0,Total number of states
2.1.0,Build a linear layer for each
2.1.0,Initialize the bridge layer
2.1.0,"s_len, batch, emb_dim = emb.size()"
2.1.0,Lengths data is wrapped inside a Tensor.
2.1.0,"LSTM has hidden and cell state, other only one"
2.1.0,Total number of states
2.1.0,Build a linear layer for each
2.1.0,"s_len, batch, emb_dim = emb.size()"
2.1.0,Run the forward pass of every layer of the tranformer.
2.1.0,Dimensions and padding for constructing the word embedding matrix
2.1.0,Dimensions and padding for feature embedding matrices
2.1.0,(these have no effect if feat_vocab_sizes is empty)
2.1.0,The embedding matrix look-up tables. The first look-up table
2.1.0,"is for words. Subsequent ones are for features, if any exist."
2.1.0,The final output size of word + feature vectors. This can vary
2.1.0,from the word vector size if and only if features are defined.
2.1.0,This is the attribute you should access if you need to know
2.1.0,how big your embeddings are going to be.
2.1.0,The sequence of operations that converts the input sequence
2.1.0,into a sequence of embeddings. At minimum this consists of
2.1.0,looking up the embeddings for each word and feature in the
2.1.0,input. Model parameters may require the sequence to contain
2.1.0,additional operations as well.
2.1.0,features must use word_vec_size
2.1.0,features will use feat_vec_size
2.1.0,Some utilitary functions for pretrained embeddings
2.1.0,is this reachable?
2.1.0,Write to file
2.1.0,set the opt in place
2.1.0,set the opt in place
2.1.0,This class is mainly used by decoder.py for RNNs but also
2.1.0,by the CNN / transformer decoder when copy attention is used
2.1.0,CNN has its own attention mechanism ConvMultiStepAttention
2.1.0,Transformer has its own MultiHeadedAttention
2.1.0,mlp wants it with bias
2.1.0,Check input sizes
2.1.0,"(batch, t_len, d) x (batch, d, s_len) --> (batch, t_len, s_len)"
2.1.0,"(batch, t_len, s_len, d)"
2.1.0,one step input
2.1.0,"compute attention scores, as in Luong et al."
2.1.0,Softmax or sparsemax to normalize attention weights
2.1.0,each context vector c_t is the weighted average
2.1.0,over all the source hidden states
2.1.0,concatenate
2.1.0,Check output sizes
2.1.0,Check output sizes
2.1.0,clamping necessary because of numerical errors: loss should be lower
2.1.0,"bounded by zero, but negative values near zero are possible without"
2.1.0,the clamp
2.1.0,from onmt.utils.misc import aeq
2.1.0,CHECKS
2.1.0,"batch, k_len, d = key.size()"
2.1.0,"batch_, k_len_, d_ = value.size()"
2.1.0,"aeq(batch, batch_)"
2.1.0,"aeq(k_len, k_len_)"
2.1.0,"aeq(d, d_)"
2.1.0,"batch_, q_len, d_ = query.size()"
2.1.0,"aeq(batch, batch_)"
2.1.0,"aeq(d, d_)"
2.1.0,"aeq(self.model_dim % 8, 0)"
2.1.0,if mask is not None:
2.1.0,"batch_, q_len_, k_len_ = mask.size()"
2.1.0,"aeq(batch_, batch)"
2.1.0,"aeq(k_len_, k_len)"
2.1.0,aeq(q_len_ == q_len)
2.1.0,END CHECKS
2.1.0,"1) Project key, value, and query."
2.1.0,1 or key_len x key_len
2.1.0,1 or key_len x key_len x dim_per_head
2.1.0,1 or key_len x key_len x dim_per_head
2.1.0,2) Calculate and scale scores.
2.1.0,batch x num_heads x query_len x key_len
2.1.0,3) Apply attention dropout and compute context vectors.
2.1.0,CHECK
2.1.0,"batch_, q_len_, d_ = output.size()"
2.1.0,"aeq(q_len, q_len_)"
2.1.0,"aeq(batch, batch_)"
2.1.0,"aeq(d, d_)"
2.1.0,Return multi-head attn
2.1.0,At the moment this class is only used by embeddings.Embeddings look-up tables
2.1.0,-*- coding: utf-8 -*-
2.1.0,checks
2.1.0,"batch, channel, height, width = base_target_emb.size()"
2.1.0,"batch_, channel_, height_, width_ = input_from_dec.size()"
2.1.0,"enc_batch, enc_channel, enc_height = encoder_out_top.size()"
2.1.0,"enc_batch_, enc_channel_, enc_height_ = encoder_out_combine.size()"
2.1.0,out_features * in_features
2.1.0,norm is out_features * 1
2.1.0,batch_size * out_features
2.1.0,out_features
2.1.0,out_features
2.1.0,batch_size * out_features
2.1.0,"out_channels, in_channels // groups, * kernel_size"
2.1.0,out_features
2.1.0,This is used nowhere in the code at the moment (Vincent Nguyen 05/18/2018)
2.1.0,"in_channels, out_channels, *kernel_size"
2.1.0,"in_channels, out_channels, *kernel_size"
2.1.0,"self.out_channels, 1"
2.1.0,out_features
2.1.0,out_features
2.1.0,store roots on diagonal
2.1.0,CHECKS
2.1.0,Original probabilities.
2.1.0,Probability of copying p(z=1) batch.
2.1.0,Probability of not copying: p_{word}(w) * (1 - p(z))
2.1.0,probabilities assigned by the model to the gold targets
2.1.0,probability of tokens copied from source
2.1.0,Set scores for unk to 0 and add eps
2.1.0,find the indices in which you do not use the copy mechanism
2.1.0,Drop padding.
2.1.0,this block does not depend on the loss value computed above
2.1.0,and is used only for stats
2.1.0,this block does not depend on the loss value computed above
2.1.0,and is used only for stats
2.1.0,Correct target copy token instead of <unk>
2.1.0,tgt[i] = align[i] + len(tgt_vocab)
2.1.0,for i such that tgt[i] == 0 and align[i] != 0
2.1.0,Compute sum of perplexities for stats
2.1.0,this part looks like it belongs in CopyGeneratorLoss
2.1.0,Compute Loss as NLL divided by seq length
2.1.0,Compute Total Loss per sequence in batch
2.1.0,Divide by length of each sequence and sum
2.1.0,Auto import python files in this directory
2.1.0,1. sample number of tokens to corrupt
2.1.0,2. sample positions to corrput
2.1.0,3. sample corrupted values
2.1.0,1. sample number of tokens to corrupt
2.1.0,2. sample positions to corrput
2.1.0,3. Drop token on chosen position
2.1.0,1. sample number of tokens to corrupt
2.1.0,2. sample positions to corrput
2.1.0,3. mask word on chosen position
2.1.0,"Sharing options among `TokenizerTransform`s, same name conflict in"
2.1.0,this scope will be resolved by remove previous occurrence in parser
2.1.0,subword regularization(or BPE dropout) options:
2.1.0,subword vocabulary restriction options:
2.1.0,derterministic subwording
2.1.0,subword sampling when nbest_size > 1 or -1
2.1.0,alpha should be 0.0 < alpha < 1.0
2.1.0,-1: keep everything (i.e. 1 mask per token)
2.1.0,0: replace everything (i.e. no mask)
2.1.0,1: 1 mask per span
2.1.0,view each subword as word start / input is word level token
2.1.0,Pretend it ends with a full stop so last span is a sentence
2.1.0,"Tokens that are full stops, where the previous token is not"
2.1.0,Make sure we have enough to mask
2.1.0,Trim to masking budget
2.1.0,Handle 0-length mask (inserts) separately
2.1.0,assert is_word_start[-1] == 0
2.1.0,assert tokens_length - 1 not in indices
2.1.0,"keep index, but replace it with [MASK]"
2.1.0,"acts as a long length, so spans don't go over the end of doc"
2.1.0,next position from each word_start
2.1.0,delete token: 1 mask/remove per span
2.1.0,"keep index, but replace it with [MASK]: 1 mask per token"
2.1.0,A bit faster when all lengths are 1
2.1.0,to cover whole token
2.1.0,delete token
2.1.0,"keep index, but replace it with [MASK]"
2.1.0,assert tokens_length - 1 not in indices
2.1.0,initialize fields at the top of each unit test to prevent
2.1.0,any undesired stateful effects
2.1.0,"this test touches the file system, so it could be considered an"
2.1.0,integration test
2.1.0,write utf-8 bytes
2.1.0,batch 0 will always predict EOS. The other batches will predict
2.1.0,non-eos scores.
2.1.0,"""best"" prediction is eos - that should be blocked"
2.1.0,include at least one prediction OTHER than EOS
2.1.0,that is greater than -1e20
2.1.0,now batch 0 has ended and no others have
2.1.0,initial step
2.1.0,batch 0 dies on step 0
2.1.0,include at least one prediction OTHER than EOS
2.1.0,that is greater than -1e20
2.1.0,step 2
2.1.0,(old) batch 8 dies on step 1
2.1.0,step 3
2.1.0,everything dies
2.1.0,initial step
2.1.0,batch 0 dies on step 0
2.1.0,include at least one prediction OTHER than EOS
2.1.0,that is greater than -1e20
2.1.0,step 2
2.1.0,(old) batch 8 dies on step 1
2.1.0,step 3
2.1.0,everything dies
2.1.0,initial step
2.1.0,finish one beam
2.1.0,include at least one prediction OTHER than EOS
2.1.0,that is greater than -1e20
2.1.0,step 2
2.1.0,finish example in last batch
2.1.0,(old) batch 8 dies on step 1
2.1.0,step 3
2.1.0,everything dies
2.1.0,initial step
2.1.0,batch 0 dies on step 0
2.1.0,include at least one prediction OTHER than EOS
2.1.0,that is greater than -1e20
2.1.0,step 2
2.1.0,(old) batch 8 dies on step 1
2.1.0,step 3
2.1.0,everything dies
2.1.0,illegal_weights_mask = torch.ByteTensor([
2.1.0,"[0, 0, 0, 0, 0, 0, 0],"
2.1.0,"[0, 0, 0, 1, 1, 1, 1],"
2.1.0,"[0, 0, 0, 0, 0, 1, 1],"
2.1.0,"[0, 0, 1, 1, 1, 1, 1]])"
2.1.0,TODO: fix for pytorch 0.3
2.1.0,illegal_weights = alignments.masked_select(illegal_weights_mask)
2.1.0,"self.assertEqual(0.0, illegal_weights.data.sum())"
2.1.0,this could be considered an integration test because it touches
2.1.0,the filesystem for the config file (and the models)
2.1.0,no dummy prefix
2.1.0,no dummy prefix
2.1.0,!/usr/bin/env python
2.1.0,-*- coding: utf-8 -*-
2.1.0,Inject some dummy training options that may needed when build fields
2.1.0,Remove the generated *pt files.
2.1.0,Remove the generated data samples
2.1.0,all beams repeat (beam >= 1 repeat dummy scores)
2.1.0,predict repeat_idx over and over again
2.1.0,"before repeat, scores are either 0 or -inf"
2.1.0,"on repeat, `repeat_idx` score is BLOCKED_SCORE"
2.1.0,"(but it's still the best score, thus we have"
2.1.0,"[BLOCKED_SCORE, -inf, -inf, -inf, -inf]"
2.1.0,repetitions keeps maximizing score
2.1.0,"index 0 has been blocked, so repeating=>+0.0 score"
2.1.0,other indexes are -inf so repeating=>BLOCKED_SCORE
2.1.0,which is higher
2.1.0,beam 0 and beam >=2 will repeat (beam >= 2 repeat dummy scores)
2.1.0,non-interesting beams are going to get dummy values
2.1.0,"on initial round, only predicted scores for beam 0"
2.1.0,matter. Make two predictions. Top one will be repeated
2.1.0,"in beam zero, second one will live on in beam 1."
2.1.0,predict the same thing in beam 0
2.1.0,continue pushing around what beam 1 predicts
2.1.0,"now beam 0 dies (along with the others), beam 1 -> beam 0"
2.1.0,"now beam 0 dies (along with the others), beam 1 -> beam 0"
2.1.0,beam 0 and beam >= 2 will repeat (beam 2 repeats excluded idx)
2.1.0,non-interesting beams are going to get dummy values
2.1.0,predict the same thing in beam 0
2.1.0,continue pushing around what beam 1 predicts
2.1.0,predict the allowed-repeat again in beam 2
2.1.0,"now beam 0 dies, beam 1 -> beam 0, beam 2 -> beam 1"
2.1.0,and the rest die
2.1.0,"since all preds after i=0 are 0, we can check"
2.1.0,that the beam is the correct idx by checking that
2.1.0,the curr score is the initial score
2.1.0,beam 0 will always predict EOS. The other beams will predict
2.1.0,non-eos scores.
2.1.0,non-interesting beams are going to get dummy values
2.1.0,"""best"" prediction is eos - that should be blocked"
2.1.0,include at least beam_sz predictions OTHER than EOS
2.1.0,that are greater than -1e20
2.1.0,predict eos in beam 0
2.1.0,provide beam_sz other good predictions
2.1.0,now the top beam has ended and no others have
2.1.0,"not of interest, but want to make sure it keeps running"
2.1.0,since only beam 0 terminates and n_best = 2
2.1.0,"this is also a test that when block_ngram_repeat=0,"
2.1.0,repeating is acceptable
2.1.0,non-interesting beams are going to get dummy values
2.1.0,"""best"" prediction is eos - that should be blocked"
2.1.0,include at least beam_sz predictions OTHER than EOS
2.1.0,that are greater than -1e20
2.1.0,predict eos in beam 1
2.1.0,provide beam_sz other good predictions in other beams
2.1.0,provide beam_sz other good predictions in other beams
2.1.0,beam 1 dies on min_length
2.1.0,beam 0 dies on the step after beam 1 dies
2.1.0,"inp_lens is tiled in initialize, reassign to make attn match"
2.1.0,non-interesting beams are going to get dummy values
2.1.0,"""best"" prediction is eos - that should be blocked"
2.1.0,include at least beam_sz predictions OTHER than EOS
2.1.0,that are greater than -1e20
2.1.0,predict eos in beam 1
2.1.0,provide beam_sz other good predictions in other beams
2.1.0,provide beam_sz other good predictions in other beams
2.1.0,no top beams are finished yet
2.1.0,beam 1 dies on min_length
2.1.0,no top beams are finished yet
2.1.0,beam 0 dies on the step after beam 1 dies
2.1.0,top beam is finished now so there are attentions
2.1.0,two beams are finished in each batch
2.1.0,second dim is cut down to the non-padded src length
2.1.0,first dim is equal to the time of death
2.1.0,(beam 0 died at current step - adjust for SOS)
2.1.0,(beam 1 died at last step - adjust for SOS)
2.1.0,behavior gets weird when beam is already done so just stop
2.1.0,this is just test_beam.TestBeamAgainstReferenceCase repeated
2.1.0,in each batch.
2.1.0,"init_preds: [4, 3, 5, 6, 7] - no EOS's"
2.1.0,no EOS's yet
2.1.0,"[5, 3, 2, 6, 0], so beam 2 predicts EOS!"
2.1.0,assumes beam 2 finished on last step
2.1.0,ended beam 2 shouldn't continue
2.1.0,"[2, 5, 3, 6, 0] repeat self.BATCH_SZ, so beam 0 predicts EOS!"
2.1.0,"[-2.4879, -3.8910, -4.1010, -4.2010, -4.4010] repeat self.BATCH_SZ"
2.1.0,another beam is finished in all batches
2.1.0,new beam 0 finished
2.1.0,new beam 0 is old beam 3
2.1.0,assumes beam 0 finished on last step
2.1.0,"[5, 2, 6, 1, 0] repeat self.BATCH_SZ, so beam 1 predicts EOS!"
2.1.0,new beam 1 finished
2.1.0,new beam 1 is old beam 4
2.1.0,this could be considered an integration test because it tests
2.1.0,interactions between the GNMT scorer and the beam
2.1.0,"-data option is required, but not used in this test, so dummy."
2.1.0,len x batch x nfeat
2.1.0,Initialize vectors to compare size with
2.1.0,Ensure correct sizes and types
2.1.0,Make sure that output has the correct size and type
2.1.0,"[('encoder_type', 'transformer'),"
2.1.0,"('word_vec_size', 16), ('rnn_size', 16)],"
2.1.0,""""""" Only do SRU test if requirment is safisfied. """""""
2.1.0,SRU doesn't support input_feed.
2.1.0,first check there's nothing unexpectedly not trainable
2.1.0,ok: word embeddings shouldn't be trainable
2.1.0,if word vecs are freezed
2.1.0,ok: positional encodings shouldn't be trainable
2.1.0,then check nothing unexpectedly trainable
2.1.0,Decoder state
2.1.0,Build the RNN.
2.1.0,Set up the context gate.
2.1.0,Set up the standard attention.
2.1.0,The encoder hidden is  (layers*directions) x batch x dim.
2.1.0,We need to convert it to layers x batch x (directions*dim).
2.1.0,Init the input feed.
2.1.0,Update the state with the result.
2.1.0,Concatenates sequence of tensors along a new dimension.
2.1.0,NOTE: v0.3 to 0.4: dec_outs / attns[*] may not be list
2.1.0,(in particular in case of SRU) it was not raising error in 0.3
2.1.0,since stack(Variable) was allowed.
2.1.0,"In 0.4, SRU returns a tensor that shouldn't be stacke"
2.1.0,Check
2.1.0,Calculate the attention.
2.1.0,Calculate the context gate.
2.1.0,Additional args check.
2.1.0,END Additional args check.
2.1.0,Input feed concatenates hidden state with
2.1.0,input at every time step.
2.1.0,TODO: context gate should be employed
2.1.0,instead of second RNN transform.
2.1.0,Update the coverage attention.
2.1.0,Decoder State
2.1.0,CNNDecoder has its own attention mechanism.
2.1.0,Set up a separate copy attention layer if needed.
2.1.0,The output of CNNEncoder.
2.1.0,The combination of output of CNNEncoder and source embeddings.
2.1.0,Process the result and update the attentions.
2.1.0,Update the state.
2.1.0,TODO change the way attns is returned dict => list or tuple (onnx)
2.1.0,Memory_lengths is a single tensor shared between all models.
2.1.0,This assumption will not hold if Translator is modified
2.1.0,to calculate memory_lengths as something other than the length
2.1.0,of the input.
2.1.0,"return _, (B, Q_len, K_len)"
2.1.0,"layer average attention across heads, get ``(B, Q, K)``"
2.1.0,"Case 1: no full_context, no align heads -> layer avg baseline"
2.1.0,"Case 2: no full_context, 1 align heads -> guided align"
2.1.0,"Case 3: full_context, 1 align heads -> full cte guided align"
2.1.0,BoolTensor was introduced in pytorch 1.2
2.1.0,T: could be 1 in the case of stepwise decoding or tgt_len
2.1.0,masking is necessary when sequence length is greater than one
2.1.0,Decoder State
2.1.0,"previously, there was a GlobalAttention module here for copy"
2.1.0,"attention. But it was never actually used -- the ""copy"" attention"
2.1.0,just reuses the context attention.
2.1.0,"attns[""align""] = torch.stack(attn_aligns, 0).mean(0)  # All avg"
2.1.0,TODO change the way attns is returned dict => list or tuple (onnx)
2.1.0,T: could be 1 in the case of stepwise decoding or tgt_len
2.1.0,masking is necessary when sequence length is greater than one
2.1.0,TODO change the way attns is returned dict => list or tuple (onnx)
2.1.0,"buffer size in bytes, determine equiv. # of elements based on data type"
2.1.0,copy tensors into buffer_t
2.1.0,all-reduce and rescale
2.1.0,copy all-reduced buffer back into tensors
2.1.0,"tensor is bigger than buffer, all-reduce and rescale directly"
2.1.0,"buffer is full, all-reduce and replace buffer with grad"
2.1.0,add tensor to buffer
2.1.0,NOTE: stride (if needed) is handled at the
2.1.0,generator (train_iter) level
2.1.0,Move batch to correspond device_id when consumer iterate
2.1.0,hack to dodge unpicklable `dict_keys`
2.1.0,"propagate exception to parent process, keeping original traceback"
2.1.0,TODO: Find a better way to check for sparse gradients.
2.1.0,we use here a FusedAdam() copy of an old Apex repo
2.1.0,In this case use the old FusedAdam with FP16_optimizer wrapper
2.1.0,Load everything from the checkpoint.
2.1.0,Build everything from scratch.
2.1.0,"Reset optimizer, keep options."
2.1.0,"Reset options, keep optimizer."
2.1.0,State can be partially restored.
2.1.0,"unscaled optimizer's gradients (already done therefore skip),"
2.1.0,skips optimizer.step() if gradients contain infs/NaNs.
2.1.0,Updates the scale for next iteration.
2.1.0,Code below is an implementation of https://arxiv.org/pdf/1804.04235.pdf
2.1.0,inspired but modified from https://github.com/DeadAt0m/adafactor-pytorch
2.1.0,backward compatibility
2.1.0,assuming a list/generator of parameter means single group
2.1.0,compute combined scale factor for this group
2.1.0,norm is in fact norm*scale
2.1.0,note: p.grad should not ever be set for correct operation of
2.1.0,mixed precision optimizer that sometimes sends None gradients
2.1.0,State initialization
2.1.0,Exponential moving average of gradient values
2.1.0,Exponential moving average of squared gradient values
2.1.0,-*- coding: utf-8 -*-
2.1.0,if the loss function operates on vectors of raw logits instead of
2.1.0,"probabilities, only the first part of the generator needs to be"
2.1.0,"passed to the NMTLossCompute. At the moment, the only supported"
2.1.0,loss function of this kind is the sparsemax loss.
2.1.0,"attn_align should be in (batch_size, pad_tgt_size, pad_src_size)"
2.1.0,"align_idx should be a Tensor in size([N, 3]), N is total number"
2.1.0,"of align src-tgt pair in current batch, each as"
2.1.0,"['sent_NÂ°_in_batch', 'tgt_id+1', 'src_id'] (check AlignField)"
2.1.0,NOTE: tgt-src ref alignement that in range_ of shard
2.1.0,(coherent with batch.tgt)
2.1.0,"align_head contains value in [0, 1) presenting attn prob,"
2.1.0,0 was resulted by the context attention src_pad_mask
2.1.0,"So, the correspand position in ref_align should also be 0"
2.1.0,"Therefore, clip align_head to > 1e-18 should be bias free."
2.1.0,non_none: the subdict of the state dictionary where the values
2.1.0,are not None.
2.1.0,"Now, the iteration:"
2.1.0,state is a dictionary of sequences of tensor-like but we
2.1.0,want a sequence of dictionaries of tensors.
2.1.0,"First, unzip the dictionary into a sequence of keys and a"
2.1.0,sequence of tensor-like sequences.
2.1.0,"Now, yield a dictionary for each shard. The keys are always"
2.1.0,the same. values is a sequence of length #keys where each
2.1.0,element is a sequence of length #shards. We want to iterate
2.1.0,"over the shards, not over the keys: therefore, the values need"
2.1.0,to be re-zipped by shard and then each shard can be paired
2.1.0,with the keys.
2.1.0,Assumed backprop'd
2.1.0,Check Transforms
2.1.0,Check path
2.1.0,tgt is src for LM task
2.1.0,Check prefix: will be used when use prefix transform
2.1.0,Check weight
2.1.0,validation when train:
2.1.0,Check embeddings stuff
2.1.0,"Backward compatibility with ""fix_word_vecs_*"" opts"
2.1.0,encoder and decoder should be same sizes
2.1.0,"Load default opt values, then overwrite with the opts in"
2.1.0,"the checkpoint. That way, if there are new options added,"
2.1.0,the defaults are used.
2.1.0,Don't do anything
2.1.0,Update best score of each criteria
2.1.0,Reset tolerance
2.1.0,Update current status
2.1.0,Decrease tolerance
2.1.0,Log
2.1.0,Log
2.1.0,Get a list of world_size lists with len(stat_list) Statistics objects
2.1.0,SRU doesn't support PackedSequence.
2.1.0,-*- coding: utf-8 -*-
2.1.0,threshold on 1 to avoid div by 0
2.1.0,treat alignment matrix one by one as each have different lengths
2.1.0,No alignment if not exist valid tgt token
2.1.0,get valid alignment (sub-matrix from full paded aligment matrix)
2.1.0,-*- coding: utf-8 -*-
2.1.0,this one is needed for torchtext random call (shuffled iterator)
2.1.0,in multi gpu it ensures datasets are read in the same order
2.1.0,some cudnn methods can be random even after fixing the seed
2.1.0,unless you tell it to be deterministic
2.1.0,This one is needed for various tranfroms
2.1.0,These ensure same initialization in multi gpu mode
2.1.0,Shift values to be >= 0
2.1.0,we need to check the model path + any tokenizer path
2.1.0,fast-forward if loaded from state
2.1.0,NOTE: `rnn.pack_padded_sequence` requires that a
2.1.0,"minibatch be sorted by decreasing order, which"
2.1.0,requires reversing relative to typical sort keys
2.1.0,Maintains the longest src and tgt length in the current batch
2.1.0,Reset current longest length at a new batch (count=1)
2.1.0,Src: [<bos> w1 ... wN <eos>]
2.1.0,Tgt: [w1 ... wM <eos>]
2.1.0,coding: utf-8
2.1.0,make a small vocab containing just the tokens in the source sequence
2.1.0,add init_token and eos_token according to src construction
2.1.0,Map source tokens to indices in the dynamic dict.
2.1.0,self.src_vocabs is used in collapse_copy_scores and Translator.py
2.1.0,this assumes src_field and tgt_field are both text
2.1.0,fields needs to have only keys that examples have as attrs
2.1.0,avoid infinite recursion when fields isn't defined
2.1.0,this is a hack: appears quicker to apply it here
2.1.0,than in the ParallelCorpusIterator
2.1.0,NOTE: moved to DatasetAdapter._process method in iterator.py
2.1.0,item = self.transform.apply(
2.1.0,"example, is_train=self.infinitely, corpus_name=self.cid)"
2.1.0,empty example: skip
2.1.0,-*- coding: utf-8 -*-
2.1.0,backwards compatibility
2.1.0,monkey-patch to make torchtext Vocab's pickleable
2.1.0,"+1 for tgt side to keep coherent after ""bos"" padding,"
2.1.0,"register ['NÂ°_in_batch', 'tgt_id+1', 'src_id']"
2.1.0,this is basically copy-pasted from torchtext.
2.1.0,counters changes in place
2.1.0,keep the order of tokens specified in the vocab file by
2.1.0,adding them to the counter with decreasing counting values
2.1.0,`tgt_vocab_size` is ignored when sharing vocabularies
2.1.0,return vocab to dump with standard name
2.1.0,empty train_dataset_files so that vocab is only loaded from
2.1.0,"given paths in src_vocab_path, tgt_vocab_path"
2.1.0,Load vocabulary
2.1.0,Drop the none-using from memory but keep the last
2.1.0,"in the long run, shouldn't it be possible to do this by calling"
2.1.0,build_vocab with both the src and tgt data?
2.1.0,coding: utf-8
2.1.0,several data readers need optional dependencies. There's no
2.1.0,appropriate builtin exception
2.1.0,NOTE: not support nfeats > 0 yet
2.1.0,-*- coding: utf-8 -*-
2.1.0,mix this with partial
2.1.0,batch (list(list(list))): batch_size x len(self.fields) x seq_len
2.1.0,lengths: batch_size
2.1.0,data: seq_len x batch_size x len(self.fields)
2.1.0,flake8: noqa
2.1.0,For command-line option parsing
2.1.0,"Check pass, set the args."
2.1.0,"This SRU version implements its own cuda-level optimization,"
2.1.0,so it requires that:
2.1.0,1. `cupy` and `pynvrtc` python package installed.
2.1.0,2. pytorch is built with cuda support.
2.1.0,3. library path set: export LD_LIBRARY_PATH=<cuda lib path>.
2.1.0,Check 1.
2.1.0,Check 2.
2.1.0,Check 3.
2.1.0,This sets up device to use.
2.1.0,-> directions x batch x dim
2.1.0,For DEBUG
2.1.0,"size = (length, batch, x.size(-1)) \"
2.1.0,"if x.dim() == 3 else (batch, x.size(-1))"
2.1.0,grad_x = x.new(*x.size()) if k_ == 3 else x.new(*size).zero_()
2.1.0,Normal use
2.1.0,"An entry check here, will catch on train side and translate side"
2.1.0,if requirements are not satisfied.
2.1.0,RNNDecoderState wraps hidden as a tuple.
2.1.0,fh -> (layers*directions) x batch x dim
2.1.0,"No encoder in LM, seq2seq count formatting kept"
2.1.0,_check_save_model_path
2.1.0,NOTE: We need to trim the vocab to remove any unk tokens that
2.1.0,were not originally here.
2.1.0,!/usr/bin/env python
2.1.0,!/usr/bin/env python
2.1.0,!/usr/bin/env python
2.1.0,-*- coding: utf-8 -*-
2.1.0,!/usr/bin/env python
2.1.0,!/usr/bin/env python
2.1.0,!/usr/bin/env python
2.1.0,import onmt.opts as opts
2.1.0,Set sharing strategy manually instead of default based on the OS.
2.1.0,"maybe prepare pretrained embeddings, if any"
2.1.0,Load checkpoint if we resume from a previous training.
2.1.0,Report src and tgt vocab sizes
2.1.0,Create a thread to listen for errors in the child processes.
2.1.0,Train with multiprocessing.
2.1.0,"This does not work if we merge with the first loop, not sure why"
2.1.0,Get the iterator to generate from
2.1.0,"Once training is done, we can terminate the producers"
2.1.0,magic indices
2.1.0,result caching
2.1.0,fix length constraint and remove eos from count
2.1.0,add one to account for BOS. Don't account for EOS because hitting
2.1.0,this implies it hasn't been found.
2.1.0,we don't block nothing if the user doesn't want it
2.1.0,we can't block nothing beam's too short
2.1.0,we check paths one by one
2.1.0,we don't forbid nothing if the user doesn't want it
2.1.0,we can't forbid nothing if beam's too short
2.1.0,Reordering forbidden_tokens following beam selection
2.1.0,We rebuild a dict to ensure we get the value and not the pointer
2.1.0,Grabing the newly selected tokens and associated ngram
2.1.0,skip the blocking if any token in current_ngram is excluded
2.1.0,"pickups: Tensor where specified index were set to 1, others 0"
2.1.0,"dropdowns: opposite of pickups, 1 for those shouldn't pick"
2.1.0,Minus dropdowns to log_probs making probabilities of
2.1.0,unspecified index close to 0
2.1.0,"prediction step have surpass length of given target_prefix,"
2.1.0,no need to further change this attr
2.1.0,keep indices until overflowing p
2.1.0,Set all logits that are not in the top-p to -10000.
2.1.0,This puts the probabilities close to 0.
2.1.0,Set all logits that are not in the top-k to -10000.
2.1.0,This puts the probabilities close to 0.
2.1.0,"For temp=0.0, take the argmax to avoid divide-by-zero errors."
2.1.0,keep_topk=1 is also equivalent to argmax.
2.1.0,maybe fix some prediction at this step by modifying log_probs
2.1.0,"shape: (sum(~ self.is_finished), 1)"
2.1.0,in LM task memory_lengths is associated with currently generated src
2.1.0,and therefore needs to follow the generation
2.1.0,!/usr/bin/env python
2.1.0,Maintains the longest src and tgt length in the current batch
2.1.0,Reset current longest length at a new batch (count=1)
2.1.0,max_tgt_in_batch = 0
2.1.0,Src: [<bos> w1 ... wN <eos>]
2.1.0,Tgt: [w1 ... wM <eos>]
2.1.0,for debugging
2.1.0,TODO: maybe add dynamic part
2.1.0,Statistics
2.1.0,Turn any copied words into UNKs.
2.1.0,"Decoder forward, takes [tgt_len, batch, nfeats] as input"
2.1.0,"and [src_len, batch, hidden] as memory_bank"
2.1.0,"in case of inference tgt_len = 1, batch = beam times batch_size"
2.1.0,"in case of Gold Scoring tgt_len = actual length, batch = 1 batch"
2.1.0,Generator forward.
2.1.0,"returns [(batch_size x beam_size) , vocab ] when 1 step"
2.1.0,"or [ tgt_len, batch_size, vocab ] when full sentence"
2.1.0,"here we have scores [tgt_lenxbatch, vocab] or [beamxbatch, vocab]"
2.1.0,"returns [(batch_size x beam_size) , vocab ] when 1 step"
2.1.0,"or [ tgt_len, batch_size, vocab ] when full sentence"
2.1.0,(0) add BOS and padding to tgt prediction
2.1.0,(1) Encoder forward.
2.1.0,(2) Repeat src objects `n_best` times.
2.1.0,"We use batch_size x n_best, get ``(src_len, batch * n_best, nfeat)``"
2.1.0,"(3) Init decoder with n_best src,"
2.1.0,"reshape tgt to ``(len, batch * n_best, nfeat)``"
2.1.0,masked_select
2.1.0,get aligned src id for each prediction's valid tgt tokens
2.1.0,TODO: support these blacklisted features
2.1.0,(0) Prep the components of the search.
2.1.0,(1) Run the encoder on the src.
2.1.0,(2) prep decode_strategy. Possibly repeat src objects.
2.1.0,(3) Begin decoding step by step:
2.1.0,Reorder states.
2.1.0,TODO: support these blacklisted features
2.1.0,hack [min_len_batch-1:] because expect <bos>
2.1.0,(0) Prep the components of the search.
2.1.0,(1) split src into src and target_prefix to avoid padding.
2.1.0,(2) init decoder
2.1.0,(3) prep decode_strategy. Possibly repeat src objects.
2.1.0,(4) Begin decoding step by step:
2.1.0,Reorder states.
2.1.0,select indexes in model state/cache
2.1.0,beam parameters
2.1.0,beam state
2.1.0,BoolTensor was introduced in pytorch 1.2
2.1.0,"""global state"" of the old beam"
2.1.0,buffers for the topk scores and 'backpointer'
2.1.0,for testing
2.1.0,maybe fix some prediction at this step by modifying log_probs
2.1.0,Flatten probs into a list of possibilities.
2.1.0,Penalize beams that finished.
2.1.0,"on real data (newstest2017) with the pretrained transformer,"
2.1.0,it's faster to not move this back to the original device
2.1.0,Store finished hypotheses for this batch.
2.1.0,End condition is the top beam finished and we can return
2.1.0,n_best hypotheses.
2.1.0,"If all sentences are translated, no need to go further."
2.1.0,Remove finished batches for the next step.
2.1.0,using integer division to get an integer _B without casting
2.1.0,force the output to be longer than self.min_length
2.1.0,Multiply probs by the beam probability.
2.1.0,"if the sequence ends now, then the penalty is the current"
2.1.0,"length + 1, to include the EOS token"
2.1.0,Avoid any direction that would repeat unwanted ngrams
2.1.0,Pick up candidate token by curr_scores
2.1.0,Recover log probs.
2.1.0,Length penalty is just a scalar. It doesn't matter if it's applied
2.1.0,before or after the topk.
2.1.0,Resolve beam origin and map to batch index flat representation.
2.1.0,Append last prediction.
2.1.0,update global state (step == 1)
2.1.0,update global state (step > 1)
2.1.0,"shape: (batch_size x beam_size, 1)"
2.1.0,in LM task memory_lengths is associated with currently generated src
2.1.0,and therefore needs to follow the generation
2.1.0,in LM task memory_lengths is associated with currently generated src
2.1.0,and therefore needs to follow the generation
2.1.0,Term will be subtracted from probability
2.1.0,Probability will be divided by this
2.1.0,these warnings indicate that either the alpha/beta
2.1.0,"forces a penalty to be a no-op, or a penalty is a no-op but"
2.1.0,the alpha/beta would suggest otherwise.
2.1.0,using some length penalty
2.1.0,using some coverage penalty
2.1.0,!/usr/bin/env python
2.1.0,semaphore doesn't have a timeout arg in Python 2.7
2.1.0,perform a first request to initialize everything
2.1.0,backwards compatibility for confs
2.1.0,every segment becomes a dict for flexibility purposes
2.1.0,NOTE: translator returns lists of `n_best` list
2.1.0,build back results with empty texts
2.1.0,load can be called multiple times: modify copy
2.1.0,output contain alignment
2.1.0,Below are all the different penalty terms implemented so far.
2.1.0,Subtract coverage penalty from topk log probs.
2.1.0,Divide topk log probs by length penalty.
2.1.0,Sorting
2.1.0,Chinese segmentation
2.1.0,Chinese simplify -> Chinese traditional standard
2.1.0,Chinese simplify -> Chinese traditional (HongKong)
2.1.0,Chinese simplify -> Chinese traditional (Taiwan)
2.1.0,Chinese traditional -> Chinese simplify (v1)
2.1.0,Chinese traditional -> Chinese simplify (v2)
2.0.1,!/usr/bin/env python
2.0.1,!/usr/bin/env python
2.0.1,!/usr/bin/env python
2.0.1,!/usr/bin/env python
2.0.1,!/usr/bin/env python
2.0.1,!/usr/bin/env python3
2.0.1,-*- coding: utf-8 -*-
2.0.1,
2.0.1,"OpenNMT-py documentation build configuration file, created by"
2.0.1,sphinx-quickstart on Sun Dec 17 12:07:14 2017.
2.0.1,
2.0.1,This file is execfile()d with the current directory set to its
2.0.1,containing dir.
2.0.1,
2.0.1,Note that not all possible configuration values are present in this
2.0.1,autogenerated file.
2.0.1,
2.0.1,All configuration values have a default; values that are commented out
2.0.1,serve to show the default.
2.0.1,"If extensions (or modules to document with autodoc) are in another directory,"
2.0.1,add these directories to sys.path here. If the directory is relative to the
2.0.1,"documentation root, use os.path.abspath to make it absolute, like shown here."
2.0.1,
2.0.1,import os
2.0.1,import sys
2.0.1,"sys.path.insert(0, os.path.abspath('.'))"
2.0.1,-- General configuration ------------------------------------------------
2.0.1,"If your documentation needs a minimal Sphinx version, state it here."
2.0.1,
2.0.1,needs_sphinx = '1.0'
2.0.1,"Add any Sphinx extension module names here, as strings. They can be"
2.0.1,extensions coming with Sphinx (named 'sphinx.ext.*') or your custom
2.0.1,ones.
2.0.1,Show base classes
2.0.1,"Use ""variables"" section for Attributes instead of weird block things"
2.0.1,mimicking the function style.
2.0.1,"Add any paths that contain templates here, relative to this directory."
2.0.1,The suffix(es) of source filenames.
2.0.1,You can specify multiple suffix as a list of string:
2.0.1,
2.0.1,"source_suffix = ['.rst', '.md']"
2.0.1,The master toctree document.
2.0.1,General information about the project.
2.0.1,"The version info for the project you're documenting, acts as replacement for"
2.0.1,"|version| and |release|, also used in various other places throughout the"
2.0.1,built documents.
2.0.1,
2.0.1,The short X.Y version.
2.0.1,"The full version, including alpha/beta/rc tags."
2.0.1,The language for content autogenerated by Sphinx. Refer to documentation
2.0.1,for a list of supported languages.
2.0.1,
2.0.1,This is also used if you do content translation via gettext catalogs.
2.0.1,"Usually you set ""language"" from the command line for these cases."
2.0.1,"List of patterns, relative to source directory, that match files and"
2.0.1,directories to ignore when looking for source files.
2.0.1,This patterns also effect to html_static_path and html_extra_path
2.0.1,The name of the Pygments (syntax highlighting) style to use.
2.0.1,"If true, `todo` and `todoList` produce output, else they produce nothing."
2.0.1,-- Options for HTML output ----------------------------------------------
2.0.1,The theme to use for HTML and HTML Help pages.  See the documentation for
2.0.1,a list of builtin themes.
2.0.1,
2.0.1,html_theme = 'sphinx_materialdesign_theme'
2.0.1,html_theme_path = [sphinx_materialdesign_theme.get_path()]
2.0.1,Theme options are theme-specific and customize the look and feel of a theme
2.0.1,"further.  For a list of options available for each theme, see the"
2.0.1,documentation.
2.0.1,
2.0.1,html_theme_options = {}
2.0.1,"Add any paths that contain custom static files (such as style sheets) here,"
2.0.1,"relative to this directory. They are copied after the builtin static files,"
2.0.1,"so a file named ""default.css"" will overwrite the builtin ""default.css""."
2.0.1,"Custom sidebar templates, must be a dictionary that maps document names"
2.0.1,to template names.
2.0.1,
2.0.1,This is required for the alabaster theme
2.0.1,refs: http://alabaster.readthedocs.io/en/latest/installation.html#sidebars
2.0.1,-- Options for HTMLHelp output ------------------------------------------
2.0.1,Output file base name for HTML help builder.
2.0.1,-- Options for LaTeX output ---------------------------------------------
2.0.1,The paper size ('letterpaper' or 'a4paper').
2.0.1,
2.0.1,"'papersize': 'letterpaper',"
2.0.1,"The font size ('10pt', '11pt' or '12pt')."
2.0.1,
2.0.1,"'pointsize': '10pt',"
2.0.1,Additional stuff for the LaTeX preamble.
2.0.1,
2.0.1,"'preamble': '',"
2.0.1,Latex figure (float) alignment
2.0.1,
2.0.1,"'figure_align': 'htbp',"
2.0.1,Grouping the document tree into LaTeX files. List of tuples
2.0.1,"(source start file, target name, title,"
2.0.1,"author, documentclass [howto, manual, or own class])."
2.0.1,-- Options for manual page output ---------------------------------------
2.0.1,One entry per manual page. List of tuples
2.0.1,"(source start file, name, description, authors, manual section)."
2.0.1,-- Options for Texinfo output -------------------------------------------
2.0.1,Grouping the document tree into Texinfo files. List of tuples
2.0.1,"(source start file, target name, title, author,"
2.0.1,"dir menu entry, description, category)"
2.0.1,!/usr/bin/env python
2.0.1,-*- coding: utf-8 -*-
2.0.1,is this reachable?
2.0.1,Read in embeddings
2.0.1,Write to file
2.0.1,converts a SentencePiece vocabulary to the format expected by dynamic data
2.0.1,"(essentially converts float expected counts to ""fixed precision"" int pseudo"
2.0.1,counts)
2.0.1,"Add in default model arguments, possibly added since training."
2.0.1,build_base_model expects updated and validated opts
2.0.1,-*- encoding: utf-8 -*-
2.0.1,!/usr/bin/env python
2.0.1,-*- coding: utf-8 -*-
2.0.1,Author: Rico Sennrich
2.0.1,flake8: noqa
2.0.1,This file is retrieved from https://github.com/rsennrich/subword-nmt
2.0.1,hack for python2/3 compatibility
2.0.1,check version information
2.0.1,some hacking to deal with duplicates (only consider first instance)
2.0.1,don't print end-of-word symbols
2.0.1,sys.stderr.write('cannot split {0} further.\n'.format(segment))
2.0.1,sys.stderr.write('OOV: {0}\n'.format(segment))
2.0.1,sys.stderr.write('OOV: {0}\n'.format(segment))
2.0.1,python 2/3 compatibility
2.0.1,read/write files as UTF-8
2.0.1,!/usr/bin/env python
2.0.1,!/usr/bin/env python
2.0.1,-*- coding: utf-8 -*-
2.0.1,Author: Rico Sennrich
2.0.1,flake8: noqa
2.0.1,This file is retrieved from https://github.com/rsennrich/subword-nmt
2.0.1,hack for python2/3 compatibility
2.0.1,"find all instances of pair, and update frequency/indices around it"
2.0.1,find first symbol
2.0.1,"if first symbol is followed by second symbol, we've found an occurrence of pair (old_word[i:i+2])"
2.0.1,"assuming a symbol sequence ""A B C"", if ""B C"" is merged, reduce the frequency of ""A B"""
2.0.1,"assuming a symbol sequence ""A B C B"", if ""B C"" is merged, reduce the frequency of ""C B""."
2.0.1,"however, skip this if the sequence is A B C B C, because the frequency of ""C B"" will be reduced by the previous code block"
2.0.1,find new pair
2.0.1,"assuming a symbol sequence ""A BC D"", if ""B C"" is merged, increase the frequency of ""A BC"""
2.0.1,"assuming a symbol sequence ""A BC B"", if ""B C"" is merged, increase the frequency of ""BC B"""
2.0.1,"however, if the sequence is A BC BC, skip this step because the count of ""BC BC"" will be incremented by the previous code block"
2.0.1,data structure of pair frequencies
2.0.1,index from pairs to words
2.0.1,version 0.2 changes the handling of the end-of-word token ('</w>');
2.0.1,version numbering allows bckward compatibility
2.0.1,"threshold is inspired by Zipfian assumption, but should only affect speed"
2.0.1,we probably missed the best pair because of pruning; go back to full statistics
2.0.1,"threshold is inspired by Zipfian assumption, but should only affect speed"
2.0.1,python 2/3 compatibility
2.0.1,read/write files as UTF-8
2.0.1,!/usr/bin/env python
2.0.1,-*- coding: utf-8 -*-
2.0.1,!/usr/bin/env python
2.0.1,Build embeddings.
2.0.1,Build encoder.
2.0.1,Build embeddings.
2.0.1,Build decoder.
2.0.1,Share the embedding matrix - preprocess with share_vocab required.
2.0.1,src/tgt vocab should be the same if `-share_vocab` is specified.
2.0.1,for back compat when attention_dropout was not defined
2.0.1,Build Model
2.0.1,Build Generator.
2.0.1,Load the model states from checkpoint or initialize them.
2.0.1,This preserves backward-compat for models using customed layernorm
2.0.1,end of patch for backward compatibility
2.0.1,!/usr/bin/env python
2.0.1,ensure tensorboard output is written in the directory
2.0.1,of previous checkpoints
2.0.1,NOTE: It's important that ``opt`` has been validated and updated
2.0.1,at this point.
2.0.1,Build model.
2.0.1,Build optimizer.
2.0.1,Build model saver
2.0.1,Move batch to specified device
2.0.1,Use Tensorboard for visualization during training
2.0.1,Options only during inference
2.0.1,"Truncation options, for text corpus"
2.0.1,"as for False, this will be added in _add_train_general_opts"
2.0.1,Embedding Options
2.0.1,Model Task Options
2.0.1,Encoder-Decoder Options
2.0.1,"group.add('--residual', '-residual',   action=""store_true"","
2.0.1,"help=""Add residual connections between RNN layers."")"
2.0.1,The following options (bridge_extra_node to n_steps) are used
2.0.1,for training with --encoder_type ggnn (Gated Graph Neural Network).
2.0.1,Attention options
2.0.1,Alignement options
2.0.1,Generator and loss options.
2.0.1,GPU
2.0.1,Init options
2.0.1,Pretrained word vectors
2.0.1,Freeze word vectors
2.0.1,Optimization options
2.0.1,learning rate
2.0.1,options relate to data preprare
2.0.1,options relate to train
2.0.1,Alpha and Beta values for Google Length + Coverage penalty
2.0.1,"Described here: https://arxiv.org/pdf/1609.08144.pdf, Section 7"
2.0.1,Length penalty options
2.0.1,Coverage penalty options
2.0.1,Decoding Length constraint
2.0.1,Decoding content constraint
2.0.1,Adding options relate to decoding strategy
2.0.1,Adding option for logging
2.0.1,Copyright 2016 The Chromium Authors. All rights reserved.
2.0.1,Use of this source code is governed by a BSD-style license that can be
2.0.1,found in the LICENSE file.
2.0.1,"Get the key 'value' in the dict, or just use 'value'"
2.0.1,Basic attributes.
2.0.1,Set model in training mode.
2.0.1,UPDATE DROPOUT
2.0.1,Run patience mechanism
2.0.1,"If the patience has reached the limit, stop training"
2.0.1,swap model params w/ moving average
2.0.1,(and keep the original parameters)
2.0.1,Set model in validating mode.
2.0.1,F-prop through the model.
2.0.1,Compute loss.
2.0.1,Update statistics.
2.0.1,Set model back to training mode.
2.0.1,Truncated BPTT: reminder not compatible with accum > 1
2.0.1,1. Create truncated target.
2.0.1,2. F-prop all but generator.
2.0.1,3. Compute loss.
2.0.1,4. Update the parameters and statistics.
2.0.1,Multi GPU gradient gather
2.0.1,"If truncated, don't backprop fully."
2.0.1,TO CHECK
2.0.1,if dec_state is not None:
2.0.1,dec_state.detach()
2.0.1,"in case of multi step gradient accumulation,"
2.0.1,update only after accum batches
2.0.1,For Flake
2.0.1,we avoid padding while mean pooling
2.0.1,incoming and outgoing edge embedding
2.0.1,Find vocab data for tree builting
2.0.1,Propogation Model
2.0.1,Initialize the bridge layer
2.0.1,Token embedding
2.0.1,Initialize graph using formatted input sequence
2.0.1,Number of flagged nodes defines node count for this sample
2.0.1,"(Nodes can have no flags on them, but must be in 'flags' list)."
2.0.1,The total number of integers in the vocab should allow
2.0.1,for all features and edges to be defined.
2.0.1,Use first extra node as only source for decoder init
2.0.1,Average all nodes to get bridge input
2.0.1,"LSTM has hidden and cell state, other only one"
2.0.1,Total number of states
2.0.1,Build a linear layer for each
2.0.1,Initialize the bridge layer
2.0.1,"s_len, batch, emb_dim = emb.size()"
2.0.1,Lengths data is wrapped inside a Tensor.
2.0.1,"LSTM has hidden and cell state, other only one"
2.0.1,Total number of states
2.0.1,Build a linear layer for each
2.0.1,"s_len, batch, emb_dim = emb.size()"
2.0.1,Run the forward pass of every layer of the tranformer.
2.0.1,Dimensions and padding for constructing the word embedding matrix
2.0.1,Dimensions and padding for feature embedding matrices
2.0.1,(these have no effect if feat_vocab_sizes is empty)
2.0.1,The embedding matrix look-up tables. The first look-up table
2.0.1,"is for words. Subsequent ones are for features, if any exist."
2.0.1,The final output size of word + feature vectors. This can vary
2.0.1,from the word vector size if and only if features are defined.
2.0.1,This is the attribute you should access if you need to know
2.0.1,how big your embeddings are going to be.
2.0.1,The sequence of operations that converts the input sequence
2.0.1,into a sequence of embeddings. At minimum this consists of
2.0.1,looking up the embeddings for each word and feature in the
2.0.1,input. Model parameters may require the sequence to contain
2.0.1,additional operations as well.
2.0.1,features must use word_vec_size
2.0.1,features will use feat_vec_size
2.0.1,Some utilitary functions for pretrained embeddings
2.0.1,is this reachable?
2.0.1,Write to file
2.0.1,set the opt in place
2.0.1,set the opt in place
2.0.1,This class is mainly used by decoder.py for RNNs but also
2.0.1,by the CNN / transformer decoder when copy attention is used
2.0.1,CNN has its own attention mechanism ConvMultiStepAttention
2.0.1,Transformer has its own MultiHeadedAttention
2.0.1,mlp wants it with bias
2.0.1,Check input sizes
2.0.1,"(batch, t_len, d) x (batch, d, s_len) --> (batch, t_len, s_len)"
2.0.1,"(batch, t_len, s_len, d)"
2.0.1,one step input
2.0.1,"compute attention scores, as in Luong et al."
2.0.1,Softmax or sparsemax to normalize attention weights
2.0.1,each context vector c_t is the weighted average
2.0.1,over all the source hidden states
2.0.1,concatenate
2.0.1,Check output sizes
2.0.1,Check output sizes
2.0.1,clamping necessary because of numerical errors: loss should be lower
2.0.1,"bounded by zero, but negative values near zero are possible without"
2.0.1,the clamp
2.0.1,from onmt.utils.misc import aeq
2.0.1,CHECKS
2.0.1,"batch, k_len, d = key.size()"
2.0.1,"batch_, k_len_, d_ = value.size()"
2.0.1,"aeq(batch, batch_)"
2.0.1,"aeq(k_len, k_len_)"
2.0.1,"aeq(d, d_)"
2.0.1,"batch_, q_len, d_ = query.size()"
2.0.1,"aeq(batch, batch_)"
2.0.1,"aeq(d, d_)"
2.0.1,"aeq(self.model_dim % 8, 0)"
2.0.1,if mask is not None:
2.0.1,"batch_, q_len_, k_len_ = mask.size()"
2.0.1,"aeq(batch_, batch)"
2.0.1,"aeq(k_len_, k_len)"
2.0.1,aeq(q_len_ == q_len)
2.0.1,END CHECKS
2.0.1,"1) Project key, value, and query."
2.0.1,1 or key_len x key_len
2.0.1,1 or key_len x key_len x dim_per_head
2.0.1,1 or key_len x key_len x dim_per_head
2.0.1,2) Calculate and scale scores.
2.0.1,batch x num_heads x query_len x key_len
2.0.1,3) Apply attention dropout and compute context vectors.
2.0.1,CHECK
2.0.1,"batch_, q_len_, d_ = output.size()"
2.0.1,"aeq(q_len, q_len_)"
2.0.1,"aeq(batch, batch_)"
2.0.1,"aeq(d, d_)"
2.0.1,Return multi-head attn
2.0.1,At the moment this class is only used by embeddings.Embeddings look-up tables
2.0.1,-*- coding: utf-8 -*-
2.0.1,checks
2.0.1,"batch, channel, height, width = base_target_emb.size()"
2.0.1,"batch_, channel_, height_, width_ = input_from_dec.size()"
2.0.1,"enc_batch, enc_channel, enc_height = encoder_out_top.size()"
2.0.1,"enc_batch_, enc_channel_, enc_height_ = encoder_out_combine.size()"
2.0.1,out_features * in_features
2.0.1,norm is out_features * 1
2.0.1,batch_size * out_features
2.0.1,out_features
2.0.1,out_features
2.0.1,batch_size * out_features
2.0.1,"out_channels, in_channels // groups, * kernel_size"
2.0.1,out_features
2.0.1,This is used nowhere in the code at the moment (Vincent Nguyen 05/18/2018)
2.0.1,"in_channels, out_channels, *kernel_size"
2.0.1,"in_channels, out_channels, *kernel_size"
2.0.1,"self.out_channels, 1"
2.0.1,out_features
2.0.1,out_features
2.0.1,store roots on diagonal
2.0.1,CHECKS
2.0.1,Original probabilities.
2.0.1,Probability of copying p(z=1) batch.
2.0.1,Probability of not copying: p_{word}(w) * (1 - p(z))
2.0.1,probabilities assigned by the model to the gold targets
2.0.1,probability of tokens copied from source
2.0.1,Set scores for unk to 0 and add eps
2.0.1,find the indices in which you do not use the copy mechanism
2.0.1,Drop padding.
2.0.1,this block does not depend on the loss value computed above
2.0.1,and is used only for stats
2.0.1,this block does not depend on the loss value computed above
2.0.1,and is used only for stats
2.0.1,Correct target copy token instead of <unk>
2.0.1,tgt[i] = align[i] + len(tgt_vocab)
2.0.1,for i such that tgt[i] == 0 and align[i] != 0
2.0.1,Compute sum of perplexities for stats
2.0.1,this part looks like it belongs in CopyGeneratorLoss
2.0.1,Compute Loss as NLL divided by seq length
2.0.1,Compute Total Loss per sequence in batch
2.0.1,Divide by length of each sequence and sum
2.0.1,Auto import python files in this directory
2.0.1,1. sample number of tokens to corrupt
2.0.1,2. sample positions to corrput
2.0.1,3. sample corrupted values
2.0.1,1. sample number of tokens to corrupt
2.0.1,2. sample positions to corrput
2.0.1,3. Drop token on chosen position
2.0.1,1. sample number of tokens to corrupt
2.0.1,2. sample positions to corrput
2.0.1,3. mask word on chosen position
2.0.1,"Sharing options among `TokenizerTransform`s, same name conflict in"
2.0.1,this scope will be resolved by remove previous occurrence in parser
2.0.1,subword regularization(or BPE dropout) options:
2.0.1,subword vocabulary restriction options:
2.0.1,derterministic subwording
2.0.1,subword sampling when nbest_size > 1 or -1
2.0.1,alpha should be 0.0 < alpha < 1.0
2.0.1,-1: keep everything (i.e. 1 mask per token)
2.0.1,0: replace everything (i.e. no mask)
2.0.1,1: 1 mask per span
2.0.1,view each subword as word start / input is word level token
2.0.1,Pretend it ends with a full stop so last span is a sentence
2.0.1,"Tokens that are full stops, where the previous token is not"
2.0.1,Make sure we have enough to mask
2.0.1,Trim to masking budget
2.0.1,Handle 0-length mask (inserts) separately
2.0.1,assert is_word_start[-1] == 0
2.0.1,assert tokens_length - 1 not in indices
2.0.1,"keep index, but replace it with [MASK]"
2.0.1,"acts as a long length, so spans don't go over the end of doc"
2.0.1,next position from each word_start
2.0.1,delete token: 1 mask/remove per span
2.0.1,"keep index, but replace it with [MASK]: 1 mask per token"
2.0.1,A bit faster when all lengths are 1
2.0.1,to cover whole token
2.0.1,delete token
2.0.1,"keep index, but replace it with [MASK]"
2.0.1,assert tokens_length - 1 not in indices
2.0.1,initialize fields at the top of each unit test to prevent
2.0.1,any undesired stateful effects
2.0.1,"this test touches the file system, so it could be considered an"
2.0.1,integration test
2.0.1,write utf-8 bytes
2.0.1,batch 0 will always predict EOS. The other batches will predict
2.0.1,non-eos scores.
2.0.1,"""best"" prediction is eos - that should be blocked"
2.0.1,include at least one prediction OTHER than EOS
2.0.1,that is greater than -1e20
2.0.1,now batch 0 has ended and no others have
2.0.1,initial step
2.0.1,batch 0 dies on step 0
2.0.1,include at least one prediction OTHER than EOS
2.0.1,that is greater than -1e20
2.0.1,step 2
2.0.1,(old) batch 8 dies on step 1
2.0.1,step 3
2.0.1,everything dies
2.0.1,initial step
2.0.1,batch 0 dies on step 0
2.0.1,include at least one prediction OTHER than EOS
2.0.1,that is greater than -1e20
2.0.1,step 2
2.0.1,(old) batch 8 dies on step 1
2.0.1,step 3
2.0.1,everything dies
2.0.1,initial step
2.0.1,finish one beam
2.0.1,include at least one prediction OTHER than EOS
2.0.1,that is greater than -1e20
2.0.1,step 2
2.0.1,finish example in last batch
2.0.1,(old) batch 8 dies on step 1
2.0.1,step 3
2.0.1,everything dies
2.0.1,initial step
2.0.1,batch 0 dies on step 0
2.0.1,include at least one prediction OTHER than EOS
2.0.1,that is greater than -1e20
2.0.1,step 2
2.0.1,(old) batch 8 dies on step 1
2.0.1,step 3
2.0.1,everything dies
2.0.1,illegal_weights_mask = torch.ByteTensor([
2.0.1,"[0, 0, 0, 0, 0, 0, 0],"
2.0.1,"[0, 0, 0, 1, 1, 1, 1],"
2.0.1,"[0, 0, 0, 0, 0, 1, 1],"
2.0.1,"[0, 0, 1, 1, 1, 1, 1]])"
2.0.1,TODO: fix for pytorch 0.3
2.0.1,illegal_weights = alignments.masked_select(illegal_weights_mask)
2.0.1,"self.assertEqual(0.0, illegal_weights.data.sum())"
2.0.1,this could be considered an integration test because it touches
2.0.1,the filesystem for the config file (and the models)
2.0.1,!/usr/bin/env python
2.0.1,-*- coding: utf-8 -*-
2.0.1,Inject some dummy training options that may needed when build fields
2.0.1,Remove the generated *pt files.
2.0.1,Remove the generated data samples
2.0.1,all beams repeat (beam >= 1 repeat dummy scores)
2.0.1,predict repeat_idx over and over again
2.0.1,"before repeat, scores are either 0 or -inf"
2.0.1,"on repeat, `repeat_idx` score is BLOCKED_SCORE"
2.0.1,"(but it's still the best score, thus we have"
2.0.1,"[BLOCKED_SCORE, -inf, -inf, -inf, -inf]"
2.0.1,repetitions keeps maximizing score
2.0.1,"index 0 has been blocked, so repeating=>+0.0 score"
2.0.1,other indexes are -inf so repeating=>BLOCKED_SCORE
2.0.1,which is higher
2.0.1,beam 0 and beam >=2 will repeat (beam >= 2 repeat dummy scores)
2.0.1,non-interesting beams are going to get dummy values
2.0.1,"on initial round, only predicted scores for beam 0"
2.0.1,matter. Make two predictions. Top one will be repeated
2.0.1,"in beam zero, second one will live on in beam 1."
2.0.1,predict the same thing in beam 0
2.0.1,continue pushing around what beam 1 predicts
2.0.1,"now beam 0 dies (along with the others), beam 1 -> beam 0"
2.0.1,"now beam 0 dies (along with the others), beam 1 -> beam 0"
2.0.1,beam 0 and beam >= 2 will repeat (beam 2 repeats excluded idx)
2.0.1,non-interesting beams are going to get dummy values
2.0.1,predict the same thing in beam 0
2.0.1,continue pushing around what beam 1 predicts
2.0.1,predict the allowed-repeat again in beam 2
2.0.1,"now beam 0 dies, beam 1 -> beam 0, beam 2 -> beam 1"
2.0.1,and the rest die
2.0.1,"since all preds after i=0 are 0, we can check"
2.0.1,that the beam is the correct idx by checking that
2.0.1,the curr score is the initial score
2.0.1,beam 0 will always predict EOS. The other beams will predict
2.0.1,non-eos scores.
2.0.1,non-interesting beams are going to get dummy values
2.0.1,"""best"" prediction is eos - that should be blocked"
2.0.1,include at least beam_sz predictions OTHER than EOS
2.0.1,that are greater than -1e20
2.0.1,predict eos in beam 0
2.0.1,provide beam_sz other good predictions
2.0.1,now the top beam has ended and no others have
2.0.1,"not of interest, but want to make sure it keeps running"
2.0.1,since only beam 0 terminates and n_best = 2
2.0.1,"this is also a test that when block_ngram_repeat=0,"
2.0.1,repeating is acceptable
2.0.1,non-interesting beams are going to get dummy values
2.0.1,"""best"" prediction is eos - that should be blocked"
2.0.1,include at least beam_sz predictions OTHER than EOS
2.0.1,that are greater than -1e20
2.0.1,predict eos in beam 1
2.0.1,provide beam_sz other good predictions in other beams
2.0.1,provide beam_sz other good predictions in other beams
2.0.1,beam 1 dies on min_length
2.0.1,beam 0 dies on the step after beam 1 dies
2.0.1,"inp_lens is tiled in initialize, reassign to make attn match"
2.0.1,non-interesting beams are going to get dummy values
2.0.1,"""best"" prediction is eos - that should be blocked"
2.0.1,include at least beam_sz predictions OTHER than EOS
2.0.1,that are greater than -1e20
2.0.1,predict eos in beam 1
2.0.1,provide beam_sz other good predictions in other beams
2.0.1,provide beam_sz other good predictions in other beams
2.0.1,no top beams are finished yet
2.0.1,beam 1 dies on min_length
2.0.1,no top beams are finished yet
2.0.1,beam 0 dies on the step after beam 1 dies
2.0.1,top beam is finished now so there are attentions
2.0.1,two beams are finished in each batch
2.0.1,second dim is cut down to the non-padded src length
2.0.1,first dim is equal to the time of death
2.0.1,(beam 0 died at current step - adjust for SOS)
2.0.1,(beam 1 died at last step - adjust for SOS)
2.0.1,behavior gets weird when beam is already done so just stop
2.0.1,this is just test_beam.TestBeamAgainstReferenceCase repeated
2.0.1,in each batch.
2.0.1,"init_preds: [4, 3, 5, 6, 7] - no EOS's"
2.0.1,no EOS's yet
2.0.1,"[5, 3, 2, 6, 0], so beam 2 predicts EOS!"
2.0.1,assumes beam 2 finished on last step
2.0.1,ended beam 2 shouldn't continue
2.0.1,"[2, 5, 3, 6, 0] repeat self.BATCH_SZ, so beam 0 predicts EOS!"
2.0.1,"[-2.4879, -3.8910, -4.1010, -4.2010, -4.4010] repeat self.BATCH_SZ"
2.0.1,another beam is finished in all batches
2.0.1,new beam 0 finished
2.0.1,new beam 0 is old beam 3
2.0.1,assumes beam 0 finished on last step
2.0.1,"[5, 2, 6, 1, 0] repeat self.BATCH_SZ, so beam 1 predicts EOS!"
2.0.1,new beam 1 finished
2.0.1,new beam 1 is old beam 4
2.0.1,this could be considered an integration test because it tests
2.0.1,interactions between the GNMT scorer and the beam
2.0.1,"-data option is required, but not used in this test, so dummy."
2.0.1,len x batch x nfeat
2.0.1,Initialize vectors to compare size with
2.0.1,Ensure correct sizes and types
2.0.1,Make sure that output has the correct size and type
2.0.1,"[('encoder_type', 'transformer'),"
2.0.1,"('word_vec_size', 16), ('rnn_size', 16)],"
2.0.1,""""""" Only do SRU test if requirment is safisfied. """""""
2.0.1,SRU doesn't support input_feed.
2.0.1,first check there's nothing unexpectedly not trainable
2.0.1,ok: word embeddings shouldn't be trainable
2.0.1,if word vecs are freezed
2.0.1,ok: positional encodings shouldn't be trainable
2.0.1,then check nothing unexpectedly trainable
2.0.1,Decoder state
2.0.1,Build the RNN.
2.0.1,Set up the context gate.
2.0.1,Set up the standard attention.
2.0.1,The encoder hidden is  (layers*directions) x batch x dim.
2.0.1,We need to convert it to layers x batch x (directions*dim).
2.0.1,Init the input feed.
2.0.1,Update the state with the result.
2.0.1,Concatenates sequence of tensors along a new dimension.
2.0.1,NOTE: v0.3 to 0.4: dec_outs / attns[*] may not be list
2.0.1,(in particular in case of SRU) it was not raising error in 0.3
2.0.1,since stack(Variable) was allowed.
2.0.1,"In 0.4, SRU returns a tensor that shouldn't be stacke"
2.0.1,Check
2.0.1,Calculate the attention.
2.0.1,Calculate the context gate.
2.0.1,Additional args check.
2.0.1,END Additional args check.
2.0.1,Input feed concatenates hidden state with
2.0.1,input at every time step.
2.0.1,TODO: context gate should be employed
2.0.1,instead of second RNN transform.
2.0.1,Update the coverage attention.
2.0.1,Decoder State
2.0.1,CNNDecoder has its own attention mechanism.
2.0.1,Set up a separate copy attention layer if needed.
2.0.1,The output of CNNEncoder.
2.0.1,The combination of output of CNNEncoder and source embeddings.
2.0.1,Process the result and update the attentions.
2.0.1,Update the state.
2.0.1,TODO change the way attns is returned dict => list or tuple (onnx)
2.0.1,Memory_lengths is a single tensor shared between all models.
2.0.1,This assumption will not hold if Translator is modified
2.0.1,to calculate memory_lengths as something other than the length
2.0.1,of the input.
2.0.1,"return _, (B, Q_len, K_len)"
2.0.1,"layer average attention across heads, get ``(B, Q, K)``"
2.0.1,"Case 1: no full_context, no align heads -> layer avg baseline"
2.0.1,"Case 2: no full_context, 1 align heads -> guided align"
2.0.1,"Case 3: full_context, 1 align heads -> full cte guided align"
2.0.1,BoolTensor was introduced in pytorch 1.2
2.0.1,T: could be 1 in the case of stepwise decoding or tgt_len
2.0.1,masking is necessary when sequence length is greater than one
2.0.1,Decoder State
2.0.1,"previously, there was a GlobalAttention module here for copy"
2.0.1,"attention. But it was never actually used -- the ""copy"" attention"
2.0.1,just reuses the context attention.
2.0.1,"attns[""align""] = torch.stack(attn_aligns, 0).mean(0)  # All avg"
2.0.1,TODO change the way attns is returned dict => list or tuple (onnx)
2.0.1,T: could be 1 in the case of stepwise decoding or tgt_len
2.0.1,masking is necessary when sequence length is greater than one
2.0.1,TODO change the way attns is returned dict => list or tuple (onnx)
2.0.1,"buffer size in bytes, determine equiv. # of elements based on data type"
2.0.1,copy tensors into buffer_t
2.0.1,all-reduce and rescale
2.0.1,copy all-reduced buffer back into tensors
2.0.1,"tensor is bigger than buffer, all-reduce and rescale directly"
2.0.1,"buffer is full, all-reduce and replace buffer with grad"
2.0.1,add tensor to buffer
2.0.1,NOTE: stride (if needed) is handled at the
2.0.1,generator (train_iter) level
2.0.1,Move batch to correspond device_id when consumer iterate
2.0.1,hack to dodge unpicklable `dict_keys`
2.0.1,"propagate exception to parent process, keeping original traceback"
2.0.1,TODO: Find a better way to check for sparse gradients.
2.0.1,we use here a FusedAdam() copy of an old Apex repo
2.0.1,In this case use the old FusedAdam with FP16_optimizer wrapper
2.0.1,Load everything from the checkpoint.
2.0.1,Build everything from scratch.
2.0.1,"Reset optimizer, keep options."
2.0.1,"Reset options, keep optimizer."
2.0.1,State can be partially restored.
2.0.1,"unscaled optimizer's gradients (already done therefore skip),"
2.0.1,skips optimizer.step() if gradients contain infs/NaNs.
2.0.1,Updates the scale for next iteration.
2.0.1,Code below is an implementation of https://arxiv.org/pdf/1804.04235.pdf
2.0.1,inspired but modified from https://github.com/DeadAt0m/adafactor-pytorch
2.0.1,backward compatibility
2.0.1,assuming a list/generator of parameter means single group
2.0.1,compute combined scale factor for this group
2.0.1,norm is in fact norm*scale
2.0.1,note: p.grad should not ever be set for correct operation of
2.0.1,mixed precision optimizer that sometimes sends None gradients
2.0.1,State initialization
2.0.1,Exponential moving average of gradient values
2.0.1,Exponential moving average of squared gradient values
2.0.1,-*- coding: utf-8 -*-
2.0.1,if the loss function operates on vectors of raw logits instead of
2.0.1,"probabilities, only the first part of the generator needs to be"
2.0.1,"passed to the NMTLossCompute. At the moment, the only supported"
2.0.1,loss function of this kind is the sparsemax loss.
2.0.1,"attn_align should be in (batch_size, pad_tgt_size, pad_src_size)"
2.0.1,"align_idx should be a Tensor in size([N, 3]), N is total number"
2.0.1,"of align src-tgt pair in current batch, each as"
2.0.1,"['sent_NÂ°_in_batch', 'tgt_id+1', 'src_id'] (check AlignField)"
2.0.1,NOTE: tgt-src ref alignement that in range_ of shard
2.0.1,(coherent with batch.tgt)
2.0.1,"align_head contains value in [0, 1) presenting attn prob,"
2.0.1,0 was resulted by the context attention src_pad_mask
2.0.1,"So, the correspand position in ref_align should also be 0"
2.0.1,"Therefore, clip align_head to > 1e-18 should be bias free."
2.0.1,non_none: the subdict of the state dictionary where the values
2.0.1,are not None.
2.0.1,"Now, the iteration:"
2.0.1,state is a dictionary of sequences of tensor-like but we
2.0.1,want a sequence of dictionaries of tensors.
2.0.1,"First, unzip the dictionary into a sequence of keys and a"
2.0.1,sequence of tensor-like sequences.
2.0.1,"Now, yield a dictionary for each shard. The keys are always"
2.0.1,the same. values is a sequence of length #keys where each
2.0.1,element is a sequence of length #shards. We want to iterate
2.0.1,"over the shards, not over the keys: therefore, the values need"
2.0.1,to be re-zipped by shard and then each shard can be paired
2.0.1,with the keys.
2.0.1,Assumed backprop'd
2.0.1,Check Transforms
2.0.1,Check path
2.0.1,tgt is src for LM task
2.0.1,Check prefix: will be used when use prefix transform
2.0.1,Check weight
2.0.1,validation when train:
2.0.1,Check embeddings stuff
2.0.1,"Backward compatibility with ""fix_word_vecs_*"" opts"
2.0.1,encoder and decoder should be same sizes
2.0.1,"Load default opt values, then overwrite with the opts in"
2.0.1,"the checkpoint. That way, if there are new options added,"
2.0.1,the defaults are used.
2.0.1,Don't do anything
2.0.1,Update best score of each criteria
2.0.1,Reset tolerance
2.0.1,Update current status
2.0.1,Decrease tolerance
2.0.1,Log
2.0.1,Log
2.0.1,Get a list of world_size lists with len(stat_list) Statistics objects
2.0.1,SRU doesn't support PackedSequence.
2.0.1,-*- coding: utf-8 -*-
2.0.1,threshold on 1 to avoid div by 0
2.0.1,treat alignment matrix one by one as each have different lengths
2.0.1,No alignment if not exist valid tgt token
2.0.1,get valid alignment (sub-matrix from full paded aligment matrix)
2.0.1,-*- coding: utf-8 -*-
2.0.1,this one is needed for torchtext random call (shuffled iterator)
2.0.1,in multi gpu it ensures datasets are read in the same order
2.0.1,some cudnn methods can be random even after fixing the seed
2.0.1,unless you tell it to be deterministic
2.0.1,This one is needed for various tranfroms
2.0.1,These ensure same initialization in multi gpu mode
2.0.1,Shift values to be >= 0
2.0.1,we need to check the model path + any tokenizer path
2.0.1,fast-forward if loaded from state
2.0.1,NOTE: `rnn.pack_padded_sequence` requires that a
2.0.1,"minibatch be sorted by decreasing order, which"
2.0.1,requires reversing relative to typical sort keys
2.0.1,Maintains the longest src and tgt length in the current batch
2.0.1,Reset current longest length at a new batch (count=1)
2.0.1,Src: [<bos> w1 ... wN <eos>]
2.0.1,Tgt: [w1 ... wM <eos>]
2.0.1,coding: utf-8
2.0.1,make a small vocab containing just the tokens in the source sequence
2.0.1,add init_token and eos_token according to src construction
2.0.1,Map source tokens to indices in the dynamic dict.
2.0.1,self.src_vocabs is used in collapse_copy_scores and Translator.py
2.0.1,this assumes src_field and tgt_field are both text
2.0.1,fields needs to have only keys that examples have as attrs
2.0.1,avoid infinite recursion when fields isn't defined
2.0.1,this is a hack: appears quicker to apply it here
2.0.1,than in the ParallelCorpusIterator
2.0.1,NOTE: moved to DatasetAdapter._process method in iterator.py
2.0.1,item = self.transform.apply(
2.0.1,"example, is_train=self.infinitely, corpus_name=self.cid)"
2.0.1,empty example: skip
2.0.1,-*- coding: utf-8 -*-
2.0.1,backwards compatibility
2.0.1,monkey-patch to make torchtext Vocab's pickleable
2.0.1,"+1 for tgt side to keep coherent after ""bos"" padding,"
2.0.1,"register ['NÂ°_in_batch', 'tgt_id+1', 'src_id']"
2.0.1,this is basically copy-pasted from torchtext.
2.0.1,counters changes in place
2.0.1,keep the order of tokens specified in the vocab file by
2.0.1,adding them to the counter with decreasing counting values
2.0.1,`tgt_vocab_size` is ignored when sharing vocabularies
2.0.1,return vocab to dump with standard name
2.0.1,empty train_dataset_files so that vocab is only loaded from
2.0.1,"given paths in src_vocab_path, tgt_vocab_path"
2.0.1,Load vocabulary
2.0.1,Drop the none-using from memory but keep the last
2.0.1,"in the long run, shouldn't it be possible to do this by calling"
2.0.1,build_vocab with both the src and tgt data?
2.0.1,coding: utf-8
2.0.1,several data readers need optional dependencies. There's no
2.0.1,appropriate builtin exception
2.0.1,NOTE: not support nfeats > 0 yet
2.0.1,-*- coding: utf-8 -*-
2.0.1,mix this with partial
2.0.1,batch (list(list(list))): batch_size x len(self.fields) x seq_len
2.0.1,lengths: batch_size
2.0.1,data: seq_len x batch_size x len(self.fields)
2.0.1,flake8: noqa
2.0.1,For command-line option parsing
2.0.1,"Check pass, set the args."
2.0.1,"This SRU version implements its own cuda-level optimization,"
2.0.1,so it requires that:
2.0.1,1. `cupy` and `pynvrtc` python package installed.
2.0.1,2. pytorch is built with cuda support.
2.0.1,3. library path set: export LD_LIBRARY_PATH=<cuda lib path>.
2.0.1,Check 1.
2.0.1,Check 2.
2.0.1,Check 3.
2.0.1,This sets up device to use.
2.0.1,-> directions x batch x dim
2.0.1,For DEBUG
2.0.1,"size = (length, batch, x.size(-1)) \"
2.0.1,"if x.dim() == 3 else (batch, x.size(-1))"
2.0.1,grad_x = x.new(*x.size()) if k_ == 3 else x.new(*size).zero_()
2.0.1,Normal use
2.0.1,"An entry check here, will catch on train side and translate side"
2.0.1,if requirements are not satisfied.
2.0.1,RNNDecoderState wraps hidden as a tuple.
2.0.1,fh -> (layers*directions) x batch x dim
2.0.1,"No encoder in LM, seq2seq count formatting kept"
2.0.1,_check_save_model_path
2.0.1,NOTE: We need to trim the vocab to remove any unk tokens that
2.0.1,were not originally here.
2.0.1,!/usr/bin/env python
2.0.1,!/usr/bin/env python
2.0.1,!/usr/bin/env python
2.0.1,-*- coding: utf-8 -*-
2.0.1,!/usr/bin/env python
2.0.1,!/usr/bin/env python
2.0.1,!/usr/bin/env python
2.0.1,import onmt.opts as opts
2.0.1,Set sharing strategy manually instead of default based on the OS.
2.0.1,"maybe prepare pretrained embeddings, if any"
2.0.1,Load checkpoint if we resume from a previous training.
2.0.1,Report src and tgt vocab sizes
2.0.1,Create a thread to listen for errors in the child processes.
2.0.1,Train with multiprocessing.
2.0.1,"This does not work if we merge with the first loop, not sure why"
2.0.1,Get the iterator to generate from
2.0.1,"Once training is done, we can terminate the producers"
2.0.1,magic indices
2.0.1,result caching
2.0.1,fix length constraint
2.0.1,add one to account for BOS. Don't account for EOS because hitting
2.0.1,this implies it hasn't been found.
2.0.1,we don't block nothing if the user doesn't want it
2.0.1,we can't block nothing beam's too short
2.0.1,we check paths one by one
2.0.1,we don't forbid nothing if the user doesn't want it
2.0.1,we can't forbid nothing if beam's too short
2.0.1,Reordering forbidden_tokens following beam selection
2.0.1,We rebuild a dict to ensure we get the value and not the pointer
2.0.1,Grabing the newly selected tokens and associated ngram
2.0.1,skip the blocking if any token in current_ngram is excluded
2.0.1,"pickups: Tensor where specified index were set to 1, others 0"
2.0.1,"dropdowns: opposite of pickups, 1 for those shouldn't pick"
2.0.1,Minus dropdowns to log_probs making probabilities of
2.0.1,unspecified index close to 0
2.0.1,"prediction step have surpass length of given target_prefix,"
2.0.1,no need to further change this attr
2.0.1,keep indices until overflowing p
2.0.1,Set all logits that are not in the top-p to -10000.
2.0.1,This puts the probabilities close to 0.
2.0.1,Set all logits that are not in the top-k to -10000.
2.0.1,This puts the probabilities close to 0.
2.0.1,"For temp=0.0, take the argmax to avoid divide-by-zero errors."
2.0.1,keep_topk=1 is also equivalent to argmax.
2.0.1,maybe fix some prediction at this step by modifying log_probs
2.0.1,"shape: (sum(~ self.is_finished), 1)"
2.0.1,in LM task memory_lengths is associated with currently generated src
2.0.1,and therefore needs to follow the generation
2.0.1,!/usr/bin/env python
2.0.1,Maintains the longest src and tgt length in the current batch
2.0.1,Reset current longest length at a new batch (count=1)
2.0.1,max_tgt_in_batch = 0
2.0.1,Src: [<bos> w1 ... wN <eos>]
2.0.1,Tgt: [w1 ... wM <eos>]
2.0.1,for debugging
2.0.1,TODO: maybe add dynamic part
2.0.1,Statistics
2.0.1,Turn any copied words into UNKs.
2.0.1,"Decoder forward, takes [tgt_len, batch, nfeats] as input"
2.0.1,"and [src_len, batch, hidden] as memory_bank"
2.0.1,"in case of inference tgt_len = 1, batch = beam times batch_size"
2.0.1,"in case of Gold Scoring tgt_len = actual length, batch = 1 batch"
2.0.1,Generator forward.
2.0.1,"returns [(batch_size x beam_size) , vocab ] when 1 step"
2.0.1,"or [ tgt_len, batch_size, vocab ] when full sentence"
2.0.1,"here we have scores [tgt_lenxbatch, vocab] or [beamxbatch, vocab]"
2.0.1,"returns [(batch_size x beam_size) , vocab ] when 1 step"
2.0.1,"or [ tgt_len, batch_size, vocab ] when full sentence"
2.0.1,(0) add BOS and padding to tgt prediction
2.0.1,(1) Encoder forward.
2.0.1,(2) Repeat src objects `n_best` times.
2.0.1,"We use batch_size x n_best, get ``(src_len, batch * n_best, nfeat)``"
2.0.1,"(3) Init decoder with n_best src,"
2.0.1,"reshape tgt to ``(len, batch * n_best, nfeat)``"
2.0.1,masked_select
2.0.1,get aligned src id for each prediction's valid tgt tokens
2.0.1,TODO: support these blacklisted features
2.0.1,(0) Prep the components of the search.
2.0.1,(1) Run the encoder on the src.
2.0.1,(2) prep decode_strategy. Possibly repeat src objects.
2.0.1,(3) Begin decoding step by step:
2.0.1,Reorder states.
2.0.1,TODO: support these blacklisted features
2.0.1,hack [min_len_batch-1:] because expect <bos>
2.0.1,(0) Prep the components of the search.
2.0.1,(1) split src into src and target_prefix to avoid padding.
2.0.1,(2) init decoder
2.0.1,(3) prep decode_strategy. Possibly repeat src objects.
2.0.1,(4) Begin decoding step by step:
2.0.1,Reorder states.
2.0.1,select indexes in model state/cache
2.0.1,beam parameters
2.0.1,beam state
2.0.1,BoolTensor was introduced in pytorch 1.2
2.0.1,"""global state"" of the old beam"
2.0.1,buffers for the topk scores and 'backpointer'
2.0.1,for testing
2.0.1,maybe fix some prediction at this step by modifying log_probs
2.0.1,Flatten probs into a list of possibilities.
2.0.1,Penalize beams that finished.
2.0.1,"on real data (newstest2017) with the pretrained transformer,"
2.0.1,it's faster to not move this back to the original device
2.0.1,Store finished hypotheses for this batch.
2.0.1,End condition is the top beam finished and we can return
2.0.1,n_best hypotheses.
2.0.1,"If all sentences are translated, no need to go further."
2.0.1,Remove finished batches for the next step.
2.0.1,using integer division to get an integer _B without casting
2.0.1,force the output to be longer than self.min_length
2.0.1,Multiply probs by the beam probability.
2.0.1,"if the sequence ends now, then the penalty is the current"
2.0.1,"length + 1, to include the EOS token"
2.0.1,Avoid any direction that would repeat unwanted ngrams
2.0.1,Pick up candidate token by curr_scores
2.0.1,Recover log probs.
2.0.1,Length penalty is just a scalar. It doesn't matter if it's applied
2.0.1,before or after the topk.
2.0.1,Resolve beam origin and map to batch index flat representation.
2.0.1,Append last prediction.
2.0.1,update global state (step == 1)
2.0.1,update global state (step > 1)
2.0.1,"shape: (batch_size x beam_size, 1)"
2.0.1,in LM task memory_lengths is associated with currently generated src
2.0.1,and therefore needs to follow the generation
2.0.1,in LM task memory_lengths is associated with currently generated src
2.0.1,and therefore needs to follow the generation
2.0.1,Term will be subtracted from probability
2.0.1,Probability will be divided by this
2.0.1,these warnings indicate that either the alpha/beta
2.0.1,"forces a penalty to be a no-op, or a penalty is a no-op but"
2.0.1,the alpha/beta would suggest otherwise.
2.0.1,using some length penalty
2.0.1,using some coverage penalty
2.0.1,!/usr/bin/env python
2.0.1,semaphore doesn't have a timeout arg in Python 2.7
2.0.1,perform a first request to initialize everything
2.0.1,backwards compatibility for confs
2.0.1,every segment becomes a dict for flexibility purposes
2.0.1,NOTE: translator returns lists of `n_best` list
2.0.1,build back results with empty texts
2.0.1,load can be called multiple times: modify copy
2.0.1,output contain alignment
2.0.1,Below are all the different penalty terms implemented so far.
2.0.1,Subtract coverage penalty from topk log probs.
2.0.1,Divide topk log probs by length penalty.
2.0.1,Sorting
2.0.1,Chinese segmentation
2.0.1,Chinese simplify -> Chinese traditional standard
2.0.1,Chinese simplify -> Chinese traditional (HongKong)
2.0.1,Chinese simplify -> Chinese traditional (Taiwan)
2.0.1,Chinese traditional -> Chinese simplify (v1)
2.0.1,Chinese traditional -> Chinese simplify (v2)
2.0.0,!/usr/bin/env python
2.0.0,!/usr/bin/env python
2.0.0,!/usr/bin/env python
2.0.0,!/usr/bin/env python
2.0.0,!/usr/bin/env python
2.0.0,!/usr/bin/env python3
2.0.0,-*- coding: utf-8 -*-
2.0.0,
2.0.0,"OpenNMT-py documentation build configuration file, created by"
2.0.0,sphinx-quickstart on Sun Dec 17 12:07:14 2017.
2.0.0,
2.0.0,This file is execfile()d with the current directory set to its
2.0.0,containing dir.
2.0.0,
2.0.0,Note that not all possible configuration values are present in this
2.0.0,autogenerated file.
2.0.0,
2.0.0,All configuration values have a default; values that are commented out
2.0.0,serve to show the default.
2.0.0,"If extensions (or modules to document with autodoc) are in another directory,"
2.0.0,add these directories to sys.path here. If the directory is relative to the
2.0.0,"documentation root, use os.path.abspath to make it absolute, like shown here."
2.0.0,
2.0.0,import os
2.0.0,import sys
2.0.0,"sys.path.insert(0, os.path.abspath('.'))"
2.0.0,-- General configuration ------------------------------------------------
2.0.0,"If your documentation needs a minimal Sphinx version, state it here."
2.0.0,
2.0.0,needs_sphinx = '1.0'
2.0.0,"Add any Sphinx extension module names here, as strings. They can be"
2.0.0,extensions coming with Sphinx (named 'sphinx.ext.*') or your custom
2.0.0,ones.
2.0.0,Show base classes
2.0.0,"Use ""variables"" section for Attributes instead of weird block things"
2.0.0,mimicking the function style.
2.0.0,"Add any paths that contain templates here, relative to this directory."
2.0.0,The suffix(es) of source filenames.
2.0.0,You can specify multiple suffix as a list of string:
2.0.0,
2.0.0,"source_suffix = ['.rst', '.md']"
2.0.0,The master toctree document.
2.0.0,General information about the project.
2.0.0,"The version info for the project you're documenting, acts as replacement for"
2.0.0,"|version| and |release|, also used in various other places throughout the"
2.0.0,built documents.
2.0.0,
2.0.0,The short X.Y version.
2.0.0,"The full version, including alpha/beta/rc tags."
2.0.0,The language for content autogenerated by Sphinx. Refer to documentation
2.0.0,for a list of supported languages.
2.0.0,
2.0.0,This is also used if you do content translation via gettext catalogs.
2.0.0,"Usually you set ""language"" from the command line for these cases."
2.0.0,"List of patterns, relative to source directory, that match files and"
2.0.0,directories to ignore when looking for source files.
2.0.0,This patterns also effect to html_static_path and html_extra_path
2.0.0,The name of the Pygments (syntax highlighting) style to use.
2.0.0,"If true, `todo` and `todoList` produce output, else they produce nothing."
2.0.0,-- Options for HTML output ----------------------------------------------
2.0.0,The theme to use for HTML and HTML Help pages.  See the documentation for
2.0.0,a list of builtin themes.
2.0.0,
2.0.0,html_theme = 'sphinx_materialdesign_theme'
2.0.0,html_theme_path = [sphinx_materialdesign_theme.get_path()]
2.0.0,Theme options are theme-specific and customize the look and feel of a theme
2.0.0,"further.  For a list of options available for each theme, see the"
2.0.0,documentation.
2.0.0,
2.0.0,html_theme_options = {}
2.0.0,"Add any paths that contain custom static files (such as style sheets) here,"
2.0.0,"relative to this directory. They are copied after the builtin static files,"
2.0.0,"so a file named ""default.css"" will overwrite the builtin ""default.css""."
2.0.0,"Custom sidebar templates, must be a dictionary that maps document names"
2.0.0,to template names.
2.0.0,
2.0.0,This is required for the alabaster theme
2.0.0,refs: http://alabaster.readthedocs.io/en/latest/installation.html#sidebars
2.0.0,-- Options for HTMLHelp output ------------------------------------------
2.0.0,Output file base name for HTML help builder.
2.0.0,-- Options for LaTeX output ---------------------------------------------
2.0.0,The paper size ('letterpaper' or 'a4paper').
2.0.0,
2.0.0,"'papersize': 'letterpaper',"
2.0.0,"The font size ('10pt', '11pt' or '12pt')."
2.0.0,
2.0.0,"'pointsize': '10pt',"
2.0.0,Additional stuff for the LaTeX preamble.
2.0.0,
2.0.0,"'preamble': '',"
2.0.0,Latex figure (float) alignment
2.0.0,
2.0.0,"'figure_align': 'htbp',"
2.0.0,Grouping the document tree into LaTeX files. List of tuples
2.0.0,"(source start file, target name, title,"
2.0.0,"author, documentclass [howto, manual, or own class])."
2.0.0,-- Options for manual page output ---------------------------------------
2.0.0,One entry per manual page. List of tuples
2.0.0,"(source start file, name, description, authors, manual section)."
2.0.0,-- Options for Texinfo output -------------------------------------------
2.0.0,Grouping the document tree into Texinfo files. List of tuples
2.0.0,"(source start file, target name, title, author,"
2.0.0,"dir menu entry, description, category)"
2.0.0,!/usr/bin/env python
2.0.0,-*- coding: utf-8 -*-
2.0.0,is this reachable?
2.0.0,Read in embeddings
2.0.0,Write to file
2.0.0,converts a SentencePiece vocabulary to the format expected by dynamic data
2.0.0,"(essentially converts float expected counts to ""fixed precision"" int pseudo"
2.0.0,counts)
2.0.0,"Add in default model arguments, possibly added since training."
2.0.0,build_base_model expects updated and validated opts
2.0.0,-*- encoding: utf-8 -*-
2.0.0,!/usr/bin/env python
2.0.0,-*- coding: utf-8 -*-
2.0.0,Author: Rico Sennrich
2.0.0,flake8: noqa
2.0.0,This file is retrieved from https://github.com/rsennrich/subword-nmt
2.0.0,hack for python2/3 compatibility
2.0.0,check version information
2.0.0,some hacking to deal with duplicates (only consider first instance)
2.0.0,don't print end-of-word symbols
2.0.0,sys.stderr.write('cannot split {0} further.\n'.format(segment))
2.0.0,sys.stderr.write('OOV: {0}\n'.format(segment))
2.0.0,sys.stderr.write('OOV: {0}\n'.format(segment))
2.0.0,python 2/3 compatibility
2.0.0,read/write files as UTF-8
2.0.0,!/usr/bin/env python
2.0.0,!/usr/bin/env python
2.0.0,-*- coding: utf-8 -*-
2.0.0,Author: Rico Sennrich
2.0.0,flake8: noqa
2.0.0,This file is retrieved from https://github.com/rsennrich/subword-nmt
2.0.0,hack for python2/3 compatibility
2.0.0,"find all instances of pair, and update frequency/indices around it"
2.0.0,find first symbol
2.0.0,"if first symbol is followed by second symbol, we've found an occurrence of pair (old_word[i:i+2])"
2.0.0,"assuming a symbol sequence ""A B C"", if ""B C"" is merged, reduce the frequency of ""A B"""
2.0.0,"assuming a symbol sequence ""A B C B"", if ""B C"" is merged, reduce the frequency of ""C B""."
2.0.0,"however, skip this if the sequence is A B C B C, because the frequency of ""C B"" will be reduced by the previous code block"
2.0.0,find new pair
2.0.0,"assuming a symbol sequence ""A BC D"", if ""B C"" is merged, increase the frequency of ""A BC"""
2.0.0,"assuming a symbol sequence ""A BC B"", if ""B C"" is merged, increase the frequency of ""BC B"""
2.0.0,"however, if the sequence is A BC BC, skip this step because the count of ""BC BC"" will be incremented by the previous code block"
2.0.0,data structure of pair frequencies
2.0.0,index from pairs to words
2.0.0,version 0.2 changes the handling of the end-of-word token ('</w>');
2.0.0,version numbering allows bckward compatibility
2.0.0,"threshold is inspired by Zipfian assumption, but should only affect speed"
2.0.0,we probably missed the best pair because of pruning; go back to full statistics
2.0.0,"threshold is inspired by Zipfian assumption, but should only affect speed"
2.0.0,python 2/3 compatibility
2.0.0,read/write files as UTF-8
2.0.0,!/usr/bin/env python
2.0.0,-*- coding: utf-8 -*-
2.0.0,!/usr/bin/env python
2.0.0,Build embeddings.
2.0.0,Build encoder.
2.0.0,Build embeddings.
2.0.0,Build decoder.
2.0.0,Share the embedding matrix - preprocess with share_vocab required.
2.0.0,src/tgt vocab should be the same if `-share_vocab` is specified.
2.0.0,for back compat when attention_dropout was not defined
2.0.0,Build Model
2.0.0,Build Generator.
2.0.0,Load the model states from checkpoint or initialize them.
2.0.0,This preserves backward-compat for models using customed layernorm
2.0.0,end of patch for backward compatibility
2.0.0,!/usr/bin/env python
2.0.0,ensure tensorboard output is written in the directory
2.0.0,of previous checkpoints
2.0.0,NOTE: It's important that ``opt`` has been validated and updated
2.0.0,at this point.
2.0.0,Build model.
2.0.0,Build optimizer.
2.0.0,Build model saver
2.0.0,Move batch to specified device
2.0.0,Use Tensorboard for visualization during training
2.0.0,Options only during inference
2.0.0,"Truncation options, for text corpus"
2.0.0,"as for False, this will be added in _add_train_general_opts"
2.0.0,Embedding Options
2.0.0,Model Task Options
2.0.0,Encoder-Decoder Options
2.0.0,"group.add('--residual', '-residual',   action=""store_true"","
2.0.0,"help=""Add residual connections between RNN layers."")"
2.0.0,The following options (bridge_extra_node to src_vocab) are used
2.0.0,for training with --encoder_type ggnn (Gated Graph Neural Network).
2.0.0,Attention options
2.0.0,Alignement options
2.0.0,Generator and loss options.
2.0.0,GPU
2.0.0,Init options
2.0.0,Pretrained word vectors
2.0.0,Freeze word vectors
2.0.0,Optimization options
2.0.0,learning rate
2.0.0,options relate to data preprare
2.0.0,options relate to train
2.0.0,Alpha and Beta values for Google Length + Coverage penalty
2.0.0,"Described here: https://arxiv.org/pdf/1609.08144.pdf, Section 7"
2.0.0,Adding options relate to decoding strategy
2.0.0,Adding option for logging
2.0.0,Copyright 2016 The Chromium Authors. All rights reserved.
2.0.0,Use of this source code is governed by a BSD-style license that can be
2.0.0,found in the LICENSE file.
2.0.0,"Get the key 'value' in the dict, or just use 'value'"
2.0.0,Basic attributes.
2.0.0,Set model in training mode.
2.0.0,UPDATE DROPOUT
2.0.0,Run patience mechanism
2.0.0,"If the patience has reached the limit, stop training"
2.0.0,swap model params w/ moving average
2.0.0,(and keep the original parameters)
2.0.0,Set model in validating mode.
2.0.0,F-prop through the model.
2.0.0,Compute loss.
2.0.0,Update statistics.
2.0.0,Set model back to training mode.
2.0.0,Truncated BPTT: reminder not compatible with accum > 1
2.0.0,1. Create truncated target.
2.0.0,2. F-prop all but generator.
2.0.0,3. Compute loss.
2.0.0,4. Update the parameters and statistics.
2.0.0,Multi GPU gradient gather
2.0.0,"If truncated, don't backprop fully."
2.0.0,TO CHECK
2.0.0,if dec_state is not None:
2.0.0,dec_state.detach()
2.0.0,"in case of multi step gradient accumulation,"
2.0.0,update only after accum batches
2.0.0,For Flake
2.0.0,we avoid padding while mean pooling
2.0.0,incoming and outgoing edge embedding
2.0.0,Find vocab data for tree builting
2.0.0,Propogation Model
2.0.0,Initialize the bridge layer
2.0.0,Initialize graph using formatted input sequence
2.0.0,Number of flagged nodes defines node count for this sample
2.0.0,"(Nodes can have no flags on them, but must be in 'flags' list)."
2.0.0,The total number of integers in the vocab should allow
2.0.0,for all features and edges to be defined.
2.0.0,Use first extra node as only source for decoder init
2.0.0,Average all nodes to get bridge input
2.0.0,"LSTM has hidden and cell state, other only one"
2.0.0,Total number of states
2.0.0,Build a linear layer for each
2.0.0,Initialize the bridge layer
2.0.0,"s_len, batch, emb_dim = emb.size()"
2.0.0,Lengths data is wrapped inside a Tensor.
2.0.0,"LSTM has hidden and cell state, other only one"
2.0.0,Total number of states
2.0.0,Build a linear layer for each
2.0.0,"s_len, batch, emb_dim = emb.size()"
2.0.0,Run the forward pass of every layer of the tranformer.
2.0.0,Dimensions and padding for constructing the word embedding matrix
2.0.0,Dimensions and padding for feature embedding matrices
2.0.0,(these have no effect if feat_vocab_sizes is empty)
2.0.0,The embedding matrix look-up tables. The first look-up table
2.0.0,"is for words. Subsequent ones are for features, if any exist."
2.0.0,The final output size of word + feature vectors. This can vary
2.0.0,from the word vector size if and only if features are defined.
2.0.0,This is the attribute you should access if you need to know
2.0.0,how big your embeddings are going to be.
2.0.0,The sequence of operations that converts the input sequence
2.0.0,into a sequence of embeddings. At minimum this consists of
2.0.0,looking up the embeddings for each word and feature in the
2.0.0,input. Model parameters may require the sequence to contain
2.0.0,additional operations as well.
2.0.0,features must use word_vec_size
2.0.0,features will use feat_vec_size
2.0.0,Some utilitary functions for pretrained embeddings
2.0.0,is this reachable?
2.0.0,Write to file
2.0.0,set the opt in place
2.0.0,set the opt in place
2.0.0,This class is mainly used by decoder.py for RNNs but also
2.0.0,by the CNN / transformer decoder when copy attention is used
2.0.0,CNN has its own attention mechanism ConvMultiStepAttention
2.0.0,Transformer has its own MultiHeadedAttention
2.0.0,mlp wants it with bias
2.0.0,Check input sizes
2.0.0,"(batch, t_len, d) x (batch, d, s_len) --> (batch, t_len, s_len)"
2.0.0,"(batch, t_len, s_len, d)"
2.0.0,one step input
2.0.0,"compute attention scores, as in Luong et al."
2.0.0,Softmax or sparsemax to normalize attention weights
2.0.0,each context vector c_t is the weighted average
2.0.0,over all the source hidden states
2.0.0,concatenate
2.0.0,Check output sizes
2.0.0,Check output sizes
2.0.0,clamping necessary because of numerical errors: loss should be lower
2.0.0,"bounded by zero, but negative values near zero are possible without"
2.0.0,the clamp
2.0.0,from onmt.utils.misc import aeq
2.0.0,CHECKS
2.0.0,"batch, k_len, d = key.size()"
2.0.0,"batch_, k_len_, d_ = value.size()"
2.0.0,"aeq(batch, batch_)"
2.0.0,"aeq(k_len, k_len_)"
2.0.0,"aeq(d, d_)"
2.0.0,"batch_, q_len, d_ = query.size()"
2.0.0,"aeq(batch, batch_)"
2.0.0,"aeq(d, d_)"
2.0.0,"aeq(self.model_dim % 8, 0)"
2.0.0,if mask is not None:
2.0.0,"batch_, q_len_, k_len_ = mask.size()"
2.0.0,"aeq(batch_, batch)"
2.0.0,"aeq(k_len_, k_len)"
2.0.0,aeq(q_len_ == q_len)
2.0.0,END CHECKS
2.0.0,"1) Project key, value, and query."
2.0.0,1 or key_len x key_len
2.0.0,1 or key_len x key_len x dim_per_head
2.0.0,1 or key_len x key_len x dim_per_head
2.0.0,2) Calculate and scale scores.
2.0.0,batch x num_heads x query_len x key_len
2.0.0,3) Apply attention dropout and compute context vectors.
2.0.0,CHECK
2.0.0,"batch_, q_len_, d_ = output.size()"
2.0.0,"aeq(q_len, q_len_)"
2.0.0,"aeq(batch, batch_)"
2.0.0,"aeq(d, d_)"
2.0.0,Return multi-head attn
2.0.0,At the moment this class is only used by embeddings.Embeddings look-up tables
2.0.0,-*- coding: utf-8 -*-
2.0.0,checks
2.0.0,"batch, channel, height, width = base_target_emb.size()"
2.0.0,"batch_, channel_, height_, width_ = input_from_dec.size()"
2.0.0,"enc_batch, enc_channel, enc_height = encoder_out_top.size()"
2.0.0,"enc_batch_, enc_channel_, enc_height_ = encoder_out_combine.size()"
2.0.0,out_features * in_features
2.0.0,norm is out_features * 1
2.0.0,batch_size * out_features
2.0.0,out_features
2.0.0,out_features
2.0.0,batch_size * out_features
2.0.0,"out_channels, in_channels // groups, * kernel_size"
2.0.0,out_features
2.0.0,This is used nowhere in the code at the moment (Vincent Nguyen 05/18/2018)
2.0.0,"in_channels, out_channels, *kernel_size"
2.0.0,"in_channels, out_channels, *kernel_size"
2.0.0,"self.out_channels, 1"
2.0.0,out_features
2.0.0,out_features
2.0.0,store roots on diagonal
2.0.0,CHECKS
2.0.0,Original probabilities.
2.0.0,Probability of copying p(z=1) batch.
2.0.0,Probability of not copying: p_{word}(w) * (1 - p(z))
2.0.0,probabilities assigned by the model to the gold targets
2.0.0,probability of tokens copied from source
2.0.0,Set scores for unk to 0 and add eps
2.0.0,find the indices in which you do not use the copy mechanism
2.0.0,Drop padding.
2.0.0,this block does not depend on the loss value computed above
2.0.0,and is used only for stats
2.0.0,this block does not depend on the loss value computed above
2.0.0,and is used only for stats
2.0.0,Correct target copy token instead of <unk>
2.0.0,tgt[i] = align[i] + len(tgt_vocab)
2.0.0,for i such that tgt[i] == 0 and align[i] != 0
2.0.0,Compute sum of perplexities for stats
2.0.0,this part looks like it belongs in CopyGeneratorLoss
2.0.0,Compute Loss as NLL divided by seq length
2.0.0,Compute Total Loss per sequence in batch
2.0.0,Divide by length of each sequence and sum
2.0.0,Auto import python files in this directory
2.0.0,1. sample number of tokens to corrupt
2.0.0,2. sample positions to corrput
2.0.0,3. sample corrupted values
2.0.0,1. sample number of tokens to corrupt
2.0.0,2. sample positions to corrput
2.0.0,3. Drop token on chosen position
2.0.0,1. sample number of tokens to corrupt
2.0.0,2. sample positions to corrput
2.0.0,3. mask word on chosen position
2.0.0,"Sharing options among `TokenizerTransform`s, same name conflict in"
2.0.0,this scope will be resolved by remove previous occurrence in parser
2.0.0,subword regularization(or BPE dropout) options:
2.0.0,subword vocabulary restriction options:
2.0.0,derterministic subwording
2.0.0,subword sampling when nbest_size > 1 or -1
2.0.0,alpha should be 0.0 < alpha < 1.0
2.0.0,-1: keep everything (i.e. 1 mask per token)
2.0.0,0: replace everything (i.e. no mask)
2.0.0,1: 1 mask per span
2.0.0,view each subword as word start / input is word level token
2.0.0,Pretend it ends with a full stop so last span is a sentence
2.0.0,"Tokens that are full stops, where the previous token is not"
2.0.0,Make sure we have enough to mask
2.0.0,Trim to masking budget
2.0.0,Handle 0-length mask (inserts) separately
2.0.0,assert is_word_start[-1] == 0
2.0.0,assert tokens_length - 1 not in indices
2.0.0,"keep index, but replace it with [MASK]"
2.0.0,"acts as a long length, so spans don't go over the end of doc"
2.0.0,next position from each word_start
2.0.0,delete token: 1 mask/remove per span
2.0.0,"keep index, but replace it with [MASK]: 1 mask per token"
2.0.0,A bit faster when all lengths are 1
2.0.0,to cover whole token
2.0.0,delete token
2.0.0,"keep index, but replace it with [MASK]"
2.0.0,assert tokens_length - 1 not in indices
2.0.0,initialize fields at the top of each unit test to prevent
2.0.0,any undesired stateful effects
2.0.0,"this test touches the file system, so it could be considered an"
2.0.0,integration test
2.0.0,write utf-8 bytes
2.0.0,batch 0 will always predict EOS. The other batches will predict
2.0.0,non-eos scores.
2.0.0,"""best"" prediction is eos - that should be blocked"
2.0.0,include at least one prediction OTHER than EOS
2.0.0,that is greater than -1e20
2.0.0,now batch 0 has ended and no others have
2.0.0,initial step
2.0.0,batch 0 dies on step 0
2.0.0,include at least one prediction OTHER than EOS
2.0.0,that is greater than -1e20
2.0.0,step 2
2.0.0,(old) batch 8 dies on step 1
2.0.0,step 3
2.0.0,everything dies
2.0.0,initial step
2.0.0,batch 0 dies on step 0
2.0.0,include at least one prediction OTHER than EOS
2.0.0,that is greater than -1e20
2.0.0,step 2
2.0.0,(old) batch 8 dies on step 1
2.0.0,step 3
2.0.0,everything dies
2.0.0,initial step
2.0.0,finish one beam
2.0.0,include at least one prediction OTHER than EOS
2.0.0,that is greater than -1e20
2.0.0,step 2
2.0.0,finish example in last batch
2.0.0,(old) batch 8 dies on step 1
2.0.0,step 3
2.0.0,everything dies
2.0.0,initial step
2.0.0,batch 0 dies on step 0
2.0.0,include at least one prediction OTHER than EOS
2.0.0,that is greater than -1e20
2.0.0,step 2
2.0.0,(old) batch 8 dies on step 1
2.0.0,step 3
2.0.0,everything dies
2.0.0,illegal_weights_mask = torch.ByteTensor([
2.0.0,"[0, 0, 0, 0, 0, 0, 0],"
2.0.0,"[0, 0, 0, 1, 1, 1, 1],"
2.0.0,"[0, 0, 0, 0, 0, 1, 1],"
2.0.0,"[0, 0, 1, 1, 1, 1, 1]])"
2.0.0,TODO: fix for pytorch 0.3
2.0.0,illegal_weights = alignments.masked_select(illegal_weights_mask)
2.0.0,"self.assertEqual(0.0, illegal_weights.data.sum())"
2.0.0,this could be considered an integration test because it touches
2.0.0,the filesystem for the config file (and the models)
2.0.0,!/usr/bin/env python
2.0.0,-*- coding: utf-8 -*-
2.0.0,Inject some dummy training options that may needed when build fields
2.0.0,Remove the generated *pt files.
2.0.0,Remove the generated data samples
2.0.0,all beams repeat (beam >= 1 repeat dummy scores)
2.0.0,predict repeat_idx over and over again
2.0.0,"before repeat, scores are either 0 or -inf"
2.0.0,"on repeat, `repeat_idx` score is BLOCKED_SCORE"
2.0.0,"(but it's still the best score, thus we have"
2.0.0,"[BLOCKED_SCORE, -inf, -inf, -inf, -inf]"
2.0.0,repetitions keeps maximizing score
2.0.0,"index 0 has been blocked, so repeating=>+0.0 score"
2.0.0,other indexes are -inf so repeating=>BLOCKED_SCORE
2.0.0,which is higher
2.0.0,beam 0 and beam >=2 will repeat (beam >= 2 repeat dummy scores)
2.0.0,non-interesting beams are going to get dummy values
2.0.0,"on initial round, only predicted scores for beam 0"
2.0.0,matter. Make two predictions. Top one will be repeated
2.0.0,"in beam zero, second one will live on in beam 1."
2.0.0,predict the same thing in beam 0
2.0.0,continue pushing around what beam 1 predicts
2.0.0,"now beam 0 dies (along with the others), beam 1 -> beam 0"
2.0.0,"now beam 0 dies (along with the others), beam 1 -> beam 0"
2.0.0,beam 0 and beam >= 2 will repeat (beam 2 repeats excluded idx)
2.0.0,non-interesting beams are going to get dummy values
2.0.0,predict the same thing in beam 0
2.0.0,continue pushing around what beam 1 predicts
2.0.0,predict the allowed-repeat again in beam 2
2.0.0,"now beam 0 dies, beam 1 -> beam 0, beam 2 -> beam 1"
2.0.0,and the rest die
2.0.0,"since all preds after i=0 are 0, we can check"
2.0.0,that the beam is the correct idx by checking that
2.0.0,the curr score is the initial score
2.0.0,beam 0 will always predict EOS. The other beams will predict
2.0.0,non-eos scores.
2.0.0,non-interesting beams are going to get dummy values
2.0.0,"""best"" prediction is eos - that should be blocked"
2.0.0,include at least beam_sz predictions OTHER than EOS
2.0.0,that are greater than -1e20
2.0.0,predict eos in beam 0
2.0.0,provide beam_sz other good predictions
2.0.0,now the top beam has ended and no others have
2.0.0,"not of interest, but want to make sure it keeps running"
2.0.0,since only beam 0 terminates and n_best = 2
2.0.0,"this is also a test that when block_ngram_repeat=0,"
2.0.0,repeating is acceptable
2.0.0,non-interesting beams are going to get dummy values
2.0.0,"""best"" prediction is eos - that should be blocked"
2.0.0,include at least beam_sz predictions OTHER than EOS
2.0.0,that are greater than -1e20
2.0.0,predict eos in beam 1
2.0.0,provide beam_sz other good predictions in other beams
2.0.0,provide beam_sz other good predictions in other beams
2.0.0,beam 1 dies on min_length
2.0.0,beam 0 dies on the step after beam 1 dies
2.0.0,"inp_lens is tiled in initialize, reassign to make attn match"
2.0.0,non-interesting beams are going to get dummy values
2.0.0,"""best"" prediction is eos - that should be blocked"
2.0.0,include at least beam_sz predictions OTHER than EOS
2.0.0,that are greater than -1e20
2.0.0,predict eos in beam 1
2.0.0,provide beam_sz other good predictions in other beams
2.0.0,provide beam_sz other good predictions in other beams
2.0.0,no top beams are finished yet
2.0.0,beam 1 dies on min_length
2.0.0,no top beams are finished yet
2.0.0,beam 0 dies on the step after beam 1 dies
2.0.0,top beam is finished now so there are attentions
2.0.0,two beams are finished in each batch
2.0.0,second dim is cut down to the non-padded src length
2.0.0,first dim is equal to the time of death
2.0.0,(beam 0 died at current step - adjust for SOS)
2.0.0,(beam 1 died at last step - adjust for SOS)
2.0.0,behavior gets weird when beam is already done so just stop
2.0.0,this is just test_beam.TestBeamAgainstReferenceCase repeated
2.0.0,in each batch.
2.0.0,"init_preds: [4, 3, 5, 6, 7] - no EOS's"
2.0.0,no EOS's yet
2.0.0,"[5, 3, 2, 6, 0], so beam 2 predicts EOS!"
2.0.0,assumes beam 2 finished on last step
2.0.0,ended beam 2 shouldn't continue
2.0.0,"[2, 5, 3, 6, 0] repeat self.BATCH_SZ, so beam 0 predicts EOS!"
2.0.0,"[-2.4879, -3.8910, -4.1010, -4.2010, -4.4010] repeat self.BATCH_SZ"
2.0.0,another beam is finished in all batches
2.0.0,new beam 0 finished
2.0.0,new beam 0 is old beam 3
2.0.0,assumes beam 0 finished on last step
2.0.0,"[5, 2, 6, 1, 0] repeat self.BATCH_SZ, so beam 1 predicts EOS!"
2.0.0,new beam 1 finished
2.0.0,new beam 1 is old beam 4
2.0.0,this could be considered an integration test because it tests
2.0.0,interactions between the GNMT scorer and the beam
2.0.0,"-data option is required, but not used in this test, so dummy."
2.0.0,len x batch x nfeat
2.0.0,Initialize vectors to compare size with
2.0.0,Ensure correct sizes and types
2.0.0,Make sure that output has the correct size and type
2.0.0,"[('encoder_type', 'transformer'),"
2.0.0,"('word_vec_size', 16), ('rnn_size', 16)],"
2.0.0,""""""" Only do SRU test if requirment is safisfied. """""""
2.0.0,SRU doesn't support input_feed.
2.0.0,first check there's nothing unexpectedly not trainable
2.0.0,ok: word embeddings shouldn't be trainable
2.0.0,if word vecs are freezed
2.0.0,ok: positional encodings shouldn't be trainable
2.0.0,then check nothing unexpectedly trainable
2.0.0,Decoder state
2.0.0,Build the RNN.
2.0.0,Set up the context gate.
2.0.0,Set up the standard attention.
2.0.0,The encoder hidden is  (layers*directions) x batch x dim.
2.0.0,We need to convert it to layers x batch x (directions*dim).
2.0.0,Init the input feed.
2.0.0,Update the state with the result.
2.0.0,Concatenates sequence of tensors along a new dimension.
2.0.0,NOTE: v0.3 to 0.4: dec_outs / attns[*] may not be list
2.0.0,(in particular in case of SRU) it was not raising error in 0.3
2.0.0,since stack(Variable) was allowed.
2.0.0,"In 0.4, SRU returns a tensor that shouldn't be stacke"
2.0.0,Check
2.0.0,Calculate the attention.
2.0.0,Calculate the context gate.
2.0.0,Additional args check.
2.0.0,END Additional args check.
2.0.0,Input feed concatenates hidden state with
2.0.0,input at every time step.
2.0.0,TODO: context gate should be employed
2.0.0,instead of second RNN transform.
2.0.0,Update the coverage attention.
2.0.0,Decoder State
2.0.0,CNNDecoder has its own attention mechanism.
2.0.0,Set up a separate copy attention layer if needed.
2.0.0,The output of CNNEncoder.
2.0.0,The combination of output of CNNEncoder and source embeddings.
2.0.0,Process the result and update the attentions.
2.0.0,Update the state.
2.0.0,TODO change the way attns is returned dict => list or tuple (onnx)
2.0.0,Memory_lengths is a single tensor shared between all models.
2.0.0,This assumption will not hold if Translator is modified
2.0.0,to calculate memory_lengths as something other than the length
2.0.0,of the input.
2.0.0,"return _, (B, Q_len, K_len)"
2.0.0,"layer average attention across heads, get ``(B, Q, K)``"
2.0.0,"Case 1: no full_context, no align heads -> layer avg baseline"
2.0.0,"Case 2: no full_context, 1 align heads -> guided align"
2.0.0,"Case 3: full_context, 1 align heads -> full cte guided align"
2.0.0,BoolTensor was introduced in pytorch 1.2
2.0.0,T: could be 1 in the case of stepwise decoding or tgt_len
2.0.0,masking is necessary when sequence length is greater than one
2.0.0,Decoder State
2.0.0,"previously, there was a GlobalAttention module here for copy"
2.0.0,"attention. But it was never actually used -- the ""copy"" attention"
2.0.0,just reuses the context attention.
2.0.0,"attns[""align""] = torch.stack(attn_aligns, 0).mean(0)  # All avg"
2.0.0,TODO change the way attns is returned dict => list or tuple (onnx)
2.0.0,T: could be 1 in the case of stepwise decoding or tgt_len
2.0.0,masking is necessary when sequence length is greater than one
2.0.0,TODO change the way attns is returned dict => list or tuple (onnx)
2.0.0,"buffer size in bytes, determine equiv. # of elements based on data type"
2.0.0,copy tensors into buffer_t
2.0.0,all-reduce and rescale
2.0.0,copy all-reduced buffer back into tensors
2.0.0,"tensor is bigger than buffer, all-reduce and rescale directly"
2.0.0,"buffer is full, all-reduce and replace buffer with grad"
2.0.0,add tensor to buffer
2.0.0,NOTE: stride (if needed) is handled at the
2.0.0,generator (train_iter) level
2.0.0,Move batch to correspond device_id when consumer iterate
2.0.0,hack to dodge unpicklable `dict_keys`
2.0.0,"propagate exception to parent process, keeping original traceback"
2.0.0,TODO: Find a better way to check for sparse gradients.
2.0.0,we use here a FusedAdam() copy of an old Apex repo
2.0.0,In this case use the old FusedAdam with FP16_optimizer wrapper
2.0.0,Load everything from the checkpoint.
2.0.0,Build everything from scratch.
2.0.0,"Reset optimizer, keep options."
2.0.0,"Reset options, keep optimizer."
2.0.0,State can be partially restored.
2.0.0,"unscaled optimizer's gradients (already done therefore skip),"
2.0.0,skips optimizer.step() if gradients contain infs/NaNs.
2.0.0,Updates the scale for next iteration.
2.0.0,Code below is an implementation of https://arxiv.org/pdf/1804.04235.pdf
2.0.0,inspired but modified from https://github.com/DeadAt0m/adafactor-pytorch
2.0.0,backward compatibility
2.0.0,assuming a list/generator of parameter means single group
2.0.0,compute combined scale factor for this group
2.0.0,norm is in fact norm*scale
2.0.0,note: p.grad should not ever be set for correct operation of
2.0.0,mixed precision optimizer that sometimes sends None gradients
2.0.0,State initialization
2.0.0,Exponential moving average of gradient values
2.0.0,Exponential moving average of squared gradient values
2.0.0,-*- coding: utf-8 -*-
2.0.0,if the loss function operates on vectors of raw logits instead of
2.0.0,"probabilities, only the first part of the generator needs to be"
2.0.0,"passed to the NMTLossCompute. At the moment, the only supported"
2.0.0,loss function of this kind is the sparsemax loss.
2.0.0,"attn_align should be in (batch_size, pad_tgt_size, pad_src_size)"
2.0.0,"align_idx should be a Tensor in size([N, 3]), N is total number"
2.0.0,"of align src-tgt pair in current batch, each as"
2.0.0,"['sent_NÂ°_in_batch', 'tgt_id+1', 'src_id'] (check AlignField)"
2.0.0,NOTE: tgt-src ref alignement that in range_ of shard
2.0.0,(coherent with batch.tgt)
2.0.0,"align_head contains value in [0, 1) presenting attn prob,"
2.0.0,0 was resulted by the context attention src_pad_mask
2.0.0,"So, the correspand position in ref_align should also be 0"
2.0.0,"Therefore, clip align_head to > 1e-18 should be bias free."
2.0.0,non_none: the subdict of the state dictionary where the values
2.0.0,are not None.
2.0.0,"Now, the iteration:"
2.0.0,state is a dictionary of sequences of tensor-like but we
2.0.0,want a sequence of dictionaries of tensors.
2.0.0,"First, unzip the dictionary into a sequence of keys and a"
2.0.0,sequence of tensor-like sequences.
2.0.0,"Now, yield a dictionary for each shard. The keys are always"
2.0.0,the same. values is a sequence of length #keys where each
2.0.0,element is a sequence of length #shards. We want to iterate
2.0.0,"over the shards, not over the keys: therefore, the values need"
2.0.0,to be re-zipped by shard and then each shard can be paired
2.0.0,with the keys.
2.0.0,Assumed backprop'd
2.0.0,Check Transforms
2.0.0,Check path
2.0.0,tgt is src for LM task
2.0.0,Check prefix: will be used when use prefix transform
2.0.0,Check weight
2.0.0,validation when train:
2.0.0,Check embeddings stuff
2.0.0,"Backward compatibility with ""fix_word_vecs_*"" opts"
2.0.0,encoder and decoder should be same sizes
2.0.0,"Load default opt values, then overwrite with the opts in"
2.0.0,"the checkpoint. That way, if there are new options added,"
2.0.0,the defaults are used.
2.0.0,Don't do anything
2.0.0,Update best score of each criteria
2.0.0,Reset tolerance
2.0.0,Update current status
2.0.0,Decrease tolerance
2.0.0,Log
2.0.0,Log
2.0.0,Get a list of world_size lists with len(stat_list) Statistics objects
2.0.0,SRU doesn't support PackedSequence.
2.0.0,-*- coding: utf-8 -*-
2.0.0,threshold on 1 to avoid div by 0
2.0.0,treat alignment matrix one by one as each have different lengths
2.0.0,No alignment if not exist valid tgt token
2.0.0,get valid alignment (sub-matrix from full paded aligment matrix)
2.0.0,-*- coding: utf-8 -*-
2.0.0,this one is needed for torchtext random call (shuffled iterator)
2.0.0,in multi gpu it ensures datasets are read in the same order
2.0.0,some cudnn methods can be random even after fixing the seed
2.0.0,unless you tell it to be deterministic
2.0.0,This one is needed for various tranfroms
2.0.0,These ensure same initialization in multi gpu mode
2.0.0,Shift values to be >= 0
2.0.0,we need to check the model path + any tokenizer path
2.0.0,fast-forward if loaded from state
2.0.0,NOTE: `rnn.pack_padded_sequence` requires that a
2.0.0,"minibatch be sorted by decreasing order, which"
2.0.0,requires reversing relative to typical sort keys
2.0.0,Maintains the longest src and tgt length in the current batch
2.0.0,Reset current longest length at a new batch (count=1)
2.0.0,Src: [<bos> w1 ... wN <eos>]
2.0.0,Tgt: [w1 ... wM <eos>]
2.0.0,coding: utf-8
2.0.0,make a small vocab containing just the tokens in the source sequence
2.0.0,add init_token and eos_token according to src construction
2.0.0,Map source tokens to indices in the dynamic dict.
2.0.0,self.src_vocabs is used in collapse_copy_scores and Translator.py
2.0.0,this assumes src_field and tgt_field are both text
2.0.0,fields needs to have only keys that examples have as attrs
2.0.0,avoid infinite recursion when fields isn't defined
2.0.0,this is a hack: appears quicker to apply it here
2.0.0,than in the ParallelCorpusIterator
2.0.0,NOTE: moved to DatasetAdapter._process method in iterator.py
2.0.0,item = self.transform.apply(
2.0.0,"example, is_train=self.infinitely, corpus_name=self.cid)"
2.0.0,empty example: skip
2.0.0,-*- coding: utf-8 -*-
2.0.0,backwards compatibility
2.0.0,monkey-patch to make torchtext Vocab's pickleable
2.0.0,"+1 for tgt side to keep coherent after ""bos"" padding,"
2.0.0,"register ['NÂ°_in_batch', 'tgt_id+1', 'src_id']"
2.0.0,this is basically copy-pasted from torchtext.
2.0.0,counters changes in place
2.0.0,keep the order of tokens specified in the vocab file by
2.0.0,adding them to the counter with decreasing counting values
2.0.0,`tgt_vocab_size` is ignored when sharing vocabularies
2.0.0,return vocab to dump with standard name
2.0.0,empty train_dataset_files so that vocab is only loaded from
2.0.0,"given paths in src_vocab_path, tgt_vocab_path"
2.0.0,Load vocabulary
2.0.0,Drop the none-using from memory but keep the last
2.0.0,"in the long run, shouldn't it be possible to do this by calling"
2.0.0,build_vocab with both the src and tgt data?
2.0.0,coding: utf-8
2.0.0,several data readers need optional dependencies. There's no
2.0.0,appropriate builtin exception
2.0.0,NOTE: not support nfeats > 0 yet
2.0.0,-*- coding: utf-8 -*-
2.0.0,mix this with partial
2.0.0,batch (list(list(list))): batch_size x len(self.fields) x seq_len
2.0.0,lengths: batch_size
2.0.0,data: seq_len x batch_size x len(self.fields)
2.0.0,flake8: noqa
2.0.0,For command-line option parsing
2.0.0,"Check pass, set the args."
2.0.0,"This SRU version implements its own cuda-level optimization,"
2.0.0,so it requires that:
2.0.0,1. `cupy` and `pynvrtc` python package installed.
2.0.0,2. pytorch is built with cuda support.
2.0.0,3. library path set: export LD_LIBRARY_PATH=<cuda lib path>.
2.0.0,Check 1.
2.0.0,Check 2.
2.0.0,Check 3.
2.0.0,This sets up device to use.
2.0.0,-> directions x batch x dim
2.0.0,For DEBUG
2.0.0,"size = (length, batch, x.size(-1)) \"
2.0.0,"if x.dim() == 3 else (batch, x.size(-1))"
2.0.0,grad_x = x.new(*x.size()) if k_ == 3 else x.new(*size).zero_()
2.0.0,Normal use
2.0.0,"An entry check here, will catch on train side and translate side"
2.0.0,if requirements are not satisfied.
2.0.0,RNNDecoderState wraps hidden as a tuple.
2.0.0,fh -> (layers*directions) x batch x dim
2.0.0,"No encoder in LM, seq2seq count formatting kept"
2.0.0,_check_save_model_path
2.0.0,NOTE: We need to trim the vocab to remove any unk tokens that
2.0.0,were not originally here.
2.0.0,!/usr/bin/env python
2.0.0,!/usr/bin/env python
2.0.0,!/usr/bin/env python
2.0.0,-*- coding: utf-8 -*-
2.0.0,!/usr/bin/env python
2.0.0,!/usr/bin/env python
2.0.0,!/usr/bin/env python
2.0.0,import onmt.opts as opts
2.0.0,Set sharing strategy manually instead of default based on the OS.
2.0.0,"maybe prepare pretrained embeddings, if any"
2.0.0,Load checkpoint if we resume from a previous training.
2.0.0,Report src and tgt vocab sizes
2.0.0,Create a thread to listen for errors in the child processes.
2.0.0,Train with multiprocessing.
2.0.0,"This does not work if we merge with the first loop, not sure why"
2.0.0,Get the iterator to generate from
2.0.0,"Once training is done, we can terminate the producers"
2.0.0,magic indices
2.0.0,result caching
2.0.0,fix length constraint
2.0.0,add one to account for BOS. Don't account for EOS because hitting
2.0.0,this implies it hasn't been found.
2.0.0,we don't block nothing if the user doesn't want it
2.0.0,we can't block nothing beam's too short
2.0.0,we check paths one by one
2.0.0,we don't forbid nothing if the user doesn't want it
2.0.0,we can't forbid nothing if beam's too short
2.0.0,Reordering forbidden_tokens following beam selection
2.0.0,We rebuild a dict to ensure we get the value and not the pointer
2.0.0,Grabing the newly selected tokens and associated ngram
2.0.0,skip the blocking if any token in current_ngram is excluded
2.0.0,"pickups: Tensor where specified index were set to 1, others 0"
2.0.0,"dropdowns: opposite of pickups, 1 for those shouldn't pick"
2.0.0,Minus dropdowns to log_probs making probabilities of
2.0.0,unspecified index close to 0
2.0.0,"prediction step have surpass length of given target_prefix,"
2.0.0,no need to further change this attr
2.0.0,keep indices until overflowing p
2.0.0,Set all logits that are not in the top-p to -10000.
2.0.0,This puts the probabilities close to 0.
2.0.0,Set all logits that are not in the top-k to -10000.
2.0.0,This puts the probabilities close to 0.
2.0.0,"For temp=0.0, take the argmax to avoid divide-by-zero errors."
2.0.0,keep_topk=1 is also equivalent to argmax.
2.0.0,maybe fix some prediction at this step by modifying log_probs
2.0.0,"shape: (sum(~ self.is_finished), 1)"
2.0.0,in LM task memory_lengths is associated with currently generated src
2.0.0,and therefore needs to follow the generation
2.0.0,!/usr/bin/env python
2.0.0,Maintains the longest src and tgt length in the current batch
2.0.0,Reset current longest length at a new batch (count=1)
2.0.0,max_tgt_in_batch = 0
2.0.0,Src: [<bos> w1 ... wN <eos>]
2.0.0,Tgt: [w1 ... wM <eos>]
2.0.0,for debugging
2.0.0,TODO: maybe add dynamic part
2.0.0,Statistics
2.0.0,Turn any copied words into UNKs.
2.0.0,"Decoder forward, takes [tgt_len, batch, nfeats] as input"
2.0.0,"and [src_len, batch, hidden] as memory_bank"
2.0.0,"in case of inference tgt_len = 1, batch = beam times batch_size"
2.0.0,"in case of Gold Scoring tgt_len = actual length, batch = 1 batch"
2.0.0,Generator forward.
2.0.0,"returns [(batch_size x beam_size) , vocab ] when 1 step"
2.0.0,"or [ tgt_len, batch_size, vocab ] when full sentence"
2.0.0,"here we have scores [tgt_lenxbatch, vocab] or [beamxbatch, vocab]"
2.0.0,"returns [(batch_size x beam_size) , vocab ] when 1 step"
2.0.0,"or [ tgt_len, batch_size, vocab ] when full sentence"
2.0.0,(0) add BOS and padding to tgt prediction
2.0.0,(1) Encoder forward.
2.0.0,(2) Repeat src objects `n_best` times.
2.0.0,"We use batch_size x n_best, get ``(src_len, batch * n_best, nfeat)``"
2.0.0,"(3) Init decoder with n_best src,"
2.0.0,"reshape tgt to ``(len, batch * n_best, nfeat)``"
2.0.0,masked_select
2.0.0,get aligned src id for each prediction's valid tgt tokens
2.0.0,TODO: support these blacklisted features
2.0.0,(0) Prep the components of the search.
2.0.0,(1) Run the encoder on the src.
2.0.0,(2) prep decode_strategy. Possibly repeat src objects.
2.0.0,(3) Begin decoding step by step:
2.0.0,Reorder states.
2.0.0,TODO: support these blacklisted features
2.0.0,hack [min_len_batch-1:] because expect <bos>
2.0.0,(0) Prep the components of the search.
2.0.0,(1) split src into src and target_prefix to avoid padding.
2.0.0,(2) init decoder
2.0.0,(3) prep decode_strategy. Possibly repeat src objects.
2.0.0,(4) Begin decoding step by step:
2.0.0,Reorder states.
2.0.0,select indexes in model state/cache
2.0.0,beam parameters
2.0.0,beam state
2.0.0,BoolTensor was introduced in pytorch 1.2
2.0.0,"""global state"" of the old beam"
2.0.0,buffers for the topk scores and 'backpointer'
2.0.0,for testing
2.0.0,maybe fix some prediction at this step by modifying log_probs
2.0.0,Flatten probs into a list of possibilities.
2.0.0,Penalize beams that finished.
2.0.0,"on real data (newstest2017) with the pretrained transformer,"
2.0.0,it's faster to not move this back to the original device
2.0.0,Store finished hypotheses for this batch.
2.0.0,End condition is the top beam finished and we can return
2.0.0,n_best hypotheses.
2.0.0,"If all sentences are translated, no need to go further."
2.0.0,Remove finished batches for the next step.
2.0.0,using integer division to get an integer _B without casting
2.0.0,force the output to be longer than self.min_length
2.0.0,Multiply probs by the beam probability.
2.0.0,"if the sequence ends now, then the penalty is the current"
2.0.0,"length + 1, to include the EOS token"
2.0.0,Avoid any direction that would repeat unwanted ngrams
2.0.0,Pick up candidate token by curr_scores
2.0.0,Recover log probs.
2.0.0,Length penalty is just a scalar. It doesn't matter if it's applied
2.0.0,before or after the topk.
2.0.0,Resolve beam origin and map to batch index flat representation.
2.0.0,Append last prediction.
2.0.0,update global state (step == 1)
2.0.0,update global state (step > 1)
2.0.0,"shape: (batch_size x beam_size, 1)"
2.0.0,in LM task memory_lengths is associated with currently generated src
2.0.0,and therefore needs to follow the generation
2.0.0,in LM task memory_lengths is associated with currently generated src
2.0.0,and therefore needs to follow the generation
2.0.0,Term will be subtracted from probability
2.0.0,Probability will be divided by this
2.0.0,these warnings indicate that either the alpha/beta
2.0.0,"forces a penalty to be a no-op, or a penalty is a no-op but"
2.0.0,the alpha/beta would suggest otherwise.
2.0.0,using some length penalty
2.0.0,using some coverage penalty
2.0.0,!/usr/bin/env python
2.0.0,semaphore doesn't have a timeout arg in Python 2.7
2.0.0,perform a first request to initialize everything
2.0.0,backwards compatibility for confs
2.0.0,every segment becomes a dict for flexibility purposes
2.0.0,NOTE: translator returns lists of `n_best` list
2.0.0,build back results with empty texts
2.0.0,load can be called multiple times: modify copy
2.0.0,output contain alignment
2.0.0,Below are all the different penalty terms implemented so far.
2.0.0,Subtract coverage penalty from topk log probs.
2.0.0,Divide topk log probs by length penalty.
2.0.0,Sorting
2.0.0,Chinese segmentation
2.0.0,Chinese simplify -> Chinese traditional standard
2.0.0,Chinese simplify -> Chinese traditional (HongKong)
2.0.0,Chinese simplify -> Chinese traditional (Taiwan)
2.0.0,Chinese traditional -> Chinese simplify (v1)
2.0.0,Chinese traditional -> Chinese simplify (v2)
2.0.0rc2,!/usr/bin/env python
2.0.0rc2,!/usr/bin/env python
2.0.0rc2,!/usr/bin/env python
2.0.0rc2,!/usr/bin/env python
2.0.0rc2,!/usr/bin/env python3
2.0.0rc2,-*- coding: utf-8 -*-
2.0.0rc2,
2.0.0rc2,"OpenNMT-py documentation build configuration file, created by"
2.0.0rc2,sphinx-quickstart on Sun Dec 17 12:07:14 2017.
2.0.0rc2,
2.0.0rc2,This file is execfile()d with the current directory set to its
2.0.0rc2,containing dir.
2.0.0rc2,
2.0.0rc2,Note that not all possible configuration values are present in this
2.0.0rc2,autogenerated file.
2.0.0rc2,
2.0.0rc2,All configuration values have a default; values that are commented out
2.0.0rc2,serve to show the default.
2.0.0rc2,"If extensions (or modules to document with autodoc) are in another directory,"
2.0.0rc2,add these directories to sys.path here. If the directory is relative to the
2.0.0rc2,"documentation root, use os.path.abspath to make it absolute, like shown here."
2.0.0rc2,
2.0.0rc2,import os
2.0.0rc2,import sys
2.0.0rc2,"sys.path.insert(0, os.path.abspath('.'))"
2.0.0rc2,-- General configuration ------------------------------------------------
2.0.0rc2,"If your documentation needs a minimal Sphinx version, state it here."
2.0.0rc2,
2.0.0rc2,needs_sphinx = '1.0'
2.0.0rc2,"Add any Sphinx extension module names here, as strings. They can be"
2.0.0rc2,extensions coming with Sphinx (named 'sphinx.ext.*') or your custom
2.0.0rc2,ones.
2.0.0rc2,Show base classes
2.0.0rc2,"Use ""variables"" section for Attributes instead of weird block things"
2.0.0rc2,mimicking the function style.
2.0.0rc2,"Add any paths that contain templates here, relative to this directory."
2.0.0rc2,The suffix(es) of source filenames.
2.0.0rc2,You can specify multiple suffix as a list of string:
2.0.0rc2,
2.0.0rc2,"source_suffix = ['.rst', '.md']"
2.0.0rc2,The master toctree document.
2.0.0rc2,General information about the project.
2.0.0rc2,"The version info for the project you're documenting, acts as replacement for"
2.0.0rc2,"|version| and |release|, also used in various other places throughout the"
2.0.0rc2,built documents.
2.0.0rc2,
2.0.0rc2,The short X.Y version.
2.0.0rc2,"The full version, including alpha/beta/rc tags."
2.0.0rc2,The language for content autogenerated by Sphinx. Refer to documentation
2.0.0rc2,for a list of supported languages.
2.0.0rc2,
2.0.0rc2,This is also used if you do content translation via gettext catalogs.
2.0.0rc2,"Usually you set ""language"" from the command line for these cases."
2.0.0rc2,"List of patterns, relative to source directory, that match files and"
2.0.0rc2,directories to ignore when looking for source files.
2.0.0rc2,This patterns also effect to html_static_path and html_extra_path
2.0.0rc2,The name of the Pygments (syntax highlighting) style to use.
2.0.0rc2,"If true, `todo` and `todoList` produce output, else they produce nothing."
2.0.0rc2,-- Options for HTML output ----------------------------------------------
2.0.0rc2,The theme to use for HTML and HTML Help pages.  See the documentation for
2.0.0rc2,a list of builtin themes.
2.0.0rc2,
2.0.0rc2,html_theme = 'sphinx_materialdesign_theme'
2.0.0rc2,html_theme_path = [sphinx_materialdesign_theme.get_path()]
2.0.0rc2,Theme options are theme-specific and customize the look and feel of a theme
2.0.0rc2,"further.  For a list of options available for each theme, see the"
2.0.0rc2,documentation.
2.0.0rc2,
2.0.0rc2,html_theme_options = {}
2.0.0rc2,"Add any paths that contain custom static files (such as style sheets) here,"
2.0.0rc2,"relative to this directory. They are copied after the builtin static files,"
2.0.0rc2,"so a file named ""default.css"" will overwrite the builtin ""default.css""."
2.0.0rc2,"Custom sidebar templates, must be a dictionary that maps document names"
2.0.0rc2,to template names.
2.0.0rc2,
2.0.0rc2,This is required for the alabaster theme
2.0.0rc2,refs: http://alabaster.readthedocs.io/en/latest/installation.html#sidebars
2.0.0rc2,-- Options for HTMLHelp output ------------------------------------------
2.0.0rc2,Output file base name for HTML help builder.
2.0.0rc2,-- Options for LaTeX output ---------------------------------------------
2.0.0rc2,The paper size ('letterpaper' or 'a4paper').
2.0.0rc2,
2.0.0rc2,"'papersize': 'letterpaper',"
2.0.0rc2,"The font size ('10pt', '11pt' or '12pt')."
2.0.0rc2,
2.0.0rc2,"'pointsize': '10pt',"
2.0.0rc2,Additional stuff for the LaTeX preamble.
2.0.0rc2,
2.0.0rc2,"'preamble': '',"
2.0.0rc2,Latex figure (float) alignment
2.0.0rc2,
2.0.0rc2,"'figure_align': 'htbp',"
2.0.0rc2,Grouping the document tree into LaTeX files. List of tuples
2.0.0rc2,"(source start file, target name, title,"
2.0.0rc2,"author, documentclass [howto, manual, or own class])."
2.0.0rc2,-- Options for manual page output ---------------------------------------
2.0.0rc2,One entry per manual page. List of tuples
2.0.0rc2,"(source start file, name, description, authors, manual section)."
2.0.0rc2,-- Options for Texinfo output -------------------------------------------
2.0.0rc2,Grouping the document tree into Texinfo files. List of tuples
2.0.0rc2,"(source start file, target name, title, author,"
2.0.0rc2,"dir menu entry, description, category)"
2.0.0rc2,!/usr/bin/env python
2.0.0rc2,-*- coding: utf-8 -*-
2.0.0rc2,is this reachable?
2.0.0rc2,Read in embeddings
2.0.0rc2,Write to file
2.0.0rc2,converts a SentencePiece vocabulary to the format expected by dynamic data
2.0.0rc2,"(essentially converts float expected counts to ""fixed precision"" int pseudo"
2.0.0rc2,counts)
2.0.0rc2,"Add in default model arguments, possibly added since training."
2.0.0rc2,build_base_model expects updated and validated opts
2.0.0rc2,-*- encoding: utf-8 -*-
2.0.0rc2,!/usr/bin/env python
2.0.0rc2,-*- coding: utf-8 -*-
2.0.0rc2,Author: Rico Sennrich
2.0.0rc2,flake8: noqa
2.0.0rc2,This file is retrieved from https://github.com/rsennrich/subword-nmt
2.0.0rc2,hack for python2/3 compatibility
2.0.0rc2,check version information
2.0.0rc2,some hacking to deal with duplicates (only consider first instance)
2.0.0rc2,don't print end-of-word symbols
2.0.0rc2,sys.stderr.write('cannot split {0} further.\n'.format(segment))
2.0.0rc2,sys.stderr.write('OOV: {0}\n'.format(segment))
2.0.0rc2,sys.stderr.write('OOV: {0}\n'.format(segment))
2.0.0rc2,python 2/3 compatibility
2.0.0rc2,read/write files as UTF-8
2.0.0rc2,!/usr/bin/env python
2.0.0rc2,!/usr/bin/env python
2.0.0rc2,-*- coding: utf-8 -*-
2.0.0rc2,Author: Rico Sennrich
2.0.0rc2,flake8: noqa
2.0.0rc2,This file is retrieved from https://github.com/rsennrich/subword-nmt
2.0.0rc2,hack for python2/3 compatibility
2.0.0rc2,"find all instances of pair, and update frequency/indices around it"
2.0.0rc2,find first symbol
2.0.0rc2,"if first symbol is followed by second symbol, we've found an occurrence of pair (old_word[i:i+2])"
2.0.0rc2,"assuming a symbol sequence ""A B C"", if ""B C"" is merged, reduce the frequency of ""A B"""
2.0.0rc2,"assuming a symbol sequence ""A B C B"", if ""B C"" is merged, reduce the frequency of ""C B""."
2.0.0rc2,"however, skip this if the sequence is A B C B C, because the frequency of ""C B"" will be reduced by the previous code block"
2.0.0rc2,find new pair
2.0.0rc2,"assuming a symbol sequence ""A BC D"", if ""B C"" is merged, increase the frequency of ""A BC"""
2.0.0rc2,"assuming a symbol sequence ""A BC B"", if ""B C"" is merged, increase the frequency of ""BC B"""
2.0.0rc2,"however, if the sequence is A BC BC, skip this step because the count of ""BC BC"" will be incremented by the previous code block"
2.0.0rc2,data structure of pair frequencies
2.0.0rc2,index from pairs to words
2.0.0rc2,version 0.2 changes the handling of the end-of-word token ('</w>');
2.0.0rc2,version numbering allows bckward compatibility
2.0.0rc2,"threshold is inspired by Zipfian assumption, but should only affect speed"
2.0.0rc2,we probably missed the best pair because of pruning; go back to full statistics
2.0.0rc2,"threshold is inspired by Zipfian assumption, but should only affect speed"
2.0.0rc2,python 2/3 compatibility
2.0.0rc2,read/write files as UTF-8
2.0.0rc2,!/usr/bin/env python
2.0.0rc2,-*- coding: utf-8 -*-
2.0.0rc2,!/usr/bin/env python
2.0.0rc2,for back compat when attention_dropout was not defined
2.0.0rc2,Build embeddings.
2.0.0rc2,Build encoder.
2.0.0rc2,Build decoder.
2.0.0rc2,Share the embedding matrix - preprocess with share_vocab required.
2.0.0rc2,src/tgt vocab should be the same if `-share_vocab` is specified.
2.0.0rc2,Build NMTModel(= encoder + decoder).
2.0.0rc2,Build Generator.
2.0.0rc2,Load the model states from checkpoint or initialize them.
2.0.0rc2,This preserves backward-compat for models using customed layernorm
2.0.0rc2,end of patch for backward compatibility
2.0.0rc2,!/usr/bin/env python
2.0.0rc2,NOTE: It's important that ``opt`` has been validated and updated
2.0.0rc2,at this point.
2.0.0rc2,Build model.
2.0.0rc2,Build optimizer.
2.0.0rc2,Build model saver
2.0.0rc2,Move batch to specified device
2.0.0rc2,Use Tensorboard for visualization during training
2.0.0rc2,Options only during inference
2.0.0rc2,"Truncation options, for text corpus"
2.0.0rc2,"as for False, this will be added in _add_train_general_opts"
2.0.0rc2,Embedding Options
2.0.0rc2,Encoder-Decoder Options
2.0.0rc2,"group.add('--residual', '-residual',   action=""store_true"","
2.0.0rc2,"help=""Add residual connections between RNN layers."")"
2.0.0rc2,The following options (bridge_extra_node to src_vocab) are used
2.0.0rc2,for training with --encoder_type ggnn (Gated Graph Neural Network).
2.0.0rc2,Attention options
2.0.0rc2,Alignement options
2.0.0rc2,Generator and loss options.
2.0.0rc2,GPU
2.0.0rc2,Init options
2.0.0rc2,Pretrained word vectors
2.0.0rc2,Freeze word vectors
2.0.0rc2,Optimization options
2.0.0rc2,learning rate
2.0.0rc2,options relate to data preprare
2.0.0rc2,options relate to train
2.0.0rc2,Alpha and Beta values for Google Length + Coverage penalty
2.0.0rc2,"Described here: https://arxiv.org/pdf/1609.08144.pdf, Section 7"
2.0.0rc2,Adding options relate to decoding strategy
2.0.0rc2,Adding option for logging
2.0.0rc2,Copyright 2016 The Chromium Authors. All rights reserved.
2.0.0rc2,Use of this source code is governed by a BSD-style license that can be
2.0.0rc2,found in the LICENSE file.
2.0.0rc2,"Get the key 'value' in the dict, or just use 'value'"
2.0.0rc2,Basic attributes.
2.0.0rc2,Set model in training mode.
2.0.0rc2,UPDATE DROPOUT
2.0.0rc2,Run patience mechanism
2.0.0rc2,"If the patience has reached the limit, stop training"
2.0.0rc2,swap model params w/ moving average
2.0.0rc2,(and keep the original parameters)
2.0.0rc2,Set model in validating mode.
2.0.0rc2,F-prop through the model.
2.0.0rc2,Compute loss.
2.0.0rc2,Update statistics.
2.0.0rc2,Set model back to training mode.
2.0.0rc2,Truncated BPTT: reminder not compatible with accum > 1
2.0.0rc2,1. Create truncated target.
2.0.0rc2,2. F-prop all but generator.
2.0.0rc2,3. Compute loss.
2.0.0rc2,4. Update the parameters and statistics.
2.0.0rc2,Multi GPU gradient gather
2.0.0rc2,"If truncated, don't backprop fully."
2.0.0rc2,TO CHECK
2.0.0rc2,if dec_state is not None:
2.0.0rc2,dec_state.detach()
2.0.0rc2,"in case of multi step gradient accumulation,"
2.0.0rc2,update only after accum batches
2.0.0rc2,For Flake
2.0.0rc2,we avoid padding while mean pooling
2.0.0rc2,incoming and outgoing edge embedding
2.0.0rc2,Find vocab data for tree builting
2.0.0rc2,Propogation Model
2.0.0rc2,Initialize the bridge layer
2.0.0rc2,Initialize graph using formatted input sequence
2.0.0rc2,Number of flagged nodes defines node count for this sample
2.0.0rc2,"(Nodes can have no flags on them, but must be in 'flags' list)."
2.0.0rc2,The total number of integers in the vocab should allow
2.0.0rc2,for all features and edges to be defined.
2.0.0rc2,Use first extra node as only source for decoder init
2.0.0rc2,Average all nodes to get bridge input
2.0.0rc2,"LSTM has hidden and cell state, other only one"
2.0.0rc2,Total number of states
2.0.0rc2,Build a linear layer for each
2.0.0rc2,Initialize the bridge layer
2.0.0rc2,"s_len, batch, emb_dim = emb.size()"
2.0.0rc2,Lengths data is wrapped inside a Tensor.
2.0.0rc2,"LSTM has hidden and cell state, other only one"
2.0.0rc2,Total number of states
2.0.0rc2,Build a linear layer for each
2.0.0rc2,"s_len, batch, emb_dim = emb.size()"
2.0.0rc2,Run the forward pass of every layer of the tranformer.
2.0.0rc2,Dimensions and padding for constructing the word embedding matrix
2.0.0rc2,Dimensions and padding for feature embedding matrices
2.0.0rc2,(these have no effect if feat_vocab_sizes is empty)
2.0.0rc2,The embedding matrix look-up tables. The first look-up table
2.0.0rc2,"is for words. Subsequent ones are for features, if any exist."
2.0.0rc2,The final output size of word + feature vectors. This can vary
2.0.0rc2,from the word vector size if and only if features are defined.
2.0.0rc2,This is the attribute you should access if you need to know
2.0.0rc2,how big your embeddings are going to be.
2.0.0rc2,The sequence of operations that converts the input sequence
2.0.0rc2,into a sequence of embeddings. At minimum this consists of
2.0.0rc2,looking up the embeddings for each word and feature in the
2.0.0rc2,input. Model parameters may require the sequence to contain
2.0.0rc2,additional operations as well.
2.0.0rc2,features must use word_vec_size
2.0.0rc2,features will use feat_vec_size
2.0.0rc2,Some utilitary functions for pretrained embeddings
2.0.0rc2,is this reachable?
2.0.0rc2,Write to file
2.0.0rc2,set the opt in place
2.0.0rc2,set the opt in place
2.0.0rc2,This class is mainly used by decoder.py for RNNs but also
2.0.0rc2,by the CNN / transformer decoder when copy attention is used
2.0.0rc2,CNN has its own attention mechanism ConvMultiStepAttention
2.0.0rc2,Transformer has its own MultiHeadedAttention
2.0.0rc2,mlp wants it with bias
2.0.0rc2,Check input sizes
2.0.0rc2,"(batch, t_len, d) x (batch, d, s_len) --> (batch, t_len, s_len)"
2.0.0rc2,"(batch, t_len, s_len, d)"
2.0.0rc2,one step input
2.0.0rc2,"compute attention scores, as in Luong et al."
2.0.0rc2,Softmax or sparsemax to normalize attention weights
2.0.0rc2,each context vector c_t is the weighted average
2.0.0rc2,over all the source hidden states
2.0.0rc2,concatenate
2.0.0rc2,Check output sizes
2.0.0rc2,Check output sizes
2.0.0rc2,clamping necessary because of numerical errors: loss should be lower
2.0.0rc2,"bounded by zero, but negative values near zero are possible without"
2.0.0rc2,the clamp
2.0.0rc2,from onmt.utils.misc import aeq
2.0.0rc2,CHECKS
2.0.0rc2,"batch, k_len, d = key.size()"
2.0.0rc2,"batch_, k_len_, d_ = value.size()"
2.0.0rc2,"aeq(batch, batch_)"
2.0.0rc2,"aeq(k_len, k_len_)"
2.0.0rc2,"aeq(d, d_)"
2.0.0rc2,"batch_, q_len, d_ = query.size()"
2.0.0rc2,"aeq(batch, batch_)"
2.0.0rc2,"aeq(d, d_)"
2.0.0rc2,"aeq(self.model_dim % 8, 0)"
2.0.0rc2,if mask is not None:
2.0.0rc2,"batch_, q_len_, k_len_ = mask.size()"
2.0.0rc2,"aeq(batch_, batch)"
2.0.0rc2,"aeq(k_len_, k_len)"
2.0.0rc2,aeq(q_len_ == q_len)
2.0.0rc2,END CHECKS
2.0.0rc2,"1) Project key, value, and query."
2.0.0rc2,1 or key_len x key_len
2.0.0rc2,1 or key_len x key_len x dim_per_head
2.0.0rc2,1 or key_len x key_len x dim_per_head
2.0.0rc2,2) Calculate and scale scores.
2.0.0rc2,batch x num_heads x query_len x key_len
2.0.0rc2,3) Apply attention dropout and compute context vectors.
2.0.0rc2,CHECK
2.0.0rc2,"batch_, q_len_, d_ = output.size()"
2.0.0rc2,"aeq(q_len, q_len_)"
2.0.0rc2,"aeq(batch, batch_)"
2.0.0rc2,"aeq(d, d_)"
2.0.0rc2,Return multi-head attn
2.0.0rc2,At the moment this class is only used by embeddings.Embeddings look-up tables
2.0.0rc2,-*- coding: utf-8 -*-
2.0.0rc2,checks
2.0.0rc2,"batch, channel, height, width = base_target_emb.size()"
2.0.0rc2,"batch_, channel_, height_, width_ = input_from_dec.size()"
2.0.0rc2,"enc_batch, enc_channel, enc_height = encoder_out_top.size()"
2.0.0rc2,"enc_batch_, enc_channel_, enc_height_ = encoder_out_combine.size()"
2.0.0rc2,out_features * in_features
2.0.0rc2,norm is out_features * 1
2.0.0rc2,batch_size * out_features
2.0.0rc2,out_features
2.0.0rc2,out_features
2.0.0rc2,batch_size * out_features
2.0.0rc2,"out_channels, in_channels // groups, * kernel_size"
2.0.0rc2,out_features
2.0.0rc2,This is used nowhere in the code at the moment (Vincent Nguyen 05/18/2018)
2.0.0rc2,"in_channels, out_channels, *kernel_size"
2.0.0rc2,"in_channels, out_channels, *kernel_size"
2.0.0rc2,"self.out_channels, 1"
2.0.0rc2,out_features
2.0.0rc2,out_features
2.0.0rc2,store roots on diagonal
2.0.0rc2,CHECKS
2.0.0rc2,Original probabilities.
2.0.0rc2,Probability of copying p(z=1) batch.
2.0.0rc2,Probability of not copying: p_{word}(w) * (1 - p(z))
2.0.0rc2,probabilities assigned by the model to the gold targets
2.0.0rc2,probability of tokens copied from source
2.0.0rc2,Set scores for unk to 0 and add eps
2.0.0rc2,find the indices in which you do not use the copy mechanism
2.0.0rc2,Drop padding.
2.0.0rc2,this block does not depend on the loss value computed above
2.0.0rc2,and is used only for stats
2.0.0rc2,this block does not depend on the loss value computed above
2.0.0rc2,and is used only for stats
2.0.0rc2,Correct target copy token instead of <unk>
2.0.0rc2,tgt[i] = align[i] + len(tgt_vocab)
2.0.0rc2,for i such that tgt[i] == 0 and align[i] != 0
2.0.0rc2,Compute sum of perplexities for stats
2.0.0rc2,this part looks like it belongs in CopyGeneratorLoss
2.0.0rc2,Compute Loss as NLL divided by seq length
2.0.0rc2,Compute Total Loss per sequence in batch
2.0.0rc2,Divide by length of each sequence and sum
2.0.0rc2,Auto import python files in this directory
2.0.0rc2,1. sample number of tokens to corrupt
2.0.0rc2,2. sample positions to corrput
2.0.0rc2,3. sample corrupted values
2.0.0rc2,1. sample number of tokens to corrupt
2.0.0rc2,2. sample positions to corrput
2.0.0rc2,3. Drop token on chosen position
2.0.0rc2,1. sample number of tokens to corrupt
2.0.0rc2,2. sample positions to corrput
2.0.0rc2,3. mask word on chosen position
2.0.0rc2,"Sharing options among `TokenizerTransform`s, same name conflict in"
2.0.0rc2,this scope will be resolved by remove previous occurrence in parser
2.0.0rc2,subword regularization(or BPE dropout) options:
2.0.0rc2,subword vocabulary restriction options:
2.0.0rc2,derterministic subwording
2.0.0rc2,subword sampling when nbest_size > 1 or -1
2.0.0rc2,alpha should be 0.0 < alpha < 1.0
2.0.0rc2,-1: keep everything (i.e. 1 mask per token)
2.0.0rc2,0: replace everything (i.e. no mask)
2.0.0rc2,1: 1 mask per span
2.0.0rc2,view each subword as word start / input is word level token
2.0.0rc2,Pretend it ends with a full stop so last span is a sentence
2.0.0rc2,"Tokens that are full stops, where the previous token is not"
2.0.0rc2,Make sure we have enough to mask
2.0.0rc2,Trim to masking budget
2.0.0rc2,Handle 0-length mask (inserts) separately
2.0.0rc2,assert is_word_start[-1] == 0
2.0.0rc2,assert tokens_length - 1 not in indices
2.0.0rc2,"keep index, but replace it with [MASK]"
2.0.0rc2,"acts as a long length, so spans don't go over the end of doc"
2.0.0rc2,next position from each word_start
2.0.0rc2,delete token: 1 mask/remove per span
2.0.0rc2,"keep index, but replace it with [MASK]: 1 mask per token"
2.0.0rc2,A bit faster when all lengths are 1
2.0.0rc2,to cover whole token
2.0.0rc2,delete token
2.0.0rc2,"keep index, but replace it with [MASK]"
2.0.0rc2,assert tokens_length - 1 not in indices
2.0.0rc2,initialize fields at the top of each unit test to prevent
2.0.0rc2,any undesired stateful effects
2.0.0rc2,"this test touches the file system, so it could be considered an"
2.0.0rc2,integration test
2.0.0rc2,write utf-8 bytes
2.0.0rc2,batch 0 will always predict EOS. The other batches will predict
2.0.0rc2,non-eos scores.
2.0.0rc2,"""best"" prediction is eos - that should be blocked"
2.0.0rc2,include at least one prediction OTHER than EOS
2.0.0rc2,that is greater than -1e20
2.0.0rc2,now batch 0 has ended and no others have
2.0.0rc2,initial step
2.0.0rc2,batch 0 dies on step 0
2.0.0rc2,include at least one prediction OTHER than EOS
2.0.0rc2,that is greater than -1e20
2.0.0rc2,step 2
2.0.0rc2,(old) batch 8 dies on step 1
2.0.0rc2,step 3
2.0.0rc2,everything dies
2.0.0rc2,initial step
2.0.0rc2,batch 0 dies on step 0
2.0.0rc2,include at least one prediction OTHER than EOS
2.0.0rc2,that is greater than -1e20
2.0.0rc2,step 2
2.0.0rc2,(old) batch 8 dies on step 1
2.0.0rc2,step 3
2.0.0rc2,everything dies
2.0.0rc2,illegal_weights_mask = torch.ByteTensor([
2.0.0rc2,"[0, 0, 0, 0, 0, 0, 0],"
2.0.0rc2,"[0, 0, 0, 1, 1, 1, 1],"
2.0.0rc2,"[0, 0, 0, 0, 0, 1, 1],"
2.0.0rc2,"[0, 0, 1, 1, 1, 1, 1]])"
2.0.0rc2,TODO: fix for pytorch 0.3
2.0.0rc2,illegal_weights = alignments.masked_select(illegal_weights_mask)
2.0.0rc2,"self.assertEqual(0.0, illegal_weights.data.sum())"
2.0.0rc2,this could be considered an integration test because it touches
2.0.0rc2,the filesystem for the config file (and the models)
2.0.0rc2,!/usr/bin/env python
2.0.0rc2,-*- coding: utf-8 -*-
2.0.0rc2,Inject some dummy training options that may needed when build fields
2.0.0rc2,Remove the generated *pt files.
2.0.0rc2,Remove the generated data samples
2.0.0rc2,all beams repeat (beam >= 1 repeat dummy scores)
2.0.0rc2,predict repeat_idx over and over again
2.0.0rc2,"before repeat, scores are either 0 or -inf"
2.0.0rc2,"on repeat, `repeat_idx` score is BLOCKED_SCORE"
2.0.0rc2,"(but it's still the best score, thus we have"
2.0.0rc2,"[BLOCKED_SCORE, -inf, -inf, -inf, -inf]"
2.0.0rc2,repetitions keeps maximizing score
2.0.0rc2,"index 0 has been blocked, so repeating=>+0.0 score"
2.0.0rc2,other indexes are -inf so repeating=>BLOCKED_SCORE
2.0.0rc2,which is higher
2.0.0rc2,beam 0 and beam >=2 will repeat (beam >= 2 repeat dummy scores)
2.0.0rc2,non-interesting beams are going to get dummy values
2.0.0rc2,"on initial round, only predicted scores for beam 0"
2.0.0rc2,matter. Make two predictions. Top one will be repeated
2.0.0rc2,"in beam zero, second one will live on in beam 1."
2.0.0rc2,predict the same thing in beam 0
2.0.0rc2,continue pushing around what beam 1 predicts
2.0.0rc2,"now beam 0 dies (along with the others), beam 1 -> beam 0"
2.0.0rc2,"now beam 0 dies (along with the others), beam 1 -> beam 0"
2.0.0rc2,beam 0 and beam >= 2 will repeat (beam 2 repeats excluded idx)
2.0.0rc2,non-interesting beams are going to get dummy values
2.0.0rc2,predict the same thing in beam 0
2.0.0rc2,continue pushing around what beam 1 predicts
2.0.0rc2,predict the allowed-repeat again in beam 2
2.0.0rc2,"now beam 0 dies, beam 1 -> beam 0, beam 2 -> beam 1"
2.0.0rc2,and the rest die
2.0.0rc2,"since all preds after i=0 are 0, we can check"
2.0.0rc2,that the beam is the correct idx by checking that
2.0.0rc2,the curr score is the initial score
2.0.0rc2,beam 0 will always predict EOS. The other beams will predict
2.0.0rc2,non-eos scores.
2.0.0rc2,non-interesting beams are going to get dummy values
2.0.0rc2,"""best"" prediction is eos - that should be blocked"
2.0.0rc2,include at least beam_sz predictions OTHER than EOS
2.0.0rc2,that are greater than -1e20
2.0.0rc2,predict eos in beam 0
2.0.0rc2,provide beam_sz other good predictions
2.0.0rc2,now the top beam has ended and no others have
2.0.0rc2,"not of interest, but want to make sure it keeps running"
2.0.0rc2,since only beam 0 terminates and n_best = 2
2.0.0rc2,"this is also a test that when block_ngram_repeat=0,"
2.0.0rc2,repeating is acceptable
2.0.0rc2,non-interesting beams are going to get dummy values
2.0.0rc2,"""best"" prediction is eos - that should be blocked"
2.0.0rc2,include at least beam_sz predictions OTHER than EOS
2.0.0rc2,that are greater than -1e20
2.0.0rc2,predict eos in beam 1
2.0.0rc2,provide beam_sz other good predictions in other beams
2.0.0rc2,provide beam_sz other good predictions in other beams
2.0.0rc2,beam 1 dies on min_length
2.0.0rc2,beam 0 dies on the step after beam 1 dies
2.0.0rc2,"inp_lens is tiled in initialize, reassign to make attn match"
2.0.0rc2,non-interesting beams are going to get dummy values
2.0.0rc2,"""best"" prediction is eos - that should be blocked"
2.0.0rc2,include at least beam_sz predictions OTHER than EOS
2.0.0rc2,that are greater than -1e20
2.0.0rc2,predict eos in beam 1
2.0.0rc2,provide beam_sz other good predictions in other beams
2.0.0rc2,provide beam_sz other good predictions in other beams
2.0.0rc2,no top beams are finished yet
2.0.0rc2,beam 1 dies on min_length
2.0.0rc2,no top beams are finished yet
2.0.0rc2,beam 0 dies on the step after beam 1 dies
2.0.0rc2,top beam is finished now so there are attentions
2.0.0rc2,two beams are finished in each batch
2.0.0rc2,second dim is cut down to the non-padded src length
2.0.0rc2,first dim is equal to the time of death
2.0.0rc2,(beam 0 died at current step - adjust for SOS)
2.0.0rc2,(beam 1 died at last step - adjust for SOS)
2.0.0rc2,behavior gets weird when beam is already done so just stop
2.0.0rc2,this is just test_beam.TestBeamAgainstReferenceCase repeated
2.0.0rc2,in each batch.
2.0.0rc2,"init_preds: [4, 3, 5, 6, 7] - no EOS's"
2.0.0rc2,no EOS's yet
2.0.0rc2,"[5, 3, 2, 6, 0], so beam 2 predicts EOS!"
2.0.0rc2,assumes beam 2 finished on last step
2.0.0rc2,ended beam 2 shouldn't continue
2.0.0rc2,"[2, 5, 3, 6, 0] repeat self.BATCH_SZ, so beam 0 predicts EOS!"
2.0.0rc2,"[-2.4879, -3.8910, -4.1010, -4.2010, -4.4010] repeat self.BATCH_SZ"
2.0.0rc2,another beam is finished in all batches
2.0.0rc2,new beam 0 finished
2.0.0rc2,new beam 0 is old beam 3
2.0.0rc2,assumes beam 0 finished on last step
2.0.0rc2,"[5, 2, 6, 1, 0] repeat self.BATCH_SZ, so beam 1 predicts EOS!"
2.0.0rc2,new beam 1 finished
2.0.0rc2,new beam 1 is old beam 4
2.0.0rc2,this could be considered an integration test because it tests
2.0.0rc2,interactions between the GNMT scorer and the beam
2.0.0rc2,"-data option is required, but not used in this test, so dummy."
2.0.0rc2,len x batch x nfeat
2.0.0rc2,Initialize vectors to compare size with
2.0.0rc2,Ensure correct sizes and types
2.0.0rc2,Make sure that output has the correct size and type
2.0.0rc2,"[('encoder_type', 'transformer'),"
2.0.0rc2,"('word_vec_size', 16), ('rnn_size', 16)],"
2.0.0rc2,""""""" Only do SRU test if requirment is safisfied. """""""
2.0.0rc2,SRU doesn't support input_feed.
2.0.0rc2,first check there's nothing unexpectedly not trainable
2.0.0rc2,ok: word embeddings shouldn't be trainable
2.0.0rc2,if word vecs are freezed
2.0.0rc2,ok: positional encodings shouldn't be trainable
2.0.0rc2,then check nothing unexpectedly trainable
2.0.0rc2,Decoder state
2.0.0rc2,Build the RNN.
2.0.0rc2,Set up the context gate.
2.0.0rc2,Set up the standard attention.
2.0.0rc2,The encoder hidden is  (layers*directions) x batch x dim.
2.0.0rc2,We need to convert it to layers x batch x (directions*dim).
2.0.0rc2,Init the input feed.
2.0.0rc2,Update the state with the result.
2.0.0rc2,Concatenates sequence of tensors along a new dimension.
2.0.0rc2,NOTE: v0.3 to 0.4: dec_outs / attns[*] may not be list
2.0.0rc2,(in particular in case of SRU) it was not raising error in 0.3
2.0.0rc2,since stack(Variable) was allowed.
2.0.0rc2,"In 0.4, SRU returns a tensor that shouldn't be stacke"
2.0.0rc2,Check
2.0.0rc2,Calculate the attention.
2.0.0rc2,Calculate the context gate.
2.0.0rc2,Additional args check.
2.0.0rc2,END Additional args check.
2.0.0rc2,Input feed concatenates hidden state with
2.0.0rc2,input at every time step.
2.0.0rc2,TODO: context gate should be employed
2.0.0rc2,instead of second RNN transform.
2.0.0rc2,Update the coverage attention.
2.0.0rc2,Decoder State
2.0.0rc2,CNNDecoder has its own attention mechanism.
2.0.0rc2,Set up a separate copy attention layer if needed.
2.0.0rc2,The output of CNNEncoder.
2.0.0rc2,The combination of output of CNNEncoder and source embeddings.
2.0.0rc2,Process the result and update the attentions.
2.0.0rc2,Update the state.
2.0.0rc2,TODO change the way attns is returned dict => list or tuple (onnx)
2.0.0rc2,Memory_lengths is a single tensor shared between all models.
2.0.0rc2,This assumption will not hold if Translator is modified
2.0.0rc2,to calculate memory_lengths as something other than the length
2.0.0rc2,of the input.
2.0.0rc2,"return _, (B, Q_len, K_len)"
2.0.0rc2,"layer average attention across heads, get ``(B, Q, K)``"
2.0.0rc2,"Case 1: no full_context, no align heads -> layer avg baseline"
2.0.0rc2,"Case 2: no full_context, 1 align heads -> guided align"
2.0.0rc2,"Case 3: full_context, 1 align heads -> full cte guided align"
2.0.0rc2,T: could be 1 in the case of stepwise decoding or tgt_len
2.0.0rc2,BoolTensor was introduced in pytorch 1.2
2.0.0rc2,Decoder State
2.0.0rc2,"previously, there was a GlobalAttention module here for copy"
2.0.0rc2,"attention. But it was never actually used -- the ""copy"" attention"
2.0.0rc2,just reuses the context attention.
2.0.0rc2,"attns[""align""] = torch.stack(attn_aligns, 0).mean(0)  # All avg"
2.0.0rc2,TODO change the way attns is returned dict => list or tuple (onnx)
2.0.0rc2,"buffer size in bytes, determine equiv. # of elements based on data type"
2.0.0rc2,copy tensors into buffer_t
2.0.0rc2,all-reduce and rescale
2.0.0rc2,copy all-reduced buffer back into tensors
2.0.0rc2,"tensor is bigger than buffer, all-reduce and rescale directly"
2.0.0rc2,"buffer is full, all-reduce and replace buffer with grad"
2.0.0rc2,add tensor to buffer
2.0.0rc2,NOTE: stride (if needed) is handled at the
2.0.0rc2,generator (train_iter) level
2.0.0rc2,Move batch to correspond device_id when consumer iterate
2.0.0rc2,hack to dodge unpicklable `dict_keys`
2.0.0rc2,"propagate exception to parent process, keeping original traceback"
2.0.0rc2,TODO: Find a better way to check for sparse gradients.
2.0.0rc2,we use here a FusedAdam() copy of an old Apex repo
2.0.0rc2,In this case use the old FusedAdam with FP16_optimizer wrapper
2.0.0rc2,Load everything from the checkpoint.
2.0.0rc2,Build everything from scratch.
2.0.0rc2,"Reset optimizer, keep options."
2.0.0rc2,"Reset options, keep optimizer."
2.0.0rc2,State can be partially restored.
2.0.0rc2,"unscaled optimizer's gradients (already done therefore skip),"
2.0.0rc2,skips optimizer.step() if gradients contain infs/NaNs.
2.0.0rc2,Updates the scale for next iteration.
2.0.0rc2,Code below is an implementation of https://arxiv.org/pdf/1804.04235.pdf
2.0.0rc2,inspired but modified from https://github.com/DeadAt0m/adafactor-pytorch
2.0.0rc2,backward compatibility
2.0.0rc2,assuming a list/generator of parameter means single group
2.0.0rc2,compute combined scale factor for this group
2.0.0rc2,norm is in fact norm*scale
2.0.0rc2,note: p.grad should not ever be set for correct operation of
2.0.0rc2,mixed precision optimizer that sometimes sends None gradients
2.0.0rc2,State initialization
2.0.0rc2,Exponential moving average of gradient values
2.0.0rc2,Exponential moving average of squared gradient values
2.0.0rc2,-*- coding: utf-8 -*-
2.0.0rc2,if the loss function operates on vectors of raw logits instead of
2.0.0rc2,"probabilities, only the first part of the generator needs to be"
2.0.0rc2,"passed to the NMTLossCompute. At the moment, the only supported"
2.0.0rc2,loss function of this kind is the sparsemax loss.
2.0.0rc2,"attn_align should be in (batch_size, pad_tgt_size, pad_src_size)"
2.0.0rc2,"align_idx should be a Tensor in size([N, 3]), N is total number"
2.0.0rc2,"of align src-tgt pair in current batch, each as"
2.0.0rc2,"['sent_NÂ°_in_batch', 'tgt_id+1', 'src_id'] (check AlignField)"
2.0.0rc2,NOTE: tgt-src ref alignement that in range_ of shard
2.0.0rc2,(coherent with batch.tgt)
2.0.0rc2,"align_head contains value in [0, 1) presenting attn prob,"
2.0.0rc2,0 was resulted by the context attention src_pad_mask
2.0.0rc2,"So, the correspand position in ref_align should also be 0"
2.0.0rc2,"Therefore, clip align_head to > 1e-18 should be bias free."
2.0.0rc2,non_none: the subdict of the state dictionary where the values
2.0.0rc2,are not None.
2.0.0rc2,"Now, the iteration:"
2.0.0rc2,state is a dictionary of sequences of tensor-like but we
2.0.0rc2,want a sequence of dictionaries of tensors.
2.0.0rc2,"First, unzip the dictionary into a sequence of keys and a"
2.0.0rc2,sequence of tensor-like sequences.
2.0.0rc2,"Now, yield a dictionary for each shard. The keys are always"
2.0.0rc2,the same. values is a sequence of length #keys where each
2.0.0rc2,element is a sequence of length #shards. We want to iterate
2.0.0rc2,"over the shards, not over the keys: therefore, the values need"
2.0.0rc2,to be re-zipped by shard and then each shard can be paired
2.0.0rc2,with the keys.
2.0.0rc2,Assumed backprop'd
2.0.0rc2,Check Transforms
2.0.0rc2,Check path
2.0.0rc2,Check prefix: will be used when use prefix transform
2.0.0rc2,Check weight
2.0.0rc2,validation when train:
2.0.0rc2,Check embeddings stuff
2.0.0rc2,"Backward compatibility with ""fix_word_vecs_*"" opts"
2.0.0rc2,encoder and decoder should be same sizes
2.0.0rc2,"Load default opt values, then overwrite with the opts in"
2.0.0rc2,"the checkpoint. That way, if there are new options added,"
2.0.0rc2,the defaults are used.
2.0.0rc2,Don't do anything
2.0.0rc2,Update best score of each criteria
2.0.0rc2,Reset tolerance
2.0.0rc2,Update current status
2.0.0rc2,Decrease tolerance
2.0.0rc2,Log
2.0.0rc2,Log
2.0.0rc2,Get a list of world_size lists with len(stat_list) Statistics objects
2.0.0rc2,SRU doesn't support PackedSequence.
2.0.0rc2,-*- coding: utf-8 -*-
2.0.0rc2,threshold on 1 to avoid div by 0
2.0.0rc2,treat alignment matrix one by one as each have different lengths
2.0.0rc2,No alignment if not exist valid tgt token
2.0.0rc2,get valid alignment (sub-matrix from full paded aligment matrix)
2.0.0rc2,-*- coding: utf-8 -*-
2.0.0rc2,this one is needed for torchtext random call (shuffled iterator)
2.0.0rc2,in multi gpu it ensures datasets are read in the same order
2.0.0rc2,some cudnn methods can be random even after fixing the seed
2.0.0rc2,unless you tell it to be deterministic
2.0.0rc2,This one is needed for various tranfroms
2.0.0rc2,These ensure same initialization in multi gpu mode
2.0.0rc2,Shift values to be >= 0
2.0.0rc2,we need to check the model path + any tokenizer path
2.0.0rc2,fast-forward if loaded from state
2.0.0rc2,NOTE: `rnn.pack_padded_sequence` requires that a
2.0.0rc2,"minibatch be sorted by decreasing order, which"
2.0.0rc2,requires reversing relative to typical sort keys
2.0.0rc2,Maintains the longest src and tgt length in the current batch
2.0.0rc2,Reset current longest length at a new batch (count=1)
2.0.0rc2,Src: [<bos> w1 ... wN <eos>]
2.0.0rc2,Tgt: [w1 ... wM <eos>]
2.0.0rc2,coding: utf-8
2.0.0rc2,make a small vocab containing just the tokens in the source sequence
2.0.0rc2,Map source tokens to indices in the dynamic dict.
2.0.0rc2,self.src_vocabs is used in collapse_copy_scores and Translator.py
2.0.0rc2,this assumes src_field and tgt_field are both text
2.0.0rc2,fields needs to have only keys that examples have as attrs
2.0.0rc2,avoid infinite recursion when fields isn't defined
2.0.0rc2,this is a hack: appears quicker to apply it here
2.0.0rc2,than in the ParallelCorpusIterator
2.0.0rc2,NOTE: moved to DatasetAdapter._process method in iterator.py
2.0.0rc2,item = self.transform.apply(
2.0.0rc2,"example, is_train=self.infinitely, corpus_name=self.cid)"
2.0.0rc2,empty example: skip
2.0.0rc2,-*- coding: utf-8 -*-
2.0.0rc2,backwards compatibility
2.0.0rc2,monkey-patch to make torchtext Vocab's pickleable
2.0.0rc2,"+1 for tgt side to keep coherent after ""bos"" padding,"
2.0.0rc2,"register ['NÂ°_in_batch', 'tgt_id+1', 'src_id']"
2.0.0rc2,this is basically copy-pasted from torchtext.
2.0.0rc2,counters changes in place
2.0.0rc2,keep the order of tokens specified in the vocab file by
2.0.0rc2,adding them to the counter with decreasing counting values
2.0.0rc2,`tgt_vocab_size` is ignored when sharing vocabularies
2.0.0rc2,return vocab to dump with standard name
2.0.0rc2,empty train_dataset_files so that vocab is only loaded from
2.0.0rc2,"given paths in src_vocab_path, tgt_vocab_path"
2.0.0rc2,Load vocabulary
2.0.0rc2,Drop the none-using from memory but keep the last
2.0.0rc2,"in the long run, shouldn't it be possible to do this by calling"
2.0.0rc2,build_vocab with both the src and tgt data?
2.0.0rc2,coding: utf-8
2.0.0rc2,several data readers need optional dependencies. There's no
2.0.0rc2,appropriate builtin exception
2.0.0rc2,NOTE: not support nfeats > 0 yet
2.0.0rc2,-*- coding: utf-8 -*-
2.0.0rc2,mix this with partial
2.0.0rc2,batch (list(list(list))): batch_size x len(self.fields) x seq_len
2.0.0rc2,lengths: batch_size
2.0.0rc2,data: seq_len x batch_size x len(self.fields)
2.0.0rc2,flake8: noqa
2.0.0rc2,For command-line option parsing
2.0.0rc2,"Check pass, set the args."
2.0.0rc2,"This SRU version implements its own cuda-level optimization,"
2.0.0rc2,so it requires that:
2.0.0rc2,1. `cupy` and `pynvrtc` python package installed.
2.0.0rc2,2. pytorch is built with cuda support.
2.0.0rc2,3. library path set: export LD_LIBRARY_PATH=<cuda lib path>.
2.0.0rc2,Check 1.
2.0.0rc2,Check 2.
2.0.0rc2,Check 3.
2.0.0rc2,This sets up device to use.
2.0.0rc2,-> directions x batch x dim
2.0.0rc2,For DEBUG
2.0.0rc2,"size = (length, batch, x.size(-1)) \"
2.0.0rc2,"if x.dim() == 3 else (batch, x.size(-1))"
2.0.0rc2,grad_x = x.new(*x.size()) if k_ == 3 else x.new(*size).zero_()
2.0.0rc2,Normal use
2.0.0rc2,"An entry check here, will catch on train side and translate side"
2.0.0rc2,if requirements are not satisfied.
2.0.0rc2,RNNDecoderState wraps hidden as a tuple.
2.0.0rc2,fh -> (layers*directions) x batch x dim
2.0.0rc2,_check_save_model_path
2.0.0rc2,NOTE: We need to trim the vocab to remove any unk tokens that
2.0.0rc2,were not originally here.
2.0.0rc2,!/usr/bin/env python
2.0.0rc2,!/usr/bin/env python
2.0.0rc2,!/usr/bin/env python
2.0.0rc2,-*- coding: utf-8 -*-
2.0.0rc2,!/usr/bin/env python
2.0.0rc2,!/usr/bin/env python
2.0.0rc2,!/usr/bin/env python
2.0.0rc2,import onmt.opts as opts
2.0.0rc2,Set sharing strategy manually instead of default based on the OS.
2.0.0rc2,"maybe prepare pretrained embeddings, if any"
2.0.0rc2,Load checkpoint if we resume from a previous training.
2.0.0rc2,Report src and tgt vocab sizes
2.0.0rc2,Create a thread to listen for errors in the child processes.
2.0.0rc2,Train with multiprocessing.
2.0.0rc2,"This does not work if we merge with the first loop, not sure why"
2.0.0rc2,Get the iterator to generate from
2.0.0rc2,"Once training is done, we can terminate the producers"
2.0.0rc2,magic indices
2.0.0rc2,result caching
2.0.0rc2,fix length constraint
2.0.0rc2,add one to account for BOS. Don't account for EOS because hitting
2.0.0rc2,this implies it hasn't been found.
2.0.0rc2,we don't block nothing if the user doesn't want it
2.0.0rc2,we can't block nothing beam's too short
2.0.0rc2,we check paths one by one
2.0.0rc2,we don't forbid nothing if the user doesn't want it
2.0.0rc2,we can't forbid nothing if beam's too short
2.0.0rc2,Reordering forbidden_tokens following beam selection
2.0.0rc2,We rebuild a dict to ensure we get the value and not the pointer
2.0.0rc2,Grabing the newly selected tokens and associated ngram
2.0.0rc2,skip the blocking if any token in current_ngram is excluded
2.0.0rc2,"pickups: Tensor where specified index were set to 1, others 0"
2.0.0rc2,"dropdowns: opposite of pickups, 1 for those shouldn't pick"
2.0.0rc2,Minus dropdowns to log_probs making probabilities of
2.0.0rc2,unspecified index close to 0
2.0.0rc2,"prediction step have surpass length of given target_prefix,"
2.0.0rc2,no need to further change this attr
2.0.0rc2,"For temp=0.0, take the argmax to avoid divide-by-zero errors."
2.0.0rc2,keep_topk=1 is also equivalent to argmax.
2.0.0rc2,Set all logits that are not in the top-k to -10000.
2.0.0rc2,This puts the probabilities close to 0.
2.0.0rc2,maybe fix some prediction at this step by modifying log_probs
2.0.0rc2,"shape: (sum(~ self.is_finished), 1)"
2.0.0rc2,!/usr/bin/env python
2.0.0rc2,Maintains the longest src and tgt length in the current batch
2.0.0rc2,Reset current longest length at a new batch (count=1)
2.0.0rc2,max_tgt_in_batch = 0
2.0.0rc2,Src: [<bos> w1 ... wN <eos>]
2.0.0rc2,Tgt: [w1 ... wM <eos>]
2.0.0rc2,for debugging
2.0.0rc2,TODO: maybe add dynamic part
2.0.0rc2,Statistics
2.0.0rc2,(0) add BOS and padding to tgt prediction
2.0.0rc2,(1) Encoder forward.
2.0.0rc2,(2) Repeat src objects `n_best` times.
2.0.0rc2,"We use batch_size x n_best, get ``(src_len, batch * n_best, nfeat)``"
2.0.0rc2,"(3) Init decoder with n_best src,"
2.0.0rc2,"reshape tgt to ``(len, batch * n_best, nfeat)``"
2.0.0rc2,masked_select
2.0.0rc2,get aligned src id for each prediction's valid tgt tokens
2.0.0rc2,TODO: support these blacklisted features
2.0.0rc2,Turn any copied words into UNKs.
2.0.0rc2,"Decoder forward, takes [tgt_len, batch, nfeats] as input"
2.0.0rc2,"and [src_len, batch, hidden] as memory_bank"
2.0.0rc2,"in case of inference tgt_len = 1, batch = beam times batch_size"
2.0.0rc2,"in case of Gold Scoring tgt_len = actual length, batch = 1 batch"
2.0.0rc2,Generator forward.
2.0.0rc2,"returns [(batch_size x beam_size) , vocab ] when 1 step"
2.0.0rc2,"or [ tgt_len, batch_size, vocab ] when full sentence"
2.0.0rc2,"here we have scores [tgt_lenxbatch, vocab] or [beamxbatch, vocab]"
2.0.0rc2,"returns [(batch_size x beam_size) , vocab ] when 1 step"
2.0.0rc2,"or [ tgt_len, batch_size, vocab ] when full sentence"
2.0.0rc2,(0) Prep the components of the search.
2.0.0rc2,(1) Run the encoder on the src.
2.0.0rc2,(2) prep decode_strategy. Possibly repeat src objects.
2.0.0rc2,(3) Begin decoding step by step:
2.0.0rc2,Reorder states.
2.0.0rc2,beam parameters
2.0.0rc2,result caching
2.0.0rc2,beam state
2.0.0rc2,BoolTensor was introduced in pytorch 1.2
2.0.0rc2,"""global state"" of the old beam"
2.0.0rc2,buffers for the topk scores and 'backpointer'
2.0.0rc2,for testing
2.0.0rc2,maybe fix some prediction at this step by modifying log_probs
2.0.0rc2,Flatten probs into a list of possibilities.
2.0.0rc2,using integer division to get an integer _B without casting
2.0.0rc2,force the output to be longer than self.min_length
2.0.0rc2,Multiply probs by the beam probability.
2.0.0rc2,"if the sequence ends now, then the penalty is the current"
2.0.0rc2,"length + 1, to include the EOS token"
2.0.0rc2,Avoid any direction that would repeat unwanted ngrams
2.0.0rc2,Pick up candidate token by curr_scores
2.0.0rc2,Recover log probs.
2.0.0rc2,Length penalty is just a scalar. It doesn't matter if it's applied
2.0.0rc2,before or after the topk.
2.0.0rc2,Resolve beam origin and map to batch index flat representation.
2.0.0rc2,Append last prediction.
2.0.0rc2,update global state (step == 1)
2.0.0rc2,update global state (step > 1)
2.0.0rc2,"shape: (batch_size x beam_size, 1)"
2.0.0rc2,Penalize beams that finished.
2.0.0rc2,"on real data (newstest2017) with the pretrained transformer,"
2.0.0rc2,it's faster to not move this back to the original device
2.0.0rc2,Store finished hypotheses for this batch.
2.0.0rc2,End condition is the top beam finished and we can return
2.0.0rc2,n_best hypotheses.
2.0.0rc2,"If all sentences are translated, no need to go further."
2.0.0rc2,Remove finished batches for the next step.
2.0.0rc2,Term will be subtracted from probability
2.0.0rc2,Probability will be divided by this
2.0.0rc2,these warnings indicate that either the alpha/beta
2.0.0rc2,"forces a penalty to be a no-op, or a penalty is a no-op but"
2.0.0rc2,the alpha/beta would suggest otherwise.
2.0.0rc2,using some length penalty
2.0.0rc2,using some coverage penalty
2.0.0rc2,!/usr/bin/env python
2.0.0rc2,semaphore doesn't have a timeout arg in Python 2.7
2.0.0rc2,perform a first request to initialize everything
2.0.0rc2,backwards compatibility for confs
2.0.0rc2,every segment becomes a dict for flexibility purposes
2.0.0rc2,NOTE: translator returns lists of `n_best` list
2.0.0rc2,build back results with empty texts
2.0.0rc2,load can be called multiple times: modify copy
2.0.0rc2,output contain alignment
2.0.0rc2,Below are all the different penalty terms implemented so far.
2.0.0rc2,Subtract coverage penalty from topk log probs.
2.0.0rc2,Divide topk log probs by length penalty.
2.0.0rc2,Sorting
2.0.0rc2,Chinese segmentation
2.0.0rc2,Chinese simplify -> Chinese traditional standard
2.0.0rc2,Chinese simplify -> Chinese traditional (HongKong)
2.0.0rc2,Chinese simplify -> Chinese traditional (Taiwan)
2.0.0rc2,Chinese traditional -> Chinese simplify (v1)
2.0.0rc2,Chinese traditional -> Chinese simplify (v2)
2.0.0rc1,!/usr/bin/env python
2.0.0rc1,!/usr/bin/env python
2.0.0rc1,!/usr/bin/env python
2.0.0rc1,!/usr/bin/env python
2.0.0rc1,!/usr/bin/env python3
2.0.0rc1,-*- coding: utf-8 -*-
2.0.0rc1,
2.0.0rc1,"OpenNMT-py documentation build configuration file, created by"
2.0.0rc1,sphinx-quickstart on Sun Dec 17 12:07:14 2017.
2.0.0rc1,
2.0.0rc1,This file is execfile()d with the current directory set to its
2.0.0rc1,containing dir.
2.0.0rc1,
2.0.0rc1,Note that not all possible configuration values are present in this
2.0.0rc1,autogenerated file.
2.0.0rc1,
2.0.0rc1,All configuration values have a default; values that are commented out
2.0.0rc1,serve to show the default.
2.0.0rc1,"If extensions (or modules to document with autodoc) are in another directory,"
2.0.0rc1,add these directories to sys.path here. If the directory is relative to the
2.0.0rc1,"documentation root, use os.path.abspath to make it absolute, like shown here."
2.0.0rc1,
2.0.0rc1,import os
2.0.0rc1,import sys
2.0.0rc1,"sys.path.insert(0, os.path.abspath('.'))"
2.0.0rc1,-- General configuration ------------------------------------------------
2.0.0rc1,"If your documentation needs a minimal Sphinx version, state it here."
2.0.0rc1,
2.0.0rc1,needs_sphinx = '1.0'
2.0.0rc1,"Add any Sphinx extension module names here, as strings. They can be"
2.0.0rc1,extensions coming with Sphinx (named 'sphinx.ext.*') or your custom
2.0.0rc1,ones.
2.0.0rc1,Show base classes
2.0.0rc1,"Use ""variables"" section for Attributes instead of weird block things"
2.0.0rc1,mimicking the function style.
2.0.0rc1,"Add any paths that contain templates here, relative to this directory."
2.0.0rc1,The suffix(es) of source filenames.
2.0.0rc1,You can specify multiple suffix as a list of string:
2.0.0rc1,
2.0.0rc1,"source_suffix = ['.rst', '.md']"
2.0.0rc1,The master toctree document.
2.0.0rc1,General information about the project.
2.0.0rc1,"The version info for the project you're documenting, acts as replacement for"
2.0.0rc1,"|version| and |release|, also used in various other places throughout the"
2.0.0rc1,built documents.
2.0.0rc1,
2.0.0rc1,The short X.Y version.
2.0.0rc1,"The full version, including alpha/beta/rc tags."
2.0.0rc1,The language for content autogenerated by Sphinx. Refer to documentation
2.0.0rc1,for a list of supported languages.
2.0.0rc1,
2.0.0rc1,This is also used if you do content translation via gettext catalogs.
2.0.0rc1,"Usually you set ""language"" from the command line for these cases."
2.0.0rc1,"List of patterns, relative to source directory, that match files and"
2.0.0rc1,directories to ignore when looking for source files.
2.0.0rc1,This patterns also effect to html_static_path and html_extra_path
2.0.0rc1,The name of the Pygments (syntax highlighting) style to use.
2.0.0rc1,"If true, `todo` and `todoList` produce output, else they produce nothing."
2.0.0rc1,-- Options for HTML output ----------------------------------------------
2.0.0rc1,The theme to use for HTML and HTML Help pages.  See the documentation for
2.0.0rc1,a list of builtin themes.
2.0.0rc1,
2.0.0rc1,html_theme = 'sphinx_materialdesign_theme'
2.0.0rc1,html_theme_path = [sphinx_materialdesign_theme.get_path()]
2.0.0rc1,Theme options are theme-specific and customize the look and feel of a theme
2.0.0rc1,"further.  For a list of options available for each theme, see the"
2.0.0rc1,documentation.
2.0.0rc1,
2.0.0rc1,html_theme_options = {}
2.0.0rc1,"Add any paths that contain custom static files (such as style sheets) here,"
2.0.0rc1,"relative to this directory. They are copied after the builtin static files,"
2.0.0rc1,"so a file named ""default.css"" will overwrite the builtin ""default.css""."
2.0.0rc1,"Custom sidebar templates, must be a dictionary that maps document names"
2.0.0rc1,to template names.
2.0.0rc1,
2.0.0rc1,This is required for the alabaster theme
2.0.0rc1,refs: http://alabaster.readthedocs.io/en/latest/installation.html#sidebars
2.0.0rc1,-- Options for HTMLHelp output ------------------------------------------
2.0.0rc1,Output file base name for HTML help builder.
2.0.0rc1,-- Options for LaTeX output ---------------------------------------------
2.0.0rc1,The paper size ('letterpaper' or 'a4paper').
2.0.0rc1,
2.0.0rc1,"'papersize': 'letterpaper',"
2.0.0rc1,"The font size ('10pt', '11pt' or '12pt')."
2.0.0rc1,
2.0.0rc1,"'pointsize': '10pt',"
2.0.0rc1,Additional stuff for the LaTeX preamble.
2.0.0rc1,
2.0.0rc1,"'preamble': '',"
2.0.0rc1,Latex figure (float) alignment
2.0.0rc1,
2.0.0rc1,"'figure_align': 'htbp',"
2.0.0rc1,Grouping the document tree into LaTeX files. List of tuples
2.0.0rc1,"(source start file, target name, title,"
2.0.0rc1,"author, documentclass [howto, manual, or own class])."
2.0.0rc1,-- Options for manual page output ---------------------------------------
2.0.0rc1,One entry per manual page. List of tuples
2.0.0rc1,"(source start file, name, description, authors, manual section)."
2.0.0rc1,-- Options for Texinfo output -------------------------------------------
2.0.0rc1,Grouping the document tree into Texinfo files. List of tuples
2.0.0rc1,"(source start file, target name, title, author,"
2.0.0rc1,"dir menu entry, description, category)"
2.0.0rc1,!/usr/bin/env python
2.0.0rc1,-*- coding: utf-8 -*-
2.0.0rc1,is this reachable?
2.0.0rc1,Read in embeddings
2.0.0rc1,Write to file
2.0.0rc1,converts a SentencePiece vocabulary to the format expected by dynamic data
2.0.0rc1,"(essentially converts float expected counts to ""fixed precision"" int pseudo"
2.0.0rc1,counts)
2.0.0rc1,"Add in default model arguments, possibly added since training."
2.0.0rc1,-*- encoding: utf-8 -*-
2.0.0rc1,!/usr/bin/env python
2.0.0rc1,-*- coding: utf-8 -*-
2.0.0rc1,Author: Rico Sennrich
2.0.0rc1,flake8: noqa
2.0.0rc1,This file is retrieved from https://github.com/rsennrich/subword-nmt
2.0.0rc1,hack for python2/3 compatibility
2.0.0rc1,check version information
2.0.0rc1,some hacking to deal with duplicates (only consider first instance)
2.0.0rc1,don't print end-of-word symbols
2.0.0rc1,sys.stderr.write('cannot split {0} further.\n'.format(segment))
2.0.0rc1,sys.stderr.write('OOV: {0}\n'.format(segment))
2.0.0rc1,sys.stderr.write('OOV: {0}\n'.format(segment))
2.0.0rc1,python 2/3 compatibility
2.0.0rc1,read/write files as UTF-8
2.0.0rc1,!/usr/bin/env python
2.0.0rc1,!/usr/bin/env python
2.0.0rc1,-*- coding: utf-8 -*-
2.0.0rc1,Author: Rico Sennrich
2.0.0rc1,flake8: noqa
2.0.0rc1,This file is retrieved from https://github.com/rsennrich/subword-nmt
2.0.0rc1,hack for python2/3 compatibility
2.0.0rc1,"find all instances of pair, and update frequency/indices around it"
2.0.0rc1,find first symbol
2.0.0rc1,"if first symbol is followed by second symbol, we've found an occurrence of pair (old_word[i:i+2])"
2.0.0rc1,"assuming a symbol sequence ""A B C"", if ""B C"" is merged, reduce the frequency of ""A B"""
2.0.0rc1,"assuming a symbol sequence ""A B C B"", if ""B C"" is merged, reduce the frequency of ""C B""."
2.0.0rc1,"however, skip this if the sequence is A B C B C, because the frequency of ""C B"" will be reduced by the previous code block"
2.0.0rc1,find new pair
2.0.0rc1,"assuming a symbol sequence ""A BC D"", if ""B C"" is merged, increase the frequency of ""A BC"""
2.0.0rc1,"assuming a symbol sequence ""A BC B"", if ""B C"" is merged, increase the frequency of ""BC B"""
2.0.0rc1,"however, if the sequence is A BC BC, skip this step because the count of ""BC BC"" will be incremented by the previous code block"
2.0.0rc1,data structure of pair frequencies
2.0.0rc1,index from pairs to words
2.0.0rc1,version 0.2 changes the handling of the end-of-word token ('</w>');
2.0.0rc1,version numbering allows bckward compatibility
2.0.0rc1,"threshold is inspired by Zipfian assumption, but should only affect speed"
2.0.0rc1,we probably missed the best pair because of pruning; go back to full statistics
2.0.0rc1,"threshold is inspired by Zipfian assumption, but should only affect speed"
2.0.0rc1,python 2/3 compatibility
2.0.0rc1,read/write files as UTF-8
2.0.0rc1,!/usr/bin/env python
2.0.0rc1,-*- coding: utf-8 -*-
2.0.0rc1,!/usr/bin/env python
2.0.0rc1,for back compat when attention_dropout was not defined
2.0.0rc1,Build embeddings.
2.0.0rc1,Build encoder.
2.0.0rc1,Build decoder.
2.0.0rc1,Share the embedding matrix - preprocess with share_vocab required.
2.0.0rc1,src/tgt vocab should be the same if `-share_vocab` is specified.
2.0.0rc1,Build NMTModel(= encoder + decoder).
2.0.0rc1,Build Generator.
2.0.0rc1,Load the model states from checkpoint or initialize them.
2.0.0rc1,This preserves backward-compat for models using customed layernorm
2.0.0rc1,end of patch for backward compatibility
2.0.0rc1,!/usr/bin/env python
2.0.0rc1,NOTE: It's important that ``opt`` has been validated and updated
2.0.0rc1,at this point.
2.0.0rc1,Build model.
2.0.0rc1,Build optimizer.
2.0.0rc1,Build model saver
2.0.0rc1,Move batch to specified device
2.0.0rc1,Use Tensorboard for visualization during training
2.0.0rc1,Options only during inference
2.0.0rc1,"Truncation options, for text corpus"
2.0.0rc1,"as for False, this will be added in _add_train_general_opts"
2.0.0rc1,Embedding Options
2.0.0rc1,Encoder-Decoder Options
2.0.0rc1,"group.add('--residual', '-residual',   action=""store_true"","
2.0.0rc1,"help=""Add residual connections between RNN layers."")"
2.0.0rc1,The following options (bridge_extra_node to src_vocab) are used
2.0.0rc1,for training with --encoder_type ggnn (Gated Graph Neural Network).
2.0.0rc1,Attention options
2.0.0rc1,Alignement options
2.0.0rc1,Generator and loss options.
2.0.0rc1,GPU
2.0.0rc1,Init options
2.0.0rc1,Pretrained word vectors
2.0.0rc1,Fixed word vectors
2.0.0rc1,Optimization options
2.0.0rc1,learning rate
2.0.0rc1,options relate to data preprare
2.0.0rc1,options relate to train
2.0.0rc1,Alpha and Beta values for Google Length + Coverage penalty
2.0.0rc1,"Described here: https://arxiv.org/pdf/1609.08144.pdf, Section 7"
2.0.0rc1,Adding options relate to decoding strategy
2.0.0rc1,Adding option for logging
2.0.0rc1,Copyright 2016 The Chromium Authors. All rights reserved.
2.0.0rc1,Use of this source code is governed by a BSD-style license that can be
2.0.0rc1,found in the LICENSE file.
2.0.0rc1,"Get the key 'value' in the dict, or just use 'value'"
2.0.0rc1,Basic attributes.
2.0.0rc1,Set model in training mode.
2.0.0rc1,UPDATE DROPOUT
2.0.0rc1,Run patience mechanism
2.0.0rc1,"If the patience has reached the limit, stop training"
2.0.0rc1,swap model params w/ moving average
2.0.0rc1,(and keep the original parameters)
2.0.0rc1,Set model in validating mode.
2.0.0rc1,F-prop through the model.
2.0.0rc1,Compute loss.
2.0.0rc1,Update statistics.
2.0.0rc1,Set model back to training mode.
2.0.0rc1,Truncated BPTT: reminder not compatible with accum > 1
2.0.0rc1,1. Create truncated target.
2.0.0rc1,2. F-prop all but generator.
2.0.0rc1,3. Compute loss.
2.0.0rc1,4. Update the parameters and statistics.
2.0.0rc1,Multi GPU gradient gather
2.0.0rc1,"If truncated, don't backprop fully."
2.0.0rc1,TO CHECK
2.0.0rc1,if dec_state is not None:
2.0.0rc1,dec_state.detach()
2.0.0rc1,"in case of multi step gradient accumulation,"
2.0.0rc1,update only after accum batches
2.0.0rc1,For Flake
2.0.0rc1,we avoid padding while mean pooling
2.0.0rc1,incoming and outgoing edge embedding
2.0.0rc1,Find vocab data for tree builting
2.0.0rc1,Propogation Model
2.0.0rc1,Initialize the bridge layer
2.0.0rc1,Initialize graph using formatted input sequence
2.0.0rc1,Number of flagged nodes defines node count for this sample
2.0.0rc1,"(Nodes can have no flags on them, but must be in 'flags' list)."
2.0.0rc1,The total number of integers in the vocab should allow
2.0.0rc1,for all features and edges to be defined.
2.0.0rc1,Use first extra node as only source for decoder init
2.0.0rc1,Average all nodes to get bridge input
2.0.0rc1,"LSTM has hidden and cell state, other only one"
2.0.0rc1,Total number of states
2.0.0rc1,Build a linear layer for each
2.0.0rc1,Initialize the bridge layer
2.0.0rc1,"s_len, batch, emb_dim = emb.size()"
2.0.0rc1,Lengths data is wrapped inside a Tensor.
2.0.0rc1,"LSTM has hidden and cell state, other only one"
2.0.0rc1,Total number of states
2.0.0rc1,Build a linear layer for each
2.0.0rc1,"s_len, batch, emb_dim = emb.size()"
2.0.0rc1,Run the forward pass of every layer of the tranformer.
2.0.0rc1,Dimensions and padding for constructing the word embedding matrix
2.0.0rc1,Dimensions and padding for feature embedding matrices
2.0.0rc1,(these have no effect if feat_vocab_sizes is empty)
2.0.0rc1,The embedding matrix look-up tables. The first look-up table
2.0.0rc1,"is for words. Subsequent ones are for features, if any exist."
2.0.0rc1,The final output size of word + feature vectors. This can vary
2.0.0rc1,from the word vector size if and only if features are defined.
2.0.0rc1,This is the attribute you should access if you need to know
2.0.0rc1,how big your embeddings are going to be.
2.0.0rc1,The sequence of operations that converts the input sequence
2.0.0rc1,into a sequence of embeddings. At minimum this consists of
2.0.0rc1,looking up the embeddings for each word and feature in the
2.0.0rc1,input. Model parameters may require the sequence to contain
2.0.0rc1,additional operations as well.
2.0.0rc1,features must use word_vec_size
2.0.0rc1,features will use feat_vec_size
2.0.0rc1,Some utilitary functions for pretrained embeddings
2.0.0rc1,is this reachable?
2.0.0rc1,Write to file
2.0.0rc1,set the opt in place
2.0.0rc1,set the opt in place
2.0.0rc1,This class is mainly used by decoder.py for RNNs but also
2.0.0rc1,by the CNN / transformer decoder when copy attention is used
2.0.0rc1,CNN has its own attention mechanism ConvMultiStepAttention
2.0.0rc1,Transformer has its own MultiHeadedAttention
2.0.0rc1,mlp wants it with bias
2.0.0rc1,Check input sizes
2.0.0rc1,"(batch, t_len, d) x (batch, d, s_len) --> (batch, t_len, s_len)"
2.0.0rc1,"(batch, t_len, s_len, d)"
2.0.0rc1,one step input
2.0.0rc1,"compute attention scores, as in Luong et al."
2.0.0rc1,Softmax or sparsemax to normalize attention weights
2.0.0rc1,each context vector c_t is the weighted average
2.0.0rc1,over all the source hidden states
2.0.0rc1,concatenate
2.0.0rc1,Check output sizes
2.0.0rc1,Check output sizes
2.0.0rc1,clamping necessary because of numerical errors: loss should be lower
2.0.0rc1,"bounded by zero, but negative values near zero are possible without"
2.0.0rc1,the clamp
2.0.0rc1,from onmt.utils.misc import aeq
2.0.0rc1,CHECKS
2.0.0rc1,"batch, k_len, d = key.size()"
2.0.0rc1,"batch_, k_len_, d_ = value.size()"
2.0.0rc1,"aeq(batch, batch_)"
2.0.0rc1,"aeq(k_len, k_len_)"
2.0.0rc1,"aeq(d, d_)"
2.0.0rc1,"batch_, q_len, d_ = query.size()"
2.0.0rc1,"aeq(batch, batch_)"
2.0.0rc1,"aeq(d, d_)"
2.0.0rc1,"aeq(self.model_dim % 8, 0)"
2.0.0rc1,if mask is not None:
2.0.0rc1,"batch_, q_len_, k_len_ = mask.size()"
2.0.0rc1,"aeq(batch_, batch)"
2.0.0rc1,"aeq(k_len_, k_len)"
2.0.0rc1,aeq(q_len_ == q_len)
2.0.0rc1,END CHECKS
2.0.0rc1,"1) Project key, value, and query."
2.0.0rc1,1 or key_len x key_len
2.0.0rc1,1 or key_len x key_len x dim_per_head
2.0.0rc1,1 or key_len x key_len x dim_per_head
2.0.0rc1,2) Calculate and scale scores.
2.0.0rc1,batch x num_heads x query_len x key_len
2.0.0rc1,3) Apply attention dropout and compute context vectors.
2.0.0rc1,CHECK
2.0.0rc1,"batch_, q_len_, d_ = output.size()"
2.0.0rc1,"aeq(q_len, q_len_)"
2.0.0rc1,"aeq(batch, batch_)"
2.0.0rc1,"aeq(d, d_)"
2.0.0rc1,Return multi-head attn
2.0.0rc1,At the moment this class is only used by embeddings.Embeddings look-up tables
2.0.0rc1,-*- coding: utf-8 -*-
2.0.0rc1,checks
2.0.0rc1,"batch, channel, height, width = base_target_emb.size()"
2.0.0rc1,"batch_, channel_, height_, width_ = input_from_dec.size()"
2.0.0rc1,"enc_batch, enc_channel, enc_height = encoder_out_top.size()"
2.0.0rc1,"enc_batch_, enc_channel_, enc_height_ = encoder_out_combine.size()"
2.0.0rc1,out_features * in_features
2.0.0rc1,norm is out_features * 1
2.0.0rc1,batch_size * out_features
2.0.0rc1,out_features
2.0.0rc1,out_features
2.0.0rc1,batch_size * out_features
2.0.0rc1,"out_channels, in_channels // groups, * kernel_size"
2.0.0rc1,out_features
2.0.0rc1,This is used nowhere in the code at the moment (Vincent Nguyen 05/18/2018)
2.0.0rc1,"in_channels, out_channels, *kernel_size"
2.0.0rc1,"in_channels, out_channels, *kernel_size"
2.0.0rc1,"self.out_channels, 1"
2.0.0rc1,out_features
2.0.0rc1,out_features
2.0.0rc1,store roots on diagonal
2.0.0rc1,CHECKS
2.0.0rc1,Original probabilities.
2.0.0rc1,Probability of copying p(z=1) batch.
2.0.0rc1,Probability of not copying: p_{word}(w) * (1 - p(z))
2.0.0rc1,probabilities assigned by the model to the gold targets
2.0.0rc1,probability of tokens copied from source
2.0.0rc1,Set scores for unk to 0 and add eps
2.0.0rc1,find the indices in which you do not use the copy mechanism
2.0.0rc1,Drop padding.
2.0.0rc1,this block does not depend on the loss value computed above
2.0.0rc1,and is used only for stats
2.0.0rc1,this block does not depend on the loss value computed above
2.0.0rc1,and is used only for stats
2.0.0rc1,Correct target copy token instead of <unk>
2.0.0rc1,tgt[i] = align[i] + len(tgt_vocab)
2.0.0rc1,for i such that tgt[i] == 0 and align[i] != 0
2.0.0rc1,Compute sum of perplexities for stats
2.0.0rc1,this part looks like it belongs in CopyGeneratorLoss
2.0.0rc1,Compute Loss as NLL divided by seq length
2.0.0rc1,Compute Total Loss per sequence in batch
2.0.0rc1,Divide by length of each sequence and sum
2.0.0rc1,Auto import python files in this directory
2.0.0rc1,1. sample number of tokens to corrupt
2.0.0rc1,2. sample positions to corrput
2.0.0rc1,3. sample corrupted values
2.0.0rc1,1. sample number of tokens to corrupt
2.0.0rc1,2. sample positions to corrput
2.0.0rc1,3. Drop token on chosen position
2.0.0rc1,1. sample number of tokens to corrupt
2.0.0rc1,2. sample positions to corrput
2.0.0rc1,3. mask word on chosen position
2.0.0rc1,"Sharing options among `TokenizerTransform`s, same name conflict in"
2.0.0rc1,this scope will be resolved by remove previous occurrence in parser
2.0.0rc1,subword regularization(or BPE dropout) options:
2.0.0rc1,derterministic subwording
2.0.0rc1,subword sampling when nbest_size > 1 or -1
2.0.0rc1,alpha should be 0.0 < alpha < 1.0
2.0.0rc1,-1: keep everything (i.e. 1 mask per token)
2.0.0rc1,0: replace everything (i.e. no mask)
2.0.0rc1,1: 1 mask per span
2.0.0rc1,view each subword as word start / input is word level token
2.0.0rc1,Pretend it ends with a full stop so last span is a sentence
2.0.0rc1,"Tokens that are full stops, where the previous token is not"
2.0.0rc1,Make sure we have enough to mask
2.0.0rc1,Trim to masking budget
2.0.0rc1,Handle 0-length mask (inserts) separately
2.0.0rc1,assert is_word_start[-1] == 0
2.0.0rc1,assert tokens_length - 1 not in indices
2.0.0rc1,"keep index, but replace it with [MASK]"
2.0.0rc1,"acts as a long length, so spans don't go over the end of doc"
2.0.0rc1,next position from each word_start
2.0.0rc1,delete token: 1 mask/remove per span
2.0.0rc1,"keep index, but replace it with [MASK]: 1 mask per token"
2.0.0rc1,A bit faster when all lengths are 1
2.0.0rc1,to cover whole token
2.0.0rc1,delete token
2.0.0rc1,"keep index, but replace it with [MASK]"
2.0.0rc1,assert tokens_length - 1 not in indices
2.0.0rc1,initialize fields at the top of each unit test to prevent
2.0.0rc1,any undesired stateful effects
2.0.0rc1,"this test touches the file system, so it could be considered an"
2.0.0rc1,integration test
2.0.0rc1,write utf-8 bytes
2.0.0rc1,batch 0 will always predict EOS. The other batches will predict
2.0.0rc1,non-eos scores.
2.0.0rc1,"""best"" prediction is eos - that should be blocked"
2.0.0rc1,include at least one prediction OTHER than EOS
2.0.0rc1,that is greater than -1e20
2.0.0rc1,now batch 0 has ended and no others have
2.0.0rc1,initial step
2.0.0rc1,batch 0 dies on step 0
2.0.0rc1,include at least one prediction OTHER than EOS
2.0.0rc1,that is greater than -1e20
2.0.0rc1,step 2
2.0.0rc1,(old) batch 8 dies on step 1
2.0.0rc1,step 3
2.0.0rc1,everything dies
2.0.0rc1,initial step
2.0.0rc1,batch 0 dies on step 0
2.0.0rc1,include at least one prediction OTHER than EOS
2.0.0rc1,that is greater than -1e20
2.0.0rc1,step 2
2.0.0rc1,(old) batch 8 dies on step 1
2.0.0rc1,step 3
2.0.0rc1,everything dies
2.0.0rc1,illegal_weights_mask = torch.ByteTensor([
2.0.0rc1,"[0, 0, 0, 0, 0, 0, 0],"
2.0.0rc1,"[0, 0, 0, 1, 1, 1, 1],"
2.0.0rc1,"[0, 0, 0, 0, 0, 1, 1],"
2.0.0rc1,"[0, 0, 1, 1, 1, 1, 1]])"
2.0.0rc1,TODO: fix for pytorch 0.3
2.0.0rc1,illegal_weights = alignments.masked_select(illegal_weights_mask)
2.0.0rc1,"self.assertEqual(0.0, illegal_weights.data.sum())"
2.0.0rc1,this could be considered an integration test because it touches
2.0.0rc1,the filesystem for the config file (and the models)
2.0.0rc1,!/usr/bin/env python
2.0.0rc1,-*- coding: utf-8 -*-
2.0.0rc1,Inject some dummy training options that may needed when build fields
2.0.0rc1,Remove the generated *pt files.
2.0.0rc1,Remove the generated data samples
2.0.0rc1,all beams repeat (beam >= 1 repeat dummy scores)
2.0.0rc1,predict repeat_idx over and over again
2.0.0rc1,"before repeat, scores are either 0 or -inf"
2.0.0rc1,"on repeat, `repeat_idx` score is BLOCKED_SCORE"
2.0.0rc1,"(but it's still the best score, thus we have"
2.0.0rc1,"[BLOCKED_SCORE, -inf, -inf, -inf, -inf]"
2.0.0rc1,repetitions keeps maximizing score
2.0.0rc1,"index 0 has been blocked, so repeating=>+0.0 score"
2.0.0rc1,other indexes are -inf so repeating=>BLOCKED_SCORE
2.0.0rc1,which is higher
2.0.0rc1,beam 0 and beam >=2 will repeat (beam >= 2 repeat dummy scores)
2.0.0rc1,non-interesting beams are going to get dummy values
2.0.0rc1,"on initial round, only predicted scores for beam 0"
2.0.0rc1,matter. Make two predictions. Top one will be repeated
2.0.0rc1,"in beam zero, second one will live on in beam 1."
2.0.0rc1,predict the same thing in beam 0
2.0.0rc1,continue pushing around what beam 1 predicts
2.0.0rc1,"now beam 0 dies (along with the others), beam 1 -> beam 0"
2.0.0rc1,"now beam 0 dies (along with the others), beam 1 -> beam 0"
2.0.0rc1,beam 0 and beam >= 2 will repeat (beam 2 repeats excluded idx)
2.0.0rc1,non-interesting beams are going to get dummy values
2.0.0rc1,predict the same thing in beam 0
2.0.0rc1,continue pushing around what beam 1 predicts
2.0.0rc1,predict the allowed-repeat again in beam 2
2.0.0rc1,"now beam 0 dies, beam 1 -> beam 0, beam 2 -> beam 1"
2.0.0rc1,and the rest die
2.0.0rc1,"since all preds after i=0 are 0, we can check"
2.0.0rc1,that the beam is the correct idx by checking that
2.0.0rc1,the curr score is the initial score
2.0.0rc1,beam 0 will always predict EOS. The other beams will predict
2.0.0rc1,non-eos scores.
2.0.0rc1,non-interesting beams are going to get dummy values
2.0.0rc1,"""best"" prediction is eos - that should be blocked"
2.0.0rc1,include at least beam_sz predictions OTHER than EOS
2.0.0rc1,that are greater than -1e20
2.0.0rc1,predict eos in beam 0
2.0.0rc1,provide beam_sz other good predictions
2.0.0rc1,now the top beam has ended and no others have
2.0.0rc1,"not of interest, but want to make sure it keeps running"
2.0.0rc1,since only beam 0 terminates and n_best = 2
2.0.0rc1,"this is also a test that when block_ngram_repeat=0,"
2.0.0rc1,repeating is acceptable
2.0.0rc1,non-interesting beams are going to get dummy values
2.0.0rc1,"""best"" prediction is eos - that should be blocked"
2.0.0rc1,include at least beam_sz predictions OTHER than EOS
2.0.0rc1,that are greater than -1e20
2.0.0rc1,predict eos in beam 1
2.0.0rc1,provide beam_sz other good predictions in other beams
2.0.0rc1,provide beam_sz other good predictions in other beams
2.0.0rc1,beam 1 dies on min_length
2.0.0rc1,beam 0 dies on the step after beam 1 dies
2.0.0rc1,"inp_lens is tiled in initialize, reassign to make attn match"
2.0.0rc1,non-interesting beams are going to get dummy values
2.0.0rc1,"""best"" prediction is eos - that should be blocked"
2.0.0rc1,include at least beam_sz predictions OTHER than EOS
2.0.0rc1,that are greater than -1e20
2.0.0rc1,predict eos in beam 1
2.0.0rc1,provide beam_sz other good predictions in other beams
2.0.0rc1,provide beam_sz other good predictions in other beams
2.0.0rc1,no top beams are finished yet
2.0.0rc1,beam 1 dies on min_length
2.0.0rc1,no top beams are finished yet
2.0.0rc1,beam 0 dies on the step after beam 1 dies
2.0.0rc1,top beam is finished now so there are attentions
2.0.0rc1,two beams are finished in each batch
2.0.0rc1,second dim is cut down to the non-padded src length
2.0.0rc1,first dim is equal to the time of death
2.0.0rc1,(beam 0 died at current step - adjust for SOS)
2.0.0rc1,(beam 1 died at last step - adjust for SOS)
2.0.0rc1,behavior gets weird when beam is already done so just stop
2.0.0rc1,this is just test_beam.TestBeamAgainstReferenceCase repeated
2.0.0rc1,in each batch.
2.0.0rc1,"init_preds: [4, 3, 5, 6, 7] - no EOS's"
2.0.0rc1,no EOS's yet
2.0.0rc1,"[5, 3, 2, 6, 0], so beam 2 predicts EOS!"
2.0.0rc1,assumes beam 2 finished on last step
2.0.0rc1,ended beam 2 shouldn't continue
2.0.0rc1,"[2, 5, 3, 6, 0] repeat self.BATCH_SZ, so beam 0 predicts EOS!"
2.0.0rc1,"[-2.4879, -3.8910, -4.1010, -4.2010, -4.4010] repeat self.BATCH_SZ"
2.0.0rc1,another beam is finished in all batches
2.0.0rc1,new beam 0 finished
2.0.0rc1,new beam 0 is old beam 3
2.0.0rc1,assumes beam 0 finished on last step
2.0.0rc1,"[5, 2, 6, 1, 0] repeat self.BATCH_SZ, so beam 1 predicts EOS!"
2.0.0rc1,new beam 1 finished
2.0.0rc1,new beam 1 is old beam 4
2.0.0rc1,this could be considered an integration test because it tests
2.0.0rc1,interactions between the GNMT scorer and the beam
2.0.0rc1,"-data option is required, but not used in this test, so dummy."
2.0.0rc1,len x batch x nfeat
2.0.0rc1,Initialize vectors to compare size with
2.0.0rc1,Ensure correct sizes and types
2.0.0rc1,Make sure that output has the correct size and type
2.0.0rc1,"[('encoder_type', 'transformer'),"
2.0.0rc1,"('word_vec_size', 16), ('rnn_size', 16)],"
2.0.0rc1,""""""" Only do SRU test if requirment is safisfied. """""""
2.0.0rc1,SRU doesn't support input_feed.
2.0.0rc1,first check there's nothing unexpectedly not trainable
2.0.0rc1,ok: word embeddings shouldn't be trainable
2.0.0rc1,if word vecs are fixed
2.0.0rc1,ok: positional encodings shouldn't be trainable
2.0.0rc1,then check nothing unexpectedly trainable
2.0.0rc1,Decoder state
2.0.0rc1,Build the RNN.
2.0.0rc1,Set up the context gate.
2.0.0rc1,Set up the standard attention.
2.0.0rc1,The encoder hidden is  (layers*directions) x batch x dim.
2.0.0rc1,We need to convert it to layers x batch x (directions*dim).
2.0.0rc1,Init the input feed.
2.0.0rc1,Update the state with the result.
2.0.0rc1,Concatenates sequence of tensors along a new dimension.
2.0.0rc1,NOTE: v0.3 to 0.4: dec_outs / attns[*] may not be list
2.0.0rc1,(in particular in case of SRU) it was not raising error in 0.3
2.0.0rc1,since stack(Variable) was allowed.
2.0.0rc1,"In 0.4, SRU returns a tensor that shouldn't be stacke"
2.0.0rc1,Check
2.0.0rc1,Calculate the attention.
2.0.0rc1,Calculate the context gate.
2.0.0rc1,Additional args check.
2.0.0rc1,END Additional args check.
2.0.0rc1,Input feed concatenates hidden state with
2.0.0rc1,input at every time step.
2.0.0rc1,TODO: context gate should be employed
2.0.0rc1,instead of second RNN transform.
2.0.0rc1,Update the coverage attention.
2.0.0rc1,Decoder State
2.0.0rc1,CNNDecoder has its own attention mechanism.
2.0.0rc1,Set up a separate copy attention layer if needed.
2.0.0rc1,The output of CNNEncoder.
2.0.0rc1,The combination of output of CNNEncoder and source embeddings.
2.0.0rc1,Process the result and update the attentions.
2.0.0rc1,Update the state.
2.0.0rc1,TODO change the way attns is returned dict => list or tuple (onnx)
2.0.0rc1,Memory_lengths is a single tensor shared between all models.
2.0.0rc1,This assumption will not hold if Translator is modified
2.0.0rc1,to calculate memory_lengths as something other than the length
2.0.0rc1,of the input.
2.0.0rc1,"return _, (B, Q_len, K_len)"
2.0.0rc1,"layer average attention across heads, get ``(B, Q, K)``"
2.0.0rc1,"Case 1: no full_context, no align heads -> layer avg baseline"
2.0.0rc1,"Case 2: no full_context, 1 align heads -> guided align"
2.0.0rc1,"Case 3: full_context, 1 align heads -> full cte guided align"
2.0.0rc1,T: could be 1 in the case of stepwise decoding or tgt_len
2.0.0rc1,BoolTensor was introduced in pytorch 1.2
2.0.0rc1,Decoder State
2.0.0rc1,"previously, there was a GlobalAttention module here for copy"
2.0.0rc1,"attention. But it was never actually used -- the ""copy"" attention"
2.0.0rc1,just reuses the context attention.
2.0.0rc1,"attns[""align""] = torch.stack(attn_aligns, 0).mean(0)  # All avg"
2.0.0rc1,TODO change the way attns is returned dict => list or tuple (onnx)
2.0.0rc1,"buffer size in bytes, determine equiv. # of elements based on data type"
2.0.0rc1,copy tensors into buffer_t
2.0.0rc1,all-reduce and rescale
2.0.0rc1,copy all-reduced buffer back into tensors
2.0.0rc1,"tensor is bigger than buffer, all-reduce and rescale directly"
2.0.0rc1,"buffer is full, all-reduce and replace buffer with grad"
2.0.0rc1,add tensor to buffer
2.0.0rc1,NOTE: stride (if needed) is handled at the
2.0.0rc1,generator (train_iter) level
2.0.0rc1,Move batch to correspond device_id when consumer iterate
2.0.0rc1,hack to dodge unpicklable `dict_keys`
2.0.0rc1,"propagate exception to parent process, keeping original traceback"
2.0.0rc1,TODO: Find a better way to check for sparse gradients.
2.0.0rc1,we use here a FusedAdam() copy of an old Apex repo
2.0.0rc1,In this case use the old FusedAdam with FP16_optimizer wrapper
2.0.0rc1,Load everything from the checkpoint.
2.0.0rc1,Build everything from scratch.
2.0.0rc1,"Reset optimizer, keep options."
2.0.0rc1,"Reset options, keep optimizer."
2.0.0rc1,State can be partially restored.
2.0.0rc1,"unscaled optimizer's gradients (already done therefore skip),"
2.0.0rc1,skips optimizer.step() if gradients contain infs/NaNs.
2.0.0rc1,Updates the scale for next iteration.
2.0.0rc1,Code below is an implementation of https://arxiv.org/pdf/1804.04235.pdf
2.0.0rc1,inspired but modified from https://github.com/DeadAt0m/adafactor-pytorch
2.0.0rc1,backward compatibility
2.0.0rc1,assuming a list/generator of parameter means single group
2.0.0rc1,compute combined scale factor for this group
2.0.0rc1,norm is in fact norm*scale
2.0.0rc1,note: p.grad should not ever be set for correct operation of
2.0.0rc1,mixed precision optimizer that sometimes sends None gradients
2.0.0rc1,State initialization
2.0.0rc1,Exponential moving average of gradient values
2.0.0rc1,Exponential moving average of squared gradient values
2.0.0rc1,-*- coding: utf-8 -*-
2.0.0rc1,if the loss function operates on vectors of raw logits instead of
2.0.0rc1,"probabilities, only the first part of the generator needs to be"
2.0.0rc1,"passed to the NMTLossCompute. At the moment, the only supported"
2.0.0rc1,loss function of this kind is the sparsemax loss.
2.0.0rc1,"attn_align should be in (batch_size, pad_tgt_size, pad_src_size)"
2.0.0rc1,"align_idx should be a Tensor in size([N, 3]), N is total number"
2.0.0rc1,"of align src-tgt pair in current batch, each as"
2.0.0rc1,"['sent_NÂ°_in_batch', 'tgt_id+1', 'src_id'] (check AlignField)"
2.0.0rc1,NOTE: tgt-src ref alignement that in range_ of shard
2.0.0rc1,(coherent with batch.tgt)
2.0.0rc1,"align_head contains value in [0, 1) presenting attn prob,"
2.0.0rc1,0 was resulted by the context attention src_pad_mask
2.0.0rc1,"So, the correspand position in ref_align should also be 0"
2.0.0rc1,"Therefore, clip align_head to > 1e-18 should be bias free."
2.0.0rc1,non_none: the subdict of the state dictionary where the values
2.0.0rc1,are not None.
2.0.0rc1,"Now, the iteration:"
2.0.0rc1,state is a dictionary of sequences of tensor-like but we
2.0.0rc1,want a sequence of dictionaries of tensors.
2.0.0rc1,"First, unzip the dictionary into a sequence of keys and a"
2.0.0rc1,sequence of tensor-like sequences.
2.0.0rc1,"Now, yield a dictionary for each shard. The keys are always"
2.0.0rc1,the same. values is a sequence of length #keys where each
2.0.0rc1,element is a sequence of length #shards. We want to iterate
2.0.0rc1,"over the shards, not over the keys: therefore, the values need"
2.0.0rc1,to be re-zipped by shard and then each shard can be paired
2.0.0rc1,with the keys.
2.0.0rc1,Assumed backprop'd
2.0.0rc1,Check Transforms
2.0.0rc1,Check path
2.0.0rc1,Check prefix: will be used when use prefix transform
2.0.0rc1,Check weight
2.0.0rc1,Check embeddings stuff
2.0.0rc1,encoder and decoder should be same sizes
2.0.0rc1,"Load default opt values, then overwrite with the opts in"
2.0.0rc1,"the checkpoint. That way, if there are new options added,"
2.0.0rc1,the defaults are used.
2.0.0rc1,Don't do anything
2.0.0rc1,Update best score of each criteria
2.0.0rc1,Reset tolerance
2.0.0rc1,Update current status
2.0.0rc1,Decrease tolerance
2.0.0rc1,Log
2.0.0rc1,Log
2.0.0rc1,Get a list of world_size lists with len(stat_list) Statistics objects
2.0.0rc1,SRU doesn't support PackedSequence.
2.0.0rc1,-*- coding: utf-8 -*-
2.0.0rc1,threshold on 1 to avoid div by 0
2.0.0rc1,treat alignment matrix one by one as each have different lengths
2.0.0rc1,No alignment if not exist valid tgt token
2.0.0rc1,get valid alignment (sub-matrix from full paded aligment matrix)
2.0.0rc1,-*- coding: utf-8 -*-
2.0.0rc1,this one is needed for torchtext random call (shuffled iterator)
2.0.0rc1,in multi gpu it ensures datasets are read in the same order
2.0.0rc1,some cudnn methods can be random even after fixing the seed
2.0.0rc1,unless you tell it to be deterministic
2.0.0rc1,This one is needed for various tranfroms
2.0.0rc1,These ensure same initialization in multi gpu mode
2.0.0rc1,Shift values to be >= 0
2.0.0rc1,we need to check the model path + any tokenizer path
2.0.0rc1,fast-forward if loaded from state
2.0.0rc1,NOTE: `rnn.pack_padded_sequence` requires that a
2.0.0rc1,"minibatch be sorted by decreasing order, which"
2.0.0rc1,requires reversing relative to typical sort keys
2.0.0rc1,Maintains the longest src and tgt length in the current batch
2.0.0rc1,Reset current longest length at a new batch (count=1)
2.0.0rc1,Src: [<bos> w1 ... wN <eos>]
2.0.0rc1,Tgt: [w1 ... wM <eos>]
2.0.0rc1,coding: utf-8
2.0.0rc1,make a small vocab containing just the tokens in the source sequence
2.0.0rc1,Map source tokens to indices in the dynamic dict.
2.0.0rc1,self.src_vocabs is used in collapse_copy_scores and Translator.py
2.0.0rc1,this assumes src_field and tgt_field are both text
2.0.0rc1,fields needs to have only keys that examples have as attrs
2.0.0rc1,avoid infinite recursion when fields isn't defined
2.0.0rc1,this is a hack: appears quicker to apply it here
2.0.0rc1,than in the ParallelCorpusIterator
2.0.0rc1,NOTE: moved to DatasetAdapter._process method in iterator.py
2.0.0rc1,item = self.transform.apply(
2.0.0rc1,"example, is_train=self.infinitely, corpus_name=self.cid)"
2.0.0rc1,empty example: skip
2.0.0rc1,-*- coding: utf-8 -*-
2.0.0rc1,backwards compatibility
2.0.0rc1,monkey-patch to make torchtext Vocab's pickleable
2.0.0rc1,"+1 for tgt side to keep coherent after ""bos"" padding,"
2.0.0rc1,"register ['NÂ°_in_batch', 'tgt_id+1', 'src_id']"
2.0.0rc1,this is basically copy-pasted from torchtext.
2.0.0rc1,counters changes in place
2.0.0rc1,keep the order of tokens specified in the vocab file by
2.0.0rc1,adding them to the counter with decreasing counting values
2.0.0rc1,`tgt_vocab_size` is ignored when sharing vocabularies
2.0.0rc1,return vocab to dump with standard name
2.0.0rc1,empty train_dataset_files so that vocab is only loaded from
2.0.0rc1,"given paths in src_vocab_path, tgt_vocab_path"
2.0.0rc1,Load vocabulary
2.0.0rc1,Drop the none-using from memory but keep the last
2.0.0rc1,"in the long run, shouldn't it be possible to do this by calling"
2.0.0rc1,build_vocab with both the src and tgt data?
2.0.0rc1,coding: utf-8
2.0.0rc1,several data readers need optional dependencies. There's no
2.0.0rc1,appropriate builtin exception
2.0.0rc1,NOTE: not support nfeats > 0 yet
2.0.0rc1,-*- coding: utf-8 -*-
2.0.0rc1,mix this with partial
2.0.0rc1,batch (list(list(list))): batch_size x len(self.fields) x seq_len
2.0.0rc1,lengths: batch_size
2.0.0rc1,data: seq_len x batch_size x len(self.fields)
2.0.0rc1,flake8: noqa
2.0.0rc1,For command-line option parsing
2.0.0rc1,"Check pass, set the args."
2.0.0rc1,"This SRU version implements its own cuda-level optimization,"
2.0.0rc1,so it requires that:
2.0.0rc1,1. `cupy` and `pynvrtc` python package installed.
2.0.0rc1,2. pytorch is built with cuda support.
2.0.0rc1,3. library path set: export LD_LIBRARY_PATH=<cuda lib path>.
2.0.0rc1,Check 1.
2.0.0rc1,Check 2.
2.0.0rc1,Check 3.
2.0.0rc1,This sets up device to use.
2.0.0rc1,-> directions x batch x dim
2.0.0rc1,For DEBUG
2.0.0rc1,"size = (length, batch, x.size(-1)) \"
2.0.0rc1,"if x.dim() == 3 else (batch, x.size(-1))"
2.0.0rc1,grad_x = x.new(*x.size()) if k_ == 3 else x.new(*size).zero_()
2.0.0rc1,Normal use
2.0.0rc1,"An entry check here, will catch on train side and translate side"
2.0.0rc1,if requirements are not satisfied.
2.0.0rc1,RNNDecoderState wraps hidden as a tuple.
2.0.0rc1,fh -> (layers*directions) x batch x dim
2.0.0rc1,_check_save_model_path
2.0.0rc1,NOTE: We need to trim the vocab to remove any unk tokens that
2.0.0rc1,were not originally here.
2.0.0rc1,!/usr/bin/env python
2.0.0rc1,!/usr/bin/env python
2.0.0rc1,!/usr/bin/env python
2.0.0rc1,-*- coding: utf-8 -*-
2.0.0rc1,!/usr/bin/env python
2.0.0rc1,!/usr/bin/env python
2.0.0rc1,!/usr/bin/env python
2.0.0rc1,import onmt.opts as opts
2.0.0rc1,Set sharing strategy manually instead of default based on the OS.
2.0.0rc1,"maybe prepare pretrained embeddings, if any"
2.0.0rc1,Load checkpoint if we resume from a previous training.
2.0.0rc1,Report src and tgt vocab sizes
2.0.0rc1,Create a thread to listen for errors in the child processes.
2.0.0rc1,Train with multiprocessing.
2.0.0rc1,"This does not work if we merge with the first loop, not sure why"
2.0.0rc1,Get the iterator to generate from
2.0.0rc1,"Once training is done, we can terminate the producers"
2.0.0rc1,magic indices
2.0.0rc1,result caching
2.0.0rc1,fix length constraint
2.0.0rc1,add one to account for BOS. Don't account for EOS because hitting
2.0.0rc1,this implies it hasn't been found.
2.0.0rc1,we don't block nothing if the user doesn't want it
2.0.0rc1,we can't block nothing beam's too short
2.0.0rc1,we check paths one by one
2.0.0rc1,we don't forbid nothing if the user doesn't want it
2.0.0rc1,we can't forbid nothing if beam's too short
2.0.0rc1,Reordering forbidden_tokens following beam selection
2.0.0rc1,We rebuild a dict to ensure we get the value and not the pointer
2.0.0rc1,Grabing the newly selected tokens and associated ngram
2.0.0rc1,skip the blocking if any token in current_ngram is excluded
2.0.0rc1,"pickups: Tensor where specified index were set to 1, others 0"
2.0.0rc1,"dropdowns: opposite of pickups, 1 for those shouldn't pick"
2.0.0rc1,Minus dropdowns to log_probs making probabilities of
2.0.0rc1,unspecified index close to 0
2.0.0rc1,"prediction step have surpass length of given target_prefix,"
2.0.0rc1,no need to further change this attr
2.0.0rc1,"For temp=0.0, take the argmax to avoid divide-by-zero errors."
2.0.0rc1,keep_topk=1 is also equivalent to argmax.
2.0.0rc1,Set all logits that are not in the top-k to -10000.
2.0.0rc1,This puts the probabilities close to 0.
2.0.0rc1,maybe fix some prediction at this step by modifying log_probs
2.0.0rc1,"shape: (sum(~ self.is_finished), 1)"
2.0.0rc1,!/usr/bin/env python
2.0.0rc1,Maintains the longest src and tgt length in the current batch
2.0.0rc1,Reset current longest length at a new batch (count=1)
2.0.0rc1,max_tgt_in_batch = 0
2.0.0rc1,Src: [<bos> w1 ... wN <eos>]
2.0.0rc1,Tgt: [w1 ... wM <eos>]
2.0.0rc1,for debugging
2.0.0rc1,TODO: maybe add dynamic part
2.0.0rc1,Statistics
2.0.0rc1,(0) add BOS and padding to tgt prediction
2.0.0rc1,(1) Encoder forward.
2.0.0rc1,(2) Repeat src objects `n_best` times.
2.0.0rc1,"We use batch_size x n_best, get ``(src_len, batch * n_best, nfeat)``"
2.0.0rc1,"(3) Init decoder with n_best src,"
2.0.0rc1,"reshape tgt to ``(len, batch * n_best, nfeat)``"
2.0.0rc1,masked_select
2.0.0rc1,get aligned src id for each prediction's valid tgt tokens
2.0.0rc1,TODO: support these blacklisted features
2.0.0rc1,Turn any copied words into UNKs.
2.0.0rc1,"Decoder forward, takes [tgt_len, batch, nfeats] as input"
2.0.0rc1,"and [src_len, batch, hidden] as memory_bank"
2.0.0rc1,"in case of inference tgt_len = 1, batch = beam times batch_size"
2.0.0rc1,"in case of Gold Scoring tgt_len = actual length, batch = 1 batch"
2.0.0rc1,Generator forward.
2.0.0rc1,"returns [(batch_size x beam_size) , vocab ] when 1 step"
2.0.0rc1,"or [ tgt_len, batch_size, vocab ] when full sentence"
2.0.0rc1,"here we have scores [tgt_lenxbatch, vocab] or [beamxbatch, vocab]"
2.0.0rc1,"returns [(batch_size x beam_size) , vocab ] when 1 step"
2.0.0rc1,"or [ tgt_len, batch_size, vocab ] when full sentence"
2.0.0rc1,(0) Prep the components of the search.
2.0.0rc1,(1) Run the encoder on the src.
2.0.0rc1,(2) prep decode_strategy. Possibly repeat src objects.
2.0.0rc1,(3) Begin decoding step by step:
2.0.0rc1,Reorder states.
2.0.0rc1,beam parameters
2.0.0rc1,result caching
2.0.0rc1,beam state
2.0.0rc1,BoolTensor was introduced in pytorch 1.2
2.0.0rc1,"""global state"" of the old beam"
2.0.0rc1,buffers for the topk scores and 'backpointer'
2.0.0rc1,for testing
2.0.0rc1,maybe fix some prediction at this step by modifying log_probs
2.0.0rc1,Flatten probs into a list of possibilities.
2.0.0rc1,using integer division to get an integer _B without casting
2.0.0rc1,force the output to be longer than self.min_length
2.0.0rc1,Multiply probs by the beam probability.
2.0.0rc1,"if the sequence ends now, then the penalty is the current"
2.0.0rc1,"length + 1, to include the EOS token"
2.0.0rc1,Avoid any direction that would repeat unwanted ngrams
2.0.0rc1,Pick up candidate token by curr_scores
2.0.0rc1,Recover log probs.
2.0.0rc1,Length penalty is just a scalar. It doesn't matter if it's applied
2.0.0rc1,before or after the topk.
2.0.0rc1,Resolve beam origin and map to batch index flat representation.
2.0.0rc1,Append last prediction.
2.0.0rc1,update global state (step == 1)
2.0.0rc1,update global state (step > 1)
2.0.0rc1,"shape: (batch_size x beam_size, 1)"
2.0.0rc1,Penalize beams that finished.
2.0.0rc1,"on real data (newstest2017) with the pretrained transformer,"
2.0.0rc1,it's faster to not move this back to the original device
2.0.0rc1,Store finished hypotheses for this batch.
2.0.0rc1,End condition is the top beam finished and we can return
2.0.0rc1,n_best hypotheses.
2.0.0rc1,"If all sentences are translated, no need to go further."
2.0.0rc1,Remove finished batches for the next step.
2.0.0rc1,Term will be subtracted from probability
2.0.0rc1,Probability will be divided by this
2.0.0rc1,these warnings indicate that either the alpha/beta
2.0.0rc1,"forces a penalty to be a no-op, or a penalty is a no-op but"
2.0.0rc1,the alpha/beta would suggest otherwise.
2.0.0rc1,using some length penalty
2.0.0rc1,using some coverage penalty
2.0.0rc1,!/usr/bin/env python
2.0.0rc1,semaphore doesn't have a timeout arg in Python 2.7
2.0.0rc1,perform a first request to initialize everything
2.0.0rc1,backwards compatibility for confs
2.0.0rc1,every segment becomes a dict for flexibility purposes
2.0.0rc1,NOTE: translator returns lists of `n_best` list
2.0.0rc1,build back results with empty texts
2.0.0rc1,load can be called multiple times: modify copy
2.0.0rc1,output contain alignment
2.0.0rc1,Below are all the different penalty terms implemented so far.
2.0.0rc1,Subtract coverage penalty from topk log probs.
2.0.0rc1,Divide topk log probs by length penalty.
2.0.0rc1,Sorting
2.0.0rc1,Chinese segmentation
2.0.0rc1,Chinese simplify -> Chinese traditional standard
2.0.0rc1,Chinese simplify -> Chinese traditional (HongKong)
2.0.0rc1,Chinese simplify -> Chinese traditional (Taiwan)
2.0.0rc1,Chinese traditional -> Chinese simplify (v1)
2.0.0rc1,Chinese traditional -> Chinese simplify (v2)
1.2.0,!/usr/bin/env python
1.2.0,!/usr/bin/env python
1.2.0,!/usr/bin/env python
1.2.0,!/usr/bin/env python
1.2.0,!/usr/bin/env python
1.2.0,!/usr/bin/env python3
1.2.0,-*- coding: utf-8 -*-
1.2.0,
1.2.0,"OpenNMT-py documentation build configuration file, created by"
1.2.0,sphinx-quickstart on Sun Dec 17 12:07:14 2017.
1.2.0,
1.2.0,This file is execfile()d with the current directory set to its
1.2.0,containing dir.
1.2.0,
1.2.0,Note that not all possible configuration values are present in this
1.2.0,autogenerated file.
1.2.0,
1.2.0,All configuration values have a default; values that are commented out
1.2.0,serve to show the default.
1.2.0,"If extensions (or modules to document with autodoc) are in another directory,"
1.2.0,add these directories to sys.path here. If the directory is relative to the
1.2.0,"documentation root, use os.path.abspath to make it absolute, like shown here."
1.2.0,
1.2.0,import os
1.2.0,import sys
1.2.0,"sys.path.insert(0, os.path.abspath('.'))"
1.2.0,-- General configuration ------------------------------------------------
1.2.0,"If your documentation needs a minimal Sphinx version, state it here."
1.2.0,
1.2.0,needs_sphinx = '1.0'
1.2.0,"Add any Sphinx extension module names here, as strings. They can be"
1.2.0,extensions coming with Sphinx (named 'sphinx.ext.*') or your custom
1.2.0,ones.
1.2.0,Show base classes
1.2.0,"Use ""variables"" section for Attributes instead of weird block things"
1.2.0,mimicking the function style.
1.2.0,"Add any paths that contain templates here, relative to this directory."
1.2.0,The suffix(es) of source filenames.
1.2.0,You can specify multiple suffix as a list of string:
1.2.0,
1.2.0,"source_suffix = ['.rst', '.md']"
1.2.0,The master toctree document.
1.2.0,General information about the project.
1.2.0,"The version info for the project you're documenting, acts as replacement for"
1.2.0,"|version| and |release|, also used in various other places throughout the"
1.2.0,built documents.
1.2.0,
1.2.0,The short X.Y version.
1.2.0,"The full version, including alpha/beta/rc tags."
1.2.0,The language for content autogenerated by Sphinx. Refer to documentation
1.2.0,for a list of supported languages.
1.2.0,
1.2.0,This is also used if you do content translation via gettext catalogs.
1.2.0,"Usually you set ""language"" from the command line for these cases."
1.2.0,"List of patterns, relative to source directory, that match files and"
1.2.0,directories to ignore when looking for source files.
1.2.0,This patterns also effect to html_static_path and html_extra_path
1.2.0,The name of the Pygments (syntax highlighting) style to use.
1.2.0,"If true, `todo` and `todoList` produce output, else they produce nothing."
1.2.0,-- Options for HTML output ----------------------------------------------
1.2.0,The theme to use for HTML and HTML Help pages.  See the documentation for
1.2.0,a list of builtin themes.
1.2.0,
1.2.0,html_theme = 'sphinx_materialdesign_theme'
1.2.0,html_theme_path = [sphinx_materialdesign_theme.get_path()]
1.2.0,Theme options are theme-specific and customize the look and feel of a theme
1.2.0,"further.  For a list of options available for each theme, see the"
1.2.0,documentation.
1.2.0,
1.2.0,html_theme_options = {}
1.2.0,"Add any paths that contain custom static files (such as style sheets) here,"
1.2.0,"relative to this directory. They are copied after the builtin static files,"
1.2.0,"so a file named ""default.css"" will overwrite the builtin ""default.css""."
1.2.0,"Custom sidebar templates, must be a dictionary that maps document names"
1.2.0,to template names.
1.2.0,
1.2.0,This is required for the alabaster theme
1.2.0,refs: http://alabaster.readthedocs.io/en/latest/installation.html#sidebars
1.2.0,-- Options for HTMLHelp output ------------------------------------------
1.2.0,Output file base name for HTML help builder.
1.2.0,-- Options for LaTeX output ---------------------------------------------
1.2.0,The paper size ('letterpaper' or 'a4paper').
1.2.0,
1.2.0,"'papersize': 'letterpaper',"
1.2.0,"The font size ('10pt', '11pt' or '12pt')."
1.2.0,
1.2.0,"'pointsize': '10pt',"
1.2.0,Additional stuff for the LaTeX preamble.
1.2.0,
1.2.0,"'preamble': '',"
1.2.0,Latex figure (float) alignment
1.2.0,
1.2.0,"'figure_align': 'htbp',"
1.2.0,Grouping the document tree into LaTeX files. List of tuples
1.2.0,"(source start file, target name, title,"
1.2.0,"author, documentclass [howto, manual, or own class])."
1.2.0,-- Options for manual page output ---------------------------------------
1.2.0,One entry per manual page. List of tuples
1.2.0,"(source start file, name, description, authors, manual section)."
1.2.0,-- Options for Texinfo output -------------------------------------------
1.2.0,Grouping the document tree into Texinfo files. List of tuples
1.2.0,"(source start file, target name, title, author,"
1.2.0,"dir menu entry, description, category)"
1.2.0,degenerate case
1.2.0,cache the features
1.2.0,mp queues don't work well between procs unless they're from a manager
1.2.0,each device has its own saver so that reconstructing is easier
1.2.0,!/usr/bin/env python
1.2.0,-*- coding: utf-8 -*-
1.2.0,is this reachable?
1.2.0,Read in embeddings
1.2.0,Write to file
1.2.0,!/usr/bin/env python
1.2.0,-*- coding: utf-8 -*-
1.2.0,"Add in default model arguments, possibly added since training."
1.2.0,-*- encoding: utf-8 -*-
1.2.0,!/usr/bin/env python
1.2.0,-*- coding: utf-8 -*-
1.2.0,Author: Rico Sennrich
1.2.0,flake8: noqa
1.2.0,This file is retrieved from https://github.com/rsennrich/subword-nmt
1.2.0,hack for python2/3 compatibility
1.2.0,check version information
1.2.0,some hacking to deal with duplicates (only consider first instance)
1.2.0,don't print end-of-word symbols
1.2.0,sys.stderr.write('cannot split {0} further.\n'.format(segment))
1.2.0,sys.stderr.write('OOV: {0}\n'.format(segment))
1.2.0,sys.stderr.write('OOV: {0}\n'.format(segment))
1.2.0,python 2/3 compatibility
1.2.0,read/write files as UTF-8
1.2.0,!/usr/bin/env python
1.2.0,!/usr/bin/env python
1.2.0,-*- coding: utf-8 -*-
1.2.0,Author: Rico Sennrich
1.2.0,flake8: noqa
1.2.0,This file is retrieved from https://github.com/rsennrich/subword-nmt
1.2.0,hack for python2/3 compatibility
1.2.0,"find all instances of pair, and update frequency/indices around it"
1.2.0,find first symbol
1.2.0,"if first symbol is followed by second symbol, we've found an occurrence of pair (old_word[i:i+2])"
1.2.0,"assuming a symbol sequence ""A B C"", if ""B C"" is merged, reduce the frequency of ""A B"""
1.2.0,"assuming a symbol sequence ""A B C B"", if ""B C"" is merged, reduce the frequency of ""C B""."
1.2.0,"however, skip this if the sequence is A B C B C, because the frequency of ""C B"" will be reduced by the previous code block"
1.2.0,find new pair
1.2.0,"assuming a symbol sequence ""A BC D"", if ""B C"" is merged, increase the frequency of ""A BC"""
1.2.0,"assuming a symbol sequence ""A BC B"", if ""B C"" is merged, increase the frequency of ""BC B"""
1.2.0,"however, if the sequence is A BC BC, skip this step because the count of ""BC BC"" will be incremented by the previous code block"
1.2.0,data structure of pair frequencies
1.2.0,index from pairs to words
1.2.0,version 0.2 changes the handling of the end-of-word token ('</w>');
1.2.0,version numbering allows bckward compatibility
1.2.0,"threshold is inspired by Zipfian assumption, but should only affect speed"
1.2.0,we probably missed the best pair because of pruning; go back to full statistics
1.2.0,"threshold is inspired by Zipfian assumption, but should only affect speed"
1.2.0,python 2/3 compatibility
1.2.0,read/write files as UTF-8
1.2.0,!/usr/bin/env python
1.2.0,for back compat when attention_dropout was not defined
1.2.0,Build embeddings.
1.2.0,Build encoder.
1.2.0,Build decoder.
1.2.0,Share the embedding matrix - preprocess with share_vocab required.
1.2.0,src/tgt vocab should be the same if `-share_vocab` is specified.
1.2.0,Build NMTModel(= encoder + decoder).
1.2.0,Build Generator.
1.2.0,Load the model states from checkpoint or initialize them.
1.2.0,This preserves backward-compat for models using customed layernorm
1.2.0,end of patch for backward compatibility
1.2.0,!/usr/bin/env python
1.2.0,NOTE: It's important that ``opt`` has been validated and updated
1.2.0,at this point.
1.2.0,Load checkpoint if we resume from a previous training.
1.2.0,check for code where vocab is saved instead of fields
1.2.0,(in the future this will be done in a smarter way)
1.2.0,patch for fields that may be missing in old data/model
1.2.0,"Report src and tgt vocab sizes, including for features"
1.2.0,Build model.
1.2.0,Build optimizer.
1.2.0,Build model saver
1.2.0,Move batch to specified device
1.2.0,Embedding Options
1.2.0,Encoder-Decoder Options
1.2.0,"group.add('--residual', '-residual',   action=""store_true"","
1.2.0,"help=""Add residual connections between RNN layers."")"
1.2.0,The following options (bridge_extra_node to src_vocab) are used
1.2.0,for training with --encoder_type ggnn (Gated Graph Neural Network).
1.2.0,The ggnn uses src_vocab during training because the graph is built
1.2.0,using edge information which requires parsing the input sequence.
1.2.0,Attention options
1.2.0,Alignement options
1.2.0,Generator and loss options.
1.2.0,Data options
1.2.0,"Dictionary options, for text corpus"
1.2.0,"if you want to pass an existing vocab.pt file, pass it to"
1.2.0,-src_vocab alone as it already contains tgt vocab.
1.2.0,"Truncation options, for text corpus"
1.2.0,Data processing options
1.2.0,Options most relevant to speech
1.2.0,Option most relevant to image input
1.2.0,Options for experimental source noising (BART style)
1.2.0,GPU
1.2.0,Init options
1.2.0,Pretrained word vectors
1.2.0,Fixed word vectors
1.2.0,Optimization options
1.2.0,learning rate
1.2.0,Use Tensorboard for visualization during training
1.2.0,Options most relevant to speech
1.2.0,Option most relevant to image input
1.2.0,Options most relevant to summarization.
1.2.0,Alpha and Beta values for Google Length + Coverage penalty
1.2.0,"Described here: https://arxiv.org/pdf/1609.08144.pdf, Section 7"
1.2.0,Options most relevant to speech.
1.2.0,Option most relevant to image input
1.2.0,Copyright 2016 The Chromium Authors. All rights reserved.
1.2.0,Use of this source code is governed by a BSD-style license that can be
1.2.0,found in the LICENSE file.
1.2.0,"Get the key 'value' in the dict, or just use 'value'"
1.2.0,Basic attributes.
1.2.0,Set model in training mode.
1.2.0,UPDATE DROPOUT
1.2.0,Run patience mechanism
1.2.0,"If the patience has reached the limit, stop training"
1.2.0,swap model params w/ moving average
1.2.0,(and keep the original parameters)
1.2.0,Set model in validating mode.
1.2.0,F-prop through the model.
1.2.0,Compute loss.
1.2.0,Update statistics.
1.2.0,Set model back to training mode.
1.2.0,Truncated BPTT: reminder not compatible with accum > 1
1.2.0,1. Create truncated target.
1.2.0,2. F-prop all but generator.
1.2.0,3. Compute loss.
1.2.0,4. Update the parameters and statistics.
1.2.0,Multi GPU gradient gather
1.2.0,"If truncated, don't backprop fully."
1.2.0,TO CHECK
1.2.0,if dec_state is not None:
1.2.0,dec_state.detach()
1.2.0,"in case of multi step gradient accumulation,"
1.2.0,update only after accum batches
1.2.0,For Flake
1.2.0,we avoid padding while mean pooling
1.2.0,incoming and outgoing edge embedding
1.2.0,Find vocab data for tree builting
1.2.0,Propogation Model
1.2.0,Initialize the bridge layer
1.2.0,Initialize graph using formatted input sequence
1.2.0,Number of flagged nodes defines node count for this sample
1.2.0,"(Nodes can have no flags on them, but must be in 'flags' list)."
1.2.0,The total number of integers in the vocab should allow
1.2.0,for all features and edges to be defined.
1.2.0,Use first extra node as only source for decoder init
1.2.0,Average all nodes to get bridge input
1.2.0,"LSTM has hidden and cell state, other only one"
1.2.0,Total number of states
1.2.0,Build a linear layer for each
1.2.0,Initialize the bridge layer
1.2.0,"s_len, batch, emb_dim = emb.size()"
1.2.0,Lengths data is wrapped inside a Tensor.
1.2.0,"LSTM has hidden and cell state, other only one"
1.2.0,Total number of states
1.2.0,Build a linear layer for each
1.2.0,The encoder hidden is  (layers*directions) x batch x dim.
1.2.0,"s_len, batch, emb_dim = emb.size()"
1.2.0,Run the forward pass of every layer of the tranformer.
1.2.0,why is the model_opt.__dict__ check necessary?
1.2.0,"(batch_size, 64, imgH, imgW)"
1.2.0,layer 1
1.2.0,"(batch_size, 64, imgH/2, imgW/2)"
1.2.0,"(batch_size, 128, imgH/2, imgW/2)"
1.2.0,layer 2
1.2.0,"(batch_size, 128, imgH/2/2, imgW/2/2)"
1.2.0,"(batch_size, 256, imgH/2/2, imgW/2/2)"
1.2.0,layer 3
1.2.0,batch norm 1
1.2.0,"(batch_size, 256, imgH/2/2, imgW/2/2)"
1.2.0,layer4
1.2.0,"(batch_size, 256, imgH/2/2/2, imgW/2/2)"
1.2.0,"(batch_size, 512, imgH/2/2/2, imgW/2/2)"
1.2.0,layer 5
1.2.0,batch norm 2
1.2.0,"(batch_size, 512, imgH/2/2/2, imgW/2/2/2)"
1.2.0,"(batch_size, 512, imgH/2/2/2, imgW/2/2/2)"
1.2.0,"# (batch_size, 512, H, W)"
1.2.0,Dimensions and padding for constructing the word embedding matrix
1.2.0,Dimensions and padding for feature embedding matrices
1.2.0,(these have no effect if feat_vocab_sizes is empty)
1.2.0,The embedding matrix look-up tables. The first look-up table
1.2.0,"is for words. Subsequent ones are for features, if any exist."
1.2.0,The final output size of word + feature vectors. This can vary
1.2.0,from the word vector size if and only if features are defined.
1.2.0,This is the attribute you should access if you need to know
1.2.0,how big your embeddings are going to be.
1.2.0,The sequence of operations that converts the input sequence
1.2.0,into a sequence of embeddings. At minimum this consists of
1.2.0,looking up the embeddings for each word and feature in the
1.2.0,input. Model parameters may require the sequence to contain
1.2.0,additional operations as well.
1.2.0,features must use word_vec_size
1.2.0,features will use feat_vec_size
1.2.0,This class is mainly used by decoder.py for RNNs but also
1.2.0,by the CNN / transformer decoder when copy attention is used
1.2.0,CNN has its own attention mechanism ConvMultiStepAttention
1.2.0,Transformer has its own MultiHeadedAttention
1.2.0,mlp wants it with bias
1.2.0,Check input sizes
1.2.0,"(batch, t_len, d) x (batch, d, s_len) --> (batch, t_len, s_len)"
1.2.0,"(batch, t_len, s_len, d)"
1.2.0,one step input
1.2.0,"compute attention scores, as in Luong et al."
1.2.0,Softmax or sparsemax to normalize attention weights
1.2.0,each context vector c_t is the weighted average
1.2.0,over all the source hidden states
1.2.0,concatenate
1.2.0,Check output sizes
1.2.0,Check output sizes
1.2.0,clamping necessary because of numerical errors: loss should be lower
1.2.0,"bounded by zero, but negative values near zero are possible without"
1.2.0,the clamp
1.2.0,from onmt.utils.misc import aeq
1.2.0,CHECKS
1.2.0,"batch, k_len, d = key.size()"
1.2.0,"batch_, k_len_, d_ = value.size()"
1.2.0,"aeq(batch, batch_)"
1.2.0,"aeq(k_len, k_len_)"
1.2.0,"aeq(d, d_)"
1.2.0,"batch_, q_len, d_ = query.size()"
1.2.0,"aeq(batch, batch_)"
1.2.0,"aeq(d, d_)"
1.2.0,"aeq(self.model_dim % 8, 0)"
1.2.0,if mask is not None:
1.2.0,"batch_, q_len_, k_len_ = mask.size()"
1.2.0,"aeq(batch_, batch)"
1.2.0,"aeq(k_len_, k_len)"
1.2.0,aeq(q_len_ == q_len)
1.2.0,END CHECKS
1.2.0,"1) Project key, value, and query."
1.2.0,1 or key_len x key_len
1.2.0,1 or key_len x key_len x dim_per_head
1.2.0,1 or key_len x key_len x dim_per_head
1.2.0,2) Calculate and scale scores.
1.2.0,batch x num_heads x query_len x key_len
1.2.0,3) Apply attention dropout and compute context vectors.
1.2.0,CHECK
1.2.0,"batch_, q_len_, d_ = output.size()"
1.2.0,"aeq(q_len, q_len_)"
1.2.0,"aeq(batch, batch_)"
1.2.0,"aeq(d, d_)"
1.2.0,Return multi-head attn
1.2.0,At the moment this class is only used by embeddings.Embeddings look-up tables
1.2.0,-*- coding: utf-8 -*-
1.2.0,checks
1.2.0,"batch, channel, height, width = base_target_emb.size()"
1.2.0,"batch_, channel_, height_, width_ = input_from_dec.size()"
1.2.0,"enc_batch, enc_channel, enc_height = encoder_out_top.size()"
1.2.0,"enc_batch_, enc_channel_, enc_height_ = encoder_out_combine.size()"
1.2.0,out_features * in_features
1.2.0,norm is out_features * 1
1.2.0,batch_size * out_features
1.2.0,out_features
1.2.0,out_features
1.2.0,batch_size * out_features
1.2.0,"out_channels, in_channels // groups, * kernel_size"
1.2.0,out_features
1.2.0,This is used nowhere in the code at the moment (Vincent Nguyen 05/18/2018)
1.2.0,"in_channels, out_channels, *kernel_size"
1.2.0,"in_channels, out_channels, *kernel_size"
1.2.0,"self.out_channels, 1"
1.2.0,out_features
1.2.0,out_features
1.2.0,store roots on diagonal
1.2.0,noise_skip = batch.noise_skip
1.2.0,aeq(len(batch.noise_skip) == source.size(1))
1.2.0,source is [src_len x bs x feats]
1.2.0,source might increase length so we need to resize the whole
1.2.0,tensor
1.2.0,remove useless pad
1.2.0,"def s(self, tokens):"
1.2.0,prob = self.prob
1.2.0,r = torch.rand([len(tokens)])
1.2.0,mask = False
1.2.0,masked = []
1.2.0,"for i, tok in enumerate(tokens):"
1.2.0,if tok.startswith(subword_prefix):
1.2.0,if r[i].item() <= prob:
1.2.0,masked.append(mask_tok)
1.2.0,mask = True
1.2.0,else:
1.2.0,masked.append(tok)
1.2.0,mask = False
1.2.0,else:
1.2.0,if mask:
1.2.0,pass
1.2.0,else:
1.2.0,masked.append(tok)
1.2.0,return masked
1.2.0,"aeq(source.size(0), length)"
1.2.0,Pretend it ends with a full stop so last span is a sentence
1.2.0,"Tokens that are full stops, where the previous token is not"
1.2.0,"aeq(source.size(0), length)"
1.2.0,-1: keep everything (i.e. 1 mask per token)
1.2.0,0: replace everything (i.e. no mask)
1.2.0,1: 1 mask per span
1.2.0,fairseq/data/denoising_dataset.py
1.2.0,"print(""src size: "", source.size())"
1.2.0,"print(""ws size: "", self.word_start_mask.size())"
1.2.0,"print(""max: "", source.max())"
1.2.0,assert source.max() < self.word_start_mask.size(0)
1.2.0,assert source.min() >= 0
1.2.0,assert source.size() == is_word_start.size()
1.2.0,"aeq(source.eq(self.pad_idx).long().sum(), 0)"
1.2.0,we manually add this hypothesis since it's required for the rest
1.2.0,of the function and kindof make sense
1.2.0,Make sure we have enough to mask
1.2.0,Trim to masking budget
1.2.0,Handle 0-length mask (inserts) separately
1.2.0,assert (lengths > 0).all()
1.2.0,assert is_word_start[-1] == 0
1.2.0,TODO why?
1.2.0,assert source_length - 1 not in indices
1.2.0,"acts as a long length, so spans don't go over the end of doc"
1.2.0,"keep index, but replace it with [MASK]"
1.2.0,random ratio disabled
1.2.0,source[indices[mask_random]] = torch.randint(
1.2.0,"1, len(self.vocab), size=(mask_random.sum(),))"
1.2.0,if self.mask_span_distribution is not None:
1.2.0,assert len(lengths.size()) == 1
1.2.0,assert lengths.size() == indices.size()
1.2.0,assert lengths.size() == indices.size()
1.2.0,mask_random = mask_random[uncompleted]
1.2.0,delete token
1.2.0,"keep index, but replace it with [MASK]"
1.2.0,random ratio disabled
1.2.0,source[indices[mask_random]] = torch.randint(
1.2.0,"1, len(self.vocab), size=(mask_random.sum(),))"
1.2.0,else:
1.2.0,# A bit faster when all lengths are 1
1.2.0,while indices.size(0) > 0:
1.2.0,uncompleted = is_word_start[indices + 1] == 0
1.2.0,indices = indices[uncompleted] + 1
1.2.0,mask_random = mask_random[uncompleted]
1.2.0,if self.replace_length != -1:
1.2.0,# delete token
1.2.0,to_keep[indices] = 0
1.2.0,else:
1.2.0,"# keep index, but replace it with [MASK]"
1.2.0,source[indices] = self.mask_idx
1.2.0,source[indices[mask_random]] = torch.randint(
1.2.0,"1, len(self.vocab), size=(mask_random.sum(),))"
1.2.0,assert source_length - 1 not in indices
1.2.0,"aeq(source.eq(self.pad_idx).long().sum(), 0)"
1.2.0,random ratio disabled
1.2.0,num_random = int(math.ceil(n * self.random_ratio))
1.2.0,result[noise_indices[:num_random]] = torch.randint(
1.2.0,"low=1, high=len(self.vocab), size=(num_random,))"
1.2.0,assert (result >= 0).all()
1.2.0,CHECKS
1.2.0,Original probabilities.
1.2.0,Probability of copying p(z=1) batch.
1.2.0,Probability of not copying: p_{word}(w) * (1 - p(z))
1.2.0,probabilities assigned by the model to the gold targets
1.2.0,probability of tokens copied from source
1.2.0,Set scores for unk to 0 and add eps
1.2.0,find the indices in which you do not use the copy mechanism
1.2.0,Drop padding.
1.2.0,this block does not depend on the loss value computed above
1.2.0,and is used only for stats
1.2.0,this block does not depend on the loss value computed above
1.2.0,and is used only for stats
1.2.0,Correct target copy token instead of <unk>
1.2.0,tgt[i] = align[i] + len(tgt_vocab)
1.2.0,for i such that tgt[i] == 0 and align[i] != 0
1.2.0,Compute sum of perplexities for stats
1.2.0,this part looks like it belongs in CopyGeneratorLoss
1.2.0,Compute Loss as NLL divided by seq length
1.2.0,Compute Total Loss per sequence in batch
1.2.0,Divide by length of each sequence and sum
1.2.0,initialize fields at the top of each unit test to prevent
1.2.0,any undesired stateful effects
1.2.0,"this test touches the file system, so it could be considered an"
1.2.0,integration test
1.2.0,write utf-8 bytes
1.2.0,batch 0 will always predict EOS. The other batches will predict
1.2.0,non-eos scores.
1.2.0,"""best"" prediction is eos - that should be blocked"
1.2.0,include at least one prediction OTHER than EOS
1.2.0,that is greater than -1e20
1.2.0,now batch 0 has ended and no others have
1.2.0,initial step
1.2.0,batch 0 dies on step 0
1.2.0,include at least one prediction OTHER than EOS
1.2.0,that is greater than -1e20
1.2.0,step 2
1.2.0,(old) batch 8 dies on step 1
1.2.0,step 3
1.2.0,everything dies
1.2.0,initial step
1.2.0,batch 0 dies on step 0
1.2.0,include at least one prediction OTHER than EOS
1.2.0,that is greater than -1e20
1.2.0,step 2
1.2.0,(old) batch 8 dies on step 1
1.2.0,step 3
1.2.0,everything dies
1.2.0,illegal_weights_mask = torch.ByteTensor([
1.2.0,"[0, 0, 0, 0, 0, 0, 0],"
1.2.0,"[0, 0, 0, 1, 1, 1, 1],"
1.2.0,"[0, 0, 0, 0, 0, 1, 1],"
1.2.0,"[0, 0, 1, 1, 1, 1, 1]])"
1.2.0,TODO: fix for pytorch 0.3
1.2.0,illegal_weights = alignments.masked_select(illegal_weights_mask)
1.2.0,"self.assertEqual(0.0, illegal_weights.data.sum())"
1.2.0,this could be considered an integration test because it touches
1.2.0,the filesystem for the config file (and the models)
1.2.0,-*- coding: utf-8 -*-
1.2.0,tests pad and numericalize integration
1.2.0,tests pad and numericalize integration
1.2.0,"this test touches the file system, so it could be considered an"
1.2.0,integration test
1.2.0,file to hold full paths to audio data
1.2.0,file to hold audio paths relative to _AUDIO_DATA_DIR (i.e. file names)
1.2.0,it's ok if non-audio files co-exist with audio files in the data dir
1.2.0,"dividing gets the noise in [-1, 1]"
1.2.0,"this test touches the file system, so it could be considered an"
1.2.0,integration test
1.2.0,file to hold full paths to image data
1.2.0,file to hold image paths relative to _IMG_DATA_DIR (i.e. file names)
1.2.0,it's ok if non-image files co-exist with image files in the data dir
1.2.0,all beams repeat (beam >= 1 repeat dummy scores)
1.2.0,predict repeat_idx over and over again
1.2.0,"before repeat, scores are either 0 or -inf"
1.2.0,"on repeat, `repeat_idx` score is BLOCKED_SCORE"
1.2.0,"(but it's still the best score, thus we have"
1.2.0,"[BLOCKED_SCORE, -inf, -inf, -inf, -inf]"
1.2.0,repetitions keeps maximizing score
1.2.0,"index 0 has been blocked, so repeating=>+0.0 score"
1.2.0,other indexes are -inf so repeating=>BLOCKED_SCORE
1.2.0,which is higher
1.2.0,beam 0 and beam >=2 will repeat (beam >= 2 repeat dummy scores)
1.2.0,non-interesting beams are going to get dummy values
1.2.0,"on initial round, only predicted scores for beam 0"
1.2.0,matter. Make two predictions. Top one will be repeated
1.2.0,"in beam zero, second one will live on in beam 1."
1.2.0,predict the same thing in beam 0
1.2.0,continue pushing around what beam 1 predicts
1.2.0,"now beam 0 dies (along with the others), beam 1 -> beam 0"
1.2.0,"now beam 0 dies (along with the others), beam 1 -> beam 0"
1.2.0,beam 0 and beam >= 2 will repeat (beam 2 repeats excluded idx)
1.2.0,non-interesting beams are going to get dummy values
1.2.0,predict the same thing in beam 0
1.2.0,continue pushing around what beam 1 predicts
1.2.0,predict the allowed-repeat again in beam 2
1.2.0,"now beam 0 dies, beam 1 -> beam 0, beam 2 -> beam 1"
1.2.0,and the rest die
1.2.0,"since all preds after i=0 are 0, we can check"
1.2.0,that the beam is the correct idx by checking that
1.2.0,the curr score is the initial score
1.2.0,beam 0 will always predict EOS. The other beams will predict
1.2.0,non-eos scores.
1.2.0,non-interesting beams are going to get dummy values
1.2.0,"""best"" prediction is eos - that should be blocked"
1.2.0,include at least beam_sz predictions OTHER than EOS
1.2.0,that are greater than -1e20
1.2.0,predict eos in beam 0
1.2.0,provide beam_sz other good predictions
1.2.0,now the top beam has ended and no others have
1.2.0,"not of interest, but want to make sure it keeps running"
1.2.0,since only beam 0 terminates and n_best = 2
1.2.0,"this is also a test that when block_ngram_repeat=0,"
1.2.0,repeating is acceptable
1.2.0,non-interesting beams are going to get dummy values
1.2.0,"""best"" prediction is eos - that should be blocked"
1.2.0,include at least beam_sz predictions OTHER than EOS
1.2.0,that are greater than -1e20
1.2.0,predict eos in beam 1
1.2.0,provide beam_sz other good predictions in other beams
1.2.0,provide beam_sz other good predictions in other beams
1.2.0,beam 1 dies on min_length
1.2.0,beam 0 dies on the step after beam 1 dies
1.2.0,"inp_lens is tiled in initialize, reassign to make attn match"
1.2.0,non-interesting beams are going to get dummy values
1.2.0,"""best"" prediction is eos - that should be blocked"
1.2.0,include at least beam_sz predictions OTHER than EOS
1.2.0,that are greater than -1e20
1.2.0,predict eos in beam 1
1.2.0,provide beam_sz other good predictions in other beams
1.2.0,provide beam_sz other good predictions in other beams
1.2.0,no top beams are finished yet
1.2.0,beam 1 dies on min_length
1.2.0,no top beams are finished yet
1.2.0,beam 0 dies on the step after beam 1 dies
1.2.0,top beam is finished now so there are attentions
1.2.0,two beams are finished in each batch
1.2.0,second dim is cut down to the non-padded src length
1.2.0,first dim is equal to the time of death
1.2.0,(beam 0 died at current step - adjust for SOS)
1.2.0,(beam 1 died at last step - adjust for SOS)
1.2.0,behavior gets weird when beam is already done so just stop
1.2.0,this is just test_beam.TestBeamAgainstReferenceCase repeated
1.2.0,in each batch.
1.2.0,"init_preds: [4, 3, 5, 6, 7] - no EOS's"
1.2.0,no EOS's yet
1.2.0,"[5, 3, 2, 6, 0], so beam 2 predicts EOS!"
1.2.0,assumes beam 2 finished on last step
1.2.0,ended beam 2 shouldn't continue
1.2.0,"[2, 5, 3, 6, 0] repeat self.BATCH_SZ, so beam 0 predicts EOS!"
1.2.0,"[-2.4879, -3.8910, -4.1010, -4.2010, -4.4010] repeat self.BATCH_SZ"
1.2.0,another beam is finished in all batches
1.2.0,new beam 0 finished
1.2.0,new beam 0 is old beam 3
1.2.0,assumes beam 0 finished on last step
1.2.0,"[5, 2, 6, 1, 0] repeat self.BATCH_SZ, so beam 1 predicts EOS!"
1.2.0,new beam 1 finished
1.2.0,new beam 1 is old beam 4
1.2.0,this could be considered an integration test because it tests
1.2.0,interactions between the GNMT scorer and the beam
1.2.0,"-data option is required, but not used in this test, so dummy."
1.2.0,len x batch x nfeat
1.2.0,batch x c x h x w
1.2.0,batch x 1 x nfft x t
1.2.0,Initialize vectors to compare size with
1.2.0,Ensure correct sizes and types
1.2.0,Make sure that output has the correct size and type
1.2.0,Make sure that output has the correct size and type
1.2.0,Make sure that output has the correct size and type
1.2.0,"[('encoder_type', 'transformer'),"
1.2.0,"('word_vec_size', 16), ('rnn_size', 16)],"
1.2.0,""""""" Only do SRU test if requirment is safisfied. """""""
1.2.0,SRU doesn't support input_feed.
1.2.0,"when reasonable, set audio_enc_pooling to 2"
1.2.0,Need lengths >= audio_enc_pooling**n_layers.
1.2.0,"That condition is unrealistic for large n_layers,"
1.2.0,so leave audio_enc_pooling at 1.
1.2.0,first check there's nothing unexpectedly not trainable
1.2.0,ok: word embeddings shouldn't be trainable
1.2.0,if word vecs are fixed
1.2.0,ok: positional encodings shouldn't be trainable
1.2.0,then check nothing unexpectedly trainable
1.2.0,!/usr/bin/env python
1.2.0,-*- coding: utf-8 -*-
1.2.0,Remove the generated *pt files.
1.2.0,Test image preprocessing
1.2.0,Test audio preprocessing
1.2.0,Decoder state
1.2.0,Build the RNN.
1.2.0,Set up the context gate.
1.2.0,Set up the standard attention.
1.2.0,The encoder hidden is  (layers*directions) x batch x dim.
1.2.0,We need to convert it to layers x batch x (directions*dim).
1.2.0,Init the input feed.
1.2.0,Update the state with the result.
1.2.0,Concatenates sequence of tensors along a new dimension.
1.2.0,NOTE: v0.3 to 0.4: dec_outs / attns[*] may not be list
1.2.0,(in particular in case of SRU) it was not raising error in 0.3
1.2.0,since stack(Variable) was allowed.
1.2.0,"In 0.4, SRU returns a tensor that shouldn't be stacke"
1.2.0,Check
1.2.0,Calculate the attention.
1.2.0,Calculate the context gate.
1.2.0,Additional args check.
1.2.0,END Additional args check.
1.2.0,Input feed concatenates hidden state with
1.2.0,input at every time step.
1.2.0,TODO: context gate should be employed
1.2.0,instead of second RNN transform.
1.2.0,Update the coverage attention.
1.2.0,Decoder State
1.2.0,CNNDecoder has its own attention mechanism.
1.2.0,Set up a separate copy attention layer if needed.
1.2.0,The output of CNNEncoder.
1.2.0,The combination of output of CNNEncoder and source embeddings.
1.2.0,Process the result and update the attentions.
1.2.0,Update the state.
1.2.0,TODO change the way attns is returned dict => list or tuple (onnx)
1.2.0,Memory_lengths is a single tensor shared between all models.
1.2.0,This assumption will not hold if Translator is modified
1.2.0,to calculate memory_lengths as something other than the length
1.2.0,of the input.
1.2.0,"return _, (B, Q_len, K_len)"
1.2.0,"layer average attention across heads, get ``(B, Q, K)``"
1.2.0,"Case 1: no full_context, no align heads -> layer avg baseline"
1.2.0,"Case 2: no full_context, 1 align heads -> guided align"
1.2.0,"Case 3: full_context, 1 align heads -> full cte guided align"
1.2.0,T: could be 1 in the case of stepwise decoding or tgt_len
1.2.0,BoolTensor was introduced in pytorch 1.2
1.2.0,Decoder State
1.2.0,"previously, there was a GlobalAttention module here for copy"
1.2.0,"attention. But it was never actually used -- the ""copy"" attention"
1.2.0,just reuses the context attention.
1.2.0,"attns[""align""] = torch.stack(attn_aligns, 0).mean(0)  # All avg"
1.2.0,TODO change the way attns is returned dict => list or tuple (onnx)
1.2.0,"buffer size in bytes, determine equiv. # of elements based on data type"
1.2.0,copy tensors into buffer_t
1.2.0,all-reduce and rescale
1.2.0,copy all-reduced buffer back into tensors
1.2.0,"tensor is bigger than buffer, all-reduce and rescale directly"
1.2.0,"buffer is full, all-reduce and replace buffer with grad"
1.2.0,add tensor to buffer
1.2.0,TODO: Find a better way to check for sparse gradients.
1.2.0,we use here a FusedAdam() copy of an old Apex repo
1.2.0,In this case use the old FusedAdam with FP16_optimizer wrapper
1.2.0,Load everything from the checkpoint.
1.2.0,Build everything from scratch.
1.2.0,"Reset optimizer, keep options."
1.2.0,"Reset options, keep optimizer."
1.2.0,State can be partially restored.
1.2.0,"unscaled optimizer's gradients (already done therefore skip),"
1.2.0,skips optimizer.step() if gradients contain infs/NaNs.
1.2.0,Updates the scale for next iteration.
1.2.0,Code below is an implementation of https://arxiv.org/pdf/1804.04235.pdf
1.2.0,inspired but modified from https://github.com/DeadAt0m/adafactor-pytorch
1.2.0,backward compatibility
1.2.0,assuming a list/generator of parameter means single group
1.2.0,compute combined scale factor for this group
1.2.0,norm is in fact norm*scale
1.2.0,note: p.grad should not ever be set for correct operation of
1.2.0,mixed precision optimizer that sometimes sends None gradients
1.2.0,State initialization
1.2.0,Exponential moving average of gradient values
1.2.0,Exponential moving average of squared gradient values
1.2.0,-*- coding: utf-8 -*-
1.2.0,if the loss function operates on vectors of raw logits instead of
1.2.0,"probabilities, only the first part of the generator needs to be"
1.2.0,"passed to the NMTLossCompute. At the moment, the only supported"
1.2.0,loss function of this kind is the sparsemax loss.
1.2.0,"attn_align should be in (batch_size, pad_tgt_size, pad_src_size)"
1.2.0,"align_idx should be a Tensor in size([N, 3]), N is total number"
1.2.0,"of align src-tgt pair in current batch, each as"
1.2.0,"['sent_NÂ°_in_batch', 'tgt_id+1', 'src_id'] (check AlignField)"
1.2.0,NOTE: tgt-src ref alignement that in range_ of shard
1.2.0,(coherent with batch.tgt)
1.2.0,"align_head contains value in [0, 1) presenting attn prob,"
1.2.0,0 was resulted by the context attention src_pad_mask
1.2.0,"So, the correspand position in ref_align should also be 0"
1.2.0,"Therefore, clip align_head to > 1e-18 should be bias free."
1.2.0,non_none: the subdict of the state dictionary where the values
1.2.0,are not None.
1.2.0,"Now, the iteration:"
1.2.0,state is a dictionary of sequences of tensor-like but we
1.2.0,want a sequence of dictionaries of tensors.
1.2.0,"First, unzip the dictionary into a sequence of keys and a"
1.2.0,sequence of tensor-like sequences.
1.2.0,"Now, yield a dictionary for each shard. The keys are always"
1.2.0,the same. values is a sequence of length #keys where each
1.2.0,element is a sequence of length #shards. We want to iterate
1.2.0,"over the shards, not over the keys: therefore, the values need"
1.2.0,to be re-zipped by shard and then each shard can be paired
1.2.0,with the keys.
1.2.0,Assumed backprop'd
1.2.0,this check is here because audio allows the encoder and decoder to
1.2.0,"be different sizes, but other model types do not yet"
1.2.0,"Load default opt values, then overwrite with the opts in"
1.2.0,"the checkpoint. That way, if there are new options added,"
1.2.0,the defaults are used.
1.2.0,Don't do anything
1.2.0,Update best score of each criteria
1.2.0,Reset tolerance
1.2.0,Update current status
1.2.0,Decrease tolerance
1.2.0,Log
1.2.0,Log
1.2.0,Get a list of world_size lists with len(stat_list) Statistics objects
1.2.0,SRU doesn't support PackedSequence.
1.2.0,-*- coding: utf-8 -*-
1.2.0,threshold on 1 to avoid div by 0
1.2.0,treat alignment matrix one by one as each have different lengths
1.2.0,No alignment if not exist valid tgt token
1.2.0,get valid alignment (sub-matrix from full paded aligment matrix)
1.2.0,-*- coding: utf-8 -*-
1.2.0,this one is needed for torchtext random call (shuffled iterator)
1.2.0,in multi gpu it ensures datasets are read in the same order
1.2.0,some cudnn methods can be random even after fixing the seed
1.2.0,unless you tell it to be deterministic
1.2.0,These ensure same initialization in multi gpu mode
1.2.0,Shift values to be >= 0
1.2.0,we need to check the model path + any tokenizer path
1.2.0,coding: utf-8
1.2.0,make a small vocab containing just the tokens in the source sequence
1.2.0,Map source tokens to indices in the dynamic dict.
1.2.0,self.src_vocabs is used in collapse_copy_scores and Translator.py
1.2.0,this assumes src_field and tgt_field are both text
1.2.0,fields needs to have only keys that examples have as attrs
1.2.0,avoid infinite recursion when fields isn't defined
1.2.0,-*- coding: utf-8 -*-
1.2.0,backwards compatibility
1.2.0,monkey-patch to make torchtext Vocab's pickleable
1.2.0,"+1 for tgt side to keep coherent after ""bos"" padding,"
1.2.0,"register ['NÂ°_in_batch', 'tgt_id+1', 'src_id']"
1.2.0,"List[Tuple[str, Vocab]] -> List[Tuple[str, Field]]"
1.2.0,"-> dict[str, Field]"
1.2.0,"Dict[str, List[Tuple[str, Field]]]"
1.2.0,doesn't change structure - don't return early.
1.2.0,"Dict[str, List[Tuple[str, Field]]] -> List[Tuple[str, Field]]"
1.2.0,"-> dict[str, Field]"
1.2.0,"if tgt isn't using TextMultiField, then no text field is."
1.2.0,this is basically copy-pasted from torchtext.
1.2.0,counters changes in place
1.2.0,keep the order of tokens specified in the vocab file by
1.2.0,adding them to the counter with decreasing counting values
1.2.0,`tgt_vocab_size` is ignored when sharing vocabularies
1.2.0,return vocab to dump with standard name
1.2.0,empty train_dataset_files so that vocab is only loaded from
1.2.0,"given paths in src_vocab_path, tgt_vocab_path"
1.2.0,Load vocabulary
1.2.0,Drop the none-using from memory but keep the last
1.2.0,"in the long run, shouldn't it be possible to do this by calling"
1.2.0,build_vocab with both the src and tgt data?
1.2.0,fast-forward if loaded from state
1.2.0,NOTE: `rnn.pack_padded_sequence` requires that a
1.2.0,"minibatch be sorted by decreasing order, which"
1.2.0,requires reversing relative to typical sort keys
1.2.0,self.weights = opt.data_weights
1.2.0,Temporarily load one shard to retrieve sort_key for data_type
1.2.0,"NOTE: This is causing some issues for consumer/producer,"
1.2.0,as we may still have some of those examples in some queue
1.2.0,cur_dataset.examples = None
1.2.0,gc.collect()
1.2.0,del cur_dataset
1.2.0,gc.collect()
1.2.0,Cycle through the shards indefinitely.
1.2.0,"When the dataset is not repeated, we might need to ensure that"
1.2.0,the number of returned batches is the multiple of a given value.
1.2.0,This is important for multi GPU training to ensure that all
1.2.0,workers have the same number of batches to process.
1.2.0,Maintains the longest src and tgt length in the current batch
1.2.0,Reset current longest length at a new batch (count=1)
1.2.0,Src: [<bos> w1 ... wN <eos>]
1.2.0,Tgt: [w1 ... wM <eos>]
1.2.0,-*- coding: utf-8 -*-
1.2.0,imports of datatype-specific dependencies
1.2.0,torchaudio loading options recently changed. It's probably
1.2.0,straightforward to rewrite the audio handling to make use of
1.2.0,"up-to-date torchaudio, but in the meantime there is a legacy"
1.2.0,method which uses the old defaults
1.2.0,STFT
1.2.0,-*- coding: utf-8 -*-
1.2.0,domain specific dependencies
1.2.0,coding: utf-8
1.2.0,several data readers need optional dependencies. There's no
1.2.0,appropriate builtin exception
1.2.0,-*- coding: utf-8 -*-
1.2.0,mix this with partial
1.2.0,batch (list(list(list))): batch_size x len(self.fields) x seq_len
1.2.0,lengths: batch_size
1.2.0,data: seq_len x batch_size x len(self.fields)
1.2.0,flake8: noqa
1.2.0,For command-line option parsing
1.2.0,"Check pass, set the args."
1.2.0,"This SRU version implements its own cuda-level optimization,"
1.2.0,so it requires that:
1.2.0,1. `cupy` and `pynvrtc` python package installed.
1.2.0,2. pytorch is built with cuda support.
1.2.0,3. library path set: export LD_LIBRARY_PATH=<cuda lib path>.
1.2.0,Check 1.
1.2.0,Check 2.
1.2.0,Check 3.
1.2.0,This sets up device to use.
1.2.0,-> directions x batch x dim
1.2.0,For DEBUG
1.2.0,"size = (length, batch, x.size(-1)) \"
1.2.0,"if x.dim() == 3 else (batch, x.size(-1))"
1.2.0,grad_x = x.new(*x.size()) if k_ == 3 else x.new(*size).zero_()
1.2.0,Normal use
1.2.0,"An entry check here, will catch on train side and translate side"
1.2.0,if requirements are not satisfied.
1.2.0,RNNDecoderState wraps hidden as a tuple.
1.2.0,fh -> (layers*directions) x batch x dim
1.2.0,NOTE: We need to trim the vocab to remove any unk tokens that
1.2.0,were not originally here.
1.2.0,!/usr/bin/env python
1.2.0,!/usr/bin/env python
1.2.0,!/usr/bin/env python
1.2.0,-*- coding: utf-8 -*-
1.2.0,!/usr/bin/env python
1.2.0,-*- coding: utf-8 -*-
1.2.0,create one counter per shard
1.2.0,"every corpus has shards, no new one"
1.2.0,patch corpus_id
1.2.0,!/usr/bin/env python
1.2.0,!/usr/bin/env python
1.2.0,Fix CPU tensor sharing strategy
1.2.0,Load checkpoint if we resume from a previous training.
1.2.0,check for code where vocab is saved instead of fields
1.2.0,(in the future this will be done in a smarter way)
1.2.0,patch for fields that may be missing in old data/model
1.2.0,Create a thread to listen for errors in the child processes.
1.2.0,Train with multiprocessing.
1.2.0,generator_to_serve = iter(generator_to_serve)
1.2.0,hack to dodge unpicklable `dict_keys`
1.2.0,"propagate exception to parent process, keeping original traceback"
1.2.0,magic indices
1.2.0,result caching
1.2.0,fix length constraint
1.2.0,add one to account for BOS. Don't account for EOS because hitting
1.2.0,this implies it hasn't been found.
1.2.0,we don't block nothing if the user doesn't want it
1.2.0,we can't block nothing beam's too short
1.2.0,we check paths one by one
1.2.0,we don't forbid nothing if the user doesn't want it
1.2.0,we can't forbid nothing if beam's too short
1.2.0,Reordering forbidden_tokens following beam selection
1.2.0,We rebuild a dict to ensure we get the value and not the pointer
1.2.0,Grabing the newly selected tokens and associated ngram
1.2.0,skip the blocking if any token in current_ngram is excluded
1.2.0,"pickups: Tensor where specified index were set to 1, others 0"
1.2.0,"dropdowns: opposite of pickups, 1 for those shouldn't pick"
1.2.0,Minus dropdowns to log_probs making probabilities of
1.2.0,unspecified index close to 0
1.2.0,"prediction step have surpass length of given target_prefix,"
1.2.0,no need to further change this attr
1.2.0,"For temp=0.0, take the argmax to avoid divide-by-zero errors."
1.2.0,keep_topk=1 is also equivalent to argmax.
1.2.0,Set all logits that are not in the top-k to -10000.
1.2.0,This puts the probabilities close to 0.
1.2.0,maybe fix some prediction at this step by modifying log_probs
1.2.0,"shape: (sum(~ self.is_finished), 1)"
1.2.0,!/usr/bin/env python
1.2.0,Maintains the longest src and tgt length in the current batch
1.2.0,Reset current longest length at a new batch (count=1)
1.2.0,max_tgt_in_batch = 0
1.2.0,Src: [<bos> w1 ... wN <eos>]
1.2.0,Tgt: [w1 ... wM <eos>]
1.2.0,for debugging
1.2.0,corpus_id field is useless here
1.2.0,Statistics
1.2.0,(0) add BOS and padding to tgt prediction
1.2.0,(1) Encoder forward.
1.2.0,(2) Repeat src objects `n_best` times.
1.2.0,"We use batch_size x n_best, get ``(src_len, batch * n_best, nfeat)``"
1.2.0,"(3) Init decoder with n_best src,"
1.2.0,"reshape tgt to ``(len, batch * n_best, nfeat)``"
1.2.0,masked_select
1.2.0,get aligned src id for each prediction's valid tgt tokens
1.2.0,TODO: support these blacklisted features
1.2.0,Turn any copied words into UNKs.
1.2.0,"Decoder forward, takes [tgt_len, batch, nfeats] as input"
1.2.0,"and [src_len, batch, hidden] as memory_bank"
1.2.0,"in case of inference tgt_len = 1, batch = beam times batch_size"
1.2.0,"in case of Gold Scoring tgt_len = actual length, batch = 1 batch"
1.2.0,Generator forward.
1.2.0,"returns [(batch_size x beam_size) , vocab ] when 1 step"
1.2.0,"or [ tgt_len, batch_size, vocab ] when full sentence"
1.2.0,"here we have scores [tgt_lenxbatch, vocab] or [beamxbatch, vocab]"
1.2.0,"returns [(batch_size x beam_size) , vocab ] when 1 step"
1.2.0,"or [ tgt_len, batch_size, vocab ] when full sentence"
1.2.0,(0) Prep the components of the search.
1.2.0,(1) Run the encoder on the src.
1.2.0,(2) prep decode_strategy. Possibly repeat src objects.
1.2.0,(3) Begin decoding step by step:
1.2.0,Reorder states.
1.2.0,beam parameters
1.2.0,result caching
1.2.0,beam state
1.2.0,BoolTensor was introduced in pytorch 1.2
1.2.0,"""global state"" of the old beam"
1.2.0,buffers for the topk scores and 'backpointer'
1.2.0,for testing
1.2.0,maybe fix some prediction at this step by modifying log_probs
1.2.0,Flatten probs into a list of possibilities.
1.2.0,using integer division to get an integer _B without casting
1.2.0,force the output to be longer than self.min_length
1.2.0,Multiply probs by the beam probability.
1.2.0,"if the sequence ends now, then the penalty is the current"
1.2.0,"length + 1, to include the EOS token"
1.2.0,Avoid any direction that would repeat unwanted ngrams
1.2.0,Pick up candidate token by curr_scores
1.2.0,Recover log probs.
1.2.0,Length penalty is just a scalar. It doesn't matter if it's applied
1.2.0,before or after the topk.
1.2.0,Resolve beam origin and map to batch index flat representation.
1.2.0,Append last prediction.
1.2.0,update global state (step == 1)
1.2.0,update global state (step > 1)
1.2.0,"shape: (batch_size x beam_size, 1)"
1.2.0,Penalize beams that finished.
1.2.0,"on real data (newstest2017) with the pretrained transformer,"
1.2.0,it's faster to not move this back to the original device
1.2.0,Store finished hypotheses for this batch.
1.2.0,End condition is the top beam finished and we can return
1.2.0,n_best hypotheses.
1.2.0,"If all sentences are translated, no need to go further."
1.2.0,Remove finished batches for the next step.
1.2.0,Term will be subtracted from probability
1.2.0,Probability will be divided by this
1.2.0,these warnings indicate that either the alpha/beta
1.2.0,"forces a penalty to be a no-op, or a penalty is a no-op but"
1.2.0,the alpha/beta would suggest otherwise.
1.2.0,using some length penalty
1.2.0,using some coverage penalty
1.2.0,!/usr/bin/env python
1.2.0,semaphore doesn't have a timeout arg in Python 2.7
1.2.0,perform a first request to initialize everything
1.2.0,backwards compatibility for confs
1.2.0,every segment becomes a dict for flexibility purposes
1.2.0,NOTE: translator returns lists of `n_best` list
1.2.0,build back results with empty texts
1.2.0,load can be called multiple times: modify copy
1.2.0,output contain alignment
1.2.0,Below are all the different penalty terms implemented so far.
1.2.0,Subtract coverage penalty from topk log probs.
1.2.0,Divide topk log probs by length penalty.
1.2.0,Sorting
1.2.0,Chinese segmentation
1.2.0,Chinese simplify -> Chinese traditional standard
1.2.0,Chinese simplify -> Chinese traditional (HongKong)
1.2.0,Chinese simplify -> Chinese traditional (Taiwan)
1.2.0,Chinese traditional -> Chinese simplify (v1)
1.2.0,Chinese traditional -> Chinese simplify (v2)
1.1.1,!/usr/bin/env python
1.1.1,!/usr/bin/env python
1.1.1,!/usr/bin/env python
1.1.1,!/usr/bin/env python
1.1.1,!/usr/bin/env python
1.1.1,!/usr/bin/env python3
1.1.1,-*- coding: utf-8 -*-
1.1.1,
1.1.1,"OpenNMT-py documentation build configuration file, created by"
1.1.1,sphinx-quickstart on Sun Dec 17 12:07:14 2017.
1.1.1,
1.1.1,This file is execfile()d with the current directory set to its
1.1.1,containing dir.
1.1.1,
1.1.1,Note that not all possible configuration values are present in this
1.1.1,autogenerated file.
1.1.1,
1.1.1,All configuration values have a default; values that are commented out
1.1.1,serve to show the default.
1.1.1,"If extensions (or modules to document with autodoc) are in another directory,"
1.1.1,add these directories to sys.path here. If the directory is relative to the
1.1.1,"documentation root, use os.path.abspath to make it absolute, like shown here."
1.1.1,
1.1.1,import os
1.1.1,import sys
1.1.1,"sys.path.insert(0, os.path.abspath('.'))"
1.1.1,-- General configuration ------------------------------------------------
1.1.1,"If your documentation needs a minimal Sphinx version, state it here."
1.1.1,
1.1.1,needs_sphinx = '1.0'
1.1.1,"Add any Sphinx extension module names here, as strings. They can be"
1.1.1,extensions coming with Sphinx (named 'sphinx.ext.*') or your custom
1.1.1,ones.
1.1.1,Show base classes
1.1.1,"Use ""variables"" section for Attributes instead of weird block things"
1.1.1,mimicking the function style.
1.1.1,"Add any paths that contain templates here, relative to this directory."
1.1.1,The suffix(es) of source filenames.
1.1.1,You can specify multiple suffix as a list of string:
1.1.1,
1.1.1,"source_suffix = ['.rst', '.md']"
1.1.1,The master toctree document.
1.1.1,General information about the project.
1.1.1,"The version info for the project you're documenting, acts as replacement for"
1.1.1,"|version| and |release|, also used in various other places throughout the"
1.1.1,built documents.
1.1.1,
1.1.1,The short X.Y version.
1.1.1,"The full version, including alpha/beta/rc tags."
1.1.1,The language for content autogenerated by Sphinx. Refer to documentation
1.1.1,for a list of supported languages.
1.1.1,
1.1.1,This is also used if you do content translation via gettext catalogs.
1.1.1,"Usually you set ""language"" from the command line for these cases."
1.1.1,"List of patterns, relative to source directory, that match files and"
1.1.1,directories to ignore when looking for source files.
1.1.1,This patterns also effect to html_static_path and html_extra_path
1.1.1,The name of the Pygments (syntax highlighting) style to use.
1.1.1,"If true, `todo` and `todoList` produce output, else they produce nothing."
1.1.1,-- Options for HTML output ----------------------------------------------
1.1.1,The theme to use for HTML and HTML Help pages.  See the documentation for
1.1.1,a list of builtin themes.
1.1.1,
1.1.1,html_theme = 'sphinx_materialdesign_theme'
1.1.1,html_theme_path = [sphinx_materialdesign_theme.get_path()]
1.1.1,Theme options are theme-specific and customize the look and feel of a theme
1.1.1,"further.  For a list of options available for each theme, see the"
1.1.1,documentation.
1.1.1,
1.1.1,html_theme_options = {}
1.1.1,"Add any paths that contain custom static files (such as style sheets) here,"
1.1.1,"relative to this directory. They are copied after the builtin static files,"
1.1.1,"so a file named ""default.css"" will overwrite the builtin ""default.css""."
1.1.1,"Custom sidebar templates, must be a dictionary that maps document names"
1.1.1,to template names.
1.1.1,
1.1.1,This is required for the alabaster theme
1.1.1,refs: http://alabaster.readthedocs.io/en/latest/installation.html#sidebars
1.1.1,-- Options for HTMLHelp output ------------------------------------------
1.1.1,Output file base name for HTML help builder.
1.1.1,-- Options for LaTeX output ---------------------------------------------
1.1.1,The paper size ('letterpaper' or 'a4paper').
1.1.1,
1.1.1,"'papersize': 'letterpaper',"
1.1.1,"The font size ('10pt', '11pt' or '12pt')."
1.1.1,
1.1.1,"'pointsize': '10pt',"
1.1.1,Additional stuff for the LaTeX preamble.
1.1.1,
1.1.1,"'preamble': '',"
1.1.1,Latex figure (float) alignment
1.1.1,
1.1.1,"'figure_align': 'htbp',"
1.1.1,Grouping the document tree into LaTeX files. List of tuples
1.1.1,"(source start file, target name, title,"
1.1.1,"author, documentclass [howto, manual, or own class])."
1.1.1,-- Options for manual page output ---------------------------------------
1.1.1,One entry per manual page. List of tuples
1.1.1,"(source start file, name, description, authors, manual section)."
1.1.1,-- Options for Texinfo output -------------------------------------------
1.1.1,Grouping the document tree into Texinfo files. List of tuples
1.1.1,"(source start file, target name, title, author,"
1.1.1,"dir menu entry, description, category)"
1.1.1,degenerate case
1.1.1,cache the features
1.1.1,mp queues don't work well between procs unless they're from a manager
1.1.1,each device has its own saver so that reconstructing is easier
1.1.1,!/usr/bin/env python
1.1.1,-*- coding: utf-8 -*-
1.1.1,is this reachable?
1.1.1,Read in embeddings
1.1.1,Write to file
1.1.1,!/usr/bin/env python
1.1.1,-*- coding: utf-8 -*-
1.1.1,"Add in default model arguments, possibly added since training."
1.1.1,-*- encoding: utf-8 -*-
1.1.1,!/usr/bin/env python
1.1.1,-*- coding: utf-8 -*-
1.1.1,Author: Rico Sennrich
1.1.1,flake8: noqa
1.1.1,This file is retrieved from https://github.com/rsennrich/subword-nmt
1.1.1,hack for python2/3 compatibility
1.1.1,check version information
1.1.1,some hacking to deal with duplicates (only consider first instance)
1.1.1,don't print end-of-word symbols
1.1.1,sys.stderr.write('cannot split {0} further.\n'.format(segment))
1.1.1,sys.stderr.write('OOV: {0}\n'.format(segment))
1.1.1,sys.stderr.write('OOV: {0}\n'.format(segment))
1.1.1,python 2/3 compatibility
1.1.1,read/write files as UTF-8
1.1.1,!/usr/bin/env python
1.1.1,!/usr/bin/env python
1.1.1,-*- coding: utf-8 -*-
1.1.1,Author: Rico Sennrich
1.1.1,flake8: noqa
1.1.1,This file is retrieved from https://github.com/rsennrich/subword-nmt
1.1.1,hack for python2/3 compatibility
1.1.1,"find all instances of pair, and update frequency/indices around it"
1.1.1,find first symbol
1.1.1,"if first symbol is followed by second symbol, we've found an occurrence of pair (old_word[i:i+2])"
1.1.1,"assuming a symbol sequence ""A B C"", if ""B C"" is merged, reduce the frequency of ""A B"""
1.1.1,"assuming a symbol sequence ""A B C B"", if ""B C"" is merged, reduce the frequency of ""C B""."
1.1.1,"however, skip this if the sequence is A B C B C, because the frequency of ""C B"" will be reduced by the previous code block"
1.1.1,find new pair
1.1.1,"assuming a symbol sequence ""A BC D"", if ""B C"" is merged, increase the frequency of ""A BC"""
1.1.1,"assuming a symbol sequence ""A BC B"", if ""B C"" is merged, increase the frequency of ""BC B"""
1.1.1,"however, if the sequence is A BC BC, skip this step because the count of ""BC BC"" will be incremented by the previous code block"
1.1.1,data structure of pair frequencies
1.1.1,index from pairs to words
1.1.1,version 0.2 changes the handling of the end-of-word token ('</w>');
1.1.1,version numbering allows bckward compatibility
1.1.1,"threshold is inspired by Zipfian assumption, but should only affect speed"
1.1.1,we probably missed the best pair because of pruning; go back to full statistics
1.1.1,"threshold is inspired by Zipfian assumption, but should only affect speed"
1.1.1,python 2/3 compatibility
1.1.1,read/write files as UTF-8
1.1.1,!/usr/bin/env python
1.1.1,for back compat when attention_dropout was not defined
1.1.1,Build embeddings.
1.1.1,Build encoder.
1.1.1,Build decoder.
1.1.1,Share the embedding matrix - preprocess with share_vocab required.
1.1.1,src/tgt vocab should be the same if `-share_vocab` is specified.
1.1.1,Build NMTModel(= encoder + decoder).
1.1.1,Build Generator.
1.1.1,Load the model states from checkpoint or initialize them.
1.1.1,This preserves backward-compat for models using customed layernorm
1.1.1,end of patch for backward compatibility
1.1.1,!/usr/bin/env python
1.1.1,NOTE: It's important that ``opt`` has been validated and updated
1.1.1,at this point.
1.1.1,Load checkpoint if we resume from a previous training.
1.1.1,check for code where vocab is saved instead of fields
1.1.1,(in the future this will be done in a smarter way)
1.1.1,"Report src and tgt vocab sizes, including for features"
1.1.1,Build model.
1.1.1,Build optimizer.
1.1.1,Build model saver
1.1.1,Embedding Options
1.1.1,Encoder-Decoder Options
1.1.1,"group.add('--residual', '-residual',   action=""store_true"","
1.1.1,"help=""Add residual connections between RNN layers."")"
1.1.1,Attention options
1.1.1,Alignement options
1.1.1,Generator and loss options.
1.1.1,Data options
1.1.1,"Dictionary options, for text corpus"
1.1.1,"if you want to pass an existing vocab.pt file, pass it to"
1.1.1,-src_vocab alone as it already contains tgt vocab.
1.1.1,"Truncation options, for text corpus"
1.1.1,Data processing options
1.1.1,Options most relevant to speech
1.1.1,Option most relevant to image input
1.1.1,Options for experimental source noising (BART style)
1.1.1,GPU
1.1.1,Init options
1.1.1,Pretrained word vectors
1.1.1,Fixed word vectors
1.1.1,Optimization options
1.1.1,learning rate
1.1.1,Use Tensorboard for visualization during training
1.1.1,Options most relevant to speech
1.1.1,Option most relevant to image input
1.1.1,Options most relevant to summarization.
1.1.1,Alpha and Beta values for Google Length + Coverage penalty
1.1.1,"Described here: https://arxiv.org/pdf/1609.08144.pdf, Section 7"
1.1.1,Options most relevant to speech.
1.1.1,Option most relevant to image input
1.1.1,Copyright 2016 The Chromium Authors. All rights reserved.
1.1.1,Use of this source code is governed by a BSD-style license that can be
1.1.1,found in the LICENSE file.
1.1.1,"Get the key 'value' in the dict, or just use 'value'"
1.1.1,Basic attributes.
1.1.1,Set model in training mode.
1.1.1,UPDATE DROPOUT
1.1.1,Run patience mechanism
1.1.1,"If the patience has reached the limit, stop training"
1.1.1,swap model params w/ moving average
1.1.1,(and keep the original parameters)
1.1.1,Set model in validating mode.
1.1.1,F-prop through the model.
1.1.1,Compute loss.
1.1.1,Update statistics.
1.1.1,Set model back to training mode.
1.1.1,Truncated BPTT: reminder not compatible with accum > 1
1.1.1,1. Create truncated target.
1.1.1,2. F-prop all but generator.
1.1.1,3. Compute loss.
1.1.1,4. Update the parameters and statistics.
1.1.1,Multi GPU gradient gather
1.1.1,"If truncated, don't backprop fully."
1.1.1,TO CHECK
1.1.1,if dec_state is not None:
1.1.1,dec_state.detach()
1.1.1,"in case of multi step gradient accumulation,"
1.1.1,update only after accum batches
1.1.1,For Flake
1.1.1,we avoid padding while mean pooling
1.1.1,Initialize the bridge layer
1.1.1,"s_len, batch, emb_dim = emb.size()"
1.1.1,Lengths data is wrapped inside a Tensor.
1.1.1,"LSTM has hidden and cell state, other only one"
1.1.1,Total number of states
1.1.1,Build a linear layer for each
1.1.1,The encoder hidden is  (layers*directions) x batch x dim.
1.1.1,"s_len, batch, emb_dim = emb.size()"
1.1.1,Run the forward pass of every layer of the tranformer.
1.1.1,why is the model_opt.__dict__ check necessary?
1.1.1,"(batch_size, 64, imgH, imgW)"
1.1.1,layer 1
1.1.1,"(batch_size, 64, imgH/2, imgW/2)"
1.1.1,"(batch_size, 128, imgH/2, imgW/2)"
1.1.1,layer 2
1.1.1,"(batch_size, 128, imgH/2/2, imgW/2/2)"
1.1.1,"(batch_size, 256, imgH/2/2, imgW/2/2)"
1.1.1,layer 3
1.1.1,batch norm 1
1.1.1,"(batch_size, 256, imgH/2/2, imgW/2/2)"
1.1.1,layer4
1.1.1,"(batch_size, 256, imgH/2/2/2, imgW/2/2)"
1.1.1,"(batch_size, 512, imgH/2/2/2, imgW/2/2)"
1.1.1,layer 5
1.1.1,batch norm 2
1.1.1,"(batch_size, 512, imgH/2/2/2, imgW/2/2/2)"
1.1.1,"(batch_size, 512, imgH/2/2/2, imgW/2/2/2)"
1.1.1,"# (batch_size, 512, H, W)"
1.1.1,Dimensions and padding for constructing the word embedding matrix
1.1.1,Dimensions and padding for feature embedding matrices
1.1.1,(these have no effect if feat_vocab_sizes is empty)
1.1.1,The embedding matrix look-up tables. The first look-up table
1.1.1,"is for words. Subsequent ones are for features, if any exist."
1.1.1,The final output size of word + feature vectors. This can vary
1.1.1,from the word vector size if and only if features are defined.
1.1.1,This is the attribute you should access if you need to know
1.1.1,how big your embeddings are going to be.
1.1.1,The sequence of operations that converts the input sequence
1.1.1,into a sequence of embeddings. At minimum this consists of
1.1.1,looking up the embeddings for each word and feature in the
1.1.1,input. Model parameters may require the sequence to contain
1.1.1,additional operations as well.
1.1.1,features must use word_vec_size
1.1.1,features will use feat_vec_size
1.1.1,This class is mainly used by decoder.py for RNNs but also
1.1.1,by the CNN / transformer decoder when copy attention is used
1.1.1,CNN has its own attention mechanism ConvMultiStepAttention
1.1.1,Transformer has its own MultiHeadedAttention
1.1.1,mlp wants it with bias
1.1.1,Check input sizes
1.1.1,"(batch, t_len, d) x (batch, d, s_len) --> (batch, t_len, s_len)"
1.1.1,"(batch, t_len, s_len, d)"
1.1.1,one step input
1.1.1,"compute attention scores, as in Luong et al."
1.1.1,Softmax or sparsemax to normalize attention weights
1.1.1,each context vector c_t is the weighted average
1.1.1,over all the source hidden states
1.1.1,concatenate
1.1.1,Check output sizes
1.1.1,Check output sizes
1.1.1,clamping necessary because of numerical errors: loss should be lower
1.1.1,"bounded by zero, but negative values near zero are possible without"
1.1.1,the clamp
1.1.1,from onmt.utils.misc import aeq
1.1.1,CHECKS
1.1.1,"batch, k_len, d = key.size()"
1.1.1,"batch_, k_len_, d_ = value.size()"
1.1.1,"aeq(batch, batch_)"
1.1.1,"aeq(k_len, k_len_)"
1.1.1,"aeq(d, d_)"
1.1.1,"batch_, q_len, d_ = query.size()"
1.1.1,"aeq(batch, batch_)"
1.1.1,"aeq(d, d_)"
1.1.1,"aeq(self.model_dim % 8, 0)"
1.1.1,if mask is not None:
1.1.1,"batch_, q_len_, k_len_ = mask.size()"
1.1.1,"aeq(batch_, batch)"
1.1.1,"aeq(k_len_, k_len)"
1.1.1,aeq(q_len_ == q_len)
1.1.1,END CHECKS
1.1.1,"1) Project key, value, and query."
1.1.1,1 or key_len x key_len
1.1.1,1 or key_len x key_len x dim_per_head
1.1.1,1 or key_len x key_len x dim_per_head
1.1.1,2) Calculate and scale scores.
1.1.1,batch x num_heads x query_len x key_len
1.1.1,3) Apply attention dropout and compute context vectors.
1.1.1,CHECK
1.1.1,"batch_, q_len_, d_ = output.size()"
1.1.1,"aeq(q_len, q_len_)"
1.1.1,"aeq(batch, batch_)"
1.1.1,"aeq(d, d_)"
1.1.1,Return multi-head attn
1.1.1,At the moment this class is only used by embeddings.Embeddings look-up tables
1.1.1,-*- coding: utf-8 -*-
1.1.1,checks
1.1.1,"batch, channel, height, width = base_target_emb.size()"
1.1.1,"batch_, channel_, height_, width_ = input_from_dec.size()"
1.1.1,"enc_batch, enc_channel, enc_height = encoder_out_top.size()"
1.1.1,"enc_batch_, enc_channel_, enc_height_ = encoder_out_combine.size()"
1.1.1,out_features * in_features
1.1.1,norm is out_features * 1
1.1.1,batch_size * out_features
1.1.1,out_features
1.1.1,out_features
1.1.1,batch_size * out_features
1.1.1,"out_channels, in_channels // groups, * kernel_size"
1.1.1,out_features
1.1.1,This is used nowhere in the code at the moment (Vincent Nguyen 05/18/2018)
1.1.1,"in_channels, out_channels, *kernel_size"
1.1.1,"in_channels, out_channels, *kernel_size"
1.1.1,"self.out_channels, 1"
1.1.1,out_features
1.1.1,out_features
1.1.1,store roots on diagonal
1.1.1,noise_skip = batch.noise_skip
1.1.1,aeq(len(batch.noise_skip) == source.size(1))
1.1.1,source is [src_len x bs x feats]
1.1.1,source might increase length so we need to resize the whole
1.1.1,tensor
1.1.1,remove useless pad
1.1.1,"def s(self, tokens):"
1.1.1,prob = self.prob
1.1.1,r = torch.rand([len(tokens)])
1.1.1,mask = False
1.1.1,masked = []
1.1.1,"for i, tok in enumerate(tokens):"
1.1.1,if tok.startswith(subword_prefix):
1.1.1,if r[i].item() <= prob:
1.1.1,masked.append(mask_tok)
1.1.1,mask = True
1.1.1,else:
1.1.1,masked.append(tok)
1.1.1,mask = False
1.1.1,else:
1.1.1,if mask:
1.1.1,pass
1.1.1,else:
1.1.1,masked.append(tok)
1.1.1,return masked
1.1.1,"aeq(source.size(0), length)"
1.1.1,Pretend it ends with a full stop so last span is a sentence
1.1.1,"Tokens that are full stops, where the previous token is not"
1.1.1,"aeq(source.size(0), length)"
1.1.1,-1: keep everything (i.e. 1 mask per token)
1.1.1,0: replace everything (i.e. no mask)
1.1.1,1: 1 mask per span
1.1.1,fairseq/data/denoising_dataset.py
1.1.1,"print(""src size: "", source.size())"
1.1.1,"print(""ws size: "", self.word_start_mask.size())"
1.1.1,"print(""max: "", source.max())"
1.1.1,assert source.max() < self.word_start_mask.size(0)
1.1.1,assert source.min() >= 0
1.1.1,assert source.size() == is_word_start.size()
1.1.1,"aeq(source.eq(self.pad_idx).long().sum(), 0)"
1.1.1,we manually add this hypothesis since it's required for the rest
1.1.1,of the function and kindof make sense
1.1.1,Make sure we have enough to mask
1.1.1,Trim to masking budget
1.1.1,Handle 0-length mask (inserts) separately
1.1.1,assert (lengths > 0).all()
1.1.1,assert is_word_start[-1] == 0
1.1.1,TODO why?
1.1.1,assert source_length - 1 not in indices
1.1.1,"acts as a long length, so spans don't go over the end of doc"
1.1.1,"keep index, but replace it with [MASK]"
1.1.1,random ratio disabled
1.1.1,source[indices[mask_random]] = torch.randint(
1.1.1,"1, len(self.vocab), size=(mask_random.sum(),))"
1.1.1,if self.mask_span_distribution is not None:
1.1.1,assert len(lengths.size()) == 1
1.1.1,assert lengths.size() == indices.size()
1.1.1,assert lengths.size() == indices.size()
1.1.1,mask_random = mask_random[uncompleted]
1.1.1,delete token
1.1.1,"keep index, but replace it with [MASK]"
1.1.1,random ratio disabled
1.1.1,source[indices[mask_random]] = torch.randint(
1.1.1,"1, len(self.vocab), size=(mask_random.sum(),))"
1.1.1,else:
1.1.1,# A bit faster when all lengths are 1
1.1.1,while indices.size(0) > 0:
1.1.1,uncompleted = is_word_start[indices + 1] == 0
1.1.1,indices = indices[uncompleted] + 1
1.1.1,mask_random = mask_random[uncompleted]
1.1.1,if self.replace_length != -1:
1.1.1,# delete token
1.1.1,to_keep[indices] = 0
1.1.1,else:
1.1.1,"# keep index, but replace it with [MASK]"
1.1.1,source[indices] = self.mask_idx
1.1.1,source[indices[mask_random]] = torch.randint(
1.1.1,"1, len(self.vocab), size=(mask_random.sum(),))"
1.1.1,assert source_length - 1 not in indices
1.1.1,"aeq(source.eq(self.pad_idx).long().sum(), 0)"
1.1.1,random ratio disabled
1.1.1,num_random = int(math.ceil(n * self.random_ratio))
1.1.1,result[noise_indices[:num_random]] = torch.randint(
1.1.1,"low=1, high=len(self.vocab), size=(num_random,))"
1.1.1,assert (result >= 0).all()
1.1.1,CHECKS
1.1.1,Original probabilities.
1.1.1,Probability of copying p(z=1) batch.
1.1.1,Probability of not copying: p_{word}(w) * (1 - p(z))
1.1.1,probabilities assigned by the model to the gold targets
1.1.1,probability of tokens copied from source
1.1.1,Set scores for unk to 0 and add eps
1.1.1,find the indices in which you do not use the copy mechanism
1.1.1,Drop padding.
1.1.1,this block does not depend on the loss value computed above
1.1.1,and is used only for stats
1.1.1,this block does not depend on the loss value computed above
1.1.1,and is used only for stats
1.1.1,Correct target copy token instead of <unk>
1.1.1,tgt[i] = align[i] + len(tgt_vocab)
1.1.1,for i such that tgt[i] == 0 and align[i] != 0
1.1.1,Compute sum of perplexities for stats
1.1.1,this part looks like it belongs in CopyGeneratorLoss
1.1.1,Compute Loss as NLL divided by seq length
1.1.1,Compute Total Loss per sequence in batch
1.1.1,Divide by length of each sequence and sum
1.1.1,initialize fields at the top of each unit test to prevent
1.1.1,any undesired stateful effects
1.1.1,"this test touches the file system, so it could be considered an"
1.1.1,integration test
1.1.1,write utf-8 bytes
1.1.1,batch 0 will always predict EOS. The other batches will predict
1.1.1,non-eos scores.
1.1.1,"""best"" prediction is eos - that should be blocked"
1.1.1,include at least one prediction OTHER than EOS
1.1.1,that is greater than -1e20
1.1.1,now batch 0 has ended and no others have
1.1.1,initial step
1.1.1,batch 0 dies on step 0
1.1.1,include at least one prediction OTHER than EOS
1.1.1,that is greater than -1e20
1.1.1,step 2
1.1.1,(old) batch 8 dies on step 1
1.1.1,step 3
1.1.1,everything dies
1.1.1,initial step
1.1.1,batch 0 dies on step 0
1.1.1,include at least one prediction OTHER than EOS
1.1.1,that is greater than -1e20
1.1.1,step 2
1.1.1,(old) batch 8 dies on step 1
1.1.1,step 3
1.1.1,everything dies
1.1.1,illegal_weights_mask = torch.ByteTensor([
1.1.1,"[0, 0, 0, 0, 0, 0, 0],"
1.1.1,"[0, 0, 0, 1, 1, 1, 1],"
1.1.1,"[0, 0, 0, 0, 0, 1, 1],"
1.1.1,"[0, 0, 1, 1, 1, 1, 1]])"
1.1.1,TODO: fix for pytorch 0.3
1.1.1,illegal_weights = alignments.masked_select(illegal_weights_mask)
1.1.1,"self.assertEqual(0.0, illegal_weights.data.sum())"
1.1.1,this could be considered an integration test because it touches
1.1.1,the filesystem for the config file (and the models)
1.1.1,-*- coding: utf-8 -*-
1.1.1,tests pad and numericalize integration
1.1.1,tests pad and numericalize integration
1.1.1,"this test touches the file system, so it could be considered an"
1.1.1,integration test
1.1.1,file to hold full paths to audio data
1.1.1,file to hold audio paths relative to _AUDIO_DATA_DIR (i.e. file names)
1.1.1,it's ok if non-audio files co-exist with audio files in the data dir
1.1.1,"dividing gets the noise in [-1, 1]"
1.1.1,"this test touches the file system, so it could be considered an"
1.1.1,integration test
1.1.1,file to hold full paths to image data
1.1.1,file to hold image paths relative to _IMG_DATA_DIR (i.e. file names)
1.1.1,it's ok if non-image files co-exist with image files in the data dir
1.1.1,all beams repeat (beam >= 1 repeat dummy scores)
1.1.1,predict repeat_idx over and over again
1.1.1,"before repeat, scores are either 0 or -inf"
1.1.1,"on repeat, `repeat_idx` score is BLOCKED_SCORE"
1.1.1,"(but it's still the best score, thus we have"
1.1.1,"[BLOCKED_SCORE, -inf, -inf, -inf, -inf]"
1.1.1,repetitions keeps maximizing score
1.1.1,"index 0 has been blocked, so repeating=>+0.0 score"
1.1.1,other indexes are -inf so repeating=>BLOCKED_SCORE
1.1.1,which is higher
1.1.1,beam 0 and beam >=2 will repeat (beam >= 2 repeat dummy scores)
1.1.1,non-interesting beams are going to get dummy values
1.1.1,"on initial round, only predicted scores for beam 0"
1.1.1,matter. Make two predictions. Top one will be repeated
1.1.1,"in beam zero, second one will live on in beam 1."
1.1.1,predict the same thing in beam 0
1.1.1,continue pushing around what beam 1 predicts
1.1.1,"now beam 0 dies (along with the others), beam 1 -> beam 0"
1.1.1,"now beam 0 dies (along with the others), beam 1 -> beam 0"
1.1.1,beam 0 and beam >= 2 will repeat (beam 2 repeats excluded idx)
1.1.1,non-interesting beams are going to get dummy values
1.1.1,predict the same thing in beam 0
1.1.1,continue pushing around what beam 1 predicts
1.1.1,predict the allowed-repeat again in beam 2
1.1.1,"now beam 0 dies, beam 1 -> beam 0, beam 2 -> beam 1"
1.1.1,and the rest die
1.1.1,"since all preds after i=0 are 0, we can check"
1.1.1,that the beam is the correct idx by checking that
1.1.1,the curr score is the initial score
1.1.1,beam 0 will always predict EOS. The other beams will predict
1.1.1,non-eos scores.
1.1.1,non-interesting beams are going to get dummy values
1.1.1,"""best"" prediction is eos - that should be blocked"
1.1.1,include at least beam_sz predictions OTHER than EOS
1.1.1,that are greater than -1e20
1.1.1,predict eos in beam 0
1.1.1,provide beam_sz other good predictions
1.1.1,now the top beam has ended and no others have
1.1.1,"not of interest, but want to make sure it keeps running"
1.1.1,since only beam 0 terminates and n_best = 2
1.1.1,"this is also a test that when block_ngram_repeat=0,"
1.1.1,repeating is acceptable
1.1.1,non-interesting beams are going to get dummy values
1.1.1,"""best"" prediction is eos - that should be blocked"
1.1.1,include at least beam_sz predictions OTHER than EOS
1.1.1,that are greater than -1e20
1.1.1,predict eos in beam 1
1.1.1,provide beam_sz other good predictions in other beams
1.1.1,provide beam_sz other good predictions in other beams
1.1.1,beam 1 dies on min_length
1.1.1,beam 0 dies on the step after beam 1 dies
1.1.1,"inp_lens is tiled in initialize, reassign to make attn match"
1.1.1,non-interesting beams are going to get dummy values
1.1.1,"""best"" prediction is eos - that should be blocked"
1.1.1,include at least beam_sz predictions OTHER than EOS
1.1.1,that are greater than -1e20
1.1.1,predict eos in beam 1
1.1.1,provide beam_sz other good predictions in other beams
1.1.1,provide beam_sz other good predictions in other beams
1.1.1,no top beams are finished yet
1.1.1,beam 1 dies on min_length
1.1.1,no top beams are finished yet
1.1.1,beam 0 dies on the step after beam 1 dies
1.1.1,top beam is finished now so there are attentions
1.1.1,two beams are finished in each batch
1.1.1,second dim is cut down to the non-padded src length
1.1.1,first dim is equal to the time of death
1.1.1,(beam 0 died at current step - adjust for SOS)
1.1.1,(beam 1 died at last step - adjust for SOS)
1.1.1,behavior gets weird when beam is already done so just stop
1.1.1,this is just test_beam.TestBeamAgainstReferenceCase repeated
1.1.1,in each batch.
1.1.1,"init_preds: [4, 3, 5, 6, 7] - no EOS's"
1.1.1,no EOS's yet
1.1.1,"[5, 3, 2, 6, 0], so beam 2 predicts EOS!"
1.1.1,assumes beam 2 finished on last step
1.1.1,ended beam 2 shouldn't continue
1.1.1,"[2, 5, 3, 6, 0] repeat self.BATCH_SZ, so beam 0 predicts EOS!"
1.1.1,"[-2.4879, -3.8910, -4.1010, -4.2010, -4.4010] repeat self.BATCH_SZ"
1.1.1,another beam is finished in all batches
1.1.1,new beam 0 finished
1.1.1,new beam 0 is old beam 3
1.1.1,assumes beam 0 finished on last step
1.1.1,"[5, 2, 6, 1, 0] repeat self.BATCH_SZ, so beam 1 predicts EOS!"
1.1.1,new beam 1 finished
1.1.1,new beam 1 is old beam 4
1.1.1,this could be considered an integration test because it tests
1.1.1,interactions between the GNMT scorer and the beam
1.1.1,"-data option is required, but not used in this test, so dummy."
1.1.1,len x batch x nfeat
1.1.1,batch x c x h x w
1.1.1,batch x 1 x nfft x t
1.1.1,Initialize vectors to compare size with
1.1.1,Ensure correct sizes and types
1.1.1,Make sure that output has the correct size and type
1.1.1,Make sure that output has the correct size and type
1.1.1,Make sure that output has the correct size and type
1.1.1,"[('encoder_type', 'transformer'),"
1.1.1,"('word_vec_size', 16), ('rnn_size', 16)],"
1.1.1,""""""" Only do SRU test if requirment is safisfied. """""""
1.1.1,SRU doesn't support input_feed.
1.1.1,"when reasonable, set audio_enc_pooling to 2"
1.1.1,Need lengths >= audio_enc_pooling**n_layers.
1.1.1,"That condition is unrealistic for large n_layers,"
1.1.1,so leave audio_enc_pooling at 1.
1.1.1,first check there's nothing unexpectedly not trainable
1.1.1,ok: word embeddings shouldn't be trainable
1.1.1,if word vecs are fixed
1.1.1,ok: positional encodings shouldn't be trainable
1.1.1,then check nothing unexpectedly trainable
1.1.1,!/usr/bin/env python
1.1.1,-*- coding: utf-8 -*-
1.1.1,Remove the generated *pt files.
1.1.1,Test image preprocessing
1.1.1,Test audio preprocessing
1.1.1,Decoder state
1.1.1,Build the RNN.
1.1.1,Set up the context gate.
1.1.1,Set up the standard attention.
1.1.1,The encoder hidden is  (layers*directions) x batch x dim.
1.1.1,We need to convert it to layers x batch x (directions*dim).
1.1.1,Init the input feed.
1.1.1,Update the state with the result.
1.1.1,Concatenates sequence of tensors along a new dimension.
1.1.1,NOTE: v0.3 to 0.4: dec_outs / attns[*] may not be list
1.1.1,(in particular in case of SRU) it was not raising error in 0.3
1.1.1,since stack(Variable) was allowed.
1.1.1,"In 0.4, SRU returns a tensor that shouldn't be stacke"
1.1.1,Check
1.1.1,Calculate the attention.
1.1.1,Calculate the context gate.
1.1.1,Additional args check.
1.1.1,END Additional args check.
1.1.1,Input feed concatenates hidden state with
1.1.1,input at every time step.
1.1.1,TODO: context gate should be employed
1.1.1,instead of second RNN transform.
1.1.1,Update the coverage attention.
1.1.1,Decoder State
1.1.1,CNNDecoder has its own attention mechanism.
1.1.1,Set up a separate copy attention layer if needed.
1.1.1,The output of CNNEncoder.
1.1.1,The combination of output of CNNEncoder and source embeddings.
1.1.1,Process the result and update the attentions.
1.1.1,Update the state.
1.1.1,TODO change the way attns is returned dict => list or tuple (onnx)
1.1.1,Memory_lengths is a single tensor shared between all models.
1.1.1,This assumption will not hold if Translator is modified
1.1.1,to calculate memory_lengths as something other than the length
1.1.1,of the input.
1.1.1,"return _, (B, Q_len, K_len)"
1.1.1,"layer average attention across heads, get ``(B, Q, K)``"
1.1.1,"Case 1: no full_context, no align heads -> layer avg baseline"
1.1.1,"Case 2: no full_context, 1 align heads -> guided align"
1.1.1,"Case 3: full_context, 1 align heads -> full cte guided align"
1.1.1,T: could be 1 in the case of stepwise decoding or tgt_len
1.1.1,BoolTensor was introduced in pytorch 1.2
1.1.1,Decoder State
1.1.1,"previously, there was a GlobalAttention module here for copy"
1.1.1,"attention. But it was never actually used -- the ""copy"" attention"
1.1.1,just reuses the context attention.
1.1.1,"attns[""align""] = torch.stack(attn_aligns, 0).mean(0)  # All avg"
1.1.1,TODO change the way attns is returned dict => list or tuple (onnx)
1.1.1,"buffer size in bytes, determine equiv. # of elements based on data type"
1.1.1,copy tensors into buffer_t
1.1.1,all-reduce and rescale
1.1.1,copy all-reduced buffer back into tensors
1.1.1,"tensor is bigger than buffer, all-reduce and rescale directly"
1.1.1,"buffer is full, all-reduce and replace buffer with grad"
1.1.1,add tensor to buffer
1.1.1,TODO: Find a better way to check for sparse gradients.
1.1.1,we use here a FusedAdam() copy of an old Apex repo
1.1.1,In this case use the new AMP API from apex
1.1.1,In this case use the old FusedAdam with FP16_optimizer wrapper
1.1.1,Load everything from the checkpoint.
1.1.1,Build everything from scratch.
1.1.1,"Reset optimizer, keep options."
1.1.1,"Reset options, keep optimizer."
1.1.1,State can be partially restored.
1.1.1,Code below is an implementation of https://arxiv.org/pdf/1804.04235.pdf
1.1.1,inspired but modified from https://github.com/DeadAt0m/adafactor-pytorch
1.1.1,backward compatibility
1.1.1,assuming a list/generator of parameter means single group
1.1.1,compute combined scale factor for this group
1.1.1,norm is in fact norm*scale
1.1.1,note: p.grad should not ever be set for correct operation of
1.1.1,mixed precision optimizer that sometimes sends None gradients
1.1.1,State initialization
1.1.1,Exponential moving average of gradient values
1.1.1,Exponential moving average of squared gradient values
1.1.1,-*- coding: utf-8 -*-
1.1.1,if the loss function operates on vectors of raw logits instead of
1.1.1,"probabilities, only the first part of the generator needs to be"
1.1.1,"passed to the NMTLossCompute. At the moment, the only supported"
1.1.1,loss function of this kind is the sparsemax loss.
1.1.1,"attn_align should be in (batch_size, pad_tgt_size, pad_src_size)"
1.1.1,"align_idx should be a Tensor in size([N, 3]), N is total number"
1.1.1,"of align src-tgt pair in current batch, each as"
1.1.1,"['sent_NÂ°_in_batch', 'tgt_id+1', 'src_id'] (check AlignField)"
1.1.1,NOTE: tgt-src ref alignement that in range_ of shard
1.1.1,(coherent with batch.tgt)
1.1.1,"align_head contains value in [0, 1) presenting attn prob,"
1.1.1,0 was resulted by the context attention src_pad_mask
1.1.1,"So, the correspand position in ref_align should also be 0"
1.1.1,"Therefore, clip align_head to > 1e-18 should be bias free."
1.1.1,non_none: the subdict of the state dictionary where the values
1.1.1,are not None.
1.1.1,"Now, the iteration:"
1.1.1,state is a dictionary of sequences of tensor-like but we
1.1.1,want a sequence of dictionaries of tensors.
1.1.1,"First, unzip the dictionary into a sequence of keys and a"
1.1.1,sequence of tensor-like sequences.
1.1.1,"Now, yield a dictionary for each shard. The keys are always"
1.1.1,the same. values is a sequence of length #keys where each
1.1.1,element is a sequence of length #shards. We want to iterate
1.1.1,"over the shards, not over the keys: therefore, the values need"
1.1.1,to be re-zipped by shard and then each shard can be paired
1.1.1,with the keys.
1.1.1,Assumed backprop'd
1.1.1,this check is here because audio allows the encoder and decoder to
1.1.1,"be different sizes, but other model types do not yet"
1.1.1,"Load default opt values, then overwrite with the opts in"
1.1.1,"the checkpoint. That way, if there are new options added,"
1.1.1,the defaults are used.
1.1.1,Don't do anything
1.1.1,Update best score of each criteria
1.1.1,Reset tolerance
1.1.1,Update current status
1.1.1,Decrease tolerance
1.1.1,Log
1.1.1,Log
1.1.1,Get a list of world_size lists with len(stat_list) Statistics objects
1.1.1,SRU doesn't support PackedSequence.
1.1.1,-*- coding: utf-8 -*-
1.1.1,threshold on 1 to avoid div by 0
1.1.1,treat alignment matrix one by one as each have different lengths
1.1.1,No alignment if not exist valid tgt token
1.1.1,get valid alignment (sub-matrix from full paded aligment matrix)
1.1.1,-*- coding: utf-8 -*-
1.1.1,this one is needed for torchtext random call (shuffled iterator)
1.1.1,in multi gpu it ensures datasets are read in the same order
1.1.1,some cudnn methods can be random even after fixing the seed
1.1.1,unless you tell it to be deterministic
1.1.1,These ensure same initialization in multi gpu mode
1.1.1,Shift values to be >= 0
1.1.1,we need to check the model path + any tokenizer path
1.1.1,coding: utf-8
1.1.1,make a small vocab containing just the tokens in the source sequence
1.1.1,Map source tokens to indices in the dynamic dict.
1.1.1,self.src_vocabs is used in collapse_copy_scores and Translator.py
1.1.1,this assumes src_field and tgt_field are both text
1.1.1,fields needs to have only keys that examples have as attrs
1.1.1,avoid infinite recursion when fields isn't defined
1.1.1,-*- coding: utf-8 -*-
1.1.1,backwards compatibility
1.1.1,monkey-patch to make torchtext Vocab's pickleable
1.1.1,"+1 for tgt side to keep coherent after ""bos"" padding,"
1.1.1,"register ['NÂ°_in_batch', 'tgt_id+1', 'src_id']"
1.1.1,"List[Tuple[str, Vocab]] -> List[Tuple[str, Field]]"
1.1.1,"-> dict[str, Field]"
1.1.1,"Dict[str, List[Tuple[str, Field]]]"
1.1.1,doesn't change structure - don't return early.
1.1.1,"Dict[str, List[Tuple[str, Field]]] -> List[Tuple[str, Field]]"
1.1.1,"-> dict[str, Field]"
1.1.1,"if tgt isn't using TextMultiField, then no text field is."
1.1.1,this is basically copy-pasted from torchtext.
1.1.1,counters changes in place
1.1.1,keep the order of tokens specified in the vocab file by
1.1.1,adding them to the counter with decreasing counting values
1.1.1,`tgt_vocab_size` is ignored when sharing vocabularies
1.1.1,return vocab to dump with standard name
1.1.1,empty train_dataset_files so that vocab is only loaded from
1.1.1,"given paths in src_vocab_path, tgt_vocab_path"
1.1.1,Load vocabulary
1.1.1,Drop the none-using from memory but keep the last
1.1.1,"in the long run, shouldn't it be possible to do this by calling"
1.1.1,build_vocab with both the src and tgt data?
1.1.1,fast-forward if loaded from state
1.1.1,NOTE: `rnn.pack_padded_sequence` requires that a
1.1.1,"minibatch be sorted by decreasing order, which"
1.1.1,requires reversing relative to typical sort keys
1.1.1,Temporarily load one shard to retrieve sort_key for data_type
1.1.1,"NOTE: This is causing some issues for consumer/producer,"
1.1.1,as we may still have some of those examples in some queue
1.1.1,cur_dataset.examples = None
1.1.1,gc.collect()
1.1.1,del cur_dataset
1.1.1,gc.collect()
1.1.1,Cycle through the shards indefinitely.
1.1.1,"When the dataset is not repeated, we might need to ensure that"
1.1.1,the number of returned batches is the multiple of a given value.
1.1.1,This is important for multi GPU training to ensure that all
1.1.1,workers have the same number of batches to process.
1.1.1,Maintains the longest src and tgt length in the current batch
1.1.1,Reset current longest length at a new batch (count=1)
1.1.1,Src: [<bos> w1 ... wN <eos>]
1.1.1,Tgt: [w1 ... wM <eos>]
1.1.1,-*- coding: utf-8 -*-
1.1.1,imports of datatype-specific dependencies
1.1.1,torchaudio loading options recently changed. It's probably
1.1.1,straightforward to rewrite the audio handling to make use of
1.1.1,"up-to-date torchaudio, but in the meantime there is a legacy"
1.1.1,method which uses the old defaults
1.1.1,STFT
1.1.1,-*- coding: utf-8 -*-
1.1.1,domain specific dependencies
1.1.1,coding: utf-8
1.1.1,several data readers need optional dependencies. There's no
1.1.1,appropriate builtin exception
1.1.1,-*- coding: utf-8 -*-
1.1.1,mix this with partial
1.1.1,batch (list(list(list))): batch_size x len(self.fields) x seq_len
1.1.1,lengths: batch_size
1.1.1,data: seq_len x batch_size x len(self.fields)
1.1.1,flake8: noqa
1.1.1,For command-line option parsing
1.1.1,"Check pass, set the args."
1.1.1,"This SRU version implements its own cuda-level optimization,"
1.1.1,so it requires that:
1.1.1,1. `cupy` and `pynvrtc` python package installed.
1.1.1,2. pytorch is built with cuda support.
1.1.1,3. library path set: export LD_LIBRARY_PATH=<cuda lib path>.
1.1.1,Check 1.
1.1.1,Check 2.
1.1.1,Check 3.
1.1.1,This sets up device to use.
1.1.1,-> directions x batch x dim
1.1.1,For DEBUG
1.1.1,"size = (length, batch, x.size(-1)) \"
1.1.1,"if x.dim() == 3 else (batch, x.size(-1))"
1.1.1,grad_x = x.new(*x.size()) if k_ == 3 else x.new(*size).zero_()
1.1.1,Normal use
1.1.1,"An entry check here, will catch on train side and translate side"
1.1.1,if requirements are not satisfied.
1.1.1,RNNDecoderState wraps hidden as a tuple.
1.1.1,fh -> (layers*directions) x batch x dim
1.1.1,NOTE: We need to trim the vocab to remove any unk tokens that
1.1.1,were not originally here.
1.1.1,!/usr/bin/env python
1.1.1,!/usr/bin/env python
1.1.1,!/usr/bin/env python
1.1.1,-*- coding: utf-8 -*-
1.1.1,!/usr/bin/env python
1.1.1,-*- coding: utf-8 -*-
1.1.1,create one counter per shard
1.1.1,"every corpus has shards, no new one"
1.1.1,!/usr/bin/env python
1.1.1,!/usr/bin/env python
1.1.1,Load checkpoint if we resume from a previous training.
1.1.1,check for code where vocab is saved instead of fields
1.1.1,(in the future this will be done in a smarter way)
1.1.1,Create a thread to listen for errors in the child processes.
1.1.1,Train with multiprocessing.
1.1.1,generator_to_serve = iter(generator_to_serve)
1.1.1,hack to dodge unpicklable `dict_keys`
1.1.1,"propagate exception to parent process, keeping original traceback"
1.1.1,magic indices
1.1.1,result caching
1.1.1,add one to account for BOS. Don't account for EOS because hitting
1.1.1,this implies it hasn't been found.
1.1.1,we don't block nothing if the user doesn't want it
1.1.1,we can't block nothing beam's too short
1.1.1,we check paths one by one
1.1.1,we don't forbid nothing if the user doesn't want it
1.1.1,we can't forbid nothing if beam's too short
1.1.1,Reordering forbidden_tokens following beam selection
1.1.1,We rebuild a dict to ensure we get the value and not the pointer
1.1.1,Grabing the newly selected tokens and associated ngram
1.1.1,skip the blocking if any token in current_ngram is excluded
1.1.1,"For temp=0.0, take the argmax to avoid divide-by-zero errors."
1.1.1,keep_topk=1 is also equivalent to argmax.
1.1.1,Set all logits that are not in the top-k to -10000.
1.1.1,This puts the probabilities close to 0.
1.1.1,"shape: (sum(~ self.is_finished), 1)"
1.1.1,!/usr/bin/env python
1.1.1,Maintains the longest src and tgt length in the current batch
1.1.1,Reset current longest length at a new batch (count=1)
1.1.1,max_tgt_in_batch = 0
1.1.1,Src: [<bos> w1 ... wN <eos>]
1.1.1,Tgt: [w1 ... wM <eos>]
1.1.1,for debugging
1.1.1,corpus_id field is useless here
1.1.1,Statistics
1.1.1,(0) add BOS and padding to tgt prediction
1.1.1,(1) Encoder forward.
1.1.1,(2) Repeat src objects `n_best` times.
1.1.1,"We use batch_size x n_best, get ``(src_len, batch * n_best, nfeat)``"
1.1.1,"(3) Init decoder with n_best src,"
1.1.1,"reshape tgt to ``(len, batch * n_best, nfeat)``"
1.1.1,masked_select
1.1.1,get aligned src id for each prediction's valid tgt tokens
1.1.1,TODO: support these blacklisted features
1.1.1,Turn any copied words into UNKs.
1.1.1,"Decoder forward, takes [tgt_len, batch, nfeats] as input"
1.1.1,"and [src_len, batch, hidden] as memory_bank"
1.1.1,"in case of inference tgt_len = 1, batch = beam times batch_size"
1.1.1,"in case of Gold Scoring tgt_len = actual length, batch = 1 batch"
1.1.1,Generator forward.
1.1.1,"returns [(batch_size x beam_size) , vocab ] when 1 step"
1.1.1,"or [ tgt_len, batch_size, vocab ] when full sentence"
1.1.1,"here we have scores [tgt_lenxbatch, vocab] or [beamxbatch, vocab]"
1.1.1,"returns [(batch_size x beam_size) , vocab ] when 1 step"
1.1.1,"or [ tgt_len, batch_size, vocab ] when full sentence"
1.1.1,(0) Prep the components of the search.
1.1.1,(1) Run the encoder on the src.
1.1.1,(2) prep decode_strategy. Possibly repeat src objects.
1.1.1,(3) Begin decoding step by step:
1.1.1,Reorder states.
1.1.1,beam parameters
1.1.1,result caching
1.1.1,beam state
1.1.1,BoolTensor was introduced in pytorch 1.2
1.1.1,"""global state"" of the old beam"
1.1.1,buffers for the topk scores and 'backpointer'
1.1.1,for testing
1.1.1,using integer division to get an integer _B without casting
1.1.1,force the output to be longer than self.min_length
1.1.1,Multiply probs by the beam probability.
1.1.1,"if the sequence ends now, then the penalty is the current"
1.1.1,"length + 1, to include the EOS token"
1.1.1,Avoid any direction that would repeat unwanted ngrams
1.1.1,Flatten probs into a list of possibilities.
1.1.1,Recover log probs.
1.1.1,Length penalty is just a scalar. It doesn't matter if it's applied
1.1.1,before or after the topk.
1.1.1,Resolve beam origin and map to batch index flat representation.
1.1.1,Append last prediction.
1.1.1,update global state (step == 1)
1.1.1,update global state (step > 1)
1.1.1,"shape: (batch_size x beam_size, 1)"
1.1.1,Penalize beams that finished.
1.1.1,"on real data (newstest2017) with the pretrained transformer,"
1.1.1,it's faster to not move this back to the original device
1.1.1,Store finished hypotheses for this batch.
1.1.1,End condition is the top beam finished and we can return
1.1.1,n_best hypotheses.
1.1.1,"If all sentences are translated, no need to go further."
1.1.1,Remove finished batches for the next step.
1.1.1,Term will be subtracted from probability
1.1.1,Probability will be divided by this
1.1.1,these warnings indicate that either the alpha/beta
1.1.1,"forces a penalty to be a no-op, or a penalty is a no-op but"
1.1.1,the alpha/beta would suggest otherwise.
1.1.1,using some length penalty
1.1.1,using some coverage penalty
1.1.1,!/usr/bin/env python
1.1.1,semaphore doesn't have a timeout arg in Python 2.7
1.1.1,perform a first request to initialize everything
1.1.1,backwards compatibility for confs
1.1.1,load can be called multiple times: modify copy
1.1.1,every segment becomes a dict for flexibility purposes
1.1.1,NOTE: translator returns lists of `n_best` list
1.1.1,build back results with empty texts
1.1.1,output contain alignment
1.1.1,Below are all the different penalty terms implemented so far.
1.1.1,Subtract coverage penalty from topk log probs.
1.1.1,Divide topk log probs by length penalty.
1.1.1,Sorting
1.1.1,Chinese segmentation
1.1.1,Chinese simplify -> Chinese traditional standard
1.1.1,Chinese simplify -> Chinese traditional (HongKong)
1.1.1,Chinese simplify -> Chinese traditional (Taiwan)
1.1.1,Chinese traditional -> Chinese simplify (v1)
1.1.1,Chinese traditional -> Chinese simplify (v2)
1.1.0,!/usr/bin/env python
1.1.0,!/usr/bin/env python
1.1.0,!/usr/bin/env python
1.1.0,!/usr/bin/env python
1.1.0,!/usr/bin/env python
1.1.0,!/usr/bin/env python3
1.1.0,-*- coding: utf-8 -*-
1.1.0,
1.1.0,"OpenNMT-py documentation build configuration file, created by"
1.1.0,sphinx-quickstart on Sun Dec 17 12:07:14 2017.
1.1.0,
1.1.0,This file is execfile()d with the current directory set to its
1.1.0,containing dir.
1.1.0,
1.1.0,Note that not all possible configuration values are present in this
1.1.0,autogenerated file.
1.1.0,
1.1.0,All configuration values have a default; values that are commented out
1.1.0,serve to show the default.
1.1.0,"If extensions (or modules to document with autodoc) are in another directory,"
1.1.0,add these directories to sys.path here. If the directory is relative to the
1.1.0,"documentation root, use os.path.abspath to make it absolute, like shown here."
1.1.0,
1.1.0,import os
1.1.0,import sys
1.1.0,"sys.path.insert(0, os.path.abspath('.'))"
1.1.0,-- General configuration ------------------------------------------------
1.1.0,"If your documentation needs a minimal Sphinx version, state it here."
1.1.0,
1.1.0,needs_sphinx = '1.0'
1.1.0,"Add any Sphinx extension module names here, as strings. They can be"
1.1.0,extensions coming with Sphinx (named 'sphinx.ext.*') or your custom
1.1.0,ones.
1.1.0,Show base classes
1.1.0,"Use ""variables"" section for Attributes instead of weird block things"
1.1.0,mimicking the function style.
1.1.0,"Add any paths that contain templates here, relative to this directory."
1.1.0,The suffix(es) of source filenames.
1.1.0,You can specify multiple suffix as a list of string:
1.1.0,
1.1.0,"source_suffix = ['.rst', '.md']"
1.1.0,The master toctree document.
1.1.0,General information about the project.
1.1.0,"The version info for the project you're documenting, acts as replacement for"
1.1.0,"|version| and |release|, also used in various other places throughout the"
1.1.0,built documents.
1.1.0,
1.1.0,The short X.Y version.
1.1.0,"The full version, including alpha/beta/rc tags."
1.1.0,The language for content autogenerated by Sphinx. Refer to documentation
1.1.0,for a list of supported languages.
1.1.0,
1.1.0,This is also used if you do content translation via gettext catalogs.
1.1.0,"Usually you set ""language"" from the command line for these cases."
1.1.0,"List of patterns, relative to source directory, that match files and"
1.1.0,directories to ignore when looking for source files.
1.1.0,This patterns also effect to html_static_path and html_extra_path
1.1.0,The name of the Pygments (syntax highlighting) style to use.
1.1.0,"If true, `todo` and `todoList` produce output, else they produce nothing."
1.1.0,-- Options for HTML output ----------------------------------------------
1.1.0,The theme to use for HTML and HTML Help pages.  See the documentation for
1.1.0,a list of builtin themes.
1.1.0,
1.1.0,html_theme = 'sphinx_materialdesign_theme'
1.1.0,html_theme_path = [sphinx_materialdesign_theme.get_path()]
1.1.0,Theme options are theme-specific and customize the look and feel of a theme
1.1.0,"further.  For a list of options available for each theme, see the"
1.1.0,documentation.
1.1.0,
1.1.0,html_theme_options = {}
1.1.0,"Add any paths that contain custom static files (such as style sheets) here,"
1.1.0,"relative to this directory. They are copied after the builtin static files,"
1.1.0,"so a file named ""default.css"" will overwrite the builtin ""default.css""."
1.1.0,"Custom sidebar templates, must be a dictionary that maps document names"
1.1.0,to template names.
1.1.0,
1.1.0,This is required for the alabaster theme
1.1.0,refs: http://alabaster.readthedocs.io/en/latest/installation.html#sidebars
1.1.0,-- Options for HTMLHelp output ------------------------------------------
1.1.0,Output file base name for HTML help builder.
1.1.0,-- Options for LaTeX output ---------------------------------------------
1.1.0,The paper size ('letterpaper' or 'a4paper').
1.1.0,
1.1.0,"'papersize': 'letterpaper',"
1.1.0,"The font size ('10pt', '11pt' or '12pt')."
1.1.0,
1.1.0,"'pointsize': '10pt',"
1.1.0,Additional stuff for the LaTeX preamble.
1.1.0,
1.1.0,"'preamble': '',"
1.1.0,Latex figure (float) alignment
1.1.0,
1.1.0,"'figure_align': 'htbp',"
1.1.0,Grouping the document tree into LaTeX files. List of tuples
1.1.0,"(source start file, target name, title,"
1.1.0,"author, documentclass [howto, manual, or own class])."
1.1.0,-- Options for manual page output ---------------------------------------
1.1.0,One entry per manual page. List of tuples
1.1.0,"(source start file, name, description, authors, manual section)."
1.1.0,-- Options for Texinfo output -------------------------------------------
1.1.0,Grouping the document tree into Texinfo files. List of tuples
1.1.0,"(source start file, target name, title, author,"
1.1.0,"dir menu entry, description, category)"
1.1.0,degenerate case
1.1.0,cache the features
1.1.0,mp queues don't work well between procs unless they're from a manager
1.1.0,each device has its own saver so that reconstructing is easier
1.1.0,!/usr/bin/env python
1.1.0,-*- coding: utf-8 -*-
1.1.0,is this reachable?
1.1.0,Read in embeddings
1.1.0,Write to file
1.1.0,!/usr/bin/env python
1.1.0,-*- coding: utf-8 -*-
1.1.0,"Add in default model arguments, possibly added since training."
1.1.0,-*- encoding: utf-8 -*-
1.1.0,!/usr/bin/env python
1.1.0,-*- coding: utf-8 -*-
1.1.0,Author: Rico Sennrich
1.1.0,flake8: noqa
1.1.0,This file is retrieved from https://github.com/rsennrich/subword-nmt
1.1.0,hack for python2/3 compatibility
1.1.0,check version information
1.1.0,some hacking to deal with duplicates (only consider first instance)
1.1.0,don't print end-of-word symbols
1.1.0,sys.stderr.write('cannot split {0} further.\n'.format(segment))
1.1.0,sys.stderr.write('OOV: {0}\n'.format(segment))
1.1.0,sys.stderr.write('OOV: {0}\n'.format(segment))
1.1.0,python 2/3 compatibility
1.1.0,read/write files as UTF-8
1.1.0,!/usr/bin/env python
1.1.0,!/usr/bin/env python
1.1.0,-*- coding: utf-8 -*-
1.1.0,Author: Rico Sennrich
1.1.0,flake8: noqa
1.1.0,This file is retrieved from https://github.com/rsennrich/subword-nmt
1.1.0,hack for python2/3 compatibility
1.1.0,"find all instances of pair, and update frequency/indices around it"
1.1.0,find first symbol
1.1.0,"if first symbol is followed by second symbol, we've found an occurrence of pair (old_word[i:i+2])"
1.1.0,"assuming a symbol sequence ""A B C"", if ""B C"" is merged, reduce the frequency of ""A B"""
1.1.0,"assuming a symbol sequence ""A B C B"", if ""B C"" is merged, reduce the frequency of ""C B""."
1.1.0,"however, skip this if the sequence is A B C B C, because the frequency of ""C B"" will be reduced by the previous code block"
1.1.0,find new pair
1.1.0,"assuming a symbol sequence ""A BC D"", if ""B C"" is merged, increase the frequency of ""A BC"""
1.1.0,"assuming a symbol sequence ""A BC B"", if ""B C"" is merged, increase the frequency of ""BC B"""
1.1.0,"however, if the sequence is A BC BC, skip this step because the count of ""BC BC"" will be incremented by the previous code block"
1.1.0,data structure of pair frequencies
1.1.0,index from pairs to words
1.1.0,version 0.2 changes the handling of the end-of-word token ('</w>');
1.1.0,version numbering allows bckward compatibility
1.1.0,"threshold is inspired by Zipfian assumption, but should only affect speed"
1.1.0,we probably missed the best pair because of pruning; go back to full statistics
1.1.0,"threshold is inspired by Zipfian assumption, but should only affect speed"
1.1.0,python 2/3 compatibility
1.1.0,read/write files as UTF-8
1.1.0,!/usr/bin/env python
1.1.0,for back compat when attention_dropout was not defined
1.1.0,Build embeddings.
1.1.0,Build encoder.
1.1.0,Build decoder.
1.1.0,Share the embedding matrix - preprocess with share_vocab required.
1.1.0,src/tgt vocab should be the same if `-share_vocab` is specified.
1.1.0,Build NMTModel(= encoder + decoder).
1.1.0,Build Generator.
1.1.0,Load the model states from checkpoint or initialize them.
1.1.0,This preserves backward-compat for models using customed layernorm
1.1.0,end of patch for backward compatibility
1.1.0,!/usr/bin/env python
1.1.0,NOTE: It's important that ``opt`` has been validated and updated
1.1.0,at this point.
1.1.0,Load checkpoint if we resume from a previous training.
1.1.0,check for code where vocab is saved instead of fields
1.1.0,(in the future this will be done in a smarter way)
1.1.0,"Report src and tgt vocab sizes, including for features"
1.1.0,Build model.
1.1.0,Build optimizer.
1.1.0,Build model saver
1.1.0,Embedding Options
1.1.0,Encoder-Decoder Options
1.1.0,"group.add('--residual', '-residual',   action=""store_true"","
1.1.0,"help=""Add residual connections between RNN layers."")"
1.1.0,Attention options
1.1.0,Alignement options
1.1.0,Generator and loss options.
1.1.0,Data options
1.1.0,"Dictionary options, for text corpus"
1.1.0,"if you want to pass an existing vocab.pt file, pass it to"
1.1.0,-src_vocab alone as it already contains tgt vocab.
1.1.0,"Truncation options, for text corpus"
1.1.0,Data processing options
1.1.0,Options most relevant to speech
1.1.0,Option most relevant to image input
1.1.0,Options for experimental source noising (BART style)
1.1.0,GPU
1.1.0,Init options
1.1.0,Pretrained word vectors
1.1.0,Fixed word vectors
1.1.0,Optimization options
1.1.0,learning rate
1.1.0,Use Tensorboard for visualization during training
1.1.0,Options most relevant to speech
1.1.0,Option most relevant to image input
1.1.0,Options most relevant to summarization.
1.1.0,Alpha and Beta values for Google Length + Coverage penalty
1.1.0,"Described here: https://arxiv.org/pdf/1609.08144.pdf, Section 7"
1.1.0,Options most relevant to speech.
1.1.0,Option most relevant to image input
1.1.0,Copyright 2016 The Chromium Authors. All rights reserved.
1.1.0,Use of this source code is governed by a BSD-style license that can be
1.1.0,found in the LICENSE file.
1.1.0,"Get the key 'value' in the dict, or just use 'value'"
1.1.0,Basic attributes.
1.1.0,Set model in training mode.
1.1.0,UPDATE DROPOUT
1.1.0,Run patience mechanism
1.1.0,"If the patience has reached the limit, stop training"
1.1.0,swap model params w/ moving average
1.1.0,(and keep the original parameters)
1.1.0,Set model in validating mode.
1.1.0,F-prop through the model.
1.1.0,Compute loss.
1.1.0,Update statistics.
1.1.0,Set model back to training mode.
1.1.0,Truncated BPTT: reminder not compatible with accum > 1
1.1.0,1. Create truncated target.
1.1.0,2. F-prop all but generator.
1.1.0,3. Compute loss.
1.1.0,4. Update the parameters and statistics.
1.1.0,Multi GPU gradient gather
1.1.0,"If truncated, don't backprop fully."
1.1.0,TO CHECK
1.1.0,if dec_state is not None:
1.1.0,dec_state.detach()
1.1.0,"in case of multi step gradient accumulation,"
1.1.0,update only after accum batches
1.1.0,For Flake
1.1.0,we avoid padding while mean pooling
1.1.0,Initialize the bridge layer
1.1.0,"s_len, batch, emb_dim = emb.size()"
1.1.0,Lengths data is wrapped inside a Tensor.
1.1.0,"LSTM has hidden and cell state, other only one"
1.1.0,Total number of states
1.1.0,Build a linear layer for each
1.1.0,The encoder hidden is  (layers*directions) x batch x dim.
1.1.0,"s_len, batch, emb_dim = emb.size()"
1.1.0,Run the forward pass of every layer of the tranformer.
1.1.0,why is the model_opt.__dict__ check necessary?
1.1.0,"(batch_size, 64, imgH, imgW)"
1.1.0,layer 1
1.1.0,"(batch_size, 64, imgH/2, imgW/2)"
1.1.0,"(batch_size, 128, imgH/2, imgW/2)"
1.1.0,layer 2
1.1.0,"(batch_size, 128, imgH/2/2, imgW/2/2)"
1.1.0,"(batch_size, 256, imgH/2/2, imgW/2/2)"
1.1.0,layer 3
1.1.0,batch norm 1
1.1.0,"(batch_size, 256, imgH/2/2, imgW/2/2)"
1.1.0,layer4
1.1.0,"(batch_size, 256, imgH/2/2/2, imgW/2/2)"
1.1.0,"(batch_size, 512, imgH/2/2/2, imgW/2/2)"
1.1.0,layer 5
1.1.0,batch norm 2
1.1.0,"(batch_size, 512, imgH/2/2/2, imgW/2/2/2)"
1.1.0,"(batch_size, 512, imgH/2/2/2, imgW/2/2/2)"
1.1.0,"# (batch_size, 512, H, W)"
1.1.0,Dimensions and padding for constructing the word embedding matrix
1.1.0,Dimensions and padding for feature embedding matrices
1.1.0,(these have no effect if feat_vocab_sizes is empty)
1.1.0,The embedding matrix look-up tables. The first look-up table
1.1.0,"is for words. Subsequent ones are for features, if any exist."
1.1.0,The final output size of word + feature vectors. This can vary
1.1.0,from the word vector size if and only if features are defined.
1.1.0,This is the attribute you should access if you need to know
1.1.0,how big your embeddings are going to be.
1.1.0,The sequence of operations that converts the input sequence
1.1.0,into a sequence of embeddings. At minimum this consists of
1.1.0,looking up the embeddings for each word and feature in the
1.1.0,input. Model parameters may require the sequence to contain
1.1.0,additional operations as well.
1.1.0,features must use word_vec_size
1.1.0,features will use feat_vec_size
1.1.0,This class is mainly used by decoder.py for RNNs but also
1.1.0,by the CNN / transformer decoder when copy attention is used
1.1.0,CNN has its own attention mechanism ConvMultiStepAttention
1.1.0,Transformer has its own MultiHeadedAttention
1.1.0,mlp wants it with bias
1.1.0,Check input sizes
1.1.0,"(batch, t_len, d) x (batch, d, s_len) --> (batch, t_len, s_len)"
1.1.0,"(batch, t_len, s_len, d)"
1.1.0,one step input
1.1.0,"compute attention scores, as in Luong et al."
1.1.0,Softmax or sparsemax to normalize attention weights
1.1.0,each context vector c_t is the weighted average
1.1.0,over all the source hidden states
1.1.0,concatenate
1.1.0,Check output sizes
1.1.0,Check output sizes
1.1.0,clamping necessary because of numerical errors: loss should be lower
1.1.0,"bounded by zero, but negative values near zero are possible without"
1.1.0,the clamp
1.1.0,from onmt.utils.misc import aeq
1.1.0,CHECKS
1.1.0,"batch, k_len, d = key.size()"
1.1.0,"batch_, k_len_, d_ = value.size()"
1.1.0,"aeq(batch, batch_)"
1.1.0,"aeq(k_len, k_len_)"
1.1.0,"aeq(d, d_)"
1.1.0,"batch_, q_len, d_ = query.size()"
1.1.0,"aeq(batch, batch_)"
1.1.0,"aeq(d, d_)"
1.1.0,"aeq(self.model_dim % 8, 0)"
1.1.0,if mask is not None:
1.1.0,"batch_, q_len_, k_len_ = mask.size()"
1.1.0,"aeq(batch_, batch)"
1.1.0,"aeq(k_len_, k_len)"
1.1.0,aeq(q_len_ == q_len)
1.1.0,END CHECKS
1.1.0,"1) Project key, value, and query."
1.1.0,1 or key_len x key_len
1.1.0,1 or key_len x key_len x dim_per_head
1.1.0,1 or key_len x key_len x dim_per_head
1.1.0,2) Calculate and scale scores.
1.1.0,batch x num_heads x query_len x key_len
1.1.0,3) Apply attention dropout and compute context vectors.
1.1.0,CHECK
1.1.0,"batch_, q_len_, d_ = output.size()"
1.1.0,"aeq(q_len, q_len_)"
1.1.0,"aeq(batch, batch_)"
1.1.0,"aeq(d, d_)"
1.1.0,Return multi-head attn
1.1.0,At the moment this class is only used by embeddings.Embeddings look-up tables
1.1.0,-*- coding: utf-8 -*-
1.1.0,checks
1.1.0,"batch, channel, height, width = base_target_emb.size()"
1.1.0,"batch_, channel_, height_, width_ = input_from_dec.size()"
1.1.0,"enc_batch, enc_channel, enc_height = encoder_out_top.size()"
1.1.0,"enc_batch_, enc_channel_, enc_height_ = encoder_out_combine.size()"
1.1.0,out_features * in_features
1.1.0,norm is out_features * 1
1.1.0,batch_size * out_features
1.1.0,out_features
1.1.0,out_features
1.1.0,batch_size * out_features
1.1.0,"out_channels, in_channels // groups, * kernel_size"
1.1.0,out_features
1.1.0,This is used nowhere in the code at the moment (Vincent Nguyen 05/18/2018)
1.1.0,"in_channels, out_channels, *kernel_size"
1.1.0,"in_channels, out_channels, *kernel_size"
1.1.0,"self.out_channels, 1"
1.1.0,out_features
1.1.0,out_features
1.1.0,store roots on diagonal
1.1.0,noise_skip = batch.noise_skip
1.1.0,aeq(len(batch.noise_skip) == source.size(1))
1.1.0,source is [src_len x bs x feats]
1.1.0,source might increase length so we need to resize the whole
1.1.0,tensor
1.1.0,remove useless pad
1.1.0,"def s(self, tokens):"
1.1.0,prob = self.prob
1.1.0,r = torch.rand([len(tokens)])
1.1.0,mask = False
1.1.0,masked = []
1.1.0,"for i, tok in enumerate(tokens):"
1.1.0,if tok.startswith(subword_prefix):
1.1.0,if r[i].item() <= prob:
1.1.0,masked.append(mask_tok)
1.1.0,mask = True
1.1.0,else:
1.1.0,masked.append(tok)
1.1.0,mask = False
1.1.0,else:
1.1.0,if mask:
1.1.0,pass
1.1.0,else:
1.1.0,masked.append(tok)
1.1.0,return masked
1.1.0,"aeq(source.size(0), length)"
1.1.0,Pretend it ends with a full stop so last span is a sentence
1.1.0,"Tokens that are full stops, where the previous token is not"
1.1.0,"aeq(source.size(0), length)"
1.1.0,-1: keep everything (i.e. 1 mask per token)
1.1.0,0: replace everything (i.e. no mask)
1.1.0,1: 1 mask per span
1.1.0,fairseq/data/denoising_dataset.py
1.1.0,"print(""src size: "", source.size())"
1.1.0,"print(""ws size: "", self.word_start_mask.size())"
1.1.0,"print(""max: "", source.max())"
1.1.0,assert source.max() < self.word_start_mask.size(0)
1.1.0,assert source.min() >= 0
1.1.0,assert source.size() == is_word_start.size()
1.1.0,"aeq(source.eq(self.pad_idx).long().sum(), 0)"
1.1.0,we manually add this hypothesis since it's required for the rest
1.1.0,of the function and kindof make sense
1.1.0,Make sure we have enough to mask
1.1.0,Trim to masking budget
1.1.0,Handle 0-length mask (inserts) separately
1.1.0,assert (lengths > 0).all()
1.1.0,assert is_word_start[-1] == 0
1.1.0,TODO why?
1.1.0,assert source_length - 1 not in indices
1.1.0,"acts as a long length, so spans don't go over the end of doc"
1.1.0,"keep index, but replace it with [MASK]"
1.1.0,random ratio disabled
1.1.0,source[indices[mask_random]] = torch.randint(
1.1.0,"1, len(self.vocab), size=(mask_random.sum(),))"
1.1.0,if self.mask_span_distribution is not None:
1.1.0,assert len(lengths.size()) == 1
1.1.0,assert lengths.size() == indices.size()
1.1.0,assert lengths.size() == indices.size()
1.1.0,mask_random = mask_random[uncompleted]
1.1.0,delete token
1.1.0,"keep index, but replace it with [MASK]"
1.1.0,random ratio disabled
1.1.0,source[indices[mask_random]] = torch.randint(
1.1.0,"1, len(self.vocab), size=(mask_random.sum(),))"
1.1.0,else:
1.1.0,# A bit faster when all lengths are 1
1.1.0,while indices.size(0) > 0:
1.1.0,uncompleted = is_word_start[indices + 1] == 0
1.1.0,indices = indices[uncompleted] + 1
1.1.0,mask_random = mask_random[uncompleted]
1.1.0,if self.replace_length != -1:
1.1.0,# delete token
1.1.0,to_keep[indices] = 0
1.1.0,else:
1.1.0,"# keep index, but replace it with [MASK]"
1.1.0,source[indices] = self.mask_idx
1.1.0,source[indices[mask_random]] = torch.randint(
1.1.0,"1, len(self.vocab), size=(mask_random.sum(),))"
1.1.0,assert source_length - 1 not in indices
1.1.0,"aeq(source.eq(self.pad_idx).long().sum(), 0)"
1.1.0,random ratio disabled
1.1.0,num_random = int(math.ceil(n * self.random_ratio))
1.1.0,result[noise_indices[:num_random]] = torch.randint(
1.1.0,"low=1, high=len(self.vocab), size=(num_random,))"
1.1.0,assert (result >= 0).all()
1.1.0,CHECKS
1.1.0,Original probabilities.
1.1.0,Probability of copying p(z=1) batch.
1.1.0,Probability of not copying: p_{word}(w) * (1 - p(z))
1.1.0,probabilities assigned by the model to the gold targets
1.1.0,probability of tokens copied from source
1.1.0,Set scores for unk to 0 and add eps
1.1.0,find the indices in which you do not use the copy mechanism
1.1.0,Drop padding.
1.1.0,this block does not depend on the loss value computed above
1.1.0,and is used only for stats
1.1.0,this block does not depend on the loss value computed above
1.1.0,and is used only for stats
1.1.0,Correct target copy token instead of <unk>
1.1.0,tgt[i] = align[i] + len(tgt_vocab)
1.1.0,for i such that tgt[i] == 0 and align[i] != 0
1.1.0,Compute sum of perplexities for stats
1.1.0,this part looks like it belongs in CopyGeneratorLoss
1.1.0,Compute Loss as NLL divided by seq length
1.1.0,Compute Total Loss per sequence in batch
1.1.0,Divide by length of each sequence and sum
1.1.0,initialize fields at the top of each unit test to prevent
1.1.0,any undesired stateful effects
1.1.0,"this test touches the file system, so it could be considered an"
1.1.0,integration test
1.1.0,write utf-8 bytes
1.1.0,batch 0 will always predict EOS. The other batches will predict
1.1.0,non-eos scores.
1.1.0,"""best"" prediction is eos - that should be blocked"
1.1.0,include at least one prediction OTHER than EOS
1.1.0,that is greater than -1e20
1.1.0,now batch 0 has ended and no others have
1.1.0,initial step
1.1.0,batch 0 dies on step 0
1.1.0,include at least one prediction OTHER than EOS
1.1.0,that is greater than -1e20
1.1.0,step 2
1.1.0,(old) batch 8 dies on step 1
1.1.0,step 3
1.1.0,everything dies
1.1.0,initial step
1.1.0,batch 0 dies on step 0
1.1.0,include at least one prediction OTHER than EOS
1.1.0,that is greater than -1e20
1.1.0,step 2
1.1.0,(old) batch 8 dies on step 1
1.1.0,step 3
1.1.0,everything dies
1.1.0,illegal_weights_mask = torch.ByteTensor([
1.1.0,"[0, 0, 0, 0, 0, 0, 0],"
1.1.0,"[0, 0, 0, 1, 1, 1, 1],"
1.1.0,"[0, 0, 0, 0, 0, 1, 1],"
1.1.0,"[0, 0, 1, 1, 1, 1, 1]])"
1.1.0,TODO: fix for pytorch 0.3
1.1.0,illegal_weights = alignments.masked_select(illegal_weights_mask)
1.1.0,"self.assertEqual(0.0, illegal_weights.data.sum())"
1.1.0,this could be considered an integration test because it touches
1.1.0,the filesystem for the config file (and the models)
1.1.0,-*- coding: utf-8 -*-
1.1.0,tests pad and numericalize integration
1.1.0,tests pad and numericalize integration
1.1.0,"this test touches the file system, so it could be considered an"
1.1.0,integration test
1.1.0,file to hold full paths to audio data
1.1.0,file to hold audio paths relative to _AUDIO_DATA_DIR (i.e. file names)
1.1.0,it's ok if non-audio files co-exist with audio files in the data dir
1.1.0,"dividing gets the noise in [-1, 1]"
1.1.0,"this test touches the file system, so it could be considered an"
1.1.0,integration test
1.1.0,file to hold full paths to image data
1.1.0,file to hold image paths relative to _IMG_DATA_DIR (i.e. file names)
1.1.0,it's ok if non-image files co-exist with image files in the data dir
1.1.0,all beams repeat (beam >= 1 repeat dummy scores)
1.1.0,predict repeat_idx over and over again
1.1.0,"before repeat, scores are either 0 or -inf"
1.1.0,"on repeat, `repeat_idx` score is BLOCKED_SCORE"
1.1.0,"(but it's still the best score, thus we have"
1.1.0,"[BLOCKED_SCORE, -inf, -inf, -inf, -inf]"
1.1.0,repetitions keeps maximizing score
1.1.0,"index 0 has been blocked, so repeating=>+0.0 score"
1.1.0,other indexes are -inf so repeating=>BLOCKED_SCORE
1.1.0,which is higher
1.1.0,beam 0 and beam >=2 will repeat (beam >= 2 repeat dummy scores)
1.1.0,non-interesting beams are going to get dummy values
1.1.0,"on initial round, only predicted scores for beam 0"
1.1.0,matter. Make two predictions. Top one will be repeated
1.1.0,"in beam zero, second one will live on in beam 1."
1.1.0,predict the same thing in beam 0
1.1.0,continue pushing around what beam 1 predicts
1.1.0,"now beam 0 dies (along with the others), beam 1 -> beam 0"
1.1.0,"now beam 0 dies (along with the others), beam 1 -> beam 0"
1.1.0,beam 0 and beam >= 2 will repeat (beam 2 repeats excluded idx)
1.1.0,non-interesting beams are going to get dummy values
1.1.0,predict the same thing in beam 0
1.1.0,continue pushing around what beam 1 predicts
1.1.0,predict the allowed-repeat again in beam 2
1.1.0,"now beam 0 dies, beam 1 -> beam 0, beam 2 -> beam 1"
1.1.0,and the rest die
1.1.0,"since all preds after i=0 are 0, we can check"
1.1.0,that the beam is the correct idx by checking that
1.1.0,the curr score is the initial score
1.1.0,beam 0 will always predict EOS. The other beams will predict
1.1.0,non-eos scores.
1.1.0,non-interesting beams are going to get dummy values
1.1.0,"""best"" prediction is eos - that should be blocked"
1.1.0,include at least beam_sz predictions OTHER than EOS
1.1.0,that are greater than -1e20
1.1.0,predict eos in beam 0
1.1.0,provide beam_sz other good predictions
1.1.0,now the top beam has ended and no others have
1.1.0,"not of interest, but want to make sure it keeps running"
1.1.0,since only beam 0 terminates and n_best = 2
1.1.0,"this is also a test that when block_ngram_repeat=0,"
1.1.0,repeating is acceptable
1.1.0,non-interesting beams are going to get dummy values
1.1.0,"""best"" prediction is eos - that should be blocked"
1.1.0,include at least beam_sz predictions OTHER than EOS
1.1.0,that are greater than -1e20
1.1.0,predict eos in beam 1
1.1.0,provide beam_sz other good predictions in other beams
1.1.0,provide beam_sz other good predictions in other beams
1.1.0,beam 1 dies on min_length
1.1.0,beam 0 dies on the step after beam 1 dies
1.1.0,"inp_lens is tiled in initialize, reassign to make attn match"
1.1.0,non-interesting beams are going to get dummy values
1.1.0,"""best"" prediction is eos - that should be blocked"
1.1.0,include at least beam_sz predictions OTHER than EOS
1.1.0,that are greater than -1e20
1.1.0,predict eos in beam 1
1.1.0,provide beam_sz other good predictions in other beams
1.1.0,provide beam_sz other good predictions in other beams
1.1.0,no top beams are finished yet
1.1.0,beam 1 dies on min_length
1.1.0,no top beams are finished yet
1.1.0,beam 0 dies on the step after beam 1 dies
1.1.0,top beam is finished now so there are attentions
1.1.0,two beams are finished in each batch
1.1.0,second dim is cut down to the non-padded src length
1.1.0,first dim is equal to the time of death
1.1.0,(beam 0 died at current step - adjust for SOS)
1.1.0,(beam 1 died at last step - adjust for SOS)
1.1.0,behavior gets weird when beam is already done so just stop
1.1.0,this is just test_beam.TestBeamAgainstReferenceCase repeated
1.1.0,in each batch.
1.1.0,"init_preds: [4, 3, 5, 6, 7] - no EOS's"
1.1.0,no EOS's yet
1.1.0,"[5, 3, 2, 6, 0], so beam 2 predicts EOS!"
1.1.0,assumes beam 2 finished on last step
1.1.0,ended beam 2 shouldn't continue
1.1.0,"[2, 5, 3, 6, 0] repeat self.BATCH_SZ, so beam 0 predicts EOS!"
1.1.0,"[-2.4879, -3.8910, -4.1010, -4.2010, -4.4010] repeat self.BATCH_SZ"
1.1.0,another beam is finished in all batches
1.1.0,new beam 0 finished
1.1.0,new beam 0 is old beam 3
1.1.0,assumes beam 0 finished on last step
1.1.0,"[5, 2, 6, 1, 0] repeat self.BATCH_SZ, so beam 1 predicts EOS!"
1.1.0,new beam 1 finished
1.1.0,new beam 1 is old beam 4
1.1.0,this could be considered an integration test because it tests
1.1.0,interactions between the GNMT scorer and the beam
1.1.0,"-data option is required, but not used in this test, so dummy."
1.1.0,len x batch x nfeat
1.1.0,batch x c x h x w
1.1.0,batch x 1 x nfft x t
1.1.0,Initialize vectors to compare size with
1.1.0,Ensure correct sizes and types
1.1.0,Make sure that output has the correct size and type
1.1.0,Make sure that output has the correct size and type
1.1.0,Make sure that output has the correct size and type
1.1.0,"[('encoder_type', 'transformer'),"
1.1.0,"('word_vec_size', 16), ('rnn_size', 16)],"
1.1.0,""""""" Only do SRU test if requirment is safisfied. """""""
1.1.0,SRU doesn't support input_feed.
1.1.0,"when reasonable, set audio_enc_pooling to 2"
1.1.0,Need lengths >= audio_enc_pooling**n_layers.
1.1.0,"That condition is unrealistic for large n_layers,"
1.1.0,so leave audio_enc_pooling at 1.
1.1.0,first check there's nothing unexpectedly not trainable
1.1.0,ok: word embeddings shouldn't be trainable
1.1.0,if word vecs are fixed
1.1.0,ok: positional encodings shouldn't be trainable
1.1.0,then check nothing unexpectedly trainable
1.1.0,!/usr/bin/env python
1.1.0,-*- coding: utf-8 -*-
1.1.0,Remove the generated *pt files.
1.1.0,Test image preprocessing
1.1.0,Test audio preprocessing
1.1.0,Decoder state
1.1.0,Build the RNN.
1.1.0,Set up the context gate.
1.1.0,Set up the standard attention.
1.1.0,The encoder hidden is  (layers*directions) x batch x dim.
1.1.0,We need to convert it to layers x batch x (directions*dim).
1.1.0,Init the input feed.
1.1.0,Update the state with the result.
1.1.0,Concatenates sequence of tensors along a new dimension.
1.1.0,NOTE: v0.3 to 0.4: dec_outs / attns[*] may not be list
1.1.0,(in particular in case of SRU) it was not raising error in 0.3
1.1.0,since stack(Variable) was allowed.
1.1.0,"In 0.4, SRU returns a tensor that shouldn't be stacke"
1.1.0,Check
1.1.0,Calculate the attention.
1.1.0,Calculate the context gate.
1.1.0,Additional args check.
1.1.0,END Additional args check.
1.1.0,Input feed concatenates hidden state with
1.1.0,input at every time step.
1.1.0,TODO: context gate should be employed
1.1.0,instead of second RNN transform.
1.1.0,Update the coverage attention.
1.1.0,Decoder State
1.1.0,CNNDecoder has its own attention mechanism.
1.1.0,Set up a separate copy attention layer if needed.
1.1.0,The output of CNNEncoder.
1.1.0,The combination of output of CNNEncoder and source embeddings.
1.1.0,Process the result and update the attentions.
1.1.0,Update the state.
1.1.0,TODO change the way attns is returned dict => list or tuple (onnx)
1.1.0,Memory_lengths is a single tensor shared between all models.
1.1.0,This assumption will not hold if Translator is modified
1.1.0,to calculate memory_lengths as something other than the length
1.1.0,of the input.
1.1.0,"return _, (B, Q_len, K_len)"
1.1.0,"layer average attention across heads, get ``(B, Q, K)``"
1.1.0,"Case 1: no full_context, no align heads -> layer avg baseline"
1.1.0,"Case 2: no full_context, 1 align heads -> guided align"
1.1.0,"Case 3: full_context, 1 align heads -> full cte guided align"
1.1.0,T: could be 1 in the case of stepwise decoding or tgt_len
1.1.0,BoolTensor was introduced in pytorch 1.2
1.1.0,Decoder State
1.1.0,"previously, there was a GlobalAttention module here for copy"
1.1.0,"attention. But it was never actually used -- the ""copy"" attention"
1.1.0,just reuses the context attention.
1.1.0,"attns[""align""] = torch.stack(attn_aligns, 0).mean(0)  # All avg"
1.1.0,TODO change the way attns is returned dict => list or tuple (onnx)
1.1.0,"buffer size in bytes, determine equiv. # of elements based on data type"
1.1.0,copy tensors into buffer_t
1.1.0,all-reduce and rescale
1.1.0,copy all-reduced buffer back into tensors
1.1.0,"tensor is bigger than buffer, all-reduce and rescale directly"
1.1.0,"buffer is full, all-reduce and replace buffer with grad"
1.1.0,add tensor to buffer
1.1.0,TODO: Find a better way to check for sparse gradients.
1.1.0,we use here a FusedAdam() copy of an old Apex repo
1.1.0,In this case use the new AMP API from apex
1.1.0,In this case use the old FusedAdam with FP16_optimizer wrapper
1.1.0,Load everything from the checkpoint.
1.1.0,Build everything from scratch.
1.1.0,"Reset optimizer, keep options."
1.1.0,"Reset options, keep optimizer."
1.1.0,State can be partially restored.
1.1.0,Code below is an implementation of https://arxiv.org/pdf/1804.04235.pdf
1.1.0,inspired but modified from https://github.com/DeadAt0m/adafactor-pytorch
1.1.0,backward compatibility
1.1.0,assuming a list/generator of parameter means single group
1.1.0,compute combined scale factor for this group
1.1.0,norm is in fact norm*scale
1.1.0,note: p.grad should not ever be set for correct operation of
1.1.0,mixed precision optimizer that sometimes sends None gradients
1.1.0,State initialization
1.1.0,Exponential moving average of gradient values
1.1.0,Exponential moving average of squared gradient values
1.1.0,-*- coding: utf-8 -*-
1.1.0,if the loss function operates on vectors of raw logits instead of
1.1.0,"probabilities, only the first part of the generator needs to be"
1.1.0,"passed to the NMTLossCompute. At the moment, the only supported"
1.1.0,loss function of this kind is the sparsemax loss.
1.1.0,"attn_align should be in (batch_size, pad_tgt_size, pad_src_size)"
1.1.0,"align_idx should be a Tensor in size([N, 3]), N is total number"
1.1.0,"of align src-tgt pair in current batch, each as"
1.1.0,"['sent_NÂ°_in_batch', 'tgt_id+1', 'src_id'] (check AlignField)"
1.1.0,NOTE: tgt-src ref alignement that in range_ of shard
1.1.0,(coherent with batch.tgt)
1.1.0,"align_head contains value in [0, 1) presenting attn prob,"
1.1.0,0 was resulted by the context attention src_pad_mask
1.1.0,"So, the correspand position in ref_align should also be 0"
1.1.0,"Therefore, clip align_head to > 1e-18 should be bias free."
1.1.0,non_none: the subdict of the state dictionary where the values
1.1.0,are not None.
1.1.0,"Now, the iteration:"
1.1.0,state is a dictionary of sequences of tensor-like but we
1.1.0,want a sequence of dictionaries of tensors.
1.1.0,"First, unzip the dictionary into a sequence of keys and a"
1.1.0,sequence of tensor-like sequences.
1.1.0,"Now, yield a dictionary for each shard. The keys are always"
1.1.0,the same. values is a sequence of length #keys where each
1.1.0,element is a sequence of length #shards. We want to iterate
1.1.0,"over the shards, not over the keys: therefore, the values need"
1.1.0,to be re-zipped by shard and then each shard can be paired
1.1.0,with the keys.
1.1.0,Assumed backprop'd
1.1.0,this check is here because audio allows the encoder and decoder to
1.1.0,"be different sizes, but other model types do not yet"
1.1.0,"Load default opt values, then overwrite with the opts in"
1.1.0,"the checkpoint. That way, if there are new options added,"
1.1.0,the defaults are used.
1.1.0,Don't do anything
1.1.0,Update best score of each criteria
1.1.0,Reset tolerance
1.1.0,Update current status
1.1.0,Decrease tolerance
1.1.0,Log
1.1.0,Log
1.1.0,Get a list of world_size lists with len(stat_list) Statistics objects
1.1.0,SRU doesn't support PackedSequence.
1.1.0,-*- coding: utf-8 -*-
1.1.0,threshold on 1 to avoid div by 0
1.1.0,treat alignment matrix one by one as each have different lengths
1.1.0,No alignment if not exist valid tgt token
1.1.0,get valid alignment (sub-matrix from full paded aligment matrix)
1.1.0,-*- coding: utf-8 -*-
1.1.0,this one is needed for torchtext random call (shuffled iterator)
1.1.0,in multi gpu it ensures datasets are read in the same order
1.1.0,some cudnn methods can be random even after fixing the seed
1.1.0,unless you tell it to be deterministic
1.1.0,These ensure same initialization in multi gpu mode
1.1.0,Shift values to be >= 0
1.1.0,we need to check the model path + any tokenizer path
1.1.0,coding: utf-8
1.1.0,make a small vocab containing just the tokens in the source sequence
1.1.0,Map source tokens to indices in the dynamic dict.
1.1.0,self.src_vocabs is used in collapse_copy_scores and Translator.py
1.1.0,this assumes src_field and tgt_field are both text
1.1.0,fields needs to have only keys that examples have as attrs
1.1.0,avoid infinite recursion when fields isn't defined
1.1.0,-*- coding: utf-8 -*-
1.1.0,backwards compatibility
1.1.0,monkey-patch to make torchtext Vocab's pickleable
1.1.0,"+1 for tgt side to keep coherent after ""bos"" padding,"
1.1.0,"register ['NÂ°_in_batch', 'tgt_id+1', 'src_id']"
1.1.0,"List[Tuple[str, Vocab]] -> List[Tuple[str, Field]]"
1.1.0,"-> dict[str, Field]"
1.1.0,"Dict[str, List[Tuple[str, Field]]]"
1.1.0,doesn't change structure - don't return early.
1.1.0,"Dict[str, List[Tuple[str, Field]]] -> List[Tuple[str, Field]]"
1.1.0,"-> dict[str, Field]"
1.1.0,"if tgt isn't using TextMultiField, then no text field is."
1.1.0,this is basically copy-pasted from torchtext.
1.1.0,counters changes in place
1.1.0,keep the order of tokens specified in the vocab file by
1.1.0,adding them to the counter with decreasing counting values
1.1.0,`tgt_vocab_size` is ignored when sharing vocabularies
1.1.0,return vocab to dump with standard name
1.1.0,empty train_dataset_files so that vocab is only loaded from
1.1.0,"given paths in src_vocab_path, tgt_vocab_path"
1.1.0,Load vocabulary
1.1.0,Drop the none-using from memory but keep the last
1.1.0,"in the long run, shouldn't it be possible to do this by calling"
1.1.0,build_vocab with both the src and tgt data?
1.1.0,fast-forward if loaded from state
1.1.0,NOTE: `rnn.pack_padded_sequence` requires that a
1.1.0,"minibatch be sorted by decreasing order, which"
1.1.0,requires reversing relative to typical sort keys
1.1.0,Temporarily load one shard to retrieve sort_key for data_type
1.1.0,"NOTE: This is causing some issues for consumer/producer,"
1.1.0,as we may still have some of those examples in some queue
1.1.0,cur_dataset.examples = None
1.1.0,gc.collect()
1.1.0,del cur_dataset
1.1.0,gc.collect()
1.1.0,Cycle through the shards indefinitely.
1.1.0,"When the dataset is not repeated, we might need to ensure that"
1.1.0,the number of returned batches is the multiple of a given value.
1.1.0,This is important for multi GPU training to ensure that all
1.1.0,workers have the same number of batches to process.
1.1.0,Maintains the longest src and tgt length in the current batch
1.1.0,Reset current longest length at a new batch (count=1)
1.1.0,Src: [<bos> w1 ... wN <eos>]
1.1.0,Tgt: [w1 ... wM <eos>]
1.1.0,-*- coding: utf-8 -*-
1.1.0,imports of datatype-specific dependencies
1.1.0,torchaudio loading options recently changed. It's probably
1.1.0,straightforward to rewrite the audio handling to make use of
1.1.0,"up-to-date torchaudio, but in the meantime there is a legacy"
1.1.0,method which uses the old defaults
1.1.0,STFT
1.1.0,-*- coding: utf-8 -*-
1.1.0,domain specific dependencies
1.1.0,coding: utf-8
1.1.0,several data readers need optional dependencies. There's no
1.1.0,appropriate builtin exception
1.1.0,-*- coding: utf-8 -*-
1.1.0,mix this with partial
1.1.0,batch (list(list(list))): batch_size x len(self.fields) x seq_len
1.1.0,lengths: batch_size
1.1.0,data: seq_len x batch_size x len(self.fields)
1.1.0,flake8: noqa
1.1.0,For command-line option parsing
1.1.0,"Check pass, set the args."
1.1.0,"This SRU version implements its own cuda-level optimization,"
1.1.0,so it requires that:
1.1.0,1. `cupy` and `pynvrtc` python package installed.
1.1.0,2. pytorch is built with cuda support.
1.1.0,3. library path set: export LD_LIBRARY_PATH=<cuda lib path>.
1.1.0,Check 1.
1.1.0,Check 2.
1.1.0,Check 3.
1.1.0,This sets up device to use.
1.1.0,-> directions x batch x dim
1.1.0,For DEBUG
1.1.0,"size = (length, batch, x.size(-1)) \"
1.1.0,"if x.dim() == 3 else (batch, x.size(-1))"
1.1.0,grad_x = x.new(*x.size()) if k_ == 3 else x.new(*size).zero_()
1.1.0,Normal use
1.1.0,"An entry check here, will catch on train side and translate side"
1.1.0,if requirements are not satisfied.
1.1.0,RNNDecoderState wraps hidden as a tuple.
1.1.0,fh -> (layers*directions) x batch x dim
1.1.0,NOTE: We need to trim the vocab to remove any unk tokens that
1.1.0,were not originally here.
1.1.0,!/usr/bin/env python
1.1.0,!/usr/bin/env python
1.1.0,!/usr/bin/env python
1.1.0,-*- coding: utf-8 -*-
1.1.0,!/usr/bin/env python
1.1.0,-*- coding: utf-8 -*-
1.1.0,create one counter per shard
1.1.0,"every corpus has shards, no new one"
1.1.0,!/usr/bin/env python
1.1.0,!/usr/bin/env python
1.1.0,Load checkpoint if we resume from a previous training.
1.1.0,check for code where vocab is saved instead of fields
1.1.0,(in the future this will be done in a smarter way)
1.1.0,Create a thread to listen for errors in the child processes.
1.1.0,Train with multiprocessing.
1.1.0,generator_to_serve = iter(generator_to_serve)
1.1.0,hack to dodge unpicklable `dict_keys`
1.1.0,"propagate exception to parent process, keeping original traceback"
1.1.0,magic indices
1.1.0,result caching
1.1.0,add one to account for BOS. Don't account for EOS because hitting
1.1.0,this implies it hasn't been found.
1.1.0,we don't block nothing if the user doesn't want it
1.1.0,we can't block nothing beam's too short
1.1.0,we check paths one by one
1.1.0,we don't forbid nothing if the user doesn't want it
1.1.0,we can't forbid nothing if beam's too short
1.1.0,Reordering forbidden_tokens following beam selection
1.1.0,We rebuild a dict to ensure we get the value and not the pointer
1.1.0,Grabing the newly selected tokens and associated ngram
1.1.0,skip the blocking if any token in current_ngram is excluded
1.1.0,"For temp=0.0, take the argmax to avoid divide-by-zero errors."
1.1.0,keep_topk=1 is also equivalent to argmax.
1.1.0,Set all logits that are not in the top-k to -10000.
1.1.0,This puts the probabilities close to 0.
1.1.0,"shape: (sum(~ self.is_finished), 1)"
1.1.0,!/usr/bin/env python
1.1.0,Maintains the longest src and tgt length in the current batch
1.1.0,Reset current longest length at a new batch (count=1)
1.1.0,max_tgt_in_batch = 0
1.1.0,Src: [<bos> w1 ... wN <eos>]
1.1.0,Tgt: [w1 ... wM <eos>]
1.1.0,for debugging
1.1.0,corpus_id field is useless here
1.1.0,Statistics
1.1.0,(0) add BOS and padding to tgt prediction
1.1.0,(1) Encoder forward.
1.1.0,(2) Repeat src objects `n_best` times.
1.1.0,"We use batch_size x n_best, get ``(src_len, batch * n_best, nfeat)``"
1.1.0,"(3) Init decoder with n_best src,"
1.1.0,"reshape tgt to ``(len, batch * n_best, nfeat)``"
1.1.0,masked_select
1.1.0,get aligned src id for each prediction's valid tgt tokens
1.1.0,TODO: support these blacklisted features
1.1.0,Turn any copied words into UNKs.
1.1.0,"Decoder forward, takes [tgt_len, batch, nfeats] as input"
1.1.0,"and [src_len, batch, hidden] as memory_bank"
1.1.0,"in case of inference tgt_len = 1, batch = beam times batch_size"
1.1.0,"in case of Gold Scoring tgt_len = actual length, batch = 1 batch"
1.1.0,Generator forward.
1.1.0,"returns [(batch_size x beam_size) , vocab ] when 1 step"
1.1.0,"or [ tgt_len, batch_size, vocab ] when full sentence"
1.1.0,"here we have scores [tgt_lenxbatch, vocab] or [beamxbatch, vocab]"
1.1.0,"returns [(batch_size x beam_size) , vocab ] when 1 step"
1.1.0,"or [ tgt_len, batch_size, vocab ] when full sentence"
1.1.0,(0) Prep the components of the search.
1.1.0,(1) Run the encoder on the src.
1.1.0,(2) prep decode_strategy. Possibly repeat src objects.
1.1.0,(3) Begin decoding step by step:
1.1.0,Reorder states.
1.1.0,beam parameters
1.1.0,result caching
1.1.0,beam state
1.1.0,BoolTensor was introduced in pytorch 1.2
1.1.0,"""global state"" of the old beam"
1.1.0,buffers for the topk scores and 'backpointer'
1.1.0,for testing
1.1.0,using integer division to get an integer _B without casting
1.1.0,force the output to be longer than self.min_length
1.1.0,Multiply probs by the beam probability.
1.1.0,"if the sequence ends now, then the penalty is the current"
1.1.0,"length + 1, to include the EOS token"
1.1.0,Avoid any direction that would repeat unwanted ngrams
1.1.0,Flatten probs into a list of possibilities.
1.1.0,Recover log probs.
1.1.0,Length penalty is just a scalar. It doesn't matter if it's applied
1.1.0,before or after the topk.
1.1.0,Resolve beam origin and map to batch index flat representation.
1.1.0,Append last prediction.
1.1.0,update global state (step == 1)
1.1.0,update global state (step > 1)
1.1.0,"shape: (batch_size x beam_size, 1)"
1.1.0,Penalize beams that finished.
1.1.0,"on real data (newstest2017) with the pretrained transformer,"
1.1.0,it's faster to not move this back to the original device
1.1.0,Store finished hypotheses for this batch.
1.1.0,End condition is the top beam finished and we can return
1.1.0,n_best hypotheses.
1.1.0,"If all sentences are translated, no need to go further."
1.1.0,Remove finished batches for the next step.
1.1.0,Term will be subtracted from probability
1.1.0,Probability will be divided by this
1.1.0,these warnings indicate that either the alpha/beta
1.1.0,"forces a penalty to be a no-op, or a penalty is a no-op but"
1.1.0,the alpha/beta would suggest otherwise.
1.1.0,using some length penalty
1.1.0,using some coverage penalty
1.1.0,!/usr/bin/env python
1.1.0,semaphore doesn't have a timeout arg in Python 2.7
1.1.0,perform a first request to initialize everything
1.1.0,backwards compatibility for confs
1.1.0,load can be called multiple times: modify copy
1.1.0,every segment becomes a dict for flexibility purposes
1.1.0,NOTE: translator returns lists of `n_best` list
1.1.0,build back results with empty texts
1.1.0,output contain alignment
1.1.0,Below are all the different penalty terms implemented so far.
1.1.0,Subtract coverage penalty from topk log probs.
1.1.0,Divide topk log probs by length penalty.
1.1.0,Sorting
1.1.0,Chinese segmentation
1.1.0,Chinese simplify -> Chinese traditional standard
1.1.0,Chinese simplify -> Chinese traditional (HongKong)
1.1.0,Chinese simplify -> Chinese traditional (Taiwan)
1.1.0,Chinese traditional -> Chinese simplify (v1)
1.1.0,Chinese traditional -> Chinese simplify (v2)
1.0.2,!/usr/bin/env python
1.0.2,!/usr/bin/env python
1.0.2,!/usr/bin/env python
1.0.2,!/usr/bin/env python
1.0.2,!/usr/bin/env python
1.0.2,!/usr/bin/env python3
1.0.2,-*- coding: utf-8 -*-
1.0.2,
1.0.2,"OpenNMT-py documentation build configuration file, created by"
1.0.2,sphinx-quickstart on Sun Dec 17 12:07:14 2017.
1.0.2,
1.0.2,This file is execfile()d with the current directory set to its
1.0.2,containing dir.
1.0.2,
1.0.2,Note that not all possible configuration values are present in this
1.0.2,autogenerated file.
1.0.2,
1.0.2,All configuration values have a default; values that are commented out
1.0.2,serve to show the default.
1.0.2,"If extensions (or modules to document with autodoc) are in another directory,"
1.0.2,add these directories to sys.path here. If the directory is relative to the
1.0.2,"documentation root, use os.path.abspath to make it absolute, like shown here."
1.0.2,
1.0.2,import os
1.0.2,import sys
1.0.2,"sys.path.insert(0, os.path.abspath('.'))"
1.0.2,-- General configuration ------------------------------------------------
1.0.2,"If your documentation needs a minimal Sphinx version, state it here."
1.0.2,
1.0.2,needs_sphinx = '1.0'
1.0.2,"Add any Sphinx extension module names here, as strings. They can be"
1.0.2,extensions coming with Sphinx (named 'sphinx.ext.*') or your custom
1.0.2,ones.
1.0.2,Show base classes
1.0.2,"Use ""variables"" section for Attributes instead of weird block things"
1.0.2,mimicking the function style.
1.0.2,"Add any paths that contain templates here, relative to this directory."
1.0.2,The suffix(es) of source filenames.
1.0.2,You can specify multiple suffix as a list of string:
1.0.2,
1.0.2,"source_suffix = ['.rst', '.md']"
1.0.2,The master toctree document.
1.0.2,General information about the project.
1.0.2,"The version info for the project you're documenting, acts as replacement for"
1.0.2,"|version| and |release|, also used in various other places throughout the"
1.0.2,built documents.
1.0.2,
1.0.2,The short X.Y version.
1.0.2,"The full version, including alpha/beta/rc tags."
1.0.2,The language for content autogenerated by Sphinx. Refer to documentation
1.0.2,for a list of supported languages.
1.0.2,
1.0.2,This is also used if you do content translation via gettext catalogs.
1.0.2,"Usually you set ""language"" from the command line for these cases."
1.0.2,"List of patterns, relative to source directory, that match files and"
1.0.2,directories to ignore when looking for source files.
1.0.2,This patterns also effect to html_static_path and html_extra_path
1.0.2,The name of the Pygments (syntax highlighting) style to use.
1.0.2,"If true, `todo` and `todoList` produce output, else they produce nothing."
1.0.2,-- Options for HTML output ----------------------------------------------
1.0.2,The theme to use for HTML and HTML Help pages.  See the documentation for
1.0.2,a list of builtin themes.
1.0.2,
1.0.2,html_theme = 'sphinx_materialdesign_theme'
1.0.2,html_theme_path = [sphinx_materialdesign_theme.get_path()]
1.0.2,Theme options are theme-specific and customize the look and feel of a theme
1.0.2,"further.  For a list of options available for each theme, see the"
1.0.2,documentation.
1.0.2,
1.0.2,html_theme_options = {}
1.0.2,"Add any paths that contain custom static files (such as style sheets) here,"
1.0.2,"relative to this directory. They are copied after the builtin static files,"
1.0.2,"so a file named ""default.css"" will overwrite the builtin ""default.css""."
1.0.2,"Custom sidebar templates, must be a dictionary that maps document names"
1.0.2,to template names.
1.0.2,
1.0.2,This is required for the alabaster theme
1.0.2,refs: http://alabaster.readthedocs.io/en/latest/installation.html#sidebars
1.0.2,-- Options for HTMLHelp output ------------------------------------------
1.0.2,Output file base name for HTML help builder.
1.0.2,-- Options for LaTeX output ---------------------------------------------
1.0.2,The paper size ('letterpaper' or 'a4paper').
1.0.2,
1.0.2,"'papersize': 'letterpaper',"
1.0.2,"The font size ('10pt', '11pt' or '12pt')."
1.0.2,
1.0.2,"'pointsize': '10pt',"
1.0.2,Additional stuff for the LaTeX preamble.
1.0.2,
1.0.2,"'preamble': '',"
1.0.2,Latex figure (float) alignment
1.0.2,
1.0.2,"'figure_align': 'htbp',"
1.0.2,Grouping the document tree into LaTeX files. List of tuples
1.0.2,"(source start file, target name, title,"
1.0.2,"author, documentclass [howto, manual, or own class])."
1.0.2,-- Options for manual page output ---------------------------------------
1.0.2,One entry per manual page. List of tuples
1.0.2,"(source start file, name, description, authors, manual section)."
1.0.2,-- Options for Texinfo output -------------------------------------------
1.0.2,Grouping the document tree into Texinfo files. List of tuples
1.0.2,"(source start file, target name, title, author,"
1.0.2,"dir menu entry, description, category)"
1.0.2,degenerate case
1.0.2,cache the features
1.0.2,mp queues don't work well between procs unless they're from a manager
1.0.2,each device has its own saver so that reconstructing is easier
1.0.2,!/usr/bin/env python
1.0.2,-*- coding: utf-8 -*-
1.0.2,is this reachable?
1.0.2,Read in embeddings
1.0.2,Write to file
1.0.2,!/usr/bin/env python
1.0.2,-*- coding: utf-8 -*-
1.0.2,"Add in default model arguments, possibly added since training."
1.0.2,-*- encoding: utf-8 -*-
1.0.2,!/usr/bin/env python
1.0.2,-*- coding: utf-8 -*-
1.0.2,Author: Rico Sennrich
1.0.2,flake8: noqa
1.0.2,This file is retrieved from https://github.com/rsennrich/subword-nmt
1.0.2,hack for python2/3 compatibility
1.0.2,check version information
1.0.2,some hacking to deal with duplicates (only consider first instance)
1.0.2,don't print end-of-word symbols
1.0.2,sys.stderr.write('cannot split {0} further.\n'.format(segment))
1.0.2,sys.stderr.write('OOV: {0}\n'.format(segment))
1.0.2,sys.stderr.write('OOV: {0}\n'.format(segment))
1.0.2,python 2/3 compatibility
1.0.2,read/write files as UTF-8
1.0.2,!/usr/bin/env python
1.0.2,!/usr/bin/env python
1.0.2,-*- coding: utf-8 -*-
1.0.2,Author: Rico Sennrich
1.0.2,flake8: noqa
1.0.2,This file is retrieved from https://github.com/rsennrich/subword-nmt
1.0.2,hack for python2/3 compatibility
1.0.2,"find all instances of pair, and update frequency/indices around it"
1.0.2,find first symbol
1.0.2,"if first symbol is followed by second symbol, we've found an occurrence of pair (old_word[i:i+2])"
1.0.2,"assuming a symbol sequence ""A B C"", if ""B C"" is merged, reduce the frequency of ""A B"""
1.0.2,"assuming a symbol sequence ""A B C B"", if ""B C"" is merged, reduce the frequency of ""C B""."
1.0.2,"however, skip this if the sequence is A B C B C, because the frequency of ""C B"" will be reduced by the previous code block"
1.0.2,find new pair
1.0.2,"assuming a symbol sequence ""A BC D"", if ""B C"" is merged, increase the frequency of ""A BC"""
1.0.2,"assuming a symbol sequence ""A BC B"", if ""B C"" is merged, increase the frequency of ""BC B"""
1.0.2,"however, if the sequence is A BC BC, skip this step because the count of ""BC BC"" will be incremented by the previous code block"
1.0.2,data structure of pair frequencies
1.0.2,index from pairs to words
1.0.2,version 0.2 changes the handling of the end-of-word token ('</w>');
1.0.2,version numbering allows bckward compatibility
1.0.2,"threshold is inspired by Zipfian assumption, but should only affect speed"
1.0.2,we probably missed the best pair because of pruning; go back to full statistics
1.0.2,"threshold is inspired by Zipfian assumption, but should only affect speed"
1.0.2,python 2/3 compatibility
1.0.2,read/write files as UTF-8
1.0.2,!/usr/bin/env python
1.0.2,for back compat when attention_dropout was not defined
1.0.2,Build embeddings.
1.0.2,Build encoder.
1.0.2,Build decoder.
1.0.2,Share the embedding matrix - preprocess with share_vocab required.
1.0.2,src/tgt vocab should be the same if `-share_vocab` is specified.
1.0.2,Build NMTModel(= encoder + decoder).
1.0.2,Build Generator.
1.0.2,Load the model states from checkpoint or initialize them.
1.0.2,This preserves backward-compat for models using customed layernorm
1.0.2,end of patch for backward compatibility
1.0.2,!/usr/bin/env python
1.0.2,NOTE: It's important that ``opt`` has been validated and updated
1.0.2,at this point.
1.0.2,Load checkpoint if we resume from a previous training.
1.0.2,check for code where vocab is saved instead of fields
1.0.2,(in the future this will be done in a smarter way)
1.0.2,"Report src and tgt vocab sizes, including for features"
1.0.2,Build model.
1.0.2,Build optimizer.
1.0.2,Build model saver
1.0.2,Embedding Options
1.0.2,Encoder-Decoder Options
1.0.2,"group.add('--residual', '-residual',   action=""store_true"","
1.0.2,"help=""Add residual connections between RNN layers."")"
1.0.2,Attention options
1.0.2,Alignement options
1.0.2,Generator and loss options.
1.0.2,Data options
1.0.2,"Dictionary options, for text corpus"
1.0.2,"if you want to pass an existing vocab.pt file, pass it to"
1.0.2,-src_vocab alone as it already contains tgt vocab.
1.0.2,"Truncation options, for text corpus"
1.0.2,Data processing options
1.0.2,Options most relevant to speech
1.0.2,Option most relevant to image input
1.0.2,GPU
1.0.2,Init options
1.0.2,Pretrained word vectors
1.0.2,Fixed word vectors
1.0.2,Optimization options
1.0.2,learning rate
1.0.2,Use Tensorboard for visualization during training
1.0.2,Options most relevant to speech
1.0.2,Option most relevant to image input
1.0.2,Options most relevant to summarization.
1.0.2,Alpha and Beta values for Google Length + Coverage penalty
1.0.2,"Described here: https://arxiv.org/pdf/1609.08144.pdf, Section 7"
1.0.2,Options most relevant to speech.
1.0.2,Option most relevant to image input
1.0.2,Copyright 2016 The Chromium Authors. All rights reserved.
1.0.2,Use of this source code is governed by a BSD-style license that can be
1.0.2,found in the LICENSE file.
1.0.2,"Get the key 'value' in the dict, or just use 'value'"
1.0.2,Basic attributes.
1.0.2,Set model in training mode.
1.0.2,UPDATE DROPOUT
1.0.2,Run patience mechanism
1.0.2,"If the patience has reached the limit, stop training"
1.0.2,swap model params w/ moving average
1.0.2,(and keep the original parameters)
1.0.2,Set model in validating mode.
1.0.2,F-prop through the model.
1.0.2,Compute loss.
1.0.2,Update statistics.
1.0.2,Set model back to training mode.
1.0.2,Truncated BPTT: reminder not compatible with accum > 1
1.0.2,1. Create truncated target.
1.0.2,2. F-prop all but generator.
1.0.2,3. Compute loss.
1.0.2,4. Update the parameters and statistics.
1.0.2,Multi GPU gradient gather
1.0.2,"If truncated, don't backprop fully."
1.0.2,TO CHECK
1.0.2,if dec_state is not None:
1.0.2,dec_state.detach()
1.0.2,"in case of multi step gradient accumulation,"
1.0.2,update only after accum batches
1.0.2,For Flake
1.0.2,we avoid padding while mean pooling
1.0.2,Initialize the bridge layer
1.0.2,"s_len, batch, emb_dim = emb.size()"
1.0.2,Lengths data is wrapped inside a Tensor.
1.0.2,"LSTM has hidden and cell state, other only one"
1.0.2,Total number of states
1.0.2,Build a linear layer for each
1.0.2,The encoder hidden is  (layers*directions) x batch x dim.
1.0.2,"s_len, batch, emb_dim = emb.size()"
1.0.2,Run the forward pass of every layer of the tranformer.
1.0.2,why is the model_opt.__dict__ check necessary?
1.0.2,"(batch_size, 64, imgH, imgW)"
1.0.2,layer 1
1.0.2,"(batch_size, 64, imgH/2, imgW/2)"
1.0.2,"(batch_size, 128, imgH/2, imgW/2)"
1.0.2,layer 2
1.0.2,"(batch_size, 128, imgH/2/2, imgW/2/2)"
1.0.2,"(batch_size, 256, imgH/2/2, imgW/2/2)"
1.0.2,layer 3
1.0.2,batch norm 1
1.0.2,"(batch_size, 256, imgH/2/2, imgW/2/2)"
1.0.2,layer4
1.0.2,"(batch_size, 256, imgH/2/2/2, imgW/2/2)"
1.0.2,"(batch_size, 512, imgH/2/2/2, imgW/2/2)"
1.0.2,layer 5
1.0.2,batch norm 2
1.0.2,"(batch_size, 512, imgH/2/2/2, imgW/2/2/2)"
1.0.2,"(batch_size, 512, imgH/2/2/2, imgW/2/2/2)"
1.0.2,"# (batch_size, 512, H, W)"
1.0.2,Dimensions and padding for constructing the word embedding matrix
1.0.2,Dimensions and padding for feature embedding matrices
1.0.2,(these have no effect if feat_vocab_sizes is empty)
1.0.2,The embedding matrix look-up tables. The first look-up table
1.0.2,"is for words. Subsequent ones are for features, if any exist."
1.0.2,The final output size of word + feature vectors. This can vary
1.0.2,from the word vector size if and only if features are defined.
1.0.2,This is the attribute you should access if you need to know
1.0.2,how big your embeddings are going to be.
1.0.2,The sequence of operations that converts the input sequence
1.0.2,into a sequence of embeddings. At minimum this consists of
1.0.2,looking up the embeddings for each word and feature in the
1.0.2,input. Model parameters may require the sequence to contain
1.0.2,additional operations as well.
1.0.2,features must use word_vec_size
1.0.2,features will use feat_vec_size
1.0.2,This class is mainly used by decoder.py for RNNs but also
1.0.2,by the CNN / transformer decoder when copy attention is used
1.0.2,CNN has its own attention mechanism ConvMultiStepAttention
1.0.2,Transformer has its own MultiHeadedAttention
1.0.2,mlp wants it with bias
1.0.2,Check input sizes
1.0.2,"(batch, t_len, d) x (batch, d, s_len) --> (batch, t_len, s_len)"
1.0.2,"(batch, t_len, s_len, d)"
1.0.2,one step input
1.0.2,"compute attention scores, as in Luong et al."
1.0.2,Softmax or sparsemax to normalize attention weights
1.0.2,each context vector c_t is the weighted average
1.0.2,over all the source hidden states
1.0.2,concatenate
1.0.2,Check output sizes
1.0.2,Check output sizes
1.0.2,clamping necessary because of numerical errors: loss should be lower
1.0.2,"bounded by zero, but negative values near zero are possible without"
1.0.2,the clamp
1.0.2,from onmt.utils.misc import aeq
1.0.2,CHECKS
1.0.2,"batch, k_len, d = key.size()"
1.0.2,"batch_, k_len_, d_ = value.size()"
1.0.2,"aeq(batch, batch_)"
1.0.2,"aeq(k_len, k_len_)"
1.0.2,"aeq(d, d_)"
1.0.2,"batch_, q_len, d_ = query.size()"
1.0.2,"aeq(batch, batch_)"
1.0.2,"aeq(d, d_)"
1.0.2,"aeq(self.model_dim % 8, 0)"
1.0.2,if mask is not None:
1.0.2,"batch_, q_len_, k_len_ = mask.size()"
1.0.2,"aeq(batch_, batch)"
1.0.2,"aeq(k_len_, k_len)"
1.0.2,aeq(q_len_ == q_len)
1.0.2,END CHECKS
1.0.2,"1) Project key, value, and query."
1.0.2,1 or key_len x key_len
1.0.2,1 or key_len x key_len x dim_per_head
1.0.2,1 or key_len x key_len x dim_per_head
1.0.2,2) Calculate and scale scores.
1.0.2,batch x num_heads x query_len x key_len
1.0.2,3) Apply attention dropout and compute context vectors.
1.0.2,CHECK
1.0.2,"batch_, q_len_, d_ = output.size()"
1.0.2,"aeq(q_len, q_len_)"
1.0.2,"aeq(batch, batch_)"
1.0.2,"aeq(d, d_)"
1.0.2,Return multi-head attn
1.0.2,At the moment this class is only used by embeddings.Embeddings look-up tables
1.0.2,-*- coding: utf-8 -*-
1.0.2,checks
1.0.2,"batch, channel, height, width = base_target_emb.size()"
1.0.2,"batch_, channel_, height_, width_ = input_from_dec.size()"
1.0.2,"enc_batch, enc_channel, enc_height = encoder_out_top.size()"
1.0.2,"enc_batch_, enc_channel_, enc_height_ = encoder_out_combine.size()"
1.0.2,out_features * in_features
1.0.2,norm is out_features * 1
1.0.2,batch_size * out_features
1.0.2,out_features
1.0.2,out_features
1.0.2,batch_size * out_features
1.0.2,"out_channels, in_channels // groups, * kernel_size"
1.0.2,out_features
1.0.2,This is used nowhere in the code at the moment (Vincent Nguyen 05/18/2018)
1.0.2,"in_channels, out_channels, *kernel_size"
1.0.2,"in_channels, out_channels, *kernel_size"
1.0.2,"self.out_channels, 1"
1.0.2,out_features
1.0.2,out_features
1.0.2,store roots on diagonal
1.0.2,CHECKS
1.0.2,Original probabilities.
1.0.2,Probability of copying p(z=1) batch.
1.0.2,Probability of not copying: p_{word}(w) * (1 - p(z))
1.0.2,probabilities assigned by the model to the gold targets
1.0.2,probability of tokens copied from source
1.0.2,Set scores for unk to 0 and add eps
1.0.2,find the indices in which you do not use the copy mechanism
1.0.2,Drop padding.
1.0.2,this block does not depend on the loss value computed above
1.0.2,and is used only for stats
1.0.2,this block does not depend on the loss value computed above
1.0.2,and is used only for stats
1.0.2,Correct target copy token instead of <unk>
1.0.2,tgt[i] = align[i] + len(tgt_vocab)
1.0.2,for i such that tgt[i] == 0 and align[i] != 0
1.0.2,Compute sum of perplexities for stats
1.0.2,this part looks like it belongs in CopyGeneratorLoss
1.0.2,Compute Loss as NLL divided by seq length
1.0.2,Compute Total Loss per sequence in batch
1.0.2,Divide by length of each sequence and sum
1.0.2,initialize fields at the top of each unit test to prevent
1.0.2,any undesired stateful effects
1.0.2,"this test touches the file system, so it could be considered an"
1.0.2,integration test
1.0.2,write utf-8 bytes
1.0.2,batch 0 will always predict EOS. The other batches will predict
1.0.2,non-eos scores.
1.0.2,"""best"" prediction is eos - that should be blocked"
1.0.2,include at least one prediction OTHER than EOS
1.0.2,that is greater than -1e20
1.0.2,now batch 0 has ended and no others have
1.0.2,initial step
1.0.2,batch 0 dies on step 0
1.0.2,include at least one prediction OTHER than EOS
1.0.2,that is greater than -1e20
1.0.2,step 2
1.0.2,(old) batch 8 dies on step 1
1.0.2,step 3
1.0.2,everything dies
1.0.2,initial step
1.0.2,batch 0 dies on step 0
1.0.2,include at least one prediction OTHER than EOS
1.0.2,that is greater than -1e20
1.0.2,step 2
1.0.2,(old) batch 8 dies on step 1
1.0.2,step 3
1.0.2,everything dies
1.0.2,illegal_weights_mask = torch.ByteTensor([
1.0.2,"[0, 0, 0, 0, 0, 0, 0],"
1.0.2,"[0, 0, 0, 1, 1, 1, 1],"
1.0.2,"[0, 0, 0, 0, 0, 1, 1],"
1.0.2,"[0, 0, 1, 1, 1, 1, 1]])"
1.0.2,TODO: fix for pytorch 0.3
1.0.2,illegal_weights = alignments.masked_select(illegal_weights_mask)
1.0.2,"self.assertEqual(0.0, illegal_weights.data.sum())"
1.0.2,this could be considered an integration test because it touches
1.0.2,the filesystem for the config file (and the models)
1.0.2,-*- coding: utf-8 -*-
1.0.2,tests pad and numericalize integration
1.0.2,tests pad and numericalize integration
1.0.2,"this test touches the file system, so it could be considered an"
1.0.2,integration test
1.0.2,file to hold full paths to audio data
1.0.2,file to hold audio paths relative to _AUDIO_DATA_DIR (i.e. file names)
1.0.2,it's ok if non-audio files co-exist with audio files in the data dir
1.0.2,"dividing gets the noise in [-1, 1]"
1.0.2,"this test touches the file system, so it could be considered an"
1.0.2,integration test
1.0.2,file to hold full paths to image data
1.0.2,file to hold image paths relative to _IMG_DATA_DIR (i.e. file names)
1.0.2,it's ok if non-image files co-exist with image files in the data dir
1.0.2,all beams repeat (beam >= 1 repeat dummy scores)
1.0.2,predict repeat_idx over and over again
1.0.2,"before repeat, scores are either 0 or -inf"
1.0.2,"on repeat, `repeat_idx` score is BLOCKED_SCORE"
1.0.2,"(but it's still the best score, thus we have"
1.0.2,"[BLOCKED_SCORE, -inf, -inf, -inf, -inf]"
1.0.2,repetitions keeps maximizing score
1.0.2,"index 0 has been blocked, so repeating=>+0.0 score"
1.0.2,other indexes are -inf so repeating=>BLOCKED_SCORE
1.0.2,which is higher
1.0.2,beam 0 and beam >=2 will repeat (beam >= 2 repeat dummy scores)
1.0.2,non-interesting beams are going to get dummy values
1.0.2,"on initial round, only predicted scores for beam 0"
1.0.2,matter. Make two predictions. Top one will be repeated
1.0.2,"in beam zero, second one will live on in beam 1."
1.0.2,predict the same thing in beam 0
1.0.2,continue pushing around what beam 1 predicts
1.0.2,"now beam 0 dies (along with the others), beam 1 -> beam 0"
1.0.2,"now beam 0 dies (along with the others), beam 1 -> beam 0"
1.0.2,beam 0 and beam >= 2 will repeat (beam 2 repeats excluded idx)
1.0.2,non-interesting beams are going to get dummy values
1.0.2,predict the same thing in beam 0
1.0.2,continue pushing around what beam 1 predicts
1.0.2,predict the allowed-repeat again in beam 2
1.0.2,"now beam 0 dies, beam 1 -> beam 0, beam 2 -> beam 1"
1.0.2,and the rest die
1.0.2,"since all preds after i=0 are 0, we can check"
1.0.2,that the beam is the correct idx by checking that
1.0.2,the curr score is the initial score
1.0.2,beam 0 will always predict EOS. The other beams will predict
1.0.2,non-eos scores.
1.0.2,non-interesting beams are going to get dummy values
1.0.2,"""best"" prediction is eos - that should be blocked"
1.0.2,include at least beam_sz predictions OTHER than EOS
1.0.2,that are greater than -1e20
1.0.2,predict eos in beam 0
1.0.2,provide beam_sz other good predictions
1.0.2,now the top beam has ended and no others have
1.0.2,"not of interest, but want to make sure it keeps running"
1.0.2,since only beam 0 terminates and n_best = 2
1.0.2,"this is also a test that when block_ngram_repeat=0,"
1.0.2,repeating is acceptable
1.0.2,non-interesting beams are going to get dummy values
1.0.2,"""best"" prediction is eos - that should be blocked"
1.0.2,include at least beam_sz predictions OTHER than EOS
1.0.2,that are greater than -1e20
1.0.2,predict eos in beam 1
1.0.2,provide beam_sz other good predictions in other beams
1.0.2,provide beam_sz other good predictions in other beams
1.0.2,beam 1 dies on min_length
1.0.2,beam 0 dies on the step after beam 1 dies
1.0.2,"inp_lens is tiled in initialize, reassign to make attn match"
1.0.2,non-interesting beams are going to get dummy values
1.0.2,"""best"" prediction is eos - that should be blocked"
1.0.2,include at least beam_sz predictions OTHER than EOS
1.0.2,that are greater than -1e20
1.0.2,predict eos in beam 1
1.0.2,provide beam_sz other good predictions in other beams
1.0.2,provide beam_sz other good predictions in other beams
1.0.2,no top beams are finished yet
1.0.2,beam 1 dies on min_length
1.0.2,no top beams are finished yet
1.0.2,beam 0 dies on the step after beam 1 dies
1.0.2,top beam is finished now so there are attentions
1.0.2,two beams are finished in each batch
1.0.2,second dim is cut down to the non-padded src length
1.0.2,first dim is equal to the time of death
1.0.2,(beam 0 died at current step - adjust for SOS)
1.0.2,(beam 1 died at last step - adjust for SOS)
1.0.2,behavior gets weird when beam is already done so just stop
1.0.2,this is just test_beam.TestBeamAgainstReferenceCase repeated
1.0.2,in each batch.
1.0.2,"init_preds: [4, 3, 5, 6, 7] - no EOS's"
1.0.2,no EOS's yet
1.0.2,"[5, 3, 2, 6, 0], so beam 2 predicts EOS!"
1.0.2,assumes beam 2 finished on last step
1.0.2,ended beam 2 shouldn't continue
1.0.2,"[2, 5, 3, 6, 0] repeat self.BATCH_SZ, so beam 0 predicts EOS!"
1.0.2,"[-2.4879, -3.8910, -4.1010, -4.2010, -4.4010] repeat self.BATCH_SZ"
1.0.2,another beam is finished in all batches
1.0.2,new beam 0 finished
1.0.2,new beam 0 is old beam 3
1.0.2,assumes beam 0 finished on last step
1.0.2,"[5, 2, 6, 1, 0] repeat self.BATCH_SZ, so beam 1 predicts EOS!"
1.0.2,new beam 1 finished
1.0.2,new beam 1 is old beam 4
1.0.2,this could be considered an integration test because it tests
1.0.2,interactions between the GNMT scorer and the beam
1.0.2,"-data option is required, but not used in this test, so dummy."
1.0.2,len x batch x nfeat
1.0.2,batch x c x h x w
1.0.2,batch x 1 x nfft x t
1.0.2,Initialize vectors to compare size with
1.0.2,Ensure correct sizes and types
1.0.2,Make sure that output has the correct size and type
1.0.2,Make sure that output has the correct size and type
1.0.2,Make sure that output has the correct size and type
1.0.2,"[('encoder_type', 'transformer'),"
1.0.2,"('word_vec_size', 16), ('rnn_size', 16)],"
1.0.2,""""""" Only do SRU test if requirment is safisfied. """""""
1.0.2,SRU doesn't support input_feed.
1.0.2,"when reasonable, set audio_enc_pooling to 2"
1.0.2,Need lengths >= audio_enc_pooling**n_layers.
1.0.2,"That condition is unrealistic for large n_layers,"
1.0.2,so leave audio_enc_pooling at 1.
1.0.2,first check there's nothing unexpectedly not trainable
1.0.2,ok: word embeddings shouldn't be trainable
1.0.2,if word vecs are fixed
1.0.2,ok: positional encodings shouldn't be trainable
1.0.2,then check nothing unexpectedly trainable
1.0.2,!/usr/bin/env python
1.0.2,-*- coding: utf-8 -*-
1.0.2,Remove the generated *pt files.
1.0.2,Test image preprocessing
1.0.2,Test audio preprocessing
1.0.2,Decoder state
1.0.2,Build the RNN.
1.0.2,Set up the context gate.
1.0.2,Set up the standard attention.
1.0.2,The encoder hidden is  (layers*directions) x batch x dim.
1.0.2,We need to convert it to layers x batch x (directions*dim).
1.0.2,Init the input feed.
1.0.2,Update the state with the result.
1.0.2,Concatenates sequence of tensors along a new dimension.
1.0.2,NOTE: v0.3 to 0.4: dec_outs / attns[*] may not be list
1.0.2,(in particular in case of SRU) it was not raising error in 0.3
1.0.2,since stack(Variable) was allowed.
1.0.2,"In 0.4, SRU returns a tensor that shouldn't be stacke"
1.0.2,Check
1.0.2,Calculate the attention.
1.0.2,Calculate the context gate.
1.0.2,Additional args check.
1.0.2,END Additional args check.
1.0.2,Input feed concatenates hidden state with
1.0.2,input at every time step.
1.0.2,TODO: context gate should be employed
1.0.2,instead of second RNN transform.
1.0.2,Update the coverage attention.
1.0.2,Decoder State
1.0.2,CNNDecoder has its own attention mechanism.
1.0.2,Set up a separate copy attention layer if needed.
1.0.2,The output of CNNEncoder.
1.0.2,The combination of output of CNNEncoder and source embeddings.
1.0.2,Process the result and update the attentions.
1.0.2,Update the state.
1.0.2,TODO change the way attns is returned dict => list or tuple (onnx)
1.0.2,Memory_lengths is a single tensor shared between all models.
1.0.2,This assumption will not hold if Translator is modified
1.0.2,to calculate memory_lengths as something other than the length
1.0.2,of the input.
1.0.2,"return _, (B, Q_len, K_len)"
1.0.2,"layer average attention across heads, get ``(B, Q, K)``"
1.0.2,"Case 1: no full_context, no align heads -> layer avg baseline"
1.0.2,"Case 2: no full_context, 1 align heads -> guided align"
1.0.2,"Case 3: full_context, 1 align heads -> full cte guided align"
1.0.2,T: could be 1 in the case of stepwise decoding or tgt_len
1.0.2,BoolTensor was introduced in pytorch 1.2
1.0.2,Decoder State
1.0.2,"previously, there was a GlobalAttention module here for copy"
1.0.2,"attention. But it was never actually used -- the ""copy"" attention"
1.0.2,just reuses the context attention.
1.0.2,"attns[""align""] = torch.stack(attn_aligns, 0).mean(0)  # All avg"
1.0.2,TODO change the way attns is returned dict => list or tuple (onnx)
1.0.2,"buffer size in bytes, determine equiv. # of elements based on data type"
1.0.2,copy tensors into buffer_t
1.0.2,all-reduce and rescale
1.0.2,copy all-reduced buffer back into tensors
1.0.2,"tensor is bigger than buffer, all-reduce and rescale directly"
1.0.2,"buffer is full, all-reduce and replace buffer with grad"
1.0.2,add tensor to buffer
1.0.2,TODO: Find a better way to check for sparse gradients.
1.0.2,we use here a FusedAdam() copy of an old Apex repo
1.0.2,In this case use the new AMP API from apex
1.0.2,In this case use the old FusedAdam with FP16_optimizer wrapper
1.0.2,Load everything from the checkpoint.
1.0.2,Build everything from scratch.
1.0.2,"Reset optimizer, keep options."
1.0.2,"Reset options, keep optimizer."
1.0.2,State can be partially restored.
1.0.2,Code below is an implementation of https://arxiv.org/pdf/1804.04235.pdf
1.0.2,inspired but modified from https://github.com/DeadAt0m/adafactor-pytorch
1.0.2,backward compatibility
1.0.2,assuming a list/generator of parameter means single group
1.0.2,compute combined scale factor for this group
1.0.2,norm is in fact norm*scale
1.0.2,note: p.grad should not ever be set for correct operation of
1.0.2,mixed precision optimizer that sometimes sends None gradients
1.0.2,State initialization
1.0.2,Exponential moving average of gradient values
1.0.2,Exponential moving average of squared gradient values
1.0.2,-*- coding: utf-8 -*-
1.0.2,if the loss function operates on vectors of raw logits instead of
1.0.2,"probabilities, only the first part of the generator needs to be"
1.0.2,"passed to the NMTLossCompute. At the moment, the only supported"
1.0.2,loss function of this kind is the sparsemax loss.
1.0.2,"attn_align should be in (batch_size, pad_tgt_size, pad_src_size)"
1.0.2,"align_idx should be a Tensor in size([N, 3]), N is total number"
1.0.2,"of align src-tgt pair in current batch, each as"
1.0.2,"['sent_NÂ°_in_batch', 'tgt_id+1', 'src_id'] (check AlignField)"
1.0.2,NOTE: tgt-src ref alignement that in range_ of shard
1.0.2,(coherent with batch.tgt)
1.0.2,"align_head contains value in [0, 1) presenting attn prob,"
1.0.2,0 was resulted by the context attention src_pad_mask
1.0.2,"So, the correspand position in ref_align should also be 0"
1.0.2,"Therefore, clip align_head to > 1e-18 should be bias free."
1.0.2,non_none: the subdict of the state dictionary where the values
1.0.2,are not None.
1.0.2,"Now, the iteration:"
1.0.2,state is a dictionary of sequences of tensor-like but we
1.0.2,want a sequence of dictionaries of tensors.
1.0.2,"First, unzip the dictionary into a sequence of keys and a"
1.0.2,sequence of tensor-like sequences.
1.0.2,"Now, yield a dictionary for each shard. The keys are always"
1.0.2,the same. values is a sequence of length #keys where each
1.0.2,element is a sequence of length #shards. We want to iterate
1.0.2,"over the shards, not over the keys: therefore, the values need"
1.0.2,to be re-zipped by shard and then each shard can be paired
1.0.2,with the keys.
1.0.2,Assumed backprop'd
1.0.2,this check is here because audio allows the encoder and decoder to
1.0.2,"be different sizes, but other model types do not yet"
1.0.2,"Load default opt values, then overwrite with the opts in"
1.0.2,"the checkpoint. That way, if there are new options added,"
1.0.2,the defaults are used.
1.0.2,Don't do anything
1.0.2,Update best score of each criteria
1.0.2,Reset tolerance
1.0.2,Update current status
1.0.2,Decrease tolerance
1.0.2,Log
1.0.2,Log
1.0.2,Get a list of world_size lists with len(stat_list) Statistics objects
1.0.2,SRU doesn't support PackedSequence.
1.0.2,-*- coding: utf-8 -*-
1.0.2,threshold on 1 to avoid div by 0
1.0.2,treat alignment matrix one by one as each have different lengths
1.0.2,get valid alignment (sub-matrix from full paded aligment matrix)
1.0.2,-*- coding: utf-8 -*-
1.0.2,this one is needed for torchtext random call (shuffled iterator)
1.0.2,in multi gpu it ensures datasets are read in the same order
1.0.2,some cudnn methods can be random even after fixing the seed
1.0.2,unless you tell it to be deterministic
1.0.2,These ensure same initialization in multi gpu mode
1.0.2,Shift values to be >= 0
1.0.2,we need to check the model path + any tokenizer path
1.0.2,coding: utf-8
1.0.2,make a small vocab containing just the tokens in the source sequence
1.0.2,Map source tokens to indices in the dynamic dict.
1.0.2,self.src_vocabs is used in collapse_copy_scores and Translator.py
1.0.2,this assumes src_field and tgt_field are both text
1.0.2,fields needs to have only keys that examples have as attrs
1.0.2,avoid infinite recursion when fields isn't defined
1.0.2,-*- coding: utf-8 -*-
1.0.2,backwards compatibility
1.0.2,monkey-patch to make torchtext Vocab's pickleable
1.0.2,"+1 for tgt side to keep coherent after ""bos"" padding,"
1.0.2,"register ['NÂ°_in_batch', 'tgt_id+1', 'src_id']"
1.0.2,"List[Tuple[str, Vocab]] -> List[Tuple[str, Field]]"
1.0.2,"-> dict[str, Field]"
1.0.2,"Dict[str, List[Tuple[str, Field]]]"
1.0.2,doesn't change structure - don't return early.
1.0.2,"Dict[str, List[Tuple[str, Field]]] -> List[Tuple[str, Field]]"
1.0.2,"-> dict[str, Field]"
1.0.2,"if tgt isn't using TextMultiField, then no text field is."
1.0.2,this is basically copy-pasted from torchtext.
1.0.2,counters changes in place
1.0.2,keep the order of tokens specified in the vocab file by
1.0.2,adding them to the counter with decreasing counting values
1.0.2,`tgt_vocab_size` is ignored when sharing vocabularies
1.0.2,return vocab to dump with standard name
1.0.2,empty train_dataset_files so that vocab is only loaded from
1.0.2,"given paths in src_vocab_path, tgt_vocab_path"
1.0.2,Load vocabulary
1.0.2,Drop the none-using from memory but keep the last
1.0.2,"in the long run, shouldn't it be possible to do this by calling"
1.0.2,build_vocab with both the src and tgt data?
1.0.2,fast-forward if loaded from state
1.0.2,NOTE: `rnn.pack_padded_sequence` requires that a
1.0.2,"minibatch be sorted by decreasing order, which"
1.0.2,requires reversing relative to typical sort keys
1.0.2,Temporarily load one shard to retrieve sort_key for data_type
1.0.2,"NOTE: This is causing some issues for consumer/producer,"
1.0.2,as we may still have some of those examples in some queue
1.0.2,cur_dataset.examples = None
1.0.2,gc.collect()
1.0.2,del cur_dataset
1.0.2,gc.collect()
1.0.2,Cycle through the shards indefinitely.
1.0.2,"When the dataset is not repeated, we might need to ensure that"
1.0.2,the number of returned batches is the multiple of a given value.
1.0.2,This is important for multi GPU training to ensure that all
1.0.2,workers have the same number of batches to process.
1.0.2,Maintains the longest src and tgt length in the current batch
1.0.2,Reset current longest length at a new batch (count=1)
1.0.2,Src: [<bos> w1 ... wN <eos>]
1.0.2,Tgt: [w1 ... wM <eos>]
1.0.2,-*- coding: utf-8 -*-
1.0.2,imports of datatype-specific dependencies
1.0.2,torchaudio loading options recently changed. It's probably
1.0.2,straightforward to rewrite the audio handling to make use of
1.0.2,"up-to-date torchaudio, but in the meantime there is a legacy"
1.0.2,method which uses the old defaults
1.0.2,STFT
1.0.2,-*- coding: utf-8 -*-
1.0.2,domain specific dependencies
1.0.2,coding: utf-8
1.0.2,several data readers need optional dependencies. There's no
1.0.2,appropriate builtin exception
1.0.2,-*- coding: utf-8 -*-
1.0.2,mix this with partial
1.0.2,batch (list(list(list))): batch_size x len(self.fields) x seq_len
1.0.2,lengths: batch_size
1.0.2,data: seq_len x batch_size x len(self.fields)
1.0.2,flake8: noqa
1.0.2,For command-line option parsing
1.0.2,"Check pass, set the args."
1.0.2,"This SRU version implements its own cuda-level optimization,"
1.0.2,so it requires that:
1.0.2,1. `cupy` and `pynvrtc` python package installed.
1.0.2,2. pytorch is built with cuda support.
1.0.2,3. library path set: export LD_LIBRARY_PATH=<cuda lib path>.
1.0.2,Check 1.
1.0.2,Check 2.
1.0.2,Check 3.
1.0.2,This sets up device to use.
1.0.2,-> directions x batch x dim
1.0.2,For DEBUG
1.0.2,"size = (length, batch, x.size(-1)) \"
1.0.2,"if x.dim() == 3 else (batch, x.size(-1))"
1.0.2,grad_x = x.new(*x.size()) if k_ == 3 else x.new(*size).zero_()
1.0.2,Normal use
1.0.2,"An entry check here, will catch on train side and translate side"
1.0.2,if requirements are not satisfied.
1.0.2,RNNDecoderState wraps hidden as a tuple.
1.0.2,fh -> (layers*directions) x batch x dim
1.0.2,NOTE: We need to trim the vocab to remove any unk tokens that
1.0.2,were not originally here.
1.0.2,!/usr/bin/env python
1.0.2,!/usr/bin/env python
1.0.2,!/usr/bin/env python
1.0.2,-*- coding: utf-8 -*-
1.0.2,!/usr/bin/env python
1.0.2,-*- coding: utf-8 -*-
1.0.2,create one counter per shard
1.0.2,"every corpus has shards, no new one"
1.0.2,!/usr/bin/env python
1.0.2,!/usr/bin/env python
1.0.2,Load checkpoint if we resume from a previous training.
1.0.2,check for code where vocab is saved instead of fields
1.0.2,(in the future this will be done in a smarter way)
1.0.2,Create a thread to listen for errors in the child processes.
1.0.2,Train with multiprocessing.
1.0.2,generator_to_serve = iter(generator_to_serve)
1.0.2,hack to dodge unpicklable `dict_keys`
1.0.2,"propagate exception to parent process, keeping original traceback"
1.0.2,magic indices
1.0.2,result caching
1.0.2,add one to account for BOS. Don't account for EOS because hitting
1.0.2,this implies it hasn't been found.
1.0.2,we don't block nothing if the user doesn't want it
1.0.2,we can't block nothing beam's too short
1.0.2,we check paths one by one
1.0.2,we don't forbid nothing if the user doesn't want it
1.0.2,we can't forbid nothing if beam's too short
1.0.2,Reordering forbidden_tokens following beam selection
1.0.2,We rebuild a dict to ensure we get the value and not the pointer
1.0.2,Grabing the newly selected tokens and associated ngram
1.0.2,skip the blocking if any token in current_ngram is excluded
1.0.2,"For temp=0.0, take the argmax to avoid divide-by-zero errors."
1.0.2,keep_topk=1 is also equivalent to argmax.
1.0.2,Set all logits that are not in the top-k to -10000.
1.0.2,This puts the probabilities close to 0.
1.0.2,"shape: (sum(~ self.is_finished), 1)"
1.0.2,!/usr/bin/env python
1.0.2,Maintains the longest src and tgt length in the current batch
1.0.2,Reset current longest length at a new batch (count=1)
1.0.2,max_tgt_in_batch = 0
1.0.2,Src: [<bos> w1 ... wN <eos>]
1.0.2,Tgt: [w1 ... wM <eos>]
1.0.2,for debugging
1.0.2,Statistics
1.0.2,(0) add BOS and padding to tgt prediction
1.0.2,(1) Encoder forward.
1.0.2,(2) Repeat src objects `n_best` times.
1.0.2,"We use batch_size x n_best, get ``(src_len, batch * n_best, nfeat)``"
1.0.2,"(3) Init decoder with n_best src,"
1.0.2,"reshape tgt to ``(len, batch * n_best, nfeat)``"
1.0.2,masked_select
1.0.2,get aligned src id for each prediction's valid tgt tokens
1.0.2,TODO: support these blacklisted features
1.0.2,Turn any copied words into UNKs.
1.0.2,"Decoder forward, takes [tgt_len, batch, nfeats] as input"
1.0.2,"and [src_len, batch, hidden] as memory_bank"
1.0.2,"in case of inference tgt_len = 1, batch = beam times batch_size"
1.0.2,"in case of Gold Scoring tgt_len = actual length, batch = 1 batch"
1.0.2,Generator forward.
1.0.2,"returns [(batch_size x beam_size) , vocab ] when 1 step"
1.0.2,"or [ tgt_len, batch_size, vocab ] when full sentence"
1.0.2,"here we have scores [tgt_lenxbatch, vocab] or [beamxbatch, vocab]"
1.0.2,"returns [(batch_size x beam_size) , vocab ] when 1 step"
1.0.2,"or [ tgt_len, batch_size, vocab ] when full sentence"
1.0.2,(0) Prep the components of the search.
1.0.2,(1) Run the encoder on the src.
1.0.2,(2) prep decode_strategy. Possibly repeat src objects.
1.0.2,(3) Begin decoding step by step:
1.0.2,Reorder states.
1.0.2,beam parameters
1.0.2,result caching
1.0.2,beam state
1.0.2,BoolTensor was introduced in pytorch 1.2
1.0.2,"""global state"" of the old beam"
1.0.2,buffers for the topk scores and 'backpointer'
1.0.2,for testing
1.0.2,using integer division to get an integer _B without casting
1.0.2,force the output to be longer than self.min_length
1.0.2,Multiply probs by the beam probability.
1.0.2,"if the sequence ends now, then the penalty is the current"
1.0.2,"length + 1, to include the EOS token"
1.0.2,Avoid any direction that would repeat unwanted ngrams
1.0.2,Flatten probs into a list of possibilities.
1.0.2,Recover log probs.
1.0.2,Length penalty is just a scalar. It doesn't matter if it's applied
1.0.2,before or after the topk.
1.0.2,Resolve beam origin and map to batch index flat representation.
1.0.2,Append last prediction.
1.0.2,update global state (step == 1)
1.0.2,update global state (step > 1)
1.0.2,"shape: (batch_size x beam_size, 1)"
1.0.2,Penalize beams that finished.
1.0.2,"on real data (newstest2017) with the pretrained transformer,"
1.0.2,it's faster to not move this back to the original device
1.0.2,Store finished hypotheses for this batch.
1.0.2,End condition is the top beam finished and we can return
1.0.2,n_best hypotheses.
1.0.2,"If all sentences are translated, no need to go further."
1.0.2,Remove finished batches for the next step.
1.0.2,Term will be subtracted from probability
1.0.2,Probability will be divided by this
1.0.2,these warnings indicate that either the alpha/beta
1.0.2,"forces a penalty to be a no-op, or a penalty is a no-op but"
1.0.2,the alpha/beta would suggest otherwise.
1.0.2,using some length penalty
1.0.2,using some coverage penalty
1.0.2,!/usr/bin/env python
1.0.2,semaphore doesn't have a timeout arg in Python 2.7
1.0.2,backwards compatibility for confs
1.0.2,load can be called multiple times: modify copy
1.0.2,NOTE: translator returns lists of `n_best` list
1.0.2,build back results with empty texts
1.0.2,output contain alignment
1.0.2,Below are all the different penalty terms implemented so far.
1.0.2,Subtract coverage penalty from topk log probs.
1.0.2,Divide topk log probs by length penalty.
1.0.2,Sorting
1.0.2,Chinese segmentation
1.0.2,Chinese simplify -> Chinese traditional standard
1.0.2,Chinese simplify -> Chinese traditional (HongKong)
1.0.2,Chinese simplify -> Chinese traditional (Taiwan)
1.0.2,Chinese traditional -> Chinese simplify (v1)
1.0.2,Chinese traditional -> Chinese simplify (v2)
1.0.1,!/usr/bin/env python
1.0.1,!/usr/bin/env python
1.0.1,!/usr/bin/env python
1.0.1,!/usr/bin/env python
1.0.1,!/usr/bin/env python
1.0.1,!/usr/bin/env python3
1.0.1,-*- coding: utf-8 -*-
1.0.1,
1.0.1,"OpenNMT-py documentation build configuration file, created by"
1.0.1,sphinx-quickstart on Sun Dec 17 12:07:14 2017.
1.0.1,
1.0.1,This file is execfile()d with the current directory set to its
1.0.1,containing dir.
1.0.1,
1.0.1,Note that not all possible configuration values are present in this
1.0.1,autogenerated file.
1.0.1,
1.0.1,All configuration values have a default; values that are commented out
1.0.1,serve to show the default.
1.0.1,"If extensions (or modules to document with autodoc) are in another directory,"
1.0.1,add these directories to sys.path here. If the directory is relative to the
1.0.1,"documentation root, use os.path.abspath to make it absolute, like shown here."
1.0.1,
1.0.1,import os
1.0.1,import sys
1.0.1,"sys.path.insert(0, os.path.abspath('.'))"
1.0.1,-- General configuration ------------------------------------------------
1.0.1,"If your documentation needs a minimal Sphinx version, state it here."
1.0.1,
1.0.1,needs_sphinx = '1.0'
1.0.1,"Add any Sphinx extension module names here, as strings. They can be"
1.0.1,extensions coming with Sphinx (named 'sphinx.ext.*') or your custom
1.0.1,ones.
1.0.1,Show base classes
1.0.1,"Use ""variables"" section for Attributes instead of weird block things"
1.0.1,mimicking the function style.
1.0.1,"Add any paths that contain templates here, relative to this directory."
1.0.1,The suffix(es) of source filenames.
1.0.1,You can specify multiple suffix as a list of string:
1.0.1,
1.0.1,"source_suffix = ['.rst', '.md']"
1.0.1,The master toctree document.
1.0.1,General information about the project.
1.0.1,"The version info for the project you're documenting, acts as replacement for"
1.0.1,"|version| and |release|, also used in various other places throughout the"
1.0.1,built documents.
1.0.1,
1.0.1,The short X.Y version.
1.0.1,"The full version, including alpha/beta/rc tags."
1.0.1,The language for content autogenerated by Sphinx. Refer to documentation
1.0.1,for a list of supported languages.
1.0.1,
1.0.1,This is also used if you do content translation via gettext catalogs.
1.0.1,"Usually you set ""language"" from the command line for these cases."
1.0.1,"List of patterns, relative to source directory, that match files and"
1.0.1,directories to ignore when looking for source files.
1.0.1,This patterns also effect to html_static_path and html_extra_path
1.0.1,The name of the Pygments (syntax highlighting) style to use.
1.0.1,"If true, `todo` and `todoList` produce output, else they produce nothing."
1.0.1,-- Options for HTML output ----------------------------------------------
1.0.1,The theme to use for HTML and HTML Help pages.  See the documentation for
1.0.1,a list of builtin themes.
1.0.1,
1.0.1,html_theme = 'sphinx_materialdesign_theme'
1.0.1,html_theme_path = [sphinx_materialdesign_theme.get_path()]
1.0.1,Theme options are theme-specific and customize the look and feel of a theme
1.0.1,"further.  For a list of options available for each theme, see the"
1.0.1,documentation.
1.0.1,
1.0.1,html_theme_options = {}
1.0.1,"Add any paths that contain custom static files (such as style sheets) here,"
1.0.1,"relative to this directory. They are copied after the builtin static files,"
1.0.1,"so a file named ""default.css"" will overwrite the builtin ""default.css""."
1.0.1,"Custom sidebar templates, must be a dictionary that maps document names"
1.0.1,to template names.
1.0.1,
1.0.1,This is required for the alabaster theme
1.0.1,refs: http://alabaster.readthedocs.io/en/latest/installation.html#sidebars
1.0.1,-- Options for HTMLHelp output ------------------------------------------
1.0.1,Output file base name for HTML help builder.
1.0.1,-- Options for LaTeX output ---------------------------------------------
1.0.1,The paper size ('letterpaper' or 'a4paper').
1.0.1,
1.0.1,"'papersize': 'letterpaper',"
1.0.1,"The font size ('10pt', '11pt' or '12pt')."
1.0.1,
1.0.1,"'pointsize': '10pt',"
1.0.1,Additional stuff for the LaTeX preamble.
1.0.1,
1.0.1,"'preamble': '',"
1.0.1,Latex figure (float) alignment
1.0.1,
1.0.1,"'figure_align': 'htbp',"
1.0.1,Grouping the document tree into LaTeX files. List of tuples
1.0.1,"(source start file, target name, title,"
1.0.1,"author, documentclass [howto, manual, or own class])."
1.0.1,-- Options for manual page output ---------------------------------------
1.0.1,One entry per manual page. List of tuples
1.0.1,"(source start file, name, description, authors, manual section)."
1.0.1,-- Options for Texinfo output -------------------------------------------
1.0.1,Grouping the document tree into Texinfo files. List of tuples
1.0.1,"(source start file, target name, title, author,"
1.0.1,"dir menu entry, description, category)"
1.0.1,degenerate case
1.0.1,cache the features
1.0.1,mp queues don't work well between procs unless they're from a manager
1.0.1,each device has its own saver so that reconstructing is easier
1.0.1,!/usr/bin/env python
1.0.1,-*- coding: utf-8 -*-
1.0.1,is this reachable?
1.0.1,Read in embeddings
1.0.1,Write to file
1.0.1,!/usr/bin/env python
1.0.1,-*- coding: utf-8 -*-
1.0.1,"Add in default model arguments, possibly added since training."
1.0.1,-*- encoding: utf-8 -*-
1.0.1,!/usr/bin/env python
1.0.1,-*- coding: utf-8 -*-
1.0.1,Author: Rico Sennrich
1.0.1,flake8: noqa
1.0.1,This file is retrieved from https://github.com/rsennrich/subword-nmt
1.0.1,hack for python2/3 compatibility
1.0.1,check version information
1.0.1,some hacking to deal with duplicates (only consider first instance)
1.0.1,don't print end-of-word symbols
1.0.1,sys.stderr.write('cannot split {0} further.\n'.format(segment))
1.0.1,sys.stderr.write('OOV: {0}\n'.format(segment))
1.0.1,sys.stderr.write('OOV: {0}\n'.format(segment))
1.0.1,python 2/3 compatibility
1.0.1,read/write files as UTF-8
1.0.1,!/usr/bin/env python
1.0.1,!/usr/bin/env python
1.0.1,-*- coding: utf-8 -*-
1.0.1,Author: Rico Sennrich
1.0.1,flake8: noqa
1.0.1,This file is retrieved from https://github.com/rsennrich/subword-nmt
1.0.1,hack for python2/3 compatibility
1.0.1,"find all instances of pair, and update frequency/indices around it"
1.0.1,find first symbol
1.0.1,"if first symbol is followed by second symbol, we've found an occurrence of pair (old_word[i:i+2])"
1.0.1,"assuming a symbol sequence ""A B C"", if ""B C"" is merged, reduce the frequency of ""A B"""
1.0.1,"assuming a symbol sequence ""A B C B"", if ""B C"" is merged, reduce the frequency of ""C B""."
1.0.1,"however, skip this if the sequence is A B C B C, because the frequency of ""C B"" will be reduced by the previous code block"
1.0.1,find new pair
1.0.1,"assuming a symbol sequence ""A BC D"", if ""B C"" is merged, increase the frequency of ""A BC"""
1.0.1,"assuming a symbol sequence ""A BC B"", if ""B C"" is merged, increase the frequency of ""BC B"""
1.0.1,"however, if the sequence is A BC BC, skip this step because the count of ""BC BC"" will be incremented by the previous code block"
1.0.1,data structure of pair frequencies
1.0.1,index from pairs to words
1.0.1,version 0.2 changes the handling of the end-of-word token ('</w>');
1.0.1,version numbering allows bckward compatibility
1.0.1,"threshold is inspired by Zipfian assumption, but should only affect speed"
1.0.1,we probably missed the best pair because of pruning; go back to full statistics
1.0.1,"threshold is inspired by Zipfian assumption, but should only affect speed"
1.0.1,python 2/3 compatibility
1.0.1,read/write files as UTF-8
1.0.1,!/usr/bin/env python
1.0.1,for back compat when attention_dropout was not defined
1.0.1,Build embeddings.
1.0.1,Build encoder.
1.0.1,Build decoder.
1.0.1,Share the embedding matrix - preprocess with share_vocab required.
1.0.1,src/tgt vocab should be the same if `-share_vocab` is specified.
1.0.1,Build NMTModel(= encoder + decoder).
1.0.1,Build Generator.
1.0.1,Load the model states from checkpoint or initialize them.
1.0.1,This preserves backward-compat for models using customed layernorm
1.0.1,end of patch for backward compatibility
1.0.1,!/usr/bin/env python
1.0.1,NOTE: It's important that ``opt`` has been validated and updated
1.0.1,at this point.
1.0.1,Load checkpoint if we resume from a previous training.
1.0.1,check for code where vocab is saved instead of fields
1.0.1,(in the future this will be done in a smarter way)
1.0.1,"Report src and tgt vocab sizes, including for features"
1.0.1,Build model.
1.0.1,Build optimizer.
1.0.1,Build model saver
1.0.1,Embedding Options
1.0.1,Encoder-Decoder Options
1.0.1,"group.add('--residual', '-residual',   action=""store_true"","
1.0.1,"help=""Add residual connections between RNN layers."")"
1.0.1,Attention options
1.0.1,Alignement options
1.0.1,Generator and loss options.
1.0.1,Data options
1.0.1,"Dictionary options, for text corpus"
1.0.1,"if you want to pass an existing vocab.pt file, pass it to"
1.0.1,-src_vocab alone as it already contains tgt vocab.
1.0.1,"Truncation options, for text corpus"
1.0.1,Data processing options
1.0.1,Options most relevant to speech
1.0.1,Option most relevant to image input
1.0.1,GPU
1.0.1,Init options
1.0.1,Pretrained word vectors
1.0.1,Fixed word vectors
1.0.1,Optimization options
1.0.1,learning rate
1.0.1,Use Tensorboard for visualization during training
1.0.1,Options most relevant to speech
1.0.1,Option most relevant to image input
1.0.1,Options most relevant to summarization.
1.0.1,Alpha and Beta values for Google Length + Coverage penalty
1.0.1,"Described here: https://arxiv.org/pdf/1609.08144.pdf, Section 7"
1.0.1,Options most relevant to speech.
1.0.1,Option most relevant to image input
1.0.1,Copyright 2016 The Chromium Authors. All rights reserved.
1.0.1,Use of this source code is governed by a BSD-style license that can be
1.0.1,found in the LICENSE file.
1.0.1,"Get the key 'value' in the dict, or just use 'value'"
1.0.1,Basic attributes.
1.0.1,Set model in training mode.
1.0.1,UPDATE DROPOUT
1.0.1,Run patience mechanism
1.0.1,"If the patience has reached the limit, stop training"
1.0.1,swap model params w/ moving average
1.0.1,(and keep the original parameters)
1.0.1,Set model in validating mode.
1.0.1,F-prop through the model.
1.0.1,Compute loss.
1.0.1,Update statistics.
1.0.1,Set model back to training mode.
1.0.1,Truncated BPTT: reminder not compatible with accum > 1
1.0.1,1. Create truncated target.
1.0.1,2. F-prop all but generator.
1.0.1,3. Compute loss.
1.0.1,4. Update the parameters and statistics.
1.0.1,Multi GPU gradient gather
1.0.1,"If truncated, don't backprop fully."
1.0.1,TO CHECK
1.0.1,if dec_state is not None:
1.0.1,dec_state.detach()
1.0.1,"in case of multi step gradient accumulation,"
1.0.1,update only after accum batches
1.0.1,For Flake
1.0.1,we avoid padding while mean pooling
1.0.1,Initialize the bridge layer
1.0.1,"s_len, batch, emb_dim = emb.size()"
1.0.1,Lengths data is wrapped inside a Tensor.
1.0.1,"LSTM has hidden and cell state, other only one"
1.0.1,Total number of states
1.0.1,Build a linear layer for each
1.0.1,The encoder hidden is  (layers*directions) x batch x dim.
1.0.1,"s_len, batch, emb_dim = emb.size()"
1.0.1,Run the forward pass of every layer of the tranformer.
1.0.1,why is the model_opt.__dict__ check necessary?
1.0.1,"(batch_size, 64, imgH, imgW)"
1.0.1,layer 1
1.0.1,"(batch_size, 64, imgH/2, imgW/2)"
1.0.1,"(batch_size, 128, imgH/2, imgW/2)"
1.0.1,layer 2
1.0.1,"(batch_size, 128, imgH/2/2, imgW/2/2)"
1.0.1,"(batch_size, 256, imgH/2/2, imgW/2/2)"
1.0.1,layer 3
1.0.1,batch norm 1
1.0.1,"(batch_size, 256, imgH/2/2, imgW/2/2)"
1.0.1,layer4
1.0.1,"(batch_size, 256, imgH/2/2/2, imgW/2/2)"
1.0.1,"(batch_size, 512, imgH/2/2/2, imgW/2/2)"
1.0.1,layer 5
1.0.1,batch norm 2
1.0.1,"(batch_size, 512, imgH/2/2/2, imgW/2/2/2)"
1.0.1,"(batch_size, 512, imgH/2/2/2, imgW/2/2/2)"
1.0.1,"# (batch_size, 512, H, W)"
1.0.1,Dimensions and padding for constructing the word embedding matrix
1.0.1,Dimensions and padding for feature embedding matrices
1.0.1,(these have no effect if feat_vocab_sizes is empty)
1.0.1,The embedding matrix look-up tables. The first look-up table
1.0.1,"is for words. Subsequent ones are for features, if any exist."
1.0.1,The final output size of word + feature vectors. This can vary
1.0.1,from the word vector size if and only if features are defined.
1.0.1,This is the attribute you should access if you need to know
1.0.1,how big your embeddings are going to be.
1.0.1,The sequence of operations that converts the input sequence
1.0.1,into a sequence of embeddings. At minimum this consists of
1.0.1,looking up the embeddings for each word and feature in the
1.0.1,input. Model parameters may require the sequence to contain
1.0.1,additional operations as well.
1.0.1,features must use word_vec_size
1.0.1,features will use feat_vec_size
1.0.1,This class is mainly used by decoder.py for RNNs but also
1.0.1,by the CNN / transformer decoder when copy attention is used
1.0.1,CNN has its own attention mechanism ConvMultiStepAttention
1.0.1,Transformer has its own MultiHeadedAttention
1.0.1,mlp wants it with bias
1.0.1,Check input sizes
1.0.1,"(batch, t_len, d) x (batch, d, s_len) --> (batch, t_len, s_len)"
1.0.1,"(batch, t_len, s_len, d)"
1.0.1,one step input
1.0.1,"compute attention scores, as in Luong et al."
1.0.1,Softmax or sparsemax to normalize attention weights
1.0.1,each context vector c_t is the weighted average
1.0.1,over all the source hidden states
1.0.1,concatenate
1.0.1,Check output sizes
1.0.1,Check output sizes
1.0.1,clamping necessary because of numerical errors: loss should be lower
1.0.1,"bounded by zero, but negative values near zero are possible without"
1.0.1,the clamp
1.0.1,from onmt.utils.misc import aeq
1.0.1,CHECKS
1.0.1,"batch, k_len, d = key.size()"
1.0.1,"batch_, k_len_, d_ = value.size()"
1.0.1,"aeq(batch, batch_)"
1.0.1,"aeq(k_len, k_len_)"
1.0.1,"aeq(d, d_)"
1.0.1,"batch_, q_len, d_ = query.size()"
1.0.1,"aeq(batch, batch_)"
1.0.1,"aeq(d, d_)"
1.0.1,"aeq(self.model_dim % 8, 0)"
1.0.1,if mask is not None:
1.0.1,"batch_, q_len_, k_len_ = mask.size()"
1.0.1,"aeq(batch_, batch)"
1.0.1,"aeq(k_len_, k_len)"
1.0.1,aeq(q_len_ == q_len)
1.0.1,END CHECKS
1.0.1,"1) Project key, value, and query."
1.0.1,1 or key_len x key_len
1.0.1,1 or key_len x key_len x dim_per_head
1.0.1,1 or key_len x key_len x dim_per_head
1.0.1,2) Calculate and scale scores.
1.0.1,batch x num_heads x query_len x key_len
1.0.1,3) Apply attention dropout and compute context vectors.
1.0.1,CHECK
1.0.1,"batch_, q_len_, d_ = output.size()"
1.0.1,"aeq(q_len, q_len_)"
1.0.1,"aeq(batch, batch_)"
1.0.1,"aeq(d, d_)"
1.0.1,Return multi-head attn
1.0.1,At the moment this class is only used by embeddings.Embeddings look-up tables
1.0.1,-*- coding: utf-8 -*-
1.0.1,checks
1.0.1,"batch, channel, height, width = base_target_emb.size()"
1.0.1,"batch_, channel_, height_, width_ = input_from_dec.size()"
1.0.1,"enc_batch, enc_channel, enc_height = encoder_out_top.size()"
1.0.1,"enc_batch_, enc_channel_, enc_height_ = encoder_out_combine.size()"
1.0.1,out_features * in_features
1.0.1,norm is out_features * 1
1.0.1,batch_size * out_features
1.0.1,out_features
1.0.1,out_features
1.0.1,batch_size * out_features
1.0.1,"out_channels, in_channels // groups, * kernel_size"
1.0.1,out_features
1.0.1,This is used nowhere in the code at the moment (Vincent Nguyen 05/18/2018)
1.0.1,"in_channels, out_channels, *kernel_size"
1.0.1,"in_channels, out_channels, *kernel_size"
1.0.1,"self.out_channels, 1"
1.0.1,out_features
1.0.1,out_features
1.0.1,store roots on diagonal
1.0.1,CHECKS
1.0.1,Original probabilities.
1.0.1,Probability of copying p(z=1) batch.
1.0.1,Probability of not copying: p_{word}(w) * (1 - p(z))
1.0.1,probabilities assigned by the model to the gold targets
1.0.1,probability of tokens copied from source
1.0.1,Set scores for unk to 0 and add eps
1.0.1,find the indices in which you do not use the copy mechanism
1.0.1,Drop padding.
1.0.1,this block does not depend on the loss value computed above
1.0.1,and is used only for stats
1.0.1,this block does not depend on the loss value computed above
1.0.1,and is used only for stats
1.0.1,Correct target copy token instead of <unk>
1.0.1,tgt[i] = align[i] + len(tgt_vocab)
1.0.1,for i such that tgt[i] == 0 and align[i] != 0
1.0.1,Compute sum of perplexities for stats
1.0.1,this part looks like it belongs in CopyGeneratorLoss
1.0.1,Compute Loss as NLL divided by seq length
1.0.1,Compute Total Loss per sequence in batch
1.0.1,Divide by length of each sequence and sum
1.0.1,initialize fields at the top of each unit test to prevent
1.0.1,any undesired stateful effects
1.0.1,"this test touches the file system, so it could be considered an"
1.0.1,integration test
1.0.1,write utf-8 bytes
1.0.1,batch 0 will always predict EOS. The other batches will predict
1.0.1,non-eos scores.
1.0.1,"""best"" prediction is eos - that should be blocked"
1.0.1,include at least one prediction OTHER than EOS
1.0.1,that is greater than -1e20
1.0.1,now batch 0 has ended and no others have
1.0.1,initial step
1.0.1,batch 0 dies on step 0
1.0.1,include at least one prediction OTHER than EOS
1.0.1,that is greater than -1e20
1.0.1,step 2
1.0.1,(old) batch 8 dies on step 1
1.0.1,step 3
1.0.1,everything dies
1.0.1,initial step
1.0.1,batch 0 dies on step 0
1.0.1,include at least one prediction OTHER than EOS
1.0.1,that is greater than -1e20
1.0.1,step 2
1.0.1,(old) batch 8 dies on step 1
1.0.1,step 3
1.0.1,everything dies
1.0.1,illegal_weights_mask = torch.ByteTensor([
1.0.1,"[0, 0, 0, 0, 0, 0, 0],"
1.0.1,"[0, 0, 0, 1, 1, 1, 1],"
1.0.1,"[0, 0, 0, 0, 0, 1, 1],"
1.0.1,"[0, 0, 1, 1, 1, 1, 1]])"
1.0.1,TODO: fix for pytorch 0.3
1.0.1,illegal_weights = alignments.masked_select(illegal_weights_mask)
1.0.1,"self.assertEqual(0.0, illegal_weights.data.sum())"
1.0.1,this could be considered an integration test because it touches
1.0.1,the filesystem for the config file (and the models)
1.0.1,-*- coding: utf-8 -*-
1.0.1,tests pad and numericalize integration
1.0.1,tests pad and numericalize integration
1.0.1,"this test touches the file system, so it could be considered an"
1.0.1,integration test
1.0.1,file to hold full paths to audio data
1.0.1,file to hold audio paths relative to _AUDIO_DATA_DIR (i.e. file names)
1.0.1,it's ok if non-audio files co-exist with audio files in the data dir
1.0.1,"dividing gets the noise in [-1, 1]"
1.0.1,"this test touches the file system, so it could be considered an"
1.0.1,integration test
1.0.1,file to hold full paths to image data
1.0.1,file to hold image paths relative to _IMG_DATA_DIR (i.e. file names)
1.0.1,it's ok if non-image files co-exist with image files in the data dir
1.0.1,all beams repeat (beam >= 1 repeat dummy scores)
1.0.1,predict repeat_idx over and over again
1.0.1,"before repeat, scores are either 0 or -inf"
1.0.1,"on repeat, `repeat_idx` score is BLOCKED_SCORE"
1.0.1,"(but it's still the best score, thus we have"
1.0.1,"[BLOCKED_SCORE, -inf, -inf, -inf, -inf]"
1.0.1,repetitions keeps maximizing score
1.0.1,"index 0 has been blocked, so repeating=>+0.0 score"
1.0.1,other indexes are -inf so repeating=>BLOCKED_SCORE
1.0.1,which is higher
1.0.1,beam 0 and beam >=2 will repeat (beam >= 2 repeat dummy scores)
1.0.1,non-interesting beams are going to get dummy values
1.0.1,"on initial round, only predicted scores for beam 0"
1.0.1,matter. Make two predictions. Top one will be repeated
1.0.1,"in beam zero, second one will live on in beam 1."
1.0.1,predict the same thing in beam 0
1.0.1,continue pushing around what beam 1 predicts
1.0.1,"now beam 0 dies (along with the others), beam 1 -> beam 0"
1.0.1,"now beam 0 dies (along with the others), beam 1 -> beam 0"
1.0.1,beam 0 and beam >= 2 will repeat (beam 2 repeats excluded idx)
1.0.1,non-interesting beams are going to get dummy values
1.0.1,predict the same thing in beam 0
1.0.1,continue pushing around what beam 1 predicts
1.0.1,predict the allowed-repeat again in beam 2
1.0.1,"now beam 0 dies, beam 1 -> beam 0, beam 2 -> beam 1"
1.0.1,and the rest die
1.0.1,"since all preds after i=0 are 0, we can check"
1.0.1,that the beam is the correct idx by checking that
1.0.1,the curr score is the initial score
1.0.1,beam 0 will always predict EOS. The other beams will predict
1.0.1,non-eos scores.
1.0.1,non-interesting beams are going to get dummy values
1.0.1,"""best"" prediction is eos - that should be blocked"
1.0.1,include at least beam_sz predictions OTHER than EOS
1.0.1,that are greater than -1e20
1.0.1,predict eos in beam 0
1.0.1,provide beam_sz other good predictions
1.0.1,now the top beam has ended and no others have
1.0.1,"not of interest, but want to make sure it keeps running"
1.0.1,since only beam 0 terminates and n_best = 2
1.0.1,"this is also a test that when block_ngram_repeat=0,"
1.0.1,repeating is acceptable
1.0.1,non-interesting beams are going to get dummy values
1.0.1,"""best"" prediction is eos - that should be blocked"
1.0.1,include at least beam_sz predictions OTHER than EOS
1.0.1,that are greater than -1e20
1.0.1,predict eos in beam 1
1.0.1,provide beam_sz other good predictions in other beams
1.0.1,provide beam_sz other good predictions in other beams
1.0.1,beam 1 dies on min_length
1.0.1,beam 0 dies on the step after beam 1 dies
1.0.1,"inp_lens is tiled in initialize, reassign to make attn match"
1.0.1,non-interesting beams are going to get dummy values
1.0.1,"""best"" prediction is eos - that should be blocked"
1.0.1,include at least beam_sz predictions OTHER than EOS
1.0.1,that are greater than -1e20
1.0.1,predict eos in beam 1
1.0.1,provide beam_sz other good predictions in other beams
1.0.1,provide beam_sz other good predictions in other beams
1.0.1,no top beams are finished yet
1.0.1,beam 1 dies on min_length
1.0.1,no top beams are finished yet
1.0.1,beam 0 dies on the step after beam 1 dies
1.0.1,top beam is finished now so there are attentions
1.0.1,two beams are finished in each batch
1.0.1,second dim is cut down to the non-padded src length
1.0.1,first dim is equal to the time of death
1.0.1,(beam 0 died at current step - adjust for SOS)
1.0.1,(beam 1 died at last step - adjust for SOS)
1.0.1,behavior gets weird when beam is already done so just stop
1.0.1,this is just test_beam.TestBeamAgainstReferenceCase repeated
1.0.1,in each batch.
1.0.1,"init_preds: [4, 3, 5, 6, 7] - no EOS's"
1.0.1,no EOS's yet
1.0.1,"[5, 3, 2, 6, 0], so beam 2 predicts EOS!"
1.0.1,assumes beam 2 finished on last step
1.0.1,ended beam 2 shouldn't continue
1.0.1,"[2, 5, 3, 6, 0] repeat self.BATCH_SZ, so beam 0 predicts EOS!"
1.0.1,"[-2.4879, -3.8910, -4.1010, -4.2010, -4.4010] repeat self.BATCH_SZ"
1.0.1,another beam is finished in all batches
1.0.1,new beam 0 finished
1.0.1,new beam 0 is old beam 3
1.0.1,assumes beam 0 finished on last step
1.0.1,"[5, 2, 6, 1, 0] repeat self.BATCH_SZ, so beam 1 predicts EOS!"
1.0.1,new beam 1 finished
1.0.1,new beam 1 is old beam 4
1.0.1,this could be considered an integration test because it tests
1.0.1,interactions between the GNMT scorer and the beam
1.0.1,"-data option is required, but not used in this test, so dummy."
1.0.1,len x batch x nfeat
1.0.1,batch x c x h x w
1.0.1,batch x 1 x nfft x t
1.0.1,Initialize vectors to compare size with
1.0.1,Ensure correct sizes and types
1.0.1,Make sure that output has the correct size and type
1.0.1,Make sure that output has the correct size and type
1.0.1,Make sure that output has the correct size and type
1.0.1,"[('encoder_type', 'transformer'),"
1.0.1,"('word_vec_size', 16), ('rnn_size', 16)],"
1.0.1,""""""" Only do SRU test if requirment is safisfied. """""""
1.0.1,SRU doesn't support input_feed.
1.0.1,"when reasonable, set audio_enc_pooling to 2"
1.0.1,Need lengths >= audio_enc_pooling**n_layers.
1.0.1,"That condition is unrealistic for large n_layers,"
1.0.1,so leave audio_enc_pooling at 1.
1.0.1,first check there's nothing unexpectedly not trainable
1.0.1,ok: word embeddings shouldn't be trainable
1.0.1,if word vecs are fixed
1.0.1,ok: positional encodings shouldn't be trainable
1.0.1,then check nothing unexpectedly trainable
1.0.1,!/usr/bin/env python
1.0.1,-*- coding: utf-8 -*-
1.0.1,Remove the generated *pt files.
1.0.1,Test image preprocessing
1.0.1,Test audio preprocessing
1.0.1,Decoder state
1.0.1,Build the RNN.
1.0.1,Set up the context gate.
1.0.1,Set up the standard attention.
1.0.1,The encoder hidden is  (layers*directions) x batch x dim.
1.0.1,We need to convert it to layers x batch x (directions*dim).
1.0.1,Init the input feed.
1.0.1,Update the state with the result.
1.0.1,Concatenates sequence of tensors along a new dimension.
1.0.1,NOTE: v0.3 to 0.4: dec_outs / attns[*] may not be list
1.0.1,(in particular in case of SRU) it was not raising error in 0.3
1.0.1,since stack(Variable) was allowed.
1.0.1,"In 0.4, SRU returns a tensor that shouldn't be stacke"
1.0.1,Check
1.0.1,Calculate the attention.
1.0.1,Calculate the context gate.
1.0.1,Additional args check.
1.0.1,END Additional args check.
1.0.1,Input feed concatenates hidden state with
1.0.1,input at every time step.
1.0.1,TODO: context gate should be employed
1.0.1,instead of second RNN transform.
1.0.1,Update the coverage attention.
1.0.1,Decoder State
1.0.1,CNNDecoder has its own attention mechanism.
1.0.1,Set up a separate copy attention layer if needed.
1.0.1,The output of CNNEncoder.
1.0.1,The combination of output of CNNEncoder and source embeddings.
1.0.1,Process the result and update the attentions.
1.0.1,Update the state.
1.0.1,TODO change the way attns is returned dict => list or tuple (onnx)
1.0.1,Memory_lengths is a single tensor shared between all models.
1.0.1,This assumption will not hold if Translator is modified
1.0.1,to calculate memory_lengths as something other than the length
1.0.1,of the input.
1.0.1,"return _, (B, Q_len, K_len)"
1.0.1,"layer average attention across heads, get ``(B, Q, K)``"
1.0.1,"Case 1: no full_context, no align heads -> layer avg baseline"
1.0.1,"Case 2: no full_context, 1 align heads -> guided align"
1.0.1,"Case 3: full_context, 1 align heads -> full cte guided align"
1.0.1,T: could be 1 in the case of stepwise decoding or tgt_len
1.0.1,BoolTensor was introduced in pytorch 1.2
1.0.1,Decoder State
1.0.1,"previously, there was a GlobalAttention module here for copy"
1.0.1,"attention. But it was never actually used -- the ""copy"" attention"
1.0.1,just reuses the context attention.
1.0.1,"attns[""align""] = torch.stack(attn_aligns, 0).mean(0)  # All avg"
1.0.1,TODO change the way attns is returned dict => list or tuple (onnx)
1.0.1,"buffer size in bytes, determine equiv. # of elements based on data type"
1.0.1,copy tensors into buffer_t
1.0.1,all-reduce and rescale
1.0.1,copy all-reduced buffer back into tensors
1.0.1,"tensor is bigger than buffer, all-reduce and rescale directly"
1.0.1,"buffer is full, all-reduce and replace buffer with grad"
1.0.1,add tensor to buffer
1.0.1,TODO: Find a better way to check for sparse gradients.
1.0.1,we use here a FusedAdam() copy of an old Apex repo
1.0.1,In this case use the new AMP API from apex
1.0.1,In this case use the old FusedAdam with FP16_optimizer wrapper
1.0.1,Load everything from the checkpoint.
1.0.1,Build everything from scratch.
1.0.1,"Reset optimizer, keep options."
1.0.1,"Reset options, keep optimizer."
1.0.1,State can be partially restored.
1.0.1,Code below is an implementation of https://arxiv.org/pdf/1804.04235.pdf
1.0.1,inspired but modified from https://github.com/DeadAt0m/adafactor-pytorch
1.0.1,backward compatibility
1.0.1,assuming a list/generator of parameter means single group
1.0.1,compute combined scale factor for this group
1.0.1,norm is in fact norm*scale
1.0.1,note: p.grad should not ever be set for correct operation of
1.0.1,mixed precision optimizer that sometimes sends None gradients
1.0.1,State initialization
1.0.1,Exponential moving average of gradient values
1.0.1,Exponential moving average of squared gradient values
1.0.1,-*- coding: utf-8 -*-
1.0.1,if the loss function operates on vectors of raw logits instead of
1.0.1,"probabilities, only the first part of the generator needs to be"
1.0.1,"passed to the NMTLossCompute. At the moment, the only supported"
1.0.1,loss function of this kind is the sparsemax loss.
1.0.1,"attn_align should be in (batch_size, pad_tgt_size, pad_src_size)"
1.0.1,"align_idx should be a Tensor in size([N, 3]), N is total number"
1.0.1,"of align src-tgt pair in current batch, each as"
1.0.1,"['sent_NÂ°_in_batch', 'tgt_id+1', 'src_id'] (check AlignField)"
1.0.1,NOTE: tgt-src ref alignement that in range_ of shard
1.0.1,(coherent with batch.tgt)
1.0.1,"align_head contains value in [0, 1) presenting attn prob,"
1.0.1,0 was resulted by the context attention src_pad_mask
1.0.1,"So, the correspand position in ref_align should also be 0"
1.0.1,"Therefore, clip align_head to > 1e-18 should be bias free."
1.0.1,non_none: the subdict of the state dictionary where the values
1.0.1,are not None.
1.0.1,"Now, the iteration:"
1.0.1,state is a dictionary of sequences of tensor-like but we
1.0.1,want a sequence of dictionaries of tensors.
1.0.1,"First, unzip the dictionary into a sequence of keys and a"
1.0.1,sequence of tensor-like sequences.
1.0.1,"Now, yield a dictionary for each shard. The keys are always"
1.0.1,the same. values is a sequence of length #keys where each
1.0.1,element is a sequence of length #shards. We want to iterate
1.0.1,"over the shards, not over the keys: therefore, the values need"
1.0.1,to be re-zipped by shard and then each shard can be paired
1.0.1,with the keys.
1.0.1,Assumed backprop'd
1.0.1,this check is here because audio allows the encoder and decoder to
1.0.1,"be different sizes, but other model types do not yet"
1.0.1,"Load default opt values, then overwrite with the opts in"
1.0.1,"the checkpoint. That way, if there are new options added,"
1.0.1,the defaults are used.
1.0.1,Don't do anything
1.0.1,Update best score of each criteria
1.0.1,Reset tolerance
1.0.1,Update current status
1.0.1,Decrease tolerance
1.0.1,Log
1.0.1,Log
1.0.1,Get a list of world_size lists with len(stat_list) Statistics objects
1.0.1,SRU doesn't support PackedSequence.
1.0.1,-*- coding: utf-8 -*-
1.0.1,threshold on 1 to avoid div by 0
1.0.1,treat alignment matrix one by one as each have different lengths
1.0.1,get valid alignment (sub-matrix from full paded aligment matrix)
1.0.1,-*- coding: utf-8 -*-
1.0.1,this one is needed for torchtext random call (shuffled iterator)
1.0.1,in multi gpu it ensures datasets are read in the same order
1.0.1,some cudnn methods can be random even after fixing the seed
1.0.1,unless you tell it to be deterministic
1.0.1,These ensure same initialization in multi gpu mode
1.0.1,Shift values to be >= 0
1.0.1,we need to check the model path + any tokenizer path
1.0.1,coding: utf-8
1.0.1,make a small vocab containing just the tokens in the source sequence
1.0.1,Map source tokens to indices in the dynamic dict.
1.0.1,self.src_vocabs is used in collapse_copy_scores and Translator.py
1.0.1,this assumes src_field and tgt_field are both text
1.0.1,fields needs to have only keys that examples have as attrs
1.0.1,avoid infinite recursion when fields isn't defined
1.0.1,-*- coding: utf-8 -*-
1.0.1,backwards compatibility
1.0.1,monkey-patch to make torchtext Vocab's pickleable
1.0.1,"+1 for tgt side to keep coherent after ""bos"" padding,"
1.0.1,"register ['NÂ°_in_batch', 'tgt_id+1', 'src_id']"
1.0.1,"List[Tuple[str, Vocab]] -> List[Tuple[str, Field]]"
1.0.1,"-> dict[str, Field]"
1.0.1,"Dict[str, List[Tuple[str, Field]]]"
1.0.1,doesn't change structure - don't return early.
1.0.1,"Dict[str, List[Tuple[str, Field]]] -> List[Tuple[str, Field]]"
1.0.1,"-> dict[str, Field]"
1.0.1,"if tgt isn't using TextMultiField, then no text field is."
1.0.1,this is basically copy-pasted from torchtext.
1.0.1,counters changes in place
1.0.1,keep the order of tokens specified in the vocab file by
1.0.1,adding them to the counter with decreasing counting values
1.0.1,`tgt_vocab_size` is ignored when sharing vocabularies
1.0.1,return vocab to dump with standard name
1.0.1,empty train_dataset_files so that vocab is only loaded from
1.0.1,"given paths in src_vocab_path, tgt_vocab_path"
1.0.1,Load vocabulary
1.0.1,Drop the none-using from memory but keep the last
1.0.1,"in the long run, shouldn't it be possible to do this by calling"
1.0.1,build_vocab with both the src and tgt data?
1.0.1,fast-forward if loaded from state
1.0.1,NOTE: `rnn.pack_padded_sequence` requires that a
1.0.1,"minibatch be sorted by decreasing order, which"
1.0.1,requires reversing relative to typical sort keys
1.0.1,Temporarily load one shard to retrieve sort_key for data_type
1.0.1,"NOTE: This is causing some issues for consumer/producer,"
1.0.1,as we may still have some of those examples in some queue
1.0.1,cur_dataset.examples = None
1.0.1,gc.collect()
1.0.1,del cur_dataset
1.0.1,gc.collect()
1.0.1,Cycle through the shards indefinitely.
1.0.1,"When the dataset is not repeated, we might need to ensure that"
1.0.1,the number of returned batches is the multiple of a given value.
1.0.1,This is important for multi GPU training to ensure that all
1.0.1,workers have the same number of batches to process.
1.0.1,Maintains the longest src and tgt length in the current batch
1.0.1,Reset current longest length at a new batch (count=1)
1.0.1,Src: [<bos> w1 ... wN <eos>]
1.0.1,Tgt: [w1 ... wM <eos>]
1.0.1,-*- coding: utf-8 -*-
1.0.1,imports of datatype-specific dependencies
1.0.1,torchaudio loading options recently changed. It's probably
1.0.1,straightforward to rewrite the audio handling to make use of
1.0.1,"up-to-date torchaudio, but in the meantime there is a legacy"
1.0.1,method which uses the old defaults
1.0.1,STFT
1.0.1,-*- coding: utf-8 -*-
1.0.1,domain specific dependencies
1.0.1,coding: utf-8
1.0.1,several data readers need optional dependencies. There's no
1.0.1,appropriate builtin exception
1.0.1,-*- coding: utf-8 -*-
1.0.1,mix this with partial
1.0.1,batch (list(list(list))): batch_size x len(self.fields) x seq_len
1.0.1,lengths: batch_size
1.0.1,data: seq_len x batch_size x len(self.fields)
1.0.1,flake8: noqa
1.0.1,For command-line option parsing
1.0.1,"Check pass, set the args."
1.0.1,"This SRU version implements its own cuda-level optimization,"
1.0.1,so it requires that:
1.0.1,1. `cupy` and `pynvrtc` python package installed.
1.0.1,2. pytorch is built with cuda support.
1.0.1,3. library path set: export LD_LIBRARY_PATH=<cuda lib path>.
1.0.1,Check 1.
1.0.1,Check 2.
1.0.1,Check 3.
1.0.1,This sets up device to use.
1.0.1,-> directions x batch x dim
1.0.1,For DEBUG
1.0.1,"size = (length, batch, x.size(-1)) \"
1.0.1,"if x.dim() == 3 else (batch, x.size(-1))"
1.0.1,grad_x = x.new(*x.size()) if k_ == 3 else x.new(*size).zero_()
1.0.1,Normal use
1.0.1,"An entry check here, will catch on train side and translate side"
1.0.1,if requirements are not satisfied.
1.0.1,RNNDecoderState wraps hidden as a tuple.
1.0.1,fh -> (layers*directions) x batch x dim
1.0.1,NOTE: We need to trim the vocab to remove any unk tokens that
1.0.1,were not originally here.
1.0.1,!/usr/bin/env python
1.0.1,!/usr/bin/env python
1.0.1,!/usr/bin/env python
1.0.1,-*- coding: utf-8 -*-
1.0.1,!/usr/bin/env python
1.0.1,-*- coding: utf-8 -*-
1.0.1,create one counter per shard
1.0.1,"every corpus has shards, no new one"
1.0.1,!/usr/bin/env python
1.0.1,!/usr/bin/env python
1.0.1,Load checkpoint if we resume from a previous training.
1.0.1,check for code where vocab is saved instead of fields
1.0.1,(in the future this will be done in a smarter way)
1.0.1,Create a thread to listen for errors in the child processes.
1.0.1,Train with multiprocessing.
1.0.1,generator_to_serve = iter(generator_to_serve)
1.0.1,hack to dodge unpicklable `dict_keys`
1.0.1,"propagate exception to parent process, keeping original traceback"
1.0.1,magic indices
1.0.1,result caching
1.0.1,add one to account for BOS. Don't account for EOS because hitting
1.0.1,this implies it hasn't been found.
1.0.1,we don't block nothing if the user doesn't want it
1.0.1,we can't block nothing beam's too short
1.0.1,we check paths one by one
1.0.1,we don't forbid nothing if the user doesn't want it
1.0.1,we can't forbid nothing if beam's too short
1.0.1,Reordering forbidden_tokens following beam selection
1.0.1,We rebuild a dict to ensure we get the value and not the pointer
1.0.1,Grabing the newly selected tokens and associated ngram
1.0.1,skip the blocking if any token in current_ngram is excluded
1.0.1,"For temp=0.0, take the argmax to avoid divide-by-zero errors."
1.0.1,keep_topk=1 is also equivalent to argmax.
1.0.1,Set all logits that are not in the top-k to -10000.
1.0.1,This puts the probabilities close to 0.
1.0.1,"shape: (sum(~ self.is_finished), 1)"
1.0.1,!/usr/bin/env python
1.0.1,Maintains the longest src and tgt length in the current batch
1.0.1,Reset current longest length at a new batch (count=1)
1.0.1,max_tgt_in_batch = 0
1.0.1,Src: [<bos> w1 ... wN <eos>]
1.0.1,Tgt: [w1 ... wM <eos>]
1.0.1,for debugging
1.0.1,Statistics
1.0.1,(0) add BOS and padding to tgt prediction
1.0.1,(1) Encoder forward.
1.0.1,(2) Repeat src objects `n_best` times.
1.0.1,"We use batch_size x n_best, get ``(src_len, batch * n_best, nfeat)``"
1.0.1,"(3) Init decoder with n_best src,"
1.0.1,"reshape tgt to ``(len, batch * n_best, nfeat)``"
1.0.1,masked_select
1.0.1,get aligned src id for each prediction's valid tgt tokens
1.0.1,TODO: support these blacklisted features
1.0.1,Turn any copied words into UNKs.
1.0.1,"Decoder forward, takes [tgt_len, batch, nfeats] as input"
1.0.1,"and [src_len, batch, hidden] as memory_bank"
1.0.1,"in case of inference tgt_len = 1, batch = beam times batch_size"
1.0.1,"in case of Gold Scoring tgt_len = actual length, batch = 1 batch"
1.0.1,Generator forward.
1.0.1,"returns [(batch_size x beam_size) , vocab ] when 1 step"
1.0.1,"or [ tgt_len, batch_size, vocab ] when full sentence"
1.0.1,"here we have scores [tgt_lenxbatch, vocab] or [beamxbatch, vocab]"
1.0.1,"returns [(batch_size x beam_size) , vocab ] when 1 step"
1.0.1,"or [ tgt_len, batch_size, vocab ] when full sentence"
1.0.1,(0) Prep the components of the search.
1.0.1,(1) Run the encoder on the src.
1.0.1,(2) prep decode_strategy. Possibly repeat src objects.
1.0.1,(3) Begin decoding step by step:
1.0.1,Reorder states.
1.0.1,beam parameters
1.0.1,result caching
1.0.1,beam state
1.0.1,BoolTensor was introduced in pytorch 1.2
1.0.1,"""global state"" of the old beam"
1.0.1,buffers for the topk scores and 'backpointer'
1.0.1,for testing
1.0.1,using integer division to get an integer _B without casting
1.0.1,force the output to be longer than self.min_length
1.0.1,Multiply probs by the beam probability.
1.0.1,"if the sequence ends now, then the penalty is the current"
1.0.1,"length + 1, to include the EOS token"
1.0.1,Avoid any direction that would repeat unwanted ngrams
1.0.1,Flatten probs into a list of possibilities.
1.0.1,Recover log probs.
1.0.1,Length penalty is just a scalar. It doesn't matter if it's applied
1.0.1,before or after the topk.
1.0.1,Resolve beam origin and map to batch index flat representation.
1.0.1,Append last prediction.
1.0.1,update global state (step == 1)
1.0.1,update global state (step > 1)
1.0.1,"shape: (batch_size x beam_size, 1)"
1.0.1,Penalize beams that finished.
1.0.1,"on real data (newstest2017) with the pretrained transformer,"
1.0.1,it's faster to not move this back to the original device
1.0.1,Store finished hypotheses for this batch.
1.0.1,End condition is the top beam finished and we can return
1.0.1,n_best hypotheses.
1.0.1,"If all sentences are translated, no need to go further."
1.0.1,Remove finished batches for the next step.
1.0.1,Term will be subtracted from probability
1.0.1,Probability will be divided by this
1.0.1,these warnings indicate that either the alpha/beta
1.0.1,"forces a penalty to be a no-op, or a penalty is a no-op but"
1.0.1,the alpha/beta would suggest otherwise.
1.0.1,using some length penalty
1.0.1,using some coverage penalty
1.0.1,!/usr/bin/env python
1.0.1,semaphore doesn't have a timeout arg in Python 2.7
1.0.1,backwards compatibility for confs
1.0.1,load can be called multiple times: modify copy
1.0.1,NOTE: translator returns lists of `n_best` list
1.0.1,build back results with empty texts
1.0.1,output contain alignment
1.0.1,Below are all the different penalty terms implemented so far.
1.0.1,Subtract coverage penalty from topk log probs.
1.0.1,Divide topk log probs by length penalty.
1.0.1,Sorting
1.0.1,Chinese segmentation
1.0.1,Chinese simplify -> Chinese traditional standard
1.0.1,Chinese simplify -> Chinese traditional (HongKong)
1.0.1,Chinese simplify -> Chinese traditional (Taiwan)
1.0.1,Chinese traditional -> Chinese simplify (v1)
1.0.1,Chinese traditional -> Chinese simplify (v2)
1.0.0,!/usr/bin/env python
1.0.0,!/usr/bin/env python
1.0.0,!/usr/bin/env python
1.0.0,!/usr/bin/env python
1.0.0,!/usr/bin/env python
1.0.0,!/usr/bin/env python3
1.0.0,-*- coding: utf-8 -*-
1.0.0,
1.0.0,"OpenNMT-py documentation build configuration file, created by"
1.0.0,sphinx-quickstart on Sun Dec 17 12:07:14 2017.
1.0.0,
1.0.0,This file is execfile()d with the current directory set to its
1.0.0,containing dir.
1.0.0,
1.0.0,Note that not all possible configuration values are present in this
1.0.0,autogenerated file.
1.0.0,
1.0.0,All configuration values have a default; values that are commented out
1.0.0,serve to show the default.
1.0.0,"If extensions (or modules to document with autodoc) are in another directory,"
1.0.0,add these directories to sys.path here. If the directory is relative to the
1.0.0,"documentation root, use os.path.abspath to make it absolute, like shown here."
1.0.0,
1.0.0,import os
1.0.0,import sys
1.0.0,"sys.path.insert(0, os.path.abspath('.'))"
1.0.0,-- General configuration ------------------------------------------------
1.0.0,"If your documentation needs a minimal Sphinx version, state it here."
1.0.0,
1.0.0,needs_sphinx = '1.0'
1.0.0,"Add any Sphinx extension module names here, as strings. They can be"
1.0.0,extensions coming with Sphinx (named 'sphinx.ext.*') or your custom
1.0.0,ones.
1.0.0,Show base classes
1.0.0,"Use ""variables"" section for Attributes instead of weird block things"
1.0.0,mimicking the function style.
1.0.0,"Add any paths that contain templates here, relative to this directory."
1.0.0,The suffix(es) of source filenames.
1.0.0,You can specify multiple suffix as a list of string:
1.0.0,
1.0.0,"source_suffix = ['.rst', '.md']"
1.0.0,The master toctree document.
1.0.0,General information about the project.
1.0.0,"The version info for the project you're documenting, acts as replacement for"
1.0.0,"|version| and |release|, also used in various other places throughout the"
1.0.0,built documents.
1.0.0,
1.0.0,The short X.Y version.
1.0.0,"The full version, including alpha/beta/rc tags."
1.0.0,The language for content autogenerated by Sphinx. Refer to documentation
1.0.0,for a list of supported languages.
1.0.0,
1.0.0,This is also used if you do content translation via gettext catalogs.
1.0.0,"Usually you set ""language"" from the command line for these cases."
1.0.0,"List of patterns, relative to source directory, that match files and"
1.0.0,directories to ignore when looking for source files.
1.0.0,This patterns also effect to html_static_path and html_extra_path
1.0.0,The name of the Pygments (syntax highlighting) style to use.
1.0.0,"If true, `todo` and `todoList` produce output, else they produce nothing."
1.0.0,-- Options for HTML output ----------------------------------------------
1.0.0,The theme to use for HTML and HTML Help pages.  See the documentation for
1.0.0,a list of builtin themes.
1.0.0,
1.0.0,html_theme = 'sphinx_materialdesign_theme'
1.0.0,html_theme_path = [sphinx_materialdesign_theme.get_path()]
1.0.0,Theme options are theme-specific and customize the look and feel of a theme
1.0.0,"further.  For a list of options available for each theme, see the"
1.0.0,documentation.
1.0.0,
1.0.0,html_theme_options = {}
1.0.0,"Add any paths that contain custom static files (such as style sheets) here,"
1.0.0,"relative to this directory. They are copied after the builtin static files,"
1.0.0,"so a file named ""default.css"" will overwrite the builtin ""default.css""."
1.0.0,"Custom sidebar templates, must be a dictionary that maps document names"
1.0.0,to template names.
1.0.0,
1.0.0,This is required for the alabaster theme
1.0.0,refs: http://alabaster.readthedocs.io/en/latest/installation.html#sidebars
1.0.0,-- Options for HTMLHelp output ------------------------------------------
1.0.0,Output file base name for HTML help builder.
1.0.0,-- Options for LaTeX output ---------------------------------------------
1.0.0,The paper size ('letterpaper' or 'a4paper').
1.0.0,
1.0.0,"'papersize': 'letterpaper',"
1.0.0,"The font size ('10pt', '11pt' or '12pt')."
1.0.0,
1.0.0,"'pointsize': '10pt',"
1.0.0,Additional stuff for the LaTeX preamble.
1.0.0,
1.0.0,"'preamble': '',"
1.0.0,Latex figure (float) alignment
1.0.0,
1.0.0,"'figure_align': 'htbp',"
1.0.0,Grouping the document tree into LaTeX files. List of tuples
1.0.0,"(source start file, target name, title,"
1.0.0,"author, documentclass [howto, manual, or own class])."
1.0.0,-- Options for manual page output ---------------------------------------
1.0.0,One entry per manual page. List of tuples
1.0.0,"(source start file, name, description, authors, manual section)."
1.0.0,-- Options for Texinfo output -------------------------------------------
1.0.0,Grouping the document tree into Texinfo files. List of tuples
1.0.0,"(source start file, target name, title, author,"
1.0.0,"dir menu entry, description, category)"
1.0.0,degenerate case
1.0.0,cache the features
1.0.0,mp queues don't work well between procs unless they're from a manager
1.0.0,each device has its own saver so that reconstructing is easier
1.0.0,!/usr/bin/env python
1.0.0,-*- coding: utf-8 -*-
1.0.0,is this reachable?
1.0.0,Read in embeddings
1.0.0,Write to file
1.0.0,!/usr/bin/env python
1.0.0,-*- coding: utf-8 -*-
1.0.0,"Add in default model arguments, possibly added since training."
1.0.0,-*- encoding: utf-8 -*-
1.0.0,!/usr/bin/env python
1.0.0,-*- coding: utf-8 -*-
1.0.0,Author: Rico Sennrich
1.0.0,flake8: noqa
1.0.0,This file is retrieved from https://github.com/rsennrich/subword-nmt
1.0.0,hack for python2/3 compatibility
1.0.0,check version information
1.0.0,some hacking to deal with duplicates (only consider first instance)
1.0.0,don't print end-of-word symbols
1.0.0,sys.stderr.write('cannot split {0} further.\n'.format(segment))
1.0.0,sys.stderr.write('OOV: {0}\n'.format(segment))
1.0.0,sys.stderr.write('OOV: {0}\n'.format(segment))
1.0.0,python 2/3 compatibility
1.0.0,read/write files as UTF-8
1.0.0,!/usr/bin/env python
1.0.0,!/usr/bin/env python
1.0.0,-*- coding: utf-8 -*-
1.0.0,Author: Rico Sennrich
1.0.0,flake8: noqa
1.0.0,This file is retrieved from https://github.com/rsennrich/subword-nmt
1.0.0,hack for python2/3 compatibility
1.0.0,"find all instances of pair, and update frequency/indices around it"
1.0.0,find first symbol
1.0.0,"if first symbol is followed by second symbol, we've found an occurrence of pair (old_word[i:i+2])"
1.0.0,"assuming a symbol sequence ""A B C"", if ""B C"" is merged, reduce the frequency of ""A B"""
1.0.0,"assuming a symbol sequence ""A B C B"", if ""B C"" is merged, reduce the frequency of ""C B""."
1.0.0,"however, skip this if the sequence is A B C B C, because the frequency of ""C B"" will be reduced by the previous code block"
1.0.0,find new pair
1.0.0,"assuming a symbol sequence ""A BC D"", if ""B C"" is merged, increase the frequency of ""A BC"""
1.0.0,"assuming a symbol sequence ""A BC B"", if ""B C"" is merged, increase the frequency of ""BC B"""
1.0.0,"however, if the sequence is A BC BC, skip this step because the count of ""BC BC"" will be incremented by the previous code block"
1.0.0,data structure of pair frequencies
1.0.0,index from pairs to words
1.0.0,version 0.2 changes the handling of the end-of-word token ('</w>');
1.0.0,version numbering allows bckward compatibility
1.0.0,"threshold is inspired by Zipfian assumption, but should only affect speed"
1.0.0,we probably missed the best pair because of pruning; go back to full statistics
1.0.0,"threshold is inspired by Zipfian assumption, but should only affect speed"
1.0.0,python 2/3 compatibility
1.0.0,read/write files as UTF-8
1.0.0,!/usr/bin/env python
1.0.0,for back compat when attention_dropout was not defined
1.0.0,Build embeddings.
1.0.0,Build encoder.
1.0.0,Build decoder.
1.0.0,Share the embedding matrix - preprocess with share_vocab required.
1.0.0,src/tgt vocab should be the same if `-share_vocab` is specified.
1.0.0,Build NMTModel(= encoder + decoder).
1.0.0,Build Generator.
1.0.0,Load the model states from checkpoint or initialize them.
1.0.0,This preserves backward-compat for models using customed layernorm
1.0.0,end of patch for backward compatibility
1.0.0,!/usr/bin/env python
1.0.0,NOTE: It's important that ``opt`` has been validated and updated
1.0.0,at this point.
1.0.0,Load checkpoint if we resume from a previous training.
1.0.0,check for code where vocab is saved instead of fields
1.0.0,(in the future this will be done in a smarter way)
1.0.0,"Report src and tgt vocab sizes, including for features"
1.0.0,Build model.
1.0.0,Build optimizer.
1.0.0,Build model saver
1.0.0,Embedding Options
1.0.0,Encoder-Decoder Options
1.0.0,"group.add('--residual', '-residual',   action=""store_true"","
1.0.0,"help=""Add residual connections between RNN layers."")"
1.0.0,Attention options
1.0.0,Alignement options
1.0.0,Generator and loss options.
1.0.0,Data options
1.0.0,"Dictionary options, for text corpus"
1.0.0,"if you want to pass an existing vocab.pt file, pass it to"
1.0.0,-src_vocab alone as it already contains tgt vocab.
1.0.0,"Truncation options, for text corpus"
1.0.0,Data processing options
1.0.0,Options most relevant to speech
1.0.0,Option most relevant to image input
1.0.0,GPU
1.0.0,Init options
1.0.0,Pretrained word vectors
1.0.0,Fixed word vectors
1.0.0,Optimization options
1.0.0,learning rate
1.0.0,Use Tensorboard for visualization during training
1.0.0,Options most relevant to speech
1.0.0,Option most relevant to image input
1.0.0,Options most relevant to summarization.
1.0.0,Alpha and Beta values for Google Length + Coverage penalty
1.0.0,"Described here: https://arxiv.org/pdf/1609.08144.pdf, Section 7"
1.0.0,Options most relevant to speech.
1.0.0,Option most relevant to image input
1.0.0,Copyright 2016 The Chromium Authors. All rights reserved.
1.0.0,Use of this source code is governed by a BSD-style license that can be
1.0.0,found in the LICENSE file.
1.0.0,"Get the key 'value' in the dict, or just use 'value'"
1.0.0,Basic attributes.
1.0.0,Set model in training mode.
1.0.0,UPDATE DROPOUT
1.0.0,Run patience mechanism
1.0.0,"If the patience has reached the limit, stop training"
1.0.0,swap model params w/ moving average
1.0.0,(and keep the original parameters)
1.0.0,Set model in validating mode.
1.0.0,F-prop through the model.
1.0.0,Compute loss.
1.0.0,Update statistics.
1.0.0,Set model back to training mode.
1.0.0,Truncated BPTT: reminder not compatible with accum > 1
1.0.0,1. Create truncated target.
1.0.0,2. F-prop all but generator.
1.0.0,3. Compute loss.
1.0.0,4. Update the parameters and statistics.
1.0.0,Multi GPU gradient gather
1.0.0,"If truncated, don't backprop fully."
1.0.0,TO CHECK
1.0.0,if dec_state is not None:
1.0.0,dec_state.detach()
1.0.0,"in case of multi step gradient accumulation,"
1.0.0,update only after accum batches
1.0.0,For Flake
1.0.0,we avoid padding while mean pooling
1.0.0,Initialize the bridge layer
1.0.0,"s_len, batch, emb_dim = emb.size()"
1.0.0,Lengths data is wrapped inside a Tensor.
1.0.0,"LSTM has hidden and cell state, other only one"
1.0.0,Total number of states
1.0.0,Build a linear layer for each
1.0.0,The encoder hidden is  (layers*directions) x batch x dim.
1.0.0,"s_len, batch, emb_dim = emb.size()"
1.0.0,Run the forward pass of every layer of the tranformer.
1.0.0,why is the model_opt.__dict__ check necessary?
1.0.0,"(batch_size, 64, imgH, imgW)"
1.0.0,layer 1
1.0.0,"(batch_size, 64, imgH/2, imgW/2)"
1.0.0,"(batch_size, 128, imgH/2, imgW/2)"
1.0.0,layer 2
1.0.0,"(batch_size, 128, imgH/2/2, imgW/2/2)"
1.0.0,"(batch_size, 256, imgH/2/2, imgW/2/2)"
1.0.0,layer 3
1.0.0,batch norm 1
1.0.0,"(batch_size, 256, imgH/2/2, imgW/2/2)"
1.0.0,layer4
1.0.0,"(batch_size, 256, imgH/2/2/2, imgW/2/2)"
1.0.0,"(batch_size, 512, imgH/2/2/2, imgW/2/2)"
1.0.0,layer 5
1.0.0,batch norm 2
1.0.0,"(batch_size, 512, imgH/2/2/2, imgW/2/2/2)"
1.0.0,"(batch_size, 512, imgH/2/2/2, imgW/2/2/2)"
1.0.0,"# (batch_size, 512, H, W)"
1.0.0,Dimensions and padding for constructing the word embedding matrix
1.0.0,Dimensions and padding for feature embedding matrices
1.0.0,(these have no effect if feat_vocab_sizes is empty)
1.0.0,The embedding matrix look-up tables. The first look-up table
1.0.0,"is for words. Subsequent ones are for features, if any exist."
1.0.0,The final output size of word + feature vectors. This can vary
1.0.0,from the word vector size if and only if features are defined.
1.0.0,This is the attribute you should access if you need to know
1.0.0,how big your embeddings are going to be.
1.0.0,The sequence of operations that converts the input sequence
1.0.0,into a sequence of embeddings. At minimum this consists of
1.0.0,looking up the embeddings for each word and feature in the
1.0.0,input. Model parameters may require the sequence to contain
1.0.0,additional operations as well.
1.0.0,features must use word_vec_size
1.0.0,features will use feat_vec_size
1.0.0,This class is mainly used by decoder.py for RNNs but also
1.0.0,by the CNN / transformer decoder when copy attention is used
1.0.0,CNN has its own attention mechanism ConvMultiStepAttention
1.0.0,Transformer has its own MultiHeadedAttention
1.0.0,mlp wants it with bias
1.0.0,Check input sizes
1.0.0,"(batch, t_len, d) x (batch, d, s_len) --> (batch, t_len, s_len)"
1.0.0,"(batch, t_len, s_len, d)"
1.0.0,one step input
1.0.0,"compute attention scores, as in Luong et al."
1.0.0,Softmax or sparsemax to normalize attention weights
1.0.0,each context vector c_t is the weighted average
1.0.0,over all the source hidden states
1.0.0,concatenate
1.0.0,Check output sizes
1.0.0,Check output sizes
1.0.0,clamping necessary because of numerical errors: loss should be lower
1.0.0,"bounded by zero, but negative values near zero are possible without"
1.0.0,the clamp
1.0.0,from onmt.utils.misc import aeq
1.0.0,CHECKS
1.0.0,"batch, k_len, d = key.size()"
1.0.0,"batch_, k_len_, d_ = value.size()"
1.0.0,"aeq(batch, batch_)"
1.0.0,"aeq(k_len, k_len_)"
1.0.0,"aeq(d, d_)"
1.0.0,"batch_, q_len, d_ = query.size()"
1.0.0,"aeq(batch, batch_)"
1.0.0,"aeq(d, d_)"
1.0.0,"aeq(self.model_dim % 8, 0)"
1.0.0,if mask is not None:
1.0.0,"batch_, q_len_, k_len_ = mask.size()"
1.0.0,"aeq(batch_, batch)"
1.0.0,"aeq(k_len_, k_len)"
1.0.0,aeq(q_len_ == q_len)
1.0.0,END CHECKS
1.0.0,"1) Project key, value, and query."
1.0.0,1 or key_len x key_len
1.0.0,1 or key_len x key_len x dim_per_head
1.0.0,1 or key_len x key_len x dim_per_head
1.0.0,2) Calculate and scale scores.
1.0.0,batch x num_heads x query_len x key_len
1.0.0,3) Apply attention dropout and compute context vectors.
1.0.0,CHECK
1.0.0,"batch_, q_len_, d_ = output.size()"
1.0.0,"aeq(q_len, q_len_)"
1.0.0,"aeq(batch, batch_)"
1.0.0,"aeq(d, d_)"
1.0.0,Return multi-head attn
1.0.0,At the moment this class is only used by embeddings.Embeddings look-up tables
1.0.0,-*- coding: utf-8 -*-
1.0.0,checks
1.0.0,"batch, channel, height, width = base_target_emb.size()"
1.0.0,"batch_, channel_, height_, width_ = input_from_dec.size()"
1.0.0,"enc_batch, enc_channel, enc_height = encoder_out_top.size()"
1.0.0,"enc_batch_, enc_channel_, enc_height_ = encoder_out_combine.size()"
1.0.0,out_features * in_features
1.0.0,norm is out_features * 1
1.0.0,batch_size * out_features
1.0.0,out_features
1.0.0,out_features
1.0.0,batch_size * out_features
1.0.0,"out_channels, in_channels // groups, * kernel_size"
1.0.0,out_features
1.0.0,This is used nowhere in the code at the moment (Vincent Nguyen 05/18/2018)
1.0.0,"in_channels, out_channels, *kernel_size"
1.0.0,"in_channels, out_channels, *kernel_size"
1.0.0,"self.out_channels, 1"
1.0.0,out_features
1.0.0,out_features
1.0.0,store roots on diagonal
1.0.0,CHECKS
1.0.0,Original probabilities.
1.0.0,Probability of copying p(z=1) batch.
1.0.0,Probability of not copying: p_{word}(w) * (1 - p(z))
1.0.0,probabilities assigned by the model to the gold targets
1.0.0,probability of tokens copied from source
1.0.0,Set scores for unk to 0 and add eps
1.0.0,find the indices in which you do not use the copy mechanism
1.0.0,Drop padding.
1.0.0,this block does not depend on the loss value computed above
1.0.0,and is used only for stats
1.0.0,this block does not depend on the loss value computed above
1.0.0,and is used only for stats
1.0.0,Correct target copy token instead of <unk>
1.0.0,tgt[i] = align[i] + len(tgt_vocab)
1.0.0,for i such that tgt[i] == 0 and align[i] != 0
1.0.0,Compute sum of perplexities for stats
1.0.0,this part looks like it belongs in CopyGeneratorLoss
1.0.0,Compute Loss as NLL divided by seq length
1.0.0,Compute Total Loss per sequence in batch
1.0.0,Divide by length of each sequence and sum
1.0.0,initialize fields at the top of each unit test to prevent
1.0.0,any undesired stateful effects
1.0.0,"this test touches the file system, so it could be considered an"
1.0.0,integration test
1.0.0,write utf-8 bytes
1.0.0,batch 0 will always predict EOS. The other batches will predict
1.0.0,non-eos scores.
1.0.0,"""best"" prediction is eos - that should be blocked"
1.0.0,include at least one prediction OTHER than EOS
1.0.0,that is greater than -1e20
1.0.0,now batch 0 has ended and no others have
1.0.0,initial step
1.0.0,batch 0 dies on step 0
1.0.0,include at least one prediction OTHER than EOS
1.0.0,that is greater than -1e20
1.0.0,step 2
1.0.0,(old) batch 8 dies on step 1
1.0.0,step 3
1.0.0,everything dies
1.0.0,initial step
1.0.0,batch 0 dies on step 0
1.0.0,include at least one prediction OTHER than EOS
1.0.0,that is greater than -1e20
1.0.0,step 2
1.0.0,(old) batch 8 dies on step 1
1.0.0,step 3
1.0.0,everything dies
1.0.0,illegal_weights_mask = torch.ByteTensor([
1.0.0,"[0, 0, 0, 0, 0, 0, 0],"
1.0.0,"[0, 0, 0, 1, 1, 1, 1],"
1.0.0,"[0, 0, 0, 0, 0, 1, 1],"
1.0.0,"[0, 0, 1, 1, 1, 1, 1]])"
1.0.0,TODO: fix for pytorch 0.3
1.0.0,illegal_weights = alignments.masked_select(illegal_weights_mask)
1.0.0,"self.assertEqual(0.0, illegal_weights.data.sum())"
1.0.0,this could be considered an integration test because it touches
1.0.0,the filesystem for the config file (and the models)
1.0.0,-*- coding: utf-8 -*-
1.0.0,tests pad and numericalize integration
1.0.0,tests pad and numericalize integration
1.0.0,"this test touches the file system, so it could be considered an"
1.0.0,integration test
1.0.0,file to hold full paths to audio data
1.0.0,file to hold audio paths relative to _AUDIO_DATA_DIR (i.e. file names)
1.0.0,it's ok if non-audio files co-exist with audio files in the data dir
1.0.0,"dividing gets the noise in [-1, 1]"
1.0.0,"this test touches the file system, so it could be considered an"
1.0.0,integration test
1.0.0,file to hold full paths to image data
1.0.0,file to hold image paths relative to _IMG_DATA_DIR (i.e. file names)
1.0.0,it's ok if non-image files co-exist with image files in the data dir
1.0.0,all beams repeat (beam >= 1 repeat dummy scores)
1.0.0,predict repeat_idx over and over again
1.0.0,"before repeat, scores are either 0 or -inf"
1.0.0,"on repeat, `repeat_idx` score is BLOCKED_SCORE"
1.0.0,"(but it's still the best score, thus we have"
1.0.0,"[BLOCKED_SCORE, -inf, -inf, -inf, -inf]"
1.0.0,repetitions keeps maximizing score
1.0.0,"index 0 has been blocked, so repeating=>+0.0 score"
1.0.0,other indexes are -inf so repeating=>BLOCKED_SCORE
1.0.0,which is higher
1.0.0,beam 0 and beam >=2 will repeat (beam >= 2 repeat dummy scores)
1.0.0,non-interesting beams are going to get dummy values
1.0.0,"on initial round, only predicted scores for beam 0"
1.0.0,matter. Make two predictions. Top one will be repeated
1.0.0,"in beam zero, second one will live on in beam 1."
1.0.0,predict the same thing in beam 0
1.0.0,continue pushing around what beam 1 predicts
1.0.0,"now beam 0 dies (along with the others), beam 1 -> beam 0"
1.0.0,"now beam 0 dies (along with the others), beam 1 -> beam 0"
1.0.0,beam 0 and beam >= 2 will repeat (beam 2 repeats excluded idx)
1.0.0,non-interesting beams are going to get dummy values
1.0.0,predict the same thing in beam 0
1.0.0,continue pushing around what beam 1 predicts
1.0.0,predict the allowed-repeat again in beam 2
1.0.0,"now beam 0 dies, beam 1 -> beam 0, beam 2 -> beam 1"
1.0.0,and the rest die
1.0.0,"since all preds after i=0 are 0, we can check"
1.0.0,that the beam is the correct idx by checking that
1.0.0,the curr score is the initial score
1.0.0,beam 0 will always predict EOS. The other beams will predict
1.0.0,non-eos scores.
1.0.0,non-interesting beams are going to get dummy values
1.0.0,"""best"" prediction is eos - that should be blocked"
1.0.0,include at least beam_sz predictions OTHER than EOS
1.0.0,that are greater than -1e20
1.0.0,predict eos in beam 0
1.0.0,provide beam_sz other good predictions
1.0.0,now the top beam has ended and no others have
1.0.0,"not of interest, but want to make sure it keeps running"
1.0.0,since only beam 0 terminates and n_best = 2
1.0.0,"this is also a test that when block_ngram_repeat=0,"
1.0.0,repeating is acceptable
1.0.0,non-interesting beams are going to get dummy values
1.0.0,"""best"" prediction is eos - that should be blocked"
1.0.0,include at least beam_sz predictions OTHER than EOS
1.0.0,that are greater than -1e20
1.0.0,predict eos in beam 1
1.0.0,provide beam_sz other good predictions in other beams
1.0.0,provide beam_sz other good predictions in other beams
1.0.0,beam 1 dies on min_length
1.0.0,beam 0 dies on the step after beam 1 dies
1.0.0,"inp_lens is tiled in initialize, reassign to make attn match"
1.0.0,non-interesting beams are going to get dummy values
1.0.0,"""best"" prediction is eos - that should be blocked"
1.0.0,include at least beam_sz predictions OTHER than EOS
1.0.0,that are greater than -1e20
1.0.0,predict eos in beam 1
1.0.0,provide beam_sz other good predictions in other beams
1.0.0,provide beam_sz other good predictions in other beams
1.0.0,no top beams are finished yet
1.0.0,beam 1 dies on min_length
1.0.0,no top beams are finished yet
1.0.0,beam 0 dies on the step after beam 1 dies
1.0.0,top beam is finished now so there are attentions
1.0.0,two beams are finished in each batch
1.0.0,second dim is cut down to the non-padded src length
1.0.0,first dim is equal to the time of death
1.0.0,(beam 0 died at current step - adjust for SOS)
1.0.0,(beam 1 died at last step - adjust for SOS)
1.0.0,behavior gets weird when beam is already done so just stop
1.0.0,this is just test_beam.TestBeamAgainstReferenceCase repeated
1.0.0,in each batch.
1.0.0,"init_preds: [4, 3, 5, 6, 7] - no EOS's"
1.0.0,no EOS's yet
1.0.0,"[5, 3, 2, 6, 0], so beam 2 predicts EOS!"
1.0.0,assumes beam 2 finished on last step
1.0.0,ended beam 2 shouldn't continue
1.0.0,"[2, 5, 3, 6, 0] repeat self.BATCH_SZ, so beam 0 predicts EOS!"
1.0.0,"[-2.4879, -3.8910, -4.1010, -4.2010, -4.4010] repeat self.BATCH_SZ"
1.0.0,another beam is finished in all batches
1.0.0,new beam 0 finished
1.0.0,new beam 0 is old beam 3
1.0.0,assumes beam 0 finished on last step
1.0.0,"[5, 2, 6, 1, 0] repeat self.BATCH_SZ, so beam 1 predicts EOS!"
1.0.0,new beam 1 finished
1.0.0,new beam 1 is old beam 4
1.0.0,this could be considered an integration test because it tests
1.0.0,interactions between the GNMT scorer and the beam
1.0.0,"-data option is required, but not used in this test, so dummy."
1.0.0,len x batch x nfeat
1.0.0,batch x c x h x w
1.0.0,batch x 1 x nfft x t
1.0.0,Initialize vectors to compare size with
1.0.0,Ensure correct sizes and types
1.0.0,Make sure that output has the correct size and type
1.0.0,Make sure that output has the correct size and type
1.0.0,Make sure that output has the correct size and type
1.0.0,"[('encoder_type', 'transformer'),"
1.0.0,"('word_vec_size', 16), ('rnn_size', 16)],"
1.0.0,""""""" Only do SRU test if requirment is safisfied. """""""
1.0.0,SRU doesn't support input_feed.
1.0.0,"when reasonable, set audio_enc_pooling to 2"
1.0.0,Need lengths >= audio_enc_pooling**n_layers.
1.0.0,"That condition is unrealistic for large n_layers,"
1.0.0,so leave audio_enc_pooling at 1.
1.0.0,first check there's nothing unexpectedly not trainable
1.0.0,ok: word embeddings shouldn't be trainable
1.0.0,if word vecs are fixed
1.0.0,ok: positional encodings shouldn't be trainable
1.0.0,then check nothing unexpectedly trainable
1.0.0,!/usr/bin/env python
1.0.0,-*- coding: utf-8 -*-
1.0.0,Remove the generated *pt files.
1.0.0,Test image preprocessing
1.0.0,Test audio preprocessing
1.0.0,Decoder state
1.0.0,Build the RNN.
1.0.0,Set up the context gate.
1.0.0,Set up the standard attention.
1.0.0,The encoder hidden is  (layers*directions) x batch x dim.
1.0.0,We need to convert it to layers x batch x (directions*dim).
1.0.0,Init the input feed.
1.0.0,Update the state with the result.
1.0.0,Concatenates sequence of tensors along a new dimension.
1.0.0,NOTE: v0.3 to 0.4: dec_outs / attns[*] may not be list
1.0.0,(in particular in case of SRU) it was not raising error in 0.3
1.0.0,since stack(Variable) was allowed.
1.0.0,"In 0.4, SRU returns a tensor that shouldn't be stacke"
1.0.0,Check
1.0.0,Calculate the attention.
1.0.0,Calculate the context gate.
1.0.0,Additional args check.
1.0.0,END Additional args check.
1.0.0,Input feed concatenates hidden state with
1.0.0,input at every time step.
1.0.0,TODO: context gate should be employed
1.0.0,instead of second RNN transform.
1.0.0,Update the coverage attention.
1.0.0,Decoder State
1.0.0,CNNDecoder has its own attention mechanism.
1.0.0,Set up a separate copy attention layer if needed.
1.0.0,The output of CNNEncoder.
1.0.0,The combination of output of CNNEncoder and source embeddings.
1.0.0,Process the result and update the attentions.
1.0.0,Update the state.
1.0.0,TODO change the way attns is returned dict => list or tuple (onnx)
1.0.0,Memory_lengths is a single tensor shared between all models.
1.0.0,This assumption will not hold if Translator is modified
1.0.0,to calculate memory_lengths as something other than the length
1.0.0,of the input.
1.0.0,"return _, (B, Q_len, K_len)"
1.0.0,"layer average attention across heads, get ``(B, Q, K)``"
1.0.0,"Case 1: no full_context, no align heads -> layer avg baseline"
1.0.0,"Case 2: no full_context, 1 align heads -> guided align"
1.0.0,"Case 3: full_context, 1 align heads -> full cte guided align"
1.0.0,TODO: change 1 to T as T could be 1 or tgt_len
1.0.0,BoolTensor was introduced in pytorch 1.2
1.0.0,Decoder State
1.0.0,"previously, there was a GlobalAttention module here for copy"
1.0.0,"attention. But it was never actually used -- the ""copy"" attention"
1.0.0,just reuses the context attention.
1.0.0,"attns[""align""] = torch.stack(attn_aligns, 0).mean(0)  # All avg"
1.0.0,TODO change the way attns is returned dict => list or tuple (onnx)
1.0.0,"buffer size in bytes, determine equiv. # of elements based on data type"
1.0.0,copy tensors into buffer_t
1.0.0,all-reduce and rescale
1.0.0,copy all-reduced buffer back into tensors
1.0.0,"tensor is bigger than buffer, all-reduce and rescale directly"
1.0.0,"buffer is full, all-reduce and replace buffer with grad"
1.0.0,add tensor to buffer
1.0.0,TODO: Find a better way to check for sparse gradients.
1.0.0,we use here a FusedAdam() copy of an old Apex repo
1.0.0,In this case use the new AMP API from apex
1.0.0,In this case use the old FusedAdam with FP16_optimizer wrapper
1.0.0,Load everything from the checkpoint.
1.0.0,Build everything from scratch.
1.0.0,"Reset optimizer, keep options."
1.0.0,"Reset options, keep optimizer."
1.0.0,State can be partially restored.
1.0.0,Code below is an implementation of https://arxiv.org/pdf/1804.04235.pdf
1.0.0,inspired but modified from https://github.com/DeadAt0m/adafactor-pytorch
1.0.0,backward compatibility
1.0.0,assuming a list/generator of parameter means single group
1.0.0,compute combined scale factor for this group
1.0.0,norm is in fact norm*scale
1.0.0,note: p.grad should not ever be set for correct operation of
1.0.0,mixed precision optimizer that sometimes sends None gradients
1.0.0,State initialization
1.0.0,Exponential moving average of gradient values
1.0.0,Exponential moving average of squared gradient values
1.0.0,-*- coding: utf-8 -*-
1.0.0,if the loss function operates on vectors of raw logits instead of
1.0.0,"probabilities, only the first part of the generator needs to be"
1.0.0,"passed to the NMTLossCompute. At the moment, the only supported"
1.0.0,loss function of this kind is the sparsemax loss.
1.0.0,"attn_align should be in (batch_size, pad_tgt_size, pad_src_size)"
1.0.0,"align_idx should be a Tensor in size([N, 3]), N is total number"
1.0.0,"of align src-tgt pair in current batch, each as"
1.0.0,"['sent_NÂ°_in_batch', 'tgt_id+1', 'src_id'] (check AlignField)"
1.0.0,NOTE: tgt-src ref alignement that in range_ of shard
1.0.0,(coherent with batch.tgt)
1.0.0,"align_head contains value in [0, 1) presenting attn prob,"
1.0.0,0 was resulted by the context attention src_pad_mask
1.0.0,"So, the correspand position in ref_align should also be 0"
1.0.0,"Therefore, clip align_head to > 1e-18 should be bias free."
1.0.0,non_none: the subdict of the state dictionary where the values
1.0.0,are not None.
1.0.0,"Now, the iteration:"
1.0.0,state is a dictionary of sequences of tensor-like but we
1.0.0,want a sequence of dictionaries of tensors.
1.0.0,"First, unzip the dictionary into a sequence of keys and a"
1.0.0,sequence of tensor-like sequences.
1.0.0,"Now, yield a dictionary for each shard. The keys are always"
1.0.0,the same. values is a sequence of length #keys where each
1.0.0,element is a sequence of length #shards. We want to iterate
1.0.0,"over the shards, not over the keys: therefore, the values need"
1.0.0,to be re-zipped by shard and then each shard can be paired
1.0.0,with the keys.
1.0.0,Assumed backprop'd
1.0.0,this check is here because audio allows the encoder and decoder to
1.0.0,"be different sizes, but other model types do not yet"
1.0.0,"Load default opt values, then overwrite with the opts in"
1.0.0,"the checkpoint. That way, if there are new options added,"
1.0.0,the defaults are used.
1.0.0,Don't do anything
1.0.0,Update best score of each criteria
1.0.0,Reset tolerance
1.0.0,Update current status
1.0.0,Decrease tolerance
1.0.0,Log
1.0.0,Log
1.0.0,Get a list of world_size lists with len(stat_list) Statistics objects
1.0.0,SRU doesn't support PackedSequence.
1.0.0,-*- coding: utf-8 -*-
1.0.0,threshold on 1 to avoid div by 0
1.0.0,treat alignment matrix one by one as each have different lengths
1.0.0,get valid alignment (sub-matrix from full paded aligment matrix)
1.0.0,-*- coding: utf-8 -*-
1.0.0,this one is needed for torchtext random call (shuffled iterator)
1.0.0,in multi gpu it ensures datasets are read in the same order
1.0.0,some cudnn methods can be random even after fixing the seed
1.0.0,unless you tell it to be deterministic
1.0.0,These ensure same initialization in multi gpu mode
1.0.0,Shift values to be >= 0
1.0.0,we need to check the model path + any tokenizer path
1.0.0,coding: utf-8
1.0.0,make a small vocab containing just the tokens in the source sequence
1.0.0,Map source tokens to indices in the dynamic dict.
1.0.0,self.src_vocabs is used in collapse_copy_scores and Translator.py
1.0.0,this assumes src_field and tgt_field are both text
1.0.0,fields needs to have only keys that examples have as attrs
1.0.0,avoid infinite recursion when fields isn't defined
1.0.0,-*- coding: utf-8 -*-
1.0.0,backwards compatibility
1.0.0,monkey-patch to make torchtext Vocab's pickleable
1.0.0,"+1 for tgt side to keep coherent after ""bos"" padding,"
1.0.0,"register ['NÂ°_in_batch', 'tgt_id+1', 'src_id']"
1.0.0,"List[Tuple[str, Vocab]] -> List[Tuple[str, Field]]"
1.0.0,"-> dict[str, Field]"
1.0.0,"Dict[str, List[Tuple[str, Field]]]"
1.0.0,doesn't change structure - don't return early.
1.0.0,"Dict[str, List[Tuple[str, Field]]] -> List[Tuple[str, Field]]"
1.0.0,"-> dict[str, Field]"
1.0.0,"if tgt isn't using TextMultiField, then no text field is."
1.0.0,this is basically copy-pasted from torchtext.
1.0.0,counters changes in place
1.0.0,keep the order of tokens specified in the vocab file by
1.0.0,adding them to the counter with decreasing counting values
1.0.0,`tgt_vocab_size` is ignored when sharing vocabularies
1.0.0,return vocab to dump with standard name
1.0.0,empty train_dataset_files so that vocab is only loaded from
1.0.0,"given paths in src_vocab_path, tgt_vocab_path"
1.0.0,Load vocabulary
1.0.0,Drop the none-using from memory but keep the last
1.0.0,"in the long run, shouldn't it be possible to do this by calling"
1.0.0,build_vocab with both the src and tgt data?
1.0.0,fast-forward if loaded from state
1.0.0,NOTE: `rnn.pack_padded_sequence` requires that a
1.0.0,"minibatch be sorted by decreasing order, which"
1.0.0,requires reversing relative to typical sort keys
1.0.0,Temporarily load one shard to retrieve sort_key for data_type
1.0.0,"NOTE: This is causing some issues for consumer/producer,"
1.0.0,as we may still have some of those examples in some queue
1.0.0,cur_dataset.examples = None
1.0.0,gc.collect()
1.0.0,del cur_dataset
1.0.0,gc.collect()
1.0.0,Cycle through the shards indefinitely.
1.0.0,"When the dataset is not repeated, we might need to ensure that"
1.0.0,the number of returned batches is the multiple of a given value.
1.0.0,This is important for multi GPU training to ensure that all
1.0.0,workers have the same number of batches to process.
1.0.0,Maintains the longest src and tgt length in the current batch
1.0.0,Reset current longest length at a new batch (count=1)
1.0.0,Src: [<bos> w1 ... wN <eos>]
1.0.0,Tgt: [w1 ... wM <eos>]
1.0.0,-*- coding: utf-8 -*-
1.0.0,imports of datatype-specific dependencies
1.0.0,torchaudio loading options recently changed. It's probably
1.0.0,straightforward to rewrite the audio handling to make use of
1.0.0,"up-to-date torchaudio, but in the meantime there is a legacy"
1.0.0,method which uses the old defaults
1.0.0,STFT
1.0.0,-*- coding: utf-8 -*-
1.0.0,domain specific dependencies
1.0.0,coding: utf-8
1.0.0,several data readers need optional dependencies. There's no
1.0.0,appropriate builtin exception
1.0.0,-*- coding: utf-8 -*-
1.0.0,mix this with partial
1.0.0,batch (list(list(list))): batch_size x len(self.fields) x seq_len
1.0.0,lengths: batch_size
1.0.0,data: seq_len x batch_size x len(self.fields)
1.0.0,flake8: noqa
1.0.0,For command-line option parsing
1.0.0,"Check pass, set the args."
1.0.0,"This SRU version implements its own cuda-level optimization,"
1.0.0,so it requires that:
1.0.0,1. `cupy` and `pynvrtc` python package installed.
1.0.0,2. pytorch is built with cuda support.
1.0.0,3. library path set: export LD_LIBRARY_PATH=<cuda lib path>.
1.0.0,Check 1.
1.0.0,Check 2.
1.0.0,Check 3.
1.0.0,This sets up device to use.
1.0.0,-> directions x batch x dim
1.0.0,For DEBUG
1.0.0,"size = (length, batch, x.size(-1)) \"
1.0.0,"if x.dim() == 3 else (batch, x.size(-1))"
1.0.0,grad_x = x.new(*x.size()) if k_ == 3 else x.new(*size).zero_()
1.0.0,Normal use
1.0.0,"An entry check here, will catch on train side and translate side"
1.0.0,if requirements are not satisfied.
1.0.0,RNNDecoderState wraps hidden as a tuple.
1.0.0,fh -> (layers*directions) x batch x dim
1.0.0,NOTE: We need to trim the vocab to remove any unk tokens that
1.0.0,were not originally here.
1.0.0,!/usr/bin/env python
1.0.0,!/usr/bin/env python
1.0.0,-*- coding: utf-8 -*-
1.0.0,!/usr/bin/env python
1.0.0,-*- coding: utf-8 -*-
1.0.0,create one counter per shard
1.0.0,"every corpus has shards, no new one"
1.0.0,!/usr/bin/env python
1.0.0,!/usr/bin/env python
1.0.0,Load checkpoint if we resume from a previous training.
1.0.0,check for code where vocab is saved instead of fields
1.0.0,(in the future this will be done in a smarter way)
1.0.0,Create a thread to listen for errors in the child processes.
1.0.0,Train with multiprocessing.
1.0.0,generator_to_serve = iter(generator_to_serve)
1.0.0,hack to dodge unpicklable `dict_keys`
1.0.0,"propagate exception to parent process, keeping original traceback"
1.0.0,magic indices
1.0.0,result caching
1.0.0,add one to account for BOS. Don't account for EOS because hitting
1.0.0,this implies it hasn't been found.
1.0.0,we don't block nothing if the user doesn't want it
1.0.0,we can't block nothing beam's too short
1.0.0,we check paths one by one
1.0.0,we don't forbid nothing if the user doesn't want it
1.0.0,we can't forbid nothing if beam's too short
1.0.0,Reordering forbidden_tokens following beam selection
1.0.0,We rebuild a dict to ensure we get the value and not the pointer
1.0.0,Grabing the newly selected tokens and associated ngram
1.0.0,skip the blocking if any token in current_ngram is excluded
1.0.0,"For temp=0.0, take the argmax to avoid divide-by-zero errors."
1.0.0,keep_topk=1 is also equivalent to argmax.
1.0.0,Set all logits that are not in the top-k to -10000.
1.0.0,This puts the probabilities close to 0.
1.0.0,"shape: (sum(~ self.is_finished), 1)"
1.0.0,!/usr/bin/env python
1.0.0,Maintains the longest src and tgt length in the current batch
1.0.0,Reset current longest length at a new batch (count=1)
1.0.0,max_tgt_in_batch = 0
1.0.0,Src: [<bos> w1 ... wN <eos>]
1.0.0,Tgt: [w1 ... wM <eos>]
1.0.0,for debugging
1.0.0,Statistics
1.0.0,(0) add BOS and padding to tgt prediction
1.0.0,(1) Encoder forward.
1.0.0,(2) Repeat src objects `n_best` times.
1.0.0,"We use batch_size x n_best, get ``(src_len, batch * n_best, nfeat)``"
1.0.0,"(3) Init decoder with n_best src,"
1.0.0,"reshape tgt to ``(len, batch * n_best, nfeat)``"
1.0.0,masked_select
1.0.0,get aligned src id for each prediction's valid tgt tokens
1.0.0,TODO: support these blacklisted features
1.0.0,Turn any copied words into UNKs.
1.0.0,"Decoder forward, takes [tgt_len, batch, nfeats] as input"
1.0.0,"and [src_len, batch, hidden] as memory_bank"
1.0.0,"in case of inference tgt_len = 1, batch = beam times batch_size"
1.0.0,"in case of Gold Scoring tgt_len = actual length, batch = 1 batch"
1.0.0,Generator forward.
1.0.0,"returns [(batch_size x beam_size) , vocab ] when 1 step"
1.0.0,"or [ tgt_len, batch_size, vocab ] when full sentence"
1.0.0,"here we have scores [tgt_lenxbatch, vocab] or [beamxbatch, vocab]"
1.0.0,"returns [(batch_size x beam_size) , vocab ] when 1 step"
1.0.0,"or [ tgt_len, batch_size, vocab ] when full sentence"
1.0.0,(0) Prep the components of the search.
1.0.0,(1) Run the encoder on the src.
1.0.0,(2) prep decode_strategy. Possibly repeat src objects.
1.0.0,(3) Begin decoding step by step:
1.0.0,Reorder states.
1.0.0,beam parameters
1.0.0,result caching
1.0.0,beam state
1.0.0,BoolTensor was introduced in pytorch 1.2
1.0.0,"""global state"" of the old beam"
1.0.0,buffers for the topk scores and 'backpointer'
1.0.0,for testing
1.0.0,using integer division to get an integer _B without casting
1.0.0,force the output to be longer than self.min_length
1.0.0,Multiply probs by the beam probability.
1.0.0,"if the sequence ends now, then the penalty is the current"
1.0.0,"length + 1, to include the EOS token"
1.0.0,Avoid any direction that would repeat unwanted ngrams
1.0.0,Flatten probs into a list of possibilities.
1.0.0,Recover log probs.
1.0.0,Length penalty is just a scalar. It doesn't matter if it's applied
1.0.0,before or after the topk.
1.0.0,Resolve beam origin and map to batch index flat representation.
1.0.0,Append last prediction.
1.0.0,update global state (step == 1)
1.0.0,update global state (step > 1)
1.0.0,"shape: (batch_size x beam_size, 1)"
1.0.0,Penalize beams that finished.
1.0.0,"on real data (newstest2017) with the pretrained transformer,"
1.0.0,it's faster to not move this back to the original device
1.0.0,Store finished hypotheses for this batch.
1.0.0,End condition is the top beam finished and we can return
1.0.0,n_best hypotheses.
1.0.0,"If all sentences are translated, no need to go further."
1.0.0,Remove finished batches for the next step.
1.0.0,Term will be subtracted from probability
1.0.0,Probability will be divided by this
1.0.0,these warnings indicate that either the alpha/beta
1.0.0,"forces a penalty to be a no-op, or a penalty is a no-op but"
1.0.0,the alpha/beta would suggest otherwise.
1.0.0,using some length penalty
1.0.0,using some coverage penalty
1.0.0,!/usr/bin/env python
1.0.0,semaphore doesn't have a timeout arg in Python 2.7
1.0.0,backwards compatibility for confs
1.0.0,load can be called multiple times: modify copy
1.0.0,NOTE: translator returns lists of `n_best` list
1.0.0,build back results with empty texts
1.0.0,output contain alignment
1.0.0,Below are all the different penalty terms implemented so far.
1.0.0,Subtract coverage penalty from topk log probs.
1.0.0,Divide topk log probs by length penalty.
1.0.0,Sorting
1.0.0,Chinese segmentation
1.0.0,Chinese simplify -> Chinese traditional standard
1.0.0,Chinese simplify -> Chinese traditional (HongKong)
1.0.0,Chinese simplify -> Chinese traditional (Taiwan)
1.0.0,Chinese traditional -> Chinese simplify (v1)
1.0.0,Chinese traditional -> Chinese simplify (v2)
1.0.0.rc1,!/usr/bin/env python
1.0.0.rc1,!/usr/bin/env python
1.0.0.rc1,!/usr/bin/env python
1.0.0.rc1,!/usr/bin/env python
1.0.0.rc1,!/usr/bin/env python
1.0.0.rc1,!/usr/bin/env python3
1.0.0.rc1,-*- coding: utf-8 -*-
1.0.0.rc1,
1.0.0.rc1,"OpenNMT-py documentation build configuration file, created by"
1.0.0.rc1,sphinx-quickstart on Sun Dec 17 12:07:14 2017.
1.0.0.rc1,
1.0.0.rc1,This file is execfile()d with the current directory set to its
1.0.0.rc1,containing dir.
1.0.0.rc1,
1.0.0.rc1,Note that not all possible configuration values are present in this
1.0.0.rc1,autogenerated file.
1.0.0.rc1,
1.0.0.rc1,All configuration values have a default; values that are commented out
1.0.0.rc1,serve to show the default.
1.0.0.rc1,"If extensions (or modules to document with autodoc) are in another directory,"
1.0.0.rc1,add these directories to sys.path here. If the directory is relative to the
1.0.0.rc1,"documentation root, use os.path.abspath to make it absolute, like shown here."
1.0.0.rc1,
1.0.0.rc1,import os
1.0.0.rc1,import sys
1.0.0.rc1,"sys.path.insert(0, os.path.abspath('.'))"
1.0.0.rc1,-- General configuration ------------------------------------------------
1.0.0.rc1,"If your documentation needs a minimal Sphinx version, state it here."
1.0.0.rc1,
1.0.0.rc1,needs_sphinx = '1.0'
1.0.0.rc1,"Add any Sphinx extension module names here, as strings. They can be"
1.0.0.rc1,extensions coming with Sphinx (named 'sphinx.ext.*') or your custom
1.0.0.rc1,ones.
1.0.0.rc1,Show base classes
1.0.0.rc1,"Use ""variables"" section for Attributes instead of weird block things"
1.0.0.rc1,mimicking the function style.
1.0.0.rc1,"Add any paths that contain templates here, relative to this directory."
1.0.0.rc1,The suffix(es) of source filenames.
1.0.0.rc1,You can specify multiple suffix as a list of string:
1.0.0.rc1,
1.0.0.rc1,"source_suffix = ['.rst', '.md']"
1.0.0.rc1,The master toctree document.
1.0.0.rc1,General information about the project.
1.0.0.rc1,"The version info for the project you're documenting, acts as replacement for"
1.0.0.rc1,"|version| and |release|, also used in various other places throughout the"
1.0.0.rc1,built documents.
1.0.0.rc1,
1.0.0.rc1,The short X.Y version.
1.0.0.rc1,"The full version, including alpha/beta/rc tags."
1.0.0.rc1,The language for content autogenerated by Sphinx. Refer to documentation
1.0.0.rc1,for a list of supported languages.
1.0.0.rc1,
1.0.0.rc1,This is also used if you do content translation via gettext catalogs.
1.0.0.rc1,"Usually you set ""language"" from the command line for these cases."
1.0.0.rc1,"List of patterns, relative to source directory, that match files and"
1.0.0.rc1,directories to ignore when looking for source files.
1.0.0.rc1,This patterns also effect to html_static_path and html_extra_path
1.0.0.rc1,The name of the Pygments (syntax highlighting) style to use.
1.0.0.rc1,"If true, `todo` and `todoList` produce output, else they produce nothing."
1.0.0.rc1,-- Options for HTML output ----------------------------------------------
1.0.0.rc1,The theme to use for HTML and HTML Help pages.  See the documentation for
1.0.0.rc1,a list of builtin themes.
1.0.0.rc1,
1.0.0.rc1,html_theme = 'sphinx_materialdesign_theme'
1.0.0.rc1,html_theme_path = [sphinx_materialdesign_theme.get_path()]
1.0.0.rc1,Theme options are theme-specific and customize the look and feel of a theme
1.0.0.rc1,"further.  For a list of options available for each theme, see the"
1.0.0.rc1,documentation.
1.0.0.rc1,
1.0.0.rc1,html_theme_options = {}
1.0.0.rc1,"Add any paths that contain custom static files (such as style sheets) here,"
1.0.0.rc1,"relative to this directory. They are copied after the builtin static files,"
1.0.0.rc1,"so a file named ""default.css"" will overwrite the builtin ""default.css""."
1.0.0.rc1,"Custom sidebar templates, must be a dictionary that maps document names"
1.0.0.rc1,to template names.
1.0.0.rc1,
1.0.0.rc1,This is required for the alabaster theme
1.0.0.rc1,refs: http://alabaster.readthedocs.io/en/latest/installation.html#sidebars
1.0.0.rc1,-- Options for HTMLHelp output ------------------------------------------
1.0.0.rc1,Output file base name for HTML help builder.
1.0.0.rc1,-- Options for LaTeX output ---------------------------------------------
1.0.0.rc1,The paper size ('letterpaper' or 'a4paper').
1.0.0.rc1,
1.0.0.rc1,"'papersize': 'letterpaper',"
1.0.0.rc1,"The font size ('10pt', '11pt' or '12pt')."
1.0.0.rc1,
1.0.0.rc1,"'pointsize': '10pt',"
1.0.0.rc1,Additional stuff for the LaTeX preamble.
1.0.0.rc1,
1.0.0.rc1,"'preamble': '',"
1.0.0.rc1,Latex figure (float) alignment
1.0.0.rc1,
1.0.0.rc1,"'figure_align': 'htbp',"
1.0.0.rc1,Grouping the document tree into LaTeX files. List of tuples
1.0.0.rc1,"(source start file, target name, title,"
1.0.0.rc1,"author, documentclass [howto, manual, or own class])."
1.0.0.rc1,-- Options for manual page output ---------------------------------------
1.0.0.rc1,One entry per manual page. List of tuples
1.0.0.rc1,"(source start file, name, description, authors, manual section)."
1.0.0.rc1,-- Options for Texinfo output -------------------------------------------
1.0.0.rc1,Grouping the document tree into Texinfo files. List of tuples
1.0.0.rc1,"(source start file, target name, title, author,"
1.0.0.rc1,"dir menu entry, description, category)"
1.0.0.rc1,degenerate case
1.0.0.rc1,cache the features
1.0.0.rc1,mp queues don't work well between procs unless they're from a manager
1.0.0.rc1,each device has its own saver so that reconstructing is easier
1.0.0.rc1,!/usr/bin/env python
1.0.0.rc1,-*- coding: utf-8 -*-
1.0.0.rc1,is this reachable?
1.0.0.rc1,Read in embeddings
1.0.0.rc1,Write to file
1.0.0.rc1,!/usr/bin/env python
1.0.0.rc1,-*- coding: utf-8 -*-
1.0.0.rc1,"Add in default model arguments, possibly added since training."
1.0.0.rc1,-*- encoding: utf-8 -*-
1.0.0.rc1,!/usr/bin/env python
1.0.0.rc1,-*- coding: utf-8 -*-
1.0.0.rc1,Author: Rico Sennrich
1.0.0.rc1,flake8: noqa
1.0.0.rc1,This file is retrieved from https://github.com/rsennrich/subword-nmt
1.0.0.rc1,hack for python2/3 compatibility
1.0.0.rc1,check version information
1.0.0.rc1,some hacking to deal with duplicates (only consider first instance)
1.0.0.rc1,don't print end-of-word symbols
1.0.0.rc1,sys.stderr.write('cannot split {0} further.\n'.format(segment))
1.0.0.rc1,sys.stderr.write('OOV: {0}\n'.format(segment))
1.0.0.rc1,sys.stderr.write('OOV: {0}\n'.format(segment))
1.0.0.rc1,python 2/3 compatibility
1.0.0.rc1,read/write files as UTF-8
1.0.0.rc1,!/usr/bin/env python
1.0.0.rc1,!/usr/bin/env python
1.0.0.rc1,-*- coding: utf-8 -*-
1.0.0.rc1,Author: Rico Sennrich
1.0.0.rc1,flake8: noqa
1.0.0.rc1,This file is retrieved from https://github.com/rsennrich/subword-nmt
1.0.0.rc1,hack for python2/3 compatibility
1.0.0.rc1,"find all instances of pair, and update frequency/indices around it"
1.0.0.rc1,find first symbol
1.0.0.rc1,"if first symbol is followed by second symbol, we've found an occurrence of pair (old_word[i:i+2])"
1.0.0.rc1,"assuming a symbol sequence ""A B C"", if ""B C"" is merged, reduce the frequency of ""A B"""
1.0.0.rc1,"assuming a symbol sequence ""A B C B"", if ""B C"" is merged, reduce the frequency of ""C B""."
1.0.0.rc1,"however, skip this if the sequence is A B C B C, because the frequency of ""C B"" will be reduced by the previous code block"
1.0.0.rc1,find new pair
1.0.0.rc1,"assuming a symbol sequence ""A BC D"", if ""B C"" is merged, increase the frequency of ""A BC"""
1.0.0.rc1,"assuming a symbol sequence ""A BC B"", if ""B C"" is merged, increase the frequency of ""BC B"""
1.0.0.rc1,"however, if the sequence is A BC BC, skip this step because the count of ""BC BC"" will be incremented by the previous code block"
1.0.0.rc1,data structure of pair frequencies
1.0.0.rc1,index from pairs to words
1.0.0.rc1,version 0.2 changes the handling of the end-of-word token ('</w>');
1.0.0.rc1,version numbering allows bckward compatibility
1.0.0.rc1,"threshold is inspired by Zipfian assumption, but should only affect speed"
1.0.0.rc1,we probably missed the best pair because of pruning; go back to full statistics
1.0.0.rc1,"threshold is inspired by Zipfian assumption, but should only affect speed"
1.0.0.rc1,python 2/3 compatibility
1.0.0.rc1,read/write files as UTF-8
1.0.0.rc1,!/usr/bin/env python
1.0.0.rc1,for back compat when attention_dropout was not defined
1.0.0.rc1,Build embeddings.
1.0.0.rc1,Build encoder.
1.0.0.rc1,Build decoder.
1.0.0.rc1,Share the embedding matrix - preprocess with share_vocab required.
1.0.0.rc1,src/tgt vocab should be the same if `-share_vocab` is specified.
1.0.0.rc1,Build NMTModel(= encoder + decoder).
1.0.0.rc1,Build Generator.
1.0.0.rc1,Load the model states from checkpoint or initialize them.
1.0.0.rc1,This preserves backward-compat for models using customed layernorm
1.0.0.rc1,end of patch for backward compatibility
1.0.0.rc1,!/usr/bin/env python
1.0.0.rc1,NOTE: It's important that ``opt`` has been validated and updated
1.0.0.rc1,at this point.
1.0.0.rc1,Load checkpoint if we resume from a previous training.
1.0.0.rc1,check for code where vocab is saved instead of fields
1.0.0.rc1,(in the future this will be done in a smarter way)
1.0.0.rc1,"Report src and tgt vocab sizes, including for features"
1.0.0.rc1,Build model.
1.0.0.rc1,Build optimizer.
1.0.0.rc1,Build model saver
1.0.0.rc1,Embedding Options
1.0.0.rc1,Encoder-Decoder Options
1.0.0.rc1,"group.add('--residual', '-residual',   action=""store_true"","
1.0.0.rc1,"help=""Add residual connections between RNN layers."")"
1.0.0.rc1,Attention options
1.0.0.rc1,Generator and loss options.
1.0.0.rc1,Data options
1.0.0.rc1,"Dictionary options, for text corpus"
1.0.0.rc1,"if you want to pass an existing vocab.pt file, pass it to"
1.0.0.rc1,-src_vocab alone as it already contains tgt vocab.
1.0.0.rc1,"Truncation options, for text corpus"
1.0.0.rc1,Data processing options
1.0.0.rc1,Options most relevant to speech
1.0.0.rc1,Option most relevant to image input
1.0.0.rc1,GPU
1.0.0.rc1,Init options
1.0.0.rc1,Pretrained word vectors
1.0.0.rc1,Fixed word vectors
1.0.0.rc1,Optimization options
1.0.0.rc1,learning rate
1.0.0.rc1,Use Tensorboard for visualization during training
1.0.0.rc1,Options most relevant to speech
1.0.0.rc1,Option most relevant to image input
1.0.0.rc1,Options most relevant to summarization.
1.0.0.rc1,Alpha and Beta values for Google Length + Coverage penalty
1.0.0.rc1,"Described here: https://arxiv.org/pdf/1609.08144.pdf, Section 7"
1.0.0.rc1,Options most relevant to speech.
1.0.0.rc1,Option most relevant to image input
1.0.0.rc1,Copyright 2016 The Chromium Authors. All rights reserved.
1.0.0.rc1,Use of this source code is governed by a BSD-style license that can be
1.0.0.rc1,found in the LICENSE file.
1.0.0.rc1,"Get the key 'value' in the dict, or just use 'value'"
1.0.0.rc1,Basic attributes.
1.0.0.rc1,Set model in training mode.
1.0.0.rc1,UPDATE DROPOUT
1.0.0.rc1,Run patience mechanism
1.0.0.rc1,"If the patience has reached the limit, stop training"
1.0.0.rc1,Set model in validating mode.
1.0.0.rc1,F-prop through the model.
1.0.0.rc1,Compute loss.
1.0.0.rc1,Update statistics.
1.0.0.rc1,Set model back to training mode.
1.0.0.rc1,Truncated BPTT: reminder not compatible with accum > 1
1.0.0.rc1,1. Create truncated target.
1.0.0.rc1,2. F-prop all but generator.
1.0.0.rc1,3. Compute loss.
1.0.0.rc1,4. Update the parameters and statistics.
1.0.0.rc1,Multi GPU gradient gather
1.0.0.rc1,"If truncated, don't backprop fully."
1.0.0.rc1,TO CHECK
1.0.0.rc1,if dec_state is not None:
1.0.0.rc1,dec_state.detach()
1.0.0.rc1,"in case of multi step gradient accumulation,"
1.0.0.rc1,update only after accum batches
1.0.0.rc1,For Flake
1.0.0.rc1,we avoid padding while mean pooling
1.0.0.rc1,Initialize the bridge layer
1.0.0.rc1,"s_len, batch, emb_dim = emb.size()"
1.0.0.rc1,Lengths data is wrapped inside a Tensor.
1.0.0.rc1,"LSTM has hidden and cell state, other only one"
1.0.0.rc1,Total number of states
1.0.0.rc1,Build a linear layer for each
1.0.0.rc1,The encoder hidden is  (layers*directions) x batch x dim.
1.0.0.rc1,"s_len, batch, emb_dim = emb.size()"
1.0.0.rc1,Run the forward pass of every layer of the tranformer.
1.0.0.rc1,why is the model_opt.__dict__ check necessary?
1.0.0.rc1,"(batch_size, 64, imgH, imgW)"
1.0.0.rc1,layer 1
1.0.0.rc1,"(batch_size, 64, imgH/2, imgW/2)"
1.0.0.rc1,"(batch_size, 128, imgH/2, imgW/2)"
1.0.0.rc1,layer 2
1.0.0.rc1,"(batch_size, 128, imgH/2/2, imgW/2/2)"
1.0.0.rc1,"(batch_size, 256, imgH/2/2, imgW/2/2)"
1.0.0.rc1,layer 3
1.0.0.rc1,batch norm 1
1.0.0.rc1,"(batch_size, 256, imgH/2/2, imgW/2/2)"
1.0.0.rc1,layer4
1.0.0.rc1,"(batch_size, 256, imgH/2/2/2, imgW/2/2)"
1.0.0.rc1,"(batch_size, 512, imgH/2/2/2, imgW/2/2)"
1.0.0.rc1,layer 5
1.0.0.rc1,batch norm 2
1.0.0.rc1,"(batch_size, 512, imgH/2/2/2, imgW/2/2/2)"
1.0.0.rc1,"(batch_size, 512, imgH/2/2/2, imgW/2/2/2)"
1.0.0.rc1,"# (batch_size, 512, H, W)"
1.0.0.rc1,Dimensions and padding for constructing the word embedding matrix
1.0.0.rc1,Dimensions and padding for feature embedding matrices
1.0.0.rc1,(these have no effect if feat_vocab_sizes is empty)
1.0.0.rc1,The embedding matrix look-up tables. The first look-up table
1.0.0.rc1,"is for words. Subsequent ones are for features, if any exist."
1.0.0.rc1,The final output size of word + feature vectors. This can vary
1.0.0.rc1,from the word vector size if and only if features are defined.
1.0.0.rc1,This is the attribute you should access if you need to know
1.0.0.rc1,how big your embeddings are going to be.
1.0.0.rc1,The sequence of operations that converts the input sequence
1.0.0.rc1,into a sequence of embeddings. At minimum this consists of
1.0.0.rc1,looking up the embeddings for each word and feature in the
1.0.0.rc1,input. Model parameters may require the sequence to contain
1.0.0.rc1,additional operations as well.
1.0.0.rc1,features must use word_vec_size
1.0.0.rc1,features will use feat_vec_size
1.0.0.rc1,This class is mainly used by decoder.py for RNNs but also
1.0.0.rc1,by the CNN / transformer decoder when copy attention is used
1.0.0.rc1,CNN has its own attention mechanism ConvMultiStepAttention
1.0.0.rc1,Transformer has its own MultiHeadedAttention
1.0.0.rc1,mlp wants it with bias
1.0.0.rc1,Check input sizes
1.0.0.rc1,"(batch, t_len, d) x (batch, d, s_len) --> (batch, t_len, s_len)"
1.0.0.rc1,"(batch, t_len, s_len, d)"
1.0.0.rc1,one step input
1.0.0.rc1,"compute attention scores, as in Luong et al."
1.0.0.rc1,Softmax or sparsemax to normalize attention weights
1.0.0.rc1,each context vector c_t is the weighted average
1.0.0.rc1,over all the source hidden states
1.0.0.rc1,concatenate
1.0.0.rc1,Check output sizes
1.0.0.rc1,Check output sizes
1.0.0.rc1,clamping necessary because of numerical errors: loss should be lower
1.0.0.rc1,"bounded by zero, but negative values near zero are possible without"
1.0.0.rc1,the clamp
1.0.0.rc1,from onmt.utils.misc import aeq
1.0.0.rc1,CHECKS
1.0.0.rc1,"batch, k_len, d = key.size()"
1.0.0.rc1,"batch_, k_len_, d_ = value.size()"
1.0.0.rc1,"aeq(batch, batch_)"
1.0.0.rc1,"aeq(k_len, k_len_)"
1.0.0.rc1,"aeq(d, d_)"
1.0.0.rc1,"batch_, q_len, d_ = query.size()"
1.0.0.rc1,"aeq(batch, batch_)"
1.0.0.rc1,"aeq(d, d_)"
1.0.0.rc1,"aeq(self.model_dim % 8, 0)"
1.0.0.rc1,if mask is not None:
1.0.0.rc1,"batch_, q_len_, k_len_ = mask.size()"
1.0.0.rc1,"aeq(batch_, batch)"
1.0.0.rc1,"aeq(k_len_, k_len)"
1.0.0.rc1,aeq(q_len_ == q_len)
1.0.0.rc1,END CHECKS
1.0.0.rc1,"1) Project key, value, and query."
1.0.0.rc1,1 or key_len x key_len
1.0.0.rc1,1 or key_len x key_len x dim_per_head
1.0.0.rc1,1 or key_len x key_len x dim_per_head
1.0.0.rc1,2) Calculate and scale scores.
1.0.0.rc1,batch x num_heads x query_len x key_len
1.0.0.rc1,3) Apply attention dropout and compute context vectors.
1.0.0.rc1,CHECK
1.0.0.rc1,"batch_, q_len_, d_ = output.size()"
1.0.0.rc1,"aeq(q_len, q_len_)"
1.0.0.rc1,"aeq(batch, batch_)"
1.0.0.rc1,"aeq(d, d_)"
1.0.0.rc1,Return one attn
1.0.0.rc1,At the moment this class is only used by embeddings.Embeddings look-up tables
1.0.0.rc1,-*- coding: utf-8 -*-
1.0.0.rc1,checks
1.0.0.rc1,"batch, channel, height, width = base_target_emb.size()"
1.0.0.rc1,"batch_, channel_, height_, width_ = input_from_dec.size()"
1.0.0.rc1,"enc_batch, enc_channel, enc_height = encoder_out_top.size()"
1.0.0.rc1,"enc_batch_, enc_channel_, enc_height_ = encoder_out_combine.size()"
1.0.0.rc1,out_features * in_features
1.0.0.rc1,norm is out_features * 1
1.0.0.rc1,batch_size * out_features
1.0.0.rc1,out_features
1.0.0.rc1,out_features
1.0.0.rc1,batch_size * out_features
1.0.0.rc1,"out_channels, in_channels // groups, * kernel_size"
1.0.0.rc1,out_features
1.0.0.rc1,This is used nowhere in the code at the moment (Vincent Nguyen 05/18/2018)
1.0.0.rc1,"in_channels, out_channels, *kernel_size"
1.0.0.rc1,"in_channels, out_channels, *kernel_size"
1.0.0.rc1,"self.out_channels, 1"
1.0.0.rc1,out_features
1.0.0.rc1,out_features
1.0.0.rc1,store roots on diagonal
1.0.0.rc1,CHECKS
1.0.0.rc1,Original probabilities.
1.0.0.rc1,Probability of copying p(z=1) batch.
1.0.0.rc1,Probability of not copying: p_{word}(w) * (1 - p(z))
1.0.0.rc1,probabilities assigned by the model to the gold targets
1.0.0.rc1,probability of tokens copied from source
1.0.0.rc1,Set scores for unk to 0 and add eps
1.0.0.rc1,find the indices in which you do not use the copy mechanism
1.0.0.rc1,Drop padding.
1.0.0.rc1,this block does not depend on the loss value computed above
1.0.0.rc1,and is used only for stats
1.0.0.rc1,this block does not depend on the loss value computed above
1.0.0.rc1,and is used only for stats
1.0.0.rc1,Correct target copy token instead of <unk>
1.0.0.rc1,tgt[i] = align[i] + len(tgt_vocab)
1.0.0.rc1,for i such that tgt[i] == 0 and align[i] != 0
1.0.0.rc1,Compute sum of perplexities for stats
1.0.0.rc1,this part looks like it belongs in CopyGeneratorLoss
1.0.0.rc1,Compute Loss as NLL divided by seq length
1.0.0.rc1,Compute Total Loss per sequence in batch
1.0.0.rc1,Divide by length of each sequence and sum
1.0.0.rc1,all beams repeat (beam >= 1 repeat dummy scores)
1.0.0.rc1,predict repeat_idx over and over again
1.0.0.rc1,beam 0 and beam >=2 will repeat (beam >= 2 repeat dummy scores)
1.0.0.rc1,non-interesting beams are going to get dummy values
1.0.0.rc1,"on initial round, only predicted scores for beam 0"
1.0.0.rc1,matter. Make two predictions. Top one will be repeated
1.0.0.rc1,"in beam zero, second one will live on in beam 1."
1.0.0.rc1,predict the same thing in beam 0
1.0.0.rc1,continue pushing around what beam 1 predicts
1.0.0.rc1,"now beam 0 dies (along with the others), beam 1 -> beam 0"
1.0.0.rc1,beam 0 and beam >= 2 will repeat (beam 2 repeats excluded idx)
1.0.0.rc1,non-interesting beams are going to get dummy values
1.0.0.rc1,predict the same thing in beam 0
1.0.0.rc1,continue pushing around what beam 1 predicts
1.0.0.rc1,predict the allowed-repeat again in beam 2
1.0.0.rc1,"now beam 0 dies, beam 1 -> beam 0, beam 2 -> beam 1"
1.0.0.rc1,and the rest die
1.0.0.rc1,"since all preds after i=0 are 0, we can check"
1.0.0.rc1,that the beam is the correct idx by checking that
1.0.0.rc1,the curr score is the initial score
1.0.0.rc1,beam 0 will always predict EOS. The other beams will predict
1.0.0.rc1,non-eos scores.
1.0.0.rc1,"this is also a test that when block_ngram_repeat=0,"
1.0.0.rc1,repeating is acceptable
1.0.0.rc1,non-interesting beams are going to get dummy values
1.0.0.rc1,"""best"" prediction is eos - that should be blocked"
1.0.0.rc1,include at least beam_sz predictions OTHER than EOS
1.0.0.rc1,that are greater than -1e20
1.0.0.rc1,predict eos in beam 0
1.0.0.rc1,provide beam_sz other good predictions
1.0.0.rc1,now the top beam has ended and no others have
1.0.0.rc1,first beam finished had length beam.min_length
1.0.0.rc1,first beam finished was 0
1.0.0.rc1,"not of interest, but want to make sure it keeps running"
1.0.0.rc1,since only beam 0 terminates and n_best = 2
1.0.0.rc1,"this is also a test that when block_ngram_repeat=0,"
1.0.0.rc1,repeating is acceptable
1.0.0.rc1,non-interesting beams are going to get dummy values
1.0.0.rc1,"""best"" prediction is eos - that should be blocked"
1.0.0.rc1,include at least beam_sz predictions OTHER than EOS
1.0.0.rc1,that are greater than -1e20
1.0.0.rc1,predict eos in beam 1
1.0.0.rc1,provide beam_sz other good predictions in other beams
1.0.0.rc1,provide beam_sz other good predictions in other beams
1.0.0.rc1,beam 1 dies on min_length
1.0.0.rc1,beam 0 dies on the step after beam 1 dies
1.0.0.rc1,"init_preds: [4, 3, 5, 6, 7] - no EOS's"
1.0.0.rc1,no EOS's yet
1.0.0.rc1,"[5, 3, 2, 6, 0], so beam 2 predicts EOS!"
1.0.0.rc1,assumes beam 2 finished on last step
1.0.0.rc1,"[2, 5, 3, 6, 0], so beam 0 predicts EOS!"
1.0.0.rc1,"[-2.4879, -3.8910, -4.1010, -4.2010, -4.4010]"
1.0.0.rc1,new beam 0 finished
1.0.0.rc1,new beam 0 is old beam 3
1.0.0.rc1,assumes beam 0 finished on last step
1.0.0.rc1,"[5, 2, 6, 1, 0], so beam 1 predicts EOS!"
1.0.0.rc1,new beam 1 finished
1.0.0.rc1,new beam 1 is old beam 4
1.0.0.rc1,this could be considered an integration test because it tests
1.0.0.rc1,interactions between the GNMT scorer and the beam
1.0.0.rc1,initialize fields at the top of each unit test to prevent
1.0.0.rc1,any undesired stateful effects
1.0.0.rc1,"this test touches the file system, so it could be considered an"
1.0.0.rc1,integration test
1.0.0.rc1,write utf-8 bytes
1.0.0.rc1,predict repeat_idx over and over again
1.0.0.rc1,"batch 0 and 7 will repeat, the rest will advance"
1.0.0.rc1,predict the same thing in batch 0 and 7 every i
1.0.0.rc1,push around what the other batches predict
1.0.0.rc1,now batch 0 and 7 die
1.0.0.rc1,"batch 0 will repeat excluded idx, batch 1 will repeat"
1.0.0.rc1,now batch 1 dies
1.0.0.rc1,batch 0 will always predict EOS. The other batches will predict
1.0.0.rc1,non-eos scores.
1.0.0.rc1,"""best"" prediction is eos - that should be blocked"
1.0.0.rc1,include at least one prediction OTHER than EOS
1.0.0.rc1,that is greater than -1e20
1.0.0.rc1,now batch 0 has ended and no others have
1.0.0.rc1,initial step
1.0.0.rc1,batch 0 dies on step 0
1.0.0.rc1,include at least one prediction OTHER than EOS
1.0.0.rc1,that is greater than -1e20
1.0.0.rc1,step 2
1.0.0.rc1,(old) batch 8 dies on step 1
1.0.0.rc1,step 3
1.0.0.rc1,everything dies
1.0.0.rc1,initial step
1.0.0.rc1,batch 0 dies on step 0
1.0.0.rc1,include at least one prediction OTHER than EOS
1.0.0.rc1,that is greater than -1e20
1.0.0.rc1,step 2
1.0.0.rc1,(old) batch 8 dies on step 1
1.0.0.rc1,step 3
1.0.0.rc1,everything dies
1.0.0.rc1,illegal_weights_mask = torch.ByteTensor([
1.0.0.rc1,"[0, 0, 0, 0, 0, 0, 0],"
1.0.0.rc1,"[0, 0, 0, 1, 1, 1, 1],"
1.0.0.rc1,"[0, 0, 0, 0, 0, 1, 1],"
1.0.0.rc1,"[0, 0, 1, 1, 1, 1, 1]])"
1.0.0.rc1,TODO: fix for pytorch 0.3
1.0.0.rc1,illegal_weights = alignments.masked_select(illegal_weights_mask)
1.0.0.rc1,"self.assertEqual(0.0, illegal_weights.data.sum())"
1.0.0.rc1,this could be considered an integration test because it touches
1.0.0.rc1,the filesystem for the config file (and the models)
1.0.0.rc1,-*- coding: utf-8 -*-
1.0.0.rc1,tests pad and numericalize integration
1.0.0.rc1,tests pad and numericalize integration
1.0.0.rc1,"this test touches the file system, so it could be considered an"
1.0.0.rc1,integration test
1.0.0.rc1,file to hold full paths to audio data
1.0.0.rc1,file to hold audio paths relative to _AUDIO_DATA_DIR (i.e. file names)
1.0.0.rc1,it's ok if non-audio files co-exist with audio files in the data dir
1.0.0.rc1,"dividing gets the noise in [-1, 1]"
1.0.0.rc1,"this test touches the file system, so it could be considered an"
1.0.0.rc1,integration test
1.0.0.rc1,file to hold full paths to image data
1.0.0.rc1,file to hold image paths relative to _IMG_DATA_DIR (i.e. file names)
1.0.0.rc1,it's ok if non-image files co-exist with image files in the data dir
1.0.0.rc1,all beams repeat (beam >= 1 repeat dummy scores)
1.0.0.rc1,predict repeat_idx over and over again
1.0.0.rc1,beam 0 and beam >=2 will repeat (beam >= 2 repeat dummy scores)
1.0.0.rc1,non-interesting beams are going to get dummy values
1.0.0.rc1,"on initial round, only predicted scores for beam 0"
1.0.0.rc1,matter. Make two predictions. Top one will be repeated
1.0.0.rc1,"in beam zero, second one will live on in beam 1."
1.0.0.rc1,predict the same thing in beam 0
1.0.0.rc1,continue pushing around what beam 1 predicts
1.0.0.rc1,"now beam 0 dies (along with the others), beam 1 -> beam 0"
1.0.0.rc1,beam 0 and beam >= 2 will repeat (beam 2 repeats excluded idx)
1.0.0.rc1,non-interesting beams are going to get dummy values
1.0.0.rc1,predict the same thing in beam 0
1.0.0.rc1,continue pushing around what beam 1 predicts
1.0.0.rc1,predict the allowed-repeat again in beam 2
1.0.0.rc1,"now beam 0 dies, beam 1 -> beam 0, beam 2 -> beam 1"
1.0.0.rc1,and the rest die
1.0.0.rc1,"since all preds after i=0 are 0, we can check"
1.0.0.rc1,that the beam is the correct idx by checking that
1.0.0.rc1,the curr score is the initial score
1.0.0.rc1,beam 0 will always predict EOS. The other beams will predict
1.0.0.rc1,non-eos scores.
1.0.0.rc1,non-interesting beams are going to get dummy values
1.0.0.rc1,"""best"" prediction is eos - that should be blocked"
1.0.0.rc1,include at least beam_sz predictions OTHER than EOS
1.0.0.rc1,that are greater than -1e20
1.0.0.rc1,predict eos in beam 0
1.0.0.rc1,provide beam_sz other good predictions
1.0.0.rc1,now the top beam has ended and no others have
1.0.0.rc1,"not of interest, but want to make sure it keeps running"
1.0.0.rc1,since only beam 0 terminates and n_best = 2
1.0.0.rc1,"this is also a test that when block_ngram_repeat=0,"
1.0.0.rc1,repeating is acceptable
1.0.0.rc1,non-interesting beams are going to get dummy values
1.0.0.rc1,"""best"" prediction is eos - that should be blocked"
1.0.0.rc1,include at least beam_sz predictions OTHER than EOS
1.0.0.rc1,that are greater than -1e20
1.0.0.rc1,predict eos in beam 1
1.0.0.rc1,provide beam_sz other good predictions in other beams
1.0.0.rc1,provide beam_sz other good predictions in other beams
1.0.0.rc1,beam 1 dies on min_length
1.0.0.rc1,beam 0 dies on the step after beam 1 dies
1.0.0.rc1,non-interesting beams are going to get dummy values
1.0.0.rc1,"""best"" prediction is eos - that should be blocked"
1.0.0.rc1,include at least beam_sz predictions OTHER than EOS
1.0.0.rc1,that are greater than -1e20
1.0.0.rc1,predict eos in beam 1
1.0.0.rc1,provide beam_sz other good predictions in other beams
1.0.0.rc1,provide beam_sz other good predictions in other beams
1.0.0.rc1,no top beams are finished yet
1.0.0.rc1,beam 1 dies on min_length
1.0.0.rc1,no top beams are finished yet
1.0.0.rc1,beam 0 dies on the step after beam 1 dies
1.0.0.rc1,top beam is finished now so there are attentions
1.0.0.rc1,two beams are finished in each batch
1.0.0.rc1,second dim is cut down to the non-padded src length
1.0.0.rc1,first dim is equal to the time of death
1.0.0.rc1,(beam 0 died at current step - adjust for SOS)
1.0.0.rc1,(beam 1 died at last step - adjust for SOS)
1.0.0.rc1,behavior gets weird when beam is already done so just stop
1.0.0.rc1,this is just test_beam.TestBeamAgainstReferenceCase repeated
1.0.0.rc1,in each batch.
1.0.0.rc1,"init_preds: [4, 3, 5, 6, 7] - no EOS's"
1.0.0.rc1,no EOS's yet
1.0.0.rc1,"[5, 3, 2, 6, 0], so beam 2 predicts EOS!"
1.0.0.rc1,assumes beam 2 finished on last step
1.0.0.rc1,ended beam 2 shouldn't continue
1.0.0.rc1,"[2, 5, 3, 6, 0] repeat self.BATCH_SZ, so beam 0 predicts EOS!"
1.0.0.rc1,"[-2.4879, -3.8910, -4.1010, -4.2010, -4.4010] repeat self.BATCH_SZ"
1.0.0.rc1,another beam is finished in all batches
1.0.0.rc1,new beam 0 finished
1.0.0.rc1,new beam 0 is old beam 3
1.0.0.rc1,assumes beam 0 finished on last step
1.0.0.rc1,"[5, 2, 6, 1, 0] repeat self.BATCH_SZ, so beam 1 predicts EOS!"
1.0.0.rc1,new beam 1 finished
1.0.0.rc1,new beam 1 is old beam 4
1.0.0.rc1,this could be considered an integration test because it tests
1.0.0.rc1,interactions between the GNMT scorer and the beam
1.0.0.rc1,"-data option is required, but not used in this test, so dummy."
1.0.0.rc1,len x batch x nfeat
1.0.0.rc1,batch x c x h x w
1.0.0.rc1,batch x 1 x nfft x t
1.0.0.rc1,Initialize vectors to compare size with
1.0.0.rc1,Ensure correct sizes and types
1.0.0.rc1,Make sure that output has the correct size and type
1.0.0.rc1,Make sure that output has the correct size and type
1.0.0.rc1,Make sure that output has the correct size and type
1.0.0.rc1,"[('encoder_type', 'transformer'),"
1.0.0.rc1,"('word_vec_size', 16), ('rnn_size', 16)],"
1.0.0.rc1,""""""" Only do SRU test if requirment is safisfied. """""""
1.0.0.rc1,SRU doesn't support input_feed.
1.0.0.rc1,"when reasonable, set audio_enc_pooling to 2"
1.0.0.rc1,Need lengths >= audio_enc_pooling**n_layers.
1.0.0.rc1,"That condition is unrealistic for large n_layers,"
1.0.0.rc1,so leave audio_enc_pooling at 1.
1.0.0.rc1,first check there's nothing unexpectedly not trainable
1.0.0.rc1,ok: word embeddings shouldn't be trainable
1.0.0.rc1,if word vecs are fixed
1.0.0.rc1,ok: positional encodings shouldn't be trainable
1.0.0.rc1,then check nothing unexpectedly trainable
1.0.0.rc1,!/usr/bin/env python
1.0.0.rc1,-*- coding: utf-8 -*-
1.0.0.rc1,Remove the generated *pt files.
1.0.0.rc1,Test image preprocessing
1.0.0.rc1,Test audio preprocessing
1.0.0.rc1,Decoder state
1.0.0.rc1,Build the RNN.
1.0.0.rc1,Set up the context gate.
1.0.0.rc1,Set up the standard attention.
1.0.0.rc1,The encoder hidden is  (layers*directions) x batch x dim.
1.0.0.rc1,We need to convert it to layers x batch x (directions*dim).
1.0.0.rc1,Init the input feed.
1.0.0.rc1,Update the state with the result.
1.0.0.rc1,Concatenates sequence of tensors along a new dimension.
1.0.0.rc1,NOTE: v0.3 to 0.4: dec_outs / attns[*] may not be list
1.0.0.rc1,(in particular in case of SRU) it was not raising error in 0.3
1.0.0.rc1,since stack(Variable) was allowed.
1.0.0.rc1,"In 0.4, SRU returns a tensor that shouldn't be stacke"
1.0.0.rc1,Check
1.0.0.rc1,Calculate the attention.
1.0.0.rc1,Calculate the context gate.
1.0.0.rc1,Additional args check.
1.0.0.rc1,END Additional args check.
1.0.0.rc1,Input feed concatenates hidden state with
1.0.0.rc1,input at every time step.
1.0.0.rc1,TODO: context gate should be employed
1.0.0.rc1,instead of second RNN transform.
1.0.0.rc1,Update the coverage attention.
1.0.0.rc1,Decoder State
1.0.0.rc1,CNNDecoder has its own attention mechanism.
1.0.0.rc1,Set up a separate copy attention layer if needed.
1.0.0.rc1,The output of CNNEncoder.
1.0.0.rc1,The combination of output of CNNEncoder and source embeddings.
1.0.0.rc1,Process the result and update the attentions.
1.0.0.rc1,Update the state.
1.0.0.rc1,TODO change the way attns is returned dict => list or tuple (onnx)
1.0.0.rc1,Memory_lengths is a single tensor shared between all models.
1.0.0.rc1,This assumption will not hold if Translator is modified
1.0.0.rc1,to calculate memory_lengths as something other than the length
1.0.0.rc1,of the input.
1.0.0.rc1,BoolTensor was introduced in pytorch 1.2
1.0.0.rc1,Decoder State
1.0.0.rc1,"previously, there was a GlobalAttention module here for copy"
1.0.0.rc1,"attention. But it was never actually used -- the ""copy"" attention"
1.0.0.rc1,just reuses the context attention.
1.0.0.rc1,TODO change the way attns is returned dict => list or tuple (onnx)
1.0.0.rc1,"buffer size in bytes, determine equiv. # of elements based on data type"
1.0.0.rc1,copy tensors into buffer_t
1.0.0.rc1,all-reduce and rescale
1.0.0.rc1,copy all-reduced buffer back into tensors
1.0.0.rc1,"tensor is bigger than buffer, all-reduce and rescale directly"
1.0.0.rc1,"buffer is full, all-reduce and replace buffer with grad"
1.0.0.rc1,add tensor to buffer
1.0.0.rc1,TODO: Find a better way to check for sparse gradients.
1.0.0.rc1,we use here a FusedAdam() copy of an old Apex repo
1.0.0.rc1,In this case use the new AMP API from apex
1.0.0.rc1,In this case use the old FusedAdam with FP16_optimizer wrapper
1.0.0.rc1,Load everything from the checkpoint.
1.0.0.rc1,Build everything from scratch.
1.0.0.rc1,"Reset optimizer, keep options."
1.0.0.rc1,"Reset options, keep optimizer."
1.0.0.rc1,State can be partially restored.
1.0.0.rc1,Code below is an implementation of https://arxiv.org/pdf/1804.04235.pdf
1.0.0.rc1,inspired but modified from https://github.com/DeadAt0m/adafactor-pytorch
1.0.0.rc1,backward compatibility
1.0.0.rc1,assuming a list/generator of parameter means single group
1.0.0.rc1,compute combined scale factor for this group
1.0.0.rc1,norm is in fact norm*scale
1.0.0.rc1,note: p.grad should not ever be set for correct operation of
1.0.0.rc1,mixed precision optimizer that sometimes sends None gradients
1.0.0.rc1,State initialization
1.0.0.rc1,Exponential moving average of gradient values
1.0.0.rc1,Exponential moving average of squared gradient values
1.0.0.rc1,-*- coding: utf-8 -*-
1.0.0.rc1,if the loss function operates on vectors of raw logits instead of
1.0.0.rc1,"probabilities, only the first part of the generator needs to be"
1.0.0.rc1,"passed to the NMTLossCompute. At the moment, the only supported"
1.0.0.rc1,loss function of this kind is the sparsemax loss.
1.0.0.rc1,non_none: the subdict of the state dictionary where the values
1.0.0.rc1,are not None.
1.0.0.rc1,"Now, the iteration:"
1.0.0.rc1,state is a dictionary of sequences of tensor-like but we
1.0.0.rc1,want a sequence of dictionaries of tensors.
1.0.0.rc1,"First, unzip the dictionary into a sequence of keys and a"
1.0.0.rc1,sequence of tensor-like sequences.
1.0.0.rc1,"Now, yield a dictionary for each shard. The keys are always"
1.0.0.rc1,the same. values is a sequence of length #keys where each
1.0.0.rc1,element is a sequence of length #shards. We want to iterate
1.0.0.rc1,"over the shards, not over the keys: therefore, the values need"
1.0.0.rc1,to be re-zipped by shard and then each shard can be paired
1.0.0.rc1,with the keys.
1.0.0.rc1,Assumed backprop'd
1.0.0.rc1,Log the progress using the number of batches on the x-axis.
1.0.0.rc1,this check is here because audio allows the encoder and decoder to
1.0.0.rc1,"be different sizes, but other model types do not yet"
1.0.0.rc1,"Load default opt values, then overwrite with the opts in"
1.0.0.rc1,"the checkpoint. That way, if there are new options added,"
1.0.0.rc1,the defaults are used.
1.0.0.rc1,Don't do anything
1.0.0.rc1,Update best score of each criteria
1.0.0.rc1,Reset tolerance
1.0.0.rc1,Update current status
1.0.0.rc1,Decrease tolerance
1.0.0.rc1,Log
1.0.0.rc1,Log
1.0.0.rc1,Get a list of world_size lists with len(stat_list) Statistics objects
1.0.0.rc1,SRU doesn't support PackedSequence.
1.0.0.rc1,-*- coding: utf-8 -*-
1.0.0.rc1,this one is needed for torchtext random call (shuffled iterator)
1.0.0.rc1,in multi gpu it ensures datasets are read in the same order
1.0.0.rc1,some cudnn methods can be random even after fixing the seed
1.0.0.rc1,unless you tell it to be deterministic
1.0.0.rc1,These ensure same initialization in multi gpu mode
1.0.0.rc1,Shift values to be >= 0
1.0.0.rc1,coding: utf-8
1.0.0.rc1,make a small vocab containing just the tokens in the source sequence
1.0.0.rc1,Map source tokens to indices in the dynamic dict.
1.0.0.rc1,self.src_vocabs is used in collapse_copy_scores and Translator.py
1.0.0.rc1,this assumes src_field and tgt_field are both text
1.0.0.rc1,fields needs to have only keys that examples have as attrs
1.0.0.rc1,avoid infinite recursion when fields isn't defined
1.0.0.rc1,-*- coding: utf-8 -*-
1.0.0.rc1,backwards compatibility
1.0.0.rc1,monkey-patch to make torchtext Vocab's pickleable
1.0.0.rc1,"List[Tuple[str, Vocab]] -> List[Tuple[str, Field]]"
1.0.0.rc1,"-> dict[str, Field]"
1.0.0.rc1,"Dict[str, List[Tuple[str, Field]]]"
1.0.0.rc1,doesn't change structure - don't return early.
1.0.0.rc1,"Dict[str, List[Tuple[str, Field]]] -> List[Tuple[str, Field]]"
1.0.0.rc1,"-> dict[str, Field]"
1.0.0.rc1,"if tgt isn't using TextMultiField, then no text field is."
1.0.0.rc1,this is basically copy-pasted from torchtext.
1.0.0.rc1,counters changes in place
1.0.0.rc1,keep the order of tokens specified in the vocab file by
1.0.0.rc1,adding them to the counter with decreasing counting values
1.0.0.rc1,`tgt_vocab_size` is ignored when sharing vocabularies
1.0.0.rc1,return vocab to dump with standard name
1.0.0.rc1,empty train_dataset_files so that vocab is only loaded from
1.0.0.rc1,"given paths in src_vocab_path, tgt_vocab_path"
1.0.0.rc1,Load vocabulary
1.0.0.rc1,Drop the none-using from memory but keep the last
1.0.0.rc1,"in the long run, shouldn't it be possible to do this by calling"
1.0.0.rc1,build_vocab with both the src and tgt data?
1.0.0.rc1,fast-forward if loaded from state
1.0.0.rc1,NOTE: `rnn.pack_padded_sequence` requires that a
1.0.0.rc1,"minibatch be sorted by decreasing order, which"
1.0.0.rc1,requires reversing relative to typical sort keys
1.0.0.rc1,Temporarily load one shard to retrieve sort_key for data_type
1.0.0.rc1,"NOTE: This is causing some issues for consumer/producer,"
1.0.0.rc1,as we may still have some of those examples in some queue
1.0.0.rc1,cur_dataset.examples = None
1.0.0.rc1,gc.collect()
1.0.0.rc1,del cur_dataset
1.0.0.rc1,gc.collect()
1.0.0.rc1,Cycle through the shards indefinitely.
1.0.0.rc1,"When the dataset is not repeated, we might need to ensure that"
1.0.0.rc1,the number of returned batches is the multiple of a given value.
1.0.0.rc1,This is important for multi GPU training to ensure that all
1.0.0.rc1,workers have the same number of batches to process.
1.0.0.rc1,Maintains the longest src and tgt length in the current batch
1.0.0.rc1,Reset current longest length at a new batch (count=1)
1.0.0.rc1,Src: [<bos> w1 ... wN <eos>]
1.0.0.rc1,Tgt: [w1 ... wM <eos>]
1.0.0.rc1,-*- coding: utf-8 -*-
1.0.0.rc1,imports of datatype-specific dependencies
1.0.0.rc1,torchaudio loading options recently changed. It's probably
1.0.0.rc1,straightforward to rewrite the audio handling to make use of
1.0.0.rc1,"up-to-date torchaudio, but in the meantime there is a legacy"
1.0.0.rc1,method which uses the old defaults
1.0.0.rc1,STFT
1.0.0.rc1,-*- coding: utf-8 -*-
1.0.0.rc1,domain specific dependencies
1.0.0.rc1,coding: utf-8
1.0.0.rc1,several data readers need optional dependencies. There's no
1.0.0.rc1,appropriate builtin exception
1.0.0.rc1,-*- coding: utf-8 -*-
1.0.0.rc1,mix this with partial
1.0.0.rc1,batch (list(list(list))): batch_size x len(self.fields) x seq_len
1.0.0.rc1,lengths: batch_size
1.0.0.rc1,data: seq_len x batch_size x len(self.fields)
1.0.0.rc1,flake8: noqa
1.0.0.rc1,For command-line option parsing
1.0.0.rc1,"Check pass, set the args."
1.0.0.rc1,"This SRU version implements its own cuda-level optimization,"
1.0.0.rc1,so it requires that:
1.0.0.rc1,1. `cupy` and `pynvrtc` python package installed.
1.0.0.rc1,2. pytorch is built with cuda support.
1.0.0.rc1,3. library path set: export LD_LIBRARY_PATH=<cuda lib path>.
1.0.0.rc1,Check 1.
1.0.0.rc1,Check 2.
1.0.0.rc1,Check 3.
1.0.0.rc1,This sets up device to use.
1.0.0.rc1,-> directions x batch x dim
1.0.0.rc1,For DEBUG
1.0.0.rc1,"size = (length, batch, x.size(-1)) \"
1.0.0.rc1,"if x.dim() == 3 else (batch, x.size(-1))"
1.0.0.rc1,grad_x = x.new(*x.size()) if k_ == 3 else x.new(*size).zero_()
1.0.0.rc1,Normal use
1.0.0.rc1,"An entry check here, will catch on train side and translate side"
1.0.0.rc1,if requirements are not satisfied.
1.0.0.rc1,RNNDecoderState wraps hidden as a tuple.
1.0.0.rc1,fh -> (layers*directions) x batch x dim
1.0.0.rc1,NOTE: We need to trim the vocab to remove any unk tokens that
1.0.0.rc1,were not originally here.
1.0.0.rc1,!/usr/bin/env python
1.0.0.rc1,!/usr/bin/env python
1.0.0.rc1,-*- coding: utf-8 -*-
1.0.0.rc1,!/usr/bin/env python
1.0.0.rc1,-*- coding: utf-8 -*-
1.0.0.rc1,create one counter per shard
1.0.0.rc1,"every corpus has shards, no new one"
1.0.0.rc1,!/usr/bin/env python
1.0.0.rc1,Load checkpoint if we resume from a previous training.
1.0.0.rc1,check for code where vocab is saved instead of fields
1.0.0.rc1,(in the future this will be done in a smarter way)
1.0.0.rc1,Create a thread to listen for errors in the child processes.
1.0.0.rc1,Train with multiprocessing.
1.0.0.rc1,generator_to_serve = iter(generator_to_serve)
1.0.0.rc1,hack to dodge unpicklable `dict_keys`
1.0.0.rc1,"propagate exception to parent process, keeping original traceback"
1.0.0.rc1,The score for each translation on the beam.
1.0.0.rc1,The backpointers at each time-step.
1.0.0.rc1,The outputs at each time-step.
1.0.0.rc1,Has EOS topped the beam yet.
1.0.0.rc1,The attentions (matrix) for each time.
1.0.0.rc1,Time and k pair for finished.
1.0.0.rc1,Information for global scoring.
1.0.0.rc1,Minimum prediction length
1.0.0.rc1,Apply Penalty at every step
1.0.0.rc1,force the output to be longer than self.min_length
1.0.0.rc1,assumes there are len(word_probs) predictions OTHER
1.0.0.rc1,than EOS that are greater than -1e20
1.0.0.rc1,Sum the previous scores.
1.0.0.rc1,Don't let EOS have children.
1.0.0.rc1,Block ngram repeats
1.0.0.rc1,"Last n tokens, n = block_ngram_repeat"
1.0.0.rc1,Skip the blocking if it is in the exclusion list
1.0.0.rc1,"best_scores_id is flattened beam x word array, so calculate which"
1.0.0.rc1,word and beam each score came from
1.0.0.rc1,End condition is when top-of-beam is EOS and no global score.
1.0.0.rc1,Add from beam until we have minimum outputs.
1.0.0.rc1,Term will be subtracted from probability
1.0.0.rc1,Probability will be divided by this
1.0.0.rc1,these warnings indicate that either the alpha/beta
1.0.0.rc1,"forces a penalty to be a no-op, or a penalty is a no-op but"
1.0.0.rc1,the alpha/beta would suggest otherwise.
1.0.0.rc1,using some length penalty
1.0.0.rc1,using some coverage penalty
1.0.0.rc1,"For temp=0.0, take the argmax to avoid divide-by-zero errors."
1.0.0.rc1,keep_topk=1 is also equivalent to argmax.
1.0.0.rc1,Set all logits that are not in the top-k to -10000.
1.0.0.rc1,This puts the probabilities close to 0.
1.0.0.rc1,"shape: (sum(~ self.is_finished), 1)"
1.0.0.rc1,magic indices
1.0.0.rc1,result caching
1.0.0.rc1,add one to account for BOS. Don't account for EOS because hitting
1.0.0.rc1,this implies it hasn't been found.
1.0.0.rc1,skip BOS
1.0.0.rc1,"Last n tokens, n = block_ngram_repeat"
1.0.0.rc1,skip the blocking if any token in gram is excluded
1.0.0.rc1,!/usr/bin/env python
1.0.0.rc1,Maintains the longest src and tgt length in the current batch
1.0.0.rc1,Reset current longest length at a new batch (count=1)
1.0.0.rc1,max_tgt_in_batch = 0
1.0.0.rc1,Src: [<bos> w1 ... wN <eos>]
1.0.0.rc1,Tgt: [w1 ... wM <eos>]
1.0.0.rc1,for debugging
1.0.0.rc1,Statistics
1.0.0.rc1,TODO: support these blacklisted features.
1.0.0.rc1,Encoder forward.
1.0.0.rc1,"Shape: (1, B, 1)"
1.0.0.rc1,Reorder states.
1.0.0.rc1,Turn any copied words into UNKs.
1.0.0.rc1,"Decoder forward, takes [tgt_len, batch, nfeats] as input"
1.0.0.rc1,"and [src_len, batch, hidden] as memory_bank"
1.0.0.rc1,"in case of inference tgt_len = 1, batch = beam times batch_size"
1.0.0.rc1,"in case of Gold Scoring tgt_len = actual length, batch = 1 batch"
1.0.0.rc1,Generator forward.
1.0.0.rc1,"returns [(batch_size x beam_size) , vocab ] when 1 step"
1.0.0.rc1,"or [ tgt_len, batch_size, vocab ] when full sentence"
1.0.0.rc1,"here we have scores [tgt_lenxbatch, vocab] or [beamxbatch, vocab]"
1.0.0.rc1,"returns [(batch_size x beam_size) , vocab ] when 1 step"
1.0.0.rc1,"or [ tgt_len, batch_size, vocab ] when full sentence"
1.0.0.rc1,TODO: support these blacklisted features.
1.0.0.rc1,(0) Prep the components of the search.
1.0.0.rc1,(1) Run the encoder on the src.
1.0.0.rc1,(2) Repeat src objects `beam_size` times.
1.0.0.rc1,We use batch_size x beam_size
1.0.0.rc1,"(0) pt 2, prep the beam object"
1.0.0.rc1,Reorder states.
1.0.0.rc1,"This is left in the code for now, but unsued"
1.0.0.rc1,(0) Prep each of the components of the search.
1.0.0.rc1,And helper method for reducing verbosity.
1.0.0.rc1,(1) Run the encoder on the src.
1.0.0.rc1,(2) Repeat src objects `beam_size` times.
1.0.0.rc1,We use now  batch_size x beam_size (same as fast mode)
1.0.0.rc1,"(3) run the decoder to generate sentences, using beam search."
1.0.0.rc1,(a) Construct batch x beam_size nxt words.
1.0.0.rc1,Get all the pending current beam words and arrange for forward.
1.0.0.rc1,(b) Decode and forward
1.0.0.rc1,(c) Advance each beam.
1.0.0.rc1,Loop over the batch_size number of beam
1.0.0.rc1,(4) Extract sentences from beam.
1.0.0.rc1,Rollback pointer to the beginning.
1.0.0.rc1,beam parameters
1.0.0.rc1,result caching
1.0.0.rc1,beam state
1.0.0.rc1,BoolTensor was introduced in pytorch 1.2
1.0.0.rc1,buffers for the topk scores and 'backpointer'
1.0.0.rc1,"""global state"" of the old beam"
1.0.0.rc1,for testing
1.0.0.rc1,using integer division to get an integer _B without casting
1.0.0.rc1,force the output to be longer than self.min_length
1.0.0.rc1,Multiply probs by the beam probability.
1.0.0.rc1,"if the sequence ends now, then the penalty is the current"
1.0.0.rc1,"length + 1, to include the EOS token"
1.0.0.rc1,Flatten probs into a list of possibilities.
1.0.0.rc1,Recover log probs.
1.0.0.rc1,Length penalty is just a scalar. It doesn't matter if it's applied
1.0.0.rc1,before or after the topk.
1.0.0.rc1,Resolve beam origin and map to batch index flat representation.
1.0.0.rc1,Append last prediction.
1.0.0.rc1,update global state (step == 1)
1.0.0.rc1,update global state (step > 1)
1.0.0.rc1,"shape: (batch_size x beam_size, 1)"
1.0.0.rc1,Penalize beams that finished.
1.0.0.rc1,"on real data (newstest2017) with the pretrained transformer,"
1.0.0.rc1,it's faster to not move this back to the original device
1.0.0.rc1,Store finished hypotheses for this batch.
1.0.0.rc1,End condition is the top beam finished and we can return
1.0.0.rc1,n_best hypotheses.
1.0.0.rc1,"If all sentences are translated, no need to go further."
1.0.0.rc1,Remove finished batches for the next step.
1.0.0.rc1,!/usr/bin/env python
1.0.0.rc1,semaphore doesn't have a timeout arg in Python 2.7
1.0.0.rc1,backwards compatibility for confs
1.0.0.rc1,load can be called multiple times: modify copy
1.0.0.rc1,NOTE: translator returns lists of `n_best` list
1.0.0.rc1,we can ignore that (i.e. flatten lists) only because
1.0.0.rc1,we restrict `n_best=1`
1.0.0.rc1,build back results with empty texts
1.0.0.rc1,Below are all the different penalty terms implemented so far.
1.0.0.rc1,Subtract coverage penalty from topk log probs.
1.0.0.rc1,Divide topk log probs by length penalty.
1.0.0.rc1,Sorting
1.0.0.rc1,Chinese segmentation
1.0.0.rc1,Chinese simplify -> Chinese traditional standard
1.0.0.rc1,Chinese simplify -> Chinese traditional (HongKong)
1.0.0.rc1,Chinese simplify -> Chinese traditional (Taiwan)
1.0.0.rc1,Chinese traditional -> Chinese simplify (v1)
1.0.0.rc1,Chinese traditional -> Chinese simplify (v2)
0.9.2,!/usr/bin/env python
0.9.2,!/usr/bin/env python
0.9.2,!/usr/bin/env python
0.9.2,-*- coding: utf-8 -*-
0.9.2,!/usr/bin/env python
0.9.2,-*- coding: utf-8 -*-
0.9.2,!/usr/bin/env python
0.9.2,Load checkpoint if we resume from a previous training.
0.9.2,check for code where vocab is saved instead of fields
0.9.2,(in the future this will be done in a smarter way)
0.9.2,Create a thread to listen for errors in the child processes.
0.9.2,Train with multiprocessing.
0.9.2,generator_to_serve = iter(generator_to_serve)
0.9.2,hack to dodge unpicklable `dict_keys`
0.9.2,"propagate exception to parent process, keeping original traceback"
0.9.2,!/usr/bin/env python3
0.9.2,-*- coding: utf-8 -*-
0.9.2,
0.9.2,"OpenNMT-py documentation build configuration file, created by"
0.9.2,sphinx-quickstart on Sun Dec 17 12:07:14 2017.
0.9.2,
0.9.2,This file is execfile()d with the current directory set to its
0.9.2,containing dir.
0.9.2,
0.9.2,Note that not all possible configuration values are present in this
0.9.2,autogenerated file.
0.9.2,
0.9.2,All configuration values have a default; values that are commented out
0.9.2,serve to show the default.
0.9.2,"If extensions (or modules to document with autodoc) are in another directory,"
0.9.2,add these directories to sys.path here. If the directory is relative to the
0.9.2,"documentation root, use os.path.abspath to make it absolute, like shown here."
0.9.2,
0.9.2,import os
0.9.2,import sys
0.9.2,"sys.path.insert(0, os.path.abspath('.'))"
0.9.2,-- General configuration ------------------------------------------------
0.9.2,"If your documentation needs a minimal Sphinx version, state it here."
0.9.2,
0.9.2,needs_sphinx = '1.0'
0.9.2,"Add any Sphinx extension module names here, as strings. They can be"
0.9.2,extensions coming with Sphinx (named 'sphinx.ext.*') or your custom
0.9.2,ones.
0.9.2,Show base classes
0.9.2,"Use ""variables"" section for Attributes instead of weird block things"
0.9.2,mimicking the function style.
0.9.2,"Add any paths that contain templates here, relative to this directory."
0.9.2,The suffix(es) of source filenames.
0.9.2,You can specify multiple suffix as a list of string:
0.9.2,
0.9.2,"source_suffix = ['.rst', '.md']"
0.9.2,The master toctree document.
0.9.2,General information about the project.
0.9.2,"The version info for the project you're documenting, acts as replacement for"
0.9.2,"|version| and |release|, also used in various other places throughout the"
0.9.2,built documents.
0.9.2,
0.9.2,The short X.Y version.
0.9.2,"The full version, including alpha/beta/rc tags."
0.9.2,The language for content autogenerated by Sphinx. Refer to documentation
0.9.2,for a list of supported languages.
0.9.2,
0.9.2,This is also used if you do content translation via gettext catalogs.
0.9.2,"Usually you set ""language"" from the command line for these cases."
0.9.2,"List of patterns, relative to source directory, that match files and"
0.9.2,directories to ignore when looking for source files.
0.9.2,This patterns also effect to html_static_path and html_extra_path
0.9.2,The name of the Pygments (syntax highlighting) style to use.
0.9.2,"If true, `todo` and `todoList` produce output, else they produce nothing."
0.9.2,-- Options for HTML output ----------------------------------------------
0.9.2,The theme to use for HTML and HTML Help pages.  See the documentation for
0.9.2,a list of builtin themes.
0.9.2,
0.9.2,html_theme = 'sphinx_materialdesign_theme'
0.9.2,html_theme_path = [sphinx_materialdesign_theme.get_path()]
0.9.2,Theme options are theme-specific and customize the look and feel of a theme
0.9.2,"further.  For a list of options available for each theme, see the"
0.9.2,documentation.
0.9.2,
0.9.2,html_theme_options = {}
0.9.2,"Add any paths that contain custom static files (such as style sheets) here,"
0.9.2,"relative to this directory. They are copied after the builtin static files,"
0.9.2,"so a file named ""default.css"" will overwrite the builtin ""default.css""."
0.9.2,"Custom sidebar templates, must be a dictionary that maps document names"
0.9.2,to template names.
0.9.2,
0.9.2,This is required for the alabaster theme
0.9.2,refs: http://alabaster.readthedocs.io/en/latest/installation.html#sidebars
0.9.2,-- Options for HTMLHelp output ------------------------------------------
0.9.2,Output file base name for HTML help builder.
0.9.2,-- Options for LaTeX output ---------------------------------------------
0.9.2,The paper size ('letterpaper' or 'a4paper').
0.9.2,
0.9.2,"'papersize': 'letterpaper',"
0.9.2,"The font size ('10pt', '11pt' or '12pt')."
0.9.2,
0.9.2,"'pointsize': '10pt',"
0.9.2,Additional stuff for the LaTeX preamble.
0.9.2,
0.9.2,"'preamble': '',"
0.9.2,Latex figure (float) alignment
0.9.2,
0.9.2,"'figure_align': 'htbp',"
0.9.2,Grouping the document tree into LaTeX files. List of tuples
0.9.2,"(source start file, target name, title,"
0.9.2,"author, documentclass [howto, manual, or own class])."
0.9.2,-- Options for manual page output ---------------------------------------
0.9.2,One entry per manual page. List of tuples
0.9.2,"(source start file, name, description, authors, manual section)."
0.9.2,-- Options for Texinfo output -------------------------------------------
0.9.2,Grouping the document tree into Texinfo files. List of tuples
0.9.2,"(source start file, target name, title, author,"
0.9.2,"dir menu entry, description, category)"
0.9.2,degenerate case
0.9.2,cache the features
0.9.2,mp queues don't work well between procs unless they're from a manager
0.9.2,each device has its own saver so that reconstructing is easier
0.9.2,!/usr/bin/env python
0.9.2,-*- coding: utf-8 -*-
0.9.2,is this reachable?
0.9.2,Read in embeddings
0.9.2,Write to file
0.9.2,!/usr/bin/env python
0.9.2,-*- coding: utf-8 -*-
0.9.2,"Add in default model arguments, possibly added since training."
0.9.2,-*- encoding: utf-8 -*-
0.9.2,!/usr/bin/env python
0.9.2,-*- coding: utf-8 -*-
0.9.2,Author: Rico Sennrich
0.9.2,flake8: noqa
0.9.2,This file is retrieved from https://github.com/rsennrich/subword-nmt
0.9.2,hack for python2/3 compatibility
0.9.2,check version information
0.9.2,some hacking to deal with duplicates (only consider first instance)
0.9.2,don't print end-of-word symbols
0.9.2,sys.stderr.write('cannot split {0} further.\n'.format(segment))
0.9.2,sys.stderr.write('OOV: {0}\n'.format(segment))
0.9.2,sys.stderr.write('OOV: {0}\n'.format(segment))
0.9.2,python 2/3 compatibility
0.9.2,read/write files as UTF-8
0.9.2,!/usr/bin/env python
0.9.2,!/usr/bin/env python
0.9.2,-*- coding: utf-8 -*-
0.9.2,Author: Rico Sennrich
0.9.2,flake8: noqa
0.9.2,This file is retrieved from https://github.com/rsennrich/subword-nmt
0.9.2,hack for python2/3 compatibility
0.9.2,"find all instances of pair, and update frequency/indices around it"
0.9.2,find first symbol
0.9.2,"if first symbol is followed by second symbol, we've found an occurrence of pair (old_word[i:i+2])"
0.9.2,"assuming a symbol sequence ""A B C"", if ""B C"" is merged, reduce the frequency of ""A B"""
0.9.2,"assuming a symbol sequence ""A B C B"", if ""B C"" is merged, reduce the frequency of ""C B""."
0.9.2,"however, skip this if the sequence is A B C B C, because the frequency of ""C B"" will be reduced by the previous code block"
0.9.2,find new pair
0.9.2,"assuming a symbol sequence ""A BC D"", if ""B C"" is merged, increase the frequency of ""A BC"""
0.9.2,"assuming a symbol sequence ""A BC B"", if ""B C"" is merged, increase the frequency of ""BC B"""
0.9.2,"however, if the sequence is A BC BC, skip this step because the count of ""BC BC"" will be incremented by the previous code block"
0.9.2,data structure of pair frequencies
0.9.2,index from pairs to words
0.9.2,version 0.2 changes the handling of the end-of-word token ('</w>');
0.9.2,version numbering allows bckward compatibility
0.9.2,"threshold is inspired by Zipfian assumption, but should only affect speed"
0.9.2,we probably missed the best pair because of pruning; go back to full statistics
0.9.2,"threshold is inspired by Zipfian assumption, but should only affect speed"
0.9.2,python 2/3 compatibility
0.9.2,read/write files as UTF-8
0.9.2,!/usr/bin/env python
0.9.2,for back compat when attention_dropout was not defined
0.9.2,Build embeddings.
0.9.2,Build encoder.
0.9.2,Build decoder.
0.9.2,Share the embedding matrix - preprocess with share_vocab required.
0.9.2,src/tgt vocab should be the same if `-share_vocab` is specified.
0.9.2,Build NMTModel(= encoder + decoder).
0.9.2,Build Generator.
0.9.2,Load the model states from checkpoint or initialize them.
0.9.2,This preserves backward-compat for models using customed layernorm
0.9.2,end of patch for backward compatibility
0.9.2,!/usr/bin/env python
0.9.2,NOTE: It's important that ``opt`` has been validated and updated
0.9.2,at this point.
0.9.2,Load checkpoint if we resume from a previous training.
0.9.2,check for code where vocab is saved instead of fields
0.9.2,(in the future this will be done in a smarter way)
0.9.2,"Report src and tgt vocab sizes, including for features"
0.9.2,Build model.
0.9.2,Build optimizer.
0.9.2,Build model saver
0.9.2,Embedding Options
0.9.2,Encoder-Decoder Options
0.9.2,"group.add('--residual', '-residual',   action=""store_true"","
0.9.2,"help=""Add residual connections between RNN layers."")"
0.9.2,Attention options
0.9.2,Generator and loss options.
0.9.2,Data options
0.9.2,"Dictionary options, for text corpus"
0.9.2,"if you want to pass an existing vocab.pt file, pass it to"
0.9.2,-src_vocab alone as it already contains tgt vocab.
0.9.2,"Truncation options, for text corpus"
0.9.2,Data processing options
0.9.2,Options most relevant to speech
0.9.2,Option most relevant to image input
0.9.2,GPU
0.9.2,Init options
0.9.2,Pretrained word vectors
0.9.2,Fixed word vectors
0.9.2,Optimization options
0.9.2,learning rate
0.9.2,Use Tensorboard for visualization during training
0.9.2,Options most relevant to speech
0.9.2,Option most relevant to image input
0.9.2,Options most relevant to summarization.
0.9.2,Alpha and Beta values for Google Length + Coverage penalty
0.9.2,"Described here: https://arxiv.org/pdf/1609.08144.pdf, Section 7"
0.9.2,Options most relevant to speech.
0.9.2,Option most relevant to image input
0.9.2,Copyright 2016 The Chromium Authors. All rights reserved.
0.9.2,Use of this source code is governed by a BSD-style license that can be
0.9.2,found in the LICENSE file.
0.9.2,"Get the key 'value' in the dict, or just use 'value'"
0.9.2,Basic attributes.
0.9.2,Set model in training mode.
0.9.2,UPDATE DROPOUT
0.9.2,Run patience mechanism
0.9.2,"If the patience has reached the limit, stop training"
0.9.2,Set model in validating mode.
0.9.2,F-prop through the model.
0.9.2,Compute loss.
0.9.2,Update statistics.
0.9.2,Set model back to training mode.
0.9.2,Truncated BPTT: reminder not compatible with accum > 1
0.9.2,1. Create truncated target.
0.9.2,2. F-prop all but generator.
0.9.2,3. Compute loss.
0.9.2,4. Update the parameters and statistics.
0.9.2,Multi GPU gradient gather
0.9.2,"If truncated, don't backprop fully."
0.9.2,TO CHECK
0.9.2,if dec_state is not None:
0.9.2,dec_state.detach()
0.9.2,"in case of multi step gradient accumulation,"
0.9.2,update only after accum batches
0.9.2,For Flake
0.9.2,we avoid padding while mean pooling
0.9.2,Initialize the bridge layer
0.9.2,"s_len, batch, emb_dim = emb.size()"
0.9.2,Lengths data is wrapped inside a Tensor.
0.9.2,"LSTM has hidden and cell state, other only one"
0.9.2,Total number of states
0.9.2,Build a linear layer for each
0.9.2,The encoder hidden is  (layers*directions) x batch x dim.
0.9.2,"s_len, batch, emb_dim = emb.size()"
0.9.2,Run the forward pass of every layer of the tranformer.
0.9.2,why is the model_opt.__dict__ check necessary?
0.9.2,"(batch_size, 64, imgH, imgW)"
0.9.2,layer 1
0.9.2,"(batch_size, 64, imgH/2, imgW/2)"
0.9.2,"(batch_size, 128, imgH/2, imgW/2)"
0.9.2,layer 2
0.9.2,"(batch_size, 128, imgH/2/2, imgW/2/2)"
0.9.2,"(batch_size, 256, imgH/2/2, imgW/2/2)"
0.9.2,layer 3
0.9.2,batch norm 1
0.9.2,"(batch_size, 256, imgH/2/2, imgW/2/2)"
0.9.2,layer4
0.9.2,"(batch_size, 256, imgH/2/2/2, imgW/2/2)"
0.9.2,"(batch_size, 512, imgH/2/2/2, imgW/2/2)"
0.9.2,layer 5
0.9.2,batch norm 2
0.9.2,"(batch_size, 512, imgH/2/2/2, imgW/2/2/2)"
0.9.2,"(batch_size, 512, imgH/2/2/2, imgW/2/2/2)"
0.9.2,"# (batch_size, 512, H, W)"
0.9.2,Dimensions and padding for constructing the word embedding matrix
0.9.2,Dimensions and padding for feature embedding matrices
0.9.2,(these have no effect if feat_vocab_sizes is empty)
0.9.2,The embedding matrix look-up tables. The first look-up table
0.9.2,"is for words. Subsequent ones are for features, if any exist."
0.9.2,The final output size of word + feature vectors. This can vary
0.9.2,from the word vector size if and only if features are defined.
0.9.2,This is the attribute you should access if you need to know
0.9.2,how big your embeddings are going to be.
0.9.2,The sequence of operations that converts the input sequence
0.9.2,into a sequence of embeddings. At minimum this consists of
0.9.2,looking up the embeddings for each word and feature in the
0.9.2,input. Model parameters may require the sequence to contain
0.9.2,additional operations as well.
0.9.2,features must use word_vec_size
0.9.2,features will use feat_vec_size
0.9.2,This class is mainly used by decoder.py for RNNs but also
0.9.2,by the CNN / transformer decoder when copy attention is used
0.9.2,CNN has its own attention mechanism ConvMultiStepAttention
0.9.2,Transformer has its own MultiHeadedAttention
0.9.2,mlp wants it with bias
0.9.2,Check input sizes
0.9.2,"(batch, t_len, d) x (batch, d, s_len) --> (batch, t_len, s_len)"
0.9.2,"(batch, t_len, s_len, d)"
0.9.2,one step input
0.9.2,"compute attention scores, as in Luong et al."
0.9.2,Softmax or sparsemax to normalize attention weights
0.9.2,each context vector c_t is the weighted average
0.9.2,over all the source hidden states
0.9.2,concatenate
0.9.2,Check output sizes
0.9.2,Check output sizes
0.9.2,clamping necessary because of numerical errors: loss should be lower
0.9.2,"bounded by zero, but negative values near zero are possible without"
0.9.2,the clamp
0.9.2,from onmt.utils.misc import aeq
0.9.2,CHECKS
0.9.2,"batch, k_len, d = key.size()"
0.9.2,"batch_, k_len_, d_ = value.size()"
0.9.2,"aeq(batch, batch_)"
0.9.2,"aeq(k_len, k_len_)"
0.9.2,"aeq(d, d_)"
0.9.2,"batch_, q_len, d_ = query.size()"
0.9.2,"aeq(batch, batch_)"
0.9.2,"aeq(d, d_)"
0.9.2,"aeq(self.model_dim % 8, 0)"
0.9.2,if mask is not None:
0.9.2,"batch_, q_len_, k_len_ = mask.size()"
0.9.2,"aeq(batch_, batch)"
0.9.2,"aeq(k_len_, k_len)"
0.9.2,aeq(q_len_ == q_len)
0.9.2,END CHECKS
0.9.2,"1) Project key, value, and query."
0.9.2,1 or key_len x key_len
0.9.2,1 or key_len x key_len x dim_per_head
0.9.2,1 or key_len x key_len x dim_per_head
0.9.2,2) Calculate and scale scores.
0.9.2,batch x num_heads x query_len x key_len
0.9.2,3) Apply attention dropout and compute context vectors.
0.9.2,CHECK
0.9.2,"batch_, q_len_, d_ = output.size()"
0.9.2,"aeq(q_len, q_len_)"
0.9.2,"aeq(batch, batch_)"
0.9.2,"aeq(d, d_)"
0.9.2,Return one attn
0.9.2,At the moment this class is only used by embeddings.Embeddings look-up tables
0.9.2,-*- coding: utf-8 -*-
0.9.2,checks
0.9.2,"batch, channel, height, width = base_target_emb.size()"
0.9.2,"batch_, channel_, height_, width_ = input_from_dec.size()"
0.9.2,"enc_batch, enc_channel, enc_height = encoder_out_top.size()"
0.9.2,"enc_batch_, enc_channel_, enc_height_ = encoder_out_combine.size()"
0.9.2,out_features * in_features
0.9.2,norm is out_features * 1
0.9.2,batch_size * out_features
0.9.2,out_features
0.9.2,out_features
0.9.2,batch_size * out_features
0.9.2,"out_channels, in_channels // groups, * kernel_size"
0.9.2,out_features
0.9.2,This is used nowhere in the code at the moment (Vincent Nguyen 05/18/2018)
0.9.2,"in_channels, out_channels, *kernel_size"
0.9.2,"in_channels, out_channels, *kernel_size"
0.9.2,"self.out_channels, 1"
0.9.2,out_features
0.9.2,out_features
0.9.2,store roots on diagonal
0.9.2,CHECKS
0.9.2,Original probabilities.
0.9.2,Probability of copying p(z=1) batch.
0.9.2,Probability of not copying: p_{word}(w) * (1 - p(z))
0.9.2,probabilities assigned by the model to the gold targets
0.9.2,probability of tokens copied from source
0.9.2,Set scores for unk to 0 and add eps
0.9.2,find the indices in which you do not use the copy mechanism
0.9.2,Drop padding.
0.9.2,this block does not depend on the loss value computed above
0.9.2,and is used only for stats
0.9.2,this block does not depend on the loss value computed above
0.9.2,and is used only for stats
0.9.2,Correct target copy token instead of <unk>
0.9.2,tgt[i] = align[i] + len(tgt_vocab)
0.9.2,for i such that tgt[i] == 0 and align[i] != 0
0.9.2,Compute sum of perplexities for stats
0.9.2,this part looks like it belongs in CopyGeneratorLoss
0.9.2,Compute Loss as NLL divided by seq length
0.9.2,Compute Total Loss per sequence in batch
0.9.2,Divide by length of each sequence and sum
0.9.2,all beams repeat (beam >= 1 repeat dummy scores)
0.9.2,predict repeat_idx over and over again
0.9.2,beam 0 and beam >=2 will repeat (beam >= 2 repeat dummy scores)
0.9.2,non-interesting beams are going to get dummy values
0.9.2,"on initial round, only predicted scores for beam 0"
0.9.2,matter. Make two predictions. Top one will be repeated
0.9.2,"in beam zero, second one will live on in beam 1."
0.9.2,predict the same thing in beam 0
0.9.2,continue pushing around what beam 1 predicts
0.9.2,"now beam 0 dies (along with the others), beam 1 -> beam 0"
0.9.2,beam 0 and beam >= 2 will repeat (beam 2 repeats excluded idx)
0.9.2,non-interesting beams are going to get dummy values
0.9.2,predict the same thing in beam 0
0.9.2,continue pushing around what beam 1 predicts
0.9.2,predict the allowed-repeat again in beam 2
0.9.2,"now beam 0 dies, beam 1 -> beam 0, beam 2 -> beam 1"
0.9.2,and the rest die
0.9.2,"since all preds after i=0 are 0, we can check"
0.9.2,that the beam is the correct idx by checking that
0.9.2,the curr score is the initial score
0.9.2,beam 0 will always predict EOS. The other beams will predict
0.9.2,non-eos scores.
0.9.2,"this is also a test that when block_ngram_repeat=0,"
0.9.2,repeating is acceptable
0.9.2,non-interesting beams are going to get dummy values
0.9.2,"""best"" prediction is eos - that should be blocked"
0.9.2,include at least beam_sz predictions OTHER than EOS
0.9.2,that are greater than -1e20
0.9.2,predict eos in beam 0
0.9.2,provide beam_sz other good predictions
0.9.2,now the top beam has ended and no others have
0.9.2,first beam finished had length beam.min_length
0.9.2,first beam finished was 0
0.9.2,"not of interest, but want to make sure it keeps running"
0.9.2,since only beam 0 terminates and n_best = 2
0.9.2,"this is also a test that when block_ngram_repeat=0,"
0.9.2,repeating is acceptable
0.9.2,non-interesting beams are going to get dummy values
0.9.2,"""best"" prediction is eos - that should be blocked"
0.9.2,include at least beam_sz predictions OTHER than EOS
0.9.2,that are greater than -1e20
0.9.2,predict eos in beam 1
0.9.2,provide beam_sz other good predictions in other beams
0.9.2,provide beam_sz other good predictions in other beams
0.9.2,beam 1 dies on min_length
0.9.2,beam 0 dies on the step after beam 1 dies
0.9.2,"init_preds: [4, 3, 5, 6, 7] - no EOS's"
0.9.2,no EOS's yet
0.9.2,"[5, 3, 2, 6, 0], so beam 2 predicts EOS!"
0.9.2,assumes beam 2 finished on last step
0.9.2,"[2, 5, 3, 6, 0], so beam 0 predicts EOS!"
0.9.2,"[-2.4879, -3.8910, -4.1010, -4.2010, -4.4010]"
0.9.2,new beam 0 finished
0.9.2,new beam 0 is old beam 3
0.9.2,assumes beam 0 finished on last step
0.9.2,"[5, 2, 6, 1, 0], so beam 1 predicts EOS!"
0.9.2,new beam 1 finished
0.9.2,new beam 1 is old beam 4
0.9.2,this could be considered an integration test because it tests
0.9.2,interactions between the GNMT scorer and the beam
0.9.2,initialize fields at the top of each unit test to prevent
0.9.2,any undesired stateful effects
0.9.2,"this test touches the file system, so it could be considered an"
0.9.2,integration test
0.9.2,write utf-8 bytes
0.9.2,predict repeat_idx over and over again
0.9.2,"batch 0 and 7 will repeat, the rest will advance"
0.9.2,predict the same thing in batch 0 and 7 every i
0.9.2,push around what the other batches predict
0.9.2,now batch 0 and 7 die
0.9.2,"batch 0 will repeat excluded idx, batch 1 will repeat"
0.9.2,now batch 1 dies
0.9.2,batch 0 will always predict EOS. The other batches will predict
0.9.2,non-eos scores.
0.9.2,"""best"" prediction is eos - that should be blocked"
0.9.2,include at least one prediction OTHER than EOS
0.9.2,that is greater than -1e20
0.9.2,now batch 0 has ended and no others have
0.9.2,initial step
0.9.2,batch 0 dies on step 0
0.9.2,include at least one prediction OTHER than EOS
0.9.2,that is greater than -1e20
0.9.2,step 2
0.9.2,(old) batch 8 dies on step 1
0.9.2,step 3
0.9.2,everything dies
0.9.2,initial step
0.9.2,batch 0 dies on step 0
0.9.2,include at least one prediction OTHER than EOS
0.9.2,that is greater than -1e20
0.9.2,step 2
0.9.2,(old) batch 8 dies on step 1
0.9.2,step 3
0.9.2,everything dies
0.9.2,illegal_weights_mask = torch.ByteTensor([
0.9.2,"[0, 0, 0, 0, 0, 0, 0],"
0.9.2,"[0, 0, 0, 1, 1, 1, 1],"
0.9.2,"[0, 0, 0, 0, 0, 1, 1],"
0.9.2,"[0, 0, 1, 1, 1, 1, 1]])"
0.9.2,TODO: fix for pytorch 0.3
0.9.2,illegal_weights = alignments.masked_select(illegal_weights_mask)
0.9.2,"self.assertEqual(0.0, illegal_weights.data.sum())"
0.9.2,this could be considered an integration test because it touches
0.9.2,the filesystem for the config file (and the models)
0.9.2,-*- coding: utf-8 -*-
0.9.2,tests pad and numericalize integration
0.9.2,tests pad and numericalize integration
0.9.2,"this test touches the file system, so it could be considered an"
0.9.2,integration test
0.9.2,file to hold full paths to audio data
0.9.2,file to hold audio paths relative to _AUDIO_DATA_DIR (i.e. file names)
0.9.2,it's ok if non-audio files co-exist with audio files in the data dir
0.9.2,"dividing gets the noise in [-1, 1]"
0.9.2,"this test touches the file system, so it could be considered an"
0.9.2,integration test
0.9.2,file to hold full paths to image data
0.9.2,file to hold image paths relative to _IMG_DATA_DIR (i.e. file names)
0.9.2,it's ok if non-image files co-exist with image files in the data dir
0.9.2,all beams repeat (beam >= 1 repeat dummy scores)
0.9.2,predict repeat_idx over and over again
0.9.2,beam 0 and beam >=2 will repeat (beam >= 2 repeat dummy scores)
0.9.2,non-interesting beams are going to get dummy values
0.9.2,"on initial round, only predicted scores for beam 0"
0.9.2,matter. Make two predictions. Top one will be repeated
0.9.2,"in beam zero, second one will live on in beam 1."
0.9.2,predict the same thing in beam 0
0.9.2,continue pushing around what beam 1 predicts
0.9.2,"now beam 0 dies (along with the others), beam 1 -> beam 0"
0.9.2,beam 0 and beam >= 2 will repeat (beam 2 repeats excluded idx)
0.9.2,non-interesting beams are going to get dummy values
0.9.2,predict the same thing in beam 0
0.9.2,continue pushing around what beam 1 predicts
0.9.2,predict the allowed-repeat again in beam 2
0.9.2,"now beam 0 dies, beam 1 -> beam 0, beam 2 -> beam 1"
0.9.2,and the rest die
0.9.2,"since all preds after i=0 are 0, we can check"
0.9.2,that the beam is the correct idx by checking that
0.9.2,the curr score is the initial score
0.9.2,beam 0 will always predict EOS. The other beams will predict
0.9.2,non-eos scores.
0.9.2,non-interesting beams are going to get dummy values
0.9.2,"""best"" prediction is eos - that should be blocked"
0.9.2,include at least beam_sz predictions OTHER than EOS
0.9.2,that are greater than -1e20
0.9.2,predict eos in beam 0
0.9.2,provide beam_sz other good predictions
0.9.2,now the top beam has ended and no others have
0.9.2,"not of interest, but want to make sure it keeps running"
0.9.2,since only beam 0 terminates and n_best = 2
0.9.2,"this is also a test that when block_ngram_repeat=0,"
0.9.2,repeating is acceptable
0.9.2,non-interesting beams are going to get dummy values
0.9.2,"""best"" prediction is eos - that should be blocked"
0.9.2,include at least beam_sz predictions OTHER than EOS
0.9.2,that are greater than -1e20
0.9.2,predict eos in beam 1
0.9.2,provide beam_sz other good predictions in other beams
0.9.2,provide beam_sz other good predictions in other beams
0.9.2,beam 1 dies on min_length
0.9.2,beam 0 dies on the step after beam 1 dies
0.9.2,non-interesting beams are going to get dummy values
0.9.2,"""best"" prediction is eos - that should be blocked"
0.9.2,include at least beam_sz predictions OTHER than EOS
0.9.2,that are greater than -1e20
0.9.2,predict eos in beam 1
0.9.2,provide beam_sz other good predictions in other beams
0.9.2,provide beam_sz other good predictions in other beams
0.9.2,no top beams are finished yet
0.9.2,beam 1 dies on min_length
0.9.2,no top beams are finished yet
0.9.2,beam 0 dies on the step after beam 1 dies
0.9.2,top beam is finished now so there are attentions
0.9.2,two beams are finished in each batch
0.9.2,second dim is cut down to the non-padded src length
0.9.2,first dim is equal to the time of death
0.9.2,(beam 0 died at current step - adjust for SOS)
0.9.2,(beam 1 died at last step - adjust for SOS)
0.9.2,behavior gets weird when beam is already done so just stop
0.9.2,this is just test_beam.TestBeamAgainstReferenceCase repeated
0.9.2,in each batch.
0.9.2,"init_preds: [4, 3, 5, 6, 7] - no EOS's"
0.9.2,no EOS's yet
0.9.2,"[5, 3, 2, 6, 0], so beam 2 predicts EOS!"
0.9.2,assumes beam 2 finished on last step
0.9.2,ended beam 2 shouldn't continue
0.9.2,"[2, 5, 3, 6, 0] repeat self.BATCH_SZ, so beam 0 predicts EOS!"
0.9.2,"[-2.4879, -3.8910, -4.1010, -4.2010, -4.4010] repeat self.BATCH_SZ"
0.9.2,another beam is finished in all batches
0.9.2,new beam 0 finished
0.9.2,new beam 0 is old beam 3
0.9.2,assumes beam 0 finished on last step
0.9.2,"[5, 2, 6, 1, 0] repeat self.BATCH_SZ, so beam 1 predicts EOS!"
0.9.2,new beam 1 finished
0.9.2,new beam 1 is old beam 4
0.9.2,this could be considered an integration test because it tests
0.9.2,interactions between the GNMT scorer and the beam
0.9.2,"-data option is required, but not used in this test, so dummy."
0.9.2,len x batch x nfeat
0.9.2,batch x c x h x w
0.9.2,batch x 1 x nfft x t
0.9.2,Initialize vectors to compare size with
0.9.2,Ensure correct sizes and types
0.9.2,Make sure that output has the correct size and type
0.9.2,Make sure that output has the correct size and type
0.9.2,Make sure that output has the correct size and type
0.9.2,"[('encoder_type', 'transformer'),"
0.9.2,"('word_vec_size', 16), ('rnn_size', 16)],"
0.9.2,""""""" Only do SRU test if requirment is safisfied. """""""
0.9.2,SRU doesn't support input_feed.
0.9.2,"when reasonable, set audio_enc_pooling to 2"
0.9.2,Need lengths >= audio_enc_pooling**n_layers.
0.9.2,"That condition is unrealistic for large n_layers,"
0.9.2,so leave audio_enc_pooling at 1.
0.9.2,first check there's nothing unexpectedly not trainable
0.9.2,ok: word embeddings shouldn't be trainable
0.9.2,if word vecs are fixed
0.9.2,ok: positional encodings shouldn't be trainable
0.9.2,then check nothing unexpectedly trainable
0.9.2,!/usr/bin/env python
0.9.2,-*- coding: utf-8 -*-
0.9.2,Remove the generated *pt files.
0.9.2,Test image preprocessing
0.9.2,Test audio preprocessing
0.9.2,Decoder state
0.9.2,Build the RNN.
0.9.2,Set up the context gate.
0.9.2,Set up the standard attention.
0.9.2,The encoder hidden is  (layers*directions) x batch x dim.
0.9.2,We need to convert it to layers x batch x (directions*dim).
0.9.2,Init the input feed.
0.9.2,Update the state with the result.
0.9.2,Concatenates sequence of tensors along a new dimension.
0.9.2,NOTE: v0.3 to 0.4: dec_outs / attns[*] may not be list
0.9.2,(in particular in case of SRU) it was not raising error in 0.3
0.9.2,since stack(Variable) was allowed.
0.9.2,"In 0.4, SRU returns a tensor that shouldn't be stacke"
0.9.2,Check
0.9.2,Calculate the attention.
0.9.2,Calculate the context gate.
0.9.2,Additional args check.
0.9.2,END Additional args check.
0.9.2,Input feed concatenates hidden state with
0.9.2,input at every time step.
0.9.2,TODO: context gate should be employed
0.9.2,instead of second RNN transform.
0.9.2,Update the coverage attention.
0.9.2,Decoder State
0.9.2,CNNDecoder has its own attention mechanism.
0.9.2,Set up a separate copy attention layer if needed.
0.9.2,The output of CNNEncoder.
0.9.2,The combination of output of CNNEncoder and source embeddings.
0.9.2,Process the result and update the attentions.
0.9.2,Update the state.
0.9.2,TODO change the way attns is returned dict => list or tuple (onnx)
0.9.2,Memory_lengths is a single tensor shared between all models.
0.9.2,This assumption will not hold if Translator is modified
0.9.2,to calculate memory_lengths as something other than the length
0.9.2,of the input.
0.9.2,BoolTensor was introduced in pytorch 1.2
0.9.2,Decoder State
0.9.2,"previously, there was a GlobalAttention module here for copy"
0.9.2,"attention. But it was never actually used -- the ""copy"" attention"
0.9.2,just reuses the context attention.
0.9.2,TODO change the way attns is returned dict => list or tuple (onnx)
0.9.2,"buffer size in bytes, determine equiv. # of elements based on data type"
0.9.2,copy tensors into buffer_t
0.9.2,all-reduce and rescale
0.9.2,copy all-reduced buffer back into tensors
0.9.2,"tensor is bigger than buffer, all-reduce and rescale directly"
0.9.2,"buffer is full, all-reduce and replace buffer with grad"
0.9.2,add tensor to buffer
0.9.2,TODO: Find a better way to check for sparse gradients.
0.9.2,Load everything from the checkpoint.
0.9.2,Build everything from scratch.
0.9.2,"Reset optimizer, keep options."
0.9.2,"Reset options, keep optimizer."
0.9.2,State can be partially restored.
0.9.2,Code below is an implementation of https://arxiv.org/pdf/1804.04235.pdf
0.9.2,inspired but modified from https://github.com/DeadAt0m/adafactor-pytorch
0.9.2,-*- coding: utf-8 -*-
0.9.2,if the loss function operates on vectors of raw logits instead of
0.9.2,"probabilities, only the first part of the generator needs to be"
0.9.2,"passed to the NMTLossCompute. At the moment, the only supported"
0.9.2,loss function of this kind is the sparsemax loss.
0.9.2,non_none: the subdict of the state dictionary where the values
0.9.2,are not None.
0.9.2,"Now, the iteration:"
0.9.2,state is a dictionary of sequences of tensor-like but we
0.9.2,want a sequence of dictionaries of tensors.
0.9.2,"First, unzip the dictionary into a sequence of keys and a"
0.9.2,sequence of tensor-like sequences.
0.9.2,"Now, yield a dictionary for each shard. The keys are always"
0.9.2,the same. values is a sequence of length #keys where each
0.9.2,element is a sequence of length #shards. We want to iterate
0.9.2,"over the shards, not over the keys: therefore, the values need"
0.9.2,to be re-zipped by shard and then each shard can be paired
0.9.2,with the keys.
0.9.2,Assumed backprop'd
0.9.2,Log the progress using the number of batches on the x-axis.
0.9.2,this check is here because audio allows the encoder and decoder to
0.9.2,"be different sizes, but other model types do not yet"
0.9.2,"Load default opt values, then overwrite with the opts in"
0.9.2,"the checkpoint. That way, if there are new options added,"
0.9.2,the defaults are used.
0.9.2,Don't do anything
0.9.2,Update best score of each criteria
0.9.2,Reset tolerance
0.9.2,Update current status
0.9.2,Decrease tolerance
0.9.2,Log
0.9.2,Log
0.9.2,Get a list of world_size lists with len(stat_list) Statistics objects
0.9.2,SRU doesn't support PackedSequence.
0.9.2,-*- coding: utf-8 -*-
0.9.2,this one is needed for torchtext random call (shuffled iterator)
0.9.2,in multi gpu it ensures datasets are read in the same order
0.9.2,some cudnn methods can be random even after fixing the seed
0.9.2,unless you tell it to be deterministic
0.9.2,These ensure same initialization in multi gpu mode
0.9.2,Shift values to be >= 0
0.9.2,coding: utf-8
0.9.2,make a small vocab containing just the tokens in the source sequence
0.9.2,Map source tokens to indices in the dynamic dict.
0.9.2,self.src_vocabs is used in collapse_copy_scores and Translator.py
0.9.2,this assumes src_field and tgt_field are both text
0.9.2,fields needs to have only keys that examples have as attrs
0.9.2,avoid infinite recursion when fields isn't defined
0.9.2,-*- coding: utf-8 -*-
0.9.2,backwards compatibility
0.9.2,monkey-patch to make torchtext Vocab's pickleable
0.9.2,"List[Tuple[str, Vocab]] -> List[Tuple[str, Field]]"
0.9.2,"-> dict[str, Field]"
0.9.2,"Dict[str, List[Tuple[str, Field]]]"
0.9.2,doesn't change structure - don't return early.
0.9.2,"Dict[str, List[Tuple[str, Field]]] -> List[Tuple[str, Field]]"
0.9.2,"-> dict[str, Field]"
0.9.2,"if tgt isn't using TextMultiField, then no text field is."
0.9.2,this is basically copy-pasted from torchtext.
0.9.2,counters changes in place
0.9.2,keep the order of tokens specified in the vocab file by
0.9.2,adding them to the counter with decreasing counting values
0.9.2,`tgt_vocab_size` is ignored when sharing vocabularies
0.9.2,return vocab to dump with standard name
0.9.2,empty train_dataset_files so that vocab is only loaded from
0.9.2,"given paths in src_vocab_path, tgt_vocab_path"
0.9.2,Load vocabulary
0.9.2,Drop the none-using from memory but keep the last
0.9.2,"in the long run, shouldn't it be possible to do this by calling"
0.9.2,build_vocab with both the src and tgt data?
0.9.2,fast-forward if loaded from state
0.9.2,NOTE: `rnn.pack_padded_sequence` requires that a
0.9.2,"minibatch be sorted by decreasing order, which"
0.9.2,requires reversing relative to typical sort keys
0.9.2,Temporarily load one shard to retrieve sort_key for data_type
0.9.2,"NOTE: This is causing some issues for consumer/producer,"
0.9.2,as we may still have some of those examples in some queue
0.9.2,cur_dataset.examples = None
0.9.2,gc.collect()
0.9.2,del cur_dataset
0.9.2,gc.collect()
0.9.2,Cycle through the shards indefinitely.
0.9.2,"When the dataset is not repeated, we might need to ensure that"
0.9.2,the number of returned batches is the multiple of a given value.
0.9.2,This is important for multi GPU training to ensure that all
0.9.2,workers have the same number of batches to process.
0.9.2,Maintains the longest src and tgt length in the current batch
0.9.2,Reset current longest length at a new batch (count=1)
0.9.2,Src: [<bos> w1 ... wN <eos>]
0.9.2,Tgt: [w1 ... wM <eos>]
0.9.2,-*- coding: utf-8 -*-
0.9.2,imports of datatype-specific dependencies
0.9.2,torchaudio loading options recently changed. It's probably
0.9.2,straightforward to rewrite the audio handling to make use of
0.9.2,"up-to-date torchaudio, but in the meantime there is a legacy"
0.9.2,method which uses the old defaults
0.9.2,STFT
0.9.2,-*- coding: utf-8 -*-
0.9.2,domain specific dependencies
0.9.2,coding: utf-8
0.9.2,several data readers need optional dependencies. There's no
0.9.2,appropriate builtin exception
0.9.2,-*- coding: utf-8 -*-
0.9.2,mix this with partial
0.9.2,batch (list(list(list))): batch_size x len(self.fields) x seq_len
0.9.2,lengths: batch_size
0.9.2,data: seq_len x batch_size x len(self.fields)
0.9.2,flake8: noqa
0.9.2,For command-line option parsing
0.9.2,"Check pass, set the args."
0.9.2,"This SRU version implements its own cuda-level optimization,"
0.9.2,so it requires that:
0.9.2,1. `cupy` and `pynvrtc` python package installed.
0.9.2,2. pytorch is built with cuda support.
0.9.2,3. library path set: export LD_LIBRARY_PATH=<cuda lib path>.
0.9.2,Check 1.
0.9.2,Check 2.
0.9.2,Check 3.
0.9.2,This sets up device to use.
0.9.2,-> directions x batch x dim
0.9.2,For DEBUG
0.9.2,"size = (length, batch, x.size(-1)) \"
0.9.2,"if x.dim() == 3 else (batch, x.size(-1))"
0.9.2,grad_x = x.new(*x.size()) if k_ == 3 else x.new(*size).zero_()
0.9.2,Normal use
0.9.2,"An entry check here, will catch on train side and translate side"
0.9.2,if requirements are not satisfied.
0.9.2,RNNDecoderState wraps hidden as a tuple.
0.9.2,fh -> (layers*directions) x batch x dim
0.9.2,NOTE: We need to trim the vocab to remove any unk tokens that
0.9.2,were not originally here.
0.9.2,The score for each translation on the beam.
0.9.2,The backpointers at each time-step.
0.9.2,The outputs at each time-step.
0.9.2,Has EOS topped the beam yet.
0.9.2,The attentions (matrix) for each time.
0.9.2,Time and k pair for finished.
0.9.2,Information for global scoring.
0.9.2,Minimum prediction length
0.9.2,Apply Penalty at every step
0.9.2,force the output to be longer than self.min_length
0.9.2,assumes there are len(word_probs) predictions OTHER
0.9.2,than EOS that are greater than -1e20
0.9.2,Sum the previous scores.
0.9.2,Don't let EOS have children.
0.9.2,Block ngram repeats
0.9.2,"Last n tokens, n = block_ngram_repeat"
0.9.2,Skip the blocking if it is in the exclusion list
0.9.2,"best_scores_id is flattened beam x word array, so calculate which"
0.9.2,word and beam each score came from
0.9.2,End condition is when top-of-beam is EOS and no global score.
0.9.2,Add from beam until we have minimum outputs.
0.9.2,Term will be subtracted from probability
0.9.2,Probability will be divided by this
0.9.2,these warnings indicate that either the alpha/beta
0.9.2,"forces a penalty to be a no-op, or a penalty is a no-op but"
0.9.2,the alpha/beta would suggest otherwise.
0.9.2,using some length penalty
0.9.2,using some coverage penalty
0.9.2,"For temp=0.0, take the argmax to avoid divide-by-zero errors."
0.9.2,keep_topk=1 is also equivalent to argmax.
0.9.2,Set all logits that are not in the top-k to -10000.
0.9.2,This puts the probabilities close to 0.
0.9.2,"shape: (sum(~ self.is_finished), 1)"
0.9.2,magic indices
0.9.2,result caching
0.9.2,add one to account for BOS. Don't account for EOS because hitting
0.9.2,this implies it hasn't been found.
0.9.2,skip BOS
0.9.2,"Last n tokens, n = block_ngram_repeat"
0.9.2,skip the blocking if any token in gram is excluded
0.9.2,!/usr/bin/env python
0.9.2,Maintains the longest src and tgt length in the current batch
0.9.2,Reset current longest length at a new batch (count=1)
0.9.2,max_tgt_in_batch = 0
0.9.2,Src: [<bos> w1 ... wN <eos>]
0.9.2,Tgt: [w1 ... wM <eos>]
0.9.2,for debugging
0.9.2,Statistics
0.9.2,TODO: support these blacklisted features.
0.9.2,Encoder forward.
0.9.2,"Shape: (1, B, 1)"
0.9.2,Reorder states.
0.9.2,Turn any copied words into UNKs.
0.9.2,"Decoder forward, takes [tgt_len, batch, nfeats] as input"
0.9.2,"and [src_len, batch, hidden] as memory_bank"
0.9.2,"in case of inference tgt_len = 1, batch = beam times batch_size"
0.9.2,"in case of Gold Scoring tgt_len = actual length, batch = 1 batch"
0.9.2,Generator forward.
0.9.2,"returns [(batch_size x beam_size) , vocab ] when 1 step"
0.9.2,"or [ tgt_len, batch_size, vocab ] when full sentence"
0.9.2,"here we have scores [tgt_lenxbatch, vocab] or [beamxbatch, vocab]"
0.9.2,"returns [(batch_size x beam_size) , vocab ] when 1 step"
0.9.2,"or [ tgt_len, batch_size, vocab ] when full sentence"
0.9.2,TODO: support these blacklisted features.
0.9.2,(0) Prep the components of the search.
0.9.2,(1) Run the encoder on the src.
0.9.2,(2) Repeat src objects `beam_size` times.
0.9.2,We use batch_size x beam_size
0.9.2,"(0) pt 2, prep the beam object"
0.9.2,Reorder states.
0.9.2,"This is left in the code for now, but unsued"
0.9.2,(0) Prep each of the components of the search.
0.9.2,And helper method for reducing verbosity.
0.9.2,(1) Run the encoder on the src.
0.9.2,(2) Repeat src objects `beam_size` times.
0.9.2,We use now  batch_size x beam_size (same as fast mode)
0.9.2,"(3) run the decoder to generate sentences, using beam search."
0.9.2,(a) Construct batch x beam_size nxt words.
0.9.2,Get all the pending current beam words and arrange for forward.
0.9.2,(b) Decode and forward
0.9.2,(c) Advance each beam.
0.9.2,Loop over the batch_size number of beam
0.9.2,(4) Extract sentences from beam.
0.9.2,Rollback pointer to the beginning.
0.9.2,beam parameters
0.9.2,result caching
0.9.2,beam state
0.9.2,BoolTensor was introduced in pytorch 1.2
0.9.2,buffers for the topk scores and 'backpointer'
0.9.2,"""global state"" of the old beam"
0.9.2,for testing
0.9.2,using integer division to get an integer _B without casting
0.9.2,force the output to be longer than self.min_length
0.9.2,Multiply probs by the beam probability.
0.9.2,"if the sequence ends now, then the penalty is the current"
0.9.2,"length + 1, to include the EOS token"
0.9.2,Flatten probs into a list of possibilities.
0.9.2,Recover log probs.
0.9.2,Length penalty is just a scalar. It doesn't matter if it's applied
0.9.2,before or after the topk.
0.9.2,Resolve beam origin and map to batch index flat representation.
0.9.2,Append last prediction.
0.9.2,update global state (step == 1)
0.9.2,update global state (step > 1)
0.9.2,"shape: (batch_size x beam_size, 1)"
0.9.2,Penalize beams that finished.
0.9.2,"on real data (newstest2017) with the pretrained transformer,"
0.9.2,it's faster to not move this back to the original device
0.9.2,Store finished hypotheses for this batch.
0.9.2,End condition is the top beam finished and we can return
0.9.2,n_best hypotheses.
0.9.2,"If all sentences are translated, no need to go further."
0.9.2,Remove finished batches for the next step.
0.9.2,!/usr/bin/env python
0.9.2,semaphore doesn't have a timeout arg in Python 2.7
0.9.2,backwards compatibility for confs
0.9.2,load can be called multiple times: modify copy
0.9.2,NOTE: translator returns lists of `n_best` list
0.9.2,we can ignore that (i.e. flatten lists) only because
0.9.2,we restrict `n_best=1`
0.9.2,build back results with empty texts
0.9.2,Below are all the different penalty terms implemented so far.
0.9.2,Subtract coverage penalty from topk log probs.
0.9.2,Divide topk log probs by length penalty.
0.9.2,Sorting
0.9.2,Chinese segmentation
0.9.2,Chinese simplify -> Chinese traditional standard
0.9.2,Chinese simplify -> Chinese traditional (HongKong)
0.9.2,Chinese simplify -> Chinese traditional (Taiwan)
0.9.2,Chinese traditional -> Chinese simplify (v1)
0.9.2,Chinese traditional -> Chinese simplify (v2)
0.9.1,!/usr/bin/env python
0.9.1,!/usr/bin/env python
0.9.1,!/usr/bin/env python
0.9.1,-*- coding: utf-8 -*-
0.9.1,!/usr/bin/env python
0.9.1,-*- coding: utf-8 -*-
0.9.1,!/usr/bin/env python
0.9.1,Load checkpoint if we resume from a previous training.
0.9.1,check for code where vocab is saved instead of fields
0.9.1,(in the future this will be done in a smarter way)
0.9.1,Create a thread to listen for errors in the child processes.
0.9.1,Train with multiprocessing.
0.9.1,generator_to_serve = iter(generator_to_serve)
0.9.1,hack to dodge unpicklable `dict_keys`
0.9.1,"propagate exception to parent process, keeping original traceback"
0.9.1,!/usr/bin/env python3
0.9.1,-*- coding: utf-8 -*-
0.9.1,
0.9.1,"OpenNMT-py documentation build configuration file, created by"
0.9.1,sphinx-quickstart on Sun Dec 17 12:07:14 2017.
0.9.1,
0.9.1,This file is execfile()d with the current directory set to its
0.9.1,containing dir.
0.9.1,
0.9.1,Note that not all possible configuration values are present in this
0.9.1,autogenerated file.
0.9.1,
0.9.1,All configuration values have a default; values that are commented out
0.9.1,serve to show the default.
0.9.1,"If extensions (or modules to document with autodoc) are in another directory,"
0.9.1,add these directories to sys.path here. If the directory is relative to the
0.9.1,"documentation root, use os.path.abspath to make it absolute, like shown here."
0.9.1,
0.9.1,import os
0.9.1,import sys
0.9.1,"sys.path.insert(0, os.path.abspath('.'))"
0.9.1,-- General configuration ------------------------------------------------
0.9.1,"If your documentation needs a minimal Sphinx version, state it here."
0.9.1,
0.9.1,needs_sphinx = '1.0'
0.9.1,"Add any Sphinx extension module names here, as strings. They can be"
0.9.1,extensions coming with Sphinx (named 'sphinx.ext.*') or your custom
0.9.1,ones.
0.9.1,Show base classes
0.9.1,"Use ""variables"" section for Attributes instead of weird block things"
0.9.1,mimicking the function style.
0.9.1,"Add any paths that contain templates here, relative to this directory."
0.9.1,The suffix(es) of source filenames.
0.9.1,You can specify multiple suffix as a list of string:
0.9.1,
0.9.1,"source_suffix = ['.rst', '.md']"
0.9.1,The master toctree document.
0.9.1,General information about the project.
0.9.1,"The version info for the project you're documenting, acts as replacement for"
0.9.1,"|version| and |release|, also used in various other places throughout the"
0.9.1,built documents.
0.9.1,
0.9.1,The short X.Y version.
0.9.1,"The full version, including alpha/beta/rc tags."
0.9.1,The language for content autogenerated by Sphinx. Refer to documentation
0.9.1,for a list of supported languages.
0.9.1,
0.9.1,This is also used if you do content translation via gettext catalogs.
0.9.1,"Usually you set ""language"" from the command line for these cases."
0.9.1,"List of patterns, relative to source directory, that match files and"
0.9.1,directories to ignore when looking for source files.
0.9.1,This patterns also effect to html_static_path and html_extra_path
0.9.1,The name of the Pygments (syntax highlighting) style to use.
0.9.1,"If true, `todo` and `todoList` produce output, else they produce nothing."
0.9.1,-- Options for HTML output ----------------------------------------------
0.9.1,The theme to use for HTML and HTML Help pages.  See the documentation for
0.9.1,a list of builtin themes.
0.9.1,
0.9.1,html_theme = 'sphinx_materialdesign_theme'
0.9.1,html_theme_path = [sphinx_materialdesign_theme.get_path()]
0.9.1,Theme options are theme-specific and customize the look and feel of a theme
0.9.1,"further.  For a list of options available for each theme, see the"
0.9.1,documentation.
0.9.1,
0.9.1,html_theme_options = {}
0.9.1,"Add any paths that contain custom static files (such as style sheets) here,"
0.9.1,"relative to this directory. They are copied after the builtin static files,"
0.9.1,"so a file named ""default.css"" will overwrite the builtin ""default.css""."
0.9.1,"Custom sidebar templates, must be a dictionary that maps document names"
0.9.1,to template names.
0.9.1,
0.9.1,This is required for the alabaster theme
0.9.1,refs: http://alabaster.readthedocs.io/en/latest/installation.html#sidebars
0.9.1,-- Options for HTMLHelp output ------------------------------------------
0.9.1,Output file base name for HTML help builder.
0.9.1,-- Options for LaTeX output ---------------------------------------------
0.9.1,The paper size ('letterpaper' or 'a4paper').
0.9.1,
0.9.1,"'papersize': 'letterpaper',"
0.9.1,"The font size ('10pt', '11pt' or '12pt')."
0.9.1,
0.9.1,"'pointsize': '10pt',"
0.9.1,Additional stuff for the LaTeX preamble.
0.9.1,
0.9.1,"'preamble': '',"
0.9.1,Latex figure (float) alignment
0.9.1,
0.9.1,"'figure_align': 'htbp',"
0.9.1,Grouping the document tree into LaTeX files. List of tuples
0.9.1,"(source start file, target name, title,"
0.9.1,"author, documentclass [howto, manual, or own class])."
0.9.1,-- Options for manual page output ---------------------------------------
0.9.1,One entry per manual page. List of tuples
0.9.1,"(source start file, name, description, authors, manual section)."
0.9.1,-- Options for Texinfo output -------------------------------------------
0.9.1,Grouping the document tree into Texinfo files. List of tuples
0.9.1,"(source start file, target name, title, author,"
0.9.1,"dir menu entry, description, category)"
0.9.1,!/usr/bin/env python
0.9.1,-*- coding: utf-8 -*-
0.9.1,is this reachable?
0.9.1,Read in embeddings
0.9.1,Write to file
0.9.1,!/usr/bin/env python
0.9.1,-*- coding: utf-8 -*-
0.9.1,"Add in default model arguments, possibly added since training."
0.9.1,-*- encoding: utf-8 -*-
0.9.1,!/usr/bin/env python
0.9.1,-*- coding: utf-8 -*-
0.9.1,Author: Rico Sennrich
0.9.1,flake8: noqa
0.9.1,This file is retrieved from https://github.com/rsennrich/subword-nmt
0.9.1,hack for python2/3 compatibility
0.9.1,check version information
0.9.1,some hacking to deal with duplicates (only consider first instance)
0.9.1,don't print end-of-word symbols
0.9.1,sys.stderr.write('cannot split {0} further.\n'.format(segment))
0.9.1,sys.stderr.write('OOV: {0}\n'.format(segment))
0.9.1,sys.stderr.write('OOV: {0}\n'.format(segment))
0.9.1,python 2/3 compatibility
0.9.1,read/write files as UTF-8
0.9.1,!/usr/bin/env python
0.9.1,!/usr/bin/env python
0.9.1,-*- coding: utf-8 -*-
0.9.1,Author: Rico Sennrich
0.9.1,flake8: noqa
0.9.1,This file is retrieved from https://github.com/rsennrich/subword-nmt
0.9.1,hack for python2/3 compatibility
0.9.1,"find all instances of pair, and update frequency/indices around it"
0.9.1,find first symbol
0.9.1,"if first symbol is followed by second symbol, we've found an occurrence of pair (old_word[i:i+2])"
0.9.1,"assuming a symbol sequence ""A B C"", if ""B C"" is merged, reduce the frequency of ""A B"""
0.9.1,"assuming a symbol sequence ""A B C B"", if ""B C"" is merged, reduce the frequency of ""C B""."
0.9.1,"however, skip this if the sequence is A B C B C, because the frequency of ""C B"" will be reduced by the previous code block"
0.9.1,find new pair
0.9.1,"assuming a symbol sequence ""A BC D"", if ""B C"" is merged, increase the frequency of ""A BC"""
0.9.1,"assuming a symbol sequence ""A BC B"", if ""B C"" is merged, increase the frequency of ""BC B"""
0.9.1,"however, if the sequence is A BC BC, skip this step because the count of ""BC BC"" will be incremented by the previous code block"
0.9.1,data structure of pair frequencies
0.9.1,index from pairs to words
0.9.1,version 0.2 changes the handling of the end-of-word token ('</w>');
0.9.1,version numbering allows bckward compatibility
0.9.1,"threshold is inspired by Zipfian assumption, but should only affect speed"
0.9.1,we probably missed the best pair because of pruning; go back to full statistics
0.9.1,"threshold is inspired by Zipfian assumption, but should only affect speed"
0.9.1,python 2/3 compatibility
0.9.1,read/write files as UTF-8
0.9.1,!/usr/bin/env python
0.9.1,Build embeddings.
0.9.1,Build encoder.
0.9.1,Build decoder.
0.9.1,Share the embedding matrix - preprocess with share_vocab required.
0.9.1,src/tgt vocab should be the same if `-share_vocab` is specified.
0.9.1,Build NMTModel(= encoder + decoder).
0.9.1,Build Generator.
0.9.1,Load the model states from checkpoint or initialize them.
0.9.1,This preserves backward-compat for models using customed layernorm
0.9.1,end of patch for backward compatibility
0.9.1,!/usr/bin/env python
0.9.1,NOTE: It's important that ``opt`` has been validated and updated
0.9.1,at this point.
0.9.1,Load checkpoint if we resume from a previous training.
0.9.1,check for code where vocab is saved instead of fields
0.9.1,(in the future this will be done in a smarter way)
0.9.1,"Report src and tgt vocab sizes, including for features"
0.9.1,Build model.
0.9.1,Build optimizer.
0.9.1,Build model saver
0.9.1,Embedding Options
0.9.1,Encoder-Decoder Options
0.9.1,"group.add('--residual', '-residual',   action=""store_true"","
0.9.1,"help=""Add residual connections between RNN layers."")"
0.9.1,Attention options
0.9.1,Generator and loss options.
0.9.1,Data options
0.9.1,"Dictionary options, for text corpus"
0.9.1,"if you want to pass an existing vocab.pt file, pass it to"
0.9.1,-src_vocab alone as it already contains tgt vocab.
0.9.1,"Truncation options, for text corpus"
0.9.1,Data processing options
0.9.1,Options most relevant to speech
0.9.1,Option most relevant to image input
0.9.1,GPU
0.9.1,Init options
0.9.1,Pretrained word vectors
0.9.1,Fixed word vectors
0.9.1,Optimization options
0.9.1,learning rate
0.9.1,Use TensorboardX for visualization during training
0.9.1,Options most relevant to speech
0.9.1,Option most relevant to image input
0.9.1,Options most relevant to summarization.
0.9.1,Alpha and Beta values for Google Length + Coverage penalty
0.9.1,"Described here: https://arxiv.org/pdf/1609.08144.pdf, Section 7"
0.9.1,Options most relevant to speech.
0.9.1,Option most relevant to image input
0.9.1,Copyright 2016 The Chromium Authors. All rights reserved.
0.9.1,Use of this source code is governed by a BSD-style license that can be
0.9.1,found in the LICENSE file.
0.9.1,"Get the key 'value' in the dict, or just use 'value'"
0.9.1,Basic attributes.
0.9.1,Set model in training mode.
0.9.1,UPDATE DROPOUT
0.9.1,Run patience mechanism
0.9.1,"If the patience has reached the limit, stop training"
0.9.1,Set model in validating mode.
0.9.1,F-prop through the model.
0.9.1,Compute loss.
0.9.1,Update statistics.
0.9.1,Set model back to training mode.
0.9.1,Truncated BPTT: reminder not compatible with accum > 1
0.9.1,1. Create truncated target.
0.9.1,2. F-prop all but generator.
0.9.1,3. Compute loss.
0.9.1,4. Update the parameters and statistics.
0.9.1,Multi GPU gradient gather
0.9.1,"If truncated, don't backprop fully."
0.9.1,TO CHECK
0.9.1,if dec_state is not None:
0.9.1,dec_state.detach()
0.9.1,"in case of multi step gradient accumulation,"
0.9.1,update only after accum batches
0.9.1,For Flake
0.9.1,we avoid padding while mean pooling
0.9.1,Initialize the bridge layer
0.9.1,"s_len, batch, emb_dim = emb.size()"
0.9.1,Lengths data is wrapped inside a Tensor.
0.9.1,"LSTM has hidden and cell state, other only one"
0.9.1,Total number of states
0.9.1,Build a linear layer for each
0.9.1,The encoder hidden is  (layers*directions) x batch x dim.
0.9.1,"s_len, batch, emb_dim = emb.size()"
0.9.1,Run the forward pass of every layer of the tranformer.
0.9.1,why is the model_opt.__dict__ check necessary?
0.9.1,"(batch_size, 64, imgH, imgW)"
0.9.1,layer 1
0.9.1,"(batch_size, 64, imgH/2, imgW/2)"
0.9.1,"(batch_size, 128, imgH/2, imgW/2)"
0.9.1,layer 2
0.9.1,"(batch_size, 128, imgH/2/2, imgW/2/2)"
0.9.1,"(batch_size, 256, imgH/2/2, imgW/2/2)"
0.9.1,layer 3
0.9.1,batch norm 1
0.9.1,"(batch_size, 256, imgH/2/2, imgW/2/2)"
0.9.1,layer4
0.9.1,"(batch_size, 256, imgH/2/2/2, imgW/2/2)"
0.9.1,"(batch_size, 512, imgH/2/2/2, imgW/2/2)"
0.9.1,layer 5
0.9.1,batch norm 2
0.9.1,"(batch_size, 512, imgH/2/2/2, imgW/2/2/2)"
0.9.1,"(batch_size, 512, imgH/2/2/2, imgW/2/2/2)"
0.9.1,"# (batch_size, 512, H, W)"
0.9.1,Dimensions and padding for constructing the word embedding matrix
0.9.1,Dimensions and padding for feature embedding matrices
0.9.1,(these have no effect if feat_vocab_sizes is empty)
0.9.1,The embedding matrix look-up tables. The first look-up table
0.9.1,"is for words. Subsequent ones are for features, if any exist."
0.9.1,The final output size of word + feature vectors. This can vary
0.9.1,from the word vector size if and only if features are defined.
0.9.1,This is the attribute you should access if you need to know
0.9.1,how big your embeddings are going to be.
0.9.1,The sequence of operations that converts the input sequence
0.9.1,into a sequence of embeddings. At minimum this consists of
0.9.1,looking up the embeddings for each word and feature in the
0.9.1,input. Model parameters may require the sequence to contain
0.9.1,additional operations as well.
0.9.1,features must use word_vec_size
0.9.1,features will use feat_vec_size
0.9.1,This class is mainly used by decoder.py for RNNs but also
0.9.1,by the CNN / transformer decoder when copy attention is used
0.9.1,CNN has its own attention mechanism ConvMultiStepAttention
0.9.1,Transformer has its own MultiHeadedAttention
0.9.1,mlp wants it with bias
0.9.1,Check input sizes
0.9.1,"(batch, t_len, d) x (batch, d, s_len) --> (batch, t_len, s_len)"
0.9.1,"(batch, t_len, s_len, d)"
0.9.1,one step input
0.9.1,"compute attention scores, as in Luong et al."
0.9.1,Softmax or sparsemax to normalize attention weights
0.9.1,each context vector c_t is the weighted average
0.9.1,over all the source hidden states
0.9.1,concatenate
0.9.1,Check output sizes
0.9.1,Check output sizes
0.9.1,clamping necessary because of numerical errors: loss should be lower
0.9.1,"bounded by zero, but negative values near zero are possible without"
0.9.1,the clamp
0.9.1,from onmt.utils.misc import aeq
0.9.1,CHECKS
0.9.1,"batch, k_len, d = key.size()"
0.9.1,"batch_, k_len_, d_ = value.size()"
0.9.1,"aeq(batch, batch_)"
0.9.1,"aeq(k_len, k_len_)"
0.9.1,"aeq(d, d_)"
0.9.1,"batch_, q_len, d_ = query.size()"
0.9.1,"aeq(batch, batch_)"
0.9.1,"aeq(d, d_)"
0.9.1,"aeq(self.model_dim % 8, 0)"
0.9.1,if mask is not None:
0.9.1,"batch_, q_len_, k_len_ = mask.size()"
0.9.1,"aeq(batch_, batch)"
0.9.1,"aeq(k_len_, k_len)"
0.9.1,aeq(q_len_ == q_len)
0.9.1,END CHECKS
0.9.1,"1) Project key, value, and query."
0.9.1,1 or key_len x key_len
0.9.1,1 or key_len x key_len x dim_per_head
0.9.1,1 or key_len x key_len x dim_per_head
0.9.1,2) Calculate and scale scores.
0.9.1,batch x num_heads x query_len x key_len
0.9.1,3) Apply attention dropout and compute context vectors.
0.9.1,CHECK
0.9.1,"batch_, q_len_, d_ = output.size()"
0.9.1,"aeq(q_len, q_len_)"
0.9.1,"aeq(batch, batch_)"
0.9.1,"aeq(d, d_)"
0.9.1,Return one attn
0.9.1,At the moment this class is only used by embeddings.Embeddings look-up tables
0.9.1,-*- coding: utf-8 -*-
0.9.1,checks
0.9.1,"batch, channel, height, width = base_target_emb.size()"
0.9.1,"batch_, channel_, height_, width_ = input_from_dec.size()"
0.9.1,"enc_batch, enc_channel, enc_height = encoder_out_top.size()"
0.9.1,"enc_batch_, enc_channel_, enc_height_ = encoder_out_combine.size()"
0.9.1,out_features * in_features
0.9.1,norm is out_features * 1
0.9.1,batch_size * out_features
0.9.1,out_features
0.9.1,out_features
0.9.1,batch_size * out_features
0.9.1,"out_channels, in_channels // groups, * kernel_size"
0.9.1,out_features
0.9.1,This is used nowhere in the code at the moment (Vincent Nguyen 05/18/2018)
0.9.1,"in_channels, out_channels, *kernel_size"
0.9.1,"in_channels, out_channels, *kernel_size"
0.9.1,"self.out_channels, 1"
0.9.1,out_features
0.9.1,out_features
0.9.1,store roots on diagonal
0.9.1,CHECKS
0.9.1,Original probabilities.
0.9.1,Probability of copying p(z=1) batch.
0.9.1,Probability of not copying: p_{word}(w) * (1 - p(z))
0.9.1,probabilities assigned by the model to the gold targets
0.9.1,probability of tokens copied from source
0.9.1,Set scores for unk to 0 and add eps
0.9.1,find the indices in which you do not use the copy mechanism
0.9.1,Drop padding.
0.9.1,this block does not depend on the loss value computed above
0.9.1,and is used only for stats
0.9.1,this block does not depend on the loss value computed above
0.9.1,and is used only for stats
0.9.1,Correct target copy token instead of <unk>
0.9.1,tgt[i] = align[i] + len(tgt_vocab)
0.9.1,for i such that tgt[i] == 0 and align[i] != 0
0.9.1,Compute sum of perplexities for stats
0.9.1,this part looks like it belongs in CopyGeneratorLoss
0.9.1,Compute Loss as NLL divided by seq length
0.9.1,Compute Total Loss per sequence in batch
0.9.1,Divide by length of each sequence and sum
0.9.1,all beams repeat (beam >= 1 repeat dummy scores)
0.9.1,predict repeat_idx over and over again
0.9.1,beam 0 and beam >=2 will repeat (beam >= 2 repeat dummy scores)
0.9.1,non-interesting beams are going to get dummy values
0.9.1,"on initial round, only predicted scores for beam 0"
0.9.1,matter. Make two predictions. Top one will be repeated
0.9.1,"in beam zero, second one will live on in beam 1."
0.9.1,predict the same thing in beam 0
0.9.1,continue pushing around what beam 1 predicts
0.9.1,"now beam 0 dies (along with the others), beam 1 -> beam 0"
0.9.1,beam 0 and beam >= 2 will repeat (beam 2 repeats excluded idx)
0.9.1,non-interesting beams are going to get dummy values
0.9.1,predict the same thing in beam 0
0.9.1,continue pushing around what beam 1 predicts
0.9.1,predict the allowed-repeat again in beam 2
0.9.1,"now beam 0 dies, beam 1 -> beam 0, beam 2 -> beam 1"
0.9.1,and the rest die
0.9.1,"since all preds after i=0 are 0, we can check"
0.9.1,that the beam is the correct idx by checking that
0.9.1,the curr score is the initial score
0.9.1,beam 0 will always predict EOS. The other beams will predict
0.9.1,non-eos scores.
0.9.1,"this is also a test that when block_ngram_repeat=0,"
0.9.1,repeating is acceptable
0.9.1,non-interesting beams are going to get dummy values
0.9.1,"""best"" prediction is eos - that should be blocked"
0.9.1,include at least beam_sz predictions OTHER than EOS
0.9.1,that are greater than -1e20
0.9.1,predict eos in beam 0
0.9.1,provide beam_sz other good predictions
0.9.1,now the top beam has ended and no others have
0.9.1,first beam finished had length beam.min_length
0.9.1,first beam finished was 0
0.9.1,"not of interest, but want to make sure it keeps running"
0.9.1,since only beam 0 terminates and n_best = 2
0.9.1,"this is also a test that when block_ngram_repeat=0,"
0.9.1,repeating is acceptable
0.9.1,non-interesting beams are going to get dummy values
0.9.1,"""best"" prediction is eos - that should be blocked"
0.9.1,include at least beam_sz predictions OTHER than EOS
0.9.1,that are greater than -1e20
0.9.1,predict eos in beam 1
0.9.1,provide beam_sz other good predictions in other beams
0.9.1,provide beam_sz other good predictions in other beams
0.9.1,beam 1 dies on min_length
0.9.1,beam 0 dies on the step after beam 1 dies
0.9.1,"init_preds: [4, 3, 5, 6, 7] - no EOS's"
0.9.1,no EOS's yet
0.9.1,"[5, 3, 2, 6, 0], so beam 2 predicts EOS!"
0.9.1,assumes beam 2 finished on last step
0.9.1,"[2, 5, 3, 6, 0], so beam 0 predicts EOS!"
0.9.1,"[-2.4879, -3.8910, -4.1010, -4.2010, -4.4010]"
0.9.1,new beam 0 finished
0.9.1,new beam 0 is old beam 3
0.9.1,assumes beam 0 finished on last step
0.9.1,"[5, 2, 6, 1, 0], so beam 1 predicts EOS!"
0.9.1,new beam 1 finished
0.9.1,new beam 1 is old beam 4
0.9.1,this could be considered an integration test because it tests
0.9.1,interactions between the GNMT scorer and the beam
0.9.1,initialize fields at the top of each unit test to prevent
0.9.1,any undesired stateful effects
0.9.1,"this test touches the file system, so it could be considered an"
0.9.1,integration test
0.9.1,write utf-8 bytes
0.9.1,predict repeat_idx over and over again
0.9.1,"batch 0 and 7 will repeat, the rest will advance"
0.9.1,predict the same thing in batch 0 and 7 every i
0.9.1,push around what the other batches predict
0.9.1,now batch 0 and 7 die
0.9.1,"batch 0 will repeat excluded idx, batch 1 will repeat"
0.9.1,now batch 1 dies
0.9.1,batch 0 will always predict EOS. The other batches will predict
0.9.1,non-eos scores.
0.9.1,"""best"" prediction is eos - that should be blocked"
0.9.1,include at least one prediction OTHER than EOS
0.9.1,that is greater than -1e20
0.9.1,now batch 0 has ended and no others have
0.9.1,initial step
0.9.1,batch 0 dies on step 0
0.9.1,include at least one prediction OTHER than EOS
0.9.1,that is greater than -1e20
0.9.1,step 2
0.9.1,(old) batch 8 dies on step 1
0.9.1,step 3
0.9.1,everything dies
0.9.1,initial step
0.9.1,batch 0 dies on step 0
0.9.1,include at least one prediction OTHER than EOS
0.9.1,that is greater than -1e20
0.9.1,step 2
0.9.1,(old) batch 8 dies on step 1
0.9.1,step 3
0.9.1,everything dies
0.9.1,illegal_weights_mask = torch.ByteTensor([
0.9.1,"[0, 0, 0, 0, 0, 0, 0],"
0.9.1,"[0, 0, 0, 1, 1, 1, 1],"
0.9.1,"[0, 0, 0, 0, 0, 1, 1],"
0.9.1,"[0, 0, 1, 1, 1, 1, 1]])"
0.9.1,TODO: fix for pytorch 0.3
0.9.1,illegal_weights = alignments.masked_select(illegal_weights_mask)
0.9.1,"self.assertEqual(0.0, illegal_weights.data.sum())"
0.9.1,this could be considered an integration test because it touches
0.9.1,the filesystem for the config file (and the models)
0.9.1,-*- coding: utf-8 -*-
0.9.1,tests pad and numericalize integration
0.9.1,tests pad and numericalize integration
0.9.1,"this test touches the file system, so it could be considered an"
0.9.1,integration test
0.9.1,file to hold full paths to audio data
0.9.1,file to hold audio paths relative to _AUDIO_DATA_DIR (i.e. file names)
0.9.1,it's ok if non-audio files co-exist with audio files in the data dir
0.9.1,"dividing gets the noise in [-1, 1]"
0.9.1,"this test touches the file system, so it could be considered an"
0.9.1,integration test
0.9.1,file to hold full paths to image data
0.9.1,file to hold image paths relative to _IMG_DATA_DIR (i.e. file names)
0.9.1,it's ok if non-image files co-exist with image files in the data dir
0.9.1,all beams repeat (beam >= 1 repeat dummy scores)
0.9.1,predict repeat_idx over and over again
0.9.1,beam 0 and beam >=2 will repeat (beam >= 2 repeat dummy scores)
0.9.1,non-interesting beams are going to get dummy values
0.9.1,"on initial round, only predicted scores for beam 0"
0.9.1,matter. Make two predictions. Top one will be repeated
0.9.1,"in beam zero, second one will live on in beam 1."
0.9.1,predict the same thing in beam 0
0.9.1,continue pushing around what beam 1 predicts
0.9.1,"now beam 0 dies (along with the others), beam 1 -> beam 0"
0.9.1,beam 0 and beam >= 2 will repeat (beam 2 repeats excluded idx)
0.9.1,non-interesting beams are going to get dummy values
0.9.1,predict the same thing in beam 0
0.9.1,continue pushing around what beam 1 predicts
0.9.1,predict the allowed-repeat again in beam 2
0.9.1,"now beam 0 dies, beam 1 -> beam 0, beam 2 -> beam 1"
0.9.1,and the rest die
0.9.1,"since all preds after i=0 are 0, we can check"
0.9.1,that the beam is the correct idx by checking that
0.9.1,the curr score is the initial score
0.9.1,beam 0 will always predict EOS. The other beams will predict
0.9.1,non-eos scores.
0.9.1,non-interesting beams are going to get dummy values
0.9.1,"""best"" prediction is eos - that should be blocked"
0.9.1,include at least beam_sz predictions OTHER than EOS
0.9.1,that are greater than -1e20
0.9.1,predict eos in beam 0
0.9.1,provide beam_sz other good predictions
0.9.1,now the top beam has ended and no others have
0.9.1,"not of interest, but want to make sure it keeps running"
0.9.1,since only beam 0 terminates and n_best = 2
0.9.1,"this is also a test that when block_ngram_repeat=0,"
0.9.1,repeating is acceptable
0.9.1,non-interesting beams are going to get dummy values
0.9.1,"""best"" prediction is eos - that should be blocked"
0.9.1,include at least beam_sz predictions OTHER than EOS
0.9.1,that are greater than -1e20
0.9.1,predict eos in beam 1
0.9.1,provide beam_sz other good predictions in other beams
0.9.1,provide beam_sz other good predictions in other beams
0.9.1,beam 1 dies on min_length
0.9.1,beam 0 dies on the step after beam 1 dies
0.9.1,non-interesting beams are going to get dummy values
0.9.1,"""best"" prediction is eos - that should be blocked"
0.9.1,include at least beam_sz predictions OTHER than EOS
0.9.1,that are greater than -1e20
0.9.1,predict eos in beam 1
0.9.1,provide beam_sz other good predictions in other beams
0.9.1,provide beam_sz other good predictions in other beams
0.9.1,no top beams are finished yet
0.9.1,beam 1 dies on min_length
0.9.1,no top beams are finished yet
0.9.1,beam 0 dies on the step after beam 1 dies
0.9.1,top beam is finished now so there are attentions
0.9.1,two beams are finished in each batch
0.9.1,second dim is cut down to the non-padded src length
0.9.1,first dim is equal to the time of death
0.9.1,(beam 0 died at current step - adjust for SOS)
0.9.1,(beam 1 died at last step - adjust for SOS)
0.9.1,behavior gets weird when beam is already done so just stop
0.9.1,this is just test_beam.TestBeamAgainstReferenceCase repeated
0.9.1,in each batch.
0.9.1,"init_preds: [4, 3, 5, 6, 7] - no EOS's"
0.9.1,no EOS's yet
0.9.1,"[5, 3, 2, 6, 0], so beam 2 predicts EOS!"
0.9.1,assumes beam 2 finished on last step
0.9.1,ended beam 2 shouldn't continue
0.9.1,"[2, 5, 3, 6, 0] repeat self.BATCH_SZ, so beam 0 predicts EOS!"
0.9.1,"[-2.4879, -3.8910, -4.1010, -4.2010, -4.4010] repeat self.BATCH_SZ"
0.9.1,another beam is finished in all batches
0.9.1,new beam 0 finished
0.9.1,new beam 0 is old beam 3
0.9.1,assumes beam 0 finished on last step
0.9.1,"[5, 2, 6, 1, 0] repeat self.BATCH_SZ, so beam 1 predicts EOS!"
0.9.1,new beam 1 finished
0.9.1,new beam 1 is old beam 4
0.9.1,this could be considered an integration test because it tests
0.9.1,interactions between the GNMT scorer and the beam
0.9.1,"-data option is required, but not used in this test, so dummy."
0.9.1,len x batch x nfeat
0.9.1,batch x c x h x w
0.9.1,batch x 1 x nfft x t
0.9.1,Initialize vectors to compare size with
0.9.1,Ensure correct sizes and types
0.9.1,Make sure that output has the correct size and type
0.9.1,Make sure that output has the correct size and type
0.9.1,Make sure that output has the correct size and type
0.9.1,"[('encoder_type', 'transformer'),"
0.9.1,"('word_vec_size', 16), ('rnn_size', 16)],"
0.9.1,""""""" Only do SRU test if requirment is safisfied. """""""
0.9.1,SRU doesn't support input_feed.
0.9.1,"when reasonable, set audio_enc_pooling to 2"
0.9.1,Need lengths >= audio_enc_pooling**n_layers.
0.9.1,"That condition is unrealistic for large n_layers,"
0.9.1,so leave audio_enc_pooling at 1.
0.9.1,first check there's nothing unexpectedly not trainable
0.9.1,ok: word embeddings shouldn't be trainable
0.9.1,if word vecs are fixed
0.9.1,ok: positional encodings shouldn't be trainable
0.9.1,then check nothing unexpectedly trainable
0.9.1,!/usr/bin/env python
0.9.1,-*- coding: utf-8 -*-
0.9.1,Remove the generated *pt files.
0.9.1,Test image preprocessing
0.9.1,Test audio preprocessing
0.9.1,Decoder state
0.9.1,Build the RNN.
0.9.1,Set up the context gate.
0.9.1,Set up the standard attention.
0.9.1,The encoder hidden is  (layers*directions) x batch x dim.
0.9.1,We need to convert it to layers x batch x (directions*dim).
0.9.1,Init the input feed.
0.9.1,Update the state with the result.
0.9.1,Concatenates sequence of tensors along a new dimension.
0.9.1,NOTE: v0.3 to 0.4: dec_outs / attns[*] may not be list
0.9.1,(in particular in case of SRU) it was not raising error in 0.3
0.9.1,since stack(Variable) was allowed.
0.9.1,"In 0.4, SRU returns a tensor that shouldn't be stacke"
0.9.1,Check
0.9.1,Calculate the attention.
0.9.1,Calculate the context gate.
0.9.1,Additional args check.
0.9.1,END Additional args check.
0.9.1,Input feed concatenates hidden state with
0.9.1,input at every time step.
0.9.1,TODO: context gate should be employed
0.9.1,instead of second RNN transform.
0.9.1,Update the coverage attention.
0.9.1,Decoder State
0.9.1,CNNDecoder has its own attention mechanism.
0.9.1,Set up a separate copy attention layer if needed.
0.9.1,The output of CNNEncoder.
0.9.1,The combination of output of CNNEncoder and source embeddings.
0.9.1,Process the result and update the attentions.
0.9.1,Update the state.
0.9.1,TODO change the way attns is returned dict => list or tuple (onnx)
0.9.1,Memory_lengths is a single tensor shared between all models.
0.9.1,This assumption will not hold if Translator is modified
0.9.1,to calculate memory_lengths as something other than the length
0.9.1,of the input.
0.9.1,Decoder State
0.9.1,"previously, there was a GlobalAttention module here for copy"
0.9.1,"attention. But it was never actually used -- the ""copy"" attention"
0.9.1,just reuses the context attention.
0.9.1,TODO change the way attns is returned dict => list or tuple (onnx)
0.9.1,"buffer size in bytes, determine equiv. # of elements based on data type"
0.9.1,copy tensors into buffer_t
0.9.1,all-reduce and rescale
0.9.1,copy all-reduced buffer back into tensors
0.9.1,"tensor is bigger than buffer, all-reduce and rescale directly"
0.9.1,"buffer is full, all-reduce and replace buffer with grad"
0.9.1,add tensor to buffer
0.9.1,TODO: Find a better way to check for sparse gradients.
0.9.1,Load everything from the checkpoint.
0.9.1,Build everything from scratch.
0.9.1,"Reset optimizer, keep options."
0.9.1,"Reset options, keep optimizer."
0.9.1,State can be partially restored.
0.9.1,Code below is an implementation of https://arxiv.org/pdf/1804.04235.pdf
0.9.1,inspired but modified from https://github.com/DeadAt0m/adafactor-pytorch
0.9.1,-*- coding: utf-8 -*-
0.9.1,if the loss function operates on vectors of raw logits instead of
0.9.1,"probabilities, only the first part of the generator needs to be"
0.9.1,"passed to the NMTLossCompute. At the moment, the only supported"
0.9.1,loss function of this kind is the sparsemax loss.
0.9.1,non_none: the subdict of the state dictionary where the values
0.9.1,are not None.
0.9.1,"Now, the iteration:"
0.9.1,state is a dictionary of sequences of tensor-like but we
0.9.1,want a sequence of dictionaries of tensors.
0.9.1,"First, unzip the dictionary into a sequence of keys and a"
0.9.1,sequence of tensor-like sequences.
0.9.1,"Now, yield a dictionary for each shard. The keys are always"
0.9.1,the same. values is a sequence of length #keys where each
0.9.1,element is a sequence of length #shards. We want to iterate
0.9.1,"over the shards, not over the keys: therefore, the values need"
0.9.1,to be re-zipped by shard and then each shard can be paired
0.9.1,with the keys.
0.9.1,Assumed backprop'd
0.9.1,Log the progress using the number of batches on the x-axis.
0.9.1,this check is here because audio allows the encoder and decoder to
0.9.1,"be different sizes, but other model types do not yet"
0.9.1,"Load default opt values, then overwrite with the opts in"
0.9.1,"the checkpoint. That way, if there are new options added,"
0.9.1,the defaults are used.
0.9.1,Don't do anything
0.9.1,Update best score of each criteria
0.9.1,Reset tolerance
0.9.1,Update current status
0.9.1,Decrease tolerance
0.9.1,Log
0.9.1,Log
0.9.1,Get a list of world_size lists with len(stat_list) Statistics objects
0.9.1,SRU doesn't support PackedSequence.
0.9.1,-*- coding: utf-8 -*-
0.9.1,this one is needed for torchtext random call (shuffled iterator)
0.9.1,in multi gpu it ensures datasets are read in the same order
0.9.1,some cudnn methods can be random even after fixing the seed
0.9.1,unless you tell it to be deterministic
0.9.1,These ensure same initialization in multi gpu mode
0.9.1,Shift values to be >= 0
0.9.1,coding: utf-8
0.9.1,make a small vocab containing just the tokens in the source sequence
0.9.1,Map source tokens to indices in the dynamic dict.
0.9.1,self.src_vocabs is used in collapse_copy_scores and Translator.py
0.9.1,this assumes src_field and tgt_field are both text
0.9.1,fields needs to have only keys that examples have as attrs
0.9.1,avoid infinite recursion when fields isn't defined
0.9.1,-*- coding: utf-8 -*-
0.9.1,backwards compatibility
0.9.1,monkey-patch to make torchtext Vocab's pickleable
0.9.1,"List[Tuple[str, Vocab]] -> List[Tuple[str, Field]]"
0.9.1,"-> dict[str, Field]"
0.9.1,"Dict[str, List[Tuple[str, Field]]]"
0.9.1,doesn't change structure - don't return early.
0.9.1,"Dict[str, List[Tuple[str, Field]]] -> List[Tuple[str, Field]]"
0.9.1,"-> dict[str, Field]"
0.9.1,"if tgt isn't using TextMultiField, then no text field is."
0.9.1,this is basically copy-pasted from torchtext.
0.9.1,counters changes in place
0.9.1,keep the order of tokens specified in the vocab file by
0.9.1,adding them to the counter with decreasing counting values
0.9.1,`tgt_vocab_size` is ignored when sharing vocabularies
0.9.1,return vocab to dump with standard name
0.9.1,empty train_dataset_files so that vocab is only loaded from
0.9.1,"given paths in src_vocab_path, tgt_vocab_path"
0.9.1,Load vocabulary
0.9.1,Drop the none-using from memory but keep the last
0.9.1,"in the long run, shouldn't it be possible to do this by calling"
0.9.1,build_vocab with both the src and tgt data?
0.9.1,fast-forward if loaded from state
0.9.1,NOTE: `rnn.pack_padded_sequence` requires that a
0.9.1,"minibatch be sorted by decreasing order, which"
0.9.1,requires reversing relative to typical sort keys
0.9.1,Temporarily load one shard to retrieve sort_key for data_type
0.9.1,"NOTE: This is causing some issues for consumer/producer,"
0.9.1,as we may still have some of those examples in some queue
0.9.1,cur_dataset.examples = None
0.9.1,gc.collect()
0.9.1,del cur_dataset
0.9.1,gc.collect()
0.9.1,Cycle through the shards indefinitely.
0.9.1,"When the dataset is not repeated, we might need to ensure that"
0.9.1,the number of returned batches is the multiple of a given value.
0.9.1,This is important for multi GPU training to ensure that all
0.9.1,workers have the same number of batches to process.
0.9.1,Maintains the longest src and tgt length in the current batch
0.9.1,Reset current longest length at a new batch (count=1)
0.9.1,Src: [<bos> w1 ... wN <eos>]
0.9.1,Tgt: [w1 ... wM <eos>]
0.9.1,-*- coding: utf-8 -*-
0.9.1,imports of datatype-specific dependencies
0.9.1,torchaudio loading options recently changed. It's probably
0.9.1,straightforward to rewrite the audio handling to make use of
0.9.1,"up-to-date torchaudio, but in the meantime there is a legacy"
0.9.1,method which uses the old defaults
0.9.1,STFT
0.9.1,-*- coding: utf-8 -*-
0.9.1,domain specific dependencies
0.9.1,coding: utf-8
0.9.1,several data readers need optional dependencies. There's no
0.9.1,appropriate builtin exception
0.9.1,-*- coding: utf-8 -*-
0.9.1,mix this with partial
0.9.1,batch (list(list(list))): batch_size x len(self.fields) x seq_len
0.9.1,lengths: batch_size
0.9.1,data: seq_len x batch_size x len(self.fields)
0.9.1,flake8: noqa
0.9.1,For command-line option parsing
0.9.1,"Check pass, set the args."
0.9.1,"This SRU version implements its own cuda-level optimization,"
0.9.1,so it requires that:
0.9.1,1. `cupy` and `pynvrtc` python package installed.
0.9.1,2. pytorch is built with cuda support.
0.9.1,3. library path set: export LD_LIBRARY_PATH=<cuda lib path>.
0.9.1,Check 1.
0.9.1,Check 2.
0.9.1,Check 3.
0.9.1,This sets up device to use.
0.9.1,-> directions x batch x dim
0.9.1,For DEBUG
0.9.1,"size = (length, batch, x.size(-1)) \"
0.9.1,"if x.dim() == 3 else (batch, x.size(-1))"
0.9.1,grad_x = x.new(*x.size()) if k_ == 3 else x.new(*size).zero_()
0.9.1,Normal use
0.9.1,"An entry check here, will catch on train side and translate side"
0.9.1,if requirements are not satisfied.
0.9.1,RNNDecoderState wraps hidden as a tuple.
0.9.1,fh -> (layers*directions) x batch x dim
0.9.1,NOTE: We need to trim the vocab to remove any unk tokens that
0.9.1,were not originally here.
0.9.1,The score for each translation on the beam.
0.9.1,The backpointers at each time-step.
0.9.1,The outputs at each time-step.
0.9.1,Has EOS topped the beam yet.
0.9.1,The attentions (matrix) for each time.
0.9.1,Time and k pair for finished.
0.9.1,Information for global scoring.
0.9.1,Minimum prediction length
0.9.1,Apply Penalty at every step
0.9.1,force the output to be longer than self.min_length
0.9.1,assumes there are len(word_probs) predictions OTHER
0.9.1,than EOS that are greater than -1e20
0.9.1,Sum the previous scores.
0.9.1,Don't let EOS have children.
0.9.1,Block ngram repeats
0.9.1,"Last n tokens, n = block_ngram_repeat"
0.9.1,Skip the blocking if it is in the exclusion list
0.9.1,"best_scores_id is flattened beam x word array, so calculate which"
0.9.1,word and beam each score came from
0.9.1,End condition is when top-of-beam is EOS and no global score.
0.9.1,Add from beam until we have minimum outputs.
0.9.1,Term will be subtracted from probability
0.9.1,Probability will be divided by this
0.9.1,these warnings indicate that either the alpha/beta
0.9.1,"forces a penalty to be a no-op, or a penalty is a no-op but"
0.9.1,the alpha/beta would suggest otherwise.
0.9.1,using some length penalty
0.9.1,using some coverage penalty
0.9.1,"For temp=0.0, take the argmax to avoid divide-by-zero errors."
0.9.1,keep_topk=1 is also equivalent to argmax.
0.9.1,Set all logits that are not in the top-k to -10000.
0.9.1,This puts the probabilities close to 0.
0.9.1,"shape: (sum(~ self.is_finished), 1)"
0.9.1,magic indices
0.9.1,result caching
0.9.1,add one to account for BOS. Don't account for EOS because hitting
0.9.1,this implies it hasn't been found.
0.9.1,skip BOS
0.9.1,"Last n tokens, n = block_ngram_repeat"
0.9.1,skip the blocking if any token in gram is excluded
0.9.1,!/usr/bin/env python
0.9.1,for debugging
0.9.1,Statistics
0.9.1,TODO: support these blacklisted features.
0.9.1,Encoder forward.
0.9.1,"Shape: (1, B, 1)"
0.9.1,Reorder states.
0.9.1,Turn any copied words into UNKs.
0.9.1,"Decoder forward, takes [tgt_len, batch, nfeats] as input"
0.9.1,"and [src_len, batch, hidden] as memory_bank"
0.9.1,"in case of inference tgt_len = 1, batch = beam times batch_size"
0.9.1,"in case of Gold Scoring tgt_len = actual length, batch = 1 batch"
0.9.1,Generator forward.
0.9.1,"returns [(batch_size x beam_size) , vocab ] when 1 step"
0.9.1,"or [ tgt_len, batch_size, vocab ] when full sentence"
0.9.1,"here we have scores [tgt_lenxbatch, vocab] or [beamxbatch, vocab]"
0.9.1,"returns [(batch_size x beam_size) , vocab ] when 1 step"
0.9.1,"or [ tgt_len, batch_size, vocab ] when full sentence"
0.9.1,TODO: support these blacklisted features.
0.9.1,(0) Prep the components of the search.
0.9.1,(1) Run the encoder on the src.
0.9.1,(2) Repeat src objects `beam_size` times.
0.9.1,We use batch_size x beam_size
0.9.1,"(0) pt 2, prep the beam object"
0.9.1,Reorder states.
0.9.1,"This is left in the code for now, but unsued"
0.9.1,(0) Prep each of the components of the search.
0.9.1,And helper method for reducing verbosity.
0.9.1,(1) Run the encoder on the src.
0.9.1,(2) Repeat src objects `beam_size` times.
0.9.1,We use now  batch_size x beam_size (same as fast mode)
0.9.1,"(3) run the decoder to generate sentences, using beam search."
0.9.1,(a) Construct batch x beam_size nxt words.
0.9.1,Get all the pending current beam words and arrange for forward.
0.9.1,(b) Decode and forward
0.9.1,(c) Advance each beam.
0.9.1,Loop over the batch_size number of beam
0.9.1,(4) Extract sentences from beam.
0.9.1,Rollback pointer to the beginning.
0.9.1,beam parameters
0.9.1,result caching
0.9.1,beam state
0.9.1,buffers for the topk scores and 'backpointer'
0.9.1,"""global state"" of the old beam"
0.9.1,for testing
0.9.1,using integer division to get an integer _B without casting
0.9.1,force the output to be longer than self.min_length
0.9.1,Multiply probs by the beam probability.
0.9.1,"if the sequence ends now, then the penalty is the current"
0.9.1,"length + 1, to include the EOS token"
0.9.1,Flatten probs into a list of possibilities.
0.9.1,Recover log probs.
0.9.1,Length penalty is just a scalar. It doesn't matter if it's applied
0.9.1,before or after the topk.
0.9.1,Resolve beam origin and map to batch index flat representation.
0.9.1,Append last prediction.
0.9.1,update global state (step == 1)
0.9.1,update global state (step > 1)
0.9.1,"shape: (batch_size x beam_size, 1)"
0.9.1,Penalize beams that finished.
0.9.1,"on real data (newstest2017) with the pretrained transformer,"
0.9.1,it's faster to not move this back to the original device
0.9.1,Store finished hypotheses for this batch.
0.9.1,End condition is the top beam finished and we can return
0.9.1,n_best hypotheses.
0.9.1,"If all sentences are translated, no need to go further."
0.9.1,Remove finished batches for the next step.
0.9.1,!/usr/bin/env python
0.9.1,semaphore doesn't have a timeout arg in Python 2.7
0.9.1,backwards compatibility for confs
0.9.1,load can be called multiple times: modify copy
0.9.1,NOTE: translator returns lists of `n_best` list
0.9.1,we can ignore that (i.e. flatten lists) only because
0.9.1,we restrict `n_best=1`
0.9.1,build back results with empty texts
0.9.1,Below are all the different penalty terms implemented so far.
0.9.1,Subtract coverage penalty from topk log probs.
0.9.1,Divide topk log probs by length penalty.
0.9.1,Sorting
0.9.0,!/usr/bin/env python
0.9.0,!/usr/bin/env python
0.9.0,!/usr/bin/env python
0.9.0,-*- coding: utf-8 -*-
0.9.0,!/usr/bin/env python
0.9.0,-*- coding: utf-8 -*-
0.9.0,!/usr/bin/env python
0.9.0,Create a thread to listen for errors in the child processes.
0.9.0,Train with multiprocessing.
0.9.0,"propagate exception to parent process, keeping original traceback"
0.9.0,!/usr/bin/env python3
0.9.0,-*- coding: utf-8 -*-
0.9.0,
0.9.0,"OpenNMT-py documentation build configuration file, created by"
0.9.0,sphinx-quickstart on Sun Dec 17 12:07:14 2017.
0.9.0,
0.9.0,This file is execfile()d with the current directory set to its
0.9.0,containing dir.
0.9.0,
0.9.0,Note that not all possible configuration values are present in this
0.9.0,autogenerated file.
0.9.0,
0.9.0,All configuration values have a default; values that are commented out
0.9.0,serve to show the default.
0.9.0,"If extensions (or modules to document with autodoc) are in another directory,"
0.9.0,add these directories to sys.path here. If the directory is relative to the
0.9.0,"documentation root, use os.path.abspath to make it absolute, like shown here."
0.9.0,
0.9.0,import os
0.9.0,import sys
0.9.0,"sys.path.insert(0, os.path.abspath('.'))"
0.9.0,-- General configuration ------------------------------------------------
0.9.0,"If your documentation needs a minimal Sphinx version, state it here."
0.9.0,
0.9.0,needs_sphinx = '1.0'
0.9.0,"Add any Sphinx extension module names here, as strings. They can be"
0.9.0,extensions coming with Sphinx (named 'sphinx.ext.*') or your custom
0.9.0,ones.
0.9.0,Show base classes
0.9.0,"Use ""variables"" section for Attributes instead of weird block things"
0.9.0,mimicking the function style.
0.9.0,"Add any paths that contain templates here, relative to this directory."
0.9.0,The suffix(es) of source filenames.
0.9.0,You can specify multiple suffix as a list of string:
0.9.0,
0.9.0,"source_suffix = ['.rst', '.md']"
0.9.0,The master toctree document.
0.9.0,General information about the project.
0.9.0,"The version info for the project you're documenting, acts as replacement for"
0.9.0,"|version| and |release|, also used in various other places throughout the"
0.9.0,built documents.
0.9.0,
0.9.0,The short X.Y version.
0.9.0,"The full version, including alpha/beta/rc tags."
0.9.0,The language for content autogenerated by Sphinx. Refer to documentation
0.9.0,for a list of supported languages.
0.9.0,
0.9.0,This is also used if you do content translation via gettext catalogs.
0.9.0,"Usually you set ""language"" from the command line for these cases."
0.9.0,"List of patterns, relative to source directory, that match files and"
0.9.0,directories to ignore when looking for source files.
0.9.0,This patterns also effect to html_static_path and html_extra_path
0.9.0,The name of the Pygments (syntax highlighting) style to use.
0.9.0,"If true, `todo` and `todoList` produce output, else they produce nothing."
0.9.0,-- Options for HTML output ----------------------------------------------
0.9.0,The theme to use for HTML and HTML Help pages.  See the documentation for
0.9.0,a list of builtin themes.
0.9.0,
0.9.0,html_theme = 'sphinx_materialdesign_theme'
0.9.0,html_theme_path = [sphinx_materialdesign_theme.get_path()]
0.9.0,Theme options are theme-specific and customize the look and feel of a theme
0.9.0,"further.  For a list of options available for each theme, see the"
0.9.0,documentation.
0.9.0,
0.9.0,html_theme_options = {}
0.9.0,"Add any paths that contain custom static files (such as style sheets) here,"
0.9.0,"relative to this directory. They are copied after the builtin static files,"
0.9.0,"so a file named ""default.css"" will overwrite the builtin ""default.css""."
0.9.0,"Custom sidebar templates, must be a dictionary that maps document names"
0.9.0,to template names.
0.9.0,
0.9.0,This is required for the alabaster theme
0.9.0,refs: http://alabaster.readthedocs.io/en/latest/installation.html#sidebars
0.9.0,-- Options for HTMLHelp output ------------------------------------------
0.9.0,Output file base name for HTML help builder.
0.9.0,-- Options for LaTeX output ---------------------------------------------
0.9.0,The paper size ('letterpaper' or 'a4paper').
0.9.0,
0.9.0,"'papersize': 'letterpaper',"
0.9.0,"The font size ('10pt', '11pt' or '12pt')."
0.9.0,
0.9.0,"'pointsize': '10pt',"
0.9.0,Additional stuff for the LaTeX preamble.
0.9.0,
0.9.0,"'preamble': '',"
0.9.0,Latex figure (float) alignment
0.9.0,
0.9.0,"'figure_align': 'htbp',"
0.9.0,Grouping the document tree into LaTeX files. List of tuples
0.9.0,"(source start file, target name, title,"
0.9.0,"author, documentclass [howto, manual, or own class])."
0.9.0,-- Options for manual page output ---------------------------------------
0.9.0,One entry per manual page. List of tuples
0.9.0,"(source start file, name, description, authors, manual section)."
0.9.0,-- Options for Texinfo output -------------------------------------------
0.9.0,Grouping the document tree into Texinfo files. List of tuples
0.9.0,"(source start file, target name, title, author,"
0.9.0,"dir menu entry, description, category)"
0.9.0,!/usr/bin/env python
0.9.0,-*- coding: utf-8 -*-
0.9.0,is this reachable?
0.9.0,Read in embeddings
0.9.0,Write to file
0.9.0,!/usr/bin/env python
0.9.0,-*- coding: utf-8 -*-
0.9.0,"Add in default model arguments, possibly added since training."
0.9.0,-*- encoding: utf-8 -*-
0.9.0,!/usr/bin/env python
0.9.0,-*- coding: utf-8 -*-
0.9.0,Author: Rico Sennrich
0.9.0,flake8: noqa
0.9.0,This file is retrieved from https://github.com/rsennrich/subword-nmt
0.9.0,hack for python2/3 compatibility
0.9.0,check version information
0.9.0,some hacking to deal with duplicates (only consider first instance)
0.9.0,don't print end-of-word symbols
0.9.0,sys.stderr.write('cannot split {0} further.\n'.format(segment))
0.9.0,sys.stderr.write('OOV: {0}\n'.format(segment))
0.9.0,sys.stderr.write('OOV: {0}\n'.format(segment))
0.9.0,python 2/3 compatibility
0.9.0,read/write files as UTF-8
0.9.0,!/usr/bin/env python
0.9.0,!/usr/bin/env python
0.9.0,-*- coding: utf-8 -*-
0.9.0,Author: Rico Sennrich
0.9.0,flake8: noqa
0.9.0,This file is retrieved from https://github.com/rsennrich/subword-nmt
0.9.0,hack for python2/3 compatibility
0.9.0,"find all instances of pair, and update frequency/indices around it"
0.9.0,find first symbol
0.9.0,"if first symbol is followed by second symbol, we've found an occurrence of pair (old_word[i:i+2])"
0.9.0,"assuming a symbol sequence ""A B C"", if ""B C"" is merged, reduce the frequency of ""A B"""
0.9.0,"assuming a symbol sequence ""A B C B"", if ""B C"" is merged, reduce the frequency of ""C B""."
0.9.0,"however, skip this if the sequence is A B C B C, because the frequency of ""C B"" will be reduced by the previous code block"
0.9.0,find new pair
0.9.0,"assuming a symbol sequence ""A BC D"", if ""B C"" is merged, increase the frequency of ""A BC"""
0.9.0,"assuming a symbol sequence ""A BC B"", if ""B C"" is merged, increase the frequency of ""BC B"""
0.9.0,"however, if the sequence is A BC BC, skip this step because the count of ""BC BC"" will be incremented by the previous code block"
0.9.0,data structure of pair frequencies
0.9.0,index from pairs to words
0.9.0,version 0.2 changes the handling of the end-of-word token ('</w>');
0.9.0,version numbering allows bckward compatibility
0.9.0,"threshold is inspired by Zipfian assumption, but should only affect speed"
0.9.0,we probably missed the best pair because of pruning; go back to full statistics
0.9.0,"threshold is inspired by Zipfian assumption, but should only affect speed"
0.9.0,python 2/3 compatibility
0.9.0,read/write files as UTF-8
0.9.0,!/usr/bin/env python
0.9.0,Build embeddings.
0.9.0,Build encoder.
0.9.0,Build decoder.
0.9.0,Share the embedding matrix - preprocess with share_vocab required.
0.9.0,src/tgt vocab should be the same if `-share_vocab` is specified.
0.9.0,Build NMTModel(= encoder + decoder).
0.9.0,Build Generator.
0.9.0,Load the model states from checkpoint or initialize them.
0.9.0,This preserves backward-compat for models using customed layernorm
0.9.0,end of patch for backward compatibility
0.9.0,!/usr/bin/env python
0.9.0,NOTE: It's important that ``opt`` has been validated and updated
0.9.0,at this point.
0.9.0,Load checkpoint if we resume from a previous training.
0.9.0,check for code where vocab is saved instead of fields
0.9.0,(in the future this will be done in a smarter way)
0.9.0,"Report src and tgt vocab sizes, including for features"
0.9.0,Build model.
0.9.0,Build optimizer.
0.9.0,Build model saver
0.9.0,Embedding Options
0.9.0,Encoder-Decoder Options
0.9.0,"group.add('--residual', '-residual',   action=""store_true"","
0.9.0,"help=""Add residual connections between RNN layers."")"
0.9.0,Attention options
0.9.0,Generator and loss options.
0.9.0,Data options
0.9.0,"Dictionary options, for text corpus"
0.9.0,"if you want to pass an existing vocab.pt file, pass it to"
0.9.0,-src_vocab alone as it already contains tgt vocab.
0.9.0,"Truncation options, for text corpus"
0.9.0,Data processing options
0.9.0,Options most relevant to speech
0.9.0,Option most relevant to image input
0.9.0,GPU
0.9.0,Init options
0.9.0,Pretrained word vectors
0.9.0,Fixed word vectors
0.9.0,Optimization options
0.9.0,learning rate
0.9.0,Use TensorboardX for visualization during training
0.9.0,Options most relevant to speech
0.9.0,Option most relevant to image input
0.9.0,Options most relevant to summarization.
0.9.0,Alpha and Beta values for Google Length + Coverage penalty
0.9.0,"Described here: https://arxiv.org/pdf/1609.08144.pdf, Section 7"
0.9.0,Options most relevant to speech.
0.9.0,Option most relevant to image input
0.9.0,Copyright 2016 The Chromium Authors. All rights reserved.
0.9.0,Use of this source code is governed by a BSD-style license that can be
0.9.0,found in the LICENSE file.
0.9.0,"Get the key 'value' in the dict, or just use 'value'"
0.9.0,Basic attributes.
0.9.0,Set model in training mode.
0.9.0,UPDATE DROPOUT
0.9.0,Run patience mechanism
0.9.0,"If the patience has reached the limit, stop training"
0.9.0,Set model in validating mode.
0.9.0,F-prop through the model.
0.9.0,Compute loss.
0.9.0,Update statistics.
0.9.0,Set model back to training mode.
0.9.0,Truncated BPTT: reminder not compatible with accum > 1
0.9.0,1. Create truncated target.
0.9.0,2. F-prop all but generator.
0.9.0,3. Compute loss.
0.9.0,4. Update the parameters and statistics.
0.9.0,Multi GPU gradient gather
0.9.0,"If truncated, don't backprop fully."
0.9.0,TO CHECK
0.9.0,if dec_state is not None:
0.9.0,dec_state.detach()
0.9.0,"in case of multi step gradient accumulation,"
0.9.0,update only after accum batches
0.9.0,For Flake
0.9.0,we avoid padding while mean pooling
0.9.0,Initialize the bridge layer
0.9.0,"s_len, batch, emb_dim = emb.size()"
0.9.0,Lengths data is wrapped inside a Tensor.
0.9.0,"LSTM has hidden and cell state, other only one"
0.9.0,Total number of states
0.9.0,Build a linear layer for each
0.9.0,The encoder hidden is  (layers*directions) x batch x dim.
0.9.0,"s_len, batch, emb_dim = emb.size()"
0.9.0,Run the forward pass of every layer of the tranformer.
0.9.0,why is the model_opt.__dict__ check necessary?
0.9.0,"(batch_size, 64, imgH, imgW)"
0.9.0,layer 1
0.9.0,"(batch_size, 64, imgH/2, imgW/2)"
0.9.0,"(batch_size, 128, imgH/2, imgW/2)"
0.9.0,layer 2
0.9.0,"(batch_size, 128, imgH/2/2, imgW/2/2)"
0.9.0,"(batch_size, 256, imgH/2/2, imgW/2/2)"
0.9.0,layer 3
0.9.0,batch norm 1
0.9.0,"(batch_size, 256, imgH/2/2, imgW/2/2)"
0.9.0,layer4
0.9.0,"(batch_size, 256, imgH/2/2/2, imgW/2/2)"
0.9.0,"(batch_size, 512, imgH/2/2/2, imgW/2/2)"
0.9.0,layer 5
0.9.0,batch norm 2
0.9.0,"(batch_size, 512, imgH/2/2/2, imgW/2/2/2)"
0.9.0,"(batch_size, 512, imgH/2/2/2, imgW/2/2/2)"
0.9.0,"# (batch_size, 512, H, W)"
0.9.0,Dimensions and padding for constructing the word embedding matrix
0.9.0,Dimensions and padding for feature embedding matrices
0.9.0,(these have no effect if feat_vocab_sizes is empty)
0.9.0,The embedding matrix look-up tables. The first look-up table
0.9.0,"is for words. Subsequent ones are for features, if any exist."
0.9.0,The final output size of word + feature vectors. This can vary
0.9.0,from the word vector size if and only if features are defined.
0.9.0,This is the attribute you should access if you need to know
0.9.0,how big your embeddings are going to be.
0.9.0,The sequence of operations that converts the input sequence
0.9.0,into a sequence of embeddings. At minimum this consists of
0.9.0,looking up the embeddings for each word and feature in the
0.9.0,input. Model parameters may require the sequence to contain
0.9.0,additional operations as well.
0.9.0,features must use word_vec_size
0.9.0,features will use feat_vec_size
0.9.0,This class is mainly used by decoder.py for RNNs but also
0.9.0,by the CNN / transformer decoder when copy attention is used
0.9.0,CNN has its own attention mechanism ConvMultiStepAttention
0.9.0,Transformer has its own MultiHeadedAttention
0.9.0,mlp wants it with bias
0.9.0,Check input sizes
0.9.0,"(batch, t_len, d) x (batch, d, s_len) --> (batch, t_len, s_len)"
0.9.0,"(batch, t_len, s_len, d)"
0.9.0,one step input
0.9.0,"compute attention scores, as in Luong et al."
0.9.0,Softmax or sparsemax to normalize attention weights
0.9.0,each context vector c_t is the weighted average
0.9.0,over all the source hidden states
0.9.0,concatenate
0.9.0,Check output sizes
0.9.0,Check output sizes
0.9.0,clamping necessary because of numerical errors: loss should be lower
0.9.0,"bounded by zero, but negative values near zero are possible without"
0.9.0,the clamp
0.9.0,from onmt.utils.misc import aeq
0.9.0,CHECKS
0.9.0,"batch, k_len, d = key.size()"
0.9.0,"batch_, k_len_, d_ = value.size()"
0.9.0,"aeq(batch, batch_)"
0.9.0,"aeq(k_len, k_len_)"
0.9.0,"aeq(d, d_)"
0.9.0,"batch_, q_len, d_ = query.size()"
0.9.0,"aeq(batch, batch_)"
0.9.0,"aeq(d, d_)"
0.9.0,"aeq(self.model_dim % 8, 0)"
0.9.0,if mask is not None:
0.9.0,"batch_, q_len_, k_len_ = mask.size()"
0.9.0,"aeq(batch_, batch)"
0.9.0,"aeq(k_len_, k_len)"
0.9.0,aeq(q_len_ == q_len)
0.9.0,END CHECKS
0.9.0,"1) Project key, value, and query."
0.9.0,1 or key_len x key_len
0.9.0,1 or key_len x key_len x dim_per_head
0.9.0,1 or key_len x key_len x dim_per_head
0.9.0,2) Calculate and scale scores.
0.9.0,batch x num_heads x query_len x key_len
0.9.0,3) Apply attention dropout and compute context vectors.
0.9.0,CHECK
0.9.0,"batch_, q_len_, d_ = output.size()"
0.9.0,"aeq(q_len, q_len_)"
0.9.0,"aeq(batch, batch_)"
0.9.0,"aeq(d, d_)"
0.9.0,Return one attn
0.9.0,At the moment this class is only used by embeddings.Embeddings look-up tables
0.9.0,-*- coding: utf-8 -*-
0.9.0,checks
0.9.0,"batch, channel, height, width = base_target_emb.size()"
0.9.0,"batch_, channel_, height_, width_ = input_from_dec.size()"
0.9.0,"enc_batch, enc_channel, enc_height = encoder_out_top.size()"
0.9.0,"enc_batch_, enc_channel_, enc_height_ = encoder_out_combine.size()"
0.9.0,out_features * in_features
0.9.0,norm is out_features * 1
0.9.0,batch_size * out_features
0.9.0,out_features
0.9.0,out_features
0.9.0,batch_size * out_features
0.9.0,"out_channels, in_channels // groups, * kernel_size"
0.9.0,out_features
0.9.0,This is used nowhere in the code at the moment (Vincent Nguyen 05/18/2018)
0.9.0,"in_channels, out_channels, *kernel_size"
0.9.0,"in_channels, out_channels, *kernel_size"
0.9.0,"self.out_channels, 1"
0.9.0,out_features
0.9.0,out_features
0.9.0,store roots on diagonal
0.9.0,CHECKS
0.9.0,Original probabilities.
0.9.0,Probability of copying p(z=1) batch.
0.9.0,Probability of not copying: p_{word}(w) * (1 - p(z))
0.9.0,probabilities assigned by the model to the gold targets
0.9.0,probability of tokens copied from source
0.9.0,Set scores for unk to 0 and add eps
0.9.0,find the indices in which you do not use the copy mechanism
0.9.0,Drop padding.
0.9.0,this block does not depend on the loss value computed above
0.9.0,and is used only for stats
0.9.0,this block does not depend on the loss value computed above
0.9.0,and is used only for stats
0.9.0,Correct target copy token instead of <unk>
0.9.0,tgt[i] = align[i] + len(tgt_vocab)
0.9.0,for i such that tgt[i] == 0 and align[i] != 0
0.9.0,Compute sum of perplexities for stats
0.9.0,this part looks like it belongs in CopyGeneratorLoss
0.9.0,Compute Loss as NLL divided by seq length
0.9.0,Compute Total Loss per sequence in batch
0.9.0,Divide by length of each sequence and sum
0.9.0,all beams repeat (beam >= 1 repeat dummy scores)
0.9.0,predict repeat_idx over and over again
0.9.0,beam 0 and beam >=2 will repeat (beam >= 2 repeat dummy scores)
0.9.0,non-interesting beams are going to get dummy values
0.9.0,"on initial round, only predicted scores for beam 0"
0.9.0,matter. Make two predictions. Top one will be repeated
0.9.0,"in beam zero, second one will live on in beam 1."
0.9.0,predict the same thing in beam 0
0.9.0,continue pushing around what beam 1 predicts
0.9.0,"now beam 0 dies (along with the others), beam 1 -> beam 0"
0.9.0,beam 0 and beam >= 2 will repeat (beam 2 repeats excluded idx)
0.9.0,non-interesting beams are going to get dummy values
0.9.0,predict the same thing in beam 0
0.9.0,continue pushing around what beam 1 predicts
0.9.0,predict the allowed-repeat again in beam 2
0.9.0,"now beam 0 dies, beam 1 -> beam 0, beam 2 -> beam 1"
0.9.0,and the rest die
0.9.0,"since all preds after i=0 are 0, we can check"
0.9.0,that the beam is the correct idx by checking that
0.9.0,the curr score is the initial score
0.9.0,beam 0 will always predict EOS. The other beams will predict
0.9.0,non-eos scores.
0.9.0,"this is also a test that when block_ngram_repeat=0,"
0.9.0,repeating is acceptable
0.9.0,non-interesting beams are going to get dummy values
0.9.0,"""best"" prediction is eos - that should be blocked"
0.9.0,include at least beam_sz predictions OTHER than EOS
0.9.0,that are greater than -1e20
0.9.0,predict eos in beam 0
0.9.0,provide beam_sz other good predictions
0.9.0,now the top beam has ended and no others have
0.9.0,first beam finished had length beam.min_length
0.9.0,first beam finished was 0
0.9.0,"not of interest, but want to make sure it keeps running"
0.9.0,since only beam 0 terminates and n_best = 2
0.9.0,"this is also a test that when block_ngram_repeat=0,"
0.9.0,repeating is acceptable
0.9.0,non-interesting beams are going to get dummy values
0.9.0,"""best"" prediction is eos - that should be blocked"
0.9.0,include at least beam_sz predictions OTHER than EOS
0.9.0,that are greater than -1e20
0.9.0,predict eos in beam 1
0.9.0,provide beam_sz other good predictions in other beams
0.9.0,provide beam_sz other good predictions in other beams
0.9.0,beam 1 dies on min_length
0.9.0,beam 0 dies on the step after beam 1 dies
0.9.0,"init_preds: [4, 3, 5, 6, 7] - no EOS's"
0.9.0,no EOS's yet
0.9.0,"[5, 3, 2, 6, 0], so beam 2 predicts EOS!"
0.9.0,assumes beam 2 finished on last step
0.9.0,"[2, 5, 3, 6, 0], so beam 0 predicts EOS!"
0.9.0,"[-2.4879, -3.8910, -4.1010, -4.2010, -4.4010]"
0.9.0,new beam 0 finished
0.9.0,new beam 0 is old beam 3
0.9.0,assumes beam 0 finished on last step
0.9.0,"[5, 2, 6, 1, 0], so beam 1 predicts EOS!"
0.9.0,new beam 1 finished
0.9.0,new beam 1 is old beam 4
0.9.0,this could be considered an integration test because it tests
0.9.0,interactions between the GNMT scorer and the beam
0.9.0,initialize fields at the top of each unit test to prevent
0.9.0,any undesired stateful effects
0.9.0,"this test touches the file system, so it could be considered an"
0.9.0,integration test
0.9.0,write utf-8 bytes
0.9.0,predict repeat_idx over and over again
0.9.0,"batch 0 and 7 will repeat, the rest will advance"
0.9.0,predict the same thing in batch 0 and 7 every i
0.9.0,push around what the other batches predict
0.9.0,now batch 0 and 7 die
0.9.0,"batch 0 will repeat excluded idx, batch 1 will repeat"
0.9.0,now batch 1 dies
0.9.0,batch 0 will always predict EOS. The other batches will predict
0.9.0,non-eos scores.
0.9.0,"""best"" prediction is eos - that should be blocked"
0.9.0,include at least one prediction OTHER than EOS
0.9.0,that is greater than -1e20
0.9.0,now batch 0 has ended and no others have
0.9.0,initial step
0.9.0,batch 0 dies on step 0
0.9.0,include at least one prediction OTHER than EOS
0.9.0,that is greater than -1e20
0.9.0,step 2
0.9.0,(old) batch 8 dies on step 1
0.9.0,step 3
0.9.0,everything dies
0.9.0,initial step
0.9.0,batch 0 dies on step 0
0.9.0,include at least one prediction OTHER than EOS
0.9.0,that is greater than -1e20
0.9.0,step 2
0.9.0,(old) batch 8 dies on step 1
0.9.0,step 3
0.9.0,everything dies
0.9.0,illegal_weights_mask = torch.ByteTensor([
0.9.0,"[0, 0, 0, 0, 0, 0, 0],"
0.9.0,"[0, 0, 0, 1, 1, 1, 1],"
0.9.0,"[0, 0, 0, 0, 0, 1, 1],"
0.9.0,"[0, 0, 1, 1, 1, 1, 1]])"
0.9.0,TODO: fix for pytorch 0.3
0.9.0,illegal_weights = alignments.masked_select(illegal_weights_mask)
0.9.0,"self.assertEqual(0.0, illegal_weights.data.sum())"
0.9.0,this could be considered an integration test because it touches
0.9.0,the filesystem for the config file (and the models)
0.9.0,-*- coding: utf-8 -*-
0.9.0,tests pad and numericalize integration
0.9.0,tests pad and numericalize integration
0.9.0,"this test touches the file system, so it could be considered an"
0.9.0,integration test
0.9.0,file to hold full paths to audio data
0.9.0,file to hold audio paths relative to _AUDIO_DATA_DIR (i.e. file names)
0.9.0,it's ok if non-audio files co-exist with audio files in the data dir
0.9.0,"dividing gets the noise in [-1, 1]"
0.9.0,"this test touches the file system, so it could be considered an"
0.9.0,integration test
0.9.0,file to hold full paths to image data
0.9.0,file to hold image paths relative to _IMG_DATA_DIR (i.e. file names)
0.9.0,it's ok if non-image files co-exist with image files in the data dir
0.9.0,all beams repeat (beam >= 1 repeat dummy scores)
0.9.0,predict repeat_idx over and over again
0.9.0,beam 0 and beam >=2 will repeat (beam >= 2 repeat dummy scores)
0.9.0,non-interesting beams are going to get dummy values
0.9.0,"on initial round, only predicted scores for beam 0"
0.9.0,matter. Make two predictions. Top one will be repeated
0.9.0,"in beam zero, second one will live on in beam 1."
0.9.0,predict the same thing in beam 0
0.9.0,continue pushing around what beam 1 predicts
0.9.0,"now beam 0 dies (along with the others), beam 1 -> beam 0"
0.9.0,beam 0 and beam >= 2 will repeat (beam 2 repeats excluded idx)
0.9.0,non-interesting beams are going to get dummy values
0.9.0,predict the same thing in beam 0
0.9.0,continue pushing around what beam 1 predicts
0.9.0,predict the allowed-repeat again in beam 2
0.9.0,"now beam 0 dies, beam 1 -> beam 0, beam 2 -> beam 1"
0.9.0,and the rest die
0.9.0,"since all preds after i=0 are 0, we can check"
0.9.0,that the beam is the correct idx by checking that
0.9.0,the curr score is the initial score
0.9.0,beam 0 will always predict EOS. The other beams will predict
0.9.0,non-eos scores.
0.9.0,non-interesting beams are going to get dummy values
0.9.0,"""best"" prediction is eos - that should be blocked"
0.9.0,include at least beam_sz predictions OTHER than EOS
0.9.0,that are greater than -1e20
0.9.0,predict eos in beam 0
0.9.0,provide beam_sz other good predictions
0.9.0,now the top beam has ended and no others have
0.9.0,"not of interest, but want to make sure it keeps running"
0.9.0,since only beam 0 terminates and n_best = 2
0.9.0,"this is also a test that when block_ngram_repeat=0,"
0.9.0,repeating is acceptable
0.9.0,non-interesting beams are going to get dummy values
0.9.0,"""best"" prediction is eos - that should be blocked"
0.9.0,include at least beam_sz predictions OTHER than EOS
0.9.0,that are greater than -1e20
0.9.0,predict eos in beam 1
0.9.0,provide beam_sz other good predictions in other beams
0.9.0,provide beam_sz other good predictions in other beams
0.9.0,beam 1 dies on min_length
0.9.0,beam 0 dies on the step after beam 1 dies
0.9.0,non-interesting beams are going to get dummy values
0.9.0,"""best"" prediction is eos - that should be blocked"
0.9.0,include at least beam_sz predictions OTHER than EOS
0.9.0,that are greater than -1e20
0.9.0,predict eos in beam 1
0.9.0,provide beam_sz other good predictions in other beams
0.9.0,provide beam_sz other good predictions in other beams
0.9.0,no top beams are finished yet
0.9.0,beam 1 dies on min_length
0.9.0,no top beams are finished yet
0.9.0,beam 0 dies on the step after beam 1 dies
0.9.0,top beam is finished now so there are attentions
0.9.0,two beams are finished in each batch
0.9.0,second dim is cut down to the non-padded src length
0.9.0,first dim is equal to the time of death
0.9.0,(beam 0 died at current step - adjust for SOS)
0.9.0,(beam 1 died at last step - adjust for SOS)
0.9.0,behavior gets weird when beam is already done so just stop
0.9.0,this is just test_beam.TestBeamAgainstReferenceCase repeated
0.9.0,in each batch.
0.9.0,"init_preds: [4, 3, 5, 6, 7] - no EOS's"
0.9.0,no EOS's yet
0.9.0,"[5, 3, 2, 6, 0], so beam 2 predicts EOS!"
0.9.0,assumes beam 2 finished on last step
0.9.0,ended beam 2 shouldn't continue
0.9.0,"[2, 5, 3, 6, 0] repeat self.BATCH_SZ, so beam 0 predicts EOS!"
0.9.0,"[-2.4879, -3.8910, -4.1010, -4.2010, -4.4010] repeat self.BATCH_SZ"
0.9.0,another beam is finished in all batches
0.9.0,new beam 0 finished
0.9.0,new beam 0 is old beam 3
0.9.0,assumes beam 0 finished on last step
0.9.0,"[5, 2, 6, 1, 0] repeat self.BATCH_SZ, so beam 1 predicts EOS!"
0.9.0,new beam 1 finished
0.9.0,new beam 1 is old beam 4
0.9.0,this could be considered an integration test because it tests
0.9.0,interactions between the GNMT scorer and the beam
0.9.0,"-data option is required, but not used in this test, so dummy."
0.9.0,len x batch x nfeat
0.9.0,batch x c x h x w
0.9.0,batch x 1 x nfft x t
0.9.0,Initialize vectors to compare size with
0.9.0,Ensure correct sizes and types
0.9.0,Make sure that output has the correct size and type
0.9.0,Make sure that output has the correct size and type
0.9.0,Make sure that output has the correct size and type
0.9.0,"[('encoder_type', 'transformer'),"
0.9.0,"('word_vec_size', 16), ('rnn_size', 16)],"
0.9.0,""""""" Only do SRU test if requirment is safisfied. """""""
0.9.0,SRU doesn't support input_feed.
0.9.0,"when reasonable, set audio_enc_pooling to 2"
0.9.0,Need lengths >= audio_enc_pooling**n_layers.
0.9.0,"That condition is unrealistic for large n_layers,"
0.9.0,so leave audio_enc_pooling at 1.
0.9.0,first check there's nothing unexpectedly not trainable
0.9.0,ok: word embeddings shouldn't be trainable
0.9.0,if word vecs are fixed
0.9.0,ok: positional encodings shouldn't be trainable
0.9.0,then check nothing unexpectedly trainable
0.9.0,!/usr/bin/env python
0.9.0,-*- coding: utf-8 -*-
0.9.0,Remove the generated *pt files.
0.9.0,Test image preprocessing
0.9.0,Test audio preprocessing
0.9.0,Decoder state
0.9.0,Build the RNN.
0.9.0,Set up the context gate.
0.9.0,Set up the standard attention.
0.9.0,The encoder hidden is  (layers*directions) x batch x dim.
0.9.0,We need to convert it to layers x batch x (directions*dim).
0.9.0,Init the input feed.
0.9.0,Update the state with the result.
0.9.0,Concatenates sequence of tensors along a new dimension.
0.9.0,NOTE: v0.3 to 0.4: dec_outs / attns[*] may not be list
0.9.0,(in particular in case of SRU) it was not raising error in 0.3
0.9.0,since stack(Variable) was allowed.
0.9.0,"In 0.4, SRU returns a tensor that shouldn't be stacke"
0.9.0,Check
0.9.0,Calculate the attention.
0.9.0,Calculate the context gate.
0.9.0,Additional args check.
0.9.0,END Additional args check.
0.9.0,Input feed concatenates hidden state with
0.9.0,input at every time step.
0.9.0,TODO: context gate should be employed
0.9.0,instead of second RNN transform.
0.9.0,Update the coverage attention.
0.9.0,Decoder State
0.9.0,CNNDecoder has its own attention mechanism.
0.9.0,Set up a separate copy attention layer if needed.
0.9.0,The output of CNNEncoder.
0.9.0,The combination of output of CNNEncoder and source embeddings.
0.9.0,Process the result and update the attentions.
0.9.0,Update the state.
0.9.0,TODO change the way attns is returned dict => list or tuple (onnx)
0.9.0,Memory_lengths is a single tensor shared between all models.
0.9.0,This assumption will not hold if Translator is modified
0.9.0,to calculate memory_lengths as something other than the length
0.9.0,of the input.
0.9.0,Decoder State
0.9.0,"previously, there was a GlobalAttention module here for copy"
0.9.0,"attention. But it was never actually used -- the ""copy"" attention"
0.9.0,just reuses the context attention.
0.9.0,TODO change the way attns is returned dict => list or tuple (onnx)
0.9.0,"buffer size in bytes, determine equiv. # of elements based on data type"
0.9.0,copy tensors into buffer_t
0.9.0,all-reduce and rescale
0.9.0,copy all-reduced buffer back into tensors
0.9.0,"tensor is bigger than buffer, all-reduce and rescale directly"
0.9.0,"buffer is full, all-reduce and replace buffer with grad"
0.9.0,add tensor to buffer
0.9.0,TODO: Find a better way to check for sparse gradients.
0.9.0,TODO: clean this up when APEX unify its optimizer API.
0.9.0,Load everything from the checkpoint.
0.9.0,Build everything from scratch.
0.9.0,"Reset optimizer, keep options."
0.9.0,"Reset options, keep optimizer."
0.9.0,State can be partially restored.
0.9.0,Code below is an implementation of https://arxiv.org/pdf/1804.04235.pdf
0.9.0,inspired but modified from https://github.com/DeadAt0m/adafactor-pytorch
0.9.0,-*- coding: utf-8 -*-
0.9.0,if the loss function operates on vectors of raw logits instead of
0.9.0,"probabilities, only the first part of the generator needs to be"
0.9.0,"passed to the NMTLossCompute. At the moment, the only supported"
0.9.0,loss function of this kind is the sparsemax loss.
0.9.0,non_none: the subdict of the state dictionary where the values
0.9.0,are not None.
0.9.0,"Now, the iteration:"
0.9.0,state is a dictionary of sequences of tensor-like but we
0.9.0,want a sequence of dictionaries of tensors.
0.9.0,"First, unzip the dictionary into a sequence of keys and a"
0.9.0,sequence of tensor-like sequences.
0.9.0,"Now, yield a dictionary for each shard. The keys are always"
0.9.0,the same. values is a sequence of length #keys where each
0.9.0,element is a sequence of length #shards. We want to iterate
0.9.0,"over the shards, not over the keys: therefore, the values need"
0.9.0,to be re-zipped by shard and then each shard can be paired
0.9.0,with the keys.
0.9.0,Assumed backprop'd
0.9.0,Log the progress using the number of batches on the x-axis.
0.9.0,this check is here because audio allows the encoder and decoder to
0.9.0,"be different sizes, but other model types do not yet"
0.9.0,"Load default opt values, then overwrite with the opts in"
0.9.0,"the checkpoint. That way, if there are new options added,"
0.9.0,the defaults are used.
0.9.0,Don't do anything
0.9.0,Update best score of each criteria
0.9.0,Reset tolerance
0.9.0,Update current status
0.9.0,Decrease tolerance
0.9.0,Log
0.9.0,Log
0.9.0,Get a list of world_size lists with len(stat_list) Statistics objects
0.9.0,SRU doesn't support PackedSequence.
0.9.0,-*- coding: utf-8 -*-
0.9.0,this one is needed for torchtext random call (shuffled iterator)
0.9.0,in multi gpu it ensures datasets are read in the same order
0.9.0,some cudnn methods can be random even after fixing the seed
0.9.0,unless you tell it to be deterministic
0.9.0,These ensure same initialization in multi gpu mode
0.9.0,Shift values to be >= 0
0.9.0,coding: utf-8
0.9.0,make a small vocab containing just the tokens in the source sequence
0.9.0,Map source tokens to indices in the dynamic dict.
0.9.0,self.src_vocabs is used in collapse_copy_scores and Translator.py
0.9.0,this assumes src_field and tgt_field are both text
0.9.0,fields needs to have only keys that examples have as attrs
0.9.0,avoid infinite recursion when fields isn't defined
0.9.0,-*- coding: utf-8 -*-
0.9.0,backwards compatibility
0.9.0,monkey-patch to make torchtext Vocab's pickleable
0.9.0,"List[Tuple[str, Vocab]] -> List[Tuple[str, Field]]"
0.9.0,"-> dict[str, Field]"
0.9.0,"Dict[str, List[Tuple[str, Field]]]"
0.9.0,doesn't change structure - don't return early.
0.9.0,"Dict[str, List[Tuple[str, Field]]] -> List[Tuple[str, Field]]"
0.9.0,"-> dict[str, Field]"
0.9.0,"if tgt isn't using TextMultiField, then no text field is."
0.9.0,this is basically copy-pasted from torchtext.
0.9.0,counters changes in place
0.9.0,keep the order of tokens specified in the vocab file by
0.9.0,adding them to the counter with decreasing counting values
0.9.0,`tgt_vocab_size` is ignored when sharing vocabularies
0.9.0,return vocab to dump with standard name
0.9.0,empty train_dataset_files so that vocab is only loaded from
0.9.0,"given paths in src_vocab_path, tgt_vocab_path"
0.9.0,Load vocabulary
0.9.0,Drop the none-using from memory but keep the last
0.9.0,"in the long run, shouldn't it be possible to do this by calling"
0.9.0,build_vocab with both the src and tgt data?
0.9.0,fast-forward if loaded from state
0.9.0,NOTE: `rnn.pack_padded_sequence` requires that a
0.9.0,"minibatch be sorted by decreasing order, which"
0.9.0,requires reversing relative to typical sort keys
0.9.0,Temporarily load one shard to retrieve sort_key for data_type
0.9.0,Cycle through the shards indefinitely.
0.9.0,"When the dataset is not repeated, we might need to ensure that"
0.9.0,the number of returned batches is the multiple of a given value.
0.9.0,This is important for multi GPU training to ensure that all
0.9.0,workers have the same number of batches to process.
0.9.0,Maintains the longest src and tgt length in the current batch
0.9.0,Reset current longest length at a new batch (count=1)
0.9.0,Src: [<bos> w1 ... wN <eos>]
0.9.0,Tgt: [w1 ... wM <eos>]
0.9.0,-*- coding: utf-8 -*-
0.9.0,imports of datatype-specific dependencies
0.9.0,torchaudio loading options recently changed. It's probably
0.9.0,straightforward to rewrite the audio handling to make use of
0.9.0,"up-to-date torchaudio, but in the meantime there is a legacy"
0.9.0,method which uses the old defaults
0.9.0,STFT
0.9.0,-*- coding: utf-8 -*-
0.9.0,domain specific dependencies
0.9.0,coding: utf-8
0.9.0,several data readers need optional dependencies. There's no
0.9.0,appropriate builtin exception
0.9.0,-*- coding: utf-8 -*-
0.9.0,mix this with partial
0.9.0,batch (list(list(list))): batch_size x len(self.fields) x seq_len
0.9.0,lengths: batch_size
0.9.0,data: seq_len x batch_size x len(self.fields)
0.9.0,flake8: noqa
0.9.0,For command-line option parsing
0.9.0,"Check pass, set the args."
0.9.0,"This SRU version implements its own cuda-level optimization,"
0.9.0,so it requires that:
0.9.0,1. `cupy` and `pynvrtc` python package installed.
0.9.0,2. pytorch is built with cuda support.
0.9.0,3. library path set: export LD_LIBRARY_PATH=<cuda lib path>.
0.9.0,Check 1.
0.9.0,Check 2.
0.9.0,Check 3.
0.9.0,This sets up device to use.
0.9.0,-> directions x batch x dim
0.9.0,For DEBUG
0.9.0,"size = (length, batch, x.size(-1)) \"
0.9.0,"if x.dim() == 3 else (batch, x.size(-1))"
0.9.0,grad_x = x.new(*x.size()) if k_ == 3 else x.new(*size).zero_()
0.9.0,Normal use
0.9.0,"An entry check here, will catch on train side and translate side"
0.9.0,if requirements are not satisfied.
0.9.0,RNNDecoderState wraps hidden as a tuple.
0.9.0,fh -> (layers*directions) x batch x dim
0.9.0,The score for each translation on the beam.
0.9.0,The backpointers at each time-step.
0.9.0,The outputs at each time-step.
0.9.0,Has EOS topped the beam yet.
0.9.0,The attentions (matrix) for each time.
0.9.0,Time and k pair for finished.
0.9.0,Information for global scoring.
0.9.0,Minimum prediction length
0.9.0,Apply Penalty at every step
0.9.0,force the output to be longer than self.min_length
0.9.0,assumes there are len(word_probs) predictions OTHER
0.9.0,than EOS that are greater than -1e20
0.9.0,Sum the previous scores.
0.9.0,Don't let EOS have children.
0.9.0,Block ngram repeats
0.9.0,"Last n tokens, n = block_ngram_repeat"
0.9.0,Skip the blocking if it is in the exclusion list
0.9.0,"best_scores_id is flattened beam x word array, so calculate which"
0.9.0,word and beam each score came from
0.9.0,End condition is when top-of-beam is EOS and no global score.
0.9.0,Add from beam until we have minimum outputs.
0.9.0,Term will be subtracted from probability
0.9.0,Probability will be divided by this
0.9.0,these warnings indicate that either the alpha/beta
0.9.0,"forces a penalty to be a no-op, or a penalty is a no-op but"
0.9.0,the alpha/beta would suggest otherwise.
0.9.0,using some length penalty
0.9.0,using some coverage penalty
0.9.0,"For temp=0.0, take the argmax to avoid divide-by-zero errors."
0.9.0,keep_topk=1 is also equivalent to argmax.
0.9.0,Set all logits that are not in the top-k to -10000.
0.9.0,This puts the probabilities close to 0.
0.9.0,"shape: (sum(~ self.is_finished), 1)"
0.9.0,magic indices
0.9.0,result caching
0.9.0,add one to account for BOS. Don't account for EOS because hitting
0.9.0,this implies it hasn't been found.
0.9.0,skip BOS
0.9.0,"Last n tokens, n = block_ngram_repeat"
0.9.0,skip the blocking if any token in gram is excluded
0.9.0,!/usr/bin/env python
0.9.0,for debugging
0.9.0,Statistics
0.9.0,TODO: support these blacklisted features.
0.9.0,Encoder forward.
0.9.0,"Shape: (1, B, 1)"
0.9.0,Reorder states.
0.9.0,Turn any copied words into UNKs.
0.9.0,"Decoder forward, takes [tgt_len, batch, nfeats] as input"
0.9.0,"and [src_len, batch, hidden] as memory_bank"
0.9.0,"in case of inference tgt_len = 1, batch = beam times batch_size"
0.9.0,"in case of Gold Scoring tgt_len = actual length, batch = 1 batch"
0.9.0,Generator forward.
0.9.0,"returns [(batch_size x beam_size) , vocab ] when 1 step"
0.9.0,"or [ tgt_len, batch_size, vocab ] when full sentence"
0.9.0,"here we have scores [tgt_lenxbatch, vocab] or [beamxbatch, vocab]"
0.9.0,"returns [(batch_size x beam_size) , vocab ] when 1 step"
0.9.0,"or [ tgt_len, batch_size, vocab ] when full sentence"
0.9.0,TODO: support these blacklisted features.
0.9.0,(0) Prep the components of the search.
0.9.0,(1) Run the encoder on the src.
0.9.0,(2) Repeat src objects `beam_size` times.
0.9.0,We use batch_size x beam_size
0.9.0,"(0) pt 2, prep the beam object"
0.9.0,Reorder states.
0.9.0,"This is left in the code for now, but unsued"
0.9.0,(0) Prep each of the components of the search.
0.9.0,And helper method for reducing verbosity.
0.9.0,(1) Run the encoder on the src.
0.9.0,(2) Repeat src objects `beam_size` times.
0.9.0,We use now  batch_size x beam_size (same as fast mode)
0.9.0,"(3) run the decoder to generate sentences, using beam search."
0.9.0,(a) Construct batch x beam_size nxt words.
0.9.0,Get all the pending current beam words and arrange for forward.
0.9.0,(b) Decode and forward
0.9.0,(c) Advance each beam.
0.9.0,Loop over the batch_size number of beam
0.9.0,(4) Extract sentences from beam.
0.9.0,Rollback pointer to the beginning.
0.9.0,beam parameters
0.9.0,result caching
0.9.0,beam state
0.9.0,buffers for the topk scores and 'backpointer'
0.9.0,"""global state"" of the old beam"
0.9.0,for testing
0.9.0,using integer division to get an integer _B without casting
0.9.0,force the output to be longer than self.min_length
0.9.0,Multiply probs by the beam probability.
0.9.0,"if the sequence ends now, then the penalty is the current"
0.9.0,"length + 1, to include the EOS token"
0.9.0,Flatten probs into a list of possibilities.
0.9.0,Recover log probs.
0.9.0,Length penalty is just a scalar. It doesn't matter if it's applied
0.9.0,before or after the topk.
0.9.0,Resolve beam origin and map to batch index flat representation.
0.9.0,Append last prediction.
0.9.0,update global state (step == 1)
0.9.0,update global state (step > 1)
0.9.0,"shape: (batch_size x beam_size, 1)"
0.9.0,Penalize beams that finished.
0.9.0,"on real data (newstest2017) with the pretrained transformer,"
0.9.0,it's faster to not move this back to the original device
0.9.0,Store finished hypotheses for this batch.
0.9.0,End condition is the top beam finished and we can return
0.9.0,n_best hypotheses.
0.9.0,"If all sentences are translated, no need to go further."
0.9.0,Remove finished batches for the next step.
0.9.0,!/usr/bin/env python
0.9.0,semaphore doesn't have a timeout arg in Python 2.7
0.9.0,backwards compatibility for confs
0.9.0,load can be called multiple times: modify copy
0.9.0,NOTE: translator returns lists of `n_best` list
0.9.0,we can ignore that (i.e. flatten lists) only because
0.9.0,we restrict `n_best=1`
0.9.0,build back results with empty texts
0.9.0,Below are all the different penalty terms implemented so far.
0.9.0,Subtract coverage penalty from topk log probs.
0.9.0,Divide topk log probs by length penalty.
0.9.0,Sorting
0.8.2,!/usr/bin/env python
0.8.2,!/usr/bin/env python
0.8.2,!/usr/bin/env python
0.8.2,-*- coding: utf-8 -*-
0.8.2,!/usr/bin/env python
0.8.2,-*- coding: utf-8 -*-
0.8.2,!/usr/bin/env python
0.8.2,Create a thread to listen for errors in the child processes.
0.8.2,Train with multiprocessing.
0.8.2,"propagate exception to parent process, keeping original traceback"
0.8.2,!/usr/bin/env python3
0.8.2,-*- coding: utf-8 -*-
0.8.2,
0.8.2,"OpenNMT-py documentation build configuration file, created by"
0.8.2,sphinx-quickstart on Sun Dec 17 12:07:14 2017.
0.8.2,
0.8.2,This file is execfile()d with the current directory set to its
0.8.2,containing dir.
0.8.2,
0.8.2,Note that not all possible configuration values are present in this
0.8.2,autogenerated file.
0.8.2,
0.8.2,All configuration values have a default; values that are commented out
0.8.2,serve to show the default.
0.8.2,"If extensions (or modules to document with autodoc) are in another directory,"
0.8.2,add these directories to sys.path here. If the directory is relative to the
0.8.2,"documentation root, use os.path.abspath to make it absolute, like shown here."
0.8.2,
0.8.2,import os
0.8.2,import sys
0.8.2,"sys.path.insert(0, os.path.abspath('.'))"
0.8.2,-- General configuration ------------------------------------------------
0.8.2,"If your documentation needs a minimal Sphinx version, state it here."
0.8.2,
0.8.2,needs_sphinx = '1.0'
0.8.2,"Add any Sphinx extension module names here, as strings. They can be"
0.8.2,extensions coming with Sphinx (named 'sphinx.ext.*') or your custom
0.8.2,ones.
0.8.2,Show base classes
0.8.2,"Use ""variables"" section for Attributes instead of weird block things"
0.8.2,mimicking the function style.
0.8.2,"Add any paths that contain templates here, relative to this directory."
0.8.2,The suffix(es) of source filenames.
0.8.2,You can specify multiple suffix as a list of string:
0.8.2,
0.8.2,"source_suffix = ['.rst', '.md']"
0.8.2,The master toctree document.
0.8.2,General information about the project.
0.8.2,"The version info for the project you're documenting, acts as replacement for"
0.8.2,"|version| and |release|, also used in various other places throughout the"
0.8.2,built documents.
0.8.2,
0.8.2,The short X.Y version.
0.8.2,"The full version, including alpha/beta/rc tags."
0.8.2,The language for content autogenerated by Sphinx. Refer to documentation
0.8.2,for a list of supported languages.
0.8.2,
0.8.2,This is also used if you do content translation via gettext catalogs.
0.8.2,"Usually you set ""language"" from the command line for these cases."
0.8.2,"List of patterns, relative to source directory, that match files and"
0.8.2,directories to ignore when looking for source files.
0.8.2,This patterns also effect to html_static_path and html_extra_path
0.8.2,The name of the Pygments (syntax highlighting) style to use.
0.8.2,"If true, `todo` and `todoList` produce output, else they produce nothing."
0.8.2,-- Options for HTML output ----------------------------------------------
0.8.2,The theme to use for HTML and HTML Help pages.  See the documentation for
0.8.2,a list of builtin themes.
0.8.2,
0.8.2,html_theme = 'sphinx_materialdesign_theme'
0.8.2,html_theme_path = [sphinx_materialdesign_theme.get_path()]
0.8.2,Theme options are theme-specific and customize the look and feel of a theme
0.8.2,"further.  For a list of options available for each theme, see the"
0.8.2,documentation.
0.8.2,
0.8.2,html_theme_options = {}
0.8.2,"Add any paths that contain custom static files (such as style sheets) here,"
0.8.2,"relative to this directory. They are copied after the builtin static files,"
0.8.2,"so a file named ""default.css"" will overwrite the builtin ""default.css""."
0.8.2,"Custom sidebar templates, must be a dictionary that maps document names"
0.8.2,to template names.
0.8.2,
0.8.2,This is required for the alabaster theme
0.8.2,refs: http://alabaster.readthedocs.io/en/latest/installation.html#sidebars
0.8.2,-- Options for HTMLHelp output ------------------------------------------
0.8.2,Output file base name for HTML help builder.
0.8.2,-- Options for LaTeX output ---------------------------------------------
0.8.2,The paper size ('letterpaper' or 'a4paper').
0.8.2,
0.8.2,"'papersize': 'letterpaper',"
0.8.2,"The font size ('10pt', '11pt' or '12pt')."
0.8.2,
0.8.2,"'pointsize': '10pt',"
0.8.2,Additional stuff for the LaTeX preamble.
0.8.2,
0.8.2,"'preamble': '',"
0.8.2,Latex figure (float) alignment
0.8.2,
0.8.2,"'figure_align': 'htbp',"
0.8.2,Grouping the document tree into LaTeX files. List of tuples
0.8.2,"(source start file, target name, title,"
0.8.2,"author, documentclass [howto, manual, or own class])."
0.8.2,-- Options for manual page output ---------------------------------------
0.8.2,One entry per manual page. List of tuples
0.8.2,"(source start file, name, description, authors, manual section)."
0.8.2,-- Options for Texinfo output -------------------------------------------
0.8.2,Grouping the document tree into Texinfo files. List of tuples
0.8.2,"(source start file, target name, title, author,"
0.8.2,"dir menu entry, description, category)"
0.8.2,!/usr/bin/env python
0.8.2,-*- coding: utf-8 -*-
0.8.2,is this reachable?
0.8.2,!/usr/bin/env python
0.8.2,-*- coding: utf-8 -*-
0.8.2,"Add in default model arguments, possibly added since training."
0.8.2,-*- encoding: utf-8 -*-
0.8.2,!/usr/bin/env python
0.8.2,-*- coding: utf-8 -*-
0.8.2,Author: Rico Sennrich
0.8.2,flake8: noqa
0.8.2,This file is retrieved from https://github.com/rsennrich/subword-nmt
0.8.2,hack for python2/3 compatibility
0.8.2,check version information
0.8.2,some hacking to deal with duplicates (only consider first instance)
0.8.2,don't print end-of-word symbols
0.8.2,sys.stderr.write('cannot split {0} further.\n'.format(segment))
0.8.2,sys.stderr.write('OOV: {0}\n'.format(segment))
0.8.2,sys.stderr.write('OOV: {0}\n'.format(segment))
0.8.2,python 2/3 compatibility
0.8.2,read/write files as UTF-8
0.8.2,!/usr/bin/env python
0.8.2,!/usr/bin/env python
0.8.2,-*- coding: utf-8 -*-
0.8.2,Author: Rico Sennrich
0.8.2,flake8: noqa
0.8.2,This file is retrieved from https://github.com/rsennrich/subword-nmt
0.8.2,hack for python2/3 compatibility
0.8.2,"find all instances of pair, and update frequency/indices around it"
0.8.2,find first symbol
0.8.2,"if first symbol is followed by second symbol, we've found an occurrence of pair (old_word[i:i+2])"
0.8.2,"assuming a symbol sequence ""A B C"", if ""B C"" is merged, reduce the frequency of ""A B"""
0.8.2,"assuming a symbol sequence ""A B C B"", if ""B C"" is merged, reduce the frequency of ""C B""."
0.8.2,"however, skip this if the sequence is A B C B C, because the frequency of ""C B"" will be reduced by the previous code block"
0.8.2,find new pair
0.8.2,"assuming a symbol sequence ""A BC D"", if ""B C"" is merged, increase the frequency of ""A BC"""
0.8.2,"assuming a symbol sequence ""A BC B"", if ""B C"" is merged, increase the frequency of ""BC B"""
0.8.2,"however, if the sequence is A BC BC, skip this step because the count of ""BC BC"" will be incremented by the previous code block"
0.8.2,data structure of pair frequencies
0.8.2,index from pairs to words
0.8.2,version 0.2 changes the handling of the end-of-word token ('</w>');
0.8.2,version numbering allows bckward compatibility
0.8.2,"threshold is inspired by Zipfian assumption, but should only affect speed"
0.8.2,we probably missed the best pair because of pruning; go back to full statistics
0.8.2,"threshold is inspired by Zipfian assumption, but should only affect speed"
0.8.2,python 2/3 compatibility
0.8.2,read/write files as UTF-8
0.8.2,!/usr/bin/env python
0.8.2,Build embeddings.
0.8.2,Build encoder.
0.8.2,Build decoder.
0.8.2,Share the embedding matrix - preprocess with share_vocab required.
0.8.2,src/tgt vocab should be the same if `-share_vocab` is specified.
0.8.2,Build NMTModel(= encoder + decoder).
0.8.2,Build Generator.
0.8.2,Load the model states from checkpoint or initialize them.
0.8.2,This preserves backward-compat for models using customed layernorm
0.8.2,end of patch for backward compatibility
0.8.2,!/usr/bin/env python
0.8.2,NOTE: It's important that ``opt`` has been validated and updated
0.8.2,at this point.
0.8.2,Load checkpoint if we resume from a previous training.
0.8.2,check for code where vocab is saved instead of fields
0.8.2,(in the future this will be done in a smarter way)
0.8.2,"Report src and tgt vocab sizes, including for features"
0.8.2,Build model.
0.8.2,Build optimizer.
0.8.2,Build model saver
0.8.2,Embedding Options
0.8.2,Encoder-Decoder Options
0.8.2,"group.add('--residual', '-residual',   action=""store_true"","
0.8.2,"help=""Add residual connections between RNN layers."")"
0.8.2,Attention options
0.8.2,Generator and loss options.
0.8.2,Data options
0.8.2,"Dictionary options, for text corpus"
0.8.2,"Truncation options, for text corpus"
0.8.2,Data processing options
0.8.2,Options most relevant to speech
0.8.2,Option most relevant to image input
0.8.2,GPU
0.8.2,Init options
0.8.2,Pretrained word vectors
0.8.2,Fixed word vectors
0.8.2,Optimization options
0.8.2,learning rate
0.8.2,Use TensorboardX for visualization during training
0.8.2,Options most relevant to speech
0.8.2,Option most relevant to image input
0.8.2,Options most relevant to summarization.
0.8.2,Alpha and Beta values for Google Length + Coverage penalty
0.8.2,"Described here: https://arxiv.org/pdf/1609.08144.pdf, Section 7"
0.8.2,Options most relevant to speech.
0.8.2,Option most relevant to image input
0.8.2,Copyright 2016 The Chromium Authors. All rights reserved.
0.8.2,Use of this source code is governed by a BSD-style license that can be
0.8.2,found in the LICENSE file.
0.8.2,"Get the key 'value' in the dict, or just use 'value'"
0.8.2,Basic attributes.
0.8.2,Set model in training mode.
0.8.2,Set model in validating mode.
0.8.2,F-prop through the model.
0.8.2,Compute loss.
0.8.2,Update statistics.
0.8.2,Set model back to training mode.
0.8.2,Truncated BPTT: reminder not compatible with accum > 1
0.8.2,1. Create truncated target.
0.8.2,2. F-prop all but generator.
0.8.2,3. Compute loss.
0.8.2,4. Update the parameters and statistics.
0.8.2,Multi GPU gradient gather
0.8.2,"If truncated, don't backprop fully."
0.8.2,TO CHECK
0.8.2,if dec_state is not None:
0.8.2,dec_state.detach()
0.8.2,"in case of multi step gradient accumulation,"
0.8.2,update only after accum batches
0.8.2,For Flake
0.8.2,Initialize the bridge layer
0.8.2,"s_len, batch, emb_dim = emb.size()"
0.8.2,Lengths data is wrapped inside a Tensor.
0.8.2,"LSTM has hidden and cell state, other only one"
0.8.2,Total number of states
0.8.2,Build a linear layer for each
0.8.2,The encoder hidden is  (layers*directions) x batch x dim.
0.8.2,"s_len, batch, emb_dim = emb.size()"
0.8.2,Run the forward pass of every layer of the tranformer.
0.8.2,why is the model_opt.__dict__ check necessary?
0.8.2,"(batch_size, 64, imgH, imgW)"
0.8.2,layer 1
0.8.2,"(batch_size, 64, imgH/2, imgW/2)"
0.8.2,"(batch_size, 128, imgH/2, imgW/2)"
0.8.2,layer 2
0.8.2,"(batch_size, 128, imgH/2/2, imgW/2/2)"
0.8.2,"(batch_size, 256, imgH/2/2, imgW/2/2)"
0.8.2,layer 3
0.8.2,batch norm 1
0.8.2,"(batch_size, 256, imgH/2/2, imgW/2/2)"
0.8.2,layer4
0.8.2,"(batch_size, 256, imgH/2/2/2, imgW/2/2)"
0.8.2,"(batch_size, 512, imgH/2/2/2, imgW/2/2)"
0.8.2,layer 5
0.8.2,batch norm 2
0.8.2,"(batch_size, 512, imgH/2/2/2, imgW/2/2/2)"
0.8.2,"(batch_size, 512, imgH/2/2/2, imgW/2/2/2)"
0.8.2,"# (batch_size, 512, H, W)"
0.8.2,Dimensions and padding for constructing the word embedding matrix
0.8.2,Dimensions and padding for feature embedding matrices
0.8.2,(these have no effect if feat_vocab_sizes is empty)
0.8.2,The embedding matrix look-up tables. The first look-up table
0.8.2,"is for words. Subsequent ones are for features, if any exist."
0.8.2,The final output size of word + feature vectors. This can vary
0.8.2,from the word vector size if and only if features are defined.
0.8.2,This is the attribute you should access if you need to know
0.8.2,how big your embeddings are going to be.
0.8.2,The sequence of operations that converts the input sequence
0.8.2,into a sequence of embeddings. At minimum this consists of
0.8.2,looking up the embeddings for each word and feature in the
0.8.2,input. Model parameters may require the sequence to contain
0.8.2,additional operations as well.
0.8.2,features must use word_vec_size
0.8.2,features will use feat_vec_size
0.8.2,This class is mainly used by decoder.py for RNNs but also
0.8.2,by the CNN / transformer decoder when copy attention is used
0.8.2,CNN has its own attention mechanism ConvMultiStepAttention
0.8.2,Transformer has its own MultiHeadedAttention
0.8.2,mlp wants it with bias
0.8.2,Check input sizes
0.8.2,"(batch, t_len, d) x (batch, d, s_len) --> (batch, t_len, s_len)"
0.8.2,"(batch, t_len, s_len, d)"
0.8.2,one step input
0.8.2,"compute attention scores, as in Luong et al."
0.8.2,Softmax or sparsemax to normalize attention weights
0.8.2,each context vector c_t is the weighted average
0.8.2,over all the source hidden states
0.8.2,concatenate
0.8.2,Check output sizes
0.8.2,Check output sizes
0.8.2,clamping necessary because of numerical errors: loss should be lower
0.8.2,"bounded by zero, but negative values near zero are possible without"
0.8.2,the clamp
0.8.2,from onmt.utils.misc import aeq
0.8.2,CHECKS
0.8.2,"batch, k_len, d = key.size()"
0.8.2,"batch_, k_len_, d_ = value.size()"
0.8.2,"aeq(batch, batch_)"
0.8.2,"aeq(k_len, k_len_)"
0.8.2,"aeq(d, d_)"
0.8.2,"batch_, q_len, d_ = query.size()"
0.8.2,"aeq(batch, batch_)"
0.8.2,"aeq(d, d_)"
0.8.2,"aeq(self.model_dim % 8, 0)"
0.8.2,if mask is not None:
0.8.2,"batch_, q_len_, k_len_ = mask.size()"
0.8.2,"aeq(batch_, batch)"
0.8.2,"aeq(k_len_, k_len)"
0.8.2,aeq(q_len_ == q_len)
0.8.2,END CHECKS
0.8.2,"1) Project key, value, and query."
0.8.2,1 or key_len x key_len
0.8.2,1 or key_len x key_len x dim_per_head
0.8.2,1 or key_len x key_len x dim_per_head
0.8.2,2) Calculate and scale scores.
0.8.2,batch x num_heads x query_len x key_len
0.8.2,3) Apply attention dropout and compute context vectors.
0.8.2,CHECK
0.8.2,"batch_, q_len_, d_ = output.size()"
0.8.2,"aeq(q_len, q_len_)"
0.8.2,"aeq(batch, batch_)"
0.8.2,"aeq(d, d_)"
0.8.2,Return one attn
0.8.2,At the moment this class is only used by embeddings.Embeddings look-up tables
0.8.2,-*- coding: utf-8 -*-
0.8.2,checks
0.8.2,"batch, channel, height, width = base_target_emb.size()"
0.8.2,"batch_, channel_, height_, width_ = input_from_dec.size()"
0.8.2,"enc_batch, enc_channel, enc_height = encoder_out_top.size()"
0.8.2,"enc_batch_, enc_channel_, enc_height_ = encoder_out_combine.size()"
0.8.2,out_features * in_features
0.8.2,norm is out_features * 1
0.8.2,batch_size * out_features
0.8.2,out_features
0.8.2,out_features
0.8.2,batch_size * out_features
0.8.2,"out_channels, in_channels // groups, * kernel_size"
0.8.2,out_features
0.8.2,This is used nowhere in the code at the moment (Vincent Nguyen 05/18/2018)
0.8.2,"in_channels, out_channels, *kernel_size"
0.8.2,"in_channels, out_channels, *kernel_size"
0.8.2,"self.out_channels, 1"
0.8.2,out_features
0.8.2,out_features
0.8.2,store roots on diagonal
0.8.2,CHECKS
0.8.2,Original probabilities.
0.8.2,Probability of copying p(z=1) batch.
0.8.2,Probability of not copying: p_{word}(w) * (1 - p(z))
0.8.2,probabilities assigned by the model to the gold targets
0.8.2,probability of tokens copied from source
0.8.2,Set scores for unk to 0 and add eps
0.8.2,find the indices in which you do not use the copy mechanism
0.8.2,Drop padding.
0.8.2,this block does not depend on the loss value computed above
0.8.2,and is used only for stats
0.8.2,this block does not depend on the loss value computed above
0.8.2,and is used only for stats
0.8.2,Correct target copy token instead of <unk>
0.8.2,tgt[i] = align[i] + len(tgt_vocab)
0.8.2,for i such that tgt[i] == 0 and align[i] != 0
0.8.2,Compute sum of perplexities for stats
0.8.2,this part looks like it belongs in CopyGeneratorLoss
0.8.2,Compute Loss as NLL divided by seq length
0.8.2,Compute Total Loss per sequence in batch
0.8.2,Divide by length of each sequence and sum
0.8.2,all beams repeat (beam >= 1 repeat dummy scores)
0.8.2,predict repeat_idx over and over again
0.8.2,beam 0 and beam >=2 will repeat (beam >= 2 repeat dummy scores)
0.8.2,non-interesting beams are going to get dummy values
0.8.2,"on initial round, only predicted scores for beam 0"
0.8.2,matter. Make two predictions. Top one will be repeated
0.8.2,"in beam zero, second one will live on in beam 1."
0.8.2,predict the same thing in beam 0
0.8.2,continue pushing around what beam 1 predicts
0.8.2,"now beam 0 dies (along with the others), beam 1 -> beam 0"
0.8.2,beam 0 and beam >= 2 will repeat (beam 2 repeats excluded idx)
0.8.2,non-interesting beams are going to get dummy values
0.8.2,predict the same thing in beam 0
0.8.2,continue pushing around what beam 1 predicts
0.8.2,predict the allowed-repeat again in beam 2
0.8.2,"now beam 0 dies, beam 1 -> beam 0, beam 2 -> beam 1"
0.8.2,and the rest die
0.8.2,"since all preds after i=0 are 0, we can check"
0.8.2,that the beam is the correct idx by checking that
0.8.2,the curr score is the initial score
0.8.2,beam 0 will always predict EOS. The other beams will predict
0.8.2,non-eos scores.
0.8.2,"this is also a test that when block_ngram_repeat=0,"
0.8.2,repeating is acceptable
0.8.2,non-interesting beams are going to get dummy values
0.8.2,"""best"" prediction is eos - that should be blocked"
0.8.2,include at least beam_sz predictions OTHER than EOS
0.8.2,that are greater than -1e20
0.8.2,predict eos in beam 0
0.8.2,provide beam_sz other good predictions
0.8.2,now the top beam has ended and no others have
0.8.2,first beam finished had length beam.min_length
0.8.2,first beam finished was 0
0.8.2,"not of interest, but want to make sure it keeps running"
0.8.2,since only beam 0 terminates and n_best = 2
0.8.2,"this is also a test that when block_ngram_repeat=0,"
0.8.2,repeating is acceptable
0.8.2,non-interesting beams are going to get dummy values
0.8.2,"""best"" prediction is eos - that should be blocked"
0.8.2,include at least beam_sz predictions OTHER than EOS
0.8.2,that are greater than -1e20
0.8.2,predict eos in beam 1
0.8.2,provide beam_sz other good predictions in other beams
0.8.2,provide beam_sz other good predictions in other beams
0.8.2,beam 1 dies on min_length
0.8.2,beam 0 dies on the step after beam 1 dies
0.8.2,"init_preds: [4, 3, 5, 6, 7] - no EOS's"
0.8.2,no EOS's yet
0.8.2,"[5, 3, 2, 6, 0], so beam 2 predicts EOS!"
0.8.2,assumes beam 2 finished on last step
0.8.2,"[2, 5, 3, 6, 0], so beam 0 predicts EOS!"
0.8.2,"[-2.4879, -3.8910, -4.1010, -4.2010, -4.4010]"
0.8.2,new beam 0 finished
0.8.2,new beam 0 is old beam 3
0.8.2,assumes beam 0 finished on last step
0.8.2,"[5, 2, 6, 1, 0], so beam 1 predicts EOS!"
0.8.2,new beam 1 finished
0.8.2,new beam 1 is old beam 4
0.8.2,this could be considered an integration test because it tests
0.8.2,interactions between the GNMT scorer and the beam
0.8.2,initialize fields at the top of each unit test to prevent
0.8.2,any undesired stateful effects
0.8.2,"this test touches the file system, so it could be considered an"
0.8.2,integration test
0.8.2,write utf-8 bytes
0.8.2,predict repeat_idx over and over again
0.8.2,"batch 0 and 7 will repeat, the rest will advance"
0.8.2,predict the same thing in batch 0 and 7 every i
0.8.2,push around what the other batches predict
0.8.2,now batch 0 and 7 die
0.8.2,"batch 0 will repeat excluded idx, batch 1 will repeat"
0.8.2,now batch 1 dies
0.8.2,batch 0 will always predict EOS. The other batches will predict
0.8.2,non-eos scores.
0.8.2,"""best"" prediction is eos - that should be blocked"
0.8.2,include at least one prediction OTHER than EOS
0.8.2,that is greater than -1e20
0.8.2,now batch 0 has ended and no others have
0.8.2,initial step
0.8.2,batch 0 dies on step 0
0.8.2,include at least one prediction OTHER than EOS
0.8.2,that is greater than -1e20
0.8.2,step 2
0.8.2,(old) batch 8 dies on step 1
0.8.2,step 3
0.8.2,everything dies
0.8.2,initial step
0.8.2,batch 0 dies on step 0
0.8.2,include at least one prediction OTHER than EOS
0.8.2,that is greater than -1e20
0.8.2,step 2
0.8.2,(old) batch 8 dies on step 1
0.8.2,step 3
0.8.2,everything dies
0.8.2,illegal_weights_mask = torch.ByteTensor([
0.8.2,"[0, 0, 0, 0, 0, 0, 0],"
0.8.2,"[0, 0, 0, 1, 1, 1, 1],"
0.8.2,"[0, 0, 0, 0, 0, 1, 1],"
0.8.2,"[0, 0, 1, 1, 1, 1, 1]])"
0.8.2,TODO: fix for pytorch 0.3
0.8.2,illegal_weights = alignments.masked_select(illegal_weights_mask)
0.8.2,"self.assertEqual(0.0, illegal_weights.data.sum())"
0.8.2,this could be considered an integration test because it touches
0.8.2,the filesystem for the config file (and the models)
0.8.2,-*- coding: utf-8 -*-
0.8.2,tests pad and numericalize integration
0.8.2,tests pad and numericalize integration
0.8.2,"this test touches the file system, so it could be considered an"
0.8.2,integration test
0.8.2,file to hold full paths to audio data
0.8.2,file to hold audio paths relative to _AUDIO_DATA_DIR (i.e. file names)
0.8.2,it's ok if non-audio files co-exist with audio files in the data dir
0.8.2,"dividing gets the noise in [-1, 1]"
0.8.2,"this test touches the file system, so it could be considered an"
0.8.2,integration test
0.8.2,file to hold full paths to image data
0.8.2,file to hold image paths relative to _IMG_DATA_DIR (i.e. file names)
0.8.2,it's ok if non-image files co-exist with image files in the data dir
0.8.2,all beams repeat (beam >= 1 repeat dummy scores)
0.8.2,predict repeat_idx over and over again
0.8.2,beam 0 and beam >=2 will repeat (beam >= 2 repeat dummy scores)
0.8.2,non-interesting beams are going to get dummy values
0.8.2,"on initial round, only predicted scores for beam 0"
0.8.2,matter. Make two predictions. Top one will be repeated
0.8.2,"in beam zero, second one will live on in beam 1."
0.8.2,predict the same thing in beam 0
0.8.2,continue pushing around what beam 1 predicts
0.8.2,"now beam 0 dies (along with the others), beam 1 -> beam 0"
0.8.2,beam 0 and beam >= 2 will repeat (beam 2 repeats excluded idx)
0.8.2,non-interesting beams are going to get dummy values
0.8.2,predict the same thing in beam 0
0.8.2,continue pushing around what beam 1 predicts
0.8.2,predict the allowed-repeat again in beam 2
0.8.2,"now beam 0 dies, beam 1 -> beam 0, beam 2 -> beam 1"
0.8.2,and the rest die
0.8.2,"since all preds after i=0 are 0, we can check"
0.8.2,that the beam is the correct idx by checking that
0.8.2,the curr score is the initial score
0.8.2,beam 0 will always predict EOS. The other beams will predict
0.8.2,non-eos scores.
0.8.2,non-interesting beams are going to get dummy values
0.8.2,"""best"" prediction is eos - that should be blocked"
0.8.2,include at least beam_sz predictions OTHER than EOS
0.8.2,that are greater than -1e20
0.8.2,predict eos in beam 0
0.8.2,provide beam_sz other good predictions
0.8.2,now the top beam has ended and no others have
0.8.2,"not of interest, but want to make sure it keeps running"
0.8.2,since only beam 0 terminates and n_best = 2
0.8.2,"this is also a test that when block_ngram_repeat=0,"
0.8.2,repeating is acceptable
0.8.2,non-interesting beams are going to get dummy values
0.8.2,"""best"" prediction is eos - that should be blocked"
0.8.2,include at least beam_sz predictions OTHER than EOS
0.8.2,that are greater than -1e20
0.8.2,predict eos in beam 1
0.8.2,provide beam_sz other good predictions in other beams
0.8.2,provide beam_sz other good predictions in other beams
0.8.2,beam 1 dies on min_length
0.8.2,beam 0 dies on the step after beam 1 dies
0.8.2,non-interesting beams are going to get dummy values
0.8.2,"""best"" prediction is eos - that should be blocked"
0.8.2,include at least beam_sz predictions OTHER than EOS
0.8.2,that are greater than -1e20
0.8.2,predict eos in beam 1
0.8.2,provide beam_sz other good predictions in other beams
0.8.2,provide beam_sz other good predictions in other beams
0.8.2,no top beams are finished yet
0.8.2,beam 1 dies on min_length
0.8.2,no top beams are finished yet
0.8.2,beam 0 dies on the step after beam 1 dies
0.8.2,top beam is finished now so there are attentions
0.8.2,two beams are finished in each batch
0.8.2,second dim is cut down to the non-padded src length
0.8.2,first dim is equal to the time of death
0.8.2,(beam 0 died at current step - adjust for SOS)
0.8.2,(beam 1 died at last step - adjust for SOS)
0.8.2,behavior gets weird when beam is already done so just stop
0.8.2,this is just test_beam.TestBeamAgainstReferenceCase repeated
0.8.2,in each batch.
0.8.2,"init_preds: [4, 3, 5, 6, 7] - no EOS's"
0.8.2,no EOS's yet
0.8.2,"[5, 3, 2, 6, 0], so beam 2 predicts EOS!"
0.8.2,assumes beam 2 finished on last step
0.8.2,ended beam 2 shouldn't continue
0.8.2,"[2, 5, 3, 6, 0] repeat self.BATCH_SZ, so beam 0 predicts EOS!"
0.8.2,"[-2.4879, -3.8910, -4.1010, -4.2010, -4.4010] repeat self.BATCH_SZ"
0.8.2,another beam is finished in all batches
0.8.2,new beam 0 finished
0.8.2,new beam 0 is old beam 3
0.8.2,assumes beam 0 finished on last step
0.8.2,"[5, 2, 6, 1, 0] repeat self.BATCH_SZ, so beam 1 predicts EOS!"
0.8.2,new beam 1 finished
0.8.2,new beam 1 is old beam 4
0.8.2,this could be considered an integration test because it tests
0.8.2,interactions between the GNMT scorer and the beam
0.8.2,"-data option is required, but not used in this test, so dummy."
0.8.2,len x batch x nfeat
0.8.2,batch x c x h x w
0.8.2,batch x 1 x nfft x t
0.8.2,Initialize vectors to compare size with
0.8.2,Ensure correct sizes and types
0.8.2,Make sure that output has the correct size and type
0.8.2,Make sure that output has the correct size and type
0.8.2,Make sure that output has the correct size and type
0.8.2,"[('encoder_type', 'transformer'),"
0.8.2,"('word_vec_size', 16), ('rnn_size', 16)],"
0.8.2,""""""" Only do SRU test if requirment is safisfied. """""""
0.8.2,SRU doesn't support input_feed.
0.8.2,"when reasonable, set audio_enc_pooling to 2"
0.8.2,Need lengths >= audio_enc_pooling**n_layers.
0.8.2,"That condition is unrealistic for large n_layers,"
0.8.2,so leave audio_enc_pooling at 1.
0.8.2,first check there's nothing unexpectedly not trainable
0.8.2,ok: word embeddings shouldn't be trainable
0.8.2,if word vecs are fixed
0.8.2,ok: positional encodings shouldn't be trainable
0.8.2,then check nothing unexpectedly trainable
0.8.2,!/usr/bin/env python
0.8.2,-*- coding: utf-8 -*-
0.8.2,Remove the generated *pt files.
0.8.2,Test image preprocessing
0.8.2,Test audio preprocessing
0.8.2,Decoder state
0.8.2,Build the RNN.
0.8.2,Set up the context gate.
0.8.2,Set up the standard attention.
0.8.2,The encoder hidden is  (layers*directions) x batch x dim.
0.8.2,We need to convert it to layers x batch x (directions*dim).
0.8.2,Init the input feed.
0.8.2,Update the state with the result.
0.8.2,Concatenates sequence of tensors along a new dimension.
0.8.2,NOTE: v0.3 to 0.4: dec_outs / attns[*] may not be list
0.8.2,(in particular in case of SRU) it was not raising error in 0.3
0.8.2,since stack(Variable) was allowed.
0.8.2,"In 0.4, SRU returns a tensor that shouldn't be stacke"
0.8.2,Check
0.8.2,Calculate the attention.
0.8.2,Calculate the context gate.
0.8.2,Additional args check.
0.8.2,END Additional args check.
0.8.2,Input feed concatenates hidden state with
0.8.2,input at every time step.
0.8.2,TODO: context gate should be employed
0.8.2,instead of second RNN transform.
0.8.2,Update the coverage attention.
0.8.2,Decoder State
0.8.2,CNNDecoder has its own attention mechanism.
0.8.2,Set up a separate copy attention layer if needed.
0.8.2,The output of CNNEncoder.
0.8.2,The combination of output of CNNEncoder and source embeddings.
0.8.2,Process the result and update the attentions.
0.8.2,Update the state.
0.8.2,TODO change the way attns is returned dict => list or tuple (onnx)
0.8.2,Memory_lengths is a single tensor shared between all models.
0.8.2,This assumption will not hold if Translator is modified
0.8.2,to calculate memory_lengths as something other than the length
0.8.2,of the input.
0.8.2,Decoder State
0.8.2,"previously, there was a GlobalAttention module here for copy"
0.8.2,"attention. But it was never actually used -- the ""copy"" attention"
0.8.2,just reuses the context attention.
0.8.2,TODO change the way attns is returned dict => list or tuple (onnx)
0.8.2,"buffer size in bytes, determine equiv. # of elements based on data type"
0.8.2,copy tensors into buffer_t
0.8.2,all-reduce and rescale
0.8.2,copy all-reduced buffer back into tensors
0.8.2,"tensor is bigger than buffer, all-reduce and rescale directly"
0.8.2,"buffer is full, all-reduce and replace buffer with grad"
0.8.2,add tensor to buffer
0.8.2,TODO: Find a better way to check for sparse gradients.
0.8.2,TODO: clean this up when APEX unify its optimizer API.
0.8.2,Load everything from the checkpoint.
0.8.2,Build everything from scratch.
0.8.2,"Reset optimizer, keep options."
0.8.2,"Reset options, keep optimizer."
0.8.2,State can be partially restored.
0.8.2,Code below is an implementation of https://arxiv.org/pdf/1804.04235.pdf
0.8.2,inspired but modified from https://github.com/DeadAt0m/adafactor-pytorch
0.8.2,-*- coding: utf-8 -*-
0.8.2,if the loss function operates on vectors of raw logits instead of
0.8.2,"probabilities, only the first part of the generator needs to be"
0.8.2,"passed to the NMTLossCompute. At the moment, the only supported"
0.8.2,loss function of this kind is the sparsemax loss.
0.8.2,non_none: the subdict of the state dictionary where the values
0.8.2,are not None.
0.8.2,"Now, the iteration:"
0.8.2,state is a dictionary of sequences of tensor-like but we
0.8.2,want a sequence of dictionaries of tensors.
0.8.2,"First, unzip the dictionary into a sequence of keys and a"
0.8.2,sequence of tensor-like sequences.
0.8.2,"Now, yield a dictionary for each shard. The keys are always"
0.8.2,the same. values is a sequence of length #keys where each
0.8.2,element is a sequence of length #shards. We want to iterate
0.8.2,"over the shards, not over the keys: therefore, the values need"
0.8.2,to be re-zipped by shard and then each shard can be paired
0.8.2,with the keys.
0.8.2,Assumed backprop'd
0.8.2,Log the progress using the number of batches on the x-axis.
0.8.2,this check is here because audio allows the encoder and decoder to
0.8.2,"be different sizes, but other model types do not yet"
0.8.2,"Load default opt values, then overwrite with the opts in"
0.8.2,"the checkpoint. That way, if there are new options added,"
0.8.2,the defaults are used.
0.8.2,Get a list of world_size lists with len(stat_list) Statistics objects
0.8.2,SRU doesn't support PackedSequence.
0.8.2,-*- coding: utf-8 -*-
0.8.2,this one is needed for torchtext random call (shuffled iterator)
0.8.2,in multi gpu it ensures datasets are read in the same order
0.8.2,some cudnn methods can be random even after fixing the seed
0.8.2,unless you tell it to be deterministic
0.8.2,These ensure same initialization in multi gpu mode
0.8.2,Shift values to be >= 0
0.8.2,coding: utf-8
0.8.2,make a small vocab containing just the tokens in the source sequence
0.8.2,Map source tokens to indices in the dynamic dict.
0.8.2,self.src_vocabs is used in collapse_copy_scores and Translator.py
0.8.2,this assumes src_field and tgt_field are both text
0.8.2,fields needs to have only keys that examples have as attrs
0.8.2,avoid infinite recursion when fields isn't defined
0.8.2,-*- coding: utf-8 -*-
0.8.2,backwards compatibility
0.8.2,monkey-patch to make torchtext Vocab's pickleable
0.8.2,"List[Tuple[str, Vocab]] -> List[Tuple[str, Field]]"
0.8.2,"-> dict[str, Field]"
0.8.2,"Dict[str, List[Tuple[str, Field]]]"
0.8.2,doesn't change structure - don't return early.
0.8.2,"Dict[str, List[Tuple[str, Field]]] -> List[Tuple[str, Field]]"
0.8.2,"-> dict[str, Field]"
0.8.2,"if tgt isn't using TextMultiField, then no text field is."
0.8.2,this is basically copy-pasted from torchtext.
0.8.2,counters changes in place
0.8.2,keep the order of tokens specified in the vocab file by
0.8.2,adding them to the counter with decreasing counting values
0.8.2,Load vocabulary
0.8.2,Drop the none-using from memory but keep the last
0.8.2,`tgt_vocab_size` is ignored when sharing vocabularies
0.8.2,"in the long run, shouldn't it be possible to do this by calling"
0.8.2,build_vocab with both the src and tgt data?
0.8.2,Cycle through the shards indefinitely.
0.8.2,"When the dataset is not repeated, we might need to ensure that"
0.8.2,the number of returned batches is the multiple of a given value.
0.8.2,This is important for multi GPU training to ensure that all
0.8.2,workers have the same number of batches to process.
0.8.2,Maintains the longest src and tgt length in the current batch
0.8.2,Reset current longest length at a new batch (count=1)
0.8.2,Src: [<bos> w1 ... wN <eos>]
0.8.2,Tgt: [w1 ... wM <eos>]
0.8.2,-*- coding: utf-8 -*-
0.8.2,imports of datatype-specific dependencies
0.8.2,torchaudio loading options recently changed. It's probably
0.8.2,straightforward to rewrite the audio handling to make use of
0.8.2,"up-to-date torchaudio, but in the meantime there is a legacy"
0.8.2,method which uses the old defaults
0.8.2,STFT
0.8.2,-*- coding: utf-8 -*-
0.8.2,domain specific dependencies
0.8.2,coding: utf-8
0.8.2,several data readers need optional dependencies. There's no
0.8.2,appropriate builtin exception
0.8.2,-*- coding: utf-8 -*-
0.8.2,mix this with partial
0.8.2,batch (list(list(list))): batch_size x len(self.fields) x seq_len
0.8.2,lengths: batch_size
0.8.2,data: seq_len x batch_size x len(self.fields)
0.8.2,flake8: noqa
0.8.2,For command-line option parsing
0.8.2,"Check pass, set the args."
0.8.2,"This SRU version implements its own cuda-level optimization,"
0.8.2,so it requires that:
0.8.2,1. `cupy` and `pynvrtc` python package installed.
0.8.2,2. pytorch is built with cuda support.
0.8.2,3. library path set: export LD_LIBRARY_PATH=<cuda lib path>.
0.8.2,Check 1.
0.8.2,Check 2.
0.8.2,Check 3.
0.8.2,This sets up device to use.
0.8.2,-> directions x batch x dim
0.8.2,For DEBUG
0.8.2,"size = (length, batch, x.size(-1)) \"
0.8.2,"if x.dim() == 3 else (batch, x.size(-1))"
0.8.2,grad_x = x.new(*x.size()) if k_ == 3 else x.new(*size).zero_()
0.8.2,Normal use
0.8.2,"An entry check here, will catch on train side and translate side"
0.8.2,if requirements are not satisfied.
0.8.2,RNNDecoderState wraps hidden as a tuple.
0.8.2,fh -> (layers*directions) x batch x dim
0.8.2,The score for each translation on the beam.
0.8.2,The backpointers at each time-step.
0.8.2,The outputs at each time-step.
0.8.2,Has EOS topped the beam yet.
0.8.2,The attentions (matrix) for each time.
0.8.2,Time and k pair for finished.
0.8.2,Information for global scoring.
0.8.2,Minimum prediction length
0.8.2,Apply Penalty at every step
0.8.2,force the output to be longer than self.min_length
0.8.2,assumes there are len(word_probs) predictions OTHER
0.8.2,than EOS that are greater than -1e20
0.8.2,Sum the previous scores.
0.8.2,Don't let EOS have children.
0.8.2,Block ngram repeats
0.8.2,"Last n tokens, n = block_ngram_repeat"
0.8.2,Skip the blocking if it is in the exclusion list
0.8.2,"best_scores_id is flattened beam x word array, so calculate which"
0.8.2,word and beam each score came from
0.8.2,End condition is when top-of-beam is EOS and no global score.
0.8.2,Add from beam until we have minimum outputs.
0.8.2,Term will be subtracted from probability
0.8.2,Probability will be divided by this
0.8.2,these warnings indicate that either the alpha/beta
0.8.2,"forces a penalty to be a no-op, or a penalty is a no-op but"
0.8.2,the alpha/beta would suggest otherwise.
0.8.2,using some length penalty
0.8.2,using some coverage penalty
0.8.2,"For temp=0.0, take the argmax to avoid divide-by-zero errors."
0.8.2,keep_topk=1 is also equivalent to argmax.
0.8.2,Set all logits that are not in the top-k to -10000.
0.8.2,This puts the probabilities close to 0.
0.8.2,"shape: (sum(~ self.is_finished), 1)"
0.8.2,magic indices
0.8.2,result caching
0.8.2,add one to account for BOS. Don't account for EOS because hitting
0.8.2,this implies it hasn't been found.
0.8.2,skip BOS
0.8.2,"Last n tokens, n = block_ngram_repeat"
0.8.2,skip the blocking if any token in gram is excluded
0.8.2,!/usr/bin/env python
0.8.2,for debugging
0.8.2,Statistics
0.8.2,TODO: support these blacklisted features.
0.8.2,Encoder forward.
0.8.2,"Shape: (1, B, 1)"
0.8.2,Reorder states.
0.8.2,Turn any copied words into UNKs.
0.8.2,"Decoder forward, takes [tgt_len, batch, nfeats] as input"
0.8.2,"and [src_len, batch, hidden] as memory_bank"
0.8.2,"in case of inference tgt_len = 1, batch = beam times batch_size"
0.8.2,"in case of Gold Scoring tgt_len = actual length, batch = 1 batch"
0.8.2,Generator forward.
0.8.2,"returns [(batch_size x beam_size) , vocab ] when 1 step"
0.8.2,"or [ tgt_len, batch_size, vocab ] when full sentence"
0.8.2,"here we have scores [tgt_lenxbatch, vocab] or [beamxbatch, vocab]"
0.8.2,"returns [(batch_size x beam_size) , vocab ] when 1 step"
0.8.2,"or [ tgt_len, batch_size, vocab ] when full sentence"
0.8.2,TODO: support these blacklisted features.
0.8.2,(0) Prep the components of the search.
0.8.2,(1) Run the encoder on the src.
0.8.2,(2) Repeat src objects `beam_size` times.
0.8.2,We use batch_size x beam_size
0.8.2,"(0) pt 2, prep the beam object"
0.8.2,Reorder states.
0.8.2,"This is left in the code for now, but unsued"
0.8.2,(0) Prep each of the components of the search.
0.8.2,And helper method for reducing verbosity.
0.8.2,(1) Run the encoder on the src.
0.8.2,(2) Repeat src objects `beam_size` times.
0.8.2,We use now  batch_size x beam_size (same as fast mode)
0.8.2,"(3) run the decoder to generate sentences, using beam search."
0.8.2,(a) Construct batch x beam_size nxt words.
0.8.2,Get all the pending current beam words and arrange for forward.
0.8.2,(b) Decode and forward
0.8.2,(c) Advance each beam.
0.8.2,Loop over the batch_size number of beam
0.8.2,(4) Extract sentences from beam.
0.8.2,Rollback pointer to the beginning.
0.8.2,beam parameters
0.8.2,result caching
0.8.2,beam state
0.8.2,buffers for the topk scores and 'backpointer'
0.8.2,"""global state"" of the old beam"
0.8.2,for testing
0.8.2,using integer division to get an integer _B without casting
0.8.2,force the output to be longer than self.min_length
0.8.2,Multiply probs by the beam probability.
0.8.2,"if the sequence ends now, then the penalty is the current"
0.8.2,"length + 1, to include the EOS token"
0.8.2,Flatten probs into a list of possibilities.
0.8.2,Recover log probs.
0.8.2,Length penalty is just a scalar. It doesn't matter if it's applied
0.8.2,before or after the topk.
0.8.2,Resolve beam origin and map to batch index flat representation.
0.8.2,Append last prediction.
0.8.2,update global state (step == 1)
0.8.2,update global state (step > 1)
0.8.2,"shape: (batch_size x beam_size, 1)"
0.8.2,Penalize beams that finished.
0.8.2,"on real data (newstest2017) with the pretrained transformer,"
0.8.2,it's faster to not move this back to the original device
0.8.2,Store finished hypotheses for this batch.
0.8.2,End condition is the top beam finished and we can return
0.8.2,n_best hypotheses.
0.8.2,"If all sentences are translated, no need to go further."
0.8.2,Remove finished batches for the next step.
0.8.2,!/usr/bin/env python
0.8.2,semaphore doesn't have a timeout arg in Python 2.7
0.8.2,backwards compatibility for confs
0.8.2,load can be called multiple times: modify copy
0.8.2,NOTE: translator returns lists of `n_best` list
0.8.2,we can ignore that (i.e. flatten lists) only because
0.8.2,we restrict `n_best=1`
0.8.2,build back results with empty texts
0.8.2,Below are all the different penalty terms implemented so far.
0.8.2,Subtract coverage penalty from topk log probs.
0.8.2,Divide topk log probs by length penalty.
0.8.2,Sorting
0.8.1,!/usr/bin/env python
0.8.1,!/usr/bin/env python
0.8.1,!/usr/bin/env python
0.8.1,-*- coding: utf-8 -*-
0.8.1,!/usr/bin/env python
0.8.1,-*- coding: utf-8 -*-
0.8.1,!/usr/bin/env python
0.8.1,Create a thread to listen for errors in the child processes.
0.8.1,Train with multiprocessing.
0.8.1,"propagate exception to parent process, keeping original traceback"
0.8.1,!/usr/bin/env python3
0.8.1,-*- coding: utf-8 -*-
0.8.1,
0.8.1,"OpenNMT-py documentation build configuration file, created by"
0.8.1,sphinx-quickstart on Sun Dec 17 12:07:14 2017.
0.8.1,
0.8.1,This file is execfile()d with the current directory set to its
0.8.1,containing dir.
0.8.1,
0.8.1,Note that not all possible configuration values are present in this
0.8.1,autogenerated file.
0.8.1,
0.8.1,All configuration values have a default; values that are commented out
0.8.1,serve to show the default.
0.8.1,"If extensions (or modules to document with autodoc) are in another directory,"
0.8.1,add these directories to sys.path here. If the directory is relative to the
0.8.1,"documentation root, use os.path.abspath to make it absolute, like shown here."
0.8.1,
0.8.1,import os
0.8.1,import sys
0.8.1,"sys.path.insert(0, os.path.abspath('.'))"
0.8.1,-- General configuration ------------------------------------------------
0.8.1,"If your documentation needs a minimal Sphinx version, state it here."
0.8.1,
0.8.1,needs_sphinx = '1.0'
0.8.1,"Add any Sphinx extension module names here, as strings. They can be"
0.8.1,extensions coming with Sphinx (named 'sphinx.ext.*') or your custom
0.8.1,ones.
0.8.1,"Add any paths that contain templates here, relative to this directory."
0.8.1,The suffix(es) of source filenames.
0.8.1,You can specify multiple suffix as a list of string:
0.8.1,
0.8.1,"source_suffix = ['.rst', '.md']"
0.8.1,The master toctree document.
0.8.1,General information about the project.
0.8.1,"The version info for the project you're documenting, acts as replacement for"
0.8.1,"|version| and |release|, also used in various other places throughout the"
0.8.1,built documents.
0.8.1,
0.8.1,The short X.Y version.
0.8.1,"The full version, including alpha/beta/rc tags."
0.8.1,The language for content autogenerated by Sphinx. Refer to documentation
0.8.1,for a list of supported languages.
0.8.1,
0.8.1,This is also used if you do content translation via gettext catalogs.
0.8.1,"Usually you set ""language"" from the command line for these cases."
0.8.1,"List of patterns, relative to source directory, that match files and"
0.8.1,directories to ignore when looking for source files.
0.8.1,This patterns also effect to html_static_path and html_extra_path
0.8.1,The name of the Pygments (syntax highlighting) style to use.
0.8.1,"If true, `todo` and `todoList` produce output, else they produce nothing."
0.8.1,-- Options for HTML output ----------------------------------------------
0.8.1,The theme to use for HTML and HTML Help pages.  See the documentation for
0.8.1,a list of builtin themes.
0.8.1,
0.8.1,html_theme = 'sphinx_materialdesign_theme'
0.8.1,html_theme_path = [sphinx_materialdesign_theme.get_path()]
0.8.1,Theme options are theme-specific and customize the look and feel of a theme
0.8.1,"further.  For a list of options available for each theme, see the"
0.8.1,documentation.
0.8.1,
0.8.1,html_theme_options = {}
0.8.1,"Add any paths that contain custom static files (such as style sheets) here,"
0.8.1,"relative to this directory. They are copied after the builtin static files,"
0.8.1,"so a file named ""default.css"" will overwrite the builtin ""default.css""."
0.8.1,"Custom sidebar templates, must be a dictionary that maps document names"
0.8.1,to template names.
0.8.1,
0.8.1,This is required for the alabaster theme
0.8.1,refs: http://alabaster.readthedocs.io/en/latest/installation.html#sidebars
0.8.1,-- Options for HTMLHelp output ------------------------------------------
0.8.1,Output file base name for HTML help builder.
0.8.1,-- Options for LaTeX output ---------------------------------------------
0.8.1,The paper size ('letterpaper' or 'a4paper').
0.8.1,
0.8.1,"'papersize': 'letterpaper',"
0.8.1,"The font size ('10pt', '11pt' or '12pt')."
0.8.1,
0.8.1,"'pointsize': '10pt',"
0.8.1,Additional stuff for the LaTeX preamble.
0.8.1,
0.8.1,"'preamble': '',"
0.8.1,Latex figure (float) alignment
0.8.1,
0.8.1,"'figure_align': 'htbp',"
0.8.1,Grouping the document tree into LaTeX files. List of tuples
0.8.1,"(source start file, target name, title,"
0.8.1,"author, documentclass [howto, manual, or own class])."
0.8.1,-- Options for manual page output ---------------------------------------
0.8.1,One entry per manual page. List of tuples
0.8.1,"(source start file, name, description, authors, manual section)."
0.8.1,-- Options for Texinfo output -------------------------------------------
0.8.1,Grouping the document tree into Texinfo files. List of tuples
0.8.1,"(source start file, target name, title, author,"
0.8.1,"dir menu entry, description, category)"
0.8.1,!/usr/bin/env python
0.8.1,-*- coding: utf-8 -*-
0.8.1,is this reachable?
0.8.1,!/usr/bin/env python
0.8.1,-*- coding: utf-8 -*-
0.8.1,"Add in default model arguments, possibly added since training."
0.8.1,-*- encoding: utf-8 -*-
0.8.1,!/usr/bin/env python
0.8.1,-*- coding: utf-8 -*-
0.8.1,Author: Rico Sennrich
0.8.1,flake8: noqa
0.8.1,This file is retrieved from https://github.com/rsennrich/subword-nmt
0.8.1,hack for python2/3 compatibility
0.8.1,check version information
0.8.1,some hacking to deal with duplicates (only consider first instance)
0.8.1,don't print end-of-word symbols
0.8.1,sys.stderr.write('cannot split {0} further.\n'.format(segment))
0.8.1,sys.stderr.write('OOV: {0}\n'.format(segment))
0.8.1,sys.stderr.write('OOV: {0}\n'.format(segment))
0.8.1,python 2/3 compatibility
0.8.1,read/write files as UTF-8
0.8.1,!/usr/bin/env python
0.8.1,!/usr/bin/env python
0.8.1,-*- coding: utf-8 -*-
0.8.1,Author: Rico Sennrich
0.8.1,flake8: noqa
0.8.1,This file is retrieved from https://github.com/rsennrich/subword-nmt
0.8.1,hack for python2/3 compatibility
0.8.1,"find all instances of pair, and update frequency/indices around it"
0.8.1,find first symbol
0.8.1,"if first symbol is followed by second symbol, we've found an occurrence of pair (old_word[i:i+2])"
0.8.1,"assuming a symbol sequence ""A B C"", if ""B C"" is merged, reduce the frequency of ""A B"""
0.8.1,"assuming a symbol sequence ""A B C B"", if ""B C"" is merged, reduce the frequency of ""C B""."
0.8.1,"however, skip this if the sequence is A B C B C, because the frequency of ""C B"" will be reduced by the previous code block"
0.8.1,find new pair
0.8.1,"assuming a symbol sequence ""A BC D"", if ""B C"" is merged, increase the frequency of ""A BC"""
0.8.1,"assuming a symbol sequence ""A BC B"", if ""B C"" is merged, increase the frequency of ""BC B"""
0.8.1,"however, if the sequence is A BC BC, skip this step because the count of ""BC BC"" will be incremented by the previous code block"
0.8.1,data structure of pair frequencies
0.8.1,index from pairs to words
0.8.1,version 0.2 changes the handling of the end-of-word token ('</w>');
0.8.1,version numbering allows bckward compatibility
0.8.1,"threshold is inspired by Zipfian assumption, but should only affect speed"
0.8.1,we probably missed the best pair because of pruning; go back to full statistics
0.8.1,"threshold is inspired by Zipfian assumption, but should only affect speed"
0.8.1,python 2/3 compatibility
0.8.1,read/write files as UTF-8
0.8.1,!/usr/bin/env python
0.8.1,for backward compatibility
0.8.1,Build embeddings.
0.8.1,Build encoder.
0.8.1,Build decoder.
0.8.1,Share the embedding matrix - preprocess with share_vocab required.
0.8.1,src/tgt vocab should be the same if `-share_vocab` is specified.
0.8.1,Build NMTModel(= encoder + decoder).
0.8.1,Build Generator.
0.8.1,Load the model states from checkpoint or initialize them.
0.8.1,This preserves backward-compat for models using customed layernorm
0.8.1,end of patch for backward compatibility
0.8.1,!/usr/bin/env python
0.8.1,this check is here because audio allows the encoder and decoder to
0.8.1,"be different sizes, but other model types do not yet"
0.8.1,Load checkpoint if we resume from a previous training.
0.8.1,Load default opts values then overwrite it with opts from
0.8.1,the checkpoint. It's usefull in order to re-train a model
0.8.1,after adding a new option (not set in checkpoint)
0.8.1,check for code where vocab is saved instead of fields
0.8.1,(in the future this will be done in a smarter way)
0.8.1,"Report src and tgt vocab sizes, including for features"
0.8.1,Build model.
0.8.1,Build optimizer.
0.8.1,Build model saver
0.8.1,this line is kind of a temporary kludge because different objects expect
0.8.1,fields to have a different structure
0.8.1,Embedding Options
0.8.1,Encoder-Decoder Options
0.8.1,"group.add('--residual', '-residual',   action=""store_true"","
0.8.1,"help=""Add residual connections between RNN layers."")"
0.8.1,Attention options
0.8.1,Generator and loss options.
0.8.1,Data options
0.8.1,"Dictionary options, for text corpus"
0.8.1,"Truncation options, for text corpus"
0.8.1,Data processing options
0.8.1,Options most relevant to speech
0.8.1,Option most relevant to image input
0.8.1,GPU
0.8.1,Init options
0.8.1,Pretrained word vectors
0.8.1,Fixed word vectors
0.8.1,Optimization options
0.8.1,learning rate
0.8.1,Use TensorboardX for visualization during training
0.8.1,Options most relevant to speech
0.8.1,Option most relevant to image input
0.8.1,Options most relevant to summarization.
0.8.1,Alpha and Beta values for Google Length + Coverage penalty
0.8.1,"Described here: https://arxiv.org/pdf/1609.08144.pdf, Section 7"
0.8.1,Options most relevant to speech.
0.8.1,Option most relevant to image input
0.8.1,MARKDOWN boilerplate
0.8.1,Copyright 2016 The Chromium Authors. All rights reserved.
0.8.1,Use of this source code is governed by a BSD-style license that can be
0.8.1,found in the LICENSE file.
0.8.1,**section heading**:
0.8.1,# **--argument-one**
0.8.1,"Get the key 'value' in the dict, or just use 'value'"
0.8.1,Basic attributes.
0.8.1,Set model in training mode.
0.8.1,Set model in validating mode.
0.8.1,F-prop through the model.
0.8.1,Compute loss.
0.8.1,Update statistics.
0.8.1,Set model back to training mode.
0.8.1,Truncated BPTT: reminder not compatible with accum > 1
0.8.1,1. Create truncated target.
0.8.1,2. F-prop all but generator.
0.8.1,3. Compute loss.
0.8.1,4. Update the parameters and statistics.
0.8.1,Multi GPU gradient gather
0.8.1,"If truncated, don't backprop fully."
0.8.1,TO CHECK
0.8.1,if dec_state is not None:
0.8.1,dec_state.detach()
0.8.1,"in case of multi step gradient accumulation,"
0.8.1,update only after accum batches
0.8.1,For Flake
0.8.1,Initialize the bridge layer
0.8.1,"s_len, batch, emb_dim = emb.size()"
0.8.1,Lengths data is wrapped inside a Tensor.
0.8.1,"LSTM has hidden and cell state, other only one"
0.8.1,Total number of states
0.8.1,Build a linear layer for each
0.8.1,The encoder hidden is  (layers*directions) x batch x dim.
0.8.1,"s_len, batch, emb_dim = emb.size()"
0.8.1,Run the forward pass of every layer of the tranformer.
0.8.1,why is the model_opt.__dict__ check necessary?
0.8.1,"(batch_size, 64, imgH, imgW)"
0.8.1,layer 1
0.8.1,"(batch_size, 64, imgH/2, imgW/2)"
0.8.1,"(batch_size, 128, imgH/2, imgW/2)"
0.8.1,layer 2
0.8.1,"(batch_size, 128, imgH/2/2, imgW/2/2)"
0.8.1,"(batch_size, 256, imgH/2/2, imgW/2/2)"
0.8.1,layer 3
0.8.1,batch norm 1
0.8.1,"(batch_size, 256, imgH/2/2, imgW/2/2)"
0.8.1,layer4
0.8.1,"(batch_size, 256, imgH/2/2/2, imgW/2/2)"
0.8.1,"(batch_size, 512, imgH/2/2/2, imgW/2/2)"
0.8.1,layer 5
0.8.1,batch norm 2
0.8.1,"(batch_size, 512, imgH/2/2/2, imgW/2/2/2)"
0.8.1,"(batch_size, 512, imgH/2/2/2, imgW/2/2/2)"
0.8.1,"# (batch_size, 512, H, W)"
0.8.1,Dimensions and padding for constructing the word embedding matrix
0.8.1,Dimensions and padding for feature embedding matrices
0.8.1,(these have no effect if feat_vocab_sizes is empty)
0.8.1,The embedding matrix look-up tables. The first look-up table
0.8.1,"is for words. Subsequent ones are for features, if any exist."
0.8.1,The final output size of word + feature vectors. This can vary
0.8.1,from the word vector size if and only if features are defined.
0.8.1,This is the attribute you should access if you need to know
0.8.1,how big your embeddings are going to be.
0.8.1,The sequence of operations that converts the input sequence
0.8.1,into a sequence of embeddings. At minimum this consists of
0.8.1,looking up the embeddings for each word and feature in the
0.8.1,input. Model parameters may require the sequence to contain
0.8.1,additional operations as well.
0.8.1,features must use word_vec_size
0.8.1,features will use feat_vec_size
0.8.1,This class is mainly used by decoder.py for RNNs but also
0.8.1,by the CNN / transformer decoder when copy attention is used
0.8.1,CNN has its own attention mechanism ConvMultiStepAttention
0.8.1,Transformer has its own MultiHeadedAttention
0.8.1,mlp wants it with bias
0.8.1,Check input sizes
0.8.1,"(batch, t_len, d) x (batch, d, s_len) --> (batch, t_len, s_len)"
0.8.1,"(batch, t_len, s_len, d)"
0.8.1,one step input
0.8.1,"compute attention scores, as in Luong et al."
0.8.1,Softmax or sparsemax to normalize attention weights
0.8.1,each context vector c_t is the weighted average
0.8.1,over all the source hidden states
0.8.1,concatenate
0.8.1,Check output sizes
0.8.1,Check output sizes
0.8.1,clamping necessary because of numerical errors: loss should be lower
0.8.1,"bounded by zero, but negative values near zero are possible without"
0.8.1,the clamp
0.8.1,from onmt.utils.misc import aeq
0.8.1,CHECKS
0.8.1,"batch, k_len, d = key.size()"
0.8.1,"batch_, k_len_, d_ = value.size()"
0.8.1,"aeq(batch, batch_)"
0.8.1,"aeq(k_len, k_len_)"
0.8.1,"aeq(d, d_)"
0.8.1,"batch_, q_len, d_ = query.size()"
0.8.1,"aeq(batch, batch_)"
0.8.1,"aeq(d, d_)"
0.8.1,"aeq(self.model_dim % 8, 0)"
0.8.1,if mask is not None:
0.8.1,"batch_, q_len_, k_len_ = mask.size()"
0.8.1,"aeq(batch_, batch)"
0.8.1,"aeq(k_len_, k_len)"
0.8.1,aeq(q_len_ == q_len)
0.8.1,END CHECKS
0.8.1,"1) Project key, value, and query."
0.8.1,1 or key_len x key_len
0.8.1,1 or key_len x key_len x dim_per_head
0.8.1,1 or key_len x key_len x dim_per_head
0.8.1,2) Calculate and scale scores.
0.8.1,batch x num_heads x query_len x key_len
0.8.1,3) Apply attention dropout and compute context vectors.
0.8.1,CHECK
0.8.1,"batch_, q_len_, d_ = output.size()"
0.8.1,"aeq(q_len, q_len_)"
0.8.1,"aeq(batch, batch_)"
0.8.1,"aeq(d, d_)"
0.8.1,Return one attn
0.8.1,At the moment this class is only used by embeddings.Embeddings look-up tables
0.8.1,-*- coding: utf-8 -*-
0.8.1,checks
0.8.1,"batch, channel, height, width = base_target_emb.size()"
0.8.1,"batch_, channel_, height_, width_ = input_from_dec.size()"
0.8.1,"enc_batch, enc_channel, enc_height = encoder_out_top.size()"
0.8.1,"enc_batch_, enc_channel_, enc_height_ = encoder_out_combine.size()"
0.8.1,out_features * in_features
0.8.1,norm is out_features * 1
0.8.1,batch_size * out_features
0.8.1,out_features
0.8.1,out_features
0.8.1,batch_size * out_features
0.8.1,"out_channels, in_channels // groups, * kernel_size"
0.8.1,out_features
0.8.1,This is used nowhere in the code at the moment (Vincent Nguyen 05/18/2018)
0.8.1,"in_channels, out_channels, *kernel_size"
0.8.1,"in_channels, out_channels, *kernel_size"
0.8.1,"self.out_channels, 1"
0.8.1,out_features
0.8.1,out_features
0.8.1,store roots on diagonal
0.8.1,CHECKS
0.8.1,Original probabilities.
0.8.1,Probability of copying p(z=1) batch.
0.8.1,Probability of not copying: p_{word}(w) * (1 - p(z))
0.8.1,probabilities assigned by the model to the gold targets
0.8.1,probability of tokens copied from source
0.8.1,Set scores for unk to 0 and add eps
0.8.1,find the indices in which you do not use the copy mechanism
0.8.1,Drop padding.
0.8.1,this block does not depend on the loss value computed above
0.8.1,and is used only for stats
0.8.1,this block does not depend on the loss value computed above
0.8.1,and is used only for stats
0.8.1,Correct target copy token instead of <unk>
0.8.1,tgt[i] = align[i] + len(tgt_vocab)
0.8.1,for i such that tgt[i] == 0 and align[i] != 0
0.8.1,Compute sum of perplexities for stats
0.8.1,this part looks like it belongs in CopyGeneratorLoss
0.8.1,Compute Loss as NLL divided by seq length
0.8.1,Compute Total Loss per sequence in batch
0.8.1,Divide by length of each sequence and sum
0.8.1,all beams repeat (beam >= 1 repeat dummy scores)
0.8.1,predict repeat_idx over and over again
0.8.1,beam 0 and beam >=2 will repeat (beam >= 2 repeat dummy scores)
0.8.1,non-interesting beams are going to get dummy values
0.8.1,"on initial round, only predicted scores for beam 0"
0.8.1,matter. Make two predictions. Top one will be repeated
0.8.1,"in beam zero, second one will live on in beam 1."
0.8.1,predict the same thing in beam 0
0.8.1,continue pushing around what beam 1 predicts
0.8.1,"now beam 0 dies (along with the others), beam 1 -> beam 0"
0.8.1,beam 0 and beam >= 2 will repeat (beam 2 repeats excluded idx)
0.8.1,non-interesting beams are going to get dummy values
0.8.1,predict the same thing in beam 0
0.8.1,continue pushing around what beam 1 predicts
0.8.1,predict the allowed-repeat again in beam 2
0.8.1,"now beam 0 dies, beam 1 -> beam 0, beam 2 -> beam 1"
0.8.1,and the rest die
0.8.1,"since all preds after i=0 are 0, we can check"
0.8.1,that the beam is the correct idx by checking that
0.8.1,the curr score is the initial score
0.8.1,beam 0 will always predict EOS. The other beams will predict
0.8.1,non-eos scores.
0.8.1,"this is also a test that when block_ngram_repeat=0,"
0.8.1,repeating is acceptable
0.8.1,non-interesting beams are going to get dummy values
0.8.1,"""best"" prediction is eos - that should be blocked"
0.8.1,include at least beam_sz predictions OTHER than EOS
0.8.1,that are greater than -1e20
0.8.1,predict eos in beam 0
0.8.1,provide beam_sz other good predictions
0.8.1,now the top beam has ended and no others have
0.8.1,first beam finished had length beam.min_length
0.8.1,first beam finished was 0
0.8.1,"not of interest, but want to make sure it keeps running"
0.8.1,since only beam 0 terminates and n_best = 2
0.8.1,"this is also a test that when block_ngram_repeat=0,"
0.8.1,repeating is acceptable
0.8.1,non-interesting beams are going to get dummy values
0.8.1,"""best"" prediction is eos - that should be blocked"
0.8.1,include at least beam_sz predictions OTHER than EOS
0.8.1,that are greater than -1e20
0.8.1,predict eos in beam 1
0.8.1,provide beam_sz other good predictions in other beams
0.8.1,provide beam_sz other good predictions in other beams
0.8.1,beam 1 dies on min_length
0.8.1,beam 0 dies on the step after beam 1 dies
0.8.1,"init_preds: [4, 3, 5, 6, 7] - no EOS's"
0.8.1,no EOS's yet
0.8.1,"[5, 3, 2, 6, 0], so beam 2 predicts EOS!"
0.8.1,assumes beam 2 finished on last step
0.8.1,"[2, 5, 3, 6, 0], so beam 0 predicts EOS!"
0.8.1,"[-2.4879, -3.8910, -4.1010, -4.2010, -4.4010]"
0.8.1,new beam 0 finished
0.8.1,new beam 0 is old beam 3
0.8.1,assumes beam 0 finished on last step
0.8.1,"[5, 2, 6, 1, 0], so beam 1 predicts EOS!"
0.8.1,new beam 1 finished
0.8.1,new beam 1 is old beam 4
0.8.1,this could be considered an integration test because it tests
0.8.1,interactions between the GNMT scorer and the beam
0.8.1,initialize fields at the top of each unit test to prevent
0.8.1,any undesired stateful effects
0.8.1,"this test touches the file system, so it could be considered an"
0.8.1,integration test
0.8.1,write utf-8 bytes
0.8.1,predict repeat_idx over and over again
0.8.1,"batch 0 and 7 will repeat, the rest will advance"
0.8.1,predict the same thing in batch 0 and 7 every i
0.8.1,push around what the other batches predict
0.8.1,now batch 0 and 7 die
0.8.1,"batch 0 will repeat excluded idx, batch 1 will repeat"
0.8.1,now batch 1 dies
0.8.1,batch 0 will always predict EOS. The other batches will predict
0.8.1,non-eos scores.
0.8.1,"""best"" prediction is eos - that should be blocked"
0.8.1,include at least one prediction OTHER than EOS
0.8.1,that is greater than -1e20
0.8.1,now batch 0 has ended and no others have
0.8.1,initial step
0.8.1,batch 0 dies on step 0
0.8.1,include at least one prediction OTHER than EOS
0.8.1,that is greater than -1e20
0.8.1,step 2
0.8.1,(old) batch 8 dies on step 1
0.8.1,step 3
0.8.1,everything dies
0.8.1,initial step
0.8.1,batch 0 dies on step 0
0.8.1,include at least one prediction OTHER than EOS
0.8.1,that is greater than -1e20
0.8.1,step 2
0.8.1,(old) batch 8 dies on step 1
0.8.1,step 3
0.8.1,everything dies
0.8.1,illegal_weights_mask = torch.ByteTensor([
0.8.1,"[0, 0, 0, 0, 0, 0, 0],"
0.8.1,"[0, 0, 0, 1, 1, 1, 1],"
0.8.1,"[0, 0, 0, 0, 0, 1, 1],"
0.8.1,"[0, 0, 1, 1, 1, 1, 1]])"
0.8.1,TODO: fix for pytorch 0.3
0.8.1,illegal_weights = alignments.masked_select(illegal_weights_mask)
0.8.1,"self.assertEqual(0.0, illegal_weights.data.sum())"
0.8.1,-*- coding: utf-8 -*-
0.8.1,tests pad and numericalize integration
0.8.1,tests pad and numericalize integration
0.8.1,"this test touches the file system, so it could be considered an"
0.8.1,integration test
0.8.1,file to hold full paths to audio data
0.8.1,file to hold audio paths relative to _AUDIO_DATA_DIR (i.e. file names)
0.8.1,it's ok if non-audio files co-exist with audio files in the data dir
0.8.1,"dividing gets the noise in [-1, 1]"
0.8.1,"this test touches the file system, so it could be considered an"
0.8.1,integration test
0.8.1,file to hold full paths to image data
0.8.1,file to hold image paths relative to _IMG_DATA_DIR (i.e. file names)
0.8.1,it's ok if non-image files co-exist with image files in the data dir
0.8.1,all beams repeat (beam >= 1 repeat dummy scores)
0.8.1,predict repeat_idx over and over again
0.8.1,beam 0 and beam >=2 will repeat (beam >= 2 repeat dummy scores)
0.8.1,non-interesting beams are going to get dummy values
0.8.1,"on initial round, only predicted scores for beam 0"
0.8.1,matter. Make two predictions. Top one will be repeated
0.8.1,"in beam zero, second one will live on in beam 1."
0.8.1,predict the same thing in beam 0
0.8.1,continue pushing around what beam 1 predicts
0.8.1,"now beam 0 dies (along with the others), beam 1 -> beam 0"
0.8.1,beam 0 and beam >= 2 will repeat (beam 2 repeats excluded idx)
0.8.1,non-interesting beams are going to get dummy values
0.8.1,predict the same thing in beam 0
0.8.1,continue pushing around what beam 1 predicts
0.8.1,predict the allowed-repeat again in beam 2
0.8.1,"now beam 0 dies, beam 1 -> beam 0, beam 2 -> beam 1"
0.8.1,and the rest die
0.8.1,"since all preds after i=0 are 0, we can check"
0.8.1,that the beam is the correct idx by checking that
0.8.1,the curr score is the initial score
0.8.1,beam 0 will always predict EOS. The other beams will predict
0.8.1,non-eos scores.
0.8.1,non-interesting beams are going to get dummy values
0.8.1,"""best"" prediction is eos - that should be blocked"
0.8.1,include at least beam_sz predictions OTHER than EOS
0.8.1,that are greater than -1e20
0.8.1,predict eos in beam 0
0.8.1,provide beam_sz other good predictions
0.8.1,now the top beam has ended and no others have
0.8.1,"not of interest, but want to make sure it keeps running"
0.8.1,since only beam 0 terminates and n_best = 2
0.8.1,"this is also a test that when block_ngram_repeat=0,"
0.8.1,repeating is acceptable
0.8.1,non-interesting beams are going to get dummy values
0.8.1,"""best"" prediction is eos - that should be blocked"
0.8.1,include at least beam_sz predictions OTHER than EOS
0.8.1,that are greater than -1e20
0.8.1,predict eos in beam 1
0.8.1,provide beam_sz other good predictions in other beams
0.8.1,provide beam_sz other good predictions in other beams
0.8.1,beam 1 dies on min_length
0.8.1,beam 0 dies on the step after beam 1 dies
0.8.1,non-interesting beams are going to get dummy values
0.8.1,"""best"" prediction is eos - that should be blocked"
0.8.1,include at least beam_sz predictions OTHER than EOS
0.8.1,that are greater than -1e20
0.8.1,predict eos in beam 1
0.8.1,provide beam_sz other good predictions in other beams
0.8.1,provide beam_sz other good predictions in other beams
0.8.1,no top beams are finished yet
0.8.1,beam 1 dies on min_length
0.8.1,no top beams are finished yet
0.8.1,beam 0 dies on the step after beam 1 dies
0.8.1,top beam is finished now so there are attentions
0.8.1,two beams are finished in each batch
0.8.1,second dim is cut down to the non-padded src length
0.8.1,first dim is equal to the time of death
0.8.1,(beam 0 died at current step - adjust for SOS)
0.8.1,(beam 1 died at last step - adjust for SOS)
0.8.1,behavior gets weird when beam is already done so just stop
0.8.1,this is just test_beam.TestBeamAgainstReferenceCase repeated
0.8.1,in each batch.
0.8.1,"init_preds: [4, 3, 5, 6, 7] - no EOS's"
0.8.1,no EOS's yet
0.8.1,"[5, 3, 2, 6, 0], so beam 2 predicts EOS!"
0.8.1,assumes beam 2 finished on last step
0.8.1,ended beam 2 shouldn't continue
0.8.1,"[2, 5, 3, 6, 0] repeat self.BATCH_SZ, so beam 0 predicts EOS!"
0.8.1,"[-2.4879, -3.8910, -4.1010, -4.2010, -4.4010] repeat self.BATCH_SZ"
0.8.1,another beam is finished in all batches
0.8.1,new beam 0 finished
0.8.1,new beam 0 is old beam 3
0.8.1,assumes beam 0 finished on last step
0.8.1,"[5, 2, 6, 1, 0] repeat self.BATCH_SZ, so beam 1 predicts EOS!"
0.8.1,new beam 1 finished
0.8.1,new beam 1 is old beam 4
0.8.1,this could be considered an integration test because it tests
0.8.1,interactions between the GNMT scorer and the beam
0.8.1,"-data option is required, but not used in this test, so dummy."
0.8.1,len x batch x nfeat
0.8.1,batch x c x h x w
0.8.1,batch x 1 x nfft x t
0.8.1,Initialize vectors to compare size with
0.8.1,Ensure correct sizes and types
0.8.1,Make sure that output has the correct size and type
0.8.1,Make sure that output has the correct size and type
0.8.1,Make sure that output has the correct size and type
0.8.1,"[('encoder_type', 'transformer'),"
0.8.1,"('word_vec_size', 16), ('rnn_size', 16)],"
0.8.1,""""""" Only do SRU test if requirment is safisfied. """""""
0.8.1,SRU doesn't support input_feed.
0.8.1,first check there's nothing unexpectedly not trainable
0.8.1,ok: word embeddings shouldn't be trainable
0.8.1,if word vecs are fixed
0.8.1,ok: positional encodings shouldn't be trainable
0.8.1,then check nothing unexpectedly trainable
0.8.1,!/usr/bin/env python
0.8.1,-*- coding: utf-8 -*-
0.8.1,Remove the generated *pt files.
0.8.1,Test image preprocessing
0.8.1,Test audio preprocessing
0.8.1,Decoder state
0.8.1,Build the RNN.
0.8.1,Set up the context gate.
0.8.1,Set up the standard attention.
0.8.1,The encoder hidden is  (layers*directions) x batch x dim.
0.8.1,We need to convert it to layers x batch x (directions*dim).
0.8.1,Init the input feed.
0.8.1,Update the state with the result.
0.8.1,Concatenates sequence of tensors along a new dimension.
0.8.1,NOTE: v0.3 to 0.4: dec_outs / attns[*] may not be list
0.8.1,(in particular in case of SRU) it was not raising error in 0.3
0.8.1,since stack(Variable) was allowed.
0.8.1,"In 0.4, SRU returns a tensor that shouldn't be stacke"
0.8.1,TODO change the way attns is returned dict => list or tuple (onnx)
0.8.1,Check
0.8.1,Calculate the attention.
0.8.1,Calculate the context gate.
0.8.1,Additional args check.
0.8.1,END Additional args check.
0.8.1,Input feed concatenates hidden state with
0.8.1,input at every time step.
0.8.1,TODO: context gate should be employed
0.8.1,instead of second RNN transform.
0.8.1,Update the coverage attention.
0.8.1,Decoder State
0.8.1,CNNDecoder has its own attention mechanism.
0.8.1,Set up a separate copy attention layer if needed.
0.8.1,The output of CNNEncoder.
0.8.1,The combination of output of CNNEncoder and source embeddings.
0.8.1,Process the result and update the attentions.
0.8.1,Update the state.
0.8.1,TODO change the way attns is returned dict => list or tuple (onnx)
0.8.1,Memory_lengths is a single tensor shared between all models.
0.8.1,This assumption will not hold if Translator is modified
0.8.1,to calculate memory_lengths as something other than the length
0.8.1,of the input.
0.8.1,Decoder State
0.8.1,"previously, there was a GlobalAttention module here for copy"
0.8.1,"attention. But it was never actually used -- the ""copy"" attention"
0.8.1,just reuses the context attention.
0.8.1,TODO change the way attns is returned dict => list or tuple (onnx)
0.8.1,"buffer size in bytes, determine equiv. # of elements based on data type"
0.8.1,copy tensors into buffer_t
0.8.1,all-reduce and rescale
0.8.1,copy all-reduced buffer back into tensors
0.8.1,"tensor is bigger than buffer, all-reduce and rescale directly"
0.8.1,"buffer is full, all-reduce and replace buffer with grad"
0.8.1,add tensor to buffer
0.8.1,TODO: Find a better way to check for sparse gradients.
0.8.1,TODO: clean this up when APEX unify its optimizer API.
0.8.1,Load everything from the checkpoint.
0.8.1,Build everything from scratch.
0.8.1,"Reset optimizer, keep options."
0.8.1,"Reset options, keep optimizer."
0.8.1,State can be partially restored.
0.8.1,Code below is an implementation of https://arxiv.org/pdf/1804.04235.pdf
0.8.1,inspired but modified from https://github.com/DeadAt0m/adafactor-pytorch
0.8.1,-*- coding: utf-8 -*-
0.8.1,if the loss function operates on vectors of raw logits instead of
0.8.1,"probabilities, only the first part of the generator needs to be"
0.8.1,"passed to the NMTLossCompute. At the moment, the only supported"
0.8.1,loss function of this kind is the sparsemax loss.
0.8.1,non_none: the subdict of the state dictionary where the values
0.8.1,are not None.
0.8.1,"Now, the iteration:"
0.8.1,state is a dictionary of sequences of tensor-like but we
0.8.1,want a sequence of dictionaries of tensors.
0.8.1,"First, unzip the dictionary into a sequence of keys and a"
0.8.1,sequence of tensor-like sequences.
0.8.1,"Now, yield a dictionary for each shard. The keys are always"
0.8.1,the same. values is a sequence of length #keys where each
0.8.1,element is a sequence of length #shards. We want to iterate
0.8.1,"over the shards, not over the keys: therefore, the values need"
0.8.1,to be re-zipped by shard and then each shard can be paired
0.8.1,with the keys.
0.8.1,Assumed backprop'd
0.8.1,Log the progress using the number of batches on the x-axis.
0.8.1,Get a list of world_size lists with len(stat_list) Statistics objects
0.8.1,SRU doesn't support PackedSequence.
0.8.1,-*- coding: utf-8 -*-
0.8.1,this one is needed for torchtext random call (shuffled iterator)
0.8.1,in multi gpu it ensures datasets are read in the same order
0.8.1,some cudnn methods can be random even after fixing the seed
0.8.1,unless you tell it to be deterministic
0.8.1,These ensure same initialization in multi gpu mode
0.8.1,Shift values to be >= 0
0.8.1,coding: utf-8
0.8.1,make a small vocab containing just the tokens in the source sequence
0.8.1,Map source tokens to indices in the dynamic dict.
0.8.1,self.src_vocabs is used in collapse_copy_scores and Translator.py
0.8.1,this assumes src_field and tgt_field are both text
0.8.1,the dataset's self.fields should have the same attributes as examples
0.8.1,avoid infinite recursion when fields isn't defined
0.8.1,-*- coding: utf-8 -*-
0.8.1,backwards compatibility
0.8.1,monkey-patch to make torchtext Vocab's pickleable
0.8.1,"if tgt isn't using TextMultiField, then no text field is."
0.8.1,this is basically copy-pasted from torchtext.
0.8.1,counters changes in place
0.8.1,keep the order of tokens specified in the vocab file by
0.8.1,adding them to the counter with decreasing counting values
0.8.1,Load vocabulary
0.8.1,Drop the none-using from memory but keep the last
0.8.1,`tgt_vocab_size` is ignored when sharing vocabularies
0.8.1,"in the long run, shouldn't it be possible to do this by calling"
0.8.1,build_vocab with both the src and tgt data?
0.8.1,Cycle through the shards indefinitely.
0.8.1,"When the dataset is not repeated, we might need to ensure that"
0.8.1,the number of returned batches is the multiple of a given value.
0.8.1,This is important for multi GPU training to ensure that all
0.8.1,workers have the same number of batches to process.
0.8.1,Maintains the longest src and tgt length in the current batch
0.8.1,Reset current longest length at a new batch (count=1)
0.8.1,Src: [<bos> w1 ... wN <eos>]
0.8.1,Tgt: [w1 ... wM <eos>]
0.8.1,-*- coding: utf-8 -*-
0.8.1,imports of datatype-specific dependencies
0.8.1,torchaudio loading options recently changed. It's probably
0.8.1,straightforward to rewrite the audio handling to make use of
0.8.1,"up-to-date torchaudio, but in the meantime there is a legacy"
0.8.1,method which uses the old defaults
0.8.1,STFT
0.8.1,-*- coding: utf-8 -*-
0.8.1,domain specific dependencies
0.8.1,coding: utf-8
0.8.1,several data readers need optional dependencies. There's no
0.8.1,appropriate builtin exception
0.8.1,-*- coding: utf-8 -*-
0.8.1,mix this with partial
0.8.1,batch (list(list(list))): batch_size x len(self.fields) x seq_len
0.8.1,lengths: batch_size
0.8.1,data: seq_len x batch_size x len(self.fields)
0.8.1,flake8: noqa
0.8.1,For command-line option parsing
0.8.1,"Check pass, set the args."
0.8.1,"This SRU version implements its own cuda-level optimization,"
0.8.1,so it requires that:
0.8.1,1. `cupy` and `pynvrtc` python package installed.
0.8.1,2. pytorch is built with cuda support.
0.8.1,3. library path set: export LD_LIBRARY_PATH=<cuda lib path>.
0.8.1,Check 1.
0.8.1,Check 2.
0.8.1,Check 3.
0.8.1,This sets up device to use.
0.8.1,-> directions x batch x dim
0.8.1,For DEBUG
0.8.1,"size = (length, batch, x.size(-1)) \"
0.8.1,"if x.dim() == 3 else (batch, x.size(-1))"
0.8.1,grad_x = x.new(*x.size()) if k_ == 3 else x.new(*size).zero_()
0.8.1,Normal use
0.8.1,"An entry check here, will catch on train side and translate side"
0.8.1,if requirements are not satisfied.
0.8.1,RNNDecoderState wraps hidden as a tuple.
0.8.1,fh -> (layers*directions) x batch x dim
0.8.1,The score for each translation on the beam.
0.8.1,The backpointers at each time-step.
0.8.1,The outputs at each time-step.
0.8.1,Has EOS topped the beam yet.
0.8.1,The attentions (matrix) for each time.
0.8.1,Time and k pair for finished.
0.8.1,Information for global scoring.
0.8.1,Minimum prediction length
0.8.1,Apply Penalty at every step
0.8.1,force the output to be longer than self.min_length
0.8.1,assumes there are len(word_probs) predictions OTHER
0.8.1,than EOS that are greater than -1e20
0.8.1,Sum the previous scores.
0.8.1,Don't let EOS have children.
0.8.1,Block ngram repeats
0.8.1,"Last n tokens, n = block_ngram_repeat"
0.8.1,Skip the blocking if it is in the exclusion list
0.8.1,"best_scores_id is flattened beam x word array, so calculate which"
0.8.1,word and beam each score came from
0.8.1,End condition is when top-of-beam is EOS and no global score.
0.8.1,Add from beam until we have minimum outputs.
0.8.1,Term will be subtracted from probability
0.8.1,Probability will be divided by this
0.8.1,these warnings indicate that either the alpha/beta
0.8.1,"forces a penalty to be a no-op, or a penalty is a no-op but"
0.8.1,the alpha/beta would suggest otherwise.
0.8.1,using some length penalty
0.8.1,using some coverage penalty
0.8.1,"For temp=0.0, take the argmax to avoid divide-by-zero errors."
0.8.1,keep_topk=1 is also equivalent to argmax.
0.8.1,Set all logits that are not in the top-k to -10000.
0.8.1,This puts the probabilities close to 0.
0.8.1,"shape: (sum(~ self.is_finished), 1)"
0.8.1,magic indices
0.8.1,result caching
0.8.1,add one to account for BOS. Don't account for EOS because hitting
0.8.1,this implies it hasn't been found.
0.8.1,skip BOS
0.8.1,"Last n tokens, n = block_ngram_repeat"
0.8.1,skip the blocking if any token in gram is excluded
0.8.1,!/usr/bin/env python
0.8.1,for debugging
0.8.1,Statistics
0.8.1,TODO: support these blacklisted features.
0.8.1,Encoder forward.
0.8.1,"Shape: (1, B, 1)"
0.8.1,Reorder states.
0.8.1,Turn any copied words into UNKs.
0.8.1,"Decoder forward, takes [tgt_len, batch, nfeats] as input"
0.8.1,"and [src_len, batch, hidden] as memory_bank"
0.8.1,"in case of inference tgt_len = 1, batch = beam times batch_size"
0.8.1,"in case of Gold Scoring tgt_len = actual length, batch = 1 batch"
0.8.1,Generator forward.
0.8.1,"returns [(batch_size x beam_size) , vocab ] when 1 step"
0.8.1,"or [ tgt_len, batch_size, vocab ] when full sentence"
0.8.1,"here we have scores [tgt_lenxbatch, vocab] or [beamxbatch, vocab]"
0.8.1,"returns [(batch_size x beam_size) , vocab ] when 1 step"
0.8.1,"or [ tgt_len, batch_size, vocab ] when full sentence"
0.8.1,TODO: support these blacklisted features.
0.8.1,(0) Prep the components of the search.
0.8.1,(1) Run the encoder on the src.
0.8.1,(2) Repeat src objects `beam_size` times.
0.8.1,We use batch_size x beam_size
0.8.1,"(0) pt 2, prep the beam object"
0.8.1,Reorder states.
0.8.1,"This is left in the code for now, but unsued"
0.8.1,(0) Prep each of the components of the search.
0.8.1,And helper method for reducing verbosity.
0.8.1,(1) Run the encoder on the src.
0.8.1,(2) Repeat src objects `beam_size` times.
0.8.1,We use now  batch_size x beam_size (same as fast mode)
0.8.1,"(3) run the decoder to generate sentences, using beam search."
0.8.1,(a) Construct batch x beam_size nxt words.
0.8.1,Get all the pending current beam words and arrange for forward.
0.8.1,(b) Decode and forward
0.8.1,(c) Advance each beam.
0.8.1,Loop over the batch_size number of beam
0.8.1,(4) Extract sentences from beam.
0.8.1,Rollback pointer to the beginning.
0.8.1,beam parameters
0.8.1,result caching
0.8.1,beam state
0.8.1,buffers for the topk scores and 'backpointer'
0.8.1,"""global state"" of the old beam"
0.8.1,for testing
0.8.1,using integer division to get an integer _B without casting
0.8.1,force the output to be longer than self.min_length
0.8.1,Multiply probs by the beam probability.
0.8.1,"if the sequence ends now, then the penalty is the current"
0.8.1,"length + 1, to include the EOS token"
0.8.1,Flatten probs into a list of possibilities.
0.8.1,Recover log probs.
0.8.1,Length penalty is just a scalar. It doesn't matter if it's applied
0.8.1,before or after the topk.
0.8.1,Resolve beam origin and map to batch index flat representation.
0.8.1,Append last prediction.
0.8.1,update global state (step == 1)
0.8.1,update global state (step > 1)
0.8.1,"shape: (batch_size x beam_size, 1)"
0.8.1,Penalize beams that finished.
0.8.1,"on real data (newstest2017) with the pretrained transformer,"
0.8.1,it's faster to not move this back to the original device
0.8.1,Store finished hypotheses for this batch.
0.8.1,End condition is the top beam finished and we can return
0.8.1,n_best hypotheses.
0.8.1,"If all sentences are translated, no need to go further."
0.8.1,Remove finished batches for the next step.
0.8.1,!/usr/bin/env python
0.8.1,backwards compatibility for confs
0.8.1,load can be called multiple times: modify copy
0.8.1,NOTE: translator returns lists of `n_best` list
0.8.1,we can ignore that (i.e. flatten lists) only because
0.8.1,we restrict `n_best=1`
0.8.1,build back results with empty texts
0.8.1,Below are all the different penalty terms implemented so far.
0.8.1,Subtract coverage penalty from topk log probs.
0.8.1,Divide topk log probs by length penalty.
0.8.1,Sorting
0.8.0,!/usr/bin/env python
0.8.0,!/usr/bin/env python
0.8.0,!/usr/bin/env python
0.8.0,-*- coding: utf-8 -*-
0.8.0,!/usr/bin/env python
0.8.0,-*- coding: utf-8 -*-
0.8.0,!/usr/bin/env python
0.8.0,Create a thread to listen for errors in the child processes.
0.8.0,Train with multiprocessing.
0.8.0,"propagate exception to parent process, keeping original traceback"
0.8.0,!/usr/bin/env python3
0.8.0,-*- coding: utf-8 -*-
0.8.0,
0.8.0,"OpenNMT-py documentation build configuration file, created by"
0.8.0,sphinx-quickstart on Sun Dec 17 12:07:14 2017.
0.8.0,
0.8.0,This file is execfile()d with the current directory set to its
0.8.0,containing dir.
0.8.0,
0.8.0,Note that not all possible configuration values are present in this
0.8.0,autogenerated file.
0.8.0,
0.8.0,All configuration values have a default; values that are commented out
0.8.0,serve to show the default.
0.8.0,"If extensions (or modules to document with autodoc) are in another directory,"
0.8.0,add these directories to sys.path here. If the directory is relative to the
0.8.0,"documentation root, use os.path.abspath to make it absolute, like shown here."
0.8.0,
0.8.0,import os
0.8.0,import sys
0.8.0,"sys.path.insert(0, os.path.abspath('.'))"
0.8.0,-- General configuration ------------------------------------------------
0.8.0,"If your documentation needs a minimal Sphinx version, state it here."
0.8.0,
0.8.0,needs_sphinx = '1.0'
0.8.0,"Add any Sphinx extension module names here, as strings. They can be"
0.8.0,extensions coming with Sphinx (named 'sphinx.ext.*') or your custom
0.8.0,ones.
0.8.0,"Add any paths that contain templates here, relative to this directory."
0.8.0,The suffix(es) of source filenames.
0.8.0,You can specify multiple suffix as a list of string:
0.8.0,
0.8.0,"source_suffix = ['.rst', '.md']"
0.8.0,The master toctree document.
0.8.0,General information about the project.
0.8.0,"The version info for the project you're documenting, acts as replacement for"
0.8.0,"|version| and |release|, also used in various other places throughout the"
0.8.0,built documents.
0.8.0,
0.8.0,The short X.Y version.
0.8.0,"The full version, including alpha/beta/rc tags."
0.8.0,The language for content autogenerated by Sphinx. Refer to documentation
0.8.0,for a list of supported languages.
0.8.0,
0.8.0,This is also used if you do content translation via gettext catalogs.
0.8.0,"Usually you set ""language"" from the command line for these cases."
0.8.0,"List of patterns, relative to source directory, that match files and"
0.8.0,directories to ignore when looking for source files.
0.8.0,This patterns also effect to html_static_path and html_extra_path
0.8.0,The name of the Pygments (syntax highlighting) style to use.
0.8.0,"If true, `todo` and `todoList` produce output, else they produce nothing."
0.8.0,-- Options for HTML output ----------------------------------------------
0.8.0,The theme to use for HTML and HTML Help pages.  See the documentation for
0.8.0,a list of builtin themes.
0.8.0,
0.8.0,html_theme = 'sphinx_materialdesign_theme'
0.8.0,html_theme_path = [sphinx_materialdesign_theme.get_path()]
0.8.0,Theme options are theme-specific and customize the look and feel of a theme
0.8.0,"further.  For a list of options available for each theme, see the"
0.8.0,documentation.
0.8.0,
0.8.0,html_theme_options = {}
0.8.0,"Add any paths that contain custom static files (such as style sheets) here,"
0.8.0,"relative to this directory. They are copied after the builtin static files,"
0.8.0,"so a file named ""default.css"" will overwrite the builtin ""default.css""."
0.8.0,"Custom sidebar templates, must be a dictionary that maps document names"
0.8.0,to template names.
0.8.0,
0.8.0,This is required for the alabaster theme
0.8.0,refs: http://alabaster.readthedocs.io/en/latest/installation.html#sidebars
0.8.0,-- Options for HTMLHelp output ------------------------------------------
0.8.0,Output file base name for HTML help builder.
0.8.0,-- Options for LaTeX output ---------------------------------------------
0.8.0,The paper size ('letterpaper' or 'a4paper').
0.8.0,
0.8.0,"'papersize': 'letterpaper',"
0.8.0,"The font size ('10pt', '11pt' or '12pt')."
0.8.0,
0.8.0,"'pointsize': '10pt',"
0.8.0,Additional stuff for the LaTeX preamble.
0.8.0,
0.8.0,"'preamble': '',"
0.8.0,Latex figure (float) alignment
0.8.0,
0.8.0,"'figure_align': 'htbp',"
0.8.0,Grouping the document tree into LaTeX files. List of tuples
0.8.0,"(source start file, target name, title,"
0.8.0,"author, documentclass [howto, manual, or own class])."
0.8.0,-- Options for manual page output ---------------------------------------
0.8.0,One entry per manual page. List of tuples
0.8.0,"(source start file, name, description, authors, manual section)."
0.8.0,-- Options for Texinfo output -------------------------------------------
0.8.0,Grouping the document tree into Texinfo files. List of tuples
0.8.0,"(source start file, target name, title, author,"
0.8.0,"dir menu entry, description, category)"
0.8.0,!/usr/bin/env python
0.8.0,-*- coding: utf-8 -*-
0.8.0,is this reachable?
0.8.0,!/usr/bin/env python
0.8.0,-*- coding: utf-8 -*-
0.8.0,"Add in default model arguments, possibly added since training."
0.8.0,-*- encoding: utf-8 -*-
0.8.0,!/usr/bin/env python
0.8.0,-*- coding: utf-8 -*-
0.8.0,Author: Rico Sennrich
0.8.0,flake8: noqa
0.8.0,This file is retrieved from https://github.com/rsennrich/subword-nmt
0.8.0,hack for python2/3 compatibility
0.8.0,check version information
0.8.0,some hacking to deal with duplicates (only consider first instance)
0.8.0,don't print end-of-word symbols
0.8.0,sys.stderr.write('cannot split {0} further.\n'.format(segment))
0.8.0,sys.stderr.write('OOV: {0}\n'.format(segment))
0.8.0,sys.stderr.write('OOV: {0}\n'.format(segment))
0.8.0,python 2/3 compatibility
0.8.0,read/write files as UTF-8
0.8.0,!/usr/bin/env python
0.8.0,!/usr/bin/env python
0.8.0,-*- coding: utf-8 -*-
0.8.0,Author: Rico Sennrich
0.8.0,flake8: noqa
0.8.0,This file is retrieved from https://github.com/rsennrich/subword-nmt
0.8.0,hack for python2/3 compatibility
0.8.0,"find all instances of pair, and update frequency/indices around it"
0.8.0,find first symbol
0.8.0,"if first symbol is followed by second symbol, we've found an occurrence of pair (old_word[i:i+2])"
0.8.0,"assuming a symbol sequence ""A B C"", if ""B C"" is merged, reduce the frequency of ""A B"""
0.8.0,"assuming a symbol sequence ""A B C B"", if ""B C"" is merged, reduce the frequency of ""C B""."
0.8.0,"however, skip this if the sequence is A B C B C, because the frequency of ""C B"" will be reduced by the previous code block"
0.8.0,find new pair
0.8.0,"assuming a symbol sequence ""A BC D"", if ""B C"" is merged, increase the frequency of ""A BC"""
0.8.0,"assuming a symbol sequence ""A BC B"", if ""B C"" is merged, increase the frequency of ""BC B"""
0.8.0,"however, if the sequence is A BC BC, skip this step because the count of ""BC BC"" will be incremented by the previous code block"
0.8.0,data structure of pair frequencies
0.8.0,index from pairs to words
0.8.0,version 0.2 changes the handling of the end-of-word token ('</w>');
0.8.0,version numbering allows bckward compatibility
0.8.0,"threshold is inspired by Zipfian assumption, but should only affect speed"
0.8.0,we probably missed the best pair because of pruning; go back to full statistics
0.8.0,"threshold is inspired by Zipfian assumption, but should only affect speed"
0.8.0,python 2/3 compatibility
0.8.0,read/write files as UTF-8
0.8.0,!/usr/bin/env python
0.8.0,for backward compatibility
0.8.0,Build embeddings.
0.8.0,Build encoder.
0.8.0,Build decoder.
0.8.0,Share the embedding matrix - preprocess with share_vocab required.
0.8.0,src/tgt vocab should be the same if `-share_vocab` is specified.
0.8.0,Build NMTModel(= encoder + decoder).
0.8.0,Build Generator.
0.8.0,Load the model states from checkpoint or initialize them.
0.8.0,This preserves backward-compat for models using customed layernorm
0.8.0,end of patch for backward compatibility
0.8.0,!/usr/bin/env python
0.8.0,this check is here because audio allows the encoder and decoder to
0.8.0,"be different sizes, but other model types do not yet"
0.8.0,Load checkpoint if we resume from a previous training.
0.8.0,Load default opts values then overwrite it with opts from
0.8.0,the checkpoint. It's usefull in order to re-train a model
0.8.0,after adding a new option (not set in checkpoint)
0.8.0,check for code where vocab is saved instead of fields
0.8.0,(in the future this will be done in a smarter way)
0.8.0,"Report src and tgt vocab sizes, including for features"
0.8.0,Build model.
0.8.0,Build optimizer.
0.8.0,Build model saver
0.8.0,this line is kind of a temporary kludge because different objects expect
0.8.0,fields to have a different structure
0.8.0,Embedding Options
0.8.0,Encoder-Decoder Options
0.8.0,"group.add('--residual', '-residual',   action=""store_true"","
0.8.0,"help=""Add residual connections between RNN layers."")"
0.8.0,Attention options
0.8.0,Generator and loss options.
0.8.0,Data options
0.8.0,"Dictionary options, for text corpus"
0.8.0,"Truncation options, for text corpus"
0.8.0,Data processing options
0.8.0,Options most relevant to speech
0.8.0,Option most relevant to image input
0.8.0,GPU
0.8.0,Init options
0.8.0,Pretrained word vectors
0.8.0,Fixed word vectors
0.8.0,Optimization options
0.8.0,learning rate
0.8.0,Use TensorboardX for visualization during training
0.8.0,Options most relevant to speech
0.8.0,Option most relevant to image input
0.8.0,Options most relevant to summarization.
0.8.0,Alpha and Beta values for Google Length + Coverage penalty
0.8.0,"Described here: https://arxiv.org/pdf/1609.08144.pdf, Section 7"
0.8.0,Options most relevant to speech.
0.8.0,Option most relevant to image input
0.8.0,MARKDOWN boilerplate
0.8.0,Copyright 2016 The Chromium Authors. All rights reserved.
0.8.0,Use of this source code is governed by a BSD-style license that can be
0.8.0,found in the LICENSE file.
0.8.0,**section heading**:
0.8.0,# **--argument-one**
0.8.0,"Get the key 'value' in the dict, or just use 'value'"
0.8.0,Basic attributes.
0.8.0,Set model in training mode.
0.8.0,Set model in validating mode.
0.8.0,F-prop through the model.
0.8.0,Compute loss.
0.8.0,Update statistics.
0.8.0,Set model back to training mode.
0.8.0,Truncated BPTT: reminder not compatible with accum > 1
0.8.0,1. Create truncated target.
0.8.0,2. F-prop all but generator.
0.8.0,3. Compute loss.
0.8.0,4. Update the parameters and statistics.
0.8.0,Multi GPU gradient gather
0.8.0,"If truncated, don't backprop fully."
0.8.0,TO CHECK
0.8.0,if dec_state is not None:
0.8.0,dec_state.detach()
0.8.0,"in case of multi step gradient accumulation,"
0.8.0,update only after accum batches
0.8.0,For Flake
0.8.0,Initialize the bridge layer
0.8.0,"s_len, batch, emb_dim = emb.size()"
0.8.0,Lengths data is wrapped inside a Tensor.
0.8.0,"LSTM has hidden and cell state, other only one"
0.8.0,Total number of states
0.8.0,Build a linear layer for each
0.8.0,The encoder hidden is  (layers*directions) x batch x dim.
0.8.0,"s_len, batch, emb_dim = emb.size()"
0.8.0,Run the forward pass of every layer of the tranformer.
0.8.0,why is the model_opt.__dict__ check necessary?
0.8.0,"(batch_size, 64, imgH, imgW)"
0.8.0,layer 1
0.8.0,"(batch_size, 64, imgH/2, imgW/2)"
0.8.0,"(batch_size, 128, imgH/2, imgW/2)"
0.8.0,layer 2
0.8.0,"(batch_size, 128, imgH/2/2, imgW/2/2)"
0.8.0,"(batch_size, 256, imgH/2/2, imgW/2/2)"
0.8.0,layer 3
0.8.0,batch norm 1
0.8.0,"(batch_size, 256, imgH/2/2, imgW/2/2)"
0.8.0,layer4
0.8.0,"(batch_size, 256, imgH/2/2/2, imgW/2/2)"
0.8.0,"(batch_size, 512, imgH/2/2/2, imgW/2/2)"
0.8.0,layer 5
0.8.0,batch norm 2
0.8.0,"(batch_size, 512, imgH/2/2/2, imgW/2/2/2)"
0.8.0,"(batch_size, 512, imgH/2/2/2, imgW/2/2/2)"
0.8.0,"# (batch_size, 512, H, W)"
0.8.0,Dimensions and padding for constructing the word embedding matrix
0.8.0,Dimensions and padding for feature embedding matrices
0.8.0,(these have no effect if feat_vocab_sizes is empty)
0.8.0,The embedding matrix look-up tables. The first look-up table
0.8.0,"is for words. Subsequent ones are for features, if any exist."
0.8.0,The final output size of word + feature vectors. This can vary
0.8.0,from the word vector size if and only if features are defined.
0.8.0,This is the attribute you should access if you need to know
0.8.0,how big your embeddings are going to be.
0.8.0,The sequence of operations that converts the input sequence
0.8.0,into a sequence of embeddings. At minimum this consists of
0.8.0,looking up the embeddings for each word and feature in the
0.8.0,input. Model parameters may require the sequence to contain
0.8.0,additional operations as well.
0.8.0,features must use word_vec_size
0.8.0,features will use feat_vec_size
0.8.0,This class is mainly used by decoder.py for RNNs but also
0.8.0,by the CNN / transformer decoder when copy attention is used
0.8.0,CNN has its own attention mechanism ConvMultiStepAttention
0.8.0,Transformer has its own MultiHeadedAttention
0.8.0,mlp wants it with bias
0.8.0,Check input sizes
0.8.0,"(batch, t_len, d) x (batch, d, s_len) --> (batch, t_len, s_len)"
0.8.0,"(batch, t_len, s_len, d)"
0.8.0,one step input
0.8.0,"compute attention scores, as in Luong et al."
0.8.0,Softmax or sparsemax to normalize attention weights
0.8.0,each context vector c_t is the weighted average
0.8.0,over all the source hidden states
0.8.0,concatenate
0.8.0,Check output sizes
0.8.0,Check output sizes
0.8.0,clamping necessary because of numerical errors: loss should be lower
0.8.0,"bounded by zero, but negative values near zero are possible without"
0.8.0,the clamp
0.8.0,from onmt.utils.misc import aeq
0.8.0,CHECKS
0.8.0,"batch, k_len, d = key.size()"
0.8.0,"batch_, k_len_, d_ = value.size()"
0.8.0,"aeq(batch, batch_)"
0.8.0,"aeq(k_len, k_len_)"
0.8.0,"aeq(d, d_)"
0.8.0,"batch_, q_len, d_ = query.size()"
0.8.0,"aeq(batch, batch_)"
0.8.0,"aeq(d, d_)"
0.8.0,"aeq(self.model_dim % 8, 0)"
0.8.0,if mask is not None:
0.8.0,"batch_, q_len_, k_len_ = mask.size()"
0.8.0,"aeq(batch_, batch)"
0.8.0,"aeq(k_len_, k_len)"
0.8.0,aeq(q_len_ == q_len)
0.8.0,END CHECKS
0.8.0,"1) Project key, value, and query."
0.8.0,1 or key_len x key_len
0.8.0,1 or key_len x key_len x dim_per_head
0.8.0,1 or key_len x key_len x dim_per_head
0.8.0,2) Calculate and scale scores.
0.8.0,batch x num_heads x query_len x key_len
0.8.0,3) Apply attention dropout and compute context vectors.
0.8.0,CHECK
0.8.0,"batch_, q_len_, d_ = output.size()"
0.8.0,"aeq(q_len, q_len_)"
0.8.0,"aeq(batch, batch_)"
0.8.0,"aeq(d, d_)"
0.8.0,Return one attn
0.8.0,At the moment this class is only used by embeddings.Embeddings look-up tables
0.8.0,-*- coding: utf-8 -*-
0.8.0,checks
0.8.0,"batch, channel, height, width = base_target_emb.size()"
0.8.0,"batch_, channel_, height_, width_ = input_from_dec.size()"
0.8.0,"enc_batch, enc_channel, enc_height = encoder_out_top.size()"
0.8.0,"enc_batch_, enc_channel_, enc_height_ = encoder_out_combine.size()"
0.8.0,out_features * in_features
0.8.0,norm is out_features * 1
0.8.0,batch_size * out_features
0.8.0,out_features
0.8.0,out_features
0.8.0,batch_size * out_features
0.8.0,"out_channels, in_channels // groups, * kernel_size"
0.8.0,out_features
0.8.0,This is used nowhere in the code at the moment (Vincent Nguyen 05/18/2018)
0.8.0,"in_channels, out_channels, *kernel_size"
0.8.0,"in_channels, out_channels, *kernel_size"
0.8.0,"self.out_channels, 1"
0.8.0,out_features
0.8.0,out_features
0.8.0,store roots on diagonal
0.8.0,CHECKS
0.8.0,Original probabilities.
0.8.0,Probability of copying p(z=1) batch.
0.8.0,Probability of not copying: p_{word}(w) * (1 - p(z))
0.8.0,probabilities assigned by the model to the gold targets
0.8.0,probability of tokens copied from source
0.8.0,Set scores for unk to 0 and add eps
0.8.0,find the indices in which you do not use the copy mechanism
0.8.0,Drop padding.
0.8.0,this block does not depend on the loss value computed above
0.8.0,and is used only for stats
0.8.0,this block does not depend on the loss value computed above
0.8.0,and is used only for stats
0.8.0,Correct target copy token instead of <unk>
0.8.0,tgt[i] = align[i] + len(tgt_vocab)
0.8.0,for i such that tgt[i] == 0 and align[i] != 0
0.8.0,Compute sum of perplexities for stats
0.8.0,this part looks like it belongs in CopyGeneratorLoss
0.8.0,Compute Loss as NLL divided by seq length
0.8.0,Compute Total Loss per sequence in batch
0.8.0,Divide by length of each sequence and sum
0.8.0,all beams repeat (beam >= 1 repeat dummy scores)
0.8.0,predict repeat_idx over and over again
0.8.0,beam 0 and beam >=2 will repeat (beam >= 2 repeat dummy scores)
0.8.0,non-interesting beams are going to get dummy values
0.8.0,"on initial round, only predicted scores for beam 0"
0.8.0,matter. Make two predictions. Top one will be repeated
0.8.0,"in beam zero, second one will live on in beam 1."
0.8.0,predict the same thing in beam 0
0.8.0,continue pushing around what beam 1 predicts
0.8.0,"now beam 0 dies (along with the others), beam 1 -> beam 0"
0.8.0,beam 0 and beam >= 2 will repeat (beam 2 repeats excluded idx)
0.8.0,non-interesting beams are going to get dummy values
0.8.0,predict the same thing in beam 0
0.8.0,continue pushing around what beam 1 predicts
0.8.0,predict the allowed-repeat again in beam 2
0.8.0,"now beam 0 dies, beam 1 -> beam 0, beam 2 -> beam 1"
0.8.0,and the rest die
0.8.0,"since all preds after i=0 are 0, we can check"
0.8.0,that the beam is the correct idx by checking that
0.8.0,the curr score is the initial score
0.8.0,beam 0 will always predict EOS. The other beams will predict
0.8.0,non-eos scores.
0.8.0,"this is also a test that when block_ngram_repeat=0,"
0.8.0,repeating is acceptable
0.8.0,beam includes start token in cur_len count.
0.8.0,Add one to its min_length to compensate
0.8.0,non-interesting beams are going to get dummy values
0.8.0,"""best"" prediction is eos - that should be blocked"
0.8.0,include at least beam_sz predictions OTHER than EOS
0.8.0,that are greater than -1e20
0.8.0,predict eos in beam 0
0.8.0,provide beam_sz other good predictions
0.8.0,now the top beam has ended and no others have
0.8.0,first beam finished had length beam.min_length
0.8.0,first beam finished was 0
0.8.0,"not of interest, but want to make sure it keeps running"
0.8.0,since only beam 0 terminates and n_best = 2
0.8.0,"this is also a test that when block_ngram_repeat=0,"
0.8.0,repeating is acceptable
0.8.0,beam includes start token in cur_len count.
0.8.0,Add one to its min_length to compensate
0.8.0,non-interesting beams are going to get dummy values
0.8.0,"""best"" prediction is eos - that should be blocked"
0.8.0,include at least beam_sz predictions OTHER than EOS
0.8.0,that are greater than -1e20
0.8.0,predict eos in beam 1
0.8.0,provide beam_sz other good predictions in other beams
0.8.0,provide beam_sz other good predictions in other beams
0.8.0,beam 1 dies on min_length
0.8.0,beam 0 dies on the step after beam 1 dies
0.8.0,"init_preds: [4, 3, 5, 6, 7] - no EOS's"
0.8.0,no EOS's yet
0.8.0,"[5, 3, 2, 6, 0], so beam 2 predicts EOS!"
0.8.0,assumes beam 2 finished on last step
0.8.0,"[2, 5, 3, 6, 0], so beam 0 predicts EOS!"
0.8.0,"[-2.4879, -3.8910, -4.1010, -4.2010, -4.4010]"
0.8.0,new beam 0 finished
0.8.0,new beam 0 is old beam 3
0.8.0,assumes beam 0 finished on last step
0.8.0,"[5, 2, 6, 1, 0], so beam 1 predicts EOS!"
0.8.0,new beam 1 finished
0.8.0,new beam 1 is old beam 4
0.8.0,this could be considered an integration test because it tests
0.8.0,interactions between the GNMT scorer and the beam
0.8.0,initialize fields at the top of each unit test to prevent
0.8.0,any undesired stateful effects
0.8.0,"this test touches the file system, so it could be considered an"
0.8.0,integration test
0.8.0,write utf-8 bytes
0.8.0,illegal_weights_mask = torch.ByteTensor([
0.8.0,"[0, 0, 0, 0, 0, 0, 0],"
0.8.0,"[0, 0, 0, 1, 1, 1, 1],"
0.8.0,"[0, 0, 0, 0, 0, 1, 1],"
0.8.0,"[0, 0, 1, 1, 1, 1, 1]])"
0.8.0,TODO: fix for pytorch 0.3
0.8.0,illegal_weights = alignments.masked_select(illegal_weights_mask)
0.8.0,"self.assertEqual(0.0, illegal_weights.data.sum())"
0.8.0,-*- coding: utf-8 -*-
0.8.0,tests pad and numericalize integration
0.8.0,tests pad and numericalize integration
0.8.0,"this test touches the file system, so it could be considered an"
0.8.0,integration test
0.8.0,file to hold full paths to audio data
0.8.0,file to hold audio paths relative to _AUDIO_DATA_DIR (i.e. file names)
0.8.0,it's ok if non-audio files co-exist with audio files in the data dir
0.8.0,"dividing gets the noise in [-1, 1]"
0.8.0,"this test touches the file system, so it could be considered an"
0.8.0,integration test
0.8.0,file to hold full paths to image data
0.8.0,file to hold image paths relative to _IMG_DATA_DIR (i.e. file names)
0.8.0,it's ok if non-image files co-exist with image files in the data dir
0.8.0,all beams repeat (beam >= 1 repeat dummy scores)
0.8.0,predict repeat_idx over and over again
0.8.0,beam 0 and beam >=2 will repeat (beam >= 2 repeat dummy scores)
0.8.0,non-interesting beams are going to get dummy values
0.8.0,"on initial round, only predicted scores for beam 0"
0.8.0,matter. Make two predictions. Top one will be repeated
0.8.0,"in beam zero, second one will live on in beam 1."
0.8.0,predict the same thing in beam 0
0.8.0,continue pushing around what beam 1 predicts
0.8.0,"now beam 0 dies (along with the others), beam 1 -> beam 0"
0.8.0,beam 0 and beam >= 2 will repeat (beam 2 repeats excluded idx)
0.8.0,non-interesting beams are going to get dummy values
0.8.0,predict the same thing in beam 0
0.8.0,continue pushing around what beam 1 predicts
0.8.0,predict the allowed-repeat again in beam 2
0.8.0,"now beam 0 dies, beam 1 -> beam 0, beam 2 -> beam 1"
0.8.0,and the rest die
0.8.0,"since all preds after i=0 are 0, we can check"
0.8.0,that the beam is the correct idx by checking that
0.8.0,the curr score is the initial score
0.8.0,beam 0 will always predict EOS. The other beams will predict
0.8.0,non-eos scores.
0.8.0,beam includes start token in cur_len count.
0.8.0,Add one to its min_length to compensate
0.8.0,non-interesting beams are going to get dummy values
0.8.0,"""best"" prediction is eos - that should be blocked"
0.8.0,include at least beam_sz predictions OTHER than EOS
0.8.0,that are greater than -1e20
0.8.0,predict eos in beam 0
0.8.0,provide beam_sz other good predictions
0.8.0,now the top beam has ended and no others have
0.8.0,"not of interest, but want to make sure it keeps running"
0.8.0,since only beam 0 terminates and n_best = 2
0.8.0,"this is also a test that when block_ngram_repeat=0,"
0.8.0,repeating is acceptable
0.8.0,beam includes start token in cur_len count.
0.8.0,Add one to its min_length to compensate
0.8.0,non-interesting beams are going to get dummy values
0.8.0,"""best"" prediction is eos - that should be blocked"
0.8.0,include at least beam_sz predictions OTHER than EOS
0.8.0,that are greater than -1e20
0.8.0,predict eos in beam 1
0.8.0,provide beam_sz other good predictions in other beams
0.8.0,provide beam_sz other good predictions in other beams
0.8.0,beam 1 dies on min_length
0.8.0,beam 0 dies on the step after beam 1 dies
0.8.0,this is just test_beam.TestBeamAgainstReferenceCase repeated
0.8.0,in each batch.
0.8.0,"init_preds: [4, 3, 5, 6, 7] - no EOS's"
0.8.0,no EOS's yet
0.8.0,"[5, 3, 2, 6, 0], so beam 2 predicts EOS!"
0.8.0,assumes beam 2 finished on last step
0.8.0,ended beam 2 shouldn't continue
0.8.0,"[2, 5, 3, 6, 0] repeat self.BATCH_SZ, so beam 0 predicts EOS!"
0.8.0,"[-2.4879, -3.8910, -4.1010, -4.2010, -4.4010] repeat self.BATCH_SZ"
0.8.0,another beam is finished in all batches
0.8.0,new beam 0 finished
0.8.0,new beam 0 is old beam 3
0.8.0,assumes beam 0 finished on last step
0.8.0,"[5, 2, 6, 1, 0] repeat self.BATCH_SZ, so beam 1 predicts EOS!"
0.8.0,new beam 1 finished
0.8.0,new beam 1 is old beam 4
0.8.0,this could be considered an integration test because it tests
0.8.0,interactions between the GNMT scorer and the beam
0.8.0,"-data option is required, but not used in this test, so dummy."
0.8.0,len x batch x nfeat
0.8.0,batch x c x h x w
0.8.0,batch x 1 x nfft x t
0.8.0,Initialize vectors to compare size with
0.8.0,Ensure correct sizes and types
0.8.0,Make sure that output has the correct size and type
0.8.0,Make sure that output has the correct size and type
0.8.0,Make sure that output has the correct size and type
0.8.0,"[('encoder_type', 'transformer'),"
0.8.0,"('word_vec_size', 16), ('rnn_size', 16)],"
0.8.0,""""""" Only do SRU test if requirment is safisfied. """""""
0.8.0,SRU doesn't support input_feed.
0.8.0,first check there's nothing unexpectedly not trainable
0.8.0,ok: word embeddings shouldn't be trainable
0.8.0,if word vecs are fixed
0.8.0,ok: positional encodings shouldn't be trainable
0.8.0,then check nothing unexpectedly trainable
0.8.0,!/usr/bin/env python
0.8.0,-*- coding: utf-8 -*-
0.8.0,Remove the generated *pt files.
0.8.0,Test image preprocessing
0.8.0,Test audio preprocessing
0.8.0,Decoder state
0.8.0,Build the RNN.
0.8.0,Set up the context gate.
0.8.0,Set up the standard attention.
0.8.0,The encoder hidden is  (layers*directions) x batch x dim.
0.8.0,We need to convert it to layers x batch x (directions*dim).
0.8.0,Init the input feed.
0.8.0,Update the state with the result.
0.8.0,Concatenates sequence of tensors along a new dimension.
0.8.0,NOTE: v0.3 to 0.4: dec_outs / attns[*] may not be list
0.8.0,(in particular in case of SRU) it was not raising error in 0.3
0.8.0,since stack(Variable) was allowed.
0.8.0,"In 0.4, SRU returns a tensor that shouldn't be stacke"
0.8.0,TODO change the way attns is returned dict => list or tuple (onnx)
0.8.0,Check
0.8.0,Calculate the attention.
0.8.0,Calculate the context gate.
0.8.0,Additional args check.
0.8.0,END Additional args check.
0.8.0,Input feed concatenates hidden state with
0.8.0,input at every time step.
0.8.0,TODO: context gate should be employed
0.8.0,instead of second RNN transform.
0.8.0,Update the coverage attention.
0.8.0,Decoder State
0.8.0,CNNDecoder has its own attention mechanism.
0.8.0,Set up a separate copy attention layer if needed.
0.8.0,The output of CNNEncoder.
0.8.0,The combination of output of CNNEncoder and source embeddings.
0.8.0,Process the result and update the attentions.
0.8.0,Update the state.
0.8.0,TODO change the way attns is returned dict => list or tuple (onnx)
0.8.0,Memory_lengths is a single tensor shared between all models.
0.8.0,This assumption will not hold if Translator is modified
0.8.0,to calculate memory_lengths as something other than the length
0.8.0,of the input.
0.8.0,Decoder State
0.8.0,"previously, there was a GlobalAttention module here for copy"
0.8.0,"attention. But it was never actually used -- the ""copy"" attention"
0.8.0,just reuses the context attention.
0.8.0,TODO change the way attns is returned dict => list or tuple (onnx)
0.8.0,"buffer size in bytes, determine equiv. # of elements based on data type"
0.8.0,copy tensors into buffer_t
0.8.0,all-reduce and rescale
0.8.0,copy all-reduced buffer back into tensors
0.8.0,"tensor is bigger than buffer, all-reduce and rescale directly"
0.8.0,"buffer is full, all-reduce and replace buffer with grad"
0.8.0,add tensor to buffer
0.8.0,TODO: Find a better way to check for sparse gradients.
0.8.0,TODO: clean this up when APEX unify its optimizer API.
0.8.0,Load everything from the checkpoint.
0.8.0,Build everything from scratch.
0.8.0,"Reset optimizer, keep options."
0.8.0,"Reset options, keep optimizer."
0.8.0,State can be partially restored.
0.8.0,Code below is an implementation of https://arxiv.org/pdf/1804.04235.pdf
0.8.0,inspired but modified from https://github.com/DeadAt0m/adafactor-pytorch
0.8.0,-*- coding: utf-8 -*-
0.8.0,if the loss function operates on vectors of raw logits instead of
0.8.0,"probabilities, only the first part of the generator needs to be"
0.8.0,"passed to the NMTLossCompute. At the moment, the only supported"
0.8.0,loss function of this kind is the sparsemax loss.
0.8.0,non_none: the subdict of the state dictionary where the values
0.8.0,are not None.
0.8.0,"Now, the iteration:"
0.8.0,state is a dictionary of sequences of tensor-like but we
0.8.0,want a sequence of dictionaries of tensors.
0.8.0,"First, unzip the dictionary into a sequence of keys and a"
0.8.0,sequence of tensor-like sequences.
0.8.0,"Now, yield a dictionary for each shard. The keys are always"
0.8.0,the same. values is a sequence of length #keys where each
0.8.0,element is a sequence of length #shards. We want to iterate
0.8.0,"over the shards, not over the keys: therefore, the values need"
0.8.0,to be re-zipped by shard and then each shard can be paired
0.8.0,with the keys.
0.8.0,Assumed backprop'd
0.8.0,Log the progress using the number of batches on the x-axis.
0.8.0,Get a list of world_size lists with len(stat_list) Statistics objects
0.8.0,SRU doesn't support PackedSequence.
0.8.0,-*- coding: utf-8 -*-
0.8.0,this one is needed for torchtext random call (shuffled iterator)
0.8.0,in multi gpu it ensures datasets are read in the same order
0.8.0,some cudnn methods can be random even after fixing the seed
0.8.0,unless you tell it to be deterministic
0.8.0,These ensure same initialization in multi gpu mode
0.8.0,Shift values to be >= 0
0.8.0,coding: utf-8
0.8.0,make a small vocab containing just the tokens in the source sequence
0.8.0,Map source tokens to indices in the dynamic dict.
0.8.0,self.src_vocabs is used in collapse_copy_scores and Translator.py
0.8.0,this assumes src_field and tgt_field are both text
0.8.0,the dataset's self.fields should have the same attributes as examples
0.8.0,avoid infinite recursion when fields isn't defined
0.8.0,-*- coding: utf-8 -*-
0.8.0,backwards compatibility
0.8.0,monkey-patch to make torchtext Vocab's pickleable
0.8.0,"if tgt isn't using TextMultiField, then no text field is."
0.8.0,this is basically copy-pasted from torchtext.
0.8.0,counters changes in place
0.8.0,keep the order of tokens specified in the vocab file by
0.8.0,adding them to the counter with decreasing counting values
0.8.0,Load vocabulary
0.8.0,Drop the none-using from memory but keep the last
0.8.0,`tgt_vocab_size` is ignored when sharing vocabularies
0.8.0,"in the long run, shouldn't it be possible to do this by calling"
0.8.0,build_vocab with both the src and tgt data?
0.8.0,Cycle through the shards indefinitely.
0.8.0,"When the dataset is not repeated, we might need to ensure that"
0.8.0,the number of returned batches is the multiple of a given value.
0.8.0,This is important for multi GPU training to ensure that all
0.8.0,workers have the same number of batches to process.
0.8.0,Maintains the longest src and tgt length in the current batch
0.8.0,Reset current longest length at a new batch (count=1)
0.8.0,Src: [<bos> w1 ... wN <eos>]
0.8.0,Tgt: [w1 ... wM <eos>]
0.8.0,-*- coding: utf-8 -*-
0.8.0,imports of datatype-specific dependencies
0.8.0,torchaudio loading options recently changed. It's probably
0.8.0,straightforward to rewrite the audio handling to make use of
0.8.0,"up-to-date torchaudio, but in the meantime there is a legacy"
0.8.0,method which uses the old defaults
0.8.0,STFT
0.8.0,-*- coding: utf-8 -*-
0.8.0,domain specific dependencies
0.8.0,coding: utf-8
0.8.0,several data readers need optional dependencies. There's no
0.8.0,appropriate builtin exception
0.8.0,-*- coding: utf-8 -*-
0.8.0,mix this with partial
0.8.0,batch (list(list(list))): batch_size x len(self.fields) x seq_len
0.8.0,lengths: batch_size
0.8.0,data: seq_len x batch_size x len(self.fields)
0.8.0,flake8: noqa
0.8.0,For command-line option parsing
0.8.0,"Check pass, set the args."
0.8.0,"This SRU version implements its own cuda-level optimization,"
0.8.0,so it requires that:
0.8.0,1. `cupy` and `pynvrtc` python package installed.
0.8.0,2. pytorch is built with cuda support.
0.8.0,3. library path set: export LD_LIBRARY_PATH=<cuda lib path>.
0.8.0,Check 1.
0.8.0,Check 2.
0.8.0,Check 3.
0.8.0,This sets up device to use.
0.8.0,-> directions x batch x dim
0.8.0,For DEBUG
0.8.0,"size = (length, batch, x.size(-1)) \"
0.8.0,"if x.dim() == 3 else (batch, x.size(-1))"
0.8.0,grad_x = x.new(*x.size()) if k_ == 3 else x.new(*size).zero_()
0.8.0,Normal use
0.8.0,"An entry check here, will catch on train side and translate side"
0.8.0,if requirements are not satisfied.
0.8.0,RNNDecoderState wraps hidden as a tuple.
0.8.0,fh -> (layers*directions) x batch x dim
0.8.0,The score for each translation on the beam.
0.8.0,The backpointers at each time-step.
0.8.0,The outputs at each time-step.
0.8.0,Has EOS topped the beam yet.
0.8.0,The attentions (matrix) for each time.
0.8.0,Time and k pair for finished.
0.8.0,Information for global scoring.
0.8.0,Minimum prediction length
0.8.0,Apply Penalty at every step
0.8.0,force the output to be longer than self.min_length
0.8.0,assumes there are len(word_probs) predictions OTHER
0.8.0,than EOS that are greater than -1e20
0.8.0,Sum the previous scores.
0.8.0,Don't let EOS have children.
0.8.0,Block ngram repeats
0.8.0,"Last n tokens, n = block_ngram_repeat"
0.8.0,Skip the blocking if it is in the exclusion list
0.8.0,"best_scores_id is flattened beam x word array, so calculate which"
0.8.0,word and beam each score came from
0.8.0,End condition is when top-of-beam is EOS and no global score.
0.8.0,Add from beam until we have minimum outputs.
0.8.0,Term will be subtracted from probability
0.8.0,Probability will be divided by this
0.8.0,these warnings indicate that either the alpha/beta
0.8.0,"forces a penalty to be a no-op, or a penalty is a no-op but"
0.8.0,the alpha/beta would suggest otherwise.
0.8.0,using some length penalty
0.8.0,using some coverage penalty
0.8.0,!/usr/bin/env python
0.8.0,for debugging
0.8.0,Statistics
0.8.0,"For temp=0.0, take the argmax to avoid divide-by-zero errors."
0.8.0,keep_topk=1 is also equivalent to argmax.
0.8.0,Set all logits that are not in the top-k to -1000.
0.8.0,This puts the probabilities close to 0.
0.8.0,TODO: support these blacklisted features.
0.8.0,Encoder forward.
0.8.0,"seq_so_far contains chosen tokens; on each step, dim 1 grows by one."
0.8.0,Note that what this code calls log_probs are actually logits.
0.8.0,Append last prediction.
0.8.0,"Store finished hypotheses for this batch. Unlike in beam search,"
0.8.0,there will only ever be 1 hypothesis per example.
0.8.0,Turn any copied words into UNKs.
0.8.0,"Decoder forward, takes [tgt_len, batch, nfeats] as input"
0.8.0,"and [src_len, batch, hidden] as memory_bank"
0.8.0,"in case of inference tgt_len = 1, batch = beam times batch_size"
0.8.0,"in case of Gold Scoring tgt_len = actual length, batch = 1 batch"
0.8.0,Generator forward.
0.8.0,"returns [(batch_size x beam_size) , vocab ] when 1 step"
0.8.0,"or [ tgt_len, batch_size, vocab ] when full sentence"
0.8.0,"here we have scores [tgt_lenxbatch, vocab] or [beamxbatch, vocab]"
0.8.0,"returns [(batch_size x beam_size) , vocab ] when 1 step"
0.8.0,"or [ tgt_len, batch_size, vocab ] when full sentence"
0.8.0,TODO: support these blacklisted features.
0.8.0,(0) Prep the components of the search.
0.8.0,(1) Run the encoder on the src.
0.8.0,(2) Repeat src objects `beam_size` times.
0.8.0,We use batch_size x beam_size
0.8.0,"(0) pt 2, prep the beam object"
0.8.0,Reorder states.
0.8.0,"This is left in the code for now, but unsued"
0.8.0,(0) Prep each of the components of the search.
0.8.0,And helper method for reducing verbosity.
0.8.0,(1) Run the encoder on the src.
0.8.0,(2) Repeat src objects `beam_size` times.
0.8.0,We use now  batch_size x beam_size (same as fast mode)
0.8.0,"(3) run the decoder to generate sentences, using beam search."
0.8.0,(a) Construct batch x beam_size nxt words.
0.8.0,Get all the pending current beam words and arrange for forward.
0.8.0,(b) Decode and forward
0.8.0,(c) Advance each beam.
0.8.0,Loop over the batch_size number of beam
0.8.0,(4) Extract sentences from beam.
0.8.0,Rollback pointer to the beginning.
0.8.0,magic indices
0.8.0,beam parameters
0.8.0,result caching
0.8.0,beam state
0.8.0,"""global state"" of the old beam"
0.8.0,for testing
0.8.0,force the output to be longer than self.min_length
0.8.0,Multiply probs by the beam probability.
0.8.0,block ngram repeats
0.8.0,"iterate over all batches, over all beams"
0.8.0,"Last n tokens, n = block_ngram_repeat"
0.8.0,skip the blocking if any token in gram is excluded
0.8.0,"if the sequence ends now, then the penalty is the current"
0.8.0,"length + 1, to include the EOS token"
0.8.0,Flatten probs into a list of possibilities.
0.8.0,Recover log probs.
0.8.0,Length penalty is just a scalar. It doesn't matter if it's applied
0.8.0,before or after the topk.
0.8.0,Resolve beam origin and true word ids.
0.8.0,Map beam_index to batch_index in the flat representation.
0.8.0,Append last prediction.
0.8.0,update global state (step == 1)
0.8.0,update global state (step > 1)
0.8.0,"shape: (batch_size x beam_size, 1)"
0.8.0,Penalize beams that finished.
0.8.0,"on real data (newstest2017) with the pretrained transformer,"
0.8.0,it's faster to not move this back to the original device
0.8.0,Store finished hypotheses for this batch.
0.8.0,End condition is the top beam finished and we can return
0.8.0,n_best hypotheses.
0.8.0,"If all sentences are translated, no need to go further."
0.8.0,Remove finished batches for the next step.
0.8.0,!/usr/bin/env python
0.8.0,backwards compatibility for confs
0.8.0,load can be called multiple times: modify copy
0.8.0,NOTE: translator returns lists of `n_best` list
0.8.0,we can ignore that (i.e. flatten lists) only because
0.8.0,we restrict `n_best=1`
0.8.0,build back results with empty texts
0.8.0,Below are all the different penalty terms implemented so far.
0.8.0,Subtract coverage penalty from topk log probs.
0.8.0,Divide topk log probs by length penalty.
0.8.0,Sorting
0.7.2,!/usr/bin/env python
0.7.2,!/usr/bin/env python
0.7.2,!/usr/bin/env python
0.7.2,-*- coding: utf-8 -*-
0.7.2,!/usr/bin/env python
0.7.2,-*- coding: utf-8 -*-
0.7.2,!/usr/bin/env python
0.7.2,Create a thread to listen for errors in the child processes.
0.7.2,Train with multiprocessing.
0.7.2,"propagate exception to parent process, keeping original traceback"
0.7.2,!/usr/bin/env python3
0.7.2,-*- coding: utf-8 -*-
0.7.2,
0.7.2,"OpenNMT-py documentation build configuration file, created by"
0.7.2,sphinx-quickstart on Sun Dec 17 12:07:14 2017.
0.7.2,
0.7.2,This file is execfile()d with the current directory set to its
0.7.2,containing dir.
0.7.2,
0.7.2,Note that not all possible configuration values are present in this
0.7.2,autogenerated file.
0.7.2,
0.7.2,All configuration values have a default; values that are commented out
0.7.2,serve to show the default.
0.7.2,"If extensions (or modules to document with autodoc) are in another directory,"
0.7.2,add these directories to sys.path here. If the directory is relative to the
0.7.2,"documentation root, use os.path.abspath to make it absolute, like shown here."
0.7.2,
0.7.2,import os
0.7.2,import sys
0.7.2,"sys.path.insert(0, os.path.abspath('.'))"
0.7.2,-- General configuration ------------------------------------------------
0.7.2,"If your documentation needs a minimal Sphinx version, state it here."
0.7.2,
0.7.2,needs_sphinx = '1.0'
0.7.2,"Add any Sphinx extension module names here, as strings. They can be"
0.7.2,extensions coming with Sphinx (named 'sphinx.ext.*') or your custom
0.7.2,ones.
0.7.2,"Add any paths that contain templates here, relative to this directory."
0.7.2,The suffix(es) of source filenames.
0.7.2,You can specify multiple suffix as a list of string:
0.7.2,
0.7.2,"source_suffix = ['.rst', '.md']"
0.7.2,The master toctree document.
0.7.2,General information about the project.
0.7.2,"The version info for the project you're documenting, acts as replacement for"
0.7.2,"|version| and |release|, also used in various other places throughout the"
0.7.2,built documents.
0.7.2,
0.7.2,The short X.Y version.
0.7.2,"The full version, including alpha/beta/rc tags."
0.7.2,The language for content autogenerated by Sphinx. Refer to documentation
0.7.2,for a list of supported languages.
0.7.2,
0.7.2,This is also used if you do content translation via gettext catalogs.
0.7.2,"Usually you set ""language"" from the command line for these cases."
0.7.2,"List of patterns, relative to source directory, that match files and"
0.7.2,directories to ignore when looking for source files.
0.7.2,This patterns also effect to html_static_path and html_extra_path
0.7.2,The name of the Pygments (syntax highlighting) style to use.
0.7.2,"If true, `todo` and `todoList` produce output, else they produce nothing."
0.7.2,-- Options for HTML output ----------------------------------------------
0.7.2,The theme to use for HTML and HTML Help pages.  See the documentation for
0.7.2,a list of builtin themes.
0.7.2,
0.7.2,html_theme = 'sphinx_materialdesign_theme'
0.7.2,html_theme_path = [sphinx_materialdesign_theme.get_path()]
0.7.2,Theme options are theme-specific and customize the look and feel of a theme
0.7.2,"further.  For a list of options available for each theme, see the"
0.7.2,documentation.
0.7.2,
0.7.2,html_theme_options = {}
0.7.2,"Add any paths that contain custom static files (such as style sheets) here,"
0.7.2,"relative to this directory. They are copied after the builtin static files,"
0.7.2,"so a file named ""default.css"" will overwrite the builtin ""default.css""."
0.7.2,"Custom sidebar templates, must be a dictionary that maps document names"
0.7.2,to template names.
0.7.2,
0.7.2,This is required for the alabaster theme
0.7.2,refs: http://alabaster.readthedocs.io/en/latest/installation.html#sidebars
0.7.2,-- Options for HTMLHelp output ------------------------------------------
0.7.2,Output file base name for HTML help builder.
0.7.2,-- Options for LaTeX output ---------------------------------------------
0.7.2,The paper size ('letterpaper' or 'a4paper').
0.7.2,
0.7.2,"'papersize': 'letterpaper',"
0.7.2,"The font size ('10pt', '11pt' or '12pt')."
0.7.2,
0.7.2,"'pointsize': '10pt',"
0.7.2,Additional stuff for the LaTeX preamble.
0.7.2,
0.7.2,"'preamble': '',"
0.7.2,Latex figure (float) alignment
0.7.2,
0.7.2,"'figure_align': 'htbp',"
0.7.2,Grouping the document tree into LaTeX files. List of tuples
0.7.2,"(source start file, target name, title,"
0.7.2,"author, documentclass [howto, manual, or own class])."
0.7.2,-- Options for manual page output ---------------------------------------
0.7.2,One entry per manual page. List of tuples
0.7.2,"(source start file, name, description, authors, manual section)."
0.7.2,-- Options for Texinfo output -------------------------------------------
0.7.2,Grouping the document tree into Texinfo files. List of tuples
0.7.2,"(source start file, target name, title, author,"
0.7.2,"dir menu entry, description, category)"
0.7.2,!/usr/bin/env python
0.7.2,-*- coding: utf-8 -*-
0.7.2,is this reachable?
0.7.2,!/usr/bin/env python
0.7.2,-*- coding: utf-8 -*-
0.7.2,"Add in default model arguments, possibly added since training."
0.7.2,-*- encoding: utf-8 -*-
0.7.2,!/usr/bin/env python
0.7.2,-*- coding: utf-8 -*-
0.7.2,Author: Rico Sennrich
0.7.2,flake8: noqa
0.7.2,This file is retrieved from https://github.com/rsennrich/subword-nmt
0.7.2,hack for python2/3 compatibility
0.7.2,check version information
0.7.2,some hacking to deal with duplicates (only consider first instance)
0.7.2,don't print end-of-word symbols
0.7.2,sys.stderr.write('cannot split {0} further.\n'.format(segment))
0.7.2,sys.stderr.write('OOV: {0}\n'.format(segment))
0.7.2,sys.stderr.write('OOV: {0}\n'.format(segment))
0.7.2,python 2/3 compatibility
0.7.2,read/write files as UTF-8
0.7.2,!/usr/bin/env python
0.7.2,!/usr/bin/env python
0.7.2,-*- coding: utf-8 -*-
0.7.2,Author: Rico Sennrich
0.7.2,flake8: noqa
0.7.2,This file is retrieved from https://github.com/rsennrich/subword-nmt
0.7.2,hack for python2/3 compatibility
0.7.2,"find all instances of pair, and update frequency/indices around it"
0.7.2,find first symbol
0.7.2,"if first symbol is followed by second symbol, we've found an occurrence of pair (old_word[i:i+2])"
0.7.2,"assuming a symbol sequence ""A B C"", if ""B C"" is merged, reduce the frequency of ""A B"""
0.7.2,"assuming a symbol sequence ""A B C B"", if ""B C"" is merged, reduce the frequency of ""C B""."
0.7.2,"however, skip this if the sequence is A B C B C, because the frequency of ""C B"" will be reduced by the previous code block"
0.7.2,find new pair
0.7.2,"assuming a symbol sequence ""A BC D"", if ""B C"" is merged, increase the frequency of ""A BC"""
0.7.2,"assuming a symbol sequence ""A BC B"", if ""B C"" is merged, increase the frequency of ""BC B"""
0.7.2,"however, if the sequence is A BC BC, skip this step because the count of ""BC BC"" will be incremented by the previous code block"
0.7.2,data structure of pair frequencies
0.7.2,index from pairs to words
0.7.2,version 0.2 changes the handling of the end-of-word token ('</w>');
0.7.2,version numbering allows bckward compatibility
0.7.2,"threshold is inspired by Zipfian assumption, but should only affect speed"
0.7.2,we probably missed the best pair because of pruning; go back to full statistics
0.7.2,"threshold is inspired by Zipfian assumption, but should only affect speed"
0.7.2,python 2/3 compatibility
0.7.2,read/write files as UTF-8
0.7.2,!/usr/bin/env python
0.7.2,for backward compatibility
0.7.2,Build embeddings.
0.7.2,Build encoder.
0.7.2,Build decoder.
0.7.2,Share the embedding matrix - preprocess with share_vocab required.
0.7.2,src/tgt vocab should be the same if `-share_vocab` is specified.
0.7.2,Build NMTModel(= encoder + decoder).
0.7.2,Build Generator.
0.7.2,Load the model states from checkpoint or initialize them.
0.7.2,This preserves backward-compat for models using customed layernorm
0.7.2,end of patch for backward compatibility
0.7.2,!/usr/bin/env python
0.7.2,this check is here because audio allows the encoder and decoder to
0.7.2,"be different sizes, but other model types do not yet"
0.7.2,Load checkpoint if we resume from a previous training.
0.7.2,Load default opts values then overwrite it with opts from
0.7.2,the checkpoint. It's usefull in order to re-train a model
0.7.2,after adding a new option (not set in checkpoint)
0.7.2,check for code where vocab is saved instead of fields
0.7.2,(in the future this will be done in a smarter way)
0.7.2,"Report src and tgt vocab sizes, including for features"
0.7.2,Build model.
0.7.2,Build optimizer.
0.7.2,Build model saver
0.7.2,this line is kind of a temporary kludge because different objects expect
0.7.2,fields to have a different structure
0.7.2,Embedding Options
0.7.2,Encoder-Decoder Options
0.7.2,"group.add('--residual', '-residual',   action=""store_true"","
0.7.2,"help=""Add residual connections between RNN layers."")"
0.7.2,Attention options
0.7.2,Generator and loss options.
0.7.2,Data options
0.7.2,"Dictionary options, for text corpus"
0.7.2,"Truncation options, for text corpus"
0.7.2,Data processing options
0.7.2,Options most relevant to speech
0.7.2,Option most relevant to image input
0.7.2,GPU
0.7.2,Init options
0.7.2,Pretrained word vectors
0.7.2,Fixed word vectors
0.7.2,Optimization options
0.7.2,learning rate
0.7.2,Use TensorboardX for visualization during training
0.7.2,Options most relevant to speech
0.7.2,Option most relevant to image input
0.7.2,Options most relevant to summarization.
0.7.2,Alpha and Beta values for Google Length + Coverage penalty
0.7.2,"Described here: https://arxiv.org/pdf/1609.08144.pdf, Section 7"
0.7.2,Options most relevant to speech.
0.7.2,Option most relevant to image input
0.7.2,MARKDOWN boilerplate
0.7.2,Copyright 2016 The Chromium Authors. All rights reserved.
0.7.2,Use of this source code is governed by a BSD-style license that can be
0.7.2,found in the LICENSE file.
0.7.2,**section heading**:
0.7.2,# **--argument-one**
0.7.2,"Get the key 'value' in the dict, or just use 'value'"
0.7.2,Basic attributes.
0.7.2,Set model in training mode.
0.7.2,Set model in validating mode.
0.7.2,F-prop through the model.
0.7.2,Compute loss.
0.7.2,Update statistics.
0.7.2,Set model back to training mode.
0.7.2,Truncated BPTT: reminder not compatible with accum > 1
0.7.2,1. Create truncated target.
0.7.2,2. F-prop all but generator.
0.7.2,3. Compute loss.
0.7.2,4. Update the parameters and statistics.
0.7.2,Multi GPU gradient gather
0.7.2,"If truncated, don't backprop fully."
0.7.2,TO CHECK
0.7.2,if dec_state is not None:
0.7.2,dec_state.detach()
0.7.2,"in case of multi step gradient accumulation,"
0.7.2,update only after accum batches
0.7.2,For Flake
0.7.2,Initialize the bridge layer
0.7.2,"s_len, batch, emb_dim = emb.size()"
0.7.2,Lengths data is wrapped inside a Tensor.
0.7.2,"LSTM has hidden and cell state, other only one"
0.7.2,Total number of states
0.7.2,Build a linear layer for each
0.7.2,The encoder hidden is  (layers*directions) x batch x dim.
0.7.2,"s_len, batch, emb_dim = emb.size()"
0.7.2,Run the forward pass of every layer of the tranformer.
0.7.2,why is the model_opt.__dict__ check necessary?
0.7.2,"(batch_size, 64, imgH, imgW)"
0.7.2,layer 1
0.7.2,"(batch_size, 64, imgH/2, imgW/2)"
0.7.2,"(batch_size, 128, imgH/2, imgW/2)"
0.7.2,layer 2
0.7.2,"(batch_size, 128, imgH/2/2, imgW/2/2)"
0.7.2,"(batch_size, 256, imgH/2/2, imgW/2/2)"
0.7.2,layer 3
0.7.2,batch norm 1
0.7.2,"(batch_size, 256, imgH/2/2, imgW/2/2)"
0.7.2,layer4
0.7.2,"(batch_size, 256, imgH/2/2/2, imgW/2/2)"
0.7.2,"(batch_size, 512, imgH/2/2/2, imgW/2/2)"
0.7.2,layer 5
0.7.2,batch norm 2
0.7.2,"(batch_size, 512, imgH/2/2/2, imgW/2/2/2)"
0.7.2,"(batch_size, 512, imgH/2/2/2, imgW/2/2/2)"
0.7.2,"# (batch_size, 512, H, W)"
0.7.2,Dimensions and padding for constructing the word embedding matrix
0.7.2,Dimensions and padding for feature embedding matrices
0.7.2,(these have no effect if feat_vocab_sizes is empty)
0.7.2,The embedding matrix look-up tables. The first look-up table
0.7.2,"is for words. Subsequent ones are for features, if any exist."
0.7.2,The final output size of word + feature vectors. This can vary
0.7.2,from the word vector size if and only if features are defined.
0.7.2,This is the attribute you should access if you need to know
0.7.2,how big your embeddings are going to be.
0.7.2,The sequence of operations that converts the input sequence
0.7.2,into a sequence of embeddings. At minimum this consists of
0.7.2,looking up the embeddings for each word and feature in the
0.7.2,input. Model parameters may require the sequence to contain
0.7.2,additional operations as well.
0.7.2,features must use word_vec_size
0.7.2,features will use feat_vec_size
0.7.2,This class is mainly used by decoder.py for RNNs but also
0.7.2,by the CNN / transformer decoder when copy attention is used
0.7.2,CNN has its own attention mechanism ConvMultiStepAttention
0.7.2,Transformer has its own MultiHeadedAttention
0.7.2,mlp wants it with bias
0.7.2,Check input sizes
0.7.2,"(batch, t_len, d) x (batch, d, s_len) --> (batch, t_len, s_len)"
0.7.2,"(batch, t_len, s_len, d)"
0.7.2,one step input
0.7.2,"compute attention scores, as in Luong et al."
0.7.2,Softmax or sparsemax to normalize attention weights
0.7.2,each context vector c_t is the weighted average
0.7.2,over all the source hidden states
0.7.2,concatenate
0.7.2,Check output sizes
0.7.2,Check output sizes
0.7.2,clamping necessary because of numerical errors: loss should be lower
0.7.2,"bounded by zero, but negative values near zero are possible without"
0.7.2,the clamp
0.7.2,from onmt.utils.misc import aeq
0.7.2,CHECKS
0.7.2,"batch, k_len, d = key.size()"
0.7.2,"batch_, k_len_, d_ = value.size()"
0.7.2,"aeq(batch, batch_)"
0.7.2,"aeq(k_len, k_len_)"
0.7.2,"aeq(d, d_)"
0.7.2,"batch_, q_len, d_ = query.size()"
0.7.2,"aeq(batch, batch_)"
0.7.2,"aeq(d, d_)"
0.7.2,"aeq(self.model_dim % 8, 0)"
0.7.2,if mask is not None:
0.7.2,"batch_, q_len_, k_len_ = mask.size()"
0.7.2,"aeq(batch_, batch)"
0.7.2,"aeq(k_len_, k_len)"
0.7.2,aeq(q_len_ == q_len)
0.7.2,END CHECKS
0.7.2,"1) Project key, value, and query."
0.7.2,2) Calculate and scale scores.
0.7.2,3) Apply attention dropout and compute context vectors.
0.7.2,CHECK
0.7.2,"batch_, q_len_, d_ = output.size()"
0.7.2,"aeq(q_len, q_len_)"
0.7.2,"aeq(batch, batch_)"
0.7.2,"aeq(d, d_)"
0.7.2,Return one attn
0.7.2,At the moment this class is only used by embeddings.Embeddings look-up tables
0.7.2,-*- coding: utf-8 -*-
0.7.2,checks
0.7.2,"batch, channel, height, width = base_target_emb.size()"
0.7.2,"batch_, channel_, height_, width_ = input_from_dec.size()"
0.7.2,"enc_batch, enc_channel, enc_height = encoder_out_top.size()"
0.7.2,"enc_batch_, enc_channel_, enc_height_ = encoder_out_combine.size()"
0.7.2,out_features * in_features
0.7.2,norm is out_features * 1
0.7.2,batch_size * out_features
0.7.2,out_features
0.7.2,out_features
0.7.2,batch_size * out_features
0.7.2,"out_channels, in_channels // groups, * kernel_size"
0.7.2,out_features
0.7.2,This is used nowhere in the code at the moment (Vincent Nguyen 05/18/2018)
0.7.2,"in_channels, out_channels, *kernel_size"
0.7.2,"in_channels, out_channels, *kernel_size"
0.7.2,"self.out_channels, 1"
0.7.2,out_features
0.7.2,out_features
0.7.2,store roots on diagonal
0.7.2,CHECKS
0.7.2,Original probabilities.
0.7.2,Probability of copying p(z=1) batch.
0.7.2,Probability of not copying: p_{word}(w) * (1 - p(z))
0.7.2,probabilities assigned by the model to the gold targets
0.7.2,probability of tokens copied from source
0.7.2,Set scores for unk to 0 and add eps
0.7.2,find the indices in which you do not use the copy mechanism
0.7.2,Drop padding.
0.7.2,this block does not depend on the loss value computed above
0.7.2,and is used only for stats
0.7.2,this block does not depend on the loss value computed above
0.7.2,and is used only for stats
0.7.2,Correct target copy token instead of <unk>
0.7.2,tgt[i] = align[i] + len(tgt_vocab)
0.7.2,for i such that tgt[i] == 0 and align[i] != 0
0.7.2,Compute sum of perplexities for stats
0.7.2,this part looks like it belongs in CopyGeneratorLoss
0.7.2,Compute Loss as NLL divided by seq length
0.7.2,Compute Total Loss per sequence in batch
0.7.2,Divide by length of each sequence and sum
0.7.2,illegal_weights_mask = torch.ByteTensor([
0.7.2,"[0, 0, 0, 0, 0, 0, 0],"
0.7.2,"[0, 0, 0, 1, 1, 1, 1],"
0.7.2,"[0, 0, 0, 0, 0, 1, 1],"
0.7.2,"[0, 0, 1, 1, 1, 1, 1]])"
0.7.2,TODO: fix for pytorch 0.3
0.7.2,illegal_weights = alignments.masked_select(illegal_weights_mask)
0.7.2,"self.assertEqual(0.0, illegal_weights.data.sum())"
0.7.2,"-data option is required, but not used in this test, so dummy."
0.7.2,len x batch x nfeat
0.7.2,batch x c x h x w
0.7.2,batch x 1 x nfft x t
0.7.2,Initialize vectors to compare size with
0.7.2,Ensure correct sizes and types
0.7.2,Make sure that output has the correct size and type
0.7.2,Make sure that output has the correct size and type
0.7.2,Make sure that output has the correct size and type
0.7.2,"[('encoder_type', 'transformer'),"
0.7.2,"('word_vec_size', 16), ('rnn_size', 16)],"
0.7.2,""""""" Only do SRU test if requirment is safisfied. """""""
0.7.2,SRU doesn't support input_feed.
0.7.2,first check there's nothing unexpectedly not trainable
0.7.2,ok: word embeddings shouldn't be trainable
0.7.2,if word vecs are fixed
0.7.2,ok: positional encodings shouldn't be trainable
0.7.2,then check nothing unexpectedly trainable
0.7.2,!/usr/bin/env python
0.7.2,-*- coding: utf-8 -*-
0.7.2,Remove the generated *pt files.
0.7.2,Test image preprocessing
0.7.2,Test audio preprocessing
0.7.2,Decoder state
0.7.2,Build the RNN.
0.7.2,Set up the context gate.
0.7.2,Set up the standard attention.
0.7.2,The encoder hidden is  (layers*directions) x batch x dim.
0.7.2,We need to convert it to layers x batch x (directions*dim).
0.7.2,Init the input feed.
0.7.2,Update the state with the result.
0.7.2,Concatenates sequence of tensors along a new dimension.
0.7.2,NOTE: v0.3 to 0.4: dec_outs / attns[*] may not be list
0.7.2,(in particular in case of SRU) it was not raising error in 0.3
0.7.2,since stack(Variable) was allowed.
0.7.2,"In 0.4, SRU returns a tensor that shouldn't be stacke"
0.7.2,TODO change the way attns is returned dict => list or tuple (onnx)
0.7.2,Check
0.7.2,Calculate the attention.
0.7.2,Calculate the context gate.
0.7.2,Additional args check.
0.7.2,END Additional args check.
0.7.2,Input feed concatenates hidden state with
0.7.2,input at every time step.
0.7.2,TODO: context gate should be employed
0.7.2,instead of second RNN transform.
0.7.2,Update the coverage attention.
0.7.2,Decoder State
0.7.2,CNNDecoder has its own attention mechanism.
0.7.2,Set up a separate copy attention layer if needed.
0.7.2,The output of CNNEncoder.
0.7.2,The combination of output of CNNEncoder and source embeddings.
0.7.2,Process the result and update the attentions.
0.7.2,Update the state.
0.7.2,TODO change the way attns is returned dict => list or tuple (onnx)
0.7.2,Memory_lengths is a single tensor shared between all models.
0.7.2,This assumption will not hold if Translator is modified
0.7.2,to calculate memory_lengths as something other than the length
0.7.2,of the input.
0.7.2,"Register self.mask as a buffer in TransformerDecoderLayer, so"
0.7.2,it gets TransformerDecoderLayer's cuda behavior automatically.
0.7.2,Decoder State
0.7.2,"previously, there was a GlobalAttention module here for copy"
0.7.2,"attention. But it was never actually used -- the ""copy"" attention"
0.7.2,just reuses the context attention.
0.7.2,TODO change the way attns is returned dict => list or tuple (onnx)
0.7.2,"buffer size in bytes, determine equiv. # of elements based on data type"
0.7.2,copy tensors into buffer_t
0.7.2,all-reduce and rescale
0.7.2,copy all-reduced buffer back into tensors
0.7.2,"tensor is bigger than buffer, all-reduce and rescale directly"
0.7.2,"buffer is full, all-reduce and replace buffer with grad"
0.7.2,add tensor to buffer
0.7.2,TODO: Find a better way to check for sparse gradients.
0.7.2,Load everything from the checkpoint.
0.7.2,Build everything from scratch.
0.7.2,"Reset optimizer, keep options."
0.7.2,"Reset options, keep optimizer."
0.7.2,State can be partially restored.
0.7.2,Code below is an implementation of https://arxiv.org/pdf/1804.04235.pdf
0.7.2,inspired but modified from https://github.com/DeadAt0m/adafactor-pytorch
0.7.2,-*- coding: utf-8 -*-
0.7.2,if the loss function operates on vectors of raw logits instead of
0.7.2,"probabilities, only the first part of the generator needs to be"
0.7.2,"passed to the NMTLossCompute. At the moment, the only supported"
0.7.2,loss function of this kind is the sparsemax loss.
0.7.2,non_none: the subdict of the state dictionary where the values
0.7.2,are not None.
0.7.2,"Now, the iteration:"
0.7.2,state is a dictionary of sequences of tensor-like but we
0.7.2,want a sequence of dictionaries of tensors.
0.7.2,"First, unzip the dictionary into a sequence of keys and a"
0.7.2,sequence of tensor-like sequences.
0.7.2,"Now, yield a dictionary for each shard. The keys are always"
0.7.2,the same. values is a sequence of length #keys where each
0.7.2,element is a sequence of length #shards. We want to iterate
0.7.2,"over the shards, not over the keys: therefore, the values need"
0.7.2,to be re-zipped by shard and then each shard can be paired
0.7.2,with the keys.
0.7.2,Log the progress using the number of batches on the x-axis.
0.7.2,Get a list of world_size lists with len(stat_list) Statistics objects
0.7.2,SRU doesn't support PackedSequence.
0.7.2,-*- coding: utf-8 -*-
0.7.2,this one is needed for torchtext random call (shuffled iterator)
0.7.2,in multi gpu it ensures datasets are read in the same order
0.7.2,some cudnn methods can be random even after fixing the seed
0.7.2,unless you tell it to be deterministic
0.7.2,These ensure same initialization in multi gpu mode
0.7.2,coding: utf-8
0.7.2,several data readers need optional dependencies. There's no
0.7.2,appropriate builtin exception
0.7.2,self.src_vocabs is used in collapse_copy_scores and Translator.py
0.7.2,this assumes src_field and tgt_field are both text
0.7.2,the dataset's self.fields should have the same attributes as examples
0.7.2,avoid infinite recursion when fields isn't defined
0.7.2,make a small vocab containing just the tokens in the source sequence
0.7.2,Map source tokens to indices in the dynamic dict.
0.7.2,-*- coding: utf-8 -*-
0.7.2,backwards compatibility
0.7.2,"if tgt isn't using TextMultiField, then no text field is."
0.7.2,"there is a truncate argument as well, but it was never set to"
0.7.2,anything besides None before
0.7.2,the second conjunct means nothing will be filtered at translation time
0.7.2,if there is no target data
0.7.2,this is basically copy-pasted from torchtext.
0.7.2,counters changes in place
0.7.2,keep the order of tokens specified in the vocab file by
0.7.2,adding them to the counter with decreasing counting values
0.7.2,Load vocabulary
0.7.2,Drop the none-using from memory but keep the last
0.7.2,`tgt_vocab_size` is ignored when sharing vocabularies
0.7.2,"in the long run, shouldn't it be possible to do this by calling"
0.7.2,build_vocab with both the src and tgt data?
0.7.2,Maintains the longest src and tgt length in the current batch
0.7.2,Reset current longest length at a new batch (count=1)
0.7.2,Src: [<bos> w1 ... wN <eos>]
0.7.2,Tgt: [w1 ... wN <eos>]
0.7.2,-*- coding: utf-8 -*-
0.7.2,imports of datatype-specific dependencies
0.7.2,torchaudio loading options recently changed. It's probably
0.7.2,straightforward to rewrite the audio handling to make use of
0.7.2,"up-to-date torchaudio, but in the meantime there is a legacy"
0.7.2,method which uses the old defaults
0.7.2,STFT
0.7.2,-*- coding: utf-8 -*-
0.7.2,domain specific dependencies
0.7.2,-*- coding: utf-8 -*-
0.7.2,mix this with partial
0.7.2,batch (list(list(list))): batch_size x len(self.fields) x seq_len
0.7.2,lengths: batch_size
0.7.2,data: seq_len x batch_size x len(self.fields)
0.7.2,flake8: noqa
0.7.2,For command-line option parsing
0.7.2,"Check pass, set the args."
0.7.2,"This SRU version implements its own cuda-level optimization,"
0.7.2,so it requires that:
0.7.2,1. `cupy` and `pynvrtc` python package installed.
0.7.2,2. pytorch is built with cuda support.
0.7.2,3. library path set: export LD_LIBRARY_PATH=<cuda lib path>.
0.7.2,Check 1.
0.7.2,Check 2.
0.7.2,Check 3.
0.7.2,This sets up device to use.
0.7.2,-> directions x batch x dim
0.7.2,For DEBUG
0.7.2,"size = (length, batch, x.size(-1)) \"
0.7.2,"if x.dim() == 3 else (batch, x.size(-1))"
0.7.2,grad_x = x.new(*x.size()) if k_ == 3 else x.new(*size).zero_()
0.7.2,Normal use
0.7.2,"An entry check here, will catch on train side and translate side"
0.7.2,if requirements are not satisfied.
0.7.2,RNNDecoderState wraps hidden as a tuple.
0.7.2,fh -> (layers*directions) x batch x dim
0.7.2,The score for each translation on the beam.
0.7.2,The backpointers at each time-step.
0.7.2,The outputs at each time-step.
0.7.2,Has EOS topped the beam yet.
0.7.2,The attentions (matrix) for each time.
0.7.2,Time and k pair for finished.
0.7.2,Information for global scoring.
0.7.2,Minimum prediction length
0.7.2,Apply Penalty at every step
0.7.2,force the output to be longer than self.min_length
0.7.2,Sum the previous scores.
0.7.2,Don't let EOS have children.
0.7.2,Block ngram repeats
0.7.2,"Last n tokens, n = block_ngram_repeat"
0.7.2,Skip the blocking if it is in the exclusion list
0.7.2,"best_scores_id is flattened beam x word array, so calculate which"
0.7.2,word and beam each score came from
0.7.2,End condition is when top-of-beam is EOS and no global score.
0.7.2,Add from beam until we have minimum outputs.
0.7.2,Term will be subtracted from probability
0.7.2,Probability will be divided by this
0.7.2,!/usr/bin/env python
0.7.2,for debugging
0.7.2,Statistics
0.7.2,"For temp=0.0, take the argmax to avoid divide-by-zero errors."
0.7.2,keep_topk=1 is also equivalent to argmax.
0.7.2,Set all logits that are not in the top-k to -1000.
0.7.2,This puts the probabilities close to 0.
0.7.2,TODO: support these blacklisted features.
0.7.2,Encoder forward.
0.7.2,"seq_so_far contains chosen tokens; on each step, dim 1 grows by one."
0.7.2,Note that what this code calls log_probs are actually logits.
0.7.2,Append last prediction.
0.7.2,"Store finished hypotheses for this batch. Unlike in beam search,"
0.7.2,there will only ever be 1 hypothesis per example.
0.7.2,Turn any copied words into UNKs.
0.7.2,"Decoder forward, takes [tgt_len, batch, nfeats] as input"
0.7.2,"and [src_len, batch, hidden] as memory_bank"
0.7.2,"in case of inference tgt_len = 1, batch = beam times batch_size"
0.7.2,"in case of Gold Scoring tgt_len = actual length, batch = 1 batch"
0.7.2,Generator forward.
0.7.2,"returns [(batch_size x beam_size) , vocab ] when 1 step"
0.7.2,"or [ tgt_len, batch_size, vocab ] when full sentence"
0.7.2,"here we have scores [tgt_lenxbatch, vocab] or [beamxbatch, vocab]"
0.7.2,"returns [(batch_size x beam_size) , vocab ] when 1 step"
0.7.2,"or [ tgt_len, batch_size, vocab ] when full sentence"
0.7.2,TODO: support these blacklisted features.
0.7.2,Encoder forward.
0.7.2,Tile states and memory beam_size times.
0.7.2,Give full probability to the first beam on the first step.
0.7.2,Structure that holds finished hypotheses.
0.7.2,Multiply probs by the beam probability.
0.7.2,Flatten probs into a list of possibilities.
0.7.2,Recover log probs.
0.7.2,Resolve beam origin and true word ids.
0.7.2,Map beam_index to batch_index in the flat representation.
0.7.2,Append last prediction.
0.7.2,Save finished hypotheses.
0.7.2,Penalize beams that finished.
0.7.2,Store finished hypotheses for this batch.
0.7.2,End condition is the top beam finished and we can return
0.7.2,n_best hypotheses.
0.7.2,"If all sentences are translated, no need to go further."
0.7.2,Remove finished batches for the next step.
0.7.2,Reorder states.
0.7.2,(0) Prep each of the components of the search.
0.7.2,And helper method for reducing verbosity.
0.7.2,Define a set of tokens to exclude from ngram-blocking
0.7.2,(1) Run the encoder on the src.
0.7.2,(2) Repeat src objects `beam_size` times.
0.7.2,We use now  batch_size x beam_size (same as fast mode)
0.7.2,"(3) run the decoder to generate sentences, using beam search."
0.7.2,(a) Construct batch x beam_size nxt words.
0.7.2,Get all the pending current beam words and arrange for forward.
0.7.2,(b) Decode and forward
0.7.2,(c) Advance each beam.
0.7.2,Loop over the batch_size number of beam
0.7.2,(4) Extract sentences from beam.
0.7.2,Rollback pointer to the beginning.
0.7.2,!/usr/bin/env python
0.7.2,backwards compatibility for confs
0.7.2,load can be called multiple times: modify copy
0.7.2,NOTE: translator returns lists of `n_best` list
0.7.2,we can ignore that (i.e. flatten lists) only because
0.7.2,we restrict `n_best=1`
0.7.2,build back results with empty texts
0.7.2,Sorting
0.7.1,!/usr/bin/env python
0.7.1,!/usr/bin/env python
0.7.1,!/usr/bin/env python
0.7.1,-*- coding: utf-8 -*-
0.7.1,!/usr/bin/env python
0.7.1,-*- coding: utf-8 -*-
0.7.1,!/usr/bin/env python
0.7.1,Create a thread to listen for errors in the child processes.
0.7.1,Train with multiprocessing.
0.7.1,"propagate exception to parent process, keeping original traceback"
0.7.1,!/usr/bin/env python3
0.7.1,-*- coding: utf-8 -*-
0.7.1,
0.7.1,"OpenNMT-py documentation build configuration file, created by"
0.7.1,sphinx-quickstart on Sun Dec 17 12:07:14 2017.
0.7.1,
0.7.1,This file is execfile()d with the current directory set to its
0.7.1,containing dir.
0.7.1,
0.7.1,Note that not all possible configuration values are present in this
0.7.1,autogenerated file.
0.7.1,
0.7.1,All configuration values have a default; values that are commented out
0.7.1,serve to show the default.
0.7.1,"If extensions (or modules to document with autodoc) are in another directory,"
0.7.1,add these directories to sys.path here. If the directory is relative to the
0.7.1,"documentation root, use os.path.abspath to make it absolute, like shown here."
0.7.1,
0.7.1,import os
0.7.1,import sys
0.7.1,"sys.path.insert(0, os.path.abspath('.'))"
0.7.1,-- General configuration ------------------------------------------------
0.7.1,"If your documentation needs a minimal Sphinx version, state it here."
0.7.1,
0.7.1,needs_sphinx = '1.0'
0.7.1,"Add any Sphinx extension module names here, as strings. They can be"
0.7.1,extensions coming with Sphinx (named 'sphinx.ext.*') or your custom
0.7.1,ones.
0.7.1,"Add any paths that contain templates here, relative to this directory."
0.7.1,The suffix(es) of source filenames.
0.7.1,You can specify multiple suffix as a list of string:
0.7.1,
0.7.1,"source_suffix = ['.rst', '.md']"
0.7.1,The master toctree document.
0.7.1,General information about the project.
0.7.1,"The version info for the project you're documenting, acts as replacement for"
0.7.1,"|version| and |release|, also used in various other places throughout the"
0.7.1,built documents.
0.7.1,
0.7.1,The short X.Y version.
0.7.1,"The full version, including alpha/beta/rc tags."
0.7.1,The language for content autogenerated by Sphinx. Refer to documentation
0.7.1,for a list of supported languages.
0.7.1,
0.7.1,This is also used if you do content translation via gettext catalogs.
0.7.1,"Usually you set ""language"" from the command line for these cases."
0.7.1,"List of patterns, relative to source directory, that match files and"
0.7.1,directories to ignore when looking for source files.
0.7.1,This patterns also effect to html_static_path and html_extra_path
0.7.1,The name of the Pygments (syntax highlighting) style to use.
0.7.1,"If true, `todo` and `todoList` produce output, else they produce nothing."
0.7.1,-- Options for HTML output ----------------------------------------------
0.7.1,The theme to use for HTML and HTML Help pages.  See the documentation for
0.7.1,a list of builtin themes.
0.7.1,
0.7.1,html_theme = 'sphinx_materialdesign_theme'
0.7.1,html_theme_path = [sphinx_materialdesign_theme.get_path()]
0.7.1,Theme options are theme-specific and customize the look and feel of a theme
0.7.1,"further.  For a list of options available for each theme, see the"
0.7.1,documentation.
0.7.1,
0.7.1,html_theme_options = {}
0.7.1,"Add any paths that contain custom static files (such as style sheets) here,"
0.7.1,"relative to this directory. They are copied after the builtin static files,"
0.7.1,"so a file named ""default.css"" will overwrite the builtin ""default.css""."
0.7.1,"Custom sidebar templates, must be a dictionary that maps document names"
0.7.1,to template names.
0.7.1,
0.7.1,This is required for the alabaster theme
0.7.1,refs: http://alabaster.readthedocs.io/en/latest/installation.html#sidebars
0.7.1,-- Options for HTMLHelp output ------------------------------------------
0.7.1,Output file base name for HTML help builder.
0.7.1,-- Options for LaTeX output ---------------------------------------------
0.7.1,The paper size ('letterpaper' or 'a4paper').
0.7.1,
0.7.1,"'papersize': 'letterpaper',"
0.7.1,"The font size ('10pt', '11pt' or '12pt')."
0.7.1,
0.7.1,"'pointsize': '10pt',"
0.7.1,Additional stuff for the LaTeX preamble.
0.7.1,
0.7.1,"'preamble': '',"
0.7.1,Latex figure (float) alignment
0.7.1,
0.7.1,"'figure_align': 'htbp',"
0.7.1,Grouping the document tree into LaTeX files. List of tuples
0.7.1,"(source start file, target name, title,"
0.7.1,"author, documentclass [howto, manual, or own class])."
0.7.1,-- Options for manual page output ---------------------------------------
0.7.1,One entry per manual page. List of tuples
0.7.1,"(source start file, name, description, authors, manual section)."
0.7.1,-- Options for Texinfo output -------------------------------------------
0.7.1,Grouping the document tree into Texinfo files. List of tuples
0.7.1,"(source start file, target name, title, author,"
0.7.1,"dir menu entry, description, category)"
0.7.1,!/usr/bin/env python
0.7.1,-*- coding: utf-8 -*-
0.7.1,is this reachable?
0.7.1,!/usr/bin/env python
0.7.1,-*- coding: utf-8 -*-
0.7.1,"Add in default model arguments, possibly added since training."
0.7.1,-*- encoding: utf-8 -*-
0.7.1,!/usr/bin/env python
0.7.1,-*- coding: utf-8 -*-
0.7.1,Author: Rico Sennrich
0.7.1,flake8: noqa
0.7.1,This file is retrieved from https://github.com/rsennrich/subword-nmt
0.7.1,hack for python2/3 compatibility
0.7.1,check version information
0.7.1,some hacking to deal with duplicates (only consider first instance)
0.7.1,don't print end-of-word symbols
0.7.1,sys.stderr.write('cannot split {0} further.\n'.format(segment))
0.7.1,sys.stderr.write('OOV: {0}\n'.format(segment))
0.7.1,sys.stderr.write('OOV: {0}\n'.format(segment))
0.7.1,python 2/3 compatibility
0.7.1,read/write files as UTF-8
0.7.1,!/usr/bin/env python
0.7.1,!/usr/bin/env python
0.7.1,-*- coding: utf-8 -*-
0.7.1,Author: Rico Sennrich
0.7.1,flake8: noqa
0.7.1,This file is retrieved from https://github.com/rsennrich/subword-nmt
0.7.1,hack for python2/3 compatibility
0.7.1,"find all instances of pair, and update frequency/indices around it"
0.7.1,find first symbol
0.7.1,"if first symbol is followed by second symbol, we've found an occurrence of pair (old_word[i:i+2])"
0.7.1,"assuming a symbol sequence ""A B C"", if ""B C"" is merged, reduce the frequency of ""A B"""
0.7.1,"assuming a symbol sequence ""A B C B"", if ""B C"" is merged, reduce the frequency of ""C B""."
0.7.1,"however, skip this if the sequence is A B C B C, because the frequency of ""C B"" will be reduced by the previous code block"
0.7.1,find new pair
0.7.1,"assuming a symbol sequence ""A BC D"", if ""B C"" is merged, increase the frequency of ""A BC"""
0.7.1,"assuming a symbol sequence ""A BC B"", if ""B C"" is merged, increase the frequency of ""BC B"""
0.7.1,"however, if the sequence is A BC BC, skip this step because the count of ""BC BC"" will be incremented by the previous code block"
0.7.1,data structure of pair frequencies
0.7.1,index from pairs to words
0.7.1,version 0.2 changes the handling of the end-of-word token ('</w>');
0.7.1,version numbering allows bckward compatibility
0.7.1,"threshold is inspired by Zipfian assumption, but should only affect speed"
0.7.1,we probably missed the best pair because of pruning; go back to full statistics
0.7.1,"threshold is inspired by Zipfian assumption, but should only affect speed"
0.7.1,python 2/3 compatibility
0.7.1,read/write files as UTF-8
0.7.1,!/usr/bin/env python
0.7.1,for backward compatibility
0.7.1,Build encoder.
0.7.1,why is build_encoder not used here?
0.7.1,why is the model_opt.__dict__ check necessary?
0.7.1,Build decoder.
0.7.1,Share the embedding matrix - preprocess with share_vocab required.
0.7.1,src/tgt vocab should be the same if `-share_vocab` is specified.
0.7.1,Build NMTModel(= encoder + decoder).
0.7.1,Build Generator.
0.7.1,Load the model states from checkpoint or initialize them.
0.7.1,This preserves backward-compat for models using customed layernorm
0.7.1,end of patch for backward compatibility
0.7.1,!/usr/bin/env python
0.7.1,this check is here because audio allows the encoder and decoder to
0.7.1,"be different sizes, but other model types do not yet"
0.7.1,Load checkpoint if we resume from a previous training.
0.7.1,Load default opts values then overwrite it with opts from
0.7.1,the checkpoint. It's usefull in order to re-train a model
0.7.1,after adding a new option (not set in checkpoint)
0.7.1,check for code where vocab is saved instead of fields
0.7.1,(in the future this will be done in a smarter way)
0.7.1,"Report src and tgt vocab sizes, including for features"
0.7.1,Build model.
0.7.1,Build optimizer.
0.7.1,Build model saver
0.7.1,this line is kind of a temporary kludge because different objects expect
0.7.1,fields to have a different structure
0.7.1,Embedding Options
0.7.1,Encoder-Decoder Options
0.7.1,"group.add('--residual', '-residual',   action=""store_true"","
0.7.1,"help=""Add residual connections between RNN layers."")"
0.7.1,Attention options
0.7.1,Generator and loss options.
0.7.1,Data options
0.7.1,"Dictionary options, for text corpus"
0.7.1,"Truncation options, for text corpus"
0.7.1,Data processing options
0.7.1,Options most relevant to speech
0.7.1,Option most relevant to image input
0.7.1,GPU
0.7.1,Init options
0.7.1,Pretrained word vectors
0.7.1,Fixed word vectors
0.7.1,Optimization options
0.7.1,learning rate
0.7.1,Use TensorboardX for visualization during training
0.7.1,Options most relevant to speech
0.7.1,Option most relevant to image input
0.7.1,Options most relevant to summarization.
0.7.1,Alpha and Beta values for Google Length + Coverage penalty
0.7.1,"Described here: https://arxiv.org/pdf/1609.08144.pdf, Section 7"
0.7.1,Options most relevant to speech.
0.7.1,Option most relevant to image input
0.7.1,MARKDOWN boilerplate
0.7.1,Copyright 2016 The Chromium Authors. All rights reserved.
0.7.1,Use of this source code is governed by a BSD-style license that can be
0.7.1,found in the LICENSE file.
0.7.1,**section heading**:
0.7.1,# **--argument-one**
0.7.1,"Get the key 'value' in the dict, or just use 'value'"
0.7.1,Basic attributes.
0.7.1,Set model in training mode.
0.7.1,Set model in validating mode.
0.7.1,F-prop through the model.
0.7.1,Compute loss.
0.7.1,Update statistics.
0.7.1,Set model back to training mode.
0.7.1,Truncated BPTT: reminder not compatible with accum > 1
0.7.1,this method unsqueezes its input
0.7.1,1. Create truncated target.
0.7.1,2. F-prop all but generator.
0.7.1,3. Compute loss.
0.7.1,4. Update the parameters and statistics.
0.7.1,Multi GPU gradient gather
0.7.1,"If truncated, don't backprop fully."
0.7.1,TO CHECK
0.7.1,if dec_state is not None:
0.7.1,dec_state.detach()
0.7.1,"in case of multi step gradient accumulation,"
0.7.1,update only after accum batches
0.7.1,For Flake
0.7.1,Initialize the bridge layer
0.7.1,"s_len, batch, emb_dim = emb.size()"
0.7.1,Lengths data is wrapped inside a Tensor.
0.7.1,"LSTM has hidden and cell state, other only one"
0.7.1,Total number of states
0.7.1,Build a linear layer for each
0.7.1,The encoder hidden is  (layers*directions) x batch x dim.
0.7.1,"s_len, batch, emb_dim = emb.size()"
0.7.1,from onmt.utils.misc import aeq
0.7.1,Run the forward pass of every layer of the tranformer.
0.7.1,"(batch_size, 64, imgH, imgW)"
0.7.1,layer 1
0.7.1,"(batch_size, 64, imgH/2, imgW/2)"
0.7.1,"(batch_size, 128, imgH/2, imgW/2)"
0.7.1,layer 2
0.7.1,"(batch_size, 128, imgH/2/2, imgW/2/2)"
0.7.1,"(batch_size, 256, imgH/2/2, imgW/2/2)"
0.7.1,layer 3
0.7.1,batch norm 1
0.7.1,"(batch_size, 256, imgH/2/2, imgW/2/2)"
0.7.1,layer4
0.7.1,"(batch_size, 256, imgH/2/2/2, imgW/2/2)"
0.7.1,"(batch_size, 512, imgH/2/2/2, imgW/2/2)"
0.7.1,layer 5
0.7.1,batch norm 2
0.7.1,"(batch_size, 512, imgH/2/2/2, imgW/2/2/2)"
0.7.1,"(batch_size, 512, imgH/2/2/2, imgW/2/2/2)"
0.7.1,"# (batch_size, 512, H, W)"
0.7.1,Dimensions and padding for constructing the word embedding matrix
0.7.1,Dimensions and padding for feature embedding matrices
0.7.1,(these have no effect if feat_vocab_sizes is empty)
0.7.1,The embedding matrix look-up tables. The first look-up table
0.7.1,"is for words. Subsequent ones are for features, if any exist."
0.7.1,The final output size of word + feature vectors. This can vary
0.7.1,from the word vector size if and only if features are defined.
0.7.1,This is the attribute you should access if you need to know
0.7.1,how big your embeddings are going to be.
0.7.1,The sequence of operations that converts the input sequence
0.7.1,into a sequence of embeddings. At minimum this consists of
0.7.1,looking up the embeddings for each word and feature in the
0.7.1,input. Model parameters may require the sequence to contain
0.7.1,additional operations as well.
0.7.1,This class is mainly used by decoder.py for RNNs but also
0.7.1,by the CNN / transformer decoder when copy attention is used
0.7.1,CNN has its own attention mechanism ConvMultiStepAttention
0.7.1,Transformer has its own MultiHeadedAttention
0.7.1,mlp wants it with bias
0.7.1,Check input sizes
0.7.1,"(batch, t_len, d) x (batch, d, s_len) --> (batch, t_len, s_len)"
0.7.1,"(batch, t_len, s_len, d)"
0.7.1,one step input
0.7.1,"compute attention scores, as in Luong et al."
0.7.1,Softmax or sparsemax to normalize attention weights
0.7.1,each context vector c_t is the weighted average
0.7.1,over all the source hidden states
0.7.1,concatenate
0.7.1,Check output sizes
0.7.1,Check output sizes
0.7.1,clamping necessary because of numerical errors: loss should be lower
0.7.1,"bounded by zero, but negative values near zero are possible without"
0.7.1,the clamp
0.7.1,from onmt.utils.misc import aeq
0.7.1,CHECKS
0.7.1,"batch, k_len, d = key.size()"
0.7.1,"batch_, k_len_, d_ = value.size()"
0.7.1,"aeq(batch, batch_)"
0.7.1,"aeq(k_len, k_len_)"
0.7.1,"aeq(d, d_)"
0.7.1,"batch_, q_len, d_ = query.size()"
0.7.1,"aeq(batch, batch_)"
0.7.1,"aeq(d, d_)"
0.7.1,"aeq(self.model_dim % 8, 0)"
0.7.1,if mask is not None:
0.7.1,"batch_, q_len_, k_len_ = mask.size()"
0.7.1,"aeq(batch_, batch)"
0.7.1,"aeq(k_len_, k_len)"
0.7.1,aeq(q_len_ == q_len)
0.7.1,END CHECKS
0.7.1,"1) Project key, value, and query."
0.7.1,2) Calculate and scale scores.
0.7.1,3) Apply attention dropout and compute context vectors.
0.7.1,CHECK
0.7.1,"batch_, q_len_, d_ = output.size()"
0.7.1,"aeq(q_len, q_len_)"
0.7.1,"aeq(batch, batch_)"
0.7.1,"aeq(d, d_)"
0.7.1,Return one attn
0.7.1,At the moment this class is only used by embeddings.Embeddings look-up tables
0.7.1,-*- coding: utf-8 -*-
0.7.1,checks
0.7.1,"batch, channel, height, width = base_target_emb.size()"
0.7.1,"batch_, channel_, height_, width_ = input_from_dec.size()"
0.7.1,"enc_batch, enc_channel, enc_height = encoder_out_top.size()"
0.7.1,"enc_batch_, enc_channel_, enc_height_ = encoder_out_combine.size()"
0.7.1,out_features * in_features
0.7.1,norm is out_features * 1
0.7.1,batch_size * out_features
0.7.1,out_features
0.7.1,out_features
0.7.1,batch_size * out_features
0.7.1,"out_channels, in_channels // groups, * kernel_size"
0.7.1,out_features
0.7.1,This is used nowhere in the code at the moment (Vincent Nguyen 05/18/2018)
0.7.1,"in_channels, out_channels, *kernel_size"
0.7.1,"in_channels, out_channels, *kernel_size"
0.7.1,"self.out_channels, 1"
0.7.1,out_features
0.7.1,out_features
0.7.1,store roots on diagonal
0.7.1,CHECKS
0.7.1,Original probabilities.
0.7.1,Probability of copying p(z=1) batch.
0.7.1,Probability of not copying: p_{word}(w) * (1 - p(z))
0.7.1,probabilities assigned by the model to the gold targets
0.7.1,probability of tokens copied from source
0.7.1,Set scores for unk to 0 and add eps
0.7.1,find the indices in which you do not use the copy mechanism
0.7.1,Drop padding.
0.7.1,this block does not depend on the loss value computed above
0.7.1,and is used only for stats
0.7.1,this block does not depend on the loss value computed above
0.7.1,and is used only for stats
0.7.1,Correct target copy token instead of <unk>
0.7.1,tgt[i] = align[i] + len(tgt_vocab)
0.7.1,for i such that tgt[i] == 0 and align[i] != 0
0.7.1,Compute sum of perplexities for stats
0.7.1,this part looks like it belongs in CopyGeneratorLoss
0.7.1,Compute Loss as NLL divided by seq length
0.7.1,Compute Total Loss per sequence in batch
0.7.1,Divide by length of each sequence and sum
0.7.1,illegal_weights_mask = torch.ByteTensor([
0.7.1,"[0, 0, 0, 0, 0, 0, 0],"
0.7.1,"[0, 0, 0, 1, 1, 1, 1],"
0.7.1,"[0, 0, 0, 0, 0, 1, 1],"
0.7.1,"[0, 0, 1, 1, 1, 1, 1]])"
0.7.1,TODO: fix for pytorch 0.3
0.7.1,illegal_weights = alignments.masked_select(illegal_weights_mask)
0.7.1,"self.assertEqual(0.0, illegal_weights.data.sum())"
0.7.1,"-data option is required, but not used in this test, so dummy."
0.7.1,len x batch x nfeat
0.7.1,batch x c x h x w
0.7.1,batch x 1 x nfft x t
0.7.1,Initialize vectors to compare size with
0.7.1,Ensure correct sizes and types
0.7.1,Make sure that output has the correct size and type
0.7.1,Make sure that output has the correct size and type
0.7.1,Make sure that output has the correct size and type
0.7.1,"[('encoder_type', 'transformer'),"
0.7.1,"('word_vec_size', 16), ('rnn_size', 16)],"
0.7.1,""""""" Only do SRU test if requirment is safisfied. """""""
0.7.1,SRU doesn't support input_feed.
0.7.1,!/usr/bin/env python
0.7.1,-*- coding: utf-8 -*-
0.7.1,Remove the generated *pt files.
0.7.1,Test image preprocessing
0.7.1,Test audio preprocessing
0.7.1,Basic attributes.
0.7.1,Decoder state
0.7.1,Build the RNN.
0.7.1,Set up the context gate.
0.7.1,Set up the standard attention.
0.7.1,"Set up a separated copy attention layer, if needed."
0.7.1,The encoder hidden is  (layers*directions) x batch x dim.
0.7.1,We need to convert it to layers x batch x (directions*dim).
0.7.1,Init the input feed.
0.7.1,Run the forward pass of the RNN.
0.7.1,Update the state with the result.
0.7.1,Concatenates sequence of tensors along a new dimension.
0.7.1,NOTE: v0.3 to 0.4: dec_outs / attns[*] may not be list
0.7.1,(in particular in case of SRU) it was not raising error in 0.3
0.7.1,since stack(Variable) was allowed.
0.7.1,"In 0.4, SRU returns a tensor that shouldn't be stacke"
0.7.1,TODO change the way attns is returned dict => list or tuple (onnx)
0.7.1,Initialize local and return variables.
0.7.1,Run the forward pass of the RNN.
0.7.1,Check
0.7.1,END
0.7.1,Calculate the attention.
0.7.1,Calculate the context gate.
0.7.1,Additional args check.
0.7.1,END Additional args check.
0.7.1,Initialize local and return variables.
0.7.1,Input feed concatenates hidden state with
0.7.1,input at every time step.
0.7.1,TODO: context gate should be employed
0.7.1,instead of second RNN transform.
0.7.1,Update the coverage attention.
0.7.1,Run the forward pass of the copy attention layer.
0.7.1,Return result.
0.7.1,Basic attributes.
0.7.1,Decoder State
0.7.1,Build the CNN.
0.7.1,CNNDecoder has its own attention mechanism.
0.7.1,"Set up a separated copy attention layer, if needed."
0.7.1,NOTE: memory_lengths is only here for compatibility reasons
0.7.1,with onmt.modules.RNNDecoderBase.forward()
0.7.1,Initialize return variables.
0.7.1,The output of CNNEncoder.
0.7.1,The combination of output of CNNEncoder and source embeddings.
0.7.1,Run the forward pass of the CNNDecoder.
0.7.1,Process the result and update the attentions.
0.7.1,Update the state.
0.7.1,TODO change the way attns is returned dict => list or tuple (onnx)
0.7.1,Memory_lengths is a single tensor shared between all models.
0.7.1,This assumption will not hold if Translator is modified
0.7.1,to calculate memory_lengths as something other than the length
0.7.1,of the input.
0.7.1,"Register self.mask as a buffer in TransformerDecoderLayer, so"
0.7.1,it gets TransformerDecoderLayer's cuda behavior automatically.
0.7.1,Basic attributes.
0.7.1,Decoder State
0.7.1,Build TransformerDecoder.
0.7.1,TransformerDecoder has its own attention mechanism.
0.7.1,"Set up a separated copy attention layer, if needed."
0.7.1,Initialize return variables.
0.7.1,Run the forward pass of the TransformerDecoder.
0.7.1,Process the result and update the attentions.
0.7.1,TODO change the way attns is returned dict => list or tuple (onnx)
0.7.1,"buffer size in bytes, determine equiv. # of elements based on data type"
0.7.1,copy tensors into buffer_t
0.7.1,all-reduce and rescale
0.7.1,copy all-reduced buffer back into tensors
0.7.1,"tensor is bigger than buffer, all-reduce and rescale directly"
0.7.1,"buffer is full, all-reduce and replace buffer with grad"
0.7.1,add tensor to buffer
0.7.1,TODO: Find a better way to check for sparse gradients.
0.7.1,Load everything from the checkpoint.
0.7.1,Build everything from scratch.
0.7.1,"Reset optimizer, keep options."
0.7.1,"Reset options, keep optimizer."
0.7.1,State can be partially restored.
0.7.1,Code below is an implementation of https://arxiv.org/pdf/1804.04235.pdf
0.7.1,inspired but modified from https://github.com/DeadAt0m/adafactor-pytorch
0.7.1,-*- coding: utf-8 -*-
0.7.1,if the loss function operates on vectors of raw logits instead of
0.7.1,"probabilities, only the first part of the generator needs to be"
0.7.1,"passed to the NMTLossCompute. At the moment, the only supported"
0.7.1,loss function of this kind is the sparsemax loss.
0.7.1,non_none: the subdict of the state dictionary where the values
0.7.1,are not None.
0.7.1,"Now, the iteration:"
0.7.1,state is a dictionary of sequences of tensor-like but we
0.7.1,want a sequence of dictionaries of tensors.
0.7.1,"First, unzip the dictionary into a sequence of keys and a"
0.7.1,sequence of tensor-like sequences.
0.7.1,"Now, yield a dictionary for each shard. The keys are always"
0.7.1,the same. values is a sequence of length #keys where each
0.7.1,element is a sequence of length #shards. We want to iterate
0.7.1,"over the shards, not over the keys: therefore, the values need"
0.7.1,to be re-zipped by shard and then each shard can be paired
0.7.1,with the keys.
0.7.1,Log the progress using the number of batches on the x-axis.
0.7.1,Get a list of world_size lists with len(stat_list) Statistics objects
0.7.1,SRU doesn't support PackedSequence.
0.7.1,-*- coding: utf-8 -*-
0.7.1,this one is needed for torchtext random call (shuffled iterator)
0.7.1,in multi gpu it ensures datasets are read in the same order
0.7.1,some cudnn methods can be random even after fixing the seed
0.7.1,unless you tell it to be deterministic
0.7.1,These ensure same initialization in multi gpu mode
0.7.1,coding: utf-8
0.7.1,several data readers need optional dependencies. There's no
0.7.1,appropriate builtin exception
0.7.1,self.src_vocabs is used in collapse_copy_scores and Translator.py
0.7.1,the dataset's self.fields should have the same attributes as examples
0.7.1,avoid infinite recursion when fields isn't defined
0.7.1,make a small vocab containing just the tokens in the source sequence
0.7.1,Map source tokens to indices in the dynamic dict.
0.7.1,-*- coding: utf-8 -*-
0.7.1,backwards compatibility
0.7.1,"cat together layers, producing a 3d output tensor for src text"
0.7.1,and for tgt (which is assumed to be text)
0.7.1,"there is a truncate argument as well, but it was never set to"
0.7.1,anything besides None before
0.7.1,the second conjunct means nothing will be filtered at translation time
0.7.1,if there is no target data
0.7.1,this is basically copy-pasted from torchtext.
0.7.1,Load vocabulary
0.7.1,keep the order of tokens specified in the vocab file by
0.7.1,adding them to the counter with decreasing counting values
0.7.1,Drop the none-using from memory but keep the last
0.7.1,`tgt_vocab_size` is ignored when sharing vocabularies
0.7.1,"in the long run, shouldn't it be possible to do this by calling"
0.7.1,build_vocab with both the src and tgt data?
0.7.1,temporary fix: See #1196
0.7.1,Maintains the longest src and tgt length in the current batch
0.7.1,Reset current longest length at a new batch (count=1)
0.7.1,Src: <bos> w1 ... wN <eos>
0.7.1,Tgt: w1 ... wN <eos>
0.7.1,-*- coding: utf-8 -*-
0.7.1,imports of datatype-specific dependencies
0.7.1,torchaudio loading options recently changed. It's probably
0.7.1,straightforward to rewrite the audio handling to make use of
0.7.1,"up-to-date torchaudio, but in the meantime there is a legacy"
0.7.1,method which uses the old defaults
0.7.1,STFT
0.7.1,-*- coding: utf-8 -*-
0.7.1,domain specific dependencies
0.7.1,-*- coding: utf-8 -*-
0.7.1,mix this with partial
0.7.1,flake8: noqa
0.7.1,For command-line option parsing
0.7.1,"Check pass, set the args."
0.7.1,"This SRU version implements its own cuda-level optimization,"
0.7.1,so it requires that:
0.7.1,1. `cupy` and `pynvrtc` python package installed.
0.7.1,2. pytorch is built with cuda support.
0.7.1,3. library path set: export LD_LIBRARY_PATH=<cuda lib path>.
0.7.1,Check 1.
0.7.1,Check 2.
0.7.1,Check 3.
0.7.1,This sets up device to use.
0.7.1,-> directions x batch x dim
0.7.1,For DEBUG
0.7.1,"size = (length, batch, x.size(-1)) \"
0.7.1,"if x.dim() == 3 else (batch, x.size(-1))"
0.7.1,grad_x = x.new(*x.size()) if k_ == 3 else x.new(*size).zero_()
0.7.1,Normal use
0.7.1,"An entry check here, will catch on train side and translate side"
0.7.1,if requirements are not satisfied.
0.7.1,RNNDecoderState wraps hidden as a tuple.
0.7.1,fh -> (layers*directions) x batch x dim
0.7.1,The score for each translation on the beam.
0.7.1,The backpointers at each time-step.
0.7.1,The outputs at each time-step.
0.7.1,Has EOS topped the beam yet.
0.7.1,The attentions (matrix) for each time.
0.7.1,Time and k pair for finished.
0.7.1,Information for global scoring.
0.7.1,Minimum prediction length
0.7.1,Apply Penalty at every step
0.7.1,force the output to be longer than self.min_length
0.7.1,Sum the previous scores.
0.7.1,Don't let EOS have children.
0.7.1,Block ngram repeats
0.7.1,"Last n tokens, n = block_ngram_repeat"
0.7.1,Skip the blocking if it is in the exclusion list
0.7.1,"best_scores_id is flattened beam x word array, so calculate which"
0.7.1,word and beam each score came from
0.7.1,End condition is when top-of-beam is EOS and no global score.
0.7.1,Add from beam until we have minimum outputs.
0.7.1,Term will be subtracted from probability
0.7.1,Probability will be divided by this
0.7.1,!/usr/bin/env python
0.7.1,for debugging
0.7.1,Statistics
0.7.1,"For temp=0.0, take the argmax to avoid divide-by-zero errors."
0.7.1,keep_topk=1 is also equivalent to argmax.
0.7.1,Set all logits that are not in the top-k to -1000.
0.7.1,This puts the probabilities close to 0.
0.7.1,TODO: support these blacklisted features.
0.7.1,Encoder forward.
0.7.1,"seq_so_far contains chosen tokens; on each step, dim 1 grows by one."
0.7.1,Note that what this code calls log_probs are actually logits.
0.7.1,Append last prediction.
0.7.1,"Store finished hypotheses for this batch. Unlike in beam search,"
0.7.1,there will only ever be 1 hypothesis per example.
0.7.1,Turn any copied words into UNKs.
0.7.1,"Decoder forward, takes [tgt_len, batch, nfeats] as input"
0.7.1,"and [src_len, batch, hidden] as memory_bank"
0.7.1,"in case of inference tgt_len = 1, batch = beam times batch_size"
0.7.1,"in case of Gold Scoring tgt_len = actual length, batch = 1 batch"
0.7.1,Generator forward.
0.7.1,"returns [(batch_size x beam_size) , vocab ] when 1 step"
0.7.1,"or [ tgt_len, batch_size, vocab ] when full sentence"
0.7.1,"here we have scores [tgt_lenxbatch, vocab] or [beamxbatch, vocab]"
0.7.1,"returns [(batch_size x beam_size) , vocab ] when 1 step"
0.7.1,"or [ tgt_len, batch_size, vocab ] when full sentence"
0.7.1,TODO: support these blacklisted features.
0.7.1,Encoder forward.
0.7.1,Tile states and memory beam_size times.
0.7.1,Give full probability to the first beam on the first step.
0.7.1,Structure that holds finished hypotheses.
0.7.1,Multiply probs by the beam probability.
0.7.1,Flatten probs into a list of possibilities.
0.7.1,Recover log probs.
0.7.1,Resolve beam origin and true word ids.
0.7.1,Map beam_index to batch_index in the flat representation.
0.7.1,Append last prediction.
0.7.1,Save finished hypotheses.
0.7.1,Penalize beams that finished.
0.7.1,Store finished hypotheses for this batch.
0.7.1,End condition is the top beam finished and we can return
0.7.1,n_best hypotheses.
0.7.1,"If all sentences are translated, no need to go further."
0.7.1,Remove finished batches for the next step.
0.7.1,Reorder states.
0.7.1,(0) Prep each of the components of the search.
0.7.1,And helper method for reducing verbosity.
0.7.1,Define a set of tokens to exclude from ngram-blocking
0.7.1,(1) Run the encoder on the src.
0.7.1,(2) Repeat src objects `beam_size` times.
0.7.1,We use now  batch_size x beam_size (same as fast mode)
0.7.1,"(3) run the decoder to generate sentences, using beam search."
0.7.1,(a) Construct batch x beam_size nxt words.
0.7.1,Get all the pending current beam words and arrange for forward.
0.7.1,(b) Decode and forward
0.7.1,(c) Advance each beam.
0.7.1,Loop over the batch_size number of beam
0.7.1,(4) Extract sentences from beam.
0.7.1,Rollback pointer to the beginning.
0.7.1,!/usr/bin/env python
0.7.1,backwards compatibility for confs
0.7.1,load can be called multiple times: modify copy
0.7.1,NOTE: translator returns lists of `n_best` list
0.7.1,we can ignore that (i.e. flatten lists) only because
0.7.1,we restrict `n_best=1`
0.7.1,build back results with empty texts
0.7.1,Sorting
0.7.0,!/usr/bin/env python
0.7.0,!/usr/bin/env python
0.7.0,!/usr/bin/env python
0.7.0,-*- coding: utf-8 -*-
0.7.0,!/usr/bin/env python
0.7.0,-*- coding: utf-8 -*-
0.7.0,!/usr/bin/env python
0.7.0,Create a thread to listen for errors in the child processes.
0.7.0,Train with multiprocessing.
0.7.0,"propagate exception to parent process, keeping original traceback"
0.7.0,!/usr/bin/env python3
0.7.0,-*- coding: utf-8 -*-
0.7.0,
0.7.0,"OpenNMT-py documentation build configuration file, created by"
0.7.0,sphinx-quickstart on Sun Dec 17 12:07:14 2017.
0.7.0,
0.7.0,This file is execfile()d with the current directory set to its
0.7.0,containing dir.
0.7.0,
0.7.0,Note that not all possible configuration values are present in this
0.7.0,autogenerated file.
0.7.0,
0.7.0,All configuration values have a default; values that are commented out
0.7.0,serve to show the default.
0.7.0,"If extensions (or modules to document with autodoc) are in another directory,"
0.7.0,add these directories to sys.path here. If the directory is relative to the
0.7.0,"documentation root, use os.path.abspath to make it absolute, like shown here."
0.7.0,
0.7.0,import os
0.7.0,import sys
0.7.0,"sys.path.insert(0, os.path.abspath('.'))"
0.7.0,-- General configuration ------------------------------------------------
0.7.0,"If your documentation needs a minimal Sphinx version, state it here."
0.7.0,
0.7.0,needs_sphinx = '1.0'
0.7.0,"Add any Sphinx extension module names here, as strings. They can be"
0.7.0,extensions coming with Sphinx (named 'sphinx.ext.*') or your custom
0.7.0,ones.
0.7.0,"Add any paths that contain templates here, relative to this directory."
0.7.0,The suffix(es) of source filenames.
0.7.0,You can specify multiple suffix as a list of string:
0.7.0,
0.7.0,"source_suffix = ['.rst', '.md']"
0.7.0,The master toctree document.
0.7.0,General information about the project.
0.7.0,"The version info for the project you're documenting, acts as replacement for"
0.7.0,"|version| and |release|, also used in various other places throughout the"
0.7.0,built documents.
0.7.0,
0.7.0,The short X.Y version.
0.7.0,"The full version, including alpha/beta/rc tags."
0.7.0,The language for content autogenerated by Sphinx. Refer to documentation
0.7.0,for a list of supported languages.
0.7.0,
0.7.0,This is also used if you do content translation via gettext catalogs.
0.7.0,"Usually you set ""language"" from the command line for these cases."
0.7.0,"List of patterns, relative to source directory, that match files and"
0.7.0,directories to ignore when looking for source files.
0.7.0,This patterns also effect to html_static_path and html_extra_path
0.7.0,The name of the Pygments (syntax highlighting) style to use.
0.7.0,"If true, `todo` and `todoList` produce output, else they produce nothing."
0.7.0,-- Options for HTML output ----------------------------------------------
0.7.0,The theme to use for HTML and HTML Help pages.  See the documentation for
0.7.0,a list of builtin themes.
0.7.0,
0.7.0,html_theme = 'sphinx_materialdesign_theme'
0.7.0,html_theme_path = [sphinx_materialdesign_theme.get_path()]
0.7.0,Theme options are theme-specific and customize the look and feel of a theme
0.7.0,"further.  For a list of options available for each theme, see the"
0.7.0,documentation.
0.7.0,
0.7.0,html_theme_options = {}
0.7.0,"Add any paths that contain custom static files (such as style sheets) here,"
0.7.0,"relative to this directory. They are copied after the builtin static files,"
0.7.0,"so a file named ""default.css"" will overwrite the builtin ""default.css""."
0.7.0,"Custom sidebar templates, must be a dictionary that maps document names"
0.7.0,to template names.
0.7.0,
0.7.0,This is required for the alabaster theme
0.7.0,refs: http://alabaster.readthedocs.io/en/latest/installation.html#sidebars
0.7.0,-- Options for HTMLHelp output ------------------------------------------
0.7.0,Output file base name for HTML help builder.
0.7.0,-- Options for LaTeX output ---------------------------------------------
0.7.0,The paper size ('letterpaper' or 'a4paper').
0.7.0,
0.7.0,"'papersize': 'letterpaper',"
0.7.0,"The font size ('10pt', '11pt' or '12pt')."
0.7.0,
0.7.0,"'pointsize': '10pt',"
0.7.0,Additional stuff for the LaTeX preamble.
0.7.0,
0.7.0,"'preamble': '',"
0.7.0,Latex figure (float) alignment
0.7.0,
0.7.0,"'figure_align': 'htbp',"
0.7.0,Grouping the document tree into LaTeX files. List of tuples
0.7.0,"(source start file, target name, title,"
0.7.0,"author, documentclass [howto, manual, or own class])."
0.7.0,-- Options for manual page output ---------------------------------------
0.7.0,One entry per manual page. List of tuples
0.7.0,"(source start file, name, description, authors, manual section)."
0.7.0,-- Options for Texinfo output -------------------------------------------
0.7.0,Grouping the document tree into Texinfo files. List of tuples
0.7.0,"(source start file, target name, title, author,"
0.7.0,"dir menu entry, description, category)"
0.7.0,!/usr/bin/env python
0.7.0,-*- coding: utf-8 -*-
0.7.0,"the vocab object is a list of tuple (name, torchtext.Vocab)"
0.7.0,we iterate over this list and associate vocabularies based on the name
0.7.0,!/usr/bin/env python
0.7.0,-*- coding: utf-8 -*-
0.7.0,"Add in default model arguments, possibly added since training."
0.7.0,"the vocab object is a list of tuple (name, torchtext.Vocab)"
0.7.0,we iterate over this list and associate vocabularies based on the name
0.7.0,-*- encoding: utf-8 -*-
0.7.0,!/usr/bin/env python
0.7.0,-*- coding: utf-8 -*-
0.7.0,Author: Rico Sennrich
0.7.0,flake8: noqa
0.7.0,This file is retrieved from https://github.com/rsennrich/subword-nmt
0.7.0,hack for python2/3 compatibility
0.7.0,check version information
0.7.0,some hacking to deal with duplicates (only consider first instance)
0.7.0,don't print end-of-word symbols
0.7.0,sys.stderr.write('cannot split {0} further.\n'.format(segment))
0.7.0,sys.stderr.write('OOV: {0}\n'.format(segment))
0.7.0,sys.stderr.write('OOV: {0}\n'.format(segment))
0.7.0,python 2/3 compatibility
0.7.0,read/write files as UTF-8
0.7.0,!/usr/bin/env python
0.7.0,!/usr/bin/env python
0.7.0,-*- coding: utf-8 -*-
0.7.0,Author: Rico Sennrich
0.7.0,flake8: noqa
0.7.0,This file is retrieved from https://github.com/rsennrich/subword-nmt
0.7.0,hack for python2/3 compatibility
0.7.0,"find all instances of pair, and update frequency/indices around it"
0.7.0,find first symbol
0.7.0,"if first symbol is followed by second symbol, we've found an occurrence of pair (old_word[i:i+2])"
0.7.0,"assuming a symbol sequence ""A B C"", if ""B C"" is merged, reduce the frequency of ""A B"""
0.7.0,"assuming a symbol sequence ""A B C B"", if ""B C"" is merged, reduce the frequency of ""C B""."
0.7.0,"however, skip this if the sequence is A B C B C, because the frequency of ""C B"" will be reduced by the previous code block"
0.7.0,find new pair
0.7.0,"assuming a symbol sequence ""A BC D"", if ""B C"" is merged, increase the frequency of ""A BC"""
0.7.0,"assuming a symbol sequence ""A BC B"", if ""B C"" is merged, increase the frequency of ""BC B"""
0.7.0,"however, if the sequence is A BC BC, skip this step because the count of ""BC BC"" will be incremented by the previous code block"
0.7.0,data structure of pair frequencies
0.7.0,index from pairs to words
0.7.0,version 0.2 changes the handling of the end-of-word token ('</w>');
0.7.0,version numbering allows bckward compatibility
0.7.0,"threshold is inspired by Zipfian assumption, but should only affect speed"
0.7.0,we probably missed the best pair because of pruning; go back to full statistics
0.7.0,"threshold is inspired by Zipfian assumption, but should only affect speed"
0.7.0,python 2/3 compatibility
0.7.0,read/write files as UTF-8
0.7.0,!/usr/bin/env python
0.7.0,for backward compatibility
0.7.0,Build encoder.
0.7.0,why is build_encoder not used here?
0.7.0,why is the model_opt.__dict__ check necessary?
0.7.0,Build decoder.
0.7.0,Share the embedding matrix - preprocess with share_vocab required.
0.7.0,src/tgt vocab should be the same if `-share_vocab` is specified.
0.7.0,Build NMTModel(= encoder + decoder).
0.7.0,Build Generator.
0.7.0,Load the model states from checkpoint or initialize them.
0.7.0,This preserves backward-compat for models using customed layernorm
0.7.0,end of patch for backward compatibility
0.7.0,!/usr/bin/env python
0.7.0,this check is here because audio allows the encoder and decoder to
0.7.0,"be different sizes, but other model types do not yet"
0.7.0,this one is needed for torchtext random call (shuffled iterator)
0.7.0,in multi gpu it ensures datasets are read in the same order
0.7.0,some cudnn methods can be random even after fixing the seed
0.7.0,unless you tell it to be deterministic
0.7.0,These ensure same initialization in multi gpu mode
0.7.0,Load checkpoint if we resume from a previous training.
0.7.0,Load default opts values then overwrite it with opts from
0.7.0,the checkpoint. It's usefull in order to re-train a model
0.7.0,after adding a new option (not set in checkpoint)
0.7.0,Load a shard dataset to determine the data_type.
0.7.0,(All datasets have the same data_type).
0.7.0,this should be refactored out of existence reasonably soon
0.7.0,Load fields generated from preprocess phase.
0.7.0,Report src/tgt features.
0.7.0,Build model.
0.7.0,Build optimizer.
0.7.0,Build model saver
0.7.0,Embedding Options
0.7.0,Encoder-Decoder Options
0.7.0,"group.add('--residual', '-residual',   action=""store_true"","
0.7.0,"help=""Add residual connections between RNN layers."")"
0.7.0,Attention options
0.7.0,Generator and loss options.
0.7.0,Data options
0.7.0,"Dictionary options, for text corpus"
0.7.0,"Truncation options, for text corpus"
0.7.0,Data processing options
0.7.0,Options most relevant to speech
0.7.0,Option most relevant to image input
0.7.0,GPU
0.7.0,Init options
0.7.0,Pretrained word vectors
0.7.0,Fixed word vectors
0.7.0,Optimization options
0.7.0,learning rate
0.7.0,Use TensorboardX for visualization during training
0.7.0,Options most relevant to speech
0.7.0,Option most relevant to image input
0.7.0,Options most relevant to summarization.
0.7.0,Alpha and Beta values for Google Length + Coverage penalty
0.7.0,"Described here: https://arxiv.org/pdf/1609.08144.pdf, Section 7"
0.7.0,Options most relevant to speech.
0.7.0,Option most relevant to image input
0.7.0,MARKDOWN boilerplate
0.7.0,Copyright 2016 The Chromium Authors. All rights reserved.
0.7.0,Use of this source code is governed by a BSD-style license that can be
0.7.0,found in the LICENSE file.
0.7.0,**section heading**:
0.7.0,# **--argument-one**
0.7.0,"Get the key 'value' in the dict, or just use 'value'"
0.7.0,Basic attributes.
0.7.0,Set model in training mode.
0.7.0,Set model in validating mode.
0.7.0,F-prop through the model.
0.7.0,Compute loss.
0.7.0,Update statistics.
0.7.0,Set model back to training mode.
0.7.0,Truncated BPTT: reminder not compatible with accum > 1
0.7.0,dec_state = None
0.7.0,1. Create truncated target.
0.7.0,2. F-prop all but generator.
0.7.0,3. Compute loss in shards for memory efficiency.
0.7.0,4. Update the parameters and statistics.
0.7.0,Multi GPU gradient gather
0.7.0,"If truncated, don't backprop fully."
0.7.0,TO CHECK
0.7.0,if dec_state is not None:
0.7.0,dec_state.detach()
0.7.0,"in case of multi step gradient accumulation,"
0.7.0,update only after accum batches
0.7.0,For Flake
0.7.0,Initialize the bridge layer
0.7.0,"s_len, batch, emb_dim = emb.size()"
0.7.0,Lengths data is wrapped inside a Tensor.
0.7.0,"LSTM has hidden and cell state, other only one"
0.7.0,Total number of states
0.7.0,Build a linear layer for each
0.7.0,The encoder hidden is  (layers*directions) x batch x dim.
0.7.0,"s_len, batch, emb_dim = emb.size()"
0.7.0,from onmt.utils.misc import aeq
0.7.0,Run the forward pass of every layer of the tranformer.
0.7.0,"(batch_size, 64, imgH, imgW)"
0.7.0,layer 1
0.7.0,"(batch_size, 64, imgH/2, imgW/2)"
0.7.0,"(batch_size, 128, imgH/2, imgW/2)"
0.7.0,layer 2
0.7.0,"(batch_size, 128, imgH/2/2, imgW/2/2)"
0.7.0,"(batch_size, 256, imgH/2/2, imgW/2/2)"
0.7.0,layer 3
0.7.0,batch norm 1
0.7.0,"(batch_size, 256, imgH/2/2, imgW/2/2)"
0.7.0,layer4
0.7.0,"(batch_size, 256, imgH/2/2/2, imgW/2/2)"
0.7.0,"(batch_size, 512, imgH/2/2/2, imgW/2/2)"
0.7.0,layer 5
0.7.0,batch norm 2
0.7.0,"(batch_size, 512, imgH/2/2/2, imgW/2/2/2)"
0.7.0,"(batch_size, 512, imgH/2/2/2, imgW/2/2/2)"
0.7.0,"# (batch_size, 512, H, W)"
0.7.0,Dimensions and padding for constructing the word embedding matrix
0.7.0,Dimensions and padding for feature embedding matrices
0.7.0,(these have no effect if feat_vocab_sizes is empty)
0.7.0,The embedding matrix look-up tables. The first look-up table
0.7.0,"is for words. Subsequent ones are for features, if any exist."
0.7.0,The final output size of word + feature vectors. This can vary
0.7.0,from the word vector size if and only if features are defined.
0.7.0,This is the attribute you should access if you need to know
0.7.0,how big your embeddings are going to be.
0.7.0,The sequence of operations that converts the input sequence
0.7.0,into a sequence of embeddings. At minimum this consists of
0.7.0,looking up the embeddings for each word and feature in the
0.7.0,input. Model parameters may require the sequence to contain
0.7.0,additional operations as well.
0.7.0,This class is mainly used by decoder.py for RNNs but also
0.7.0,by the CNN / transformer decoder when copy attention is used
0.7.0,CNN has its own attention mechanism ConvMultiStepAttention
0.7.0,Transformer has its own MultiHeadedAttention
0.7.0,mlp wants it with bias
0.7.0,Check input sizes
0.7.0,"(batch, t_len, d) x (batch, d, s_len) --> (batch, t_len, s_len)"
0.7.0,"(batch, t_len, s_len, d)"
0.7.0,one step input
0.7.0,"compute attention scores, as in Luong et al."
0.7.0,Softmax or sparsemax to normalize attention weights
0.7.0,each context vector c_t is the weighted average
0.7.0,over all the source hidden states
0.7.0,concatenate
0.7.0,Check output sizes
0.7.0,Check output sizes
0.7.0,clamping necessary because of numerical errors: loss should be lower
0.7.0,"bounded by zero, but negative values near zero are possible without"
0.7.0,the clamp
0.7.0,from onmt.utils.misc import aeq
0.7.0,CHECKS
0.7.0,"batch, k_len, d = key.size()"
0.7.0,"batch_, k_len_, d_ = value.size()"
0.7.0,"aeq(batch, batch_)"
0.7.0,"aeq(k_len, k_len_)"
0.7.0,"aeq(d, d_)"
0.7.0,"batch_, q_len, d_ = query.size()"
0.7.0,"aeq(batch, batch_)"
0.7.0,"aeq(d, d_)"
0.7.0,"aeq(self.model_dim % 8, 0)"
0.7.0,if mask is not None:
0.7.0,"batch_, q_len_, k_len_ = mask.size()"
0.7.0,"aeq(batch_, batch)"
0.7.0,"aeq(k_len_, k_len)"
0.7.0,aeq(q_len_ == q_len)
0.7.0,END CHECKS
0.7.0,"1) Project key, value, and query."
0.7.0,2) Calculate and scale scores.
0.7.0,3) Apply attention dropout and compute context vectors.
0.7.0,CHECK
0.7.0,"batch_, q_len_, d_ = output.size()"
0.7.0,"aeq(q_len, q_len_)"
0.7.0,"aeq(batch, batch_)"
0.7.0,"aeq(d, d_)"
0.7.0,Return one attn
0.7.0,At the moment this class is only used by embeddings.Embeddings look-up tables
0.7.0,-*- coding: utf-8 -*-
0.7.0,checks
0.7.0,"batch, channel, height, width = base_target_emb.size()"
0.7.0,"batch_, channel_, height_, width_ = input_from_dec.size()"
0.7.0,"enc_batch, enc_channel, enc_height = encoder_out_top.size()"
0.7.0,"enc_batch_, enc_channel_, enc_height_ = encoder_out_combine.size()"
0.7.0,out_features * in_features
0.7.0,norm is out_features * 1
0.7.0,batch_size * out_features
0.7.0,out_features
0.7.0,out_features
0.7.0,batch_size * out_features
0.7.0,"out_channels, in_channels // groups, * kernel_size"
0.7.0,out_features
0.7.0,This is used nowhere in the code at the moment (Vincent Nguyen 05/18/2018)
0.7.0,"in_channels, out_channels, *kernel_size"
0.7.0,"in_channels, out_channels, *kernel_size"
0.7.0,"self.out_channels, 1"
0.7.0,out_features
0.7.0,out_features
0.7.0,store roots on diagonal
0.7.0,CHECKS
0.7.0,Original probabilities.
0.7.0,Probability of copying p(z=1) batch.
0.7.0,Probability of not copying: p_{word}(w) * (1 - p(z))
0.7.0,probabilities assigned by the model to the gold targets
0.7.0,probability of tokens copied from source
0.7.0,Set scores for unk to 0 and add eps
0.7.0,find the indices in which you do not use the copy mechanism
0.7.0,Drop padding.
0.7.0,this block does not depend on the loss value computed above
0.7.0,and is used only for stats
0.7.0,this block does not depend on the loss value computed above
0.7.0,and is used only for stats
0.7.0,Correct target copy token instead of <unk>
0.7.0,tgt[i] = align[i] + len(tgt_vocab)
0.7.0,for i such that tgt[i] == 0 and align[i] != 0
0.7.0,Compute sum of perplexities for stats
0.7.0,this part looks like it belongs in CopyGeneratorLoss
0.7.0,Compute Loss as NLL divided by seq length
0.7.0,Compute Total Loss per sequence in batch
0.7.0,Divide by length of each sequence and sum
0.7.0,illegal_weights_mask = torch.ByteTensor([
0.7.0,"[0, 0, 0, 0, 0, 0, 0],"
0.7.0,"[0, 0, 0, 1, 1, 1, 1],"
0.7.0,"[0, 0, 0, 0, 0, 1, 1],"
0.7.0,"[0, 0, 1, 1, 1, 1, 1]])"
0.7.0,TODO: fix for pytorch 0.3
0.7.0,illegal_weights = alignments.masked_select(illegal_weights_mask)
0.7.0,"self.assertEqual(0.0, illegal_weights.data.sum())"
0.7.0,"-data option is required, but not used in this test, so dummy."
0.7.0,len x batch x nfeat
0.7.0,batch x c x h x w
0.7.0,batch x 1 x nfft x t
0.7.0,Initialize vectors to compare size with
0.7.0,Ensure correct sizes and types
0.7.0,Make sure that output has the correct size and type
0.7.0,Make sure that output has the correct size and type
0.7.0,Make sure that output has the correct size and type
0.7.0,"[('encoder_type', 'transformer'),"
0.7.0,"('word_vec_size', 16), ('rnn_size', 16)],"
0.7.0,""""""" Only do SRU test if requirment is safisfied. """""""
0.7.0,SRU doesn't support input_feed.
0.7.0,!/usr/bin/env python
0.7.0,-*- coding: utf-8 -*-
0.7.0,Remove the generated *pt files.
0.7.0,Test image preprocessing
0.7.0,Test audio preprocessing
0.7.0,Basic attributes.
0.7.0,Decoder state
0.7.0,Build the RNN.
0.7.0,Set up the context gate.
0.7.0,Set up the standard attention.
0.7.0,"Set up a separated copy attention layer, if needed."
0.7.0,The encoder hidden is  (layers*directions) x batch x dim.
0.7.0,We need to convert it to layers x batch x (directions*dim).
0.7.0,Init the input feed.
0.7.0,Run the forward pass of the RNN.
0.7.0,Update the state with the result.
0.7.0,Concatenates sequence of tensors along a new dimension.
0.7.0,NOTE: v0.3 to 0.4: dec_outs / attns[*] may not be list
0.7.0,(in particular in case of SRU) it was not raising error in 0.3
0.7.0,since stack(Variable) was allowed.
0.7.0,"In 0.4, SRU returns a tensor that shouldn't be stacke"
0.7.0,TODO change the way attns is returned dict => list or tuple (onnx)
0.7.0,Initialize local and return variables.
0.7.0,Run the forward pass of the RNN.
0.7.0,Check
0.7.0,END
0.7.0,Calculate the attention.
0.7.0,Calculate the context gate.
0.7.0,Additional args check.
0.7.0,END Additional args check.
0.7.0,Initialize local and return variables.
0.7.0,Input feed concatenates hidden state with
0.7.0,input at every time step.
0.7.0,TODO: context gate should be employed
0.7.0,instead of second RNN transform.
0.7.0,Update the coverage attention.
0.7.0,Run the forward pass of the copy attention layer.
0.7.0,Return result.
0.7.0,Basic attributes.
0.7.0,Decoder State
0.7.0,Build the CNN.
0.7.0,CNNDecoder has its own attention mechanism.
0.7.0,"Set up a separated copy attention layer, if needed."
0.7.0,NOTE: memory_lengths is only here for compatibility reasons
0.7.0,with onmt.modules.RNNDecoderBase.forward()
0.7.0,Initialize return variables.
0.7.0,The output of CNNEncoder.
0.7.0,The combination of output of CNNEncoder and source embeddings.
0.7.0,Run the forward pass of the CNNDecoder.
0.7.0,Process the result and update the attentions.
0.7.0,Update the state.
0.7.0,TODO change the way attns is returned dict => list or tuple (onnx)
0.7.0,Memory_lengths is a single tensor shared between all models.
0.7.0,This assumption will not hold if Translator is modified
0.7.0,to calculate memory_lengths as something other than the length
0.7.0,of the input.
0.7.0,"Register self.mask as a buffer in TransformerDecoderLayer, so"
0.7.0,it gets TransformerDecoderLayer's cuda behavior automatically.
0.7.0,Basic attributes.
0.7.0,Decoder State
0.7.0,Build TransformerDecoder.
0.7.0,TransformerDecoder has its own attention mechanism.
0.7.0,"Set up a separated copy attention layer, if needed."
0.7.0,Initialize return variables.
0.7.0,Run the forward pass of the TransformerDecoder.
0.7.0,Process the result and update the attentions.
0.7.0,TODO change the way attns is returned dict => list or tuple (onnx)
0.7.0,"buffer size in bytes, determine equiv. # of elements based on data type"
0.7.0,copy tensors into buffer_t
0.7.0,all-reduce and rescale
0.7.0,copy all-reduced buffer back into tensors
0.7.0,"tensor is bigger than buffer, all-reduce and rescale directly"
0.7.0,"buffer is full, all-reduce and replace buffer with grad"
0.7.0,add tensor to buffer
0.7.0,We need to save a copy of optim.optimizer.state_dict() for setting
0.7.0,"the, optimizer state later on in Stage 2 in this method, since"
0.7.0,the method optim.set_parameters(model) will overwrite
0.7.0,"optim.optimizer, and with ith the values stored in"
0.7.0,optim.optimizer.state_dict()
0.7.0,Stage 1:
0.7.0,Essentially optim.set_parameters (re-)creates and optimizer using
0.7.0,model.paramters() as parameters that will be stored in the
0.7.0,optim.optimizer.param_groups field of the torch optimizer class.
0.7.0,"Importantly, this method does not yet load the optimizer state, as"
0.7.0,essentially it builds a new optimizer with empty optimizer state and
0.7.0,parameters from the model.
0.7.0,"Stage 2: In this stage, which is only performed when loading an"
0.7.0,"optimizer from a checkpoint, we load the saved_optimizer_state_dict"
0.7.0,"into the re-created optimizer, to set the optim.optimizer.state"
0.7.0,"field, which was previously empty. For this, we use the optimizer"
0.7.0,"state saved in the ""saved_optimizer_state_dict"" variable for"
0.7.0,this purpose.
0.7.0,See also: https://github.com/pytorch/pytorch/issues/2830
0.7.0,Convert back the state values to cuda type if applicable
0.7.0,We want to make sure that indeed we have a non-empty optimizer state
0.7.0,when we loaded an existing model. This should be at least the case
0.7.0,"for Adam, which saves ""exp_avg"" and ""exp_avg_sq"" state"
0.7.0,(Exponential moving average of gradient and squared gradient values)
0.7.0,TODO: Find a better way to check for sparse gradients.
0.7.0,Decay method used in tensor2tensor.
0.7.0,Decay based on start_decay_steps every decay_steps
0.7.0,Code below is an implementation of https://arxiv.org/pdf/1804.04235.pdf
0.7.0,inspired but modified from https://github.com/DeadAt0m/adafactor-pytorch
0.7.0,default value from paper
0.7.0,-*- coding: utf-8 -*-
0.7.0,if the loss function operates on vectors of raw logits instead of
0.7.0,"probabilities, only the first part of the generator needs to be"
0.7.0,"passed to the NMTLossCompute. At the moment, the only supported"
0.7.0,loss function of this kind is the sparsemax loss.
0.7.0,non_none: the subdict of the state dictionary where the values
0.7.0,are not None.
0.7.0,"Now, the iteration:"
0.7.0,state is a dictionary of sequences of tensor-like but we
0.7.0,want a sequence of dictionaries of tensors.
0.7.0,"First, unzip the dictionary into a sequence of keys and a"
0.7.0,sequence of tensor-like sequences.
0.7.0,"Now, yield a dictionary for each shard. The keys are always"
0.7.0,the same. values is a sequence of length #keys where each
0.7.0,element is a sequence of length #shards. We want to iterate
0.7.0,"over the shards, not over the keys: therefore, the values need"
0.7.0,to be re-zipped by shard and then each shard can be paired
0.7.0,with the keys.
0.7.0,Assumed backprop'd
0.7.0,Log the progress using the number of batches on the x-axis.
0.7.0,Get a list of world_size lists with len(stat_list) Statistics objects
0.7.0,SRU doesn't support PackedSequence.
0.7.0,-*- coding: utf-8 -*-
0.7.0,coding: utf-8
0.7.0,This is a hack. Something is broken with torch pickle.
0.7.0,Each element of an example is a dictionary whose keys represents
0.7.0,at minimum the src tokens and their indices and potentially also
0.7.0,the src and tgt features and alignment information.
0.7.0,self.src_vocabs is used in collapse_copy_scores and in Translator.py
0.7.0,Peek at the first to see which fields are used.
0.7.0,why do we need to use different keys from the ones passed in?
0.7.0,why does this exist?
0.7.0,it would not be necessary to pass unk and pad if the method were
0.7.0,called after fields becomes an attribute of self
0.7.0,Map source tokens to indices in the dynamic dict.
0.7.0,-*- coding: utf-8 -*-
0.7.0,only audio has src_lengths
0.7.0,everything except audio has src_map and alignment
0.7.0,below this: things defined no matter what the data source type is
0.7.0,"there is a truncate argument as well, but it was never set to"
0.7.0,anything besides None before
0.7.0,the second conjunct means nothing will be filtered at translation time
0.7.0,if there is no target data
0.7.0,Prop src from field to get lower memory using when training with image
0.7.0,Load vocabulary
0.7.0,keep the order of tokens specified in the vocab file by
0.7.0,adding them to the counter with decreasing counting values
0.7.0,Drop the none-using from memory but keep the last
0.7.0,"All datasets have same num of n_tgt_features,"
0.7.0,getting the last one is OK.
0.7.0,"All datasets have same num of n_src_features,"
0.7.0,getting the last one is OK.
0.7.0,`tgt_vocab_size` is ignored when sharing vocabularies
0.7.0,"in the long run, shouldn't it be possible to do this by calling"
0.7.0,build_vocab with both the src and tgt data?
0.7.0,Maintains the longest src and tgt length in the current batch
0.7.0,Reset current longest length at a new batch (count=1)
0.7.0,Src: <bos> w1 ... wN <eos>
0.7.0,Tgt: w1 ... wN <eos>
0.7.0,-*- coding: utf-8 -*-
0.7.0,torchaudio loading options recently changed. It's probably
0.7.0,straightforward to rewrite the audio handling to make use of
0.7.0,"up-to-date torchaudio, but in the meantime there is a legacy"
0.7.0,method which uses the old defaults
0.7.0,STFT
0.7.0,-*- coding: utf-8 -*-
0.7.0,-*- coding: utf-8 -*-
0.7.0,the implicit assumption here is that data that does not come
0.7.0,"from a file is already at least semi-tokenized, i.e. split on"
0.7.0,whitespace. We cannot do modular/user-specified tokenization
0.7.0,until that is no longer the case. The fields should handle this.
0.7.0,flake8: noqa
0.7.0,For command-line option parsing
0.7.0,"Check pass, set the args."
0.7.0,"This SRU version implements its own cuda-level optimization,"
0.7.0,so it requires that:
0.7.0,1. `cupy` and `pynvrtc` python package installed.
0.7.0,2. pytorch is built with cuda support.
0.7.0,3. library path set: export LD_LIBRARY_PATH=<cuda lib path>.
0.7.0,Check 1.
0.7.0,Check 2.
0.7.0,Check 3.
0.7.0,This sets up device to use.
0.7.0,-> directions x batch x dim
0.7.0,For DEBUG
0.7.0,"size = (length, batch, x.size(-1)) \"
0.7.0,"if x.dim() == 3 else (batch, x.size(-1))"
0.7.0,grad_x = x.new(*x.size()) if k_ == 3 else x.new(*size).zero_()
0.7.0,Normal use
0.7.0,"An entry check here, will catch on train side and translate side"
0.7.0,if requirements are not satisfied.
0.7.0,RNNDecoderState wraps hidden as a tuple.
0.7.0,fh -> (layers*directions) x batch x dim
0.7.0,The score for each translation on the beam.
0.7.0,The backpointers at each time-step.
0.7.0,The outputs at each time-step.
0.7.0,Has EOS topped the beam yet.
0.7.0,The attentions (matrix) for each time.
0.7.0,Time and k pair for finished.
0.7.0,Information for global scoring.
0.7.0,Minimum prediction length
0.7.0,Apply Penalty at every step
0.7.0,force the output to be longer than self.min_length
0.7.0,Sum the previous scores.
0.7.0,Don't let EOS have children.
0.7.0,Block ngram repeats
0.7.0,"Last n tokens, n = block_ngram_repeat"
0.7.0,Skip the blocking if it is in the exclusion list
0.7.0,"best_scores_id is flattened beam x word array, so calculate which"
0.7.0,word and beam each score came from
0.7.0,End condition is when top-of-beam is EOS and no global score.
0.7.0,Add from beam until we have minimum outputs.
0.7.0,Term will be subtracted from probability
0.7.0,Probability will be divided by this
0.7.0,!/usr/bin/env python
0.7.0,for debugging
0.7.0,Statistics
0.7.0,Turn any copied words into UNKs.
0.7.0,"Decoder forward, takes [tgt_len, batch, nfeats] as input"
0.7.0,"and [src_len, batch, hidden] as memory_bank"
0.7.0,"in case of inference tgt_len = 1, batch = beam times batch_size"
0.7.0,"in case of Gold Scoring tgt_len = actual length, batch = 1 batch"
0.7.0,Generator forward.
0.7.0,"returns [(batch_size x beam_size) , vocab ] when 1 step"
0.7.0,"or [ tgt_len, batch_size, vocab ] when full sentence"
0.7.0,"here we have scores [tgt_lenxbatch, vocab] or [beamxbatch, vocab]"
0.7.0,"returns [(batch_size x beam_size) , vocab ] when 1 step"
0.7.0,"or [ tgt_len, batch_size, vocab ] when full sentence"
0.7.0,TODO: faster code path for beam_size == 1.
0.7.0,TODO: support these blacklisted features.
0.7.0,Encoder forward.
0.7.0,Tile states and memory beam_size times.
0.7.0,Give full probability to the first beam on the first step.
0.7.0,Structure that holds finished hypotheses.
0.7.0,Multiply probs by the beam probability.
0.7.0,Flatten probs into a list of possibilities.
0.7.0,Recover log probs.
0.7.0,Resolve beam origin and true word ids.
0.7.0,Map beam_index to batch_index in the flat representation.
0.7.0,Append last prediction.
0.7.0,Save finished hypotheses.
0.7.0,Penalize beams that finished.
0.7.0,Store finished hypotheses for this batch.
0.7.0,End condition is the top beam finished and we can return
0.7.0,n_best hypotheses.
0.7.0,"If all sentences are translated, no need to go further."
0.7.0,Remove finished batches for the next step.
0.7.0,Reorder states.
0.7.0,(0) Prep each of the components of the search.
0.7.0,And helper method for reducing verbosity.
0.7.0,Define a set of tokens to exclude from ngram-blocking
0.7.0,(1) Run the encoder on the src.
0.7.0,(2) Repeat src objects `beam_size` times.
0.7.0,We use now  batch_size x beam_size (same as fast mode)
0.7.0,"(3) run the decoder to generate sentences, using beam search."
0.7.0,(a) Construct batch x beam_size nxt words.
0.7.0,Get all the pending current beam words and arrange for forward.
0.7.0,(b) Decode and forward
0.7.0,(c) Advance each beam.
0.7.0,Loop over the batch_size number of beam
0.7.0,(4) Extract sentences from beam.
0.7.0,Rollback pointer to the beginning.
0.7.0,!/usr/bin/env python
0.7.0,backwards compatibility for confs
0.7.0,load can be called multiple times: modify copy
0.7.0,NOTE: translator returns lists of `n_best` list
0.7.0,we can ignore that (i.e. flatten lists) only because
0.7.0,we restrict `n_best=1`
0.7.0,build back results with empty texts
0.7.0,Sorting
0.6.0,!/usr/bin/env python
0.6.0,!/usr/bin/env python
0.6.0,!/usr/bin/env python
0.6.0,-*- coding: utf-8 -*-
0.6.0,!/usr/bin/env python
0.6.0,-*- coding: utf-8 -*-
0.6.0,We will use glob.glob() to find sharded {train|valid}.[0-9]*.pt
0.6.0,"when training, so check to avoid tampering with existing pt files"
0.6.0,or mixing them up.
0.6.0,"We save fields in vocab.pt seperately, so make it empty."
0.6.0,"For data_type == 'img' or 'audio', currently we don't do"
0.6.0,preprocess sharding. We only build a monolithic dataset.
0.6.0,"But since the interfaces are uniform, it would be not hard"
0.6.0,to do this should users need this feature.
0.6.0,"We save fields in vocab.pt seperately, so make it empty."
0.6.0,"Can't save fields, so remove/reconstruct at training time."
0.6.0,!/usr/bin/env python
0.6.0,Create a thread to listen for errors in the child processes.
0.6.0,Train with multiprocessing.
0.6.0,"propagate exception to parent process, keeping original traceback"
0.6.0,!/usr/bin/env python3
0.6.0,-*- coding: utf-8 -*-
0.6.0,
0.6.0,"OpenNMT-py documentation build configuration file, created by"
0.6.0,sphinx-quickstart on Sun Dec 17 12:07:14 2017.
0.6.0,
0.6.0,This file is execfile()d with the current directory set to its
0.6.0,containing dir.
0.6.0,
0.6.0,Note that not all possible configuration values are present in this
0.6.0,autogenerated file.
0.6.0,
0.6.0,All configuration values have a default; values that are commented out
0.6.0,serve to show the default.
0.6.0,"If extensions (or modules to document with autodoc) are in another directory,"
0.6.0,add these directories to sys.path here. If the directory is relative to the
0.6.0,"documentation root, use os.path.abspath to make it absolute, like shown here."
0.6.0,
0.6.0,import os
0.6.0,import sys
0.6.0,"sys.path.insert(0, os.path.abspath('.'))"
0.6.0,-- General configuration ------------------------------------------------
0.6.0,"If your documentation needs a minimal Sphinx version, state it here."
0.6.0,
0.6.0,needs_sphinx = '1.0'
0.6.0,"Add any Sphinx extension module names here, as strings. They can be"
0.6.0,extensions coming with Sphinx (named 'sphinx.ext.*') or your custom
0.6.0,ones.
0.6.0,"Add any paths that contain templates here, relative to this directory."
0.6.0,The suffix(es) of source filenames.
0.6.0,You can specify multiple suffix as a list of string:
0.6.0,
0.6.0,"source_suffix = ['.rst', '.md']"
0.6.0,The master toctree document.
0.6.0,General information about the project.
0.6.0,"The version info for the project you're documenting, acts as replacement for"
0.6.0,"|version| and |release|, also used in various other places throughout the"
0.6.0,built documents.
0.6.0,
0.6.0,The short X.Y version.
0.6.0,"The full version, including alpha/beta/rc tags."
0.6.0,The language for content autogenerated by Sphinx. Refer to documentation
0.6.0,for a list of supported languages.
0.6.0,
0.6.0,This is also used if you do content translation via gettext catalogs.
0.6.0,"Usually you set ""language"" from the command line for these cases."
0.6.0,"List of patterns, relative to source directory, that match files and"
0.6.0,directories to ignore when looking for source files.
0.6.0,This patterns also effect to html_static_path and html_extra_path
0.6.0,The name of the Pygments (syntax highlighting) style to use.
0.6.0,"If true, `todo` and `todoList` produce output, else they produce nothing."
0.6.0,-- Options for HTML output ----------------------------------------------
0.6.0,The theme to use for HTML and HTML Help pages.  See the documentation for
0.6.0,a list of builtin themes.
0.6.0,
0.6.0,html_theme = 'sphinx_materialdesign_theme'
0.6.0,html_theme_path = [sphinx_materialdesign_theme.get_path()]
0.6.0,Theme options are theme-specific and customize the look and feel of a theme
0.6.0,"further.  For a list of options available for each theme, see the"
0.6.0,documentation.
0.6.0,
0.6.0,html_theme_options = {}
0.6.0,"Add any paths that contain custom static files (such as style sheets) here,"
0.6.0,"relative to this directory. They are copied after the builtin static files,"
0.6.0,"so a file named ""default.css"" will overwrite the builtin ""default.css""."
0.6.0,"Custom sidebar templates, must be a dictionary that maps document names"
0.6.0,to template names.
0.6.0,
0.6.0,This is required for the alabaster theme
0.6.0,refs: http://alabaster.readthedocs.io/en/latest/installation.html#sidebars
0.6.0,-- Options for HTMLHelp output ------------------------------------------
0.6.0,Output file base name for HTML help builder.
0.6.0,-- Options for LaTeX output ---------------------------------------------
0.6.0,The paper size ('letterpaper' or 'a4paper').
0.6.0,
0.6.0,"'papersize': 'letterpaper',"
0.6.0,"The font size ('10pt', '11pt' or '12pt')."
0.6.0,
0.6.0,"'pointsize': '10pt',"
0.6.0,Additional stuff for the LaTeX preamble.
0.6.0,
0.6.0,"'preamble': '',"
0.6.0,Latex figure (float) alignment
0.6.0,
0.6.0,"'figure_align': 'htbp',"
0.6.0,Grouping the document tree into LaTeX files. List of tuples
0.6.0,"(source start file, target name, title,"
0.6.0,"author, documentclass [howto, manual, or own class])."
0.6.0,-- Options for manual page output ---------------------------------------
0.6.0,One entry per manual page. List of tuples
0.6.0,"(source start file, name, description, authors, manual section)."
0.6.0,-- Options for Texinfo output -------------------------------------------
0.6.0,Grouping the document tree into Texinfo files. List of tuples
0.6.0,"(source start file, target name, title, author,"
0.6.0,"dir menu entry, description, category)"
0.6.0,!/usr/bin/env python
0.6.0,-*- coding: utf-8 -*-
0.6.0,"the vocab object is a list of tuple (name, torchtext.Vocab)"
0.6.0,we iterate over this list and associate vocabularies based on the name
0.6.0,!/usr/bin/env python
0.6.0,-*- coding: utf-8 -*-
0.6.0,"Add in default model arguments, possibly added since training."
0.6.0,"the vocab object is a list of tuple (name, torchtext.Vocab)"
0.6.0,we iterate over this list and associate vocabularies based on the name
0.6.0,-*- encoding: utf-8 -*-
0.6.0,!/usr/bin/env python
0.6.0,-*- coding: utf-8 -*-
0.6.0,Author: Rico Sennrich
0.6.0,flake8: noqa
0.6.0,This file is retrieved from https://github.com/rsennrich/subword-nmt
0.6.0,hack for python2/3 compatibility
0.6.0,check version information
0.6.0,some hacking to deal with duplicates (only consider first instance)
0.6.0,don't print end-of-word symbols
0.6.0,sys.stderr.write('cannot split {0} further.\n'.format(segment))
0.6.0,sys.stderr.write('OOV: {0}\n'.format(segment))
0.6.0,sys.stderr.write('OOV: {0}\n'.format(segment))
0.6.0,python 2/3 compatibility
0.6.0,read/write files as UTF-8
0.6.0,!/usr/bin/env python
0.6.0,!/usr/bin/env python
0.6.0,-*- coding: utf-8 -*-
0.6.0,Author: Rico Sennrich
0.6.0,flake8: noqa
0.6.0,This file is retrieved from https://github.com/rsennrich/subword-nmt
0.6.0,hack for python2/3 compatibility
0.6.0,"find all instances of pair, and update frequency/indices around it"
0.6.0,find first symbol
0.6.0,"if first symbol is followed by second symbol, we've found an occurrence of pair (old_word[i:i+2])"
0.6.0,"assuming a symbol sequence ""A B C"", if ""B C"" is merged, reduce the frequency of ""A B"""
0.6.0,"assuming a symbol sequence ""A B C B"", if ""B C"" is merged, reduce the frequency of ""C B""."
0.6.0,"however, skip this if the sequence is A B C B C, because the frequency of ""C B"" will be reduced by the previous code block"
0.6.0,find new pair
0.6.0,"assuming a symbol sequence ""A BC D"", if ""B C"" is merged, increase the frequency of ""A BC"""
0.6.0,"assuming a symbol sequence ""A BC B"", if ""B C"" is merged, increase the frequency of ""BC B"""
0.6.0,"however, if the sequence is A BC BC, skip this step because the count of ""BC BC"" will be incremented by the previous code block"
0.6.0,data structure of pair frequencies
0.6.0,index from pairs to words
0.6.0,version 0.2 changes the handling of the end-of-word token ('</w>');
0.6.0,version numbering allows bckward compatibility
0.6.0,"threshold is inspired by Zipfian assumption, but should only affect speed"
0.6.0,we probably missed the best pair because of pruning; go back to full statistics
0.6.0,"threshold is inspired by Zipfian assumption, but should only affect speed"
0.6.0,python 2/3 compatibility
0.6.0,read/write files as UTF-8
0.6.0,!/usr/bin/env python
0.6.0,"""rnn"" or ""brnn"""
0.6.0,for backward compatibility
0.6.0,Build encoder.
0.6.0,Build decoder.
0.6.0,Share the embedding matrix - preprocess with share_vocab required.
0.6.0,src/tgt vocab should be the same if `-share_vocab` is specified.
0.6.0,Build NMTModel(= encoder + decoder).
0.6.0,Build Generator.
0.6.0,Load the model states from checkpoint or initialize them.
0.6.0,This preserves backward-compat for models using customed layernorm
0.6.0,end of patch for backward compatibility
0.6.0,Add generator to model (this registers it as parameter of model).
0.6.0,!/usr/bin/env python
0.6.0,this one is needed for torchtext random call (shuffled iterator)
0.6.0,in multi gpu it ensures datasets are read in the same order
0.6.0,some cudnn methods can be random even after fixing the seed
0.6.0,unless you tell it to be deterministic
0.6.0,These ensure same initialization in multi gpu mode
0.6.0,Load checkpoint if we resume from a previous training.
0.6.0,Load default opts values then overwrite it with opts from
0.6.0,the checkpoint. It's usefull in order to re-train a model
0.6.0,after adding a new option (not set in checkpoint)
0.6.0,Peek the first dataset to determine the data_type.
0.6.0,(All datasets have the same data_type).
0.6.0,Load fields generated from preprocess phase.
0.6.0,Report src/tgt features.
0.6.0,Build model.
0.6.0,Build optimizer.
0.6.0,Build model saver
0.6.0,Do training.
0.6.0,Embedding Options
0.6.0,Encoder-Decoder Options
0.6.0,"group.add('--residual', '-residual',   action=""store_true"","
0.6.0,"help=""Add residual connections between RNN layers."")"
0.6.0,Attention options
0.6.0,Generator and loss options.
0.6.0,Data options
0.6.0,"Dictionary options, for text corpus"
0.6.0,"Truncation options, for text corpus"
0.6.0,Data processing options
0.6.0,Options most relevant to speech
0.6.0,Option most relevant to image input
0.6.0,GPU
0.6.0,Init options
0.6.0,Pretrained word vectors
0.6.0,Fixed word vectors
0.6.0,Optimization options
0.6.0,learning rate
0.6.0,Use TensorboardX for visualization during training
0.6.0,Options most relevant to speech
0.6.0,Option most relevant to image input
0.6.0,Options most relevant to summarization.
0.6.0,Alpha and Beta values for Google Length + Coverage penalty
0.6.0,"Described here: https://arxiv.org/pdf/1609.08144.pdf, Section 7"
0.6.0,Options most relevant to speech.
0.6.0,Option most relevant to image input
0.6.0,MARKDOWN boilerplate
0.6.0,Copyright 2016 The Chromium Authors. All rights reserved.
0.6.0,Use of this source code is governed by a BSD-style license that can be
0.6.0,found in the LICENSE file.
0.6.0,**section heading**:
0.6.0,# **--argument-one**
0.6.0,"Get the key 'value' in the dict, or just use 'value'"
0.6.0,Basic attributes.
0.6.0,Set model in training mode.
0.6.0,Set model in validating mode.
0.6.0,F-prop through the model.
0.6.0,Compute loss.
0.6.0,Update statistics.
0.6.0,Set model back to training mode.
0.6.0,Truncated BPTT: reminder not compatible with accum > 1
0.6.0,dec_state = None
0.6.0,1. Create truncated target.
0.6.0,2. F-prop all but generator.
0.6.0,3. Compute loss in shards for memory efficiency.
0.6.0,4. Update the parameters and statistics.
0.6.0,Multi GPU gradient gather
0.6.0,"If truncated, don't backprop fully."
0.6.0,TO CHECK
0.6.0,if dec_state is not None:
0.6.0,dec_state.detach()
0.6.0,"in case of multi step gradient accumulation,"
0.6.0,update only after accum batches
0.6.0,For Flake
0.6.0,Initialize the bridge layer
0.6.0,"s_len, batch, emb_dim = emb.size()"
0.6.0,Lengths data is wrapped inside a Tensor.
0.6.0,"LSTM has hidden and cell state, other only one"
0.6.0,Total number of states
0.6.0,Build a linear layer for each
0.6.0,The encoder hidden is  (layers*directions) x batch x dim.
0.6.0,"s_len, batch, emb_dim = emb.size()"
0.6.0,from onmt.utils.misc import aeq
0.6.0,Run the forward pass of every layer of the tranformer.
0.6.0,"(batch_size, 64, imgH, imgW)"
0.6.0,layer 1
0.6.0,"(batch_size, 64, imgH/2, imgW/2)"
0.6.0,"(batch_size, 128, imgH/2, imgW/2)"
0.6.0,layer 2
0.6.0,"(batch_size, 128, imgH/2/2, imgW/2/2)"
0.6.0,"(batch_size, 256, imgH/2/2, imgW/2/2)"
0.6.0,layer 3
0.6.0,batch norm 1
0.6.0,"(batch_size, 256, imgH/2/2, imgW/2/2)"
0.6.0,layer4
0.6.0,"(batch_size, 256, imgH/2/2/2, imgW/2/2)"
0.6.0,"(batch_size, 512, imgH/2/2/2, imgW/2/2)"
0.6.0,layer 5
0.6.0,batch norm 2
0.6.0,"(batch_size, 512, imgH/2/2/2, imgW/2/2/2)"
0.6.0,"(batch_size, 512, imgH/2/2/2, imgW/2/2/2)"
0.6.0,"# (batch_size, 512, H, W)"
0.6.0,Dimensions and padding for constructing the word embedding matrix
0.6.0,Dimensions and padding for feature embedding matrices
0.6.0,(these have no effect if feat_vocab_sizes is empty)
0.6.0,The embedding matrix look-up tables. The first look-up table
0.6.0,"is for words. Subsequent ones are for features, if any exist."
0.6.0,The final output size of word + feature vectors. This can vary
0.6.0,from the word vector size if and only if features are defined.
0.6.0,This is the attribute you should access if you need to know
0.6.0,how big your embeddings are going to be.
0.6.0,The sequence of operations that converts the input sequence
0.6.0,into a sequence of embeddings. At minimum this consists of
0.6.0,looking up the embeddings for each word and feature in the
0.6.0,input. Model parameters may require the sequence to contain
0.6.0,additional operations as well.
0.6.0,This class is mainly used by decoder.py for RNNs but also
0.6.0,by the CNN / transformer decoder when copy attention is used
0.6.0,CNN has its own attention mechanism ConvMultiStepAttention
0.6.0,Transformer has its own MultiHeadedAttention
0.6.0,mlp wants it with bias
0.6.0,Check input sizes
0.6.0,"(batch, t_len, d) x (batch, d, s_len) --> (batch, t_len, s_len)"
0.6.0,"(batch, t_len, s_len, d)"
0.6.0,one step input
0.6.0,"compute attention scores, as in Luong et al."
0.6.0,Softmax or sparsemax to normalize attention weights
0.6.0,each context vector c_t is the weighted average
0.6.0,over all the source hidden states
0.6.0,concatenate
0.6.0,Check output sizes
0.6.0,Check output sizes
0.6.0,clamping necessary because of numerical errors: loss should be lower
0.6.0,"bounded by zero, but negative values near zero are possible without"
0.6.0,the clamp
0.6.0,from onmt.utils.misc import aeq
0.6.0,CHECKS
0.6.0,"batch, k_len, d = key.size()"
0.6.0,"batch_, k_len_, d_ = value.size()"
0.6.0,"aeq(batch, batch_)"
0.6.0,"aeq(k_len, k_len_)"
0.6.0,"aeq(d, d_)"
0.6.0,"batch_, q_len, d_ = query.size()"
0.6.0,"aeq(batch, batch_)"
0.6.0,"aeq(d, d_)"
0.6.0,"aeq(self.model_dim % 8, 0)"
0.6.0,if mask is not None:
0.6.0,"batch_, q_len_, k_len_ = mask.size()"
0.6.0,"aeq(batch_, batch)"
0.6.0,"aeq(k_len_, k_len)"
0.6.0,aeq(q_len_ == q_len)
0.6.0,END CHECKS
0.6.0,"1) Project key, value, and query."
0.6.0,2) Calculate and scale scores.
0.6.0,3) Apply attention dropout and compute context vectors.
0.6.0,CHECK
0.6.0,"batch_, q_len_, d_ = output.size()"
0.6.0,"aeq(q_len, q_len_)"
0.6.0,"aeq(batch, batch_)"
0.6.0,"aeq(d, d_)"
0.6.0,Return one attn
0.6.0,At the moment this class is only used by embeddings.Embeddings look-up tables
0.6.0,-*- coding: utf-8 -*-
0.6.0,checks
0.6.0,"batch, channel, height, width = base_target_emb.size()"
0.6.0,"batch_, channel_, height_, width_ = input_from_dec.size()"
0.6.0,"enc_batch, enc_channel, enc_height = encoder_out_top.size()"
0.6.0,"enc_batch_, enc_channel_, enc_height_ = encoder_out_combine.size()"
0.6.0,out_features * in_features
0.6.0,norm is out_features * 1
0.6.0,batch_size * out_features
0.6.0,out_features
0.6.0,out_features
0.6.0,batch_size * out_features
0.6.0,"out_channels, in_channels // groups, * kernel_size"
0.6.0,out_features
0.6.0,This is used nowhere in the code at the moment (Vincent Nguyen 05/18/2018)
0.6.0,"in_channels, out_channels, *kernel_size"
0.6.0,"in_channels, out_channels, *kernel_size"
0.6.0,"self.out_channels, 1"
0.6.0,out_features
0.6.0,out_features
0.6.0,store roots on diagonal
0.6.0,CHECKS
0.6.0,Original probabilities.
0.6.0,Probability of copying p(z=1) batch.
0.6.0,Probibility of not copying: p_{word}(w) * (1 - p(z))
0.6.0,probabilities assigned by the model to the gold targets
0.6.0,probability of tokens copied from source
0.6.0,Set scores for unk to 0 and add eps
0.6.0,find the indices in which you do not use the copy mechanism
0.6.0,Drop padding.
0.6.0,this block does not depend on the loss value computed above
0.6.0,and is used only for stats
0.6.0,this block does not depend on the loss value computed above
0.6.0,and is used only for stats
0.6.0,Correct target copy token instead of <unk>
0.6.0,tgt[i] = align[i] + len(tgt_vocab)
0.6.0,for i such that tgt[i] == 0 and align[i] != 0
0.6.0,Compute sum of perplexities for stats
0.6.0,this part looks like it belongs in CopyGeneratorLoss
0.6.0,Compute Loss as NLL divided by seq length
0.6.0,Compute Total Loss per sequence in batch
0.6.0,Divide by length of each sequence and sum
0.6.0,illegal_weights_mask = torch.ByteTensor([
0.6.0,"[0, 0, 0, 0, 0, 0, 0],"
0.6.0,"[0, 0, 0, 1, 1, 1, 1],"
0.6.0,"[0, 0, 0, 0, 0, 1, 1],"
0.6.0,"[0, 0, 1, 1, 1, 1, 1]])"
0.6.0,TODO: fix for pytorch 0.3
0.6.0,illegal_weights = alignments.masked_select(illegal_weights_mask)
0.6.0,"self.assertEqual(0.0, illegal_weights.data.sum())"
0.6.0,"-data option is required, but not used in this test, so dummy."
0.6.0,Helper to generate a vocabulary
0.6.0,len x batch x nfeat
0.6.0,batch x c x h x w
0.6.0,batch x 1 x nfft x t
0.6.0,Initialize vectors to compare size with
0.6.0,Ensure correct sizes and types
0.6.0,Make sure that output has the correct size and type
0.6.0,Make sure that output has the correct size and type
0.6.0,Make sure that output has the correct size and type
0.6.0,"[('encoder_type', 'transformer'),"
0.6.0,"('word_vec_size', 16), ('rnn_size', 16)],"
0.6.0,""""""" Only do SRU test if requirment is safisfied. """""""
0.6.0,SRU doesn't support input_feed.
0.6.0,!/usr/bin/env python
0.6.0,-*- coding: utf-8 -*-
0.6.0,Remove the generated *pt files.
0.6.0,4 specicials + 2 words (since we pass 2 to merge_vocabs)
0.6.0,Test image preprocessing
0.6.0,Test audio preprocessing
0.6.0,Basic attributes.
0.6.0,Decoder state
0.6.0,Build the RNN.
0.6.0,Set up the context gate.
0.6.0,Set up the standard attention.
0.6.0,"Set up a separated copy attention layer, if needed."
0.6.0,The encoder hidden is  (layers*directions) x batch x dim.
0.6.0,We need to convert it to layers x batch x (directions*dim).
0.6.0,Init the input feed.
0.6.0,Run the forward pass of the RNN.
0.6.0,Update the state with the result.
0.6.0,Concatenates sequence of tensors along a new dimension.
0.6.0,NOTE: v0.3 to 0.4: dec_outs / attns[*] may not be list
0.6.0,(in particular in case of SRU) it was not raising error in 0.3
0.6.0,since stack(Variable) was allowed.
0.6.0,"In 0.4, SRU returns a tensor that shouldn't be stacke"
0.6.0,TODO change the way attns is returned dict => list or tuple (onnx)
0.6.0,Initialize local and return variables.
0.6.0,Run the forward pass of the RNN.
0.6.0,Check
0.6.0,END
0.6.0,Calculate the attention.
0.6.0,Calculate the context gate.
0.6.0,Additional args check.
0.6.0,END Additional args check.
0.6.0,Initialize local and return variables.
0.6.0,Input feed concatenates hidden state with
0.6.0,input at every time step.
0.6.0,TODO: context gate should be employed
0.6.0,instead of second RNN transform.
0.6.0,Update the coverage attention.
0.6.0,Run the forward pass of the copy attention layer.
0.6.0,Return result.
0.6.0,Basic attributes.
0.6.0,Decoder State
0.6.0,Build the CNN.
0.6.0,CNNDecoder has its own attention mechanism.
0.6.0,"Set up a separated copy attention layer, if needed."
0.6.0,NOTE: memory_lengths is only here for compatibility reasons
0.6.0,with onmt.modules.RNNDecoderBase.forward()
0.6.0,Initialize return variables.
0.6.0,The output of CNNEncoder.
0.6.0,The combination of output of CNNEncoder and source embeddings.
0.6.0,Run the forward pass of the CNNDecoder.
0.6.0,Process the result and update the attentions.
0.6.0,Update the state.
0.6.0,TODO change the way attns is returned dict => list or tuple (onnx)
0.6.0,Memory_lengths is a single tensor shared between all models.
0.6.0,This assumption will not hold if Translator is modified
0.6.0,to calculate memory_lengths as something other than the length
0.6.0,of the input.
0.6.0,"Register self.mask as a buffer in TransformerDecoderLayer, so"
0.6.0,it gets TransformerDecoderLayer's cuda behavior automatically.
0.6.0,Basic attributes.
0.6.0,Decoder State
0.6.0,Build TransformerDecoder.
0.6.0,TransformerDecoder has its own attention mechanism.
0.6.0,"Set up a separated copy attention layer, if needed."
0.6.0,Initialize return variables.
0.6.0,Run the forward pass of the TransformerDecoder.
0.6.0,Process the result and update the attentions.
0.6.0,TODO change the way attns is returned dict => list or tuple (onnx)
0.6.0,"buffer size in bytes, determine equiv. # of elements based on data type"
0.6.0,copy tensors into buffer_t
0.6.0,all-reduce and rescale
0.6.0,copy all-reduced buffer back into tensors
0.6.0,"tensor is bigger than buffer, all-reduce and rescale directly"
0.6.0,"buffer is full, all-reduce and replace buffer with grad"
0.6.0,add tensor to buffer
0.6.0,We need to save a copy of optim.optimizer.state_dict() for setting
0.6.0,"the, optimizer state later on in Stage 2 in this method, since"
0.6.0,the method optim.set_parameters(model.parameters()) will overwrite
0.6.0,"optim.optimizer, and with ith the values stored in"
0.6.0,optim.optimizer.state_dict()
0.6.0,Stage 1:
0.6.0,Essentially optim.set_parameters (re-)creates and optimizer using
0.6.0,model.paramters() as parameters that will be stored in the
0.6.0,optim.optimizer.param_groups field of the torch optimizer class.
0.6.0,"Importantly, this method does not yet load the optimizer state, as"
0.6.0,essentially it builds a new optimizer with empty optimizer state and
0.6.0,parameters from the model.
0.6.0,"Stage 2: In this stage, which is only performed when loading an"
0.6.0,"optimizer from a checkpoint, we load the saved_optimizer_state_dict"
0.6.0,"into the re-created optimizer, to set the optim.optimizer.state"
0.6.0,"field, which was previously empty. For this, we use the optimizer"
0.6.0,"state saved in the ""saved_optimizer_state_dict"" variable for"
0.6.0,this purpose.
0.6.0,See also: https://github.com/pytorch/pytorch/issues/2830
0.6.0,Convert back the state values to cuda type if applicable
0.6.0,We want to make sure that indeed we have a non-empty optimizer state
0.6.0,when we loaded an existing model. This should be at least the case
0.6.0,"for Adam, which saves ""exp_avg"" and ""exp_avg_sq"" state"
0.6.0,(Exponential moving average of gradient and squared gradient values)
0.6.0,Decay method used in tensor2tensor.
0.6.0,Decay based on start_decay_steps every decay_steps
0.6.0,-*- coding: utf-8 -*-
0.6.0,if the loss function operates on vectors of raw logits instead of
0.6.0,"probabilities, only the first part of the generator needs to be"
0.6.0,"passed to the NMTLossCompute. At the moment, the only supported"
0.6.0,loss function of this kind is the sparsemax loss.
0.6.0,non_none: the subdict of the state dictionary where the values
0.6.0,are not None.
0.6.0,"Now, the iteration:"
0.6.0,state is a dictionary of sequences of tensor-like but we
0.6.0,want a sequence of dictionaries of tensors.
0.6.0,"First, unzip the dictionary into a sequence of keys and a"
0.6.0,sequence of tensor-like sequences.
0.6.0,"Now, yield a dictionary for each shard. The keys are always"
0.6.0,the same. values is a sequence of length #keys where each
0.6.0,element is a sequence of length #shards. We want to iterate
0.6.0,"over the shards, not over the keys: therefore, the values need"
0.6.0,to be re-zipped by shard and then each shard can be paired
0.6.0,with the keys.
0.6.0,Assumed backprop'd
0.6.0,Log the progress using the number of batches on the x-axis.
0.6.0,Get a list of world_size lists with len(stat_list) Statistics objects
0.6.0,SRU doesn't support PackedSequence.
0.6.0,-*- coding: utf-8 -*-
0.6.0,coding: utf-8
0.6.0,Below are helper functions for intra-class use only.
0.6.0,-*- coding: utf-8 -*-
0.6.0,Hack. Can't pickle defaultdict :(
0.6.0,"For all data types, the tgt side corpus is in form of text."
0.6.0,Prop src from field to get lower memory using when training with image
0.6.0,Load vocabulary
0.6.0,keep the order of tokens specified in the vocab file by
0.6.0,adding them to the counter with decreasing counting values
0.6.0,Drop the none-using from memory but keep the last
0.6.0,"All datasets have same num of n_tgt_features,"
0.6.0,getting the last one is OK.
0.6.0,"All datasets have same num of n_src_features,"
0.6.0,getting the last one is OK.
0.6.0,Merge the input and output vocabularies.
0.6.0,`tgt_vocab_size` is ignored when sharing vocabularies
0.6.0,We have at least one dataset.
0.6.0,"We return the len of cur_dataset, otherwise we need to load"
0.6.0,"all datasets to determine the real len, which loses the benefit"
0.6.0,of lazy loading.
0.6.0,Drop the current dataset for decreasing memory
0.6.0,"We clear `fields` when saving, restore when loading."
0.6.0,Sort batch by decreasing lengths of sentence required by pytorch.
0.6.0,"sort=False means ""Use dataset's sortkey instead of iterator's""."
0.6.0,Maintains the longest src and tgt length in the current batch
0.6.0,Reset current longest length at a new batch (count=1)
0.6.0,Src: <bos> w1 ... wN <eos>
0.6.0,Tgt: w1 ... wN <eos>
0.6.0,Sort the glob output by file name (by increasing indexes).
0.6.0,"Only one inputters.*Dataset, simple!"
0.6.0,-*- coding: utf-8 -*-
0.6.0,Peek at the first to see which fields are used.
0.6.0,"If out_examples is a generator, we need to save the filter_pred"
0.6.0,"function in serialization too, which would cause a problem when"
0.6.0,`torch.save()`. Thus we materialize it as a list.
0.6.0,STFT
0.6.0,"The codecs module seems to have bugs with seek()/tell(),"
0.6.0,so we use io.open().
0.6.0,"We have associate iterator, just yields tuples"
0.6.0,util we run parallel with it.
0.6.0,Yield tuples util this shard's size reaches the threshold.
0.6.0,-*- coding: utf-8 -*-
0.6.0,Peek at the first to see which fields are used.
0.6.0,"If out_examples is a generator, we need to save the filter_pred"
0.6.0,"function in serialization too, which would cause a problem when"
0.6.0,`torch.save()`. Thus we materialize it as a list.
0.6.0,-*- coding: utf-8 -*-
0.6.0,"self.src_vocabs: mutated in dynamic_dict, used in"
0.6.0,collapse_copy_scores and in Translator.py
0.6.0,Each element of an example is a dictionary whose keys represents
0.6.0,at minimum the src tokens and their indices and potentially also
0.6.0,the src and tgt features and alignment information.
0.6.0,Peek at the first to see which fields are used.
0.6.0,"If out_examples is a generator, we need to save the filter_pred"
0.6.0,"function in serialization too, which would cause a problem when"
0.6.0,`torch.save()`. Thus we materialize it as a list.
0.6.0,"Default to a balanced sort, prioritizing tgt len match."
0.6.0,TODO: make this configurable.
0.6.0,"All examples have same number of features, so we peek first one"
0.6.0,to get the num_feats.
0.6.0,Chain back the first element - we only want to peek it.
0.6.0,Below are helper functions for intra-class use only.
0.6.0,Mapping source tokens to indices in the dynamic dict.
0.6.0,"The codecs module seems to have bugs with seek()/tell(),"
0.6.0,so we use io.open().
0.6.0,"We have associate iterator, just yields tuples"
0.6.0,util we run parallel with it.
0.6.0,Yield tuples util this shard's size reaches the threshold.
0.6.0,This part of check is time consuming on Py2 (but
0.6.0,"it is quite fast on Py3, weird!). So we don't bother"
0.6.0,to check for very line. Instead we chekc every 64
0.6.0,lines. Thus we are not dividing exactly per
0.6.0,"`shard_size`, but it is not too much difference."
0.6.0,All examples must have same number of features.
0.6.0,flake8: noqa
0.6.0,For command-line option parsing
0.6.0,"Check pass, set the args."
0.6.0,"This SRU version implements its own cuda-level optimization,"
0.6.0,so it requires that:
0.6.0,1. `cupy` and `pynvrtc` python package installed.
0.6.0,2. pytorch is built with cuda support.
0.6.0,3. library path set: export LD_LIBRARY_PATH=<cuda lib path>.
0.6.0,Check 1.
0.6.0,Check 2.
0.6.0,Check 3.
0.6.0,This sets up device to use.
0.6.0,-> directions x batch x dim
0.6.0,For DEBUG
0.6.0,"size = (length, batch, x.size(-1)) \"
0.6.0,"if x.dim() == 3 else (batch, x.size(-1))"
0.6.0,grad_x = x.new(*x.size()) if k_ == 3 else x.new(*size).zero_()
0.6.0,Normal use
0.6.0,"An entry check here, will catch on train side and translate side"
0.6.0,if requirements are not satisfied.
0.6.0,RNNDecoderState wraps hidden as a tuple.
0.6.0,fh -> (layers*directions) x batch x dim
0.6.0,The score for each translation on the beam.
0.6.0,The backpointers at each time-step.
0.6.0,The outputs at each time-step.
0.6.0,Has EOS topped the beam yet.
0.6.0,The attentions (matrix) for each time.
0.6.0,Time and k pair for finished.
0.6.0,Information for global scoring.
0.6.0,Minimum prediction length
0.6.0,Apply Penalty at every step
0.6.0,force the output to be longer than self.min_length
0.6.0,Sum the previous scores.
0.6.0,Don't let EOS have children.
0.6.0,Block ngram repeats
0.6.0,"Last n tokens, n = block_ngram_repeat"
0.6.0,Skip the blocking if it is in the exclusion list
0.6.0,"best_scores_id is flattened beam x word array, so calculate which"
0.6.0,word and beam each score came from
0.6.0,End condition is when top-of-beam is EOS and no global score.
0.6.0,Add from beam until we have minimum outputs.
0.6.0,Term will be subtracted from probability
0.6.0,Probability will be divided by this
0.6.0,!/usr/bin/env python
0.6.0,for debugging
0.6.0,Statistics
0.6.0,Debug attention.
0.6.0,Turn any copied words to UNKs (index 0).
0.6.0,"Decoder forward, takes [tgt_len, batch, nfeats] as input"
0.6.0,"and [src_len, batch, hidden] as memory_bank"
0.6.0,"in case of inference tgt_len = 1, batch = beam times batch_size"
0.6.0,"in case of Gold Scoring tgt_len = actual length, batch = 1 batch"
0.6.0,Generator forward.
0.6.0,"returns [(batch_size x beam_size) , vocab ] when 1 step"
0.6.0,"or [ tgt_len, batch_size, vocab ] when full sentence"
0.6.0,"here we have scores [tgt_lenxbatch, vocab] or [beamxbatch, vocab]"
0.6.0,"returns [(batch_size x beam_size) , vocab ] when 1 step"
0.6.0,"or [ tgt_len, batch_size, vocab ] when full sentence"
0.6.0,TODO: faster code path for beam_size == 1.
0.6.0,TODO: support these blacklisted features.
0.6.0,Encoder forward.
0.6.0,Tile states and memory beam_size times.
0.6.0,Give full probability to the first beam on the first step.
0.6.0,Structure that holds finished hypotheses.
0.6.0,Multiply probs by the beam probability.
0.6.0,Flatten probs into a list of possibilities.
0.6.0,Recover log probs.
0.6.0,Resolve beam origin and true word ids.
0.6.0,Map beam_index to batch_index in the flat representation.
0.6.0,Append last prediction.
0.6.0,Save finished hypotheses.
0.6.0,Penalize beams that finished.
0.6.0,Store finished hypotheses for this batch.
0.6.0,End condition is the top beam finished and we can return
0.6.0,n_best hypotheses.
0.6.0,"If all sentences are translated, no need to go further."
0.6.0,Remove finished batches for the next step.
0.6.0,Reorder states.
0.6.0,(0) Prep each of the components of the search.
0.6.0,And helper method for reducing verbosity.
0.6.0,Define a list of tokens to exclude from ngram-blocking
0.6.0,"exclusion_list = [""<t>"", ""</t>"", "".""]"
0.6.0,(1) Run the encoder on the src.
0.6.0,(2) Repeat src objects `beam_size` times.
0.6.0,We use now  batch_size x beam_size (same as fast mode)
0.6.0,"(3) run the decoder to generate sentences, using beam search."
0.6.0,(a) Construct batch x beam_size nxt words.
0.6.0,Get all the pending current beam words and arrange for forward.
0.6.0,(b) Decode and forward
0.6.0,(c) Advance each beam.
0.6.0,Loop over the batch_size number of beam
0.6.0,(4) Extract sentences from beam.
0.6.0,Rollback pointer to the beginning.
0.6.0,!/usr/bin/env python
0.6.0,backwards compatibility for confs
0.6.0,load can be called multiple times: modify copy
0.6.0,NOTE: translator returns lists of `n_best` list
0.6.0,we can ignore that (i.e. flatten lists) only because
0.6.0,we restrict `n_best=1`
0.6.0,build back results with empty texts
0.6.0,Sorting
0.5.0,!/usr/bin/env python
0.5.0,!/usr/bin/env python
0.5.0,!/usr/bin/env python
0.5.0,-*- coding: utf-8 -*-
0.5.0,!/usr/bin/env python
0.5.0,-*- coding: utf-8 -*-
0.5.0,We will use glob.glob() to find sharded {train|valid}.[0-9]*.pt
0.5.0,"when training, so check to avoid tampering with existing pt files"
0.5.0,or mixing them up.
0.5.0,"We save fields in vocab.pt seperately, so make it empty."
0.5.0,"For data_type == 'img' or 'audio', currently we don't do"
0.5.0,preprocess sharding. We only build a monolithic dataset.
0.5.0,"But since the interfaces are uniform, it would be not hard"
0.5.0,to do this should users need this feature.
0.5.0,"We save fields in vocab.pt seperately, so make it empty."
0.5.0,"Can't save fields, so remove/reconstruct at training time."
0.5.0,!/usr/bin/env python
0.5.0,Create a thread to listen for errors in the child processes.
0.5.0,Train with multiprocessing.
0.5.0,"propagate exception to parent process, keeping original traceback"
0.5.0,!/usr/bin/env python3
0.5.0,-*- coding: utf-8 -*-
0.5.0,
0.5.0,"OpenNMT-py documentation build configuration file, created by"
0.5.0,sphinx-quickstart on Sun Dec 17 12:07:14 2017.
0.5.0,
0.5.0,This file is execfile()d with the current directory set to its
0.5.0,containing dir.
0.5.0,
0.5.0,Note that not all possible configuration values are present in this
0.5.0,autogenerated file.
0.5.0,
0.5.0,All configuration values have a default; values that are commented out
0.5.0,serve to show the default.
0.5.0,"If extensions (or modules to document with autodoc) are in another directory,"
0.5.0,add these directories to sys.path here. If the directory is relative to the
0.5.0,"documentation root, use os.path.abspath to make it absolute, like shown here."
0.5.0,
0.5.0,import os
0.5.0,import sys
0.5.0,"sys.path.insert(0, os.path.abspath('.'))"
0.5.0,-- General configuration ------------------------------------------------
0.5.0,"If your documentation needs a minimal Sphinx version, state it here."
0.5.0,
0.5.0,needs_sphinx = '1.0'
0.5.0,"Add any Sphinx extension module names here, as strings. They can be"
0.5.0,extensions coming with Sphinx (named 'sphinx.ext.*') or your custom
0.5.0,ones.
0.5.0,"Add any paths that contain templates here, relative to this directory."
0.5.0,The suffix(es) of source filenames.
0.5.0,You can specify multiple suffix as a list of string:
0.5.0,
0.5.0,"source_suffix = ['.rst', '.md']"
0.5.0,The master toctree document.
0.5.0,General information about the project.
0.5.0,"The version info for the project you're documenting, acts as replacement for"
0.5.0,"|version| and |release|, also used in various other places throughout the"
0.5.0,built documents.
0.5.0,
0.5.0,The short X.Y version.
0.5.0,"The full version, including alpha/beta/rc tags."
0.5.0,The language for content autogenerated by Sphinx. Refer to documentation
0.5.0,for a list of supported languages.
0.5.0,
0.5.0,This is also used if you do content translation via gettext catalogs.
0.5.0,"Usually you set ""language"" from the command line for these cases."
0.5.0,"List of patterns, relative to source directory, that match files and"
0.5.0,directories to ignore when looking for source files.
0.5.0,This patterns also effect to html_static_path and html_extra_path
0.5.0,The name of the Pygments (syntax highlighting) style to use.
0.5.0,"If true, `todo` and `todoList` produce output, else they produce nothing."
0.5.0,-- Options for HTML output ----------------------------------------------
0.5.0,The theme to use for HTML and HTML Help pages.  See the documentation for
0.5.0,a list of builtin themes.
0.5.0,
0.5.0,html_theme = 'sphinx_materialdesign_theme'
0.5.0,html_theme_path = [sphinx_materialdesign_theme.get_path()]
0.5.0,Theme options are theme-specific and customize the look and feel of a theme
0.5.0,"further.  For a list of options available for each theme, see the"
0.5.0,documentation.
0.5.0,
0.5.0,html_theme_options = {}
0.5.0,"Add any paths that contain custom static files (such as style sheets) here,"
0.5.0,"relative to this directory. They are copied after the builtin static files,"
0.5.0,"so a file named ""default.css"" will overwrite the builtin ""default.css""."
0.5.0,"Custom sidebar templates, must be a dictionary that maps document names"
0.5.0,to template names.
0.5.0,
0.5.0,This is required for the alabaster theme
0.5.0,refs: http://alabaster.readthedocs.io/en/latest/installation.html#sidebars
0.5.0,-- Options for HTMLHelp output ------------------------------------------
0.5.0,Output file base name for HTML help builder.
0.5.0,-- Options for LaTeX output ---------------------------------------------
0.5.0,The paper size ('letterpaper' or 'a4paper').
0.5.0,
0.5.0,"'papersize': 'letterpaper',"
0.5.0,"The font size ('10pt', '11pt' or '12pt')."
0.5.0,
0.5.0,"'pointsize': '10pt',"
0.5.0,Additional stuff for the LaTeX preamble.
0.5.0,
0.5.0,"'preamble': '',"
0.5.0,Latex figure (float) alignment
0.5.0,
0.5.0,"'figure_align': 'htbp',"
0.5.0,Grouping the document tree into LaTeX files. List of tuples
0.5.0,"(source start file, target name, title,"
0.5.0,"author, documentclass [howto, manual, or own class])."
0.5.0,-- Options for manual page output ---------------------------------------
0.5.0,One entry per manual page. List of tuples
0.5.0,"(source start file, name, description, authors, manual section)."
0.5.0,-- Options for Texinfo output -------------------------------------------
0.5.0,Grouping the document tree into Texinfo files. List of tuples
0.5.0,"(source start file, target name, title, author,"
0.5.0,"dir menu entry, description, category)"
0.5.0,!/usr/bin/env python
0.5.0,-*- coding: utf-8 -*-
0.5.0,"the vocab object is a list of tuple (name, torchtext.Vocab)"
0.5.0,we iterate over this list and associate vocabularies based on the name
0.5.0,!/usr/bin/env python
0.5.0,-*- coding: utf-8 -*-
0.5.0,"Add in default model arguments, possibly added since training."
0.5.0,"the vocab object is a list of tuple (name, torchtext.Vocab)"
0.5.0,we iterate over this list and associate vocabularies based on the name
0.5.0,-*- encoding: utf-8 -*-
0.5.0,!/usr/bin/env python
0.5.0,-*- coding: utf-8 -*-
0.5.0,Author: Rico Sennrich
0.5.0,flake8: noqa
0.5.0,This file is retrieved from https://github.com/rsennrich/subword-nmt
0.5.0,hack for python2/3 compatibility
0.5.0,check version information
0.5.0,some hacking to deal with duplicates (only consider first instance)
0.5.0,don't print end-of-word symbols
0.5.0,sys.stderr.write('cannot split {0} further.\n'.format(segment))
0.5.0,sys.stderr.write('OOV: {0}\n'.format(segment))
0.5.0,sys.stderr.write('OOV: {0}\n'.format(segment))
0.5.0,python 2/3 compatibility
0.5.0,read/write files as UTF-8
0.5.0,!/usr/bin/env python
0.5.0,!/usr/bin/env python
0.5.0,-*- coding: utf-8 -*-
0.5.0,Author: Rico Sennrich
0.5.0,flake8: noqa
0.5.0,This file is retrieved from https://github.com/rsennrich/subword-nmt
0.5.0,hack for python2/3 compatibility
0.5.0,"find all instances of pair, and update frequency/indices around it"
0.5.0,find first symbol
0.5.0,"if first symbol is followed by second symbol, we've found an occurrence of pair (old_word[i:i+2])"
0.5.0,"assuming a symbol sequence ""A B C"", if ""B C"" is merged, reduce the frequency of ""A B"""
0.5.0,"assuming a symbol sequence ""A B C B"", if ""B C"" is merged, reduce the frequency of ""C B""."
0.5.0,"however, skip this if the sequence is A B C B C, because the frequency of ""C B"" will be reduced by the previous code block"
0.5.0,find new pair
0.5.0,"assuming a symbol sequence ""A BC D"", if ""B C"" is merged, increase the frequency of ""A BC"""
0.5.0,"assuming a symbol sequence ""A BC B"", if ""B C"" is merged, increase the frequency of ""BC B"""
0.5.0,"however, if the sequence is A BC BC, skip this step because the count of ""BC BC"" will be incremented by the previous code block"
0.5.0,data structure of pair frequencies
0.5.0,index from pairs to words
0.5.0,version 0.2 changes the handling of the end-of-word token ('</w>');
0.5.0,version numbering allows bckward compatibility
0.5.0,"threshold is inspired by Zipfian assumption, but should only affect speed"
0.5.0,we probably missed the best pair because of pruning; go back to full statistics
0.5.0,"threshold is inspired by Zipfian assumption, but should only affect speed"
0.5.0,python 2/3 compatibility
0.5.0,read/write files as UTF-8
0.5.0,!/usr/bin/env python
0.5.0,"""rnn"" or ""brnn"""
0.5.0,for backward compatibility
0.5.0,Build encoder.
0.5.0,Build decoder.
0.5.0,Share the embedding matrix - preprocess with share_vocab required.
0.5.0,src/tgt vocab should be the same if `-share_vocab` is specified.
0.5.0,Build NMTModel(= encoder + decoder).
0.5.0,Build Generator.
0.5.0,Load the model states from checkpoint or initialize them.
0.5.0,Add generator to model (this registers it as parameter of model).
0.5.0,!/usr/bin/env python
0.5.0,this one is needed for torchtext random call (shuffled iterator)
0.5.0,in multi gpu it ensures datasets are read in the same order
0.5.0,some cudnn methods can be random even after fixing the seed
0.5.0,unless you tell it to be deterministic
0.5.0,These ensure same initialization in multi gpu mode
0.5.0,Load checkpoint if we resume from a previous training.
0.5.0,Peek the first dataset to determine the data_type.
0.5.0,(All datasets have the same data_type).
0.5.0,Load fields generated from preprocess phase.
0.5.0,Report src/tgt features.
0.5.0,Build model.
0.5.0,Build optimizer.
0.5.0,Build model saver
0.5.0,Do training.
0.5.0,Embedding Options
0.5.0,Encoder-Decoder Options
0.5.0,"group.add_argument('-residual',   action=""store_true"","
0.5.0,"help=""Add residual connections between RNN layers."")"
0.5.0,Attention options
0.5.0,Generator and loss options.
0.5.0,Data options
0.5.0,"Dictionary options, for text corpus"
0.5.0,"Truncation options, for text corpus"
0.5.0,Data processing options
0.5.0,Options most relevant to speech
0.5.0,Option most relevant to image input
0.5.0,GPU
0.5.0,Init options
0.5.0,Pretrained word vectors
0.5.0,Fixed word vectors
0.5.0,Optimization options
0.5.0,learning rate
0.5.0,Use TensorboardX for visualization during training
0.5.0,Options most relevant to speech
0.5.0,Option most relevant to image input
0.5.0,Options most relevant to summarization.
0.5.0,Alpha and Beta values for Google Length + Coverage penalty
0.5.0,"Described here: https://arxiv.org/pdf/1609.08144.pdf, Section 7"
0.5.0,Options most relevant to speech.
0.5.0,Option most relevant to image input
0.5.0,MARKDOWN boilerplate
0.5.0,Copyright 2016 The Chromium Authors. All rights reserved.
0.5.0,Use of this source code is governed by a BSD-style license that can be
0.5.0,found in the LICENSE file.
0.5.0,**section heading**:
0.5.0,# **--argument-one**
0.5.0,Basic attributes.
0.5.0,Set model in training mode.
0.5.0,Set model in validating mode.
0.5.0,F-prop through the model.
0.5.0,Compute loss.
0.5.0,Update statistics.
0.5.0,Set model back to training mode.
0.5.0,Truncated BPTT: reminder not compatible with accum > 1
0.5.0,1. Create truncated target.
0.5.0,2. F-prop all but generator.
0.5.0,3. Compute loss in shards for memory efficiency.
0.5.0,4. Update the parameters and statistics.
0.5.0,Multi GPU gradient gather
0.5.0,"If truncated, don't backprop fully."
0.5.0,"in case of multi step gradient accumulation,"
0.5.0,update only after accum batches
0.5.0,For Flake
0.5.0,Initialize the bridge layer
0.5.0,"s_len, batch, emb_dim = emb.size()"
0.5.0,Lengths data is wrapped inside a Tensor.
0.5.0,"LSTM has hidden and cell state, other only one"
0.5.0,Total number of states
0.5.0,Build a linear layer for each
0.5.0,The encoder hidden is  (layers*directions) x batch x dim.
0.5.0,"s_len, batch, emb_dim = emb.size()"
0.5.0,from onmt.utils.misc import aeq
0.5.0,Run the forward pass of every layer of the tranformer.
0.5.0,"(batch_size, 64, imgH, imgW)"
0.5.0,layer 1
0.5.0,"(batch_size, 64, imgH/2, imgW/2)"
0.5.0,"(batch_size, 128, imgH/2, imgW/2)"
0.5.0,layer 2
0.5.0,"(batch_size, 128, imgH/2/2, imgW/2/2)"
0.5.0,"(batch_size, 256, imgH/2/2, imgW/2/2)"
0.5.0,layer 3
0.5.0,batch norm 1
0.5.0,"(batch_size, 256, imgH/2/2, imgW/2/2)"
0.5.0,layer4
0.5.0,"(batch_size, 256, imgH/2/2/2, imgW/2/2)"
0.5.0,"(batch_size, 512, imgH/2/2/2, imgW/2/2)"
0.5.0,layer 5
0.5.0,batch norm 2
0.5.0,"(batch_size, 512, imgH/2/2/2, imgW/2/2/2)"
0.5.0,"(batch_size, 512, imgH/2/2/2, imgW/2/2/2)"
0.5.0,"# (batch_size, 512, H, W)"
0.5.0,Dimensions and padding for constructing the word embedding matrix
0.5.0,Dimensions and padding for feature embedding matrices
0.5.0,(these have no effect if feat_vocab_sizes is empty)
0.5.0,The embedding matrix look-up tables. The first look-up table
0.5.0,"is for words. Subsequent ones are for features, if any exist."
0.5.0,The final output size of word + feature vectors. This can vary
0.5.0,from the word vector size if and only if features are defined.
0.5.0,This is the attribute you should access if you need to know
0.5.0,how big your embeddings are going to be.
0.5.0,The sequence of operations that converts the input sequence
0.5.0,into a sequence of embeddings. At minimum this consists of
0.5.0,looking up the embeddings for each word and feature in the
0.5.0,input. Model parameters may require the sequence to contain
0.5.0,additional operations as well.
0.5.0,This class is mainly used by decoder.py for RNNs but also
0.5.0,by the CNN / transformer decoder when copy attention is used
0.5.0,CNN has its own attention mechanism ConvMultiStepAttention
0.5.0,Transformer has its own MultiHeadedAttention
0.5.0,mlp wants it with bias
0.5.0,Check input sizes
0.5.0,"(batch, t_len, d) x (batch, d, s_len) --> (batch, t_len, s_len)"
0.5.0,"(batch, t_len, s_len, d)"
0.5.0,one step input
0.5.0,"compute attention scores, as in Luong et al."
0.5.0,Softmax or sparsemax to normalize attention weights
0.5.0,each context vector c_t is the weighted average
0.5.0,over all the source hidden states
0.5.0,concatenate
0.5.0,Check output sizes
0.5.0,Check output sizes
0.5.0,clamping necessary because of numerical errors: loss should be lower
0.5.0,"bounded by zero, but negative values near zero are possible without"
0.5.0,the clamp
0.5.0,from onmt.utils.misc import aeq
0.5.0,CHECKS
0.5.0,"batch, k_len, d = key.size()"
0.5.0,"batch_, k_len_, d_ = value.size()"
0.5.0,"aeq(batch, batch_)"
0.5.0,"aeq(k_len, k_len_)"
0.5.0,"aeq(d, d_)"
0.5.0,"batch_, q_len, d_ = query.size()"
0.5.0,"aeq(batch, batch_)"
0.5.0,"aeq(d, d_)"
0.5.0,"aeq(self.model_dim % 8, 0)"
0.5.0,if mask is not None:
0.5.0,"batch_, q_len_, k_len_ = mask.size()"
0.5.0,"aeq(batch_, batch)"
0.5.0,"aeq(k_len_, k_len)"
0.5.0,aeq(q_len_ == q_len)
0.5.0,END CHECKS
0.5.0,"1) Project key, value, and query."
0.5.0,2) Calculate and scale scores.
0.5.0,3) Apply attention dropout and compute context vectors.
0.5.0,CHECK
0.5.0,"batch_, q_len_, d_ = output.size()"
0.5.0,"aeq(q_len, q_len_)"
0.5.0,"aeq(batch, batch_)"
0.5.0,"aeq(d, d_)"
0.5.0,Return one attn
0.5.0,At the moment this class is only used by embeddings.Embeddings look-up tables
0.5.0,-*- coding: utf-8 -*-
0.5.0,checks
0.5.0,"batch, channel, height, width = base_target_emb.size()"
0.5.0,"batch_, channel_, height_, width_ = input_from_dec.size()"
0.5.0,"enc_batch, enc_channel, enc_height = encoder_out_top.size()"
0.5.0,"enc_batch_, enc_channel_, enc_height_ = encoder_out_combine.size()"
0.5.0,out_features * in_features
0.5.0,norm is out_features * 1
0.5.0,batch_size * out_features
0.5.0,out_features
0.5.0,out_features
0.5.0,batch_size * out_features
0.5.0,"out_channels, in_channels // groups, * kernel_size"
0.5.0,out_features
0.5.0,This is used nowhere in the code at the moment (Vincent Nguyen 05/18/2018)
0.5.0,"in_channels, out_channels, *kernel_size"
0.5.0,"in_channels, out_channels, *kernel_size"
0.5.0,"self.out_channels, 1"
0.5.0,out_features
0.5.0,out_features
0.5.0,store roots on diagonal
0.5.0,CHECKS
0.5.0,Original probabilities.
0.5.0,Probability of copying p(z=1) batch.
0.5.0,Probibility of not copying: p_{word}(w) * (1 - p(z))
0.5.0,Compute unks in align and target for readability
0.5.0,Copy probability of tokens in source
0.5.0,Set scores for unk to 0 and add eps
0.5.0,Get scores for tokens in target
0.5.0,Regular prob (no unks and unks that can't be copied)
0.5.0,Add score for non-unks in target
0.5.0,Add score for when word is unk in both align and tgt
0.5.0,Forced copy. Add only probability for not-copied tokens
0.5.0,Drop padding.
0.5.0,Correct target copy token instead of <unk>
0.5.0,tgt[i] = align[i] + len(tgt_vocab)
0.5.0,for i such that tgt[i] == 0 and align[i] != 0
0.5.0,Compute sum of perplexities for stats
0.5.0,Compute Loss as NLL divided by seq length
0.5.0,Compute Sequence Lengths
0.5.0,Compute Total Loss per sequence in batch
0.5.0,Divide by length of each sequence and sum
0.5.0,illegal_weights_mask = torch.ByteTensor([
0.5.0,"[0, 0, 0, 0, 0, 0, 0],"
0.5.0,"[0, 0, 0, 1, 1, 1, 1],"
0.5.0,"[0, 0, 0, 0, 0, 1, 1],"
0.5.0,"[0, 0, 1, 1, 1, 1, 1]])"
0.5.0,TODO: fix for pytorch 0.3
0.5.0,illegal_weights = alignments.masked_select(illegal_weights_mask)
0.5.0,"self.assertEqual(0.0, illegal_weights.data.sum())"
0.5.0,"-data option is required, but not used in this test, so dummy."
0.5.0,Helper to generate a vocabulary
0.5.0,len x batch x nfeat
0.5.0,batch x c x h x w
0.5.0,batch x 1 x nfft x t
0.5.0,Initialize vectors to compare size with
0.5.0,Ensure correct sizes and types
0.5.0,Make sure that output has the correct size and type
0.5.0,Make sure that output has the correct size and type
0.5.0,Make sure that output has the correct size and type
0.5.0,"[('encoder_type', 'transformer'),"
0.5.0,"('word_vec_size', 16), ('rnn_size', 16)],"
0.5.0,""""""" Only do SRU test if requirment is safisfied. """""""
0.5.0,SRU doesn't support input_feed.
0.5.0,!/usr/bin/env python
0.5.0,-*- coding: utf-8 -*-
0.5.0,Remove the generated *pt files.
0.5.0,4 specicials + 2 words (since we pass 2 to merge_vocabs)
0.5.0,Test image preprocessing
0.5.0,Test audio preprocessing
0.5.0,Basic attributes.
0.5.0,Build the RNN.
0.5.0,Set up the context gate.
0.5.0,Set up the standard attention.
0.5.0,"Set up a separated copy attention layer, if needed."
0.5.0,Check
0.5.0,tgt.size() returns tgt length and batch
0.5.0,END
0.5.0,Run the forward pass of the RNN.
0.5.0,Update the state with the result.
0.5.0,Concatenates sequence of tensors along a new dimension.
0.5.0,NOTE: v0.3 to 0.4: decoder_outputs / attns[*] may not be list
0.5.0,(in particular in case of SRU) it was not raising error in 0.3
0.5.0,since stack(Variable) was allowed.
0.5.0,"In 0.4, SRU returns a tensor that shouldn't be stacke"
0.5.0,The encoder hidden is  (layers*directions) x batch x dim.
0.5.0,We need to convert it to layers x batch x (directions*dim).
0.5.0,Initialize local and return variables.
0.5.0,Run the forward pass of the RNN.
0.5.0,Check
0.5.0,END
0.5.0,Calculate the attention.
0.5.0,Calculate the context gate.
0.5.0,Additional args check.
0.5.0,END Additional args check.
0.5.0,Initialize local and return variables.
0.5.0,Input feed concatenates hidden state with
0.5.0,input at every time step.
0.5.0,TODO: context gate should be employed
0.5.0,instead of second RNN transform.
0.5.0,Update the coverage attention.
0.5.0,Run the forward pass of the copy attention layer.
0.5.0,Return result.
0.5.0,Init the input feed.
0.5.0,Basic attributes.
0.5.0,Build the CNN.
0.5.0,CNNDecoder has its own attention mechanism.
0.5.0,"Set up a separated copy attention layer, if needed."
0.5.0,NOTE: memory_lengths is only here for compatibility reasons
0.5.0,with onmt.modules.RNNDecoderBase.forward()
0.5.0,CHECKS
0.5.0,END CHECKS
0.5.0,Initialize return variables.
0.5.0,The output of CNNEncoder.
0.5.0,The combination of output of CNNEncoder and source embeddings.
0.5.0,Run the forward pass of the CNNDecoder.
0.5.0,Process the result and update the attentions.
0.5.0,Update the state.
0.5.0,Memory_lengths is a single tensor shared between all models.
0.5.0,This assumption will not hold if Translator is modified
0.5.0,to calculate memory_lengths as something other than the length
0.5.0,of the input.
0.5.0,"Register self.mask as a buffer in TransformerDecoderLayer, so"
0.5.0,it gets TransformerDecoderLayer's cuda behavior automatically.
0.5.0,Basic attributes.
0.5.0,Build TransformerDecoder.
0.5.0,TransformerDecoder has its own attention mechanism.
0.5.0,"Set up a separated copy attention layer, if needed."
0.5.0,Initialize return variables.
0.5.0,Run the forward pass of the TransformerDecoder.
0.5.0,Process the result and update the attentions.
0.5.0,"buffer size in bytes, determine equiv. # of elements based on data type"
0.5.0,copy tensors into buffer_t
0.5.0,all-reduce and rescale
0.5.0,copy all-reduced buffer back into tensors
0.5.0,"tensor is bigger than buffer, all-reduce and rescale directly"
0.5.0,"buffer is full, all-reduce and replace buffer with grad"
0.5.0,add tensor to buffer
0.5.0,We need to save a copy of optim.optimizer.state_dict() for setting
0.5.0,"the, optimizer state later on in Stage 2 in this method, since"
0.5.0,the method optim.set_parameters(model.parameters()) will overwrite
0.5.0,"optim.optimizer, and with ith the values stored in"
0.5.0,optim.optimizer.state_dict()
0.5.0,Stage 1:
0.5.0,Essentially optim.set_parameters (re-)creates and optimizer using
0.5.0,model.paramters() as parameters that will be stored in the
0.5.0,optim.optimizer.param_groups field of the torch optimizer class.
0.5.0,"Importantly, this method does not yet load the optimizer state, as"
0.5.0,essentially it builds a new optimizer with empty optimizer state and
0.5.0,parameters from the model.
0.5.0,"Stage 2: In this stage, which is only performed when loading an"
0.5.0,"optimizer from a checkpoint, we load the saved_optimizer_state_dict"
0.5.0,"into the re-created optimizer, to set the optim.optimizer.state"
0.5.0,"field, which was previously empty. For this, we use the optimizer"
0.5.0,"state saved in the ""saved_optimizer_state_dict"" variable for"
0.5.0,this purpose.
0.5.0,See also: https://github.com/pytorch/pytorch/issues/2830
0.5.0,Convert back the state values to cuda type if applicable
0.5.0,We want to make sure that indeed we have a non-empty optimizer state
0.5.0,when we loaded an existing model. This should be at least the case
0.5.0,"for Adam, which saves ""exp_avg"" and ""exp_avg_sq"" state"
0.5.0,(Exponential moving average of gradient and squared gradient values)
0.5.0,Decay method used in tensor2tensor.
0.5.0,Decay based on start_decay_steps every decay_steps
0.5.0,-*- coding: utf-8 -*-
0.5.0,"for sparsemax loss, the loss function operates on the raw output"
0.5.0,"vector, not a probability vector. Hence it's only necessary to"
0.5.0,apply the first part of the generator here.
0.5.0,non_none: the subdict of the state dictionary where the values
0.5.0,are not None.
0.5.0,"Now, the iteration:"
0.5.0,state is a dictionary of sequences of tensor-like but we
0.5.0,want a sequence of dictionaries of tensors.
0.5.0,"First, unzip the dictionary into a sequence of keys and a"
0.5.0,sequence of tensor-like sequences.
0.5.0,"Now, yield a dictionary for each shard. The keys are always"
0.5.0,the same. values is a sequence of length #keys where each
0.5.0,element is a sequence of length #shards. We want to iterate
0.5.0,"over the shards, not over the keys: therefore, the values need"
0.5.0,to be re-zipped by shard and then each shard can be paired
0.5.0,with the keys.
0.5.0,Assumed backprop'd
0.5.0,Log the progress using the number of batches on the x-axis.
0.5.0,Get a list of world_size lists with len(stat_list) Statistics objects
0.5.0,SRU doesn't support PackedSequence.
0.5.0,-*- coding: utf-8 -*-
0.5.0,coding: utf-8
0.5.0,Below are helper functions for intra-class use only.
0.5.0,-*- coding: utf-8 -*-
0.5.0,Hack. Can't pickle defaultdict :(
0.5.0,"For all data types, the tgt side corpus is in form of text."
0.5.0,Prop src from field to get lower memory using when training with image
0.5.0,Load vocabulary
0.5.0,keep the order of tokens specified in the vocab file by
0.5.0,adding them to the counter with decreasing counting values
0.5.0,Drop the none-using from memory but keep the last
0.5.0,"All datasets have same num of n_tgt_features,"
0.5.0,getting the last one is OK.
0.5.0,"All datasets have same num of n_src_features,"
0.5.0,getting the last one is OK.
0.5.0,Merge the input and output vocabularies.
0.5.0,`tgt_vocab_size` is ignored when sharing vocabularies
0.5.0,We have at least one dataset.
0.5.0,"We return the len of cur_dataset, otherwise we need to load"
0.5.0,"all datasets to determine the real len, which loses the benefit"
0.5.0,of lazy loading.
0.5.0,Drop the current dataset for decreasing memory
0.5.0,"We clear `fields` when saving, restore when loading."
0.5.0,Sort batch by decreasing lengths of sentence required by pytorch.
0.5.0,"sort=False means ""Use dataset's sortkey instead of iterator's""."
0.5.0,Maintains the longest src and tgt length in the current batch
0.5.0,Reset current longest length at a new batch (count=1)
0.5.0,Src: <bos> w1 ... wN <eos>
0.5.0,Tgt: w1 ... wN <eos>
0.5.0,Sort the glob output by file name (by increasing indexes).
0.5.0,"Only one inputters.*Dataset, simple!"
0.5.0,-*- coding: utf-8 -*-
0.5.0,Peek at the first to see which fields are used.
0.5.0,"If out_examples is a generator, we need to save the filter_pred"
0.5.0,"function in serialization too, which would cause a problem when"
0.5.0,`torch.save()`. Thus we materialize it as a list.
0.5.0,STFT
0.5.0,"The codecs module seems to have bugs with seek()/tell(),"
0.5.0,so we use io.open().
0.5.0,"We have associate iterator, just yields tuples"
0.5.0,util we run parallel with it.
0.5.0,Yield tuples util this shard's size reaches the threshold.
0.5.0,-*- coding: utf-8 -*-
0.5.0,Peek at the first to see which fields are used.
0.5.0,"If out_examples is a generator, we need to save the filter_pred"
0.5.0,"function in serialization too, which would cause a problem when"
0.5.0,`torch.save()`. Thus we materialize it as a list.
0.5.0,-*- coding: utf-8 -*-
0.5.0,"self.src_vocabs: mutated in dynamic_dict, used in"
0.5.0,collapse_copy_scores and in Translator.py
0.5.0,Each element of an example is a dictionary whose keys represents
0.5.0,at minimum the src tokens and their indices and potentially also
0.5.0,the src and tgt features and alignment information.
0.5.0,Peek at the first to see which fields are used.
0.5.0,"If out_examples is a generator, we need to save the filter_pred"
0.5.0,"function in serialization too, which would cause a problem when"
0.5.0,`torch.save()`. Thus we materialize it as a list.
0.5.0,"Default to a balanced sort, prioritizing tgt len match."
0.5.0,TODO: make this configurable.
0.5.0,"All examples have same number of features, so we peek first one"
0.5.0,to get the num_feats.
0.5.0,Chain back the first element - we only want to peek it.
0.5.0,Below are helper functions for intra-class use only.
0.5.0,Mapping source tokens to indices in the dynamic dict.
0.5.0,"The codecs module seems to have bugs with seek()/tell(),"
0.5.0,so we use io.open().
0.5.0,"We have associate iterator, just yields tuples"
0.5.0,util we run parallel with it.
0.5.0,Yield tuples util this shard's size reaches the threshold.
0.5.0,This part of check is time consuming on Py2 (but
0.5.0,"it is quite fast on Py3, weird!). So we don't bother"
0.5.0,to check for very line. Instead we chekc every 64
0.5.0,lines. Thus we are not dividing exactly per
0.5.0,"`shard_size`, but it is not too much difference."
0.5.0,All examples must have same number of features.
0.5.0,flake8: noqa
0.5.0,For command-line option parsing
0.5.0,"Check pass, set the args."
0.5.0,"This SRU version implements its own cuda-level optimization,"
0.5.0,so it requires that:
0.5.0,1. `cupy` and `pynvrtc` python package installed.
0.5.0,2. pytorch is built with cuda support.
0.5.0,3. library path set: export LD_LIBRARY_PATH=<cuda lib path>.
0.5.0,Check 1.
0.5.0,Check 2.
0.5.0,Check 3.
0.5.0,This sets up device to use.
0.5.0,-> directions x batch x dim
0.5.0,For DEBUG
0.5.0,"size = (length, batch, x.size(-1)) \"
0.5.0,"if x.dim() == 3 else (batch, x.size(-1))"
0.5.0,grad_x = x.new(*x.size()) if k_ == 3 else x.new(*size).zero_()
0.5.0,Normal use
0.5.0,"An entry check here, will catch on train side and translate side"
0.5.0,if requirements are not satisfied.
0.5.0,RNNDecoderState wraps hidden as a tuple.
0.5.0,fh -> (layers*directions) x batch x dim
0.5.0,Not yet supported on multi-gpu
0.5.0,The score for each translation on the beam.
0.5.0,The backpointers at each time-step.
0.5.0,The outputs at each time-step.
0.5.0,Has EOS topped the beam yet.
0.5.0,The attentions (matrix) for each time.
0.5.0,Time and k pair for finished.
0.5.0,Information for global scoring.
0.5.0,Minimum prediction length
0.5.0,Apply Penalty at every step
0.5.0,force the output to be longer than self.min_length
0.5.0,Sum the previous scores.
0.5.0,Don't let EOS have children.
0.5.0,Block ngram repeats
0.5.0,"Last n tokens, n = block_ngram_repeat"
0.5.0,Skip the blocking if it is in the exclusion list
0.5.0,"best_scores_id is flattened beam x word array, so calculate which"
0.5.0,word and beam each score came from
0.5.0,End condition is when top-of-beam is EOS and no global score.
0.5.0,Add from beam until we have minimum outputs.
0.5.0,Term will be subtracted from probability
0.5.0,Probability will be divided by this
0.5.0,!/usr/bin/env python
0.5.0,use ensemble decoding if more than one model is specified
0.5.0,for debugging
0.5.0,Statistics
0.5.0,Debug attention.
0.5.0,TODO: faster code path for beam_size == 1.
0.5.0,TODO: support these blacklisted features.
0.5.0,Encoder forward.
0.5.0,Tile states and memory beam_size times.
0.5.0,Give full probability to the first beam on the first step.
0.5.0,Structure that holds finished hypotheses.
0.5.0,Decoder forward.
0.5.0,Generator forward.
0.5.0,Multiply probs by the beam probability.
0.5.0,Flatten probs into a list of possibilities.
0.5.0,Recover log probs.
0.5.0,Resolve beam origin and true word ids.
0.5.0,Map beam_index to batch_index in the flat representation.
0.5.0,Append last prediction.
0.5.0,Save finished hypotheses.
0.5.0,Penalize beams that finished.
0.5.0,Store finished hypotheses for this batch.
0.5.0,End condition is the top beam finished and we can return
0.5.0,n_best hypotheses.
0.5.0,"If all sentences are translated, no need to go further."
0.5.0,Remove finished batches for the next step.
0.5.0,Reorder states.
0.5.0,(0) Prep each of the components of the search.
0.5.0,And helper method for reducing verbosity.
0.5.0,Define a list of tokens to exclude from ngram-blocking
0.5.0,"exclusion_list = [""<t>"", ""</t>"", "".""]"
0.5.0,Help functions for working with beams and batches
0.5.0,(1) Run the encoder on the src.
0.5.0,(2) Repeat src objects `beam_size` times.
0.5.0,"(3) run the decoder to generate sentences, using beam search."
0.5.0,Construct batch x beam_size nxt words.
0.5.0,Get all the pending current beam words and arrange for forward.
0.5.0,Turn any copied words to UNKs
0.5.0,0 is unk
0.5.0,Temporary kludge solution to handle changed dim expectation
0.5.0,in the decoder
0.5.0,Run one step.
0.5.0,dec_out: beam x rnn_size
0.5.0,(b) Compute a vector of batch x beam word scores.
0.5.0,beam x tgt_vocab
0.5.0,beam x (tgt_vocab + extra_vocab)
0.5.0,beam x tgt_vocab
0.5.0,(c) Advance each beam.
0.5.0,(4) Extract sentences from beam.
0.5.0,(1) run the encoder on the src
0.5.0,"(2) if a target is specified, compute the 'goldScore'"
0.5.0,(i.e. log likelihood) of the target under the model
0.5.0,Log prob of each word.
0.5.0,Rollback pointer to the beginning.
0.5.0,!/usr/bin/env python
0.5.0,backwards compatibility for confs
0.5.0,load can be called multiple times: modify copy
0.5.0,NOTE: translator returns lists of `n_best` list
0.5.0,we can ignore that (i.e. flatten lists) only because
0.5.0,we restrict `n_best=1`
0.5.0,build back results with empty texts
0.5.0,Sorting
0.4.1,!/usr/bin/env python
0.4.1,!/usr/bin/env python
0.4.1,!/usr/bin/env python
0.4.1,-*- coding: utf-8 -*-
0.4.1,!/usr/bin/env python
0.4.1,-*- coding: utf-8 -*-
0.4.1,We will use glob.glob() to find sharded {train|valid}.[0-9]*.pt
0.4.1,"when training, so check to avoid tampering with existing pt files"
0.4.1,or mixing them up.
0.4.1,"We save fields in vocab.pt seperately, so make it empty."
0.4.1,"For data_type == 'img' or 'audio', currently we don't do"
0.4.1,preprocess sharding. We only build a monolithic dataset.
0.4.1,"But since the interfaces are uniform, it would be not hard"
0.4.1,to do this should users need this feature.
0.4.1,"We save fields in vocab.pt seperately, so make it empty."
0.4.1,"Can't save fields, so remove/reconstruct at training time."
0.4.1,!/usr/bin/env python
0.4.1,Create a thread to listen for errors in the child processes.
0.4.1,Train with multiprocessing.
0.4.1,"propagate exception to parent process, keeping original traceback"
0.4.1,!/usr/bin/env python3
0.4.1,-*- coding: utf-8 -*-
0.4.1,
0.4.1,"OpenNMT-py documentation build configuration file, created by"
0.4.1,sphinx-quickstart on Sun Dec 17 12:07:14 2017.
0.4.1,
0.4.1,This file is execfile()d with the current directory set to its
0.4.1,containing dir.
0.4.1,
0.4.1,Note that not all possible configuration values are present in this
0.4.1,autogenerated file.
0.4.1,
0.4.1,All configuration values have a default; values that are commented out
0.4.1,serve to show the default.
0.4.1,"If extensions (or modules to document with autodoc) are in another directory,"
0.4.1,add these directories to sys.path here. If the directory is relative to the
0.4.1,"documentation root, use os.path.abspath to make it absolute, like shown here."
0.4.1,
0.4.1,import os
0.4.1,import sys
0.4.1,"sys.path.insert(0, os.path.abspath('.'))"
0.4.1,-- General configuration ------------------------------------------------
0.4.1,"If your documentation needs a minimal Sphinx version, state it here."
0.4.1,
0.4.1,needs_sphinx = '1.0'
0.4.1,"Add any Sphinx extension module names here, as strings. They can be"
0.4.1,extensions coming with Sphinx (named 'sphinx.ext.*') or your custom
0.4.1,ones.
0.4.1,"Add any paths that contain templates here, relative to this directory."
0.4.1,The suffix(es) of source filenames.
0.4.1,You can specify multiple suffix as a list of string:
0.4.1,
0.4.1,"source_suffix = ['.rst', '.md']"
0.4.1,The master toctree document.
0.4.1,General information about the project.
0.4.1,"The version info for the project you're documenting, acts as replacement for"
0.4.1,"|version| and |release|, also used in various other places throughout the"
0.4.1,built documents.
0.4.1,
0.4.1,The short X.Y version.
0.4.1,"The full version, including alpha/beta/rc tags."
0.4.1,The language for content autogenerated by Sphinx. Refer to documentation
0.4.1,for a list of supported languages.
0.4.1,
0.4.1,This is also used if you do content translation via gettext catalogs.
0.4.1,"Usually you set ""language"" from the command line for these cases."
0.4.1,"List of patterns, relative to source directory, that match files and"
0.4.1,directories to ignore when looking for source files.
0.4.1,This patterns also effect to html_static_path and html_extra_path
0.4.1,The name of the Pygments (syntax highlighting) style to use.
0.4.1,"If true, `todo` and `todoList` produce output, else they produce nothing."
0.4.1,-- Options for HTML output ----------------------------------------------
0.4.1,The theme to use for HTML and HTML Help pages.  See the documentation for
0.4.1,a list of builtin themes.
0.4.1,
0.4.1,html_theme = 'sphinx_materialdesign_theme'
0.4.1,html_theme_path = [sphinx_materialdesign_theme.get_path()]
0.4.1,Theme options are theme-specific and customize the look and feel of a theme
0.4.1,"further.  For a list of options available for each theme, see the"
0.4.1,documentation.
0.4.1,
0.4.1,html_theme_options = {}
0.4.1,"Add any paths that contain custom static files (such as style sheets) here,"
0.4.1,"relative to this directory. They are copied after the builtin static files,"
0.4.1,"so a file named ""default.css"" will overwrite the builtin ""default.css""."
0.4.1,"Custom sidebar templates, must be a dictionary that maps document names"
0.4.1,to template names.
0.4.1,
0.4.1,This is required for the alabaster theme
0.4.1,refs: http://alabaster.readthedocs.io/en/latest/installation.html#sidebars
0.4.1,-- Options for HTMLHelp output ------------------------------------------
0.4.1,Output file base name for HTML help builder.
0.4.1,-- Options for LaTeX output ---------------------------------------------
0.4.1,The paper size ('letterpaper' or 'a4paper').
0.4.1,
0.4.1,"'papersize': 'letterpaper',"
0.4.1,"The font size ('10pt', '11pt' or '12pt')."
0.4.1,
0.4.1,"'pointsize': '10pt',"
0.4.1,Additional stuff for the LaTeX preamble.
0.4.1,
0.4.1,"'preamble': '',"
0.4.1,Latex figure (float) alignment
0.4.1,
0.4.1,"'figure_align': 'htbp',"
0.4.1,Grouping the document tree into LaTeX files. List of tuples
0.4.1,"(source start file, target name, title,"
0.4.1,"author, documentclass [howto, manual, or own class])."
0.4.1,-- Options for manual page output ---------------------------------------
0.4.1,One entry per manual page. List of tuples
0.4.1,"(source start file, name, description, authors, manual section)."
0.4.1,-- Options for Texinfo output -------------------------------------------
0.4.1,Grouping the document tree into Texinfo files. List of tuples
0.4.1,"(source start file, target name, title, author,"
0.4.1,"dir menu entry, description, category)"
0.4.1,!/usr/bin/env python
0.4.1,-*- coding: utf-8 -*-
0.4.1,"the vocab object is a list of tuple (name, torchtext.Vocab)"
0.4.1,we iterate over this list and associate vocabularies based on the name
0.4.1,"Add in default model arguments, possibly added since training."
0.4.1,"the vocab object is a list of tuple (name, torchtext.Vocab)"
0.4.1,we iterate over this list and associate vocabularies based on the name
0.4.1,-*- encoding: utf-8 -*-
0.4.1,!/usr/bin/env python
0.4.1,-*- coding: utf-8 -*-
0.4.1,Author: Rico Sennrich
0.4.1,flake8: noqa
0.4.1,This file is retrieved from https://github.com/rsennrich/subword-nmt
0.4.1,hack for python2/3 compatibility
0.4.1,check version information
0.4.1,some hacking to deal with duplicates (only consider first instance)
0.4.1,don't print end-of-word symbols
0.4.1,sys.stderr.write('cannot split {0} further.\n'.format(segment))
0.4.1,sys.stderr.write('OOV: {0}\n'.format(segment))
0.4.1,sys.stderr.write('OOV: {0}\n'.format(segment))
0.4.1,python 2/3 compatibility
0.4.1,read/write files as UTF-8
0.4.1,!/usr/bin/env python
0.4.1,!/usr/bin/env python
0.4.1,-*- coding: utf-8 -*-
0.4.1,Author: Rico Sennrich
0.4.1,flake8: noqa
0.4.1,This file is retrieved from https://github.com/rsennrich/subword-nmt
0.4.1,hack for python2/3 compatibility
0.4.1,"find all instances of pair, and update frequency/indices around it"
0.4.1,find first symbol
0.4.1,"if first symbol is followed by second symbol, we've found an occurrence of pair (old_word[i:i+2])"
0.4.1,"assuming a symbol sequence ""A B C"", if ""B C"" is merged, reduce the frequency of ""A B"""
0.4.1,"assuming a symbol sequence ""A B C B"", if ""B C"" is merged, reduce the frequency of ""C B""."
0.4.1,"however, skip this if the sequence is A B C B C, because the frequency of ""C B"" will be reduced by the previous code block"
0.4.1,find new pair
0.4.1,"assuming a symbol sequence ""A BC D"", if ""B C"" is merged, increase the frequency of ""A BC"""
0.4.1,"assuming a symbol sequence ""A BC B"", if ""B C"" is merged, increase the frequency of ""BC B"""
0.4.1,"however, if the sequence is A BC BC, skip this step because the count of ""BC BC"" will be incremented by the previous code block"
0.4.1,data structure of pair frequencies
0.4.1,index from pairs to words
0.4.1,version 0.2 changes the handling of the end-of-word token ('</w>');
0.4.1,version numbering allows bckward compatibility
0.4.1,"threshold is inspired by Zipfian assumption, but should only affect speed"
0.4.1,we probably missed the best pair because of pruning; go back to full statistics
0.4.1,"threshold is inspired by Zipfian assumption, but should only affect speed"
0.4.1,python 2/3 compatibility
0.4.1,read/write files as UTF-8
0.4.1,!/usr/bin/env python
0.4.1,"""rnn"" or ""brnn"""
0.4.1,Build encoder.
0.4.1,Build decoder.
0.4.1,Share the embedding matrix - preprocess with share_vocab required.
0.4.1,src/tgt vocab should be the same if `-share_vocab` is specified.
0.4.1,Build NMTModel(= encoder + decoder).
0.4.1,Build Generator.
0.4.1,Load the model states from checkpoint or initialize them.
0.4.1,Add generator to model (this registers it as parameter of model).
0.4.1,!/usr/bin/env python
0.4.1,this one is needed for torchtext random call (shuffled iterator)
0.4.1,in multi gpu it ensures datasets are read in the same order
0.4.1,some cudnn methods can be random even after fixing the seed
0.4.1,unless you tell it to be deterministic
0.4.1,These ensure same initialization in multi gpu mode
0.4.1,Load checkpoint if we resume from a previous training.
0.4.1,Peek the first dataset to determine the data_type.
0.4.1,(All datasets have the same data_type).
0.4.1,Load fields generated from preprocess phase.
0.4.1,Report src/tgt features.
0.4.1,Build model.
0.4.1,Build optimizer.
0.4.1,Build model saver
0.4.1,Do training.
0.4.1,Embedding Options
0.4.1,Encoder-Decoder Options
0.4.1,"group.add_argument('-residual',   action=""store_true"","
0.4.1,"help=""Add residual connections between RNN layers."")"
0.4.1,Attention options
0.4.1,Generator and loss options.
0.4.1,Data options
0.4.1,"Dictionary options, for text corpus"
0.4.1,"Truncation options, for text corpus"
0.4.1,Data processing options
0.4.1,Options most relevant to speech
0.4.1,Option most relevant to image input
0.4.1,GPU
0.4.1,Init options
0.4.1,Pretrained word vectors
0.4.1,Fixed word vectors
0.4.1,Optimization options
0.4.1,learning rate
0.4.1,Use TensorboardX for visualization during training
0.4.1,Options most relevant to speech
0.4.1,Option most relevant to image input
0.4.1,Options most relevant to summarization.
0.4.1,Alpha and Beta values for Google Length + Coverage penalty
0.4.1,"Described here: https://arxiv.org/pdf/1609.08144.pdf, Section 7"
0.4.1,Options most relevant to speech.
0.4.1,Option most relevant to image input
0.4.1,MARKDOWN boilerplate
0.4.1,Copyright 2016 The Chromium Authors. All rights reserved.
0.4.1,Use of this source code is governed by a BSD-style license that can be
0.4.1,found in the LICENSE file.
0.4.1,**section heading**:
0.4.1,# **--argument-one**
0.4.1,Basic attributes.
0.4.1,Set model in training mode.
0.4.1,Set model in validating mode.
0.4.1,F-prop through the model.
0.4.1,Compute loss.
0.4.1,Update statistics.
0.4.1,Set model back to training mode.
0.4.1,Truncated BPTT: reminder not compatible with accum > 1
0.4.1,1. Create truncated target.
0.4.1,2. F-prop all but generator.
0.4.1,3. Compute loss in shards for memory efficiency.
0.4.1,4. Update the parameters and statistics.
0.4.1,Multi GPU gradient gather
0.4.1,"If truncated, don't backprop fully."
0.4.1,"in case of multi step gradient accumulation,"
0.4.1,update only after accum batches
0.4.1,For Flake
0.4.1,Initialize the bridge layer
0.4.1,"s_len, batch, emb_dim = emb.size()"
0.4.1,Lengths data is wrapped inside a Tensor.
0.4.1,"LSTM has hidden and cell state, other only one"
0.4.1,Total number of states
0.4.1,Build a linear layer for each
0.4.1,The encoder hidden is  (layers*directions) x batch x dim.
0.4.1,"s_len, batch, emb_dim = emb.size()"
0.4.1,from onmt.utils.misc import aeq
0.4.1,Run the forward pass of every layer of the tranformer.
0.4.1,"(batch_size, 64, imgH, imgW)"
0.4.1,layer 1
0.4.1,"(batch_size, 64, imgH/2, imgW/2)"
0.4.1,"(batch_size, 128, imgH/2, imgW/2)"
0.4.1,layer 2
0.4.1,"(batch_size, 128, imgH/2/2, imgW/2/2)"
0.4.1,"(batch_size, 256, imgH/2/2, imgW/2/2)"
0.4.1,layer 3
0.4.1,batch norm 1
0.4.1,"(batch_size, 256, imgH/2/2, imgW/2/2)"
0.4.1,layer4
0.4.1,"(batch_size, 256, imgH/2/2/2, imgW/2/2)"
0.4.1,"(batch_size, 512, imgH/2/2/2, imgW/2/2)"
0.4.1,layer 5
0.4.1,batch norm 2
0.4.1,"(batch_size, 512, imgH/2/2/2, imgW/2/2/2)"
0.4.1,"(batch_size, 512, imgH/2/2/2, imgW/2/2/2)"
0.4.1,"# (batch_size, 512, H, W)"
0.4.1,Dimensions and padding for constructing the word embedding matrix
0.4.1,Dimensions and padding for feature embedding matrices
0.4.1,(these have no effect if feat_vocab_sizes is empty)
0.4.1,The embedding matrix look-up tables. The first look-up table
0.4.1,"is for words. Subsequent ones are for features, if any exist."
0.4.1,The final output size of word + feature vectors. This can vary
0.4.1,from the word vector size if and only if features are defined.
0.4.1,This is the attribute you should access if you need to know
0.4.1,how big your embeddings are going to be.
0.4.1,The sequence of operations that converts the input sequence
0.4.1,into a sequence of embeddings. At minimum this consists of
0.4.1,looking up the embeddings for each word and feature in the
0.4.1,input. Model parameters may require the sequence to contain
0.4.1,additional operations as well.
0.4.1,This class is mainly used by decoder.py for RNNs but also
0.4.1,by the CNN / transformer decoder when copy attention is used
0.4.1,CNN has its own attention mechanism ConvMultiStepAttention
0.4.1,Transformer has its own MultiHeadedAttention
0.4.1,mlp wants it with bias
0.4.1,Check input sizes
0.4.1,"(batch, t_len, d) x (batch, d, s_len) --> (batch, t_len, s_len)"
0.4.1,"(batch, t_len, s_len, d)"
0.4.1,one step input
0.4.1,"compute attention scores, as in Luong et al."
0.4.1,Softmax or sparsemax to normalize attention weights
0.4.1,each context vector c_t is the weighted average
0.4.1,over all the source hidden states
0.4.1,concatenate
0.4.1,Check output sizes
0.4.1,Check output sizes
0.4.1,clamping necessary because of numerical errors: loss should be lower
0.4.1,"bounded by zero, but negative values near zero are possible without"
0.4.1,the clamp
0.4.1,from onmt.utils.misc import aeq
0.4.1,CHECKS
0.4.1,"batch, k_len, d = key.size()"
0.4.1,"batch_, k_len_, d_ = value.size()"
0.4.1,"aeq(batch, batch_)"
0.4.1,"aeq(k_len, k_len_)"
0.4.1,"aeq(d, d_)"
0.4.1,"batch_, q_len, d_ = query.size()"
0.4.1,"aeq(batch, batch_)"
0.4.1,"aeq(d, d_)"
0.4.1,"aeq(self.model_dim % 8, 0)"
0.4.1,if mask is not None:
0.4.1,"batch_, q_len_, k_len_ = mask.size()"
0.4.1,"aeq(batch_, batch)"
0.4.1,"aeq(k_len_, k_len)"
0.4.1,aeq(q_len_ == q_len)
0.4.1,END CHECKS
0.4.1,"1) Project key, value, and query."
0.4.1,2) Calculate and scale scores.
0.4.1,3) Apply attention dropout and compute context vectors.
0.4.1,CHECK
0.4.1,"batch_, q_len_, d_ = output.size()"
0.4.1,"aeq(q_len, q_len_)"
0.4.1,"aeq(batch, batch_)"
0.4.1,"aeq(d, d_)"
0.4.1,Return one attn
0.4.1,At the moment this class is only used by embeddings.Embeddings look-up tables
0.4.1,-*- coding: utf-8 -*-
0.4.1,checks
0.4.1,"batch, channel, height, width = base_target_emb.size()"
0.4.1,"batch_, channel_, height_, width_ = input_from_dec.size()"
0.4.1,"enc_batch, enc_channel, enc_height = encoder_out_top.size()"
0.4.1,"enc_batch_, enc_channel_, enc_height_ = encoder_out_combine.size()"
0.4.1,out_features * in_features
0.4.1,norm is out_features * 1
0.4.1,batch_size * out_features
0.4.1,out_features
0.4.1,out_features
0.4.1,batch_size * out_features
0.4.1,"out_channels, in_channels // groups, * kernel_size"
0.4.1,out_features
0.4.1,This is used nowhere in the code at the moment (Vincent Nguyen 05/18/2018)
0.4.1,"in_channels, out_channels, *kernel_size"
0.4.1,"in_channels, out_channels, *kernel_size"
0.4.1,"self.out_channels, 1"
0.4.1,out_features
0.4.1,out_features
0.4.1,store roots on diagonal
0.4.1,CHECKS
0.4.1,Original probabilities.
0.4.1,Probability of copying p(z=1) batch.
0.4.1,Probibility of not copying: p_{word}(w) * (1 - p(z))
0.4.1,Compute unks in align and target for readability
0.4.1,Copy probability of tokens in source
0.4.1,Set scores for unk to 0 and add eps
0.4.1,Get scores for tokens in target
0.4.1,Regular prob (no unks and unks that can't be copied)
0.4.1,Add score for non-unks in target
0.4.1,Add score for when word is unk in both align and tgt
0.4.1,Forced copy. Add only probability for not-copied tokens
0.4.1,Drop padding.
0.4.1,Correct target copy token instead of <unk>
0.4.1,tgt[i] = align[i] + len(tgt_vocab)
0.4.1,for i such that tgt[i] == 0 and align[i] != 0
0.4.1,Compute sum of perplexities for stats
0.4.1,Compute Loss as NLL divided by seq length
0.4.1,Compute Sequence Lengths
0.4.1,Compute Total Loss per sequence in batch
0.4.1,Divide by length of each sequence and sum
0.4.1,illegal_weights_mask = torch.ByteTensor([
0.4.1,"[0, 0, 0, 0, 0, 0, 0],"
0.4.1,"[0, 0, 0, 1, 1, 1, 1],"
0.4.1,"[0, 0, 0, 0, 0, 1, 1],"
0.4.1,"[0, 0, 1, 1, 1, 1, 1]])"
0.4.1,TODO: fix for pytorch 0.3
0.4.1,illegal_weights = alignments.masked_select(illegal_weights_mask)
0.4.1,"self.assertEqual(0.0, illegal_weights.data.sum())"
0.4.1,"-data option is required, but not used in this test, so dummy."
0.4.1,Helper to generate a vocabulary
0.4.1,len x batch x nfeat
0.4.1,batch x c x h x w
0.4.1,batch x 1 x nfft x t
0.4.1,Initialize vectors to compare size with
0.4.1,Ensure correct sizes and types
0.4.1,Make sure that output has the correct size and type
0.4.1,Make sure that output has the correct size and type
0.4.1,Make sure that output has the correct size and type
0.4.1,"[('encoder_type', 'transformer'),"
0.4.1,"('word_vec_size', 16), ('rnn_size', 16)],"
0.4.1,""""""" Only do SRU test if requirment is safisfied. """""""
0.4.1,SRU doesn't support input_feed.
0.4.1,!/usr/bin/env python
0.4.1,-*- coding: utf-8 -*-
0.4.1,Remove the generated *pt files.
0.4.1,4 specicials + 2 words (since we pass 2 to merge_vocabs)
0.4.1,Test image preprocessing
0.4.1,Test audio preprocessing
0.4.1,Basic attributes.
0.4.1,Build the RNN.
0.4.1,Set up the context gate.
0.4.1,Set up the standard attention.
0.4.1,"Set up a separated copy attention layer, if needed."
0.4.1,Check
0.4.1,tgt.size() returns tgt length and batch
0.4.1,END
0.4.1,Run the forward pass of the RNN.
0.4.1,Update the state with the result.
0.4.1,Concatenates sequence of tensors along a new dimension.
0.4.1,NOTE: v0.3 to 0.4: decoder_outputs / attns[*] may not be list
0.4.1,(in particular in case of SRU) it was not raising error in 0.3
0.4.1,since stack(Variable) was allowed.
0.4.1,"In 0.4, SRU returns a tensor that shouldn't be stacke"
0.4.1,The encoder hidden is  (layers*directions) x batch x dim.
0.4.1,We need to convert it to layers x batch x (directions*dim).
0.4.1,Initialize local and return variables.
0.4.1,Run the forward pass of the RNN.
0.4.1,Check
0.4.1,END
0.4.1,Calculate the attention.
0.4.1,Calculate the context gate.
0.4.1,Additional args check.
0.4.1,END Additional args check.
0.4.1,Initialize local and return variables.
0.4.1,Input feed concatenates hidden state with
0.4.1,input at every time step.
0.4.1,TODO: context gate should be employed
0.4.1,instead of second RNN transform.
0.4.1,Update the coverage attention.
0.4.1,Run the forward pass of the copy attention layer.
0.4.1,Return result.
0.4.1,Init the input feed.
0.4.1,Basic attributes.
0.4.1,Build the CNN.
0.4.1,CNNDecoder has its own attention mechanism.
0.4.1,"Set up a separated copy attention layer, if needed."
0.4.1,NOTE: memory_lengths is only here for compatibility reasons
0.4.1,with onmt.modules.RNNDecoderBase.forward()
0.4.1,CHECKS
0.4.1,END CHECKS
0.4.1,Initialize return variables.
0.4.1,The output of CNNEncoder.
0.4.1,The combination of output of CNNEncoder and source embeddings.
0.4.1,Run the forward pass of the CNNDecoder.
0.4.1,Process the result and update the attentions.
0.4.1,Update the state.
0.4.1,Memory_lengths is a single tensor shared between all models.
0.4.1,This assumption will not hold if Translator is modified
0.4.1,to calculate memory_lengths as something other than the length
0.4.1,of the input.
0.4.1,"Register self.mask as a buffer in TransformerDecoderLayer, so"
0.4.1,it gets TransformerDecoderLayer's cuda behavior automatically.
0.4.1,Basic attributes.
0.4.1,Build TransformerDecoder.
0.4.1,TransformerDecoder has its own attention mechanism.
0.4.1,"Set up a separated copy attention layer, if needed."
0.4.1,Initialize return variables.
0.4.1,Run the forward pass of the TransformerDecoder.
0.4.1,Process the result and update the attentions.
0.4.1,"buffer size in bytes, determine equiv. # of elements based on data type"
0.4.1,copy tensors into buffer_t
0.4.1,all-reduce and rescale
0.4.1,copy all-reduced buffer back into tensors
0.4.1,"tensor is bigger than buffer, all-reduce and rescale directly"
0.4.1,"buffer is full, all-reduce and replace buffer with grad"
0.4.1,add tensor to buffer
0.4.1,We need to save a copy of optim.optimizer.state_dict() for setting
0.4.1,"the, optimizer state later on in Stage 2 in this method, since"
0.4.1,the method optim.set_parameters(model.parameters()) will overwrite
0.4.1,"optim.optimizer, and with ith the values stored in"
0.4.1,optim.optimizer.state_dict()
0.4.1,Stage 1:
0.4.1,Essentially optim.set_parameters (re-)creates and optimizer using
0.4.1,model.paramters() as parameters that will be stored in the
0.4.1,optim.optimizer.param_groups field of the torch optimizer class.
0.4.1,"Importantly, this method does not yet load the optimizer state, as"
0.4.1,essentially it builds a new optimizer with empty optimizer state and
0.4.1,parameters from the model.
0.4.1,"Stage 2: In this stage, which is only performed when loading an"
0.4.1,"optimizer from a checkpoint, we load the saved_optimizer_state_dict"
0.4.1,"into the re-created optimizer, to set the optim.optimizer.state"
0.4.1,"field, which was previously empty. For this, we use the optimizer"
0.4.1,"state saved in the ""saved_optimizer_state_dict"" variable for"
0.4.1,this purpose.
0.4.1,See also: https://github.com/pytorch/pytorch/issues/2830
0.4.1,Convert back the state values to cuda type if applicable
0.4.1,We want to make sure that indeed we have a non-empty optimizer state
0.4.1,when we loaded an existing model. This should be at least the case
0.4.1,"for Adam, which saves ""exp_avg"" and ""exp_avg_sq"" state"
0.4.1,(Exponential moving average of gradient and squared gradient values)
0.4.1,Decay method used in tensor2tensor.
0.4.1,Decay based on start_decay_steps every decay_steps
0.4.1,-*- coding: utf-8 -*-
0.4.1,"for sparsemax loss, the loss function operates on the raw output"
0.4.1,"vector, not a probability vector. Hence it's only necessary to"
0.4.1,apply the first part of the generator here.
0.4.1,non_none: the subdict of the state dictionary where the values
0.4.1,are not None.
0.4.1,"Now, the iteration:"
0.4.1,state is a dictionary of sequences of tensor-like but we
0.4.1,want a sequence of dictionaries of tensors.
0.4.1,"First, unzip the dictionary into a sequence of keys and a"
0.4.1,sequence of tensor-like sequences.
0.4.1,"Now, yield a dictionary for each shard. The keys are always"
0.4.1,the same. values is a sequence of length #keys where each
0.4.1,element is a sequence of length #shards. We want to iterate
0.4.1,"over the shards, not over the keys: therefore, the values need"
0.4.1,to be re-zipped by shard and then each shard can be paired
0.4.1,with the keys.
0.4.1,Assumed backprop'd
0.4.1,Log the progress using the number of batches on the x-axis.
0.4.1,Get a list of world_size lists with len(stat_list) Statistics objects
0.4.1,SRU doesn't support PackedSequence.
0.4.1,-*- coding: utf-8 -*-
0.4.1,coding: utf-8
0.4.1,Below are helper functions for intra-class use only.
0.4.1,-*- coding: utf-8 -*-
0.4.1,Hack. Can't pickle defaultdict :(
0.4.1,"For all data types, the tgt side corpus is in form of text."
0.4.1,Prop src from field to get lower memory using when training with image
0.4.1,Load vocabulary
0.4.1,Drop the none-using from memory but keep the last
0.4.1,"All datasets have same num of n_tgt_features,"
0.4.1,getting the last one is OK.
0.4.1,"All datasets have same num of n_src_features,"
0.4.1,getting the last one is OK.
0.4.1,Merge the input and output vocabularies.
0.4.1,`tgt_vocab_size` is ignored when sharing vocabularies
0.4.1,We have at least one dataset.
0.4.1,"We return the len of cur_dataset, otherwise we need to load"
0.4.1,"all datasets to determine the real len, which loses the benefit"
0.4.1,of lazy loading.
0.4.1,Drop the current dataset for decreasing memory
0.4.1,"We clear `fields` when saving, restore when loading."
0.4.1,Sort batch by decreasing lengths of sentence required by pytorch.
0.4.1,"sort=False means ""Use dataset's sortkey instead of iterator's""."
0.4.1,Maintains the longest src and tgt length in the current batch
0.4.1,Reset current longest length at a new batch (count=1)
0.4.1,Src: <bos> w1 ... wN <eos>
0.4.1,Tgt: w1 ... wN <eos>
0.4.1,Sort the glob output by file name (by increasing indexes).
0.4.1,"Only one inputters.*Dataset, simple!"
0.4.1,-*- coding: utf-8 -*-
0.4.1,Peek at the first to see which fields are used.
0.4.1,"If out_examples is a generator, we need to save the filter_pred"
0.4.1,"function in serialization too, which would cause a problem when"
0.4.1,`torch.save()`. Thus we materialize it as a list.
0.4.1,STFT
0.4.1,"The codecs module seems to have bugs with seek()/tell(),"
0.4.1,so we use io.open().
0.4.1,"We have associate iterator, just yields tuples"
0.4.1,util we run parallel with it.
0.4.1,Yield tuples util this shard's size reaches the threshold.
0.4.1,-*- coding: utf-8 -*-
0.4.1,Peek at the first to see which fields are used.
0.4.1,"If out_examples is a generator, we need to save the filter_pred"
0.4.1,"function in serialization too, which would cause a problem when"
0.4.1,`torch.save()`. Thus we materialize it as a list.
0.4.1,-*- coding: utf-8 -*-
0.4.1,"self.src_vocabs: mutated in dynamic_dict, used in"
0.4.1,collapse_copy_scores and in Translator.py
0.4.1,Each element of an example is a dictionary whose keys represents
0.4.1,at minimum the src tokens and their indices and potentially also
0.4.1,the src and tgt features and alignment information.
0.4.1,Peek at the first to see which fields are used.
0.4.1,"If out_examples is a generator, we need to save the filter_pred"
0.4.1,"function in serialization too, which would cause a problem when"
0.4.1,`torch.save()`. Thus we materialize it as a list.
0.4.1,"Default to a balanced sort, prioritizing tgt len match."
0.4.1,TODO: make this configurable.
0.4.1,"All examples have same number of features, so we peek first one"
0.4.1,to get the num_feats.
0.4.1,Chain back the first element - we only want to peek it.
0.4.1,Below are helper functions for intra-class use only.
0.4.1,Mapping source tokens to indices in the dynamic dict.
0.4.1,"The codecs module seems to have bugs with seek()/tell(),"
0.4.1,so we use io.open().
0.4.1,"We have associate iterator, just yields tuples"
0.4.1,util we run parallel with it.
0.4.1,Yield tuples util this shard's size reaches the threshold.
0.4.1,This part of check is time consuming on Py2 (but
0.4.1,"it is quite fast on Py3, weird!). So we don't bother"
0.4.1,to check for very line. Instead we chekc every 64
0.4.1,lines. Thus we are not dividing exactly per
0.4.1,"`shard_size`, but it is not too much difference."
0.4.1,All examples must have same number of features.
0.4.1,flake8: noqa
0.4.1,For command-line option parsing
0.4.1,"Check pass, set the args."
0.4.1,"This SRU version implements its own cuda-level optimization,"
0.4.1,so it requires that:
0.4.1,1. `cupy` and `pynvrtc` python package installed.
0.4.1,2. pytorch is built with cuda support.
0.4.1,3. library path set: export LD_LIBRARY_PATH=<cuda lib path>.
0.4.1,Check 1.
0.4.1,Check 2.
0.4.1,Check 3.
0.4.1,This sets up device to use.
0.4.1,-> directions x batch x dim
0.4.1,For DEBUG
0.4.1,"size = (length, batch, x.size(-1)) \"
0.4.1,"if x.dim() == 3 else (batch, x.size(-1))"
0.4.1,grad_x = x.new(*x.size()) if k_ == 3 else x.new(*size).zero_()
0.4.1,Normal use
0.4.1,"An entry check here, will catch on train side and translate side"
0.4.1,if requirements are not satisfied.
0.4.1,RNNDecoderState wraps hidden as a tuple.
0.4.1,fh -> (layers*directions) x batch x dim
0.4.1,Not yet supported on multi-gpu
0.4.1,The score for each translation on the beam.
0.4.1,The backpointers at each time-step.
0.4.1,The outputs at each time-step.
0.4.1,Has EOS topped the beam yet.
0.4.1,The attentions (matrix) for each time.
0.4.1,Time and k pair for finished.
0.4.1,Information for global scoring.
0.4.1,Minimum prediction length
0.4.1,Apply Penalty at every step
0.4.1,force the output to be longer than self.min_length
0.4.1,Sum the previous scores.
0.4.1,Don't let EOS have children.
0.4.1,Block ngram repeats
0.4.1,"Last n tokens, n = block_ngram_repeat"
0.4.1,Skip the blocking if it is in the exclusion list
0.4.1,"best_scores_id is flattened beam x word array, so calculate which"
0.4.1,word and beam each score came from
0.4.1,End condition is when top-of-beam is EOS and no global score.
0.4.1,Add from beam until we have minimum outputs.
0.4.1,Term will be subtracted from probability
0.4.1,Probability will be divided by this
0.4.1,!/usr/bin/env python
0.4.1,use ensemble decoding if more than one model is specified
0.4.1,for debugging
0.4.1,Statistics
0.4.1,Debug attention.
0.4.1,TODO: faster code path for beam_size == 1.
0.4.1,TODO: support these blacklisted features.
0.4.1,Encoder forward.
0.4.1,Tile states and memory beam_size times.
0.4.1,Give full probability to the first beam on the first step.
0.4.1,Structure that holds finished hypotheses.
0.4.1,Decoder forward.
0.4.1,Generator forward.
0.4.1,Multiply probs by the beam probability.
0.4.1,Flatten probs into a list of possibilities.
0.4.1,Recover log probs.
0.4.1,Resolve beam origin and true word ids.
0.4.1,Map beam_index to batch_index in the flat representation.
0.4.1,Append last prediction.
0.4.1,End condition is top beam is finished.
0.4.1,Save finished hypotheses.
0.4.1,Store finished hypotheses for this batch.
0.4.1,"If the batch reached the end, save the n_best hypotheses."
0.4.1,"If all sentences are translated, no need to go further."
0.4.1,Remove finished batches for the next step.
0.4.1,Reorder states.
0.4.1,(0) Prep each of the components of the search.
0.4.1,And helper method for reducing verbosity.
0.4.1,Define a list of tokens to exclude from ngram-blocking
0.4.1,"exclusion_list = [""<t>"", ""</t>"", "".""]"
0.4.1,Help functions for working with beams and batches
0.4.1,(1) Run the encoder on the src.
0.4.1,(2) Repeat src objects `beam_size` times.
0.4.1,"(3) run the decoder to generate sentences, using beam search."
0.4.1,Construct batch x beam_size nxt words.
0.4.1,Get all the pending current beam words and arrange for forward.
0.4.1,Turn any copied words to UNKs
0.4.1,0 is unk
0.4.1,Temporary kludge solution to handle changed dim expectation
0.4.1,in the decoder
0.4.1,Run one step.
0.4.1,dec_out: beam x rnn_size
0.4.1,(b) Compute a vector of batch x beam word scores.
0.4.1,beam x tgt_vocab
0.4.1,beam x (tgt_vocab + extra_vocab)
0.4.1,beam x tgt_vocab
0.4.1,(c) Advance each beam.
0.4.1,(4) Extract sentences from beam.
0.4.1,(1) run the encoder on the src
0.4.1,"(2) if a target is specified, compute the 'goldScore'"
0.4.1,(i.e. log likelihood) of the target under the model
0.4.1,Log prob of each word.
0.4.1,Rollback pointer to the beginning.
0.4.1,!/usr/bin/env python
0.4.1,backwards compatibility for confs
0.4.1,load can be called multiple times: modify copy
0.4.1,NOTE: translator returns lists of `n_best` list
0.4.1,we can ignore that (i.e. flatten lists) only because
0.4.1,we restrict `n_best=1`
0.4.1,build back results with empty texts
0.4.1,Sorting
0.4.0,!/usr/bin/env python
0.4.0,!/usr/bin/env python
0.4.0,!/usr/bin/env python
0.4.0,-*- coding: utf-8 -*-
0.4.0,!/usr/bin/env python
0.4.0,-*- coding: utf-8 -*-
0.4.0,We will use glob.glob() to find sharded {train|valid}.[0-9]*.pt
0.4.0,"when training, so check to avoid tampering with existing pt files"
0.4.0,or mixing them up.
0.4.0,"We save fields in vocab.pt seperately, so make it empty."
0.4.0,"For data_type == 'img' or 'audio', currently we don't do"
0.4.0,preprocess sharding. We only build a monolithic dataset.
0.4.0,"But since the interfaces are uniform, it would be not hard"
0.4.0,to do this should users need this feature.
0.4.0,"We save fields in vocab.pt seperately, so make it empty."
0.4.0,"Can't save fields, so remove/reconstruct at training time."
0.4.0,!/usr/bin/env python
0.4.0,Create a thread to listen for errors in the child processes.
0.4.0,Train with multiprocessing.
0.4.0,"propagate exception to parent process, keeping original traceback"
0.4.0,!/usr/bin/env python3
0.4.0,-*- coding: utf-8 -*-
0.4.0,
0.4.0,"OpenNMT-py documentation build configuration file, created by"
0.4.0,sphinx-quickstart on Sun Dec 17 12:07:14 2017.
0.4.0,
0.4.0,This file is execfile()d with the current directory set to its
0.4.0,containing dir.
0.4.0,
0.4.0,Note that not all possible configuration values are present in this
0.4.0,autogenerated file.
0.4.0,
0.4.0,All configuration values have a default; values that are commented out
0.4.0,serve to show the default.
0.4.0,"If extensions (or modules to document with autodoc) are in another directory,"
0.4.0,add these directories to sys.path here. If the directory is relative to the
0.4.0,"documentation root, use os.path.abspath to make it absolute, like shown here."
0.4.0,
0.4.0,import os
0.4.0,import sys
0.4.0,"sys.path.insert(0, os.path.abspath('.'))"
0.4.0,-- General configuration ------------------------------------------------
0.4.0,"If your documentation needs a minimal Sphinx version, state it here."
0.4.0,
0.4.0,needs_sphinx = '1.0'
0.4.0,"Add any Sphinx extension module names here, as strings. They can be"
0.4.0,extensions coming with Sphinx (named 'sphinx.ext.*') or your custom
0.4.0,ones.
0.4.0,"Add any paths that contain templates here, relative to this directory."
0.4.0,The suffix(es) of source filenames.
0.4.0,You can specify multiple suffix as a list of string:
0.4.0,
0.4.0,"source_suffix = ['.rst', '.md']"
0.4.0,The master toctree document.
0.4.0,General information about the project.
0.4.0,"The version info for the project you're documenting, acts as replacement for"
0.4.0,"|version| and |release|, also used in various other places throughout the"
0.4.0,built documents.
0.4.0,
0.4.0,The short X.Y version.
0.4.0,"The full version, including alpha/beta/rc tags."
0.4.0,The language for content autogenerated by Sphinx. Refer to documentation
0.4.0,for a list of supported languages.
0.4.0,
0.4.0,This is also used if you do content translation via gettext catalogs.
0.4.0,"Usually you set ""language"" from the command line for these cases."
0.4.0,"List of patterns, relative to source directory, that match files and"
0.4.0,directories to ignore when looking for source files.
0.4.0,This patterns also effect to html_static_path and html_extra_path
0.4.0,The name of the Pygments (syntax highlighting) style to use.
0.4.0,"If true, `todo` and `todoList` produce output, else they produce nothing."
0.4.0,-- Options for HTML output ----------------------------------------------
0.4.0,The theme to use for HTML and HTML Help pages.  See the documentation for
0.4.0,a list of builtin themes.
0.4.0,
0.4.0,html_theme = 'sphinx_materialdesign_theme'
0.4.0,html_theme_path = [sphinx_materialdesign_theme.get_path()]
0.4.0,Theme options are theme-specific and customize the look and feel of a theme
0.4.0,"further.  For a list of options available for each theme, see the"
0.4.0,documentation.
0.4.0,
0.4.0,html_theme_options = {}
0.4.0,"Add any paths that contain custom static files (such as style sheets) here,"
0.4.0,"relative to this directory. They are copied after the builtin static files,"
0.4.0,"so a file named ""default.css"" will overwrite the builtin ""default.css""."
0.4.0,"Custom sidebar templates, must be a dictionary that maps document names"
0.4.0,to template names.
0.4.0,
0.4.0,This is required for the alabaster theme
0.4.0,refs: http://alabaster.readthedocs.io/en/latest/installation.html#sidebars
0.4.0,-- Options for HTMLHelp output ------------------------------------------
0.4.0,Output file base name for HTML help builder.
0.4.0,-- Options for LaTeX output ---------------------------------------------
0.4.0,The paper size ('letterpaper' or 'a4paper').
0.4.0,
0.4.0,"'papersize': 'letterpaper',"
0.4.0,"The font size ('10pt', '11pt' or '12pt')."
0.4.0,
0.4.0,"'pointsize': '10pt',"
0.4.0,Additional stuff for the LaTeX preamble.
0.4.0,
0.4.0,"'preamble': '',"
0.4.0,Latex figure (float) alignment
0.4.0,
0.4.0,"'figure_align': 'htbp',"
0.4.0,Grouping the document tree into LaTeX files. List of tuples
0.4.0,"(source start file, target name, title,"
0.4.0,"author, documentclass [howto, manual, or own class])."
0.4.0,-- Options for manual page output ---------------------------------------
0.4.0,One entry per manual page. List of tuples
0.4.0,"(source start file, name, description, authors, manual section)."
0.4.0,-- Options for Texinfo output -------------------------------------------
0.4.0,Grouping the document tree into Texinfo files. List of tuples
0.4.0,"(source start file, target name, title, author,"
0.4.0,"dir menu entry, description, category)"
0.4.0,!/usr/bin/env python
0.4.0,-*- coding: utf-8 -*-
0.4.0,"the vocab object is a list of tuple (name, torchtext.Vocab)"
0.4.0,we iterate over this list and associate vocabularies based on the name
0.4.0,"Add in default model arguments, possibly added since training."
0.4.0,"the vocab object is a list of tuple (name, torchtext.Vocab)"
0.4.0,we iterate over this list and associate vocabularies based on the name
0.4.0,-*- encoding: utf-8 -*-
0.4.0,!/usr/bin/env python
0.4.0,-*- coding: utf-8 -*-
0.4.0,Author: Rico Sennrich
0.4.0,flake8: noqa
0.4.0,This file is retrieved from https://github.com/rsennrich/subword-nmt
0.4.0,hack for python2/3 compatibility
0.4.0,check version information
0.4.0,some hacking to deal with duplicates (only consider first instance)
0.4.0,don't print end-of-word symbols
0.4.0,sys.stderr.write('cannot split {0} further.\n'.format(segment))
0.4.0,sys.stderr.write('OOV: {0}\n'.format(segment))
0.4.0,sys.stderr.write('OOV: {0}\n'.format(segment))
0.4.0,python 2/3 compatibility
0.4.0,read/write files as UTF-8
0.4.0,!/usr/bin/env python
0.4.0,!/usr/bin/env python
0.4.0,-*- coding: utf-8 -*-
0.4.0,Author: Rico Sennrich
0.4.0,flake8: noqa
0.4.0,This file is retrieved from https://github.com/rsennrich/subword-nmt
0.4.0,hack for python2/3 compatibility
0.4.0,"find all instances of pair, and update frequency/indices around it"
0.4.0,find first symbol
0.4.0,"if first symbol is followed by second symbol, we've found an occurrence of pair (old_word[i:i+2])"
0.4.0,"assuming a symbol sequence ""A B C"", if ""B C"" is merged, reduce the frequency of ""A B"""
0.4.0,"assuming a symbol sequence ""A B C B"", if ""B C"" is merged, reduce the frequency of ""C B""."
0.4.0,"however, skip this if the sequence is A B C B C, because the frequency of ""C B"" will be reduced by the previous code block"
0.4.0,find new pair
0.4.0,"assuming a symbol sequence ""A BC D"", if ""B C"" is merged, increase the frequency of ""A BC"""
0.4.0,"assuming a symbol sequence ""A BC B"", if ""B C"" is merged, increase the frequency of ""BC B"""
0.4.0,"however, if the sequence is A BC BC, skip this step because the count of ""BC BC"" will be incremented by the previous code block"
0.4.0,data structure of pair frequencies
0.4.0,index from pairs to words
0.4.0,version 0.2 changes the handling of the end-of-word token ('</w>');
0.4.0,version numbering allows bckward compatibility
0.4.0,"threshold is inspired by Zipfian assumption, but should only affect speed"
0.4.0,we probably missed the best pair because of pruning; go back to full statistics
0.4.0,"threshold is inspired by Zipfian assumption, but should only affect speed"
0.4.0,python 2/3 compatibility
0.4.0,read/write files as UTF-8
0.4.0,!/usr/bin/env python
0.4.0,"""rnn"" or ""brnn"""
0.4.0,Build encoder.
0.4.0,Build decoder.
0.4.0,Share the embedding matrix - preprocess with share_vocab required.
0.4.0,src/tgt vocab should be the same if `-share_vocab` is specified.
0.4.0,Build NMTModel(= encoder + decoder).
0.4.0,Build Generator.
0.4.0,Load the model states from checkpoint or initialize them.
0.4.0,Add generator to model (this registers it as parameter of model).
0.4.0,!/usr/bin/env python
0.4.0,this one is needed for torchtext random call (shuffled iterator)
0.4.0,in multi gpu it ensures datasets are read in the same order
0.4.0,some cudnn methods can be random even after fixing the seed
0.4.0,unless you tell it to be deterministic
0.4.0,These ensure same initialization in multi gpu mode
0.4.0,Load checkpoint if we resume from a previous training.
0.4.0,Peek the first dataset to determine the data_type.
0.4.0,(All datasets have the same data_type).
0.4.0,Load fields generated from preprocess phase.
0.4.0,Report src/tgt features.
0.4.0,Build model.
0.4.0,Build optimizer.
0.4.0,Build model saver
0.4.0,Do training.
0.4.0,Embedding Options
0.4.0,Encoder-Decoder Options
0.4.0,"group.add_argument('-residual',   action=""store_true"","
0.4.0,"help=""Add residual connections between RNN layers."")"
0.4.0,Attention options
0.4.0,Generator and loss options.
0.4.0,Data options
0.4.0,"Dictionary options, for text corpus"
0.4.0,"Truncation options, for text corpus"
0.4.0,Data processing options
0.4.0,Options most relevant to speech
0.4.0,Option most relevant to image input
0.4.0,GPU
0.4.0,Init options
0.4.0,Pretrained word vectors
0.4.0,Fixed word vectors
0.4.0,Optimization options
0.4.0,learning rate
0.4.0,Use TensorboardX for visualization during training
0.4.0,Options most relevant to speech
0.4.0,Option most relevant to image input
0.4.0,Options most relevant to summarization.
0.4.0,Alpha and Beta values for Google Length + Coverage penalty
0.4.0,"Described here: https://arxiv.org/pdf/1609.08144.pdf, Section 7"
0.4.0,Options most relevant to speech.
0.4.0,Option most relevant to image input
0.4.0,MARKDOWN boilerplate
0.4.0,Copyright 2016 The Chromium Authors. All rights reserved.
0.4.0,Use of this source code is governed by a BSD-style license that can be
0.4.0,found in the LICENSE file.
0.4.0,**section heading**:
0.4.0,# **--argument-one**
0.4.0,Basic attributes.
0.4.0,Set model in training mode.
0.4.0,Set model in validating mode.
0.4.0,F-prop through the model.
0.4.0,Compute loss.
0.4.0,Update statistics.
0.4.0,Set model back to training mode.
0.4.0,Truncated BPTT: reminder not compatible with accum > 1
0.4.0,1. Create truncated target.
0.4.0,2. F-prop all but generator.
0.4.0,3. Compute loss in shards for memory efficiency.
0.4.0,4. Update the parameters and statistics.
0.4.0,Multi GPU gradient gather
0.4.0,"If truncated, don't backprop fully."
0.4.0,"in case of multi step gradient accumulation,"
0.4.0,update only after accum batches
0.4.0,For Flake
0.4.0,Initialize the bridge layer
0.4.0,"s_len, batch, emb_dim = emb.size()"
0.4.0,Lengths data is wrapped inside a Tensor.
0.4.0,"LSTM has hidden and cell state, other only one"
0.4.0,Total number of states
0.4.0,Build a linear layer for each
0.4.0,The encoder hidden is  (layers*directions) x batch x dim.
0.4.0,"s_len, batch, emb_dim = emb.size()"
0.4.0,from onmt.utils.misc import aeq
0.4.0,Run the forward pass of every layer of the tranformer.
0.4.0,"(batch_size, 64, imgH, imgW)"
0.4.0,layer 1
0.4.0,"(batch_size, 64, imgH/2, imgW/2)"
0.4.0,"(batch_size, 128, imgH/2, imgW/2)"
0.4.0,layer 2
0.4.0,"(batch_size, 128, imgH/2/2, imgW/2/2)"
0.4.0,"(batch_size, 256, imgH/2/2, imgW/2/2)"
0.4.0,layer 3
0.4.0,batch norm 1
0.4.0,"(batch_size, 256, imgH/2/2, imgW/2/2)"
0.4.0,layer4
0.4.0,"(batch_size, 256, imgH/2/2/2, imgW/2/2)"
0.4.0,"(batch_size, 512, imgH/2/2/2, imgW/2/2)"
0.4.0,layer 5
0.4.0,batch norm 2
0.4.0,"(batch_size, 512, imgH/2/2/2, imgW/2/2/2)"
0.4.0,"(batch_size, 512, imgH/2/2/2, imgW/2/2/2)"
0.4.0,"# (batch_size, 512, H, W)"
0.4.0,Dimensions and padding for constructing the word embedding matrix
0.4.0,Dimensions and padding for feature embedding matrices
0.4.0,(these have no effect if feat_vocab_sizes is empty)
0.4.0,The embedding matrix look-up tables. The first look-up table
0.4.0,"is for words. Subsequent ones are for features, if any exist."
0.4.0,The final output size of word + feature vectors. This can vary
0.4.0,from the word vector size if and only if features are defined.
0.4.0,This is the attribute you should access if you need to know
0.4.0,how big your embeddings are going to be.
0.4.0,The sequence of operations that converts the input sequence
0.4.0,into a sequence of embeddings. At minimum this consists of
0.4.0,looking up the embeddings for each word and feature in the
0.4.0,input. Model parameters may require the sequence to contain
0.4.0,additional operations as well.
0.4.0,This class is mainly used by decoder.py for RNNs but also
0.4.0,by the CNN / transformer decoder when copy attention is used
0.4.0,CNN has its own attention mechanism ConvMultiStepAttention
0.4.0,Transformer has its own MultiHeadedAttention
0.4.0,mlp wants it with bias
0.4.0,Check input sizes
0.4.0,"(batch, t_len, d) x (batch, d, s_len) --> (batch, t_len, s_len)"
0.4.0,"(batch, t_len, s_len, d)"
0.4.0,one step input
0.4.0,"compute attention scores, as in Luong et al."
0.4.0,Softmax or sparsemax to normalize attention weights
0.4.0,each context vector c_t is the weighted average
0.4.0,over all the source hidden states
0.4.0,concatenate
0.4.0,Check output sizes
0.4.0,Check output sizes
0.4.0,clamping necessary because of numerical errors: loss should be lower
0.4.0,"bounded by zero, but negative values near zero are possible without"
0.4.0,the clamp
0.4.0,from onmt.utils.misc import aeq
0.4.0,CHECKS
0.4.0,"batch, k_len, d = key.size()"
0.4.0,"batch_, k_len_, d_ = value.size()"
0.4.0,"aeq(batch, batch_)"
0.4.0,"aeq(k_len, k_len_)"
0.4.0,"aeq(d, d_)"
0.4.0,"batch_, q_len, d_ = query.size()"
0.4.0,"aeq(batch, batch_)"
0.4.0,"aeq(d, d_)"
0.4.0,"aeq(self.model_dim % 8, 0)"
0.4.0,if mask is not None:
0.4.0,"batch_, q_len_, k_len_ = mask.size()"
0.4.0,"aeq(batch_, batch)"
0.4.0,"aeq(k_len_, k_len)"
0.4.0,aeq(q_len_ == q_len)
0.4.0,END CHECKS
0.4.0,"1) Project key, value, and query."
0.4.0,2) Calculate and scale scores.
0.4.0,3) Apply attention dropout and compute context vectors.
0.4.0,CHECK
0.4.0,"batch_, q_len_, d_ = output.size()"
0.4.0,"aeq(q_len, q_len_)"
0.4.0,"aeq(batch, batch_)"
0.4.0,"aeq(d, d_)"
0.4.0,Return one attn
0.4.0,At the moment this class is only used by embeddings.Embeddings look-up tables
0.4.0,-*- coding: utf-8 -*-
0.4.0,checks
0.4.0,"batch, channel, height, width = base_target_emb.size()"
0.4.0,"batch_, channel_, height_, width_ = input_from_dec.size()"
0.4.0,"enc_batch, enc_channel, enc_height = encoder_out_top.size()"
0.4.0,"enc_batch_, enc_channel_, enc_height_ = encoder_out_combine.size()"
0.4.0,out_features * in_features
0.4.0,norm is out_features * 1
0.4.0,batch_size * out_features
0.4.0,out_features
0.4.0,out_features
0.4.0,batch_size * out_features
0.4.0,"out_channels, in_channels // groups, * kernel_size"
0.4.0,out_features
0.4.0,This is used nowhere in the code at the moment (Vincent Nguyen 05/18/2018)
0.4.0,"in_channels, out_channels, *kernel_size"
0.4.0,"in_channels, out_channels, *kernel_size"
0.4.0,"self.out_channels, 1"
0.4.0,out_features
0.4.0,out_features
0.4.0,store roots on diagonal
0.4.0,CHECKS
0.4.0,Original probabilities.
0.4.0,Probability of copying p(z=1) batch.
0.4.0,Probibility of not copying: p_{word}(w) * (1 - p(z))
0.4.0,Compute unks in align and target for readability
0.4.0,Copy probability of tokens in source
0.4.0,Set scores for unk to 0 and add eps
0.4.0,Get scores for tokens in target
0.4.0,Regular prob (no unks and unks that can't be copied)
0.4.0,Add score for non-unks in target
0.4.0,Add score for when word is unk in both align and tgt
0.4.0,Forced copy. Add only probability for not-copied tokens
0.4.0,Drop padding.
0.4.0,Correct target copy token instead of <unk>
0.4.0,tgt[i] = align[i] + len(tgt_vocab)
0.4.0,for i such that tgt[i] == 0 and align[i] != 0
0.4.0,Compute sum of perplexities for stats
0.4.0,Compute Loss as NLL divided by seq length
0.4.0,Compute Sequence Lengths
0.4.0,Compute Total Loss per sequence in batch
0.4.0,Divide by length of each sequence and sum
0.4.0,illegal_weights_mask = torch.ByteTensor([
0.4.0,"[0, 0, 0, 0, 0, 0, 0],"
0.4.0,"[0, 0, 0, 1, 1, 1, 1],"
0.4.0,"[0, 0, 0, 0, 0, 1, 1],"
0.4.0,"[0, 0, 1, 1, 1, 1, 1]])"
0.4.0,TODO: fix for pytorch 0.3
0.4.0,illegal_weights = alignments.masked_select(illegal_weights_mask)
0.4.0,"self.assertEqual(0.0, illegal_weights.data.sum())"
0.4.0,"-data option is required, but not used in this test, so dummy."
0.4.0,Helper to generate a vocabulary
0.4.0,len x batch x nfeat
0.4.0,batch x c x h x w
0.4.0,batch x 1 x nfft x t
0.4.0,Initialize vectors to compare size with
0.4.0,Ensure correct sizes and types
0.4.0,Make sure that output has the correct size and type
0.4.0,Make sure that output has the correct size and type
0.4.0,Make sure that output has the correct size and type
0.4.0,"[('encoder_type', 'transformer'),"
0.4.0,"('word_vec_size', 16), ('rnn_size', 16)],"
0.4.0,""""""" Only do SRU test if requirment is safisfied. """""""
0.4.0,SRU doesn't support input_feed.
0.4.0,!/usr/bin/env python
0.4.0,-*- coding: utf-8 -*-
0.4.0,Remove the generated *pt files.
0.4.0,4 specicials + 2 words (since we pass 2 to merge_vocabs)
0.4.0,Test image preprocessing
0.4.0,Test audio preprocessing
0.4.0,Basic attributes.
0.4.0,Build the RNN.
0.4.0,Set up the context gate.
0.4.0,Set up the standard attention.
0.4.0,"Set up a separated copy attention layer, if needed."
0.4.0,Check
0.4.0,tgt.size() returns tgt length and batch
0.4.0,END
0.4.0,Run the forward pass of the RNN.
0.4.0,Update the state with the result.
0.4.0,Concatenates sequence of tensors along a new dimension.
0.4.0,NOTE: v0.3 to 0.4: decoder_outputs / attns[*] may not be list
0.4.0,(in particular in case of SRU) it was not raising error in 0.3
0.4.0,since stack(Variable) was allowed.
0.4.0,"In 0.4, SRU returns a tensor that shouldn't be stacke"
0.4.0,The encoder hidden is  (layers*directions) x batch x dim.
0.4.0,We need to convert it to layers x batch x (directions*dim).
0.4.0,Initialize local and return variables.
0.4.0,Run the forward pass of the RNN.
0.4.0,Check
0.4.0,END
0.4.0,Calculate the attention.
0.4.0,Calculate the context gate.
0.4.0,Additional args check.
0.4.0,END Additional args check.
0.4.0,Initialize local and return variables.
0.4.0,Input feed concatenates hidden state with
0.4.0,input at every time step.
0.4.0,TODO: context gate should be employed
0.4.0,instead of second RNN transform.
0.4.0,Update the coverage attention.
0.4.0,Run the forward pass of the copy attention layer.
0.4.0,Return result.
0.4.0,Init the input feed.
0.4.0,Basic attributes.
0.4.0,Build the CNN.
0.4.0,CNNDecoder has its own attention mechanism.
0.4.0,"Set up a separated copy attention layer, if needed."
0.4.0,NOTE: memory_lengths is only here for compatibility reasons
0.4.0,with onmt.modules.RNNDecoderBase.forward()
0.4.0,CHECKS
0.4.0,END CHECKS
0.4.0,Initialize return variables.
0.4.0,The output of CNNEncoder.
0.4.0,The combination of output of CNNEncoder and source embeddings.
0.4.0,Run the forward pass of the CNNDecoder.
0.4.0,Process the result and update the attentions.
0.4.0,Update the state.
0.4.0,Memory_lengths is a single tensor shared between all models.
0.4.0,This assumption will not hold if Translator is modified
0.4.0,to calculate memory_lengths as something other than the length
0.4.0,of the input.
0.4.0,"Register self.mask as a buffer in TransformerDecoderLayer, so"
0.4.0,it gets TransformerDecoderLayer's cuda behavior automatically.
0.4.0,Basic attributes.
0.4.0,Build TransformerDecoder.
0.4.0,TransformerDecoder has its own attention mechanism.
0.4.0,"Set up a separated copy attention layer, if needed."
0.4.0,Initialize return variables.
0.4.0,Run the forward pass of the TransformerDecoder.
0.4.0,Process the result and update the attentions.
0.4.0,"buffer size in bytes, determine equiv. # of elements based on data type"
0.4.0,copy tensors into buffer_t
0.4.0,all-reduce and rescale
0.4.0,copy all-reduced buffer back into tensors
0.4.0,"tensor is bigger than buffer, all-reduce and rescale directly"
0.4.0,"buffer is full, all-reduce and replace buffer with grad"
0.4.0,add tensor to buffer
0.4.0,We need to save a copy of optim.optimizer.state_dict() for setting
0.4.0,"the, optimizer state later on in Stage 2 in this method, since"
0.4.0,the method optim.set_parameters(model.parameters()) will overwrite
0.4.0,"optim.optimizer, and with ith the values stored in"
0.4.0,optim.optimizer.state_dict()
0.4.0,Stage 1:
0.4.0,Essentially optim.set_parameters (re-)creates and optimizer using
0.4.0,model.paramters() as parameters that will be stored in the
0.4.0,optim.optimizer.param_groups field of the torch optimizer class.
0.4.0,"Importantly, this method does not yet load the optimizer state, as"
0.4.0,essentially it builds a new optimizer with empty optimizer state and
0.4.0,parameters from the model.
0.4.0,"Stage 2: In this stage, which is only performed when loading an"
0.4.0,"optimizer from a checkpoint, we load the saved_optimizer_state_dict"
0.4.0,"into the re-created optimizer, to set the optim.optimizer.state"
0.4.0,"field, which was previously empty. For this, we use the optimizer"
0.4.0,"state saved in the ""saved_optimizer_state_dict"" variable for"
0.4.0,this purpose.
0.4.0,See also: https://github.com/pytorch/pytorch/issues/2830
0.4.0,Convert back the state values to cuda type if applicable
0.4.0,We want to make sure that indeed we have a non-empty optimizer state
0.4.0,when we loaded an existing model. This should be at least the case
0.4.0,"for Adam, which saves ""exp_avg"" and ""exp_avg_sq"" state"
0.4.0,(Exponential moving average of gradient and squared gradient values)
0.4.0,Decay method used in tensor2tensor.
0.4.0,Decay based on start_decay_steps every decay_steps
0.4.0,-*- coding: utf-8 -*-
0.4.0,"for sparsemax loss, the loss function operates on the raw output"
0.4.0,"vector, not a probability vector. Hence it's only necessary to"
0.4.0,apply the first part of the generator here.
0.4.0,non_none: the subdict of the state dictionary where the values
0.4.0,are not None.
0.4.0,"Now, the iteration:"
0.4.0,state is a dictionary of sequences of tensor-like but we
0.4.0,want a sequence of dictionaries of tensors.
0.4.0,"First, unzip the dictionary into a sequence of keys and a"
0.4.0,sequence of tensor-like sequences.
0.4.0,"Now, yield a dictionary for each shard. The keys are always"
0.4.0,the same. values is a sequence of length #keys where each
0.4.0,element is a sequence of length #shards. We want to iterate
0.4.0,"over the shards, not over the keys: therefore, the values need"
0.4.0,to be re-zipped by shard and then each shard can be paired
0.4.0,with the keys.
0.4.0,Assumed backprop'd
0.4.0,Log the progress using the number of batches on the x-axis.
0.4.0,Get a list of world_size lists with len(stat_list) Statistics objects
0.4.0,SRU doesn't support PackedSequence.
0.4.0,-*- coding: utf-8 -*-
0.4.0,coding: utf-8
0.4.0,Below are helper functions for intra-class use only.
0.4.0,-*- coding: utf-8 -*-
0.4.0,Hack. Can't pickle defaultdict :(
0.4.0,"For all data types, the tgt side corpus is in form of text."
0.4.0,Prop src from field to get lower memory using when training with image
0.4.0,Load vocabulary
0.4.0,Drop the none-using from memory but keep the last
0.4.0,"All datasets have same num of n_tgt_features,"
0.4.0,getting the last one is OK.
0.4.0,"All datasets have same num of n_src_features,"
0.4.0,getting the last one is OK.
0.4.0,Merge the input and output vocabularies.
0.4.0,`tgt_vocab_size` is ignored when sharing vocabularies
0.4.0,We have at least one dataset.
0.4.0,"We return the len of cur_dataset, otherwise we need to load"
0.4.0,"all datasets to determine the real len, which loses the benefit"
0.4.0,of lazy loading.
0.4.0,Drop the current dataset for decreasing memory
0.4.0,"We clear `fields` when saving, restore when loading."
0.4.0,Sort batch by decreasing lengths of sentence required by pytorch.
0.4.0,"sort=False means ""Use dataset's sortkey instead of iterator's""."
0.4.0,Maintains the longest src and tgt length in the current batch
0.4.0,Reset current longest length at a new batch (count=1)
0.4.0,Src: <bos> w1 ... wN <eos>
0.4.0,Tgt: w1 ... wN <eos>
0.4.0,Sort the glob output by file name (by increasing indexes).
0.4.0,"Only one inputters.*Dataset, simple!"
0.4.0,-*- coding: utf-8 -*-
0.4.0,Peek at the first to see which fields are used.
0.4.0,"If out_examples is a generator, we need to save the filter_pred"
0.4.0,"function in serialization too, which would cause a problem when"
0.4.0,`torch.save()`. Thus we materialize it as a list.
0.4.0,STFT
0.4.0,"The codecs module seems to have bugs with seek()/tell(),"
0.4.0,so we use io.open().
0.4.0,"We have associate iterator, just yields tuples"
0.4.0,util we run parallel with it.
0.4.0,Yield tuples util this shard's size reaches the threshold.
0.4.0,-*- coding: utf-8 -*-
0.4.0,Peek at the first to see which fields are used.
0.4.0,"If out_examples is a generator, we need to save the filter_pred"
0.4.0,"function in serialization too, which would cause a problem when"
0.4.0,`torch.save()`. Thus we materialize it as a list.
0.4.0,-*- coding: utf-8 -*-
0.4.0,"self.src_vocabs: mutated in dynamic_dict, used in"
0.4.0,collapse_copy_scores and in Translator.py
0.4.0,Each element of an example is a dictionary whose keys represents
0.4.0,at minimum the src tokens and their indices and potentially also
0.4.0,the src and tgt features and alignment information.
0.4.0,Peek at the first to see which fields are used.
0.4.0,"If out_examples is a generator, we need to save the filter_pred"
0.4.0,"function in serialization too, which would cause a problem when"
0.4.0,`torch.save()`. Thus we materialize it as a list.
0.4.0,"Default to a balanced sort, prioritizing tgt len match."
0.4.0,TODO: make this configurable.
0.4.0,"All examples have same number of features, so we peek first one"
0.4.0,to get the num_feats.
0.4.0,Chain back the first element - we only want to peek it.
0.4.0,Below are helper functions for intra-class use only.
0.4.0,Mapping source tokens to indices in the dynamic dict.
0.4.0,"The codecs module seems to have bugs with seek()/tell(),"
0.4.0,so we use io.open().
0.4.0,"We have associate iterator, just yields tuples"
0.4.0,util we run parallel with it.
0.4.0,Yield tuples util this shard's size reaches the threshold.
0.4.0,This part of check is time consuming on Py2 (but
0.4.0,"it is quite fast on Py3, weird!). So we don't bother"
0.4.0,to check for very line. Instead we chekc every 64
0.4.0,lines. Thus we are not dividing exactly per
0.4.0,"`shard_size`, but it is not too much difference."
0.4.0,All examples must have same number of features.
0.4.0,flake8: noqa
0.4.0,For command-line option parsing
0.4.0,"Check pass, set the args."
0.4.0,"This SRU version implements its own cuda-level optimization,"
0.4.0,so it requires that:
0.4.0,1. `cupy` and `pynvrtc` python package installed.
0.4.0,2. pytorch is built with cuda support.
0.4.0,3. library path set: export LD_LIBRARY_PATH=<cuda lib path>.
0.4.0,Check 1.
0.4.0,Check 2.
0.4.0,Check 3.
0.4.0,This sets up device to use.
0.4.0,-> directions x batch x dim
0.4.0,For DEBUG
0.4.0,"size = (length, batch, x.size(-1)) \"
0.4.0,"if x.dim() == 3 else (batch, x.size(-1))"
0.4.0,grad_x = x.new(*x.size()) if k_ == 3 else x.new(*size).zero_()
0.4.0,Normal use
0.4.0,"An entry check here, will catch on train side and translate side"
0.4.0,if requirements are not satisfied.
0.4.0,RNNDecoderState wraps hidden as a tuple.
0.4.0,fh -> (layers*directions) x batch x dim
0.4.0,Not yet supported on multi-gpu
0.4.0,The score for each translation on the beam.
0.4.0,The backpointers at each time-step.
0.4.0,The outputs at each time-step.
0.4.0,Has EOS topped the beam yet.
0.4.0,The attentions (matrix) for each time.
0.4.0,Time and k pair for finished.
0.4.0,Information for global scoring.
0.4.0,Minimum prediction length
0.4.0,Apply Penalty at every step
0.4.0,force the output to be longer than self.min_length
0.4.0,Sum the previous scores.
0.4.0,Don't let EOS have children.
0.4.0,Block ngram repeats
0.4.0,"Last n tokens, n = block_ngram_repeat"
0.4.0,Skip the blocking if it is in the exclusion list
0.4.0,"best_scores_id is flattened beam x word array, so calculate which"
0.4.0,word and beam each score came from
0.4.0,End condition is when top-of-beam is EOS and no global score.
0.4.0,Add from beam until we have minimum outputs.
0.4.0,Term will be subtracted from probability
0.4.0,Probability will be divided by this
0.4.0,!/usr/bin/env python
0.4.0,use ensemble decoding if more than one model is specified
0.4.0,for debugging
0.4.0,Statistics
0.4.0,Debug attention.
0.4.0,TODO: faster code path for beam_size == 1.
0.4.0,TODO: support these blacklisted features.
0.4.0,Encoder forward.
0.4.0,Tile states and memory beam_size times.
0.4.0,Give full probability to the first beam on the first step.
0.4.0,Structure that holds finished hypotheses.
0.4.0,Decoder forward.
0.4.0,Generator forward.
0.4.0,Multiply probs by the beam probability.
0.4.0,Flatten probs into a list of possibilities.
0.4.0,Recover log probs.
0.4.0,Resolve beam origin and true word ids.
0.4.0,Map beam_index to batch_index in the flat representation.
0.4.0,Append last prediction.
0.4.0,End condition is top beam is finished.
0.4.0,Save finished hypotheses.
0.4.0,Store finished hypotheses for this batch.
0.4.0,"If the batch reached the end, save the n_best hypotheses."
0.4.0,"If all sentences are translated, no need to go further."
0.4.0,Remove finished batches for the next step.
0.4.0,Reorder states.
0.4.0,(0) Prep each of the components of the search.
0.4.0,And helper method for reducing verbosity.
0.4.0,Define a list of tokens to exclude from ngram-blocking
0.4.0,"exclusion_list = [""<t>"", ""</t>"", "".""]"
0.4.0,Help functions for working with beams and batches
0.4.0,(1) Run the encoder on the src.
0.4.0,(2) Repeat src objects `beam_size` times.
0.4.0,"(3) run the decoder to generate sentences, using beam search."
0.4.0,Construct batch x beam_size nxt words.
0.4.0,Get all the pending current beam words and arrange for forward.
0.4.0,Turn any copied words to UNKs
0.4.0,0 is unk
0.4.0,Temporary kludge solution to handle changed dim expectation
0.4.0,in the decoder
0.4.0,Run one step.
0.4.0,dec_out: beam x rnn_size
0.4.0,(b) Compute a vector of batch x beam word scores.
0.4.0,beam x tgt_vocab
0.4.0,beam x (tgt_vocab + extra_vocab)
0.4.0,beam x tgt_vocab
0.4.0,(c) Advance each beam.
0.4.0,(4) Extract sentences from beam.
0.4.0,(1) run the encoder on the src
0.4.0,"(2) if a target is specified, compute the 'goldScore'"
0.4.0,(i.e. log likelihood) of the target under the model
0.4.0,Log prob of each word.
0.4.0,Rollback pointer to the beginning.
0.4.0,!/usr/bin/env python
0.4.0,backwards compatibility for confs
0.4.0,load can be called multiple times: modify copy
0.4.0,NOTE: translator returns lists of `n_best` list
0.4.0,we can ignore that (i.e. flatten lists) only because
0.4.0,we restrict `n_best=1`
0.4.0,build back results with empty texts
0.4.0,Sorting
0.3.0,!/usr/bin/env python
0.3.0,!/usr/bin/env python
0.3.0,!/usr/bin/env python
0.3.0,-*- coding: utf-8 -*-
0.3.0,!/usr/bin/env python
0.3.0,-*- coding: utf-8 -*-
0.3.0,We will use glob.glob() to find sharded {train|valid}.[0-9]*.pt
0.3.0,"when training, so check to avoid tampering with existing pt files"
0.3.0,or mixing them up.
0.3.0,"We save fields in vocab.pt separately, so make it empty."
0.3.0,"We save fields in vocab.pt seperately, so make it empty."
0.3.0,Currently we only do preprocess sharding for corpus: data_type=='text'.
0.3.0,"For data_type == 'img' or 'audio', currently we don't do"
0.3.0,preprocess sharding. We only build a monolithic dataset.
0.3.0,"But since the interfaces are uniform, it would be not hard"
0.3.0,to do this should users need this feature.
0.3.0,"We save fields in vocab.pt seperately, so make it empty."
0.3.0,"Can't save fields, so remove/reconstruct at training time."
0.3.0,!/usr/bin/env python
0.3.0,Create a thread to listen for errors in the child processes.
0.3.0,Train with multiprocessing.
0.3.0,"propagate exception to parent process, keeping original traceback"
0.3.0,!/usr/bin/env python3
0.3.0,-*- coding: utf-8 -*-
0.3.0,
0.3.0,"OpenNMT-py documentation build configuration file, created by"
0.3.0,sphinx-quickstart on Sun Dec 17 12:07:14 2017.
0.3.0,
0.3.0,This file is execfile()d with the current directory set to its
0.3.0,containing dir.
0.3.0,
0.3.0,Note that not all possible configuration values are present in this
0.3.0,autogenerated file.
0.3.0,
0.3.0,All configuration values have a default; values that are commented out
0.3.0,serve to show the default.
0.3.0,"If extensions (or modules to document with autodoc) are in another directory,"
0.3.0,add these directories to sys.path here. If the directory is relative to the
0.3.0,"documentation root, use os.path.abspath to make it absolute, like shown here."
0.3.0,
0.3.0,import os
0.3.0,import sys
0.3.0,"sys.path.insert(0, os.path.abspath('.'))"
0.3.0,-- General configuration ------------------------------------------------
0.3.0,"If your documentation needs a minimal Sphinx version, state it here."
0.3.0,
0.3.0,needs_sphinx = '1.0'
0.3.0,"Add any Sphinx extension module names here, as strings. They can be"
0.3.0,extensions coming with Sphinx (named 'sphinx.ext.*') or your custom
0.3.0,ones.
0.3.0,"Add any paths that contain templates here, relative to this directory."
0.3.0,The suffix(es) of source filenames.
0.3.0,You can specify multiple suffix as a list of string:
0.3.0,
0.3.0,"source_suffix = ['.rst', '.md']"
0.3.0,The master toctree document.
0.3.0,General information about the project.
0.3.0,"The version info for the project you're documenting, acts as replacement for"
0.3.0,"|version| and |release|, also used in various other places throughout the"
0.3.0,built documents.
0.3.0,
0.3.0,The short X.Y version.
0.3.0,"The full version, including alpha/beta/rc tags."
0.3.0,The language for content autogenerated by Sphinx. Refer to documentation
0.3.0,for a list of supported languages.
0.3.0,
0.3.0,This is also used if you do content translation via gettext catalogs.
0.3.0,"Usually you set ""language"" from the command line for these cases."
0.3.0,"List of patterns, relative to source directory, that match files and"
0.3.0,directories to ignore when looking for source files.
0.3.0,This patterns also effect to html_static_path and html_extra_path
0.3.0,The name of the Pygments (syntax highlighting) style to use.
0.3.0,"If true, `todo` and `todoList` produce output, else they produce nothing."
0.3.0,-- Options for HTML output ----------------------------------------------
0.3.0,The theme to use for HTML and HTML Help pages.  See the documentation for
0.3.0,a list of builtin themes.
0.3.0,
0.3.0,html_theme = 'sphinx_materialdesign_theme'
0.3.0,html_theme_path = [sphinx_materialdesign_theme.get_path()]
0.3.0,Theme options are theme-specific and customize the look and feel of a theme
0.3.0,"further.  For a list of options available for each theme, see the"
0.3.0,documentation.
0.3.0,
0.3.0,html_theme_options = {}
0.3.0,"Add any paths that contain custom static files (such as style sheets) here,"
0.3.0,"relative to this directory. They are copied after the builtin static files,"
0.3.0,"so a file named ""default.css"" will overwrite the builtin ""default.css""."
0.3.0,"Custom sidebar templates, must be a dictionary that maps document names"
0.3.0,to template names.
0.3.0,
0.3.0,This is required for the alabaster theme
0.3.0,refs: http://alabaster.readthedocs.io/en/latest/installation.html#sidebars
0.3.0,-- Options for HTMLHelp output ------------------------------------------
0.3.0,Output file base name for HTML help builder.
0.3.0,-- Options for LaTeX output ---------------------------------------------
0.3.0,The paper size ('letterpaper' or 'a4paper').
0.3.0,
0.3.0,"'papersize': 'letterpaper',"
0.3.0,"The font size ('10pt', '11pt' or '12pt')."
0.3.0,
0.3.0,"'pointsize': '10pt',"
0.3.0,Additional stuff for the LaTeX preamble.
0.3.0,
0.3.0,"'preamble': '',"
0.3.0,Latex figure (float) alignment
0.3.0,
0.3.0,"'figure_align': 'htbp',"
0.3.0,Grouping the document tree into LaTeX files. List of tuples
0.3.0,"(source start file, target name, title,"
0.3.0,"author, documentclass [howto, manual, or own class])."
0.3.0,-- Options for manual page output ---------------------------------------
0.3.0,One entry per manual page. List of tuples
0.3.0,"(source start file, name, description, authors, manual section)."
0.3.0,-- Options for Texinfo output -------------------------------------------
0.3.0,Grouping the document tree into Texinfo files. List of tuples
0.3.0,"(source start file, target name, title, author,"
0.3.0,"dir menu entry, description, category)"
0.3.0,!/usr/bin/env python
0.3.0,-*- coding: utf-8 -*-
0.3.0,"the vocab object is a list of tuple (name, torchtext.Vocab)"
0.3.0,we iterate over this list and associate vocabularies based on the name
0.3.0,"Add in default model arguments, possibly added since training."
0.3.0,"the vocab object is a list of tuple (name, torchtext.Vocab)"
0.3.0,we iterate over this list and associate vocabularies based on the name
0.3.0,-*- encoding: utf-8 -*-
0.3.0,!/usr/bin/env python
0.3.0,-*- coding: utf-8 -*-
0.3.0,Author: Rico Sennrich
0.3.0,flake8: noqa
0.3.0,This file is retrieved from https://github.com/rsennrich/subword-nmt
0.3.0,hack for python2/3 compatibility
0.3.0,check version information
0.3.0,some hacking to deal with duplicates (only consider first instance)
0.3.0,don't print end-of-word symbols
0.3.0,sys.stderr.write('cannot split {0} further.\n'.format(segment))
0.3.0,sys.stderr.write('OOV: {0}\n'.format(segment))
0.3.0,sys.stderr.write('OOV: {0}\n'.format(segment))
0.3.0,python 2/3 compatibility
0.3.0,read/write files as UTF-8
0.3.0,!/usr/bin/env python
0.3.0,!/usr/bin/env python
0.3.0,-*- coding: utf-8 -*-
0.3.0,Author: Rico Sennrich
0.3.0,flake8: noqa
0.3.0,This file is retrieved from https://github.com/rsennrich/subword-nmt
0.3.0,hack for python2/3 compatibility
0.3.0,"find all instances of pair, and update frequency/indices around it"
0.3.0,find first symbol
0.3.0,"if first symbol is followed by second symbol, we've found an occurrence of pair (old_word[i:i+2])"
0.3.0,"assuming a symbol sequence ""A B C"", if ""B C"" is merged, reduce the frequency of ""A B"""
0.3.0,"assuming a symbol sequence ""A B C B"", if ""B C"" is merged, reduce the frequency of ""C B""."
0.3.0,"however, skip this if the sequence is A B C B C, because the frequency of ""C B"" will be reduced by the previous code block"
0.3.0,find new pair
0.3.0,"assuming a symbol sequence ""A BC D"", if ""B C"" is merged, increase the frequency of ""A BC"""
0.3.0,"assuming a symbol sequence ""A BC B"", if ""B C"" is merged, increase the frequency of ""BC B"""
0.3.0,"however, if the sequence is A BC BC, skip this step because the count of ""BC BC"" will be incremented by the previous code block"
0.3.0,data structure of pair frequencies
0.3.0,index from pairs to words
0.3.0,version 0.2 changes the handling of the end-of-word token ('</w>');
0.3.0,version numbering allows bckward compatibility
0.3.0,"threshold is inspired by Zipfian assumption, but should only affect speed"
0.3.0,we probably missed the best pair because of pruning; go back to full statistics
0.3.0,"threshold is inspired by Zipfian assumption, but should only affect speed"
0.3.0,python 2/3 compatibility
0.3.0,read/write files as UTF-8
0.3.0,!/usr/bin/env python
0.3.0,"""rnn"" or ""brnn"""
0.3.0,Build encoder.
0.3.0,Build decoder.
0.3.0,Share the embedding matrix - preprocess with share_vocab required.
0.3.0,src/tgt vocab should be the same if `-share_vocab` is specified.
0.3.0,Build NMTModel(= encoder + decoder).
0.3.0,Build Generator.
0.3.0,Load the model states from checkpoint or initialize them.
0.3.0,Add generator to model (this registers it as parameter of model).
0.3.0,!/usr/bin/env python
0.3.0,this one is needed for torchtext random call (shuffled iterator)
0.3.0,in multi gpu it ensures datasets are read in the same order
0.3.0,some cudnn methods can be random even after fixing the seed
0.3.0,unless you tell it to be deterministic
0.3.0,These ensure same initialization in multi gpu mode
0.3.0,Load checkpoint if we resume from a previous training.
0.3.0,Peek the first dataset to determine the data_type.
0.3.0,(All datasets have the same data_type).
0.3.0,Load fields generated from preprocess phase.
0.3.0,Report src/tgt features.
0.3.0,Build model.
0.3.0,Build optimizer.
0.3.0,Build model saver
0.3.0,Do training.
0.3.0,Embedding Options
0.3.0,Encoder-Decoder Options
0.3.0,"group.add_argument('-residual',   action=""store_true"","
0.3.0,"help=""Add residual connections between RNN layers."")"
0.3.0,Attention options
0.3.0,Generator and loss options.
0.3.0,Data options
0.3.0,"Dictionary options, for text corpus"
0.3.0,"Truncation options, for text corpus"
0.3.0,Data processing options
0.3.0,Options most relevant to speech
0.3.0,Option most relevant to image input
0.3.0,GPU
0.3.0,Init options
0.3.0,Pretrained word vectors
0.3.0,Fixed word vectors
0.3.0,Optimization options
0.3.0,learning rate
0.3.0,Use TensorboardX for visualization during training
0.3.0,Options most relevant to speech
0.3.0,Option most relevant to image input
0.3.0,Options most relevant to summarization.
0.3.0,Alpha and Beta values for Google Length + Coverage penalty
0.3.0,"Described here: https://arxiv.org/pdf/1609.08144.pdf, Section 7"
0.3.0,Options most relevant to speech.
0.3.0,Option most relevant to image input
0.3.0,MARKDOWN boilerplate
0.3.0,Copyright 2016 The Chromium Authors. All rights reserved.
0.3.0,Use of this source code is governed by a BSD-style license that can be
0.3.0,found in the LICENSE file.
0.3.0,**section heading**:
0.3.0,# **--argument-one**
0.3.0,Basic attributes.
0.3.0,Set model in training mode.
0.3.0,Set model in validating mode.
0.3.0,F-prop through the model.
0.3.0,Compute loss.
0.3.0,Update statistics.
0.3.0,Set model back to training mode.
0.3.0,Truncated BPTT: reminder not compatible with accum > 1
0.3.0,1. Create truncated target.
0.3.0,2. F-prop all but generator.
0.3.0,3. Compute loss in shards for memory efficiency.
0.3.0,4. Update the parameters and statistics.
0.3.0,Multi GPU gradient gather
0.3.0,"If truncated, don't backprop fully."
0.3.0,"in case of multi step gradient accumulation,"
0.3.0,update only after accum batches
0.3.0,For Flake
0.3.0,Initialize the bridge layer
0.3.0,"s_len, batch, emb_dim = emb.size()"
0.3.0,Lengths data is wrapped inside a Tensor.
0.3.0,"LSTM has hidden and cell state, other only one"
0.3.0,Total number of states
0.3.0,Build a linear layer for each
0.3.0,"(batch_size, 1, nfft, t)"
0.3.0,layer 1
0.3.0,"(batch_size, 32, nfft/2, t/2)"
0.3.0,"(batch_size, 32, nfft/2/2, t/2)"
0.3.0,layer 2
0.3.0,"(batch_size, 32, nfft/2/2, t/2)"
0.3.0,"s_len, batch, emb_dim = emb.size()"
0.3.0,from onmt.utils.misc import aeq
0.3.0,Run the forward pass of every layer of the tranformer.
0.3.0,"(batch_size, 64, imgH, imgW)"
0.3.0,layer 1
0.3.0,"(batch_size, 64, imgH/2, imgW/2)"
0.3.0,"(batch_size, 128, imgH/2, imgW/2)"
0.3.0,layer 2
0.3.0,"(batch_size, 128, imgH/2/2, imgW/2/2)"
0.3.0,"(batch_size, 256, imgH/2/2, imgW/2/2)"
0.3.0,layer 3
0.3.0,batch norm 1
0.3.0,"(batch_size, 256, imgH/2/2, imgW/2/2)"
0.3.0,layer4
0.3.0,"(batch_size, 256, imgH/2/2/2, imgW/2/2)"
0.3.0,"(batch_size, 512, imgH/2/2/2, imgW/2/2)"
0.3.0,layer 5
0.3.0,batch norm 2
0.3.0,"(batch_size, 512, imgH/2/2/2, imgW/2/2/2)"
0.3.0,"(batch_size, 512, imgH/2/2/2, imgW/2/2/2)"
0.3.0,"# (batch_size, 512, H, W)"
0.3.0,Dimensions and padding for constructing the word embedding matrix
0.3.0,Dimensions and padding for feature embedding matrices
0.3.0,(these have no effect if feat_vocab_sizes is empty)
0.3.0,The embedding matrix look-up tables. The first look-up table
0.3.0,"is for words. Subsequent ones are for features, if any exist."
0.3.0,The final output size of word + feature vectors. This can vary
0.3.0,from the word vector size if and only if features are defined.
0.3.0,This is the attribute you should access if you need to know
0.3.0,how big your embeddings are going to be.
0.3.0,The sequence of operations that converts the input sequence
0.3.0,into a sequence of embeddings. At minimum this consists of
0.3.0,looking up the embeddings for each word and feature in the
0.3.0,input. Model parameters may require the sequence to contain
0.3.0,additional operations as well.
0.3.0,This class is mainly used by decoder.py for RNNs but also
0.3.0,by the CNN / transformer decoder when copy attention is used
0.3.0,CNN has its own attention mechanism ConvMultiStepAttention
0.3.0,Transformer has its own MultiHeadedAttention
0.3.0,mlp wants it with bias
0.3.0,Check input sizes
0.3.0,"(batch, t_len, d) x (batch, d, s_len) --> (batch, t_len, s_len)"
0.3.0,"(batch, t_len, s_len, d)"
0.3.0,one step input
0.3.0,"compute attention scores, as in Luong et al."
0.3.0,Softmax or sparsemax to normalize attention weights
0.3.0,each context vector c_t is the weighted average
0.3.0,over all the source hidden states
0.3.0,concatenate
0.3.0,Check output sizes
0.3.0,Check output sizes
0.3.0,clamping necessary because of numerical errors: loss should be lower
0.3.0,"bounded by zero, but negative values near zero are possible without"
0.3.0,the clamp
0.3.0,from onmt.utils.misc import aeq
0.3.0,CHECKS
0.3.0,"batch, k_len, d = key.size()"
0.3.0,"batch_, k_len_, d_ = value.size()"
0.3.0,"aeq(batch, batch_)"
0.3.0,"aeq(k_len, k_len_)"
0.3.0,"aeq(d, d_)"
0.3.0,"batch_, q_len, d_ = query.size()"
0.3.0,"aeq(batch, batch_)"
0.3.0,"aeq(d, d_)"
0.3.0,"aeq(self.model_dim % 8, 0)"
0.3.0,if mask is not None:
0.3.0,"batch_, q_len_, k_len_ = mask.size()"
0.3.0,"aeq(batch_, batch)"
0.3.0,"aeq(k_len_, k_len)"
0.3.0,aeq(q_len_ == q_len)
0.3.0,END CHECKS
0.3.0,"1) Project key, value, and query."
0.3.0,2) Calculate and scale scores.
0.3.0,3) Apply attention dropout and compute context vectors.
0.3.0,CHECK
0.3.0,"batch_, q_len_, d_ = output.size()"
0.3.0,"aeq(q_len, q_len_)"
0.3.0,"aeq(batch, batch_)"
0.3.0,"aeq(d, d_)"
0.3.0,Return one attn
0.3.0,At the moment this class is only used by embeddings.Embeddings look-up tables
0.3.0,-*- coding: utf-8 -*-
0.3.0,checks
0.3.0,"batch, channel, height, width = base_target_emb.size()"
0.3.0,"batch_, channel_, height_, width_ = input_from_dec.size()"
0.3.0,"enc_batch, enc_channel, enc_height = encoder_out_top.size()"
0.3.0,"enc_batch_, enc_channel_, enc_height_ = encoder_out_combine.size()"
0.3.0,out_features * in_features
0.3.0,norm is out_features * 1
0.3.0,batch_size * out_features
0.3.0,out_features
0.3.0,out_features
0.3.0,batch_size * out_features
0.3.0,"out_channels, in_channels // groups, * kernel_size"
0.3.0,out_features
0.3.0,This is used nowhere in the code at the moment (Vincent Nguyen 05/18/2018)
0.3.0,"in_channels, out_channels, *kernel_size"
0.3.0,"in_channels, out_channels, *kernel_size"
0.3.0,"self.out_channels, 1"
0.3.0,out_features
0.3.0,out_features
0.3.0,store roots on diagonal
0.3.0,CHECKS
0.3.0,Original probabilities.
0.3.0,Probability of copying p(z=1) batch.
0.3.0,Probibility of not copying: p_{word}(w) * (1 - p(z))
0.3.0,Compute unks in align and target for readability
0.3.0,Copy probability of tokens in source
0.3.0,Set scores for unk to 0 and add eps
0.3.0,Get scores for tokens in target
0.3.0,Regular prob (no unks and unks that can't be copied)
0.3.0,Add score for non-unks in target
0.3.0,Add score for when word is unk in both align and tgt
0.3.0,Forced copy. Add only probability for not-copied tokens
0.3.0,Drop padding.
0.3.0,Correct target copy token instead of <unk>
0.3.0,tgt[i] = align[i] + len(tgt_vocab)
0.3.0,for i such that tgt[i] == 0 and align[i] != 0
0.3.0,Compute sum of perplexities for stats
0.3.0,Compute Loss as NLL divided by seq length
0.3.0,Compute Sequence Lengths
0.3.0,Compute Total Loss per sequence in batch
0.3.0,Divide by length of each sequence and sum
0.3.0,illegal_weights_mask = torch.ByteTensor([
0.3.0,"[0, 0, 0, 0, 0, 0, 0],"
0.3.0,"[0, 0, 0, 1, 1, 1, 1],"
0.3.0,"[0, 0, 0, 0, 0, 1, 1],"
0.3.0,"[0, 0, 1, 1, 1, 1, 1]])"
0.3.0,TODO: fix for pytorch 0.3
0.3.0,illegal_weights = alignments.masked_select(illegal_weights_mask)
0.3.0,"self.assertEqual(0.0, illegal_weights.data.sum())"
0.3.0,"-data option is required, but not used in this test, so dummy."
0.3.0,Helper to generate a vocabulary
0.3.0,len x batch x nfeat
0.3.0,batch x c x h x w
0.3.0,batch x 1 x nfft x t
0.3.0,Initialize vectors to compare size with
0.3.0,Ensure correct sizes and types
0.3.0,Make sure that output has the correct size and type
0.3.0,Make sure that output has the correct size and type
0.3.0,Make sure that output has the correct size and type
0.3.0,"[('encoder_type', 'transformer'),"
0.3.0,"('word_vec_size', 16), ('rnn_size', 16)],"
0.3.0,"[('encoder_type', 'transformer'),"
0.3.0,"('word_vec_size', 16),"
0.3.0,"('rnn_size', 16)],"
0.3.0,""""""" Only do SRU test if requirment is safisfied. """""""
0.3.0,SRU doesn't support input_feed.
0.3.0,!/usr/bin/env python
0.3.0,-*- coding: utf-8 -*-
0.3.0,Remove the generated *pt files.
0.3.0,4 specicials + 2 words (since we pass 2 to merge_vocabs)
0.3.0,Test image preprocessing
0.3.0,Test audio preprocessing
0.3.0,Basic attributes.
0.3.0,Build the RNN.
0.3.0,Set up the context gate.
0.3.0,Set up the standard attention.
0.3.0,"Set up a separated copy attention layer, if needed."
0.3.0,Check
0.3.0,tgt.size() returns tgt length and batch
0.3.0,END
0.3.0,Run the forward pass of the RNN.
0.3.0,Update the state with the result.
0.3.0,Concatenates sequence of tensors along a new dimension.
0.3.0,NOTE: v0.3 to 0.4: decoder_outputs / attns[*] may not be list
0.3.0,(in particular in case of SRU) it was not raising error in 0.3
0.3.0,since stack(Variable) was allowed.
0.3.0,"In 0.4, SRU returns a tensor that shouldn't be stacke"
0.3.0,The encoder hidden is  (layers*directions) x batch x dim.
0.3.0,We need to convert it to layers x batch x (directions*dim).
0.3.0,Initialize local and return variables.
0.3.0,Run the forward pass of the RNN.
0.3.0,Check
0.3.0,END
0.3.0,Calculate the attention.
0.3.0,Calculate the context gate.
0.3.0,Additional args check.
0.3.0,END Additional args check.
0.3.0,Initialize local and return variables.
0.3.0,Input feed concatenates hidden state with
0.3.0,input at every time step.
0.3.0,TODO: context gate should be employed
0.3.0,instead of second RNN transform.
0.3.0,Update the coverage attention.
0.3.0,Run the forward pass of the copy attention layer.
0.3.0,Return result.
0.3.0,Init the input feed.
0.3.0,Basic attributes.
0.3.0,Build the CNN.
0.3.0,CNNDecoder has its own attention mechanism.
0.3.0,"Set up a separated copy attention layer, if needed."
0.3.0,NOTE: memory_lengths is only here for compatibility reasons
0.3.0,with onmt.modules.RNNDecoderBase.forward()
0.3.0,CHECKS
0.3.0,END CHECKS
0.3.0,Initialize return variables.
0.3.0,The output of CNNEncoder.
0.3.0,The combination of output of CNNEncoder and source embeddings.
0.3.0,Run the forward pass of the CNNDecoder.
0.3.0,Process the result and update the attentions.
0.3.0,Update the state.
0.3.0,Memory_lengths is a single tensor shared between all models.
0.3.0,This assumption will not hold if Translator is modified
0.3.0,to calculate memory_lengths as something other than the length
0.3.0,of the input.
0.3.0,"Register self.mask as a buffer in TransformerDecoderLayer, so"
0.3.0,it gets TransformerDecoderLayer's cuda behavior automatically.
0.3.0,Basic attributes.
0.3.0,Build TransformerDecoder.
0.3.0,TransformerDecoder has its own attention mechanism.
0.3.0,"Set up a separated copy attention layer, if needed."
0.3.0,Initialize return variables.
0.3.0,Run the forward pass of the TransformerDecoder.
0.3.0,Process the result and update the attentions.
0.3.0,"buffer size in bytes, determine equiv. # of elements based on data type"
0.3.0,copy tensors into buffer_t
0.3.0,all-reduce and rescale
0.3.0,copy all-reduced buffer back into tensors
0.3.0,"tensor is bigger than buffer, all-reduce and rescale directly"
0.3.0,"buffer is full, all-reduce and replace buffer with grad"
0.3.0,add tensor to buffer
0.3.0,We need to save a copy of optim.optimizer.state_dict() for setting
0.3.0,"the, optimizer state later on in Stage 2 in this method, since"
0.3.0,the method optim.set_parameters(model.parameters()) will overwrite
0.3.0,"optim.optimizer, and with ith the values stored in"
0.3.0,optim.optimizer.state_dict()
0.3.0,Stage 1:
0.3.0,Essentially optim.set_parameters (re-)creates and optimizer using
0.3.0,model.paramters() as parameters that will be stored in the
0.3.0,optim.optimizer.param_groups field of the torch optimizer class.
0.3.0,"Importantly, this method does not yet load the optimizer state, as"
0.3.0,essentially it builds a new optimizer with empty optimizer state and
0.3.0,parameters from the model.
0.3.0,"Stage 2: In this stage, which is only performed when loading an"
0.3.0,"optimizer from a checkpoint, we load the saved_optimizer_state_dict"
0.3.0,"into the re-created optimizer, to set the optim.optimizer.state"
0.3.0,"field, which was previously empty. For this, we use the optimizer"
0.3.0,"state saved in the ""saved_optimizer_state_dict"" variable for"
0.3.0,this purpose.
0.3.0,See also: https://github.com/pytorch/pytorch/issues/2830
0.3.0,Convert back the state values to cuda type if applicable
0.3.0,We want to make sure that indeed we have a non-empty optimizer state
0.3.0,when we loaded an existing model. This should be at least the case
0.3.0,"for Adam, which saves ""exp_avg"" and ""exp_avg_sq"" state"
0.3.0,(Exponential moving average of gradient and squared gradient values)
0.3.0,Decay method used in tensor2tensor.
0.3.0,Decay based on start_decay_steps every decay_steps
0.3.0,-*- coding: utf-8 -*-
0.3.0,"for sparsemax loss, the loss function operates on the raw output"
0.3.0,"vector, not a probability vector. Hence it's only necessary to"
0.3.0,apply the first part of the generator here.
0.3.0,non_none: the subdict of the state dictionary where the values
0.3.0,are not None.
0.3.0,"Now, the iteration:"
0.3.0,state is a dictionary of sequences of tensor-like but we
0.3.0,want a sequence of dictionaries of tensors.
0.3.0,"First, unzip the dictionary into a sequence of keys and a"
0.3.0,sequence of tensor-like sequences.
0.3.0,"Now, yield a dictionary for each shard. The keys are always"
0.3.0,the same. values is a sequence of length #keys where each
0.3.0,element is a sequence of length #shards. We want to iterate
0.3.0,"over the shards, not over the keys: therefore, the values need"
0.3.0,to be re-zipped by shard and then each shard can be paired
0.3.0,with the keys.
0.3.0,Assumed backprop'd
0.3.0,Log the progress using the number of batches on the x-axis.
0.3.0,Get a list of world_size lists with len(stat_list) Statistics objects
0.3.0,SRU doesn't support PackedSequence.
0.3.0,-*- coding: utf-8 -*-
0.3.0,coding: utf-8
0.3.0,Below are helper functions for intra-class use only.
0.3.0,-*- coding: utf-8 -*-
0.3.0,Hack. Can't pickle defaultdict :(
0.3.0,"For all data types, the tgt side corpus is in form of text."
0.3.0,Prop src from field to get lower memory using when training with image
0.3.0,Load vocabulary
0.3.0,Drop the none-using from memory but keep the last
0.3.0,"All datasets have same num of n_tgt_features,"
0.3.0,getting the last one is OK.
0.3.0,"All datasets have same num of n_src_features,"
0.3.0,getting the last one is OK.
0.3.0,Merge the input and output vocabularies.
0.3.0,`tgt_vocab_size` is ignored when sharing vocabularies
0.3.0,We have at least one dataset.
0.3.0,"We return the len of cur_dataset, otherwise we need to load"
0.3.0,"all datasets to determine the real len, which loses the benefit"
0.3.0,of lazy loading.
0.3.0,Drop the current dataset for decreasing memory
0.3.0,"We clear `fields` when saving, restore when loading."
0.3.0,Sort batch by decreasing lengths of sentence required by pytorch.
0.3.0,"sort=False means ""Use dataset's sortkey instead of iterator's""."
0.3.0,Maintains the longest src and tgt length in the current batch
0.3.0,Reset current longest length at a new batch (count=1)
0.3.0,Src: <bos> w1 ... wN <eos>
0.3.0,Tgt: w1 ... wN <eos>
0.3.0,Sort the glob output by file name (by increasing indexes).
0.3.0,"Only one inputters.*Dataset, simple!"
0.3.0,-*- coding: utf-8 -*-
0.3.0,Peek at the first to see which fields are used.
0.3.0,"If out_examples is a generator, we need to save the filter_pred"
0.3.0,"function in serialization too, which would cause a problem when"
0.3.0,`torch.save()`. Thus we materialize it as a list.
0.3.0,STFT
0.3.0,-*- coding: utf-8 -*-
0.3.0,Peek at the first to see which fields are used.
0.3.0,"If out_examples is a generator, we need to save the filter_pred"
0.3.0,"function in serialization too, which would cause a problem when"
0.3.0,`torch.save()`. Thus we materialize it as a list.
0.3.0,-*- coding: utf-8 -*-
0.3.0,"self.src_vocabs: mutated in dynamic_dict, used in"
0.3.0,collapse_copy_scores and in Translator.py
0.3.0,Each element of an example is a dictionary whose keys represents
0.3.0,at minimum the src tokens and their indices and potentially also
0.3.0,the src and tgt features and alignment information.
0.3.0,Peek at the first to see which fields are used.
0.3.0,"If out_examples is a generator, we need to save the filter_pred"
0.3.0,"function in serialization too, which would cause a problem when"
0.3.0,`torch.save()`. Thus we materialize it as a list.
0.3.0,"Default to a balanced sort, prioritizing tgt len match."
0.3.0,TODO: make this configurable.
0.3.0,"All examples have same number of features, so we peek first one"
0.3.0,to get the num_feats.
0.3.0,Chain back the first element - we only want to peek it.
0.3.0,Below are helper functions for intra-class use only.
0.3.0,Mapping source tokens to indices in the dynamic dict.
0.3.0,"The codecs module seems to have bugs with seek()/tell(),"
0.3.0,so we use io.open().
0.3.0,"We have associate iterator, just yields tuples"
0.3.0,util we run parallel with it.
0.3.0,Yield tuples util this shard's size reaches the threshold.
0.3.0,This part of check is time consuming on Py2 (but
0.3.0,"it is quite fast on Py3, weird!). So we don't bother"
0.3.0,to check for very line. Instead we chekc every 64
0.3.0,lines. Thus we are not dividing exactly per
0.3.0,"`shard_size`, but it is not too much difference."
0.3.0,All examples must have same number of features.
0.3.0,flake8: noqa
0.3.0,For command-line option parsing
0.3.0,"Check pass, set the args."
0.3.0,"This SRU version implements its own cuda-level optimization,"
0.3.0,so it requires that:
0.3.0,1. `cupy` and `pynvrtc` python package installed.
0.3.0,2. pytorch is built with cuda support.
0.3.0,3. library path set: export LD_LIBRARY_PATH=<cuda lib path>.
0.3.0,Check 1.
0.3.0,Check 2.
0.3.0,Check 3.
0.3.0,This sets up device to use.
0.3.0,-> directions x batch x dim
0.3.0,For DEBUG
0.3.0,"size = (length, batch, x.size(-1)) \"
0.3.0,"if x.dim() == 3 else (batch, x.size(-1))"
0.3.0,grad_x = x.new(*x.size()) if k_ == 3 else x.new(*size).zero_()
0.3.0,Normal use
0.3.0,"An entry check here, will catch on train side and translate side"
0.3.0,if requirements are not satisfied.
0.3.0,RNNDecoderState wraps hidden as a tuple.
0.3.0,fh -> (layers*directions) x batch x dim
0.3.0,Not yet supported on multi-gpu
0.3.0,The score for each translation on the beam.
0.3.0,The backpointers at each time-step.
0.3.0,The outputs at each time-step.
0.3.0,Has EOS topped the beam yet.
0.3.0,The attentions (matrix) for each time.
0.3.0,Time and k pair for finished.
0.3.0,Information for global scoring.
0.3.0,Minimum prediction length
0.3.0,Apply Penalty at every step
0.3.0,force the output to be longer than self.min_length
0.3.0,Sum the previous scores.
0.3.0,Don't let EOS have children.
0.3.0,Block ngram repeats
0.3.0,"Last n tokens, n = block_ngram_repeat"
0.3.0,Skip the blocking if it is in the exclusion list
0.3.0,"best_scores_id is flattened beam x word array, so calculate which"
0.3.0,word and beam each score came from
0.3.0,End condition is when top-of-beam is EOS and no global score.
0.3.0,Add from beam until we have minimum outputs.
0.3.0,Term will be subtracted from probability
0.3.0,Probability will be divided by this
0.3.0,!/usr/bin/env python
0.3.0,use ensemble decoding if more than one model is specified
0.3.0,for debugging
0.3.0,Statistics
0.3.0,Debug attention.
0.3.0,TODO: faster code path for beam_size == 1.
0.3.0,TODO: support these blacklisted features.
0.3.0,Encoder forward.
0.3.0,Tile states and memory beam_size times.
0.3.0,Give full probability to the first beam on the first step.
0.3.0,Structure that holds finished hypotheses.
0.3.0,Decoder forward.
0.3.0,Generator forward.
0.3.0,Multiply probs by the beam probability.
0.3.0,Flatten probs into a list of possibilities.
0.3.0,Recover log probs.
0.3.0,Resolve beam origin and true word ids.
0.3.0,Map beam_index to batch_index in the flat representation.
0.3.0,Append last prediction.
0.3.0,End condition is top beam is finished.
0.3.0,Save finished hypotheses.
0.3.0,Store finished hypotheses for this batch.
0.3.0,"If the batch reached the end, save the n_best hypotheses."
0.3.0,"If all sentences are translated, no need to go further."
0.3.0,Remove finished batches for the next step.
0.3.0,Reorder states.
0.3.0,(0) Prep each of the components of the search.
0.3.0,And helper method for reducing verbosity.
0.3.0,Define a list of tokens to exclude from ngram-blocking
0.3.0,"exclusion_list = [""<t>"", ""</t>"", "".""]"
0.3.0,Help functions for working with beams and batches
0.3.0,(1) Run the encoder on the src.
0.3.0,(2) Repeat src objects `beam_size` times.
0.3.0,"(3) run the decoder to generate sentences, using beam search."
0.3.0,Construct batch x beam_size nxt words.
0.3.0,Get all the pending current beam words and arrange for forward.
0.3.0,Turn any copied words to UNKs
0.3.0,0 is unk
0.3.0,Temporary kludge solution to handle changed dim expectation
0.3.0,in the decoder
0.3.0,Run one step.
0.3.0,dec_out: beam x rnn_size
0.3.0,(b) Compute a vector of batch x beam word scores.
0.3.0,beam x tgt_vocab
0.3.0,beam x (tgt_vocab + extra_vocab)
0.3.0,beam x tgt_vocab
0.3.0,(c) Advance each beam.
0.3.0,(4) Extract sentences from beam.
0.3.0,(1) run the encoder on the src
0.3.0,"(2) if a target is specified, compute the 'goldScore'"
0.3.0,(i.e. log likelihood) of the target under the model
0.3.0,Log prob of each word.
0.3.0,Rollback pointer to the beginning.
0.3.0,!/usr/bin/env python
0.3.0,backwards compatibility for confs
0.3.0,load can be called multiple times: modify copy
0.3.0,NOTE: translator returns lists of `n_best` list
0.3.0,we can ignore that (i.e. flatten lists) only because
0.3.0,we restrict `n_best=1`
0.3.0,build back results with empty texts
0.3.0,Sorting
0.2.1,!/usr/bin/env python
0.2.1,!/usr/bin/env python
0.2.1,!/usr/bin/env python
0.2.1,-*- coding: utf-8 -*-
0.2.1,!/usr/bin/env python
0.2.1,-*- coding: utf-8 -*-
0.2.1,We will use glob.glob() to find sharded {train|valid}.[0-9]*.pt
0.2.1,"when training, so check to avoid tampering with existing pt files"
0.2.1,or mixing them up.
0.2.1,"We save fields in vocab.pt separately, so make it empty."
0.2.1,Currently we only do preprocess sharding for corpus: data_type=='text'.
0.2.1,"For data_type == 'img' or 'audio', currently we don't do"
0.2.1,preprocess sharding. We only build a monolithic dataset.
0.2.1,"But since the interfaces are uniform, it would be not hard"
0.2.1,to do this should users need this feature.
0.2.1,"We save fields in vocab.pt seperately, so make it empty."
0.2.1,"Can't save fields, so remove/reconstruct at training time."
0.2.1,!/usr/bin/env python
0.2.1,!/usr/bin/env python3
0.2.1,-*- coding: utf-8 -*-
0.2.1,
0.2.1,"OpenNMT-py documentation build configuration file, created by"
0.2.1,sphinx-quickstart on Sun Dec 17 12:07:14 2017.
0.2.1,
0.2.1,This file is execfile()d with the current directory set to its
0.2.1,containing dir.
0.2.1,
0.2.1,Note that not all possible configuration values are present in this
0.2.1,autogenerated file.
0.2.1,
0.2.1,All configuration values have a default; values that are commented out
0.2.1,serve to show the default.
0.2.1,"If extensions (or modules to document with autodoc) are in another directory,"
0.2.1,add these directories to sys.path here. If the directory is relative to the
0.2.1,"documentation root, use os.path.abspath to make it absolute, like shown here."
0.2.1,
0.2.1,import os
0.2.1,import sys
0.2.1,"sys.path.insert(0, os.path.abspath('.'))"
0.2.1,-- General configuration ------------------------------------------------
0.2.1,"If your documentation needs a minimal Sphinx version, state it here."
0.2.1,
0.2.1,needs_sphinx = '1.0'
0.2.1,"Add any Sphinx extension module names here, as strings. They can be"
0.2.1,extensions coming with Sphinx (named 'sphinx.ext.*') or your custom
0.2.1,ones.
0.2.1,"Add any paths that contain templates here, relative to this directory."
0.2.1,The suffix(es) of source filenames.
0.2.1,You can specify multiple suffix as a list of string:
0.2.1,
0.2.1,"source_suffix = ['.rst', '.md']"
0.2.1,The master toctree document.
0.2.1,General information about the project.
0.2.1,"The version info for the project you're documenting, acts as replacement for"
0.2.1,"|version| and |release|, also used in various other places throughout the"
0.2.1,built documents.
0.2.1,
0.2.1,The short X.Y version.
0.2.1,"The full version, including alpha/beta/rc tags."
0.2.1,The language for content autogenerated by Sphinx. Refer to documentation
0.2.1,for a list of supported languages.
0.2.1,
0.2.1,This is also used if you do content translation via gettext catalogs.
0.2.1,"Usually you set ""language"" from the command line for these cases."
0.2.1,"List of patterns, relative to source directory, that match files and"
0.2.1,directories to ignore when looking for source files.
0.2.1,This patterns also effect to html_static_path and html_extra_path
0.2.1,The name of the Pygments (syntax highlighting) style to use.
0.2.1,"If true, `todo` and `todoList` produce output, else they produce nothing."
0.2.1,-- Options for HTML output ----------------------------------------------
0.2.1,The theme to use for HTML and HTML Help pages.  See the documentation for
0.2.1,a list of builtin themes.
0.2.1,
0.2.1,html_theme = 'sphinx_materialdesign_theme'
0.2.1,html_theme_path = [sphinx_materialdesign_theme.get_path()]
0.2.1,Theme options are theme-specific and customize the look and feel of a theme
0.2.1,"further.  For a list of options available for each theme, see the"
0.2.1,documentation.
0.2.1,
0.2.1,html_theme_options = {}
0.2.1,"Add any paths that contain custom static files (such as style sheets) here,"
0.2.1,"relative to this directory. They are copied after the builtin static files,"
0.2.1,"so a file named ""default.css"" will overwrite the builtin ""default.css""."
0.2.1,"Custom sidebar templates, must be a dictionary that maps document names"
0.2.1,to template names.
0.2.1,
0.2.1,This is required for the alabaster theme
0.2.1,refs: http://alabaster.readthedocs.io/en/latest/installation.html#sidebars
0.2.1,-- Options for HTMLHelp output ------------------------------------------
0.2.1,Output file base name for HTML help builder.
0.2.1,-- Options for LaTeX output ---------------------------------------------
0.2.1,The paper size ('letterpaper' or 'a4paper').
0.2.1,
0.2.1,"'papersize': 'letterpaper',"
0.2.1,"The font size ('10pt', '11pt' or '12pt')."
0.2.1,
0.2.1,"'pointsize': '10pt',"
0.2.1,Additional stuff for the LaTeX preamble.
0.2.1,
0.2.1,"'preamble': '',"
0.2.1,Latex figure (float) alignment
0.2.1,
0.2.1,"'figure_align': 'htbp',"
0.2.1,Grouping the document tree into LaTeX files. List of tuples
0.2.1,"(source start file, target name, title,"
0.2.1,"author, documentclass [howto, manual, or own class])."
0.2.1,-- Options for manual page output ---------------------------------------
0.2.1,One entry per manual page. List of tuples
0.2.1,"(source start file, name, description, authors, manual section)."
0.2.1,-- Options for Texinfo output -------------------------------------------
0.2.1,Grouping the document tree into Texinfo files. List of tuples
0.2.1,"(source start file, target name, title, author,"
0.2.1,"dir menu entry, description, category)"
0.2.1,!/usr/bin/env python
0.2.1,-*- coding: utf-8 -*-
0.2.1,"the vocab object is a list of tuple (name, torchtext.Vocab)"
0.2.1,we iterate over this list and associate vocabularies based on the name
0.2.1,"Add in default model arguments, possibly added since training."
0.2.1,"the vocab object is a list of tuple (name, torchtext.Vocab)"
0.2.1,we iterate over this list and associate vocabularies based on the name
0.2.1,-*- encoding: utf-8 -*-
0.2.1,!/usr/bin/env python
0.2.1,-*- coding: utf-8 -*-
0.2.1,Author: Rico Sennrich
0.2.1,flake8: noqa
0.2.1,This file is retrieved from https://github.com/rsennrich/subword-nmt
0.2.1,hack for python2/3 compatibility
0.2.1,check version information
0.2.1,some hacking to deal with duplicates (only consider first instance)
0.2.1,don't print end-of-word symbols
0.2.1,sys.stderr.write('cannot split {0} further.\n'.format(segment))
0.2.1,sys.stderr.write('OOV: {0}\n'.format(segment))
0.2.1,sys.stderr.write('OOV: {0}\n'.format(segment))
0.2.1,python 2/3 compatibility
0.2.1,read/write files as UTF-8
0.2.1,!/usr/bin/env python
0.2.1,!/usr/bin/env python
0.2.1,-*- coding: utf-8 -*-
0.2.1,Author: Rico Sennrich
0.2.1,flake8: noqa
0.2.1,This file is retrieved from https://github.com/rsennrich/subword-nmt
0.2.1,hack for python2/3 compatibility
0.2.1,"find all instances of pair, and update frequency/indices around it"
0.2.1,find first symbol
0.2.1,"if first symbol is followed by second symbol, we've found an occurrence of pair (old_word[i:i+2])"
0.2.1,"assuming a symbol sequence ""A B C"", if ""B C"" is merged, reduce the frequency of ""A B"""
0.2.1,"assuming a symbol sequence ""A B C B"", if ""B C"" is merged, reduce the frequency of ""C B""."
0.2.1,"however, skip this if the sequence is A B C B C, because the frequency of ""C B"" will be reduced by the previous code block"
0.2.1,find new pair
0.2.1,"assuming a symbol sequence ""A BC D"", if ""B C"" is merged, increase the frequency of ""A BC"""
0.2.1,"assuming a symbol sequence ""A BC B"", if ""B C"" is merged, increase the frequency of ""BC B"""
0.2.1,"however, if the sequence is A BC BC, skip this step because the count of ""BC BC"" will be incremented by the previous code block"
0.2.1,data structure of pair frequencies
0.2.1,index from pairs to words
0.2.1,version 0.2 changes the handling of the end-of-word token ('</w>');
0.2.1,version numbering allows bckward compatibility
0.2.1,"threshold is inspired by Zipfian assumption, but should only affect speed"
0.2.1,we probably missed the best pair because of pruning; go back to full statistics
0.2.1,"threshold is inspired by Zipfian assumption, but should only affect speed"
0.2.1,python 2/3 compatibility
0.2.1,read/write files as UTF-8
0.2.1,!/usr/bin/env python
0.2.1,"""rnn"" or ""brnn"""
0.2.1,Build encoder.
0.2.1,Build decoder.
0.2.1,Share the embedding matrix - preprocess with share_vocab required.
0.2.1,src/tgt vocab should be the same if `-share_vocab` is specified.
0.2.1,Build NMTModel(= encoder + decoder).
0.2.1,Build Generator.
0.2.1,Load the model states from checkpoint or initialize them.
0.2.1,Add generator to model (this registers it as parameter of model).
0.2.1,!/usr/bin/env python
0.2.1,Create a thread to listen for errors in the child processes.
0.2.1,Train with multiprocessing.
0.2.1,"propagate exception to parent process, keeping original traceback"
0.2.1,!/usr/bin/env python
0.2.1,this one is needed for torchtext random call (shuffled iterator)
0.2.1,in multi gpu it ensures datasets are read in the same order
0.2.1,These ensure same initialization in multi gpu mode
0.2.1,Load checkpoint if we resume from a previous training.
0.2.1,Peek the first dataset to determine the data_type.
0.2.1,(All datasets have the same data_type).
0.2.1,Load fields generated from preprocess phase.
0.2.1,Report src/tgt features.
0.2.1,Build model.
0.2.1,Build optimizer.
0.2.1,Build model saver
0.2.1,Do training.
0.2.1,Embedding Options
0.2.1,Encoder-Decoder Options
0.2.1,"group.add_argument('-residual',   action=""store_true"","
0.2.1,"help=""Add residual connections between RNN layers."")"
0.2.1,Attention options
0.2.1,Generator and loss options.
0.2.1,Data options
0.2.1,"Dictionary options, for text corpus"
0.2.1,"Truncation options, for text corpus"
0.2.1,Data processing options
0.2.1,Options most relevant to speech
0.2.1,GPU
0.2.1,Init options
0.2.1,Pretrained word vectors
0.2.1,Fixed word vectors
0.2.1,Optimization options
0.2.1,learning rate
0.2.1,Use TensorboardX for visualization during training
0.2.1,Options most relevant to speech
0.2.1,Options most relevant to summarization.
0.2.1,Alpha and Beta values for Google Length + Coverage penalty
0.2.1,"Described here: https://arxiv.org/pdf/1609.08144.pdf, Section 7"
0.2.1,Options most relevant to speech.
0.2.1,MARKDOWN boilerplate
0.2.1,Copyright 2016 The Chromium Authors. All rights reserved.
0.2.1,Use of this source code is governed by a BSD-style license that can be
0.2.1,found in the LICENSE file.
0.2.1,**section heading**:
0.2.1,# **--argument-one**
0.2.1,Basic attributes.
0.2.1,Set model in training mode.
0.2.1,Set model in validating mode.
0.2.1,F-prop through the model.
0.2.1,Compute loss.
0.2.1,Update statistics.
0.2.1,Set model back to training mode.
0.2.1,Truncated BPTT: reminder not compatible with accum > 1
0.2.1,1. Create truncated target.
0.2.1,2. F-prop all but generator.
0.2.1,3. Compute loss in shards for memory efficiency.
0.2.1,4. Update the parameters and statistics.
0.2.1,Multi GPU gradient gather
0.2.1,"If truncated, don't backprop fully."
0.2.1,"in case of multi step gradient accumulation,"
0.2.1,update only after accum batches
0.2.1,For Flake
0.2.1,Initialize the bridge layer
0.2.1,"s_len, batch, emb_dim = emb.size()"
0.2.1,Lengths data is wrapped inside a Tensor.
0.2.1,"LSTM has hidden and cell state, other only one"
0.2.1,Total number of states
0.2.1,Build a linear layer for each
0.2.1,"(batch_size, 1, nfft, t)"
0.2.1,layer 1
0.2.1,"(batch_size, 32, nfft/2, t/2)"
0.2.1,"(batch_size, 32, nfft/2/2, t/2)"
0.2.1,layer 2
0.2.1,"(batch_size, 32, nfft/2/2, t/2)"
0.2.1,"s_len, batch, emb_dim = emb.size()"
0.2.1,from onmt.utils.misc import aeq
0.2.1,Run the forward pass of every layer of the tranformer.
0.2.1,"(batch_size, 64, imgH, imgW)"
0.2.1,layer 1
0.2.1,"(batch_size, 64, imgH/2, imgW/2)"
0.2.1,"(batch_size, 128, imgH/2, imgW/2)"
0.2.1,layer 2
0.2.1,"(batch_size, 128, imgH/2/2, imgW/2/2)"
0.2.1,"(batch_size, 256, imgH/2/2, imgW/2/2)"
0.2.1,layer 3
0.2.1,batch norm 1
0.2.1,"(batch_size, 256, imgH/2/2, imgW/2/2)"
0.2.1,layer4
0.2.1,"(batch_size, 256, imgH/2/2/2, imgW/2/2)"
0.2.1,"(batch_size, 512, imgH/2/2/2, imgW/2/2)"
0.2.1,layer 5
0.2.1,batch norm 2
0.2.1,"(batch_size, 512, imgH/2/2/2, imgW/2/2/2)"
0.2.1,"(batch_size, 512, imgH/2/2/2, imgW/2/2/2)"
0.2.1,"# (batch_size, 512, H, W)"
0.2.1,Dimensions and padding for constructing the word embedding matrix
0.2.1,Dimensions and padding for feature embedding matrices
0.2.1,(these have no effect if feat_vocab_sizes is empty)
0.2.1,The embedding matrix look-up tables. The first look-up table
0.2.1,"is for words. Subsequent ones are for features, if any exist."
0.2.1,The final output size of word + feature vectors. This can vary
0.2.1,from the word vector size if and only if features are defined.
0.2.1,This is the attribute you should access if you need to know
0.2.1,how big your embeddings are going to be.
0.2.1,The sequence of operations that converts the input sequence
0.2.1,into a sequence of embeddings. At minimum this consists of
0.2.1,looking up the embeddings for each word and feature in the
0.2.1,input. Model parameters may require the sequence to contain
0.2.1,additional operations as well.
0.2.1,This class is mainly used by decoder.py for RNNs but also
0.2.1,by the CNN / transformer decoder when copy attention is used
0.2.1,CNN has its own attention mechanism ConvMultiStepAttention
0.2.1,Transformer has its own MultiHeadedAttention
0.2.1,mlp wants it with bias
0.2.1,Check input sizes
0.2.1,"(batch, t_len, d) x (batch, d, s_len) --> (batch, t_len, s_len)"
0.2.1,"(batch, t_len, s_len, d)"
0.2.1,one step input
0.2.1,"compute attention scores, as in Luong et al."
0.2.1,Softmax or sparsemax to normalize attention weights
0.2.1,each context vector c_t is the weighted average
0.2.1,over all the source hidden states
0.2.1,concatenate
0.2.1,Check output sizes
0.2.1,Check output sizes
0.2.1,clamping necessary because of numerical errors: loss should be lower
0.2.1,"bounded by zero, but negative values near zero are possible without"
0.2.1,the clamp
0.2.1,from onmt.utils.misc import aeq
0.2.1,CHECKS
0.2.1,"batch, k_len, d = key.size()"
0.2.1,"batch_, k_len_, d_ = value.size()"
0.2.1,"aeq(batch, batch_)"
0.2.1,"aeq(k_len, k_len_)"
0.2.1,"aeq(d, d_)"
0.2.1,"batch_, q_len, d_ = query.size()"
0.2.1,"aeq(batch, batch_)"
0.2.1,"aeq(d, d_)"
0.2.1,"aeq(self.model_dim % 8, 0)"
0.2.1,if mask is not None:
0.2.1,"batch_, q_len_, k_len_ = mask.size()"
0.2.1,"aeq(batch_, batch)"
0.2.1,"aeq(k_len_, k_len)"
0.2.1,aeq(q_len_ == q_len)
0.2.1,END CHECKS
0.2.1,"1) Project key, value, and query."
0.2.1,2) Calculate and scale scores.
0.2.1,3) Apply attention dropout and compute context vectors.
0.2.1,CHECK
0.2.1,"batch_, q_len_, d_ = output.size()"
0.2.1,"aeq(q_len, q_len_)"
0.2.1,"aeq(batch, batch_)"
0.2.1,"aeq(d, d_)"
0.2.1,Return one attn
0.2.1,At the moment this class is only used by embeddings.Embeddings look-up tables
0.2.1,-*- coding: utf-8 -*-
0.2.1,checks
0.2.1,"batch, channel, height, width = base_target_emb.size()"
0.2.1,"batch_, channel_, height_, width_ = input_from_dec.size()"
0.2.1,"enc_batch, enc_channel, enc_height = encoder_out_top.size()"
0.2.1,"enc_batch_, enc_channel_, enc_height_ = encoder_out_combine.size()"
0.2.1,out_features * in_features
0.2.1,norm is out_features * 1
0.2.1,batch_size * out_features
0.2.1,out_features
0.2.1,out_features
0.2.1,batch_size * out_features
0.2.1,"out_channels, in_channels // groups, * kernel_size"
0.2.1,out_features
0.2.1,This is used nowhere in the code at the moment (Vincent Nguyen 05/18/2018)
0.2.1,"in_channels, out_channels, *kernel_size"
0.2.1,"in_channels, out_channels, *kernel_size"
0.2.1,"self.out_channels, 1"
0.2.1,out_features
0.2.1,out_features
0.2.1,store roots on diagonal
0.2.1,CHECKS
0.2.1,Original probabilities.
0.2.1,Probability of copying p(z=1) batch.
0.2.1,Probibility of not copying: p_{word}(w) * (1 - p(z))
0.2.1,Compute unks in align and target for readability
0.2.1,Copy probability of tokens in source
0.2.1,Set scores for unk to 0 and add eps
0.2.1,Get scores for tokens in target
0.2.1,Regular prob (no unks and unks that can't be copied)
0.2.1,Add score for non-unks in target
0.2.1,Add score for when word is unk in both align and tgt
0.2.1,Forced copy. Add only probability for not-copied tokens
0.2.1,Drop padding.
0.2.1,"We lazily load datasets when there are more than one, so postpone"
0.2.1,the setting of cur_dataset.
0.2.1,Correct target copy token instead of <unk>
0.2.1,tgt[i] = align[i] + len(tgt_vocab)
0.2.1,for i such that tgt[i] == 0 and align[i] != 0
0.2.1,Compute sum of perplexities for stats
0.2.1,Compute Loss as NLL divided by seq length
0.2.1,Compute Sequence Lengths
0.2.1,Compute Total Loss per sequence in batch
0.2.1,Divide by length of each sequence and sum
0.2.1,illegal_weights_mask = torch.ByteTensor([
0.2.1,"[0, 0, 0, 0, 0, 0, 0],"
0.2.1,"[0, 0, 0, 1, 1, 1, 1],"
0.2.1,"[0, 0, 0, 0, 0, 1, 1],"
0.2.1,"[0, 0, 1, 1, 1, 1, 1]])"
0.2.1,TODO: fix for pytorch 0.3
0.2.1,illegal_weights = alignments.masked_select(illegal_weights_mask)
0.2.1,"self.assertEqual(0.0, illegal_weights.data.sum())"
0.2.1,"-data option is required, but not used in this test, so dummy."
0.2.1,Helper to generate a vocabulary
0.2.1,len x batch x nfeat
0.2.1,batch x c x h x w
0.2.1,batch x 1 x nfft x t
0.2.1,Initialize vectors to compare size with
0.2.1,Ensure correct sizes and types
0.2.1,Make sure that output has the correct size and type
0.2.1,Make sure that output has the correct size and type
0.2.1,Make sure that output has the correct size and type
0.2.1,"[('encoder_type', 'transformer'),"
0.2.1,"('word_vec_size', 16), ('rnn_size', 16)],"
0.2.1,"[('encoder_type', 'transformer'),"
0.2.1,"('word_vec_size', 16),"
0.2.1,"('rnn_size', 16)],"
0.2.1,""""""" Only do SRU test if requirment is safisfied. """""""
0.2.1,SRU doesn't support input_feed.
0.2.1,!/usr/bin/env python
0.2.1,-*- coding: utf-8 -*-
0.2.1,Remove the generated *pt files.
0.2.1,4 specicials + 2 words (since we pass 2 to merge_vocabs)
0.2.1,Test image preprocessing
0.2.1,Test audio preprocessing
0.2.1,Basic attributes.
0.2.1,Build the RNN.
0.2.1,Set up the context gate.
0.2.1,Set up the standard attention.
0.2.1,"Set up a separated copy attention layer, if needed."
0.2.1,Check
0.2.1,tgt.size() returns tgt length and batch
0.2.1,END
0.2.1,Run the forward pass of the RNN.
0.2.1,Update the state with the result.
0.2.1,Concatenates sequence of tensors along a new dimension.
0.2.1,NOTE: v0.3 to 0.4: decoder_outputs / attns[*] may not be list
0.2.1,(in particular in case of SRU) it was not raising error in 0.3
0.2.1,since stack(Variable) was allowed.
0.2.1,"In 0.4, SRU returns a tensor that shouldn't be stacke"
0.2.1,The encoder hidden is  (layers*directions) x batch x dim.
0.2.1,We need to convert it to layers x batch x (directions*dim).
0.2.1,Initialize local and return variables.
0.2.1,Run the forward pass of the RNN.
0.2.1,Check
0.2.1,END
0.2.1,Calculate the attention.
0.2.1,Calculate the context gate.
0.2.1,Additional args check.
0.2.1,END Additional args check.
0.2.1,Initialize local and return variables.
0.2.1,Input feed concatenates hidden state with
0.2.1,input at every time step.
0.2.1,TODO: context gate should be employed
0.2.1,instead of second RNN transform.
0.2.1,Update the coverage attention.
0.2.1,Run the forward pass of the copy attention layer.
0.2.1,Return result.
0.2.1,Init the input feed.
0.2.1,Basic attributes.
0.2.1,Build the CNN.
0.2.1,CNNDecoder has its own attention mechanism.
0.2.1,"Set up a separated copy attention layer, if needed."
0.2.1,NOTE: memory_lengths is only here for compatibility reasons
0.2.1,with onmt.modules.RNNDecoderBase.forward()
0.2.1,CHECKS
0.2.1,END CHECKS
0.2.1,Initialize return variables.
0.2.1,The output of CNNEncoder.
0.2.1,The combination of output of CNNEncoder and source embeddings.
0.2.1,Run the forward pass of the CNNDecoder.
0.2.1,Process the result and update the attentions.
0.2.1,Update the state.
0.2.1,Memory_lengths is a single tensor shared between all models.
0.2.1,This assumption will not hold if Translator is modified
0.2.1,to calculate memory_lengths as something other than the length
0.2.1,of the input.
0.2.1,"Register self.mask as a buffer in TransformerDecoderLayer, so"
0.2.1,it gets TransformerDecoderLayer's cuda behavior automatically.
0.2.1,Basic attributes.
0.2.1,Build TransformerDecoder.
0.2.1,TransformerDecoder has its own attention mechanism.
0.2.1,"Set up a separated copy attention layer, if needed."
0.2.1,Initialize return variables.
0.2.1,Run the forward pass of the TransformerDecoder.
0.2.1,Process the result and update the attentions.
0.2.1,"buffer size in bytes, determine equiv. # of elements based on data type"
0.2.1,copy tensors into buffer_t
0.2.1,all-reduce and rescale
0.2.1,copy all-reduced buffer back into tensors
0.2.1,"tensor is bigger than buffer, all-reduce and rescale directly"
0.2.1,"buffer is full, all-reduce and replace buffer with grad"
0.2.1,add tensor to buffer
0.2.1,We need to save a copy of optim.optimizer.state_dict() for setting
0.2.1,"the, optimizer state later on in Stage 2 in this method, since"
0.2.1,the method optim.set_parameters(model.parameters()) will overwrite
0.2.1,"optim.optimizer, and with ith the values stored in"
0.2.1,optim.optimizer.state_dict()
0.2.1,Stage 1:
0.2.1,Essentially optim.set_parameters (re-)creates and optimizer using
0.2.1,model.paramters() as parameters that will be stored in the
0.2.1,optim.optimizer.param_groups field of the torch optimizer class.
0.2.1,"Importantly, this method does not yet load the optimizer state, as"
0.2.1,essentially it builds a new optimizer with empty optimizer state and
0.2.1,parameters from the model.
0.2.1,"Stage 2: In this stage, which is only performed when loading an"
0.2.1,"optimizer from a checkpoint, we load the saved_optimizer_state_dict"
0.2.1,"into the re-created optimizer, to set the optim.optimizer.state"
0.2.1,"field, which was previously empty. For this, we use the optimizer"
0.2.1,"state saved in the ""saved_optimizer_state_dict"" variable for"
0.2.1,this purpose.
0.2.1,See also: https://github.com/pytorch/pytorch/issues/2830
0.2.1,Convert back the state values to cuda type if applicable
0.2.1,We want to make sure that indeed we have a non-empty optimizer state
0.2.1,when we loaded an existing model. This should be at least the case
0.2.1,"for Adam, which saves ""exp_avg"" and ""exp_avg_sq"" state"
0.2.1,(Exponential moving average of gradient and squared gradient values)
0.2.1,Decay method used in tensor2tensor.
0.2.1,Decay based on start_decay_steps every decay_steps
0.2.1,-*- coding: utf-8 -*-
0.2.1,"for sparsemax loss, the loss function operates on the raw output"
0.2.1,"vector, not a probability vector. Hence it's only necessary to"
0.2.1,apply the first part of the generator here.
0.2.1,non_none: the subdict of the state dictionary where the values
0.2.1,are not None.
0.2.1,"Now, the iteration:"
0.2.1,state is a dictionary of sequences of tensor-like but we
0.2.1,want a sequence of dictionaries of tensors.
0.2.1,"First, unzip the dictionary into a sequence of keys and a"
0.2.1,sequence of tensor-like sequences.
0.2.1,"Now, yield a dictionary for each shard. The keys are always"
0.2.1,the same. values is a sequence of length #keys where each
0.2.1,element is a sequence of length #shards. We want to iterate
0.2.1,"over the shards, not over the keys: therefore, the values need"
0.2.1,to be re-zipped by shard and then each shard can be paired
0.2.1,with the keys.
0.2.1,Assumed backprop'd
0.2.1,Log the progress using the number of batches on the x-axis.
0.2.1,Get a list of world_size lists with len(stat_list) Statistics objects
0.2.1,SRU doesn't support PackedSequence.
0.2.1,-*- coding: utf-8 -*-
0.2.1,coding: utf-8
0.2.1,Below are helper functions for intra-class use only.
0.2.1,-*- coding: utf-8 -*-
0.2.1,Hack. Can't pickle defaultdict :(
0.2.1,"For all data types, the tgt side corpus is in form of text."
0.2.1,Load vocabulary
0.2.1,"All datasets have same num of n_tgt_features,"
0.2.1,getting the last one is OK.
0.2.1,"All datasets have same num of n_src_features,"
0.2.1,getting the last one is OK.
0.2.1,Merge the input and output vocabularies.
0.2.1,`tgt_vocab_size` is ignored when sharing vocabularies
0.2.1,We have at least one dataset.
0.2.1,"We return the len of cur_dataset, otherwise we need to load"
0.2.1,"all datasets to determine the real len, which loses the benefit"
0.2.1,of lazy loading.
0.2.1,"We clear `fields` when saving, restore when loading."
0.2.1,Sort batch by decreasing lengths of sentence required by pytorch.
0.2.1,"sort=False means ""Use dataset's sortkey instead of iterator's""."
0.2.1,Maintains the longest src and tgt length in the current batch
0.2.1,Reset current longest length at a new batch (count=1)
0.2.1,Src: <bos> w1 ... wN <eos>
0.2.1,Tgt: w1 ... wN <eos>
0.2.1,device = opt.device_id if opt.gpuid else -1
0.2.1,breaking change torchtext 0.3
0.2.1,Sort the glob output by file name (by increasing indexes).
0.2.1,"Only one inputters.*Dataset, simple!"
0.2.1,-*- coding: utf-8 -*-
0.2.1,Peek at the first to see which fields are used.
0.2.1,"If out_examples is a generator, we need to save the filter_pred"
0.2.1,"function in serialization too, which would cause a problem when"
0.2.1,`torch.save()`. Thus we materialize it as a list.
0.2.1,STFT
0.2.1,-*- coding: utf-8 -*-
0.2.1,Peek at the first to see which fields are used.
0.2.1,"If out_examples is a generator, we need to save the filter_pred"
0.2.1,"function in serialization too, which would cause a problem when"
0.2.1,`torch.save()`. Thus we materialize it as a list.
0.2.1,-*- coding: utf-8 -*-
0.2.1,"self.src_vocabs: mutated in dynamic_dict, used in"
0.2.1,collapse_copy_scores and in Translator.py
0.2.1,Each element of an example is a dictionary whose keys represents
0.2.1,at minimum the src tokens and their indices and potentially also
0.2.1,the src and tgt features and alignment information.
0.2.1,Peek at the first to see which fields are used.
0.2.1,"If out_examples is a generator, we need to save the filter_pred"
0.2.1,"function in serialization too, which would cause a problem when"
0.2.1,`torch.save()`. Thus we materialize it as a list.
0.2.1,"Default to a balanced sort, prioritizing tgt len match."
0.2.1,TODO: make this configurable.
0.2.1,"All examples have same number of features, so we peek first one"
0.2.1,to get the num_feats.
0.2.1,Chain back the first element - we only want to peek it.
0.2.1,Below are helper functions for intra-class use only.
0.2.1,Mapping source tokens to indices in the dynamic dict.
0.2.1,"The codecs module seems to have bugs with seek()/tell(),"
0.2.1,so we use io.open().
0.2.1,"We have associate iterator, just yields tuples"
0.2.1,util we run parallel with it.
0.2.1,Yield tuples util this shard's size reaches the threshold.
0.2.1,This part of check is time consuming on Py2 (but
0.2.1,"it is quite fast on Py3, weird!). So we don't bother"
0.2.1,to check for very line. Instead we chekc every 64
0.2.1,lines. Thus we are not dividing exactly per
0.2.1,"`shard_size`, but it is not too much difference."
0.2.1,All examples must have same number of features.
0.2.1,flake8: noqa
0.2.1,For command-line option parsing
0.2.1,"Check pass, set the args."
0.2.1,"This SRU version implements its own cuda-level optimization,"
0.2.1,so it requires that:
0.2.1,1. `cupy` and `pynvrtc` python package installed.
0.2.1,2. pytorch is built with cuda support.
0.2.1,3. library path set: export LD_LIBRARY_PATH=<cuda lib path>.
0.2.1,Check 1.
0.2.1,Check 2.
0.2.1,Check 3.
0.2.1,This sets up device to use.
0.2.1,-> directions x batch x dim
0.2.1,For DEBUG
0.2.1,"size = (length, batch, x.size(-1)) \"
0.2.1,"if x.dim() == 3 else (batch, x.size(-1))"
0.2.1,grad_x = x.new(*x.size()) if k_ == 3 else x.new(*size).zero_()
0.2.1,Normal use
0.2.1,"An entry check here, will catch on train side and translate side"
0.2.1,if requirements are not satisfied.
0.2.1,RNNDecoderState wraps hidden as a tuple.
0.2.1,fh -> (layers*directions) x batch x dim
0.2.1,Not yet supported on multi-gpu
0.2.1,The score for each translation on the beam.
0.2.1,The backpointers at each time-step.
0.2.1,The outputs at each time-step.
0.2.1,Has EOS topped the beam yet.
0.2.1,The attentions (matrix) for each time.
0.2.1,Time and k pair for finished.
0.2.1,Information for global scoring.
0.2.1,Minimum prediction length
0.2.1,Apply Penalty at every step
0.2.1,force the output to be longer than self.min_length
0.2.1,Sum the previous scores.
0.2.1,Don't let EOS have children.
0.2.1,Block ngram repeats
0.2.1,"Last n tokens, n = block_ngram_repeat"
0.2.1,Skip the blocking if it is in the exclusion list
0.2.1,"best_scores_id is flattened beam x word array, so calculate which"
0.2.1,word and beam each score came from
0.2.1,End condition is when top-of-beam is EOS and no global score.
0.2.1,Add from beam until we have minimum outputs.
0.2.1,Term will be subtracted from probability
0.2.1,Probability will be divided by this
0.2.1,!/usr/bin/env python
0.2.1,use ensemble decoding if more than one model is specified
0.2.1,for debugging
0.2.1,Statistics
0.2.1,Debug attention.
0.2.1,TODO: faster code path for beam_size == 1.
0.2.1,TODO: support these blacklisted features.
0.2.1,Encoder forward.
0.2.1,Tile states and memory beam_size times.
0.2.1,Give full probability to the first beam on the first step.
0.2.1,Decoder forward.
0.2.1,Generator forward.
0.2.1,Multiply probs by the beam probability.
0.2.1,Flatten probs into a list of possibilities.
0.2.1,Recover log probs.
0.2.1,Resolve beam origin and true word ids.
0.2.1,Map beam_index to batch_index in the flat representation.
0.2.1,End condition is the top beam reached end_token.
0.2.1,Save result of finished sentences.
0.2.1,"If all sentences are translated, no need to go further."
0.2.1,Remove finished batches for the next step.
0.2.1,Select and reorder alive batches.
0.2.1,Append last prediction.
0.2.1,(0) Prep each of the components of the search.
0.2.1,And helper method for reducing verbosity.
0.2.1,Define a list of tokens to exclude from ngram-blocking
0.2.1,"exclusion_list = [""<t>"", ""</t>"", "".""]"
0.2.1,Help functions for working with beams and batches
0.2.1,(1) Run the encoder on the src.
0.2.1,(2) Repeat src objects `beam_size` times.
0.2.1,"(3) run the decoder to generate sentences, using beam search."
0.2.1,Construct batch x beam_size nxt words.
0.2.1,Get all the pending current beam words and arrange for forward.
0.2.1,Turn any copied words to UNKs
0.2.1,0 is unk
0.2.1,Temporary kludge solution to handle changed dim expectation
0.2.1,in the decoder
0.2.1,Run one step.
0.2.1,dec_out: beam x rnn_size
0.2.1,(b) Compute a vector of batch x beam word scores.
0.2.1,beam x tgt_vocab
0.2.1,beam x (tgt_vocab + extra_vocab)
0.2.1,beam x tgt_vocab
0.2.1,(c) Advance each beam.
0.2.1,(4) Extract sentences from beam.
0.2.1,(1) run the encoder on the src
0.2.1,"(2) if a target is specified, compute the 'goldScore'"
0.2.1,(i.e. log likelihood) of the target under the model
0.2.1,Log prob of each word.
0.2.1,Rollback pointer to the beginning.
0.2.1,!/usr/bin/env python
0.2.1,backwards compatibility for confs
0.2.1,NOTE: translator returns lists of `n_best` list
0.2.1,we can ignore that (i.e. flatten lists) only because
0.2.1,we restrict `n_best=1`
0.2.1,build back results with empty texts
0.2.1,Sorting
v0.2,!/usr/bin/env python
v0.2,!/usr/bin/env python
v0.2,!/usr/bin/env python
v0.2,-*- coding: utf-8 -*-
v0.2,!/usr/bin/env python
v0.2,-*- coding: utf-8 -*-
v0.2,We will use glob.glob() to find sharded {train|valid}.[0-9]*.pt
v0.2,"when training, so check to avoid tampering with existing pt files"
v0.2,or mixing them up.
v0.2,"We save fields in vocab.pt separately, so make it empty."
v0.2,Currently we only do preprocess sharding for corpus: data_type=='text'.
v0.2,"For data_type == 'img' or 'audio', currently we don't do"
v0.2,preprocess sharding. We only build a monolithic dataset.
v0.2,"But since the interfaces are uniform, it would be not hard"
v0.2,to do this should users need this feature.
v0.2,"We save fields in vocab.pt seperately, so make it empty."
v0.2,"Can't save fields, so remove/reconstruct at training time."
v0.2,!/usr/bin/env python
v0.2,!/usr/bin/env python3
v0.2,-*- coding: utf-8 -*-
v0.2,
v0.2,"OpenNMT-py documentation build configuration file, created by"
v0.2,sphinx-quickstart on Sun Dec 17 12:07:14 2017.
v0.2,
v0.2,This file is execfile()d with the current directory set to its
v0.2,containing dir.
v0.2,
v0.2,Note that not all possible configuration values are present in this
v0.2,autogenerated file.
v0.2,
v0.2,All configuration values have a default; values that are commented out
v0.2,serve to show the default.
v0.2,"If extensions (or modules to document with autodoc) are in another directory,"
v0.2,add these directories to sys.path here. If the directory is relative to the
v0.2,"documentation root, use os.path.abspath to make it absolute, like shown here."
v0.2,
v0.2,import os
v0.2,import sys
v0.2,"sys.path.insert(0, os.path.abspath('.'))"
v0.2,-- General configuration ------------------------------------------------
v0.2,"If your documentation needs a minimal Sphinx version, state it here."
v0.2,
v0.2,needs_sphinx = '1.0'
v0.2,"Add any Sphinx extension module names here, as strings. They can be"
v0.2,extensions coming with Sphinx (named 'sphinx.ext.*') or your custom
v0.2,ones.
v0.2,"Add any paths that contain templates here, relative to this directory."
v0.2,The suffix(es) of source filenames.
v0.2,You can specify multiple suffix as a list of string:
v0.2,
v0.2,"source_suffix = ['.rst', '.md']"
v0.2,The master toctree document.
v0.2,General information about the project.
v0.2,"The version info for the project you're documenting, acts as replacement for"
v0.2,"|version| and |release|, also used in various other places throughout the"
v0.2,built documents.
v0.2,
v0.2,The short X.Y version.
v0.2,"The full version, including alpha/beta/rc tags."
v0.2,The language for content autogenerated by Sphinx. Refer to documentation
v0.2,for a list of supported languages.
v0.2,
v0.2,This is also used if you do content translation via gettext catalogs.
v0.2,"Usually you set ""language"" from the command line for these cases."
v0.2,"List of patterns, relative to source directory, that match files and"
v0.2,directories to ignore when looking for source files.
v0.2,This patterns also effect to html_static_path and html_extra_path
v0.2,The name of the Pygments (syntax highlighting) style to use.
v0.2,"If true, `todo` and `todoList` produce output, else they produce nothing."
v0.2,-- Options for HTML output ----------------------------------------------
v0.2,The theme to use for HTML and HTML Help pages.  See the documentation for
v0.2,a list of builtin themes.
v0.2,
v0.2,html_theme = 'sphinx_materialdesign_theme'
v0.2,html_theme_path = [sphinx_materialdesign_theme.get_path()]
v0.2,Theme options are theme-specific and customize the look and feel of a theme
v0.2,"further.  For a list of options available for each theme, see the"
v0.2,documentation.
v0.2,
v0.2,html_theme_options = {}
v0.2,"Add any paths that contain custom static files (such as style sheets) here,"
v0.2,"relative to this directory. They are copied after the builtin static files,"
v0.2,"so a file named ""default.css"" will overwrite the builtin ""default.css""."
v0.2,"Custom sidebar templates, must be a dictionary that maps document names"
v0.2,to template names.
v0.2,
v0.2,This is required for the alabaster theme
v0.2,refs: http://alabaster.readthedocs.io/en/latest/installation.html#sidebars
v0.2,-- Options for HTMLHelp output ------------------------------------------
v0.2,Output file base name for HTML help builder.
v0.2,-- Options for LaTeX output ---------------------------------------------
v0.2,The paper size ('letterpaper' or 'a4paper').
v0.2,
v0.2,"'papersize': 'letterpaper',"
v0.2,"The font size ('10pt', '11pt' or '12pt')."
v0.2,
v0.2,"'pointsize': '10pt',"
v0.2,Additional stuff for the LaTeX preamble.
v0.2,
v0.2,"'preamble': '',"
v0.2,Latex figure (float) alignment
v0.2,
v0.2,"'figure_align': 'htbp',"
v0.2,Grouping the document tree into LaTeX files. List of tuples
v0.2,"(source start file, target name, title,"
v0.2,"author, documentclass [howto, manual, or own class])."
v0.2,-- Options for manual page output ---------------------------------------
v0.2,One entry per manual page. List of tuples
v0.2,"(source start file, name, description, authors, manual section)."
v0.2,-- Options for Texinfo output -------------------------------------------
v0.2,Grouping the document tree into Texinfo files. List of tuples
v0.2,"(source start file, target name, title, author,"
v0.2,"dir menu entry, description, category)"
v0.2,!/usr/bin/env python
v0.2,-*- coding: utf-8 -*-
v0.2,"the vocab object is a list of tuple (name, torchtext.Vocab)"
v0.2,we iterate over this list and associate vocabularies based on the name
v0.2,"Add in default model arguments, possibly added since training."
v0.2,-*- encoding: utf-8 -*-
v0.2,!/usr/bin/env python
v0.2,-*- coding: utf-8 -*-
v0.2,Author: Rico Sennrich
v0.2,flake8: noqa
v0.2,This file is retrieved from https://github.com/rsennrich/subword-nmt
v0.2,hack for python2/3 compatibility
v0.2,check version information
v0.2,some hacking to deal with duplicates (only consider first instance)
v0.2,don't print end-of-word symbols
v0.2,sys.stderr.write('cannot split {0} further.\n'.format(segment))
v0.2,sys.stderr.write('OOV: {0}\n'.format(segment))
v0.2,sys.stderr.write('OOV: {0}\n'.format(segment))
v0.2,python 2/3 compatibility
v0.2,read/write files as UTF-8
v0.2,!/usr/bin/env python
v0.2,!/usr/bin/env python
v0.2,-*- coding: utf-8 -*-
v0.2,Author: Rico Sennrich
v0.2,flake8: noqa
v0.2,This file is retrieved from https://github.com/rsennrich/subword-nmt
v0.2,hack for python2/3 compatibility
v0.2,"find all instances of pair, and update frequency/indices around it"
v0.2,find first symbol
v0.2,"if first symbol is followed by second symbol, we've found an occurrence of pair (old_word[i:i+2])"
v0.2,"assuming a symbol sequence ""A B C"", if ""B C"" is merged, reduce the frequency of ""A B"""
v0.2,"assuming a symbol sequence ""A B C B"", if ""B C"" is merged, reduce the frequency of ""C B""."
v0.2,"however, skip this if the sequence is A B C B C, because the frequency of ""C B"" will be reduced by the previous code block"
v0.2,find new pair
v0.2,"assuming a symbol sequence ""A BC D"", if ""B C"" is merged, increase the frequency of ""A BC"""
v0.2,"assuming a symbol sequence ""A BC B"", if ""B C"" is merged, increase the frequency of ""BC B"""
v0.2,"however, if the sequence is A BC BC, skip this step because the count of ""BC BC"" will be incremented by the previous code block"
v0.2,data structure of pair frequencies
v0.2,index from pairs to words
v0.2,version 0.2 changes the handling of the end-of-word token ('</w>');
v0.2,version numbering allows bckward compatibility
v0.2,"threshold is inspired by Zipfian assumption, but should only affect speed"
v0.2,we probably missed the best pair because of pruning; go back to full statistics
v0.2,"threshold is inspired by Zipfian assumption, but should only affect speed"
v0.2,python 2/3 compatibility
v0.2,read/write files as UTF-8
v0.2,!/usr/bin/env python
v0.2,"""rnn"" or ""brnn"""
v0.2,Build encoder.
v0.2,Build decoder.
v0.2,Share the embedding matrix - preprocess with share_vocab required.
v0.2,src/tgt vocab should be the same if `-share_vocab` is specified.
v0.2,Build NMTModel(= encoder + decoder).
v0.2,Build Generator.
v0.2,Load the model states from checkpoint or initialize them.
v0.2,Add generator to model (this registers it as parameter of model).
v0.2,!/usr/bin/env python
v0.2,Create a thread to listen for errors in the child processes.
v0.2,Train with multiprocessing.
v0.2,"propagate exception to parent process, keeping original traceback"
v0.2,!/usr/bin/env python
v0.2,this one is needed for torchtext random call (shuffled iterator)
v0.2,in multi gpu it ensures datasets are read in the same order
v0.2,These ensure same initialization in multi gpu mode
v0.2,Load checkpoint if we resume from a previous training.
v0.2,Peek the first dataset to determine the data_type.
v0.2,(All datasets have the same data_type).
v0.2,Load fields generated from preprocess phase.
v0.2,Report src/tgt features.
v0.2,Build model.
v0.2,Build optimizer.
v0.2,Build model saver
v0.2,Do training.
v0.2,Embedding Options
v0.2,Encoder-Decoder Options
v0.2,"group.add_argument('-residual',   action=""store_true"","
v0.2,"help=""Add residual connections between RNN layers."")"
v0.2,Attention options
v0.2,Generator and loss options.
v0.2,Data options
v0.2,"Dictionary options, for text corpus"
v0.2,"Truncation options, for text corpus"
v0.2,Data processing options
v0.2,Options most relevant to speech
v0.2,GPU
v0.2,Init options
v0.2,Pretrained word vectors
v0.2,Fixed word vectors
v0.2,Optimization options
v0.2,learning rate
v0.2,Use TensorboardX for visualization during training
v0.2,Options most relevant to speech
v0.2,Options most relevant to summarization.
v0.2,Alpha and Beta values for Google Length + Coverage penalty
v0.2,"Described here: https://arxiv.org/pdf/1609.08144.pdf, Section 7"
v0.2,Options most relevant to speech.
v0.2,MARKDOWN boilerplate
v0.2,Copyright 2016 The Chromium Authors. All rights reserved.
v0.2,Use of this source code is governed by a BSD-style license that can be
v0.2,found in the LICENSE file.
v0.2,**section heading**:
v0.2,# **--argument-one**
v0.2,Basic attributes.
v0.2,Set model in training mode.
v0.2,Set model in validating mode.
v0.2,F-prop through the model.
v0.2,Compute loss.
v0.2,Update statistics.
v0.2,Set model back to training mode.
v0.2,Truncated BPTT: reminder not compatible with accum > 1
v0.2,1. Create truncated target.
v0.2,2. F-prop all but generator.
v0.2,3. Compute loss in shards for memory efficiency.
v0.2,4. Update the parameters and statistics.
v0.2,Multi GPU gradient gather
v0.2,"If truncated, don't backprop fully."
v0.2,"in case of multi step gradient accumulation,"
v0.2,update only after accum batches
v0.2,For Flake
v0.2,Initialize the bridge layer
v0.2,"s_len, batch, emb_dim = emb.size()"
v0.2,Lengths data is wrapped inside a Tensor.
v0.2,"LSTM has hidden and cell state, other only one"
v0.2,Total number of states
v0.2,Build a linear layer for each
v0.2,"(batch_size, 1, nfft, t)"
v0.2,layer 1
v0.2,"(batch_size, 32, nfft/2, t/2)"
v0.2,"(batch_size, 32, nfft/2/2, t/2)"
v0.2,layer 2
v0.2,"(batch_size, 32, nfft/2/2, t/2)"
v0.2,"s_len, batch, emb_dim = emb.size()"
v0.2,from onmt.utils.misc import aeq
v0.2,Run the forward pass of every layer of the tranformer.
v0.2,"(batch_size, 64, imgH, imgW)"
v0.2,layer 1
v0.2,"(batch_size, 64, imgH/2, imgW/2)"
v0.2,"(batch_size, 128, imgH/2, imgW/2)"
v0.2,layer 2
v0.2,"(batch_size, 128, imgH/2/2, imgW/2/2)"
v0.2,"(batch_size, 256, imgH/2/2, imgW/2/2)"
v0.2,layer 3
v0.2,batch norm 1
v0.2,"(batch_size, 256, imgH/2/2, imgW/2/2)"
v0.2,layer4
v0.2,"(batch_size, 256, imgH/2/2/2, imgW/2/2)"
v0.2,"(batch_size, 512, imgH/2/2/2, imgW/2/2)"
v0.2,layer 5
v0.2,batch norm 2
v0.2,"(batch_size, 512, imgH/2/2/2, imgW/2/2/2)"
v0.2,"(batch_size, 512, imgH/2/2/2, imgW/2/2/2)"
v0.2,"# (batch_size, 512, H, W)"
v0.2,Dimensions and padding for constructing the word embedding matrix
v0.2,Dimensions and padding for feature embedding matrices
v0.2,(these have no effect if feat_vocab_sizes is empty)
v0.2,The embedding matrix look-up tables. The first look-up table
v0.2,"is for words. Subsequent ones are for features, if any exist."
v0.2,The final output size of word + feature vectors. This can vary
v0.2,from the word vector size if and only if features are defined.
v0.2,This is the attribute you should access if you need to know
v0.2,how big your embeddings are going to be.
v0.2,The sequence of operations that converts the input sequence
v0.2,into a sequence of embeddings. At minimum this consists of
v0.2,looking up the embeddings for each word and feature in the
v0.2,input. Model parameters may require the sequence to contain
v0.2,additional operations as well.
v0.2,This class is mainly used by decoder.py for RNNs but also
v0.2,by the CNN / transformer decoder when copy attention is used
v0.2,CNN has its own attention mechanism ConvMultiStepAttention
v0.2,Transformer has its own MultiHeadedAttention
v0.2,mlp wants it with bias
v0.2,Check input sizes
v0.2,"(batch, t_len, d) x (batch, d, s_len) --> (batch, t_len, s_len)"
v0.2,"(batch, t_len, s_len, d)"
v0.2,one step input
v0.2,"compute attention scores, as in Luong et al."
v0.2,Softmax or sparsemax to normalize attention weights
v0.2,each context vector c_t is the weighted average
v0.2,over all the source hidden states
v0.2,concatenate
v0.2,Check output sizes
v0.2,Check output sizes
v0.2,clamping necessary because of numerical errors: loss should be lower
v0.2,"bounded by zero, but negative values near zero are possible without"
v0.2,the clamp
v0.2,from onmt.utils.misc import aeq
v0.2,CHECKS
v0.2,"batch, k_len, d = key.size()"
v0.2,"batch_, k_len_, d_ = value.size()"
v0.2,"aeq(batch, batch_)"
v0.2,"aeq(k_len, k_len_)"
v0.2,"aeq(d, d_)"
v0.2,"batch_, q_len, d_ = query.size()"
v0.2,"aeq(batch, batch_)"
v0.2,"aeq(d, d_)"
v0.2,"aeq(self.model_dim % 8, 0)"
v0.2,if mask is not None:
v0.2,"batch_, q_len_, k_len_ = mask.size()"
v0.2,"aeq(batch_, batch)"
v0.2,"aeq(k_len_, k_len)"
v0.2,aeq(q_len_ == q_len)
v0.2,END CHECKS
v0.2,"1) Project key, value, and query."
v0.2,2) Calculate and scale scores.
v0.2,3) Apply attention dropout and compute context vectors.
v0.2,CHECK
v0.2,"batch_, q_len_, d_ = output.size()"
v0.2,"aeq(q_len, q_len_)"
v0.2,"aeq(batch, batch_)"
v0.2,"aeq(d, d_)"
v0.2,Return one attn
v0.2,At the moment this class is only used by embeddings.Embeddings look-up tables
v0.2,-*- coding: utf-8 -*-
v0.2,checks
v0.2,"batch, channel, height, width = base_target_emb.size()"
v0.2,"batch_, channel_, height_, width_ = input_from_dec.size()"
v0.2,"enc_batch, enc_channel, enc_height = encoder_out_top.size()"
v0.2,"enc_batch_, enc_channel_, enc_height_ = encoder_out_combine.size()"
v0.2,out_features * in_features
v0.2,norm is out_features * 1
v0.2,batch_size * out_features
v0.2,out_features
v0.2,out_features
v0.2,batch_size * out_features
v0.2,"out_channels, in_channels // groups, * kernel_size"
v0.2,out_features
v0.2,This is used nowhere in the code at the moment (Vincent Nguyen 05/18/2018)
v0.2,"in_channels, out_channels, *kernel_size"
v0.2,"in_channels, out_channels, *kernel_size"
v0.2,"self.out_channels, 1"
v0.2,out_features
v0.2,out_features
v0.2,store roots on diagonal
v0.2,CHECKS
v0.2,Original probabilities.
v0.2,Probability of copying p(z=1) batch.
v0.2,Probibility of not copying: p_{word}(w) * (1 - p(z))
v0.2,Compute unks in align and target for readability
v0.2,Copy probability of tokens in source
v0.2,Set scores for unk to 0 and add eps
v0.2,Get scores for tokens in target
v0.2,Regular prob (no unks and unks that can't be copied)
v0.2,Add score for non-unks in target
v0.2,Add score for when word is unk in both align and tgt
v0.2,Forced copy. Add only probability for not-copied tokens
v0.2,Drop padding.
v0.2,"We lazily load datasets when there are more than one, so postpone"
v0.2,the setting of cur_dataset.
v0.2,Correct target copy token instead of <unk>
v0.2,tgt[i] = align[i] + len(tgt_vocab)
v0.2,for i such that tgt[i] == 0 and align[i] != 0
v0.2,Compute sum of perplexities for stats
v0.2,Compute Loss as NLL divided by seq length
v0.2,Compute Sequence Lengths
v0.2,Compute Total Loss per sequence in batch
v0.2,Divide by length of each sequence and sum
v0.2,illegal_weights_mask = torch.ByteTensor([
v0.2,"[0, 0, 0, 0, 0, 0, 0],"
v0.2,"[0, 0, 0, 1, 1, 1, 1],"
v0.2,"[0, 0, 0, 0, 0, 1, 1],"
v0.2,"[0, 0, 1, 1, 1, 1, 1]])"
v0.2,TODO: fix for pytorch 0.3
v0.2,illegal_weights = alignments.masked_select(illegal_weights_mask)
v0.2,"self.assertEqual(0.0, illegal_weights.data.sum())"
v0.2,"-data option is required, but not used in this test, so dummy."
v0.2,Helper to generate a vocabulary
v0.2,len x batch x nfeat
v0.2,batch x c x h x w
v0.2,batch x 1 x nfft x t
v0.2,Initialize vectors to compare size with
v0.2,Ensure correct sizes and types
v0.2,Make sure that output has the correct size and type
v0.2,Make sure that output has the correct size and type
v0.2,Make sure that output has the correct size and type
v0.2,"[('encoder_type', 'transformer'),"
v0.2,"('word_vec_size', 16), ('rnn_size', 16)],"
v0.2,"[('encoder_type', 'transformer'),"
v0.2,"('word_vec_size', 16),"
v0.2,"('rnn_size', 16)],"
v0.2,""""""" Only do SRU test if requirment is safisfied. """""""
v0.2,SRU doesn't support input_feed.
v0.2,!/usr/bin/env python
v0.2,-*- coding: utf-8 -*-
v0.2,Remove the generated *pt files.
v0.2,4 specicials + 2 words (since we pass 2 to merge_vocabs)
v0.2,Test image preprocessing
v0.2,Test audio preprocessing
v0.2,Basic attributes.
v0.2,Build the RNN.
v0.2,Set up the context gate.
v0.2,Set up the standard attention.
v0.2,"Set up a separated copy attention layer, if needed."
v0.2,Check
v0.2,tgt.size() returns tgt length and batch
v0.2,END
v0.2,Run the forward pass of the RNN.
v0.2,Update the state with the result.
v0.2,Concatenates sequence of tensors along a new dimension.
v0.2,NOTE: v0.3 to 0.4: decoder_outputs / attns[*] may not be list
v0.2,(in particular in case of SRU) it was not raising error in 0.3
v0.2,since stack(Variable) was allowed.
v0.2,"In 0.4, SRU returns a tensor that shouldn't be stacke"
v0.2,The encoder hidden is  (layers*directions) x batch x dim.
v0.2,We need to convert it to layers x batch x (directions*dim).
v0.2,Initialize local and return variables.
v0.2,Run the forward pass of the RNN.
v0.2,Check
v0.2,END
v0.2,Calculate the attention.
v0.2,Calculate the context gate.
v0.2,Additional args check.
v0.2,END Additional args check.
v0.2,Initialize local and return variables.
v0.2,Input feed concatenates hidden state with
v0.2,input at every time step.
v0.2,TODO: context gate should be employed
v0.2,instead of second RNN transform.
v0.2,Update the coverage attention.
v0.2,Run the forward pass of the copy attention layer.
v0.2,Return result.
v0.2,Init the input feed.
v0.2,Basic attributes.
v0.2,Build the CNN.
v0.2,CNNDecoder has its own attention mechanism.
v0.2,"Set up a separated copy attention layer, if needed."
v0.2,NOTE: memory_lengths is only here for compatibility reasons
v0.2,with onmt.modules.RNNDecoderBase.forward()
v0.2,CHECKS
v0.2,END CHECKS
v0.2,Initialize return variables.
v0.2,The output of CNNEncoder.
v0.2,The combination of output of CNNEncoder and source embeddings.
v0.2,Run the forward pass of the CNNDecoder.
v0.2,Process the result and update the attentions.
v0.2,Update the state.
v0.2,"Register self.mask as a buffer in TransformerDecoderLayer, so"
v0.2,it gets TransformerDecoderLayer's cuda behavior automatically.
v0.2,Basic attributes.
v0.2,Build TransformerDecoder.
v0.2,TransformerDecoder has its own attention mechanism.
v0.2,"Set up a separated copy attention layer, if needed."
v0.2,Initialize return variables.
v0.2,Run the forward pass of the TransformerDecoder.
v0.2,Process the result and update the attentions.
v0.2,"buffer size in bytes, determine equiv. # of elements based on data type"
v0.2,copy tensors into buffer_t
v0.2,all-reduce and rescale
v0.2,copy all-reduced buffer back into tensors
v0.2,"tensor is bigger than buffer, all-reduce and rescale directly"
v0.2,"buffer is full, all-reduce and replace buffer with grad"
v0.2,add tensor to buffer
v0.2,We need to save a copy of optim.optimizer.state_dict() for setting
v0.2,"the, optimizer state later on in Stage 2 in this method, since"
v0.2,the method optim.set_parameters(model.parameters()) will overwrite
v0.2,"optim.optimizer, and with ith the values stored in"
v0.2,optim.optimizer.state_dict()
v0.2,Stage 1:
v0.2,Essentially optim.set_parameters (re-)creates and optimizer using
v0.2,model.paramters() as parameters that will be stored in the
v0.2,optim.optimizer.param_groups field of the torch optimizer class.
v0.2,"Importantly, this method does not yet load the optimizer state, as"
v0.2,essentially it builds a new optimizer with empty optimizer state and
v0.2,parameters from the model.
v0.2,"Stage 2: In this stage, which is only performed when loading an"
v0.2,"optimizer from a checkpoint, we load the saved_optimizer_state_dict"
v0.2,"into the re-created optimizer, to set the optim.optimizer.state"
v0.2,"field, which was previously empty. For this, we use the optimizer"
v0.2,"state saved in the ""saved_optimizer_state_dict"" variable for"
v0.2,this purpose.
v0.2,See also: https://github.com/pytorch/pytorch/issues/2830
v0.2,Convert back the state values to cuda type if applicable
v0.2,We want to make sure that indeed we have a non-empty optimizer state
v0.2,when we loaded an existing model. This should be at least the case
v0.2,"for Adam, which saves ""exp_avg"" and ""exp_avg_sq"" state"
v0.2,(Exponential moving average of gradient and squared gradient values)
v0.2,Decay method used in tensor2tensor.
v0.2,Decay based on start_decay_steps every decay_steps
v0.2,-*- coding: utf-8 -*-
v0.2,"for sparsemax loss, the loss function operates on the raw output"
v0.2,"vector, not a probability vector. Hence it's only necessary to"
v0.2,apply the first part of the generator here.
v0.2,non_none: the subdict of the state dictionary where the values
v0.2,are not None.
v0.2,"Now, the iteration:"
v0.2,state is a dictionary of sequences of tensor-like but we
v0.2,want a sequence of dictionaries of tensors.
v0.2,"First, unzip the dictionary into a sequence of keys and a"
v0.2,sequence of tensor-like sequences.
v0.2,"Now, yield a dictionary for each shard. The keys are always"
v0.2,the same. values is a sequence of length #keys where each
v0.2,element is a sequence of length #shards. We want to iterate
v0.2,"over the shards, not over the keys: therefore, the values need"
v0.2,to be re-zipped by shard and then each shard can be paired
v0.2,with the keys.
v0.2,Assumed backprop'd
v0.2,Log the progress using the number of batches on the x-axis.
v0.2,Get a list of world_size lists with len(stat_list) Statistics objects
v0.2,SRU doesn't support PackedSequence.
v0.2,-*- coding: utf-8 -*-
v0.2,coding: utf-8
v0.2,Below are helper functions for intra-class use only.
v0.2,-*- coding: utf-8 -*-
v0.2,Hack. Can't pickle defaultdict :(
v0.2,"For all data types, the tgt side corpus is in form of text."
v0.2,Load vocabulary
v0.2,"All datasets have same num of n_tgt_features,"
v0.2,getting the last one is OK.
v0.2,"All datasets have same num of n_src_features,"
v0.2,getting the last one is OK.
v0.2,Merge the input and output vocabularies.
v0.2,`tgt_vocab_size` is ignored when sharing vocabularies
v0.2,We have at least one dataset.
v0.2,"We return the len of cur_dataset, otherwise we need to load"
v0.2,"all datasets to determine the real len, which loses the benefit"
v0.2,of lazy loading.
v0.2,"We clear `fields` when saving, restore when loading."
v0.2,Sort batch by decreasing lengths of sentence required by pytorch.
v0.2,"sort=False means ""Use dataset's sortkey instead of iterator's""."
v0.2,Maintains the longest src and tgt length in the current batch
v0.2,Reset current longest length at a new batch (count=1)
v0.2,Src: <bos> w1 ... wN <eos>
v0.2,Tgt: w1 ... wN <eos>
v0.2,device = opt.device_id if opt.gpuid else -1
v0.2,breaking change torchtext 0.3
v0.2,Sort the glob output by file name (by increasing indexes).
v0.2,"Only one inputters.*Dataset, simple!"
v0.2,-*- coding: utf-8 -*-
v0.2,Peek at the first to see which fields are used.
v0.2,"If out_examples is a generator, we need to save the filter_pred"
v0.2,"function in serialization too, which would cause a problem when"
v0.2,`torch.save()`. Thus we materialize it as a list.
v0.2,STFT
v0.2,-*- coding: utf-8 -*-
v0.2,Peek at the first to see which fields are used.
v0.2,"If out_examples is a generator, we need to save the filter_pred"
v0.2,"function in serialization too, which would cause a problem when"
v0.2,`torch.save()`. Thus we materialize it as a list.
v0.2,-*- coding: utf-8 -*-
v0.2,"self.src_vocabs: mutated in dynamic_dict, used in"
v0.2,collapse_copy_scores and in Translator.py
v0.2,Each element of an example is a dictionary whose keys represents
v0.2,at minimum the src tokens and their indices and potentially also
v0.2,the src and tgt features and alignment information.
v0.2,Peek at the first to see which fields are used.
v0.2,"If out_examples is a generator, we need to save the filter_pred"
v0.2,"function in serialization too, which would cause a problem when"
v0.2,`torch.save()`. Thus we materialize it as a list.
v0.2,"Default to a balanced sort, prioritizing tgt len match."
v0.2,TODO: make this configurable.
v0.2,"All examples have same number of features, so we peek first one"
v0.2,to get the num_feats.
v0.2,Chain back the first element - we only want to peek it.
v0.2,Below are helper functions for intra-class use only.
v0.2,Mapping source tokens to indices in the dynamic dict.
v0.2,"The codecs module seems to have bugs with seek()/tell(),"
v0.2,so we use io.open().
v0.2,"We have associate iterator, just yields tuples"
v0.2,util we run parallel with it.
v0.2,Yield tuples util this shard's size reaches the threshold.
v0.2,This part of check is time consuming on Py2 (but
v0.2,"it is quite fast on Py3, weird!). So we don't bother"
v0.2,to check for very line. Instead we chekc every 64
v0.2,lines. Thus we are not dividing exactly per
v0.2,"`shard_size`, but it is not too much difference."
v0.2,All examples must have same number of features.
v0.2,flake8: noqa
v0.2,For command-line option parsing
v0.2,"Check pass, set the args."
v0.2,"This SRU version implements its own cuda-level optimization,"
v0.2,so it requires that:
v0.2,1. `cupy` and `pynvrtc` python package installed.
v0.2,2. pytorch is built with cuda support.
v0.2,3. library path set: export LD_LIBRARY_PATH=<cuda lib path>.
v0.2,Check 1.
v0.2,Check 2.
v0.2,Check 3.
v0.2,This sets up device to use.
v0.2,-> directions x batch x dim
v0.2,For DEBUG
v0.2,"size = (length, batch, x.size(-1)) \"
v0.2,"if x.dim() == 3 else (batch, x.size(-1))"
v0.2,grad_x = x.new(*x.size()) if k_ == 3 else x.new(*size).zero_()
v0.2,Normal use
v0.2,"An entry check here, will catch on train side and translate side"
v0.2,if requirements are not satisfied.
v0.2,RNNDecoderState wraps hidden as a tuple.
v0.2,fh -> (layers*directions) x batch x dim
v0.2,Not yet supported on multi-gpu
v0.2,The score for each translation on the beam.
v0.2,The backpointers at each time-step.
v0.2,The outputs at each time-step.
v0.2,Has EOS topped the beam yet.
v0.2,The attentions (matrix) for each time.
v0.2,Time and k pair for finished.
v0.2,Information for global scoring.
v0.2,Minimum prediction length
v0.2,Apply Penalty at every step
v0.2,force the output to be longer than self.min_length
v0.2,Sum the previous scores.
v0.2,Don't let EOS have children.
v0.2,Block ngram repeats
v0.2,"Last n tokens, n = block_ngram_repeat"
v0.2,Skip the blocking if it is in the exclusion list
v0.2,"best_scores_id is flattened beam x word array, so calculate which"
v0.2,word and beam each score came from
v0.2,End condition is when top-of-beam is EOS and no global score.
v0.2,Add from beam until we have minimum outputs.
v0.2,Term will be subtracted from probability
v0.2,Probability will be divided by this
v0.2,!/usr/bin/env python
v0.2,for debugging
v0.2,Statistics
v0.2,Debug attention.
v0.2,TODO: faster code path for beam_size == 1.
v0.2,TODO: support these blacklisted features.
v0.2,Encoder forward.
v0.2,Tile states and memory beam_size times.
v0.2,Give full probability to the first beam on the first step.
v0.2,Decoder forward.
v0.2,Generator forward.
v0.2,Multiply probs by the beam probability.
v0.2,Flatten probs into a list of possibilities.
v0.2,Recover log probs.
v0.2,Resolve beam origin and true word ids.
v0.2,Map beam_index to batch_index in the flat representation.
v0.2,End condition is the top beam reached end_token.
v0.2,Save result of finished sentences.
v0.2,"If all sentences are translated, no need to go further."
v0.2,Remove finished batches for the next step.
v0.2,Select and reorder alive batches.
v0.2,Append last prediction.
v0.2,(0) Prep each of the components of the search.
v0.2,And helper method for reducing verbosity.
v0.2,Define a list of tokens to exclude from ngram-blocking
v0.2,"exclusion_list = [""<t>"", ""</t>"", "".""]"
v0.2,Help functions for working with beams and batches
v0.2,(1) Run the encoder on the src.
v0.2,(2) Repeat src objects `beam_size` times.
v0.2,"(3) run the decoder to generate sentences, using beam search."
v0.2,Construct batch x beam_size nxt words.
v0.2,Get all the pending current beam words and arrange for forward.
v0.2,Turn any copied words to UNKs
v0.2,0 is unk
v0.2,Temporary kludge solution to handle changed dim expectation
v0.2,in the decoder
v0.2,Run one step.
v0.2,dec_out: beam x rnn_size
v0.2,(b) Compute a vector of batch x beam word scores.
v0.2,beam x tgt_vocab
v0.2,beam x (tgt_vocab + extra_vocab)
v0.2,beam x tgt_vocab
v0.2,(c) Advance each beam.
v0.2,(4) Extract sentences from beam.
v0.2,(1) run the encoder on the src
v0.2,"(2) if a target is specified, compute the 'goldScore'"
v0.2,(i.e. log likelihood) of the target under the model
v0.2,Log prob of each word.
v0.2,Rollback pointer to the beginning.
v0.2,!/usr/bin/env python
v0.2,NOTE: translator returns lists of `n_best` list
v0.2,we can ignore that (i.e. flatten lists) only because
v0.2,we restrict `n_best=1`
v0.2,build back results with empty texts
v0.2,Sorting
v0.1,!/usr/bin/env python
v0.1,!/usr/bin/env python
v0.1,!/usr/bin/env python
v0.1,!/usr/bin/env python
v0.1,-*- coding: utf-8 -*-
v0.1,We will use glob.glob() to find sharded {train|valid}.[0-9]*.pt
v0.1,"when training, so check to avoid tampering with existing pt files"
v0.1,or mixing them up.
v0.1,"We save fields in vocab.pt seperately, so make it empty."
v0.1,Currently we only do preprocess sharding for corpus: data_type=='text'.
v0.1,"For data_type == 'img' or 'audio', currently we don't do"
v0.1,preprocess sharding. We only build a monolithic dataset.
v0.1,"But since the interfaces are uniform, it would be not hard"
v0.1,to do this should users need this feature.
v0.1,"We save fields in vocab.pt seperately, so make it empty."
v0.1,"Can't save fields, so remove/reconstruct at training time."
v0.1,!/usr/bin/env python
v0.1,onmt.opts.py
v0.1,Set up the Crayon logging server.
v0.1,Log the progress using the number of batches on the x-axis.
v0.1,We have at least one dataset.
v0.1,"We return the len of cur_dataset, otherwise we need to load"
v0.1,"all datasets to determine the real len, which loses the benefit"
v0.1,of lazy loading.
v0.1,"We clear `fields` when saving, restore when loading."
v0.1,Sort batch by decreasing lengths of sentence required by pytorch.
v0.1,"sort=False means ""Use dataset's sortkey instead of iterator's""."
v0.1,"In token batching scheme, the number of sequences is limited"
v0.1,such that the total number of src/tgt tokens (including padding)
v0.1,in a batch <= batch_size
v0.1,Maintains the longest src and tgt length in the current batch
v0.1,Reset current longest length at a new batch (count=1)
v0.1,Src: <bos> w1 ... wN <eos>
v0.1,Tgt: w1 ... wN <eos>
v0.1,1. Train for one epoch on the training set.
v0.1,2. Validate on the validation set.
v0.1,3. Log to remote server.
v0.1,4. Update the learning rate
v0.1,5. Drop a checkpoint if needed.
v0.1,Sort the glob output by file name (by increasing indexes).
v0.1,"Only one onmt.io.*Dataset, simple!"
v0.1,We need to save a copy of optim.optimizer.state_dict() for setting
v0.1,"the, optimizer state later on in Stage 2 in this method, since"
v0.1,the method optim.set_parameters(model.parameters()) will overwrite
v0.1,"optim.optimizer, and with ith the values stored in"
v0.1,optim.optimizer.state_dict()
v0.1,Stage 1:
v0.1,Essentially optim.set_parameters (re-)creates and optimizer using
v0.1,model.paramters() as parameters that will be stored in the
v0.1,optim.optimizer.param_groups field of the torch optimizer class.
v0.1,"Importantly, this method does not yet load the optimizer state, as"
v0.1,essentially it builds a new optimizer with empty optimizer state and
v0.1,parameters from the model.
v0.1,"Stage 2: In this stage, which is only performed when loading an"
v0.1,"optimizer from a checkpoint, we load the saved_optimizer_state_dict"
v0.1,"into the re-created optimizer, to set the optim.optimizer.state"
v0.1,"field, which was previously empty. For this, we use the optimizer"
v0.1,"state saved in the ""saved_optimizer_state_dict"" variable for"
v0.1,this purpose.
v0.1,See also: https://github.com/pytorch/pytorch/issues/2830
v0.1,Convert back the state values to cuda type if applicable
v0.1,We want to make sure that indeed we have a non-empty optimizer state
v0.1,when we loaded an existing model. This should be at least the case
v0.1,"for Adam, which saves ""exp_avg"" and ""exp_avg_sq"" state"
v0.1,(Exponential moving average of gradient and squared gradient values)
v0.1,Debugging method for showing the optimizer state
v0.1,Load checkpoint if we resume from a previous training.
v0.1,I don't like reassigning attributes of opt: it's not clear.
v0.1,Peek the fisrt dataset to determine the data_type.
v0.1,(All datasets have the same data_type).
v0.1,Load fields generated from preprocess phase.
v0.1,Report src/tgt features.
v0.1,Build model.
v0.1,Build optimizer.
v0.1,Do training.
v0.1,"If using tensorboard for logging, close the writer after training."
v0.1,illegal_weights_mask = torch.ByteTensor([
v0.1,"[0, 0, 0, 0, 0, 0, 0],"
v0.1,"[0, 0, 0, 1, 1, 1, 1],"
v0.1,"[0, 0, 0, 0, 0, 1, 1],"
v0.1,"[0, 0, 1, 1, 1, 1, 1]])"
v0.1,TODO: fix for pytorch 0.3
v0.1,illegal_weights = alignments.masked_select(illegal_weights_mask)
v0.1,"self.assertEqual(0.0, illegal_weights.data.sum())"
v0.1,"-data option is required, but not used in this test, so dummy."
v0.1,Helper to generate a vocabulary
v0.1,len x batch x nfeat
v0.1,batch x c x h x w
v0.1,batch x 1 x nfft x t
v0.1,Initialize vectors to compare size with
v0.1,Ensure correct sizes and types
v0.1,Make sure that output has the correct size and type
v0.1,Make sure that output has the correct size and type
v0.1,Make sure that output has the correct size and type
v0.1,"[('encoder_type', 'transformer'),"
v0.1,"('word_vec_size', 16), ('rnn_size', 16)],"
v0.1,"[('encoder_type', 'transformer'),"
v0.1,"('word_vec_size', 16),"
v0.1,"('rnn_size', 16)],"
v0.1,SRU doesn't support input_feed.
v0.1,Remove the generated *pt files.
v0.1,4 specicials + 2 words (since we pass 2 to merge_vocabs)
v0.1,Test image preprocessing
v0.1,Test audio preprocessing
v0.1,!/usr/bin/env python3
v0.1,-*- coding: utf-8 -*-
v0.1,
v0.1,"OpenNMT-py documentation build configuration file, created by"
v0.1,sphinx-quickstart on Sun Dec 17 12:07:14 2017.
v0.1,
v0.1,This file is execfile()d with the current directory set to its
v0.1,containing dir.
v0.1,
v0.1,Note that not all possible configuration values are present in this
v0.1,autogenerated file.
v0.1,
v0.1,All configuration values have a default; values that are commented out
v0.1,serve to show the default.
v0.1,"If extensions (or modules to document with autodoc) are in another directory,"
v0.1,add these directories to sys.path here. If the directory is relative to the
v0.1,"documentation root, use os.path.abspath to make it absolute, like shown here."
v0.1,
v0.1,import os
v0.1,import sys
v0.1,"sys.path.insert(0, os.path.abspath('.'))"
v0.1,-- General configuration ------------------------------------------------
v0.1,"If your documentation needs a minimal Sphinx version, state it here."
v0.1,
v0.1,needs_sphinx = '1.0'
v0.1,"Add any Sphinx extension module names here, as strings. They can be"
v0.1,extensions coming with Sphinx (named 'sphinx.ext.*') or your custom
v0.1,ones.
v0.1,"Add any paths that contain templates here, relative to this directory."
v0.1,The suffix(es) of source filenames.
v0.1,You can specify multiple suffix as a list of string:
v0.1,
v0.1,"source_suffix = ['.rst', '.md']"
v0.1,The master toctree document.
v0.1,General information about the project.
v0.1,"The version info for the project you're documenting, acts as replacement for"
v0.1,"|version| and |release|, also used in various other places throughout the"
v0.1,built documents.
v0.1,
v0.1,The short X.Y version.
v0.1,"The full version, including alpha/beta/rc tags."
v0.1,The language for content autogenerated by Sphinx. Refer to documentation
v0.1,for a list of supported languages.
v0.1,
v0.1,This is also used if you do content translation via gettext catalogs.
v0.1,"Usually you set ""language"" from the command line for these cases."
v0.1,"List of patterns, relative to source directory, that match files and"
v0.1,directories to ignore when looking for source files.
v0.1,This patterns also effect to html_static_path and html_extra_path
v0.1,The name of the Pygments (syntax highlighting) style to use.
v0.1,"If true, `todo` and `todoList` produce output, else they produce nothing."
v0.1,-- Options for HTML output ----------------------------------------------
v0.1,The theme to use for HTML and HTML Help pages.  See the documentation for
v0.1,a list of builtin themes.
v0.1,
v0.1,html_theme = 'sphinx_materialdesign_theme'
v0.1,html_theme_path = [sphinx_materialdesign_theme.get_path()]
v0.1,Theme options are theme-specific and customize the look and feel of a theme
v0.1,"further.  For a list of options available for each theme, see the"
v0.1,documentation.
v0.1,
v0.1,html_theme_options = {}
v0.1,"Add any paths that contain custom static files (such as style sheets) here,"
v0.1,"relative to this directory. They are copied after the builtin static files,"
v0.1,"so a file named ""default.css"" will overwrite the builtin ""default.css""."
v0.1,"Custom sidebar templates, must be a dictionary that maps document names"
v0.1,to template names.
v0.1,
v0.1,This is required for the alabaster theme
v0.1,refs: http://alabaster.readthedocs.io/en/latest/installation.html#sidebars
v0.1,-- Options for HTMLHelp output ------------------------------------------
v0.1,Output file base name for HTML help builder.
v0.1,-- Options for LaTeX output ---------------------------------------------
v0.1,The paper size ('letterpaper' or 'a4paper').
v0.1,
v0.1,"'papersize': 'letterpaper',"
v0.1,"The font size ('10pt', '11pt' or '12pt')."
v0.1,
v0.1,"'pointsize': '10pt',"
v0.1,Additional stuff for the LaTeX preamble.
v0.1,
v0.1,"'preamble': '',"
v0.1,Latex figure (float) alignment
v0.1,
v0.1,"'figure_align': 'htbp',"
v0.1,Grouping the document tree into LaTeX files. List of tuples
v0.1,"(source start file, target name, title,"
v0.1,"author, documentclass [howto, manual, or own class])."
v0.1,-- Options for manual page output ---------------------------------------
v0.1,One entry per manual page. List of tuples
v0.1,"(source start file, name, description, authors, manual section)."
v0.1,-- Options for Texinfo output -------------------------------------------
v0.1,Grouping the document tree into Texinfo files. List of tuples
v0.1,"(source start file, target name, title, author,"
v0.1,"dir menu entry, description, category)"
v0.1,!/usr/bin/env python
v0.1,-*- coding: utf-8 -*-
v0.1,"the vocab object is a list of tuple (name, torchtext.Vocab)"
v0.1,we iterate over this list and associate vocabularies based on the name
v0.1,"Add in default model arguments, possibly added since training."
v0.1,-*- encoding: utf-8 -*-
v0.1,!/usr/bin/env python
v0.1,-*- coding: utf-8 -*-
v0.1,Author: Rico Sennrich
v0.1,flake8: noqa
v0.1,This file is retrieved from https://github.com/rsennrich/subword-nmt
v0.1,hack for python2/3 compatibility
v0.1,check version information
v0.1,some hacking to deal with duplicates (only consider first instance)
v0.1,don't print end-of-word symbols
v0.1,sys.stderr.write('cannot split {0} further.\n'.format(segment))
v0.1,sys.stderr.write('OOV: {0}\n'.format(segment))
v0.1,sys.stderr.write('OOV: {0}\n'.format(segment))
v0.1,python 2/3 compatibility
v0.1,read/write files as UTF-8
v0.1,!/usr/bin/env python
v0.1,!/usr/bin/env python
v0.1,-*- coding: utf-8 -*-
v0.1,Author: Rico Sennrich
v0.1,flake8: noqa
v0.1,This file is retrieved from https://github.com/rsennrich/subword-nmt
v0.1,hack for python2/3 compatibility
v0.1,"find all instances of pair, and update frequency/indices around it"
v0.1,find first symbol
v0.1,"if first symbol is followed by second symbol, we've found an occurrence of pair (old_word[i:i+2])"
v0.1,"assuming a symbol sequence ""A B C"", if ""B C"" is merged, reduce the frequency of ""A B"""
v0.1,"assuming a symbol sequence ""A B C B"", if ""B C"" is merged, reduce the frequency of ""C B""."
v0.1,"however, skip this if the sequence is A B C B C, because the frequency of ""C B"" will be reduced by the previous code block"
v0.1,find new pair
v0.1,"assuming a symbol sequence ""A BC D"", if ""B C"" is merged, increase the frequency of ""A BC"""
v0.1,"assuming a symbol sequence ""A BC B"", if ""B C"" is merged, increase the frequency of ""BC B"""
v0.1,"however, if the sequence is A BC BC, skip this step because the count of ""BC BC"" will be incremented by the previous code block"
v0.1,data structure of pair frequencies
v0.1,index from pairs to words
v0.1,version 0.2 changes the handling of the end-of-word token ('</w>');
v0.1,version numbering allows bckward compatibility
v0.1,"threshold is inspired by Zipfian assumption, but should only affect speed"
v0.1,we probably missed the best pair because of pruning; go back to full statistics
v0.1,"threshold is inspired by Zipfian assumption, but should only affect speed"
v0.1,python 2/3 compatibility
v0.1,read/write files as UTF-8
v0.1,!/usr/bin/env python
v0.1,Use pytorch version when available.
v0.1,SRU doesn't support PackedSequence.
v0.1,Initialize the bridge layer
v0.1,Lengths data is wrapped inside a Variable.
v0.1,"LSTM has hidden and cell state, other only one"
v0.1,Total number of states
v0.1,Build a linear layer for each
v0.1,Basic attributes.
v0.1,Build the RNN.
v0.1,Set up the context gate.
v0.1,Set up the standard attention.
v0.1,"Set up a separated copy attention layer, if needed."
v0.1,Check
v0.1,END
v0.1,Run the forward pass of the RNN.
v0.1,Update the state with the result.
v0.1,Concatenates sequence of tensors along a new dimension.
v0.1,The encoder hidden is  (layers*directions) x batch x dim.
v0.1,We need to convert it to layers x batch x (directions*dim).
v0.1,Initialize local and return variables.
v0.1,Run the forward pass of the RNN.
v0.1,Check
v0.1,END
v0.1,Calculate the attention.
v0.1,Calculate the context gate.
v0.1,Additional args check.
v0.1,END Additional args check.
v0.1,Initialize local and return variables.
v0.1,Input feed concatenates hidden state with
v0.1,input at every time step.
v0.1,TODO: context gate should be employed
v0.1,instead of second RNN transform.
v0.1,Update the coverage attention.
v0.1,Run the forward pass of the copy attention layer.
v0.1,Return result.
v0.1,Not yet supported on multi-gpu
v0.1,Init the input feed.
v0.1,Embedding Options
v0.1,Encoder-Deocder Options
v0.1,"group.add_argument('-residual',   action=""store_true"","
v0.1,"help=""Add residual connections between RNN layers."")"
v0.1,Attention options
v0.1,Genenerator and loss options.
v0.1,Data options
v0.1,"Dictionary options, for text corpus"
v0.1,"Truncation options, for text corpus"
v0.1,Data processing options
v0.1,Options most relevant to speech
v0.1,Model loading/saving options
v0.1,GPU
v0.1,Init options
v0.1,Pretrained word vectors
v0.1,Fixed word vectors
v0.1,Optimization options
v0.1,learning rate
v0.1,Use TensorboardX for visualization during training
v0.1,Options most relevant to speech
v0.1,Options most relevant to summarization.
v0.1,Alpha and Beta values for Google Length + Coverage penalty
v0.1,"Described here: https://arxiv.org/pdf/1609.08144.pdf, Section 7"
v0.1,Options most relevant to speech.
v0.1,MARKDOWN boilerplate
v0.1,Copyright 2016 The Chromium Authors. All rights reserved.
v0.1,Use of this source code is governed by a BSD-style license that can be
v0.1,found in the LICENSE file.
v0.1,**section heading**:
v0.1,# **--argument-one**
v0.1,"When label smoothing is turned on,"
v0.1,KL-divergence between q_{smoothed ground truth prob.}(w)
v0.1,and p_{prob. computed by model}(w) is minimized.
v0.1,"If label smoothing value is set to zero, the loss"
v0.1,is equivalent to NLLLoss or CrossEntropyLoss.
v0.1,All non-true labels are uniformly set to low-confidence.
v0.1,Default: report smoothed ppl.
v0.1,loss_data = -log_likelihood.sum(0)
v0.1,non_none: the subdict of the state dictionary where the values
v0.1,are not None.
v0.1,"Now, the iteration:"
v0.1,state is a dictionary of sequences of tensor-like but we
v0.1,want a sequence of dictionaries of tensors.
v0.1,"First, unzip the dictionary into a sequence of keys and a"
v0.1,sequence of tensor-like sequences.
v0.1,"Now, yield a dictionary for each shard. The keys are always"
v0.1,the same. values is a sequence of length #keys where each
v0.1,element is a sequence of length #shards. We want to iterate
v0.1,"over the shards, not over the keys: therefore, the values need"
v0.1,to be re-zipped by shard and then each shard can be paired
v0.1,with the keys.
v0.1,Assumed backprop'd
v0.1,We use the default parameters for Adam that are suggested by
v0.1,the original paper https://arxiv.org/pdf/1412.6980.pdf
v0.1,"These values are also used by other established implementations,"
v0.1,e.g. https://www.tensorflow.org/api_docs/python/tf/train/AdamOptimizer
v0.1,https://keras.io/optimizers/
v0.1,Recently there are slightly different values used in the paper
v0.1,"""Attention is all you need"""
v0.1,"https://arxiv.org/pdf/1706.03762.pdf, particularly the value beta2=0.98"
v0.1,"was used there however, beta2=0.999 is still arguably the more"
v0.1,"established value, so we use that here as well"
v0.1,Decay method used in tensor2tensor.
v0.1,For flake8 compatibility
v0.1,Basic attributes.
v0.1,Set model in training mode.
v0.1,Dynamic batching
v0.1,Set model in validating mode.
v0.1,F-prop through the model.
v0.1,Compute loss.
v0.1,Update statistics.
v0.1,Set model back to training mode.
v0.1,Truncated BPTT
v0.1,1. Create truncated target.
v0.1,2. F-prop all but generator.
v0.1,3. Compute loss in shards for memory efficiency.
v0.1,4. Update the parameters and statistics.
v0.1,"If truncated, don't backprop fully."
v0.1,"""rnn"" or ""brnn"""
v0.1,Make encoder.
v0.1,Make decoder.
v0.1,Share the embedding matrix - preprocess with share_vocab required.
v0.1,src/tgt vocab should be the same if `-share_vocab` is specified.
v0.1,Make NMTModel(= encoder + decoder).
v0.1,Make Generator.
v0.1,Load the model states from checkpoint or initialize them.
v0.1,Add generator to model (this registers it as parameter of model).
v0.1,Make the whole model leverage GPU if indicated to do so.
v0.1,linear transform for 3-d tensor
v0.1,checks
v0.1,flake8: noqa
v0.1,For command-line option parsing
v0.1,"Check pass, set the args."
v0.1,"This SRU version implements its own cuda-level optimization,"
v0.1,so it requires that:
v0.1,1. `cupy` and `pynvrtc` python package installed.
v0.1,2. pytorch is built with cuda support.
v0.1,3. library path set: export LD_LIBRARY_PATH=<cuda lib path>.
v0.1,Check 1.
v0.1,Check 2.
v0.1,Check 3.
v0.1,"This cuda() is important, it sets up device to use."
v0.1,-> directions x batch x dim
v0.1,For DEBUG
v0.1,"size = (length, batch, x.size(-1)) \"
v0.1,"if x.dim() == 3 else (batch, x.size(-1))"
v0.1,grad_x = x.new(*x.size()) if k_ == 3 else x.new(*size).zero_()
v0.1,Normal use
v0.1,"An entry check here, will catch on train side and translate side"
v0.1,if requirements are not satisfied.
v0.1,RNNDecoderState wraps hidden as a tuple.
v0.1,fh -> (layers*directions) x batch x dim
v0.1,mlp wants it with bias
v0.1,Check input sizes
v0.1,"(batch, t_len, d) x (batch, d, s_len) --> (batch, t_len, s_len)"
v0.1,"(batch, t_len, s_len, d)"
v0.1,one step input
v0.1,"compute attention scores, as in Luong et al."
v0.1,Softmax to normalize attention weights
v0.1,each context vector c_t is the weighted average
v0.1,over all the source hidden states
v0.1,concatenate
v0.1,Check output sizes
v0.1,Check output sizes
v0.1,Basic attributes.
v0.1,Build the CNN.
v0.1,CNNDecoder has its own attention mechanism.
v0.1,"Set up a separated copy attention layer, if needed."
v0.1,CHECKS
v0.1,END CHECKS
v0.1,Initialize return variables.
v0.1,The output of CNNEncoder.
v0.1,The combination of output of CNNEncoder and source embeddings.
v0.1,Run the forward pass of the CNNDecoder.
v0.1,Process the result and update the attentions.
v0.1,Update the state.
v0.1,"Save a little memory, by doing inplace."
v0.1,CHECKS
v0.1,END CHECKS
v0.1,Make mask.
v0.1,Run the forward pass of every layer of the tranformer.
v0.1,"Register self.mask as a buffer in TransformerDecoderLayer, so"
v0.1,it gets TransformerDecoderLayer's cuda behavior automatically.
v0.1,Args Checks
v0.1,"aeq(t_len, t_len_, t_len__, input_len)"
v0.1,END Args Checks
v0.1,CHECKS
v0.1,END CHECKS
v0.1,Basic attributes.
v0.1,Build TransformerDecoder.
v0.1,TransformerDecoder has its own attention mechanism.
v0.1,"Set up a separated copy attention layer, if needed."
v0.1,CHECKS
v0.1,END CHECKS
v0.1,Initialize return variables.
v0.1,Run the forward pass of the TransformerDecoder.
v0.1,Process the result and update the attentions.
v0.1,Update the state.
v0.1,utility for retrieving polyak averaged params
v0.1,Update average
v0.1,utility for retrieving polyak averaged params
v0.1,out_features * in_features
v0.1,norm is out_features * 1
v0.1,batch_size * out_features
v0.1,out_features
v0.1,out_features
v0.1,batch_size * out_features
v0.1,"out_channels, in_channels // groups, * kernel_size"
v0.1,out_features
v0.1,"in_channels, out_channels, *kernel_size"
v0.1,"in_channels, out_channels, *kernel_size"
v0.1,"self.out_channels, 1"
v0.1,out_features
v0.1,out_features
v0.1,CHECKS
v0.1,Original probabilities.
v0.1,Probability of copying p(z=1) batch.
v0.1,Probibility of not copying: p_{word}(w) * (1 - p(z))
v0.1,Compute unks in align and target for readability
v0.1,Copy probability of tokens in source
v0.1,Set scores for unk to 0 and add eps
v0.1,Get scores for tokens in target
v0.1,Regular prob (no unks and unks that can't be copied)
v0.1,Add score for non-unks in target
v0.1,Add score for when word is unk in both align and tgt
v0.1,Forced copy. Add only probability for not-copied tokens
v0.1,Drop padding.
v0.1,"We lazily load datasets when there are more than one, so postpone"
v0.1,the setting of cur_dataset.
v0.1,Correct target copy token instead of <unk>
v0.1,tgt[i] = align[i] + len(tgt_vocab)
v0.1,for i such that tgt[i] == 0 and align[i] != 0
v0.1,Compute sum of perplexities for stats
v0.1,Compute Loss as NLL divided by seq length
v0.1,Compute Sequence Lengths
v0.1,Compute Total Loss per sequence in batch
v0.1,Divide by length of each sequence and sum
v0.1,store roots on diagonal
v0.1,"We must wrap the self.pe in Variable to compute, not the other"
v0.1,way - unwrap emb(i.e. emb.data). Otherwise the computation
v0.1,wouldn't be watched to build the compute graph.
v0.1,Dimensions and padding for constructing the word embedding matrix
v0.1,Dimensions and padding for feature embedding matrices
v0.1,(these have no effect if feat_vocab_sizes is empty)
v0.1,The embedding matrix look-up tables. The first look-up table
v0.1,"is for words. Subsequent ones are for features, if any exist."
v0.1,The final output size of word + feature vectors. This can vary
v0.1,from the word vector size if and only if features are defined.
v0.1,This is the attribute you should access if you need to know
v0.1,how big your embeddings are going to be.
v0.1,The sequence of operations that converts the input sequence
v0.1,into a sequence of embeddings. At minimum this consists of
v0.1,looking up the embeddings for each word and feature in the
v0.1,input. Model parameters may require the sequence to contain
v0.1,additional operations as well.
v0.1,For flake8 compatibility.
v0.1,Pass in needed options only when modify function definition.
v0.1,"(batch_size, 1, nfft, t)"
v0.1,layer 1
v0.1,"(batch_size, 32, nfft/2, t/2)"
v0.1,"(batch_size, 32, nfft/2/2, t/2)"
v0.1,layer 2
v0.1,"(batch_size, 32, nfft/2/2, t/2)"
v0.1,CHECKS
v0.1,END CHECKS
v0.1,"1) Project key, value, and query."
v0.1,2) Calculate and scale scores.
v0.1,3) Apply attention dropout and compute context vectors.
v0.1,CHECK
v0.1,Return one attn
v0.1,END CHECK
v0.1,Pass in needed options only when modify function definition.
v0.1,"(batch_size, 64, imgH, imgW)"
v0.1,layer 1
v0.1,"(batch_size, 64, imgH/2, imgW/2)"
v0.1,"(batch_size, 128, imgH/2, imgW/2)"
v0.1,layer 2
v0.1,"(batch_size, 128, imgH/2/2, imgW/2/2)"
v0.1,"(batch_size, 256, imgH/2/2, imgW/2/2)"
v0.1,layer 3
v0.1,batch norm 1
v0.1,"(batch_size, 256, imgH/2/2, imgW/2/2)"
v0.1,layer4
v0.1,"(batch_size, 256, imgH/2/2/2, imgW/2/2)"
v0.1,"(batch_size, 512, imgH/2/2/2, imgW/2/2)"
v0.1,layer 5
v0.1,batch norm 2
v0.1,"(batch_size, 512, imgH/2/2/2, imgW/2/2/2)"
v0.1,"(batch_size, 512, imgH/2/2/2, imgW/2/2/2)"
v0.1,"# (batch_size, 512, H, W)"
v0.1,-*- coding: utf-8 -*-
v0.1,Peek at the first to see which fields are used.
v0.1,"If out_examples is a generator, we need to save the filter_pred"
v0.1,"function in serialization too, which would cause a problem when"
v0.1,`torch.save()`. Thus we materialize it as a list.
v0.1,-*- coding: utf-8 -*-
v0.1,Hack. Can't pickle defaultdict :(
v0.1,"Build src/tgt examples iterator from corpus files, also extract"
v0.1,number of features.
v0.1,"For all data types, the tgt side corpus is in form of text."
v0.1,Load vocabulary
v0.1,"All datasets have same num of n_tgt_features,"
v0.1,getting the last one is OK.
v0.1,"All datasets have same num of n_src_features,"
v0.1,getting the last one is OK.
v0.1,Merge the input and output vocabularies.
v0.1,`tgt_vocab_size` is ignored when sharing vocabularies
v0.1,-*- coding: utf-8 -*-
v0.1,Below are helper functions for intra-class use only.
v0.1,-*- coding: utf-8 -*-
v0.1,"self.src_vocabs: mutated in dynamic_dict, used in"
v0.1,collapse_copy_scores and in Translator.py
v0.1,Each element of an example is a dictionary whose keys represents
v0.1,at minimum the src tokens and their indices and potentially also
v0.1,the src and tgt features and alignment information.
v0.1,Peek at the first to see which fields are used.
v0.1,"If out_examples is a generator, we need to save the filter_pred"
v0.1,"function in serialization too, which would cause a problem when"
v0.1,`torch.save()`. Thus we materialize it as a list.
v0.1,"Default to a balanced sort, prioritizing tgt len match."
v0.1,TODO: make this configurable.
v0.1,"All examples have same number of features, so we peek first one"
v0.1,to get the num_feats.
v0.1,Chain back the first element - we only want to peek it.
v0.1,Below are helper functions for intra-class use only.
v0.1,Mapping source tokens to indices in the dynamic dict.
v0.1,"The codecs module seems to have bugs with seek()/tell(),"
v0.1,so we use io.open().
v0.1,"We have associate iterator, just yields tuples"
v0.1,util we run parallel with it.
v0.1,Yield tuples util this shard's size reaches the threshold.
v0.1,This part of check is time consuming on Py2 (but
v0.1,"it is quite fast on Py3, weird!). So we don't bother"
v0.1,to check for very line. Instead we chekc every 64
v0.1,lines. Thus we are not dividing exactly per
v0.1,"`shard_size`, but it is not too much difference."
v0.1,We peek the first line and seek back to
v0.1,the beginning of the file.
v0.1,All examples must have same number of features.
v0.1,-*- coding: utf-8 -*-
v0.1,Peek at the first to see which fields are used.
v0.1,"If out_examples is a generator, we need to save the filter_pred"
v0.1,"function in serialization too, which would cause a problem when"
v0.1,`torch.save()`. Thus we materialize it as a list.
v0.1,STFT
v0.1,NOTE: the translator exept a filepath as parameter
v0.1,therefore we write the data as a temp file.
v0.1,NOTE: If an input contains an line separator \n we split it
v0.1,into subsegments that we translate independantly
v0.1,we then merge the translations together with the same
v0.1,line breaks
v0.1,Creating a new object is faster
v0.1,Sorting
v0.1,"output += (""GOLD SCORE: {:.4f}"".format(self.gold_score))"
v0.1,for debugging
v0.1,Statistics
v0.1,Debug attention.
v0.1,(0) Prep each of the components of the search.
v0.1,And helper method for reducing verbosity.
v0.1,Define a list of tokens to exclude from ngram-blocking
v0.1,"exclusion_list = [""<t>"", ""</t>"", "".""]"
v0.1,Help functions for working with beams and batches
v0.1,(1) Run the encoder on the src.
v0.1,(2) Repeat src objects `beam_size` times.
v0.1,"(3) run the decoder to generate sentences, using beam search."
v0.1,Construct batch x beam_size nxt words.
v0.1,Get all the pending current beam words and arrange for forward.
v0.1,Turn any copied words to UNKs
v0.1,0 is unk
v0.1,Temporary kludge solution to handle changed dim expectation
v0.1,in the decoder
v0.1,Run one step.
v0.1,dec_out: beam x rnn_size
v0.1,(b) Compute a vector of batch x beam word scores.
v0.1,beam x tgt_vocab
v0.1,beam x (tgt_vocab + extra_vocab)
v0.1,beam x tgt_vocab
v0.1,(c) Advance each beam.
v0.1,(4) Extract sentences from beam.
v0.1,(1) run the encoder on the src
v0.1,"(2) if a target is specified, compute the 'goldScore'"
v0.1,(i.e. log likelihood) of the target under the model
v0.1,Log prob of each word.
v0.1,The score for each translation on the beam.
v0.1,The backpointers at each time-step.
v0.1,The outputs at each time-step.
v0.1,Has EOS topped the beam yet.
v0.1,The attentions (matrix) for each time.
v0.1,Time and k pair for finished.
v0.1,Information for global scoring.
v0.1,Minimum prediction length
v0.1,Apply Penalty at every step
v0.1,force the output to be longer than self.min_length
v0.1,Sum the previous scores.
v0.1,Don't let EOS have children.
v0.1,Block ngram repeats
v0.1,"Last n tokens, n = block_ngram_repeat"
v0.1,Skip the blocking if it is in the exclusion list
v0.1,"best_scores_id is flattened beam x word array, so calculate which"
v0.1,word and beam each score came from
v0.1,End condition is when top-of-beam is EOS and no global score.
v0.1,Add from beam until we have minimum outputs.
v0.1,Term will be subtracted from probability
v0.1,Probability will be divided by this
