Version,Commit Message,SATD
v0.10.4,HACK(geoffrey): `hyperopt_resources` is a required resource for hyperopt to prevent deadlocks in Ludwig tests.,1
v0.10.4,TODO(geoffrey): remove for Ray 2.2,1
v0.10.4,"HACK(Arnav): Remove configs that have LARS, LAMB or Lion optimizers, or Paged or 8-bit optimizers.",1
v0.10.4,is there a better way to do this?,1
v0.10.4,todo the hidden output is actually a tensor. May need modification,1
v0.10.4,todo figure out the output size for parallel 1d conv,1
v0.10.4,TODO:  Need variant of assert_model_parameters_updated() to account for the two step calling sequence,1
v0.10.4,todo: remove code,1
v0.10.4,TODO(ksbrar): Circle back after discussing whether additional properties should be allowed long-term.,1
v0.10.4,TODO(ksbrar): Circle back after discussing whether additional properties should be allowed long-term.,1
v0.10.4,TODO(Arnav): Re-enable once https://github.com/ludwig-ai/ludwig/issues/3150 is resolved since the GBM,1
v0.10.4,todo: re-add 'attention' after further research in implication of torch,1
v0.10.4,TODO(#1333): Refactor this test once torch sequence generator work is complete.,1
v0.10.4,"TODO(Justin): Move these to test_sequence_generator unit tests, and reintroduce decoder attention, beam_width, and",1
v0.10.4,TODO(travis): add when we support pretrained text models for gbms,1
v0.10.4,TODO: find a smaller model for testing,1
v0.10.4,TODO: figure out how to get mocks to work with Ray backend,1
v0.10.4,TODO(travis): We may want to expand upon this in the future to include some checks on model,1
v0.10.4,TODO(travis): We may want to expand upon this in the future to include some checks on model,1
v0.10.4,TODO(travis): We may want to expand upon this in the future to include some checks on model,1
v0.10.4,TODO: Determine whether this is desired behavior. Tracked here:,1
v0.10.4,TODO: feature type not yet supported,1
v0.10.4,handled non-determinism when comparing the metrics between the local and Ray backends. We work around this by,1
v0.10.4,TODO: feature type not yet supported,1
v0.10.4,TODO(geoffrey): Fold modin tests into test_ray_image as @pytest.mark.parametrized once tests are optimized,1
v0.10.4,TODO(travis): move this to separate gpu module so we only have one ray cluster running at a time,1
v0.10.4,TODO(travis): https://github.com/ludwig-ai/ludwig/issues/1956,1
v0.10.4,TODO: Determine if we still need this if-then-else construct,1
v0.10.4,TODO(travis): once we support GBM text features,1
v0.10.4,TODO(travis): once we support GBM text features,1
v0.10.4,TODO(travis): need unit tests to test the get_embedding_layer() of every encoder to ensure it is,1
v0.10.4,TODO: fix LLM model loading,1
v0.10.4,TODO(arnav): p-tuning and prefix tuning have errors when enabled that seem to stem from DDP:,1
v0.10.4,TODO(Arnav): Re-enable once we can run tests on GPUs,1
v0.10.4,TODO: <Alex>02/21/2024: Disabling AdaptionPrompt (waiting for PEFT release to fix,1
v0.10.4,TODO: Re-enable once we can run tests on GPUs,1
v0.10.4,TODO: <Alex>02/21/2024: Disabling AdaptionPrompt (waiting for PEFT release to fix,1
v0.10.4,"For a full explanation of this 8-bit workaround, see https://github.com/ludwig-ai/ludwig/pull/3606",1
v0.10.4,"TODO: Uncomment ""filter_for_weight_format()"" method definition and enable its usage once GPU tests are set up.",1
v0.10.4,TODO(shreya): Uncomment when https://github.com/ludwig-ai/ludwig/issues/2039 is fixed.,1
v0.10.4,TODO: remove this workaround when audio preprocessing is fixed.,1
v0.10.4,TODO: these are the only outputs we provide from Torchscript for now,1
v0.10.4,TODO all shadows built in name - come up with a more descriptive name,1
v0.10.4,TODO -> Fix OOM on large models e.g. llama 3 8B,1
v0.10.4,TODO (Connor): Refactor to use self.config_obj,1
v0.10.4,TODO (ASN): add support for substitute_with_max parameter,1
v0.10.4,TODO(Justin): Decide if it's worth folding padding_side handling into llm.py's tokenizer initialization.,1
v0.10.4,TODO(travis): WIP,1
v0.10.4,"TODO(travis): there's a lot of redundancy in this approach, since we are preprocessing the same DataFrame",1
v0.10.4,"TODO(travis): can optimize the preprocessing part here, since we only need to preprocess / predict",1
v0.10.4,TODO(travis): generalize this to support any pandas output format,1
v0.10.4,TODO (Connor): Refactor to use self.config_obj,1
v0.10.4,"TODO: In the future, it may be possible to move up the model type check into the BaseModel class.",1
v0.10.4,TODO: fix for Ray where workers may be of different skus,1
v0.10.4,TODO(travis): remove in favor of on_hyperopt_end for naming consistency,1
v0.10.4,TODO: https://github.com/ludwig-ai/ludwig/issues/2633,1
v0.10.4,todo: revise docstring,1
v0.10.4,todo: assess how to specify padding for equivalent to 'same',1
v0.10.4,todo: determine how to pool_padding equivalent of 'same',1
v0.10.4,todo: fixup docstring,1
v0.10.4,todo: review docstring,1
v0.10.4,todo: fix up docstring,1
v0.10.4,todo: fix up docstring,1
v0.10.4,todo: update docstring as needed,1
v0.10.4,TODO(shreya): Make sure this is updated when FCStack is updated,1
v0.10.4,TODO(Arnav): Remove this once we have reduce_output options set for,1
v0.10.4,TODO(travis): get_hf_config_param_names should be implemented as abstract in HFEncoderConfig,1
v0.10.4,TODO(shreya): Confirm that this is it,1
v0.10.4,TODO(shreya): Confirm that this is it,1
v0.10.4,TODO(justin): This may need to be conditioned on which AutoModel gets chosen.,1
v0.10.4,TODO(Arnav): This needs be more flexible to account for RoPE Scaling,1
v0.10.4,TODO: this implementation will not work if resuming from a previous checkpoint. Need to fix this.,1
v0.10.4,TODO (ASN): Decide how we want to proceed if at least one trial has,1
v0.10.4,TODO(shreya): Should this hyperopt config param be set here?,1
v0.10.4,TODO (ASN): add image heuristics,1
v0.10.4,"TODO(travis): less hacky way to do this, we should probably allow ModelConfig to be created without output",1
v0.10.4,"todo future: this may be redundant, check",1
v0.10.4,Workaround for including additional tensors from output of input encoders for,1
v0.10.4,TODO(Justin): Think about how to make this communication work for multi-sequence,1
v0.10.4,todo: when https://github.com/ludwig-ai/ludwig/issues/810 is closed,1
v0.10.4,"todo: can we just use projector_size? # hidden_size,",1
v0.10.4,"todo future: this may be redundant, check",1
v0.10.4,"todo future: this may be redundant, check",1
v0.10.4,Workaround for including additional tensors from output of input encoders for,1
v0.10.4,TODO(Justin): Think about how to make this communication work for multi-sequence,1
v0.10.4,todo future: maybe reintroduce these attention function,1
v0.10.4,todo future: maybe reintroduce these attention function,1
v0.10.4,todo future: maybe reintroduce these attention function,1
v0.10.4,TODO(travis): consider moving this behind a general BatchNorm interface to avoid this kludge.,1
v0.10.4,"todo: enumerate for debugging, remove after testing",1
v0.10.4,TODO(shreya): Combine with ResNetBlock by adding a flag.,1
v0.10.4,TODO(shreya): Implement sparse embedding lookup.,1
v0.10.4,# TODO(shreya): Check if this is equivalent,1
v0.10.4,# TODO(shreya): Check if supported in torch,1
v0.10.4,todo: review for generality,1
v0.10.4,TODO: Simplify this.,1
v0.10.4,TODO(travis): make this more general to other cumulative loss functions,1
v0.10.4,Dummy implementation.,1
v0.10.4,"TODO(Justin): Add a confusion matrix, see",1
v0.10.4,TODO: add a mechanism for letting the user decide to save it,1
v0.10.4,todo: re-evaluate need for separate handling of `attention` reducer,1
v0.10.4,TODO(Justin): Clean this up.,1
v0.10.4,TODO: change to debug level before merging,1
v0.10.4,TODO: should this raise an exception if not in training mode?,1
v0.10.4,TODO(travis): do this through an interface rather than conditional logic,1
v0.10.4,Nested conditional is a workaround to short-circuit boolean evaluation.,1
v0.10.4,TODO: alternatively use get_average_image() for unreachable images,1
v0.10.4,todo future add multiprocessing/multithreading,1
v0.10.4,TODO(travis): do we even need a user param for vector size if we're going to auto-infer it in all,1
v0.10.4,TODO(1891): Remove backward compatibility hack once all models have been retrained with Ludwig after,1
v0.10.4,todo: refactor to reuse SequenceOutputFeature.postprocess_predictions,1
v0.10.4,TODO: Add a mechanism that lets the user save the full probability distribution if they want.,1
v0.10.4,TODO(geoffrey): add support for Dask DataFrames,1
v0.10.4,TODO(Arnav): see if there's a way to only remove them if the entry does't have quotes. This currently,1
v0.10.4,"removes all "" from the string (even those not added by json.dumps), which is not ideal.",1
v0.10.4,Convert datetime to int64 to workaround Dask limitation,1
v0.10.4,"TODO pyarrow: this is needed for caching to work with pyarrow. if removed, the following error is raised:",1
v0.10.4,TODO(travis): passing in MODEL_ECD is a hack here that can be removed once we move to using,1
v0.10.4,"encoder schema at all. This hack works for now because all encoders are supported by ECD, so",1
v0.10.4,todo figure out if additional parameters are needed,1
v0.10.4,"TODO(travis): instead of using raw dictionary, this should be loaded into a proper PreprocessingConfig",1
v0.10.4,TODO dask: this needs to work with DataFrames,1
v0.10.4,TODO(travis): decouple config from training_set_metadata so we don't need to,1
v0.10.4,TODO(joppe): support out of memory negative sampling using Dask,1
v0.10.4,TODO(travis): revisit in the future to make this more precise,1
v0.10.4,TODO: Add link to windowing docs.,1
v0.10.4,TODO: figure out correct typing for augmentation_pipeline after refactoring is done,1
v0.10.4,TODO: consider removing this. doesn't work currently and read performance seems generally,1
v0.10.4,TODO: convert to debug message when done with development,1
v0.10.4,"TODO(travis): could try hashing the in-memory dataset, but this is tricky for Dask",1
v0.10.4,todo future: reintroduce the bucketed batcher,1
v0.10.4,TODO ray: implement dynamic batch size,1
v0.10.4,TODO: Change annotation to PublicAPI once Ludwig 0.7 is released,1
v0.10.4,TODO(travis): is this redundant with `clipglobalnorm`?,1
v0.10.4,TODO(travis) consider removing this in the future after deprecation period,1
v0.10.4,TODO: use registry pattern for trainers,1
v0.10.4,TODO: Change to RAISE and update descriptions once we want to enforce strict schemas.,1
v0.10.4,TODO: Maybe need to plumb 'required' through here,1
v0.10.4,"TODO(travis): this seems much too verbose, does the validation error not show the specific error?",1
v0.10.4,"TODO(travis): too much boilerplate here, we should find a way to abstract all this and only require specifying the",1
v0.10.4,"TODO(travis): this seems much too verbose, does the validation error not show the specific error?",1
v0.10.4,TODO(#1673): Need some more logic here for validating against output features,1
v0.10.4,"TODO: Re-enable ""goss"" when supported: https://github.com/ludwig-ai/ludwig/issues/2988",1
v0.10.4,TODO(travis): figure out why we need these imports to avoid circular import error,1
v0.10.4,"TODO (Connor): Add nesting logic for fc_layers, see fully_connected_module.py",1
v0.10.4,"TODO (Connor): Add nesting logic for fc_layers, see fully_connected_module.py",1
v0.10.4,TODO(travis): seems like this is not really a valid user option. We should probably just remove these,1
v0.10.4,TODO: uncomment when sentencepiece doesn't cause segfaults: https://github.com/ludwig-ai/ludwig/issues/2983,1
v0.10.4,TODO: uncomment once we figure out host memory issue: https://github.com/ludwig-ai/ludwig/issues/3107,1
v0.10.4,TODO: uncomment when CTRL bug (https://github.com/ludwig-ai/ludwig/issues/2977) has been fixed to add back in,1
v0.10.4,TODO(#1673): Add conditional logic for fields like this one:,1
v0.10.4,"TODO(travis): below type comparison is not perfect, as it doesn't consider the case where the default type",1
v0.10.4,"TODO(travis): explore similar contraints for GBMs, which don't have epochs",1
v0.10.4,HACK(Arnav): Set Mixtral target modules when using LoRA,1
v0.10.4,HACK(Arnav): Set Phi-2 target modules when using LoRA,1
v0.10.4,HACK(Arnav): Set Phi-3 target modules when using LoRA,1
v0.10.4,HACK(Arnav): Set Gemma target modules when using LoRA,1
v0.10.4,"This is not perfect since it includes tokens from both input + output features, but this at least",1
v0.10.4,TODO (Arnav): Figure out how to factor in rope scaling factor into this calculation.,1
v0.10.4,TODO: Add better support for category output features,1
v0.10.4,TODO(travis): handle this with helper function,1
v0.10.4,TODO(travis): not needed once we remove existing model config implementation,1
v0.10.4,TODO(ksbrar): What is this?,1
v0.10.4,"TODO(travis): this should be done through marshmallow dataclass' `required` field param,",1
v0.10.4,TODO(Arnav): Remove the hard check on max_length once we support multiple output features.,1
v0.10.4,TODO(Arnav): Refactor LossDataclassField to only accept loss types that are valid for the model,1
v0.10.4,TODO: Add schema support for Callable,1
v0.10.4,TODO: This should technically be a required paremeter. Do we need to add support for required params?,1
v0.10.4,TODO: Double-check support for this,1
v0.10.4,TODO: Double-check support for this as well as whether Callable args work properly,1
v0.10.4,TODO: create a search alg metadata class to register in place of individual metadata args,1
v0.10.4,TODO: Add a registry mapping string names to nevergrad optimizers,1
v0.10.4,TODO: Add schemas for nevergrad optimizer kwargs,1
v0.10.4,TODO: Add a registry of Optuna samplers schemas,1
v0.10.4,TODO(travis): figure out why calling this `bias` doesn't work,1
v0.10.4,TODO(travis): fix text generation when using prompt tuning:,1
v0.10.4,TODO(travis): fix prefix tuning and p-tuning to work with DDP,1
v0.10.4,TODO: <Alex>02/21/2024: Disabling AdaptionPrompt (waiting for PEFT release to fix,1
v0.10.4,TODO(This needs to be defined based on the Constraint class),1
v0.10.4,TODO: This is an unfortunate side-effect of dataclass init order - you cannot have non-default fields follow,1
v0.10.4,todo v0.4: currently not clear way to set model graph,1
v0.10.4,TODO(daniel): delete this.,1
v0.10.4,TODO(travis): figure out a good way to support this. The problem with,1
v0.10.4,"TODO(Justin): This should probably live in on_ludwig_end, once that's implemented.",1
v0.10.4,TODO: need to also include a filename for this figure,1
v0.10.4,"TODO(geoffrey, arnav): uncomment this when we have reconciled the config with the backend kwarg in api.py",1
v0.10.4,"TODO: `prompt` by default should be set to null, not a default dict:",1
v0.10.4,"TODO: retrieval by default should be set to null, not a default dict:",1
v0.10.4,TODO: len(template_refs) is a hacky attempt to check that there are references to *something* in the,1
v0.10.4,TODO(travis): figure out why we need these imports to avoid circular import error,1
v0.10.4,TODO: Replace with more robust required logic later.,1
v0.10.4,TODO(Arnav): Refactor to split into strategies like splitters,1
v0.10.4,"TODO(geoffrey): figure out where self.max_sequence_length is used– if not used, we might consider removing it.",1
v0.10.4,It's confusing to have both this and `max_new_tokens` as a mandatory param in the `forward` function.,1
v0.10.4,TODO(Arnav): Add support for probabilities and logits,1
v0.10.4,"TODO(Arnav): Figure out how to compute logits. For now, we return",1
v0.10.4,Dummy implementation.,1
v0.10.4,TODO: Use a configurable ratio for how often to use teacher forcing during training.,1
v0.10.4,TODO: Use a configurable ratio for how often to use teacher forcing during training.,1
v0.10.4,Dummy implementation.,1
v0.10.4,TODO(Arnav): Re-enable once we add DotProduct Combiner: https://github.com/ludwig-ai/ludwig/issues/3150,1
v0.10.4,"TODO(travis): we assume here that False is always the default, which may not be true. We should dervice",1
v0.10.4,TODO: Check if subscription has expired,1
v0.10.4,"TODO(travis): stopgap solution, we should make it so we don't need to do this",1
v0.10.4,todo (Wael): tests for all types.,1
v0.10.4,todo (Wael): tests for all types.,1
v0.10.4,Still needed for preprocessing  TODO(Connor): Refactor ludwig/data/preprocessing to use schema,1
v0.10.4,"TODO(travis): remove this, make type a protected string for each subclass",1
v0.10.4,"ORRIBLE HACK, IT'S THE ONLY WAY TO REMOVE PADDING",1
v0.10.4,END OF HORRIBLE HACK,1
v0.10.4,"TODO: more research needed on how to handle RayTune ""sample_from"" search space",1
v0.10.4,HACK(geoffrey): gpt2 has no pad token. Recommendation is to use eos token instead.,1
v0.10.4,TODO(geoffrey): can we better validate tokenizer parity before swapping in the TorchText tokenizer?,1
v0.10.4,TODO: improve this,1
v0.10.4,Workaround for: https://issues.apache.org/jira/browse/ARROW-5645,1
v0.10.4,TODO: make this configurable in the future. These parameters are from FastChat:,1
v0.10.4,"TODO: Wrap device_map=""auto"" in a try-except block since it may not be supported for all models (E.g. BertLMHead)  # noqa",1
v0.10.4,TODO(travis): move to cached_property when we drop Python 3.7.,1
v0.10.4,TODO: remove reshaping once https://github.com/microsoft/LightGBM/pull/4925 is released,1
v0.10.4,TODO: remove ravel once https://github.com/microsoft/LightGBM/pull/4925 is released,1
v0.10.4,TODO (jeffkinnison): revert to use the requested device once torch device usage is standardized,1
v0.10.4,TODO(shreya): Confirm types of args,1
v0.10.4,"TODO: when loading an existing model, this loses metric values for all but the last epoch.",1
v0.10.4,TODO(travis): implement imbalance ratio,1
v0.10.4,"TODO (ASN): add other modalities (image, etc. )",1
v0.10.4,"TODO(travis): this assumes ECD is the selected model type, which is not problematic for now, as",1
v0.10.4,"diverge, so we should find a way to remove this. The best solution is to the change the input params from",1
v0.10.4,TODO(travis): consolidate with implementation in data/ray.py,1
v0.10.4,TODO: only single task currently,1
v0.10.4,TODO(travis): include encoder and decoder steps during inference,1
v0.10.4,TODO(Arnav): This needs be more flexible to account for RoPE Scaling,1
v0.10.4,TODO: this implementation will not work if resuming from a previous checkpoint. Need to fix this.,1
v0.10.4,TODO: only single task currently,1
v0.10.4,TODO (jeffkinnison): Determine why the 8-bit `SCB` and `CB` matrices are deleted in the forward pass,1
v0.10.4,HACK (Tim): get the device of the targets to transfer self.eval_loss_metric to the same device,1
v0.10.4,"TODO(Arnav): Seems like doing this again and going between these format types in unnecessary, but",1
v0.10.4,"HACK(geoffrey): we need a non-empty loss, so we just fill it with zeros",1
v0.10.4,TODO(travis): this will need to change when we support multiple output features,1
v0.10.4,"TODO(travis): use the implementation of trainer itself to decide whether to save the model, to",1
v0.10.4,avoid this hack,1
v0.10.4,"TODO: see the ""TODO"" statement from ""LLM.save()"" in this module.",1
v0.10.4,TODO (jeffkinnison): revert to using the requested device for GBMs when device usage is fixed,1
v0.10.4,TODO: with change to misc_utils.set_random_seed() this may be redundant,1
v0.10.4,TODO(Justin): Check that the semantics of input_size align with what the combiner's output shape returns,1
v0.10.4,TODO(justin): Remove dummy implementation. Make input_shape and output_shape functions.,1
v0.10.4,TODO(geoffrey): figure out why self.index.search segfaults with larger batch sizes,1
v0.10.4,TODO(travis): support local feature importance,1
v0.10.4,TODO:,1
v0.10.4,TODO(travis): add back skip encoders at the end in finally. Shouldn't be an issue in most cases as we,1
v0.10.4,"TODO(travis): for force plots, need something similar to SHAP E[X]",1
v0.10.4,"TODO(travis): for force plots, need something similar to SHAP E[X]",1
v0.10.4,"TODO(travis): Consider changing to `if not torch.is_floating_point(t.dtype)` to simplify, then handle bool",1
v0.10.4,TODO(travis): pre-compute this during training from the full training dataset.,1
v0.10.4,"TODO(travis): for force plots, need something similar to SHAP E[X]",1
v0.10.4,"TODO(travis): for force plots, need something similar to SHAP E[X]",1
v0.10.4,"TODO: construct new datasets by running encoders (for text, image)",1
v0.10.4,TODO(shreya): Refactor preprocessing so that this can be moved upstream.,1
v0.10.4,TODO(Arnav): Add support for gradient checkpointing in the compiled model,1
v0.10.4,"For a full explanation of this 8-bit workaround, see https://github.com/ludwig-ai/ludwig/pull/3606",1
v0.10.4,TODO (jeffkinnison): Determine why `SCB` and `CB` are deleted from parameter state,1
v0.10.4,matrices are part of model state. This workaround is necessary because the matrices are,1
v0.10.4,"TODO: Implement batch size tuning for LLM, currently just returns the default batch size",1
v0.10.4,TODO: refactor this into an interface,1
v0.10.4,workaround type limitations of the underlying frameworks,1
v0.10.4,TODO(Arnav): Re-enable in Ray 2.3,1
v0.10.4,"TODO(travis): we should revisit the user format here, as it silently breaks situations",1
v0.10.4,"TODO ray: find an informed way to set the parallelism, in practice",1
v0.10.4,TODO: When this is implemented we also need to update the,1
v0.10.4,TODO(travis): consolidate with implementation in data/ray.py,1
v0.10.4,"This is needed because Daft only supports Dataframes, not Series",1
v0.10.4,TODO(ekl) deprecate this once read fusion is available.,1
v0.10.4,TODO(geoffrey): ensure this subclasses ray.data.datasource.Reader in ray 1.14,1
v0.10.4,TODO(ekl) deprecate this once read fusion is available.,1
v0.10.4,"doesn't work for Snappy, so we double-check ourselves.",1
v0.10.4,TODO(xwjiang): Remove this later.,1
v0.10.4,TODO(travis): need to fix checkpoint saving/loading for DeepSpeed to enable tuning,1
v0.10.4,"TODO(travis): this double-counts on the same device, it should use a cross-communicator instead",1
v0.10.4,"TODO: remove LOCAL_BACKEND as a global constant, replace with singleton LocalBackend.shared_instance().",1
v0.10.4,Temporary workaround to prevent tests from automatically using the Ray backend. Taken from,1
v0.10.4,TODO(Alex): Standardize load() signature as interface method in DatasetLoader and adhere to it in all subclasses.,1
v0.10.4,TODO(travis): add other indexing structures,1
v0.10.4,TODO(travis): open question if this is needed to ensure all workers using same weights,1
v0.10.4,TODO(travis): open question if this is needed to ensure all workers using same optimizer state,1
v0.10.4,"TODO(travis): currently Ray handles this for us, but is subject to hangs if one of the workers raises an",1
v0.10.4,TODO(geoffrey): Add a boolean arg to function to control load_module_strict behavior.,1
v0.10.3,HACK(geoffrey): `hyperopt_resources` is a required resource for hyperopt to prevent deadlocks in Ludwig tests.,1
v0.10.3,TODO(geoffrey): remove for Ray 2.2,1
v0.10.3,"HACK(Arnav): Remove configs that have LARS, LAMB or Lion optimizers, or Paged or 8-bit optimizers.",1
v0.10.3,is there a better way to do this?,1
v0.10.3,todo the hidden output is actually a tensor. May need modification,1
v0.10.3,todo figure out the output size for parallel 1d conv,1
v0.10.3,TODO:  Need variant of assert_model_parameters_updated() to account for the two step calling sequence,1
v0.10.3,todo: remove code,1
v0.10.3,TODO(ksbrar): Circle back after discussing whether additional properties should be allowed long-term.,1
v0.10.3,TODO(ksbrar): Circle back after discussing whether additional properties should be allowed long-term.,1
v0.10.3,TODO(Arnav): Re-enable once https://github.com/ludwig-ai/ludwig/issues/3150 is resolved since the GBM,1
v0.10.3,todo: re-add 'attention' after further research in implication of torch,1
v0.10.3,TODO(#1333): Refactor this test once torch sequence generator work is complete.,1
v0.10.3,"TODO(Justin): Move these to test_sequence_generator unit tests, and reintroduce decoder attention, beam_width, and",1
v0.10.3,TODO(travis): add when we support pretrained text models for gbms,1
v0.10.3,TODO: find a smaller model for testing,1
v0.10.3,TODO: figure out how to get mocks to work with Ray backend,1
v0.10.3,TODO(travis): We may want to expand upon this in the future to include some checks on model,1
v0.10.3,TODO(travis): We may want to expand upon this in the future to include some checks on model,1
v0.10.3,TODO(travis): We may want to expand upon this in the future to include some checks on model,1
v0.10.3,TODO: Determine whether this is desired behavior. Tracked here:,1
v0.10.3,TODO: feature type not yet supported,1
v0.10.3,handled non-determinism when comparing the metrics between the local and Ray backends. We work around this by,1
v0.10.3,TODO: feature type not yet supported,1
v0.10.3,TODO(geoffrey): Fold modin tests into test_ray_image as @pytest.mark.parametrized once tests are optimized,1
v0.10.3,TODO(travis): move this to separate gpu module so we only have one ray cluster running at a time,1
v0.10.3,TODO(travis): https://github.com/ludwig-ai/ludwig/issues/1956,1
v0.10.3,TODO: Determine if we still need this if-then-else construct,1
v0.10.3,TODO(travis): once we support GBM text features,1
v0.10.3,TODO(travis): once we support GBM text features,1
v0.10.3,TODO(travis): need unit tests to test the get_embedding_layer() of every encoder to ensure it is,1
v0.10.3,TODO: fix LLM model loading,1
v0.10.3,TODO(arnav): p-tuning and prefix tuning have errors when enabled that seem to stem from DDP:,1
v0.10.3,TODO(Arnav): Re-enable once we can run tests on GPUs,1
v0.10.3,TODO: <Alex>02/21/2024: Disabling AdaptionPrompt (waiting for PEFT release to fix,1
v0.10.3,TODO: Re-enable once we can run tests on GPUs,1
v0.10.3,TODO: <Alex>02/21/2024: Disabling AdaptionPrompt (waiting for PEFT release to fix,1
v0.10.3,"For a full explanation of this 8-bit workaround, see https://github.com/ludwig-ai/ludwig/pull/3606",1
v0.10.3,"TODO: Uncomment ""filter_for_weight_format()"" method definition and enable its usage once GPU tests are set up.",1
v0.10.3,TODO(shreya): Uncomment when https://github.com/ludwig-ai/ludwig/issues/2039 is fixed.,1
v0.10.3,TODO: remove this workaround when audio preprocessing is fixed.,1
v0.10.3,TODO: these are the only outputs we provide from Torchscript for now,1
v0.10.3,TODO all shadows built in name - come up with a more descriptive name,1
v0.10.3,TODO (Connor): Refactor to use self.config_obj,1
v0.10.3,TODO (ASN): add support for substitute_with_max parameter,1
v0.10.3,TODO(Justin): Decide if it's worth folding padding_side handling into llm.py's tokenizer initialization.,1
v0.10.3,TODO(travis): WIP,1
v0.10.3,"TODO(travis): there's a lot of redundancy in this approach, since we are preprocessing the same DataFrame",1
v0.10.3,"TODO(travis): can optimize the preprocessing part here, since we only need to preprocess / predict",1
v0.10.3,TODO(travis): generalize this to support any pandas output format,1
v0.10.3,TODO (Connor): Refactor to use self.config_obj,1
v0.10.3,"TODO: In the future, it may be possible to move up the model type check into the BaseModel class.",1
v0.10.3,TODO: fix for Ray where workers may be of different skus,1
v0.10.3,TODO(travis): remove in favor of on_hyperopt_end for naming consistency,1
v0.10.3,TODO: https://github.com/ludwig-ai/ludwig/issues/2633,1
v0.10.3,todo: revise docstring,1
v0.10.3,todo: assess how to specify padding for equivalent to 'same',1
v0.10.3,todo: determine how to pool_padding equivalent of 'same',1
v0.10.3,todo: fixup docstring,1
v0.10.3,todo: review docstring,1
v0.10.3,todo: fix up docstring,1
v0.10.3,todo: fix up docstring,1
v0.10.3,todo: update docstring as needed,1
v0.10.3,TODO(shreya): Make sure this is updated when FCStack is updated,1
v0.10.3,TODO(Arnav): Remove this once we have reduce_output options set for,1
v0.10.3,TODO(travis): get_hf_config_param_names should be implemented as abstract in HFEncoderConfig,1
v0.10.3,TODO(shreya): Confirm that this is it,1
v0.10.3,TODO(shreya): Confirm that this is it,1
v0.10.3,TODO(justin): This may need to be conditioned on which AutoModel gets chosen.,1
v0.10.3,TODO(Arnav): This needs be more flexible to account for RoPE Scaling,1
v0.10.3,TODO: this implementation will not work if resuming from a previous checkpoint. Need to fix this.,1
v0.10.3,TODO (ASN): Decide how we want to proceed if at least one trial has,1
v0.10.3,TODO(shreya): Should this hyperopt config param be set here?,1
v0.10.3,TODO (ASN): add image heuristics,1
v0.10.3,"TODO(travis): less hacky way to do this, we should probably allow ModelConfig to be created without output",1
v0.10.3,"todo future: this may be redundant, check",1
v0.10.3,Workaround for including additional tensors from output of input encoders for,1
v0.10.3,TODO(Justin): Think about how to make this communication work for multi-sequence,1
v0.10.3,todo: when https://github.com/ludwig-ai/ludwig/issues/810 is closed,1
v0.10.3,"todo: can we just use projector_size? # hidden_size,",1
v0.10.3,"todo future: this may be redundant, check",1
v0.10.3,"todo future: this may be redundant, check",1
v0.10.3,Workaround for including additional tensors from output of input encoders for,1
v0.10.3,TODO(Justin): Think about how to make this communication work for multi-sequence,1
v0.10.3,todo future: maybe reintroduce these attention function,1
v0.10.3,todo future: maybe reintroduce these attention function,1
v0.10.3,todo future: maybe reintroduce these attention function,1
v0.10.3,TODO(travis): consider moving this behind a general BatchNorm interface to avoid this kludge.,1
v0.10.3,"todo: enumerate for debugging, remove after testing",1
v0.10.3,TODO(shreya): Combine with ResNetBlock by adding a flag.,1
v0.10.3,TODO(shreya): Implement sparse embedding lookup.,1
v0.10.3,# TODO(shreya): Check if this is equivalent,1
v0.10.3,# TODO(shreya): Check if supported in torch,1
v0.10.3,todo: review for generality,1
v0.10.3,TODO: Simplify this.,1
v0.10.3,TODO(travis): make this more general to other cumulative loss functions,1
v0.10.3,Dummy implementation.,1
v0.10.3,"TODO(Justin): Add a confusion matrix, see",1
v0.10.3,TODO: add a mechanism for letting the user decide to save it,1
v0.10.3,todo: re-evaluate need for separate handling of `attention` reducer,1
v0.10.3,TODO(Justin): Clean this up.,1
v0.10.3,TODO: change to debug level before merging,1
v0.10.3,TODO: should this raise an exception if not in training mode?,1
v0.10.3,TODO(travis): do this through an interface rather than conditional logic,1
v0.10.3,Nested conditional is a workaround to short-circuit boolean evaluation.,1
v0.10.3,TODO: alternatively use get_average_image() for unreachable images,1
v0.10.3,todo future add multiprocessing/multithreading,1
v0.10.3,TODO(travis): do we even need a user param for vector size if we're going to auto-infer it in all,1
v0.10.3,TODO(1891): Remove backward compatibility hack once all models have been retrained with Ludwig after,1
v0.10.3,todo: refactor to reuse SequenceOutputFeature.postprocess_predictions,1
v0.10.3,TODO: Add a mechanism that lets the user save the full probability distribution if they want.,1
v0.10.3,TODO(geoffrey): add support for Dask DataFrames,1
v0.10.3,TODO(Arnav): see if there's a way to only remove them if the entry does't have quotes. This currently,1
v0.10.3,"removes all "" from the string (even those not added by json.dumps), which is not ideal.",1
v0.10.3,Convert datetime to int64 to workaround Dask limitation,1
v0.10.3,"TODO pyarrow: this is needed for caching to work with pyarrow. if removed, the following error is raised:",1
v0.10.3,TODO(travis): passing in MODEL_ECD is a hack here that can be removed once we move to using,1
v0.10.3,"encoder schema at all. This hack works for now because all encoders are supported by ECD, so",1
v0.10.3,todo figure out if additional parameters are needed,1
v0.10.3,"TODO(travis): instead of using raw dictionary, this should be loaded into a proper PreprocessingConfig",1
v0.10.3,TODO dask: this needs to work with DataFrames,1
v0.10.3,TODO(travis): decouple config from training_set_metadata so we don't need to,1
v0.10.3,TODO(joppe): support out of memory negative sampling using Dask,1
v0.10.3,TODO(travis): revisit in the future to make this more precise,1
v0.10.3,TODO: Add link to windowing docs.,1
v0.10.3,TODO: figure out correct typing for augmentation_pipeline after refactoring is done,1
v0.10.3,TODO: consider removing this. doesn't work currently and read performance seems generally,1
v0.10.3,TODO: convert to debug message when done with development,1
v0.10.3,"TODO(travis): could try hashing the in-memory dataset, but this is tricky for Dask",1
v0.10.3,todo future: reintroduce the bucketed batcher,1
v0.10.3,TODO ray: implement dynamic batch size,1
v0.10.3,TODO: Change annotation to PublicAPI once Ludwig 0.7 is released,1
v0.10.3,TODO(travis): is this redundant with `clipglobalnorm`?,1
v0.10.3,TODO(travis) consider removing this in the future after deprecation period,1
v0.10.3,TODO: use registry pattern for trainers,1
v0.10.3,TODO: Change to RAISE and update descriptions once we want to enforce strict schemas.,1
v0.10.3,TODO: Maybe need to plumb 'required' through here,1
v0.10.3,"TODO(travis): this seems much too verbose, does the validation error not show the specific error?",1
v0.10.3,"TODO(travis): too much boilerplate here, we should find a way to abstract all this and only require specifying the",1
v0.10.3,"TODO(travis): this seems much too verbose, does the validation error not show the specific error?",1
v0.10.3,TODO(#1673): Need some more logic here for validating against output features,1
v0.10.3,"TODO: Re-enable ""goss"" when supported: https://github.com/ludwig-ai/ludwig/issues/2988",1
v0.10.3,TODO(travis): figure out why we need these imports to avoid circular import error,1
v0.10.3,"TODO (Connor): Add nesting logic for fc_layers, see fully_connected_module.py",1
v0.10.3,"TODO (Connor): Add nesting logic for fc_layers, see fully_connected_module.py",1
v0.10.3,TODO(travis): seems like this is not really a valid user option. We should probably just remove these,1
v0.10.3,TODO: uncomment when sentencepiece doesn't cause segfaults: https://github.com/ludwig-ai/ludwig/issues/2983,1
v0.10.3,TODO: uncomment once we figure out host memory issue: https://github.com/ludwig-ai/ludwig/issues/3107,1
v0.10.3,TODO: uncomment when CTRL bug (https://github.com/ludwig-ai/ludwig/issues/2977) has been fixed to add back in,1
v0.10.3,TODO(#1673): Add conditional logic for fields like this one:,1
v0.10.3,"TODO(travis): below type comparison is not perfect, as it doesn't consider the case where the default type",1
v0.10.3,"TODO(travis): explore similar contraints for GBMs, which don't have epochs",1
v0.10.3,HACK(Arnav): Set Mixtral target modules when using LoRA,1
v0.10.3,HACK(Arnav): Set Phi-2 target modules when using LoRA,1
v0.10.3,HACK(Arnav): Set Gemma target modules when using LoRA,1
v0.10.3,"This is not perfect since it includes tokens from both input + output features, but this at least",1
v0.10.3,TODO (Arnav): Figure out how to factor in rope scaling factor into this calculation.,1
v0.10.3,TODO: Add better support for category output features,1
v0.10.3,TODO(travis): handle this with helper function,1
v0.10.3,TODO(travis): not needed once we remove existing model config implementation,1
v0.10.3,TODO(ksbrar): What is this?,1
v0.10.3,"TODO(travis): this should be done through marshmallow dataclass' `required` field param,",1
v0.10.3,TODO(Arnav): Remove the hard check on max_length once we support multiple output features.,1
v0.10.3,TODO(Arnav): Refactor LossDataclassField to only accept loss types that are valid for the model,1
v0.10.3,TODO: Add schema support for Callable,1
v0.10.3,TODO: This should technically be a required paremeter. Do we need to add support for required params?,1
v0.10.3,TODO: Double-check support for this,1
v0.10.3,TODO: Double-check support for this as well as whether Callable args work properly,1
v0.10.3,TODO: create a search alg metadata class to register in place of individual metadata args,1
v0.10.3,TODO: Add a registry mapping string names to nevergrad optimizers,1
v0.10.3,TODO: Add schemas for nevergrad optimizer kwargs,1
v0.10.3,TODO: Add a registry of Optuna samplers schemas,1
v0.10.3,TODO(travis): figure out why calling this `bias` doesn't work,1
v0.10.3,TODO(travis): fix text generation when using prompt tuning:,1
v0.10.3,TODO(travis): fix prefix tuning and p-tuning to work with DDP,1
v0.10.3,TODO: <Alex>02/21/2024: Disabling AdaptionPrompt (waiting for PEFT release to fix,1
v0.10.3,TODO(This needs to be defined based on the Constraint class),1
v0.10.3,TODO: This is an unfortunate side-effect of dataclass init order - you cannot have non-default fields follow,1
v0.10.3,todo v0.4: currently not clear way to set model graph,1
v0.10.3,TODO(daniel): delete this.,1
v0.10.3,TODO(travis): figure out a good way to support this. The problem with,1
v0.10.3,"TODO(Justin): This should probably live in on_ludwig_end, once that's implemented.",1
v0.10.3,TODO: need to also include a filename for this figure,1
v0.10.3,"TODO(geoffrey, arnav): uncomment this when we have reconciled the config with the backend kwarg in api.py",1
v0.10.3,"TODO: `prompt` by default should be set to null, not a default dict:",1
v0.10.3,"TODO: retrieval by default should be set to null, not a default dict:",1
v0.10.3,TODO: len(template_refs) is a hacky attempt to check that there are references to *something* in the,1
v0.10.3,TODO(travis): figure out why we need these imports to avoid circular import error,1
v0.10.3,TODO: Replace with more robust required logic later.,1
v0.10.3,TODO(Arnav): Refactor to split into strategies like splitters,1
v0.10.3,"TODO(geoffrey): figure out where self.max_sequence_length is used– if not used, we might consider removing it.",1
v0.10.3,It's confusing to have both this and `max_new_tokens` as a mandatory param in the `forward` function.,1
v0.10.3,TODO(Arnav): Add support for probabilities and logits,1
v0.10.3,"TODO(Arnav): Figure out how to compute logits. For now, we return",1
v0.10.3,Dummy implementation.,1
v0.10.3,TODO: Use a configurable ratio for how often to use teacher forcing during training.,1
v0.10.3,TODO: Use a configurable ratio for how often to use teacher forcing during training.,1
v0.10.3,Dummy implementation.,1
v0.10.3,TODO(Arnav): Re-enable once we add DotProduct Combiner: https://github.com/ludwig-ai/ludwig/issues/3150,1
v0.10.3,"TODO(travis): we assume here that False is always the default, which may not be true. We should dervice",1
v0.10.3,TODO: Check if subscription has expired,1
v0.10.3,"TODO(travis): stopgap solution, we should make it so we don't need to do this",1
v0.10.3,todo (Wael): tests for all types.,1
v0.10.3,todo (Wael): tests for all types.,1
v0.10.3,Still needed for preprocessing  TODO(Connor): Refactor ludwig/data/preprocessing to use schema,1
v0.10.3,"TODO(travis): remove this, make type a protected string for each subclass",1
v0.10.3,"ORRIBLE HACK, IT'S THE ONLY WAY TO REMOVE PADDING",1
v0.10.3,END OF HORRIBLE HACK,1
v0.10.3,"TODO: more research needed on how to handle RayTune ""sample_from"" search space",1
v0.10.3,HACK(geoffrey): gpt2 has no pad token. Recommendation is to use eos token instead.,1
v0.10.3,TODO(geoffrey): can we better validate tokenizer parity before swapping in the TorchText tokenizer?,1
v0.10.3,TODO: improve this,1
v0.10.3,Workaround for: https://issues.apache.org/jira/browse/ARROW-5645,1
v0.10.3,TODO: make this configurable in the future. These parameters are from FastChat:,1
v0.10.3,"TODO: Wrap device_map=""auto"" in a try-except block since it may not be supported for all models (E.g. BertLMHead)  # noqa",1
v0.10.3,TODO(travis): move to cached_property when we drop Python 3.7.,1
v0.10.3,TODO: remove reshaping once https://github.com/microsoft/LightGBM/pull/4925 is released,1
v0.10.3,TODO: remove ravel once https://github.com/microsoft/LightGBM/pull/4925 is released,1
v0.10.3,TODO (jeffkinnison): revert to use the requested device once torch device usage is standardized,1
v0.10.3,TODO(shreya): Confirm types of args,1
v0.10.3,"TODO: when loading an existing model, this loses metric values for all but the last epoch.",1
v0.10.3,TODO(travis): implement imbalance ratio,1
v0.10.3,"TODO (ASN): add other modalities (image, etc. )",1
v0.10.3,"TODO(travis): this assumes ECD is the selected model type, which is not problematic for now, as",1
v0.10.3,"diverge, so we should find a way to remove this. The best solution is to the change the input params from",1
v0.10.3,TODO(travis): consolidate with implementation in data/ray.py,1
v0.10.3,TODO: only single task currently,1
v0.10.3,TODO(travis): include encoder and decoder steps during inference,1
v0.10.3,TODO(Arnav): This needs be more flexible to account for RoPE Scaling,1
v0.10.3,TODO: this implementation will not work if resuming from a previous checkpoint. Need to fix this.,1
v0.10.3,TODO: only single task currently,1
v0.10.3,TODO (jeffkinnison): Determine why the 8-bit `SCB` and `CB` matrices are deleted in the forward pass,1
v0.10.3,HACK (Tim): get the device of the targets to transfer self.eval_loss_metric to the same device,1
v0.10.3,"TODO(Arnav): Seems like doing this again and going between these format types in unnecessary, but",1
v0.10.3,"HACK(geoffrey): we need a non-empty loss, so we just fill it with zeros",1
v0.10.3,TODO(travis): this will need to change when we support multiple output features,1
v0.10.3,"TODO(travis): use the implementation of trainer itself to decide whether to save the model, to",1
v0.10.3,avoid this hack,1
v0.10.3,"TODO: see the ""TODO"" statement from ""LLM.save()"" in this module.",1
v0.10.3,TODO (jeffkinnison): revert to using the requested device for GBMs when device usage is fixed,1
v0.10.3,TODO: with change to misc_utils.set_random_seed() this may be redundant,1
v0.10.3,TODO(Justin): Check that the semantics of input_size align with what the combiner's output shape returns,1
v0.10.3,TODO(justin): Remove dummy implementation. Make input_shape and output_shape functions.,1
v0.10.3,TODO(geoffrey): figure out why self.index.search segfaults with larger batch sizes,1
v0.10.3,TODO(travis): support local feature importance,1
v0.10.3,TODO:,1
v0.10.3,TODO(travis): add back skip encoders at the end in finally. Shouldn't be an issue in most cases as we,1
v0.10.3,"TODO(travis): for force plots, need something similar to SHAP E[X]",1
v0.10.3,"TODO(travis): for force plots, need something similar to SHAP E[X]",1
v0.10.3,"TODO(travis): Consider changing to `if not torch.is_floating_point(t.dtype)` to simplify, then handle bool",1
v0.10.3,TODO(travis): pre-compute this during training from the full training dataset.,1
v0.10.3,"TODO(travis): for force plots, need something similar to SHAP E[X]",1
v0.10.3,"TODO(travis): for force plots, need something similar to SHAP E[X]",1
v0.10.3,"TODO: construct new datasets by running encoders (for text, image)",1
v0.10.3,TODO(shreya): Refactor preprocessing so that this can be moved upstream.,1
v0.10.3,TODO(Arnav): Add support for gradient checkpointing in the compiled model,1
v0.10.3,"For a full explanation of this 8-bit workaround, see https://github.com/ludwig-ai/ludwig/pull/3606",1
v0.10.3,TODO (jeffkinnison): Determine why `SCB` and `CB` are deleted from parameter state,1
v0.10.3,matrices are part of model state. This workaround is necessary because the matrices are,1
v0.10.3,"TODO: Implement batch size tuning for LLM, currently just returns the default batch size",1
v0.10.3,TODO: refactor this into an interface,1
v0.10.3,workaround type limitations of the underlying frameworks,1
v0.10.3,TODO(Arnav): Re-enable in Ray 2.3,1
v0.10.3,"TODO(travis): we should revisit the user format here, as it silently breaks situations",1
v0.10.3,"TODO ray: find an informed way to set the parallelism, in practice",1
v0.10.3,TODO: When this is implemented we also need to update the,1
v0.10.3,TODO(travis): consolidate with implementation in data/ray.py,1
v0.10.3,"This is needed because Daft only supports Dataframes, not Series",1
v0.10.3,TODO(ekl) deprecate this once read fusion is available.,1
v0.10.3,TODO(geoffrey): ensure this subclasses ray.data.datasource.Reader in ray 1.14,1
v0.10.3,TODO(ekl) deprecate this once read fusion is available.,1
v0.10.3,"doesn't work for Snappy, so we double-check ourselves.",1
v0.10.3,TODO(xwjiang): Remove this later.,1
v0.10.3,TODO(travis): need to fix checkpoint saving/loading for DeepSpeed to enable tuning,1
v0.10.3,"TODO(travis): this double-counts on the same device, it should use a cross-communicator instead",1
v0.10.3,"TODO: remove LOCAL_BACKEND as a global constant, replace with singleton LocalBackend.shared_instance().",1
v0.10.3,Temporary workaround to prevent tests from automatically using the Ray backend. Taken from,1
v0.10.3,TODO(Alex): Standardize load() signature as interface method in DatasetLoader and adhere to it in all subclasses.,1
v0.10.3,TODO(travis): add other indexing structures,1
v0.10.3,TODO(travis): open question if this is needed to ensure all workers using same weights,1
v0.10.3,TODO(travis): open question if this is needed to ensure all workers using same optimizer state,1
v0.10.3,"TODO(travis): currently Ray handles this for us, but is subject to hangs if one of the workers raises an",1
v0.10.3,TODO(geoffrey): Add a boolean arg to function to control load_module_strict behavior.,1
v0.10.2,HACK(geoffrey): `hyperopt_resources` is a required resource for hyperopt to prevent deadlocks in Ludwig tests.,1
v0.10.2,TODO(geoffrey): remove for Ray 2.2,1
v0.10.2,"HACK(Arnav): Remove configs that have LARS, LAMB or Lion optimizers, or Paged or 8-bit optimizers.",1
v0.10.2,is there a better way to do this?,1
v0.10.2,todo the hidden output is actually a tensor. May need modification,1
v0.10.2,todo figure out the output size for parallel 1d conv,1
v0.10.2,TODO:  Need variant of assert_model_parameters_updated() to account for the two step calling sequence,1
v0.10.2,todo: remove code,1
v0.10.2,TODO(ksbrar): Circle back after discussing whether additional properties should be allowed long-term.,1
v0.10.2,TODO(ksbrar): Circle back after discussing whether additional properties should be allowed long-term.,1
v0.10.2,TODO(Arnav): Re-enable once https://github.com/ludwig-ai/ludwig/issues/3150 is resolved since the GBM,1
v0.10.2,todo: re-add 'attention' after further research in implication of torch,1
v0.10.2,TODO(#1333): Refactor this test once torch sequence generator work is complete.,1
v0.10.2,"TODO(Justin): Move these to test_sequence_generator unit tests, and reintroduce decoder attention, beam_width, and",1
v0.10.2,TODO(travis): add when we support pretrained text models for gbms,1
v0.10.2,TODO: find a smaller model for testing,1
v0.10.2,TODO: figure out how to get mocks to work with Ray backend,1
v0.10.2,TODO(travis): We may want to expand upon this in the future to include some checks on model,1
v0.10.2,TODO(travis): We may want to expand upon this in the future to include some checks on model,1
v0.10.2,TODO(travis): We may want to expand upon this in the future to include some checks on model,1
v0.10.2,TODO: Determine whether this is desired behavior. Tracked here:,1
v0.10.2,TODO: feature type not yet supported,1
v0.10.2,handled non-determinism when comparing the metrics between the local and Ray backends. We work around this by,1
v0.10.2,TODO: feature type not yet supported,1
v0.10.2,TODO(geoffrey): Fold modin tests into test_ray_image as @pytest.mark.parametrized once tests are optimized,1
v0.10.2,TODO(travis): move this to separate gpu module so we only have one ray cluster running at a time,1
v0.10.2,TODO(travis): https://github.com/ludwig-ai/ludwig/issues/1956,1
v0.10.2,TODO: Determine if we still need this if-then-else construct,1
v0.10.2,TODO(travis): once we support GBM text features,1
v0.10.2,TODO(travis): once we support GBM text features,1
v0.10.2,TODO(travis): need unit tests to test the get_embedding_layer() of every encoder to ensure it is,1
v0.10.2,TODO: fix LLM model loading,1
v0.10.2,TODO(arnav): p-tuning and prefix tuning have errors when enabled that seem to stem from DDP:,1
v0.10.2,TODO(Arnav): Re-enable once we can run tests on GPUs,1
v0.10.2,TODO: <Alex>02/21/2024: Disabling AdaptionPrompt (waiting for PEFT release to fix,1
v0.10.2,TODO: Re-enable once we can run tests on GPUs,1
v0.10.2,TODO: <Alex>02/21/2024: Disabling AdaptionPrompt (waiting for PEFT release to fix,1
v0.10.2,"For a full explanation of this 8-bit workaround, see https://github.com/ludwig-ai/ludwig/pull/3606",1
v0.10.2,"TODO: Uncomment ""filter_for_weight_format()"" method definition and enable its usage once GPU tests are set up.",1
v0.10.2,TODO(shreya): Uncomment when https://github.com/ludwig-ai/ludwig/issues/2039 is fixed.,1
v0.10.2,TODO: remove this workaround when audio preprocessing is fixed.,1
v0.10.2,TODO: these are the only outputs we provide from Torchscript for now,1
v0.10.2,TODO all shadows built in name - come up with a more descriptive name,1
v0.10.2,TODO (Connor): Refactor to use self.config_obj,1
v0.10.2,TODO (ASN): add support for substitute_with_max parameter,1
v0.10.2,TODO(Justin): Decide if it's worth folding padding_side handling into llm.py's tokenizer initialization.,1
v0.10.2,TODO(travis): WIP,1
v0.10.2,"TODO(travis): there's a lot of redundancy in this approach, since we are preprocessing the same DataFrame",1
v0.10.2,"TODO(travis): can optimize the preprocessing part here, since we only need to preprocess / predict",1
v0.10.2,TODO(travis): generalize this to support any pandas output format,1
v0.10.2,TODO (Connor): Refactor to use self.config_obj,1
v0.10.2,"TODO: In the future, it may be possible to move up the model type check into the BaseModel class.",1
v0.10.2,TODO: fix for Ray where workers may be of different skus,1
v0.10.2,TODO(travis): remove in favor of on_hyperopt_end for naming consistency,1
v0.10.2,TODO: https://github.com/ludwig-ai/ludwig/issues/2633,1
v0.10.2,todo: revise docstring,1
v0.10.2,todo: assess how to specify padding for equivalent to 'same',1
v0.10.2,todo: determine how to pool_padding equivalent of 'same',1
v0.10.2,todo: fixup docstring,1
v0.10.2,todo: review docstring,1
v0.10.2,todo: fix up docstring,1
v0.10.2,todo: fix up docstring,1
v0.10.2,todo: update docstring as needed,1
v0.10.2,TODO(shreya): Make sure this is updated when FCStack is updated,1
v0.10.2,TODO(Arnav): Remove this once we have reduce_output options set for,1
v0.10.2,TODO(travis): get_hf_config_param_names should be implemented as abstract in HFEncoderConfig,1
v0.10.2,TODO(shreya): Confirm that this is it,1
v0.10.2,TODO(shreya): Confirm that this is it,1
v0.10.2,TODO(justin): This may need to be conditioned on which AutoModel gets chosen.,1
v0.10.2,TODO(Arnav): This needs be more flexible to account for RoPE Scaling,1
v0.10.2,TODO: this implementation will not work if resuming from a previous checkpoint. Need to fix this.,1
v0.10.2,TODO (ASN): Decide how we want to proceed if at least one trial has,1
v0.10.2,TODO(shreya): Should this hyperopt config param be set here?,1
v0.10.2,TODO (ASN): add image heuristics,1
v0.10.2,"TODO(travis): less hacky way to do this, we should probably allow ModelConfig to be created without output",1
v0.10.2,"todo future: this may be redundant, check",1
v0.10.2,Workaround for including additional tensors from output of input encoders for,1
v0.10.2,TODO(Justin): Think about how to make this communication work for multi-sequence,1
v0.10.2,todo: when https://github.com/ludwig-ai/ludwig/issues/810 is closed,1
v0.10.2,"todo: can we just use projector_size? # hidden_size,",1
v0.10.2,"todo future: this may be redundant, check",1
v0.10.2,"todo future: this may be redundant, check",1
v0.10.2,Workaround for including additional tensors from output of input encoders for,1
v0.10.2,TODO(Justin): Think about how to make this communication work for multi-sequence,1
v0.10.2,todo future: maybe reintroduce these attention function,1
v0.10.2,todo future: maybe reintroduce these attention function,1
v0.10.2,todo future: maybe reintroduce these attention function,1
v0.10.2,TODO(travis): consider moving this behind a general BatchNorm interface to avoid this kludge.,1
v0.10.2,"todo: enumerate for debugging, remove after testing",1
v0.10.2,TODO(shreya): Combine with ResNetBlock by adding a flag.,1
v0.10.2,TODO(shreya): Implement sparse embedding lookup.,1
v0.10.2,# TODO(shreya): Check if this is equivalent,1
v0.10.2,# TODO(shreya): Check if supported in torch,1
v0.10.2,todo: review for generality,1
v0.10.2,TODO: Simplify this.,1
v0.10.2,TODO(travis): make this more general to other cumulative loss functions,1
v0.10.2,Dummy implementation.,1
v0.10.2,"TODO(Justin): Add a confusion matrix, see",1
v0.10.2,TODO: add a mechanism for letting the user decide to save it,1
v0.10.2,todo: re-evaluate need for separate handling of `attention` reducer,1
v0.10.2,TODO(Justin): Clean this up.,1
v0.10.2,TODO: change to debug level before merging,1
v0.10.2,TODO: should this raise an exception if not in training mode?,1
v0.10.2,TODO(travis): do this through an interface rather than conditional logic,1
v0.10.2,Nested conditional is a workaround to short-circuit boolean evaluation.,1
v0.10.2,TODO: alternatively use get_average_image() for unreachable images,1
v0.10.2,todo future add multiprocessing/multithreading,1
v0.10.2,TODO(travis): do we even need a user param for vector size if we're going to auto-infer it in all,1
v0.10.2,TODO(1891): Remove backward compatibility hack once all models have been retrained with Ludwig after,1
v0.10.2,todo: refactor to reuse SequenceOutputFeature.postprocess_predictions,1
v0.10.2,TODO: Add a mechanism that lets the user save the full probability distribution if they want.,1
v0.10.2,TODO(geoffrey): add support for Dask DataFrames,1
v0.10.2,TODO(Arnav): see if there's a way to only remove them if the entry does't have quotes. This currently,1
v0.10.2,"removes all "" from the string (even those not added by json.dumps), which is not ideal.",1
v0.10.2,Convert datetime to int64 to workaround Dask limitation,1
v0.10.2,"TODO pyarrow: this is needed for caching to work with pyarrow. if removed, the following error is raised:",1
v0.10.2,TODO(travis): passing in MODEL_ECD is a hack here that can be removed once we move to using,1
v0.10.2,"encoder schema at all. This hack works for now because all encoders are supported by ECD, so",1
v0.10.2,todo figure out if additional parameters are needed,1
v0.10.2,"TODO(travis): instead of using raw dictionary, this should be loaded into a proper PreprocessingConfig",1
v0.10.2,TODO dask: this needs to work with DataFrames,1
v0.10.2,TODO(travis): decouple config from training_set_metadata so we don't need to,1
v0.10.2,TODO(joppe): support out of memory negative sampling using Dask,1
v0.10.2,TODO(travis): revisit in the future to make this more precise,1
v0.10.2,TODO: Add link to windowing docs.,1
v0.10.2,TODO: figure out correct typing for augmentation_pipeline after refactoring is done,1
v0.10.2,TODO: consider removing this. doesn't work currently and read performance seems generally,1
v0.10.2,TODO: convert to debug message when done with development,1
v0.10.2,"TODO(travis): could try hashing the in-memory dataset, but this is tricky for Dask",1
v0.10.2,todo future: reintroduce the bucketed batcher,1
v0.10.2,TODO ray: implement dynamic batch size,1
v0.10.2,TODO: Change annotation to PublicAPI once Ludwig 0.7 is released,1
v0.10.2,TODO(travis): is this redundant with `clipglobalnorm`?,1
v0.10.2,TODO(travis) consider removing this in the future after deprecation period,1
v0.10.2,TODO: use registry pattern for trainers,1
v0.10.2,TODO: Change to RAISE and update descriptions once we want to enforce strict schemas.,1
v0.10.2,TODO: Maybe need to plumb 'required' through here,1
v0.10.2,"TODO(travis): this seems much too verbose, does the validation error not show the specific error?",1
v0.10.2,"TODO(travis): too much boilerplate here, we should find a way to abstract all this and only require specifying the",1
v0.10.2,"TODO(travis): this seems much too verbose, does the validation error not show the specific error?",1
v0.10.2,TODO(#1673): Need some more logic here for validating against output features,1
v0.10.2,"TODO: Re-enable ""goss"" when supported: https://github.com/ludwig-ai/ludwig/issues/2988",1
v0.10.2,TODO(travis): figure out why we need these imports to avoid circular import error,1
v0.10.2,"TODO (Connor): Add nesting logic for fc_layers, see fully_connected_module.py",1
v0.10.2,"TODO (Connor): Add nesting logic for fc_layers, see fully_connected_module.py",1
v0.10.2,TODO(travis): seems like this is not really a valid user option. We should probably just remove these,1
v0.10.2,TODO: uncomment when sentencepiece doesn't cause segfaults: https://github.com/ludwig-ai/ludwig/issues/2983,1
v0.10.2,TODO: uncomment once we figure out host memory issue: https://github.com/ludwig-ai/ludwig/issues/3107,1
v0.10.2,TODO: uncomment when CTRL bug (https://github.com/ludwig-ai/ludwig/issues/2977) has been fixed to add back in,1
v0.10.2,TODO(#1673): Add conditional logic for fields like this one:,1
v0.10.2,"TODO(travis): below type comparison is not perfect, as it doesn't consider the case where the default type",1
v0.10.2,"TODO(travis): explore similar contraints for GBMs, which don't have epochs",1
v0.10.2,HACK(Arnav): Set Mixtral target modules when using LoRA,1
v0.10.2,HACK(Arnav): Set Phi-2 target modules when using LoRA,1
v0.10.2,HACK(Arnav): Set Gemma target modules when using LoRA,1
v0.10.2,"This is not perfect since it includes tokens from both input + output features, but this at least",1
v0.10.2,TODO (Arnav): Figure out how to factor in rope scaling factor into this calculation.,1
v0.10.2,TODO: Add better support for category output features,1
v0.10.2,TODO(travis): handle this with helper function,1
v0.10.2,TODO(travis): not needed once we remove existing model config implementation,1
v0.10.2,TODO(ksbrar): What is this?,1
v0.10.2,"TODO(travis): this should be done through marshmallow dataclass' `required` field param,",1
v0.10.2,TODO(Arnav): Remove the hard check on max_length once we support multiple output features.,1
v0.10.2,TODO(Arnav): Refactor LossDataclassField to only accept loss types that are valid for the model,1
v0.10.2,TODO: Add schema support for Callable,1
v0.10.2,TODO: This should technically be a required paremeter. Do we need to add support for required params?,1
v0.10.2,TODO: Double-check support for this,1
v0.10.2,TODO: Double-check support for this as well as whether Callable args work properly,1
v0.10.2,TODO: create a search alg metadata class to register in place of individual metadata args,1
v0.10.2,TODO: Add a registry mapping string names to nevergrad optimizers,1
v0.10.2,TODO: Add schemas for nevergrad optimizer kwargs,1
v0.10.2,TODO: Add a registry of Optuna samplers schemas,1
v0.10.2,TODO(travis): figure out why calling this `bias` doesn't work,1
v0.10.2,TODO(travis): fix text generation when using prompt tuning:,1
v0.10.2,TODO(travis): fix prefix tuning and p-tuning to work with DDP,1
v0.10.2,TODO: <Alex>02/21/2024: Disabling AdaptionPrompt (waiting for PEFT release to fix,1
v0.10.2,TODO(This needs to be defined based on the Constraint class),1
v0.10.2,TODO: This is an unfortunate side-effect of dataclass init order - you cannot have non-default fields follow,1
v0.10.2,todo v0.4: currently not clear way to set model graph,1
v0.10.2,TODO(daniel): delete this.,1
v0.10.2,TODO(travis): figure out a good way to support this. The problem with,1
v0.10.2,"TODO(Justin): This should probably live in on_ludwig_end, once that's implemented.",1
v0.10.2,TODO: need to also include a filename for this figure,1
v0.10.2,"TODO(geoffrey, arnav): uncomment this when we have reconciled the config with the backend kwarg in api.py",1
v0.10.2,"TODO: `prompt` by default should be set to null, not a default dict:",1
v0.10.2,"TODO: retrieval by default should be set to null, not a default dict:",1
v0.10.2,TODO: len(template_refs) is a hacky attempt to check that there are references to *something* in the,1
v0.10.2,TODO(travis): figure out why we need these imports to avoid circular import error,1
v0.10.2,TODO: Replace with more robust required logic later.,1
v0.10.2,TODO(Arnav): Refactor to split into strategies like splitters,1
v0.10.2,"TODO(geoffrey): figure out where self.max_sequence_length is used– if not used, we might consider removing it.",1
v0.10.2,It's confusing to have both this and `max_new_tokens` as a mandatory param in the `forward` function.,1
v0.10.2,TODO(Arnav): Add support for probabilities and logits,1
v0.10.2,"TODO(Arnav): Figure out how to compute logits. For now, we return",1
v0.10.2,Dummy implementation.,1
v0.10.2,TODO: Use a configurable ratio for how often to use teacher forcing during training.,1
v0.10.2,TODO: Use a configurable ratio for how often to use teacher forcing during training.,1
v0.10.2,Dummy implementation.,1
v0.10.2,TODO(Arnav): Re-enable once we add DotProduct Combiner: https://github.com/ludwig-ai/ludwig/issues/3150,1
v0.10.2,"TODO(travis): we assume here that False is always the default, which may not be true. We should dervice",1
v0.10.2,TODO: Check if subscription has expired,1
v0.10.2,"TODO(travis): stopgap solution, we should make it so we don't need to do this",1
v0.10.2,todo (Wael): tests for all types.,1
v0.10.2,todo (Wael): tests for all types.,1
v0.10.2,Still needed for preprocessing  TODO(Connor): Refactor ludwig/data/preprocessing to use schema,1
v0.10.2,"TODO(travis): remove this, make type a protected string for each subclass",1
v0.10.2,"ORRIBLE HACK, IT'S THE ONLY WAY TO REMOVE PADDING",1
v0.10.2,END OF HORRIBLE HACK,1
v0.10.2,"TODO: more research needed on how to handle RayTune ""sample_from"" search space",1
v0.10.2,HACK(geoffrey): gpt2 has no pad token. Recommendation is to use eos token instead.,1
v0.10.2,TODO(geoffrey): can we better validate tokenizer parity before swapping in the TorchText tokenizer?,1
v0.10.2,TODO: improve this,1
v0.10.2,Workaround for: https://issues.apache.org/jira/browse/ARROW-5645,1
v0.10.2,TODO: make this configurable in the future. These parameters are from FastChat:,1
v0.10.2,"TODO: Wrap device_map=""auto"" in a try-except block since it may not be supported for all models (E.g. BertLMHead)  # noqa",1
v0.10.2,TODO(travis): move to cached_property when we drop Python 3.7.,1
v0.10.2,TODO: remove reshaping once https://github.com/microsoft/LightGBM/pull/4925 is released,1
v0.10.2,TODO: remove ravel once https://github.com/microsoft/LightGBM/pull/4925 is released,1
v0.10.2,TODO (jeffkinnison): revert to use the requested device once torch device usage is standardized,1
v0.10.2,TODO(shreya): Confirm types of args,1
v0.10.2,"TODO: when loading an existing model, this loses metric values for all but the last epoch.",1
v0.10.2,TODO(travis): implement imbalance ratio,1
v0.10.2,"TODO (ASN): add other modalities (image, etc. )",1
v0.10.2,"TODO(travis): this assumes ECD is the selected model type, which is not problematic for now, as",1
v0.10.2,"diverge, so we should find a way to remove this. The best solution is to the change the input params from",1
v0.10.2,TODO(travis): consolidate with implementation in data/ray.py,1
v0.10.2,TODO: only single task currently,1
v0.10.2,TODO(travis): include encoder and decoder steps during inference,1
v0.10.2,TODO(Arnav): This needs be more flexible to account for RoPE Scaling,1
v0.10.2,TODO: this implementation will not work if resuming from a previous checkpoint. Need to fix this.,1
v0.10.2,TODO: only single task currently,1
v0.10.2,TODO (jeffkinnison): Determine why the 8-bit `SCB` and `CB` matrices are deleted in the forward pass,1
v0.10.2,HACK (Tim): get the device of the targets to transfer self.eval_loss_metric to the same device,1
v0.10.2,"TODO(Arnav): Seems like doing this again and going between these format types in unnecessary, but",1
v0.10.2,"HACK(geoffrey): we need a non-empty loss, so we just fill it with zeros",1
v0.10.2,TODO(travis): this will need to change when we support multiple output features,1
v0.10.2,"TODO(travis): use the implementation of trainer itself to decide whether to save the model, to",1
v0.10.2,avoid this hack,1
v0.10.2,"TODO: see the ""TODO"" statement from ""LLM.save()"" in this module.",1
v0.10.2,TODO (jeffkinnison): revert to using the requested device for GBMs when device usage is fixed,1
v0.10.2,TODO: with change to misc_utils.set_random_seed() this may be redundant,1
v0.10.2,TODO(Justin): Check that the semantics of input_size align with what the combiner's output shape returns,1
v0.10.2,TODO(justin): Remove dummy implementation. Make input_shape and output_shape functions.,1
v0.10.2,TODO(geoffrey): figure out why self.index.search segfaults with larger batch sizes,1
v0.10.2,TODO(travis): support local feature importance,1
v0.10.2,TODO:,1
v0.10.2,TODO(travis): add back skip encoders at the end in finally. Shouldn't be an issue in most cases as we,1
v0.10.2,"TODO(travis): for force plots, need something similar to SHAP E[X]",1
v0.10.2,"TODO(travis): for force plots, need something similar to SHAP E[X]",1
v0.10.2,"TODO(travis): Consider changing to `if not torch.is_floating_point(t.dtype)` to simplify, then handle bool",1
v0.10.2,TODO(travis): pre-compute this during training from the full training dataset.,1
v0.10.2,"TODO(travis): for force plots, need something similar to SHAP E[X]",1
v0.10.2,"TODO(travis): for force plots, need something similar to SHAP E[X]",1
v0.10.2,"TODO: construct new datasets by running encoders (for text, image)",1
v0.10.2,TODO(shreya): Refactor preprocessing so that this can be moved upstream.,1
v0.10.2,TODO(Arnav): Add support for gradient checkpointing in the compiled model,1
v0.10.2,"For a full explanation of this 8-bit workaround, see https://github.com/ludwig-ai/ludwig/pull/3606",1
v0.10.2,TODO (jeffkinnison): Determine why `SCB` and `CB` are deleted from parameter state,1
v0.10.2,matrices are part of model state. This workaround is necessary because the matrices are,1
v0.10.2,"TODO: Implement batch size tuning for LLM, currently just returns the default batch size",1
v0.10.2,TODO: refactor this into an interface,1
v0.10.2,workaround type limitations of the underlying frameworks,1
v0.10.2,TODO(Arnav): Re-enable in Ray 2.3,1
v0.10.2,"TODO(travis): we should revisit the user format here, as it silently breaks situations",1
v0.10.2,"TODO ray: find an informed way to set the parallelism, in practice",1
v0.10.2,TODO: When this is implemented we also need to update the,1
v0.10.2,TODO(travis): consolidate with implementation in data/ray.py,1
v0.10.2,"This is needed because Daft only supports Dataframes, not Series",1
v0.10.2,TODO(ekl) deprecate this once read fusion is available.,1
v0.10.2,TODO(geoffrey): ensure this subclasses ray.data.datasource.Reader in ray 1.14,1
v0.10.2,TODO(ekl) deprecate this once read fusion is available.,1
v0.10.2,"doesn't work for Snappy, so we double-check ourselves.",1
v0.10.2,TODO(xwjiang): Remove this later.,1
v0.10.2,TODO(travis): need to fix checkpoint saving/loading for DeepSpeed to enable tuning,1
v0.10.2,"TODO(travis): this double-counts on the same device, it should use a cross-communicator instead",1
v0.10.2,"TODO: remove LOCAL_BACKEND as a global constant, replace with singleton LocalBackend.shared_instance().",1
v0.10.2,Temporary workaround to prevent tests from automatically using the Ray backend. Taken from,1
v0.10.2,TODO(Alex): Standardize load() signature as interface method in DatasetLoader and adhere to it in all subclasses.,1
v0.10.2,TODO(travis): add other indexing structures,1
v0.10.2,TODO(travis): open question if this is needed to ensure all workers using same weights,1
v0.10.2,TODO(travis): open question if this is needed to ensure all workers using same optimizer state,1
v0.10.2,"TODO(travis): currently Ray handles this for us, but is subject to hangs if one of the workers raises an",1
v0.10.2,TODO(geoffrey): Add a boolean arg to function to control load_module_strict behavior.,1
v0.10.1,HACK(geoffrey): `hyperopt_resources` is a required resource for hyperopt to prevent deadlocks in Ludwig tests.,1
v0.10.1,TODO(geoffrey): remove for Ray 2.2,1
v0.10.1,"HACK(Arnav): Remove configs that have LARS, LAMB or Lion optimizers, or Paged or 8-bit optimizers.",1
v0.10.1,is there a better way to do this?,1
v0.10.1,todo the hidden output is actually a tensor. May need modification,1
v0.10.1,todo figure out the output size for parallel 1d conv,1
v0.10.1,TODO:  Need variant of assert_model_parameters_updated() to account for the two step calling sequence,1
v0.10.1,todo: remove code,1
v0.10.1,TODO(ksbrar): Circle back after discussing whether additional properties should be allowed long-term.,1
v0.10.1,TODO(ksbrar): Circle back after discussing whether additional properties should be allowed long-term.,1
v0.10.1,TODO(Arnav): Re-enable once https://github.com/ludwig-ai/ludwig/issues/3150 is resolved since the GBM,1
v0.10.1,todo: re-add 'attention' after further research in implication of torch,1
v0.10.1,TODO(#1333): Refactor this test once torch sequence generator work is complete.,1
v0.10.1,"TODO(Justin): Move these to test_sequence_generator unit tests, and reintroduce decoder attention, beam_width, and",1
v0.10.1,TODO(travis): add when we support pretrained text models for gbms,1
v0.10.1,TODO: find a smaller model for testing,1
v0.10.1,TODO: figure out how to get mocks to work with Ray backend,1
v0.10.1,TODO(travis): We may want to expand upon this in the future to include some checks on model,1
v0.10.1,TODO(travis): We may want to expand upon this in the future to include some checks on model,1
v0.10.1,TODO(travis): We may want to expand upon this in the future to include some checks on model,1
v0.10.1,TODO: Determine whether this is desired behavior. Tracked here:,1
v0.10.1,TODO: feature type not yet supported,1
v0.10.1,handled non-determinism when comparing the metrics between the local and Ray backends. We work around this by,1
v0.10.1,TODO: feature type not yet supported,1
v0.10.1,TODO(geoffrey): Fold modin tests into test_ray_image as @pytest.mark.parametrized once tests are optimized,1
v0.10.1,TODO(travis): move this to separate gpu module so we only have one ray cluster running at a time,1
v0.10.1,TODO(travis): https://github.com/ludwig-ai/ludwig/issues/1956,1
v0.10.1,TODO: Determine if we still need this if-then-else construct,1
v0.10.1,TODO(travis): once we support GBM text features,1
v0.10.1,TODO(travis): once we support GBM text features,1
v0.10.1,TODO(travis): need unit tests to test the get_embedding_layer() of every encoder to ensure it is,1
v0.10.1,TODO: fix LLM model loading,1
v0.10.1,TODO(arnav): p-tuning and prefix tuning have errors when enabled that seem to stem from DDP:,1
v0.10.1,TODO(Arnav): Re-enable once we can run tests on GPUs,1
v0.10.1,TODO: <Alex>02/21/2024: Disabling AdaptionPrompt (waiting for PEFT release to fix,1
v0.10.1,TODO: Re-enable once we can run tests on GPUs,1
v0.10.1,TODO: <Alex>02/21/2024: Disabling AdaptionPrompt (waiting for PEFT release to fix,1
v0.10.1,"For a full explanation of this 8-bit workaround, see https://github.com/ludwig-ai/ludwig/pull/3606",1
v0.10.1,"TODO: Uncomment ""filter_for_weight_format()"" method definition and enable its usage once GPU tests are set up.",1
v0.10.1,TODO(shreya): Uncomment when https://github.com/ludwig-ai/ludwig/issues/2039 is fixed.,1
v0.10.1,TODO: remove this workaround when audio preprocessing is fixed.,1
v0.10.1,TODO: these are the only outputs we provide from Torchscript for now,1
v0.10.1,TODO all shadows built in name - come up with a more descriptive name,1
v0.10.1,TODO (Connor): Refactor to use self.config_obj,1
v0.10.1,TODO (ASN): add support for substitute_with_max parameter,1
v0.10.1,TODO(Justin): Decide if it's worth folding padding_side handling into llm.py's tokenizer initialization.,1
v0.10.1,TODO(travis): WIP,1
v0.10.1,"TODO(travis): there's a lot of redundancy in this approach, since we are preprocessing the same DataFrame",1
v0.10.1,"TODO(travis): can optimize the preprocessing part here, since we only need to preprocess / predict",1
v0.10.1,TODO(travis): generalize this to support any pandas output format,1
v0.10.1,TODO (Connor): Refactor to use self.config_obj,1
v0.10.1,"TODO: In the future, it may be possible to move up the model type check into the BaseModel class.",1
v0.10.1,TODO: fix for Ray where workers may be of different skus,1
v0.10.1,TODO(travis): remove in favor of on_hyperopt_end for naming consistency,1
v0.10.1,TODO: https://github.com/ludwig-ai/ludwig/issues/2633,1
v0.10.1,todo: revise docstring,1
v0.10.1,todo: assess how to specify padding for equivalent to 'same',1
v0.10.1,todo: determine how to pool_padding equivalent of 'same',1
v0.10.1,todo: fixup docstring,1
v0.10.1,todo: review docstring,1
v0.10.1,todo: fix up docstring,1
v0.10.1,todo: fix up docstring,1
v0.10.1,todo: update docstring as needed,1
v0.10.1,TODO(shreya): Make sure this is updated when FCStack is updated,1
v0.10.1,TODO(Arnav): Remove this once we have reduce_output options set for,1
v0.10.1,TODO(travis): get_hf_config_param_names should be implemented as abstract in HFEncoderConfig,1
v0.10.1,TODO(shreya): Confirm that this is it,1
v0.10.1,TODO(shreya): Confirm that this is it,1
v0.10.1,TODO(justin): This may need to be conditioned on which AutoModel gets chosen.,1
v0.10.1,TODO(Arnav): This needs be more flexible to account for RoPE Scaling,1
v0.10.1,TODO: this implementation will not work if resuming from a previous checkpoint. Need to fix this.,1
v0.10.1,TODO (ASN): Decide how we want to proceed if at least one trial has,1
v0.10.1,TODO(shreya): Should this hyperopt config param be set here?,1
v0.10.1,TODO (ASN): add image heuristics,1
v0.10.1,"TODO(travis): less hacky way to do this, we should probably allow ModelConfig to be created without output",1
v0.10.1,"todo future: this may be redundant, check",1
v0.10.1,Workaround for including additional tensors from output of input encoders for,1
v0.10.1,TODO(Justin): Think about how to make this communication work for multi-sequence,1
v0.10.1,todo: when https://github.com/ludwig-ai/ludwig/issues/810 is closed,1
v0.10.1,"todo: can we just use projector_size? # hidden_size,",1
v0.10.1,"todo future: this may be redundant, check",1
v0.10.1,"todo future: this may be redundant, check",1
v0.10.1,Workaround for including additional tensors from output of input encoders for,1
v0.10.1,TODO(Justin): Think about how to make this communication work for multi-sequence,1
v0.10.1,todo future: maybe reintroduce these attention function,1
v0.10.1,todo future: maybe reintroduce these attention function,1
v0.10.1,todo future: maybe reintroduce these attention function,1
v0.10.1,TODO(travis): consider moving this behind a general BatchNorm interface to avoid this kludge.,1
v0.10.1,"todo: enumerate for debugging, remove after testing",1
v0.10.1,TODO(shreya): Combine with ResNetBlock by adding a flag.,1
v0.10.1,TODO(shreya): Implement sparse embedding lookup.,1
v0.10.1,# TODO(shreya): Check if this is equivalent,1
v0.10.1,# TODO(shreya): Check if supported in torch,1
v0.10.1,todo: review for generality,1
v0.10.1,TODO: Simplify this.,1
v0.10.1,TODO(travis): make this more general to other cumulative loss functions,1
v0.10.1,Dummy implementation.,1
v0.10.1,"TODO(Justin): Add a confusion matrix, see",1
v0.10.1,TODO: add a mechanism for letting the user decide to save it,1
v0.10.1,todo: re-evaluate need for separate handling of `attention` reducer,1
v0.10.1,TODO(Justin): Clean this up.,1
v0.10.1,TODO: change to debug level before merging,1
v0.10.1,TODO: should this raise an exception if not in training mode?,1
v0.10.1,TODO(travis): do this through an interface rather than conditional logic,1
v0.10.1,Nested conditional is a workaround to short-circuit boolean evaluation.,1
v0.10.1,TODO: alternatively use get_average_image() for unreachable images,1
v0.10.1,todo future add multiprocessing/multithreading,1
v0.10.1,TODO(travis): do we even need a user param for vector size if we're going to auto-infer it in all,1
v0.10.1,TODO(1891): Remove backward compatibility hack once all models have been retrained with Ludwig after,1
v0.10.1,todo: refactor to reuse SequenceOutputFeature.postprocess_predictions,1
v0.10.1,TODO: Add a mechanism that lets the user save the full probability distribution if they want.,1
v0.10.1,TODO(geoffrey): add support for Dask DataFrames,1
v0.10.1,TODO(Arnav): see if there's a way to only remove them if the entry does't have quotes. This currently,1
v0.10.1,"removes all "" from the string (even those not added by json.dumps), which is not ideal.",1
v0.10.1,Convert datetime to int64 to workaround Dask limitation,1
v0.10.1,"TODO pyarrow: this is needed for caching to work with pyarrow. if removed, the following error is raised:",1
v0.10.1,TODO(travis): passing in MODEL_ECD is a hack here that can be removed once we move to using,1
v0.10.1,"encoder schema at all. This hack works for now because all encoders are supported by ECD, so",1
v0.10.1,todo figure out if additional parameters are needed,1
v0.10.1,"TODO(travis): instead of using raw dictionary, this should be loaded into a proper PreprocessingConfig",1
v0.10.1,TODO dask: this needs to work with DataFrames,1
v0.10.1,TODO(travis): decouple config from training_set_metadata so we don't need to,1
v0.10.1,TODO(joppe): support out of memory negative sampling using Dask,1
v0.10.1,TODO(travis): revisit in the future to make this more precise,1
v0.10.1,TODO: Add link to windowing docs.,1
v0.10.1,TODO: figure out correct typing for augmentation_pipeline after refactoring is done,1
v0.10.1,TODO: consider removing this. doesn't work currently and read performance seems generally,1
v0.10.1,TODO: convert to debug message when done with development,1
v0.10.1,"TODO(travis): could try hashing the in-memory dataset, but this is tricky for Dask",1
v0.10.1,todo future: reintroduce the bucketed batcher,1
v0.10.1,TODO ray: implement dynamic batch size,1
v0.10.1,TODO: Change annotation to PublicAPI once Ludwig 0.7 is released,1
v0.10.1,TODO(travis): is this redundant with `clipglobalnorm`?,1
v0.10.1,TODO(travis) consider removing this in the future after deprecation period,1
v0.10.1,TODO: use registry pattern for trainers,1
v0.10.1,TODO: Change to RAISE and update descriptions once we want to enforce strict schemas.,1
v0.10.1,TODO: Maybe need to plumb 'required' through here,1
v0.10.1,"TODO(travis): this seems much too verbose, does the validation error not show the specific error?",1
v0.10.1,"TODO(travis): too much boilerplate here, we should find a way to abstract all this and only require specifying the",1
v0.10.1,"TODO(travis): this seems much too verbose, does the validation error not show the specific error?",1
v0.10.1,TODO(#1673): Need some more logic here for validating against output features,1
v0.10.1,"TODO: Re-enable ""goss"" when supported: https://github.com/ludwig-ai/ludwig/issues/2988",1
v0.10.1,TODO(travis): figure out why we need these imports to avoid circular import error,1
v0.10.1,"TODO (Connor): Add nesting logic for fc_layers, see fully_connected_module.py",1
v0.10.1,"TODO (Connor): Add nesting logic for fc_layers, see fully_connected_module.py",1
v0.10.1,TODO(travis): seems like this is not really a valid user option. We should probably just remove these,1
v0.10.1,TODO: uncomment when sentencepiece doesn't cause segfaults: https://github.com/ludwig-ai/ludwig/issues/2983,1
v0.10.1,TODO: uncomment once we figure out host memory issue: https://github.com/ludwig-ai/ludwig/issues/3107,1
v0.10.1,TODO: uncomment when CTRL bug (https://github.com/ludwig-ai/ludwig/issues/2977) has been fixed to add back in,1
v0.10.1,TODO(#1673): Add conditional logic for fields like this one:,1
v0.10.1,"TODO(travis): below type comparison is not perfect, as it doesn't consider the case where the default type",1
v0.10.1,"TODO(travis): explore similar contraints for GBMs, which don't have epochs",1
v0.10.1,HACK(Arnav): Set Mixtral target modules when using LoRA,1
v0.10.1,HACK(Arnav): Set Phi-2 target modules when using LoRA,1
v0.10.1,HACK(Arnav): Set Gemma target modules when using LoRA,1
v0.10.1,"This is not perfect since it includes tokens from both input + output features, but this at least",1
v0.10.1,TODO (Arnav): Figure out how to factor in rope scaling factor into this calculation.,1
v0.10.1,TODO: Add better support for category output features,1
v0.10.1,TODO(travis): handle this with helper function,1
v0.10.1,TODO(travis): not needed once we remove existing model config implementation,1
v0.10.1,TODO(ksbrar): What is this?,1
v0.10.1,"TODO(travis): this should be done through marshmallow dataclass' `required` field param,",1
v0.10.1,TODO(Arnav): Remove the hard check on max_length once we support multiple output features.,1
v0.10.1,TODO(Arnav): Refactor LossDataclassField to only accept loss types that are valid for the model,1
v0.10.1,TODO: Add schema support for Callable,1
v0.10.1,TODO: This should technically be a required paremeter. Do we need to add support for required params?,1
v0.10.1,TODO: Double-check support for this,1
v0.10.1,TODO: Double-check support for this as well as whether Callable args work properly,1
v0.10.1,TODO: create a search alg metadata class to register in place of individual metadata args,1
v0.10.1,TODO: Add a registry mapping string names to nevergrad optimizers,1
v0.10.1,TODO: Add schemas for nevergrad optimizer kwargs,1
v0.10.1,TODO: Add a registry of Optuna samplers schemas,1
v0.10.1,TODO(travis): figure out why calling this `bias` doesn't work,1
v0.10.1,TODO(travis): fix text generation when using prompt tuning:,1
v0.10.1,TODO(travis): fix prefix tuning and p-tuning to work with DDP,1
v0.10.1,TODO: <Alex>02/21/2024: Disabling AdaptionPrompt (waiting for PEFT release to fix,1
v0.10.1,TODO(This needs to be defined based on the Constraint class),1
v0.10.1,TODO: This is an unfortunate side-effect of dataclass init order - you cannot have non-default fields follow,1
v0.10.1,todo v0.4: currently not clear way to set model graph,1
v0.10.1,TODO(daniel): delete this.,1
v0.10.1,TODO(travis): figure out a good way to support this. The problem with,1
v0.10.1,"TODO(Justin): This should probably live in on_ludwig_end, once that's implemented.",1
v0.10.1,TODO: need to also include a filename for this figure,1
v0.10.1,"TODO(geoffrey, arnav): uncomment this when we have reconciled the config with the backend kwarg in api.py",1
v0.10.1,"TODO: `prompt` by default should be set to null, not a default dict:",1
v0.10.1,"TODO: retrieval by default should be set to null, not a default dict:",1
v0.10.1,TODO: len(template_refs) is a hacky attempt to check that there are references to *something* in the,1
v0.10.1,TODO(travis): figure out why we need these imports to avoid circular import error,1
v0.10.1,TODO: Replace with more robust required logic later.,1
v0.10.1,TODO(Arnav): Refactor to split into strategies like splitters,1
v0.10.1,"TODO(geoffrey): figure out where self.max_sequence_length is used– if not used, we might consider removing it.",1
v0.10.1,It's confusing to have both this and `max_new_tokens` as a mandatory param in the `forward` function.,1
v0.10.1,TODO(Arnav): Add support for probabilities and logits,1
v0.10.1,"TODO(Arnav): Figure out how to compute logits. For now, we return",1
v0.10.1,Dummy implementation.,1
v0.10.1,TODO: Use a configurable ratio for how often to use teacher forcing during training.,1
v0.10.1,TODO: Use a configurable ratio for how often to use teacher forcing during training.,1
v0.10.1,Dummy implementation.,1
v0.10.1,TODO(Arnav): Re-enable once we add DotProduct Combiner: https://github.com/ludwig-ai/ludwig/issues/3150,1
v0.10.1,"TODO(travis): we assume here that False is always the default, which may not be true. We should dervice",1
v0.10.1,TODO: Check if subscription has expired,1
v0.10.1,"TODO(travis): stopgap solution, we should make it so we don't need to do this",1
v0.10.1,todo (Wael): tests for all types.,1
v0.10.1,todo (Wael): tests for all types.,1
v0.10.1,Still needed for preprocessing  TODO(Connor): Refactor ludwig/data/preprocessing to use schema,1
v0.10.1,"TODO(travis): remove this, make type a protected string for each subclass",1
v0.10.1,"ORRIBLE HACK, IT'S THE ONLY WAY TO REMOVE PADDING",1
v0.10.1,END OF HORRIBLE HACK,1
v0.10.1,"TODO: more research needed on how to handle RayTune ""sample_from"" search space",1
v0.10.1,HACK(geoffrey): gpt2 has no pad token. Recommendation is to use eos token instead.,1
v0.10.1,TODO(geoffrey): can we better validate tokenizer parity before swapping in the TorchText tokenizer?,1
v0.10.1,TODO: improve this,1
v0.10.1,Workaround for: https://issues.apache.org/jira/browse/ARROW-5645,1
v0.10.1,TODO: make this configurable in the future. These parameters are from FastChat:,1
v0.10.1,"TODO: Wrap device_map=""auto"" in a try-except block since it may not be supported for all models (E.g. BertLMHead)  # noqa",1
v0.10.1,TODO(travis): move to cached_property when we drop Python 3.7.,1
v0.10.1,TODO: remove reshaping once https://github.com/microsoft/LightGBM/pull/4925 is released,1
v0.10.1,TODO: remove ravel once https://github.com/microsoft/LightGBM/pull/4925 is released,1
v0.10.1,TODO (jeffkinnison): revert to use the requested device once torch device usage is standardized,1
v0.10.1,TODO(shreya): Confirm types of args,1
v0.10.1,"TODO: when loading an existing model, this loses metric values for all but the last epoch.",1
v0.10.1,TODO(travis): implement imbalance ratio,1
v0.10.1,"TODO (ASN): add other modalities (image, etc. )",1
v0.10.1,"TODO(travis): this assumes ECD is the selected model type, which is not problematic for now, as",1
v0.10.1,"diverge, so we should find a way to remove this. The best solution is to the change the input params from",1
v0.10.1,TODO(travis): consolidate with implementation in data/ray.py,1
v0.10.1,TODO: only single task currently,1
v0.10.1,TODO(travis): include encoder and decoder steps during inference,1
v0.10.1,TODO(Arnav): This needs be more flexible to account for RoPE Scaling,1
v0.10.1,TODO: this implementation will not work if resuming from a previous checkpoint. Need to fix this.,1
v0.10.1,TODO: only single task currently,1
v0.10.1,TODO (jeffkinnison): Determine why the 8-bit `SCB` and `CB` matrices are deleted in the forward pass,1
v0.10.1,HACK (Tim): get the device of the targets to transfer self.eval_loss_metric to the same device,1
v0.10.1,"TODO(Arnav): Seems like doing this again and going between these format types in unnecessary, but",1
v0.10.1,"HACK(geoffrey): we need a non-empty loss, so we just fill it with zeros",1
v0.10.1,TODO(travis): this will need to change when we support multiple output features,1
v0.10.1,"TODO(travis): use the implementation of trainer itself to decide whether to save the model, to",1
v0.10.1,avoid this hack,1
v0.10.1,"TODO: see the ""TODO"" statement from ""LLM.save()"" in this module.",1
v0.10.1,TODO (jeffkinnison): revert to using the requested device for GBMs when device usage is fixed,1
v0.10.1,TODO: with change to misc_utils.set_random_seed() this may be redundant,1
v0.10.1,TODO(Justin): Check that the semantics of input_size align with what the combiner's output shape returns,1
v0.10.1,TODO(justin): Remove dummy implementation. Make input_shape and output_shape functions.,1
v0.10.1,TODO(geoffrey): figure out why self.index.search segfaults with larger batch sizes,1
v0.10.1,TODO(travis): support local feature importance,1
v0.10.1,TODO:,1
v0.10.1,TODO(travis): add back skip encoders at the end in finally. Shouldn't be an issue in most cases as we,1
v0.10.1,"TODO(travis): for force plots, need something similar to SHAP E[X]",1
v0.10.1,"TODO(travis): for force plots, need something similar to SHAP E[X]",1
v0.10.1,"TODO(travis): Consider changing to `if not torch.is_floating_point(t.dtype)` to simplify, then handle bool",1
v0.10.1,TODO(travis): pre-compute this during training from the full training dataset.,1
v0.10.1,"TODO(travis): for force plots, need something similar to SHAP E[X]",1
v0.10.1,"TODO(travis): for force plots, need something similar to SHAP E[X]",1
v0.10.1,"TODO: construct new datasets by running encoders (for text, image)",1
v0.10.1,TODO(shreya): Refactor preprocessing so that this can be moved upstream.,1
v0.10.1,TODO(Arnav): Add support for gradient checkpointing in the compiled model,1
v0.10.1,"For a full explanation of this 8-bit workaround, see https://github.com/ludwig-ai/ludwig/pull/3606",1
v0.10.1,TODO (jeffkinnison): Determine why `SCB` and `CB` are deleted from parameter state,1
v0.10.1,matrices are part of model state. This workaround is necessary because the matrices are,1
v0.10.1,"TODO: Implement batch size tuning for LLM, currently just returns the default batch size",1
v0.10.1,TODO: refactor this into an interface,1
v0.10.1,workaround type limitations of the underlying frameworks,1
v0.10.1,TODO(Arnav): Re-enable in Ray 2.3,1
v0.10.1,"TODO(travis): we should revisit the user format here, as it silently breaks situations",1
v0.10.1,"TODO ray: find an informed way to set the parallelism, in practice",1
v0.10.1,TODO: When this is implemented we also need to update the,1
v0.10.1,TODO(travis): consolidate with implementation in data/ray.py,1
v0.10.1,"This is needed because Daft only supports Dataframes, not Series",1
v0.10.1,TODO(ekl) deprecate this once read fusion is available.,1
v0.10.1,TODO(geoffrey): ensure this subclasses ray.data.datasource.Reader in ray 1.14,1
v0.10.1,TODO(ekl) deprecate this once read fusion is available.,1
v0.10.1,"doesn't work for Snappy, so we double-check ourselves.",1
v0.10.1,TODO(xwjiang): Remove this later.,1
v0.10.1,TODO(travis): need to fix checkpoint saving/loading for DeepSpeed to enable tuning,1
v0.10.1,"TODO(travis): this double-counts on the same device, it should use a cross-communicator instead",1
v0.10.1,"TODO: remove LOCAL_BACKEND as a global constant, replace with singleton LocalBackend.shared_instance().",1
v0.10.1,Temporary workaround to prevent tests from automatically using the Ray backend. Taken from,1
v0.10.1,TODO(Alex): Standardize load() signature as interface method in DatasetLoader and adhere to it in all subclasses.,1
v0.10.1,TODO(travis): add other indexing structures,1
v0.10.1,TODO(travis): open question if this is needed to ensure all workers using same weights,1
v0.10.1,TODO(travis): open question if this is needed to ensure all workers using same optimizer state,1
v0.10.1,"TODO(travis): currently Ray handles this for us, but is subject to hangs if one of the workers raises an",1
v0.10.1,TODO(geoffrey): Add a boolean arg to function to control load_module_strict behavior.,1
v0.10.0,HACK(geoffrey): `hyperopt_resources` is a required resource for hyperopt to prevent deadlocks in Ludwig tests.,1
v0.10.0,TODO(geoffrey): remove for Ray 2.2,1
v0.10.0,"HACK(Arnav): Remove configs that have LARS, LAMB or Lion optimizers, or Paged or 8-bit optimizers.",1
v0.10.0,is there a better way to do this?,1
v0.10.0,todo the hidden output is actually a tensor. May need modification,1
v0.10.0,todo figure out the output size for parallel 1d conv,1
v0.10.0,TODO:  Need variant of assert_model_parameters_updated() to account for the two step calling sequence,1
v0.10.0,todo: remove code,1
v0.10.0,TODO(ksbrar): Circle back after discussing whether additional properties should be allowed long-term.,1
v0.10.0,TODO(ksbrar): Circle back after discussing whether additional properties should be allowed long-term.,1
v0.10.0,TODO(Arnav): Re-enable once https://github.com/ludwig-ai/ludwig/issues/3150 is resolved since the GBM,1
v0.10.0,todo: re-add 'attention' after further research in implication of torch,1
v0.10.0,TODO(#1333): Refactor this test once torch sequence generator work is complete.,1
v0.10.0,"TODO(Justin): Move these to test_sequence_generator unit tests, and reintroduce decoder attention, beam_width, and",1
v0.10.0,TODO(travis): add when we support pretrained text models for gbms,1
v0.10.0,TODO: find a smaller model for testing,1
v0.10.0,TODO: figure out how to get mocks to work with Ray backend,1
v0.10.0,TODO(travis): We may want to expand upon this in the future to include some checks on model,1
v0.10.0,TODO(travis): We may want to expand upon this in the future to include some checks on model,1
v0.10.0,TODO(travis): We may want to expand upon this in the future to include some checks on model,1
v0.10.0,TODO: Determine whether this is desired behavior. Tracked here:,1
v0.10.0,TODO: feature type not yet supported,1
v0.10.0,handled non-determinism when comparing the metrics between the local and Ray backends. We work around this by,1
v0.10.0,TODO: feature type not yet supported,1
v0.10.0,TODO(geoffrey): Fold modin tests into test_ray_image as @pytest.mark.parametrized once tests are optimized,1
v0.10.0,TODO(travis): move this to separate gpu module so we only have one ray cluster running at a time,1
v0.10.0,TODO(travis): https://github.com/ludwig-ai/ludwig/issues/1956,1
v0.10.0,TODO: Determine if we still need this if-then-else construct,1
v0.10.0,TODO(travis): once we support GBM text features,1
v0.10.0,TODO(travis): once we support GBM text features,1
v0.10.0,TODO(travis): need unit tests to test the get_embedding_layer() of every encoder to ensure it is,1
v0.10.0,TODO: fix LLM model loading,1
v0.10.0,TODO(arnav): p-tuning and prefix tuning have errors when enabled that seem to stem from DDP:,1
v0.10.0,TODO(Arnav): Re-enable once we can run tests on GPUs,1
v0.10.0,TODO: <Alex>02/21/2024: Disabling AdaptionPrompt (waiting for PEFT release to fix,1
v0.10.0,TODO: Re-enable once we can run tests on GPUs,1
v0.10.0,TODO: <Alex>02/21/2024: Disabling AdaptionPrompt (waiting for PEFT release to fix,1
v0.10.0,"For a full explanation of this 8-bit workaround, see https://github.com/ludwig-ai/ludwig/pull/3606",1
v0.10.0,"TODO: Uncomment ""filter_for_weight_format()"" method definition and enable its usage once GPU tests are set up.",1
v0.10.0,TODO(shreya): Uncomment when https://github.com/ludwig-ai/ludwig/issues/2039 is fixed.,1
v0.10.0,TODO: remove this workaround when audio preprocessing is fixed.,1
v0.10.0,TODO: these are the only outputs we provide from Torchscript for now,1
v0.10.0,TODO all shadows built in name - come up with a more descriptive name,1
v0.10.0,TODO (Connor): Refactor to use self.config_obj,1
v0.10.0,TODO (ASN): add support for substitute_with_max parameter,1
v0.10.0,TODO(Justin): Decide if it's worth folding padding_side handling into llm.py's tokenizer initialization.,1
v0.10.0,TODO(travis): WIP,1
v0.10.0,"TODO(travis): there's a lot of redundancy in this approach, since we are preprocessing the same DataFrame",1
v0.10.0,"TODO(travis): can optimize the preprocessing part here, since we only need to preprocess / predict",1
v0.10.0,TODO(travis): generalize this to support any pandas output format,1
v0.10.0,TODO (Connor): Refactor to use self.config_obj,1
v0.10.0,"TODO: In the future, it may be possible to move up the model type check into the BaseModel class.",1
v0.10.0,TODO: fix for Ray where workers may be of different skus,1
v0.10.0,TODO(travis): remove in favor of on_hyperopt_end for naming consistency,1
v0.10.0,TODO: https://github.com/ludwig-ai/ludwig/issues/2633,1
v0.10.0,todo: revise docstring,1
v0.10.0,todo: assess how to specify padding for equivalent to 'same',1
v0.10.0,todo: determine how to pool_padding equivalent of 'same',1
v0.10.0,todo: fixup docstring,1
v0.10.0,todo: review docstring,1
v0.10.0,todo: fix up docstring,1
v0.10.0,todo: fix up docstring,1
v0.10.0,todo: update docstring as needed,1
v0.10.0,TODO(shreya): Make sure this is updated when FCStack is updated,1
v0.10.0,TODO(Arnav): Remove this once we have reduce_output options set for,1
v0.10.0,TODO(travis): get_hf_config_param_names should be implemented as abstract in HFEncoderConfig,1
v0.10.0,TODO(shreya): Confirm that this is it,1
v0.10.0,TODO(shreya): Confirm that this is it,1
v0.10.0,TODO(justin): This may need to be conditioned on which AutoModel gets chosen.,1
v0.10.0,TODO(Arnav): This needs be more flexible to account for RoPE Scaling,1
v0.10.0,TODO: this implementation will not work if resuming from a previous checkpoint. Need to fix this.,1
v0.10.0,TODO (ASN): Decide how we want to proceed if at least one trial has,1
v0.10.0,TODO(shreya): Should this hyperopt config param be set here?,1
v0.10.0,TODO (ASN): add image heuristics,1
v0.10.0,"TODO(travis): less hacky way to do this, we should probably allow ModelConfig to be created without output",1
v0.10.0,"todo future: this may be redundant, check",1
v0.10.0,Workaround for including additional tensors from output of input encoders for,1
v0.10.0,TODO(Justin): Think about how to make this communication work for multi-sequence,1
v0.10.0,todo: when https://github.com/ludwig-ai/ludwig/issues/810 is closed,1
v0.10.0,"todo: can we just use projector_size? # hidden_size,",1
v0.10.0,"todo future: this may be redundant, check",1
v0.10.0,"todo future: this may be redundant, check",1
v0.10.0,Workaround for including additional tensors from output of input encoders for,1
v0.10.0,TODO(Justin): Think about how to make this communication work for multi-sequence,1
v0.10.0,todo future: maybe reintroduce these attention function,1
v0.10.0,todo future: maybe reintroduce these attention function,1
v0.10.0,todo future: maybe reintroduce these attention function,1
v0.10.0,TODO(travis): consider moving this behind a general BatchNorm interface to avoid this kludge.,1
v0.10.0,"todo: enumerate for debugging, remove after testing",1
v0.10.0,TODO(shreya): Combine with ResNetBlock by adding a flag.,1
v0.10.0,TODO(shreya): Implement sparse embedding lookup.,1
v0.10.0,# TODO(shreya): Check if this is equivalent,1
v0.10.0,# TODO(shreya): Check if supported in torch,1
v0.10.0,todo: review for generality,1
v0.10.0,TODO: Simplify this.,1
v0.10.0,TODO(travis): make this more general to other cumulative loss functions,1
v0.10.0,Dummy implementation.,1
v0.10.0,"TODO(Justin): Add a confusion matrix, see",1
v0.10.0,TODO: add a mechanism for letting the user decide to save it,1
v0.10.0,todo: re-evaluate need for separate handling of `attention` reducer,1
v0.10.0,TODO(Justin): Clean this up.,1
v0.10.0,TODO: change to debug level before merging,1
v0.10.0,TODO: should this raise an exception if not in training mode?,1
v0.10.0,TODO(travis): do this through an interface rather than conditional logic,1
v0.10.0,Nested conditional is a workaround to short-circuit boolean evaluation.,1
v0.10.0,TODO: alternatively use get_average_image() for unreachable images,1
v0.10.0,todo future add multiprocessing/multithreading,1
v0.10.0,TODO(travis): do we even need a user param for vector size if we're going to auto-infer it in all,1
v0.10.0,TODO(1891): Remove backward compatibility hack once all models have been retrained with Ludwig after,1
v0.10.0,todo: refactor to reuse SequenceOutputFeature.postprocess_predictions,1
v0.10.0,TODO: Add a mechanism that lets the user save the full probability distribution if they want.,1
v0.10.0,TODO(geoffrey): add support for Dask DataFrames,1
v0.10.0,TODO(Arnav): see if there's a way to only remove them if the entry does't have quotes. This currently,1
v0.10.0,"removes all "" from the string (even those not added by json.dumps), which is not ideal.",1
v0.10.0,Convert datetime to int64 to workaround Dask limitation,1
v0.10.0,"TODO pyarrow: this is needed for caching to work with pyarrow. if removed, the following error is raised:",1
v0.10.0,TODO(travis): passing in MODEL_ECD is a hack here that can be removed once we move to using,1
v0.10.0,"encoder schema at all. This hack works for now because all encoders are supported by ECD, so",1
v0.10.0,todo figure out if additional parameters are needed,1
v0.10.0,"TODO(travis): instead of using raw dictionary, this should be loaded into a proper PreprocessingConfig",1
v0.10.0,TODO dask: this needs to work with DataFrames,1
v0.10.0,TODO(travis): decouple config from training_set_metadata so we don't need to,1
v0.10.0,TODO(joppe): support out of memory negative sampling using Dask,1
v0.10.0,TODO(travis): revisit in the future to make this more precise,1
v0.10.0,TODO: Add link to windowing docs.,1
v0.10.0,TODO: figure out correct typing for augmentation_pipeline after refactoring is done,1
v0.10.0,TODO: consider removing this. doesn't work currently and read performance seems generally,1
v0.10.0,TODO: convert to debug message when done with development,1
v0.10.0,"TODO(travis): could try hashing the in-memory dataset, but this is tricky for Dask",1
v0.10.0,todo future: reintroduce the bucketed batcher,1
v0.10.0,TODO ray: implement dynamic batch size,1
v0.10.0,TODO: Change annotation to PublicAPI once Ludwig 0.7 is released,1
v0.10.0,TODO(travis): is this redundant with `clipglobalnorm`?,1
v0.10.0,TODO(travis) consider removing this in the future after deprecation period,1
v0.10.0,TODO: use registry pattern for trainers,1
v0.10.0,TODO: Change to RAISE and update descriptions once we want to enforce strict schemas.,1
v0.10.0,TODO: Maybe need to plumb 'required' through here,1
v0.10.0,"TODO(travis): this seems much too verbose, does the validation error not show the specific error?",1
v0.10.0,"TODO(travis): too much boilerplate here, we should find a way to abstract all this and only require specifying the",1
v0.10.0,"TODO(travis): this seems much too verbose, does the validation error not show the specific error?",1
v0.10.0,TODO(#1673): Need some more logic here for validating against output features,1
v0.10.0,"TODO: Re-enable ""goss"" when supported: https://github.com/ludwig-ai/ludwig/issues/2988",1
v0.10.0,TODO(travis): figure out why we need these imports to avoid circular import error,1
v0.10.0,"TODO (Connor): Add nesting logic for fc_layers, see fully_connected_module.py",1
v0.10.0,"TODO (Connor): Add nesting logic for fc_layers, see fully_connected_module.py",1
v0.10.0,TODO(travis): seems like this is not really a valid user option. We should probably just remove these,1
v0.10.0,TODO: uncomment when sentencepiece doesn't cause segfaults: https://github.com/ludwig-ai/ludwig/issues/2983,1
v0.10.0,TODO: uncomment once we figure out host memory issue: https://github.com/ludwig-ai/ludwig/issues/3107,1
v0.10.0,TODO: uncomment when CTRL bug (https://github.com/ludwig-ai/ludwig/issues/2977) has been fixed to add back in,1
v0.10.0,TODO(#1673): Add conditional logic for fields like this one:,1
v0.10.0,"TODO(travis): below type comparison is not perfect, as it doesn't consider the case where the default type",1
v0.10.0,"TODO(travis): explore similar contraints for GBMs, which don't have epochs",1
v0.10.0,HACK(Arnav): Set Mixtral target modules when using LoRA,1
v0.10.0,HACK(Arnav): Set Phi-2 target modules when using LoRA,1
v0.10.0,HACK(Arnav): Set Gemma target modules when using LoRA,1
v0.10.0,"This is not perfect since it includes tokens from both input + output features, but this at least",1
v0.10.0,TODO (Arnav): Figure out how to factor in rope scaling factor into this calculation.,1
v0.10.0,TODO: Add better support for category output features,1
v0.10.0,TODO(travis): handle this with helper function,1
v0.10.0,TODO(travis): not needed once we remove existing model config implementation,1
v0.10.0,TODO(ksbrar): What is this?,1
v0.10.0,"TODO(travis): this should be done through marshmallow dataclass' `required` field param,",1
v0.10.0,TODO(Arnav): Remove the hard check on max_length once we support multiple output features.,1
v0.10.0,TODO(Arnav): Refactor LossDataclassField to only accept loss types that are valid for the model,1
v0.10.0,TODO: Add schema support for Callable,1
v0.10.0,TODO: This should technically be a required paremeter. Do we need to add support for required params?,1
v0.10.0,TODO: Double-check support for this,1
v0.10.0,TODO: Double-check support for this as well as whether Callable args work properly,1
v0.10.0,TODO: create a search alg metadata class to register in place of individual metadata args,1
v0.10.0,TODO: Add a registry mapping string names to nevergrad optimizers,1
v0.10.0,TODO: Add schemas for nevergrad optimizer kwargs,1
v0.10.0,TODO: Add a registry of Optuna samplers schemas,1
v0.10.0,TODO(travis): figure out why calling this `bias` doesn't work,1
v0.10.0,TODO(travis): fix text generation when using prompt tuning:,1
v0.10.0,TODO(travis): fix prefix tuning and p-tuning to work with DDP,1
v0.10.0,TODO: <Alex>02/21/2024: Disabling AdaptionPrompt (waiting for PEFT release to fix,1
v0.10.0,TODO(This needs to be defined based on the Constraint class),1
v0.10.0,TODO: This is an unfortunate side-effect of dataclass init order - you cannot have non-default fields follow,1
v0.10.0,todo v0.4: currently not clear way to set model graph,1
v0.10.0,TODO(daniel): delete this.,1
v0.10.0,TODO(travis): figure out a good way to support this. The problem with,1
v0.10.0,"TODO(Justin): This should probably live in on_ludwig_end, once that's implemented.",1
v0.10.0,TODO: need to also include a filename for this figure,1
v0.10.0,"TODO(geoffrey, arnav): uncomment this when we have reconciled the config with the backend kwarg in api.py",1
v0.10.0,"TODO: `prompt` by default should be set to null, not a default dict:",1
v0.10.0,"TODO: retrieval by default should be set to null, not a default dict:",1
v0.10.0,TODO: len(template_refs) is a hacky attempt to check that there are references to *something* in the,1
v0.10.0,TODO(travis): figure out why we need these imports to avoid circular import error,1
v0.10.0,TODO: Replace with more robust required logic later.,1
v0.10.0,TODO(Arnav): Refactor to split into strategies like splitters,1
v0.10.0,"TODO(geoffrey): figure out where self.max_sequence_length is used– if not used, we might consider removing it.",1
v0.10.0,It's confusing to have both this and `max_new_tokens` as a mandatory param in the `forward` function.,1
v0.10.0,TODO(Arnav): Add support for probabilities and logits,1
v0.10.0,"TODO(Arnav): Figure out how to compute logits. For now, we return",1
v0.10.0,Dummy implementation.,1
v0.10.0,TODO: Use a configurable ratio for how often to use teacher forcing during training.,1
v0.10.0,TODO: Use a configurable ratio for how often to use teacher forcing during training.,1
v0.10.0,Dummy implementation.,1
v0.10.0,TODO(Arnav): Re-enable once we add DotProduct Combiner: https://github.com/ludwig-ai/ludwig/issues/3150,1
v0.10.0,"TODO(travis): we assume here that False is always the default, which may not be true. We should dervice",1
v0.10.0,TODO: Check if subscription has expired,1
v0.10.0,"TODO(travis): stopgap solution, we should make it so we don't need to do this",1
v0.10.0,todo (Wael): tests for all types.,1
v0.10.0,todo (Wael): tests for all types.,1
v0.10.0,Still needed for preprocessing  TODO(Connor): Refactor ludwig/data/preprocessing to use schema,1
v0.10.0,"TODO(travis): remove this, make type a protected string for each subclass",1
v0.10.0,"ORRIBLE HACK, IT'S THE ONLY WAY TO REMOVE PADDING",1
v0.10.0,END OF HORRIBLE HACK,1
v0.10.0,"TODO: more research needed on how to handle RayTune ""sample_from"" search space",1
v0.10.0,HACK(geoffrey): gpt2 has no pad token. Recommendation is to use eos token instead.,1
v0.10.0,TODO(geoffrey): can we better validate tokenizer parity before swapping in the TorchText tokenizer?,1
v0.10.0,TODO: improve this,1
v0.10.0,Workaround for: https://issues.apache.org/jira/browse/ARROW-5645,1
v0.10.0,TODO: make this configurable in the future. These parameters are from FastChat:,1
v0.10.0,"TODO: Wrap device_map=""auto"" in a try-except block since it may not be supported for all models (E.g. BertLMHead)  # noqa",1
v0.10.0,TODO(travis): move to cached_property when we drop Python 3.7.,1
v0.10.0,TODO: remove reshaping once https://github.com/microsoft/LightGBM/pull/4925 is released,1
v0.10.0,TODO: remove ravel once https://github.com/microsoft/LightGBM/pull/4925 is released,1
v0.10.0,TODO (jeffkinnison): revert to use the requested device once torch device usage is standardized,1
v0.10.0,TODO(shreya): Confirm types of args,1
v0.10.0,"TODO: when loading an existing model, this loses metric values for all but the last epoch.",1
v0.10.0,TODO(travis): implement imbalance ratio,1
v0.10.0,"TODO (ASN): add other modalities (image, etc. )",1
v0.10.0,"TODO(travis): this assumes ECD is the selected model type, which is not problematic for now, as",1
v0.10.0,"diverge, so we should find a way to remove this. The best solution is to the change the input params from",1
v0.10.0,TODO(travis): consolidate with implementation in data/ray.py,1
v0.10.0,TODO: only single task currently,1
v0.10.0,TODO(travis): include encoder and decoder steps during inference,1
v0.10.0,TODO(Arnav): This needs be more flexible to account for RoPE Scaling,1
v0.10.0,TODO: this implementation will not work if resuming from a previous checkpoint. Need to fix this.,1
v0.10.0,TODO: only single task currently,1
v0.10.0,TODO (jeffkinnison): Determine why the 8-bit `SCB` and `CB` matrices are deleted in the forward pass,1
v0.10.0,HACK (Tim): get the device of the targets to transfer self.eval_loss_metric to the same device,1
v0.10.0,"TODO(Arnav): Seems like doing this again and going between these format types in unnecessary, but",1
v0.10.0,"HACK(geoffrey): we need a non-empty loss, so we just fill it with zeros",1
v0.10.0,TODO(travis): this will need to change when we support multiple output features,1
v0.10.0,"TODO(travis): use the implementation of trainer itself to decide whether to save the model, to",1
v0.10.0,avoid this hack,1
v0.10.0,"TODO: see the ""TODO"" statement from ""LLM.save()"" in this module.",1
v0.10.0,TODO (jeffkinnison): revert to using the requested device for GBMs when device usage is fixed,1
v0.10.0,TODO: with change to misc_utils.set_random_seed() this may be redundant,1
v0.10.0,TODO(Justin): Check that the semantics of input_size align with what the combiner's output shape returns,1
v0.10.0,TODO(justin): Remove dummy implementation. Make input_shape and output_shape functions.,1
v0.10.0,TODO(geoffrey): figure out why self.index.search segfaults with larger batch sizes,1
v0.10.0,TODO(travis): support local feature importance,1
v0.10.0,TODO:,1
v0.10.0,TODO(travis): add back skip encoders at the end in finally. Shouldn't be an issue in most cases as we,1
v0.10.0,"TODO(travis): for force plots, need something similar to SHAP E[X]",1
v0.10.0,"TODO(travis): for force plots, need something similar to SHAP E[X]",1
v0.10.0,"TODO(travis): Consider changing to `if not torch.is_floating_point(t.dtype)` to simplify, then handle bool",1
v0.10.0,TODO(travis): pre-compute this during training from the full training dataset.,1
v0.10.0,"TODO(travis): for force plots, need something similar to SHAP E[X]",1
v0.10.0,"TODO(travis): for force plots, need something similar to SHAP E[X]",1
v0.10.0,"TODO: construct new datasets by running encoders (for text, image)",1
v0.10.0,TODO(shreya): Refactor preprocessing so that this can be moved upstream.,1
v0.10.0,TODO(Arnav): Add support for gradient checkpointing in the compiled model,1
v0.10.0,"For a full explanation of this 8-bit workaround, see https://github.com/ludwig-ai/ludwig/pull/3606",1
v0.10.0,TODO (jeffkinnison): Determine why `SCB` and `CB` are deleted from parameter state,1
v0.10.0,matrices are part of model state. This workaround is necessary because the matrices are,1
v0.10.0,"TODO: Implement batch size tuning for LLM, currently just returns the default batch size",1
v0.10.0,TODO: refactor this into an interface,1
v0.10.0,workaround type limitations of the underlying frameworks,1
v0.10.0,TODO(Arnav): Re-enable in Ray 2.3,1
v0.10.0,"TODO(travis): we should revisit the user format here, as it silently breaks situations",1
v0.10.0,"TODO ray: find an informed way to set the parallelism, in practice",1
v0.10.0,TODO: When this is implemented we also need to update the,1
v0.10.0,TODO(travis): consolidate with implementation in data/ray.py,1
v0.10.0,"This is needed because Daft only supports Dataframes, not Series",1
v0.10.0,TODO(ekl) deprecate this once read fusion is available.,1
v0.10.0,TODO(geoffrey): ensure this subclasses ray.data.datasource.Reader in ray 1.14,1
v0.10.0,TODO(ekl) deprecate this once read fusion is available.,1
v0.10.0,"doesn't work for Snappy, so we double-check ourselves.",1
v0.10.0,TODO(xwjiang): Remove this later.,1
v0.10.0,TODO(travis): need to fix checkpoint saving/loading for DeepSpeed to enable tuning,1
v0.10.0,"TODO(travis): this double-counts on the same device, it should use a cross-communicator instead",1
v0.10.0,"TODO: remove LOCAL_BACKEND as a global constant, replace with singleton LocalBackend.shared_instance().",1
v0.10.0,Temporary workaround to prevent tests from automatically using the Ray backend. Taken from,1
v0.10.0,TODO(Alex): Standardize load() signature as interface method in DatasetLoader and adhere to it in all subclasses.,1
v0.10.0,TODO(travis): add other indexing structures,1
v0.10.0,TODO(travis): open question if this is needed to ensure all workers using same weights,1
v0.10.0,TODO(travis): open question if this is needed to ensure all workers using same optimizer state,1
v0.10.0,"TODO(travis): currently Ray handles this for us, but is subject to hangs if one of the workers raises an",1
v0.10.0,TODO(geoffrey): Add a boolean arg to function to control load_module_strict behavior.,1
v0.9.3,HACK(geoffrey): `hyperopt_resources` is a required resource for hyperopt to prevent deadlocks in Ludwig tests.,1
v0.9.3,TODO(geoffrey): remove for Ray 2.2,1
v0.9.3,"HACK(Arnav): Remove configs that have LARS, LAMB or Lion optimizers, or Paged or 8-bit optimizers.",1
v0.9.3,is there a better way to do this?,1
v0.9.3,todo the hidden output is actually a tensor. May need modification,1
v0.9.3,todo figure out the output size for parallel 1d conv,1
v0.9.3,TODO:  Need variant of assert_model_parameters_updated() to account for the two step calling sequence,1
v0.9.3,todo: remove code,1
v0.9.3,TODO(ksbrar): Circle back after discussing whether additional properties should be allowed long-term.,1
v0.9.3,TODO(ksbrar): Circle back after discussing whether additional properties should be allowed long-term.,1
v0.9.3,TODO(Arnav): Re-enable once https://github.com/ludwig-ai/ludwig/issues/3150 is resolved since the GBM,1
v0.9.3,todo: re-add 'attention' after further research in implication of torch,1
v0.9.3,TODO(#1333): Refactor this test once torch sequence generator work is complete.,1
v0.9.3,"TODO(Justin): Move these to test_sequence_generator unit tests, and reintroduce decoder attention, beam_width, and",1
v0.9.3,TODO(travis): add when we support pretrained text models for gbms,1
v0.9.3,TODO: find a smaller model for testing,1
v0.9.3,TODO: figure out how to get mocks to work with Ray backend,1
v0.9.3,TODO(travis): We may want to expand upon this in the future to include some checks on model,1
v0.9.3,TODO(travis): We may want to expand upon this in the future to include some checks on model,1
v0.9.3,TODO(travis): We may want to expand upon this in the future to include some checks on model,1
v0.9.3,TODO: Determine whether this is desired behavior. Tracked here:,1
v0.9.3,TODO: feature type not yet supported,1
v0.9.3,handled non-determinism when comparing the metrics between the local and Ray backends. We work around this by,1
v0.9.3,TODO: feature type not yet supported,1
v0.9.3,TODO(geoffrey): Fold modin tests into test_ray_image as @pytest.mark.parametrized once tests are optimized,1
v0.9.3,TODO(travis): move this to separate gpu module so we only have one ray cluster running at a time,1
v0.9.3,TODO(travis): https://github.com/ludwig-ai/ludwig/issues/1956,1
v0.9.3,TODO: Determine if we still need this if-then-else construct,1
v0.9.3,TODO(travis): once we support GBM text features,1
v0.9.3,TODO(travis): once we support GBM text features,1
v0.9.3,TODO(travis): need unit tests to test the get_embedding_layer() of every encoder to ensure it is,1
v0.9.3,TODO: fix LLM model loading,1
v0.9.3,TODO(arnav): p-tuning and prefix tuning have errors when enabled that seem to stem from DDP:,1
v0.9.3,TODO(Arnav): Re-enable once we can run tests on GPUs,1
v0.9.3,TODO: Re-enable once we can run tests on GPUs,1
v0.9.3,"For a full explanation of this 8-bit workaround, see https://github.com/ludwig-ai/ludwig/pull/3606",1
v0.9.3,"TODO: Uncomment ""filter_for_weight_format()"" method definition and enable its usage once GPU tests are set up.",1
v0.9.3,TODO(shreya): Uncomment when https://github.com/ludwig-ai/ludwig/issues/2039 is fixed.,1
v0.9.3,TODO: remove this workaround when audio preprocessing is fixed.,1
v0.9.3,TODO: these are the only outputs we provide from Torchscript for now,1
v0.9.3,TODO all shadows built in name - come up with a more descriptive name,1
v0.9.3,TODO (Connor): Refactor to use self.config_obj,1
v0.9.3,TODO (ASN): add support for substitute_with_max parameter,1
v0.9.3,TODO(Justin): Decide if it's worth folding padding_side handling into llm.py's tokenizer initialization.,1
v0.9.3,TODO(travis): WIP,1
v0.9.3,"TODO(travis): there's a lot of redundancy in this approach, since we are preprocessing the same DataFrame",1
v0.9.3,"TODO(travis): can optimize the preprocessing part here, since we only need to preprocess / predict",1
v0.9.3,TODO(travis): generalize this to support any pandas output format,1
v0.9.3,TODO (Connor): Refactor to use self.config_obj,1
v0.9.3,"TODO: In the future, it may be possible to move up the model type check into the BaseModel class.",1
v0.9.3,TODO: fix for Ray where workers may be of different skus,1
v0.9.3,TODO(travis): remove in favor of on_hyperopt_end for naming consistency,1
v0.9.3,TODO: https://github.com/ludwig-ai/ludwig/issues/2633,1
v0.9.3,todo: revise docstring,1
v0.9.3,todo: assess how to specify padding for equivalent to 'same',1
v0.9.3,todo: determine how to pool_padding equivalent of 'same',1
v0.9.3,todo: fixup docstring,1
v0.9.3,todo: review docstring,1
v0.9.3,todo: fix up docstring,1
v0.9.3,todo: fix up docstring,1
v0.9.3,todo: update docstring as needed,1
v0.9.3,TODO(shreya): Make sure this is updated when FCStack is updated,1
v0.9.3,TODO(Arnav): Remove this once we have reduce_output options set for,1
v0.9.3,TODO(travis): get_hf_config_param_names should be implemented as abstract in HFEncoderConfig,1
v0.9.3,TODO(shreya): Confirm that this is it,1
v0.9.3,TODO(shreya): Confirm that this is it,1
v0.9.3,TODO(justin): This may need to be conditioned on which AutoModel gets chosen.,1
v0.9.3,TODO(Arnav): This needs be more flexible to account for RoPE Scaling,1
v0.9.3,TODO: this implementation will not work if resuming from a previous checkpoint. Need to fix this.,1
v0.9.3,TODO (ASN): Decide how we want to proceed if at least one trial has,1
v0.9.3,TODO(shreya): Should this hyperopt config param be set here?,1
v0.9.3,TODO (ASN): add image heuristics,1
v0.9.3,"TODO(travis): less hacky way to do this, we should probably allow ModelConfig to be created without output",1
v0.9.3,"todo future: this may be redundant, check",1
v0.9.3,Workaround for including additional tensors from output of input encoders for,1
v0.9.3,TODO(Justin): Think about how to make this communication work for multi-sequence,1
v0.9.3,todo: when https://github.com/ludwig-ai/ludwig/issues/810 is closed,1
v0.9.3,"todo: can we just use projector_size? # hidden_size,",1
v0.9.3,"todo future: this may be redundant, check",1
v0.9.3,"todo future: this may be redundant, check",1
v0.9.3,Workaround for including additional tensors from output of input encoders for,1
v0.9.3,TODO(Justin): Think about how to make this communication work for multi-sequence,1
v0.9.3,todo future: maybe reintroduce these attention function,1
v0.9.3,todo future: maybe reintroduce these attention function,1
v0.9.3,todo future: maybe reintroduce these attention function,1
v0.9.3,TODO(travis): consider moving this behind a general BatchNorm interface to avoid this kludge.,1
v0.9.3,"todo: enumerate for debugging, remove after testing",1
v0.9.3,TODO(shreya): Combine with ResNetBlock by adding a flag.,1
v0.9.3,TODO(shreya): Implement sparse embedding lookup.,1
v0.9.3,# TODO(shreya): Check if this is equivalent,1
v0.9.3,# TODO(shreya): Check if supported in torch,1
v0.9.3,todo: review for generality,1
v0.9.3,TODO: Simplify this.,1
v0.9.3,TODO(travis): make this more general to other cumulative loss functions,1
v0.9.3,Dummy implementation.,1
v0.9.3,"TODO(Justin): Add a confusion matrix, see",1
v0.9.3,TODO: add a mechanism for letting the user decide to save it,1
v0.9.3,todo: re-evaluate need for separate handling of `attention` reducer,1
v0.9.3,TODO(Justin): Clean this up.,1
v0.9.3,TODO: change to debug level before merging,1
v0.9.3,TODO: should this raise an exception if not in training mode?,1
v0.9.3,Nested conditional is a workaround to short-circuit boolean evaluation.,1
v0.9.3,TODO: alternatively use get_average_image() for unreachable images,1
v0.9.3,todo future add multiprocessing/multithreading,1
v0.9.3,TODO(travis): do we even need a user param for vector size if we're going to auto-infer it in all,1
v0.9.3,TODO(1891): Remove backward compatibility hack once all models have been retrained with Ludwig after,1
v0.9.3,todo: refactor to reuse SequenceOutputFeature.postprocess_predictions,1
v0.9.3,TODO: Add a mechanism that lets the user save the full probability distribution if they want.,1
v0.9.3,TODO(geoffrey): add support for Dask DataFrames,1
v0.9.3,TODO(Arnav): see if there's a way to only remove them if the entry does't have quotes. This currently,1
v0.9.3,"removes all "" from the string (even those not added by json.dumps), which is not ideal.",1
v0.9.3,Convert datetime to int64 to workaround Dask limitation,1
v0.9.3,"TODO pyarrow: this is needed for caching to work with pyarrow. if removed, the following error is raised:",1
v0.9.3,TODO(travis): passing in MODEL_ECD is a hack here that can be removed once we move to using,1
v0.9.3,"encoder schema at all. This hack works for now because all encoders are supported by ECD, so",1
v0.9.3,todo figure out if additional parameters are needed,1
v0.9.3,"TODO(travis): instead of using raw dictionary, this should be loaded into a proper PreprocessingConfig",1
v0.9.3,TODO dask: this needs to work with DataFrames,1
v0.9.3,TODO(travis): decouple config from training_set_metadata so we don't need to,1
v0.9.3,TODO(joppe): support out of memory negative sampling using Dask,1
v0.9.3,TODO(travis): revisit in the future to make this more precise,1
v0.9.3,TODO: Add link to windowing docs.,1
v0.9.3,TODO: figure out correct typing for augmentation_pipeline after refactoring is done,1
v0.9.3,TODO: consider removing this. doesn't work currently and read performance seems generally,1
v0.9.3,TODO: convert to debug message when done with development,1
v0.9.3,"TODO(travis): could try hashing the in-memory dataset, but this is tricky for Dask",1
v0.9.3,todo future: reintroduce the bucketed batcher,1
v0.9.3,TODO ray: implement dynamic batch size,1
v0.9.3,TODO: Change annotation to PublicAPI once Ludwig 0.7 is released,1
v0.9.3,TODO(travis): is this redundant with `clipglobalnorm`?,1
v0.9.3,TODO(travis) consider removing this in the future after deprecation period,1
v0.9.3,TODO: use registry pattern for trainers,1
v0.9.3,TODO: Change to RAISE and update descriptions once we want to enforce strict schemas.,1
v0.9.3,TODO: Maybe need to plumb 'required' through here,1
v0.9.3,"TODO(travis): this seems much too verbose, does the validation error not show the specific error?",1
v0.9.3,"TODO(travis): too much boilerplate here, we should find a way to abstract all this and only require specifying the",1
v0.9.3,"TODO(travis): this seems much too verbose, does the validation error not show the specific error?",1
v0.9.3,TODO(#1673): Need some more logic here for validating against output features,1
v0.9.3,"TODO: Re-enable ""goss"" when supported: https://github.com/ludwig-ai/ludwig/issues/2988",1
v0.9.3,TODO(travis): figure out why we need these imports to avoid circular import error,1
v0.9.3,"TODO (Connor): Add nesting logic for fc_layers, see fully_connected_module.py",1
v0.9.3,"TODO (Connor): Add nesting logic for fc_layers, see fully_connected_module.py",1
v0.9.3,TODO(travis): seems like this is not really a valid user option. We should probably just remove these,1
v0.9.3,TODO: uncomment when sentencepiece doesn't cause segfaults: https://github.com/ludwig-ai/ludwig/issues/2983,1
v0.9.3,TODO: uncomment once we figure out host memory issue: https://github.com/ludwig-ai/ludwig/issues/3107,1
v0.9.3,TODO: uncomment when CTRL bug (https://github.com/ludwig-ai/ludwig/issues/2977) has been fixed to add back in,1
v0.9.3,TODO(#1673): Add conditional logic for fields like this one:,1
v0.9.3,"TODO(travis): below type comparison is not perfect, as it doesn't consider the case where the default type",1
v0.9.3,"TODO(travis): explore similar contraints for GBMs, which don't have epochs",1
v0.9.3,HACK(Arnav): Set Mixtral target modules when using LoRA,1
v0.9.3,"This is not perfect since it includes tokens from both input + output features, but this at least",1
v0.9.3,TODO (Arnav): Figure out how to factor in rope scaling factor into this calculation.,1
v0.9.3,TODO: Add better support for category output features,1
v0.9.3,TODO(travis): handle this with helper function,1
v0.9.3,TODO(travis): not needed once we remove existing model config implementation,1
v0.9.3,TODO(ksbrar): What is this?,1
v0.9.3,"TODO(travis): this should be done through marshmallow dataclass' `required` field param,",1
v0.9.3,TODO(Arnav): Remove the hard check on max_length once we support multiple output features.,1
v0.9.3,TODO(Arnav): Refactor LossDataclassField to only accept loss types that are valid for the model,1
v0.9.3,TODO: Add schema support for Callable,1
v0.9.3,TODO: This should technically be a required paremeter. Do we need to add support for required params?,1
v0.9.3,TODO: Double-check support for this,1
v0.9.3,TODO: Double-check support for this as well as whether Callable args work properly,1
v0.9.3,TODO: create a search alg metadata class to register in place of individual metadata args,1
v0.9.3,TODO: Add a registry mapping string names to nevergrad optimizers,1
v0.9.3,TODO: Add schemas for nevergrad optimizer kwargs,1
v0.9.3,TODO: Add a registry of Optuna samplers schemas,1
v0.9.3,TODO(travis): figure out why calling this `bias` doesn't work,1
v0.9.3,TODO(travis): fix text generation when using prompt tuning:,1
v0.9.3,TODO(travis): fix prefix tuning and p-tuning to work with DDP,1
v0.9.3,TODO(This needs to be defined based on the Constraint class),1
v0.9.3,TODO: This is an unfortunate side-effect of dataclass init order - you cannot have non-default fields follow,1
v0.9.3,todo v0.4: currently not clear way to set model graph,1
v0.9.3,TODO(daniel): delete this.,1
v0.9.3,TODO(travis): figure out a good way to support this. The problem with,1
v0.9.3,"TODO(Justin): This should probably live in on_ludwig_end, once that's implemented.",1
v0.9.3,TODO: need to also include a filename for this figure,1
v0.9.3,"TODO(geoffrey, arnav): uncomment this when we have reconciled the config with the backend kwarg in api.py",1
v0.9.3,"TODO: `prompt` by default should be set to null, not a default dict:",1
v0.9.3,"TODO: retrieval by default should be set to null, not a default dict:",1
v0.9.3,TODO: len(template_refs) is a hacky attempt to check that there are references to *something* in the,1
v0.9.3,TODO(travis): figure out why we need these imports to avoid circular import error,1
v0.9.3,TODO: Replace with more robust required logic later.,1
v0.9.3,TODO(Arnav): Refactor to split into strategies like splitters,1
v0.9.3,"TODO(geoffrey): figure out where self.max_sequence_length is used– if not used, we might consider removing it.",1
v0.9.3,It's confusing to have both this and `max_new_tokens` as a mandatory param in the `forward` function.,1
v0.9.3,TODO(Arnav): Add support for probabilities and logits,1
v0.9.3,"TODO(Arnav): Figure out how to compute logits. For now, we return",1
v0.9.3,Dummy implementation.,1
v0.9.3,TODO: Use a configurable ratio for how often to use teacher forcing during training.,1
v0.9.3,TODO: Use a configurable ratio for how often to use teacher forcing during training.,1
v0.9.3,Dummy implementation.,1
v0.9.3,TODO(Arnav): Re-enable once we add DotProduct Combiner: https://github.com/ludwig-ai/ludwig/issues/3150,1
v0.9.3,"TODO(travis): we assume here that False is always the default, which may not be true. We should dervice",1
v0.9.3,TODO: Check if subscription has expired,1
v0.9.3,"TODO(travis): stopgap solution, we should make it so we don't need to do this",1
v0.9.3,todo (Wael): tests for all types.,1
v0.9.3,todo (Wael): tests for all types.,1
v0.9.3,Still needed for preprocessing  TODO(Connor): Refactor ludwig/data/preprocessing to use schema,1
v0.9.3,"TODO(travis): remove this, make type a protected string for each subclass",1
v0.9.3,"ORRIBLE HACK, IT'S THE ONLY WAY TO REMOVE PADDING",1
v0.9.3,END OF HORRIBLE HACK,1
v0.9.3,"TODO: more research needed on how to handle RayTune ""sample_from"" search space",1
v0.9.3,HACK(geoffrey): gpt2 has no pad token. Recommendation is to use eos token instead.,1
v0.9.3,TODO(geoffrey): can we better validate tokenizer parity before swapping in the TorchText tokenizer?,1
v0.9.3,TODO: improve this,1
v0.9.3,Workaround for: https://issues.apache.org/jira/browse/ARROW-5645,1
v0.9.3,TODO: make this configurable in the future. These parameters are from FastChat:,1
v0.9.3,"TODO: Wrap device_map=""auto"" in a try-except block since it may not be supported for all models (E.g. BertLMHead)  # noqa",1
v0.9.3,TODO(travis): do this through an interface rather than conditional logic,1
v0.9.3,TODO(travis): move to cached_property when we drop Python 3.7.,1
v0.9.3,TODO: remove reshaping once https://github.com/microsoft/LightGBM/pull/4925 is released,1
v0.9.3,TODO: remove ravel once https://github.com/microsoft/LightGBM/pull/4925 is released,1
v0.9.3,TODO (jeffkinnison): revert to use the requested device once torch device usage is standardized,1
v0.9.3,TODO(shreya): Confirm types of args,1
v0.9.3,"TODO: when loading an existing model, this loses metric values for all but the last epoch.",1
v0.9.3,TODO(travis): implement imbalance ratio,1
v0.9.3,"TODO (ASN): add other modalities (image, etc. )",1
v0.9.3,"TODO(travis): this assumes ECD is the selected model type, which is not problematic for now, as",1
v0.9.3,"diverge, so we should find a way to remove this. The best solution is to the change the input params from",1
v0.9.3,TODO(travis): consolidate with implementation in data/ray.py,1
v0.9.3,TODO: only single task currently,1
v0.9.3,TODO(travis): include encoder and decoder steps during inference,1
v0.9.3,TODO(Arnav): This needs be more flexible to account for RoPE Scaling,1
v0.9.3,TODO: this implementation will not work if resuming from a previous checkpoint. Need to fix this.,1
v0.9.3,TODO: only single task currently,1
v0.9.3,TODO (jeffkinnison): Determine why the 8-bit `SCB` and `CB` matrices are deleted in the forward pass,1
v0.9.3,HACK (Tim): get the device of the targets to transfer self.eval_loss_metric to the same device,1
v0.9.3,"TODO(Arnav): Seems like doing this again and going between these format types in unnecessary, but",1
v0.9.3,"HACK(geoffrey): we need a non-empty loss, so we just fill it with zeros",1
v0.9.3,TODO(travis): this will need to change when we support multiple output features,1
v0.9.3,"TODO(travis): use the implementation of trainer itself to decide whether to save the model, to",1
v0.9.3,avoid this hack,1
v0.9.3,"TODO: see the ""TODO"" statement from ""LLM.save()"" in this module.",1
v0.9.3,TODO (jeffkinnison): revert to using the requested device for GBMs when device usage is fixed,1
v0.9.3,TODO: with change to misc_utils.set_random_seed() this may be redundant,1
v0.9.3,TODO(Justin): Check that the semantics of input_size align with what the combiner's output shape returns,1
v0.9.3,TODO(justin): Remove dummy implementation. Make input_shape and output_shape functions.,1
v0.9.3,TODO(geoffrey): figure out why self.index.search segfaults with larger batch sizes,1
v0.9.3,TODO(travis): support local feature importance,1
v0.9.3,TODO:,1
v0.9.3,TODO(travis): add back skip encoders at the end in finally. Shouldn't be an issue in most cases as we,1
v0.9.3,"TODO(travis): for force plots, need something similar to SHAP E[X]",1
v0.9.3,"TODO(travis): for force plots, need something similar to SHAP E[X]",1
v0.9.3,"TODO(travis): Consider changing to `if not torch.is_floating_point(t.dtype)` to simplify, then handle bool",1
v0.9.3,TODO(travis): pre-compute this during training from the full training dataset.,1
v0.9.3,"TODO(travis): for force plots, need something similar to SHAP E[X]",1
v0.9.3,"TODO(travis): for force plots, need something similar to SHAP E[X]",1
v0.9.3,"TODO: construct new datasets by running encoders (for text, image)",1
v0.9.3,TODO(shreya): Refactor preprocessing so that this can be moved upstream.,1
v0.9.3,TODO(Arnav): Add support for gradient checkpointing in the compiled model,1
v0.9.3,"For a full explanation of this 8-bit workaround, see https://github.com/ludwig-ai/ludwig/pull/3606",1
v0.9.3,TODO (jeffkinnison): Determine why `SCB` and `CB` are deleted from parameter state,1
v0.9.3,matrices are part of model state. This workaround is necessary because the matrices are,1
v0.9.3,"TODO: Implement batch size tuning for LLM, currently just returns the default batch size",1
v0.9.3,TODO: refactor this into an interface,1
v0.9.3,workaround type limitations of the underlying frameworks,1
v0.9.3,TODO(Arnav): Re-enable in Ray 2.3,1
v0.9.3,"TODO(travis): we should revisit the user format here, as it silently breaks situations",1
v0.9.3,"TODO ray: find an informed way to set the parallelism, in practice",1
v0.9.3,TODO: When this is implemented we also need to update the,1
v0.9.3,TODO(travis): consolidate with implementation in data/ray.py,1
v0.9.3,"This is needed because Daft only supports Dataframes, not Series",1
v0.9.3,TODO(ekl) deprecate this once read fusion is available.,1
v0.9.3,TODO(geoffrey): ensure this subclasses ray.data.datasource.Reader in ray 1.14,1
v0.9.3,TODO(ekl) deprecate this once read fusion is available.,1
v0.9.3,"doesn't work for Snappy, so we double-check ourselves.",1
v0.9.3,TODO(xwjiang): Remove this later.,1
v0.9.3,TODO(travis): need to fix checkpoint saving/loading for DeepSpeed to enable tuning,1
v0.9.3,"TODO(travis): this double-counts on the same device, it should use a cross-communicator instead",1
v0.9.3,"TODO: remove LOCAL_BACKEND as a global constant, replace with singleton LocalBackend.shared_instance().",1
v0.9.3,Temporary workaround to prevent tests from automatically using the Ray backend. Taken from,1
v0.9.3,TODO(Alex): Standardize load() signature as interface method in DatasetLoader and adhere to it in all subclasses.,1
v0.9.3,TODO(travis): add other indexing structures,1
v0.9.3,TODO(travis): open question if this is needed to ensure all workers using same weights,1
v0.9.3,TODO(travis): open question if this is needed to ensure all workers using same optimizer state,1
v0.9.3,"TODO(travis): currently Ray handles this for us, but is subject to hangs if one of the workers raises an",1
v0.9.3,TODO(geoffrey): Add a boolean arg to function to control load_module_strict behavior.,1
v0.9.2,HACK(geoffrey): `hyperopt_resources` is a required resource for hyperopt to prevent deadlocks in Ludwig tests.,1
v0.9.2,TODO(geoffrey): remove for Ray 2.2,1
v0.9.2,"HACK(Arnav): Remove configs that have LARS, LAMB or Lion optimizers, or Paged or 8-bit optimizers.",1
v0.9.2,is there a better way to do this?,1
v0.9.2,todo the hidden output is actually a tensor. May need modification,1
v0.9.2,todo figure out the output size for parallel 1d conv,1
v0.9.2,TODO:  Need variant of assert_model_parameters_updated() to account for the two step calling sequence,1
v0.9.2,todo: remove code,1
v0.9.2,TODO(ksbrar): Circle back after discussing whether additional properties should be allowed long-term.,1
v0.9.2,TODO(ksbrar): Circle back after discussing whether additional properties should be allowed long-term.,1
v0.9.2,TODO(Arnav): Re-enable once https://github.com/ludwig-ai/ludwig/issues/3150 is resolved since the GBM,1
v0.9.2,todo: re-add 'attention' after further research in implication of torch,1
v0.9.2,TODO(#1333): Refactor this test once torch sequence generator work is complete.,1
v0.9.2,"TODO(Justin): Move these to test_sequence_generator unit tests, and reintroduce decoder attention, beam_width, and",1
v0.9.2,TODO(travis): add when we support pretrained text models for gbms,1
v0.9.2,TODO: find a smaller model for testing,1
v0.9.2,TODO: figure out how to get mocks to work with Ray backend,1
v0.9.2,TODO(travis): We may want to expand upon this in the future to include some checks on model,1
v0.9.2,TODO(travis): We may want to expand upon this in the future to include some checks on model,1
v0.9.2,TODO(travis): We may want to expand upon this in the future to include some checks on model,1
v0.9.2,TODO: Determine whether this is desired behavior. Tracked here:,1
v0.9.2,TODO: feature type not yet supported,1
v0.9.2,handled non-determinism when comparing the metrics between the local and Ray backends. We work around this by,1
v0.9.2,TODO: feature type not yet supported,1
v0.9.2,TODO(geoffrey): Fold modin tests into test_ray_image as @pytest.mark.parametrized once tests are optimized,1
v0.9.2,TODO(travis): move this to separate gpu module so we only have one ray cluster running at a time,1
v0.9.2,TODO(travis): https://github.com/ludwig-ai/ludwig/issues/1956,1
v0.9.2,TODO: Determine if we still need this if-then-else construct,1
v0.9.2,TODO(travis): once we support GBM text features,1
v0.9.2,TODO(travis): once we support GBM text features,1
v0.9.2,TODO(travis): need unit tests to test the get_embedding_layer() of every encoder to ensure it is,1
v0.9.2,TODO: fix LLM model loading,1
v0.9.2,TODO(arnav): p-tuning and prefix tuning have errors when enabled that seem to stem from DDP:,1
v0.9.2,TODO(Arnav): Re-enable once we can run tests on GPUs,1
v0.9.2,TODO: Re-enable once we can run tests on GPUs,1
v0.9.2,"For a full explanation of this 8-bit workaround, see https://github.com/ludwig-ai/ludwig/pull/3606",1
v0.9.2,"TODO: Uncomment ""filter_for_weight_format()"" method definition and enable its usage once GPU tests are set up.",1
v0.9.2,TODO(shreya): Uncomment when https://github.com/ludwig-ai/ludwig/issues/2039 is fixed.,1
v0.9.2,TODO: remove this workaround when audio preprocessing is fixed.,1
v0.9.2,TODO: these are the only outputs we provide from Torchscript for now,1
v0.9.2,TODO all shadows built in name - come up with a more descriptive name,1
v0.9.2,TODO (Connor): Refactor to use self.config_obj,1
v0.9.2,TODO (ASN): add support for substitute_with_max parameter,1
v0.9.2,TODO(Justin): Decide if it's worth folding padding_side handling into llm.py's tokenizer initialization.,1
v0.9.2,TODO(travis): WIP,1
v0.9.2,"TODO(travis): there's a lot of redundancy in this approach, since we are preprocessing the same DataFrame",1
v0.9.2,"TODO(travis): can optimize the preprocessing part here, since we only need to preprocess / predict",1
v0.9.2,TODO(travis): generalize this to support any pandas output format,1
v0.9.2,TODO (Connor): Refactor to use self.config_obj,1
v0.9.2,"TODO: In the future, it may be possible to move up the model type check into the BaseModel class.",1
v0.9.2,TODO: fix for Ray where workers may be of different skus,1
v0.9.2,TODO(travis): remove in favor of on_hyperopt_end for naming consistency,1
v0.9.2,TODO: https://github.com/ludwig-ai/ludwig/issues/2633,1
v0.9.2,todo: revise docstring,1
v0.9.2,todo: assess how to specify padding for equivalent to 'same',1
v0.9.2,todo: determine how to pool_padding equivalent of 'same',1
v0.9.2,todo: fixup docstring,1
v0.9.2,todo: review docstring,1
v0.9.2,todo: fix up docstring,1
v0.9.2,todo: fix up docstring,1
v0.9.2,todo: update docstring as needed,1
v0.9.2,TODO(shreya): Make sure this is updated when FCStack is updated,1
v0.9.2,TODO(Arnav): Remove this once we have reduce_output options set for,1
v0.9.2,TODO(travis): get_hf_config_param_names should be implemented as abstract in HFEncoderConfig,1
v0.9.2,TODO(shreya): Confirm that this is it,1
v0.9.2,TODO(shreya): Confirm that this is it,1
v0.9.2,TODO(justin): This may need to be conditioned on which AutoModel gets chosen.,1
v0.9.2,TODO(Arnav): This needs be more flexible to account for RoPE Scaling,1
v0.9.2,TODO: this implementation will not work if resuming from a previous checkpoint. Need to fix this.,1
v0.9.2,TODO (ASN): Decide how we want to proceed if at least one trial has,1
v0.9.2,TODO(shreya): Should this hyperopt config param be set here?,1
v0.9.2,TODO (ASN): add image heuristics,1
v0.9.2,"TODO(travis): less hacky way to do this, we should probably allow ModelConfig to be created without output",1
v0.9.2,"todo future: this may be redundant, check",1
v0.9.2,Workaround for including additional tensors from output of input encoders for,1
v0.9.2,TODO(Justin): Think about how to make this communication work for multi-sequence,1
v0.9.2,todo: when https://github.com/ludwig-ai/ludwig/issues/810 is closed,1
v0.9.2,"todo: can we just use projector_size? # hidden_size,",1
v0.9.2,"todo future: this may be redundant, check",1
v0.9.2,"todo future: this may be redundant, check",1
v0.9.2,Workaround for including additional tensors from output of input encoders for,1
v0.9.2,TODO(Justin): Think about how to make this communication work for multi-sequence,1
v0.9.2,todo future: maybe reintroduce these attention function,1
v0.9.2,todo future: maybe reintroduce these attention function,1
v0.9.2,todo future: maybe reintroduce these attention function,1
v0.9.2,TODO(travis): consider moving this behind a general BatchNorm interface to avoid this kludge.,1
v0.9.2,"todo: enumerate for debugging, remove after testing",1
v0.9.2,TODO(shreya): Combine with ResNetBlock by adding a flag.,1
v0.9.2,TODO(shreya): Implement sparse embedding lookup.,1
v0.9.2,# TODO(shreya): Check if this is equivalent,1
v0.9.2,# TODO(shreya): Check if supported in torch,1
v0.9.2,todo: review for generality,1
v0.9.2,TODO: Simplify this.,1
v0.9.2,TODO(travis): make this more general to other cumulative loss functions,1
v0.9.2,Dummy implementation.,1
v0.9.2,"TODO(Justin): Add a confusion matrix, see",1
v0.9.2,TODO: add a mechanism for letting the user decide to save it,1
v0.9.2,todo: re-evaluate need for separate handling of `attention` reducer,1
v0.9.2,TODO(Justin): Clean this up.,1
v0.9.2,TODO: change to debug level before merging,1
v0.9.2,TODO: should this raise an exception if not in training mode?,1
v0.9.2,Nested conditional is a workaround to short-circuit boolean evaluation.,1
v0.9.2,TODO: alternatively use get_average_image() for unreachable images,1
v0.9.2,todo future add multiprocessing/multithreading,1
v0.9.2,TODO(travis): do we even need a user param for vector size if we're going to auto-infer it in all,1
v0.9.2,TODO(1891): Remove backward compatibility hack once all models have been retrained with Ludwig after,1
v0.9.2,todo: refactor to reuse SequenceOutputFeature.postprocess_predictions,1
v0.9.2,TODO: Add a mechanism that lets the user save the full probability distribution if they want.,1
v0.9.2,TODO(geoffrey): add support for Dask DataFrames,1
v0.9.2,TODO(Arnav): see if there's a way to only remove them if the entry does't have quotes. This currently,1
v0.9.2,"removes all "" from the string (even those not added by json.dumps), which is not ideal.",1
v0.9.2,Convert datetime to int64 to workaround Dask limitation,1
v0.9.2,"TODO pyarrow: this is needed for caching to work with pyarrow. if removed, the following error is raised:",1
v0.9.2,TODO(travis): passing in MODEL_ECD is a hack here that can be removed once we move to using,1
v0.9.2,"encoder schema at all. This hack works for now because all encoders are supported by ECD, so",1
v0.9.2,todo figure out if additional parameters are needed,1
v0.9.2,"TODO(travis): instead of using raw dictionary, this should be loaded into a proper PreprocessingConfig",1
v0.9.2,TODO dask: this needs to work with DataFrames,1
v0.9.2,TODO(travis): decouple config from training_set_metadata so we don't need to,1
v0.9.2,TODO(joppe): support out of memory negative sampling using Dask,1
v0.9.2,TODO(travis): revisit in the future to make this more precise,1
v0.9.2,TODO: Add link to windowing docs.,1
v0.9.2,TODO: figure out correct typing for augmentation_pipeline after refactoring is done,1
v0.9.2,TODO: consider removing this. doesn't work currently and read performance seems generally,1
v0.9.2,TODO: convert to debug message when done with development,1
v0.9.2,"TODO(travis): could try hashing the in-memory dataset, but this is tricky for Dask",1
v0.9.2,todo future: reintroduce the bucketed batcher,1
v0.9.2,TODO ray: implement dynamic batch size,1
v0.9.2,TODO: Change annotation to PublicAPI once Ludwig 0.7 is released,1
v0.9.2,TODO(travis): is this redundant with `clipglobalnorm`?,1
v0.9.2,TODO(travis) consider removing this in the future after deprecation period,1
v0.9.2,TODO: use registry pattern for trainers,1
v0.9.2,TODO: Change to RAISE and update descriptions once we want to enforce strict schemas.,1
v0.9.2,TODO: Maybe need to plumb 'required' through here,1
v0.9.2,"TODO(travis): this seems much too verbose, does the validation error not show the specific error?",1
v0.9.2,"TODO(travis): too much boilerplate here, we should find a way to abstract all this and only require specifying the",1
v0.9.2,"TODO(travis): this seems much too verbose, does the validation error not show the specific error?",1
v0.9.2,TODO(#1673): Need some more logic here for validating against output features,1
v0.9.2,"TODO: Re-enable ""goss"" when supported: https://github.com/ludwig-ai/ludwig/issues/2988",1
v0.9.2,TODO(travis): figure out why we need these imports to avoid circular import error,1
v0.9.2,"TODO (Connor): Add nesting logic for fc_layers, see fully_connected_module.py",1
v0.9.2,"TODO (Connor): Add nesting logic for fc_layers, see fully_connected_module.py",1
v0.9.2,TODO(travis): seems like this is not really a valid user option. We should probably just remove these,1
v0.9.2,TODO: uncomment when sentencepiece doesn't cause segfaults: https://github.com/ludwig-ai/ludwig/issues/2983,1
v0.9.2,TODO: uncomment once we figure out host memory issue: https://github.com/ludwig-ai/ludwig/issues/3107,1
v0.9.2,TODO: uncomment when CTRL bug (https://github.com/ludwig-ai/ludwig/issues/2977) has been fixed to add back in,1
v0.9.2,TODO(#1673): Add conditional logic for fields like this one:,1
v0.9.2,"TODO(travis): below type comparison is not perfect, as it doesn't consider the case where the default type",1
v0.9.2,"TODO(travis): explore similar contraints for GBMs, which don't have epochs",1
v0.9.2,HACK(Arnav): Set Mixtral target modules when using LoRA,1
v0.9.2,"This is not perfect since it includes tokens from both input + output features, but this at least",1
v0.9.2,TODO (Arnav): Figure out how to factor in rope scaling factor into this calculation.,1
v0.9.2,TODO: Add better support for category output features,1
v0.9.2,TODO(travis): handle this with helper function,1
v0.9.2,TODO(travis): not needed once we remove existing model config implementation,1
v0.9.2,TODO(ksbrar): What is this?,1
v0.9.2,"TODO(travis): this should be done through marshmallow dataclass' `required` field param,",1
v0.9.2,TODO(Arnav): Remove the hard check on max_length once we support multiple output features.,1
v0.9.2,TODO(Arnav): Refactor LossDataclassField to only accept loss types that are valid for the model,1
v0.9.2,TODO: Add schema support for Callable,1
v0.9.2,TODO: This should technically be a required paremeter. Do we need to add support for required params?,1
v0.9.2,TODO: Double-check support for this,1
v0.9.2,TODO: Double-check support for this as well as whether Callable args work properly,1
v0.9.2,TODO: create a search alg metadata class to register in place of individual metadata args,1
v0.9.2,TODO: Add a registry mapping string names to nevergrad optimizers,1
v0.9.2,TODO: Add schemas for nevergrad optimizer kwargs,1
v0.9.2,TODO: Add a registry of Optuna samplers schemas,1
v0.9.2,TODO(travis): figure out why calling this `bias` doesn't work,1
v0.9.2,TODO(travis): fix text generation when using prompt tuning:,1
v0.9.2,TODO(travis): fix prefix tuning and p-tuning to work with DDP,1
v0.9.2,TODO(This needs to be defined based on the Constraint class),1
v0.9.2,TODO: This is an unfortunate side-effect of dataclass init order - you cannot have non-default fields follow,1
v0.9.2,todo v0.4: currently not clear way to set model graph,1
v0.9.2,TODO(daniel): delete this.,1
v0.9.2,TODO(travis): figure out a good way to support this. The problem with,1
v0.9.2,"TODO(Justin): This should probably live in on_ludwig_end, once that's implemented.",1
v0.9.2,TODO: need to also include a filename for this figure,1
v0.9.2,"TODO(geoffrey, arnav): uncomment this when we have reconciled the config with the backend kwarg in api.py",1
v0.9.2,"TODO: `prompt` by default should be set to null, not a default dict:",1
v0.9.2,"TODO: retrieval by default should be set to null, not a default dict:",1
v0.9.2,TODO: len(template_refs) is a hacky attempt to check that there are references to *something* in the,1
v0.9.2,TODO(travis): figure out why we need these imports to avoid circular import error,1
v0.9.2,TODO: Replace with more robust required logic later.,1
v0.9.2,TODO(Arnav): Refactor to split into strategies like splitters,1
v0.9.2,"TODO(geoffrey): figure out where self.max_sequence_length is used– if not used, we might consider removing it.",1
v0.9.2,It's confusing to have both this and `max_new_tokens` as a mandatory param in the `forward` function.,1
v0.9.2,TODO(Arnav): Add support for probabilities and logits,1
v0.9.2,"TODO(Arnav): Figure out how to compute logits. For now, we return",1
v0.9.2,Dummy implementation.,1
v0.9.2,TODO: Use a configurable ratio for how often to use teacher forcing during training.,1
v0.9.2,TODO: Use a configurable ratio for how often to use teacher forcing during training.,1
v0.9.2,Dummy implementation.,1
v0.9.2,TODO(Arnav): Re-enable once we add DotProduct Combiner: https://github.com/ludwig-ai/ludwig/issues/3150,1
v0.9.2,"TODO(travis): we assume here that False is always the default, which may not be true. We should dervice",1
v0.9.2,TODO: Check if subscription has expired,1
v0.9.2,"TODO(travis): stopgap solution, we should make it so we don't need to do this",1
v0.9.2,todo (Wael): tests for all types.,1
v0.9.2,todo (Wael): tests for all types.,1
v0.9.2,Still needed for preprocessing  TODO(Connor): Refactor ludwig/data/preprocessing to use schema,1
v0.9.2,"TODO(travis): remove this, make type a protected string for each subclass",1
v0.9.2,"ORRIBLE HACK, IT'S THE ONLY WAY TO REMOVE PADDING",1
v0.9.2,END OF HORRIBLE HACK,1
v0.9.2,"TODO: more research needed on how to handle RayTune ""sample_from"" search space",1
v0.9.2,TODO(geoffrey): can we better validate tokenizer parity before swapping in the TorchText tokenizer?,1
v0.9.2,TODO: improve this,1
v0.9.2,Workaround for: https://issues.apache.org/jira/browse/ARROW-5645,1
v0.9.2,TODO: make this configurable in the future. These parameters are from FastChat:,1
v0.9.2,"TODO: Wrap device_map=""auto"" in a try-except block since it may not be supported for all models (E.g. BertLMHead)  # noqa",1
v0.9.2,TODO(travis): do this through an interface rather than conditional logic,1
v0.9.2,TODO(travis): move to cached_property when we drop Python 3.7.,1
v0.9.2,TODO: remove reshaping once https://github.com/microsoft/LightGBM/pull/4925 is released,1
v0.9.2,TODO: remove ravel once https://github.com/microsoft/LightGBM/pull/4925 is released,1
v0.9.2,TODO (jeffkinnison): revert to use the requested device once torch device usage is standardized,1
v0.9.2,TODO(shreya): Confirm types of args,1
v0.9.2,"TODO: when loading an existing model, this loses metric values for all but the last epoch.",1
v0.9.2,TODO(travis): implement imbalance ratio,1
v0.9.2,"TODO (ASN): add other modalities (image, etc. )",1
v0.9.2,"TODO(travis): this assumes ECD is the selected model type, which is not problematic for now, as",1
v0.9.2,"diverge, so we should find a way to remove this. The best solution is to the change the input params from",1
v0.9.2,TODO(travis): consolidate with implementation in data/ray.py,1
v0.9.2,TODO: only single task currently,1
v0.9.2,TODO(travis): include encoder and decoder steps during inference,1
v0.9.2,TODO(Arnav): This needs be more flexible to account for RoPE Scaling,1
v0.9.2,TODO: this implementation will not work if resuming from a previous checkpoint. Need to fix this.,1
v0.9.2,TODO: only single task currently,1
v0.9.2,TODO (jeffkinnison): Determine why the 8-bit `SCB` and `CB` matrices are deleted in the forward pass,1
v0.9.2,HACK (Tim): get the device of the targets to transfer self.eval_loss_metric to the same device,1
v0.9.2,"TODO(Arnav): Seems like doing this again and going between these format types in unnecessary, but",1
v0.9.2,"HACK(geoffrey): we need a non-empty loss, so we just fill it with zeros",1
v0.9.2,TODO(travis): this will need to change when we support multiple output features,1
v0.9.2,"TODO(travis): use the implementation of trainer itself to decide whether to save the model, to",1
v0.9.2,avoid this hack,1
v0.9.2,"TODO: see the ""TODO"" statement from ""LLM.save()"" in this module.",1
v0.9.2,TODO (jeffkinnison): revert to using the requested device for GBMs when device usage is fixed,1
v0.9.2,TODO: with change to misc_utils.set_random_seed() this may be redundant,1
v0.9.2,TODO(Justin): Check that the semantics of input_size align with what the combiner's output shape returns,1
v0.9.2,TODO(justin): Remove dummy implementation. Make input_shape and output_shape functions.,1
v0.9.2,TODO(geoffrey): figure out why self.index.search segfaults with larger batch sizes,1
v0.9.2,TODO(travis): support local feature importance,1
v0.9.2,TODO:,1
v0.9.2,TODO(travis): add back skip encoders at the end in finally. Shouldn't be an issue in most cases as we,1
v0.9.2,"TODO(travis): for force plots, need something similar to SHAP E[X]",1
v0.9.2,"TODO(travis): for force plots, need something similar to SHAP E[X]",1
v0.9.2,"TODO(travis): Consider changing to `if not torch.is_floating_point(t.dtype)` to simplify, then handle bool",1
v0.9.2,TODO(travis): pre-compute this during training from the full training dataset.,1
v0.9.2,"TODO(travis): for force plots, need something similar to SHAP E[X]",1
v0.9.2,"TODO(travis): for force plots, need something similar to SHAP E[X]",1
v0.9.2,"TODO: construct new datasets by running encoders (for text, image)",1
v0.9.2,TODO(shreya): Refactor preprocessing so that this can be moved upstream.,1
v0.9.2,TODO(Arnav): Add support for gradient checkpointing in the compiled model,1
v0.9.2,"For a full explanation of this 8-bit workaround, see https://github.com/ludwig-ai/ludwig/pull/3606",1
v0.9.2,TODO (jeffkinnison): Determine why `SCB` and `CB` are deleted from parameter state,1
v0.9.2,matrices are part of model state. This workaround is necessary because the matrices are,1
v0.9.2,"TODO: Implement batch size tuning for LLM, currently just returns the default batch size",1
v0.9.2,TODO: refactor this into an interface,1
v0.9.2,workaround type limitations of the underlying frameworks,1
v0.9.2,TODO(Arnav): Re-enable in Ray 2.3,1
v0.9.2,"TODO(travis): we should revisit the user format here, as it silently breaks situations",1
v0.9.2,"TODO ray: find an informed way to set the parallelism, in practice",1
v0.9.2,TODO: When this is implemented we also need to update the,1
v0.9.2,TODO(travis): consolidate with implementation in data/ray.py,1
v0.9.2,"This is needed because Daft only supports Dataframes, not Series",1
v0.9.2,TODO(ekl) deprecate this once read fusion is available.,1
v0.9.2,TODO(geoffrey): ensure this subclasses ray.data.datasource.Reader in ray 1.14,1
v0.9.2,TODO(ekl) deprecate this once read fusion is available.,1
v0.9.2,"doesn't work for Snappy, so we double-check ourselves.",1
v0.9.2,TODO(xwjiang): Remove this later.,1
v0.9.2,TODO(travis): need to fix checkpoint saving/loading for DeepSpeed to enable tuning,1
v0.9.2,"TODO(travis): this double-counts on the same device, it should use a cross-communicator instead",1
v0.9.2,"TODO: remove LOCAL_BACKEND as a global constant, replace with singleton LocalBackend.shared_instance().",1
v0.9.2,Temporary workaround to prevent tests from automatically using the Ray backend. Taken from,1
v0.9.2,TODO(Alex): Standardize load() signature as interface method in DatasetLoader and adhere to it in all subclasses.,1
v0.9.2,TODO(travis): add other indexing structures,1
v0.9.2,TODO(travis): open question if this is needed to ensure all workers using same weights,1
v0.9.2,TODO(travis): open question if this is needed to ensure all workers using same optimizer state,1
v0.9.2,"TODO(travis): currently Ray handles this for us, but is subject to hangs if one of the workers raises an",1
v0.9.2,TODO(geoffrey): Add a boolean arg to function to control load_module_strict behavior.,1
v0.9.1,HACK(geoffrey): `hyperopt_resources` is a required resource for hyperopt to prevent deadlocks in Ludwig tests.,1
v0.9.1,TODO(geoffrey): remove for Ray 2.2,1
v0.9.1,"HACK(Arnav): Remove configs that have LARS, LAMB or Lion optimizers, or Paged or 8-bit optimizers.",1
v0.9.1,is there a better way to do this?,1
v0.9.1,todo the hidden output is actually a tensor. May need modification,1
v0.9.1,todo figure out the output size for parallel 1d conv,1
v0.9.1,TODO:  Need variant of assert_model_parameters_updated() to account for the two step calling sequence,1
v0.9.1,todo: remove code,1
v0.9.1,TODO(ksbrar): Circle back after discussing whether additional properties should be allowed long-term.,1
v0.9.1,TODO(ksbrar): Circle back after discussing whether additional properties should be allowed long-term.,1
v0.9.1,TODO(Arnav): Re-enable once https://github.com/ludwig-ai/ludwig/issues/3150 is resolved since the GBM,1
v0.9.1,todo: re-add 'attention' after further research in implication of torch,1
v0.9.1,TODO(#1333): Refactor this test once torch sequence generator work is complete.,1
v0.9.1,"TODO(Justin): Move these to test_sequence_generator unit tests, and reintroduce decoder attention, beam_width, and",1
v0.9.1,TODO(travis): add when we support pretrained text models for gbms,1
v0.9.1,TODO: find a smaller model for testing,1
v0.9.1,TODO: figure out how to get mocks to work with Ray backend,1
v0.9.1,TODO(travis): We may want to expand upon this in the future to include some checks on model,1
v0.9.1,TODO(travis): We may want to expand upon this in the future to include some checks on model,1
v0.9.1,TODO(travis): We may want to expand upon this in the future to include some checks on model,1
v0.9.1,TODO: Determine whether this is desired behavior. Tracked here:,1
v0.9.1,TODO: feature type not yet supported,1
v0.9.1,handled non-determinism when comparing the metrics between the local and Ray backends. We work around this by,1
v0.9.1,TODO: feature type not yet supported,1
v0.9.1,TODO(geoffrey): Fold modin tests into test_ray_image as @pytest.mark.parametrized once tests are optimized,1
v0.9.1,TODO(travis): move this to separate gpu module so we only have one ray cluster running at a time,1
v0.9.1,TODO(travis): https://github.com/ludwig-ai/ludwig/issues/1956,1
v0.9.1,TODO: Determine if we still need this if-then-else construct,1
v0.9.1,TODO(travis): once we support GBM text features,1
v0.9.1,TODO(travis): once we support GBM text features,1
v0.9.1,TODO(travis): need unit tests to test the get_embedding_layer() of every encoder to ensure it is,1
v0.9.1,TODO: fix LLM model loading,1
v0.9.1,TODO(arnav): p-tuning and prefix tuning have errors when enabled that seem to stem from DDP:,1
v0.9.1,TODO(Arnav): Re-enable once we can run tests on GPUs,1
v0.9.1,TODO: Re-enable once we can run tests on GPUs,1
v0.9.1,"For a full explanation of this 8-bit workaround, see https://github.com/ludwig-ai/ludwig/pull/3606",1
v0.9.1,"TODO: Uncomment ""filter_for_weight_format()"" method definition and enable its usage once GPU tests are set up.",1
v0.9.1,TODO(shreya): Uncomment when https://github.com/ludwig-ai/ludwig/issues/2039 is fixed.,1
v0.9.1,TODO: remove this workaround when audio preprocessing is fixed.,1
v0.9.1,TODO: these are the only outputs we provide from Torchscript for now,1
v0.9.1,TODO all shadows built in name - come up with a more descriptive name,1
v0.9.1,TODO (Connor): Refactor to use self.config_obj,1
v0.9.1,TODO (ASN): add support for substitute_with_max parameter,1
v0.9.1,TODO(Justin): Decide if it's worth folding padding_side handling into llm.py's tokenizer initialization.,1
v0.9.1,TODO(travis): WIP,1
v0.9.1,"TODO(travis): there's a lot of redundancy in this approach, since we are preprocessing the same DataFrame",1
v0.9.1,"TODO(travis): can optimize the preprocessing part here, since we only need to preprocess / predict",1
v0.9.1,TODO(travis): generalize this to support any pandas output format,1
v0.9.1,TODO (Connor): Refactor to use self.config_obj,1
v0.9.1,"TODO: In the future, it may be possible to move up the model type check into the BaseModel class.",1
v0.9.1,TODO: fix for Ray where workers may be of different skus,1
v0.9.1,TODO(travis): remove in favor of on_hyperopt_end for naming consistency,1
v0.9.1,TODO: https://github.com/ludwig-ai/ludwig/issues/2633,1
v0.9.1,todo: revise docstring,1
v0.9.1,todo: assess how to specify padding for equivalent to 'same',1
v0.9.1,todo: determine how to pool_padding equivalent of 'same',1
v0.9.1,todo: fixup docstring,1
v0.9.1,todo: review docstring,1
v0.9.1,todo: fix up docstring,1
v0.9.1,todo: fix up docstring,1
v0.9.1,todo: update docstring as needed,1
v0.9.1,TODO(shreya): Make sure this is updated when FCStack is updated,1
v0.9.1,TODO(Arnav): Remove this once we have reduce_output options set for,1
v0.9.1,TODO(travis): get_hf_config_param_names should be implemented as abstract in HFEncoderConfig,1
v0.9.1,TODO(shreya): Confirm that this is it,1
v0.9.1,TODO(shreya): Confirm that this is it,1
v0.9.1,TODO(justin): This may need to be conditioned on which AutoModel gets chosen.,1
v0.9.1,TODO(Arnav): This needs be more flexible to account for RoPE Scaling,1
v0.9.1,TODO: this implementation will not work if resuming from a previous checkpoint. Need to fix this.,1
v0.9.1,TODO (ASN): Decide how we want to proceed if at least one trial has,1
v0.9.1,TODO(shreya): Should this hyperopt config param be set here?,1
v0.9.1,TODO (ASN): add image heuristics,1
v0.9.1,"TODO(travis): less hacky way to do this, we should probably allow ModelConfig to be created without output",1
v0.9.1,"todo future: this may be redundant, check",1
v0.9.1,Workaround for including additional tensors from output of input encoders for,1
v0.9.1,TODO(Justin): Think about how to make this communication work for multi-sequence,1
v0.9.1,todo: when https://github.com/ludwig-ai/ludwig/issues/810 is closed,1
v0.9.1,"todo: can we just use projector_size? # hidden_size,",1
v0.9.1,"todo future: this may be redundant, check",1
v0.9.1,"todo future: this may be redundant, check",1
v0.9.1,Workaround for including additional tensors from output of input encoders for,1
v0.9.1,TODO(Justin): Think about how to make this communication work for multi-sequence,1
v0.9.1,todo future: maybe reintroduce these attention function,1
v0.9.1,todo future: maybe reintroduce these attention function,1
v0.9.1,todo future: maybe reintroduce these attention function,1
v0.9.1,TODO(travis): consider moving this behind a general BatchNorm interface to avoid this kludge.,1
v0.9.1,"todo: enumerate for debugging, remove after testing",1
v0.9.1,TODO(shreya): Combine with ResNetBlock by adding a flag.,1
v0.9.1,TODO(shreya): Implement sparse embedding lookup.,1
v0.9.1,# TODO(shreya): Check if this is equivalent,1
v0.9.1,# TODO(shreya): Check if supported in torch,1
v0.9.1,todo: review for generality,1
v0.9.1,TODO: Simplify this.,1
v0.9.1,TODO(travis): make this more general to other cumulative loss functions,1
v0.9.1,Dummy implementation.,1
v0.9.1,"TODO(Justin): Add a confusion matrix, see",1
v0.9.1,TODO: add a mechanism for letting the user decide to save it,1
v0.9.1,todo: re-evaluate need for separate handling of `attention` reducer,1
v0.9.1,TODO(Justin): Clean this up.,1
v0.9.1,TODO: change to debug level before merging,1
v0.9.1,TODO: should this raise an exception if not in training mode?,1
v0.9.1,Nested conditional is a workaround to short-circuit boolean evaluation.,1
v0.9.1,TODO: alternatively use get_average_image() for unreachable images,1
v0.9.1,todo future add multiprocessing/multithreading,1
v0.9.1,TODO(travis): do we even need a user param for vector size if we're going to auto-infer it in all,1
v0.9.1,TODO(1891): Remove backward compatibility hack once all models have been retrained with Ludwig after,1
v0.9.1,todo: refactor to reuse SequenceOutputFeature.postprocess_predictions,1
v0.9.1,TODO: Add a mechanism that lets the user save the full probability distribution if they want.,1
v0.9.1,TODO(geoffrey): add support for Dask DataFrames,1
v0.9.1,TODO(Arnav): see if there's a way to only remove them if the entry does't have quotes. This currently,1
v0.9.1,"removes all "" from the string (even those not added by json.dumps), which is not ideal.",1
v0.9.1,Convert datetime to int64 to workaround Dask limitation,1
v0.9.1,"TODO pyarrow: this is needed for caching to work with pyarrow. if removed, the following error is raised:",1
v0.9.1,TODO(travis): passing in MODEL_ECD is a hack here that can be removed once we move to using,1
v0.9.1,"encoder schema at all. This hack works for now because all encoders are supported by ECD, so",1
v0.9.1,todo figure out if additional parameters are needed,1
v0.9.1,"TODO(travis): instead of using raw dictionary, this should be loaded into a proper PreprocessingConfig",1
v0.9.1,TODO dask: this needs to work with DataFrames,1
v0.9.1,TODO(travis): decouple config from training_set_metadata so we don't need to,1
v0.9.1,TODO(joppe): support out of memory negative sampling using Dask,1
v0.9.1,TODO(travis): revisit in the future to make this more precise,1
v0.9.1,TODO: Add link to windowing docs.,1
v0.9.1,TODO: figure out correct typing for augmentation_pipeline after refactoring is done,1
v0.9.1,TODO: consider removing this. doesn't work currently and read performance seems generally,1
v0.9.1,TODO: convert to debug message when done with development,1
v0.9.1,"TODO(travis): could try hashing the in-memory dataset, but this is tricky for Dask",1
v0.9.1,todo future: reintroduce the bucketed batcher,1
v0.9.1,TODO ray: implement dynamic batch size,1
v0.9.1,TODO: Change annotation to PublicAPI once Ludwig 0.7 is released,1
v0.9.1,TODO(travis): is this redundant with `clipglobalnorm`?,1
v0.9.1,TODO(travis) consider removing this in the future after deprecation period,1
v0.9.1,TODO: use registry pattern for trainers,1
v0.9.1,TODO: Change to RAISE and update descriptions once we want to enforce strict schemas.,1
v0.9.1,TODO: Maybe need to plumb 'required' through here,1
v0.9.1,"TODO(travis): this seems much too verbose, does the validation error not show the specific error?",1
v0.9.1,"TODO(travis): too much boilerplate here, we should find a way to abstract all this and only require specifying the",1
v0.9.1,"TODO(travis): this seems much too verbose, does the validation error not show the specific error?",1
v0.9.1,TODO(#1673): Need some more logic here for validating against output features,1
v0.9.1,"TODO: Re-enable ""goss"" when supported: https://github.com/ludwig-ai/ludwig/issues/2988",1
v0.9.1,TODO(travis): figure out why we need these imports to avoid circular import error,1
v0.9.1,"TODO (Connor): Add nesting logic for fc_layers, see fully_connected_module.py",1
v0.9.1,"TODO (Connor): Add nesting logic for fc_layers, see fully_connected_module.py",1
v0.9.1,TODO(travis): seems like this is not really a valid user option. We should probably just remove these,1
v0.9.1,TODO: uncomment when sentencepiece doesn't cause segfaults: https://github.com/ludwig-ai/ludwig/issues/2983,1
v0.9.1,TODO: uncomment once we figure out host memory issue: https://github.com/ludwig-ai/ludwig/issues/3107,1
v0.9.1,TODO: uncomment when CTRL bug (https://github.com/ludwig-ai/ludwig/issues/2977) has been fixed to add back in,1
v0.9.1,TODO(#1673): Add conditional logic for fields like this one:,1
v0.9.1,"TODO(travis): below type comparison is not perfect, as it doesn't consider the case where the default type",1
v0.9.1,"TODO(travis): explore similar contraints for GBMs, which don't have epochs",1
v0.9.1,"This is not perfect since it includes tokens from both input + output features, but this at least",1
v0.9.1,TODO (Arnav): Figure out how to factor in rope scaling factor into this calculation.,1
v0.9.1,TODO: Add better support for category output features,1
v0.9.1,TODO(travis): handle this with helper function,1
v0.9.1,TODO(travis): not needed once we remove existing model config implementation,1
v0.9.1,TODO(ksbrar): What is this?,1
v0.9.1,"TODO(travis): this should be done through marshmallow dataclass' `required` field param,",1
v0.9.1,TODO(Arnav): Remove the hard check on max_length once we support multiple output features.,1
v0.9.1,TODO(Arnav): Refactor LossDataclassField to only accept loss types that are valid for the model,1
v0.9.1,TODO: Add schema support for Callable,1
v0.9.1,TODO: This should technically be a required paremeter. Do we need to add support for required params?,1
v0.9.1,TODO: Double-check support for this,1
v0.9.1,TODO: Double-check support for this as well as whether Callable args work properly,1
v0.9.1,TODO: create a search alg metadata class to register in place of individual metadata args,1
v0.9.1,TODO: Add a registry mapping string names to nevergrad optimizers,1
v0.9.1,TODO: Add schemas for nevergrad optimizer kwargs,1
v0.9.1,TODO: Add a registry of Optuna samplers schemas,1
v0.9.1,TODO(travis): figure out why calling this `bias` doesn't work,1
v0.9.1,TODO(travis): fix text generation when using prompt tuning:,1
v0.9.1,TODO(travis): fix prefix tuning and p-tuning to work with DDP,1
v0.9.1,TODO(This needs to be defined based on the Constraint class),1
v0.9.1,TODO: This is an unfortunate side-effect of dataclass init order - you cannot have non-default fields follow,1
v0.9.1,todo v0.4: currently not clear way to set model graph,1
v0.9.1,TODO(daniel): delete this.,1
v0.9.1,TODO(travis): figure out a good way to support this. The problem with,1
v0.9.1,"TODO(Justin): This should probably live in on_ludwig_end, once that's implemented.",1
v0.9.1,TODO: need to also include a filename for this figure,1
v0.9.1,"TODO(geoffrey, arnav): uncomment this when we have reconciled the config with the backend kwarg in api.py",1
v0.9.1,"TODO: `prompt` by default should be set to null, not a default dict:",1
v0.9.1,"TODO: retrieval by default should be set to null, not a default dict:",1
v0.9.1,TODO: len(template_refs) is a hacky attempt to check that there are references to *something* in the,1
v0.9.1,TODO(travis): figure out why we need these imports to avoid circular import error,1
v0.9.1,TODO: Replace with more robust required logic later.,1
v0.9.1,TODO(Arnav): Refactor to split into strategies like splitters,1
v0.9.1,"TODO(geoffrey): figure out where self.max_sequence_length is used– if not used, we might consider removing it.",1
v0.9.1,It's confusing to have both this and `max_new_tokens` as a mandatory param in the `forward` function.,1
v0.9.1,TODO(Arnav): Add support for probabilities and logits,1
v0.9.1,"TODO(Arnav): Figure out how to compute logits. For now, we return",1
v0.9.1,Dummy implementation.,1
v0.9.1,TODO: Use a configurable ratio for how often to use teacher forcing during training.,1
v0.9.1,TODO: Use a configurable ratio for how often to use teacher forcing during training.,1
v0.9.1,Dummy implementation.,1
v0.9.1,TODO(Arnav): Re-enable once we add DotProduct Combiner: https://github.com/ludwig-ai/ludwig/issues/3150,1
v0.9.1,"TODO(travis): we assume here that False is always the default, which may not be true. We should dervice",1
v0.9.1,TODO: Check if subscription has expired,1
v0.9.1,"TODO(travis): stopgap solution, we should make it so we don't need to do this",1
v0.9.1,todo (Wael): tests for all types.,1
v0.9.1,todo (Wael): tests for all types.,1
v0.9.1,Still needed for preprocessing  TODO(Connor): Refactor ludwig/data/preprocessing to use schema,1
v0.9.1,"TODO(travis): remove this, make type a protected string for each subclass",1
v0.9.1,"ORRIBLE HACK, IT'S THE ONLY WAY TO REMOVE PADDING",1
v0.9.1,END OF HORRIBLE HACK,1
v0.9.1,"TODO: more research needed on how to handle RayTune ""sample_from"" search space",1
v0.9.1,TODO(geoffrey): can we better validate tokenizer parity before swapping in the TorchText tokenizer?,1
v0.9.1,Workaround for: https://issues.apache.org/jira/browse/ARROW-5645,1
v0.9.1,TODO: make this configurable in the future. These parameters are from FastChat:,1
v0.9.1,"TODO: Wrap device_map=""auto"" in a try-except block since it may not be supported for all models (E.g. BertLMHead)  # noqa",1
v0.9.1,TODO(travis): do this through an interface rather than conditional logic,1
v0.9.1,TODO(travis): move to cached_property when we drop Python 3.7.,1
v0.9.1,TODO: remove reshaping once https://github.com/microsoft/LightGBM/pull/4925 is released,1
v0.9.1,TODO: remove ravel once https://github.com/microsoft/LightGBM/pull/4925 is released,1
v0.9.1,TODO (jeffkinnison): revert to use the requested device once torch device usage is standardized,1
v0.9.1,TODO(shreya): Confirm types of args,1
v0.9.1,"TODO: when loading an existing model, this loses metric values for all but the last epoch.",1
v0.9.1,TODO(travis): implement imbalance ratio,1
v0.9.1,"TODO (ASN): add other modalities (image, etc. )",1
v0.9.1,"TODO(travis): this assumes ECD is the selected model type, which is not problematic for now, as",1
v0.9.1,"diverge, so we should find a way to remove this. The best solution is to the change the input params from",1
v0.9.1,TODO(travis): consolidate with implementation in data/ray.py,1
v0.9.1,TODO: only single task currently,1
v0.9.1,TODO(travis): include encoder and decoder steps during inference,1
v0.9.1,TODO(Arnav): This needs be more flexible to account for RoPE Scaling,1
v0.9.1,TODO: this implementation will not work if resuming from a previous checkpoint. Need to fix this.,1
v0.9.1,TODO: only single task currently,1
v0.9.1,TODO (jeffkinnison): Determine why the 8-bit `SCB` and `CB` matrices are deleted in the forward pass,1
v0.9.1,HACK (Tim): get the device of the targets to transfer self.eval_loss_metric to the same device,1
v0.9.1,"TODO(Arnav): Seems like doing this again and going between these format types in unnecessary, but",1
v0.9.1,"HACK(geoffrey): we need a non-empty loss, so we just fill it with zeros",1
v0.9.1,TODO(travis): this will need to change when we support multiple output features,1
v0.9.1,"TODO(travis): use the implementation of trainer itself to decide whether to save the model, to",1
v0.9.1,avoid this hack,1
v0.9.1,"TODO: see the ""TODO"" statement from ""LLM.save()"" in this module.",1
v0.9.1,TODO (jeffkinnison): revert to using the requested device for GBMs when device usage is fixed,1
v0.9.1,TODO: with change to misc_utils.set_random_seed() this may be redundant,1
v0.9.1,TODO(Justin): Check that the semantics of input_size align with what the combiner's output shape returns,1
v0.9.1,TODO(justin): Remove dummy implementation. Make input_shape and output_shape functions.,1
v0.9.1,TODO(geoffrey): figure out why self.index.search segfaults with larger batch sizes,1
v0.9.1,TODO(travis): support local feature importance,1
v0.9.1,TODO:,1
v0.9.1,TODO(travis): add back skip encoders at the end in finally. Shouldn't be an issue in most cases as we,1
v0.9.1,"TODO(travis): for force plots, need something similar to SHAP E[X]",1
v0.9.1,"TODO(travis): for force plots, need something similar to SHAP E[X]",1
v0.9.1,"TODO(travis): Consider changing to `if not torch.is_floating_point(t.dtype)` to simplify, then handle bool",1
v0.9.1,TODO(travis): pre-compute this during training from the full training dataset.,1
v0.9.1,"TODO(travis): for force plots, need something similar to SHAP E[X]",1
v0.9.1,"TODO(travis): for force plots, need something similar to SHAP E[X]",1
v0.9.1,"TODO: construct new datasets by running encoders (for text, image)",1
v0.9.1,TODO(shreya): Refactor preprocessing so that this can be moved upstream.,1
v0.9.1,TODO(Arnav): Add support for gradient checkpointing in the compiled model,1
v0.9.1,"For a full explanation of this 8-bit workaround, see https://github.com/ludwig-ai/ludwig/pull/3606",1
v0.9.1,TODO (jeffkinnison): Determine why `SCB` and `CB` are deleted from parameter state,1
v0.9.1,matrices are part of model state. This workaround is necessary because the matrices are,1
v0.9.1,"TODO: Implement batch size tuning for LLM, currently just returns the default batch size",1
v0.9.1,TODO: refactor this into an interface,1
v0.9.1,workaround type limitations of the underlying frameworks,1
v0.9.1,TODO(Arnav): Re-enable in Ray 2.3,1
v0.9.1,"TODO(travis): we should revisit the user format here, as it silently breaks situations",1
v0.9.1,"TODO ray: find an informed way to set the parallelism, in practice",1
v0.9.1,TODO: When this is implemented we also need to update the,1
v0.9.1,TODO(travis): consolidate with implementation in data/ray.py,1
v0.9.1,"This is needed because Daft only supports Dataframes, not Series",1
v0.9.1,TODO(ekl) deprecate this once read fusion is available.,1
v0.9.1,TODO(geoffrey): ensure this subclasses ray.data.datasource.Reader in ray 1.14,1
v0.9.1,TODO(ekl) deprecate this once read fusion is available.,1
v0.9.1,"doesn't work for Snappy, so we double-check ourselves.",1
v0.9.1,TODO(xwjiang): Remove this later.,1
v0.9.1,TODO(travis): need to fix checkpoint saving/loading for DeepSpeed to enable tuning,1
v0.9.1,"TODO(travis): this double-counts on the same device, it should use a cross-communicator instead",1
v0.9.1,"TODO: remove LOCAL_BACKEND as a global constant, replace with singleton LocalBackend.shared_instance().",1
v0.9.1,Temporary workaround to prevent tests from automatically using the Ray backend. Taken from,1
v0.9.1,TODO(Alex): Standardize load() signature as interface method in DatasetLoader and adhere to it in all subclasses.,1
v0.9.1,TODO(travis): add other indexing structures,1
v0.9.1,TODO(travis): open question if this is needed to ensure all workers using same weights,1
v0.9.1,TODO(travis): open question if this is needed to ensure all workers using same optimizer state,1
v0.9.1,"TODO(travis): currently Ray handles this for us, but is subject to hangs if one of the workers raises an",1
v0.9.1,TODO(geoffrey): Add a boolean arg to function to control load_module_strict behavior.,1
v0.9.1,HACK(geoffrey): use vanilla model.eval(),1
v0.9.1,HACK(geoffrey): use vanilla model.train(prev_model_training_mode),1
v0.9.1,This hack ignores module.training updates if the model is already in training mode,1
v0.9,HACK(geoffrey): `hyperopt_resources` is a required resource for hyperopt to prevent deadlocks in Ludwig tests.,1
v0.9,TODO(geoffrey): remove for Ray 2.2,1
v0.9,"HACK(Arnav): Remove configs that have LARS, LAMB or Lion optimizers, or Paged or 8-bit optimizers.",1
v0.9,is there a better way to do this?,1
v0.9,todo the hidden output is actually a tensor. May need modification,1
v0.9,todo figure out the output size for parallel 1d conv,1
v0.9,TODO:  Need variant of assert_model_parameters_updated() to account for the two step calling sequence,1
v0.9,todo: remove code,1
v0.9,TODO(ksbrar): Circle back after discussing whether additional properties should be allowed long-term.,1
v0.9,TODO(ksbrar): Circle back after discussing whether additional properties should be allowed long-term.,1
v0.9,TODO(Arnav): Re-enable once https://github.com/ludwig-ai/ludwig/issues/3150 is resolved since the GBM,1
v0.9,todo: re-add 'attention' after further research in implication of torch,1
v0.9,TODO(#1333): Refactor this test once torch sequence generator work is complete.,1
v0.9,"TODO(Justin): Move these to test_sequence_generator unit tests, and reintroduce decoder attention, beam_width, and",1
v0.9,TODO(travis): add when we support pretrained text models for gbms,1
v0.9,TODO: find a smaller model for testing,1
v0.9,TODO: figure out how to get mocks to work with Ray backend,1
v0.9,TODO(travis): We may want to expand upon this in the future to include some checks on model,1
v0.9,TODO(travis): We may want to expand upon this in the future to include some checks on model,1
v0.9,TODO(travis): We may want to expand upon this in the future to include some checks on model,1
v0.9,TODO: Determine whether this is desired behavior. Tracked here:,1
v0.9,TODO: feature type not yet supported,1
v0.9,handled non-determinism when comparing the metrics between the local and Ray backends. We work around this by,1
v0.9,TODO: feature type not yet supported,1
v0.9,TODO(geoffrey): Fold modin tests into test_ray_image as @pytest.mark.parametrized once tests are optimized,1
v0.9,TODO(travis): move this to separate gpu module so we only have one ray cluster running at a time,1
v0.9,TODO(travis): https://github.com/ludwig-ai/ludwig/issues/1956,1
v0.9,TODO: Determine if we still need this if-then-else construct,1
v0.9,TODO(travis): once we support GBM text features,1
v0.9,TODO(travis): once we support GBM text features,1
v0.9,TODO(travis): need unit tests to test the get_embedding_layer() of every encoder to ensure it is,1
v0.9,TODO: fix LLM model loading,1
v0.9,TODO(arnav): p-tuning and prefix tuning have errors when enabled that seem to stem from DDP:,1
v0.9,TODO(Arnav): Re-enable once we can run tests on GPUs,1
v0.9,TODO: Re-enable once we can run tests on GPUs,1
v0.9,"For a full explanation of this 8-bit workaround, see https://github.com/ludwig-ai/ludwig/pull/3606",1
v0.9,"TODO: Uncomment ""filter_for_weight_format()"" method definition and enable its usage once GPU tests are set up.",1
v0.9,TODO(shreya): Uncomment when https://github.com/ludwig-ai/ludwig/issues/2039 is fixed.,1
v0.9,TODO: remove this workaround when audio preprocessing is fixed.,1
v0.9,TODO: these are the only outputs we provide from Torchscript for now,1
v0.9,TODO all shadows built in name - come up with a more descriptive name,1
v0.9,TODO (Connor): Refactor to use self.config_obj,1
v0.9,TODO (ASN): add support for substitute_with_max parameter,1
v0.9,TODO(Justin): Decide if it's worth folding padding_side handling into llm.py's tokenizer initialization.,1
v0.9,TODO(travis): WIP,1
v0.9,"TODO(travis): there's a lot of redundancy in this approach, since we are preprocessing the same DataFrame",1
v0.9,"TODO(travis): can optimize the preprocessing part here, since we only need to preprocess / predict",1
v0.9,TODO(travis): generalize this to support any pandas output format,1
v0.9,TODO (Connor): Refactor to use self.config_obj,1
v0.9,"TODO: In the future, it may be possible to move up the model type check into the BaseModel class.",1
v0.9,TODO: fix for Ray where workers may be of different skus,1
v0.9,TODO(travis): remove in favor of on_hyperopt_end for naming consistency,1
v0.9,TODO: https://github.com/ludwig-ai/ludwig/issues/2633,1
v0.9,todo: revise docstring,1
v0.9,todo: assess how to specify padding for equivalent to 'same',1
v0.9,todo: determine how to pool_padding equivalent of 'same',1
v0.9,todo: fixup docstring,1
v0.9,todo: review docstring,1
v0.9,todo: fix up docstring,1
v0.9,todo: fix up docstring,1
v0.9,todo: update docstring as needed,1
v0.9,TODO(shreya): Make sure this is updated when FCStack is updated,1
v0.9,TODO(Arnav): Remove this once we have reduce_output options set for,1
v0.9,TODO(travis): get_hf_config_param_names should be implemented as abstract in HFEncoderConfig,1
v0.9,TODO(shreya): Confirm that this is it,1
v0.9,TODO(shreya): Confirm that this is it,1
v0.9,TODO(justin): This may need to be conditioned on which AutoModel gets chosen.,1
v0.9,TODO(Arnav): This needs be more flexible to account for RoPE Scaling,1
v0.9,TODO: this implementation will not work if resuming from a previous checkpoint. Need to fix this.,1
v0.9,TODO (ASN): Decide how we want to proceed if at least one trial has,1
v0.9,TODO(shreya): Should this hyperopt config param be set here?,1
v0.9,TODO (ASN): add image heuristics,1
v0.9,"TODO(travis): less hacky way to do this, we should probably allow ModelConfig to be created without output",1
v0.9,"todo future: this may be redundant, check",1
v0.9,Workaround for including additional tensors from output of input encoders for,1
v0.9,TODO(Justin): Think about how to make this communication work for multi-sequence,1
v0.9,todo: when https://github.com/ludwig-ai/ludwig/issues/810 is closed,1
v0.9,"todo: can we just use projector_size? # hidden_size,",1
v0.9,"todo future: this may be redundant, check",1
v0.9,"todo future: this may be redundant, check",1
v0.9,Workaround for including additional tensors from output of input encoders for,1
v0.9,TODO(Justin): Think about how to make this communication work for multi-sequence,1
v0.9,todo future: maybe reintroduce these attention function,1
v0.9,todo future: maybe reintroduce these attention function,1
v0.9,todo future: maybe reintroduce these attention function,1
v0.9,TODO(travis): consider moving this behind a general BatchNorm interface to avoid this kludge.,1
v0.9,"todo: enumerate for debugging, remove after testing",1
v0.9,TODO(shreya): Combine with ResNetBlock by adding a flag.,1
v0.9,TODO(shreya): Implement sparse embedding lookup.,1
v0.9,# TODO(shreya): Check if this is equivalent,1
v0.9,# TODO(shreya): Check if supported in torch,1
v0.9,todo: review for generality,1
v0.9,TODO: Simplify this.,1
v0.9,TODO(travis): make this more general to other cumulative loss functions,1
v0.9,Dummy implementation.,1
v0.9,"TODO(Justin): Add a confusion matrix, see",1
v0.9,TODO: add a mechanism for letting the user decide to save it,1
v0.9,todo: re-evaluate need for separate handling of `attention` reducer,1
v0.9,TODO(Justin): Clean this up.,1
v0.9,TODO: change to debug level before merging,1
v0.9,TODO: should this raise an exception if not in training mode?,1
v0.9,Nested conditional is a workaround to short-circuit boolean evaluation.,1
v0.9,TODO: alternatively use get_average_image() for unreachable images,1
v0.9,todo future add multiprocessing/multithreading,1
v0.9,TODO(travis): do we even need a user param for vector size if we're going to auto-infer it in all,1
v0.9,TODO(1891): Remove backward compatibility hack once all models have been retrained with Ludwig after,1
v0.9,todo: refactor to reuse SequenceOutputFeature.postprocess_predictions,1
v0.9,TODO: Add a mechanism that lets the user save the full probability distribution if they want.,1
v0.9,TODO(geoffrey): add support for Dask DataFrames,1
v0.9,TODO(Arnav): see if there's a way to only remove them if the entry does't have quotes. This currently,1
v0.9,"removes all "" from the string (even those not added by json.dumps), which is not ideal.",1
v0.9,Convert datetime to int64 to workaround Dask limitation,1
v0.9,"TODO pyarrow: this is needed for caching to work with pyarrow. if removed, the following error is raised:",1
v0.9,TODO(travis): passing in MODEL_ECD is a hack here that can be removed once we move to using,1
v0.9,"encoder schema at all. This hack works for now because all encoders are supported by ECD, so",1
v0.9,todo figure out if additional parameters are needed,1
v0.9,"TODO(travis): instead of using raw dictionary, this should be loaded into a proper PreprocessingConfig",1
v0.9,TODO dask: this needs to work with DataFrames,1
v0.9,TODO(travis): decouple config from training_set_metadata so we don't need to,1
v0.9,TODO(joppe): support out of memory negative sampling using Dask,1
v0.9,TODO(travis): revisit in the future to make this more precise,1
v0.9,TODO: Add link to windowing docs.,1
v0.9,TODO: figure out correct typing for augmentation_pipeline after refactoring is done,1
v0.9,TODO: consider removing this. doesn't work currently and read performance seems generally,1
v0.9,TODO: convert to debug message when done with development,1
v0.9,"TODO(travis): could try hashing the in-memory dataset, but this is tricky for Dask",1
v0.9,todo future: reintroduce the bucketed batcher,1
v0.9,TODO ray: implement dynamic batch size,1
v0.9,TODO: Change annotation to PublicAPI once Ludwig 0.7 is released,1
v0.9,TODO(travis): is this redundant with `clipglobalnorm`?,1
v0.9,TODO(travis) consider removing this in the future after deprecation period,1
v0.9,TODO: use registry pattern for trainers,1
v0.9,TODO: Change to RAISE and update descriptions once we want to enforce strict schemas.,1
v0.9,TODO: Maybe need to plumb 'required' through here,1
v0.9,"TODO(travis): this seems much too verbose, does the validation error not show the specific error?",1
v0.9,"TODO(travis): too much boilerplate here, we should find a way to abstract all this and only require specifying the",1
v0.9,"TODO(travis): this seems much too verbose, does the validation error not show the specific error?",1
v0.9,TODO(#1673): Need some more logic here for validating against output features,1
v0.9,"TODO: Re-enable ""goss"" when supported: https://github.com/ludwig-ai/ludwig/issues/2988",1
v0.9,TODO(travis): figure out why we need these imports to avoid circular import error,1
v0.9,"TODO (Connor): Add nesting logic for fc_layers, see fully_connected_module.py",1
v0.9,"TODO (Connor): Add nesting logic for fc_layers, see fully_connected_module.py",1
v0.9,TODO(travis): seems like this is not really a valid user option. We should probably just remove these,1
v0.9,TODO: uncomment when sentencepiece doesn't cause segfaults: https://github.com/ludwig-ai/ludwig/issues/2983,1
v0.9,TODO: uncomment once we figure out host memory issue: https://github.com/ludwig-ai/ludwig/issues/3107,1
v0.9,TODO: uncomment when CTRL bug (https://github.com/ludwig-ai/ludwig/issues/2977) has been fixed to add back in,1
v0.9,TODO(#1673): Add conditional logic for fields like this one:,1
v0.9,"TODO(travis): below type comparison is not perfect, as it doesn't consider the case where the default type",1
v0.9,"TODO(travis): explore similar contraints for GBMs, which don't have epochs",1
v0.9,"This is not perfect since it includes tokens from both input + output features, but this at least",1
v0.9,TODO (Arnav): Figure out how to factor in rope scaling factor into this calculation.,1
v0.9,TODO: Add better support for category output features,1
v0.9,TODO(travis): handle this with helper function,1
v0.9,TODO(travis): not needed once we remove existing model config implementation,1
v0.9,TODO(ksbrar): What is this?,1
v0.9,"TODO(travis): this should be done through marshmallow dataclass' `required` field param,",1
v0.9,TODO(Arnav): Remove the hard check on max_length once we support multiple output features.,1
v0.9,TODO(Arnav): Refactor LossDataclassField to only accept loss types that are valid for the model,1
v0.9,TODO: Add schema support for Callable,1
v0.9,TODO: This should technically be a required paremeter. Do we need to add support for required params?,1
v0.9,TODO: Double-check support for this,1
v0.9,TODO: Double-check support for this as well as whether Callable args work properly,1
v0.9,TODO: create a search alg metadata class to register in place of individual metadata args,1
v0.9,TODO: Add a registry mapping string names to nevergrad optimizers,1
v0.9,TODO: Add schemas for nevergrad optimizer kwargs,1
v0.9,TODO: Add a registry of Optuna samplers schemas,1
v0.9,TODO(travis): figure out why calling this `bias` doesn't work,1
v0.9,TODO(travis): fix text generation when using prompt tuning:,1
v0.9,TODO(travis): fix prefix tuning and p-tuning to work with DDP,1
v0.9,TODO(This needs to be defined based on the Constraint class),1
v0.9,TODO: This is an unfortunate side-effect of dataclass init order - you cannot have non-default fields follow,1
v0.9,todo v0.4: currently not clear way to set model graph,1
v0.9,TODO(daniel): delete this.,1
v0.9,TODO(travis): figure out a good way to support this. The problem with,1
v0.9,"TODO(Justin): This should probably live in on_ludwig_end, once that's implemented.",1
v0.9,TODO: need to also include a filename for this figure,1
v0.9,"TODO(geoffrey, arnav): uncomment this when we have reconciled the config with the backend kwarg in api.py",1
v0.9,"TODO: `prompt` by default should be set to null, not a default dict:",1
v0.9,"TODO: retrieval by default should be set to null, not a default dict:",1
v0.9,TODO: len(template_refs) is a hacky attempt to check that there are references to *something* in the,1
v0.9,TODO(travis): figure out why we need these imports to avoid circular import error,1
v0.9,TODO: Replace with more robust required logic later.,1
v0.9,TODO(Arnav): Refactor to split into strategies like splitters,1
v0.9,"TODO(geoffrey): figure out where self.max_sequence_length is used– if not used, we might consider removing it.",1
v0.9,It's confusing to have both this and `max_new_tokens` as a mandatory param in the `forward` function.,1
v0.9,TODO(Arnav): Add support for probabilities and logits,1
v0.9,"TODO(Arnav): Figure out how to compute logits. For now, we return",1
v0.9,Dummy implementation.,1
v0.9,TODO: Use a configurable ratio for how often to use teacher forcing during training.,1
v0.9,TODO: Use a configurable ratio for how often to use teacher forcing during training.,1
v0.9,Dummy implementation.,1
v0.9,TODO(Arnav): Re-enable once we add DotProduct Combiner: https://github.com/ludwig-ai/ludwig/issues/3150,1
v0.9,"TODO(travis): we assume here that False is always the default, which may not be true. We should dervice",1
v0.9,TODO: Check if subscription has expired,1
v0.9,"TODO(travis): stopgap solution, we should make it so we don't need to do this",1
v0.9,todo (Wael): tests for all types.,1
v0.9,todo (Wael): tests for all types.,1
v0.9,Still needed for preprocessing  TODO(Connor): Refactor ludwig/data/preprocessing to use schema,1
v0.9,"TODO(travis): remove this, make type a protected string for each subclass",1
v0.9,"ORRIBLE HACK, IT'S THE ONLY WAY TO REMOVE PADDING",1
v0.9,END OF HORRIBLE HACK,1
v0.9,"TODO: more research needed on how to handle RayTune ""sample_from"" search space",1
v0.9,TODO(geoffrey): can we better validate tokenizer parity before swapping in the TorchText tokenizer?,1
v0.9,Workaround for: https://issues.apache.org/jira/browse/ARROW-5645,1
v0.9,TODO: make this configurable in the future. These parameters are from FastChat:,1
v0.9,"TODO: Wrap device_map=""auto"" in a try-except block since it may not be supported for all models (E.g. BertLMHead)  # noqa",1
v0.9,TODO(travis): do this through an interface rather than conditional logic,1
v0.9,TODO(travis): move to cached_property when we drop Python 3.7.,1
v0.9,TODO: remove reshaping once https://github.com/microsoft/LightGBM/pull/4925 is released,1
v0.9,TODO: remove ravel once https://github.com/microsoft/LightGBM/pull/4925 is released,1
v0.9,TODO (jeffkinnison): revert to use the requested device once torch device usage is standardized,1
v0.9,TODO(shreya): Confirm types of args,1
v0.9,"TODO: when loading an existing model, this loses metric values for all but the last epoch.",1
v0.9,TODO(travis): implement imbalance ratio,1
v0.9,"TODO (ASN): add other modalities (image, etc. )",1
v0.9,"TODO(travis): this assumes ECD is the selected model type, which is not problematic for now, as",1
v0.9,"diverge, so we should find a way to remove this. The best solution is to the change the input params from",1
v0.9,TODO(travis): consolidate with implementation in data/ray.py,1
v0.9,TODO: only single task currently,1
v0.9,TODO(travis): include encoder and decoder steps during inference,1
v0.9,TODO(Arnav): This needs be more flexible to account for RoPE Scaling,1
v0.9,TODO: this implementation will not work if resuming from a previous checkpoint. Need to fix this.,1
v0.9,TODO: only single task currently,1
v0.9,TODO (jeffkinnison): Determine why the 8-bit `SCB` and `CB` matrices are deleted in the forward pass,1
v0.9,HACK (Tim): get the device of the targets to transfer self.eval_loss_metric to the same device,1
v0.9,"TODO(Arnav): Seems like doing this again and going between these format types in unnecessary, but",1
v0.9,"HACK(geoffrey): we need a non-empty loss, so we just fill it with zeros",1
v0.9,TODO(travis): this will need to change when we support multiple output features,1
v0.9,"TODO(travis): use the implementation of trainer itself to decide whether to save the model, to",1
v0.9,avoid this hack,1
v0.9,"TODO: see the ""TODO"" statement from ""LLM.save()"" in this module.",1
v0.9,TODO (jeffkinnison): revert to using the requested device for GBMs when device usage is fixed,1
v0.9,TODO: with change to misc_utils.set_random_seed() this may be redundant,1
v0.9,TODO(Justin): Check that the semantics of input_size align with what the combiner's output shape returns,1
v0.9,TODO(justin): Remove dummy implementation. Make input_shape and output_shape functions.,1
v0.9,TODO(geoffrey): figure out why self.index.search segfaults with larger batch sizes,1
v0.9,TODO(travis): support local feature importance,1
v0.9,TODO:,1
v0.9,TODO(travis): add back skip encoders at the end in finally. Shouldn't be an issue in most cases as we,1
v0.9,"TODO(travis): for force plots, need something similar to SHAP E[X]",1
v0.9,"TODO(travis): for force plots, need something similar to SHAP E[X]",1
v0.9,"TODO(travis): Consider changing to `if not torch.is_floating_point(t.dtype)` to simplify, then handle bool",1
v0.9,TODO(travis): pre-compute this during training from the full training dataset.,1
v0.9,"TODO(travis): for force plots, need something similar to SHAP E[X]",1
v0.9,"TODO(travis): for force plots, need something similar to SHAP E[X]",1
v0.9,"TODO: construct new datasets by running encoders (for text, image)",1
v0.9,TODO(shreya): Refactor preprocessing so that this can be moved upstream.,1
v0.9,TODO(Arnav): Add support for gradient checkpointing in the compiled model,1
v0.9,"For a full explanation of this 8-bit workaround, see https://github.com/ludwig-ai/ludwig/pull/3606",1
v0.9,TODO (jeffkinnison): Determine why `SCB` and `CB` are deleted from parameter state,1
v0.9,matrices are part of model state. This workaround is necessary because the matrices are,1
v0.9,"TODO: Implement batch size tuning for LLM, currently just returns the default batch size",1
v0.9,TODO: refactor this into an interface,1
v0.9,workaround type limitations of the underlying frameworks,1
v0.9,TODO(Arnav): Re-enable in Ray 2.3,1
v0.9,"TODO(travis): we should revisit the user format here, as it silently breaks situations",1
v0.9,"TODO ray: find an informed way to set the parallelism, in practice",1
v0.9,TODO: When this is implemented we also need to update the,1
v0.9,TODO(travis): consolidate with implementation in data/ray.py,1
v0.9,"This is needed because Daft only supports Dataframes, not Series",1
v0.9,TODO(ekl) deprecate this once read fusion is available.,1
v0.9,TODO(geoffrey): ensure this subclasses ray.data.datasource.Reader in ray 1.14,1
v0.9,TODO(ekl) deprecate this once read fusion is available.,1
v0.9,"doesn't work for Snappy, so we double-check ourselves.",1
v0.9,TODO(xwjiang): Remove this later.,1
v0.9,TODO(travis): need to fix checkpoint saving/loading for DeepSpeed to enable tuning,1
v0.9,"TODO(travis): this double-counts on the same device, it should use a cross-communicator instead",1
v0.9,"TODO: remove LOCAL_BACKEND as a global constant, replace with singleton LocalBackend.shared_instance().",1
v0.9,Temporary workaround to prevent tests from automatically using the Ray backend. Taken from,1
v0.9,TODO(Alex): Standardize load() signature as interface method in DatasetLoader and adhere to it in all subclasses.,1
v0.9,TODO(travis): add other indexing structures,1
v0.9,TODO(travis): open question if this is needed to ensure all workers using same weights,1
v0.9,TODO(travis): open question if this is needed to ensure all workers using same optimizer state,1
v0.9,"TODO(travis): currently Ray handles this for us, but is subject to hangs if one of the workers raises an",1
v0.9,TODO(geoffrey): Add a boolean arg to function to control load_module_strict behavior.,1
v0.9,HACK(geoffrey): use vanilla model.eval(),1
v0.9,HACK(geoffrey): use vanilla model.train(prev_model_training_mode),1
v0.9,This hack ignores module.training updates if the model is already in training mode,1
v0.8.6,HACK(geoffrey): `hyperopt_resources` is a required resource for hyperopt to prevent deadlocks in Ludwig tests.,1
v0.8.6,TODO(geoffrey): remove for Ray 2.2,1
v0.8.6,"HACK(Arnav): Remove configs that have LARS, LAMB or Lion optimizers, or Paged or 8-bit optimizers.",1
v0.8.6,is there a better way to do this?,1
v0.8.6,todo the hidden output is actually a tensor. May need modification,1
v0.8.6,todo figure out the output size for parallel 1d conv,1
v0.8.6,TODO:  Need variant of assert_model_parameters_updated() to account for the two step calling sequence,1
v0.8.6,todo: remove code,1
v0.8.6,TODO(ksbrar): Circle back after discussing whether additional properties should be allowed long-term.,1
v0.8.6,TODO(ksbrar): Circle back after discussing whether additional properties should be allowed long-term.,1
v0.8.6,TODO(Arnav): Re-enable once https://github.com/ludwig-ai/ludwig/issues/3150 is resolved since the GBM,1
v0.8.6,todo: re-add 'attention' after further research in implication of torch,1
v0.8.6,TODO(#1333): Refactor this test once torch sequence generator work is complete.,1
v0.8.6,"TODO(Justin): Move these to test_sequence_generator unit tests, and reintroduce decoder attention, beam_width, and",1
v0.8.6,TODO(travis): add when we support pretrained text models for gbms,1
v0.8.6,TODO: find a smaller model for testing,1
v0.8.6,TODO: figure out how to get mocks to work with Ray backend,1
v0.8.6,TODO(travis): We may want to expand upon this in the future to include some checks on model,1
v0.8.6,TODO(travis): We may want to expand upon this in the future to include some checks on model,1
v0.8.6,TODO(travis): We may want to expand upon this in the future to include some checks on model,1
v0.8.6,TODO: Determine whether this is desired behavior. Tracked here:,1
v0.8.6,TODO: feature type not yet supported,1
v0.8.6,handled non-determinism when comparing the metrics between the local and Ray backends. We work around this by,1
v0.8.6,TODO: feature type not yet supported,1
v0.8.6,TODO(geoffrey): Fold modin tests into test_ray_image as @pytest.mark.parametrized once tests are optimized,1
v0.8.6,TODO(travis): move this to separate gpu module so we only have one ray cluster running at a time,1
v0.8.6,TODO(travis): https://github.com/ludwig-ai/ludwig/issues/1956,1
v0.8.6,TODO: Determine if we still need this if-then-else construct,1
v0.8.6,TODO(travis): once we support GBM text features,1
v0.8.6,TODO(travis): once we support GBM text features,1
v0.8.6,TODO(travis): need unit tests to test the get_embedding_layer() of every encoder to ensure it is,1
v0.8.6,TODO: fix LLM model loading,1
v0.8.6,TODO(arnav): p-tuning and prefix tuning have errors when enabled that seem to stem from DDP:,1
v0.8.6,TODO(Arnav): Re-enable once we can run tests on GPUs,1
v0.8.6,TODO: Re-enable once we can run tests on GPUs,1
v0.8.6,"For a full explanation of this 8-bit workaround, see https://github.com/ludwig-ai/ludwig/pull/3606",1
v0.8.6,"TODO: Uncomment ""filter_for_weight_format()"" method definition and enable its usage once GPU tests are set up.",1
v0.8.6,TODO(shreya): Uncomment when https://github.com/ludwig-ai/ludwig/issues/2039 is fixed.,1
v0.8.6,TODO: remove this workaround when audio preprocessing is fixed.,1
v0.8.6,TODO: these are the only outputs we provide from Torchscript for now,1
v0.8.6,TODO all shadows built in name - come up with a more descriptive name,1
v0.8.6,TODO (Connor): Refactor to use self.config_obj,1
v0.8.6,TODO (ASN): add support for substitute_with_max parameter,1
v0.8.6,TODO(travis): WIP,1
v0.8.6,"TODO(travis): there's a lot of redundancy in this approach, since we are preprocessing the same DataFrame",1
v0.8.6,"TODO(travis): can optimize the preprocessing part here, since we only need to preprocess / predict",1
v0.8.6,TODO(travis): generalize this to support any pandas output format,1
v0.8.6,TODO (Connor): Refactor to use self.config_obj,1
v0.8.6,"TODO: In the future, it may be possible to move up the model type check into the BaseModel class.",1
v0.8.6,TODO: fix for Ray where workers may be of different skus,1
v0.8.6,TODO(travis): remove in favor of on_hyperopt_end for naming consistency,1
v0.8.6,TODO: https://github.com/ludwig-ai/ludwig/issues/2633,1
v0.8.6,todo: revise docstring,1
v0.8.6,todo: assess how to specify padding for equivalent to 'same',1
v0.8.6,todo: determine how to pool_padding equivalent of 'same',1
v0.8.6,todo: fixup docstring,1
v0.8.6,todo: review docstring,1
v0.8.6,todo: fix up docstring,1
v0.8.6,todo: fix up docstring,1
v0.8.6,todo: update docstring as needed,1
v0.8.6,TODO(shreya): Make sure this is updated when FCStack is updated,1
v0.8.6,TODO(Arnav): Remove this once we have reduce_output options set for,1
v0.8.6,TODO(travis): get_hf_config_param_names should be implemented as abstract in HFEncoderConfig,1
v0.8.6,TODO(shreya): Confirm that this is it,1
v0.8.6,TODO(shreya): Confirm that this is it,1
v0.8.6,TODO(justin): This may need to be conditioned on which AutoModel gets chosen.,1
v0.8.6,TODO (ASN): Decide how we want to proceed if at least one trial has,1
v0.8.6,TODO(shreya): Should this hyperopt config param be set here?,1
v0.8.6,TODO (ASN): add image heuristics,1
v0.8.6,"TODO(travis): less hacky way to do this, we should probably allow ModelConfig to be created without output",1
v0.8.6,"todo future: this may be redundant, check",1
v0.8.6,Workaround for including additional tensors from output of input encoders for,1
v0.8.6,TODO(Justin): Think about how to make this communication work for multi-sequence,1
v0.8.6,todo: when https://github.com/ludwig-ai/ludwig/issues/810 is closed,1
v0.8.6,"todo: can we just use projector_size? # hidden_size,",1
v0.8.6,"todo future: this may be redundant, check",1
v0.8.6,"todo future: this may be redundant, check",1
v0.8.6,Workaround for including additional tensors from output of input encoders for,1
v0.8.6,TODO(Justin): Think about how to make this communication work for multi-sequence,1
v0.8.6,todo future: maybe reintroduce these attention function,1
v0.8.6,todo future: maybe reintroduce these attention function,1
v0.8.6,todo future: maybe reintroduce these attention function,1
v0.8.6,TODO(travis): consider moving this behind a general BatchNorm interface to avoid this kludge.,1
v0.8.6,"todo: enumerate for debugging, remove after testing",1
v0.8.6,TODO(shreya): Combine with ResNetBlock by adding a flag.,1
v0.8.6,TODO(shreya): Implement sparse embedding lookup.,1
v0.8.6,# TODO(shreya): Check if this is equivalent,1
v0.8.6,# TODO(shreya): Check if supported in torch,1
v0.8.6,todo: review for generality,1
v0.8.6,TODO: Simplify this.,1
v0.8.6,TODO(travis): make this more general to other cumulative loss functions,1
v0.8.6,Dummy implementation.,1
v0.8.6,"TODO(Justin): Add a confusion matrix, see",1
v0.8.6,TODO: add a mechanism for letting the user decide to save it,1
v0.8.6,todo: re-evaluate need for separate handling of `attention` reducer,1
v0.8.6,TODO(Justin): Clean this up.,1
v0.8.6,TODO: change to debug level before merging,1
v0.8.6,TODO: should this raise an exception if not in training mode?,1
v0.8.6,Nested conditional is a workaround to short-circuit boolean evaluation.,1
v0.8.6,TODO: alternatively use get_average_image() for unreachable images,1
v0.8.6,todo future add multiprocessing/multithreading,1
v0.8.6,TODO(travis): do we even need a user param for vector size if we're going to auto-infer it in all,1
v0.8.6,TODO(1891): Remove backward compatibility hack once all models have been retrained with Ludwig after,1
v0.8.6,todo: refactor to reuse SequenceOutputFeature.postprocess_predictions,1
v0.8.6,TODO: Add a mechanism that lets the user save the full probability distribution if they want.,1
v0.8.6,TODO(geoffrey): add support for Dask DataFrames,1
v0.8.6,TODO(Arnav): see if there's a way to only remove them if the entry does't have quotes. This currently,1
v0.8.6,"removes all "" from the string (even those not added by json.dumps), which is not ideal.",1
v0.8.6,Convert datetime to int64 to workaround Dask limitation,1
v0.8.6,"TODO pyarrow: this is needed for caching to work with pyarrow. if removed, the following error is raised:",1
v0.8.6,TODO(travis): passing in MODEL_ECD is a hack here that can be removed once we move to using,1
v0.8.6,"encoder schema at all. This hack works for now because all encoders are supported by ECD, so",1
v0.8.6,todo figure out if additional parameters are needed,1
v0.8.6,"TODO(travis): instead of using raw dictionary, this should be loaded into a proper PreprocessingConfig",1
v0.8.6,TODO dask: this needs to work with DataFrames,1
v0.8.6,TODO(travis): decouple config from training_set_metadata so we don't need to,1
v0.8.6,TODO(joppe): support out of memory negative sampling using Dask,1
v0.8.6,TODO(travis): revisit in the future to make this more precise,1
v0.8.6,TODO: Add link to windowing docs.,1
v0.8.6,TODO: figure out correct typing for augmentation_pipeline after refactoring is done,1
v0.8.6,TODO: consider removing this. doesn't work currently and read performance seems generally,1
v0.8.6,TODO: convert to debug message when done with development,1
v0.8.6,"TODO(travis): could try hashing the in-memory dataset, but this is tricky for Dask",1
v0.8.6,todo future: reintroduce the bucketed batcher,1
v0.8.6,TODO ray: implement dynamic batch size,1
v0.8.6,TODO: Change annotation to PublicAPI once Ludwig 0.7 is released,1
v0.8.6,TODO(travis): is this redundant with `clipglobalnorm`?,1
v0.8.6,TODO(travis) consider removing this in the future after deprecation period,1
v0.8.6,TODO: use registry pattern for trainers,1
v0.8.6,TODO: Change to RAISE and update descriptions once we want to enforce strict schemas.,1
v0.8.6,TODO: Maybe need to plumb 'required' through here,1
v0.8.6,"TODO(travis): this seems much too verbose, does the validation error not show the specific error?",1
v0.8.6,"TODO(travis): too much boilerplate here, we should find a way to abstract all this and only require specifying the",1
v0.8.6,"TODO(travis): this seems much too verbose, does the validation error not show the specific error?",1
v0.8.6,TODO(#1673): Need some more logic here for validating against output features,1
v0.8.6,"TODO: Re-enable ""goss"" when supported: https://github.com/ludwig-ai/ludwig/issues/2988",1
v0.8.6,TODO(travis): figure out why we need these imports to avoid circular import error,1
v0.8.6,"TODO (Connor): Add nesting logic for fc_layers, see fully_connected_module.py",1
v0.8.6,"TODO (Connor): Add nesting logic for fc_layers, see fully_connected_module.py",1
v0.8.6,TODO(travis): seems like this is not really a valid user option. We should probably just remove these,1
v0.8.6,TODO: uncomment when sentencepiece doesn't cause segfaults: https://github.com/ludwig-ai/ludwig/issues/2983,1
v0.8.6,TODO: uncomment once we figure out host memory issue: https://github.com/ludwig-ai/ludwig/issues/3107,1
v0.8.6,TODO: uncomment when CTRL bug (https://github.com/ludwig-ai/ludwig/issues/2977) has been fixed to add back in,1
v0.8.6,TODO(#1673): Add conditional logic for fields like this one:,1
v0.8.6,"TODO(travis): below type comparison is not perfect, as it doesn't consider the case where the default type",1
v0.8.6,"TODO(travis): explore similar contraints for GBMs, which don't have epochs",1
v0.8.6,"This is not perfect since it includes tokens from both input + output features, but this at least",1
v0.8.6,TODO (Arnav): Figure out how to factor in rope scaling factor into this calculation.,1
v0.8.6,TODO: Add better support for category output features,1
v0.8.6,TODO(travis): handle this with helper function,1
v0.8.6,TODO(travis): not needed once we remove existing model config implementation,1
v0.8.6,TODO(ksbrar): What is this?,1
v0.8.6,"TODO(travis): this should be done through marshmallow dataclass' `required` field param,",1
v0.8.6,TODO(Arnav): Remove the hard check on max_length once we support multiple output features.,1
v0.8.6,TODO(Arnav): Refactor LossDataclassField to only accept loss types that are valid for the model,1
v0.8.6,TODO: Add schema support for Callable,1
v0.8.6,TODO: This should technically be a required paremeter. Do we need to add support for required params?,1
v0.8.6,TODO: Double-check support for this,1
v0.8.6,TODO: Double-check support for this as well as whether Callable args work properly,1
v0.8.6,TODO: create a search alg metadata class to register in place of individual metadata args,1
v0.8.6,TODO: Add a registry mapping string names to nevergrad optimizers,1
v0.8.6,TODO: Add schemas for nevergrad optimizer kwargs,1
v0.8.6,TODO: Add a registry of Optuna samplers schemas,1
v0.8.6,TODO(travis): figure out why calling this `bias` doesn't work,1
v0.8.6,TODO(travis): fix text generation when using prompt tuning:,1
v0.8.6,TODO(travis): fix prefix tuning and p-tuning to work with DDP,1
v0.8.6,TODO(This needs to be defined based on the Constraint class),1
v0.8.6,TODO: This is an unfortunate side-effect of dataclass init order - you cannot have non-default fields follow,1
v0.8.6,todo v0.4: currently not clear way to set model graph,1
v0.8.6,TODO(daniel): delete this.,1
v0.8.6,TODO(travis): figure out a good way to support this. The problem with,1
v0.8.6,"TODO(Justin): This should probably live in on_ludwig_end, once that's implemented.",1
v0.8.6,TODO: need to also include a filename for this figure,1
v0.8.6,"TODO(geoffrey, arnav): uncomment this when we have reconciled the config with the backend kwarg in api.py",1
v0.8.6,"TODO: `prompt` by default should be set to null, not a default dict:",1
v0.8.6,"TODO: retrieval by default should be set to null, not a default dict:",1
v0.8.6,TODO: len(template_refs) is a hacky attempt to check that there are references to *something* in the,1
v0.8.6,TODO(travis): figure out why we need these imports to avoid circular import error,1
v0.8.6,TODO: Replace with more robust required logic later.,1
v0.8.6,TODO(Arnav): Refactor to split into strategies like splitters,1
v0.8.6,"TODO(geoffrey): figure out where self.max_sequence_length is used– if not used, we might consider removing it.",1
v0.8.6,It's confusing to have both this and `max_new_tokens` as a mandatory param in the `forward` function.,1
v0.8.6,TODO(Arnav): Add support for probabilities and logits,1
v0.8.6,"TODO(Arnav): Figure out how to compute logits. For now, we return",1
v0.8.6,Dummy implementation.,1
v0.8.6,TODO: Use a configurable ratio for how often to use teacher forcing during training.,1
v0.8.6,TODO: Use a configurable ratio for how often to use teacher forcing during training.,1
v0.8.6,Dummy implementation.,1
v0.8.6,TODO(Arnav): Re-enable once we add DotProduct Combiner: https://github.com/ludwig-ai/ludwig/issues/3150,1
v0.8.6,"TODO(travis): we assume here that False is always the default, which may not be true. We should dervice",1
v0.8.6,TODO: Check if subscription has expired,1
v0.8.6,"TODO(travis): stopgap solution, we should make it so we don't need to do this",1
v0.8.6,todo (Wael): tests for all types.,1
v0.8.6,todo (Wael): tests for all types.,1
v0.8.6,Still needed for preprocessing  TODO(Connor): Refactor ludwig/data/preprocessing to use schema,1
v0.8.6,"TODO(travis): remove this, make type a protected string for each subclass",1
v0.8.6,"ORRIBLE HACK, IT'S THE ONLY WAY TO REMOVE PADDING",1
v0.8.6,END OF HORRIBLE HACK,1
v0.8.6,"TODO: more research needed on how to handle RayTune ""sample_from"" search space",1
v0.8.6,HACK(geoffrey): gpt2 has no pad token. Recommendation is to use eos token instead.,1
v0.8.6,TODO(geoffrey): can we better validate tokenizer parity before swapping in the TorchText tokenizer?,1
v0.8.6,Workaround for: https://issues.apache.org/jira/browse/ARROW-5645,1
v0.8.6,"HACK(Arnav): gpt, gpt2 and llama tokenizers had no pad tokens.",1
v0.8.6,TODO(travis): do this through an interface rather than conditional logic,1
v0.8.6,TODO(travis): move to cached_property when we drop Python 3.7.,1
v0.8.6,TODO: remove reshaping once https://github.com/microsoft/LightGBM/pull/4925 is released,1
v0.8.6,TODO: remove ravel once https://github.com/microsoft/LightGBM/pull/4925 is released,1
v0.8.6,TODO (jeffkinnison): revert to use the requested device once torch device usage is standardized,1
v0.8.6,TODO(shreya): Confirm types of args,1
v0.8.6,"TODO: when loading an existing model, this loses metric values for all but the last epoch.",1
v0.8.6,TODO(travis): implement imbalance ratio,1
v0.8.6,"TODO (ASN): add other modalities (image, etc. )",1
v0.8.6,"TODO(travis): this assumes ECD is the selected model type, which is not problematic for now, as",1
v0.8.6,"diverge, so we should find a way to remove this. The best solution is to the change the input params from",1
v0.8.6,TODO(travis): consolidate with implementation in data/ray.py,1
v0.8.6,TODO: only single task currently,1
v0.8.6,TODO(travis): include encoder and decoder steps during inference,1
v0.8.6,TODO(Arnav): This needs be more flexible to account for RoPE Scaling,1
v0.8.6,TODO: this implementation will not work if resuming from a previous checkpoint. Need to fix this.,1
v0.8.6,TODO: make this configurable in the future. These parameters are from FastChat:,1
v0.8.6,"TODO: Wrap device_map=""auto"" in a try-except block since it may not be supported for all models (E.g. BertLMHead)  # noqa",1
v0.8.6,TODO: only single task currently,1
v0.8.6,TODO (jeffkinnison): Determine why the 8-bit `SCB` and `CB` matrices are deleted in the forward pass,1
v0.8.6,HACK (Tim): get the device of the targets to transfer self.eval_loss_metric to the same device,1
v0.8.6,HACK (Tim): get the device of the targets to transfer self.eval_loss_metric to the same device,1
v0.8.6,"TODO(Arnav): Seems like doing this again and going between these format types in unnecessary, but",1
v0.8.6,"HACK(geoffrey): we need a non-empty loss, so we just fill it with zeros",1
v0.8.6,TODO(travis): this will need to change when we support multiple output features,1
v0.8.6,"TODO(travis): use the implementation of trainer itself to decide whether to save the model, to",1
v0.8.6,avoid this hack,1
v0.8.6,"TODO: see the ""TODO"" statement from ""LLM.save()"" in this module.",1
v0.8.6,TODO (jeffkinnison): revert to using the requested device for GBMs when device usage is fixed,1
v0.8.6,TODO: with change to misc_utils.set_random_seed() this may be redundant,1
v0.8.6,TODO(Justin): Check that the semantics of input_size align with what the combiner's output shape returns,1
v0.8.6,TODO(justin): Remove dummy implementation. Make input_shape and output_shape functions.,1
v0.8.6,TODO(geoffrey): figure out why self.index.search segfaults with larger batch sizes,1
v0.8.6,TODO(travis): support local feature importance,1
v0.8.6,TODO:,1
v0.8.6,TODO(travis): add back skip encoders at the end in finally. Shouldn't be an issue in most cases as we,1
v0.8.6,"TODO(travis): for force plots, need something similar to SHAP E[X]",1
v0.8.6,"TODO(travis): for force plots, need something similar to SHAP E[X]",1
v0.8.6,"TODO(travis): Consider changing to `if not torch.is_floating_point(t.dtype)` to simplify, then handle bool",1
v0.8.6,TODO(travis): pre-compute this during training from the full training dataset.,1
v0.8.6,"TODO(travis): for force plots, need something similar to SHAP E[X]",1
v0.8.6,"TODO(travis): for force plots, need something similar to SHAP E[X]",1
v0.8.6,"TODO: construct new datasets by running encoders (for text, image)",1
v0.8.6,TODO(shreya): Refactor preprocessing so that this can be moved upstream.,1
v0.8.6,TODO(Arnav): Add support for gradient checkpointing in the compiled model,1
v0.8.6,"For a full explanation of this 8-bit workaround, see https://github.com/ludwig-ai/ludwig/pull/3606",1
v0.8.6,TODO (jeffkinnison): Determine why `SCB` and `CB` are deleted from parameter state,1
v0.8.6,matrices are part of model state. This workaround is necessary because the matrices are,1
v0.8.6,"TODO: Implement batch size tuning for LLM, currently just returns the default batch size",1
v0.8.6,TODO: refactor this into an interface,1
v0.8.6,workaround type limitations of the underlying frameworks,1
v0.8.6,TODO(Arnav): Re-enable in Ray 2.3,1
v0.8.6,"TODO(travis): we should revisit the user format here, as it silently breaks situations",1
v0.8.6,"TODO ray: find an informed way to set the parallelism, in practice",1
v0.8.6,TODO: When this is implemented we also need to update the,1
v0.8.6,TODO(travis): consolidate with implementation in data/ray.py,1
v0.8.6,"This is needed because Daft only supports Dataframes, not Series",1
v0.8.6,TODO(ekl) deprecate this once read fusion is available.,1
v0.8.6,TODO(geoffrey): ensure this subclasses ray.data.datasource.Reader in ray 1.14,1
v0.8.6,TODO(ekl) deprecate this once read fusion is available.,1
v0.8.6,"doesn't work for Snappy, so we double-check ourselves.",1
v0.8.6,TODO(xwjiang): Remove this later.,1
v0.8.6,TODO(travis): need to fix checkpoint saving/loading for DeepSpeed to enable tuning,1
v0.8.6,"TODO(travis): this double-counts on the same device, it should use a cross-communicator instead",1
v0.8.6,"TODO: remove LOCAL_BACKEND as a global constant, replace with singleton LocalBackend.shared_instance().",1
v0.8.6,Temporary workaround to prevent tests from automatically using the Ray backend. Taken from,1
v0.8.6,TODO(travis): add other indexing structures,1
v0.8.6,TODO(travis): open question if this is needed to ensure all workers using same weights,1
v0.8.6,TODO(travis): open question if this is needed to ensure all workers using same optimizer state,1
v0.8.6,"TODO(travis): currently Ray handles this for us, but is subject to hangs if one of the workers raises an",1
v0.8.5,HACK(geoffrey): `hyperopt_resources` is a required resource for hyperopt to prevent deadlocks in Ludwig tests.,1
v0.8.5,TODO(geoffrey): remove for Ray 2.2,1
v0.8.5,"HACK(Arnav): Remove configs that have LARS, LAMB or Lion optimizers, or Paged or 8-bit optimizers.",1
v0.8.5,is there a better way to do this?,1
v0.8.5,todo the hidden output is actually a tensor. May need modification,1
v0.8.5,todo figure out the output size for parallel 1d conv,1
v0.8.5,TODO:  Need variant of assert_model_parameters_updated() to account for the two step calling sequence,1
v0.8.5,todo: remove code,1
v0.8.5,TODO(ksbrar): Circle back after discussing whether additional properties should be allowed long-term.,1
v0.8.5,TODO(ksbrar): Circle back after discussing whether additional properties should be allowed long-term.,1
v0.8.5,TODO(Arnav): Re-enable once https://github.com/ludwig-ai/ludwig/issues/3150 is resolved since the GBM,1
v0.8.5,todo: re-add 'attention' after further research in implication of torch,1
v0.8.5,TODO(#1333): Refactor this test once torch sequence generator work is complete.,1
v0.8.5,"TODO(Justin): Move these to test_sequence_generator unit tests, and reintroduce decoder attention, beam_width, and",1
v0.8.5,TODO(travis): add when we support pretrained text models for gbms,1
v0.8.5,TODO: find a smaller model for testing,1
v0.8.5,TODO: figure out how to get mocks to work with Ray backend,1
v0.8.5,TODO(travis): We may want to expand upon this in the future to include some checks on model,1
v0.8.5,TODO(travis): We may want to expand upon this in the future to include some checks on model,1
v0.8.5,TODO(travis): We may want to expand upon this in the future to include some checks on model,1
v0.8.5,TODO: Determine whether this is desired behavior. Tracked here:,1
v0.8.5,TODO: feature type not yet supported,1
v0.8.5,handled non-determinism when comparing the metrics between the local and Ray backends. We work around this by,1
v0.8.5,TODO: feature type not yet supported,1
v0.8.5,TODO(geoffrey): Fold modin tests into test_ray_image as @pytest.mark.parametrized once tests are optimized,1
v0.8.5,TODO(travis): move this to separate gpu module so we only have one ray cluster running at a time,1
v0.8.5,TODO(travis): https://github.com/ludwig-ai/ludwig/issues/1956,1
v0.8.5,TODO: Determine if we still need this if-then-else construct,1
v0.8.5,TODO(travis): once we support GBM text features,1
v0.8.5,TODO(travis): once we support GBM text features,1
v0.8.5,TODO(travis): need unit tests to test the get_embedding_layer() of every encoder to ensure it is,1
v0.8.5,TODO: fix LLM model loading,1
v0.8.5,TODO(arnav): p-tuning and prefix tuning have errors when enabled that seem to stem from DDP:,1
v0.8.5,TODO(Arnav): Re-enable once we can run tests on GPUs,1
v0.8.5,TODO: Re-enable once we can run tests on GPUs,1
v0.8.5,"For a full explanation of this 8-bit workaround, see https://github.com/ludwig-ai/ludwig/pull/3606",1
v0.8.5,"TODO: Uncomment ""filter_for_weight_format()"" method definition and enable its usage once GPU tests are set up.",1
v0.8.5,TODO(shreya): Uncomment when https://github.com/ludwig-ai/ludwig/issues/2039 is fixed.,1
v0.8.5,TODO: remove this workaround when audio preprocessing is fixed.,1
v0.8.5,TODO: these are the only outputs we provide from Torchscript for now,1
v0.8.5,TODO all shadows built in name - come up with a more descriptive name,1
v0.8.5,TODO (Connor): Refactor to use self.config_obj,1
v0.8.5,TODO (ASN): add support for substitute_with_max parameter,1
v0.8.5,TODO(travis): WIP,1
v0.8.5,"TODO(travis): there's a lot of redundancy in this approach, since we are preprocessing the same DataFrame",1
v0.8.5,"TODO(travis): can optimize the preprocessing part here, since we only need to preprocess / predict",1
v0.8.5,TODO(travis): generalize this to support any pandas output format,1
v0.8.5,TODO (Connor): Refactor to use self.config_obj,1
v0.8.5,"TODO: In the future, it may be possible to move up the model type check into the BaseModel class.",1
v0.8.5,TODO: fix for Ray where workers may be of different skus,1
v0.8.5,TODO(travis): remove in favor of on_hyperopt_end for naming consistency,1
v0.8.5,TODO: https://github.com/ludwig-ai/ludwig/issues/2633,1
v0.8.5,todo: revise docstring,1
v0.8.5,todo: assess how to specify padding for equivalent to 'same',1
v0.8.5,todo: determine how to pool_padding equivalent of 'same',1
v0.8.5,todo: fixup docstring,1
v0.8.5,todo: review docstring,1
v0.8.5,todo: fix up docstring,1
v0.8.5,todo: fix up docstring,1
v0.8.5,todo: update docstring as needed,1
v0.8.5,TODO(shreya): Make sure this is updated when FCStack is updated,1
v0.8.5,TODO(Arnav): Remove this once we have reduce_output options set for,1
v0.8.5,TODO(travis): get_hf_config_param_names should be implemented as abstract in HFEncoderConfig,1
v0.8.5,TODO(shreya): Confirm that this is it,1
v0.8.5,TODO(shreya): Confirm that this is it,1
v0.8.5,TODO(justin): This may need to be conditioned on which AutoModel gets chosen.,1
v0.8.5,TODO (ASN): Decide how we want to proceed if at least one trial has,1
v0.8.5,TODO(shreya): Should this hyperopt config param be set here?,1
v0.8.5,TODO (ASN): add image heuristics,1
v0.8.5,"TODO(travis): less hacky way to do this, we should probably allow ModelConfig to be created without output",1
v0.8.5,"todo future: this may be redundant, check",1
v0.8.5,Workaround for including additional tensors from output of input encoders for,1
v0.8.5,TODO(Justin): Think about how to make this communication work for multi-sequence,1
v0.8.5,todo: when https://github.com/ludwig-ai/ludwig/issues/810 is closed,1
v0.8.5,"todo: can we just use projector_size? # hidden_size,",1
v0.8.5,"todo future: this may be redundant, check",1
v0.8.5,"todo future: this may be redundant, check",1
v0.8.5,Workaround for including additional tensors from output of input encoders for,1
v0.8.5,TODO(Justin): Think about how to make this communication work for multi-sequence,1
v0.8.5,todo future: maybe reintroduce these attention function,1
v0.8.5,todo future: maybe reintroduce these attention function,1
v0.8.5,todo future: maybe reintroduce these attention function,1
v0.8.5,TODO(travis): consider moving this behind a general BatchNorm interface to avoid this kludge.,1
v0.8.5,"todo: enumerate for debugging, remove after testing",1
v0.8.5,TODO(shreya): Combine with ResNetBlock by adding a flag.,1
v0.8.5,TODO(shreya): Implement sparse embedding lookup.,1
v0.8.5,# TODO(shreya): Check if this is equivalent,1
v0.8.5,# TODO(shreya): Check if supported in torch,1
v0.8.5,todo: review for generality,1
v0.8.5,TODO: Simplify this.,1
v0.8.5,TODO(travis): make this more general to other cumulative loss functions,1
v0.8.5,Dummy implementation.,1
v0.8.5,"TODO(Justin): Add a confusion matrix, see",1
v0.8.5,TODO: add a mechanism for letting the user decide to save it,1
v0.8.5,todo: re-evaluate need for separate handling of `attention` reducer,1
v0.8.5,TODO(Justin): Clean this up.,1
v0.8.5,TODO: change to debug level before merging,1
v0.8.5,TODO: should this raise an exception if not in training mode?,1
v0.8.5,Nested conditional is a workaround to short-circuit boolean evaluation.,1
v0.8.5,TODO: alternatively use get_average_image() for unreachable images,1
v0.8.5,todo future add multiprocessing/multithreading,1
v0.8.5,TODO(travis): do we even need a user param for vector size if we're going to auto-infer it in all,1
v0.8.5,TODO(1891): Remove backward compatibility hack once all models have been retrained with Ludwig after,1
v0.8.5,todo: refactor to reuse SequenceOutputFeature.postprocess_predictions,1
v0.8.5,TODO: Add a mechanism that lets the user save the full probability distribution if they want.,1
v0.8.5,TODO(geoffrey): add support for Dask DataFrames,1
v0.8.5,TODO(Arnav): see if there's a way to only remove them if the entry does't have quotes. This currently,1
v0.8.5,"removes all "" from the string (even those not added by json.dumps), which is not ideal.",1
v0.8.5,Convert datetime to int64 to workaround Dask limitation,1
v0.8.5,"TODO pyarrow: this is needed for caching to work with pyarrow. if removed, the following error is raised:",1
v0.8.5,TODO(travis): passing in MODEL_ECD is a hack here that can be removed once we move to using,1
v0.8.5,"encoder schema at all. This hack works for now because all encoders are supported by ECD, so",1
v0.8.5,todo figure out if additional parameters are needed,1
v0.8.5,"TODO(travis): instead of using raw dictionary, this should be loaded into a proper PreprocessingConfig",1
v0.8.5,TODO dask: this needs to work with DataFrames,1
v0.8.5,TODO(travis): decouple config from training_set_metadata so we don't need to,1
v0.8.5,TODO(joppe): support out of memory negative sampling using Dask,1
v0.8.5,TODO(travis): revisit in the future to make this more precise,1
v0.8.5,TODO: Add link to windowing docs.,1
v0.8.5,TODO: figure out correct typing for augmentation_pipeline after refactoring is done,1
v0.8.5,TODO: consider removing this. doesn't work currently and read performance seems generally,1
v0.8.5,TODO: convert to debug message when done with development,1
v0.8.5,"TODO(travis): could try hashing the in-memory dataset, but this is tricky for Dask",1
v0.8.5,todo future: reintroduce the bucketed batcher,1
v0.8.5,TODO ray: implement dynamic batch size,1
v0.8.5,TODO: Change annotation to PublicAPI once Ludwig 0.7 is released,1
v0.8.5,TODO(travis): is this redundant with `clipglobalnorm`?,1
v0.8.5,TODO(travis) consider removing this in the future after deprecation period,1
v0.8.5,TODO: use registry pattern for trainers,1
v0.8.5,TODO: Change to RAISE and update descriptions once we want to enforce strict schemas.,1
v0.8.5,TODO: Maybe need to plumb 'required' through here,1
v0.8.5,"TODO(travis): this seems much too verbose, does the validation error not show the specific error?",1
v0.8.5,"TODO(travis): too much boilerplate here, we should find a way to abstract all this and only require specifying the",1
v0.8.5,"TODO(travis): this seems much too verbose, does the validation error not show the specific error?",1
v0.8.5,TODO(#1673): Need some more logic here for validating against output features,1
v0.8.5,"TODO: Re-enable ""goss"" when supported: https://github.com/ludwig-ai/ludwig/issues/2988",1
v0.8.5,TODO(travis): figure out why we need these imports to avoid circular import error,1
v0.8.5,"TODO (Connor): Add nesting logic for fc_layers, see fully_connected_module.py",1
v0.8.5,"TODO (Connor): Add nesting logic for fc_layers, see fully_connected_module.py",1
v0.8.5,TODO(travis): seems like this is not really a valid user option. We should probably just remove these,1
v0.8.5,TODO: uncomment when sentencepiece doesn't cause segfaults: https://github.com/ludwig-ai/ludwig/issues/2983,1
v0.8.5,TODO: uncomment once we figure out host memory issue: https://github.com/ludwig-ai/ludwig/issues/3107,1
v0.8.5,TODO: uncomment when CTRL bug (https://github.com/ludwig-ai/ludwig/issues/2977) has been fixed to add back in,1
v0.8.5,TODO(#1673): Add conditional logic for fields like this one:,1
v0.8.5,"TODO(travis): below type comparison is not perfect, as it doesn't consider the case where the default type",1
v0.8.5,"TODO(travis): explore similar contraints for GBMs, which don't have epochs",1
v0.8.5,TODO(travis): handle this with helper function,1
v0.8.5,TODO(travis): not needed once we remove existing model config implementation,1
v0.8.5,TODO(ksbrar): What is this?,1
v0.8.5,"TODO(travis): this should be done through marshmallow dataclass' `required` field param,",1
v0.8.5,TODO(Arnav): Remove the hard check on max_length once we support multiple output features.,1
v0.8.5,TODO(Arnav): Refactor LossDataclassField to only accept loss types that are valid for the model,1
v0.8.5,TODO: Add schema support for Callable,1
v0.8.5,TODO: This should technically be a required paremeter. Do we need to add support for required params?,1
v0.8.5,TODO: Double-check support for this,1
v0.8.5,TODO: Double-check support for this as well as whether Callable args work properly,1
v0.8.5,TODO: create a search alg metadata class to register in place of individual metadata args,1
v0.8.5,TODO: Add a registry mapping string names to nevergrad optimizers,1
v0.8.5,TODO: Add schemas for nevergrad optimizer kwargs,1
v0.8.5,TODO: Add a registry of Optuna samplers schemas,1
v0.8.5,TODO(travis): figure out why calling this `bias` doesn't work,1
v0.8.5,TODO(travis): fix text generation when using prompt tuning:,1
v0.8.5,TODO(travis): fix prefix tuning and p-tuning to work with DDP,1
v0.8.5,TODO(This needs to be defined based on the Constraint class),1
v0.8.5,TODO: This is an unfortunate side-effect of dataclass init order - you cannot have non-default fields follow,1
v0.8.5,todo v0.4: currently not clear way to set model graph,1
v0.8.5,TODO(daniel): delete this.,1
v0.8.5,TODO(travis): figure out a good way to support this. The problem with,1
v0.8.5,"TODO(Justin): This should probably live in on_ludwig_end, once that's implemented.",1
v0.8.5,TODO: need to also include a filename for this figure,1
v0.8.5,"TODO(geoffrey, arnav): uncomment this when we have reconciled the config with the backend kwarg in api.py",1
v0.8.5,"TODO: `prompt` by default should be set to null, not a default dict:",1
v0.8.5,"TODO: retrieval by default should be set to null, not a default dict:",1
v0.8.5,TODO: len(template_refs) is a hacky attempt to check that there are references to *something* in the,1
v0.8.5,TODO(travis): figure out why we need these imports to avoid circular import error,1
v0.8.5,TODO: Replace with more robust required logic later.,1
v0.8.5,TODO(Arnav): Refactor to split into strategies like splitters,1
v0.8.5,"TODO(geoffrey): figure out where self.max_sequence_length is used– if not used, we might consider removing it.",1
v0.8.5,It's confusing to have both this and `max_new_tokens` as a mandatory param in the `forward` function.,1
v0.8.5,TODO(Arnav): Add support for probabilities and logits,1
v0.8.5,"TODO(Arnav): Figure out how to compute logits. For now, we return",1
v0.8.5,Dummy implementation.,1
v0.8.5,TODO: Use a configurable ratio for how often to use teacher forcing during training.,1
v0.8.5,TODO: Use a configurable ratio for how often to use teacher forcing during training.,1
v0.8.5,Dummy implementation.,1
v0.8.5,TODO(Arnav): Re-enable once we add DotProduct Combiner: https://github.com/ludwig-ai/ludwig/issues/3150,1
v0.8.5,"TODO(travis): we assume here that False is always the default, which may not be true. We should dervice",1
v0.8.5,"TODO(travis): stopgap solution, we should make it so we don't need to do this",1
v0.8.5,todo (Wael): tests for all types.,1
v0.8.5,todo (Wael): tests for all types.,1
v0.8.5,Still needed for preprocessing  TODO(Connor): Refactor ludwig/data/preprocessing to use schema,1
v0.8.5,"TODO(travis): remove this, make type a protected string for each subclass",1
v0.8.5,"ORRIBLE HACK, IT'S THE ONLY WAY TO REMOVE PADDING",1
v0.8.5,END OF HORRIBLE HACK,1
v0.8.5,"TODO: more research needed on how to handle RayTune ""sample_from"" search space",1
v0.8.5,HACK(geoffrey): gpt2 has no pad token. Recommendation is to use eos token instead.,1
v0.8.5,TODO(geoffrey): can we better validate tokenizer parity before swapping in the TorchText tokenizer?,1
v0.8.5,Workaround for: https://issues.apache.org/jira/browse/ARROW-5645,1
v0.8.5,"HACK(Arnav): gpt, gpt2 and llama tokenizers had no pad tokens.",1
v0.8.5,TODO(travis): do this through an interface rather than conditional logic,1
v0.8.5,TODO(travis): move to cached_property when we drop Python 3.7.,1
v0.8.5,TODO: remove reshaping once https://github.com/microsoft/LightGBM/pull/4925 is released,1
v0.8.5,TODO: remove ravel once https://github.com/microsoft/LightGBM/pull/4925 is released,1
v0.8.5,TODO (jeffkinnison): revert to use the requested device once torch device usage is standardized,1
v0.8.5,TODO(shreya): Confirm types of args,1
v0.8.5,"TODO: when loading an existing model, this loses metric values for all but the last epoch.",1
v0.8.5,TODO(travis): implement imbalance ratio,1
v0.8.5,"TODO (ASN): add other modalities (image, etc. )",1
v0.8.5,"TODO(travis): this assumes ECD is the selected model type, which is not problematic for now, as",1
v0.8.5,"diverge, so we should find a way to remove this. The best solution is to the change the input params from",1
v0.8.5,TODO(travis): consolidate with implementation in data/ray.py,1
v0.8.5,TODO: only single task currently,1
v0.8.5,TODO(travis): include encoder and decoder steps during inference,1
v0.8.5,TODO(Arnav): This needs be more flexible to account for RoPE Scaling,1
v0.8.5,TODO: this implementation will not work if resuming from a previous checkpoint. Need to fix this.,1
v0.8.5,TODO: make this configurable in the future. These parameters are from FastChat:,1
v0.8.5,"TODO: Wrap device_map=""auto"" in a try-except block since it may not be supported for all models (E.g. BertLMHead)  # noqa",1
v0.8.5,TODO: only single task currently,1
v0.8.5,TODO (jeffkinnison): Determine why the 8-bit `SCB` and `CB` matrices are deleted in the forward pass,1
v0.8.5,HACK (Tim): get the device of the targets to transfer self.eval_loss_metric to the same device,1
v0.8.5,HACK (Tim): get the device of the targets to transfer self.eval_loss_metric to the same device,1
v0.8.5,"TODO(Arnav): Seems like doing this again and going between these format types in unnecessary, but",1
v0.8.5,"HACK(geoffrey): we need a non-empty loss, so we just fill it with zeros",1
v0.8.5,TODO(travis): this will need to change when we support multiple output features,1
v0.8.5,"TODO(travis): use the implementation of trainer itself to decide whether to save the model, to",1
v0.8.5,avoid this hack,1
v0.8.5,"TODO: see the ""TODO"" statement from ""LLM.save()"" in this module.",1
v0.8.5,TODO (jeffkinnison): revert to using the requested device for GBMs when device usage is fixed,1
v0.8.5,TODO: with change to misc_utils.set_random_seed() this may be redundant,1
v0.8.5,TODO(Justin): Check that the semantics of input_size align with what the combiner's output shape returns,1
v0.8.5,TODO(justin): Remove dummy implementation. Make input_shape and output_shape functions.,1
v0.8.5,TODO(geoffrey): figure out why self.index.search segfaults with larger batch sizes,1
v0.8.5,TODO(travis): support local feature importance,1
v0.8.5,TODO:,1
v0.8.5,TODO(travis): add back skip encoders at the end in finally. Shouldn't be an issue in most cases as we,1
v0.8.5,"TODO(travis): for force plots, need something similar to SHAP E[X]",1
v0.8.5,"TODO(travis): for force plots, need something similar to SHAP E[X]",1
v0.8.5,"TODO(travis): Consider changing to `if not torch.is_floating_point(t.dtype)` to simplify, then handle bool",1
v0.8.5,TODO(travis): pre-compute this during training from the full training dataset.,1
v0.8.5,"TODO(travis): for force plots, need something similar to SHAP E[X]",1
v0.8.5,"TODO(travis): for force plots, need something similar to SHAP E[X]",1
v0.8.5,"TODO: construct new datasets by running encoders (for text, image)",1
v0.8.5,TODO(shreya): Refactor preprocessing so that this can be moved upstream.,1
v0.8.5,TODO(Arnav): Add support for gradient checkpointing in the compiled model,1
v0.8.5,"For a full explanation of this 8-bit workaround, see https://github.com/ludwig-ai/ludwig/pull/3606",1
v0.8.5,TODO (jeffkinnison): Determine why `SCB` and `CB` are deleted from parameter state,1
v0.8.5,matrices are part of model state. This workaround is necessary because the matrices are,1
v0.8.5,"TODO: Implement batch size tuning for LLM, currently just returns the default batch size",1
v0.8.5,TODO: refactor this into an interface,1
v0.8.5,workaround type limitations of the underlying frameworks,1
v0.8.5,TODO(Arnav): Re-enable in Ray 2.3,1
v0.8.5,"TODO(travis): we should revisit the user format here, as it silently breaks situations",1
v0.8.5,"TODO ray: find an informed way to set the parallelism, in practice",1
v0.8.5,TODO: When this is implemented we also need to update the,1
v0.8.5,TODO(travis): consolidate with implementation in data/ray.py,1
v0.8.5,"This is needed because Daft only supports Dataframes, not Series",1
v0.8.5,TODO(ekl) deprecate this once read fusion is available.,1
v0.8.5,TODO(geoffrey): ensure this subclasses ray.data.datasource.Reader in ray 1.14,1
v0.8.5,TODO(ekl) deprecate this once read fusion is available.,1
v0.8.5,"doesn't work for Snappy, so we double-check ourselves.",1
v0.8.5,TODO(xwjiang): Remove this later.,1
v0.8.5,TODO(travis): need to fix checkpoint saving/loading for DeepSpeed to enable tuning,1
v0.8.5,"TODO(travis): this double-counts on the same device, it should use a cross-communicator instead",1
v0.8.5,"TODO: remove LOCAL_BACKEND as a global constant, replace with singleton LocalBackend.shared_instance().",1
v0.8.5,Temporary workaround to prevent tests from automatically using the Ray backend. Taken from,1
v0.8.5,TODO(travis): add other indexing structures,1
v0.8.5,TODO(travis): open question if this is needed to ensure all workers using same weights,1
v0.8.5,TODO(travis): open question if this is needed to ensure all workers using same optimizer state,1
v0.8.5,"TODO(travis): currently Ray handles this for us, but is subject to hangs if one of the workers raises an",1
v0.8.4,HACK(geoffrey): `hyperopt_resources` is a required resource for hyperopt to prevent deadlocks in Ludwig tests.,1
v0.8.4,TODO(geoffrey): remove for Ray 2.2,1
v0.8.4,"HACK(Arnav): Remove configs that have LARS, LAMB or Lion optimizers, or Paged or 8-bit optimizers.",1
v0.8.4,is there a better way to do this?,1
v0.8.4,todo the hidden output is actually a tensor. May need modification,1
v0.8.4,todo figure out the output size for parallel 1d conv,1
v0.8.4,TODO:  Need variant of assert_model_parameters_updated() to account for the two step calling sequence,1
v0.8.4,todo: remove code,1
v0.8.4,TODO(ksbrar): Circle back after discussing whether additional properties should be allowed long-term.,1
v0.8.4,TODO(ksbrar): Circle back after discussing whether additional properties should be allowed long-term.,1
v0.8.4,TODO(Arnav): Re-enable once https://github.com/ludwig-ai/ludwig/issues/3150 is resolved since the GBM,1
v0.8.4,todo: re-add 'attention' after further research in implication of torch,1
v0.8.4,TODO(#1333): Refactor this test once torch sequence generator work is complete.,1
v0.8.4,"TODO(Justin): Move these to test_sequence_generator unit tests, and reintroduce decoder attention, beam_width, and",1
v0.8.4,TODO(travis): add when we support pretrained text models for gbms,1
v0.8.4,TODO: find a smaller model for testing,1
v0.8.4,TODO: figure out how to get mocks to work with Ray backend,1
v0.8.4,TODO(travis): We may want to expand upon this in the future to include some checks on model,1
v0.8.4,TODO(travis): We may want to expand upon this in the future to include some checks on model,1
v0.8.4,TODO(travis): We may want to expand upon this in the future to include some checks on model,1
v0.8.4,TODO: Determine whether this is desired behavior. Tracked here:,1
v0.8.4,TODO: feature type not yet supported,1
v0.8.4,handled non-determinism when comparing the metrics between the local and Ray backends. We work around this by,1
v0.8.4,TODO: feature type not yet supported,1
v0.8.4,TODO(geoffrey): Fold modin tests into test_ray_image as @pytest.mark.parametrized once tests are optimized,1
v0.8.4,TODO(travis): move this to separate gpu module so we only have one ray cluster running at a time,1
v0.8.4,TODO(travis): https://github.com/ludwig-ai/ludwig/issues/1956,1
v0.8.4,TODO: Determine if we still need this if-then-else construct,1
v0.8.4,TODO(travis): once we support GBM text features,1
v0.8.4,TODO(travis): once we support GBM text features,1
v0.8.4,TODO(travis): need unit tests to test the get_embedding_layer() of every encoder to ensure it is,1
v0.8.4,TODO: fix LLM model loading,1
v0.8.4,TODO(arnav): p-tuning and prefix tuning have errors when enabled that seem to stem from DDP:,1
v0.8.4,TODO(Arnav): Re-enable once we can run tests on GPUs,1
v0.8.4,"For a full explanation of this 8-bit workaround, see https://github.com/ludwig-ai/ludwig/pull/3606",1
v0.8.4,TODO(shreya): Uncomment when https://github.com/ludwig-ai/ludwig/issues/2039 is fixed.,1
v0.8.4,TODO: remove this workaround when audio preprocessing is fixed.,1
v0.8.4,TODO: these are the only outputs we provide from Torchscript for now,1
v0.8.4,TODO all shadows built in name - come up with a more descriptive name,1
v0.8.4,TODO (Connor): Refactor to use self.config_obj,1
v0.8.4,TODO (ASN): add support for substitute_with_max parameter,1
v0.8.4,TODO(travis): WIP,1
v0.8.4,"TODO(travis): there's a lot of redundancy in this approach, since we are preprocessing the same DataFrame",1
v0.8.4,"TODO(travis): can optimize the preprocessing part here, since we only need to preprocess / predict",1
v0.8.4,TODO(travis): generalize this to support any pandas output format,1
v0.8.4,TODO (Connor): Refactor to use self.config_obj,1
v0.8.4,TODO: fix for Ray where workers may be of different skus,1
v0.8.4,TODO(travis): remove in favor of on_hyperopt_end for naming consistency,1
v0.8.4,TODO: https://github.com/ludwig-ai/ludwig/issues/2633,1
v0.8.4,todo: revise docstring,1
v0.8.4,todo: assess how to specify padding for equivalent to 'same',1
v0.8.4,todo: determine how to pool_padding equivalent of 'same',1
v0.8.4,todo: fixup docstring,1
v0.8.4,todo: review docstring,1
v0.8.4,todo: fix up docstring,1
v0.8.4,todo: fix up docstring,1
v0.8.4,todo: update docstring as needed,1
v0.8.4,TODO(shreya): Make sure this is updated when FCStack is updated,1
v0.8.4,TODO(Arnav): Remove this once we have reduce_output options set for,1
v0.8.4,TODO(travis): get_hf_config_param_names should be implemented as abstract in HFEncoderConfig,1
v0.8.4,TODO(shreya): Confirm that this is it,1
v0.8.4,TODO(shreya): Confirm that this is it,1
v0.8.4,TODO(justin): This may need to be conditioned on which AutoModel gets chosen.,1
v0.8.4,TODO (ASN): Decide how we want to proceed if at least one trial has,1
v0.8.4,TODO(shreya): Should this hyperopt config param be set here?,1
v0.8.4,TODO (ASN): add image heuristics,1
v0.8.4,"TODO(travis): less hacky way to do this, we should probably allow ModelConfig to be created without output",1
v0.8.4,"todo future: this may be redundant, check",1
v0.8.4,Workaround for including additional tensors from output of input encoders for,1
v0.8.4,TODO(Justin): Think about how to make this communication work for multi-sequence,1
v0.8.4,todo: when https://github.com/ludwig-ai/ludwig/issues/810 is closed,1
v0.8.4,"todo: can we just use projector_size? # hidden_size,",1
v0.8.4,"todo future: this may be redundant, check",1
v0.8.4,"todo future: this may be redundant, check",1
v0.8.4,Workaround for including additional tensors from output of input encoders for,1
v0.8.4,TODO(Justin): Think about how to make this communication work for multi-sequence,1
v0.8.4,todo future: maybe reintroduce these attention function,1
v0.8.4,todo future: maybe reintroduce these attention function,1
v0.8.4,todo future: maybe reintroduce these attention function,1
v0.8.4,TODO(travis): consider moving this behind a general BatchNorm interface to avoid this kludge.,1
v0.8.4,"todo: enumerate for debugging, remove after testing",1
v0.8.4,TODO(shreya): Combine with ResNetBlock by adding a flag.,1
v0.8.4,TODO(shreya): Implement sparse embedding lookup.,1
v0.8.4,# TODO(shreya): Check if this is equivalent,1
v0.8.4,# TODO(shreya): Check if supported in torch,1
v0.8.4,todo: review for generality,1
v0.8.4,TODO: Simplify this.,1
v0.8.4,TODO(travis): make this more general to other cumulative loss functions,1
v0.8.4,Dummy implementation.,1
v0.8.4,"TODO(Justin): Add a confusion matrix, see",1
v0.8.4,TODO: add a mechanism for letting the user decide to save it,1
v0.8.4,todo: re-evaluate need for separate handling of `attention` reducer,1
v0.8.4,TODO(Justin): Clean this up.,1
v0.8.4,TODO: change to debug level before merging,1
v0.8.4,TODO: should this raise an exception if not in training mode?,1
v0.8.4,Nested conditional is a workaround to short-circuit boolean evaluation.,1
v0.8.4,TODO: alternatively use get_average_image() for unreachable images,1
v0.8.4,todo future add multiprocessing/multithreading,1
v0.8.4,TODO(travis): do we even need a user param for vector size if we're going to auto-infer it in all,1
v0.8.4,TODO(1891): Remove backward compatibility hack once all models have been retrained with Ludwig after,1
v0.8.4,todo: refactor to reuse SequenceOutputFeature.postprocess_predictions,1
v0.8.4,TODO: Add a mechanism that lets the user save the full probability distribution if they want.,1
v0.8.4,TODO(geoffrey): add support for Dask DataFrames,1
v0.8.4,TODO(Arnav): see if there's a way to only remove them if the entry does't have quotes. This currently,1
v0.8.4,"removes all "" from the string (even those not added by json.dumps), which is not ideal.",1
v0.8.4,Convert datetime to int64 to workaround Dask limitation,1
v0.8.4,"TODO pyarrow: this is needed for caching to work with pyarrow. if removed, the following error is raised:",1
v0.8.4,TODO(travis): passing in MODEL_ECD is a hack here that can be removed once we move to using,1
v0.8.4,"encoder schema at all. This hack works for now because all encoders are supported by ECD, so",1
v0.8.4,todo figure out if additional parameters are needed,1
v0.8.4,"TODO(travis): instead of using raw dictionary, this should be loaded into a proper PreprocessingConfig",1
v0.8.4,TODO dask: this needs to work with DataFrames,1
v0.8.4,TODO(travis): decouple config from training_set_metadata so we don't need to,1
v0.8.4,TODO(joppe): support out of memory negative sampling using Dask,1
v0.8.4,TODO(travis): revisit in the future to make this more precise,1
v0.8.4,TODO: Add link to windowing docs.,1
v0.8.4,TODO: figure out correct typing for augmentation_pipeline after refactoring is done,1
v0.8.4,TODO: consider removing this. doesn't work currently and read performance seems generally,1
v0.8.4,TODO: convert to debug message when done with development,1
v0.8.4,"TODO(travis): could try hashing the in-memory dataset, but this is tricky for Dask",1
v0.8.4,todo future: reintroduce the bucketed batcher,1
v0.8.4,TODO ray: implement dynamic batch size,1
v0.8.4,TODO: Change annotation to PublicAPI once Ludwig 0.7 is released,1
v0.8.4,TODO(travis): is this redundant with `clipglobalnorm`?,1
v0.8.4,TODO(travis) consider removing this in the future after deprecation period,1
v0.8.4,TODO: use registry pattern for trainers,1
v0.8.4,TODO: Change to RAISE and update descriptions once we want to enforce strict schemas.,1
v0.8.4,TODO: Maybe need to plumb 'required' through here,1
v0.8.4,"TODO(travis): this seems much too verbose, does the validation error not show the specific error?",1
v0.8.4,"TODO(travis): too much boilerplate here, we should find a way to abstract all this and only require specifying the",1
v0.8.4,"TODO(travis): this seems much too verbose, does the validation error not show the specific error?",1
v0.8.4,TODO(#1673): Need some more logic here for validating against output features,1
v0.8.4,"TODO: Re-enable ""goss"" when supported: https://github.com/ludwig-ai/ludwig/issues/2988",1
v0.8.4,TODO(travis): figure out why we need these imports to avoid circular import error,1
v0.8.4,"TODO (Connor): Add nesting logic for fc_layers, see fully_connected_module.py",1
v0.8.4,"TODO (Connor): Add nesting logic for fc_layers, see fully_connected_module.py",1
v0.8.4,TODO(travis): seems like this is not really a valid user option. We should probably just remove these,1
v0.8.4,TODO: uncomment when sentencepiece doesn't cause segfaults: https://github.com/ludwig-ai/ludwig/issues/2983,1
v0.8.4,TODO: uncomment once we figure out host memory issue: https://github.com/ludwig-ai/ludwig/issues/3107,1
v0.8.4,TODO: uncomment when CTRL bug (https://github.com/ludwig-ai/ludwig/issues/2977) has been fixed to add back in,1
v0.8.4,TODO(#1673): Add conditional logic for fields like this one:,1
v0.8.4,"TODO(travis): below type comparison is not perfect, as it doesn't consider the case where the default type",1
v0.8.4,"TODO(travis): explore similar contraints for GBMs, which don't have epochs",1
v0.8.4,TODO(travis): handle this with helper function,1
v0.8.4,TODO(travis): not needed once we remove existing model config implementation,1
v0.8.4,TODO(ksbrar): What is this?,1
v0.8.4,"TODO(travis): this should be done through marshmallow dataclass' `required` field param,",1
v0.8.4,TODO(Arnav): Remove the hard check on max_length once we support multiple output features.,1
v0.8.4,TODO(Arnav): Refactor LossDataclassField to only accept loss types that are valid for the model,1
v0.8.4,TODO: Add schema support for Callable,1
v0.8.4,TODO: This should technically be a required paremeter. Do we need to add support for required params?,1
v0.8.4,TODO: Double-check support for this,1
v0.8.4,TODO: Double-check support for this as well as whether Callable args work properly,1
v0.8.4,TODO: create a search alg metadata class to register in place of individual metadata args,1
v0.8.4,TODO: Add a registry mapping string names to nevergrad optimizers,1
v0.8.4,TODO: Add schemas for nevergrad optimizer kwargs,1
v0.8.4,TODO: Add a registry of Optuna samplers schemas,1
v0.8.4,TODO(travis): figure out why calling this `bias` doesn't work,1
v0.8.4,TODO(travis): fix text generation when using prompt tuning:,1
v0.8.4,TODO(travis): fix prefix tuning and p-tuning to work with DDP,1
v0.8.4,TODO(This needs to be defined based on the Constraint class),1
v0.8.4,TODO: This is an unfortunate side-effect of dataclass init order - you cannot have non-default fields follow,1
v0.8.4,todo v0.4: currently not clear way to set model graph,1
v0.8.4,TODO(daniel): delete this.,1
v0.8.4,TODO(travis): figure out a good way to support this. The problem with,1
v0.8.4,TODO: need to also include a filename for this figure,1
v0.8.4,"TODO: `prompt` by default should be set to null, not a default dict:",1
v0.8.4,"TODO: retrieval by default should be set to null, not a default dict:",1
v0.8.4,TODO: len(template_refs) is a hacky attempt to check that there are references to *something* in the,1
v0.8.4,TODO(travis): figure out why we need these imports to avoid circular import error,1
v0.8.4,TODO: Replace with more robust required logic later.,1
v0.8.4,TODO(Arnav): Refactor to split into strategies like splitters,1
v0.8.4,TODO(Arnav): Add support for probabilities and logits,1
v0.8.4,"TODO(Arnav): Figure out how to compute logits. For now, we return",1
v0.8.4,Dummy implementation.,1
v0.8.4,TODO: Use a configurable ratio for how often to use teacher forcing during training.,1
v0.8.4,TODO: Use a configurable ratio for how often to use teacher forcing during training.,1
v0.8.4,Dummy implementation.,1
v0.8.4,TODO(Arnav): Re-enable once we add DotProduct Combiner: https://github.com/ludwig-ai/ludwig/issues/3150,1
v0.8.4,"TODO(travis): we assume here that False is always the default, which may not be true. We should dervice",1
v0.8.4,"TODO(travis): stopgap solution, we should make it so we don't need to do this",1
v0.8.4,todo (Wael): tests for all types.,1
v0.8.4,todo (Wael): tests for all types.,1
v0.8.4,Still needed for preprocessing  TODO(Connor): Refactor ludwig/data/preprocessing to use schema,1
v0.8.4,"TODO(travis): remove this, make type a protected string for each subclass",1
v0.8.4,"ORRIBLE HACK, IT'S THE ONLY WAY TO REMOVE PADDING",1
v0.8.4,END OF HORRIBLE HACK,1
v0.8.4,"TODO: more research needed on how to handle RayTune ""sample_from"" search space",1
v0.8.4,HACK(geoffrey): gpt2 has no pad token. Recommendation is to use eos token instead.,1
v0.8.4,TODO(geoffrey): can we better validate tokenizer parity before swapping in the TorchText tokenizer?,1
v0.8.4,Workaround for: https://issues.apache.org/jira/browse/ARROW-5645,1
v0.8.4,"HACK(Arnav): gpt, gpt2 and llama tokenizers had no pad tokens.",1
v0.8.4,TODO(travis): do this through an interface rather than conditional logic,1
v0.8.4,TODO(travis): move to cached_property when we drop Python 3.7.,1
v0.8.4,TODO: remove reshaping once https://github.com/microsoft/LightGBM/pull/4925 is released,1
v0.8.4,TODO: remove ravel once https://github.com/microsoft/LightGBM/pull/4925 is released,1
v0.8.4,TODO (jeffkinnison): revert to use the requested device once torch device usage is standardized,1
v0.8.4,TODO(shreya): Confirm types of args,1
v0.8.4,"TODO: when loading an existing model, this loses metric values for all but the last epoch.",1
v0.8.4,TODO(travis): implement imbalance ratio,1
v0.8.4,"TODO (ASN): add other modalities (image, etc. )",1
v0.8.4,"TODO(travis): this assumes ECD is the selected model type, which is not problematic for now, as",1
v0.8.4,"diverge, so we should find a way to remove this. The best solution is to the change the input params from",1
v0.8.4,TODO(travis): consolidate with implementation in data/ray.py,1
v0.8.4,TODO: only single task currently,1
v0.8.4,TODO(travis): include encoder and decoder steps during inference,1
v0.8.4,TODO(Arnav): This needs be more flexible to account for RoPE Scaling,1
v0.8.4,TODO: this implementation will not work if resuming from a previous checkpoint. Need to fix this.,1
v0.8.4,TODO: make this configurable in the future. These parameters are from FastChat:,1
v0.8.4,"TODO: Wrap device_map=""auto"" in a try-except block since it may not be supported for all models (E.g. BertLMHead)  # noqa",1
v0.8.4,TODO: only single task currently,1
v0.8.4,TODO (jeffkinnison): Determine why the 8-bit `SCB` and `CB` matrices are deleted in the forward pass,1
v0.8.4,HACK (Tim): get the device of the targets to transfer self.eval_loss_metric to the same device,1
v0.8.4,HACK (Tim): get the device of the targets to transfer self.eval_loss_metric to the same device,1
v0.8.4,"TODO(Arnav): Seems like doing this again and going between these format types in unnecessary, but",1
v0.8.4,"HACK(geoffrey): we need a non-empty loss, so we just fill it with zeros",1
v0.8.4,TODO(travis): this will need to change when we support multiple output features,1
v0.8.4,"TODO(travis): use the implementation of trainer itself to decide whether to save the model, to",1
v0.8.4,avoid this hack,1
v0.8.4,TODO (jeffkinnison): revert to using the requested device for GBMs when device usage is fixed,1
v0.8.4,TODO: with change to misc_utils.set_random_seed() this may be redundant,1
v0.8.4,TODO(Justin): Check that the semantics of input_size align with what the combiner's output shape returns,1
v0.8.4,TODO(justin): Remove dummy implementation. Make input_shape and output_shape functions.,1
v0.8.4,TODO(geoffrey): figure out why self.index.search segfaults with larger batch sizes,1
v0.8.4,TODO(travis): support local feature importance,1
v0.8.4,TODO:,1
v0.8.4,TODO(travis): add back skip encoders at the end in finally. Shouldn't be an issue in most cases as we,1
v0.8.4,"TODO(travis): for force plots, need something similar to SHAP E[X]",1
v0.8.4,"TODO(travis): for force plots, need something similar to SHAP E[X]",1
v0.8.4,"TODO(travis): Consider changing to `if not torch.is_floating_point(t.dtype)` to simplify, then handle bool",1
v0.8.4,TODO(travis): pre-compute this during training from the full training dataset.,1
v0.8.4,"TODO(travis): for force plots, need something similar to SHAP E[X]",1
v0.8.4,"TODO(travis): for force plots, need something similar to SHAP E[X]",1
v0.8.4,"TODO: construct new datasets by running encoders (for text, image)",1
v0.8.4,TODO(shreya): Refactor preprocessing so that this can be moved upstream.,1
v0.8.4,TODO(Arnav): Add support for gradient checkpointing in the compiled model,1
v0.8.4,"For a full explanation of this 8-bit workaround, see https://github.com/ludwig-ai/ludwig/pull/3606",1
v0.8.4,TODO (jeffkinnison): Determine why `SCB` and `CB` are deleted from parameter state,1
v0.8.4,matrices are part of model state. This workaround is necessary because the matrices are,1
v0.8.4,"TODO: Implement batch size tuning for LLM, currently just returns the default batch size",1
v0.8.4,TODO: refactor this into an interface,1
v0.8.4,workaround type limitations of the underlying frameworks,1
v0.8.4,TODO(Arnav): Re-enable in Ray 2.3,1
v0.8.4,"TODO(travis): we should revisit the user format here, as it silently breaks situations",1
v0.8.4,"TODO ray: find an informed way to set the parallelism, in practice",1
v0.8.4,TODO: When this is implemented we also need to update the,1
v0.8.4,TODO(travis): consolidate with implementation in data/ray.py,1
v0.8.4,"This is needed because Daft only supports Dataframes, not Series",1
v0.8.4,TODO(ekl) deprecate this once read fusion is available.,1
v0.8.4,TODO(geoffrey): ensure this subclasses ray.data.datasource.Reader in ray 1.14,1
v0.8.4,TODO(ekl) deprecate this once read fusion is available.,1
v0.8.4,"doesn't work for Snappy, so we double-check ourselves.",1
v0.8.4,TODO(xwjiang): Remove this later.,1
v0.8.4,TODO(travis): need to fix checkpoint saving/loading for DeepSpeed to enable tuning,1
v0.8.4,"TODO(travis): this double-counts on the same device, it should use a cross-communicator instead",1
v0.8.4,"TODO: remove LOCAL_BACKEND as a global constant, replace with singleton LocalBackend.shared_instance().",1
v0.8.4,Temporary workaround to prevent tests from automatically using the Ray backend. Taken from,1
v0.8.4,TODO(travis): add other indexing structures,1
v0.8.4,TODO(travis): open question if this is needed to ensure all workers using same weights,1
v0.8.4,TODO(travis): open question if this is needed to ensure all workers using same optimizer state,1
v0.8.4,"TODO(travis): currently Ray handles this for us, but is subject to hangs if one of the workers raises an",1
v0.8.3,HACK(geoffrey): `hyperopt_resources` is a required resource for hyperopt to prevent deadlocks in Ludwig tests.,1
v0.8.3,TODO(geoffrey): remove for Ray 2.2,1
v0.8.3,"HACK(Arnav): Remove configs that have LARS, LAMB or Lion optimizers, or Paged or 8-bit optimizers.",1
v0.8.3,is there a better way to do this?,1
v0.8.3,todo the hidden output is actually a tensor. May need modification,1
v0.8.3,todo figure out the output size for parallel 1d conv,1
v0.8.3,TODO:  Need variant of assert_model_parameters_updated() to account for the two step calling sequence,1
v0.8.3,todo: remove code,1
v0.8.3,TODO(ksbrar): Circle back after discussing whether additional properties should be allowed long-term.,1
v0.8.3,TODO(ksbrar): Circle back after discussing whether additional properties should be allowed long-term.,1
v0.8.3,TODO(Arnav): Re-enable once https://github.com/ludwig-ai/ludwig/issues/3150 is resolved since the GBM,1
v0.8.3,todo: re-add 'attention' after further research in implication of torch,1
v0.8.3,TODO(#1333): Refactor this test once torch sequence generator work is complete.,1
v0.8.3,"TODO(Justin): Move these to test_sequence_generator unit tests, and reintroduce decoder attention, beam_width, and",1
v0.8.3,TODO(travis): add when we support pretrained text models for gbms,1
v0.8.3,TODO: find a smaller model for testing,1
v0.8.3,TODO: figure out how to get mocks to work with Ray backend,1
v0.8.3,TODO(travis): We may want to expand upon this in the future to include some checks on model,1
v0.8.3,TODO(travis): We may want to expand upon this in the future to include some checks on model,1
v0.8.3,TODO(travis): We may want to expand upon this in the future to include some checks on model,1
v0.8.3,TODO: Determine whether this is desired behavior. Tracked here:,1
v0.8.3,TODO: feature type not yet supported,1
v0.8.3,handled non-determinism when comparing the metrics between the local and Ray backends. We work around this by,1
v0.8.3,TODO: feature type not yet supported,1
v0.8.3,TODO(geoffrey): Fold modin tests into test_ray_image as @pytest.mark.parametrized once tests are optimized,1
v0.8.3,TODO(travis): move this to separate gpu module so we only have one ray cluster running at a time,1
v0.8.3,TODO(travis): https://github.com/ludwig-ai/ludwig/issues/1956,1
v0.8.3,TODO: Determine if we still need this if-then-else construct,1
v0.8.3,TODO(travis): once we support GBM text features,1
v0.8.3,TODO(travis): once we support GBM text features,1
v0.8.3,TODO(travis): need unit tests to test the get_embedding_layer() of every encoder to ensure it is,1
v0.8.3,TODO: fix LLM model loading,1
v0.8.3,TODO(arnav): p-tuning and prefix tuning have errors when enabled that seem to stem from DDP:,1
v0.8.3,TODO(Arnav): Re-enable once we can run tests on GPUs,1
v0.8.3,TODO(shreya): Uncomment when https://github.com/ludwig-ai/ludwig/issues/2039 is fixed.,1
v0.8.3,TODO: remove this workaround when audio preprocessing is fixed.,1
v0.8.3,TODO: these are the only outputs we provide from Torchscript for now,1
v0.8.3,TODO all shadows built in name - come up with a more descriptive name,1
v0.8.3,TODO (Connor): Refactor to use self.config_obj,1
v0.8.3,TODO (ASN): add support for substitute_with_max parameter,1
v0.8.3,TODO(travis): WIP,1
v0.8.3,"TODO(travis): there's a lot of redundancy in this approach, since we are preprocessing the same DataFrame",1
v0.8.3,"TODO(travis): can optimize the preprocessing part here, since we only need to preprocess / predict",1
v0.8.3,TODO(travis): generalize this to support any pandas output format,1
v0.8.3,TODO (Connor): Refactor to use self.config_obj,1
v0.8.3,TODO: fix for Ray where workers may be of different skus,1
v0.8.3,TODO(travis): remove in favor of on_hyperopt_end for naming consistency,1
v0.8.3,TODO: https://github.com/ludwig-ai/ludwig/issues/2633,1
v0.8.3,todo: revise docstring,1
v0.8.3,todo: assess how to specify padding for equivalent to 'same',1
v0.8.3,todo: determine how to pool_padding equivalent of 'same',1
v0.8.3,todo: fixup docstring,1
v0.8.3,todo: review docstring,1
v0.8.3,todo: fix up docstring,1
v0.8.3,todo: fix up docstring,1
v0.8.3,todo: update docstring as needed,1
v0.8.3,TODO(shreya): Make sure this is updated when FCStack is updated,1
v0.8.3,TODO(Arnav): Remove this once we have reduce_output options set for,1
v0.8.3,TODO(travis): get_hf_config_param_names should be implemented as abstract in HFEncoderConfig,1
v0.8.3,TODO(shreya): Confirm that this is it,1
v0.8.3,TODO(shreya): Confirm that this is it,1
v0.8.3,TODO(justin): This may need to be conditioned on which AutoModel gets chosen.,1
v0.8.3,TODO (ASN): Decide how we want to proceed if at least one trial has,1
v0.8.3,TODO(shreya): Should this hyperopt config param be set here?,1
v0.8.3,TODO (ASN): add image heuristics,1
v0.8.3,"TODO(travis): less hacky way to do this, we should probably allow ModelConfig to be created without output",1
v0.8.3,"todo future: this may be redundant, check",1
v0.8.3,Workaround for including additional tensors from output of input encoders for,1
v0.8.3,TODO(Justin): Think about how to make this communication work for multi-sequence,1
v0.8.3,todo: when https://github.com/ludwig-ai/ludwig/issues/810 is closed,1
v0.8.3,"todo: can we just use projector_size? # hidden_size,",1
v0.8.3,"todo future: this may be redundant, check",1
v0.8.3,"todo future: this may be redundant, check",1
v0.8.3,Workaround for including additional tensors from output of input encoders for,1
v0.8.3,TODO(Justin): Think about how to make this communication work for multi-sequence,1
v0.8.3,todo future: maybe reintroduce these attention function,1
v0.8.3,todo future: maybe reintroduce these attention function,1
v0.8.3,todo future: maybe reintroduce these attention function,1
v0.8.3,TODO(travis): consider moving this behind a general BatchNorm interface to avoid this kludge.,1
v0.8.3,"todo: enumerate for debugging, remove after testing",1
v0.8.3,TODO(shreya): Combine with ResNetBlock by adding a flag.,1
v0.8.3,TODO(shreya): Implement sparse embedding lookup.,1
v0.8.3,# TODO(shreya): Check if this is equivalent,1
v0.8.3,# TODO(shreya): Check if supported in torch,1
v0.8.3,todo: review for generality,1
v0.8.3,TODO: Simplify this.,1
v0.8.3,TODO(travis): make this more general to other cumulative loss functions,1
v0.8.3,Dummy implementation.,1
v0.8.3,"TODO(Justin): Add a confusion matrix, see",1
v0.8.3,TODO: add a mechanism for letting the user decide to save it,1
v0.8.3,todo: re-evaluate need for separate handling of `attention` reducer,1
v0.8.3,TODO(Justin): Clean this up.,1
v0.8.3,TODO: change to debug level before merging,1
v0.8.3,TODO: should this raise an exception if not in training mode?,1
v0.8.3,Nested conditional is a workaround to short-circuit boolean evaluation.,1
v0.8.3,TODO: alternatively use get_average_image() for unreachable images,1
v0.8.3,todo future add multiprocessing/multithreading,1
v0.8.3,TODO(travis): do we even need a user param for vector size if we're going to auto-infer it in all,1
v0.8.3,TODO(1891): Remove backward compatibility hack once all models have been retrained with Ludwig after,1
v0.8.3,todo: refactor to reuse SequenceOutputFeature.postprocess_predictions,1
v0.8.3,TODO: Add a mechanism that lets the user save the full probability distribution if they want.,1
v0.8.3,TODO(geoffrey): add support for Dask DataFrames,1
v0.8.3,TODO(Arnav): see if there's a way to only remove them if the entry does't have quotes. This currently,1
v0.8.3,"removes all "" from the string (even those not added by json.dumps), which is not ideal.",1
v0.8.3,Convert datetime to int64 to workaround Dask limitation,1
v0.8.3,"TODO pyarrow: this is needed for caching to work with pyarrow. if removed, the following error is raised:",1
v0.8.3,TODO(travis): passing in MODEL_ECD is a hack here that can be removed once we move to using,1
v0.8.3,"encoder schema at all. This hack works for now because all encoders are supported by ECD, so",1
v0.8.3,todo figure out if additional parameters are needed,1
v0.8.3,"TODO(travis): instead of using raw dictionary, this should be loaded into a proper PreprocessingConfig",1
v0.8.3,TODO dask: this needs to work with DataFrames,1
v0.8.3,TODO(travis): decouple config from training_set_metadata so we don't need to,1
v0.8.3,TODO(joppe): support out of memory negative sampling using Dask,1
v0.8.3,TODO(travis): revisit in the future to make this more precise,1
v0.8.3,TODO: Add link to windowing docs.,1
v0.8.3,TODO: figure out correct typing for augmentation_pipeline after refactoring is done,1
v0.8.3,TODO: consider removing this. doesn't work currently and read performance seems generally,1
v0.8.3,TODO: convert to debug message when done with development,1
v0.8.3,"TODO(travis): could try hashing the in-memory dataset, but this is tricky for Dask",1
v0.8.3,todo future: reintroduce the bucketed batcher,1
v0.8.3,TODO ray: implement dynamic batch size,1
v0.8.3,TODO: Change annotation to PublicAPI once Ludwig 0.7 is released,1
v0.8.3,TODO(travis): is this redundant with `clipglobalnorm`?,1
v0.8.3,TODO(travis) consider removing this in the future after deprecation period,1
v0.8.3,TODO: use registry pattern for trainers,1
v0.8.3,TODO: Change to RAISE and update descriptions once we want to enforce strict schemas.,1
v0.8.3,TODO: Maybe need to plumb 'required' through here,1
v0.8.3,"TODO(travis): this seems much too verbose, does the validation error not show the specific error?",1
v0.8.3,"TODO(travis): too much boilerplate here, we should find a way to abstract all this and only require specifying the",1
v0.8.3,"TODO(travis): this seems much too verbose, does the validation error not show the specific error?",1
v0.8.3,TODO(#1673): Need some more logic here for validating against output features,1
v0.8.3,"TODO: Re-enable ""goss"" when supported: https://github.com/ludwig-ai/ludwig/issues/2988",1
v0.8.3,TODO(travis): figure out why we need these imports to avoid circular import error,1
v0.8.3,"TODO (Connor): Add nesting logic for fc_layers, see fully_connected_module.py",1
v0.8.3,"TODO (Connor): Add nesting logic for fc_layers, see fully_connected_module.py",1
v0.8.3,TODO(travis): seems like this is not really a valid user option. We should probably just remove these,1
v0.8.3,TODO: uncomment when sentencepiece doesn't cause segfaults: https://github.com/ludwig-ai/ludwig/issues/2983,1
v0.8.3,TODO: uncomment once we figure out host memory issue: https://github.com/ludwig-ai/ludwig/issues/3107,1
v0.8.3,TODO: uncomment when CTRL bug (https://github.com/ludwig-ai/ludwig/issues/2977) has been fixed to add back in,1
v0.8.3,TODO(#1673): Add conditional logic for fields like this one:,1
v0.8.3,"TODO(travis): below type comparison is not perfect, as it doesn't consider the case where the default type",1
v0.8.3,"TODO(travis): explore similar contraints for GBMs, which don't have epochs",1
v0.8.3,TODO(travis): handle this with helper function,1
v0.8.3,TODO(travis): not needed once we remove existing model config implementation,1
v0.8.3,TODO(ksbrar): What is this?,1
v0.8.3,"TODO(travis): this should be done through marshmallow dataclass' `required` field param,",1
v0.8.3,TODO(Arnav): Remove the hard check on max_length once we support multiple output features.,1
v0.8.3,TODO(Arnav): Refactor LossDataclassField to only accept loss types that are valid for the model,1
v0.8.3,TODO: Add schema support for Callable,1
v0.8.3,TODO: This should technically be a required paremeter. Do we need to add support for required params?,1
v0.8.3,TODO: Double-check support for this,1
v0.8.3,TODO: Double-check support for this as well as whether Callable args work properly,1
v0.8.3,TODO: create a search alg metadata class to register in place of individual metadata args,1
v0.8.3,TODO: Add a registry mapping string names to nevergrad optimizers,1
v0.8.3,TODO: Add schemas for nevergrad optimizer kwargs,1
v0.8.3,TODO: Add a registry of Optuna samplers schemas,1
v0.8.3,TODO(travis): figure out why calling this `bias` doesn't work,1
v0.8.3,TODO(travis): fix text generation when using prompt tuning:,1
v0.8.3,TODO(travis): fix prefix tuning and p-tuning to work with DDP,1
v0.8.3,TODO(This needs to be defined based on the Constraint class),1
v0.8.3,TODO: This is an unfortunate side-effect of dataclass init order - you cannot have non-default fields follow,1
v0.8.3,todo v0.4: currently not clear way to set model graph,1
v0.8.3,TODO(daniel): delete this.,1
v0.8.3,TODO(travis): figure out a good way to support this. The problem with,1
v0.8.3,TODO: need to also include a filename for this figure,1
v0.8.3,"TODO: `prompt` by default should be set to null, not a default dict:",1
v0.8.3,"TODO: retrieval by default should be set to null, not a default dict:",1
v0.8.3,TODO: len(template_refs) is a hacky attempt to check that there are references to *something* in the,1
v0.8.3,TODO(travis): figure out why we need these imports to avoid circular import error,1
v0.8.3,TODO: Replace with more robust required logic later.,1
v0.8.3,TODO(Arnav): Refactor to split into strategies like splitters,1
v0.8.3,TODO(Arnav): Add support for probabilities and logits,1
v0.8.3,"TODO(Arnav): Figure out how to compute logits. For now, we return",1
v0.8.3,Dummy implementation.,1
v0.8.3,TODO: Use a configurable ratio for how often to use teacher forcing during training.,1
v0.8.3,TODO: Use a configurable ratio for how often to use teacher forcing during training.,1
v0.8.3,Dummy implementation.,1
v0.8.3,TODO(Arnav): Re-enable once we add DotProduct Combiner: https://github.com/ludwig-ai/ludwig/issues/3150,1
v0.8.3,"TODO(travis): we assume here that False is always the default, which may not be true. We should dervice",1
v0.8.3,"HACK: Llama fast tokenizer takes about 2-4 minutes to load, so we disable it for now.",1
v0.8.3,"TODO(travis): stopgap solution, we should make it so we don't need to do this",1
v0.8.3,todo (Wael): tests for all types.,1
v0.8.3,todo (Wael): tests for all types.,1
v0.8.3,Still needed for preprocessing  TODO(Connor): Refactor ludwig/data/preprocessing to use schema,1
v0.8.3,"TODO(travis): remove this, make type a protected string for each subclass",1
v0.8.3,"ORRIBLE HACK, IT'S THE ONLY WAY TO REMOVE PADDING",1
v0.8.3,END OF HORRIBLE HACK,1
v0.8.3,"TODO: more research needed on how to handle RayTune ""sample_from"" search space",1
v0.8.3,HACK(geoffrey): gpt2 has no pad token. Recommendation is to use eos token instead.,1
v0.8.3,TODO(geoffrey): can we better validate tokenizer parity before swapping in the TorchText tokenizer?,1
v0.8.3,Workaround for: https://issues.apache.org/jira/browse/ARROW-5645,1
v0.8.3,"HACK(Arnav): gpt, gpt2 and llama tokenizers had no pad tokens.",1
v0.8.3,TODO(travis): do this through an interface rather than conditional logic,1
v0.8.3,TODO(travis): move to cached_property when we drop Python 3.7.,1
v0.8.3,TODO: remove reshaping once https://github.com/microsoft/LightGBM/pull/4925 is released,1
v0.8.3,TODO: remove ravel once https://github.com/microsoft/LightGBM/pull/4925 is released,1
v0.8.3,TODO (jeffkinnison): revert to use the requested device once torch device usage is standardized,1
v0.8.3,TODO(shreya): Confirm types of args,1
v0.8.3,"TODO: when loading an existing model, this loses metric values for all but the last epoch.",1
v0.8.3,TODO(travis): implement imbalance ratio,1
v0.8.3,"TODO (ASN): add other modalities (image, etc. )",1
v0.8.3,"TODO(travis): this assumes ECD is the selected model type, which is not problematic for now, as",1
v0.8.3,"diverge, so we should find a way to remove this. The best solution is to the change the input params from",1
v0.8.3,TODO(travis): consolidate with implementation in data/ray.py,1
v0.8.3,TODO: only single task currently,1
v0.8.3,TODO(travis): include encoder and decoder steps during inference,1
v0.8.3,TODO(Arnav): This needs be more flexible to account for RoPE Scaling,1
v0.8.3,"HACK: Llama fast tokenizer takes about 2-4 minutes to load, so we disable it for now.",1
v0.8.3,TODO: this implementation will not work if resuming from a previous checkpoint. Need to fix this.,1
v0.8.3,TODO: make this configurable in the future. These parameters are from FastChat:,1
v0.8.3,"TODO: Wrap device_map=""auto"" in a try-except block since it may not be supported for all models (E.g. BertLMHead)  # noqa",1
v0.8.3,TODO: only single task currently,1
v0.8.3,HACK (Tim): get the device of the targets to transfer self.eval_loss_metric to the same device,1
v0.8.3,HACK (Tim): get the device of the targets to transfer self.eval_loss_metric to the same device,1
v0.8.3,"TODO(Arnav): Seems like doing this again and going between these format types in unnecessary, but",1
v0.8.3,"HACK(geoffrey): we need a non-empty loss, so we just fill it with zeros",1
v0.8.3,TODO(travis): this will need to change when we support multiple output features,1
v0.8.3,"TODO(travis): use the implementation of trainer itself to decide whether to save the model, to",1
v0.8.3,avoid this hack,1
v0.8.3,TODO (jeffkinnison): revert to using the requested device for GBMs when device usage is fixed,1
v0.8.3,TODO: with change to misc_utils.set_random_seed() this may be redundant,1
v0.8.3,TODO(Justin): Check that the semantics of input_size align with what the combiner's output shape returns,1
v0.8.3,TODO(justin): Remove dummy implementation. Make input_shape and output_shape functions.,1
v0.8.3,TODO(geoffrey): figure out why self.index.search segfaults with larger batch sizes,1
v0.8.3,TODO(travis): support local feature importance,1
v0.8.3,TODO:,1
v0.8.3,TODO(travis): add back skip encoders at the end in finally. Shouldn't be an issue in most cases as we,1
v0.8.3,"TODO(travis): for force plots, need something similar to SHAP E[X]",1
v0.8.3,"TODO(travis): for force plots, need something similar to SHAP E[X]",1
v0.8.3,"TODO(travis): Consider changing to `if not torch.is_floating_point(t.dtype)` to simplify, then handle bool",1
v0.8.3,TODO(travis): pre-compute this during training from the full training dataset.,1
v0.8.3,"TODO(travis): for force plots, need something similar to SHAP E[X]",1
v0.8.3,"TODO(travis): for force plots, need something similar to SHAP E[X]",1
v0.8.3,"TODO: construct new datasets by running encoders (for text, image)",1
v0.8.3,TODO(shreya): Refactor preprocessing so that this can be moved upstream.,1
v0.8.3,"TODO: Implement batch size tuning for LLM, currently just returns the default batch size",1
v0.8.3,TODO: refactor this into an interface,1
v0.8.3,workaround type limitations of the underlying frameworks,1
v0.8.3,TODO(Arnav): Re-enable in Ray 2.3,1
v0.8.3,"TODO(travis): we should revisit the user format here, as it silently breaks situations",1
v0.8.3,"TODO ray: find an informed way to set the parallelism, in practice",1
v0.8.3,TODO: When this is implemented we also need to update the,1
v0.8.3,TODO(travis): consolidate with implementation in data/ray.py,1
v0.8.3,"This is needed because Daft only supports Dataframes, not Series",1
v0.8.3,TODO(ekl) deprecate this once read fusion is available.,1
v0.8.3,TODO(geoffrey): ensure this subclasses ray.data.datasource.Reader in ray 1.14,1
v0.8.3,TODO(ekl) deprecate this once read fusion is available.,1
v0.8.3,"doesn't work for Snappy, so we double-check ourselves.",1
v0.8.3,TODO(xwjiang): Remove this later.,1
v0.8.3,TODO(travis): need to fix checkpoint saving/loading for DeepSpeed to enable tuning,1
v0.8.3,"TODO(travis): this double-counts on the same device, it should use a cross-communicator instead",1
v0.8.3,"TODO: remove LOCAL_BACKEND as a global constant, replace with singleton LocalBackend.shared_instance().",1
v0.8.3,Temporary workaround to prevent tests from automatically using the Ray backend. Taken from,1
v0.8.3,TODO(travis): add other indexing structures,1
v0.8.3,TODO(travis): open question if this is needed to ensure all workers using same weights,1
v0.8.3,TODO(travis): open question if this is needed to ensure all workers using same optimizer state,1
v0.8.3,"TODO(travis): currently Ray handles this for us, but is subject to hangs if one of the workers raises an",1
v0.8.2,HACK(geoffrey): `hyperopt_resources` is a required resource for hyperopt to prevent deadlocks in Ludwig tests.,1
v0.8.2,TODO(geoffrey): remove for Ray 2.2,1
v0.8.2,is there a better way to do this?,1
v0.8.2,todo the hidden output is actually a tensor. May need modification,1
v0.8.2,todo figure out the output size for parallel 1d conv,1
v0.8.2,TODO:  Need variant of assert_model_parameters_updated() to account for the two step calling sequence,1
v0.8.2,todo: remove code,1
v0.8.2,TODO(ksbrar): Circle back after discussing whether additional properties should be allowed long-term.,1
v0.8.2,TODO(ksbrar): Circle back after discussing whether additional properties should be allowed long-term.,1
v0.8.2,TODO(Arnav): Re-enable once https://github.com/ludwig-ai/ludwig/issues/3150 is resolved since the GBM,1
v0.8.2,todo: re-add 'attention' after further research in implication of torch,1
v0.8.2,TODO(#1333): Refactor this test once torch sequence generator work is complete.,1
v0.8.2,"TODO(Justin): Move these to test_sequence_generator unit tests, and reintroduce decoder attention, beam_width, and",1
v0.8.2,TODO(travis): add when we support pretrained text models for gbms,1
v0.8.2,TODO: find a smaller model for testing,1
v0.8.2,TODO: figure out how to get mocks to work with Ray backend,1
v0.8.2,TODO(travis): We may want to expand upon this in the future to include some checks on model,1
v0.8.2,TODO(travis): We may want to expand upon this in the future to include some checks on model,1
v0.8.2,TODO(travis): We may want to expand upon this in the future to include some checks on model,1
v0.8.2,TODO: Determine whether this is desired behavior. Tracked here:,1
v0.8.2,TODO: feature type not yet supported,1
v0.8.2,handled non-determinism when comparing the metrics between the local and Ray backends. We work around this by,1
v0.8.2,TODO: feature type not yet supported,1
v0.8.2,TODO(geoffrey): Fold modin tests into test_ray_image as @pytest.mark.parametrized once tests are optimized,1
v0.8.2,TODO(travis): move this to separate gpu module so we only have one ray cluster running at a time,1
v0.8.2,TODO(travis): https://github.com/ludwig-ai/ludwig/issues/1956,1
v0.8.2,TODO: Determine if we still need this if-then-else construct,1
v0.8.2,TODO(travis): once we support GBM text features,1
v0.8.2,TODO(travis): once we support GBM text features,1
v0.8.2,TODO(travis): need unit tests to test the get_embedding_layer() of every encoder to ensure it is,1
v0.8.2,TODO: fix LLM model loading,1
v0.8.2,TODO(arnav): p-tuning and prefix tuning have errors when enabled that seem to stem from DDP:,1
v0.8.2,TODO(Arnav): Re-enable once we can run tests on GPUs,1
v0.8.2,TODO(shreya): Uncomment when https://github.com/ludwig-ai/ludwig/issues/2039 is fixed.,1
v0.8.2,TODO: remove this workaround when audio preprocessing is fixed.,1
v0.8.2,TODO: these are the only outputs we provide from Torchscript for now,1
v0.8.2,TODO all shadows built in name - come up with a more descriptive name,1
v0.8.2,TODO (Connor): Refactor to use self.config_obj,1
v0.8.2,TODO (ASN): add support for substitute_with_max parameter,1
v0.8.2,TODO(travis): WIP,1
v0.8.2,"TODO(travis): there's a lot of redundancy in this approach, since we are preprocessing the same DataFrame",1
v0.8.2,"TODO(travis): can optimize the preprocessing part here, since we only need to preprocess / predict",1
v0.8.2,TODO(travis): generalize this to support any pandas output format,1
v0.8.2,TODO (Connor): Refactor to use self.config_obj,1
v0.8.2,TODO: fix for Ray where workers may be of different skus,1
v0.8.2,TODO(travis): remove in favor of on_hyperopt_end for naming consistency,1
v0.8.2,TODO: https://github.com/ludwig-ai/ludwig/issues/2633,1
v0.8.2,todo: revise docstring,1
v0.8.2,todo: assess how to specify padding for equivalent to 'same',1
v0.8.2,todo: determine how to pool_padding equivalent of 'same',1
v0.8.2,todo: fixup docstring,1
v0.8.2,todo: review docstring,1
v0.8.2,todo: fix up docstring,1
v0.8.2,todo: fix up docstring,1
v0.8.2,todo: update docstring as needed,1
v0.8.2,TODO(shreya): Make sure this is updated when FCStack is updated,1
v0.8.2,TODO(Arnav): Remove this once we have reduce_output options set for,1
v0.8.2,TODO(travis): get_hf_config_param_names should be implemented as abstract in HFEncoderConfig,1
v0.8.2,TODO(shreya): Confirm that this is it,1
v0.8.2,TODO(shreya): Confirm that this is it,1
v0.8.2,TODO(justin): This may need to be conditioned on which AutoModel gets chosen.,1
v0.8.2,TODO (ASN): Decide how we want to proceed if at least one trial has,1
v0.8.2,TODO(shreya): Should this hyperopt config param be set here?,1
v0.8.2,TODO (ASN): add image heuristics,1
v0.8.2,"TODO(travis): less hacky way to do this, we should probably allow ModelConfig to be created without output",1
v0.8.2,"todo future: this may be redundant, check",1
v0.8.2,Workaround for including additional tensors from output of input encoders for,1
v0.8.2,TODO(Justin): Think about how to make this communication work for multi-sequence,1
v0.8.2,todo: when https://github.com/ludwig-ai/ludwig/issues/810 is closed,1
v0.8.2,"todo: can we just use projector_size? # hidden_size,",1
v0.8.2,"todo future: this may be redundant, check",1
v0.8.2,"todo future: this may be redundant, check",1
v0.8.2,Workaround for including additional tensors from output of input encoders for,1
v0.8.2,TODO(Justin): Think about how to make this communication work for multi-sequence,1
v0.8.2,todo future: maybe reintroduce these attention function,1
v0.8.2,todo future: maybe reintroduce these attention function,1
v0.8.2,todo future: maybe reintroduce these attention function,1
v0.8.2,TODO(travis): consider moving this behind a general BatchNorm interface to avoid this kludge.,1
v0.8.2,"todo: enumerate for debugging, remove after testing",1
v0.8.2,TODO(shreya): Combine with ResNetBlock by adding a flag.,1
v0.8.2,TODO(shreya): Implement sparse embedding lookup.,1
v0.8.2,# TODO(shreya): Check if this is equivalent,1
v0.8.2,# TODO(shreya): Check if supported in torch,1
v0.8.2,todo: review for generality,1
v0.8.2,TODO: Simplify this.,1
v0.8.2,TODO(travis): make this more general to other cumulative loss functions,1
v0.8.2,Dummy implementation.,1
v0.8.2,"TODO(Justin): Add a confusion matrix, see",1
v0.8.2,TODO: add a mechanism for letting the user decide to save it,1
v0.8.2,todo: re-evaluate need for separate handling of `attention` reducer,1
v0.8.2,TODO(Justin): Clean this up.,1
v0.8.2,TODO: change to debug level before merging,1
v0.8.2,TODO: should this raise an exception if not in training mode?,1
v0.8.2,Nested conditional is a workaround to short-circuit boolean evaluation.,1
v0.8.2,TODO: alternatively use get_average_image() for unreachable images,1
v0.8.2,todo future add multiprocessing/multithreading,1
v0.8.2,TODO(travis): do we even need a user param for vector size if we're going to auto-infer it in all,1
v0.8.2,TODO(1891): Remove backward compatibility hack once all models have been retrained with Ludwig after,1
v0.8.2,todo: refactor to reuse SequenceOutputFeature.postprocess_predictions,1
v0.8.2,TODO: Add a mechanism that lets the user save the full probability distribution if they want.,1
v0.8.2,TODO(geoffrey): add support for Dask DataFrames,1
v0.8.2,TODO(Arnav): see if there's a way to only remove them if the entry does't have quotes. This currently,1
v0.8.2,"removes all "" from the string (even those not added by json.dumps), which is not ideal.",1
v0.8.2,Convert datetime to int64 to workaround Dask limitation,1
v0.8.2,"TODO pyarrow: this is needed for caching to work with pyarrow. if removed, the following error is raised:",1
v0.8.2,TODO(travis): passing in MODEL_ECD is a hack here that can be removed once we move to using,1
v0.8.2,"encoder schema at all. This hack works for now because all encoders are supported by ECD, so",1
v0.8.2,todo figure out if additional parameters are needed,1
v0.8.2,"TODO(travis): instead of using raw dictionary, this should be loaded into a proper PreprocessingConfig",1
v0.8.2,TODO dask: this needs to work with DataFrames,1
v0.8.2,TODO(travis): decouple config from training_set_metadata so we don't need to,1
v0.8.2,TODO(joppe): support out of memory negative sampling using Dask,1
v0.8.2,TODO(travis): revisit in the future to make this more precise,1
v0.8.2,TODO: Add link to windowing docs.,1
v0.8.2,TODO: figure out correct typing for augmentation_pipeline after refactoring is done,1
v0.8.2,TODO: consider removing this. doesn't work currently and read performance seems generally,1
v0.8.2,TODO: convert to debug message when done with development,1
v0.8.2,"TODO(travis): could try hashing the in-memory dataset, but this is tricky for Dask",1
v0.8.2,todo future: reintroduce the bucketed batcher,1
v0.8.2,TODO ray: implement dynamic batch size,1
v0.8.2,TODO: Change annotation to PublicAPI once Ludwig 0.7 is released,1
v0.8.2,TODO(travis): is this redundant with `clipglobalnorm`?,1
v0.8.2,TODO(travis) consider removing this in the future after deprecation period,1
v0.8.2,TODO: use registry pattern for trainers,1
v0.8.2,TODO: Change to RAISE and update descriptions once we want to enforce strict schemas.,1
v0.8.2,TODO: Maybe need to plumb 'required' through here,1
v0.8.2,"TODO(travis): this seems much too verbose, does the validation error not show the specific error?",1
v0.8.2,"TODO(travis): too much boilerplate here, we should find a way to abstract all this and only require specifying the",1
v0.8.2,"TODO(travis): this seems much too verbose, does the validation error not show the specific error?",1
v0.8.2,TODO(#1673): Need some more logic here for validating against output features,1
v0.8.2,"TODO: Re-enable ""goss"" when supported: https://github.com/ludwig-ai/ludwig/issues/2988",1
v0.8.2,TODO(travis): figure out why we need these imports to avoid circular import error,1
v0.8.2,"TODO (Connor): Add nesting logic for fc_layers, see fully_connected_module.py",1
v0.8.2,"TODO (Connor): Add nesting logic for fc_layers, see fully_connected_module.py",1
v0.8.2,TODO(travis): seems like this is not really a valid user option. We should probably just remove these,1
v0.8.2,TODO: uncomment when sentencepiece doesn't cause segfaults: https://github.com/ludwig-ai/ludwig/issues/2983,1
v0.8.2,TODO: uncomment once we figure out host memory issue: https://github.com/ludwig-ai/ludwig/issues/3107,1
v0.8.2,TODO: uncomment when CTRL bug (https://github.com/ludwig-ai/ludwig/issues/2977) has been fixed to add back in,1
v0.8.2,TODO(#1673): Add conditional logic for fields like this one:,1
v0.8.2,"TODO(travis): below type comparison is not perfect, as it doesn't consider the case where the default type",1
v0.8.2,"TODO(travis): explore similar contraints for GBMs, which don't have epochs",1
v0.8.2,TODO(travis): handle this with helper function,1
v0.8.2,TODO(travis): not needed once we remove existing model config implementation,1
v0.8.2,TODO(ksbrar): What is this?,1
v0.8.2,"TODO(travis): this should be done through marshmallow dataclass' `required` field param,",1
v0.8.2,TODO(Arnav): Remove the hard check on max_length once we support multiple output features.,1
v0.8.2,TODO(Arnav): Refactor LossDataclassField to only accept loss types that are valid for the model,1
v0.8.2,TODO: Add schema support for Callable,1
v0.8.2,TODO: This should technically be a required paremeter. Do we need to add support for required params?,1
v0.8.2,TODO: Double-check support for this,1
v0.8.2,TODO: Double-check support for this as well as whether Callable args work properly,1
v0.8.2,TODO: create a search alg metadata class to register in place of individual metadata args,1
v0.8.2,TODO: Add a registry mapping string names to nevergrad optimizers,1
v0.8.2,TODO: Add schemas for nevergrad optimizer kwargs,1
v0.8.2,TODO: Add a registry of Optuna samplers schemas,1
v0.8.2,TODO(travis): figure out why calling this `bias` doesn't work,1
v0.8.2,TODO(travis): fix text generation when using prompt tuning:,1
v0.8.2,TODO(travis): fix prefix tuning and p-tuning to work with DDP,1
v0.8.2,TODO(This needs to be defined based on the Constraint class),1
v0.8.2,TODO: This is an unfortunate side-effect of dataclass init order - you cannot have non-default fields follow,1
v0.8.2,todo v0.4: currently not clear way to set model graph,1
v0.8.2,TODO(daniel): delete this.,1
v0.8.2,TODO(travis): figure out a good way to support this. The problem with,1
v0.8.2,TODO: need to also include a filename for this figure,1
v0.8.2,TODO(travis): figure out why we need these imports to avoid circular import error,1
v0.8.2,TODO: Replace with more robust required logic later.,1
v0.8.2,TODO(Arnav): Refactor to split into strategies like splitters,1
v0.8.2,TODO(Arnav): Add support for probabilities and logits,1
v0.8.2,"TODO(Arnav): Figure out how to compute logits. For now, we return",1
v0.8.2,Dummy implementation.,1
v0.8.2,TODO: Use a configurable ratio for how often to use teacher forcing during training.,1
v0.8.2,TODO: Use a configurable ratio for how often to use teacher forcing during training.,1
v0.8.2,Dummy implementation.,1
v0.8.2,TODO(Arnav): Re-enable once we add DotProduct Combiner: https://github.com/ludwig-ai/ludwig/issues/3150,1
v0.8.2,"TODO(travis): we assume here that False is always the default, which may not be true. We should dervice",1
v0.8.2,"HACK: Llama fast tokenizer takes about 2-4 minutes to load, so we disable it for now.",1
v0.8.2,"TODO(travis): stopgap solution, we should make it so we don't need to do this",1
v0.8.2,todo (Wael): tests for all types.,1
v0.8.2,todo (Wael): tests for all types.,1
v0.8.2,Still needed for preprocessing  TODO(Connor): Refactor ludwig/data/preprocessing to use schema,1
v0.8.2,"TODO(travis): remove this, make type a protected string for each subclass",1
v0.8.2,"ORRIBLE HACK, IT'S THE ONLY WAY TO REMOVE PADDING",1
v0.8.2,END OF HORRIBLE HACK,1
v0.8.2,"TODO: more research needed on how to handle RayTune ""sample_from"" search space",1
v0.8.2,HACK(geoffrey): gpt2 has no pad token. Recommendation is to use eos token instead.,1
v0.8.2,TODO(geoffrey): can we better validate tokenizer parity before swapping in the TorchText tokenizer?,1
v0.8.2,Workaround for: https://issues.apache.org/jira/browse/ARROW-5645,1
v0.8.2,"HACK(Arnav): gpt, gpt2 and llama tokenizers had no pad tokens.",1
v0.8.2,TODO(travis): do this through an interface rather than conditional logic,1
v0.8.2,TODO(travis): move to cached_property when we drop Python 3.7.,1
v0.8.2,TODO: remove reshaping once https://github.com/microsoft/LightGBM/pull/4925 is released,1
v0.8.2,TODO: remove ravel once https://github.com/microsoft/LightGBM/pull/4925 is released,1
v0.8.2,TODO (jeffkinnison): revert to use the requested device once torch device usage is standardized,1
v0.8.2,TODO(shreya): Confirm types of args,1
v0.8.2,"TODO: when loading an existing model, this loses metric values for all but the last epoch.",1
v0.8.2,TODO(travis): implement imbalance ratio,1
v0.8.2,"TODO (ASN): add other modalities (image, etc. )",1
v0.8.2,"TODO(travis): this assumes ECD is the selected model type, which is not problematic for now, as",1
v0.8.2,"diverge, so we should find a way to remove this. The best solution is to the change the input params from",1
v0.8.2,TODO(travis): consolidate with implementation in data/ray.py,1
v0.8.2,TODO: only single task currently,1
v0.8.2,TODO(travis): include encoder and decoder steps during inference,1
v0.8.2,TODO(Arnav): This needs be more flexible to account for RoPE Scaling,1
v0.8.2,"HACK: Llama fast tokenizer takes about 2-4 minutes to load, so we disable it for now.",1
v0.8.2,TODO: this implementation will not work if resuming from a previous checkpoint. Need to fix this.,1
v0.8.2,TODO: make this configurable in the future. These parameters are from FastChat:,1
v0.8.2,"TODO: Wrap device_map=""auto"" in a try-except block since it may not be supported for all models (E.g. BertLMHead)  # noqa",1
v0.8.2,TODO: only single task currently,1
v0.8.2,HACK (Tim): get the device of the targets to transfer self.eval_loss_metric to the same device,1
v0.8.2,HACK (Tim): get the device of the targets to transfer self.eval_loss_metric to the same device,1
v0.8.2,"TODO(Arnav): Seems like doing this again and going between these format types in unnecessary, but",1
v0.8.2,"HACK(geoffrey): we need a non-empty loss, so we just fill it with zeros",1
v0.8.2,TODO(travis): this will need to change when we support multiple output features,1
v0.8.2,"TODO(travis): use the implementation of trainer itself to decide whether to save the model, to",1
v0.8.2,avoid this hack,1
v0.8.2,TODO (jeffkinnison): revert to using the requested device for GBMs when device usage is fixed,1
v0.8.2,TODO: with change to misc_utils.set_random_seed() this may be redundant,1
v0.8.2,TODO(Justin): Check that the semantics of input_size align with what the combiner's output shape returns,1
v0.8.2,TODO(justin): Remove dummy implementation. Make input_shape and output_shape functions.,1
v0.8.2,TODO(geoffrey): figure out why self.index.search segfaults with larger batch sizes,1
v0.8.2,TODO(travis): support local feature importance,1
v0.8.2,TODO:,1
v0.8.2,TODO(travis): add back skip encoders at the end in finally. Shouldn't be an issue in most cases as we,1
v0.8.2,"TODO(travis): for force plots, need something similar to SHAP E[X]",1
v0.8.2,"TODO(travis): for force plots, need something similar to SHAP E[X]",1
v0.8.2,"TODO(travis): Consider changing to `if not torch.is_floating_point(t.dtype)` to simplify, then handle bool",1
v0.8.2,TODO(travis): pre-compute this during training from the full training dataset.,1
v0.8.2,"TODO(travis): for force plots, need something similar to SHAP E[X]",1
v0.8.2,"TODO(travis): for force plots, need something similar to SHAP E[X]",1
v0.8.2,"TODO: construct new datasets by running encoders (for text, image)",1
v0.8.2,TODO(shreya): Refactor preprocessing so that this can be moved upstream.,1
v0.8.2,"TODO: Implement batch size tuning for LLM, currently just returns the default batch size",1
v0.8.2,TODO: refactor this into an interface,1
v0.8.2,workaround type limitations of the underlying frameworks,1
v0.8.2,TODO(Arnav): Re-enable in Ray 2.3,1
v0.8.2,"TODO(travis): we should revisit the user format here, as it silently breaks situations",1
v0.8.2,"TODO ray: find an informed way to set the parallelism, in practice",1
v0.8.2,TODO: When this is implemented we also need to update the,1
v0.8.2,TODO(travis): consolidate with implementation in data/ray.py,1
v0.8.2,"This is needed because Daft only supports Dataframes, not Series",1
v0.8.2,TODO(ekl) deprecate this once read fusion is available.,1
v0.8.2,TODO(geoffrey): ensure this subclasses ray.data.datasource.Reader in ray 1.14,1
v0.8.2,TODO(ekl) deprecate this once read fusion is available.,1
v0.8.2,"doesn't work for Snappy, so we double-check ourselves.",1
v0.8.2,TODO(xwjiang): Remove this later.,1
v0.8.2,TODO(travis): need to fix checkpoint saving/loading for DeepSpeed to enable tuning,1
v0.8.2,"TODO(travis): this double-counts on the same device, it should use a cross-communicator instead",1
v0.8.2,"TODO: remove LOCAL_BACKEND as a global constant, replace with singleton LocalBackend.shared_instance().",1
v0.8.2,Temporary workaround to prevent tests from automatically using the Ray backend. Taken from,1
v0.8.2,TODO(travis): add other indexing structures,1
v0.8.2,TODO(travis): open question if this is needed to ensure all workers using same weights,1
v0.8.2,TODO(travis): open question if this is needed to ensure all workers using same optimizer state,1
v0.8.2,"TODO(travis): currently Ray handles this for us, but is subject to hangs if one of the workers raises an",1
v0.8.1.post1,HACK(geoffrey): `hyperopt_resources` is a required resource for hyperopt to prevent deadlocks in Ludwig tests.,1
v0.8.1.post1,TODO(geoffrey): remove for Ray 2.2,1
v0.8.1.post1,is there a better way to do this?,1
v0.8.1.post1,todo the hidden output is actually a tensor. May need modification,1
v0.8.1.post1,todo figure out the output size for parallel 1d conv,1
v0.8.1.post1,TODO:  Need variant of assert_model_parameters_updated() to account for the two step calling sequence,1
v0.8.1.post1,todo: remove code,1
v0.8.1.post1,TODO(ksbrar): Circle back after discussing whether additional properties should be allowed long-term.,1
v0.8.1.post1,TODO(ksbrar): Circle back after discussing whether additional properties should be allowed long-term.,1
v0.8.1.post1,TODO(Arnav): Re-enable once https://github.com/ludwig-ai/ludwig/issues/3150 is resolved since the GBM,1
v0.8.1.post1,todo: re-add 'attention' after further research in implication of torch,1
v0.8.1.post1,TODO(#1333): Refactor this test once torch sequence generator work is complete.,1
v0.8.1.post1,"TODO(Justin): Move these to test_sequence_generator unit tests, and reintroduce decoder attention, beam_width, and",1
v0.8.1.post1,TODO(travis): add when we support pretrained text models for gbms,1
v0.8.1.post1,TODO: find a smaller model for testing,1
v0.8.1.post1,TODO: figure out how to get mocks to work with Ray backend,1
v0.8.1.post1,TODO(travis): We may want to expand upon this in the future to include some checks on model,1
v0.8.1.post1,TODO(travis): We may want to expand upon this in the future to include some checks on model,1
v0.8.1.post1,TODO(travis): We may want to expand upon this in the future to include some checks on model,1
v0.8.1.post1,TODO: Determine whether this is desired behavior. Tracked here:,1
v0.8.1.post1,TODO: feature type not yet supported,1
v0.8.1.post1,handled non-determinism when comparing the metrics between the local and Ray backends. We work around this by,1
v0.8.1.post1,TODO: feature type not yet supported,1
v0.8.1.post1,TODO(geoffrey): Fold modin tests into test_ray_image as @pytest.mark.parametrized once tests are optimized,1
v0.8.1.post1,TODO(travis): move this to separate gpu module so we only have one ray cluster running at a time,1
v0.8.1.post1,TODO(travis): https://github.com/ludwig-ai/ludwig/issues/1956,1
v0.8.1.post1,TODO: Determine if we still need this if-then-else construct,1
v0.8.1.post1,TODO(travis): once we support GBM text features,1
v0.8.1.post1,TODO(travis): once we support GBM text features,1
v0.8.1.post1,TODO(travis): need unit tests to test the get_embedding_layer() of every encoder to ensure it is,1
v0.8.1.post1,TODO: fix LLM model loading,1
v0.8.1.post1,TODO(arnav): p-tuning and prefix tuning have errors when enabled that seem to stem from DDP:,1
v0.8.1.post1,TODO(Arnav): Re-enable once we can run tests on GPUs,1
v0.8.1.post1,TODO(shreya): Uncomment when https://github.com/ludwig-ai/ludwig/issues/2039 is fixed.,1
v0.8.1.post1,TODO: remove this workaround when audio preprocessing is fixed.,1
v0.8.1.post1,TODO: these are the only outputs we provide from Torchscript for now,1
v0.8.1.post1,TODO all shadows built in name - come up with a more descriptive name,1
v0.8.1.post1,TODO (Connor): Refactor to use self.config_obj,1
v0.8.1.post1,TODO (ASN): add support for substitute_with_max parameter,1
v0.8.1.post1,TODO(travis): WIP,1
v0.8.1.post1,"TODO(travis): there's a lot of redundancy in this approach, since we are preprocessing the same DataFrame",1
v0.8.1.post1,"TODO(travis): can optimize the preprocessing part here, since we only need to preprocess / predict",1
v0.8.1.post1,TODO(travis): generalize this to support any pandas output format,1
v0.8.1.post1,TODO (Connor): Refactor to use self.config_obj,1
v0.8.1.post1,TODO: fix for Ray where workers may be of different skus,1
v0.8.1.post1,TODO(travis): remove in favor of on_hyperopt_end for naming consistency,1
v0.8.1.post1,TODO: https://github.com/ludwig-ai/ludwig/issues/2633,1
v0.8.1.post1,todo: revise docstring,1
v0.8.1.post1,todo: assess how to specify padding for equivalent to 'same',1
v0.8.1.post1,todo: determine how to pool_padding equivalent of 'same',1
v0.8.1.post1,todo: fixup docstring,1
v0.8.1.post1,todo: review docstring,1
v0.8.1.post1,todo: fix up docstring,1
v0.8.1.post1,todo: fix up docstring,1
v0.8.1.post1,todo: update docstring as needed,1
v0.8.1.post1,TODO(shreya): Make sure this is updated when FCStack is updated,1
v0.8.1.post1,TODO(Arnav): Remove this once we have reduce_output options set for,1
v0.8.1.post1,TODO(travis): get_hf_config_param_names should be implemented as abstract in HFEncoderConfig,1
v0.8.1.post1,TODO(shreya): Confirm that this is it,1
v0.8.1.post1,TODO(shreya): Confirm that this is it,1
v0.8.1.post1,TODO(justin): This may need to be conditioned on which AutoModel gets chosen.,1
v0.8.1.post1,TODO (ASN): Decide how we want to proceed if at least one trial has,1
v0.8.1.post1,TODO(shreya): Should this hyperopt config param be set here?,1
v0.8.1.post1,TODO (ASN): add image heuristics,1
v0.8.1.post1,"TODO(travis): less hacky way to do this, we should probably allow ModelConfig to be created without output",1
v0.8.1.post1,"todo future: this may be redundant, check",1
v0.8.1.post1,Workaround for including additional tensors from output of input encoders for,1
v0.8.1.post1,TODO(Justin): Think about how to make this communication work for multi-sequence,1
v0.8.1.post1,todo: when https://github.com/ludwig-ai/ludwig/issues/810 is closed,1
v0.8.1.post1,"todo: can we just use projector_size? # hidden_size,",1
v0.8.1.post1,"todo future: this may be redundant, check",1
v0.8.1.post1,"todo future: this may be redundant, check",1
v0.8.1.post1,Workaround for including additional tensors from output of input encoders for,1
v0.8.1.post1,TODO(Justin): Think about how to make this communication work for multi-sequence,1
v0.8.1.post1,todo future: maybe reintroduce these attention function,1
v0.8.1.post1,todo future: maybe reintroduce these attention function,1
v0.8.1.post1,todo future: maybe reintroduce these attention function,1
v0.8.1.post1,TODO(travis): consider moving this behind a general BatchNorm interface to avoid this kludge.,1
v0.8.1.post1,"todo: enumerate for debugging, remove after testing",1
v0.8.1.post1,TODO(shreya): Combine with ResNetBlock by adding a flag.,1
v0.8.1.post1,TODO(shreya): Implement sparse embedding lookup.,1
v0.8.1.post1,# TODO(shreya): Check if this is equivalent,1
v0.8.1.post1,# TODO(shreya): Check if supported in torch,1
v0.8.1.post1,todo: review for generality,1
v0.8.1.post1,TODO: Simplify this.,1
v0.8.1.post1,TODO(travis): make this more general to other cumulative loss functions,1
v0.8.1.post1,Dummy implementation.,1
v0.8.1.post1,"TODO(Justin): Add a confusion matrix, see",1
v0.8.1.post1,TODO: add a mechanism for letting the user decide to save it,1
v0.8.1.post1,todo: re-evaluate need for separate handling of `attention` reducer,1
v0.8.1.post1,TODO(Justin): Clean this up.,1
v0.8.1.post1,TODO: change to debug level before merging,1
v0.8.1.post1,TODO: should this raise an exception if not in training mode?,1
v0.8.1.post1,Nested conditional is a workaround to short-circuit boolean evaluation.,1
v0.8.1.post1,TODO: alternatively use get_average_image() for unreachable images,1
v0.8.1.post1,todo future add multiprocessing/multithreading,1
v0.8.1.post1,TODO(travis): do we even need a user param for vector size if we're going to auto-infer it in all,1
v0.8.1.post1,TODO(1891): Remove backward compatibility hack once all models have been retrained with Ludwig after,1
v0.8.1.post1,todo: refactor to reuse SequenceOutputFeature.postprocess_predictions,1
v0.8.1.post1,TODO: Add a mechanism that lets the user save the full probability distribution if they want.,1
v0.8.1.post1,TODO(geoffrey): add support for Dask DataFrames,1
v0.8.1.post1,TODO(Arnav): see if there's a way to only remove them if the entry does't have quotes. This currently,1
v0.8.1.post1,"removes all "" from the string (even those not added by json.dumps), which is not ideal.",1
v0.8.1.post1,Convert datetime to int64 to workaround Dask limitation,1
v0.8.1.post1,"TODO pyarrow: this is needed for caching to work with pyarrow. if removed, the following error is raised:",1
v0.8.1.post1,TODO(travis): passing in MODEL_ECD is a hack here that can be removed once we move to using,1
v0.8.1.post1,"encoder schema at all. This hack works for now because all encoders are supported by ECD, so",1
v0.8.1.post1,todo figure out if additional parameters are needed,1
v0.8.1.post1,"TODO(travis): instead of using raw dictionary, this should be loaded into a proper PreprocessingConfig",1
v0.8.1.post1,TODO dask: this needs to work with DataFrames,1
v0.8.1.post1,TODO(travis): decouple config from training_set_metadata so we don't need to,1
v0.8.1.post1,TODO(joppe): support out of memory negative sampling using Dask,1
v0.8.1.post1,TODO(travis): revisit in the future to make this more precise,1
v0.8.1.post1,TODO: Add link to windowing docs.,1
v0.8.1.post1,TODO: figure out correct typing for augmentation_pipeline after refactoring is done,1
v0.8.1.post1,TODO: consider removing this. doesn't work currently and read performance seems generally,1
v0.8.1.post1,TODO: convert to debug message when done with development,1
v0.8.1.post1,"TODO(travis): could try hashing the in-memory dataset, but this is tricky for Dask",1
v0.8.1.post1,todo future: reintroduce the bucketed batcher,1
v0.8.1.post1,TODO ray: implement dynamic batch size,1
v0.8.1.post1,TODO: Change annotation to PublicAPI once Ludwig 0.7 is released,1
v0.8.1.post1,TODO(travis): is this redundant with `clipglobalnorm`?,1
v0.8.1.post1,TODO(travis) consider removing this in the future after deprecation period,1
v0.8.1.post1,TODO: use registry pattern for trainers,1
v0.8.1.post1,TODO: Change to RAISE and update descriptions once we want to enforce strict schemas.,1
v0.8.1.post1,TODO: Maybe need to plumb 'required' through here,1
v0.8.1.post1,"TODO(travis): this seems much too verbose, does the validation error not show the specific error?",1
v0.8.1.post1,"TODO(travis): too much boilerplate here, we should find a way to abstract all this and only require specifying the",1
v0.8.1.post1,"TODO(travis): this seems much too verbose, does the validation error not show the specific error?",1
v0.8.1.post1,TODO(#1673): Need some more logic here for validating against output features,1
v0.8.1.post1,"TODO: Re-enable ""goss"" when supported: https://github.com/ludwig-ai/ludwig/issues/2988",1
v0.8.1.post1,TODO(travis): figure out why we need these imports to avoid circular import error,1
v0.8.1.post1,"TODO (Connor): Add nesting logic for fc_layers, see fully_connected_module.py",1
v0.8.1.post1,"TODO (Connor): Add nesting logic for fc_layers, see fully_connected_module.py",1
v0.8.1.post1,TODO(travis): seems like this is not really a valid user option. We should probably just remove these,1
v0.8.1.post1,TODO: uncomment when sentencepiece doesn't cause segfaults: https://github.com/ludwig-ai/ludwig/issues/2983,1
v0.8.1.post1,TODO: uncomment once we figure out host memory issue: https://github.com/ludwig-ai/ludwig/issues/3107,1
v0.8.1.post1,TODO: uncomment when CTRL bug (https://github.com/ludwig-ai/ludwig/issues/2977) has been fixed to add back in,1
v0.8.1.post1,TODO(#1673): Add conditional logic for fields like this one:,1
v0.8.1.post1,"TODO(travis): below type comparison is not perfect, as it doesn't consider the case where the default type",1
v0.8.1.post1,"TODO(travis): explore similar contraints for GBMs, which don't have epochs",1
v0.8.1.post1,TODO(travis): handle this with helper function,1
v0.8.1.post1,TODO(travis): not needed once we remove existing model config implementation,1
v0.8.1.post1,TODO(ksbrar): What is this?,1
v0.8.1.post1,"TODO(travis): this should be done through marshmallow dataclass' `required` field param,",1
v0.8.1.post1,TODO(Arnav): Remove the hard check on max_length once we support multiple output features.,1
v0.8.1.post1,TODO(Arnav): Refactor LossDataclassField to only accept loss types that are valid for the model,1
v0.8.1.post1,TODO: Add schema support for Callable,1
v0.8.1.post1,TODO: This should technically be a required paremeter. Do we need to add support for required params?,1
v0.8.1.post1,TODO: Double-check support for this,1
v0.8.1.post1,TODO: Double-check support for this as well as whether Callable args work properly,1
v0.8.1.post1,TODO: create a search alg metadata class to register in place of individual metadata args,1
v0.8.1.post1,TODO: Add a registry mapping string names to nevergrad optimizers,1
v0.8.1.post1,TODO: Add schemas for nevergrad optimizer kwargs,1
v0.8.1.post1,TODO: Add a registry of Optuna samplers schemas,1
v0.8.1.post1,TODO(travis): figure out why calling this `bias` doesn't work,1
v0.8.1.post1,TODO(travis): fix text generation when using prompt tuning:,1
v0.8.1.post1,TODO(travis): fix prefix tuning and p-tuning to work with DDP,1
v0.8.1.post1,TODO(This needs to be defined based on the Constraint class),1
v0.8.1.post1,TODO: This is an unfortunate side-effect of dataclass init order - you cannot have non-default fields follow,1
v0.8.1.post1,todo v0.4: currently not clear way to set model graph,1
v0.8.1.post1,TODO(daniel): delete this.,1
v0.8.1.post1,TODO(travis): figure out a good way to support this. The problem with,1
v0.8.1.post1,TODO: need to also include a filename for this figure,1
v0.8.1.post1,TODO(travis): figure out why we need these imports to avoid circular import error,1
v0.8.1.post1,TODO: Replace with more robust required logic later.,1
v0.8.1.post1,TODO(Arnav): Refactor to split into strategies like splitters,1
v0.8.1.post1,TODO(Arnav): Add support for probabilities and logits,1
v0.8.1.post1,"TODO(Arnav): Figure out how to compute logits. For now, we return",1
v0.8.1.post1,Dummy implementation.,1
v0.8.1.post1,TODO: Use a configurable ratio for how often to use teacher forcing during training.,1
v0.8.1.post1,TODO: Use a configurable ratio for how often to use teacher forcing during training.,1
v0.8.1.post1,Dummy implementation.,1
v0.8.1.post1,TODO(Arnav): Re-enable once we add DotProduct Combiner: https://github.com/ludwig-ai/ludwig/issues/3150,1
v0.8.1.post1,"TODO(travis): we assume here that False is always the default, which may not be true. We should dervice",1
v0.8.1.post1,"HACK: Llama fast tokenizer takes about 2-4 minutes to load, so we disable it for now.",1
v0.8.1.post1,"TODO(travis): stopgap solution, we should make it so we don't need to do this",1
v0.8.1.post1,todo (Wael): tests for all types.,1
v0.8.1.post1,todo (Wael): tests for all types.,1
v0.8.1.post1,Still needed for preprocessing  TODO(Connor): Refactor ludwig/data/preprocessing to use schema,1
v0.8.1.post1,"TODO(travis): remove this, make type a protected string for each subclass",1
v0.8.1.post1,"ORRIBLE HACK, IT'S THE ONLY WAY TO REMOVE PADDING",1
v0.8.1.post1,END OF HORRIBLE HACK,1
v0.8.1.post1,"TODO: more research needed on how to handle RayTune ""sample_from"" search space",1
v0.8.1.post1,HACK(geoffrey): gpt2 has no pad token. Recommendation is to use eos token instead.,1
v0.8.1.post1,TODO(geoffrey): can we better validate tokenizer parity before swapping in the TorchText tokenizer?,1
v0.8.1.post1,Workaround for: https://issues.apache.org/jira/browse/ARROW-5645,1
v0.8.1.post1,"HACK(Arnav): gpt, gpt2 and llama tokenizers had no pad tokens.",1
v0.8.1.post1,TODO(travis): do this through an interface rather than conditional logic,1
v0.8.1.post1,TODO(travis): move to cached_property when we drop Python 3.7.,1
v0.8.1.post1,TODO: remove reshaping once https://github.com/microsoft/LightGBM/pull/4925 is released,1
v0.8.1.post1,TODO: remove ravel once https://github.com/microsoft/LightGBM/pull/4925 is released,1
v0.8.1.post1,TODO (jeffkinnison): revert to use the requested device once torch device usage is standardized,1
v0.8.1.post1,TODO(shreya): Confirm types of args,1
v0.8.1.post1,"TODO: when loading an existing model, this loses metric values for all but the last epoch.",1
v0.8.1.post1,TODO(travis): implement imbalance ratio,1
v0.8.1.post1,"TODO (ASN): add other modalities (image, etc. )",1
v0.8.1.post1,"TODO(travis): this assumes ECD is the selected model type, which is not problematic for now, as",1
v0.8.1.post1,"diverge, so we should find a way to remove this. The best solution is to the change the input params from",1
v0.8.1.post1,TODO(travis): consolidate with implementation in data/ray.py,1
v0.8.1.post1,TODO: only single task currently,1
v0.8.1.post1,TODO(travis): include encoder and decoder steps during inference,1
v0.8.1.post1,"HACK: Llama fast tokenizer takes about 2-4 minutes to load, so we disable it for now.",1
v0.8.1.post1,TODO: this implementation will not work if resuming from a previous checkpoint. Need to fix this.,1
v0.8.1.post1,TODO: make this configurable in the future. These parameters are from FastChat:,1
v0.8.1.post1,"TODO: Wrap device_map=""auto"" in a try-except block since it may not be supported for all models (E.g. BertLMHead)  # noqa",1
v0.8.1.post1,TODO: only single task currently,1
v0.8.1.post1,HACK (Tim): get the device of the targets to transfer self.eval_loss_metric to the same device,1
v0.8.1.post1,"TODO(Arnav): Seems like doing this again and going between these format types in unnecessary, but",1
v0.8.1.post1,"HACK(geoffrey): we need a non-empty loss, so we just fill it with zeros",1
v0.8.1.post1,TODO(travis): this will need to change when we support multiple output features,1
v0.8.1.post1,"TODO(travis): use the implementation of trainer itself to decide whether to save the model, to",1
v0.8.1.post1,avoid this hack,1
v0.8.1.post1,TODO (jeffkinnison): revert to using the requested device for GBMs when device usage is fixed,1
v0.8.1.post1,TODO: with change to misc_utils.set_random_seed() this may be redundant,1
v0.8.1.post1,TODO(Justin): Check that the semantics of input_size align with what the combiner's output shape returns,1
v0.8.1.post1,TODO(justin): Remove dummy implementation. Make input_shape and output_shape functions.,1
v0.8.1.post1,TODO(geoffrey): figure out why self.index.search segfaults with larger batch sizes,1
v0.8.1.post1,TODO(travis): support local feature importance,1
v0.8.1.post1,TODO:,1
v0.8.1.post1,TODO(travis): add back skip encoders at the end in finally. Shouldn't be an issue in most cases as we,1
v0.8.1.post1,"TODO(travis): for force plots, need something similar to SHAP E[X]",1
v0.8.1.post1,"TODO(travis): for force plots, need something similar to SHAP E[X]",1
v0.8.1.post1,"TODO(travis): Consider changing to `if not torch.is_floating_point(t.dtype)` to simplify, then handle bool",1
v0.8.1.post1,TODO(travis): pre-compute this during training from the full training dataset.,1
v0.8.1.post1,"TODO(travis): for force plots, need something similar to SHAP E[X]",1
v0.8.1.post1,"TODO(travis): for force plots, need something similar to SHAP E[X]",1
v0.8.1.post1,"TODO: construct new datasets by running encoders (for text, image)",1
v0.8.1.post1,TODO(shreya): Refactor preprocessing so that this can be moved upstream.,1
v0.8.1.post1,"TODO: Implement batch size tuning for LLM, currently just returns the default batch size",1
v0.8.1.post1,TODO: refactor this into an interface,1
v0.8.1.post1,workaround type limitations of the underlying frameworks,1
v0.8.1.post1,TODO(Arnav): Re-enable in Ray 2.3,1
v0.8.1.post1,"TODO(travis): we should revisit the user format here, as it silently breaks situations",1
v0.8.1.post1,"TODO ray: find an informed way to set the parallelism, in practice",1
v0.8.1.post1,TODO: When this is implemented we also need to update the,1
v0.8.1.post1,TODO(travis): consolidate with implementation in data/ray.py,1
v0.8.1.post1,"This is needed because Daft only supports Dataframes, not Series",1
v0.8.1.post1,TODO(ekl) deprecate this once read fusion is available.,1
v0.8.1.post1,TODO(geoffrey): ensure this subclasses ray.data.datasource.Reader in ray 1.14,1
v0.8.1.post1,TODO(ekl) deprecate this once read fusion is available.,1
v0.8.1.post1,"doesn't work for Snappy, so we double-check ourselves.",1
v0.8.1.post1,TODO(xwjiang): Remove this later.,1
v0.8.1.post1,TODO(travis): need to fix checkpoint saving/loading for DeepSpeed to enable tuning,1
v0.8.1.post1,"TODO(travis): this double-counts on the same device, it should use a cross-communicator instead",1
v0.8.1.post1,"TODO: remove LOCAL_BACKEND as a global constant, replace with singleton LocalBackend.shared_instance().",1
v0.8.1.post1,Temporary workaround to prevent tests from automatically using the Ray backend. Taken from,1
v0.8.1.post1,TODO(travis): add other indexing structures,1
v0.8.1.post1,TODO(travis): open question if this is needed to ensure all workers using same weights,1
v0.8.1.post1,TODO(travis): open question if this is needed to ensure all workers using same optimizer state,1
v0.8.1.post1,"TODO(travis): currently Ray handles this for us, but is subject to hangs if one of the workers raises an",1
v0.8.1,HACK(geoffrey): `hyperopt_resources` is a required resource for hyperopt to prevent deadlocks in Ludwig tests.,1
v0.8.1,TODO(geoffrey): remove for Ray 2.2,1
v0.8.1,is there a better way to do this?,1
v0.8.1,todo the hidden output is actually a tensor. May need modification,1
v0.8.1,todo figure out the output size for parallel 1d conv,1
v0.8.1,TODO:  Need variant of assert_model_parameters_updated() to account for the two step calling sequence,1
v0.8.1,todo: remove code,1
v0.8.1,TODO(ksbrar): Circle back after discussing whether additional properties should be allowed long-term.,1
v0.8.1,TODO(ksbrar): Circle back after discussing whether additional properties should be allowed long-term.,1
v0.8.1,TODO(Arnav): Re-enable once https://github.com/ludwig-ai/ludwig/issues/3150 is resolved since the GBM,1
v0.8.1,todo: re-add 'attention' after further research in implication of torch,1
v0.8.1,TODO(#1333): Refactor this test once torch sequence generator work is complete.,1
v0.8.1,"TODO(Justin): Move these to test_sequence_generator unit tests, and reintroduce decoder attention, beam_width, and",1
v0.8.1,TODO(travis): add when we support pretrained text models for gbms,1
v0.8.1,TODO: find a smaller model for testing,1
v0.8.1,TODO: figure out how to get mocks to work with Ray backend,1
v0.8.1,TODO(travis): We may want to expand upon this in the future to include some checks on model,1
v0.8.1,TODO(travis): We may want to expand upon this in the future to include some checks on model,1
v0.8.1,TODO(travis): We may want to expand upon this in the future to include some checks on model,1
v0.8.1,TODO: Determine whether this is desired behavior. Tracked here:,1
v0.8.1,TODO: feature type not yet supported,1
v0.8.1,handled non-determinism when comparing the metrics between the local and Ray backends. We work around this by,1
v0.8.1,TODO: feature type not yet supported,1
v0.8.1,TODO(geoffrey): Fold modin tests into test_ray_image as @pytest.mark.parametrized once tests are optimized,1
v0.8.1,TODO(travis): move this to separate gpu module so we only have one ray cluster running at a time,1
v0.8.1,TODO(travis): https://github.com/ludwig-ai/ludwig/issues/1956,1
v0.8.1,TODO: Determine if we still need this if-then-else construct,1
v0.8.1,TODO(travis): once we support GBM text features,1
v0.8.1,TODO(travis): once we support GBM text features,1
v0.8.1,TODO(travis): need unit tests to test the get_embedding_layer() of every encoder to ensure it is,1
v0.8.1,TODO: fix LLM model loading,1
v0.8.1,TODO(arnav): p-tuning and prefix tuning have errors when enabled that seem to stem from DDP:,1
v0.8.1,TODO(Arnav): Re-enable once we can run tests on GPUs,1
v0.8.1,TODO(shreya): Uncomment when https://github.com/ludwig-ai/ludwig/issues/2039 is fixed.,1
v0.8.1,TODO: remove this workaround when audio preprocessing is fixed.,1
v0.8.1,TODO: these are the only outputs we provide from Torchscript for now,1
v0.8.1,TODO all shadows built in name - come up with a more descriptive name,1
v0.8.1,TODO (Connor): Refactor to use self.config_obj,1
v0.8.1,TODO (ASN): add support for substitute_with_max parameter,1
v0.8.1,TODO(travis): WIP,1
v0.8.1,"TODO(travis): there's a lot of redundancy in this approach, since we are preprocessing the same DataFrame",1
v0.8.1,"TODO(travis): can optimize the preprocessing part here, since we only need to preprocess / predict",1
v0.8.1,TODO(travis): generalize this to support any pandas output format,1
v0.8.1,TODO (Connor): Refactor to use self.config_obj,1
v0.8.1,TODO: fix for Ray where workers may be of different skus,1
v0.8.1,TODO(travis): remove in favor of on_hyperopt_end for naming consistency,1
v0.8.1,TODO: https://github.com/ludwig-ai/ludwig/issues/2633,1
v0.8.1,todo: revise docstring,1
v0.8.1,todo: assess how to specify padding for equivalent to 'same',1
v0.8.1,todo: determine how to pool_padding equivalent of 'same',1
v0.8.1,todo: fixup docstring,1
v0.8.1,todo: review docstring,1
v0.8.1,todo: fix up docstring,1
v0.8.1,todo: fix up docstring,1
v0.8.1,todo: update docstring as needed,1
v0.8.1,TODO(shreya): Make sure this is updated when FCStack is updated,1
v0.8.1,TODO(Arnav): Remove this once we have reduce_output options set for,1
v0.8.1,TODO(travis): get_hf_config_param_names should be implemented as abstract in HFEncoderConfig,1
v0.8.1,TODO(shreya): Confirm that this is it,1
v0.8.1,TODO(shreya): Confirm that this is it,1
v0.8.1,TODO(justin): This may need to be conditioned on which AutoModel gets chosen.,1
v0.8.1,TODO (ASN): Decide how we want to proceed if at least one trial has,1
v0.8.1,TODO(shreya): Should this hyperopt config param be set here?,1
v0.8.1,TODO (ASN): add image heuristics,1
v0.8.1,"TODO(travis): less hacky way to do this, we should probably allow ModelConfig to be created without output",1
v0.8.1,"todo future: this may be redundant, check",1
v0.8.1,Workaround for including additional tensors from output of input encoders for,1
v0.8.1,TODO(Justin): Think about how to make this communication work for multi-sequence,1
v0.8.1,todo: when https://github.com/ludwig-ai/ludwig/issues/810 is closed,1
v0.8.1,"todo: can we just use projector_size? # hidden_size,",1
v0.8.1,"todo future: this may be redundant, check",1
v0.8.1,"todo future: this may be redundant, check",1
v0.8.1,Workaround for including additional tensors from output of input encoders for,1
v0.8.1,TODO(Justin): Think about how to make this communication work for multi-sequence,1
v0.8.1,todo future: maybe reintroduce these attention function,1
v0.8.1,todo future: maybe reintroduce these attention function,1
v0.8.1,todo future: maybe reintroduce these attention function,1
v0.8.1,TODO(travis): consider moving this behind a general BatchNorm interface to avoid this kludge.,1
v0.8.1,"todo: enumerate for debugging, remove after testing",1
v0.8.1,TODO(shreya): Combine with ResNetBlock by adding a flag.,1
v0.8.1,TODO(shreya): Implement sparse embedding lookup.,1
v0.8.1,# TODO(shreya): Check if this is equivalent,1
v0.8.1,# TODO(shreya): Check if supported in torch,1
v0.8.1,todo: review for generality,1
v0.8.1,TODO: Simplify this.,1
v0.8.1,TODO(travis): make this more general to other cumulative loss functions,1
v0.8.1,Dummy implementation.,1
v0.8.1,"TODO(Justin): Add a confusion matrix, see",1
v0.8.1,TODO: add a mechanism for letting the user decide to save it,1
v0.8.1,todo: re-evaluate need for separate handling of `attention` reducer,1
v0.8.1,TODO(Justin): Clean this up.,1
v0.8.1,TODO: change to debug level before merging,1
v0.8.1,TODO: should this raise an exception if not in training mode?,1
v0.8.1,Nested conditional is a workaround to short-circuit boolean evaluation.,1
v0.8.1,TODO: alternatively use get_average_image() for unreachable images,1
v0.8.1,todo future add multiprocessing/multithreading,1
v0.8.1,TODO(travis): do we even need a user param for vector size if we're going to auto-infer it in all,1
v0.8.1,TODO(1891): Remove backward compatibility hack once all models have been retrained with Ludwig after,1
v0.8.1,todo: refactor to reuse SequenceOutputFeature.postprocess_predictions,1
v0.8.1,TODO: Add a mechanism that lets the user save the full probability distribution if they want.,1
v0.8.1,TODO(geoffrey): add support for Dask DataFrames,1
v0.8.1,TODO(Arnav): see if there's a way to only remove them if the entry does't have quotes. This currently,1
v0.8.1,"removes all "" from the string (even those not added by json.dumps), which is not ideal.",1
v0.8.1,Convert datetime to int64 to workaround Dask limitation,1
v0.8.1,"TODO pyarrow: this is needed for caching to work with pyarrow. if removed, the following error is raised:",1
v0.8.1,TODO(travis): passing in MODEL_ECD is a hack here that can be removed once we move to using,1
v0.8.1,"encoder schema at all. This hack works for now because all encoders are supported by ECD, so",1
v0.8.1,todo figure out if additional parameters are needed,1
v0.8.1,"TODO(travis): instead of using raw dictionary, this should be loaded into a proper PreprocessingConfig",1
v0.8.1,TODO dask: this needs to work with DataFrames,1
v0.8.1,TODO(travis): decouple config from training_set_metadata so we don't need to,1
v0.8.1,TODO(joppe): support out of memory negative sampling using Dask,1
v0.8.1,TODO(travis): revisit in the future to make this more precise,1
v0.8.1,TODO: Add link to windowing docs.,1
v0.8.1,TODO: figure out correct typing for augmentation_pipeline after refactoring is done,1
v0.8.1,TODO: consider removing this. doesn't work currently and read performance seems generally,1
v0.8.1,TODO: convert to debug message when done with development,1
v0.8.1,"TODO(travis): could try hashing the in-memory dataset, but this is tricky for Dask",1
v0.8.1,todo future: reintroduce the bucketed batcher,1
v0.8.1,TODO ray: implement dynamic batch size,1
v0.8.1,TODO: Change annotation to PublicAPI once Ludwig 0.7 is released,1
v0.8.1,TODO(travis): is this redundant with `clipglobalnorm`?,1
v0.8.1,TODO(travis) consider removing this in the future after deprecation period,1
v0.8.1,TODO: use registry pattern for trainers,1
v0.8.1,TODO: Change to RAISE and update descriptions once we want to enforce strict schemas.,1
v0.8.1,TODO: Maybe need to plumb 'required' through here,1
v0.8.1,"TODO(travis): this seems much too verbose, does the validation error not show the specific error?",1
v0.8.1,"TODO(travis): too much boilerplate here, we should find a way to abstract all this and only require specifying the",1
v0.8.1,"TODO(travis): this seems much too verbose, does the validation error not show the specific error?",1
v0.8.1,TODO(#1673): Need some more logic here for validating against output features,1
v0.8.1,"TODO: Re-enable ""goss"" when supported: https://github.com/ludwig-ai/ludwig/issues/2988",1
v0.8.1,TODO(travis): figure out why we need these imports to avoid circular import error,1
v0.8.1,"TODO (Connor): Add nesting logic for fc_layers, see fully_connected_module.py",1
v0.8.1,"TODO (Connor): Add nesting logic for fc_layers, see fully_connected_module.py",1
v0.8.1,TODO(travis): seems like this is not really a valid user option. We should probably just remove these,1
v0.8.1,TODO: uncomment when sentencepiece doesn't cause segfaults: https://github.com/ludwig-ai/ludwig/issues/2983,1
v0.8.1,TODO: uncomment once we figure out host memory issue: https://github.com/ludwig-ai/ludwig/issues/3107,1
v0.8.1,TODO: uncomment when CTRL bug (https://github.com/ludwig-ai/ludwig/issues/2977) has been fixed to add back in,1
v0.8.1,TODO(#1673): Add conditional logic for fields like this one:,1
v0.8.1,"TODO(travis): below type comparison is not perfect, as it doesn't consider the case where the default type",1
v0.8.1,"TODO(travis): explore similar contraints for GBMs, which don't have epochs",1
v0.8.1,TODO(travis): handle this with helper function,1
v0.8.1,TODO(travis): not needed once we remove existing model config implementation,1
v0.8.1,TODO(ksbrar): What is this?,1
v0.8.1,"TODO(travis): this should be done through marshmallow dataclass' `required` field param,",1
v0.8.1,TODO(Arnav): Remove the hard check on max_length once we support multiple output features.,1
v0.8.1,TODO(Arnav): Refactor LossDataclassField to only accept loss types that are valid for the model,1
v0.8.1,TODO: Add schema support for Callable,1
v0.8.1,TODO: This should technically be a required paremeter. Do we need to add support for required params?,1
v0.8.1,TODO: Double-check support for this,1
v0.8.1,TODO: Double-check support for this as well as whether Callable args work properly,1
v0.8.1,TODO: create a search alg metadata class to register in place of individual metadata args,1
v0.8.1,TODO: Add a registry mapping string names to nevergrad optimizers,1
v0.8.1,TODO: Add schemas for nevergrad optimizer kwargs,1
v0.8.1,TODO: Add a registry of Optuna samplers schemas,1
v0.8.1,TODO(travis): figure out why calling this `bias` doesn't work,1
v0.8.1,TODO(travis): fix text generation when using prompt tuning:,1
v0.8.1,TODO(travis): fix prefix tuning and p-tuning to work with DDP,1
v0.8.1,TODO(This needs to be defined based on the Constraint class),1
v0.8.1,TODO: This is an unfortunate side-effect of dataclass init order - you cannot have non-default fields follow,1
v0.8.1,todo v0.4: currently not clear way to set model graph,1
v0.8.1,TODO(daniel): delete this.,1
v0.8.1,TODO(travis): figure out a good way to support this. The problem with,1
v0.8.1,TODO: need to also include a filename for this figure,1
v0.8.1,TODO(travis): figure out why we need these imports to avoid circular import error,1
v0.8.1,TODO: Replace with more robust required logic later.,1
v0.8.1,TODO(Arnav): Refactor to split into strategies like splitters,1
v0.8.1,TODO(Arnav): Add support for probabilities and logits,1
v0.8.1,"TODO(Arnav): Figure out how to compute logits. For now, we return",1
v0.8.1,Dummy implementation.,1
v0.8.1,TODO: Use a configurable ratio for how often to use teacher forcing during training.,1
v0.8.1,TODO: Use a configurable ratio for how often to use teacher forcing during training.,1
v0.8.1,Dummy implementation.,1
v0.8.1,TODO(Arnav): Re-enable once we add DotProduct Combiner: https://github.com/ludwig-ai/ludwig/issues/3150,1
v0.8.1,"TODO(travis): we assume here that False is always the default, which may not be true. We should dervice",1
v0.8.1,"HACK: Llama fast tokenizer takes about 2-4 minutes to load, so we disable it for now.",1
v0.8.1,"TODO(travis): stopgap solution, we should make it so we don't need to do this",1
v0.8.1,todo (Wael): tests for all types.,1
v0.8.1,todo (Wael): tests for all types.,1
v0.8.1,Still needed for preprocessing  TODO(Connor): Refactor ludwig/data/preprocessing to use schema,1
v0.8.1,"TODO(travis): remove this, make type a protected string for each subclass",1
v0.8.1,"ORRIBLE HACK, IT'S THE ONLY WAY TO REMOVE PADDING",1
v0.8.1,END OF HORRIBLE HACK,1
v0.8.1,"TODO: more research needed on how to handle RayTune ""sample_from"" search space",1
v0.8.1,HACK(geoffrey): gpt2 has no pad token. Recommendation is to use eos token instead.,1
v0.8.1,TODO(geoffrey): can we better validate tokenizer parity before swapping in the TorchText tokenizer?,1
v0.8.1,Workaround for: https://issues.apache.org/jira/browse/ARROW-5645,1
v0.8.1,"HACK(Arnav): gpt, gpt2 and llama tokenizers had no pad tokens.",1
v0.8.1,TODO(travis): do this through an interface rather than conditional logic,1
v0.8.1,TODO(travis): move to cached_property when we drop Python 3.7.,1
v0.8.1,TODO: remove reshaping once https://github.com/microsoft/LightGBM/pull/4925 is released,1
v0.8.1,TODO: remove ravel once https://github.com/microsoft/LightGBM/pull/4925 is released,1
v0.8.1,TODO (jeffkinnison): revert to use the requested device once torch device usage is standardized,1
v0.8.1,TODO(shreya): Confirm types of args,1
v0.8.1,"TODO: when loading an existing model, this loses metric values for all but the last epoch.",1
v0.8.1,TODO(travis): implement imbalance ratio,1
v0.8.1,"TODO (ASN): add other modalities (image, etc. )",1
v0.8.1,"TODO(travis): this assumes ECD is the selected model type, which is not problematic for now, as",1
v0.8.1,"diverge, so we should find a way to remove this. The best solution is to the change the input params from",1
v0.8.1,TODO(travis): consolidate with implementation in data/ray.py,1
v0.8.1,TODO: only single task currently,1
v0.8.1,TODO(travis): include encoder and decoder steps during inference,1
v0.8.1,"HACK: Llama fast tokenizer takes about 2-4 minutes to load, so we disable it for now.",1
v0.8.1,TODO: this implementation will not work if resuming from a previous checkpoint. Need to fix this.,1
v0.8.1,TODO: make this configurable in the future. These parameters are from FastChat:,1
v0.8.1,"TODO: Wrap device_map=""auto"" in a try-except block since it may not be supported for all models (E.g. BertLMHead)  # noqa",1
v0.8.1,TODO: only single task currently,1
v0.8.1,HACK (Tim): get the device of the targets to transfer self.eval_loss_metric to the same device,1
v0.8.1,"TODO(Arnav): Seems like doing this again and going between these format types in unnecessary, but",1
v0.8.1,"HACK(geoffrey): we need a non-empty loss, so we just fill it with zeros",1
v0.8.1,TODO(travis): this will need to change when we support multiple output features,1
v0.8.1,"TODO(travis): use the implementation of trainer itself to decide whether to save the model, to",1
v0.8.1,avoid this hack,1
v0.8.1,TODO (jeffkinnison): revert to using the requested device for GBMs when device usage is fixed,1
v0.8.1,TODO: with change to misc_utils.set_random_seed() this may be redundant,1
v0.8.1,TODO(Justin): Check that the semantics of input_size align with what the combiner's output shape returns,1
v0.8.1,TODO(justin): Remove dummy implementation. Make input_shape and output_shape functions.,1
v0.8.1,TODO(geoffrey): figure out why self.index.search segfaults with larger batch sizes,1
v0.8.1,TODO(travis): support local feature importance,1
v0.8.1,TODO:,1
v0.8.1,TODO(travis): add back skip encoders at the end in finally. Shouldn't be an issue in most cases as we,1
v0.8.1,"TODO(travis): for force plots, need something similar to SHAP E[X]",1
v0.8.1,"TODO(travis): for force plots, need something similar to SHAP E[X]",1
v0.8.1,"TODO(travis): Consider changing to `if not torch.is_floating_point(t.dtype)` to simplify, then handle bool",1
v0.8.1,TODO(travis): pre-compute this during training from the full training dataset.,1
v0.8.1,"TODO(travis): for force plots, need something similar to SHAP E[X]",1
v0.8.1,"TODO(travis): for force plots, need something similar to SHAP E[X]",1
v0.8.1,"TODO: construct new datasets by running encoders (for text, image)",1
v0.8.1,TODO(shreya): Refactor preprocessing so that this can be moved upstream.,1
v0.8.1,"TODO: Implement batch size tuning for LLM, currently just returns the default batch size",1
v0.8.1,TODO: refactor this into an interface,1
v0.8.1,workaround type limitations of the underlying frameworks,1
v0.8.1,TODO(Arnav): Re-enable in Ray 2.3,1
v0.8.1,"TODO(travis): we should revisit the user format here, as it silently breaks situations",1
v0.8.1,"TODO ray: find an informed way to set the parallelism, in practice",1
v0.8.1,TODO: When this is implemented we also need to update the,1
v0.8.1,TODO(travis): consolidate with implementation in data/ray.py,1
v0.8.1,"This is needed because Daft only supports Dataframes, not Series",1
v0.8.1,TODO(ekl) deprecate this once read fusion is available.,1
v0.8.1,TODO(geoffrey): ensure this subclasses ray.data.datasource.Reader in ray 1.14,1
v0.8.1,TODO(ekl) deprecate this once read fusion is available.,1
v0.8.1,"doesn't work for Snappy, so we double-check ourselves.",1
v0.8.1,TODO(xwjiang): Remove this later.,1
v0.8.1,TODO(travis): need to fix checkpoint saving/loading for DeepSpeed to enable tuning,1
v0.8.1,"TODO(travis): this double-counts on the same device, it should use a cross-communicator instead",1
v0.8.1,"TODO: remove LOCAL_BACKEND as a global constant, replace with singleton LocalBackend.shared_instance().",1
v0.8.1,Temporary workaround to prevent tests from automatically using the Ray backend. Taken from,1
v0.8.1,TODO(travis): add other indexing structures,1
v0.8.1,TODO(travis): open question if this is needed to ensure all workers using same weights,1
v0.8.1,TODO(travis): open question if this is needed to ensure all workers using same optimizer state,1
v0.8.1,"TODO(travis): currently Ray handles this for us, but is subject to hangs if one of the workers raises an",1
v0.8,HACK(geoffrey): `hyperopt_resources` is a required resource for hyperopt to prevent deadlocks in Ludwig tests.,1
v0.8,TODO(geoffrey): remove for Ray 2.2,1
v0.8,is there a better way to do this?,1
v0.8,todo the hidden output is actually a tensor. May need modification,1
v0.8,todo figure out the output size for parallel 1d conv,1
v0.8,TODO:  Need variant of assert_model_parameters_updated() to account for the two step calling sequence,1
v0.8,todo: remove code,1
v0.8,TODO(ksbrar): Circle back after discussing whether additional properties should be allowed long-term.,1
v0.8,TODO(ksbrar): Circle back after discussing whether additional properties should be allowed long-term.,1
v0.8,TODO(Arnav): Re-enable once https://github.com/ludwig-ai/ludwig/issues/3150 is resolved since the GBM,1
v0.8,todo: re-add 'attention' after further research in implication of torch,1
v0.8,TODO(#1333): Refactor this test once torch sequence generator work is complete.,1
v0.8,"TODO(Justin): Move these to test_sequence_generator unit tests, and reintroduce decoder attention, beam_width, and",1
v0.8,TODO(travis): add when we support pretrained text models for gbms,1
v0.8,TODO: find a smaller model for testing,1
v0.8,TODO: figure out how to get mocks to work with Ray backend,1
v0.8,TODO(travis): We may want to expand upon this in the future to include some checks on model,1
v0.8,TODO(travis): We may want to expand upon this in the future to include some checks on model,1
v0.8,TODO(travis): We may want to expand upon this in the future to include some checks on model,1
v0.8,TODO: Determine whether this is desired behavior. Tracked here:,1
v0.8,TODO: feature type not yet supported,1
v0.8,handled non-determinism when comparing the metrics between the local and Ray backends. We work around this by,1
v0.8,TODO: feature type not yet supported,1
v0.8,TODO(geoffrey): Fold modin tests into test_ray_image as @pytest.mark.parametrized once tests are optimized,1
v0.8,TODO(travis): move this to separate gpu module so we only have one ray cluster running at a time,1
v0.8,TODO(travis): https://github.com/ludwig-ai/ludwig/issues/1956,1
v0.8,TODO: Determine if we still need this if-then-else construct,1
v0.8,TODO(travis): once we support GBM text features,1
v0.8,TODO(travis): once we support GBM text features,1
v0.8,TODO(travis): need unit tests to test the get_embedding_layer() of every encoder to ensure it is,1
v0.8,TODO: fix LLM model loading,1
v0.8,TODO(arnav): p-tuning and prefix tuning have errors when enabled that seem to stem from DDP:,1
v0.8,TODO(Arnav): Re-enable once we can run tests on GPUs,1
v0.8,TODO(shreya): Uncomment when https://github.com/ludwig-ai/ludwig/issues/2039 is fixed.,1
v0.8,TODO: remove this workaround when audio preprocessing is fixed.,1
v0.8,TODO: these are the only outputs we provide from Torchscript for now,1
v0.8,TODO all shadows built in name - come up with a more descriptive name,1
v0.8,TODO (Connor): Refactor to use self.config_obj,1
v0.8,TODO (ASN): add support for substitute_with_max parameter,1
v0.8,TODO(travis): WIP,1
v0.8,"TODO(travis): there's a lot of redundancy in this approach, since we are preprocessing the same DataFrame",1
v0.8,"TODO(travis): can optimize the preprocessing part here, since we only need to preprocess / predict",1
v0.8,TODO(travis): generalize this to support any pandas output format,1
v0.8,TODO (Connor): Refactor to use self.config_obj,1
v0.8,TODO: fix for Ray where workers may be of different skus,1
v0.8,TODO(travis): remove in favor of on_hyperopt_end for naming consistency,1
v0.8,TODO: https://github.com/ludwig-ai/ludwig/issues/2633,1
v0.8,todo: revise docstring,1
v0.8,todo: assess how to specify padding for equivalent to 'same',1
v0.8,todo: determine how to pool_padding equivalent of 'same',1
v0.8,todo: fixup docstring,1
v0.8,todo: review docstring,1
v0.8,todo: fix up docstring,1
v0.8,todo: fix up docstring,1
v0.8,todo: update docstring as needed,1
v0.8,TODO(shreya): Make sure this is updated when FCStack is updated,1
v0.8,TODO(Arnav): Remove this once we have reduce_output options set for,1
v0.8,TODO(travis): get_hf_config_param_names should be implemented as abstract in HFEncoderConfig,1
v0.8,TODO(shreya): Confirm that this is it,1
v0.8,TODO(shreya): Confirm that this is it,1
v0.8,TODO(justin): This may need to be conditioned on which AutoModel gets chosen.,1
v0.8,TODO (ASN): Decide how we want to proceed if at least one trial has,1
v0.8,TODO(shreya): Should this hyperopt config param be set here?,1
v0.8,TODO (ASN): add image heuristics,1
v0.8,"TODO(travis): less hacky way to do this, we should probably allow ModelConfig to be created without output",1
v0.8,"todo future: this may be redundant, check",1
v0.8,Workaround for including additional tensors from output of input encoders for,1
v0.8,TODO(Justin): Think about how to make this communication work for multi-sequence,1
v0.8,todo: when https://github.com/ludwig-ai/ludwig/issues/810 is closed,1
v0.8,"todo: can we just use projector_size? # hidden_size,",1
v0.8,"todo future: this may be redundant, check",1
v0.8,"todo future: this may be redundant, check",1
v0.8,Workaround for including additional tensors from output of input encoders for,1
v0.8,TODO(Justin): Think about how to make this communication work for multi-sequence,1
v0.8,todo future: maybe reintroduce these attention function,1
v0.8,todo future: maybe reintroduce these attention function,1
v0.8,todo future: maybe reintroduce these attention function,1
v0.8,TODO(travis): consider moving this behind a general BatchNorm interface to avoid this kludge.,1
v0.8,"todo: enumerate for debugging, remove after testing",1
v0.8,TODO(shreya): Combine with ResNetBlock by adding a flag.,1
v0.8,TODO(shreya): Implement sparse embedding lookup.,1
v0.8,# TODO(shreya): Check if this is equivalent,1
v0.8,# TODO(shreya): Check if supported in torch,1
v0.8,todo: review for generality,1
v0.8,TODO: Simplify this.,1
v0.8,TODO(travis): make this more general to other cumulative loss functions,1
v0.8,Dummy implementation.,1
v0.8,"TODO(Justin): Add a confusion matrix, see",1
v0.8,TODO: add a mechanism for letting the user decide to save it,1
v0.8,todo: re-evaluate need for separate handling of `attention` reducer,1
v0.8,TODO(Justin): Clean this up.,1
v0.8,TODO: change to debug level before merging,1
v0.8,TODO: should this raise an exception if not in training mode?,1
v0.8,Nested conditional is a workaround to short-circuit boolean evaluation.,1
v0.8,TODO: alternatively use get_average_image() for unreachable images,1
v0.8,todo future add multiprocessing/multithreading,1
v0.8,TODO(travis): do we even need a user param for vector size if we're going to auto-infer it in all,1
v0.8,TODO(1891): Remove backward compatibility hack once all models have been retrained with Ludwig after,1
v0.8,todo: refactor to reuse SequenceOutputFeature.postprocess_predictions,1
v0.8,TODO: Add a mechanism that lets the user save the full probability distribution if they want.,1
v0.8,TODO(geoffrey): add support for Dask DataFrames,1
v0.8,TODO(Arnav): see if there's a way to only remove them if the entry does't have quotes. This currently,1
v0.8,"removes all "" from the string (even those not added by json.dumps), which is not ideal.",1
v0.8,Convert datetime to int64 to workaround Dask limitation,1
v0.8,"TODO pyarrow: this is needed for caching to work with pyarrow. if removed, the following error is raised:",1
v0.8,TODO(travis): passing in MODEL_ECD is a hack here that can be removed once we move to using,1
v0.8,"encoder schema at all. This hack works for now because all encoders are supported by ECD, so",1
v0.8,todo figure out if additional parameters are needed,1
v0.8,"TODO(travis): instead of using raw dictionary, this should be loaded into a proper PreprocessingConfig",1
v0.8,TODO dask: this needs to work with DataFrames,1
v0.8,TODO(travis): decouple config from training_set_metadata so we don't need to,1
v0.8,TODO(joppe): support out of memory negative sampling using Dask,1
v0.8,TODO(travis): revisit in the future to make this more precise,1
v0.8,TODO: Add link to windowing docs.,1
v0.8,TODO: figure out correct typing for augmentation_pipeline after refactoring is done,1
v0.8,TODO: consider removing this. doesn't work currently and read performance seems generally,1
v0.8,TODO: convert to debug message when done with development,1
v0.8,"TODO(travis): could try hashing the in-memory dataset, but this is tricky for Dask",1
v0.8,todo future: reintroduce the bucketed batcher,1
v0.8,TODO ray: implement dynamic batch size,1
v0.8,TODO: Change annotation to PublicAPI once Ludwig 0.7 is released,1
v0.8,TODO(travis): is this redundant with `clipglobalnorm`?,1
v0.8,TODO(travis) consider removing this in the future after deprecation period,1
v0.8,TODO: use registry pattern for trainers,1
v0.8,TODO: Change to RAISE and update descriptions once we want to enforce strict schemas.,1
v0.8,TODO: Maybe need to plumb 'required' through here,1
v0.8,"TODO(travis): this seems much too verbose, does the validation error not show the specific error?",1
v0.8,"TODO(travis): too much boilerplate here, we should find a way to abstract all this and only require specifying the",1
v0.8,"TODO(travis): this seems much too verbose, does the validation error not show the specific error?",1
v0.8,TODO(#1673): Need some more logic here for validating against output features,1
v0.8,"TODO: Re-enable ""goss"" when supported: https://github.com/ludwig-ai/ludwig/issues/2988",1
v0.8,TODO(travis): figure out why we need these imports to avoid circular import error,1
v0.8,"TODO (Connor): Add nesting logic for fc_layers, see fully_connected_module.py",1
v0.8,"TODO (Connor): Add nesting logic for fc_layers, see fully_connected_module.py",1
v0.8,TODO(travis): seems like this is not really a valid user option. We should probably just remove these,1
v0.8,TODO: uncomment when sentencepiece doesn't cause segfaults: https://github.com/ludwig-ai/ludwig/issues/2983,1
v0.8,TODO: uncomment once we figure out host memory issue: https://github.com/ludwig-ai/ludwig/issues/3107,1
v0.8,TODO: uncomment when CTRL bug (https://github.com/ludwig-ai/ludwig/issues/2977) has been fixed to add back in,1
v0.8,TODO(#1673): Add conditional logic for fields like this one:,1
v0.8,"TODO(travis): below type comparison is not perfect, as it doesn't consider the case where the default type",1
v0.8,"TODO(travis): explore similar contraints for GBMs, which don't have epochs",1
v0.8,TODO(travis): handle this with helper function,1
v0.8,TODO(travis): not needed once we remove existing model config implementation,1
v0.8,TODO(ksbrar): What is this?,1
v0.8,"TODO(travis): this should be done through marshmallow dataclass' `required` field param,",1
v0.8,TODO(Arnav): Remove the hard check on max_length once we support multiple output features.,1
v0.8,TODO(Arnav): Refactor LossDataclassField to only accept loss types that are valid for the model,1
v0.8,TODO: Add schema support for Callable,1
v0.8,TODO: This should technically be a required paremeter. Do we need to add support for required params?,1
v0.8,TODO: Double-check support for this,1
v0.8,TODO: Double-check support for this as well as whether Callable args work properly,1
v0.8,TODO: create a search alg metadata class to register in place of individual metadata args,1
v0.8,TODO: Add a registry mapping string names to nevergrad optimizers,1
v0.8,TODO: Add schemas for nevergrad optimizer kwargs,1
v0.8,TODO: Add a registry of Optuna samplers schemas,1
v0.8,TODO(travis): figure out why calling this `bias` doesn't work,1
v0.8,TODO(travis): fix text generation when using prompt tuning:,1
v0.8,TODO(travis): fix prefix tuning and p-tuning to work with DDP,1
v0.8,TODO(This needs to be defined based on the Constraint class),1
v0.8,TODO: This is an unfortunate side-effect of dataclass init order - you cannot have non-default fields follow,1
v0.8,todo v0.4: currently not clear way to set model graph,1
v0.8,TODO(daniel): delete this.,1
v0.8,TODO(travis): figure out a good way to support this. The problem with,1
v0.8,TODO: need to also include a filename for this figure,1
v0.8,TODO(travis): figure out why we need these imports to avoid circular import error,1
v0.8,TODO: Replace with more robust required logic later.,1
v0.8,TODO(Arnav): Refactor to split into strategies like splitters,1
v0.8,TODO(Arnav): Add support for probabilities and logits,1
v0.8,"TODO(Arnav): Figure out how to compute logits. For now, we return",1
v0.8,Dummy implementation.,1
v0.8,TODO: Use a configurable ratio for how often to use teacher forcing during training.,1
v0.8,TODO: Use a configurable ratio for how often to use teacher forcing during training.,1
v0.8,Dummy implementation.,1
v0.8,TODO(Arnav): Re-enable once we add DotProduct Combiner: https://github.com/ludwig-ai/ludwig/issues/3150,1
v0.8,"TODO(travis): we assume here that False is always the default, which may not be true. We should dervice",1
v0.8,"HACK: Llama fast tokenizer takes about 2-4 minutes to load, so we disable it for now.",1
v0.8,"TODO(travis): stopgap solution, we should make it so we don't need to do this",1
v0.8,todo (Wael): tests for all types.,1
v0.8,todo (Wael): tests for all types.,1
v0.8,Still needed for preprocessing  TODO(Connor): Refactor ludwig/data/preprocessing to use schema,1
v0.8,"TODO(travis): remove this, make type a protected string for each subclass",1
v0.8,"ORRIBLE HACK, IT'S THE ONLY WAY TO REMOVE PADDING",1
v0.8,END OF HORRIBLE HACK,1
v0.8,"TODO: more research needed on how to handle RayTune ""sample_from"" search space",1
v0.8,HACK(geoffrey): gpt2 has no pad token. Recommendation is to use eos token instead.,1
v0.8,TODO(geoffrey): can we better validate tokenizer parity before swapping in the TorchText tokenizer?,1
v0.8,Workaround for: https://issues.apache.org/jira/browse/ARROW-5645,1
v0.8,"HACK(Arnav): gpt, gpt2 and llama tokenizers had no pad tokens.",1
v0.8,TODO(travis): do this through an interface rather than conditional logic,1
v0.8,TODO(travis): move to cached_property when we drop Python 3.7.,1
v0.8,TODO: remove reshaping once https://github.com/microsoft/LightGBM/pull/4925 is released,1
v0.8,TODO: remove ravel once https://github.com/microsoft/LightGBM/pull/4925 is released,1
v0.8,TODO (jeffkinnison): revert to use the requested device once torch device usage is standardized,1
v0.8,TODO(shreya): Confirm types of args,1
v0.8,"TODO: when loading an existing model, this loses metric values for all but the last epoch.",1
v0.8,TODO(travis): implement imbalance ratio,1
v0.8,"TODO (ASN): add other modalities (image, etc. )",1
v0.8,"TODO(travis): this assumes ECD is the selected model type, which is not problematic for now, as",1
v0.8,"diverge, so we should find a way to remove this. The best solution is to the change the input params from",1
v0.8,TODO(travis): consolidate with implementation in data/ray.py,1
v0.8,TODO: only single task currently,1
v0.8,TODO(travis): include encoder and decoder steps during inference,1
v0.8,"HACK: Llama fast tokenizer takes about 2-4 minutes to load, so we disable it for now.",1
v0.8,TODO: this implementation will not work if resuming from a previous checkpoint. Need to fix this.,1
v0.8,TODO: make this configurable in the future. These parameters are from FastChat:,1
v0.8,"TODO: Wrap device_map=""auto"" in a try-except block since it may not be supported for all models (E.g. BertLMHead)  # noqa",1
v0.8,TODO: only single task currently,1
v0.8,"TODO(Arnav): Seems like doing this again and going between these format types in unnecessary, but",1
v0.8,"HACK(geoffrey): we need a non-empty loss, so we just fill it with zeros",1
v0.8,TODO(travis): this will need to change when we support multiple output features,1
v0.8,"TODO(travis): use the implementation of trainer itself to decide whether to save the model, to",1
v0.8,avoid this hack,1
v0.8,TODO (jeffkinnison): revert to using the requested device for GBMs when device usage is fixed,1
v0.8,TODO: with change to misc_utils.set_random_seed() this may be redundant,1
v0.8,TODO(Justin): Check that the semantics of input_size align with what the combiner's output shape returns,1
v0.8,TODO(justin): Remove dummy implementation. Make input_shape and output_shape functions.,1
v0.8,TODO(geoffrey): figure out why self.index.search segfaults with larger batch sizes,1
v0.8,TODO(travis): support local feature importance,1
v0.8,TODO:,1
v0.8,TODO(travis): add back skip encoders at the end in finally. Shouldn't be an issue in most cases as we,1
v0.8,"TODO(travis): for force plots, need something similar to SHAP E[X]",1
v0.8,"TODO(travis): for force plots, need something similar to SHAP E[X]",1
v0.8,"TODO(travis): Consider changing to `if not torch.is_floating_point(t.dtype)` to simplify, then handle bool",1
v0.8,TODO(travis): pre-compute this during training from the full training dataset.,1
v0.8,"TODO(travis): for force plots, need something similar to SHAP E[X]",1
v0.8,"TODO(travis): for force plots, need something similar to SHAP E[X]",1
v0.8,"TODO: construct new datasets by running encoders (for text, image)",1
v0.8,TODO(shreya): Refactor preprocessing so that this can be moved upstream.,1
v0.8,TODO: refactor this into an interface,1
v0.8,workaround type limitations of the underlying frameworks,1
v0.8,TODO(Arnav): Re-enable in Ray 2.3,1
v0.8,"TODO(travis): we should revisit the user format here, as it silently breaks situations",1
v0.8,"TODO ray: find an informed way to set the parallelism, in practice",1
v0.8,TODO: When this is implemented we also need to update the,1
v0.8,TODO(travis): consolidate with implementation in data/ray.py,1
v0.8,"This is needed because Daft only supports Dataframes, not Series",1
v0.8,TODO(ekl) deprecate this once read fusion is available.,1
v0.8,TODO(geoffrey): ensure this subclasses ray.data.datasource.Reader in ray 1.14,1
v0.8,TODO(ekl) deprecate this once read fusion is available.,1
v0.8,"doesn't work for Snappy, so we double-check ourselves.",1
v0.8,TODO(xwjiang): Remove this later.,1
v0.8,TODO(travis): need to fix checkpoint saving/loading for DeepSpeed to enable tuning,1
v0.8,"TODO(travis): this double-counts on the same device, it should use a cross-communicator instead",1
v0.8,"TODO: remove LOCAL_BACKEND as a global constant, replace with singleton LocalBackend.shared_instance().",1
v0.8,Temporary workaround to prevent tests from automatically using the Ray backend. Taken from,1
v0.8,TODO(travis): add other indexing structures,1
v0.8,TODO(travis): open question if this is needed to ensure all workers using same weights,1
v0.8,TODO(travis): open question if this is needed to ensure all workers using same optimizer state,1
v0.8,"TODO(travis): currently Ray handles this for us, but is subject to hangs if one of the workers raises an",1
v0.7.5,HACK(geoffrey): `hyperopt_resources` is a required resource for hyperopt to prevent deadlocks in Ludwig tests.,1
v0.7.5,TODO(geoffrey): remove for Ray 2.2,1
v0.7.5,is there a better way to do this?,1
v0.7.5,todo the hidden output is actually a tensor. May need modification,1
v0.7.5,todo figure out the output size for parallel 1d conv,1
v0.7.5,TODO:  Need variant of assert_model_parameters_updated() to account for the two step calling sequence,1
v0.7.5,todo: remove code,1
v0.7.5,TODO(ksbrar): Circle back after discussing whether additional properties should be allowed long-term.,1
v0.7.5,TODO(ksbrar): Circle back after discussing whether additional properties should be allowed long-term.,1
v0.7.5,todo: re-add 'attention' after further research in implication of torch,1
v0.7.5,TODO(#1333): Refactor this test once torch sequence generator work is complete.,1
v0.7.5,"TODO(Justin): Move these to test_sequence_generator unit tests, and reintroduce decoder attention, beam_width, and",1
v0.7.5,TODO(travis): We may want to expand upon this in the future to include some checks on model,1
v0.7.5,TODO(travis): We may want to expand upon this in the future to include some checks on model,1
v0.7.5,TODO: Determine whether this is desired behavior. Tracked here:,1
v0.7.5,handled non-determinism when comparing the metrics between the local and Ray backends. We work around this by,1
v0.7.5,TODO: feature type not yet supported,1
v0.7.5,TODO: feature type not yet supported,1
v0.7.5,TODO(geoffrey): Fold modin tests into test_ray_image as @pytest.mark.parametrized once tests are optimized,1
v0.7.5,TODO(travis): move this to separate gpu module so we only have one ray cluster running at a time,1
v0.7.5,TODO(travis): https://github.com/ludwig-ai/ludwig/issues/1956,1
v0.7.5,TODO: Determine if we still need this if-then-else construct,1
v0.7.5,TODO(travis): once we support GBM text features,1
v0.7.5,TODO(shreya): Uncomment when https://github.com/ludwig-ai/ludwig/issues/2039 is fixed.,1
v0.7.5,TODO: remove this workaround when audio preprocessing is fixed.,1
v0.7.5,TODO: these are the only outputs we provide from Torchscript for now,1
v0.7.5,TODO all shadows built in name - come up with a more descriptive name,1
v0.7.5,TODO (Connor): Refactor to use self.config_obj,1
v0.7.5,TODO (ASN): add support for substitute_with_max parameter,1
v0.7.5,TODO (Connor): Refactor to use self.config_obj,1
v0.7.5,TODO: fix for Ray where workers may be of different skus,1
v0.7.5,TODO(travis): remove in favor of on_hyperopt_end for naming consistency,1
v0.7.5,TODO: https://github.com/ludwig-ai/ludwig/issues/2633,1
v0.7.5,todo: revise docstring,1
v0.7.5,todo: assess how to specify padding for equivalent to 'same',1
v0.7.5,todo: determine how to pool_padding equivalent of 'same',1
v0.7.5,todo: fixup docstring,1
v0.7.5,todo: review docstring,1
v0.7.5,todo: fix up docstring,1
v0.7.5,todo: fix up docstring,1
v0.7.5,todo: update docstring as needed,1
v0.7.5,TODO(shreya): Make sure this is updated when FCStack is updated,1
v0.7.5,TODO(Arnav): Remove this once we have reduce_output options set for,1
v0.7.5,TODO(shreya): Confirm that this is it,1
v0.7.5,TODO(shreya): Confirm that this is it,1
v0.7.5,TODO(justin): This may need to be conditioned on which AutoModel gets chosen.,1
v0.7.5,TODO(travis): consolidate this logic with `create_auto_config` to reduce duplication and make the,1
v0.7.5,TODO: Adjust preprocessing parameters according to output feature imbalance.,1
v0.7.5,TODO (ASN): Decide how we want to proceed if at least one trial has,1
v0.7.5,TODO(shreya): Should this hyperopt config param be set here?,1
v0.7.5,TODO (ASN): add image heuristics,1
v0.7.5,"TODO(travis): less hacky way to do this, we should probably allow ModelConfig to be created without output",1
v0.7.5,"todo future: this may be redundant, check",1
v0.7.5,Workaround for including additional tensors from output of input encoders for,1
v0.7.5,TODO(Justin): Think about how to make this communication work for multi-sequence,1
v0.7.5,todo: when https://github.com/ludwig-ai/ludwig/issues/810 is closed,1
v0.7.5,"todo: can we just use projector_size? # hidden_size,",1
v0.7.5,"todo future: this may be redundant, check",1
v0.7.5,"todo future: this may be redundant, check",1
v0.7.5,Workaround for including additional tensors from output of input encoders for,1
v0.7.5,TODO(Justin): Think about how to make this communication work for multi-sequence,1
v0.7.5,todo future: maybe reintroduce these attention function,1
v0.7.5,todo future: maybe reintroduce these attention function,1
v0.7.5,todo future: maybe reintroduce these attention function,1
v0.7.5,TODO(travis): consider moving this behind a general BatchNorm interface to avoid this kludge.,1
v0.7.5,"todo: enumerate for debugging, remove after testing",1
v0.7.5,TODO(shreya): Combine with ResNetBlock by adding a flag.,1
v0.7.5,TODO(shreya): Implement sparse embedding lookup.,1
v0.7.5,# TODO(shreya): Check if this is equivalent,1
v0.7.5,# TODO(shreya): Check if supported in torch,1
v0.7.5,todo: review for generality,1
v0.7.5,TODO: Simplify this.,1
v0.7.5,Dummy implementation.,1
v0.7.5,"TODO(Justin): Add a confusion matrix, see",1
v0.7.5,TODO: add a mechanism for letting the user decide to save it,1
v0.7.5,todo: re-evaluate need for separate handling of `attention` reducer,1
v0.7.5,TODO(Justin): Clean this up.,1
v0.7.5,TODO: change to debug level before merging,1
v0.7.5,TODO: should this raise an exception if not in training mode?,1
v0.7.5,Nested conditional is a workaround to short-circuit boolean evaluation.,1
v0.7.5,TODO: alternatively use get_average_image() for unreachable images,1
v0.7.5,todo future add multiprocessing/multithreading,1
v0.7.5,TODO(travis): do we even need a user param for vector size if we're going to auto-infer it in all,1
v0.7.5,TODO(1891): Remove backward compatibility hack once all models have been retrained with Ludwig after,1
v0.7.5,todo: refactor to reuse SequenceOutputFeature.postprocess_predictions,1
v0.7.5,TODO: add a mechanism for letting the user decide to save it,1
v0.7.5,Convert datetime to int64 to workaround Dask limitation,1
v0.7.5,"TODO pyarrow: this is needed for caching to work with pyarrow. if removed, the following error is raised:",1
v0.7.5,TODO(travis): passing in MODEL_ECD is a hack here that can be removed once we move to using,1
v0.7.5,"encoder schema at all. This hack works for now because all encoders are supported by ECD, so",1
v0.7.5,todo figure out if additional parameters are needed,1
v0.7.5,TODO dask: this needs to work with DataFrames,1
v0.7.5,TODO(travis): decouple config from training_set_metadata so we don't need to,1
v0.7.5,TODO(joppe): support out of memory negative sampling using Dask,1
v0.7.5,TODO(travis): revisit in the future to make this more precise,1
v0.7.5,TODO: Add link to windowing docs.,1
v0.7.5,TODO: figure out correct typing for augmentation_pipeline after refactoring is done,1
v0.7.5,TODO: consider removing this. doesn't work currently and read performance seems generally,1
v0.7.5,TODO: convert to debug message when done with development,1
v0.7.5,"TODO(travis): could try hashing the in-memory dataset, but this is tricky for Dask",1
v0.7.5,todo future: reintroduce the bucketed batcher,1
v0.7.5,TODO ray: implement dynamic batch size,1
v0.7.5,TODO: Change annotation to PublicAPI once Ludwig 0.7 is released,1
v0.7.5,"TODO(Justin): Add this back, or reconsider where this should be computed.",1
v0.7.5,TODO(travis): is this redundant with `clipglobalnorm`?,1
v0.7.5,TODO(travis) consider removing this in the future after deprecation period,1
v0.7.5,TODO: Change to RAISE and update descriptions once we want to enforce strict schemas.,1
v0.7.5,"TODO(travis): this seems much too verbose, does the validation error not show the specific error?",1
v0.7.5,"TODO(travis): too much boilerplate here, we should find a way to abstract all this and only require specifying the",1
v0.7.5,"TODO(travis): this seems much too verbose, does the validation error not show the specific error?",1
v0.7.5,TODO(#1673): Need some more logic here for validating against output features,1
v0.7.5,"TODO: Re-enable ""goss"" when supported: https://github.com/ludwig-ai/ludwig/issues/2988",1
v0.7.5,TODO(travis): figure out why we need these imports to avoid circular import error,1
v0.7.5,"TODO (Connor): Add nesting logic for fc_layers, see fully_connected_module.py",1
v0.7.5,"TODO (Connor): Add nesting logic for fc_layers, see fully_connected_module.py",1
v0.7.5,TODO(travis): seems like this is not really a valid user option. We should probably just remove these,1
v0.7.5,TODO: uncomment when sentencepiece doesn't cause segfaults: https://github.com/ludwig-ai/ludwig/issues/2983,1
v0.7.5,TODO: uncomment once we figure out host memory issue: https://github.com/ludwig-ai/ludwig/issues/3107,1
v0.7.5,TODO: uncomment when CTRL bug (https://github.com/ludwig-ai/ludwig/issues/2977) has been fixed to add back in,1
v0.7.5,TODO(#1673): Add conditional logic for fields like this one:,1
v0.7.5,"TODO(travis): below type comparison is not perfect, as it doesn't consider the case where the default type",1
v0.7.5,"TODO(travis): explore similar contraints for GBMs, which don't have epochs",1
v0.7.5,TODO(travis): move this into helper function,1
v0.7.5,TODO(travis): handle this with helper function,1
v0.7.5,TODO(travis): not needed once we remove existing model config implementation,1
v0.7.5,TODO: Add schema support for Callable,1
v0.7.5,TODO: This should technically be a required paremeter. Do we need to add support for required params?,1
v0.7.5,TODO: Double-check support for this,1
v0.7.5,TODO: Double-check support for this as well as whether Callable args work properly,1
v0.7.5,todo v0.4: currently not clear way to set model graph,1
v0.7.5,TODO(daniel): delete this.,1
v0.7.5,TODO(travis): figure out a good way to support this. The problem with,1
v0.7.5,TODO: need to also include a filename for this figure,1
v0.7.5,TODO(travis): figure out why we need these imports to avoid circular import error,1
v0.7.5,TODO(travis): implement full backend schema,1
v0.7.5,Dummy implementation.,1
v0.7.5,TODO: Use a configurable ratio for how often to use teacher forcing during training.,1
v0.7.5,TODO: Use a configurable ratio for how often to use teacher forcing during training.,1
v0.7.5,Dummy implementation.,1
v0.7.5,"TODO(travis): we assume here that False is always the default, which may not be true. We should dervice",1
v0.7.5,"TODO(travis): stopgap solution, we should make it so we don't need to do this",1
v0.7.5,todo (Wael): tests for all types.,1
v0.7.5,todo (Wael): tests for all types.,1
v0.7.5,Still needed for preprocessing  TODO(Connor): Refactor ludwig/data/preprocessing to use schema,1
v0.7.5,"ORRIBLE HACK, IT'S THE ONLY WAY TO REMOVE PADDING",1
v0.7.5,END OF HORRIBLE HACK,1
v0.7.5,"TODO: more research needed on how to handle RayTune ""sample_from"" search space",1
v0.7.5,HACK(geoffrey): gpt2 has no pad token. Recommendation is to use eos token instead.,1
v0.7.5,TODO(geoffrey): can we better validate tokenizer parity before swapping in the TorchText tokenizer?,1
v0.7.5,Workaround for: https://issues.apache.org/jira/browse/ARROW-5645,1
v0.7.5,TODO(travis): do this through an interface rather than conditional logic,1
v0.7.5,TODO(travis): move to cached_property when we drop Python 3.7.,1
v0.7.5,TODO: remove reshaping once https://github.com/microsoft/LightGBM/pull/4925 is released,1
v0.7.5,TODO: remove ravel once https://github.com/microsoft/LightGBM/pull/4925 is released,1
v0.7.5,TODO (jeffkinnison): revert to use the requested device once torch device usage is standardized,1
v0.7.5,TODO(shreya): Confirm types of args,1
v0.7.5,"TODO: when loading an existing model, this loses metric values for all but the last epoch.",1
v0.7.5,TODO(travis): implement imbalance ratio,1
v0.7.5,"TODO (ASN): add other modalities (image, etc. )",1
v0.7.5,TODO: only single task currently,1
v0.7.5,TODO(travis): include encoder and decoder steps during inference,1
v0.7.5,TODO (jeffkinnison): revert to using the requested device for GBMs when device usage is fixed,1
v0.7.5,TODO: with change to misc_utils.set_random_seed() this may be redundant,1
v0.7.5,TODO(Justin): Check that the semantics of input_size align with what the combiner's output shape returns,1
v0.7.5,TODO(justin): Remove dummy implementation. Make input_shape and output_shape functions.,1
v0.7.5,TODO(travis): support local feature importance,1
v0.7.5,TODO:,1
v0.7.5,TODO(travis): add back skip encoders at the end in finally. Shouldn't be an issue in most cases as we,1
v0.7.5,"TODO(travis): for force plots, need something similar to SHAP E[X]",1
v0.7.5,"TODO(travis): for force plots, need something similar to SHAP E[X]",1
v0.7.5,"TODO(travis): Consider changing to `if not torch.is_floating_point(t.dtype)` to simplify, then handle bool",1
v0.7.5,TODO(travis): pre-compute this during training from the full training dataset.,1
v0.7.5,"TODO(travis): for force plots, need something similar to SHAP E[X]",1
v0.7.5,"TODO(travis): for force plots, need something similar to SHAP E[X]",1
v0.7.5,"TODO: construct new datasets by running encoders (for text, image)",1
v0.7.5,TODO(shreya): Refactor preprocessing so that this can be moved upstream.,1
v0.7.5,"workaround this, we maintain a separate attribute for the wrapped model, which will be used for training",1
v0.7.5,TODO: refactor this into an interface,1
v0.7.5,workaround type limitations of the underlying frameworks,1
v0.7.5,TODO(Arnav): Re-enable in Ray 2.3,1
v0.7.5,"TODO(travis): we should revisit the user format here, as it silently breaks situations",1
v0.7.5,"TODO ray: find an informed way to set the parallelism, in practice",1
v0.7.5,TODO: When this is implemented we also need to update the,1
v0.7.5,TODO(travis): determine if there is a performance penalty to passing in individual files instead of,1
v0.7.5,TODO(ekl) deprecate this once read fusion is available.,1
v0.7.5,TODO(geoffrey): ensure this subclasses ray.data.datasource.Reader in ray 1.14,1
v0.7.5,TODO(ekl) deprecate this once read fusion is available.,1
v0.7.5,"doesn't work for Snappy, so we double-check ourselves.",1
v0.7.5,TODO(xwjiang): Remove this later.,1
v0.7.5,"TODO(travis): this double-counts on the same device, it should use a cross-communicator instead",1
v0.7.5,"TODO: remove LOCAL_BACKEND as a global constant, replace with singleton LocalBackend.shared_instance().",1
v0.7.5,Temporary workaround to prevent tests from automatically using the Ray backend. Taken from,1
v0.7.5,TODO(travis): open question if this is needed to ensure all workers using same weights,1
v0.7.5,TODO(travis): open question if this is needed to ensure all workers using same optimizer state,1
v0.7.5,"TODO(travis): currently Ray handles this for us, but is subject to hangs if one of the workers raises an",1
v0.7.4,HACK(geoffrey): `hyperopt_resources` is a required resource for hyperopt to prevent deadlocks in Ludwig tests.,1
v0.7.4,TODO(geoffrey): remove for Ray 2.2,1
v0.7.4,is there a better way to do this?,1
v0.7.4,todo the hidden output is actually a tensor. May need modification,1
v0.7.4,todo figure out the output size for parallel 1d conv,1
v0.7.4,TODO:  Need variant of assert_model_parameters_updated() to account for the two step calling sequence,1
v0.7.4,todo: remove code,1
v0.7.4,TODO(ksbrar): Circle back after discussing whether additional properties should be allowed long-term.,1
v0.7.4,TODO(ksbrar): Circle back after discussing whether additional properties should be allowed long-term.,1
v0.7.4,todo: re-add 'attention' after further research in implication of torch,1
v0.7.4,TODO(#1333): Refactor this test once torch sequence generator work is complete.,1
v0.7.4,"TODO(Justin): Move these to test_sequence_generator unit tests, and reintroduce decoder attention, beam_width, and",1
v0.7.4,TODO(travis): We may want to expand upon this in the future to include some checks on model,1
v0.7.4,TODO(travis): We may want to expand upon this in the future to include some checks on model,1
v0.7.4,TODO: Determine whether this is desired behavior. Tracked here:,1
v0.7.4,handled non-determinism when comparing the metrics between the local and Ray backends. We work around this by,1
v0.7.4,TODO: feature type not yet supported,1
v0.7.4,TODO: feature type not yet supported,1
v0.7.4,TODO(geoffrey): Fold modin tests into test_ray_image as @pytest.mark.parametrized once tests are optimized,1
v0.7.4,TODO(travis): move this to separate gpu module so we only have one ray cluster running at a time,1
v0.7.4,TODO(travis): https://github.com/ludwig-ai/ludwig/issues/1956,1
v0.7.4,TODO: Determine if we still need this if-then-else construct,1
v0.7.4,TODO(travis): once we support GBM text features,1
v0.7.4,TODO(shreya): Uncomment when https://github.com/ludwig-ai/ludwig/issues/2039 is fixed.,1
v0.7.4,TODO: remove this workaround when audio preprocessing is fixed.,1
v0.7.4,TODO: these are the only outputs we provide from Torchscript for now,1
v0.7.4,TODO all shadows built in name - come up with a more descriptive name,1
v0.7.4,TODO (Connor): Refactor to use self.config_obj,1
v0.7.4,TODO (ASN): add support for substitute_with_max parameter,1
v0.7.4,TODO (Connor): Refactor to use self.config_obj,1
v0.7.4,TODO: fix for Ray where workers may be of different skus,1
v0.7.4,TODO(travis): remove in favor of on_hyperopt_end for naming consistency,1
v0.7.4,TODO: https://github.com/ludwig-ai/ludwig/issues/2633,1
v0.7.4,todo: revise docstring,1
v0.7.4,todo: assess how to specify padding for equivalent to 'same',1
v0.7.4,todo: determine how to pool_padding equivalent of 'same',1
v0.7.4,todo: fixup docstring,1
v0.7.4,todo: review docstring,1
v0.7.4,todo: fix up docstring,1
v0.7.4,todo: fix up docstring,1
v0.7.4,todo: update docstring as needed,1
v0.7.4,TODO(shreya): Make sure this is updated when FCStack is updated,1
v0.7.4,TODO(Arnav): Remove this once we have reduce_output options set for,1
v0.7.4,TODO(shreya): Confirm that this is it,1
v0.7.4,TODO(shreya): Confirm that this is it,1
v0.7.4,TODO(justin): This may need to be conditioned on which AutoModel gets chosen.,1
v0.7.4,TODO(travis): consolidate this logic with `create_auto_config` to reduce duplication and make the,1
v0.7.4,TODO: Adjust preprocessing parameters according to output feature imbalance.,1
v0.7.4,TODO (ASN): Decide how we want to proceed if at least one trial has,1
v0.7.4,TODO(shreya): Should this hyperopt config param be set here?,1
v0.7.4,TODO (ASN): add image heuristics,1
v0.7.4,"TODO(travis): less hacky way to do this, we should probably allow ModelConfig to be created without output",1
v0.7.4,"todo future: this may be redundant, check",1
v0.7.4,Workaround for including additional tensors from output of input encoders for,1
v0.7.4,TODO(Justin): Think about how to make this communication work for multi-sequence,1
v0.7.4,todo: when https://github.com/ludwig-ai/ludwig/issues/810 is closed,1
v0.7.4,"todo: can we just use projector_size? # hidden_size,",1
v0.7.4,"todo future: this may be redundant, check",1
v0.7.4,"todo future: this may be redundant, check",1
v0.7.4,Workaround for including additional tensors from output of input encoders for,1
v0.7.4,TODO(Justin): Think about how to make this communication work for multi-sequence,1
v0.7.4,todo future: maybe reintroduce these attention function,1
v0.7.4,todo future: maybe reintroduce these attention function,1
v0.7.4,todo future: maybe reintroduce these attention function,1
v0.7.4,TODO(travis): consider moving this behind a general BatchNorm interface to avoid this kludge.,1
v0.7.4,"todo: enumerate for debugging, remove after testing",1
v0.7.4,TODO(shreya): Combine with ResNetBlock by adding a flag.,1
v0.7.4,TODO(shreya): Implement sparse embedding lookup.,1
v0.7.4,# TODO(shreya): Check if this is equivalent,1
v0.7.4,# TODO(shreya): Check if supported in torch,1
v0.7.4,todo: review for generality,1
v0.7.4,TODO: Simplify this.,1
v0.7.4,Dummy implementation.,1
v0.7.4,"TODO(Justin): Add a confusion matrix, see",1
v0.7.4,TODO: add a mechanism for letting the user decide to save it,1
v0.7.4,todo: re-evaluate need for separate handling of `attention` reducer,1
v0.7.4,TODO(Justin): Clean this up.,1
v0.7.4,TODO: change to debug level before merging,1
v0.7.4,TODO: should this raise an exception if not in training mode?,1
v0.7.4,Nested conditional is a workaround to short-circuit boolean evaluation.,1
v0.7.4,TODO: alternatively use get_average_image() for unreachable images,1
v0.7.4,todo future add multiprocessing/multithreading,1
v0.7.4,TODO(travis): do we even need a user param for vector size if we're going to auto-infer it in all,1
v0.7.4,TODO(1891): Remove backward compatibility hack once all models have been retrained with Ludwig after,1
v0.7.4,todo: refactor to reuse SequenceOutputFeature.postprocess_predictions,1
v0.7.4,TODO: add a mechanism for letting the user decide to save it,1
v0.7.4,Convert datetime to int64 to workaround Dask limitation,1
v0.7.4,"TODO pyarrow: this is needed for caching to work with pyarrow. if removed, the following error is raised:",1
v0.7.4,TODO(travis): passing in MODEL_ECD is a hack here that can be removed once we move to using,1
v0.7.4,"encoder schema at all. This hack works for now because all encoders are supported by ECD, so",1
v0.7.4,todo figure out if additional parameters are needed,1
v0.7.4,TODO dask: this needs to work with DataFrames,1
v0.7.4,TODO(travis): decouple config from training_set_metadata so we don't need to,1
v0.7.4,TODO(joppe): support out of memory negative sampling using Dask,1
v0.7.4,TODO(travis): revisit in the future to make this more precise,1
v0.7.4,TODO: Add link to windowing docs.,1
v0.7.4,TODO: figure out correct typing for augmentation_pipeline after refactoring is done,1
v0.7.4,TODO: consider removing this. doesn't work currently and read performance seems generally,1
v0.7.4,TODO: convert to debug message when done with development,1
v0.7.4,"TODO(travis): could try hashing the in-memory dataset, but this is tricky for Dask",1
v0.7.4,todo future: reintroduce the bucketed batcher,1
v0.7.4,TODO ray: implement dynamic batch size,1
v0.7.4,TODO: Change annotation to PublicAPI once Ludwig 0.7 is released,1
v0.7.4,"TODO(Justin): Add this back, or reconsider where this should be computed.",1
v0.7.4,TODO(travis): is this redundant with `clipglobalnorm`?,1
v0.7.4,TODO(travis) consider removing this in the future after deprecation period,1
v0.7.4,TODO: Change to RAISE and update descriptions once we want to enforce strict schemas.,1
v0.7.4,"TODO(travis): this seems much too verbose, does the validation error not show the specific error?",1
v0.7.4,"TODO(travis): too much boilerplate here, we should find a way to abstract all this and only require specifying the",1
v0.7.4,"TODO(travis): this seems much too verbose, does the validation error not show the specific error?",1
v0.7.4,TODO(#1673): Need some more logic here for validating against output features,1
v0.7.4,"TODO: Re-enable ""goss"" when supported: https://github.com/ludwig-ai/ludwig/issues/2988",1
v0.7.4,TODO(travis): figure out why we need these imports to avoid circular import error,1
v0.7.4,"TODO (Connor): Add nesting logic for fc_layers, see fully_connected_module.py",1
v0.7.4,"TODO (Connor): Add nesting logic for fc_layers, see fully_connected_module.py",1
v0.7.4,TODO(travis): seems like this is not really a valid user option. We should probably just remove these,1
v0.7.4,TODO: uncomment when sentencepiece doesn't cause segfaults: https://github.com/ludwig-ai/ludwig/issues/2983,1
v0.7.4,TODO: uncomment once we figure out host memory issue: https://github.com/ludwig-ai/ludwig/issues/3107,1
v0.7.4,TODO: uncomment when CTRL bug (https://github.com/ludwig-ai/ludwig/issues/2977) has been fixed to add back in,1
v0.7.4,TODO(#1673): Add conditional logic for fields like this one:,1
v0.7.4,"TODO(travis): below type comparison is not perfect, as it doesn't consider the case where the default type",1
v0.7.4,"TODO(travis): explore similar contraints for GBMs, which don't have epochs",1
v0.7.4,TODO(travis): move this into helper function,1
v0.7.4,TODO(travis): handle this with helper function,1
v0.7.4,TODO(travis): not needed once we remove existing model config implementation,1
v0.7.4,TODO: Add schema support for Callable,1
v0.7.4,TODO: This should technically be a required paremeter. Do we need to add support for required params?,1
v0.7.4,TODO: Double-check support for this,1
v0.7.4,TODO: Double-check support for this as well as whether Callable args work properly,1
v0.7.4,todo v0.4: currently not clear way to set model graph,1
v0.7.4,TODO(daniel): delete this.,1
v0.7.4,TODO(travis): figure out a good way to support this. The problem with,1
v0.7.4,TODO: need to also include a filename for this figure,1
v0.7.4,TODO(travis): figure out why we need these imports to avoid circular import error,1
v0.7.4,TODO(travis): implement full backend schema,1
v0.7.4,Dummy implementation.,1
v0.7.4,TODO: Use a configurable ratio for how often to use teacher forcing during training.,1
v0.7.4,TODO: Use a configurable ratio for how often to use teacher forcing during training.,1
v0.7.4,Dummy implementation.,1
v0.7.4,"TODO(travis): we assume here that False is always the default, which may not be true. We should dervice",1
v0.7.4,"TODO(travis): stopgap solution, we should make it so we don't need to do this",1
v0.7.4,todo (Wael): tests for all types.,1
v0.7.4,todo (Wael): tests for all types.,1
v0.7.4,Still needed for preprocessing  TODO(Connor): Refactor ludwig/data/preprocessing to use schema,1
v0.7.4,"ORRIBLE HACK, IT'S THE ONLY WAY TO REMOVE PADDING",1
v0.7.4,END OF HORRIBLE HACK,1
v0.7.4,"TODO: more research needed on how to handle RayTune ""sample_from"" search space",1
v0.7.4,HACK(geoffrey): gpt2 has no pad token. Recommendation is to use eos token instead.,1
v0.7.4,TODO(geoffrey): can we better validate tokenizer parity before swapping in the TorchText tokenizer?,1
v0.7.4,Workaround for: https://issues.apache.org/jira/browse/ARROW-5645,1
v0.7.4,TODO(travis): do this through an interface rather than conditional logic,1
v0.7.4,TODO(travis): move to cached_property when we drop Python 3.7.,1
v0.7.4,TODO: remove reshaping once https://github.com/microsoft/LightGBM/pull/4925 is released,1
v0.7.4,TODO: remove ravel once https://github.com/microsoft/LightGBM/pull/4925 is released,1
v0.7.4,TODO (jeffkinnison): revert to use the requested device once torch device usage is standardized,1
v0.7.4,TODO(shreya): Confirm types of args,1
v0.7.4,"TODO: when loading an existing model, this loses metric values for all but the last epoch.",1
v0.7.4,TODO(travis): implement imbalance ratio,1
v0.7.4,"TODO (ASN): add other modalities (image, etc. )",1
v0.7.4,TODO: only single task currently,1
v0.7.4,TODO(travis): include encoder and decoder steps during inference,1
v0.7.4,TODO (jeffkinnison): revert to using the requested device for GBMs when device usage is fixed,1
v0.7.4,TODO: with change to misc_utils.set_random_seed() this may be redundant,1
v0.7.4,TODO(Justin): Check that the semantics of input_size align with what the combiner's output shape returns,1
v0.7.4,TODO(justin): Remove dummy implementation. Make input_shape and output_shape functions.,1
v0.7.4,TODO(travis): support local feature importance,1
v0.7.4,TODO:,1
v0.7.4,TODO(travis): add back skip encoders at the end in finally. Shouldn't be an issue in most cases as we,1
v0.7.4,"TODO(travis): for force plots, need something similar to SHAP E[X]",1
v0.7.4,"TODO(travis): for force plots, need something similar to SHAP E[X]",1
v0.7.4,"TODO(travis): Consider changing to `if not torch.is_floating_point(t.dtype)` to simplify, then handle bool",1
v0.7.4,TODO(travis): pre-compute this during training from the full training dataset.,1
v0.7.4,"TODO(travis): for force plots, need something similar to SHAP E[X]",1
v0.7.4,"TODO(travis): for force plots, need something similar to SHAP E[X]",1
v0.7.4,"TODO: construct new datasets by running encoders (for text, image)",1
v0.7.4,TODO(shreya): Refactor preprocessing so that this can be moved upstream.,1
v0.7.4,"workaround this, we maintain a separate attribute for the wrapped model, which will be used for training",1
v0.7.4,TODO: refactor this into an interface,1
v0.7.4,workaround type limitations of the underlying frameworks,1
v0.7.4,TODO(Arnav): Re-enable in Ray 2.3,1
v0.7.4,"TODO(travis): we should revisit the user format here, as it silently breaks situations",1
v0.7.4,"TODO ray: find an informed way to set the parallelism, in practice",1
v0.7.4,TODO: When this is implemented we also need to update the,1
v0.7.4,TODO(travis): determine if there is a performance penalty to passing in individual files instead of,1
v0.7.4,TODO(ekl) deprecate this once read fusion is available.,1
v0.7.4,TODO(geoffrey): ensure this subclasses ray.data.datasource.Reader in ray 1.14,1
v0.7.4,TODO(ekl) deprecate this once read fusion is available.,1
v0.7.4,"doesn't work for Snappy, so we double-check ourselves.",1
v0.7.4,TODO(xwjiang): Remove this later.,1
v0.7.4,"TODO(travis): this double-counts on the same device, it should use a cross-communicator instead",1
v0.7.4,"TODO: remove LOCAL_BACKEND as a global constant, replace with singleton LocalBackend.shared_instance().",1
v0.7.4,Temporary workaround to prevent tests from automatically using the Ray backend. Taken from,1
v0.7.4,TODO(travis): open question if this is needed to ensure all workers using same weights,1
v0.7.4,TODO(travis): open question if this is needed to ensure all workers using same optimizer state,1
v0.7.4,"TODO(travis): currently Ray handles this for us, but is subject to hangs if one of the workers raises an",1
v0.7.3,HACK(geoffrey): `hyperopt_resources` is a required resource for hyperopt to prevent deadlocks in Ludwig tests.,1
v0.7.3,TODO(geoffrey): remove for Ray 2.2,1
v0.7.3,is there a better way to do this?,1
v0.7.3,todo the hidden output is actually a tensor. May need modification,1
v0.7.3,todo figure out the output size for parallel 1d conv,1
v0.7.3,TODO:  Need variant of assert_model_parameters_updated() to account for the two step calling sequence,1
v0.7.3,todo: remove code,1
v0.7.3,TODO(ksbrar): Circle back after discussing whether additional properties should be allowed long-term.,1
v0.7.3,TODO(ksbrar): Circle back after discussing whether additional properties should be allowed long-term.,1
v0.7.3,todo: re-add 'attention' after further research in implication of torch,1
v0.7.3,TODO(#1333): Refactor this test once torch sequence generator work is complete.,1
v0.7.3,"TODO(Justin): Move these to test_sequence_generator unit tests, and reintroduce decoder attention, beam_width, and",1
v0.7.3,TODO(travis): We may want to expand upon this in the future to include some checks on model,1
v0.7.3,TODO(travis): We may want to expand upon this in the future to include some checks on model,1
v0.7.3,TODO: Determine whether this is desired behavior. Tracked here:,1
v0.7.3,handled non-determinism when comparing the metrics between the local and Ray backends. We work around this by,1
v0.7.3,TODO: feature type not yet supported,1
v0.7.3,TODO: feature type not yet supported,1
v0.7.3,TODO(geoffrey): Fold modin tests into test_ray_image as @pytest.mark.parametrized once tests are optimized,1
v0.7.3,TODO(travis): move this to separate gpu module so we only have one ray cluster running at a time,1
v0.7.3,TODO(travis): https://github.com/ludwig-ai/ludwig/issues/1956,1
v0.7.3,TODO: Determine if we still need this if-then-else construct,1
v0.7.3,TODO(travis): once we support GBM text features,1
v0.7.3,TODO(shreya): Uncomment when https://github.com/ludwig-ai/ludwig/issues/2039 is fixed.,1
v0.7.3,TODO: remove this workaround when audio preprocessing is fixed.,1
v0.7.3,TODO: these are the only outputs we provide from Torchscript for now,1
v0.7.3,TODO all shadows built in name - come up with a more descriptive name,1
v0.7.3,TODO (Connor): Refactor to use self.config_obj,1
v0.7.3,TODO (ASN): add support for substitute_with_max parameter,1
v0.7.3,TODO (Connor): Refactor to use self.config_obj,1
v0.7.3,TODO: fix for Ray where workers may be of different skus,1
v0.7.3,TODO(travis): remove in favor of on_hyperopt_end for naming consistency,1
v0.7.3,TODO: https://github.com/ludwig-ai/ludwig/issues/2633,1
v0.7.3,todo: revise docstring,1
v0.7.3,todo: assess how to specify padding for equivalent to 'same',1
v0.7.3,todo: determine how to pool_padding equivalent of 'same',1
v0.7.3,todo: fixup docstring,1
v0.7.3,todo: review docstring,1
v0.7.3,todo: fix up docstring,1
v0.7.3,todo: fix up docstring,1
v0.7.3,todo: update docstring as needed,1
v0.7.3,TODO(shreya): Make sure this is updated when FCStack is updated,1
v0.7.3,TODO(Arnav): Remove this once we have reduce_output options set for,1
v0.7.3,TODO(shreya): Confirm that this is it,1
v0.7.3,TODO(shreya): Confirm that this is it,1
v0.7.3,TODO(justin): This may need to be conditioned on which AutoModel gets chosen.,1
v0.7.3,TODO(travis): consolidate this logic with `create_auto_config` to reduce duplication and make the,1
v0.7.3,TODO: Adjust preprocessing parameters according to output feature imbalance.,1
v0.7.3,TODO (ASN): Decide how we want to proceed if at least one trial has,1
v0.7.3,TODO(shreya): Should this hyperopt config param be set here?,1
v0.7.3,TODO (ASN): add image heuristics,1
v0.7.3,"TODO(travis): less hacky way to do this, we should probably allow ModelConfig to be created without output",1
v0.7.3,"todo future: this may be redundant, check",1
v0.7.3,Workaround for including additional tensors from output of input encoders for,1
v0.7.3,TODO(Justin): Think about how to make this communication work for multi-sequence,1
v0.7.3,todo: when https://github.com/ludwig-ai/ludwig/issues/810 is closed,1
v0.7.3,"todo: can we just use projector_size? # hidden_size,",1
v0.7.3,"todo future: this may be redundant, check",1
v0.7.3,"todo future: this may be redundant, check",1
v0.7.3,Workaround for including additional tensors from output of input encoders for,1
v0.7.3,TODO(Justin): Think about how to make this communication work for multi-sequence,1
v0.7.3,todo future: maybe reintroduce these attention function,1
v0.7.3,todo future: maybe reintroduce these attention function,1
v0.7.3,todo future: maybe reintroduce these attention function,1
v0.7.3,TODO(travis): consider moving this behind a general BatchNorm interface to avoid this kludge.,1
v0.7.3,"todo: enumerate for debugging, remove after testing",1
v0.7.3,TODO(shreya): Combine with ResNetBlock by adding a flag.,1
v0.7.3,TODO(shreya): Implement sparse embedding lookup.,1
v0.7.3,# TODO(shreya): Check if this is equivalent,1
v0.7.3,# TODO(shreya): Check if supported in torch,1
v0.7.3,todo: review for generality,1
v0.7.3,TODO: Simplify this.,1
v0.7.3,Dummy implementation.,1
v0.7.3,"TODO(Justin): Add a confusion matrix, see",1
v0.7.3,TODO: add a mechanism for letting the user decide to save it,1
v0.7.3,todo: re-evaluate need for separate handling of `attention` reducer,1
v0.7.3,TODO(Justin): Clean this up.,1
v0.7.3,TODO: change to debug level before merging,1
v0.7.3,TODO: should this raise an exception if not in training mode?,1
v0.7.3,Nested conditional is a workaround to short-circuit boolean evaluation.,1
v0.7.3,TODO: alternatively use get_average_image() for unreachable images,1
v0.7.3,todo future add multiprocessing/multithreading,1
v0.7.3,TODO(travis): do we even need a user param for vector size if we're going to auto-infer it in all,1
v0.7.3,TODO(1891): Remove backward compatibility hack once all models have been retrained with Ludwig after,1
v0.7.3,todo: refactor to reuse SequenceOutputFeature.postprocess_predictions,1
v0.7.3,TODO: add a mechanism for letting the user decide to save it,1
v0.7.3,Convert datetime to int64 to workaround Dask limitation,1
v0.7.3,"TODO pyarrow: this is needed for caching to work with pyarrow. if removed, the following error is raised:",1
v0.7.3,TODO(travis): passing in MODEL_ECD is a hack here that can be removed once we move to using,1
v0.7.3,"encoder schema at all. This hack works for now because all encoders are supported by ECD, so",1
v0.7.3,todo figure out if additional parameters are needed,1
v0.7.3,TODO dask: this needs to work with DataFrames,1
v0.7.3,TODO(travis): decouple config from training_set_metadata so we don't need to,1
v0.7.3,TODO(joppe): support out of memory negative sampling using Dask,1
v0.7.3,TODO(travis): revisit in the future to make this more precise,1
v0.7.3,TODO: Add link to windowing docs.,1
v0.7.3,TODO: figure out correct typing for augmentation_pipeline after refactoring is done,1
v0.7.3,TODO: consider removing this. doesn't work currently and read performance seems generally,1
v0.7.3,TODO: convert to debug message when done with development,1
v0.7.3,"TODO(travis): could try hashing the in-memory dataset, but this is tricky for Dask",1
v0.7.3,todo future: reintroduce the bucketed batcher,1
v0.7.3,TODO ray: implement dynamic batch size,1
v0.7.3,TODO: Change annotation to PublicAPI once Ludwig 0.7 is released,1
v0.7.3,"TODO(Justin): Add this back, or reconsider where this should be computed.",1
v0.7.3,TODO(travis): is this redundant with `clipglobalnorm`?,1
v0.7.3,TODO(travis) consider removing this in the future after deprecation period,1
v0.7.3,TODO: Change to RAISE and update descriptions once we want to enforce strict schemas.,1
v0.7.3,"TODO(travis): this seems much too verbose, does the validation error not show the specific error?",1
v0.7.3,"TODO(travis): too much boilerplate here, we should find a way to abstract all this and only require specifying the",1
v0.7.3,"TODO(travis): this seems much too verbose, does the validation error not show the specific error?",1
v0.7.3,TODO(#1673): Need some more logic here for validating against output features,1
v0.7.3,"TODO: Re-enable ""goss"" when supported: https://github.com/ludwig-ai/ludwig/issues/2988",1
v0.7.3,TODO(travis): figure out why we need these imports to avoid circular import error,1
v0.7.3,"TODO (Connor): Add nesting logic for fc_layers, see fully_connected_module.py",1
v0.7.3,"TODO (Connor): Add nesting logic for fc_layers, see fully_connected_module.py",1
v0.7.3,TODO(travis): seems like this is not really a valid user option. We should probably just remove these,1
v0.7.3,TODO: uncomment when sentencepiece doesn't cause segfaults: https://github.com/ludwig-ai/ludwig/issues/2983,1
v0.7.3,TODO: uncomment once we figure out host memory issue: https://github.com/ludwig-ai/ludwig/issues/3107,1
v0.7.3,TODO: uncomment when CTRL bug (https://github.com/ludwig-ai/ludwig/issues/2977) has been fixed to add back in,1
v0.7.3,TODO(#1673): Add conditional logic for fields like this one:,1
v0.7.3,"TODO(travis): below type comparison is not perfect, as it doesn't consider the case where the default type",1
v0.7.3,"TODO(travis): explore similar contraints for GBMs, which don't have epochs",1
v0.7.3,TODO(travis): move this into helper function,1
v0.7.3,TODO(travis): handle this with helper function,1
v0.7.3,TODO(travis): not needed once we remove existing model config implementation,1
v0.7.3,TODO: Add schema support for Callable,1
v0.7.3,TODO: This should technically be a required paremeter. Do we need to add support for required params?,1
v0.7.3,TODO: Double-check support for this,1
v0.7.3,TODO: Double-check support for this as well as whether Callable args work properly,1
v0.7.3,todo v0.4: currently not clear way to set model graph,1
v0.7.3,TODO(daniel): delete this.,1
v0.7.3,TODO(travis): figure out a good way to support this. The problem with,1
v0.7.3,TODO: need to also include a filename for this figure,1
v0.7.3,TODO(travis): figure out why we need these imports to avoid circular import error,1
v0.7.3,TODO(travis): implement full backend schema,1
v0.7.3,Dummy implementation.,1
v0.7.3,TODO: Use a configurable ratio for how often to use teacher forcing during training.,1
v0.7.3,TODO: Use a configurable ratio for how often to use teacher forcing during training.,1
v0.7.3,Dummy implementation.,1
v0.7.3,"TODO(travis): we assume here that False is always the default, which may not be true. We should dervice",1
v0.7.3,"TODO(travis): stopgap solution, we should make it so we don't need to do this",1
v0.7.3,todo (Wael): tests for all types.,1
v0.7.3,todo (Wael): tests for all types.,1
v0.7.3,Still needed for preprocessing  TODO(Connor): Refactor ludwig/data/preprocessing to use schema,1
v0.7.3,"ORRIBLE HACK, IT'S THE ONLY WAY TO REMOVE PADDING",1
v0.7.3,END OF HORRIBLE HACK,1
v0.7.3,"TODO: more research needed on how to handle RayTune ""sample_from"" search space",1
v0.7.3,HACK(geoffrey): gpt2 has no pad token. Recommendation is to use eos token instead.,1
v0.7.3,TODO(geoffrey): can we better validate tokenizer parity before swapping in the TorchText tokenizer?,1
v0.7.3,Workaround for: https://issues.apache.org/jira/browse/ARROW-5645,1
v0.7.3,TODO(travis): do this through an interface rather than conditional logic,1
v0.7.3,TODO(travis): move to cached_property when we drop Python 3.7.,1
v0.7.3,TODO: remove reshaping once https://github.com/microsoft/LightGBM/pull/4925 is released,1
v0.7.3,TODO: remove ravel once https://github.com/microsoft/LightGBM/pull/4925 is released,1
v0.7.3,TODO (jeffkinnison): revert to use the requested device once torch device usage is standardized,1
v0.7.3,TODO(shreya): Confirm types of args,1
v0.7.3,"TODO: when loading an existing model, this loses metric values for all but the last epoch.",1
v0.7.3,TODO(travis): implement imbalance ratio,1
v0.7.3,"TODO (ASN): add other modalities (image, etc. )",1
v0.7.3,TODO: only single task currently,1
v0.7.3,TODO(travis): include encoder and decoder steps during inference,1
v0.7.3,TODO (jeffkinnison): revert to using the requested device for GBMs when device usage is fixed,1
v0.7.3,TODO: with change to misc_utils.set_random_seed() this may be redundant,1
v0.7.3,TODO(Justin): Check that the semantics of input_size align with what the combiner's output shape returns,1
v0.7.3,TODO(justin): Remove dummy implementation. Make input_shape and output_shape functions.,1
v0.7.3,TODO(travis): support local feature importance,1
v0.7.3,TODO:,1
v0.7.3,TODO(travis): add back skip encoders at the end in finally. Shouldn't be an issue in most cases as we,1
v0.7.3,"TODO(travis): for force plots, need something similar to SHAP E[X]",1
v0.7.3,"TODO(travis): for force plots, need something similar to SHAP E[X]",1
v0.7.3,"TODO(travis): Consider changing to `if not torch.is_floating_point(t.dtype)` to simplify, then handle bool",1
v0.7.3,TODO(travis): pre-compute this during training from the full training dataset.,1
v0.7.3,"TODO(travis): for force plots, need something similar to SHAP E[X]",1
v0.7.3,"TODO(travis): for force plots, need something similar to SHAP E[X]",1
v0.7.3,"TODO: construct new datasets by running encoders (for text, image)",1
v0.7.3,TODO(shreya): Refactor preprocessing so that this can be moved upstream.,1
v0.7.3,"workaround this, we maintain a separate attribute for the wrapped model, which will be used for training",1
v0.7.3,TODO: refactor this into an interface,1
v0.7.3,workaround type limitations of the underlying frameworks,1
v0.7.3,TODO(Arnav): Re-enable in Ray 2.3,1
v0.7.3,"TODO(travis): we should revisit the user format here, as it silently breaks situations",1
v0.7.3,"TODO ray: find an informed way to set the parallelism, in practice",1
v0.7.3,TODO: When this is implemented we also need to update the,1
v0.7.3,TODO(travis): determine if there is a performance penalty to passing in individual files instead of,1
v0.7.3,TODO(ekl) deprecate this once read fusion is available.,1
v0.7.3,TODO(geoffrey): ensure this subclasses ray.data.datasource.Reader in ray 1.14,1
v0.7.3,TODO(ekl) deprecate this once read fusion is available.,1
v0.7.3,"doesn't work for Snappy, so we double-check ourselves.",1
v0.7.3,TODO(xwjiang): Remove this later.,1
v0.7.3,"TODO(travis): this double-counts on the same device, it should use a cross-communicator instead",1
v0.7.3,"TODO: remove LOCAL_BACKEND as a global constant, replace with singleton LocalBackend.shared_instance().",1
v0.7.3,Temporary workaround to prevent tests from automatically using the Ray backend. Taken from,1
v0.7.3,TODO(travis): open question if this is needed to ensure all workers using same weights,1
v0.7.3,TODO(travis): open question if this is needed to ensure all workers using same optimizer state,1
v0.7.3,"TODO(travis): currently Ray handles this for us, but is subject to hangs if one of the workers raises an",1
v0.7.2,HACK(geoffrey): `hyperopt_resources` is a required resource for hyperopt to prevent deadlocks in Ludwig tests.,1
v0.7.2,TODO(geoffrey): remove for Ray 2.2,1
v0.7.2,is there a better way to do this?,1
v0.7.2,todo the hidden output is actually a tensor. May need modification,1
v0.7.2,todo figure out the output size for parallel 1d conv,1
v0.7.2,TODO:  Need variant of assert_model_parameters_updated() to account for the two step calling sequence,1
v0.7.2,todo: remove code,1
v0.7.2,TODO(ksbrar): Circle back after discussing whether additional properties should be allowed long-term.,1
v0.7.2,TODO(ksbrar): Circle back after discussing whether additional properties should be allowed long-term.,1
v0.7.2,todo: re-add 'attention' after further research in implication of torch,1
v0.7.2,TODO(#1333): Refactor this test once torch sequence generator work is complete.,1
v0.7.2,"TODO(Justin): Move these to test_sequence_generator unit tests, and reintroduce decoder attention, beam_width, and",1
v0.7.2,TODO(travis): We may want to expand upon this in the future to include some checks on model,1
v0.7.2,TODO: Determine whether this is desired behavior. Tracked here:,1
v0.7.2,handled non-determinism when comparing the metrics between the local and Ray backends. We work around this by,1
v0.7.2,TODO: feature type not yet supported,1
v0.7.2,TODO: feature type not yet supported,1
v0.7.2,TODO(geoffrey): Fold modin tests into test_ray_image as @pytest.mark.parametrized once tests are optimized,1
v0.7.2,TODO(travis): move this to separate gpu module so we only have one ray cluster running at a time,1
v0.7.2,TODO(travis): https://github.com/ludwig-ai/ludwig/issues/1956,1
v0.7.2,TODO: Determine if we still need this if-then-else construct,1
v0.7.2,TODO(travis): once we support GBM text features,1
v0.7.2,TODO(shreya): Uncomment when https://github.com/ludwig-ai/ludwig/issues/2039 is fixed.,1
v0.7.2,TODO: remove this workaround when audio preprocessing is fixed.,1
v0.7.2,TODO: these are the only outputs we provide from Torchscript for now,1
v0.7.2,TODO all shadows built in name - come up with a more descriptive name,1
v0.7.2,TODO (Connor): Refactor to use self.config_obj,1
v0.7.2,TODO (ASN): add support for substitute_with_max parameter,1
v0.7.2,TODO (Connor): Refactor to use self.config_obj,1
v0.7.2,TODO: fix for Ray where workers may be of different skus,1
v0.7.2,TODO(travis): remove in favor of on_hyperopt_end for naming consistency,1
v0.7.2,TODO: https://github.com/ludwig-ai/ludwig/issues/2633,1
v0.7.2,todo: revise docstring,1
v0.7.2,todo: assess how to specify padding for equivalent to 'same',1
v0.7.2,todo: determine how to pool_padding equivalent of 'same',1
v0.7.2,todo: fixup docstring,1
v0.7.2,todo: review docstring,1
v0.7.2,todo: fix up docstring,1
v0.7.2,todo: fix up docstring,1
v0.7.2,todo: update docstring as needed,1
v0.7.2,TODO(shreya): Make sure this is updated when FCStack is updated,1
v0.7.2,TODO(Arnav): Remove this once we have reduce_output options set for,1
v0.7.2,TODO(shreya): Confirm that this is it,1
v0.7.2,TODO(shreya): Confirm that this is it,1
v0.7.2,TODO(justin): This may need to be conditioned on which AutoModel gets chosen.,1
v0.7.2,TODO(travis): consolidate this logic with `create_auto_config` to reduce duplication and make the,1
v0.7.2,TODO: Adjust preprocessing parameters according to output feature imbalance.,1
v0.7.2,TODO (ASN): Decide how we want to proceed if at least one trial has,1
v0.7.2,TODO(shreya): Should this hyperopt config param be set here?,1
v0.7.2,TODO (ASN): add image heuristics,1
v0.7.2,"TODO(travis): less hacky way to do this, we should probably allow ModelConfig to be created without output",1
v0.7.2,"todo future: this may be redundant, check",1
v0.7.2,Workaround for including additional tensors from output of input encoders for,1
v0.7.2,TODO(Justin): Think about how to make this communication work for multi-sequence,1
v0.7.2,todo: when https://github.com/ludwig-ai/ludwig/issues/810 is closed,1
v0.7.2,"todo: can we just use projector_size? # hidden_size,",1
v0.7.2,"todo future: this may be redundant, check",1
v0.7.2,"todo future: this may be redundant, check",1
v0.7.2,Workaround for including additional tensors from output of input encoders for,1
v0.7.2,TODO(Justin): Think about how to make this communication work for multi-sequence,1
v0.7.2,todo future: maybe reintroduce these attention function,1
v0.7.2,todo future: maybe reintroduce these attention function,1
v0.7.2,todo future: maybe reintroduce these attention function,1
v0.7.2,TODO(travis): consider moving this behind a general BatchNorm interface to avoid this kludge.,1
v0.7.2,"todo: enumerate for debugging, remove after testing",1
v0.7.2,TODO(shreya): Combine with ResNetBlock by adding a flag.,1
v0.7.2,TODO(shreya): Implement sparse embedding lookup.,1
v0.7.2,# TODO(shreya): Check if this is equivalent,1
v0.7.2,# TODO(shreya): Check if supported in torch,1
v0.7.2,todo: review for generality,1
v0.7.2,TODO: Simplify this.,1
v0.7.2,Dummy implementation.,1
v0.7.2,"TODO(Justin): Add a confusion matrix, see",1
v0.7.2,TODO: add a mechanism for letting the user decide to save it,1
v0.7.2,todo: re-evaluate need for separate handling of `attention` reducer,1
v0.7.2,TODO(Justin): Clean this up.,1
v0.7.2,TODO: change to debug level before merging,1
v0.7.2,TODO: should this raise an exception if not in training mode?,1
v0.7.2,Nested conditional is a workaround to short-circuit boolean evaluation.,1
v0.7.2,TODO: alternatively use get_average_image() for unreachable images,1
v0.7.2,todo future add multiprocessing/multithreading,1
v0.7.2,TODO(travis): do we even need a user param for vector size if we're going to auto-infer it in all,1
v0.7.2,TODO(1891): Remove backward compatibility hack once all models have been retrained with Ludwig after,1
v0.7.2,todo: refactor to reuse SequenceOutputFeature.postprocess_predictions,1
v0.7.2,TODO: add a mechanism for letting the user decide to save it,1
v0.7.2,Convert datetime to int64 to workaround Dask limitation,1
v0.7.2,"TODO pyarrow: this is needed for caching to work with pyarrow. if removed, the following error is raised:",1
v0.7.2,TODO(travis): passing in MODEL_ECD is a hack here that can be removed once we move to using,1
v0.7.2,"encoder schema at all. This hack works for now because all encoders are supported by ECD, so",1
v0.7.2,todo figure out if additional parameters are needed,1
v0.7.2,TODO dask: this needs to work with DataFrames,1
v0.7.2,TODO(travis): decouple config from training_set_metadata so we don't need to,1
v0.7.2,TODO(joppe): support out of memory negative sampling using Dask,1
v0.7.2,TODO(travis): revisit in the future to make this more precise,1
v0.7.2,TODO: Add link to windowing docs.,1
v0.7.2,TODO: figure out correct typing for augmentation_pipeline after refactoring is done,1
v0.7.2,TODO: consider removing this. doesn't work currently and read performance seems generally,1
v0.7.2,TODO: convert to debug message when done with development,1
v0.7.2,"TODO(travis): could try hashing the in-memory dataset, but this is tricky for Dask",1
v0.7.2,todo future: reintroduce the bucketed batcher,1
v0.7.2,TODO ray: implement dynamic batch size,1
v0.7.2,TODO: Change annotation to PublicAPI once Ludwig 0.7 is released,1
v0.7.2,"TODO(Justin): Add this back, or reconsider where this should be computed.",1
v0.7.2,TODO(travis): is this redundant with `clipglobalnorm`?,1
v0.7.2,TODO(travis) consider removing this in the future after deprecation period,1
v0.7.2,"TODO(travis): this seems much too verbose, does the validation error not show the specific error?",1
v0.7.2,"TODO(travis): too much boilerplate here, we should find a way to abstract all this and only require specifying the",1
v0.7.2,"TODO(travis): this seems much too verbose, does the validation error not show the specific error?",1
v0.7.2,TODO(#1673): Need some more logic here for validating against output features,1
v0.7.2,"TODO: Re-enable ""goss"" when supported: https://github.com/ludwig-ai/ludwig/issues/2988",1
v0.7.2,TODO(travis): figure out why we need these imports to avoid circular import error,1
v0.7.2,"TODO (Connor): Add nesting logic for fc_layers, see fully_connected_module.py",1
v0.7.2,"TODO (Connor): Add nesting logic for fc_layers, see fully_connected_module.py",1
v0.7.2,TODO(travis): seems like this is not really a valid user option. We should probably just remove these,1
v0.7.2,TODO: uncomment when sentencepiece doesn't cause segfaults: https://github.com/ludwig-ai/ludwig/issues/2983,1
v0.7.2,TODO: uncomment once we figure out host memory issue: https://github.com/ludwig-ai/ludwig/issues/3107,1
v0.7.2,TODO: uncomment when CTRL bug (https://github.com/ludwig-ai/ludwig/issues/2977) has been fixed to add back in,1
v0.7.2,TODO(#1673): Add conditional logic for fields like this one:,1
v0.7.2,"TODO(travis): below type comparison is not perfect, as it doesn't consider the case where the default type",1
v0.7.2,"TODO(travis): explore similar contraints for GBMs, which don't have epochs",1
v0.7.2,TODO(travis): move this into helper function,1
v0.7.2,TODO(travis): handle this with helper function,1
v0.7.2,TODO(travis): not needed once we remove existing model config implementation,1
v0.7.2,TODO: Add schema support for Callable,1
v0.7.2,TODO: This should technically be a required paremeter. Do we need to add support for required params?,1
v0.7.2,TODO: Double-check support for this,1
v0.7.2,TODO: Double-check support for this as well as whether Callable args work properly,1
v0.7.2,todo v0.4: currently not clear way to set model graph,1
v0.7.2,TODO(daniel): delete this.,1
v0.7.2,TODO(travis): figure out a good way to support this. The problem with,1
v0.7.2,TODO: need to also include a filename for this figure,1
v0.7.2,TODO(travis): figure out why we need these imports to avoid circular import error,1
v0.7.2,TODO(travis): implement full backend schema,1
v0.7.2,Dummy implementation.,1
v0.7.2,TODO: Use a configurable ratio for how often to use teacher forcing during training.,1
v0.7.2,TODO: Use a configurable ratio for how often to use teacher forcing during training.,1
v0.7.2,Dummy implementation.,1
v0.7.2,"TODO(travis): we assume here that False is always the default, which may not be true. We should dervice",1
v0.7.2,"TODO(travis): stopgap solution, we should make it so we don't need to do this",1
v0.7.2,todo (Wael): tests for all types.,1
v0.7.2,todo (Wael): tests for all types.,1
v0.7.2,Still needed for preprocessing  TODO(Connor): Refactor ludwig/data/preprocessing to use schema,1
v0.7.2,"ORRIBLE HACK, IT'S THE ONLY WAY TO REMOVE PADDING",1
v0.7.2,END OF HORRIBLE HACK,1
v0.7.2,"TODO: more research needed on how to handle RayTune ""sample_from"" search space",1
v0.7.2,HACK(geoffrey): gpt2 has no pad token. Recommendation is to use eos token instead.,1
v0.7.2,TODO(geoffrey): can we better validate tokenizer parity before swapping in the TorchText tokenizer?,1
v0.7.2,Workaround for: https://issues.apache.org/jira/browse/ARROW-5645,1
v0.7.2,TODO(travis): do this through an interface rather than conditional logic,1
v0.7.2,TODO(travis): move to cached_property when we drop Python 3.7.,1
v0.7.2,TODO: remove reshaping once https://github.com/microsoft/LightGBM/pull/4925 is released,1
v0.7.2,TODO: remove ravel once https://github.com/microsoft/LightGBM/pull/4925 is released,1
v0.7.2,TODO (jeffkinnison): revert to use the requested device once torch device usage is standardized,1
v0.7.2,TODO(shreya): Confirm types of args,1
v0.7.2,"TODO: when loading an existing model, this loses metric values for all but the last epoch.",1
v0.7.2,TODO(travis): implement imbalance ratio,1
v0.7.2,"TODO (ASN): add other modalities (image, etc. )",1
v0.7.2,TODO: only single task currently,1
v0.7.2,TODO(travis): include encoder and decoder steps during inference,1
v0.7.2,TODO (jeffkinnison): revert to using the requested device for GBMs when device usage is fixed,1
v0.7.2,TODO: with change to misc_utils.set_random_seed() this may be redundant,1
v0.7.2,TODO(Justin): Check that the semantics of input_size align with what the combiner's output shape returns,1
v0.7.2,TODO(justin): Remove dummy implementation. Make input_shape and output_shape functions.,1
v0.7.2,TODO(travis): support local feature importance,1
v0.7.2,TODO:,1
v0.7.2,TODO(travis): add back skip encoders at the end in finally. Shouldn't be an issue in most cases as we,1
v0.7.2,"TODO(travis): for force plots, need something similar to SHAP E[X]",1
v0.7.2,"TODO(travis): for force plots, need something similar to SHAP E[X]",1
v0.7.2,"TODO(travis): Consider changing to `if not torch.is_floating_point(t.dtype)` to simplify, then handle bool",1
v0.7.2,TODO(travis): pre-compute this during training from the full training dataset.,1
v0.7.2,"TODO(travis): for force plots, need something similar to SHAP E[X]",1
v0.7.2,"TODO(travis): for force plots, need something similar to SHAP E[X]",1
v0.7.2,"TODO: construct new datasets by running encoders (for text, image)",1
v0.7.2,TODO(shreya): Refactor preprocessing so that this can be moved upstream.,1
v0.7.2,"workaround this, we maintain a separate attribute for the wrapped model, which will be used for training",1
v0.7.2,TODO: refactor this into an interface,1
v0.7.2,workaround type limitations of the underlying frameworks,1
v0.7.2,TODO(Arnav): Re-enable in Ray 2.3,1
v0.7.2,"TODO(travis): we should revisit the user format here, as it silently breaks situations",1
v0.7.2,"TODO ray: find an informed way to set the parallelism, in practice",1
v0.7.2,TODO: When this is implemented we also need to update the,1
v0.7.2,TODO(travis): determine if there is a performance penalty to passing in individual files instead of,1
v0.7.2,TODO(ekl) deprecate this once read fusion is available.,1
v0.7.2,TODO(geoffrey): ensure this subclasses ray.data.datasource.Reader in ray 1.14,1
v0.7.2,TODO(ekl) deprecate this once read fusion is available.,1
v0.7.2,"doesn't work for Snappy, so we double-check ourselves.",1
v0.7.2,TODO(xwjiang): Remove this later.,1
v0.7.2,"TODO(travis): this double-counts on the same device, it should use a cross-communicator instead",1
v0.7.2,"TODO: remove LOCAL_BACKEND as a global constant, replace with singleton LocalBackend.shared_instance().",1
v0.7.2,Temporary workaround to prevent tests from automatically using the Ray backend. Taken from,1
v0.7.2,TODO(travis): open question if this is needed to ensure all workers using same weights,1
v0.7.2,TODO(travis): open question if this is needed to ensure all workers using same optimizer state,1
v0.7.2,"TODO(travis): currently Ray handles this for us, but is subject to hangs if one of the workers raises an",1
v0.7.1,HACK(geoffrey): `hyperopt_resources` is a required resource for hyperopt to prevent deadlocks in Ludwig tests.,1
v0.7.1,TODO(geoffrey): remove for Ray 2.2,1
v0.7.1,is there a better way to do this?,1
v0.7.1,todo the hidden output is actually a tensor. May need modification,1
v0.7.1,todo figure out the output size for parallel 1d conv,1
v0.7.1,TODO:  Need variant of assert_model_parameters_updated() to account for the two step calling sequence,1
v0.7.1,todo: remove code,1
v0.7.1,TODO(ksbrar): Circle back after discussing whether additional properties should be allowed long-term.,1
v0.7.1,TODO(ksbrar): Circle back after discussing whether additional properties should be allowed long-term.,1
v0.7.1,todo: re-add 'attention' after further research in implication of torch,1
v0.7.1,TODO(#1333): Refactor this test once torch sequence generator work is complete.,1
v0.7.1,"TODO(Justin): Move these to test_sequence_generator unit tests, and reintroduce decoder attention, beam_width, and",1
v0.7.1,TODO(travis): We may want to expand upon this in the future to include some checks on model,1
v0.7.1,TODO: Determine whether this is desired behavior. Tracked here:,1
v0.7.1,handled non-determinism when comparing the metrics between the local and Ray backends. We work around this by,1
v0.7.1,TODO: feature type not yet supported,1
v0.7.1,TODO: feature type not yet supported,1
v0.7.1,TODO(geoffrey): Fold modin tests into test_ray_image as @pytest.mark.parametrized once tests are optimized,1
v0.7.1,TODO(travis): move this to separate gpu module so we only have one ray cluster running at a time,1
v0.7.1,TODO(travis): https://github.com/ludwig-ai/ludwig/issues/1956,1
v0.7.1,TODO: Determine if we still need this if-then-else construct,1
v0.7.1,TODO(travis): once we support GBM text features,1
v0.7.1,TODO(shreya): Uncomment when https://github.com/ludwig-ai/ludwig/issues/2039 is fixed.,1
v0.7.1,TODO: remove this workaround when audio preprocessing is fixed.,1
v0.7.1,TODO: these are the only outputs we provide from Torchscript for now,1
v0.7.1,TODO all shadows built in name - come up with a more descriptive name,1
v0.7.1,TODO (Connor): Refactor to use self.config_obj,1
v0.7.1,TODO (ASN): add support for substitute_with_max parameter,1
v0.7.1,TODO (Connor): Refactor to use self.config_obj,1
v0.7.1,TODO: fix for Ray where workers may be of different skus,1
v0.7.1,TODO(travis): remove in favor of on_hyperopt_end for naming consistency,1
v0.7.1,TODO: https://github.com/ludwig-ai/ludwig/issues/2633,1
v0.7.1,todo: revise docstring,1
v0.7.1,todo: assess how to specify padding for equivalent to 'same',1
v0.7.1,todo: determine how to pool_padding equivalent of 'same',1
v0.7.1,todo: fixup docstring,1
v0.7.1,todo: review docstring,1
v0.7.1,todo: fix up docstring,1
v0.7.1,todo: fix up docstring,1
v0.7.1,todo: update docstring as needed,1
v0.7.1,TODO(shreya): Make sure this is updated when FCStack is updated,1
v0.7.1,TODO(Arnav): Remove this once we have reduce_output options set for,1
v0.7.1,TODO(shreya): Confirm that this is it,1
v0.7.1,TODO(shreya): Confirm that this is it,1
v0.7.1,TODO(justin): This may need to be conditioned on which AutoModel gets chosen.,1
v0.7.1,TODO(travis): consolidate this logic with `create_auto_config` to reduce duplication and make the,1
v0.7.1,TODO: Adjust preprocessing parameters according to output feature imbalance.,1
v0.7.1,TODO (ASN): Decide how we want to proceed if at least one trial has,1
v0.7.1,TODO(shreya): Should this hyperopt config param be set here?,1
v0.7.1,TODO (ASN): add image heuristics,1
v0.7.1,"TODO(travis): less hacky way to do this, we should probably allow ModelConfig to be created without output",1
v0.7.1,"todo future: this may be redundant, check",1
v0.7.1,Workaround for including additional tensors from output of input encoders for,1
v0.7.1,TODO(Justin): Think about how to make this communication work for multi-sequence,1
v0.7.1,todo: when https://github.com/ludwig-ai/ludwig/issues/810 is closed,1
v0.7.1,"todo: can we just use projector_size? # hidden_size,",1
v0.7.1,"todo future: this may be redundant, check",1
v0.7.1,"todo future: this may be redundant, check",1
v0.7.1,Workaround for including additional tensors from output of input encoders for,1
v0.7.1,TODO(Justin): Think about how to make this communication work for multi-sequence,1
v0.7.1,todo future: maybe reintroduce these attention function,1
v0.7.1,todo future: maybe reintroduce these attention function,1
v0.7.1,todo future: maybe reintroduce these attention function,1
v0.7.1,TODO(travis): consider moving this behind a general BatchNorm interface to avoid this kludge.,1
v0.7.1,"todo: enumerate for debugging, remove after testing",1
v0.7.1,TODO(shreya): Combine with ResNetBlock by adding a flag.,1
v0.7.1,TODO(shreya): Implement sparse embedding lookup.,1
v0.7.1,# TODO(shreya): Check if this is equivalent,1
v0.7.1,# TODO(shreya): Check if supported in torch,1
v0.7.1,todo: review for generality,1
v0.7.1,TODO: Simplify this.,1
v0.7.1,Dummy implementation.,1
v0.7.1,"TODO(Justin): Add a confusion matrix, see",1
v0.7.1,TODO: add a mechanism for letting the user decide to save it,1
v0.7.1,todo: re-evaluate need for separate handling of `attention` reducer,1
v0.7.1,TODO(Justin): Clean this up.,1
v0.7.1,TODO: change to debug level before merging,1
v0.7.1,TODO: should this raise an exception if not in training mode?,1
v0.7.1,Nested conditional is a workaround to short-circuit boolean evaluation.,1
v0.7.1,TODO: alternatively use get_average_image() for unreachable images,1
v0.7.1,todo future add multiprocessing/multithreading,1
v0.7.1,TODO(travis): do we even need a user param for vector size if we're going to auto-infer it in all,1
v0.7.1,TODO(1891): Remove backward compatibility hack once all models have been retrained with Ludwig after,1
v0.7.1,todo: refactor to reuse SequenceOutputFeature.postprocess_predictions,1
v0.7.1,TODO: add a mechanism for letting the user decide to save it,1
v0.7.1,Convert datetime to int64 to workaround Dask limitation,1
v0.7.1,"TODO pyarrow: this is needed for caching to work with pyarrow. if removed, the following error is raised:",1
v0.7.1,TODO(travis): passing in MODEL_ECD is a hack here that can be removed once we move to using,1
v0.7.1,"encoder schema at all. This hack works for now because all encoders are supported by ECD, so",1
v0.7.1,todo figure out if additional parameters are needed,1
v0.7.1,TODO dask: this needs to work with DataFrames,1
v0.7.1,TODO(travis): decouple config from training_set_metadata so we don't need to,1
v0.7.1,TODO(joppe): support out of memory negative sampling using Dask,1
v0.7.1,TODO(travis): revisit in the future to make this more precise,1
v0.7.1,TODO: Add link to windowing docs.,1
v0.7.1,TODO: figure out correct typing for augmentation_pipeline after refactoring is done,1
v0.7.1,TODO: consider removing this. doesn't work currently and read performance seems generally,1
v0.7.1,TODO: convert to debug message when done with development,1
v0.7.1,"TODO(travis): could try hashing the in-memory dataset, but this is tricky for Dask",1
v0.7.1,todo future: reintroduce the bucketed batcher,1
v0.7.1,TODO ray: implement dynamic batch size,1
v0.7.1,TODO: Change annotation to PublicAPI once Ludwig 0.7 is released,1
v0.7.1,"TODO(Justin): Add this back, or reconsider where this should be computed.",1
v0.7.1,TODO(travis): is this redundant with `clipglobalnorm`?,1
v0.7.1,TODO(travis) consider removing this in the future after deprecation period,1
v0.7.1,"TODO(travis): this seems much too verbose, does the validation error not show the specific error?",1
v0.7.1,"TODO(travis): too much boilerplate here, we should find a way to abstract all this and only require specifying the",1
v0.7.1,"TODO(travis): this seems much too verbose, does the validation error not show the specific error?",1
v0.7.1,TODO(#1673): Need some more logic here for validating against output features,1
v0.7.1,"TODO: Re-enable ""goss"" when supported: https://github.com/ludwig-ai/ludwig/issues/2988",1
v0.7.1,TODO(travis): figure out why we need these imports to avoid circular import error,1
v0.7.1,"TODO (Connor): Add nesting logic for fc_layers, see fully_connected_module.py",1
v0.7.1,"TODO (Connor): Add nesting logic for fc_layers, see fully_connected_module.py",1
v0.7.1,TODO(travis): seems like this is not really a valid user option. We should probably just remove these,1
v0.7.1,TODO: uncomment when sentencepiece doesn't cause segfaults: https://github.com/ludwig-ai/ludwig/issues/2983,1
v0.7.1,TODO: uncomment once we figure out host memory issue: https://github.com/ludwig-ai/ludwig/issues/3107,1
v0.7.1,TODO: uncomment when CTRL bug (https://github.com/ludwig-ai/ludwig/issues/2977) has been fixed to add back in,1
v0.7.1,TODO(#1673): Add conditional logic for fields like this one:,1
v0.7.1,"TODO(travis): below type comparison is not perfect, as it doesn't consider the case where the default type",1
v0.7.1,"TODO(travis): explore similar contraints for GBMs, which don't have epochs",1
v0.7.1,TODO(travis): move this into helper function,1
v0.7.1,TODO(travis): handle this with helper function,1
v0.7.1,TODO(travis): not needed once we remove existing model config implementation,1
v0.7.1,TODO: Add schema support for Callable,1
v0.7.1,TODO: This should technically be a required paremeter. Do we need to add support for required params?,1
v0.7.1,TODO: Double-check support for this,1
v0.7.1,TODO: Double-check support for this as well as whether Callable args work properly,1
v0.7.1,todo v0.4: currently not clear way to set model graph,1
v0.7.1,TODO(daniel): delete this.,1
v0.7.1,TODO(travis): figure out a good way to support this. The problem with,1
v0.7.1,TODO: need to also include a filename for this figure,1
v0.7.1,TODO(travis): figure out why we need these imports to avoid circular import error,1
v0.7.1,TODO(travis): implement full backend schema,1
v0.7.1,Dummy implementation.,1
v0.7.1,TODO: Use a configurable ratio for how often to use teacher forcing during training.,1
v0.7.1,TODO: Use a configurable ratio for how often to use teacher forcing during training.,1
v0.7.1,Dummy implementation.,1
v0.7.1,"TODO(travis): we assume here that False is always the default, which may not be true. We should dervice",1
v0.7.1,"TODO(travis): stopgap solution, we should make it so we don't need to do this",1
v0.7.1,todo (Wael): tests for all types.,1
v0.7.1,todo (Wael): tests for all types.,1
v0.7.1,Still needed for preprocessing  TODO(Connor): Refactor ludwig/data/preprocessing to use schema,1
v0.7.1,"ORRIBLE HACK, IT'S THE ONLY WAY TO REMOVE PADDING",1
v0.7.1,END OF HORRIBLE HACK,1
v0.7.1,"TODO: more research needed on how to handle RayTune ""sample_from"" search space",1
v0.7.1,HACK(geoffrey): gpt2 has no pad token. Recommendation is to use eos token instead.,1
v0.7.1,TODO(geoffrey): can we better validate tokenizer parity before swapping in the TorchText tokenizer?,1
v0.7.1,Workaround for: https://issues.apache.org/jira/browse/ARROW-5645,1
v0.7.1,TODO(travis): do this through an interface rather than conditional logic,1
v0.7.1,TODO(travis): move to cached_property when we drop Python 3.7.,1
v0.7.1,TODO: remove reshaping once https://github.com/microsoft/LightGBM/pull/4925 is released,1
v0.7.1,TODO: remove ravel once https://github.com/microsoft/LightGBM/pull/4925 is released,1
v0.7.1,TODO (jeffkinnison): revert to use the requested device once torch device usage is standardized,1
v0.7.1,TODO(shreya): Confirm types of args,1
v0.7.1,"TODO: when loading an existing model, this loses metric values for all but the last epoch.",1
v0.7.1,TODO(travis): implement imbalance ratio,1
v0.7.1,"TODO (ASN): add other modalities (image, etc. )",1
v0.7.1,TODO: only single task currently,1
v0.7.1,TODO(travis): include encoder and decoder steps during inference,1
v0.7.1,TODO (jeffkinnison): revert to using the requested device for GBMs when device usage is fixed,1
v0.7.1,TODO: with change to misc_utils.set_random_seed() this may be redundant,1
v0.7.1,TODO(Justin): Check that the semantics of input_size align with what the combiner's output shape returns,1
v0.7.1,TODO(justin): Remove dummy implementation. Make input_shape and output_shape functions.,1
v0.7.1,TODO(travis): support local feature importance,1
v0.7.1,TODO:,1
v0.7.1,TODO(travis): add back skip encoders at the end in finally. Shouldn't be an issue in most cases as we,1
v0.7.1,"TODO(travis): for force plots, need something similar to SHAP E[X]",1
v0.7.1,"TODO(travis): for force plots, need something similar to SHAP E[X]",1
v0.7.1,"TODO(travis): Consider changing to `if not torch.is_floating_point(t.dtype)` to simplify, then handle bool",1
v0.7.1,TODO(travis): pre-compute this during training from the full training dataset.,1
v0.7.1,"TODO(travis): for force plots, need something similar to SHAP E[X]",1
v0.7.1,"TODO(travis): for force plots, need something similar to SHAP E[X]",1
v0.7.1,"TODO: construct new datasets by running encoders (for text, image)",1
v0.7.1,TODO(shreya): Refactor preprocessing so that this can be moved upstream.,1
v0.7.1,"workaround this, we maintain a separate attribute for the wrapped model, which will be used for training",1
v0.7.1,TODO: refactor this into an interface,1
v0.7.1,workaround type limitations of the underlying frameworks,1
v0.7.1,TODO(Arnav): Re-enable in Ray 2.3,1
v0.7.1,"TODO(travis): we should revisit the user format here, as it silently breaks situations",1
v0.7.1,"TODO ray: find an informed way to set the parallelism, in practice",1
v0.7.1,TODO: When this is implemented we also need to update the,1
v0.7.1,TODO(travis): determine if there is a performance penalty to passing in individual files instead of,1
v0.7.1,TODO(ekl) deprecate this once read fusion is available.,1
v0.7.1,TODO(geoffrey): ensure this subclasses ray.data.datasource.Reader in ray 1.14,1
v0.7.1,TODO(ekl) deprecate this once read fusion is available.,1
v0.7.1,"doesn't work for Snappy, so we double-check ourselves.",1
v0.7.1,TODO(xwjiang): Remove this later.,1
v0.7.1,"TODO(travis): this double-counts on the same device, it should use a cross-communicator instead",1
v0.7.1,"TODO: remove LOCAL_BACKEND as a global constant, replace with singleton LocalBackend.shared_instance().",1
v0.7.1,Temporary workaround to prevent tests from automatically using the Ray backend. Taken from,1
v0.7.1,TODO(travis): open question if this is needed to ensure all workers using same weights,1
v0.7.1,TODO(travis): open question if this is needed to ensure all workers using same optimizer state,1
v0.7.1,"TODO(travis): currently Ray handles this for us, but is subject to hangs if one of the workers raises an",1
v0.7,HACK(geoffrey): `hyperopt_resources` is a required resource for hyperopt to prevent deadlocks in Ludwig tests.,1
v0.7,TODO(geoffrey): remove for Ray 2.2,1
v0.7,is there a better way to do this?,1
v0.7,todo the hidden output is actually a tensor. May need modification,1
v0.7,todo figure out the output size for parallel 1d conv,1
v0.7,TODO:  Need variant of assert_model_parameters_updated() to account for the two step calling sequence,1
v0.7,todo: remove code,1
v0.7,TODO(ksbrar): Circle back after discussing whether additional properties should be allowed long-term.,1
v0.7,TODO(ksbrar): Circle back after discussing whether additional properties should be allowed long-term.,1
v0.7,todo: re-add 'attention' after further research in implication of torch,1
v0.7,TODO(#1333): Refactor this test once torch sequence generator work is complete.,1
v0.7,"TODO(Justin): Move these to test_sequence_generator unit tests, and reintroduce decoder attention, beam_width, and",1
v0.7,TODO(travis): We may want to expand upon this in the future to include some checks on model,1
v0.7,TODO: Determine whether this is desired behavior. Tracked here:,1
v0.7,handled non-determinism when comparing the metrics between the local and Ray backends. We work around this by,1
v0.7,TODO: feature type not yet supported,1
v0.7,TODO: feature type not yet supported,1
v0.7,TODO(geoffrey): Fold modin tests into test_ray_image as @pytest.mark.parametrized once tests are optimized,1
v0.7,TODO(travis): move this to separate gpu module so we only have one ray cluster running at a time,1
v0.7,TODO(travis): https://github.com/ludwig-ai/ludwig/issues/1956,1
v0.7,TODO: Determine if we still need this if-then-else construct,1
v0.7,TODO(travis): once we support GBM text features,1
v0.7,TODO(shreya): Uncomment when https://github.com/ludwig-ai/ludwig/issues/2039 is fixed.,1
v0.7,TODO: remove this workaround when audio preprocessing is fixed.,1
v0.7,TODO: these are the only outputs we provide from Torchscript for now,1
v0.7,TODO all shadows built in name - come up with a more descriptive name,1
v0.7,TODO (Connor): Refactor to use self.config_obj,1
v0.7,TODO (ASN): add support for substitute_with_max parameter,1
v0.7,TODO (Connor): Refactor to use self.config_obj,1
v0.7,TODO: fix for Ray where workers may be of different skus,1
v0.7,TODO(travis): remove in favor of on_hyperopt_end for naming consistency,1
v0.7,TODO: https://github.com/ludwig-ai/ludwig/issues/2633,1
v0.7,todo: revise docstring,1
v0.7,todo: assess how to specify padding for equivalent to 'same',1
v0.7,todo: determine how to pool_padding equivalent of 'same',1
v0.7,todo: fixup docstring,1
v0.7,todo: review docstring,1
v0.7,todo: fix up docstring,1
v0.7,todo: fix up docstring,1
v0.7,todo: update docstring as needed,1
v0.7,TODO(shreya): Make sure this is updated when FCStack is updated,1
v0.7,TODO(Arnav): Remove this once we have reduce_output options set for,1
v0.7,TODO(shreya): Confirm that this is it,1
v0.7,TODO(shreya): Confirm that this is it,1
v0.7,TODO(justin): This may need to be conditioned on which AutoModel gets chosen.,1
v0.7,TODO(travis): consolidate this logic with `create_auto_config` to reduce duplication and make the,1
v0.7,TODO: Adjust preprocessing parameters according to output feature imbalance.,1
v0.7,TODO (ASN): Decide how we want to proceed if at least one trial has,1
v0.7,TODO(shreya): Should this hyperopt config param be set here?,1
v0.7,TODO (ASN): add image heuristics,1
v0.7,"TODO(travis): less hacky way to do this, we should probably allow ModelConfig to be created without output",1
v0.7,"todo future: this may be redundant, check",1
v0.7,Workaround for including additional tensors from output of input encoders for,1
v0.7,TODO(Justin): Think about how to make this communication work for multi-sequence,1
v0.7,todo: when https://github.com/ludwig-ai/ludwig/issues/810 is closed,1
v0.7,"todo: can we just use projector_size? # hidden_size,",1
v0.7,"todo future: this may be redundant, check",1
v0.7,"todo future: this may be redundant, check",1
v0.7,Workaround for including additional tensors from output of input encoders for,1
v0.7,TODO(Justin): Think about how to make this communication work for multi-sequence,1
v0.7,todo future: maybe reintroduce these attention function,1
v0.7,todo future: maybe reintroduce these attention function,1
v0.7,todo future: maybe reintroduce these attention function,1
v0.7,TODO(travis): consider moving this behind a general BatchNorm interface to avoid this kludge.,1
v0.7,"todo: enumerate for debugging, remove after testing",1
v0.7,TODO(shreya): Combine with ResNetBlock by adding a flag.,1
v0.7,TODO(shreya): Implement sparse embedding lookup.,1
v0.7,# TODO(shreya): Check if this is equivalent,1
v0.7,# TODO(shreya): Check if supported in torch,1
v0.7,todo: review for generality,1
v0.7,TODO: Simplify this.,1
v0.7,Dummy implementation.,1
v0.7,"TODO(Justin): Add a confusion matrix, see",1
v0.7,TODO: add a mechanism for letting the user decide to save it,1
v0.7,todo: re-evaluate need for separate handling of `attention` reducer,1
v0.7,TODO(Justin): Clean this up.,1
v0.7,TODO: change to debug level before merging,1
v0.7,TODO: should this raise an exception if not in training mode?,1
v0.7,Nested conditional is a workaround to short-circuit boolean evaluation.,1
v0.7,TODO: alternatively use get_average_image() for unreachable images,1
v0.7,todo future add multiprocessing/multithreading,1
v0.7,TODO(travis): do we even need a user param for vector size if we're going to auto-infer it in all,1
v0.7,TODO(1891): Remove backward compatibility hack once all models have been retrained with Ludwig after,1
v0.7,todo: refactor to reuse SequenceOutputFeature.postprocess_predictions,1
v0.7,TODO: add a mechanism for letting the user decide to save it,1
v0.7,Convert datetime to int64 to workaround Dask limitation,1
v0.7,"TODO pyarrow: this is needed for caching to work with pyarrow. if removed, the following error is raised:",1
v0.7,TODO(travis): passing in MODEL_ECD is a hack here that can be removed once we move to using,1
v0.7,"encoder schema at all. This hack works for now because all encoders are supported by ECD, so",1
v0.7,todo figure out if additional parameters are needed,1
v0.7,TODO dask: this needs to work with DataFrames,1
v0.7,TODO(travis): decouple config from training_set_metadata so we don't need to,1
v0.7,TODO(joppe): support out of memory negative sampling using Dask,1
v0.7,TODO(travis): revisit in the future to make this more precise,1
v0.7,TODO: Add link to windowing docs.,1
v0.7,TODO: figure out correct typing for augmentation_pipeline after refactoring is done,1
v0.7,TODO: consider removing this. doesn't work currently and read performance seems generally,1
v0.7,TODO: convert to debug message when done with development,1
v0.7,"TODO(travis): could try hashing the in-memory dataset, but this is tricky for Dask",1
v0.7,todo future: reintroduce the bucketed batcher,1
v0.7,TODO ray: implement dynamic batch size,1
v0.7,TODO: Change annotation to PublicAPI once Ludwig 0.7 is released,1
v0.7,"TODO(Justin): Add this back, or reconsider where this should be computed.",1
v0.7,TODO(travis): is this redundant with `clipglobalnorm`?,1
v0.7,TODO(travis) consider removing this in the future after deprecation period,1
v0.7,"TODO(travis): this seems much too verbose, does the validation error not show the specific error?",1
v0.7,"TODO(travis): too much boilerplate here, we should find a way to abstract all this and only require specifying the",1
v0.7,"TODO(travis): this seems much too verbose, does the validation error not show the specific error?",1
v0.7,TODO(#1673): Need some more logic here for validating against output features,1
v0.7,"TODO: Re-enable ""goss"" when supported: https://github.com/ludwig-ai/ludwig/issues/2988",1
v0.7,TODO(travis): figure out why we need these imports to avoid circular import error,1
v0.7,"TODO (Connor): Add nesting logic for fc_layers, see fully_connected_module.py",1
v0.7,"TODO (Connor): Add nesting logic for fc_layers, see fully_connected_module.py",1
v0.7,TODO(travis): seems like this is not really a valid user option. We should probably just remove these,1
v0.7,TODO: uncomment when sentencepiece doesn't cause segfaults: https://github.com/ludwig-ai/ludwig/issues/2983,1
v0.7,TODO: uncomment once we figure out host memory issue: https://github.com/ludwig-ai/ludwig/issues/3107,1
v0.7,TODO: uncomment when CTRL bug (https://github.com/ludwig-ai/ludwig/issues/2977) has been fixed to add back in,1
v0.7,TODO(#1673): Add conditional logic for fields like this one:,1
v0.7,"TODO(travis): below type comparison is not perfect, as it doesn't consider the case where the default type",1
v0.7,"TODO(travis): explore similar contraints for GBMs, which don't have epochs",1
v0.7,TODO(travis): move this into helper function,1
v0.7,TODO(travis): handle this with helper function,1
v0.7,TODO(travis): not needed once we remove existing model config implementation,1
v0.7,TODO: Add schema support for Callable,1
v0.7,TODO: This should technically be a required paremeter. Do we need to add support for required params?,1
v0.7,TODO: Double-check support for this,1
v0.7,TODO: Double-check support for this as well as whether Callable args work properly,1
v0.7,todo v0.4: currently not clear way to set model graph,1
v0.7,TODO(daniel): delete this.,1
v0.7,TODO(travis): figure out a good way to support this. The problem with,1
v0.7,TODO: need to also include a filename for this figure,1
v0.7,TODO(travis): figure out why we need these imports to avoid circular import error,1
v0.7,TODO(travis): implement full backend schema,1
v0.7,Dummy implementation.,1
v0.7,TODO: Use a configurable ratio for how often to use teacher forcing during training.,1
v0.7,TODO: Use a configurable ratio for how often to use teacher forcing during training.,1
v0.7,Dummy implementation.,1
v0.7,"TODO(travis): we assume here that False is always the default, which may not be true. We should dervice",1
v0.7,"TODO(travis): stopgap solution, we should make it so we don't need to do this",1
v0.7,todo (Wael): tests for all types.,1
v0.7,todo (Wael): tests for all types.,1
v0.7,Still needed for preprocessing  TODO(Connor): Refactor ludwig/data/preprocessing to use schema,1
v0.7,"ORRIBLE HACK, IT'S THE ONLY WAY TO REMOVE PADDING",1
v0.7,END OF HORRIBLE HACK,1
v0.7,"TODO: more research needed on how to handle RayTune ""sample_from"" search space",1
v0.7,HACK(geoffrey): gpt2 has no pad token. Recommendation is to use eos token instead.,1
v0.7,TODO(geoffrey): can we better validate tokenizer parity before swapping in the TorchText tokenizer?,1
v0.7,Workaround for: https://issues.apache.org/jira/browse/ARROW-5645,1
v0.7,TODO(travis): do this through an interface rather than conditional logic,1
v0.7,TODO(travis): move to cached_property when we drop Python 3.7.,1
v0.7,TODO: remove reshaping once https://github.com/microsoft/LightGBM/pull/4925 is released,1
v0.7,TODO: remove ravel once https://github.com/microsoft/LightGBM/pull/4925 is released,1
v0.7,TODO (jeffkinnison): revert to use the requested device once torch device usage is standardized,1
v0.7,TODO(shreya): Confirm types of args,1
v0.7,"TODO: when loading an existing model, this loses metric values for all but the last epoch.",1
v0.7,TODO(travis): implement imbalance ratio,1
v0.7,"TODO (ASN): add other modalities (image, etc. )",1
v0.7,TODO: only single task currently,1
v0.7,TODO(travis): include encoder and decoder steps during inference,1
v0.7,TODO (jeffkinnison): revert to using the requested device for GBMs when device usage is fixed,1
v0.7,TODO: with change to misc_utils.set_random_seed() this may be redundant,1
v0.7,TODO(Justin): Check that the semantics of input_size align with what the combiner's output shape returns,1
v0.7,TODO(justin): Remove dummy implementation. Make input_shape and output_shape functions.,1
v0.7,TODO(travis): support local feature importance,1
v0.7,TODO:,1
v0.7,TODO(travis): add back skip encoders at the end in finally. Shouldn't be an issue in most cases as we,1
v0.7,"TODO(travis): for force plots, need something similar to SHAP E[X]",1
v0.7,"TODO(travis): for force plots, need something similar to SHAP E[X]",1
v0.7,"TODO(travis): Consider changing to `if not torch.is_floating_point(t.dtype)` to simplify, then handle bool",1
v0.7,TODO(travis): pre-compute this during training from the full training dataset.,1
v0.7,"TODO(travis): for force plots, need something similar to SHAP E[X]",1
v0.7,"TODO(travis): for force plots, need something similar to SHAP E[X]",1
v0.7,"TODO: construct new datasets by running encoders (for text, image)",1
v0.7,TODO(shreya): Refactor preprocessing so that this can be moved upstream.,1
v0.7,"workaround this, we maintain a separate attribute for the wrapped model, which will be used for training",1
v0.7,TODO: refactor this into an interface,1
v0.7,workaround type limitations of the underlying frameworks,1
v0.7,TODO(Arnav): Re-enable in Ray 2.3,1
v0.7,"TODO(travis): we should revisit the user format here, as it silently breaks situations",1
v0.7,"TODO ray: find an informed way to set the parallelism, in practice",1
v0.7,TODO: When this is implemented we also need to update the,1
v0.7,TODO(travis): determine if there is a performance penalty to passing in individual files instead of,1
v0.7,TODO(ekl) deprecate this once read fusion is available.,1
v0.7,TODO(geoffrey): ensure this subclasses ray.data.datasource.Reader in ray 1.14,1
v0.7,TODO(ekl) deprecate this once read fusion is available.,1
v0.7,"doesn't work for Snappy, so we double-check ourselves.",1
v0.7,TODO(xwjiang): Remove this later.,1
v0.7,"TODO(travis): this double-counts on the same device, it should use a cross-communicator instead",1
v0.7,"TODO: remove LOCAL_BACKEND as a global constant, replace with singleton LocalBackend.shared_instance().",1
v0.7,Temporary workaround to prevent tests from automatically using the Ray backend. Taken from,1
v0.7,TODO(travis): open question if this is needed to ensure all workers using same weights,1
v0.7,TODO(travis): open question if this is needed to ensure all workers using same optimizer state,1
v0.7,"TODO(travis): currently Ray handles this for us, but is subject to hangs if one of the workers raises an",1
v0.7.beta,HACK(geoffrey): `hyperopt_resources` is a required resource for hyperopt to prevent deadlocks in Ludwig tests.,1
v0.7.beta,TODO(geoffrey): remove for Ray 2.2,1
v0.7.beta,is there a better way to do this?,1
v0.7.beta,todo the hidden output is actually a tensor. May need modification,1
v0.7.beta,todo figure out the output size for parallel 1d conv,1
v0.7.beta,TODO:  Need variant of assert_model_parameters_updated() to account for the two step calling sequence,1
v0.7.beta,todo: remove code,1
v0.7.beta,TODO(ksbrar): Circle back after discussing whether additional properties should be allowed long-term.,1
v0.7.beta,TODO(ksbrar): Circle back after discussing whether additional properties should be allowed long-term.,1
v0.7.beta,TODO(ksbrar): Circle back after discussing whether additional properties should be allowed long-term.,1
v0.7.beta,todo: re-add 'attention' after further research in implication of torch,1
v0.7.beta,TODO(#1333): Refactor this test once torch sequence generator work is complete.,1
v0.7.beta,"TODO(Justin): Move these to test_sequence_generator unit tests, and reintroduce decoder attention, beam_width, and",1
v0.7.beta,TODO(travis): We may want to expand upon this in the future to include some checks on model,1
v0.7.beta,TODO: Determine whether this is desired behavior. Tracked here:,1
v0.7.beta,handled non-determinism when comparing the metrics between the local and Ray backends. We work around this by,1
v0.7.beta,TODO: feature type not yet supported,1
v0.7.beta,TODO: feature type not yet supported,1
v0.7.beta,TODO(geoffrey): Fold modin tests into test_ray_image as @pytest.mark.parametrized once tests are optimized,1
v0.7.beta,TODO(travis): move this to separate gpu module so we only have one ray cluster running at a time,1
v0.7.beta,TODO(travis): https://github.com/ludwig-ai/ludwig/issues/1956,1
v0.7.beta,TODO: Determine if we still need this if-then-else construct,1
v0.7.beta,TODO(travis): once we support GBM text features,1
v0.7.beta,TODO(shreya): Uncomment when https://github.com/ludwig-ai/ludwig/issues/2039 is fixed.,1
v0.7.beta,TODO: remove this workaround when audio preprocessing is fixed.,1
v0.7.beta,TODO: these are the only outputs we provide from Torchscript for now,1
v0.7.beta,TODO all shadows built in name - come up with a more descriptive name,1
v0.7.beta,TODO (Connor): Refactor to use self.config_obj,1
v0.7.beta,TODO (ASN): add support for substitute_with_max parameter,1
v0.7.beta,TODO (Connor): Refactor to use self.config_obj,1
v0.7.beta,TODO: fix for Ray where workers may be of different skus,1
v0.7.beta,TODO(travis): remove in favor of on_hyperopt_end for naming consistency,1
v0.7.beta,TODO: https://github.com/ludwig-ai/ludwig/issues/2633,1
v0.7.beta,todo: revise docstring,1
v0.7.beta,todo: assess how to specify padding for equivalent to 'same',1
v0.7.beta,todo: determine how to pool_padding equivalent of 'same',1
v0.7.beta,todo: fixup docstring,1
v0.7.beta,todo: review docstring,1
v0.7.beta,todo: fix up docstring,1
v0.7.beta,todo: fix up docstring,1
v0.7.beta,todo: update docstring as needed,1
v0.7.beta,TODO(shreya): Make sure this is updated when FCStack is updated,1
v0.7.beta,TODO(Arnav): Remove this once we have reduce_output options set for,1
v0.7.beta,TODO(shreya): Confirm that this is it,1
v0.7.beta,TODO(shreya): Confirm that this is it,1
v0.7.beta,TODO(justin): This may need to be conditioned on which AutoModel gets chosen.,1
v0.7.beta,TODO(travis): consolidate this logic with `create_auto_config` to reduce duplication and make the,1
v0.7.beta,TODO: Adjust preprocessing parameters according to output feature imbalance.,1
v0.7.beta,TODO (ASN): Decide how we want to proceed if at least one trial has,1
v0.7.beta,TODO(shreya): Should this hyperopt config param be set here?,1
v0.7.beta,TODO (ASN): add image heuristics,1
v0.7.beta,"TODO(travis): less hacky way to do this, we should probably allow ModelConfig to be created without output",1
v0.7.beta,"todo future: this may be redundant, check",1
v0.7.beta,Workaround for including additional tensors from output of input encoders for,1
v0.7.beta,TODO(Justin): Think about how to make this communication work for multi-sequence,1
v0.7.beta,todo: when https://github.com/ludwig-ai/ludwig/issues/810 is closed,1
v0.7.beta,"todo: can we just use projector_size? # hidden_size,",1
v0.7.beta,"todo future: this may be redundant, check",1
v0.7.beta,"todo future: this may be redundant, check",1
v0.7.beta,Workaround for including additional tensors from output of input encoders for,1
v0.7.beta,TODO(Justin): Think about how to make this communication work for multi-sequence,1
v0.7.beta,todo future: maybe reintroduce these attention function,1
v0.7.beta,todo future: maybe reintroduce these attention function,1
v0.7.beta,todo future: maybe reintroduce these attention function,1
v0.7.beta,TODO(travis): consider moving this behind a general BatchNorm interface to avoid this kludge.,1
v0.7.beta,TODO(Justin): Re-register metric for CATEGORY features when aggregation using Ray/Horovod is clearer.,1
v0.7.beta,TODO: double check,1
v0.7.beta,"todo: enumerate for debugging, remove after testing",1
v0.7.beta,TODO(shreya): Combine with ResNetBlock by adding a flag.,1
v0.7.beta,TODO(shreya): Implement sparse embedding lookup.,1
v0.7.beta,# TODO(shreya): Check if this is equivalent,1
v0.7.beta,# TODO(shreya): Check if supported in torch,1
v0.7.beta,todo: review for generality,1
v0.7.beta,TODO: Simplify this.,1
v0.7.beta,Dummy implementation.,1
v0.7.beta,"TODO(Justin): Add a confusion matrix, see",1
v0.7.beta,TODO: add a mechanism for letting the user decide to save it,1
v0.7.beta,todo: re-evaluate need for separate handling of `attention` reducer,1
v0.7.beta,TODO(shreya): Metrics should ideally just move to the correct device,1
v0.7.beta,and not require the user to do this. This is a temporary fix. See,1
v0.7.beta,TODO(Justin): Clean this up.,1
v0.7.beta,Nested conditional is a workaround to short-circuit boolean evaluation.,1
v0.7.beta,TODO: alternatively use get_average_image() for unreachable images,1
v0.7.beta,todo future add multiprocessing/multithreading,1
v0.7.beta,TODO(travis): do we even need a user param for vector size if we're going to auto-infer it in all,1
v0.7.beta,TODO(1891): Remove backward compatibility hack once all models have been retrained with Ludwig after,1
v0.7.beta,todo: refactor to reuse SequenceOutputFeature.postprocess_predictions,1
v0.7.beta,TODO: add a mechanism for letting the user decide to save it,1
v0.7.beta,Convert datetime to int64 to workaround Dask limitation,1
v0.7.beta,"TODO pyarrow: this is needed for caching to work with pyarrow. if removed, the following error is raised:",1
v0.7.beta,todo figure out if additional parameters are needed,1
v0.7.beta,TODO dask: this needs to work with DataFrames,1
v0.7.beta,TODO(travis): decouple config from training_set_metadata so we don't need to,1
v0.7.beta,TODO(joppe): support out of memory negative sampling using Dask,1
v0.7.beta,TODO(travis): revisit in the future to make this more precise,1
v0.7.beta,TODO: Add link to windowing docs.,1
v0.7.beta,"TODO(travis): find way to avoid calling this, as it's expensive",1
v0.7.beta,TODO: consider removing this. doesn't work currently and read performance seems generally,1
v0.7.beta,"TODO(travis): could try hashing the in-memory dataset, but this is tricky for Dask",1
v0.7.beta,todo future: reintroduce the bucketed batcher,1
v0.7.beta,TODO ray: implement dynamic batch size,1
v0.7.beta,TODO: Change annotation to PublicAPI once Ludwig 0.7 is released,1
v0.7.beta,"TODO(Justin): Add this back, or reconsider where this should be computed.",1
v0.7.beta,TODO depending on defaults section conversation may want to enable,1
v0.7.beta,TODO(Justin): Validate that validation_field is valid.,1
v0.7.beta,"TODO(travis): too much boilerplate here, we should find a way to abstract all this and only require specifying the",1
v0.7.beta,"TODO(travis): this seems much too verbose, does the validation error not show the specific error?",1
v0.7.beta,TODO(#1673): Need some more logic here for validating against output features,1
v0.7.beta,"TODO: Re-enable ""goss"" when supported: https://github.com/ludwig-ai/ludwig/issues/2988",1
v0.7.beta,TODO(travis): implement full backend schema,1
v0.7.beta,"TODO (Connor): Add nesting logic for fc_layers, see fully_connected_module.py",1
v0.7.beta,"TODO (Connor): Add nesting logic for fc_layers, see fully_connected_module.py",1
v0.7.beta,TODO: uncomment when sentencepiece doesn't cause segfaults: https://github.com/ludwig-ai/ludwig/issues/2983,1
v0.7.beta,TODO: uncomment when CTRL bug (https://github.com/ludwig-ai/ludwig/issues/2977) has been fixed to add back in,1
v0.7.beta,TODO(#1673): Add conditional logic for fields like this one:,1
v0.7.beta,TODO: Add schema support for Callable,1
v0.7.beta,TODO: This should technically be a required paremeter. Do we need to add support for required params?,1
v0.7.beta,TODO: Double-check support for this,1
v0.7.beta,TODO: Double-check support for this as well as whether Callable args work properly,1
v0.7.beta,todo v0.4: currently not clear way to set model graph,1
v0.7.beta,TODO(daniel): delete this.,1
v0.7.beta,TODO(travis): figure out a good way to support this. The problem with,1
v0.7.beta,TODO: need to also include a filename for this figure,1
v0.7.beta,TODO(travis): implement full backend schema,1
v0.7.beta,Dummy implementation.,1
v0.7.beta,TODO: Use a configurable ratio for how often to use teacher forcing during training.,1
v0.7.beta,TODO: Use a configurable ratio for how often to use teacher forcing during training.,1
v0.7.beta,Dummy implementation.,1
v0.7.beta,"TODO(travis): we assume here that False is always the default, which may not be true. We should dervice",1
v0.7.beta,"TODO(travis): stopgap solution, we should make it so we don't need to do this",1
v0.7.beta,todo (Wael): tests for all types.,1
v0.7.beta,todo (Wael): tests for all types.,1
v0.7.beta,Still needed for preprocessing  TODO(Connor): Refactor ludwig/data/preprocessing to use schema,1
v0.7.beta,"ORRIBLE HACK, IT'S THE ONLY WAY TO REMOVE PADDING",1
v0.7.beta,END OF HORRIBLE HACK,1
v0.7.beta,"TODO: more research needed on how to handle RayTune ""sample_from"" search space",1
v0.7.beta,"TODO(geoffrey): If we move to torchtext>=0.13.0, we can use return_tokens kwarg to get tokens directly",1
v0.7.beta,TODO(geoffrey): can we better validate tokenizer parity before swapping in the TorchText tokenizer?,1
v0.7.beta,Workaround for: https://issues.apache.org/jira/browse/ARROW-5645,1
v0.7.beta,TODO(travis): move to cached_property when we drop Python 3.7.,1
v0.7.beta,TODO: remove reshaping once https://github.com/microsoft/LightGBM/pull/4925 is released,1
v0.7.beta,TODO: remove ravel once https://github.com/microsoft/LightGBM/pull/4925 is released,1
v0.7.beta,TODO(shreya): Confirm types of args,1
v0.7.beta,"TODO: when loading an existing model, this loses metric values for all but the last epoch.",1
v0.7.beta,TODO(travis): implement imbalance ratio,1
v0.7.beta,"TODO (ASN): add other modalities (image, etc. )",1
v0.7.beta,TODO: only single task currently,1
v0.7.beta,TODO(travis): include encoder and decoder steps during inference,1
v0.7.beta,TODO: with change to misc_utils.set_random_seed() this may be redundant,1
v0.7.beta,TODO(Justin): Check that the semantics of input_size align with what the combiner's output shape returns,1
v0.7.beta,TODO(justin): Remove dummy implementation. Make input_shape and output_shape functions.,1
v0.7.beta,TODO(travis): support local feature importance,1
v0.7.beta,TODO:,1
v0.7.beta,TODO(travis): add back skip encoders at the end in finally. Shouldn't be an issue in most cases as we,1
v0.7.beta,"TODO(travis): for force plots, need something similar to SHAP E[X]",1
v0.7.beta,"TODO(travis): for force plots, need something similar to SHAP E[X]",1
v0.7.beta,"TODO(travis): Consider changing to `if not torch.is_floating_point(t.dtype)` to simplify, then handle bool",1
v0.7.beta,TODO(travis): pre-compute this during training from the full training dataset.,1
v0.7.beta,"TODO(travis): for force plots, need something similar to SHAP E[X]",1
v0.7.beta,"TODO(travis): for force plots, need something similar to SHAP E[X]",1
v0.7.beta,"TODO: construct new datasets by running encoders (for text, image)",1
v0.7.beta,TODO(shreya): Refactor preprocessing so that this can be moved upstream.,1
v0.7.beta,"workaround this, we maintain a separate attribute for the wrapped model, which will be used for training",1
v0.7.beta,TODO(Justin): Move to config validation when that's ready.,1
v0.7.beta,TODO: refactor this into an interface,1
v0.7.beta,workaround type limitations of the underlying frameworks,1
v0.7.beta,TODO(Arnav): Re-enable in Ray 2.3,1
v0.7.beta,"TODO(travis): we should revisit the user format here, as it silently breaks situations",1
v0.7.beta,"TODO ray: find an informed way to set the parallelism, in practice",1
v0.7.beta,TODO: When this is implemented we also need to update the,1
v0.7.beta,TODO(travis): determine if there is a performance penalty to passing in individual files instead of,1
v0.7.beta,TODO(ekl) deprecate this once read fusion is available.,1
v0.7.beta,TODO(geoffrey): ensure this subclasses ray.data.datasource.Reader in ray 1.14,1
v0.7.beta,TODO(ekl) deprecate this once read fusion is available.,1
v0.7.beta,"doesn't work for Snappy, so we double-check ourselves.",1
v0.7.beta,TODO(xwjiang): Remove this later.,1
v0.7.beta,"TODO(travis): this double-counts on the same device, it should use a cross-communicator instead",1
v0.7.beta,"TODO: remove LOCAL_BACKEND as a global constant, replace with singleton LocalBackend.shared_instance().",1
v0.7.beta,Temporary workaround to prevent tests from automatically using the Ray backend. Taken from,1
v0.7.beta,TODO(travis): open question if this is needed to ensure all workers using same weights,1
v0.7.beta,TODO(travis): open question if this is needed to ensure all workers using same optimizer state,1
v0.7.beta,"TODO(travis): currently Ray handles this for us, but is subject to hangs if one of the workers raises an",1
v0.6.4,is there a better way to do this?,1
v0.6.4,todo the hidden output is actually a tensor. May need modification,1
v0.6.4,todo figure out the output size for parallel 1d conv,1
v0.6.4,TODO:  Need variant of assert_model_parameters_updated() to account for the two step calling sequence,1
v0.6.4,todo: remove code,1
v0.6.4,todo: re-add 'attention' after further research in implication of torch,1
v0.6.4,TODO(#1333): Refactor this test once torch sequence generator work is complete.,1
v0.6.4,"TODO(Justin): Move these to test_sequence_generator unit tests, and reintroduce decoder attention, beam_width, and",1
v0.6.4,"TODO(geoffrey): Add dataset_type=""csv"" back to parameters if we can prevent CI timeouts.",1
v0.6.4,TODO: NaN handling not supported. See `test_ray_save_inputs_and_outputs_without_nans` below.,1
v0.6.4,TODO: feature type not yet supported,1
v0.6.4,TODO: feature type not yet supported,1
v0.6.4,TODO(geoffrey): Fold modin tests into test_ray_image as @pytest.mark.parametrized once tests are optimized,1
v0.6.4,TODO(travis): move this to separate gpu module so we only have one ray cluster running at a time,1
v0.6.4,TODO(travis): https://github.com/ludwig-ai/ludwig/issues/1956,1
v0.6.4,TODO: Determine if we still need this if-then-else construct,1
v0.6.4,TODO(shreya): Uncomment when https://github.com/ludwig-ai/ludwig/issues/2039 is fixed.,1
v0.6.4,TODO ray: replace legacy mode when Ray Train supports placement groups,1
v0.6.4,TODO: remove this workaround when audio preprocessing is fixed.,1
v0.6.4,TODO: these are the only outputs we provide from Torchscript for now,1
v0.6.4,TODO all shadows built in name - come up with a more descriptive name,1
v0.6.4,TODO (ASN): add support for substitute_with_max parameter,1
v0.6.4,TODO: fix for Ray where workers may be of different skus,1
v0.6.4,TODO(travis): remove in favor of on_hyperopt_end for naming consistency,1
v0.6.4,TODO: https://github.com/ludwig-ai/ludwig/issues/2633,1
v0.6.4,TODO(shreya): Add type hints for missing args,1
v0.6.4,todo: revise docstring,1
v0.6.4,todo: assess how to specify padding for equivalent to 'same',1
v0.6.4,todo: determine how to pool_padding equivalent of 'same',1
v0.6.4,todo: fixup docstring,1
v0.6.4,todo: review docstring,1
v0.6.4,todo: fix up docstring,1
v0.6.4,todo: fix up docstring,1
v0.6.4,todo: update docstring as needed,1
v0.6.4,TODO(shreya): Make sure this is updated when FCStack is updated,1
v0.6.4,TODO(justin): Use official class properties.,1
v0.6.4,TODO(shreya): Confirm that this is it,1
v0.6.4,TODO(shreya): Confirm that this is it,1
v0.6.4,TODO(justin): This may need to be conditioned on which AutoModel gets chosen.,1
v0.6.4,TODO (ASN): Decide how we want to proceed if at least one trial has,1
v0.6.4,TODO (ASN): add image heuristics,1
v0.6.4,"todo future: this may be redundant, check",1
v0.6.4,Workaround for including additional tensors from output of input encoders for,1
v0.6.4,TODO(Justin): Think about how to make this communication work for multi-sequence,1
v0.6.4,todo: when https://github.com/ludwig-ai/ludwig/issues/810 is closed,1
v0.6.4,todo future: maybe modify this with TF2 mask mechanics,1
v0.6.4,"todo: can we just use projector_size? # hidden_size,",1
v0.6.4,"todo future: this may be redundant, check",1
v0.6.4,"todo future: this may be redundant, check",1
v0.6.4,Workaround for including additional tensors from output of input encoders for,1
v0.6.4,TODO(Justin): Think about how to make this communication work for multi-sequence,1
v0.6.4,todo future: maybe reintroduce these attention function,1
v0.6.4,todo future: maybe reintroduce these attention function,1
v0.6.4,todo future: maybe reintroduce these attention function,1
v0.6.4,TODO: double check,1
v0.6.4,"todo: enumerate for debugging, remove after testing",1
v0.6.4,TODO(shreya): Combine with ResNetBlock by adding a flag.,1
v0.6.4,TODO(shreya): Implement sparse embedding lookup.,1
v0.6.4,# TODO(shreya): Check if this is equivalent,1
v0.6.4,# TODO(shreya): Check if supported in torch,1
v0.6.4,todo: review for generality,1
v0.6.4,TODO: Simplify this.,1
v0.6.4,Dummy implementation.,1
v0.6.4,"TODO(Justin): Add a confusion matrix, see",1
v0.6.4,TODO: add a mechanism for letting the user decide to save it,1
v0.6.4,todo: re-evaluate need for separate handling of `attention` reducer,1
v0.6.4,TODO(shreya): Metrics should ideally just move to the correct device,1
v0.6.4,and not require the user to do this. This is a temporary fix. See,1
v0.6.4,TODO(Justin): Clean this up.,1
v0.6.4,TODO(shreya): Confirm if it's ok to do per channel normalization,1
v0.6.4,TODO(shreya): Also confirm if this is being used anywhere,1
v0.6.4,TODO(shreya): Confirm if ok to use imagenet means and std devs,1
v0.6.4,Nested conditional is a workaround to short-circuit boolean evaluation.,1
v0.6.4,TODO: alternatively use get_average_image() for unreachable images,1
v0.6.4,todo future add multiprocessing/multithreading,1
v0.6.4,TODO(1891): Remove backward compatibility hack once all models have been retrained with Ludwig after,1
v0.6.4,todo: refactor to reuse SequenceOutputFeature.postprocess_predictions,1
v0.6.4,TODO: add a mechanism for letting the user decide to save it,1
v0.6.4,Convert datetime to int64 to workaround Dask limitation,1
v0.6.4,TODO ray: this is needed because ray 1.7 doesn't support Dask to RayDataset,1
v0.6.4,todo figure out if additional parameters are needed,1
v0.6.4,TODO dask: this needs to work with DataFrames,1
v0.6.4,TODO(travis): decouple config from training_set_metadata so we don't need to,1
v0.6.4,TODO(travis): revisit in the future to make this more precise,1
v0.6.4,TODO(geoffrey): remove this once Ray > 1.13 in our CI.,1
v0.6.4,TODO ray 1.8: convert to Tensors before shuffle,1
v0.6.4,Workaround for: https://github.com/ray-project/ray/issues/25643,1
v0.6.4,TODO(travis): remove after 1.13.1,1
v0.6.4,"TODO(travis): find way to avoid calling this, as it's expensive",1
v0.6.4,TODO: consider removing this. doesn't work currently and read performance seems generally,1
v0.6.4,TODO(travis): figure out why Ray is converting these into object types by default,1
v0.6.4,"TODO(travis): could try hashing the in-memory dataset, but this is tricky for Dask",1
v0.6.4,todo future: reintroduce the bucketed batcher,1
v0.6.4,TODO ray: implement dynamic batch size,1
v0.6.4,TODO(#1673): Need some more logic here for validating against output features,1
v0.6.4,"TODO (Connor): Add nesting logic for fc_layers, see fully_connected_module.py",1
v0.6.4,"TODO (Connor): Add nesting logic for fc_layers, see fully_connected_module.py",1
v0.6.4,TODO(#1673): Add conditional logic for fields like this one:,1
v0.6.4,todo v0.4: currently not clear way to set model graph,1
v0.6.4,TODO(travis): figure out a good way to support this. The problem with,1
v0.6.4,TODO: need to also include a filename for this figure,1
v0.6.4,Dummy implementation.,1
v0.6.4,TODO: Use a configurable ratio for how often to use teacher forcing during training.,1
v0.6.4,TODO: Use a configurable ratio for how often to use teacher forcing during training.,1
v0.6.4,Dummy implementation.,1
v0.6.4,"TODO(travis): stopgap solution, we should make it so we don't need to do this",1
v0.6.4,todo (Wael): tests for all types.,1
v0.6.4,todo (Wael): tests for all types.,1
v0.6.4,TODO(#2125): This code block needs some refactoring.,1
v0.6.4,"ORRIBLE HACK, IT'S THE ONLY WAY TO REMOVE PADDING",1
v0.6.4,END OF HORRIBLE HACK,1
v0.6.4,"TODO: more research needed on how to handle RayTune ""sample_from"" search space",1
v0.6.4,todo future: maybe modify this with TF2 mask mechanics,1
v0.6.4,"TODO(geoffrey): If we move to torchtext>=0.13.0, we can use return_tokens kwarg to get tokens directly",1
v0.6.4,Workaround for: https://issues.apache.org/jira/browse/ARROW-5645,1
v0.6.4,TODO(travis): move to cached_property when we drop Python 3.7.,1
v0.6.4,TODO(shreya): Confirm types of args,1
v0.6.4,"TODO: when loading an existing model, this loses metric values for all but the last epoch.",1
v0.6.4,TODO(travis): implement imbalance ratio,1
v0.6.4,"TODO (ASN): add other modalities (image, etc. )",1
v0.6.4,TODO: only single task currently,1
v0.6.4,TODO(travis): include encoder and decoder steps during inference,1
v0.6.4,TODO: with change to misc_utils.set_random_seed() this may be redundant,1
v0.6.4,TODO(Justin): Check that the semantics of input_size align with what the combiner's output shape returns,1
v0.6.4,TODO(justin): Remove dummy implementation. Make input_shape and output_shape functions.,1
v0.6.4,TODO:,1
v0.6.4,"TODO(travis): this won't work for text decoders, but we don't support explanations for those yet",1
v0.6.4,TODO(travis): pre-compute this during training from the full training dataset.,1
v0.6.4,"TODO(travis): this isn't really the expected value as it is for shap, so",1
v0.6.4,TODO (ASN): Circle back on how we want to set default placeholder value,1
v0.6.4,"TODO: construct new datasets by running encoders (for text, image)",1
v0.6.4,TODO (ASN): Circle back on how we want to set default placeholder value,1
v0.6.4,TODO(Justin): Move to config validation when that's ready.,1
v0.6.4,TODO(geoffrey): Add support for batch size tuning on CPU,1
v0.6.4,TODO (ASN) : Circle back on how we want to set default placeholder value,1
v0.6.4,TODO: refactor this into an interface,1
v0.6.4,workaround type limitations of the underlying frameworks,1
v0.6.4,"TODO(travis): we should revisit the user format here, as it silently breaks situations",1
v0.6.4,TODO: deprecated v0.5,1
v0.6.4,"TODO ray: find an informed way to set the parallelism, in practice",1
v0.6.4,TODO: When this is implemented we also need to update the,1
v0.6.4,"TODO ray: make this more configurable by allowing YAML overrides of timeout_s, etc.",1
v0.6.4,Deep copy to workaround https://github.com/ray-project/ray/issues/24139,1
v0.6.4,TODO: deprecated 0.5,1
v0.6.4,TODO(travis): determine if there is a performance penalty to passing in individual files instead of,1
v0.6.4,HACK: Workaround for https://github.com/modin-project/modin/issues/4686,1
v0.6.4,TODO(ekl) deprecate this once read fusion is available.,1
v0.6.4,TODO(geoffrey): ensure this subclasses ray.data.datasource.Reader in ray 1.14,1
v0.6.4,TODO(ekl) deprecate this once read fusion is available.,1
v0.6.4,"doesn't work for Snappy, so we double-check ourselves.",1
v0.6.4,"TODO(tgaddair): temporary workaround for Horovod's worker discovery logic,",1
v0.6.4,TODO(matt): Implement placement group strategies in BackendExecutor.,1
v0.6.4,Temporary workaround to prevent tests from automatically using the Ray backend. Taken from,1
v0.6.4,TODO(shreya): DataFrame created twice: here + CSVMixin. Figure out,1
v0.6.3,is there a better way to do this?,1
v0.6.3,todo the hidden output is actually a tensor. May need modification,1
v0.6.3,todo figure out the output size for parallel 1d conv,1
v0.6.3,TODO:  Need variant of assert_model_parameters_updated() to account for the two step calling sequence,1
v0.6.3,todo: remove code,1
v0.6.3,todo: re-add 'attention' after further research in implication of torch,1
v0.6.3,TODO(#1333): Refactor this test once torch sequence generator work is complete.,1
v0.6.3,"TODO(Justin): Move these to test_sequence_generator unit tests, and reintroduce decoder attention, beam_width, and",1
v0.6.3,"TODO(geoffrey): Add dataset_type=""csv"" back to parameters if we can prevent CI timeouts.",1
v0.6.3,TODO: NaN handling not supported. See `test_ray_save_inputs_and_outputs_without_nans` below.,1
v0.6.3,TODO: feature type not yet supported,1
v0.6.3,TODO: feature type not yet supported,1
v0.6.3,TODO(geoffrey): Fold modin tests into test_ray_image as @pytest.mark.parametrized once tests are optimized,1
v0.6.3,TODO(travis): move this to separate gpu module so we only have one ray cluster running at a time,1
v0.6.3,TODO(travis): https://github.com/ludwig-ai/ludwig/issues/1956,1
v0.6.3,TODO: Determine if we still need this if-then-else construct,1
v0.6.3,TODO(shreya): Uncomment when https://github.com/ludwig-ai/ludwig/issues/2039 is fixed.,1
v0.6.3,TODO ray: replace legacy mode when Ray Train supports placement groups,1
v0.6.3,TODO: remove this workaround when audio preprocessing is fixed.,1
v0.6.3,TODO: these are the only outputs we provide from Torchscript for now,1
v0.6.3,TODO all shadows built in name - come up with a more descriptive name,1
v0.6.3,TODO (ASN): add support for substitute_with_max parameter,1
v0.6.3,TODO: fix for Ray where workers may be of different skus,1
v0.6.3,TODO(travis): remove in favor of on_hyperopt_end for naming consistency,1
v0.6.3,TODO: https://github.com/ludwig-ai/ludwig/issues/2633,1
v0.6.3,TODO(shreya): Add type hints for missing args,1
v0.6.3,todo: revise docstring,1
v0.6.3,todo: assess how to specify padding for equivalent to 'same',1
v0.6.3,todo: determine how to pool_padding equivalent of 'same',1
v0.6.3,todo: fixup docstring,1
v0.6.3,todo: review docstring,1
v0.6.3,todo: fix up docstring,1
v0.6.3,todo: fix up docstring,1
v0.6.3,todo: update docstring as needed,1
v0.6.3,TODO(shreya): Make sure this is updated when FCStack is updated,1
v0.6.3,TODO(justin): Use official class properties.,1
v0.6.3,TODO(shreya): Confirm that this is it,1
v0.6.3,TODO(shreya): Confirm that this is it,1
v0.6.3,TODO(justin): This may need to be conditioned on which AutoModel gets chosen.,1
v0.6.3,TODO (ASN): Decide how we want to proceed if at least one trial has,1
v0.6.3,TODO (ASN): add image heuristics,1
v0.6.3,"todo future: this may be redundant, check",1
v0.6.3,Workaround for including additional tensors from output of input encoders for,1
v0.6.3,TODO(Justin): Think about how to make this communication work for multi-sequence,1
v0.6.3,todo: when https://github.com/ludwig-ai/ludwig/issues/810 is closed,1
v0.6.3,todo future: maybe modify this with TF2 mask mechanics,1
v0.6.3,"todo: can we just use projector_size? # hidden_size,",1
v0.6.3,"todo future: this may be redundant, check",1
v0.6.3,"todo future: this may be redundant, check",1
v0.6.3,Workaround for including additional tensors from output of input encoders for,1
v0.6.3,TODO(Justin): Think about how to make this communication work for multi-sequence,1
v0.6.3,todo future: maybe reintroduce these attention function,1
v0.6.3,todo future: maybe reintroduce these attention function,1
v0.6.3,todo future: maybe reintroduce these attention function,1
v0.6.3,TODO: double check,1
v0.6.3,"todo: enumerate for debugging, remove after testing",1
v0.6.3,TODO(shreya): Combine with ResNetBlock by adding a flag.,1
v0.6.3,TODO(shreya): Implement sparse embedding lookup.,1
v0.6.3,# TODO(shreya): Check if this is equivalent,1
v0.6.3,# TODO(shreya): Check if supported in torch,1
v0.6.3,todo: review for generality,1
v0.6.3,TODO: Simplify this.,1
v0.6.3,Dummy implementation.,1
v0.6.3,"TODO(Justin): Add a confusion matrix, see",1
v0.6.3,TODO: add a mechanism for letting the user decide to save it,1
v0.6.3,todo: re-evaluate need for separate handling of `attention` reducer,1
v0.6.3,TODO(shreya): Metrics should ideally just move to the correct device,1
v0.6.3,and not require the user to do this. This is a temporary fix. See,1
v0.6.3,TODO(Justin): Clean this up.,1
v0.6.3,TODO(shreya): Confirm if it's ok to do per channel normalization,1
v0.6.3,TODO(shreya): Also confirm if this is being used anywhere,1
v0.6.3,TODO(shreya): Confirm if ok to use imagenet means and std devs,1
v0.6.3,Nested conditional is a workaround to short-circuit boolean evaluation.,1
v0.6.3,TODO: alternatively use get_average_image() for unreachable images,1
v0.6.3,todo future add multiprocessing/multithreading,1
v0.6.3,TODO(1891): Remove backward compatibility hack once all models have been retrained with Ludwig after,1
v0.6.3,todo: refactor to reuse SequenceOutputFeature.postprocess_predictions,1
v0.6.3,TODO: add a mechanism for letting the user decide to save it,1
v0.6.3,Convert datetime to int64 to workaround Dask limitation,1
v0.6.3,TODO ray: this is needed because ray 1.7 doesn't support Dask to RayDataset,1
v0.6.3,todo figure out if additional parameters are needed,1
v0.6.3,TODO dask: this needs to work with DataFrames,1
v0.6.3,TODO(travis): decouple config from training_set_metadata so we don't need to,1
v0.6.3,TODO(travis): revisit in the future to make this more precise,1
v0.6.3,TODO(geoffrey): remove this once Ray > 1.13 in our CI.,1
v0.6.3,TODO ray 1.8: convert to Tensors before shuffle,1
v0.6.3,Workaround for: https://github.com/ray-project/ray/issues/25643,1
v0.6.3,TODO(travis): remove after 1.13.1,1
v0.6.3,"TODO(travis): find way to avoid calling this, as it's expensive",1
v0.6.3,TODO: consider removing this. doesn't work currently and read performance seems generally,1
v0.6.3,TODO(travis): figure out why Ray is converting these into object types by default,1
v0.6.3,"TODO(travis): could try hashing the in-memory dataset, but this is tricky for Dask",1
v0.6.3,todo future: reintroduce the bucketed batcher,1
v0.6.3,TODO ray: implement dynamic batch size,1
v0.6.3,TODO(#1673): Need some more logic here for validating against output features,1
v0.6.3,"TODO (Connor): Add nesting logic for fc_layers, see fully_connected_module.py",1
v0.6.3,"TODO (Connor): Add nesting logic for fc_layers, see fully_connected_module.py",1
v0.6.3,TODO(#1673): Add conditional logic for fields like this one:,1
v0.6.3,todo v0.4: currently not clear way to set model graph,1
v0.6.3,TODO(travis): figure out a good way to support this. The problem with,1
v0.6.3,TODO: need to also include a filename for this figure,1
v0.6.3,Dummy implementation.,1
v0.6.3,TODO: Use a configurable ratio for how often to use teacher forcing during training.,1
v0.6.3,TODO: Use a configurable ratio for how often to use teacher forcing during training.,1
v0.6.3,Dummy implementation.,1
v0.6.3,"TODO(travis): stopgap solution, we should make it so we don't need to do this",1
v0.6.3,todo (Wael): tests for all types.,1
v0.6.3,todo (Wael): tests for all types.,1
v0.6.3,TODO(#2125): This code block needs some refactoring.,1
v0.6.3,"ORRIBLE HACK, IT'S THE ONLY WAY TO REMOVE PADDING",1
v0.6.3,END OF HORRIBLE HACK,1
v0.6.3,"TODO: more research needed on how to handle RayTune ""sample_from"" search space",1
v0.6.3,todo future: maybe modify this with TF2 mask mechanics,1
v0.6.3,"TODO(geoffrey): If we move to torchtext>=0.13.0, we can use return_tokens kwarg to get tokens directly",1
v0.6.3,Workaround for: https://issues.apache.org/jira/browse/ARROW-5645,1
v0.6.3,TODO(travis): move to cached_property when we drop Python 3.7.,1
v0.6.3,TODO(shreya): Confirm types of args,1
v0.6.3,"TODO: when loading an existing model, this loses metric values for all but the last epoch.",1
v0.6.3,TODO(travis): implement imbalance ratio,1
v0.6.3,"TODO (ASN): add other modalities (image, etc. )",1
v0.6.3,TODO: only single task currently,1
v0.6.3,TODO(travis): include encoder and decoder steps during inference,1
v0.6.3,TODO: with change to misc_utils.set_random_seed() this may be redundant,1
v0.6.3,TODO(Justin): Check that the semantics of input_size align with what the combiner's output shape returns,1
v0.6.3,TODO(justin): Remove dummy implementation. Make input_shape and output_shape functions.,1
v0.6.3,TODO:,1
v0.6.3,"TODO(travis): this won't work for text decoders, but we don't support explanations for those yet",1
v0.6.3,TODO(travis): pre-compute this during training from the full training dataset.,1
v0.6.3,"TODO(travis): this isn't really the expected value as it is for shap, so",1
v0.6.3,TODO (ASN): Circle back on how we want to set default placeholder value,1
v0.6.3,"TODO: construct new datasets by running encoders (for text, image)",1
v0.6.3,TODO (ASN): Circle back on how we want to set default placeholder value,1
v0.6.3,TODO(Justin): Move to config validation when that's ready.,1
v0.6.3,TODO(geoffrey): Add support for batch size tuning on CPU,1
v0.6.3,TODO (ASN) : Circle back on how we want to set default placeholder value,1
v0.6.3,TODO: refactor this into an interface,1
v0.6.3,workaround type limitations of the underlying frameworks,1
v0.6.3,"TODO(travis): we should revisit the user format here, as it silently breaks situations",1
v0.6.3,TODO: deprecated v0.5,1
v0.6.3,"TODO ray: find an informed way to set the parallelism, in practice",1
v0.6.3,TODO: When this is implemented we also need to update the,1
v0.6.3,"TODO ray: make this more configurable by allowing YAML overrides of timeout_s, etc.",1
v0.6.3,Deep copy to workaround https://github.com/ray-project/ray/issues/24139,1
v0.6.3,TODO: deprecated 0.5,1
v0.6.3,TODO(travis): determine if there is a performance penalty to passing in individual files instead of,1
v0.6.3,HACK: Workaround for https://github.com/modin-project/modin/issues/4686,1
v0.6.3,TODO(ekl) deprecate this once read fusion is available.,1
v0.6.3,TODO(geoffrey): ensure this subclasses ray.data.datasource.Reader in ray 1.14,1
v0.6.3,TODO(ekl) deprecate this once read fusion is available.,1
v0.6.3,"doesn't work for Snappy, so we double-check ourselves.",1
v0.6.3,"TODO(tgaddair): temporary workaround for Horovod's worker discovery logic,",1
v0.6.3,TODO(matt): Implement placement group strategies in BackendExecutor.,1
v0.6.3,Temporary workaround to prevent tests from automatically using the Ray backend. Taken from,1
v0.6.3,TODO(shreya): DataFrame created twice: here + CSVMixin. Figure out,1
v0.6.2,is there a better way to do this?,1
v0.6.2,todo the hidden output is actually a tensor. May need modification,1
v0.6.2,todo figure out the output size for parallel 1d conv,1
v0.6.2,TODO:  Need variant of assert_model_parameters_updated() to account for the two step calling sequence,1
v0.6.2,todo: remove code,1
v0.6.2,todo: re-add 'attention' after further research in implication of torch,1
v0.6.2,TODO(#1333): Refactor this test once torch sequence generator work is complete.,1
v0.6.2,"TODO(Justin): Move these to test_sequence_generator unit tests, and reintroduce decoder attention, beam_width, and",1
v0.6.2,"TODO(geoffrey): Add dataset_type=""csv"" back to parameters if we can prevent CI timeouts.",1
v0.6.2,TODO: NaN handling not supported. See `test_ray_save_inputs_and_outputs_without_nans` below.,1
v0.6.2,TODO: feature type not yet supported,1
v0.6.2,TODO: feature type not yet supported,1
v0.6.2,TODO(geoffrey): Fold modin tests into test_ray_image as @pytest.mark.parametrized once tests are optimized,1
v0.6.2,TODO(travis): move this to separate gpu module so we only have one ray cluster running at a time,1
v0.6.2,TODO(travis): https://github.com/ludwig-ai/ludwig/issues/1956,1
v0.6.2,TODO: Determine if we still need this if-then-else construct,1
v0.6.2,TODO(shreya): Uncomment when https://github.com/ludwig-ai/ludwig/issues/2039 is fixed.,1
v0.6.2,TODO ray: replace legacy mode when Ray Train supports placement groups,1
v0.6.2,TODO: remove this workaround when audio preprocessing is fixed.,1
v0.6.2,TODO: these are the only outputs we provide from Torchscript for now,1
v0.6.2,TODO all shadows built in name - come up with a more descriptive name,1
v0.6.2,TODO (ASN): add support for substitute_with_max parameter,1
v0.6.2,TODO: fix for Ray where workers may be of different skus,1
v0.6.2,TODO(travis): remove in favor of on_hyperopt_end for naming consistency,1
v0.6.2,TODO: https://github.com/ludwig-ai/ludwig/issues/2633,1
v0.6.2,TODO(shreya): Add type hints for missing args,1
v0.6.2,todo: revise docstring,1
v0.6.2,todo: assess how to specify padding for equivalent to 'same',1
v0.6.2,todo: determine how to pool_padding equivalent of 'same',1
v0.6.2,todo: fixup docstring,1
v0.6.2,todo: review docstring,1
v0.6.2,todo: fix up docstring,1
v0.6.2,todo: fix up docstring,1
v0.6.2,todo: update docstring as needed,1
v0.6.2,TODO(shreya): Make sure this is updated when FCStack is updated,1
v0.6.2,TODO(justin): Use official class properties.,1
v0.6.2,TODO(shreya): Confirm that this is it,1
v0.6.2,TODO(shreya): Confirm that this is it,1
v0.6.2,TODO(justin): This may need to be conditioned on which AutoModel gets chosen.,1
v0.6.2,TODO (ASN): Decide how we want to proceed if at least one trial has,1
v0.6.2,TODO (ASN): add image heuristics,1
v0.6.2,"todo future: this may be redundant, check",1
v0.6.2,Workaround for including additional tensors from output of input encoders for,1
v0.6.2,TODO(Justin): Think about how to make this communication work for multi-sequence,1
v0.6.2,todo: when https://github.com/ludwig-ai/ludwig/issues/810 is closed,1
v0.6.2,todo future: maybe modify this with TF2 mask mechanics,1
v0.6.2,"todo: can we just use projector_size? # hidden_size,",1
v0.6.2,"todo future: this may be redundant, check",1
v0.6.2,"todo future: this may be redundant, check",1
v0.6.2,Workaround for including additional tensors from output of input encoders for,1
v0.6.2,TODO(Justin): Think about how to make this communication work for multi-sequence,1
v0.6.2,todo future: maybe reintroduce these attention function,1
v0.6.2,todo future: maybe reintroduce these attention function,1
v0.6.2,todo future: maybe reintroduce these attention function,1
v0.6.2,TODO: double check,1
v0.6.2,"todo: enumerate for debugging, remove after testing",1
v0.6.2,TODO(shreya): Combine with ResNetBlock by adding a flag.,1
v0.6.2,TODO(shreya): Implement sparse embedding lookup.,1
v0.6.2,# TODO(shreya): Check if this is equivalent,1
v0.6.2,# TODO(shreya): Check if supported in torch,1
v0.6.2,todo: review for generality,1
v0.6.2,TODO: Simplify this.,1
v0.6.2,Dummy implementation.,1
v0.6.2,"TODO(Justin): Add a confusion matrix, see",1
v0.6.2,TODO: add a mechanism for letting the user decide to save it,1
v0.6.2,todo: re-evaluate need for separate handling of `attention` reducer,1
v0.6.2,TODO(shreya): Metrics should ideally just move to the correct device,1
v0.6.2,and not require the user to do this. This is a temporary fix. See,1
v0.6.2,TODO(Justin): Clean this up.,1
v0.6.2,TODO(shreya): Confirm if it's ok to do per channel normalization,1
v0.6.2,TODO(shreya): Also confirm if this is being used anywhere,1
v0.6.2,TODO(shreya): Confirm if ok to use imagenet means and std devs,1
v0.6.2,Nested conditional is a workaround to short-circuit boolean evaluation.,1
v0.6.2,TODO: alternatively use get_average_image() for unreachable images,1
v0.6.2,todo future add multiprocessing/multithreading,1
v0.6.2,TODO(1891): Remove backward compatibility hack once all models have been retrained with Ludwig after,1
v0.6.2,todo: refactor to reuse SequenceOutputFeature.postprocess_predictions,1
v0.6.2,TODO: add a mechanism for letting the user decide to save it,1
v0.6.2,Convert datetime to int64 to workaround Dask limitation,1
v0.6.2,TODO ray: this is needed because ray 1.7 doesn't support Dask to RayDataset,1
v0.6.2,todo figure out if additional parameters are needed,1
v0.6.2,TODO dask: this needs to work with DataFrames,1
v0.6.2,TODO(travis): decouple config from training_set_metadata so we don't need to,1
v0.6.2,TODO(travis): revisit in the future to make this more precise,1
v0.6.2,TODO(geoffrey): remove this once Ray > 1.13 in our CI.,1
v0.6.2,TODO ray 1.8: convert to Tensors before shuffle,1
v0.6.2,Workaround for: https://github.com/ray-project/ray/issues/25643,1
v0.6.2,TODO(travis): remove after 1.13.1,1
v0.6.2,"TODO(travis): find way to avoid calling this, as it's expensive",1
v0.6.2,TODO: consider removing this. doesn't work currently and read performance seems generally,1
v0.6.2,TODO(travis): figure out why Ray is converting these into object types by default,1
v0.6.2,"TODO(travis): could try hashing the in-memory dataset, but this is tricky for Dask",1
v0.6.2,todo future: reintroduce the bucketed batcher,1
v0.6.2,TODO ray: implement dynamic batch size,1
v0.6.2,TODO(#1673): Need some more logic here for validating against output features,1
v0.6.2,"TODO (Connor): Add nesting logic for fc_layers, see fully_connected_module.py",1
v0.6.2,"TODO (Connor): Add nesting logic for fc_layers, see fully_connected_module.py",1
v0.6.2,TODO(#1673): Add conditional logic for fields like this one:,1
v0.6.2,todo v0.4: currently not clear way to set model graph,1
v0.6.2,TODO(travis): figure out a good way to support this. The problem with,1
v0.6.2,TODO: need to also include a filename for this figure,1
v0.6.2,Dummy implementation.,1
v0.6.2,TODO: Use a configurable ratio for how often to use teacher forcing during training.,1
v0.6.2,TODO: Use a configurable ratio for how often to use teacher forcing during training.,1
v0.6.2,Dummy implementation.,1
v0.6.2,"TODO(travis): stopgap solution, we should make it so we don't need to do this",1
v0.6.2,todo (Wael): tests for all types.,1
v0.6.2,todo (Wael): tests for all types.,1
v0.6.2,TODO(#2125): This code block needs some refactoring.,1
v0.6.2,"ORRIBLE HACK, IT'S THE ONLY WAY TO REMOVE PADDING",1
v0.6.2,END OF HORRIBLE HACK,1
v0.6.2,"TODO: more research needed on how to handle RayTune ""sample_from"" search space",1
v0.6.2,todo future: maybe modify this with TF2 mask mechanics,1
v0.6.2,"TODO(geoffrey): If we move to torchtext>=0.13.0, we can use return_tokens kwarg to get tokens directly",1
v0.6.2,Workaround for: https://issues.apache.org/jira/browse/ARROW-5645,1
v0.6.2,TODO(travis): move to cached_property when we drop Python 3.7.,1
v0.6.2,TODO(shreya): Confirm types of args,1
v0.6.2,"TODO: when loading an existing model, this loses metric values for all but the last epoch.",1
v0.6.2,TODO(travis): implement imbalance ratio,1
v0.6.2,"TODO (ASN): add other modalities (image, etc. )",1
v0.6.2,TODO: only single task currently,1
v0.6.2,TODO(travis): include encoder and decoder steps during inference,1
v0.6.2,TODO: with change to misc_utils.set_random_seed() this may be redundant,1
v0.6.2,TODO(Justin): Check that the semantics of input_size align with what the combiner's output shape returns,1
v0.6.2,TODO(justin): Remove dummy implementation. Make input_shape and output_shape functions.,1
v0.6.2,TODO:,1
v0.6.2,"TODO(travis): this won't work for text decoders, but we don't support explanations for those yet",1
v0.6.2,TODO(travis): pre-compute this during training from the full training dataset.,1
v0.6.2,"TODO(travis): this isn't really the expected value as it is for shap, so",1
v0.6.2,TODO (ASN): Circle back on how we want to set default placeholder value,1
v0.6.2,"TODO: construct new datasets by running encoders (for text, image)",1
v0.6.2,TODO (ASN): Circle back on how we want to set default placeholder value,1
v0.6.2,TODO(Justin): Move to config validation when that's ready.,1
v0.6.2,TODO(geoffrey): Add support for batch size tuning on CPU,1
v0.6.2,TODO (ASN) : Circle back on how we want to set default placeholder value,1
v0.6.2,TODO: refactor this into an interface,1
v0.6.2,workaround type limitations of the underlying frameworks,1
v0.6.2,"TODO(travis): we should revisit the user format here, as it silently breaks situations",1
v0.6.2,TODO: deprecated v0.5,1
v0.6.2,"TODO ray: find an informed way to set the parallelism, in practice",1
v0.6.2,TODO: When this is implemented we also need to update the,1
v0.6.2,"TODO ray: make this more configurable by allowing YAML overrides of timeout_s, etc.",1
v0.6.2,Deep copy to workaround https://github.com/ray-project/ray/issues/24139,1
v0.6.2,TODO: deprecated 0.5,1
v0.6.2,TODO(travis): determine if there is a performance penalty to passing in individual files instead of,1
v0.6.2,HACK: Workaround for https://github.com/modin-project/modin/issues/4686,1
v0.6.2,TODO(ekl) deprecate this once read fusion is available.,1
v0.6.2,TODO(geoffrey): ensure this subclasses ray.data.datasource.Reader in ray 1.14,1
v0.6.2,TODO(ekl) deprecate this once read fusion is available.,1
v0.6.2,"doesn't work for Snappy, so we double-check ourselves.",1
v0.6.2,"TODO(tgaddair): temporary workaround for Horovod's worker discovery logic,",1
v0.6.2,TODO(matt): Implement placement group strategies in BackendExecutor.,1
v0.6.2,Temporary workaround to prevent tests from automatically using the Ray backend. Taken from,1
v0.6.2,TODO(shreya): DataFrame created twice: here + CSVMixin. Figure out,1
v0.6.1,is there a better way to do this?,1
v0.6.1,todo the hidden output is actually a tensor. May need modification,1
v0.6.1,todo figure out the output size for parallel 1d conv,1
v0.6.1,TODO:  Need variant of assert_model_parameters_updated() to account for the two step calling sequence,1
v0.6.1,todo: remove code,1
v0.6.1,todo: re-add 'attention' after further research in implication of torch,1
v0.6.1,TODO(#1333): Refactor this test once torch sequence generator work is complete.,1
v0.6.1,"TODO(Justin): Move these to test_sequence_generator unit tests, and reintroduce decoder attention, beam_width, and",1
v0.6.1,"TODO(geoffrey): Add dataset_type=""csv"" back to parameters if we can prevent CI timeouts.",1
v0.6.1,TODO: NaN handling not supported. See `test_ray_save_inputs_and_outputs_without_nans` below.,1
v0.6.1,TODO: feature type not yet supported,1
v0.6.1,TODO: feature type not yet supported,1
v0.6.1,TODO(geoffrey): Fold modin tests into test_ray_image as @pytest.mark.parametrized once tests are optimized,1
v0.6.1,TODO(travis): move this to separate gpu module so we only have one ray cluster running at a time,1
v0.6.1,TODO(travis): https://github.com/ludwig-ai/ludwig/issues/1956,1
v0.6.1,TODO: Determine if we still need this if-then-else construct,1
v0.6.1,TODO(shreya): Uncomment when https://github.com/ludwig-ai/ludwig/issues/2039 is fixed.,1
v0.6.1,TODO ray: replace legacy mode when Ray Train supports placement groups,1
v0.6.1,TODO: remove this workaround when audio preprocessing is fixed.,1
v0.6.1,TODO: these are the only outputs we provide from Torchscript for now,1
v0.6.1,TODO all shadows built in name - come up with a more descriptive name,1
v0.6.1,TODO (ASN): add support for substitute_with_max parameter,1
v0.6.1,TODO: fix for Ray where workers may be of different skus,1
v0.6.1,TODO(travis): remove in favor of on_hyperopt_end for naming consistency,1
v0.6.1,TODO(shreya): Add type hints for missing args,1
v0.6.1,todo: revise docstring,1
v0.6.1,todo: assess how to specify padding for equivalent to 'same',1
v0.6.1,todo: determine how to pool_padding equivalent of 'same',1
v0.6.1,todo: fixup docstring,1
v0.6.1,todo: review docstring,1
v0.6.1,todo: fix up docstring,1
v0.6.1,todo: fix up docstring,1
v0.6.1,todo: update docstring as needed,1
v0.6.1,TODO(shreya): Make sure this is updated when FCStack is updated,1
v0.6.1,TODO(justin): Use official class properties.,1
v0.6.1,TODO(shreya): Confirm that this is it,1
v0.6.1,TODO(shreya): Confirm that this is it,1
v0.6.1,TODO(justin): This may need to be conditioned on which AutoModel gets chosen.,1
v0.6.1,TODO (ASN): Decide how we want to proceed if at least one trial has,1
v0.6.1,TODO (ASN): add image heuristics,1
v0.6.1,"todo future: this may be redundant, check",1
v0.6.1,Workaround for including additional tensors from output of input encoders for,1
v0.6.1,TODO(Justin): Think about how to make this communication work for multi-sequence,1
v0.6.1,todo: when https://github.com/ludwig-ai/ludwig/issues/810 is closed,1
v0.6.1,todo future: maybe modify this with TF2 mask mechanics,1
v0.6.1,"todo: can we just use projector_size? # hidden_size,",1
v0.6.1,"todo future: this may be redundant, check",1
v0.6.1,"todo future: this may be redundant, check",1
v0.6.1,Workaround for including additional tensors from output of input encoders for,1
v0.6.1,TODO(Justin): Think about how to make this communication work for multi-sequence,1
v0.6.1,todo future: maybe reintroduce these attention function,1
v0.6.1,todo future: maybe reintroduce these attention function,1
v0.6.1,todo future: maybe reintroduce these attention function,1
v0.6.1,TODO: double check,1
v0.6.1,"todo: enumerate for debugging, remove after testing",1
v0.6.1,TODO(shreya): Combine with ResNetBlock by adding a flag.,1
v0.6.1,TODO(shreya): Implement sparse embedding lookup.,1
v0.6.1,# TODO(shreya): Check if this is equivalent,1
v0.6.1,# TODO(shreya): Check if supported in torch,1
v0.6.1,todo: review for generality,1
v0.6.1,TODO: Simplify this.,1
v0.6.1,Dummy implementation.,1
v0.6.1,"TODO(Justin): Add a confusion matrix, see",1
v0.6.1,TODO: add a mechanism for letting the user decide to save it,1
v0.6.1,todo: re-evaluate need for separate handling of `attention` reducer,1
v0.6.1,TODO(shreya): Metrics should ideally just move to the correct device,1
v0.6.1,and not require the user to do this. This is a temporary fix. See,1
v0.6.1,TODO(Justin): Clean this up.,1
v0.6.1,TODO(shreya): Confirm if it's ok to do per channel normalization,1
v0.6.1,TODO(shreya): Also confirm if this is being used anywhere,1
v0.6.1,TODO(shreya): Confirm if ok to use imagenet means and std devs,1
v0.6.1,Nested conditional is a workaround to short-circuit boolean evaluation.,1
v0.6.1,TODO: alternatively use get_average_image() for unreachable images,1
v0.6.1,todo future add multiprocessing/multithreading,1
v0.6.1,TODO(1891): Remove backward compatibility hack once all models have been retrained with Ludwig after,1
v0.6.1,todo: refactor to reuse SequenceOutputFeature.postprocess_predictions,1
v0.6.1,TODO: add a mechanism for letting the user decide to save it,1
v0.6.1,Convert datetime to int64 to workaround Dask limitation,1
v0.6.1,TODO ray: this is needed because ray 1.7 doesn't support Dask to RayDataset,1
v0.6.1,todo figure out if additional parameters are needed,1
v0.6.1,TODO dask: this needs to work with DataFrames,1
v0.6.1,TODO(travis): decouple config from training_set_metadata so we don't need to,1
v0.6.1,TODO(travis): revisit in the future to make this more precise,1
v0.6.1,TODO(geoffrey): remove this once Ray > 1.13 in our CI.,1
v0.6.1,TODO ray 1.8: convert to Tensors before shuffle,1
v0.6.1,Workaround for: https://github.com/ray-project/ray/issues/25643,1
v0.6.1,TODO(travis): remove after 1.13.1,1
v0.6.1,"TODO(travis): find way to avoid calling this, as it's expensive",1
v0.6.1,TODO: consider removing this. doesn't work currently and read performance seems generally,1
v0.6.1,TODO(travis): figure out why Ray is converting these into object types by default,1
v0.6.1,"TODO(travis): could try hashing the in-memory dataset, but this is tricky for Dask",1
v0.6.1,todo future: reintroduce the bucketed batcher,1
v0.6.1,TODO ray: implement dynamic batch size,1
v0.6.1,TODO(#1673): Need some more logic here for validating against output features,1
v0.6.1,"TODO (Connor): Add nesting logic for fc_layers, see fully_connected_module.py",1
v0.6.1,"TODO (Connor): Add nesting logic for fc_layers, see fully_connected_module.py",1
v0.6.1,TODO(#1673): Add conditional logic for fields like this one:,1
v0.6.1,todo v0.4: currently not clear way to set model graph,1
v0.6.1,TODO(travis): figure out a good way to support this. The problem with,1
v0.6.1,TODO: need to also include a filename for this figure,1
v0.6.1,Dummy implementation.,1
v0.6.1,TODO: Use a configurable ratio for how often to use teacher forcing during training.,1
v0.6.1,TODO: Use a configurable ratio for how often to use teacher forcing during training.,1
v0.6.1,Dummy implementation.,1
v0.6.1,"TODO(travis): stopgap solution, we should make it so we don't need to do this",1
v0.6.1,todo (Wael): tests for all types.,1
v0.6.1,todo (Wael): tests for all types.,1
v0.6.1,TODO(#2125): This code block needs some refactoring.,1
v0.6.1,"ORRIBLE HACK, IT'S THE ONLY WAY TO REMOVE PADDING",1
v0.6.1,END OF HORRIBLE HACK,1
v0.6.1,"TODO: more research needed on how to handle RayTune ""sample_from"" search space",1
v0.6.1,todo future: maybe modify this with TF2 mask mechanics,1
v0.6.1,"TODO(geoffrey): If we move to torchtext>=0.13.0, we can use return_tokens kwarg to get tokens directly",1
v0.6.1,Workaround for: https://issues.apache.org/jira/browse/ARROW-5645,1
v0.6.1,TODO(travis): move to cached_property when we drop Python 3.7.,1
v0.6.1,TODO(shreya): Confirm types of args,1
v0.6.1,"TODO: when loading an existing model, this loses metric values for all but the last epoch.",1
v0.6.1,TODO(travis): implement imbalance ratio,1
v0.6.1,"TODO (ASN): add other modalities (image, etc. )",1
v0.6.1,TODO: only single task currently,1
v0.6.1,TODO(travis): include encoder and decoder steps during inference,1
v0.6.1,TODO: with change to misc_utils.set_random_seed() this may be redundant,1
v0.6.1,TODO(Justin): Check that the semantics of input_size align with what the combiner's output shape returns,1
v0.6.1,TODO(justin): Remove dummy implementation. Make input_shape and output_shape functions.,1
v0.6.1,"TODO(travis): this won't work for text decoders, but we don't support explanations for those yet",1
v0.6.1,TODO(travis): pre-compute this during training from the full training dataset.,1
v0.6.1,"TODO(travis): this isn't really the expected value as it is for shap, so",1
v0.6.1,TODO (ASN): Circle back on how we want to set default placeholder value,1
v0.6.1,"TODO: construct new datasets by running encoders (for text, image)",1
v0.6.1,TODO (ASN): Circle back on how we want to set default placeholder value,1
v0.6.1,TODO(Justin): Move to config validation when that's ready.,1
v0.6.1,TODO(geoffrey): Add support for batch size tuning on CPU,1
v0.6.1,TODO (ASN) : Circle back on how we want to set default placeholder value,1
v0.6.1,TODO: refactor this into an interface,1
v0.6.1,workaround type limitations of the underlying frameworks,1
v0.6.1,"TODO(travis): we should revisit the user format here, as it silently breaks situations",1
v0.6.1,TODO: deprecated v0.5,1
v0.6.1,TODO: use placement groups or otherwise spread across nodes,1
v0.6.1,TODO travis: replace backend here once ray 1.8 released,1
v0.6.1,"TODO ray: find an informed way to set the parallelism, in practice",1
v0.6.1,TODO: When this is implemented we also need to update the,1
v0.6.1,"TODO ray: make this more configurable by allowing YAML overrides of timeout_s, etc.",1
v0.6.1,Deep copy to workaround https://github.com/ray-project/ray/issues/24139,1
v0.6.1,TODO: deprecated 0.5,1
v0.6.1,TODO(travis): determine if there is a performance penalty to passing in individual files instead of,1
v0.6.1,HACK: Workaround for https://github.com/modin-project/modin/issues/4686,1
v0.6.1,TODO(ekl) deprecate this once read fusion is available.,1
v0.6.1,TODO(geoffrey): ensure this subclasses ray.data.datasource.Reader in ray 1.14,1
v0.6.1,TODO(ekl) deprecate this once read fusion is available.,1
v0.6.1,"doesn't work for Snappy, so we double-check ourselves.",1
v0.6.1,"TODO(tgaddair): temporary workaround for Horovod's worker discovery logic,",1
v0.6.1,TODO(matt): Implement placement group strategies in BackendExecutor.,1
v0.6.1,Temporary workaround to prevent tests from automatically using the Ray backend. Taken from,1
v0.6.1,TODO(shreya): DataFrame created twice: here + CSVMixin. Figure out,1
v0.6,is there a better way to do this?,1
v0.6,todo the hidden output is actually a tensor. May need modification,1
v0.6,todo figure out the output size for parallel 1d conv,1
v0.6,TODO:  Need variant of assert_model_parameters_updated() to account for the two step calling sequence,1
v0.6,todo: remove code,1
v0.6,todo: re-add 'attention' after further research in implication of torch,1
v0.6,TODO(#1333): Refactor this test once torch sequence generator work is complete.,1
v0.6,"TODO(Justin): Move these to test_sequence_generator unit tests, and reintroduce decoder attention, beam_width, and",1
v0.6,TODO(geoffrey): Fold modin tests into test_ray_image as @pytest.mark.parametrized once tests are optimized,1
v0.6,TODO(travis): move this to separate gpu module so we only have one ray cluster running at a time,1
v0.6,TODO(travis): https://github.com/ludwig-ai/ludwig/issues/1956,1
v0.6,TODO: Determine if we still need this if-then-else construct,1
v0.6,TODO(shreya): Uncomment when https://github.com/ludwig-ai/ludwig/issues/2039 is fixed.,1
v0.6,TODO ray: replace legacy mode when Ray Train supports placement groups,1
v0.6,TODO: remove this workaround when audio preprocessing is fixed.,1
v0.6,TODO: these are the only outputs we provide from Torchscript for now,1
v0.6,TODO all shadows built in name - come up with a more descriptive name,1
v0.6,TODO (ASN): add support for substitute_with_max parameter,1
v0.6,TODO: fix for Ray where workers may be of different skus,1
v0.6,TODO(travis): remove in favor of on_hyperopt_end for naming consistency,1
v0.6,TODO(shreya): Add type hints for missing args,1
v0.6,todo: revise docstring,1
v0.6,todo: assess how to specify padding for equivalent to 'same',1
v0.6,todo: determine how to pool_padding equivalent of 'same',1
v0.6,todo: fixup docstring,1
v0.6,todo: review docstring,1
v0.6,todo: fix up docstring,1
v0.6,todo: fix up docstring,1
v0.6,todo: update docstring as needed,1
v0.6,TODO(shreya): Make sure this is updated when FCStack is updated,1
v0.6,TODO(justin): Use official class properties.,1
v0.6,TODO(shreya): Confirm that this is it,1
v0.6,TODO(shreya): Confirm that this is it,1
v0.6,TODO(justin): This may need to be conditioned on which AutoModel gets chosen.,1
v0.6,TODO (ASN): Decide how we want to proceed if at least one trial has,1
v0.6,TODO (ASN): add image heuristics,1
v0.6,"todo future: this may be redundant, check",1
v0.6,Workaround for including additional tensors from output of input encoders for,1
v0.6,TODO(Justin): Think about how to make this communication work for multi-sequence,1
v0.6,todo: when https://github.com/ludwig-ai/ludwig/issues/810 is closed,1
v0.6,todo future: maybe modify this with TF2 mask mechanics,1
v0.6,"todo: can we just use projector_size? # hidden_size,",1
v0.6,"todo future: this may be redundant, check",1
v0.6,"todo future: this may be redundant, check",1
v0.6,Workaround for including additional tensors from output of input encoders for,1
v0.6,TODO(Justin): Think about how to make this communication work for multi-sequence,1
v0.6,todo future: maybe reintroduce these attention function,1
v0.6,todo future: maybe reintroduce these attention function,1
v0.6,todo future: maybe reintroduce these attention function,1
v0.6,TODO: double check,1
v0.6,"todo: enumerate for debugging, remove after testing",1
v0.6,TODO(shreya): Combine with ResNetBlock by adding a flag.,1
v0.6,TODO(shreya): Implement sparse embedding lookup.,1
v0.6,# TODO(shreya): Check if this is equivalent,1
v0.6,# TODO(shreya): Check if supported in torch,1
v0.6,todo: review for generality,1
v0.6,TODO: Simplify this.,1
v0.6,Dummy implementation.,1
v0.6,"TODO(Justin): Add a confusion matrix, see",1
v0.6,TODO: add a mechanism for letting the user decide to save it,1
v0.6,todo: re-evaluate need for separate handling of `attention` reducer,1
v0.6,TODO(shreya): Metrics should ideally just move to the correct device,1
v0.6,and not require the user to do this. This is a temporary fix. See,1
v0.6,TODO(Justin): Clean this up.,1
v0.6,TODO(shreya): Confirm if it's ok to do per channel normalization,1
v0.6,TODO(shreya): Also confirm if this is being used anywhere,1
v0.6,TODO(shreya): Confirm if ok to use imagenet means and std devs,1
v0.6,Nested conditional is a workaround to short-circuit boolean evaluation.,1
v0.6,TODO: alternatively use get_average_image() for unreachable images,1
v0.6,todo future add multiprocessing/multithreading,1
v0.6,TODO(1891): Remove backward compatibility hack once all models have been retrained with Ludwig after,1
v0.6,todo: refactor to reuse SequenceOutputFeature.postprocess_predictions,1
v0.6,TODO: add a mechanism for letting the user decide to save it,1
v0.6,Convert datetime to int64 to workaround Dask limitation,1
v0.6,TODO ray: this is needed because ray 1.7 doesn't support Dask to RayDataset,1
v0.6,todo figure out if additional parameters are needed,1
v0.6,TODO dask: this needs to work with DataFrames,1
v0.6,TODO(travis): decouple config from training_set_metadata so we don't need to,1
v0.6,TODO(travis): revisit in the future to make this more precise,1
v0.6,TODO(geoffrey): remove this once Ray > 1.13 in our CI.,1
v0.6,TODO ray 1.8: convert to Tensors before shuffle,1
v0.6,Workaround for: https://github.com/ray-project/ray/issues/25643,1
v0.6,TODO(travis): remove after 1.13.1,1
v0.6,"TODO(travis): find way to avoid calling this, as it's expensive",1
v0.6,TODO: consider removing this. doesn't work currently and read performance seems generally,1
v0.6,TODO(travis): figure out why Ray is converting these into object types by default,1
v0.6,"TODO(travis): could try hashing the in-memory dataset, but this is tricky for Dask",1
v0.6,todo future: reintroduce the bucketed batcher,1
v0.6,TODO ray: implement dynamic batch size,1
v0.6,TODO(#1673): Need some more logic here for validating against output features,1
v0.6,"TODO (Connor): Add nesting logic for fc_layers, see fully_connected_module.py",1
v0.6,"TODO (Connor): Add nesting logic for fc_layers, see fully_connected_module.py",1
v0.6,TODO(#1673): Add conditional logic for fields like this one:,1
v0.6,todo v0.4: currently not clear way to set model graph,1
v0.6,TODO(travis): figure out a good way to support this. The problem with,1
v0.6,TODO: need to also include a filename for this figure,1
v0.6,Dummy implementation.,1
v0.6,TODO: Use a configurable ratio for how often to use teacher forcing during training.,1
v0.6,TODO: Use a configurable ratio for how often to use teacher forcing during training.,1
v0.6,Dummy implementation.,1
v0.6,"TODO(travis): stopgap solution, we should make it so we don't need to do this",1
v0.6,todo (Wael): tests for all types.,1
v0.6,todo (Wael): tests for all types.,1
v0.6,TODO(#2125): This code block needs some refactoring.,1
v0.6,"ORRIBLE HACK, IT'S THE ONLY WAY TO REMOVE PADDING",1
v0.6,END OF HORRIBLE HACK,1
v0.6,"TODO: more research needed on how to handle RayTune ""sample_from"" search space",1
v0.6,todo future: maybe modify this with TF2 mask mechanics,1
v0.6,"TODO(geoffrey): If we move to torchtext>=0.13.0, we can use return_tokens kwarg to get tokens directly",1
v0.6,Workaround for: https://issues.apache.org/jira/browse/ARROW-5645,1
v0.6,TODO(travis): move to cached_property when we drop Python 3.7.,1
v0.6,TODO(shreya): Confirm types of args,1
v0.6,"TODO: when loading an existing model, this loses metric values for all but the last epoch.",1
v0.6,TODO(travis): implement imbalance ratio,1
v0.6,"TODO (ASN): add other modalities (image, etc. )",1
v0.6,TODO: only single task currently,1
v0.6,TODO(travis): include encoder and decoder steps during inference,1
v0.6,TODO: with change to misc_utils.set_random_seed() this may be redundant,1
v0.6,TODO(Justin): Check that the semantics of input_size align with what the combiner's output shape returns,1
v0.6,TODO(justin): Remove dummy implementation. Make input_shape and output_shape functions.,1
v0.6,"TODO(travis): this won't work for text decoders, but we don't support explanations for those yet",1
v0.6,TODO(travis): pre-compute this during training from the full training dataset.,1
v0.6,"TODO(travis): this isn't really the expected value as it is for shap, so",1
v0.6,TODO (ASN): Circle back on how we want to set default placeholder value,1
v0.6,"TODO: construct new datasets by running encoders (for text, image)",1
v0.6,TODO: only single task currently,1
v0.6,TODO (ASN): Circle back on how we want to set default placeholder value,1
v0.6,TODO(Justin): Move to config validation when that's ready.,1
v0.6,TODO(geoffrey): Add support for batch size tuning on CPU,1
v0.6,TODO (ASN) : Circle back on how we want to set default placeholder value,1
v0.6,TODO: refactor this into an interface,1
v0.6,workaround type limitations of the underlying frameworks,1
v0.6,"TODO(travis): we should revisit the user format here, as it silently breaks situations",1
v0.6,TODO: deprecated v0.5,1
v0.6,TODO: use placement groups or otherwise spread across nodes,1
v0.6,TODO travis: replace backend here once ray 1.8 released,1
v0.6,"TODO ray: find an informed way to set the parallelism, in practice",1
v0.6,TODO: When this is implemented we also need to update the,1
v0.6,"TODO ray: make this more configurable by allowing YAML overrides of timeout_s, etc.",1
v0.6,Deep copy to workaround https://github.com/ray-project/ray/issues/24139,1
v0.6,TODO: deprecated 0.5,1
v0.6,TODO(travis): determine if there is a performance penalty to passing in individual files instead of,1
v0.6,HACK: Workaround for https://github.com/modin-project/modin/issues/4686,1
v0.6,TODO(ekl) deprecate this once read fusion is available.,1
v0.6,TODO(geoffrey): ensure this subclasses ray.data.datasource.Reader in ray 1.14,1
v0.6,TODO(ekl) deprecate this once read fusion is available.,1
v0.6,"doesn't work for Snappy, so we double-check ourselves.",1
v0.6,"TODO(tgaddair): temporary workaround for Horovod's worker discovery logic,",1
v0.6,TODO(matt): Implement placement group strategies in BackendExecutor.,1
v0.6,Temporary workaround to prevent tests from automatically using the Ray backend. Taken from,1
v0.6,TODO(shreya): DataFrame created twice: here + CSVMixin. Figure out,1
v0.6rc1,is there a better way to do this?,1
v0.6rc1,todo the hidden output is actually a tensor. May need modification,1
v0.6rc1,todo figure out the output size for parallel 1d conv,1
v0.6rc1,TODO:  Need variant of assert_model_parameters_updated() to account for the two step calling sequence,1
v0.6rc1,todo: remove code,1
v0.6rc1,todo: re-add 'attention' after further research in implication of torch,1
v0.6rc1,TODO(#1333): Refactor this test once torch sequence generator work is complete.,1
v0.6rc1,"TODO(Justin): Move these to test_sequence_generator unit tests, and reintroduce decoder attention, beam_width, and",1
v0.6rc1,TODO(geoffrey): Fold modin tests into test_ray_image as @pytest.mark.parametrized once tests are optimized,1
v0.6rc1,TODO(travis): move this to separate gpu module so we only have one ray cluster running at a time,1
v0.6rc1,TODO(travis): https://github.com/ludwig-ai/ludwig/issues/1956,1
v0.6rc1,TODO: Determine if we still need this if-then-else construct,1
v0.6rc1,TODO(shreya): Uncomment when https://github.com/ludwig-ai/ludwig/issues/2039 is fixed.,1
v0.6rc1,TODO ray: replace legacy mode when Ray Train supports placement groups,1
v0.6rc1,TODO: remove this workaround when audio preprocessing is fixed.,1
v0.6rc1,TODO: these are the only outputs we provide from Torchscript for now,1
v0.6rc1,TODO all shadows built in name - come up with a more descriptive name,1
v0.6rc1,TODO (ASN): add support for substitute_with_max parameter,1
v0.6rc1,TODO: fix for Ray where workers may be of different skus,1
v0.6rc1,TODO(travis): remove in favor of on_hyperopt_end for naming consistency,1
v0.6rc1,TODO(shreya): Add type hints for missing args,1
v0.6rc1,todo: revise docstring,1
v0.6rc1,todo: assess how to specify padding for equivalent to 'same',1
v0.6rc1,todo: determine how to pool_padding equivalent of 'same',1
v0.6rc1,todo: fixup docstring,1
v0.6rc1,todo: review docstring,1
v0.6rc1,todo: fix up docstring,1
v0.6rc1,todo: fix up docstring,1
v0.6rc1,todo: update docstring as needed,1
v0.6rc1,TODO(shreya): Make sure this is updated when FCStack is updated,1
v0.6rc1,TODO(justin): Use official class properties.,1
v0.6rc1,TODO(shreya): Confirm that this is it,1
v0.6rc1,TODO(shreya): Confirm that this is it,1
v0.6rc1,TODO(justin): This may need to be conditioned on which AutoModel gets chosen.,1
v0.6rc1,TODO (ASN): Decide how we want to proceed if at least one trial has,1
v0.6rc1,TODO (ASN): add image heuristics,1
v0.6rc1,"todo future: this may be redundant, check",1
v0.6rc1,Workaround for including additional tensors from output of input encoders for,1
v0.6rc1,TODO(Justin): Think about how to make this communication work for multi-sequence,1
v0.6rc1,todo: when https://github.com/ludwig-ai/ludwig/issues/810 is closed,1
v0.6rc1,todo future: maybe modify this with TF2 mask mechanics,1
v0.6rc1,"todo: can we just use projector_size? # hidden_size,",1
v0.6rc1,"todo future: this may be redundant, check",1
v0.6rc1,"todo future: this may be redundant, check",1
v0.6rc1,Workaround for including additional tensors from output of input encoders for,1
v0.6rc1,TODO(Justin): Think about how to make this communication work for multi-sequence,1
v0.6rc1,todo future: maybe reintroduce these attention function,1
v0.6rc1,todo future: maybe reintroduce these attention function,1
v0.6rc1,todo future: maybe reintroduce these attention function,1
v0.6rc1,TODO: double check,1
v0.6rc1,"todo: enumerate for debugging, remove after testing",1
v0.6rc1,TODO(shreya): Combine with ResNetBlock by adding a flag.,1
v0.6rc1,TODO(shreya): Implement sparse embedding lookup.,1
v0.6rc1,# TODO(shreya): Check if this is equivalent,1
v0.6rc1,# TODO(shreya): Check if supported in torch,1
v0.6rc1,todo: review for generality,1
v0.6rc1,TODO: Simplify this.,1
v0.6rc1,Dummy implementation.,1
v0.6rc1,"TODO(Justin): Add a confusion matrix, see",1
v0.6rc1,TODO: add a mechanism for letting the user decide to save it,1
v0.6rc1,todo: re-evaluate need for separate handling of `attention` reducer,1
v0.6rc1,TODO(shreya): Metrics should ideally just move to the correct device,1
v0.6rc1,and not require the user to do this. This is a temporary fix. See,1
v0.6rc1,TODO(Justin): Clean this up.,1
v0.6rc1,TODO(shreya): Confirm if it's ok to do per channel normalization,1
v0.6rc1,TODO(shreya): Also confirm if this is being used anywhere,1
v0.6rc1,TODO(shreya): Confirm if ok to use imagenet means and std devs,1
v0.6rc1,Nested conditional is a workaround to short-circuit boolean evaluation.,1
v0.6rc1,TODO: alternatively use get_average_image() for unreachable images,1
v0.6rc1,todo future add multiprocessing/multithreading,1
v0.6rc1,TODO(1891): Remove backward compatibility hack once all models have been retrained with Ludwig after,1
v0.6rc1,todo: refactor to reuse SequenceOutputFeature.postprocess_predictions,1
v0.6rc1,TODO: add a mechanism for letting the user decide to save it,1
v0.6rc1,Convert datetime to int64 to workaround Dask limitation,1
v0.6rc1,TODO ray: this is needed because ray 1.7 doesn't support Dask to RayDataset,1
v0.6rc1,todo figure out if additional parameters are needed,1
v0.6rc1,TODO dask: this needs to work with DataFrames,1
v0.6rc1,TODO(travis): decouple config from training_set_metadata so we don't need to,1
v0.6rc1,TODO(travis): revisit in the future to make this more precise,1
v0.6rc1,TODO(geoffrey): remove this once Ray > 1.13 in our CI.,1
v0.6rc1,TODO ray 1.8: convert to Tensors before shuffle,1
v0.6rc1,Workaround for: https://github.com/ray-project/ray/issues/25643,1
v0.6rc1,TODO(travis): remove after 1.13.1,1
v0.6rc1,"TODO(travis): find way to avoid calling this, as it's expensive",1
v0.6rc1,TODO: consider removing this. doesn't work currently and read performance seems generally,1
v0.6rc1,TODO(travis): figure out why Ray is converting these into object types by default,1
v0.6rc1,"TODO(travis): could try hashing the in-memory dataset, but this is tricky for Dask",1
v0.6rc1,todo future: reintroduce the bucketed batcher,1
v0.6rc1,TODO ray: implement dynamic batch size,1
v0.6rc1,TODO(#1673): Need some more logic here for validating against output features,1
v0.6rc1,"TODO (Connor): Add nesting logic for fc_layers, see fully_connected_module.py",1
v0.6rc1,"TODO (Connor): Add nesting logic for fc_layers, see fully_connected_module.py",1
v0.6rc1,TODO(#1673): Add conditional logic for fields like this one:,1
v0.6rc1,todo v0.4: currently not clear way to set model graph,1
v0.6rc1,TODO(travis): figure out a good way to support this. The problem with,1
v0.6rc1,TODO: need to also include a filename for this figure,1
v0.6rc1,Dummy implementation.,1
v0.6rc1,TODO: Use a configurable ratio for how often to use teacher forcing during training.,1
v0.6rc1,TODO: Use a configurable ratio for how often to use teacher forcing during training.,1
v0.6rc1,Dummy implementation.,1
v0.6rc1,"TODO(travis): stopgap solution, we should make it so we don't need to do this",1
v0.6rc1,todo (Wael): tests for all types.,1
v0.6rc1,todo (Wael): tests for all types.,1
v0.6rc1,TODO(#2125): This code block needs some refactoring.,1
v0.6rc1,"ORRIBLE HACK, IT'S THE ONLY WAY TO REMOVE PADDING",1
v0.6rc1,END OF HORRIBLE HACK,1
v0.6rc1,"TODO: more research needed on how to handle RayTune ""sample_from"" search space",1
v0.6rc1,todo future: maybe modify this with TF2 mask mechanics,1
v0.6rc1,"TODO(geoffrey): If we move to torchtext>=0.13.0, we can use return_tokens kwarg to get tokens directly",1
v0.6rc1,Workaround for: https://issues.apache.org/jira/browse/ARROW-5645,1
v0.6rc1,TODO(travis): move to cached_property when we drop Python 3.7.,1
v0.6rc1,TODO(shreya): Confirm types of args,1
v0.6rc1,"TODO: when loading an existing model, this loses metric values for all but the last epoch.",1
v0.6rc1,TODO(travis): implement imbalance ratio,1
v0.6rc1,"TODO (ASN): add other modalities (image, etc. )",1
v0.6rc1,TODO: only single task currently,1
v0.6rc1,TODO(travis): include encoder and decoder steps during inference,1
v0.6rc1,TODO: with change to misc_utils.set_random_seed() this may be redundant,1
v0.6rc1,TODO(Justin): Check that the semantics of input_size align with what the combiner's output shape returns,1
v0.6rc1,TODO(justin): Remove dummy implementation. Make input_shape and output_shape functions.,1
v0.6rc1,"TODO(travis): this won't work for text decoders, but we don't support explanations for those yet",1
v0.6rc1,TODO(travis): pre-compute this during training from the full training dataset.,1
v0.6rc1,"TODO(travis): this isn't really the expected value as it is for shap, so",1
v0.6rc1,TODO (ASN): Circle back on how we want to set default placeholder value,1
v0.6rc1,"TODO: construct new datasets by running encoders (for text, image)",1
v0.6rc1,TODO: only single task currently,1
v0.6rc1,TODO (ASN): Circle back on how we want to set default placeholder value,1
v0.6rc1,TODO(Justin): Move to config validation when that's ready.,1
v0.6rc1,TODO(geoffrey): Add support for batch size tuning on CPU,1
v0.6rc1,TODO (ASN) : Circle back on how we want to set default placeholder value,1
v0.6rc1,TODO: refactor this into an interface,1
v0.6rc1,workaround type limitations of the underlying frameworks,1
v0.6rc1,"TODO(travis): we should revisit the user format here, as it silently breaks situations",1
v0.6rc1,TODO: deprecated v0.5,1
v0.6rc1,TODO: use placement groups or otherwise spread across nodes,1
v0.6rc1,TODO travis: replace backend here once ray 1.8 released,1
v0.6rc1,"TODO ray: find an informed way to set the parallelism, in practice",1
v0.6rc1,TODO: When this is implemented we also need to update the,1
v0.6rc1,"TODO ray: make this more configurable by allowing YAML overrides of timeout_s, etc.",1
v0.6rc1,Deep copy to workaround https://github.com/ray-project/ray/issues/24139,1
v0.6rc1,TODO: deprecated 0.5,1
v0.6rc1,TODO(travis): determine if there is a performance penalty to passing in individual files instead of,1
v0.6rc1,HACK: Workaround for https://github.com/modin-project/modin/issues/4686,1
v0.6rc1,TODO(ekl) deprecate this once read fusion is available.,1
v0.6rc1,TODO(geoffrey): ensure this subclasses ray.data.datasource.Reader in ray 1.14,1
v0.6rc1,TODO(ekl) deprecate this once read fusion is available.,1
v0.6rc1,"doesn't work for Snappy, so we double-check ourselves.",1
v0.6rc1,"TODO(tgaddair): temporary workaround for Horovod's worker discovery logic,",1
v0.6rc1,TODO(matt): Implement placement group strategies in BackendExecutor.,1
v0.6rc1,Temporary workaround to prevent tests from automatically using the Ray backend. Taken from,1
v0.6rc1,TODO(shreya): DataFrame created twice: here + CSVMixin. Figure out,1
v0.6.beta,is there a better way to do this?,1
v0.6.beta,todo the hidden output is actually a tensor. May need modification,1
v0.6.beta,todo figure out the output size for parallel 1d conv,1
v0.6.beta,TODO:  Need variant of assert_model_parameters_updated() to account for the two step calling sequence,1
v0.6.beta,todo: remove code,1
v0.6.beta,todo: re-add 'attention' after further research in implication of torch,1
v0.6.beta,TODO(#1333): Refactor this test once torch sequence generator work is complete.,1
v0.6.beta,"TODO(Justin): Move these to test_sequence_generator unit tests, and reintroduce decoder attention, beam_width, and",1
v0.6.beta,TODO(geoffrey): Fold modin tests into test_ray_image as @pytest.mark.parametrized once tests are optimized,1
v0.6.beta,TODO(travis): move this to separate gpu module so we only have one ray cluster running at a time,1
v0.6.beta,TODO(travis): https://github.com/ludwig-ai/ludwig/issues/1956,1
v0.6.beta,TODO: Determine if we still need this if-then-else construct,1
v0.6.beta,TODO(shreya): Uncomment when https://github.com/ludwig-ai/ludwig/issues/2039 is fixed.,1
v0.6.beta,TODO ray: replace legacy mode when Ray Train supports placement groups,1
v0.6.beta,TODO: remove this workaround when audio preprocessing is fixed.,1
v0.6.beta,TODO: these are the only outputs we provide from Torchscript for now,1
v0.6.beta,TODO all shadows built in name - come up with a more descriptive name,1
v0.6.beta,TODO (ASN): add support for substitute_with_max parameter,1
v0.6.beta,TODO: fix for Ray where workers may be of different skus,1
v0.6.beta,TODO(travis): remove in favor of on_hyperopt_end for naming consistency,1
v0.6.beta,TODO(shreya): Add type hints for missing args,1
v0.6.beta,todo: revise docstring,1
v0.6.beta,todo: assess how to specify padding for equivalent to 'same',1
v0.6.beta,todo: determine how to pool_padding equivalent of 'same',1
v0.6.beta,todo: fixup docstring,1
v0.6.beta,todo: review docstring,1
v0.6.beta,todo: fix up docstring,1
v0.6.beta,todo: fix up docstring,1
v0.6.beta,todo: update docstring as needed,1
v0.6.beta,TODO(shreya): Make sure this is updated when FCStack is updated,1
v0.6.beta,TODO(justin): Use official class properties.,1
v0.6.beta,TODO(shreya): Confirm that this is it,1
v0.6.beta,TODO(shreya): Confirm that this is it,1
v0.6.beta,TODO(justin): This may need to be conditioned on which AutoModel gets chosen.,1
v0.6.beta,TODO (ASN): Decide how we want to proceed if at least one trial has,1
v0.6.beta,TODO (ASN): add image heuristics,1
v0.6.beta,"todo future: this may be redundant, check",1
v0.6.beta,Workaround for including additional tensors from output of input encoders for,1
v0.6.beta,TODO(Justin): Think about how to make this communication work for multi-sequence,1
v0.6.beta,todo: when https://github.com/ludwig-ai/ludwig/issues/810 is closed,1
v0.6.beta,todo future: maybe modify this with TF2 mask mechanics,1
v0.6.beta,"todo: can we just use projector_size? # hidden_size,",1
v0.6.beta,"todo future: this may be redundant, check",1
v0.6.beta,"todo future: this may be redundant, check",1
v0.6.beta,Workaround for including additional tensors from output of input encoders for,1
v0.6.beta,TODO(Justin): Think about how to make this communication work for multi-sequence,1
v0.6.beta,todo future: maybe reintroduce these attention function,1
v0.6.beta,todo future: maybe reintroduce these attention function,1
v0.6.beta,todo future: maybe reintroduce these attention function,1
v0.6.beta,TODO: double check,1
v0.6.beta,"todo: enumerate for debugging, remove after testing",1
v0.6.beta,TODO(shreya): Combine with ResNetBlock by adding a flag.,1
v0.6.beta,TODO(shreya): Implement sparse embedding lookup.,1
v0.6.beta,# TODO(shreya): Check if this is equivalent,1
v0.6.beta,# TODO(shreya): Check if supported in torch,1
v0.6.beta,todo: review for generality,1
v0.6.beta,TODO: Simplify this.,1
v0.6.beta,Dummy implementation.,1
v0.6.beta,"TODO(Justin): Add a confusion matrix, see",1
v0.6.beta,TODO: add a mechanism for letting the user decide to save it,1
v0.6.beta,todo: re-evaluate need for separate handling of `attention` reducer,1
v0.6.beta,TODO(shreya): Metrics should ideally just move to the correct device,1
v0.6.beta,and not require the user to do this. This is a temporary fix. See,1
v0.6.beta,TODO(Justin): Clean this up.,1
v0.6.beta,TODO(shreya): Confirm if it's ok to do per channel normalization,1
v0.6.beta,TODO(shreya): Also confirm if this is being used anywhere,1
v0.6.beta,TODO(shreya): Confirm if ok to use imagenet means and std devs,1
v0.6.beta,Nested conditional is a workaround to short-circuit boolean evaluation.,1
v0.6.beta,TODO: alternatively use get_average_image() for unreachable images,1
v0.6.beta,todo future add multiprocessing/multithreading,1
v0.6.beta,TODO(1891): Remove backward compatibility hack once all models have been retrained with Ludwig after,1
v0.6.beta,todo: refactor to reuse SequenceOutputFeature.postprocess_predictions,1
v0.6.beta,TODO: add a mechanism for letting the user decide to save it,1
v0.6.beta,TODO dask: find a way to support this method,1
v0.6.beta,Convert datetime to int64 to workaround Dask limitation,1
v0.6.beta,TODO ray: this is needed because ray 1.7 doesn't support Dask to RayDataset,1
v0.6.beta,todo figure out if additional parameters are needed,1
v0.6.beta,TODO dask: this needs to work with DataFrames,1
v0.6.beta,TODO(travis): revisit in the future to make this more precise,1
v0.6.beta,TODO(geoffrey): remove this once Ray > 1.13 in our CI.,1
v0.6.beta,TODO ray 1.8: convert to Tensors before shuffle,1
v0.6.beta,Workaround for: https://github.com/ray-project/ray/issues/25643,1
v0.6.beta,TODO(travis): remove after 1.13.1,1
v0.6.beta,"TODO(travis): find way to avoid calling this, as it's expensive",1
v0.6.beta,TODO: consider removing this. doesn't work currently and read performance seems generally,1
v0.6.beta,TODO(travis): figure out why Ray is converting these into object types by default,1
v0.6.beta,"TODO(travis): could try hashing the in-memory dataset, but this is tricky for Dask",1
v0.6.beta,todo future: reintroduce the bucketed batcher,1
v0.6.beta,TODO ray: implement dynamic batch size,1
v0.6.beta,TODO(#1673): Need some more logic here for validating against output features,1
v0.6.beta,"TODO (Connor): Add nesting logic for fc_layers, see fully_connected_module.py",1
v0.6.beta,"TODO (Connor): Add nesting logic for fc_layers, see fully_connected_module.py",1
v0.6.beta,TODO(#1673): Add conditional logic for fields like this one:,1
v0.6.beta,todo v0.4: currently not clear way to set model graph,1
v0.6.beta,TODO(travis): figure out a good way to support this. The problem with,1
v0.6.beta,TODO: need to also include a filename for this figure,1
v0.6.beta,Dummy implementation.,1
v0.6.beta,TODO: Use a configurable ratio for how often to use teacher forcing during training.,1
v0.6.beta,TODO: Use a configurable ratio for how often to use teacher forcing during training.,1
v0.6.beta,Dummy implementation.,1
v0.6.beta,todo (Wael): tests for all types.,1
v0.6.beta,todo (Wael): tests for all types.,1
v0.6.beta,TODO(#2125): This code block needs some refactoring.,1
v0.6.beta,"ORRIBLE HACK, IT'S THE ONLY WAY TO REMOVE PADDING",1
v0.6.beta,END OF HORRIBLE HACK,1
v0.6.beta,"TODO: more research needed on how to handle RayTune ""sample_from"" search space",1
v0.6.beta,todo future: maybe modify this with TF2 mask mechanics,1
v0.6.beta,"TODO(geoffrey): If we move to torchtext>=0.13.0, we can use return_tokens kwarg to get tokens directly",1
v0.6.beta,Workaround for: https://issues.apache.org/jira/browse/ARROW-5645,1
v0.6.beta,TODO(shreya): Confirm types of args,1
v0.6.beta,"TODO (ASN): add other modalities (image, etc. )",1
v0.6.beta,TODO: only single task currently,1
v0.6.beta,TODO(travis): include encoder and decoder steps during inference,1
v0.6.beta,TODO: with change to misc_utils.set_random_seed() this may be redundant,1
v0.6.beta,TODO(Justin): Check that the semantics of input_size align with what the combiner's output shape returns,1
v0.6.beta,TODO(justin): Remove dummy implementation. Make input_shape and output_shape functions.,1
v0.6.beta,"TODO(travis): this won't work for text decoders, but we don't support explanations for those yet",1
v0.6.beta,TODO(travis): pre-compute this during training from the full training dataset.,1
v0.6.beta,"TODO(travis): this isn't really the expected value as it is for shap, so",1
v0.6.beta,TODO (ASN): Circle back on how we want to set default placeholder value,1
v0.6.beta,"TODO: construct new datasets by running encoders (for text, image)",1
v0.6.beta,TODO: only single task currently,1
v0.6.beta,TODO (ASN): Circle back on how we want to set default placeholder value,1
v0.6.beta,TODO(Justin): Move to config validation when that's ready.,1
v0.6.beta,TODO(geoffrey): Add support for batch size tuning on CPU,1
v0.6.beta,TODO (ASN) : Circle back on how we want to set default placeholder value,1
v0.6.beta,TODO: refactor this into an interface,1
v0.6.beta,workaround type limitations of the underlying frameworks,1
v0.6.beta,"TODO(travis): we should revisit the user format here, as it silently breaks situations",1
v0.6.beta,TODO: deprecated v0.5,1
v0.6.beta,TODO: use placement groups or otherwise spread across nodes,1
v0.6.beta,TODO travis: replace backend here once ray 1.8 released,1
v0.6.beta,"TODO ray: find an informed way to set the parallelism, in practice",1
v0.6.beta,TODO: When this is implemented we also need to update the,1
v0.6.beta,"TODO ray: make this more configurable by allowing YAML overrides of timeout_s, etc.",1
v0.6.beta,Deep copy to workaround https://github.com/ray-project/ray/issues/24139,1
v0.6.beta,TODO: deprecated 0.5,1
v0.6.beta,TODO(travis): determine if there is a performance penalty to passing in individual files instead of,1
v0.6.beta,HACK: Workaround for https://github.com/modin-project/modin/issues/4686,1
v0.6.beta,TODO(ekl) deprecate this once read fusion is available.,1
v0.6.beta,TODO(geoffrey): ensure this subclasses ray.data.datasource.Reader in ray 1.14,1
v0.6.beta,TODO(ekl) deprecate this once read fusion is available.,1
v0.6.beta,"doesn't work for Snappy, so we double-check ourselves.",1
v0.6.beta,"TODO(tgaddair): temporary workaround for Horovod's worker discovery logic,",1
v0.6.beta,TODO(matt): Implement placement group strategies in BackendExecutor.,1
v0.6.beta,Temporary workaround to prevent tests from automatically using the Ray backend. Taken from,1
v0.6.beta,TODO(shreya): DataFrame created twice: here + CSVMixin. Figure out,1
v0.5.5,is there a better way to do this?,1
v0.5.5,todo the hidden output is actually a tensor. May need modification,1
v0.5.5,todo figure out the output size for parallel 1d conv,1
v0.5.5,todo: remove code,1
v0.5.5,todo: re-add 'attention' after further research in implication of torch,1
v0.5.5,TODO(#1333): Refactor this test once torch sequence generator work is complete.,1
v0.5.5,"TODO(Justin): Move these to test_sequence_generator unit tests, and reintroduce decoder attention, beam_width, and",1
v0.5.5,TODO(travis): https://github.com/ludwig-ai/ludwig/issues/1956,1
v0.5.5,TODO: Determine if we still need this if-then-else construct,1
v0.5.5,TODO(shreya): Uncomment when https://github.com/ludwig-ai/ludwig/issues/2039 is fixed.,1
v0.5.5,TODO ray: replace legacy mode when Ray Train supports placement groups,1
v0.5.5,TODO: remove this workaround when audio preprocessing is fixed.,1
v0.5.5,TODO: these are the only outputs we provide from Torchscript for now,1
v0.5.5,TODO all shadows built in name - come up with a more descriptive name,1
v0.5.5,TODO (ASN): add support for substitute_with_max parameter,1
v0.5.5,todo: support loading other model types based on config,1
v0.5.5,TODO: fix for Ray where workers may be of different skus,1
v0.5.5,TODO(travis): remove in favor of on_hyperopt_end for naming consistency,1
v0.5.5,TODO(shreya): Add type hints for missing args,1
v0.5.5,todo: revise docstring,1
v0.5.5,todo: assess how to specify padding for equivalent to 'same',1
v0.5.5,todo: determine how to pool_padding equivalent of 'same',1
v0.5.5,todo: fixup docstring,1
v0.5.5,todo: review docstring,1
v0.5.5,todo: fix up docstring,1
v0.5.5,todo: fix up docstring,1
v0.5.5,todo: update docstring as needed,1
v0.5.5,TODO(shreya): Make sure this is updated when FCStack is updated,1
v0.5.5,TODO(justin): Use official class properties.,1
v0.5.5,TODO(shreya): Confirm that this is it,1
v0.5.5,TODO(shreya): Confirm that this is it,1
v0.5.5,TODO(justin): This may need to be conditioned on which AutoModel gets chosen.,1
v0.5.5,"TODO (ASN): add other modalities (image, etc. )",1
v0.5.5,TODO (ASN): Decide how we want to proceed if at least one trial has,1
v0.5.5,TODO (ASN): add image heuristics,1
v0.5.5,"todo future: this may be redundant, check",1
v0.5.5,Workaround for including additional tensors from output of input encoders for,1
v0.5.5,TODO(Justin): Think about how to make this communication work for multi-sequence,1
v0.5.5,todo: when https://github.com/ludwig-ai/ludwig/issues/810 is closed,1
v0.5.5,todo future: maybe modify this with TF2 mask mechanics,1
v0.5.5,"todo: can we just use projector_size? # hidden_size,",1
v0.5.5,"todo future: this may be redundant, check",1
v0.5.5,"todo future: this may be redundant, check",1
v0.5.5,Workaround for including additional tensors from output of input encoders for,1
v0.5.5,TODO(Justin): Think about how to make this communication work for multi-sequence,1
v0.5.5,todo future: maybe reintroduce these attention function,1
v0.5.5,todo future: maybe reintroduce these attention function,1
v0.5.5,todo future: maybe reintroduce these attention function,1
v0.5.5,TODO: double check,1
v0.5.5,"todo: enumerate for debugging, remove after testing",1
v0.5.5,TODO(shreya): Combine with ResNetBlock by adding a flag.,1
v0.5.5,TODO(shreya): Implement sparse embedding lookup.,1
v0.5.5,# TODO(shreya): Check if this is equivalent,1
v0.5.5,# TODO(shreya): Check if supported in torch,1
v0.5.5,todo: review for generality,1
v0.5.5,TODO: Simplify this.,1
v0.5.5,TODO: Potentially abstract this feature-specific attribute overwrite to a consolidated design.,1
v0.5.5,Dummy implementation.,1
v0.5.5,"TODO(Justin): Add a confusion matrix, see",1
v0.5.5,TODO: add a mechanism for letting the user decide to save it,1
v0.5.5,todo: re-evaluate need for separate handling of `attention` reducer,1
v0.5.5,TODO(shreya): Metrics should ideally just move to the correct device,1
v0.5.5,and not require the user to do this. This is a temporary fix. See,1
v0.5.5,TODO(Justin): Clean this up.,1
v0.5.5,TODO(shreya): Confirm if it's ok to do per channel normalization,1
v0.5.5,TODO(shreya): Also confirm if this is being used anywhere,1
v0.5.5,TODO(shreya): Confirm if ok to use imagenet means and std devs,1
v0.5.5,Nested conditional is a workaround to short-circuit boolean evaluation.,1
v0.5.5,TODO: alternatively use get_average_image() for unreachable images,1
v0.5.5,todo future add multiprocessing/multithreading,1
v0.5.5,TODO(1891): Remove backward compatibility hack once all models have been retrained with Ludwig after,1
v0.5.5,todo: refactor to reuse SequenceOutputFeature.postprocess_predictions,1
v0.5.5,TODO: add a mechanism for letting the user decide to save it,1
v0.5.5,TODO ray: this is needed because ray 1.7 doesn't support Dask to RayDataset,1
v0.5.5,todo figure out if additional parameters are needed,1
v0.5.5,TODO dask: find a way to better parallelize this operation,1
v0.5.5,TODO dask: this needs to work with DataFrames,1
v0.5.5,TODO(travis): implement saving split for Ray,1
v0.5.5,TODO ray 1.8: convert to Tensors before shuffle,1
v0.5.5,Workaround for: https://github.com/ray-project/ray/issues/25643,1
v0.5.5,TODO(travis): remove after 1.13.1,1
v0.5.5,"TODO(travis): find way to avoid calling this, as it's expensive",1
v0.5.5,TODO: consider removing this. doesn't work currently and read performance seems generally,1
v0.5.5,TODO(travis): figure out why Ray is converting these into object types by default,1
v0.5.5,"TODO(travis): could try hashing the in-memory dataset, but this is tricky for Dask",1
v0.5.5,todo future: reintroduce the bucketed batcher,1
v0.5.5,TODO ray: implement dynamic batch size,1
v0.5.5,TODO(#1673): Need some more logic here for validating against output features,1
v0.5.5,TODO(#1673): Add conditional logic for fields like this one:,1
v0.5.5,todo v0.4: currently not clear way to set model graph,1
v0.5.5,TODO(travis): figure out a good way to support this. The problem with,1
v0.5.5,TODO: need to also include a filename for this figure,1
v0.5.5,Dummy implementation.,1
v0.5.5,TODO: Use a configurable ratio for how often to use teacher forcing during training.,1
v0.5.5,TODO: Use a configurable ratio for how often to use teacher forcing during training.,1
v0.5.5,Dummy implementation.,1
v0.5.5,"ORRIBLE HACK, IT'S THE ONLY WAY TO REMOVE PADDING",1
v0.5.5,END OF HORRIBLE HACK,1
v0.5.5,"TODO: more research needed on how to handle RayTune ""sample_from"" search space",1
v0.5.5,todo future: maybe modify this with TF2 mask mechanics,1
v0.5.5,Workaround for: https://issues.apache.org/jira/browse/ARROW-5645,1
v0.5.5,TODO(shreya): Confirm types of args,1
v0.5.5,TODO: with change to misc_utils.set_random_seed() this may be redundant,1
v0.5.5,TODO(justin): Remove dummy implementation. Make input_shape and output_shape functions.,1
v0.5.5,TODO(Justin): Check that the semantics of input_size align with what the combiner's output shape returns for,1
v0.5.5,TODO (ASN): Circle back on how we want to set default placeholder value,1
v0.5.5,TODO(Justin): Move to config validation when that's ready.,1
v0.5.5,TODO (ASN) : Circle back on how we want to set default placeholder value,1
v0.5.5,TODO: refactor this into an interface,1
v0.5.5,workaround type limitations of the underlying frameworks,1
v0.5.5,TODO: deprecated v0.5,1
v0.5.5,TODO: use placement groups or otherwise spread across nodes,1
v0.5.5,TODO travis: replace backend here once ray 1.8 released,1
v0.5.5,"TODO ray: find an informed way to set the parallelism, in practice",1
v0.5.5,TODO: When this is implemented we also need to update the,1
v0.5.5,"TODO ray: make this more configurable by allowing YAML overrides of timeout_s, etc.",1
v0.5.5,TODO(shreya): self.trainer_kwargs should have the correct resources; debug.,1
v0.5.5,Deep copy to workaround https://github.com/ray-project/ray/issues/24139,1
v0.5.5,TODO: deprecated 0.5,1
v0.5.5,"TODO(tgaddair): temporary workaround for Horovod's worker discovery logic,",1
v0.5.5,TODO(matt): Implement placement group strategies in BackendExecutor.,1
v0.5.5,Temporary workaround to prevent tests from automatically using the Ray backend. Taken from,1
v0.5.5,TODO(shreya): DataFrame created twice: here + CSVMixin. Figure out,1
v0.5.4,is there a better way to do this?,1
v0.5.4,todo the hidden output is actually a tensor. May need modification,1
v0.5.4,todo figure out the output size for parallel 1d conv,1
v0.5.4,todo: remove code,1
v0.5.4,todo: re-add 'attention' after further research in implication of torch,1
v0.5.4,TODO(#1333): Refactor this test once torch sequence generator work is complete.,1
v0.5.4,"TODO(Justin): Move these to test_sequence_generator unit tests, and reintroduce decoder attention, beam_width, and",1
v0.5.4,TODO(travis): https://github.com/ludwig-ai/ludwig/issues/1956,1
v0.5.4,TODO: Determine if we still need this if-then-else construct,1
v0.5.4,TODO(shreya): Uncomment when https://github.com/ludwig-ai/ludwig/issues/2039 is fixed.,1
v0.5.4,TODO ray: replace legacy mode when Ray Train supports placement groups,1
v0.5.4,TODO: remove this workaround when audio preprocessing is fixed.,1
v0.5.4,TODO: these are the only outputs we provide from Torchscript for now,1
v0.5.4,TODO all shadows built in name - come up with a more descriptive name,1
v0.5.4,TODO (ASN): add support for substitute_with_max parameter,1
v0.5.4,todo: support loading other model types based on config,1
v0.5.4,TODO: fix for Ray where workers may be of different skus,1
v0.5.4,TODO(travis): remove in favor of on_hyperopt_end for naming consistency,1
v0.5.4,TODO(shreya): Add type hints for missing args,1
v0.5.4,todo: revise docstring,1
v0.5.4,todo: assess how to specify padding for equivalent to 'same',1
v0.5.4,todo: determine how to pool_padding equivalent of 'same',1
v0.5.4,todo: fixup docstring,1
v0.5.4,todo: review docstring,1
v0.5.4,todo: fix up docstring,1
v0.5.4,todo: fix up docstring,1
v0.5.4,todo: update docstring as needed,1
v0.5.4,TODO(shreya): Make sure this is updated when FCStack is updated,1
v0.5.4,TODO(justin): Use official class properties.,1
v0.5.4,TODO(shreya): Confirm that this is it,1
v0.5.4,TODO(shreya): Confirm that this is it,1
v0.5.4,TODO(justin): This may need to be conditioned on which AutoModel gets chosen.,1
v0.5.4,"TODO (ASN): add other modalities (image, etc. )",1
v0.5.4,TODO (ASN): Decide how we want to proceed if at least one trial has,1
v0.5.4,TODO (ASN): add image heuristics,1
v0.5.4,"todo future: this may be redundant, check",1
v0.5.4,Workaround for including additional tensors from output of input encoders for,1
v0.5.4,TODO(Justin): Think about how to make this communication work for multi-sequence,1
v0.5.4,todo: when https://github.com/ludwig-ai/ludwig/issues/810 is closed,1
v0.5.4,todo future: maybe modify this with TF2 mask mechanics,1
v0.5.4,"todo: can we just use projector_size? # hidden_size,",1
v0.5.4,"todo future: this may be redundant, check",1
v0.5.4,"todo future: this may be redundant, check",1
v0.5.4,Workaround for including additional tensors from output of input encoders for,1
v0.5.4,TODO(Justin): Think about how to make this communication work for multi-sequence,1
v0.5.4,todo future: maybe reintroduce these attention function,1
v0.5.4,todo future: maybe reintroduce these attention function,1
v0.5.4,todo future: maybe reintroduce these attention function,1
v0.5.4,TODO(shreya): Double check difference in computation.,1
v0.5.4,TODO: double check,1
v0.5.4,"todo: enumerate for debugging, remove after testing",1
v0.5.4,TODO(shreya): Combine with ResNetBlock by adding a flag.,1
v0.5.4,TODO(shreya): Implement sparse embedding lookup.,1
v0.5.4,# TODO(shreya): Check if this is equivalent,1
v0.5.4,# TODO(shreya): Check if supported in torch,1
v0.5.4,todo: review for generality,1
v0.5.4,TODO: Simplify this.,1
v0.5.4,TODO: Potentially abstract this feature-specific attribute overwrite to a consolidated design.,1
v0.5.4,Dummy implementation.,1
v0.5.4,"TODO(Justin): Add a confusion matrix, see",1
v0.5.4,TODO: add a mechanism for letting the user decide to save it,1
v0.5.4,todo: re-evaluate need for separate handling of `attention` reducer,1
v0.5.4,TODO(shreya): Metrics should ideally just move to the correct device,1
v0.5.4,and not require the user to do this. This is a temporary fix. See,1
v0.5.4,TODO(Justin): Clean this up.,1
v0.5.4,TODO(shreya): Confirm if it's ok to do per channel normalization,1
v0.5.4,TODO(shreya): Also confirm if this is being used anywhere,1
v0.5.4,TODO(shreya): Confirm if ok to use imagenet means and std devs,1
v0.5.4,Nested conditional is a workaround to short-circuit boolean evaluation.,1
v0.5.4,TODO: alternatively use get_average_image() for unreachable images,1
v0.5.4,todo future add multiprocessing/multithreading,1
v0.5.4,TODO(1891): Remove backward compatibility hack once all models have been retrained with Ludwig after,1
v0.5.4,todo: refactor to reuse SequenceOutputFeature.postprocess_predictions,1
v0.5.4,TODO: add a mechanism for letting the user decide to save it,1
v0.5.4,TODO ray: this is needed because ray 1.7 doesn't support Dask to RayDataset,1
v0.5.4,todo figure out if additional parameters are needed,1
v0.5.4,TODO dask: find a way to better parallelize this operation,1
v0.5.4,TODO dask: this needs to work with DataFrames,1
v0.5.4,TODO(travis): implement saving split for Ray,1
v0.5.4,TODO ray 1.8: convert to Tensors before shuffle,1
v0.5.4,Workaround for: https://github.com/ray-project/ray/issues/25643,1
v0.5.4,TODO(travis): remove after 1.13.1,1
v0.5.4,"TODO(travis): find way to avoid calling this, as it's expensive",1
v0.5.4,TODO: consider removing this. doesn't work currently and read performance seems generally,1
v0.5.4,TODO(travis): figure out why Ray is converting these into object types by default,1
v0.5.4,"TODO(travis): could try hashing the in-memory dataset, but this is tricky for Dask",1
v0.5.4,todo future: reintroduce the bucketed batcher,1
v0.5.4,TODO ray: implement dynamic batch size,1
v0.5.4,TODO(#1673): Need some more logic here for validating against output features,1
v0.5.4,TODO(#1673): Add conditional logic for fields like this one:,1
v0.5.4,todo v0.4: currently not clear way to set model graph,1
v0.5.4,TODO(travis): figure out a good way to support this. The problem with,1
v0.5.4,TODO: need to also include a filename for this figure,1
v0.5.4,Dummy implementation.,1
v0.5.4,TODO: Use a configurable ratio for how often to use teacher forcing during training.,1
v0.5.4,TODO: Use a configurable ratio for how often to use teacher forcing during training.,1
v0.5.4,Dummy implementation.,1
v0.5.4,"ORRIBLE HACK, IT'S THE ONLY WAY TO REMOVE PADDING",1
v0.5.4,END OF HORRIBLE HACK,1
v0.5.4,"TODO: more research needed on how to handle RayTune ""sample_from"" search space",1
v0.5.4,todo future: maybe modify this with TF2 mask mechanics,1
v0.5.4,Workaround for: https://issues.apache.org/jira/browse/ARROW-5645,1
v0.5.4,TODO(shreya): Confirm types of args,1
v0.5.4,TODO: with change to misc_utils.set_random_seed() this may be redundant,1
v0.5.4,TODO(justin): Remove dummy implementation. Make input_shape and output_shape functions.,1
v0.5.4,TODO(Justin): Check that the semantics of input_size align with what the combiner's output shape returns for,1
v0.5.4,TODO (ASN): Circle back on how we want to set default placeholder value,1
v0.5.4,TODO(Justin): Move to config validation when that's ready.,1
v0.5.4,TODO (ASN) : Circle back on how we want to set default placeholder value,1
v0.5.4,TODO: refactor this into an interface,1
v0.5.4,workaround type limitations of the underlying frameworks,1
v0.5.4,TODO: deprecated v0.5,1
v0.5.4,TODO: use placement groups or otherwise spread across nodes,1
v0.5.4,TODO travis: replace backend here once ray 1.8 released,1
v0.5.4,"TODO ray: find an informed way to set the parallelism, in practice",1
v0.5.4,TODO: When this is implemented we also need to update the,1
v0.5.4,"TODO ray: make this more configurable by allowing YAML overrides of timeout_s, etc.",1
v0.5.4,TODO(shreya): self.trainer_kwargs should have the correct resources; debug.,1
v0.5.4,Deep copy to workaround https://github.com/ray-project/ray/issues/24139,1
v0.5.4,TODO: deprecated 0.5,1
v0.5.4,"TODO(tgaddair): temporary workaround for Horovod's worker discovery logic,",1
v0.5.4,TODO(matt): Implement placement group strategies in BackendExecutor.,1
v0.5.4,Temporary workaround to prevent tests from automatically using the Ray backend. Taken from,1
v0.5.4,TODO(shreya): DataFrame created twice: here + CSVMixin. Figure out,1
v0.5.3,is there a better way to do this?,1
v0.5.3,todo the hidden output is actually a tensor. May need modification,1
v0.5.3,todo figure out the output size for parallel 1d conv,1
v0.5.3,todo: remove code,1
v0.5.3,todo: re-add 'attention' after further research in implication of torch,1
v0.5.3,TODO(#1333): Refactor this test once torch sequence generator work is complete.,1
v0.5.3,"TODO(Justin): Move these to test_sequence_generator unit tests, and reintroduce decoder attention, beam_width, and",1
v0.5.3,TODO(travis): https://github.com/ludwig-ai/ludwig/issues/1956,1
v0.5.3,TODO: Determine if we still need this if-then-else construct,1
v0.5.3,TODO(shreya): Uncomment when https://github.com/ludwig-ai/ludwig/issues/2039 is fixed.,1
v0.5.3,TODO ray: replace legacy mode when Ray Train supports placement groups,1
v0.5.3,TODO: remove this workaround when audio preprocessing is fixed.,1
v0.5.3,TODO: these are the only outputs we provide from Torchscript for now,1
v0.5.3,TODO all shadows built in name - come up with a more descriptive name,1
v0.5.3,TODO (ASN): add support for substitute_with_max parameter,1
v0.5.3,todo: support loading other model types based on config,1
v0.5.3,TODO: fix for Ray where workers may be of different skus,1
v0.5.3,TODO(travis): remove in favor of on_hyperopt_end for naming consistency,1
v0.5.3,TODO(shreya): Add type hints for missing args,1
v0.5.3,todo: revise docstring,1
v0.5.3,todo: assess how to specify padding for equivalent to 'same',1
v0.5.3,todo: determine how to pool_padding equivalent of 'same',1
v0.5.3,todo: fixup docstring,1
v0.5.3,todo: review docstring,1
v0.5.3,todo: fix up docstring,1
v0.5.3,todo: fix up docstring,1
v0.5.3,todo: update docstring as needed,1
v0.5.3,TODO(shreya): Make sure this is updated when FCStack is updated,1
v0.5.3,TODO(justin): Use official class properties.,1
v0.5.3,TODO(shreya): Confirm that this is it,1
v0.5.3,TODO(shreya): Confirm that this is it,1
v0.5.3,TODO(justin): This may need to be conditioned on which AutoModel gets chosen.,1
v0.5.3,"TODO (ASN): add other modalities (image, etc. )",1
v0.5.3,TODO (ASN): Decide how we want to proceed if at least one trial has,1
v0.5.3,TODO (ASN): add image heuristics,1
v0.5.3,"todo future: this may be redundant, check",1
v0.5.3,Workaround for including additional tensors from output of input encoders for,1
v0.5.3,TODO(Justin): Think about how to make this communication work for multi-sequence,1
v0.5.3,todo: when https://github.com/ludwig-ai/ludwig/issues/810 is closed,1
v0.5.3,todo future: maybe modify this with TF2 mask mechanics,1
v0.5.3,"todo: can we just use projector_size? # hidden_size,",1
v0.5.3,"todo future: this may be redundant, check",1
v0.5.3,"todo future: this may be redundant, check",1
v0.5.3,Workaround for including additional tensors from output of input encoders for,1
v0.5.3,TODO(Justin): Think about how to make this communication work for multi-sequence,1
v0.5.3,todo future: maybe reintroduce these attention function,1
v0.5.3,todo future: maybe reintroduce these attention function,1
v0.5.3,todo future: maybe reintroduce these attention function,1
v0.5.3,TODO(shreya): Double check difference in computation.,1
v0.5.3,TODO: double check,1
v0.5.3,"todo: enumerate for debugging, remove after testing",1
v0.5.3,TODO(shreya): Combine with ResNetBlock by adding a flag.,1
v0.5.3,TODO(shreya): Implement sparse embedding lookup.,1
v0.5.3,# TODO(shreya): Check if this is equivalent,1
v0.5.3,# TODO(shreya): Check if supported in torch,1
v0.5.3,todo: review for generality,1
v0.5.3,TODO: Simplify this.,1
v0.5.3,TODO: Potentially abstract this feature-specific attribute overwrite to a consolidated design.,1
v0.5.3,Dummy implementation.,1
v0.5.3,"TODO(Justin): Add a confusion matrix, see",1
v0.5.3,TODO: add a mechanism for letting the user decide to save it,1
v0.5.3,todo: re-evaluate need for separate handling of `attention` reducer,1
v0.5.3,TODO(shreya): Metrics should ideally just move to the correct device,1
v0.5.3,and not require the user to do this. This is a temporary fix. See,1
v0.5.3,TODO(Justin): Clean this up.,1
v0.5.3,TODO(shreya): Confirm if it's ok to do per channel normalization,1
v0.5.3,TODO(shreya): Also confirm if this is being used anywhere,1
v0.5.3,TODO(shreya): Confirm if ok to use imagenet means and std devs,1
v0.5.3,Nested conditional is a workaround to short-circuit boolean evaluation.,1
v0.5.3,TODO: alternatively use get_average_image() for unreachable images,1
v0.5.3,todo future add multiprocessing/multithreading,1
v0.5.3,TODO(1891): Remove backward compatibility hack once all models have been retrained with Ludwig after,1
v0.5.3,todo: refactor to reuse SequenceOutputFeature.postprocess_predictions,1
v0.5.3,TODO: add a mechanism for letting the user decide to save it,1
v0.5.3,TODO ray: this is needed because ray 1.7 doesn't support Dask to RayDataset,1
v0.5.3,todo figure out if additional parameters are needed,1
v0.5.3,TODO dask: find a way to better parallelize this operation,1
v0.5.3,TODO dask: this needs to work with DataFrames,1
v0.5.3,TODO(travis): implement saving split for Ray,1
v0.5.3,TODO ray 1.8: convert to Tensors before shuffle,1
v0.5.3,Workaround for: https://github.com/ray-project/ray/issues/25643,1
v0.5.3,TODO(travis): remove after 1.13.1,1
v0.5.3,"TODO(travis): find way to avoid calling this, as it's expensive",1
v0.5.3,TODO: consider removing this. doesn't work currently and read performance seems generally,1
v0.5.3,TODO(travis): figure out why Ray is converting these into object types by default,1
v0.5.3,"TODO(travis): could try hashing the in-memory dataset, but this is tricky for Dask",1
v0.5.3,todo future: reintroduce the bucketed batcher,1
v0.5.3,TODO ray: implement dynamic batch size,1
v0.5.3,TODO(#1673): Need some more logic here for validating against output features,1
v0.5.3,TODO(#1673): Add conditional logic for fields like this one:,1
v0.5.3,todo v0.4: currently not clear way to set model graph,1
v0.5.3,TODO(travis): figure out a good way to support this. The problem with,1
v0.5.3,TODO: need to also include a filename for this figure,1
v0.5.3,Dummy implementation.,1
v0.5.3,TODO: Use a configurable ratio for how often to use teacher forcing during training.,1
v0.5.3,TODO: Use a configurable ratio for how often to use teacher forcing during training.,1
v0.5.3,Dummy implementation.,1
v0.5.3,"ORRIBLE HACK, IT'S THE ONLY WAY TO REMOVE PADDING",1
v0.5.3,END OF HORRIBLE HACK,1
v0.5.3,"TODO: more research needed on how to handle RayTune ""sample_from"" search space",1
v0.5.3,todo future: maybe modify this with TF2 mask mechanics,1
v0.5.3,Workaround for: https://issues.apache.org/jira/browse/ARROW-5645,1
v0.5.3,TODO(shreya): Confirm types of args,1
v0.5.3,TODO: with change to misc_utils.set_random_seed() this may be redundant,1
v0.5.3,TODO(justin): Remove dummy implementation. Make input_shape and output_shape functions.,1
v0.5.3,TODO(Justin): Check that the semantics of input_size align with what the combiner's output shape returns for,1
v0.5.3,TODO (ASN): Circle back on how we want to set default placeholder value,1
v0.5.3,TODO(Justin): Move to config validation when that's ready.,1
v0.5.3,TODO (ASN) : Circle back on how we want to set default placeholder value,1
v0.5.3,TODO: refactor this into an interface,1
v0.5.3,workaround type limitations of the underlying frameworks,1
v0.5.3,TODO: deprecated v0.5,1
v0.5.3,TODO: use placement groups or otherwise spread across nodes,1
v0.5.3,TODO travis: replace backend here once ray 1.8 released,1
v0.5.3,"TODO ray: find an informed way to set the parallelism, in practice",1
v0.5.3,TODO: When this is implemented we also need to update the,1
v0.5.3,"TODO ray: make this more configurable by allowing YAML overrides of timeout_s, etc.",1
v0.5.3,TODO(shreya): self.trainer_kwargs should have the correct resources; debug.,1
v0.5.3,Deep copy to workaround https://github.com/ray-project/ray/issues/24139,1
v0.5.3,TODO: deprecated 0.5,1
v0.5.3,"TODO(tgaddair): temporary workaround for Horovod's worker discovery logic,",1
v0.5.3,TODO(matt): Implement placement group strategies in BackendExecutor.,1
v0.5.3,Temporary workaround to prevent tests from automatically using the Ray backend. Taken from,1
v0.5.3,TODO(shreya): DataFrame created twice: here + CSVMixin. Figure out,1
v0.5.2,is there a better way to do this?,1
v0.5.2,todo the hidden output is actually a tensor. May need modification,1
v0.5.2,todo figure out the output size for parallel 1d conv,1
v0.5.2,todo: remove code,1
v0.5.2,todo: re-add 'attention' after further research in implication of torch,1
v0.5.2,TODO(#1333): Refactor this test once torch sequence generator work is complete.,1
v0.5.2,"TODO(Justin): Move these to test_sequence_generator unit tests, and reintroduce decoder attention, beam_width, and",1
v0.5.2,TODO(travis): https://github.com/ludwig-ai/ludwig/issues/1956,1
v0.5.2,TODO: Determine if we still need this if-then-else construct,1
v0.5.2,TODO(shreya): Uncomment when https://github.com/ludwig-ai/ludwig/issues/2039 is fixed.,1
v0.5.2,TODO ray: replace legacy mode when Ray Train supports placement groups,1
v0.5.2,TODO: remove this workaround when audio preprocessing is fixed.,1
v0.5.2,TODO: these are the only outputs we provide from Torchscript for now,1
v0.5.2,TODO all shadows built in name - come up with a more descriptive name,1
v0.5.2,TODO (ASN): add support for substitute_with_max parameter,1
v0.5.2,todo: support loading other model types based on config,1
v0.5.2,TODO: fix for Ray where workers may be of different skus,1
v0.5.2,TODO(travis): remove in favor of on_hyperopt_end for naming consistency,1
v0.5.2,TODO(shreya): Add type hints for missing args,1
v0.5.2,todo: revise docstring,1
v0.5.2,todo: assess how to specify padding for equivalent to 'same',1
v0.5.2,todo: determine how to pool_padding equivalent of 'same',1
v0.5.2,todo: fixup docstring,1
v0.5.2,todo: review docstring,1
v0.5.2,todo: fix up docstring,1
v0.5.2,todo: fix up docstring,1
v0.5.2,todo: update docstring as needed,1
v0.5.2,TODO(shreya): Make sure this is updated when FCStack is updated,1
v0.5.2,TODO(justin): Use official class properties.,1
v0.5.2,TODO(shreya): Confirm that this is it,1
v0.5.2,TODO(shreya): Confirm that this is it,1
v0.5.2,TODO(justin): This may need to be conditioned on which AutoModel gets chosen.,1
v0.5.2,"TODO (ASN): add other modalities (image, etc. )",1
v0.5.2,TODO (ASN): Decide how we want to proceed if at least one trial has,1
v0.5.2,TODO (ASN): add image heuristics,1
v0.5.2,"todo future: this may be redundant, check",1
v0.5.2,Workaround for including additional tensors from output of input encoders for,1
v0.5.2,TODO(Justin): Think about how to make this communication work for multi-sequence,1
v0.5.2,todo: when https://github.com/ludwig-ai/ludwig/issues/810 is closed,1
v0.5.2,todo future: maybe modify this with TF2 mask mechanics,1
v0.5.2,"todo: can we just use projector_size? # hidden_size,",1
v0.5.2,"todo future: this may be redundant, check",1
v0.5.2,"todo future: this may be redundant, check",1
v0.5.2,Workaround for including additional tensors from output of input encoders for,1
v0.5.2,TODO(Justin): Think about how to make this communication work for multi-sequence,1
v0.5.2,todo future: maybe reintroduce these attention function,1
v0.5.2,todo future: maybe reintroduce these attention function,1
v0.5.2,todo future: maybe reintroduce these attention function,1
v0.5.2,TODO(shreya): Double check difference in computation.,1
v0.5.2,TODO: double check,1
v0.5.2,"todo: enumerate for debugging, remove after testing",1
v0.5.2,TODO(shreya): Combine with ResNetBlock by adding a flag.,1
v0.5.2,TODO(shreya): Implement sparse embedding lookup.,1
v0.5.2,# TODO(shreya): Check if this is equivalent,1
v0.5.2,# TODO(shreya): Check if supported in torch,1
v0.5.2,todo: review for generality,1
v0.5.2,TODO: Simplify this.,1
v0.5.2,todo maybe move code from add_feature_data here,1
v0.5.2,todo: add cast to int64,1
v0.5.2,TODO: Potentially abstract this feature-specific attribute overwrite to a consolidated design.,1
v0.5.2,Dummy implementation.,1
v0.5.2,"TODO(Justin): Add a confusion matrix, see",1
v0.5.2,todo: add a mechanism for letting the user decide to save it,1
v0.5.2,todo: re-evaluate need for separate handling of `attention` reducer,1
v0.5.2,TODO(shreya): Metrics should ideally just move to the correct device,1
v0.5.2,and not require the user to do this. This is a temporary fix. See,1
v0.5.2,TODO(Justin): Clean this up.,1
v0.5.2,TODO(shreya): Confirm if it's ok to do per channel normalization,1
v0.5.2,TODO(shreya): Also confirm if this is being used anywhere,1
v0.5.2,TODO(shreya): Confirm if ok to use imagenet means and std devs,1
v0.5.2,Nested conditional is a workaround to short-circuit boolean evaluation.,1
v0.5.2,TODO: alternatively use get_average_image() for unreachable images,1
v0.5.2,todo future add multiprocessing/multithreading,1
v0.5.2,TODO(1891): Remove backward compatibility hack once all models have been retrained with Ludwig after,1
v0.5.2,todo: refactor to reuse SequenceOutputFeature.postprocess_predictions,1
v0.5.2,todo: add a mechanism for letting the user decide to save it,1
v0.5.2,TODO ray: this is needed because ray 1.7 doesn't support Dask to RayDataset,1
v0.5.2,todo figure out if additional parameters are needed,1
v0.5.2,TODO dask: find a way to better parallelize this operation,1
v0.5.2,TODO dask: this needs to work with DataFrames,1
v0.5.2,TODO(travis): implement saving split for Ray,1
v0.5.2,TODO ray 1.8: convert to Tensors before shuffle,1
v0.5.2,"TODO(travis): find way to avoid calling this, as it's expensive",1
v0.5.2,TODO: consider removing this. doesn't work currently and read performance seems generally,1
v0.5.2,TODO(travis): figure out why Ray is converting these into object types by default,1
v0.5.2,"TODO(travis): could try hashing the in-memory dataset, but this is tricky for Dask",1
v0.5.2,todo future: reintroduce the bucketed batcher,1
v0.5.2,TODO ray: implement dynamic batch size,1
v0.5.2,TODO(#1673): Need some more logic here for validating against output features,1
v0.5.2,TODO(#1673): Add conditional logic for fields like this one:,1
v0.5.2,todo v0.4: currently not clear way to set model graph,1
v0.5.2,TODO(travis): figure out a good way to support this. The problem with,1
v0.5.2,TODO: need to also include a filename for this figure,1
v0.5.2,TODO(travis): remove after Ray 1.12.1,1
v0.5.2,Dummy implementation.,1
v0.5.2,TODO: Use a configurable ratio for how often to use teacher forcing during training.,1
v0.5.2,TODO: Use a configurable ratio for how often to use teacher forcing during training.,1
v0.5.2,Dummy implementation.,1
v0.5.2,"ORRIBLE HACK, IT'S THE ONLY WAY TO REMOVE PADDING",1
v0.5.2,END OF HORRIBLE HACK,1
v0.5.2,"TODO: more research needed on how to handle RayTune ""sample_from"" search space",1
v0.5.2,todo future: maybe modify this with TF2 mask mechanics,1
v0.5.2,Workaround for: https://issues.apache.org/jira/browse/ARROW-5645,1
v0.5.2,TODO(shreya): Confirm types of args,1
v0.5.2,TODO: with change to misc_utils.set_random_seed() this may be redundant,1
v0.5.2,TODO(justin): Remove dummy implementation. Make input_shape and output_shape functions.,1
v0.5.2,TODO(Justin): Check that the semantics of input_size align with what the combiner's output shape returns for,1
v0.5.2,TODO (ASN): Circle back on how we want to set default placeholder value,1
v0.5.2,TODO(Justin): Move to config validation when that's ready.,1
v0.5.2,TODO (ASN) : Circle back on how we want to set default placeholder value,1
v0.5.2,TODO: refactor this into an interface,1
v0.5.2,workaround type limitations of the underlying frameworks,1
v0.5.2,TODO: deprecated v0.5,1
v0.5.2,TODO: use placement groups or otherwise spread across nodes,1
v0.5.2,TODO travis: replace backend here once ray 1.8 released,1
v0.5.2,"TODO ray: find an informed way to set the parallelism, in practice",1
v0.5.2,TODO: When this is implemented we also need to update the,1
v0.5.2,"TODO ray: make this more configurable by allowing YAML overrides of timeout_s, etc.",1
v0.5.2,TODO(shreya): self.trainer_kwargs should have the correct resources; debug.,1
v0.5.2,Deep copy to workaround https://github.com/ray-project/ray/issues/24139,1
v0.5.2,TODO: deprecated 0.5,1
v0.5.2,"TODO(tgaddair): temporary workaround for Horovod's worker discovery logic,",1
v0.5.2,TODO(matt): Implement placement group strategies in BackendExecutor.,1
v0.5.2,Temporary workaround to prevent tests from automatically using the Ray backend. Taken from,1
v0.5.2,TODO(shreya): DataFrame created twice: here + CSVMixin. Figure out,1
v0.5.1,is there a better way to do this?,1
v0.5.1,todo the hidden output is actually a tensor. May need modification,1
v0.5.1,todo figure out the output size for parallel 1d conv,1
v0.5.1,todo: remove code,1
v0.5.1,todo: re-add 'attention' after further research in implication of torch,1
v0.5.1,TODO(#1333): Refactor this test once torch sequence generator work is complete.,1
v0.5.1,"TODO(Justin): Move these to test_sequence_generator unit tests, and reintroduce decoder attention, beam_width, and",1
v0.5.1,TODO(travis): https://github.com/ludwig-ai/ludwig/issues/1956,1
v0.5.1,TODO: Determine if we still need this if-then-else construct,1
v0.5.1,TODO ray: replace legacy mode when Ray Train supports placement groups,1
v0.5.1,TODO: these are the only outputs we provide from Torchscript for now,1
v0.5.1,TODO all shadows built in name - come up with a more descriptive name,1
v0.5.1,TODO (ASN): add support for substitute_with_max parameter,1
v0.5.1,TODO ray: support calculating stats on partitioned datasets,1
v0.5.1,todo: support loading other model types based on config,1
v0.5.1,TODO: fix for Ray where workers may be of different skus,1
v0.5.1,TODO(travis): remove in favor of on_hyperopt_end for naming consistency,1
v0.5.1,TODO(shreya): Add type hints for missing args,1
v0.5.1,todo: revise docstring,1
v0.5.1,todo: assess how to specify padding for equivalent to 'same',1
v0.5.1,todo: determine how to pool_padding equivalent of 'same',1
v0.5.1,todo: fixup docstring,1
v0.5.1,todo: review docstring,1
v0.5.1,todo: fix up docstring,1
v0.5.1,todo: fix up docstring,1
v0.5.1,todo: update docstring as needed,1
v0.5.1,TODO(shreya): Make sure this is updated when FCStack is updated,1
v0.5.1,TODO(justin): Use official class properties.,1
v0.5.1,TODO(shreya): Confirm that this is it,1
v0.5.1,TODO(shreya): Confirm that this is it,1
v0.5.1,TODO(justin): This may need to be conditioned on which AutoModel gets chosen.,1
v0.5.1,"TODO (ASN): add other modalities (image, etc. )",1
v0.5.1,TODO (ASN): Decide how we want to proceed if at least one trial has,1
v0.5.1,TODO (ASN): add image heuristics,1
v0.5.1,"todo future: this may be redundant, check",1
v0.5.1,Workaround for including additional tensors from output of input encoders for,1
v0.5.1,TODO(Justin): Think about how to make this communication work for multi-sequence,1
v0.5.1,todo: when https://github.com/ludwig-ai/ludwig/issues/810 is closed,1
v0.5.1,todo future: maybe modify this with TF2 mask mechanics,1
v0.5.1,"todo: can we just use projector_size? # hidden_size,",1
v0.5.1,"todo future: this may be redundant, check",1
v0.5.1,"todo future: this may be redundant, check",1
v0.5.1,Workaround for including additional tensors from output of input encoders for,1
v0.5.1,TODO(Justin): Think about how to make this communication work for multi-sequence,1
v0.5.1,todo future: maybe reintroduce these attention function,1
v0.5.1,todo future: maybe reintroduce these attention function,1
v0.5.1,todo future: maybe reintroduce these attention function,1
v0.5.1,TODO(shreya): Double check difference in computation.,1
v0.5.1,TODO: double check,1
v0.5.1,todo(jmt): confirm correct interpretation of LayerNorm parameters,1
v0.5.1,todo: determine how to handle layer.name,1
v0.5.1,"todo: enumerate for debugging, remove after testing",1
v0.5.1,TODO(shreya): Combine with ResNetBlock by adding a flag.,1
v0.5.1,TODO(shreya): Implement sparse embedding lookup.,1
v0.5.1,# TODO(shreya): Check if this is equivalent,1
v0.5.1,# TODO(shreya): Check if supported in torch,1
v0.5.1,todo: review for generality,1
v0.5.1,TODO: Simplify this.,1
v0.5.1,todo maybe move code from add_feature_data here,1
v0.5.1,todo: add cast to int64,1
v0.5.1,TODO: Potentially abstract this feature-specific attribute overwrite to a consolidated design.,1
v0.5.1,Dummy implementation.,1
v0.5.1,"TODO(Justin): Add a confusion matrix, see",1
v0.5.1,todo: add a mechanism for letting the user decide to save it,1
v0.5.1,todo: re-evaluate need for separate handling of `attention` reducer,1
v0.5.1,TODO(shreya): Metrics should ideally just move to the correct device,1
v0.5.1,and not require the user to do this. This is a temporary fix. See,1
v0.5.1,TODO(Justin): Clean this up.,1
v0.5.1,TODO(shreya): Confirm if it's ok to do per channel normalization,1
v0.5.1,TODO(shreya): Also confirm if this is being used anywhere,1
v0.5.1,TODO(shreya): Confirm if ok to use imagenet means and std devs,1
v0.5.1,TODO: alternatively use get_average_image() for unreachable images,1
v0.5.1,todo future add multiprocessing/multithreading,1
v0.5.1,TODO(1891): Remove backward compatibility hack once all models have been retrained with Ludwig after,1
v0.5.1,todo: refactor to reuse SequenceOutputFeature.postprocess_predictions,1
v0.5.1,todo: add a mechanism for letting the user decide to save it,1
v0.5.1,TODO ray: this is needed because ray 1.7 doesn't support Dask to RayDataset,1
v0.5.1,todo figure out if additional parameters are needed,1
v0.5.1,TODO dask: find a way to better parallelize this operation,1
v0.5.1,TODO dask: this needs to work with DataFrames,1
v0.5.1,TODO(travis): implement saving split for Ray,1
v0.5.1,TODO ray 1.8: convert to Tensors before shuffle,1
v0.5.1,"TODO(travis): find way to avoid calling this, as it's expensive",1
v0.5.1,TODO: consider removing this. doesn't work currently and read performance seems generally,1
v0.5.1,TODO(travis): figure out why Ray is converting these into object types by default,1
v0.5.1,"TODO(travis): could try hashing the in-memory dataset, but this is tricky for Dask",1
v0.5.1,todo future: reintroduce the bucketed batcher,1
v0.5.1,TODO ray: implement dynamic batch size,1
v0.5.1,TODO(#1673): Need some more logic here for validating against output features,1
v0.5.1,TODO(#1673): Add conditional logic for fields like this one:,1
v0.5.1,todo v0.4: currently not clear way to set model graph,1
v0.5.1,TODO(travis): figure out a good way to support this. The problem with,1
v0.5.1,TODO: need to also include a filename for this figure,1
v0.5.1,TODO(travis): remove after Ray 1.12.1,1
v0.5.1,Dummy implementation.,1
v0.5.1,TODO: Use a configurable ratio for how often to use teacher forcing during training.,1
v0.5.1,TODO: Use a configurable ratio for how often to use teacher forcing during training.,1
v0.5.1,Dummy implementation.,1
v0.5.1,"ORRIBLE HACK, IT'S THE ONLY WAY TO REMOVE PADDING",1
v0.5.1,END OF HORRIBLE HACK,1
v0.5.1,"TODO: more research needed on how to handle RayTune ""sample_from"" search space",1
v0.5.1,todo future: maybe modify this with TF2 mask mechanics,1
v0.5.1,Workaround for: https://issues.apache.org/jira/browse/ARROW-5645,1
v0.5.1,TODO(shreya): Confirm types of args,1
v0.5.1,TODO: with change to misc_utils.set_random_seed() this may be redundant,1
v0.5.1,TODO(justin): Remove dummy implementation. Make input_shape and output_shape functions.,1
v0.5.1,TODO(Justin): Check that the semantics of input_size align with what the combiner's output shape returns for,1
v0.5.1,TODO (ASN): Circle back on how we want to set default placeholder value,1
v0.5.1,TODO(Justin): Move to config validation when that's ready.,1
v0.5.1,TODO (ASN) : Circle back on how we want to set default placeholder value,1
v0.5.1,TODO: refactor this into an interface,1
v0.5.1,workaround type limitations of the underlying frameworks,1
v0.5.1,TODO: split to separate module?,1
v0.5.1,TODO: deprecated v0.5,1
v0.5.1,TODO: use placement groups or otherwise spread across nodes,1
v0.5.1,TODO travis: replace backend here once ray 1.8 released,1
v0.5.1,"TODO ray: find an informed way to set the parallelism, in practice",1
v0.5.1,"TODO ray: make this more configurable by allowing YAML overrides of timeout_s, etc.",1
v0.5.1,Deep copy to workaround https://github.com/ray-project/ray/issues/24139,1
v0.5.1,TODO: deprecated 0.5,1
v0.5.1,"TODO(tgaddair): temporary workaround for Horovod's worker discovery logic,",1
v0.5.1,TODO(matt): Implement placement group strategies in BackendExecutor.,1
v0.5.1,Temporary workaround to prevent tests from automatically using the Ray backend. Taken from,1
v0.5.1,TODO(shreya): DataFrame created twice: here + CSVMixin. Figure out,1
v0.5,is there a better way to do this?,1
v0.5,todo the hidden output is actually a tensor. May need modification,1
v0.5,todo figure out the output size for parallel 1d conv,1
v0.5,todo: remove code,1
v0.5,todo: re-add 'attention' after further research in implication of torch,1
v0.5,TODO(#1333): Refactor this test once torch sequence generator work is complete.,1
v0.5,"TODO(Justin): Move these to test_sequence_generator unit tests, and reintroduce decoder attention, beam_width, and",1
v0.5,TODO(travis): https://github.com/ludwig-ai/ludwig/issues/1956,1
v0.5,TODO ray: replace legacy mode when Ray Train supports placement groups,1
v0.5,TODO: these are the only outputs we provide from Torchscript for now,1
v0.5,TODO all shadows built in name - come up with a more descriptive name,1
v0.5,TODO (ASN): add support for substitute_with_max parameter,1
v0.5,TODO ray: support calculating stats on partitioned datasets,1
v0.5,todo: support loading other model types based on config,1
v0.5,TODO: fix for Ray where workers may be of different skus,1
v0.5,TODO(travis): remove in favor of on_hyperopt_end for naming consistency,1
v0.5,TODO(shreya): Add type hints for missing args,1
v0.5,todo: revise docstring,1
v0.5,todo: assess how to specify padding for equivalent to 'same',1
v0.5,todo: determine how to pool_padding equivalent of 'same',1
v0.5,todo: fixup docstring,1
v0.5,todo: review docstring,1
v0.5,todo: fix up docstring,1
v0.5,todo: fix up docstring,1
v0.5,todo: update docstring as needed,1
v0.5,TODO(shreya): Make sure this is updated when FCStack is updated,1
v0.5,TODO(justin): Use official class properties.,1
v0.5,TODO(shreya): Confirm that this is it,1
v0.5,TODO(shreya): Confirm that this is it,1
v0.5,TODO(justin): This may need to be conditioned on which AutoModel gets chosen.,1
v0.5,"TODO (ASN): add other modalities (image, etc. )",1
v0.5,TODO (ASN): Decide how we want to proceed if at least one trial has,1
v0.5,TODO (ASN): add image heuristics,1
v0.5,"todo future: this may be redundant, check",1
v0.5,Workaround for including additional tensors from output of input encoders for,1
v0.5,TODO(Justin): Think about how to make this communication work for multi-sequence,1
v0.5,todo: when https://github.com/ludwig-ai/ludwig/issues/810 is closed,1
v0.5,todo future: maybe modify this with TF2 mask mechanics,1
v0.5,TODO(#1673): Add conditional logic for fields like this one:,1
v0.5,TODO(ksbrar): Refactor transformers into using base class for common attrs?,1
v0.5,"todo: can we just use projector_size? # hidden_size,",1
v0.5,"todo future: this may be redundant, check",1
v0.5,"todo future: this may be redundant, check",1
v0.5,Workaround for including additional tensors from output of input encoders for,1
v0.5,TODO(Justin): Think about how to make this communication work for multi-sequence,1
v0.5,todo future: maybe reintroduce these attention function,1
v0.5,todo future: maybe reintroduce these attention function,1
v0.5,todo future: maybe reintroduce these attention function,1
v0.5,TODO(shreya): Double check difference in computation.,1
v0.5,TODO: double check,1
v0.5,todo(jmt): confirm correct interpretation of LayerNorm parameters,1
v0.5,todo: determine how to handle layer.name,1
v0.5,"todo: enumerate for debugging, remove after testing",1
v0.5,TODO(shreya): Combine with ResNetBlock by adding a flag.,1
v0.5,TODO(shreya): Implement sparse embedding lookup.,1
v0.5,# TODO(shreya): Check if this is equivalent,1
v0.5,# TODO(shreya): Check if supported in torch,1
v0.5,todo: review for generality,1
v0.5,TODO: Simplify this.,1
v0.5,todo maybe move code from add_feature_data here,1
v0.5,todo: add cast to int64,1
v0.5,TODO: Potentially abstract this feature-specific attribute overwrite to a consolidated design.,1
v0.5,Dummy implementation.,1
v0.5,"TODO(Justin): Add a confusion matrix, see",1
v0.5,todo: add a mechanism for letting the user decide to save it,1
v0.5,todo: re-evaluate need for separate handling of `attention` reducer,1
v0.5,TODO(shreya): Metrics should ideally just move to the correct device,1
v0.5,and not require the user to do this. This is a temporary fix. See,1
v0.5,TODO(Justin): Clean this up.,1
v0.5,TODO(shreya): Confirm if it's ok to do per channel normalization,1
v0.5,TODO(shreya): Also confirm if this is being used anywhere,1
v0.5,TODO(shreya): Confirm if ok to use imagenet means and std devs,1
v0.5,TODO: alternatively use get_average_image() for unreachable images,1
v0.5,todo future add multiprocessing/multithreading,1
v0.5,TODO(1891): Remove backward compatibility hack once all models have been retrained with Ludwig after,1
v0.5,todo: refactor to reuse SequenceOutputFeature.postprocess_predictions,1
v0.5,todo: add a mechanism for letting the user decide to save it,1
v0.5,TODO ray: this is needed because ray 1.7 doesn't support Dask to RayDataset,1
v0.5,todo figure out if additional parameters are needed,1
v0.5,TODO dask: find a way to better parallelize this operation,1
v0.5,TODO dask: this needs to work with DataFrames,1
v0.5,TODO(travis): implement saving split for Ray,1
v0.5,TODO: address if following results in fragmented DataFrame,1
v0.5,TODO ray 1.8: convert to Tensors before shuffle,1
v0.5,"TODO(travis): find way to avoid calling this, as it's expensive",1
v0.5,TODO: consider removing this. doesn't work currently and read performance seems generally,1
v0.5,TODO(travis): figure out why Ray is converting these into object types by default,1
v0.5,"TODO(travis): could try hashing the in-memory dataset, but this is tricky for Dask",1
v0.5,todo future: reintroduce the bucketed batcher,1
v0.5,TODO ray: implement dynamic batch size,1
v0.5,todo v0.4: currently not clear way to set model graph,1
v0.5,TODO(travis): figure out a good way to support this. The problem with,1
v0.5,TODO: need to also include a filename for this figure,1
v0.5,TODO(travis): remove after Ray 1.12.1,1
v0.5,Dummy implementation.,1
v0.5,TODO: Use a configurable ratio for how often to use teacher forcing during training.,1
v0.5,TODO: Use a configurable ratio for how often to use teacher forcing during training.,1
v0.5,Dummy implementation.,1
v0.5,TODO(ksbrar): Should the default choice here be null?,1
v0.5,TODO(#1783): Change to Draft7Validator to _LATEST_VERSION or Draft202012Validator when py3.6 deprecated:,1
v0.5,"ORRIBLE HACK, IT'S THE ONLY WAY TO REMOVE PADDING",1
v0.5,END OF HORRIBLE HACK,1
v0.5,todo future: maybe modify this with TF2 mask mechanics,1
v0.5,Workaround for: https://issues.apache.org/jira/browse/ARROW-5645,1
v0.5,TODO(shreya): Confirm types of args,1
v0.5,TODO: with change to misc_utils.set_random_seed() this may be redundant,1
v0.5,TODO(justin): Remove dummy implementation. Make input_shape and output_shape functions.,1
v0.5,TODO(Justin): Check that the semantics of input_size align with what the combiner's output shape returns for,1
v0.5,TODO(#1673): Need some more logic here for validating against output features,1
v0.5,TODO (ASN): Circle back on how we want to set default placeholder value,1
v0.5,TODO(Justin): Move to config validation when that's ready.,1
v0.5,TODO (ASN) : Circle back on how we want to set default placeholder value,1
v0.5,TODO: refactor this into an interface,1
v0.5,workaround type limitations of the underlying frameworks,1
v0.5,TODO: deprecated v0.5,1
v0.5,TODO: use placement groups or otherwise spread across nodes,1
v0.5,TODO travis: replace backend here once ray 1.8 released,1
v0.5,"TODO ray: find an informed way to set the parallelism, in practice",1
v0.5,"TODO ray: make this more configurable by allowing YAML overrides of timeout_s, etc.",1
v0.5,Deep copy to workaround https://github.com/ray-project/ray/issues/24139,1
v0.5,TODO: deprecated 0.5,1
v0.5,"TODO(tgaddair): temporary workaround for Horovod's worker discovery logic,",1
v0.5,TODO(matt): Implement placement group strategies in BackendExecutor.,1
v0.5,Temporary workaround to prevent tests from automatically using the Ray backend. Taken from,1
v0.5,TODO(shreya): DataFrame created twice: here + CSVMixin. Figure out,1
v0.5rc2,is there a better way to do this?,1
v0.5rc2,todo the hidden output is actually a tensor. May need modification,1
v0.5rc2,todo figure out the output size for parallel 1d conv,1
v0.5rc2,todo: add unit test for sequence output feature,1
v0.5rc2,todo: remove code,1
v0.5rc2,todo: re-add 'attention' after further research in implication of torch,1
v0.5rc2,TODO(#1333): Refactor this test once torch sequence generator work is complete.,1
v0.5rc2,"TODO(Justin): Move these to test_sequence_generator unit tests, and reintroduce decoder attention, beam_width, and",1
v0.5rc2,TODO ray: replace legacy mode when Ray Train supports placement groups,1
v0.5rc2,TODO: these are the only outputs we provide from Torchscript for now,1
v0.5rc2,TODO all shadows built in name - come up with a more descriptive name,1
v0.5rc2,TODO (ASN): add support for substitute_with_max parameter,1
v0.5rc2,TODO ray: support calculating stats on partitioned datasets,1
v0.5rc2,todo: support loading other model types based on config,1
v0.5rc2,TODO: fix for Ray where workers may be of different skus,1
v0.5rc2,TODO(travis): remove in favor of on_hyperopt_end for naming consistency,1
v0.5rc2,TODO(shreya): Add type hints for missing args,1
v0.5rc2,todo: revise docstring,1
v0.5rc2,todo: assess how to specify padding for equivalent to 'same',1
v0.5rc2,todo: determine how to pool_padding equivalent of 'same',1
v0.5rc2,todo: fixup docstring,1
v0.5rc2,todo: review docstring,1
v0.5rc2,todo: fix up docstring,1
v0.5rc2,todo: fix up docstring,1
v0.5rc2,todo: update docstring as needed,1
v0.5rc2,TODO(shreya): Make sure this is updated when FCStack is updated,1
v0.5rc2,TODO(justin): Use official class properties.,1
v0.5rc2,TODO(shreya): Confirm that this is it,1
v0.5rc2,TODO(shreya): Confirm that this is it,1
v0.5rc2,TODO(justin): This may need to be conditioned on which AutoModel gets chosen.,1
v0.5rc2,"TODO (ASN): add other modalities (image, etc. )",1
v0.5rc2,TODO (ASN): Decide how we want to proceed if at least one trial has,1
v0.5rc2,TODO (ASN): add more robust heuristics,1
v0.5rc2,TODO (ASN): add image heuristics,1
v0.5rc2,"todo future: this may be redundant, check",1
v0.5rc2,Workaround for including additional tensors from output of input encoders for,1
v0.5rc2,TODO(Justin): Think about how to make this communication work for multi-sequence,1
v0.5rc2,todo: when https://github.com/ludwig-ai/ludwig/issues/810 is closed,1
v0.5rc2,todo future: maybe modify this with TF2 mask mechanics,1
v0.5rc2,"todo: can we just use projector_size? # hidden_size,",1
v0.5rc2,"todo future: this may be redundant, check",1
v0.5rc2,todo future: maybe reintroduce these attention function,1
v0.5rc2,todo future: maybe reintroduce these attention function,1
v0.5rc2,todo future: maybe reintroduce these attention function,1
v0.5rc2,TODO(shreya): Double check difference in computation.,1
v0.5rc2,TODO: double check,1
v0.5rc2,todo(jmt): confirm correct interpretation of LayerNorm parameters,1
v0.5rc2,todo: determine how to handle layer.name,1
v0.5rc2,"todo: enumerate for debugging, remove after testing",1
v0.5rc2,TODO(shreya): Combine with ResNetBlock by adding a flag.,1
v0.5rc2,TODO(shreya): Implement sparse embedding lookup.,1
v0.5rc2,# TODO(shreya): Check if this is equivalent,1
v0.5rc2,# TODO(shreya): Check if supported in torch,1
v0.5rc2,todo: review for generality,1
v0.5rc2,TODO: Simplify this.,1
v0.5rc2,todo maybe move code from add_feature_data here,1
v0.5rc2,todo: add cast to int64,1
v0.5rc2,TODO: Potentially abstract this feature-specific attribute overwrite to a consolidated design.,1
v0.5rc2,Dummy implementation.,1
v0.5rc2,"TODO(Justin): Add a confusion matrix, see",1
v0.5rc2,todo: add a mechanism for letting the user decide to save it,1
v0.5rc2,"this is not super nice, but works both and DFs and lists",1
v0.5rc2,todo: re-evaluate need for separate handling of `attention` reducer,1
v0.5rc2,TODO(shreya): Metrics should ideally just move to the correct device,1
v0.5rc2,and not require the user to do this. This is a temporary fix. See,1
v0.5rc2,TODO(Justin): Clean this up.,1
v0.5rc2,TODO(shreya): Confirm if it's ok to do per channel normalization,1
v0.5rc2,TODO(shreya): Also confirm if this is being used anywhere,1
v0.5rc2,TODO(shreya): Confirm if ok to use imagenet means and std devs,1
v0.5rc2,TODO: alternatively use get_average_image() for unreachable images,1
v0.5rc2,todo future add multiprocessing/multithreading,1
v0.5rc2,todo: refactor to reuse SequenceOutputFeature.postprocess_predictions,1
v0.5rc2,todo: add a mechanism for letting the user decide to save it,1
v0.5rc2,TODO ray: this is needed because ray 1.7 doesn't support Dask to RayDataset,1
v0.5rc2,todo figure out if additional parameters are needed,1
v0.5rc2,TODO dask: find a way to better parallelize this operation,1
v0.5rc2,TODO dask: this needs to work with DataFrames,1
v0.5rc2,TODO(travis): implement saving split for Ray,1
v0.5rc2,TODO: address if following results in fragmented DataFrame,1
v0.5rc2,TODO ray 1.8: convert to Tensors before shuffle,1
v0.5rc2,"TODO(travis): find way to avoid calling this, as it's expensive",1
v0.5rc2,TODO: consider removing this. doesn't work currently and read performance seems generally,1
v0.5rc2,"TODO(travis): could try hashing the in-memory dataset, but this is tricky for Dask",1
v0.5rc2,todo future: reintroduce the bucketed batcher,1
v0.5rc2,TODO ray: implement dynamic batch size,1
v0.5rc2,todo v0.4: currently not clear way to set model graph,1
v0.5rc2,TODO: need to also include a filename for this figure,1
v0.5rc2,Dummy implementation.,1
v0.5rc2,TODO: Use a configurable ratio for how often to use teacher forcing during training.,1
v0.5rc2,TODO: Use a configurable ratio for how often to use teacher forcing during training.,1
v0.5rc2,Dummy implementation.,1
v0.5rc2,"ORRIBLE HACK, IT'S THE ONLY WAY TO REMOVE PADDING",1
v0.5rc2,END OF HORRIBLE HACK,1
v0.5rc2,todo future: maybe modify this with TF2 mask mechanics,1
v0.5rc2,Workaround for: https://issues.apache.org/jira/browse/ARROW-5645,1
v0.5rc2,TODO(shreya): Confirm types of args,1
v0.5rc2,TODO(justin): Remove dummy implementation. Make input_shape and output_shape functions.,1
v0.5rc2,TODO(Justin): Check that the semantics of input_size align with what the combiner's output shape returns for,1
v0.5rc2,TODO (ASN): Circle back on how we want to set default placeholder value,1
v0.5rc2,TODO (ASN) : Circle back on how we want to set default placeholder value,1
v0.5rc2,TODO: refactor this into an interface,1
v0.5rc2,workaround type limitations of the underlying frameworks,1
v0.5rc2,TODO: deprecated v0.5,1
v0.5rc2,TODO: use placement groups or otherwise spread across nodes,1
v0.5rc2,TODO travis: replace backend here once ray 1.8 released,1
v0.5rc2,"TODO ray: find an informed way to set the parallelism, in practice",1
v0.5rc2,"TODO ray: make this more configurable by allowing YAML overrides of timeout_s, etc.",1
v0.5rc2,TODO: deprecated 0.5,1
v0.5rc2,TODO(matt): Implement placement group strategies in BackendExecutor.,1
v0.5rc2,Temporary workaround to prevent tests from automatically using the Ray backend. Taken from,1
v0.5rc2,TODO(shreya): DataFrame created twice: here + CSVMixin. Figure out,1
v0.5rc1,is there a better way to do this?,1
v0.5rc1,todo the hidden output is actually a tensor. May need modification,1
v0.5rc1,todo figure out the output size for parallel 1d conv,1
v0.5rc1,todo: add unit test for sequence output feature,1
v0.5rc1,todo: remove code,1
v0.5rc1,todo: re-add 'attention' after further research in implication of torch,1
v0.5rc1,TODO(#1333): Refactor this test once torch sequence generator work is complete.,1
v0.5rc1,"TODO(Justin): Move these to test_sequence_generator unit tests, and reintroduce decoder attention, beam_width, and",1
v0.5rc1,TODO ray: replace legacy mode when Ray Train supports placement groups,1
v0.5rc1,TODO: these are the only outputs we provide from Torchscript for now,1
v0.5rc1,DOTO all shadows built in name - come up with a more descriptive name,1
v0.5rc1,TODO (ASN): add support for substitute_with_max parameter,1
v0.5rc1,TODO ray: support calculating stats on partitioned datasets,1
v0.5rc1,todo: support loading other model types based on config,1
v0.5rc1,TODO: fix for Ray where workers may be of different skus,1
v0.5rc1,TODO(travis): remove in favor of on_hyperopt_end for naming consistency,1
v0.5rc1,TODO(shreya): Add type hints for missing args,1
v0.5rc1,todo: revise docstring,1
v0.5rc1,todo: assess how to specify padding for equivalent to 'same',1
v0.5rc1,todo: determine how to pool_padding equivalent of 'same',1
v0.5rc1,todo: fixup docstring,1
v0.5rc1,todo: review docstring,1
v0.5rc1,todo: fix up docstring,1
v0.5rc1,todo: fix up docstring,1
v0.5rc1,todo: update docstring as needed,1
v0.5rc1,TODO(shreya): Make sure this is updated when FCStack is updated,1
v0.5rc1,TODO(justin): Use official class properties.,1
v0.5rc1,TODO(shreya): Confirm that this is it,1
v0.5rc1,TODO(shreya): Confirm that this is it,1
v0.5rc1,TODO(justin): This may need to be conditioned on which AutoModel gets chosen.,1
v0.5rc1,"TODO (tgaddair): come up with something better than this, maybe attempt to fit to Gaussian",1
v0.5rc1,"TODO (ASN): add other modalities (image, etc. )",1
v0.5rc1,TODO (ASN): Decide how we want to proceed if at least one trial has,1
v0.5rc1,TODO (ASN): add more robust heuristics,1
v0.5rc1,TODO (ASN): add image heuristics,1
v0.5rc1,"todo future: this may be redundant, check",1
v0.5rc1,Workaround for including additional tensors from output of input encoders for,1
v0.5rc1,TODO(Justin): Think about how to make this communication work for multi-sequence,1
v0.5rc1,todo: when https://github.com/ludwig-ai/ludwig/issues/810 is closed,1
v0.5rc1,todo future: maybe modify this with TF2 mask mechanics,1
v0.5rc1,"todo: can we just use projector_size? # hidden_size,",1
v0.5rc1,"todo future: this may be redundant, check",1
v0.5rc1,todo future: maybe reintroduce these attention function,1
v0.5rc1,todo future: maybe reintroduce these attention function,1
v0.5rc1,todo future: maybe reintroduce these attention function,1
v0.5rc1,TODO(shreya): Double check difference in computation.,1
v0.5rc1,TODO: double check,1
v0.5rc1,todo(jmt): confirm correct interpretation of LayerNorm paramters,1
v0.5rc1,todo: determine how to handle layer.name,1
v0.5rc1,"todo: enumerate for debugging, remove after testing",1
v0.5rc1,TODO(shreya): Combine with ResNetBlock by adding a flag.,1
v0.5rc1,TODO(shreya): Implement sparse embedding lookup.,1
v0.5rc1,# TODO(shreya): Check if this is equivalent,1
v0.5rc1,# TODO(shreya): Check if supported in torch,1
v0.5rc1,self.sparsemax = CustomSparsemax()  # todo: tf implementation,1
v0.5rc1,todo: review for generality,1
v0.5rc1,TODO: Simplify this.,1
v0.5rc1,todo maybe move code from add_feature_data here,1
v0.5rc1,todo: add cast to int64,1
v0.5rc1,TODO: Potentially abstract this feature-specific attribute overwrite to a consolidated design.,1
v0.5rc1,Dummy implementation.,1
v0.5rc1,"TODO(Justin): Add a confusion matrix, see",1
v0.5rc1,todo: add a mechanism for letting the user decide to save it,1
v0.5rc1,"this is not super nice, but works both and DFs and lists",1
v0.5rc1,todo: re-evaluate need for separate handling of `attention` reducer,1
v0.5rc1,TODO(shreya): Metrics should ideally just move to the correct device,1
v0.5rc1,and not require the user to do this. This is a temporary fix. See,1
v0.5rc1,TODO(Justin): Clean this up.,1
v0.5rc1,TODO(shreya): Confirm if it's ok to do per channel normalization,1
v0.5rc1,TODO(shreya): Also confirm if this is being used anywhere,1
v0.5rc1,TODO(shreya): Confirm if ok to use imagenet means and std devs,1
v0.5rc1,TODO: alternatively use get_average_image() for unreachable images,1
v0.5rc1,todo future add multiprocessing/multithreading,1
v0.5rc1,todo: refactor to reuse SequenceOutputFeature.postprocess_predictions,1
v0.5rc1,todo: add a mechanism for letting the user decide to save it,1
v0.5rc1,TODO ray: this is needed because ray 1.7 doesn't support Dask to RayDataset,1
v0.5rc1,todo figure out if additional parameters are needed,1
v0.5rc1,TODO dask: find a way to better parallelize this operation,1
v0.5rc1,TODO dask: this needs to work with DataFrames,1
v0.5rc1,TODO(travis): implement saving split for Ray,1
v0.5rc1,TODO: address if following results in fragmented DataFrame,1
v0.5rc1,TODO ray 1.8: convert to Tensors before shuffle,1
v0.5rc1,"TODO(travis): find way to avoid calling this, as it's expensive",1
v0.5rc1,TODO: consider removing this. doesn't work currently and read performance seems generally,1
v0.5rc1,"TODO(travis): could try hashing the in-memory dataset, but this is tricky for Dask",1
v0.5rc1,todo future: reintroduce the bucketed batcher,1
v0.5rc1,TODO ray: implement dynamic batch size,1
v0.5rc1,todo v0.4: currently not clear way to set model graph,1
v0.5rc1,TODO: need to also include a filename for this figure,1
v0.5rc1,Dummy implementation.,1
v0.5rc1,TODO: Use a configurable ratio for how often to use teacher forcing during training.,1
v0.5rc1,TODO: Use a configurable ratio for how often to use teacher forcing during training.,1
v0.5rc1,Dummy implementation.,1
v0.5rc1,"ORRIBLE HACK, IT'S THE ONLY WAY TO REMOVE PADDING",1
v0.5rc1,END OF HORRIBLE HACK,1
v0.5rc1,todo future: maybe modify this with TF2 mask mechanics,1
v0.5rc1,Workaround for: https://issues.apache.org/jira/browse/ARROW-5645,1
v0.5rc1,TODO(shreya): Confirm types of args,1
v0.5rc1,TODO(justin): Remove dummy implementation. Make input_shape and output_shape functions.,1
v0.5rc1,TODO(Justin): Check that the semantics of input_size align with what the combiner's output shape returns for,1
v0.5rc1,TODO (ASN): Circle back on how we want to set default placeholder value,1
v0.5rc1,TODO (ASN) : Circle back on how we want to set default placeholder value,1
v0.5rc1,TODO: refactor this into an interface,1
v0.5rc1,workaround type limitations of the underlying frameworks,1
v0.5rc1,TODO: deprecated v0.5,1
v0.5rc1,TODO: use placement groups,1
v0.5rc1,TODO travis: replace backend here once ray 1.8 released,1
v0.5rc1,"TODO ray: find an informed way to set the parallelism, in practice",1
v0.5rc1,"TODO ray: make this more configurable by allowing YAML overrides of timeout_s, etc.",1
v0.5rc1,TODO: deprecated 0.5,1
v0.5rc1,TODO(matt): Implement placement group strategies in BackendExecutor.,1
v0.5rc1,Temporary workaround to prevent tests from automatically using the Ray backend. Taken from,1
v0.5rc1,TODO(shreya): DataFrame created twice: here + CSVMixin. Figure out,1
v0.4.1,todo the hidden output is actually a tensor. May need modification,1
v0.4.1,todo figure out the output size for parallel 1d conv,1
v0.4.1,is there a better way to do this?,1
v0.4.1,TODO ray: replace legacy mode when Ray Train supports placement groups,1
v0.4.1,DOTO all shadows built in name - come up with a more descriptive name,1
v0.4.1,TODO (ASN): add support for substitute_with_max parameter,1
v0.4.1,TODO ray: support calculating stats on partitioned datasets,1
v0.4.1,todo: support loading other model types based on config,1
v0.4.1,TODO(travis): remove in favor of on_hyperopt_end for naming consistency,1
v0.4.1,"TODO (tgaddair): come up with something better than this, maybe attempt to fit to Gaussian",1
v0.4.1,"TODO (ASN): add other modalities (image, etc. )",1
v0.4.1,TODO (ASN): Decide how we want to proceed if at least one trial has,1
v0.4.1,TODO (ASN): add more robust heuristics,1
v0.4.1,TODO (ASN): add image heuristics,1
v0.4.1,"todo future: this may be redundant, check",1
v0.4.1,todo: when https://github.com/ludwig-ai/ludwig/issues/810 is closed,1
v0.4.1,todo future: maybe modify this with TF2 mask mechanics,1
v0.4.1,"todo future: this may be redundant, check",1
v0.4.1,"todo: this should actually be the size of the last fc layer,",1
v0.4.1,todo future: maybe reintroduce these attention function,1
v0.4.1,todo future: maybe reintroduce these attention function,1
v0.4.1,todo future: maybe reintroduce these attention function,1
v0.4.1,labels_smoothing=labels_smoothing  # todo reintroduce,1
v0.4.1,todo maybe move code from add_feature_data here,1
v0.4.1,todo: add cast to int64,1
v0.4.1,todo: add a mechanism for letting the user decide to save it,1
v0.4.1,"this is not super nice, but works both and DFs and lists",1
v0.4.1,todo future: maybe modify this with TF2 mask mechanics,1
v0.4.1,TODO: alternatively use get_average_image() for unreachable images,1
v0.4.1,todo future add multiprocessing/multithreading,1
v0.4.1,todo: refactor to reuse SequenceOutputFeature.postprocess_predictions,1
v0.4.1,todo: add a mechanism for letting the user decide to save it,1
v0.4.1,TODO ray: this is needed because ray 1.7 doesn't support Dask to RayDataset,1
v0.4.1,todo figure out if global_preprocessing_parameters is needed,1
v0.4.1,todo figure out if additional parameters are needed,1
v0.4.1,TODO dask: find a way to better parallelize this operation,1
v0.4.1,TODO dask: this needs to work with DataFrames,1
v0.4.1,TODO(travis): implement saving split for Ray,1
v0.4.1,TODO: address if following results in fragmented DataFrame,1
v0.4.1,TODO(travis): move read_parquet to cache layer after removing petastorm,1
v0.4.1,TODO ray 1.8: convert to Tensors before shuffle,1
v0.4.1,TODO(travis): consider combining this with `create` when Petastorm is dropped,1
v0.4.1,"TODO(travis): find way to avoid calling this, as it's expensive",1
v0.4.1,TODO: consider removing this. doesn't work currently and read performance seems generally,1
v0.4.1,TODO(travis): will not need shuffle_buffer_size after removing Petastorm,1
v0.4.1,Workaround for https://issues.apache.org/jira/browse/ARROW-1614,1
v0.4.1,"TODO(travis): could try hashing the in-memory dataset, but this is tricky for Dask",1
v0.4.1,todo future: reintroduce the bucketed batcher,1
v0.4.1,TODO ray: implement dynamic batch size,1
v0.4.1,todo v0.4: currently not clear way to set model graph,1
v0.4.1,TODO: need to also include a filename for this figure,1
v0.4.1,todo future: try to find a way to distinguish among these two cases,1
v0.4.1,"ORRIBLE HACK, IT'S THE ONLY WAY TO REMOVE PADDING",1
v0.4.1,END OF HORRIBLE HACK,1
v0.4.1,Workaround for: https://issues.apache.org/jira/browse/ARROW-5645,1
v0.4.1,TODO (ASN): Circle back on how we want to set default placeholder value,1
v0.4.1,TODO (ASN) : Circle back on how we want to set default placeholder value,1
v0.4.1,TODO: refactor this into an interface,1
v0.4.1,workaround type limitations of the underlying frameworks,1
v0.4.1,TODO: deprecated v0.5,1
v0.4.1,TODO: use placement groups,1
v0.4.1,TODO travis: replace backend here once ray 1.8 released,1
v0.4.1,"TODO ray: find an informed way to set the parallelism, in practice",1
v0.4.1,"TODO ray: make this more configurable by allowing YAML overrides of timeout_s, etc.",1
v0.4.1,TODO(travis): enable after dropping petastorm,1
v0.4.1,TODO: deprecated 0.5,1
v0.4.1,TODO(matt): Implement placement group strategies in BackendExecutor.,1
v0.4,todo the hidden output is actually a tensor. May need modification,1
v0.4,todo figure out the output size for parallel 1d conv,1
v0.4,is there a better way to do this?,1
v0.4,DOTO all shadows built in name - come up with a more descriptive name,1
v0.4,TODO ray: support calculating stats on partitioned datasets,1
v0.4,todo: support loading other model types based on config,1
v0.4,"todo future: this may be redundant, check",1
v0.4,todo: when https://github.com/ludwig-ai/ludwig/issues/810 is closed,1
v0.4,todo future: maybe modify this with TF2 mask mechanics,1
v0.4,"todo future: this may be redundant, check",1
v0.4,"todo: this should actually be the size of the last fc layer,",1
v0.4,todo future: maybe reintroduce these attention function,1
v0.4,todo future: maybe reintroduce these attention function,1
v0.4,todo future: maybe reintroduce these attention function,1
v0.4,labels_smoothing=labels_smoothing  # todo reintroduce,1
v0.4,todo maybe move code from add_feature_data here,1
v0.4,todo: add cast to int64,1
v0.4,todo: add a mechanism for letting the user decide to save it,1
v0.4,"this is not super nice, but works both and DFs and lists",1
v0.4,todo future: maybe modify this with TF2 mask mechanics,1
v0.4,todo future add multiprocessing/multithreading,1
v0.4,todo: refactor to reuse SequenceOutputFeature.postprocess_predictions,1
v0.4,todo: add a mechanism for letting the user decide to save it,1
v0.4,todo figure out if global_preprocessing_parameters is needed,1
v0.4,todo figure out if additional parameters are needed,1
v0.4,TODO dask: find a way to better parallelize this operation,1
v0.4,TODO dask: this needs to work with DataFrames,1
v0.4,TODO dask: https://docs.dask.org/en/latest/dataframe-api.html#dask.dataframe.DataFrame.random_split,1
v0.4,Workaround for https://issues.apache.org/jira/browse/ARROW-1614,1
v0.4,"TODO(travis): could try hashing the in-memory dataset, but this is tricky for Dask",1
v0.4,todo future: reintroduce the bucketed batcher,1
v0.4,todo v0.4: currently not clear way to set model graph,1
v0.4,TODO: need to also include a filename for this figure,1
v0.4,todo future: try to find a way to distinguish among these two cases,1
v0.4,"ORRIBLE HACK, IT'S THE ONLY WAY TO REMOVE PADDING",1
v0.4,END OF HORRIBLE HACK,1
v0.4,Workaround for: https://issues.apache.org/jira/browse/ARROW-5645,1
v0.4,workaround type limitations of the underlying frameworks,1
v0.4,"TODO ray: select this more intelligently,",1
v0.4,TODO ray: https://github.com/horovod/horovod/issues/2702,1
v0.4,"TODO ray: make this more configurable by allowing YAML overrides of timeout_s, etc.",1
v0.4,TODO(travis): get_connected_model is needed here because TF will not init,1
v0.3.3,todo the hidden output is actually a tensor. May need modification,1
v0.3.3,todo figure out the output size for parallel 1d conv,1
v0.3.3,DOTO all shadows built in name - come up with a more descriptive name,1
v0.3.3,todo: support loading other model types based on config,1
v0.3.3,"todo future: this may be redundant, check",1
v0.3.3,todo: when https://github.com/ludwig-ai/ludwig/issues/810 is closed,1
v0.3.3,todo future: maybe modify this with TF2 mask mechanics,1
v0.3.3,todo future: maybe reintroduce these attention function,1
v0.3.3,todo future: maybe reintroduce these attention function,1
v0.3.3,todo future: maybe reintroduce these attention function,1
v0.3.3,labels_smoothing=labels_smoothing  # todo reintroduce,1
v0.3.3,todo maybe move code from add_feature_data here,1
v0.3.3,todo: add cast to int64,1
v0.3.3,todo: add a mechanism for letting the user decide to save it,1
v0.3.3,"this is not super nice, but works both and DFs and lists",1
v0.3.3,todo future: maybe modify this with TF2 mask mechanics,1
v0.3.3,todo future add multiprocessing/multithreading,1
v0.3.3,todo: refactor to reuse SequenceOutputFeature.postprocess_predictions,1
v0.3.3,todo: add a mechanism for letting the user decide to save it,1
v0.3.3,todo figure out if global_preprocessing_parameters is needed,1
v0.3.3,todo figure out if additional parameters are needed,1
v0.3.3,TODO dask: find a way to better parallelize this operation,1
v0.3.3,to the checksum calculation? maybe it's redundant,1
v0.3.3,TODO dask: consolidate hdf5 cache with backend cache,1
v0.3.3,TODO dask: https://docs.dask.org/en/latest/dataframe-api.html#dask.dataframe.DataFrame.random_split,1
v0.3.3,TODO dask: support postprocessing using Backend,1
v0.3.3,todo v0.4: currently not clear way to set model graph,1
v0.3.3,todo future: try to find a way to distinguish among these two cases,1
v0.3.3,todo future: reintroduce the bucketed batcher,1
v0.3.3,"ORRIBLE HACK, IT'S THE ONLY WAY TO REMOVE PADDING",1
v0.3.3,END OF HORRIBLE HACK,1
v0.3.2,todo the hidden output is actually a tensor. May need modification,1
v0.3.2,todo figure out the output size for parallel 1d conv,1
v0.3.2,DOTO all shadows built in name - come up with a more descriptive name,1
v0.3.2,todo: support loading other model types based on config,1
v0.3.2,"todo future: this may be redundant, check",1
v0.3.2,todo: when https://github.com/ludwig-ai/ludwig/issues/810 is closed,1
v0.3.2,todo future: maybe modify this with TF2 mask mechanics,1
v0.3.2,todo future: maybe reintroduce these attention function,1
v0.3.2,todo future: maybe reintroduce these attention function,1
v0.3.2,todo future: maybe reintroduce these attention function,1
v0.3.2,labels_smoothing=labels_smoothing  # todo reintroduce,1
v0.3.2,todo maybe move code from add_feature_data here,1
v0.3.2,todo: add cast to int64,1
v0.3.2,todo: add a mechanism for letting the user decide to save it,1
v0.3.2,"this is not super nice, but works both and DFs and lists",1
v0.3.2,todo future: maybe modify this with TF2 mask mechanics,1
v0.3.2,todo future add multiprocessing/multithreading,1
v0.3.2,todo: refactor to reuse SequenceOutputFeature.postprocess_predictions,1
v0.3.2,todo: add a mechanism for letting the user decide to save it,1
v0.3.2,todo figure out if global_preprocessing_parameters is needed,1
v0.3.2,todo figure out if additional parameters are needed,1
v0.3.2,TODO dask: find a way to better parallelize this operation,1
v0.3.2,to the checksum calculation? maybe it's redundant,1
v0.3.2,TODO dask: consolidate hdf5 cache with backend cache,1
v0.3.2,TODO dask: https://docs.dask.org/en/latest/dataframe-api.html#dask.dataframe.DataFrame.random_split,1
v0.3.2,TODO dask: support postprocessing using Backend,1
v0.3.2,todo v0.4: currently not clear way to set model graph,1
v0.3.2,todo future: try to find a way to distinguish among these two cases,1
v0.3.2,todo future: reintroduce the bucketed batcher,1
v0.3.2,"ORRIBLE HACK, IT'S THE ONLY WAY TO REMOVE PADDING",1
v0.3.2,END OF HORRIBLE HACK,1
v0.3.1,todo the hidden output is actually a tensor. May need modification,1
v0.3.1,todo figure out the output size for parallel 1d conv,1
v0.3.1,todo determine feasibility of putting Experiment() into a pytest.fixture,1
v0.3.1,DOTO all shadows built in name - come up with a more descriptive name,1
v0.3.1,todo: support loading other model types based on config,1
v0.3.1,"todo future: this may be redundant, check",1
v0.3.1,todo: when https://github.com/uber/ludwig/issues/810 is closed,1
v0.3.1,todo future: maybe modify this with TF2 mask mechanics,1
v0.3.1,todo future: maybe reintroduce these attention function,1
v0.3.1,todo future: maybe reintroduce these attention function,1
v0.3.1,todo future: maybe reintroduce these attention function,1
v0.3.1,labels_smoothing=labels_smoothing  # todo reintroduce,1
v0.3.1,todo: should adapt for the case of beam > 1,1
v0.3.1,todo: add a mechanism for letting the user decide to save it,1
v0.3.1,"this is not super nice, but works both and DFs and lists",1
v0.3.1,todo future: maybe modify this with TF2 mask mechanics,1
v0.3.1,todo future add multiprocessing/multithreading,1
v0.3.1,todo: refactor to reuse SequenceOutputFeature.postprocess_predictions,1
v0.3.1,todo: add a mechanism for letting the user decide to save it,1
v0.3.1,to the checksum calculation? maybe it's redundant,1
v0.3.1,todo v0.4: currently not clear way to set model graph,1
v0.3.1,todo future: try to find a way to distinguish among these two cases,1
v0.3.1,todo future: reintroduce the bucketed batcher,1
v0.3.1,"ORRIBLE HACK, IT'S THE ONLY WAY TO REMOVE PADDING",1
v0.3.1,END OF HORRIBLE HACK,1
v0.3,todo the hidden output is actually a tensor. May need modification,1
v0.3,todo figure out the output size for parallel 1d conv,1
v0.3,todo determine feasibility of putting Experiment() into a pytest.fixture,1
v0.3,DOTO all shadows built in name - come up with a more descriptive name,1
v0.3,todo: support loading other model types based on config,1
v0.3,"todo future: this may be redundant, check",1
v0.3,todo: when https://github.com/uber/ludwig/issues/810 is closed,1
v0.3,todo future: maybe modify this with TF2 mask mechanics,1
v0.3,todo future: maybe reintroduce these attention function,1
v0.3,todo future: maybe reintroduce these attention function,1
v0.3,todo future: maybe reintroduce these attention function,1
v0.3,todo: should adapt for the case of beam > 1,1
v0.3,todo: add a mechanism for letting the user decide to save it,1
v0.3,"this is not super nice, but works both and DFs and lists",1
v0.3,todo future: maybe modify this with TF2 mask mechanics,1
v0.3,todo future add multiprocessing/multithreading,1
v0.3,todo: refactor to reuse SequenceOutputFeature.postprocess_predictions,1
v0.3,todo: add a mechanism for letting the user decide to save it,1
v0.3,todo v0.4: currently not clear way to set model graph,1
v0.3,todo future: try to find a way to distinguish among these two cases,1
v0.3,todo future: reintroduce the bucketed batcher,1
v0.3,"ORRIBLE HACK, IT'S THE ONLY WAY TO REMOVE PADDING",1
v0.3,END OF HORRIBLE HACK,1
v0.2.2,todo why does it only work when we render a mesh before>?>?????,1
v0.2.2,TODO the hidden output is actually a tensor. May need modification,1
v0.2.2,TODO Figure out the output size for parallel 1d conv,1
v0.2.2,todo why does it only work when we render a mesh before>?>?????,1
v0.2.2,DOTO all shadows built in name - come up with a more descriptive name,1
v0.2.2,todo: should adapt for the case of beam > 1,1
v0.2.2,todo: add a mechanism for letting the user decide to save it,1
v0.2.2,TODO add multiprocessing/multithreading,1
v0.2.2,TODO - not sure if this is correct,1
v0.2.2,todo: refactor to reuse SeuuqnceOutputFeature.postprocess_results,1
v0.2.2,todo: add a mechanism for letting the user decide to save it,1
v0.2.2,"ORRIBLE HACK, IT'S THE ONLY WAY TO REMOVE PADDING",1
v0.2.2,END OF HORRIBLE HACK,1
v0.2.1,todo why does it only work when we render a mesh before>?>?????,1
v0.2.1,TODO the hidden output is actually a tensor. May need modification,1
v0.2.1,TODO Figure out the output size for parallel 1d conv,1
v0.2.1,todo why does it only work when we render a mesh before>?>?????,1
v0.2.1,DOTO all shadows built in name - come up with a more descriptive name,1
v0.2.1,todo: should adapt for the case of beam > 1,1
v0.2.1,TODO add multiprocessing/multithreading,1
v0.2.1,TODO - not sure if this is correct,1
v0.2.1,"ORRIBLE HACK, IT'S THE ONLY WAY TO REMOVE PADDING",1
v0.2.1,END OF HORRIBLE HACK,1
v0.2,todo why does it only work when we render a mesh before>?>?????,1
v0.2,TODO the hidden output is actually a tensor. May need modification,1
v0.2,TODO Figure out the output size for parallel 1d conv,1
v0.2,todo why does it only work when we render a mesh before>?>?????,1
v0.2,DOTO all shadows built in name - come up with a more descriptive name,1
v0.2,todo: should adapt for the case of beam > 1,1
v0.2,"ORRIBLE HACK, IT'S THE ONLY WAY TO REMOVE PADDING",1
v0.2,END OF HORRIBLE HACK,1
v0.1.2,todo why does it only work when we render a mesh before>?>?????,1
v0.1.2,TODO the hidden output is actually a tensor. May need modification,1
v0.1.2,todo why does it only work when we render a mesh before>?>?????,1
v0.1.2,DOTO all shadows built in name - come up with a more descriptive name,1
v0.1.2,"ORRIBLE HACK, IT'S THE ONLY WAY TO REMOVE PADDING",1
v0.1.2,END OF HORRIBLE HACK,1
v0.1.1,todo why does it only work when we render a mesh before>?>?????,1
v0.1.1,todo why does it only work when we render a mesh before>?>?????,1
v0.1.1,DOTO all shadows built in name - come up with a more descriptive name,1
v0.1.1,"ORRIBLE HACK, IT'S THE ONLY WAY TO REMOVE PADDING",1
v0.1.1,END OF HORRIBLE HACK,1
v0.1.0,DOTO all shadows built in name - come up with a more descriptive name,1
v0.1.0,"ORRIBLE HACK, IT'S THE ONLY WAY TO REMOVE PADDING",1
v0.1.0,END OF HORRIBLE HACK,1
