Version,Commit Message,SATD
v0.15.1,TODO: enable type aliases,1
v0.15.1,-- Options for todo extension ----------------------------------------------,1
v0.15.1,"If true, `todo` and `todoList` produce output, else they produce nothing.",1
v0.15.1,TODO. Deal with multi-class instrument,1
v0.15.1,TODO. Deal with multi-class instrument,1
v0.15.1,TODO. The solution below is not really a valid cross-fitting,1
v0.15.1,"HACK: DRIV doesn't expect a treatment dimension, so pretend we got a vector even if we really had a one-column array",1
v0.15.1,"Once multiple treatments are supported, we'll need to fix this",1
v0.15.1,TODO. Deal with multi-class instrument/treatment,1
v0.15.1,TODO: any way to avoid creating a copy if the array was already dense?,1
v0.15.1,TODO: might be faster to break into connected components first,1
v0.15.1,TODO: Consider investigating other performance ideas for these cases,1
v0.15.1,TODO: would using einsum's paths to optimize the order of merging help?,1
v0.15.1,HACK: We're relying on some of sklearn's non-public classes which are not completely stable.,1
v0.15.1,"HACK: it's not optimal to use a regex like this, but the base class's node_to_str doesn't expose any",1
v0.15.1,TODO. Create our own color pallete for multiple treatments. The one below is for binary treatments.,1
v0.15.1,"TODO: what if input is sparse? - there's no equivalent to einsum,",1
v0.15.1,TODO Share some logic with non-discrete version,1
v0.15.1,"TODO: This could be extended to also work with our sparse and 2SLS estimators,",1
v0.15.1,TODO: This seems hacky; is there a better abstraction to maintain these?,1
v0.15.1,HACK: sklearn 1.3 enforces that the input to plot_tree is a DecisionTreeClassifier or DecisionTreeRegressor,1
v0.15.1,This is a hack to get around that restriction by declaring that PolicyTree inherits from DecisionTreeClassifier,1
v0.15.1,TODO: support freq_weight and sample_var in debiased lasso,1
v0.15.1,TODO: support freq_weight and sample_var in debiased lasso,1
v0.15.1,"TODO: consider whether we need more care around stateful featurizers,",1
v0.15.1,"TODO: consider whether we need more care around stateful featurizers,",1
v0.15.1,TODO: Add a __dir__ implementation?,1
v0.15.1,TODO: studentized bootstrap? this would be more accurate in most cases but can we avoid,1
v0.15.1,TODO: support freq_weight and sample_var,1
v0.15.1,TODO: update docs,1
v0.15.1,Doesn't work with CNTK backend as of 2018-07-17 - see https://github.com/keras-team/keras/issues/10715,1
v0.15.1,For some reason this doesn't work at all when run against the CNTK backend...,1
v0.15.1,TODO: test that the estimated effect is usually within the bounds,1
v0.15.1,TODO: test that the estimated effect is usually within the bounds,1
v0.15.1,TODO: set up proper flag for this,1
v0.15.1,TODO: test something rather than just print...,1
v0.15.1,TODO: this tests that we can run the method; how do we test that the results are reasonable?,1
v0.15.1,TODO: serializing/deserializing for every combination -- is this necessary?,1
v0.15.1,does not work well with parallelism.,1
v0.15.1,does not work well with parallelism.,1
v0.15.1,"fourth column: 5 ones, also not enough but barely works even with forest heterogeneity",1
v0.15.1,TODO: we don't recover the correct values with enough accuracy to enable this assertion,1
v0.15.1,is there a different way to verify that we are learning the correct coefficients?,1
v0.15.1,"TODO There is a matrix dimension mismatch between multiple outcome and single outcome, should solve that",1
v0.15.1,"TODO: make sure that the public API consistently uses ""T"" instead of ""P"" for the treatment",1
v0.15.1,TODO: does the numeric stability actually make any difference?,1
v0.15.1,"CNTK backend can't randomize across batches and doesn't implement cumsum (at least as of June 2018,",1
v0.15.1,TODO: is there a more robust way to do this?,1
v0.15.1,TODO: do we need to give the user more control over other arguments to fit?,1
v0.15.1,TODO: do we need to give the user more control over other arguments to fit?,1
v0.15.1,"TODO: it seems like we need to sum over the batch because we can only apply gradient to a scalar,",1
v0.15.1,TODO: any way to get this to work on batches of arbitrary size?,1
v0.15.1,TODO: prel_model_effect could allow sample_var and freq_weight?,1
v0.15.1,TODO: support freq_weight and sample_var in debiased lasso,1
v0.15.1,TODO: do correct adjustment for sample_var,1
v0.15.1,TODO: is it right that the effective number of intruments is the,1
v0.15.1,TODO: this utility is documented but internal; reimplement?,1
v0.15.1,TODO: this utility is even less public...,1
v0.15.1,TODO: remove once older sklearn support is no longer needed,1
v0.15.1,"the transformation logic here is somewhat tricky; we always need to encode the categorical columns,",1
v0.15.1,TODO: we can't currently handle unseen values of the feature column when getting the effect;,1
v0.15.1,HACK: this is slightly ugly because we rely on the fact that DML passes [X;W] to the first stage models,1
v0.15.1,TODO: consider addding an API to DML that allows for better understanding of how the nuisance inputs are,1
v0.15.1,"TODO: Add other nuisance model options, such as {'azure_automl', 'forests', 'boosting'} that will use particular",1
v0.15.1,"TODO: Add other heterogeneity model options, such as {'automl'} for performing",1
v0.15.1,TODO: Enable multi-class classification (post-MVP),1
v0.15.1,TODO: check compatibility of X and Y lengths,1
v0.15.1,"TODO: bail out also if categorical columns, classification, random_state changed?",1
v0.15.1,TODO: should we also train a new model_y under any circumstances when warm_start is True?,1
v0.15.1,"don't train a model, but suggest workaround since there are enough instances of least",1
v0.15.1,TODO: enrich outcome logic for multi-class classification when that is supported,1
v0.15.1,TODO: Note that there's no column metadata for the sample number - should there be?,1
v0.15.1,TODO: it seems like it would be better to just return the tree itself rather than plot it;,1
v0.15.1,TODO: it seems like it would be better to just return the tree itself rather than plot it;,1
v0.15.1,TODO: conisder working around relying on sklearn implementation details,1
v0.15.1,TODO: consider working around relying on sklearn implementation details,1
v0.15.1,"TODO: Add other types of covariance estimation (e.g. Newey-West (HAC), HC2, HC3)",1
v0.15.1,TODO: consider working around relying on sklearn implementation details,1
v0.15.1,TODO: Check performance,1
v0.15.1,TODO: ideally the below private attribute logic should be in .fit but is needed in init,1
v0.15.1,should refactor later,1
v0.15.1,Need to redefine fit here for auto inference to work due to a quirk in how,1
v0.15.0,TODO: enable type aliases,1
v0.15.0,-- Options for todo extension ----------------------------------------------,1
v0.15.0,"If true, `todo` and `todoList` produce output, else they produce nothing.",1
v0.15.0,TODO. Deal with multi-class instrument,1
v0.15.0,TODO. Deal with multi-class instrument,1
v0.15.0,TODO. The solution below is not really a valid cross-fitting,1
v0.15.0,"HACK: DRIV doesn't expect a treatment dimension, so pretend we got a vector even if we really had a one-column array",1
v0.15.0,"Once multiple treatments are supported, we'll need to fix this",1
v0.15.0,TODO. Deal with multi-class instrument/treatment,1
v0.15.0,TODO: any way to avoid creating a copy if the array was already dense?,1
v0.15.0,TODO: might be faster to break into connected components first,1
v0.15.0,TODO: Consider investigating other performance ideas for these cases,1
v0.15.0,TODO: would using einsum's paths to optimize the order of merging help?,1
v0.15.0,HACK: We're relying on some of sklearn's non-public classes which are not completely stable.,1
v0.15.0,"HACK: it's not optimal to use a regex like this, but the base class's node_to_str doesn't expose any",1
v0.15.0,TODO. Create our own color pallete for multiple treatments. The one below is for binary treatments.,1
v0.15.0,"TODO: what if input is sparse? - there's no equivalent to einsum,",1
v0.15.0,TODO Share some logic with non-discrete version,1
v0.15.0,"TODO: This could be extended to also work with our sparse and 2SLS estimators,",1
v0.15.0,TODO: This seems hacky; is there a better abstraction to maintain these?,1
v0.15.0,HACK: sklearn 1.3 enforces that the input to plot_tree is a DecisionTreeClassifier or DecisionTreeRegressor,1
v0.15.0,This is a hack to get around that restriction by declaring that PolicyTree inherits from DecisionTreeClassifier,1
v0.15.0,TODO: support freq_weight and sample_var in debiased lasso,1
v0.15.0,TODO: support freq_weight and sample_var in debiased lasso,1
v0.15.0,"TODO: consider whether we need more care around stateful featurizers,",1
v0.15.0,"TODO: consider whether we need more care around stateful featurizers,",1
v0.15.0,TODO: Add a __dir__ implementation?,1
v0.15.0,TODO: studentized bootstrap? this would be more accurate in most cases but can we avoid,1
v0.15.0,TODO: support freq_weight and sample_var,1
v0.15.0,TODO: update docs,1
v0.15.0,Doesn't work with CNTK backend as of 2018-07-17 - see https://github.com/keras-team/keras/issues/10715,1
v0.15.0,For some reason this doesn't work at all when run against the CNTK backend...,1
v0.15.0,TODO: test that the estimated effect is usually within the bounds,1
v0.15.0,TODO: test that the estimated effect is usually within the bounds,1
v0.15.0,TODO: set up proper flag for this,1
v0.15.0,TODO: test something rather than just print...,1
v0.15.0,TODO: this tests that we can run the method; how do we test that the results are reasonable?,1
v0.15.0,TODO: serializing/deserializing for every combination -- is this necessary?,1
v0.15.0,does not work well with parallelism.,1
v0.15.0,does not work well with parallelism.,1
v0.15.0,"fourth column: 5 ones, also not enough but barely works even with forest heterogeneity",1
v0.15.0,TODO: we don't recover the correct values with enough accuracy to enable this assertion,1
v0.15.0,is there a different way to verify that we are learning the correct coefficients?,1
v0.15.0,"TODO There is a matrix dimension mismatch between multiple outcome and single outcome, should solve that",1
v0.15.0,"TODO: make sure that the public API consistently uses ""T"" instead of ""P"" for the treatment",1
v0.15.0,TODO: does the numeric stability actually make any difference?,1
v0.15.0,"CNTK backend can't randomize across batches and doesn't implement cumsum (at least as of June 2018,",1
v0.15.0,TODO: is there a more robust way to do this?,1
v0.15.0,TODO: do we need to give the user more control over other arguments to fit?,1
v0.15.0,TODO: do we need to give the user more control over other arguments to fit?,1
v0.15.0,"TODO: it seems like we need to sum over the batch because we can only apply gradient to a scalar,",1
v0.15.0,TODO: any way to get this to work on batches of arbitrary size?,1
v0.15.0,TODO: prel_model_effect could allow sample_var and freq_weight?,1
v0.15.0,TODO: support freq_weight and sample_var in debiased lasso,1
v0.15.0,TODO: do correct adjustment for sample_var,1
v0.15.0,TODO: is it right that the effective number of intruments is the,1
v0.15.0,TODO: this utility is documented but internal; reimplement?,1
v0.15.0,TODO: this utility is even less public...,1
v0.15.0,TODO: remove once older sklearn support is no longer needed,1
v0.15.0,"the transformation logic here is somewhat tricky; we always need to encode the categorical columns,",1
v0.15.0,TODO: we can't currently handle unseen values of the feature column when getting the effect;,1
v0.15.0,HACK: this is slightly ugly because we rely on the fact that DML passes [X;W] to the first stage models,1
v0.15.0,TODO: consider addding an API to DML that allows for better understanding of how the nuisance inputs are,1
v0.15.0,"TODO: Add other nuisance model options, such as {'azure_automl', 'forests', 'boosting'} that will use particular",1
v0.15.0,"TODO: Add other heterogeneity model options, such as {'automl'} for performing",1
v0.15.0,TODO: Enable multi-class classification (post-MVP),1
v0.15.0,TODO: check compatibility of X and Y lengths,1
v0.15.0,"TODO: bail out also if categorical columns, classification, random_state changed?",1
v0.15.0,TODO: should we also train a new model_y under any circumstances when warm_start is True?,1
v0.15.0,"don't train a model, but suggest workaround since there are enough instances of least",1
v0.15.0,TODO: enrich outcome logic for multi-class classification when that is supported,1
v0.15.0,TODO: Note that there's no column metadata for the sample number - should there be?,1
v0.15.0,TODO: it seems like it would be better to just return the tree itself rather than plot it;,1
v0.15.0,TODO: it seems like it would be better to just return the tree itself rather than plot it;,1
v0.15.0,TODO: conisder working around relying on sklearn implementation details,1
v0.15.0,TODO: consider working around relying on sklearn implementation details,1
v0.15.0,"TODO: Add other types of covariance estimation (e.g. Newey-West (HAC), HC2, HC3)",1
v0.15.0,TODO: consider working around relying on sklearn implementation details,1
v0.15.0,TODO: Check performance,1
v0.15.0,TODO: ideally the below private attribute logic should be in .fit but is needed in init,1
v0.15.0,should refactor later,1
v0.15.0,Need to redefine fit here for auto inference to work due to a quirk in how,1
v0.15.0b1,TODO: enable type aliases,1
v0.15.0b1,-- Options for todo extension ----------------------------------------------,1
v0.15.0b1,"If true, `todo` and `todoList` produce output, else they produce nothing.",1
v0.15.0b1,TODO. Deal with multi-class instrument,1
v0.15.0b1,TODO. Deal with multi-class instrument,1
v0.15.0b1,TODO. The solution below is not really a valid cross-fitting,1
v0.15.0b1,"HACK: DRIV doesn't expect a treatment dimension, so pretend we got a vector even if we really had a one-column array",1
v0.15.0b1,"Once multiple treatments are supported, we'll need to fix this",1
v0.15.0b1,TODO. Deal with multi-class instrument/treatment,1
v0.15.0b1,TODO: any way to avoid creating a copy if the array was already dense?,1
v0.15.0b1,TODO: might be faster to break into connected components first,1
v0.15.0b1,TODO: Consider investigating other performance ideas for these cases,1
v0.15.0b1,TODO: would using einsum's paths to optimize the order of merging help?,1
v0.15.0b1,HACK: We're relying on some of sklearn's non-public classes which are not completely stable.,1
v0.15.0b1,"HACK: it's not optimal to use a regex like this, but the base class's node_to_str doesn't expose any",1
v0.15.0b1,TODO. Create our own color pallete for multiple treatments. The one below is for binary treatments.,1
v0.15.0b1,"TODO: what if input is sparse? - there's no equivalent to einsum,",1
v0.15.0b1,TODO Share some logic with non-discrete version,1
v0.15.0b1,"TODO: This could be extended to also work with our sparse and 2SLS estimators,",1
v0.15.0b1,TODO: This seems hacky; is there a better abstraction to maintain these?,1
v0.15.0b1,HACK: sklearn 1.3 enforces that the input to plot_tree is a DecisionTreeClassifier or DecisionTreeRegressor,1
v0.15.0b1,This is a hack to get around that restriction by declaring that PolicyTree inherits from DecisionTreeClassifier,1
v0.15.0b1,TODO: support freq_weight and sample_var in debiased lasso,1
v0.15.0b1,TODO: support freq_weight and sample_var in debiased lasso,1
v0.15.0b1,"TODO: consider whether we need more care around stateful featurizers,",1
v0.15.0b1,"TODO: consider whether we need more care around stateful featurizers,",1
v0.15.0b1,TODO: Add a __dir__ implementation?,1
v0.15.0b1,TODO: studentized bootstrap? this would be more accurate in most cases but can we avoid,1
v0.15.0b1,TODO: support freq_weight and sample_var,1
v0.15.0b1,TODO: update docs,1
v0.15.0b1,Doesn't work with CNTK backend as of 2018-07-17 - see https://github.com/keras-team/keras/issues/10715,1
v0.15.0b1,For some reason this doesn't work at all when run against the CNTK backend...,1
v0.15.0b1,TODO: test that the estimated effect is usually within the bounds,1
v0.15.0b1,TODO: test that the estimated effect is usually within the bounds,1
v0.15.0b1,TODO: set up proper flag for this,1
v0.15.0b1,TODO: test something rather than just print...,1
v0.15.0b1,TODO: this tests that we can run the method; how do we test that the results are reasonable?,1
v0.15.0b1,TODO: serializing/deserializing for every combination -- is this necessary?,1
v0.15.0b1,does not work well with parallelism.,1
v0.15.0b1,does not work well with parallelism.,1
v0.15.0b1,"fourth column: 5 ones, also not enough but barely works even with forest heterogeneity",1
v0.15.0b1,TODO: we don't recover the correct values with enough accuracy to enable this assertion,1
v0.15.0b1,is there a different way to verify that we are learning the correct coefficients?,1
v0.15.0b1,"TODO There is a matrix dimension mismatch between multiple outcome and single outcome, should solve that",1
v0.15.0b1,"TODO: make sure that the public API consistently uses ""T"" instead of ""P"" for the treatment",1
v0.15.0b1,TODO: does the numeric stability actually make any difference?,1
v0.15.0b1,"CNTK backend can't randomize across batches and doesn't implement cumsum (at least as of June 2018,",1
v0.15.0b1,TODO: is there a more robust way to do this?,1
v0.15.0b1,TODO: do we need to give the user more control over other arguments to fit?,1
v0.15.0b1,TODO: do we need to give the user more control over other arguments to fit?,1
v0.15.0b1,"TODO: it seems like we need to sum over the batch because we can only apply gradient to a scalar,",1
v0.15.0b1,TODO: any way to get this to work on batches of arbitrary size?,1
v0.15.0b1,"TODO: if the T and Z models overfit, then this will be biased towards 0;",1
v0.15.0b1,TODO: prel_model_effect could allow sample_var and freq_weight?,1
v0.15.0b1,TODO: support freq_weight and sample_var in debiased lasso,1
v0.15.0b1,TODO: do correct adjustment for sample_var,1
v0.15.0b1,TODO: is it right that the effective number of intruments is the,1
v0.15.0b1,TODO: this utility is documented but internal; reimplement?,1
v0.15.0b1,TODO: this utility is even less public...,1
v0.15.0b1,TODO: remove once older sklearn support is no longer needed,1
v0.15.0b1,"the transformation logic here is somewhat tricky; we always need to encode the categorical columns,",1
v0.15.0b1,TODO: we can't currently handle unseen values of the feature column when getting the effect;,1
v0.15.0b1,HACK: this is slightly ugly because we rely on the fact that DML passes [X;W] to the first stage models,1
v0.15.0b1,TODO: consider addding an API to DML that allows for better understanding of how the nuisance inputs are,1
v0.15.0b1,"TODO: Add other nuisance model options, such as {'azure_automl', 'forests', 'boosting'} that will use particular",1
v0.15.0b1,"TODO: Add other heterogeneity model options, such as {'automl'} for performing",1
v0.15.0b1,TODO: Enable multi-class classification (post-MVP),1
v0.15.0b1,TODO: check compatibility of X and Y lengths,1
v0.15.0b1,"TODO: bail out also if categorical columns, classification, random_state changed?",1
v0.15.0b1,TODO: should we also train a new model_y under any circumstances when warm_start is True?,1
v0.15.0b1,"don't train a model, but suggest workaround since there are enough instances of least",1
v0.15.0b1,TODO: enrich outcome logic for multi-class classification when that is supported,1
v0.15.0b1,TODO: Note that there's no column metadata for the sample number - should there be?,1
v0.15.0b1,TODO: it seems like it would be better to just return the tree itself rather than plot it;,1
v0.15.0b1,TODO: it seems like it would be better to just return the tree itself rather than plot it;,1
v0.15.0b1,TODO: conisder working around relying on sklearn implementation details,1
v0.15.0b1,"TODO: we need to alter this to use out-of-sample score here, which",1
v0.15.0b1,TODO: consider working around relying on sklearn implementation details,1
v0.15.0b1,"TODO: Add other types of covariance estimation (e.g. Newey-West (HAC), HC2, HC3)",1
v0.15.0b1,TODO: consider working around relying on sklearn implementation details,1
v0.15.0b1,TODO: Check performance,1
v0.15.0b1,TODO: ideally the below private attribute logic should be in .fit but is needed in init,1
v0.15.0b1,should refactor later,1
v0.15.0b1,Need to redefine fit here for auto inference to work due to a quirk in how,1
v0.14.1,TODO: enable type aliases,1
v0.14.1,-- Options for todo extension ----------------------------------------------,1
v0.14.1,"If true, `todo` and `todoList` produce output, else they produce nothing.",1
v0.14.1,TODO. Deal with multi-class instrument,1
v0.14.1,TODO. Deal with multi-class instrument,1
v0.14.1,TODO. The solution below is not really a valid cross-fitting,1
v0.14.1,"HACK: DRIV doesn't expect a treatment dimension, so pretend we got a vector even if we really had a one-column array",1
v0.14.1,"Once multiple treatments are supported, we'll need to fix this",1
v0.14.1,TODO. Deal with multi-class instrument/treatment,1
v0.14.1,TODO: any way to avoid creating a copy if the array was already dense?,1
v0.14.1,TODO: might be faster to break into connected components first,1
v0.14.1,TODO: Consider investigating other performance ideas for these cases,1
v0.14.1,TODO: would using einsum's paths to optimize the order of merging help?,1
v0.14.1,HACK: We're relying on some of sklearn's non-public classes which are not completely stable.,1
v0.14.1,"HACK: it's not optimal to use a regex like this, but the base class's node_to_str doesn't expose any",1
v0.14.1,TODO. Create our own color pallete for multiple treatments. The one below is for binary treatments.,1
v0.14.1,"TODO: what if input is sparse? - there's no equivalent to einsum,",1
v0.14.1,TODO Share some logic with non-discrete version,1
v0.14.1,TODO: support freq_weight and sample_var in debiased lasso,1
v0.14.1,"TODO: consider whether we need more care around stateful featurizers,",1
v0.14.1,TODO: support freq_weight and sample_var in debiased lasso,1
v0.14.1,"TODO: consider whether we need more care around stateful featurizers,",1
v0.14.1,"TODO: consider whether we need more care around stateful featurizers,",1
v0.14.1,TODO: Add a __dir__ implementation?,1
v0.14.1,TODO: studentized bootstrap? this would be more accurate in most cases but can we avoid,1
v0.14.1,TODO: support freq_weight and sample_var,1
v0.14.1,TODO: update docs,1
v0.14.1,Doesn't work with CNTK backend as of 2018-07-17 - see https://github.com/keras-team/keras/issues/10715,1
v0.14.1,For some reason this doesn't work at all when run against the CNTK backend...,1
v0.14.1,TODO: test that the estimated effect is usually within the bounds,1
v0.14.1,TODO: test that the estimated effect is usually within the bounds,1
v0.14.1,TODO: set up proper flag for this,1
v0.14.1,TODO: test something rather than just print...,1
v0.14.1,TODO: this tests that we can run the method; how do we test that the results are reasonable?,1
v0.14.1,HACK: work around bug in assertWarns (https://bugs.python.org/issue29620),1
v0.14.1,TODO: serializing/deserializing for every combination -- is this necessary?,1
v0.14.1,does not work well with parallelism.,1
v0.14.1,does not work well with parallelism.,1
v0.14.1,TODO: does this imply we should change some defaults to make this more likely to succeed?,1
v0.14.1,TODO: does this imply we should change some defaults to make this more likely to succeed?,1
v0.14.1,"fourth column: 5 ones, also not enough but barely works even with forest heterogeneity",1
v0.14.1,TODO: we don't recover the correct values with enough accuracy to enable this assertion,1
v0.14.1,is there a different way to verify that we are learning the correct coefficients?,1
v0.14.1,"TODO There is a matrix dimension mismatch between multiple outcome and single outcome, should solve that",1
v0.14.1,"TODO: make sure that the public API consistently uses ""T"" instead of ""P"" for the treatment",1
v0.14.1,TODO: does the numeric stability actually make any difference?,1
v0.14.1,"CNTK backend can't randomize across batches and doesn't implement cumsum (at least as of June 2018,",1
v0.14.1,TODO: is there a more robust way to do this?,1
v0.14.1,TODO: do we need to give the user more control over other arguments to fit?,1
v0.14.1,TODO: do we need to give the user more control over other arguments to fit?,1
v0.14.1,"TODO: it seems like we need to sum over the batch because we can only apply gradient to a scalar,",1
v0.14.1,TODO: any way to get this to work on batches of arbitrary size?,1
v0.14.1,TODO: prel_model_effect could allow sample_var and freq_weight?,1
v0.14.1,TODO: support freq_weight and sample_var in debiased lasso,1
v0.14.1,TODO: do correct adjustment for sample_var,1
v0.14.1,TODO: is it right that the effective number of intruments is the,1
v0.14.1,TODO: this utility is documented but internal; reimplement?,1
v0.14.1,TODO: this utility is even less public...,1
v0.14.1,TODO: remove once older sklearn support is no longer needed,1
v0.14.1,"the transformation logic here is somewhat tricky; we always need to encode the categorical columns,",1
v0.14.1,TODO: we can't currently handle unseen values of the feature column when getting the effect;,1
v0.14.1,HACK: this is slightly ugly because we rely on the fact that DML passes [X;W] to the first stage models,1
v0.14.1,TODO: consider addding an API to DML that allows for better understanding of how the nuisance inputs are,1
v0.14.1,"TODO: Add other nuisance model options, such as {'azure_automl', 'forests', 'boosting'} that will use particular",1
v0.14.1,"TODO: Add other heterogeneity model options, such as {'automl'} for performing",1
v0.14.1,TODO: Enable multi-class classification (post-MVP),1
v0.14.1,TODO: check compatibility of X and Y lengths,1
v0.14.1,"TODO: bail out also if categorical columns, classification, random_state changed?",1
v0.14.1,TODO: should we also train a new model_y under any circumstances when warm_start is True?,1
v0.14.1,"don't train a model, but suggest workaround since there are enough instances of least",1
v0.14.1,TODO: enrich outcome logic for multi-class classification when that is supported,1
v0.14.1,TODO: Note that there's no column metadata for the sample number - should there be?,1
v0.14.1,TODO: it seems like it would be better to just return the tree itself rather than plot it;,1
v0.14.1,TODO: it seems like it would be better to just return the tree itself rather than plot it;,1
v0.14.1,TODO: conisder working around relying on sklearn implementation details,1
v0.14.1,TODO: consider working around relying on sklearn implementation details,1
v0.14.1,"TODO: once we drop support for sklearn < 1.0, we can remove this",1
v0.14.1,"TODO: Add other types of covariance estimation (e.g. Newey-West (HAC), HC2, HC3)",1
v0.14.1,TODO: consider working around relying on sklearn implementation details,1
v0.14.1,TODO: Check performance,1
v0.14.1,Need to redefine fit here for auto inference to work due to a quirk in how,1
v0.14.0,TODO: enable type aliases,1
v0.14.0,-- Options for todo extension ----------------------------------------------,1
v0.14.0,"If true, `todo` and `todoList` produce output, else they produce nothing.",1
v0.14.0,TODO. Deal with multi-class instrument,1
v0.14.0,TODO. Deal with multi-class instrument,1
v0.14.0,TODO. The solution below is not really a valid cross-fitting,1
v0.14.0,"HACK: DRIV doesn't expect a treatment dimension, so pretend we got a vector even if we really had a one-column array",1
v0.14.0,"Once multiple treatments are supported, we'll need to fix this",1
v0.14.0,TODO. Deal with multi-class instrument/treatment,1
v0.14.0,TODO: any way to avoid creating a copy if the array was already dense?,1
v0.14.0,TODO: might be faster to break into connected components first,1
v0.14.0,TODO: Consider investigating other performance ideas for these cases,1
v0.14.0,TODO: would using einsum's paths to optimize the order of merging help?,1
v0.14.0,HACK: We're relying on some of sklearn's non-public classes which are not completely stable.,1
v0.14.0,"HACK: it's not optimal to use a regex like this, but the base class's node_to_str doesn't expose any",1
v0.14.0,TODO. Create our own color pallete for multiple treatments. The one below is for binary treatments.,1
v0.14.0,"TODO: what if input is sparse? - there's no equivalent to einsum,",1
v0.14.0,TODO Share some logic with non-discrete version,1
v0.14.0,TODO: support freq_weight and sample_var in debiased lasso,1
v0.14.0,"TODO: consider whether we need more care around stateful featurizers,",1
v0.14.0,TODO: support freq_weight and sample_var in debiased lasso,1
v0.14.0,"TODO: consider whether we need more care around stateful featurizers,",1
v0.14.0,"TODO: consider whether we need more care around stateful featurizers,",1
v0.14.0,TODO: Add a __dir__ implementation?,1
v0.14.0,TODO: studentized bootstrap? this would be more accurate in most cases but can we avoid,1
v0.14.0,TODO: support freq_weight and sample_var,1
v0.14.0,TODO: update docs,1
v0.14.0,Doesn't work with CNTK backend as of 2018-07-17 - see https://github.com/keras-team/keras/issues/10715,1
v0.14.0,For some reason this doesn't work at all when run against the CNTK backend...,1
v0.14.0,TODO: test that the estimated effect is usually within the bounds,1
v0.14.0,TODO: test that the estimated effect is usually within the bounds,1
v0.14.0,TODO: set up proper flag for this,1
v0.14.0,TODO: test something rather than just print...,1
v0.14.0,TODO: this tests that we can run the method; how do we test that the results are reasonable?,1
v0.14.0,HACK: work around bug in assertWarns (https://bugs.python.org/issue29620),1
v0.14.0,TODO: serializing/deserializing for every combination -- is this necessary?,1
v0.14.0,does not work well with parallelism.,1
v0.14.0,does not work well with parallelism.,1
v0.14.0,TODO: does this imply we should change some defaults to make this more likely to succeed?,1
v0.14.0,TODO: does this imply we should change some defaults to make this more likely to succeed?,1
v0.14.0,"fourth column: 5 ones, also not enough but barely works even with forest heterogeneity",1
v0.14.0,TODO: we don't recover the correct values with enough accuracy to enable this assertion,1
v0.14.0,is there a different way to verify that we are learning the correct coefficients?,1
v0.14.0,"TODO There is a matrix dimension mismatch between multiple outcome and single outcome, should solve that",1
v0.14.0,"TODO: make sure that the public API consistently uses ""T"" instead of ""P"" for the treatment",1
v0.14.0,TODO: does the numeric stability actually make any difference?,1
v0.14.0,"CNTK backend can't randomize across batches and doesn't implement cumsum (at least as of June 2018,",1
v0.14.0,TODO: is there a more robust way to do this?,1
v0.14.0,TODO: do we need to give the user more control over other arguments to fit?,1
v0.14.0,TODO: do we need to give the user more control over other arguments to fit?,1
v0.14.0,"TODO: it seems like we need to sum over the batch because we can only apply gradient to a scalar,",1
v0.14.0,TODO: any way to get this to work on batches of arbitrary size?,1
v0.14.0,TODO: prel_model_effect could allow sample_var and freq_weight?,1
v0.14.0,TODO: support freq_weight and sample_var in debiased lasso,1
v0.14.0,TODO: do correct adjustment for sample_var,1
v0.14.0,TODO: is it right that the effective number of intruments is the,1
v0.14.0,TODO: this utility is documented but internal; reimplement?,1
v0.14.0,TODO: this utility is even less public...,1
v0.14.0,"the transformation logic here is somewhat tricky; we always need to encode the categorical columns,",1
v0.14.0,TODO: we can't currently handle unseen values of the feature column when getting the effect;,1
v0.14.0,HACK: this is slightly ugly because we rely on the fact that DML passes [X;W] to the first stage models,1
v0.14.0,TODO: consider addding an API to DML that allows for better understanding of how the nuisance inputs are,1
v0.14.0,"TODO: Add other nuisance model options, such as {'azure_automl', 'forests', 'boosting'} that will use particular",1
v0.14.0,"TODO: Add other heterogeneity model options, such as {'automl'} for performing",1
v0.14.0,TODO: Enable multi-class classification (post-MVP),1
v0.14.0,TODO: check compatibility of X and Y lengths,1
v0.14.0,"TODO: bail out also if categorical columns, classification, random_state changed?",1
v0.14.0,TODO: should we also train a new model_y under any circumstances when warm_start is True?,1
v0.14.0,"don't train a model, but suggest workaround since there are enough instances of least",1
v0.14.0,TODO: enrich outcome logic for multi-class classification when that is supported,1
v0.14.0,TODO: Note that there's no column metadata for the sample number - should there be?,1
v0.14.0,TODO: it seems like it would be better to just return the tree itself rather than plot it;,1
v0.14.0,TODO: it seems like it would be better to just return the tree itself rather than plot it;,1
v0.14.0,TODO: conisder working around relying on sklearn implementation details,1
v0.14.0,TODO: consider working around relying on sklearn implementation details,1
v0.14.0,"TODO: once we drop support for sklearn < 1.0, we can remove this",1
v0.14.0,"TODO: Add other types of covariance estimation (e.g. Newey-West (HAC), HC2, HC3)",1
v0.14.0,TODO: consider working around relying on sklearn implementation details,1
v0.14.0,TODO: Check performance,1
v0.14.0,Need to redefine fit here for auto inference to work due to a quirk in how,1
v0.13.1,-- Options for todo extension ----------------------------------------------,1
v0.13.1,"If true, `todo` and `todoList` produce output, else they produce nothing.",1
v0.13.1,TODO. Deal with multi-class instrument,1
v0.13.1,TODO. Deal with multi-class instrument,1
v0.13.1,TODO. The solution below is not really a valid cross-fitting,1
v0.13.1,"HACK: DRIV doesn't expect a treatment dimension, so pretend we got a vector even if we really had a one-column array",1
v0.13.1,"Once multiple treatments are supported, we'll need to fix this",1
v0.13.1,TODO. Deal with multi-class instrument/treatment,1
v0.13.1,TODO: any way to avoid creating a copy if the array was already dense?,1
v0.13.1,TODO: might be faster to break into connected components first,1
v0.13.1,TODO: Consider investigating other performance ideas for these cases,1
v0.13.1,TODO: would using einsum's paths to optimize the order of merging help?,1
v0.13.1,HACK: We're relying on some of sklearn's non-public classes which are not completely stable.,1
v0.13.1,"HACK: it's not optimal to use a regex like this, but the base class's node_to_str doesn't expose any",1
v0.13.1,TODO. Create our own color pallete for multiple treatments. The one below is for binary treatments.,1
v0.13.1,"TODO: what if input is sparse? - there's no equivalent to einsum,",1
v0.13.1,TODO Share some logic with non-discrete version,1
v0.13.1,TODO: support freq_weight and sample_var in debiased lasso,1
v0.13.1,"TODO: consider whether we need more care around stateful featurizers,",1
v0.13.1,TODO: support freq_weight and sample_var in debiased lasso,1
v0.13.1,"TODO: consider whether we need more care around stateful featurizers,",1
v0.13.1,"TODO: consider whether we need more care around stateful featurizers,",1
v0.13.1,TODO: Add a __dir__ implementation?,1
v0.13.1,TODO: studentized bootstrap? this would be more accurate in most cases but can we avoid,1
v0.13.1,TODO: support freq_weight and sample_var,1
v0.13.1,Doesn't work with CNTK backend as of 2018-07-17 - see https://github.com/keras-team/keras/issues/10715,1
v0.13.1,For some reason this doesn't work at all when run against the CNTK backend...,1
v0.13.1,TODO: test that the estimated effect is usually within the bounds,1
v0.13.1,TODO: test that the estimated effect is usually within the bounds,1
v0.13.1,TODO: set up proper flag for this,1
v0.13.1,TODO: test something rather than just print...,1
v0.13.1,TODO: this tests that we can run the method; how do we test that the results are reasonable?,1
v0.13.1,HACK: work around bug in assertWarns (https://bugs.python.org/issue29620),1
v0.13.1,TODO: serializing/deserializing for every combination -- is this necessary?,1
v0.13.1,does not work well with parallelism.,1
v0.13.1,does not work well with parallelism.,1
v0.13.1,TODO: does this imply we should change some defaults to make this more likely to succeed?,1
v0.13.1,TODO: does this imply we should change some defaults to make this more likely to succeed?,1
v0.13.1,"fourth column: 5 ones, also not enough but barely works even with forest heterogeneity",1
v0.13.1,TODO: we don't recover the correct values with enough accuracy to enable this assertion,1
v0.13.1,is there a different way to verify that we are learning the correct coefficients?,1
v0.13.1,"TODO There is a matrix dimension mismatch between multiple outcome and single outcome, should solve that",1
v0.13.1,"TODO: make sure that the public API consistently uses ""T"" instead of ""P"" for the treatment",1
v0.13.1,TODO: does the numeric stability actually make any difference?,1
v0.13.1,"CNTK backend can't randomize across batches and doesn't implement cumsum (at least as of June 2018,",1
v0.13.1,TODO: is there a more robust way to do this?,1
v0.13.1,TODO: do we need to give the user more control over other arguments to fit?,1
v0.13.1,TODO: do we need to give the user more control over other arguments to fit?,1
v0.13.1,"TODO: it seems like we need to sum over the batch because we can only apply gradient to a scalar,",1
v0.13.1,TODO: any way to get this to work on batches of arbitrary size?,1
v0.13.1,TODO: prel_model_effect could allow sample_var and freq_weight?,1
v0.13.1,TODO: support freq_weight and sample_var in debiased lasso,1
v0.13.1,TODO: do correct adjustment for sample_var,1
v0.13.1,TODO: is it right that the effective number of intruments is the,1
v0.13.1,TODO: this utility is documented but internal; reimplement?,1
v0.13.1,TODO: this utility is even less public...,1
v0.13.1,"the transformation logic here is somewhat tricky; we always need to encode the categorical columns,",1
v0.13.1,TODO: we can't currently handle unseen values of the feature column when getting the effect;,1
v0.13.1,HACK: this is slightly ugly because we rely on the fact that DML passes [X;W] to the first stage models,1
v0.13.1,TODO: consider addding an API to DML that allows for better understanding of how the nuisance inputs are,1
v0.13.1,"TODO: Add other nuisance model options, such as {'azure_automl', 'forests', 'boosting'} that will use particular",1
v0.13.1,"TODO: Add other heterogeneity model options, such as {'automl'} for performing",1
v0.13.1,TODO: Enable multi-class classification (post-MVP),1
v0.13.1,TODO: check compatibility of X and Y lengths,1
v0.13.1,"TODO: bail out also if categorical columns, classification, random_state changed?",1
v0.13.1,TODO: should we also train a new model_y under any circumstances when warm_start is True?,1
v0.13.1,"don't train a model, but suggest workaround since there are enough instances of least",1
v0.13.1,TODO: enrich outcome logic for multi-class classification when that is supported,1
v0.13.1,TODO: Note that there's no column metadata for the sample number - should there be?,1
v0.13.1,TODO: it seems like it would be better to just return the tree itself rather than plot it;,1
v0.13.1,TODO: it seems like it would be better to just return the tree itself rather than plot it;,1
v0.13.1,TODO: conisder working around relying on sklearn implementation details,1
v0.13.1,TODO: consider working around relying on sklearn implementation details,1
v0.13.1,"TODO: once we drop support for sklearn < 1.0, we can remove this",1
v0.13.1,"TODO: Add other types of covariance estimation (e.g. Newey-West (HAC), HC2, HC3)",1
v0.13.1,TODO: consider working around relying on sklearn implementation details,1
v0.13.1,TODO: Check performance,1
v0.13.1,Need to redefine fit here for auto inference to work due to a quirk in how,1
v0.13.1,TODO: update docs,1
v0.13.0,-- Options for todo extension ----------------------------------------------,1
v0.13.0,"If true, `todo` and `todoList` produce output, else they produce nothing.",1
v0.13.0,TODO. Deal with multi-class instrument,1
v0.13.0,TODO. Deal with multi-class instrument,1
v0.13.0,TODO. The solution below is not really a valid cross-fitting,1
v0.13.0,"HACK: DRIV doesn't expect a treatment dimension, so pretend we got a vector even if we really had a one-column array",1
v0.13.0,"Once multiple treatments are supported, we'll need to fix this",1
v0.13.0,TODO. Deal with multi-class instrument/treatment,1
v0.13.0,TODO: any way to avoid creating a copy if the array was already dense?,1
v0.13.0,TODO: might be faster to break into connected components first,1
v0.13.0,TODO: Consider investigating other performance ideas for these cases,1
v0.13.0,TODO: would using einsum's paths to optimize the order of merging help?,1
v0.13.0,HACK: We're relying on some of sklearn's non-public classes which are not completely stable.,1
v0.13.0,"HACK: it's not optimal to use a regex like this, but the base class's node_to_str doesn't expose any",1
v0.13.0,TODO. Create our own color pallete for multiple treatments. The one below is for binary treatments.,1
v0.13.0,"TODO: what if input is sparse? - there's no equivalent to einsum,",1
v0.13.0,TODO Share some logic with non-discrete version,1
v0.13.0,TODO: support freq_weight and sample_var in debiased lasso,1
v0.13.0,"TODO: consider whether we need more care around stateful featurizers,",1
v0.13.0,TODO: support freq_weight and sample_var in debiased lasso,1
v0.13.0,"TODO: consider whether we need more care around stateful featurizers,",1
v0.13.0,"TODO: consider whether we need more care around stateful featurizers,",1
v0.13.0,TODO: Add a __dir__ implementation?,1
v0.13.0,TODO: studentized bootstrap? this would be more accurate in most cases but can we avoid,1
v0.13.0,TODO: support freq_weight and sample_var,1
v0.13.0,Doesn't work with CNTK backend as of 2018-07-17 - see https://github.com/keras-team/keras/issues/10715,1
v0.13.0,For some reason this doesn't work at all when run against the CNTK backend...,1
v0.13.0,TODO: test that the estimated effect is usually within the bounds,1
v0.13.0,TODO: test that the estimated effect is usually within the bounds,1
v0.13.0,TODO: set up proper flag for this,1
v0.13.0,TODO: test something rather than just print...,1
v0.13.0,TODO: this tests that we can run the method; how do we test that the results are reasonable?,1
v0.13.0,HACK: work around bug in assertWarns (https://bugs.python.org/issue29620),1
v0.13.0,does not work well with parallelism.,1
v0.13.0,does not work well with parallelism.,1
v0.13.0,TODO: does this imply we should change some defaults to make this more likely to succeed?,1
v0.13.0,TODO: does this imply we should change some defaults to make this more likely to succeed?,1
v0.13.0,"fourth column: 5 ones, also not enough but barely works even with forest heterogeneity",1
v0.13.0,TODO: we don't recover the correct values with enough accuracy to enable this assertion,1
v0.13.0,is there a different way to verify that we are learning the correct coefficients?,1
v0.13.0,"TODO There is a matrix dimension mismatch between multiple outcome and single outcome, should solve that",1
v0.13.0,"TODO: make sure that the public API consistently uses ""T"" instead of ""P"" for the treatment",1
v0.13.0,TODO: does the numeric stability actually make any difference?,1
v0.13.0,"CNTK backend can't randomize across batches and doesn't implement cumsum (at least as of June 2018,",1
v0.13.0,TODO: is there a more robust way to do this?,1
v0.13.0,TODO: do we need to give the user more control over other arguments to fit?,1
v0.13.0,TODO: do we need to give the user more control over other arguments to fit?,1
v0.13.0,"TODO: it seems like we need to sum over the batch because we can only apply gradient to a scalar,",1
v0.13.0,TODO: any way to get this to work on batches of arbitrary size?,1
v0.13.0,TODO: prel_model_effect could allow sample_var and freq_weight?,1
v0.13.0,TODO: support freq_weight and sample_var in debiased lasso,1
v0.13.0,TODO: do correct adjustment for sample_var,1
v0.13.0,TODO: is it right that the effective number of intruments is the,1
v0.13.0,TODO: this utility is documented but internal; reimplement?,1
v0.13.0,TODO: this utility is even less public...,1
v0.13.0,"the transformation logic here is somewhat tricky; we always need to encode the categorical columns,",1
v0.13.0,TODO: we can't currently handle unseen values of the feature column when getting the effect;,1
v0.13.0,HACK: this is slightly ugly because we rely on the fact that DML passes [X;W] to the first stage models,1
v0.13.0,TODO: consider addding an API to DML that allows for better understanding of how the nuisance inputs are,1
v0.13.0,"TODO: Add other nuisance model options, such as {'azure_automl', 'forests', 'boosting'} that will use particular",1
v0.13.0,"TODO: Add other heterogeneity model options, such as {'automl'} for performing",1
v0.13.0,TODO: Enable multi-class classification (post-MVP),1
v0.13.0,TODO: check compatibility of X and Y lengths,1
v0.13.0,"TODO: bail out also if categorical columns, classification, random_state changed?",1
v0.13.0,TODO: should we also train a new model_y under any circumstances when warm_start is True?,1
v0.13.0,"don't train a model, but suggest workaround since there are enough instances of least",1
v0.13.0,TODO: enrich outcome logic for multi-class classification when that is supported,1
v0.13.0,TODO: Note that there's no column metadata for the sample number - should there be?,1
v0.13.0,TODO: it seems like it would be better to just return the tree itself rather than plot it;,1
v0.13.0,TODO: it seems like it would be better to just return the tree itself rather than plot it;,1
v0.13.0,TODO: conisder working around relying on sklearn implementation details,1
v0.13.0,TODO: consider working around relying on sklearn implementation details,1
v0.13.0,"TODO: Add other types of covariance estimation (e.g. Newey-West (HAC), HC2, HC3)",1
v0.13.0,TODO: consider working around relying on sklearn implementation details,1
v0.13.0,TODO: Check performance,1
v0.13.0,Need to redefine fit here for auto inference to work due to a quirk in how,1
v0.13.0,TODO: update docs,1
v0.12.0,-- Options for todo extension ----------------------------------------------,1
v0.12.0,"If true, `todo` and `todoList` produce output, else they produce nothing.",1
v0.12.0,TODO. Deal with multi-class instrument,1
v0.12.0,TODO. Deal with multi-class instrument,1
v0.12.0,TODO. The solution below is not really a valid cross-fitting,1
v0.12.0,"HACK: DRIV doesn't expect a treatment dimension, so pretend we got a vector even if we really had a one-column array",1
v0.12.0,"Once multiple treatments are supported, we'll need to fix this",1
v0.12.0,TODO. Deal with multi-class instrument/treatment,1
v0.12.0,TODO: any way to avoid creating a copy if the array was already dense?,1
v0.12.0,TODO: might be faster to break into connected components first,1
v0.12.0,TODO: Consider investigating other performance ideas for these cases,1
v0.12.0,TODO: would using einsum's paths to optimize the order of merging help?,1
v0.12.0,HACK: We're relying on some of sklearn's non-public classes which are not completely stable.,1
v0.12.0,"HACK: it's not optimal to use a regex like this, but the base class's node_to_str doesn't expose any",1
v0.12.0,TODO. Create our own color pallete for multiple treatments. The one below is for binary treatments.,1
v0.12.0,"TODO: what if input is sparse? - there's no equivalent to einsum,",1
v0.12.0,TODO Share some logic with non-discrete version,1
v0.12.0,TODO: support freq_weight and sample_var in debiased lasso,1
v0.12.0,"TODO: consider whether we need more care around stateful featurizers,",1
v0.12.0,TODO: support freq_weight and sample_var in debiased lasso,1
v0.12.0,"TODO: consider whether we need more care around stateful featurizers,",1
v0.12.0,"TODO: consider whether we need more care around stateful featurizers,",1
v0.12.0,TODO: Add a __dir__ implementation?,1
v0.12.0,TODO: studentized bootstrap? this would be more accurate in most cases but can we avoid,1
v0.12.0,TODO: support freq_weight and sample_var,1
v0.12.0,Doesn't work with CNTK backend as of 2018-07-17 - see https://github.com/keras-team/keras/issues/10715,1
v0.12.0,For some reason this doesn't work at all when run against the CNTK backend...,1
v0.12.0,TODO: test that the estimated effect is usually within the bounds,1
v0.12.0,TODO: test that the estimated effect is usually within the bounds,1
v0.12.0,TODO: set up proper flag for this,1
v0.12.0,TODO: test something rather than just print...,1
v0.12.0,TODO: this tests that we can run the method; how do we test that the results are reasonable?,1
v0.12.0,HACK: work around bug in assertWarns (https://bugs.python.org/issue29620),1
v0.12.0,does not work well with parallelism.,1
v0.12.0,does not work well with parallelism.,1
v0.12.0,TODO: does this imply we should change some defaults to make this more likely to succeed?,1
v0.12.0,TODO: does this imply we should change some defaults to make this more likely to succeed?,1
v0.12.0,"fourth column: 5 ones, also not enough but barely works even with forest heterogeneity",1
v0.12.0,TODO: we don't recover the correct values with enough accuracy to enable this assertion,1
v0.12.0,is there a different way to verify that we are learning the correct coefficients?,1
v0.12.0,"TODO There is a matrix dimension mismatch between multiple outcome and single outcome, should solve that",1
v0.12.0,"TODO: make sure that the public API consistently uses ""T"" instead of ""P"" for the treatment",1
v0.12.0,TODO: does the numeric stability actually make any difference?,1
v0.12.0,"CNTK backend can't randomize across batches and doesn't implement cumsum (at least as of June 2018,",1
v0.12.0,TODO: is there a more robust way to do this?,1
v0.12.0,TODO: do we need to give the user more control over other arguments to fit?,1
v0.12.0,TODO: do we need to give the user more control over other arguments to fit?,1
v0.12.0,"TODO: it seems like we need to sum over the batch because we can only apply gradient to a scalar,",1
v0.12.0,TODO: any way to get this to work on batches of arbitrary size?,1
v0.12.0,TODO: prel_model_effect could allow sample_var and freq_weight?,1
v0.12.0,TODO: support freq_weight and sample_var in debiased lasso,1
v0.12.0,TODO: do correct adjustment for sample_var,1
v0.12.0,TODO: is it right that the effective number of intruments is the,1
v0.12.0,TODO: this utility is documented but internal; reimplement?,1
v0.12.0,TODO: this utility is even less public...,1
v0.12.0,"the transformation logic here is somewhat tricky; we always need to encode the categorical columns,",1
v0.12.0,TODO: we can't currently handle unseen values of the feature column when getting the effect;,1
v0.12.0,HACK: this is slightly ugly because we rely on the fact that DML passes [X;W] to the first stage models,1
v0.12.0,TODO: consider addding an API to DML that allows for better understanding of how the nuisance inputs are,1
v0.12.0,TODO: check compatibility of X and Y lengths,1
v0.12.0,"TODO: bail out also if categorical columns, classification, random_state changed?",1
v0.12.0,TODO: should we also train a new model_y under any circumstances when warm_start is True?,1
v0.12.0,"don't train a model, but suggest workaround since there are enough instances of least",1
v0.12.0,TODO: enrich outcome logic for multi-class classification when that is supported,1
v0.12.0,TODO: Note that there's no column metadata for the sample number - should there be?,1
v0.12.0,TODO: it seems like it would be better to just return the tree itself rather than plot it;,1
v0.12.0,TODO: it seems like it would be better to just return the tree itself rather than plot it;,1
v0.12.0,TODO: conisder working around relying on sklearn implementation details,1
v0.12.0,TODO: consider working around relying on sklearn implementation details,1
v0.12.0,"TODO: Add other types of covariance estimation (e.g. Newey-West (HAC), HC2, HC3)",1
v0.12.0,TODO: consider working around relying on sklearn implementation details,1
v0.12.0,TODO: Check performance,1
v0.12.0,Need to redefine fit here for auto inference to work due to a quirk in how,1
v0.12.0,TODO: update docs,1
v0.12.0b6,-- Options for todo extension ----------------------------------------------,1
v0.12.0b6,"If true, `todo` and `todoList` produce output, else they produce nothing.",1
v0.12.0b6,TODO. Deal with multi-class instrument,1
v0.12.0b6,TODO. Deal with multi-class instrument,1
v0.12.0b6,TODO. The solution below is not really a valid cross-fitting,1
v0.12.0b6,"HACK: DRIV doesn't expect a treatment dimension, so pretend we got a vector even if we really had a one-column array",1
v0.12.0b6,"Once multiple treatments are supported, we'll need to fix this",1
v0.12.0b6,TODO. Deal with multi-class instrument/treatment,1
v0.12.0b6,TODO: any way to avoid creating a copy if the array was already dense?,1
v0.12.0b6,TODO: might be faster to break into connected components first,1
v0.12.0b6,TODO: Consider investigating other performance ideas for these cases,1
v0.12.0b6,TODO: would using einsum's paths to optimize the order of merging help?,1
v0.12.0b6,HACK: We're relying on some of sklearn's non-public classes which are not completely stable.,1
v0.12.0b6,"HACK: it's not optimal to use a regex like this, but the base class's node_to_str doesn't expose any",1
v0.12.0b6,TODO. Create our own color pallete for multiple treatments. The one below is for binary treatments.,1
v0.12.0b6,"TODO: what if input is sparse? - there's no equivalent to einsum,",1
v0.12.0b6,TODO Share some logic with non-discrete version,1
v0.12.0b6,TODO: support freq_weight and sample_var in debiased lasso,1
v0.12.0b6,"TODO: consider whether we need more care around stateful featurizers,",1
v0.12.0b6,TODO: support freq_weight and sample_var in debiased lasso,1
v0.12.0b6,"TODO: consider whether we need more care around stateful featurizers,",1
v0.12.0b6,"TODO: consider whether we need more care around stateful featurizers,",1
v0.12.0b6,TODO: Add a __dir__ implementation?,1
v0.12.0b6,TODO: studentized bootstrap? this would be more accurate in most cases but can we avoid,1
v0.12.0b6,TODO: support freq_weight and sample_var,1
v0.12.0b6,Doesn't work with CNTK backend as of 2018-07-17 - see https://github.com/keras-team/keras/issues/10715,1
v0.12.0b6,For some reason this doesn't work at all when run against the CNTK backend...,1
v0.12.0b6,TODO: test that the estimated effect is usually within the bounds,1
v0.12.0b6,TODO: test that the estimated effect is usually within the bounds,1
v0.12.0b6,TODO: set up proper flag for this,1
v0.12.0b6,TODO: test something rather than just print...,1
v0.12.0b6,TODO: this tests that we can run the method; how do we test that the results are reasonable?,1
v0.12.0b6,TODO: add tests for extra properties like coef_ where they exist,1
v0.12.0b6,TODO: add tests for extra properties like coef_ where they exist,1
v0.12.0b6,TODO: ideally we could also test whether Z and X are jointly okay when both discrete,1
v0.12.0b6,HACK: work around bug in assertWarns (https://bugs.python.org/issue29620),1
v0.12.0b6,does not work well with parallelism.,1
v0.12.0b6,does not work well with parallelism.,1
v0.12.0b6,TODO: does this imply we should change some defaults to make this more likely to succeed?,1
v0.12.0b6,TODO: does this imply we should change some defaults to make this more likely to succeed?,1
v0.12.0b6,"fourth column: 5 ones, also not enough but barely works even with forest heterogeneity",1
v0.12.0b6,"TODO There is a matrix dimension mismatch between multiple outcome and single outcome, should solve that",1
v0.12.0b6,"TODO: make sure that the public API consistently uses ""T"" instead of ""P"" for the treatment",1
v0.12.0b6,TODO: does the numeric stability actually make any difference?,1
v0.12.0b6,"CNTK backend can't randomize across batches and doesn't implement cumsum (at least as of June 2018,",1
v0.12.0b6,TODO: is there a more robust way to do this?,1
v0.12.0b6,TODO: do we need to give the user more control over other arguments to fit?,1
v0.12.0b6,TODO: do we need to give the user more control over other arguments to fit?,1
v0.12.0b6,"TODO: it seems like we need to sum over the batch because we can only apply gradient to a scalar,",1
v0.12.0b6,TODO: any way to get this to work on batches of arbitrary size?,1
v0.12.0b6,"TODO: if opt_reweighted is False, we could change the logic to support multidimensional treatments,",1
v0.12.0b6,TODO: do correct adjustment for sample_var,1
v0.12.0b6,TODO: allow the final model to actually use X? Then we'd need to rename the class,1
v0.12.0b6,TODO: allow the final model to actually use X?,1
v0.12.0b6,TODO: allow the final model to actually use X?,1
v0.12.0b6,TODO: would it be useful to extend to handle controls ala vanilla DML?,1
v0.12.0b6,TODO: is it right that the effective number of intruments is the,1
v0.12.0b6,TODO: this utility is documented but internal; reimplement?,1
v0.12.0b6,TODO: this utility is even less public...,1
v0.12.0b6,TODO: we can't currently handle unseen values of the feature column when getting the effect;,1
v0.12.0b6,TODO: check compatibility of X and Y lengths,1
v0.12.0b6,"TODO: bail out also if categorical columns, classification, random_state changed?",1
v0.12.0b6,TODO: should we also train a new model_y under any circumstances when warm_start is True?,1
v0.12.0b6,"don't train a model, but suggest workaround since there are enough instances of least",1
v0.12.0b6,TODO: enrich outcome logic for multi-class classification when that is supported,1
v0.12.0b6,TODO: Note that there's no column metadata for the sample number - should there be?,1
v0.12.0b6,TODO: it seems like it would be better to just return the tree itself rather than plot it;,1
v0.12.0b6,TODO: it seems like it would be better to just return the tree itself rather than plot it;,1
v0.12.0b6,TODO: conisder working around relying on sklearn implementation details,1
v0.12.0b6,TODO: consider working around relying on sklearn implementation details,1
v0.12.0b6,"TODO: Add other types of covariance estimation (e.g. Newey-West (HAC), HC2, HC3)",1
v0.12.0b6,TODO: consider working around relying on sklearn implementation details,1
v0.12.0b6,TODO: Check performance,1
v0.12.0b6,Need to redefine fit here for auto inference to work due to a quirk in how,1
v0.12.0b5,-- Options for todo extension ----------------------------------------------,1
v0.12.0b5,"If true, `todo` and `todoList` produce output, else they produce nothing.",1
v0.12.0b5,TODO. Deal with multi-class instrument,1
v0.12.0b5,TODO. Deal with multi-class instrument,1
v0.12.0b5,TODO. The solution below is not really a valid cross-fitting,1
v0.12.0b5,"HACK: DRIV doesn't expect a treatment dimension, so pretend we got a vector even if we really had a one-column array",1
v0.12.0b5,"Once multiple treatments are supported, we'll need to fix this",1
v0.12.0b5,TODO. Deal with multi-class instrument/treatment,1
v0.12.0b5,TODO: any way to avoid creating a copy if the array was already dense?,1
v0.12.0b5,TODO: might be faster to break into connected components first,1
v0.12.0b5,TODO: Consider investigating other performance ideas for these cases,1
v0.12.0b5,TODO: would using einsum's paths to optimize the order of merging help?,1
v0.12.0b5,HACK: We're relying on some of sklearn's non-public classes which are not completely stable.,1
v0.12.0b5,"HACK: it's not optimal to use a regex like this, but the base class's node_to_str doesn't expose any",1
v0.12.0b5,TODO. Create our own color pallete for multiple treatments. The one below is for binary treatments.,1
v0.12.0b5,"TODO: what if input is sparse? - there's no equivalent to einsum,",1
v0.12.0b5,TODO Share some logic with non-discrete version,1
v0.12.0b5,TODO: support freq_weight and sample_var in debiased lasso,1
v0.12.0b5,"TODO: consider whether we need more care around stateful featurizers,",1
v0.12.0b5,TODO: support freq_weight and sample_var in debiased lasso,1
v0.12.0b5,"TODO: consider whether we need more care around stateful featurizers,",1
v0.12.0b5,"TODO: consider whether we need more care around stateful featurizers,",1
v0.12.0b5,TODO: Add a __dir__ implementation?,1
v0.12.0b5,TODO: studentized bootstrap? this would be more accurate in most cases but can we avoid,1
v0.12.0b5,TODO: support freq_weight and sample_var,1
v0.12.0b5,Doesn't work with CNTK backend as of 2018-07-17 - see https://github.com/keras-team/keras/issues/10715,1
v0.12.0b5,For some reason this doesn't work at all when run against the CNTK backend...,1
v0.12.0b5,TODO: test that the estimated effect is usually within the bounds,1
v0.12.0b5,TODO: test that the estimated effect is usually within the bounds,1
v0.12.0b5,TODO: set up proper flag for this,1
v0.12.0b5,TODO: test something rather than just print...,1
v0.12.0b5,TODO: this tests that we can run the method; how do we test that the results are reasonable?,1
v0.12.0b5,TODO: add tests for extra properties like coef_ where they exist,1
v0.12.0b5,TODO: add tests for extra properties like coef_ where they exist,1
v0.12.0b5,TODO: ideally we could also test whether Z and X are jointly okay when both discrete,1
v0.12.0b5,HACK: work around bug in assertWarns (https://bugs.python.org/issue29620),1
v0.12.0b5,does not work well with parallelism.,1
v0.12.0b5,does not work well with parallelism.,1
v0.12.0b5,TODO: does this imply we should change some defaults to make this more likely to succeed?,1
v0.12.0b5,TODO: does this imply we should change some defaults to make this more likely to succeed?,1
v0.12.0b5,"fourth column: 5 ones, also not enough but barely works even with forest heterogeneity",1
v0.12.0b5,"TODO There is a matrix dimension mismatch between multiple outcome and single outcome, should solve that",1
v0.12.0b5,"TODO: make sure that the public API consistently uses ""T"" instead of ""P"" for the treatment",1
v0.12.0b5,TODO: does the numeric stability actually make any difference?,1
v0.12.0b5,"CNTK backend can't randomize across batches and doesn't implement cumsum (at least as of June 2018,",1
v0.12.0b5,TODO: is there a more robust way to do this?,1
v0.12.0b5,TODO: do we need to give the user more control over other arguments to fit?,1
v0.12.0b5,TODO: do we need to give the user more control over other arguments to fit?,1
v0.12.0b5,"TODO: it seems like we need to sum over the batch because we can only apply gradient to a scalar,",1
v0.12.0b5,TODO: any way to get this to work on batches of arbitrary size?,1
v0.12.0b5,"TODO: if opt_reweighted is False, we could change the logic to support multidimensional treatments,",1
v0.12.0b5,TODO: do correct adjustment for sample_var,1
v0.12.0b5,TODO: allow the final model to actually use X? Then we'd need to rename the class,1
v0.12.0b5,TODO: allow the final model to actually use X?,1
v0.12.0b5,TODO: allow the final model to actually use X?,1
v0.12.0b5,TODO: would it be useful to extend to handle controls ala vanilla DML?,1
v0.12.0b5,TODO: is it right that the effective number of intruments is the,1
v0.12.0b5,TODO: this utility is documented but internal; reimplement?,1
v0.12.0b5,TODO: this utility is even less public...,1
v0.12.0b5,TODO: we can't currently handle unseen values of the feature column when getting the effect;,1
v0.12.0b5,TODO: check compatibility of X and Y lengths,1
v0.12.0b5,"TODO: bail out also if categorical columns, classification, random_state changed?",1
v0.12.0b5,TODO: should we also train a new model_y under any circumstances when warm_start is True?,1
v0.12.0b5,"don't train a model, but suggest workaround since there are enough instances of least",1
v0.12.0b5,TODO: enrich outcome logic for multi-class classification when that is supported,1
v0.12.0b5,TODO: Note that there's no column metadata for the sample number - should there be?,1
v0.12.0b5,TODO: it seems like it would be better to just return the tree itself rather than plot it;,1
v0.12.0b5,TODO: it seems like it would be better to just return the tree itself rather than plot it;,1
v0.12.0b5,TODO: conisder working around relying on sklearn implementation details,1
v0.12.0b5,TODO: consider working around relying on sklearn implementation details,1
v0.12.0b5,"TODO: Add other types of covariance estimation (e.g. Newey-West (HAC), HC2, HC3)",1
v0.12.0b5,TODO: consider working around relying on sklearn implementation details,1
v0.12.0b5,TODO: Check performance,1
v0.12.0b5,Need to redefine fit here for auto inference to work due to a quirk in how,1
v0.12.0b4,-- Options for todo extension ----------------------------------------------,1
v0.12.0b4,"If true, `todo` and `todoList` produce output, else they produce nothing.",1
v0.12.0b4,TODO. Deal with multi-class instrument,1
v0.12.0b4,TODO. Deal with multi-class instrument,1
v0.12.0b4,TODO. The solution below is not really a valid cross-fitting,1
v0.12.0b4,"HACK: DRIV doesn't expect a treatment dimension, so pretend we got a vector even if we really had a one-column array",1
v0.12.0b4,"Once multiple treatments are supported, we'll need to fix this",1
v0.12.0b4,TODO. Deal with multi-class instrument/treatment,1
v0.12.0b4,TODO: any way to avoid creating a copy if the array was already dense?,1
v0.12.0b4,TODO: might be faster to break into connected components first,1
v0.12.0b4,TODO: Consider investigating other performance ideas for these cases,1
v0.12.0b4,TODO: would using einsum's paths to optimize the order of merging help?,1
v0.12.0b4,HACK: We're relying on some of sklearn's non-public classes which are not completely stable.,1
v0.12.0b4,"HACK: it's not optimal to use a regex like this, but the base class's node_to_str doesn't expose any",1
v0.12.0b4,TODO. Create our own color pallete for multiple treatments. The one below is for binary treatments.,1
v0.12.0b4,"TODO: what if input is sparse? - there's no equivalent to einsum,",1
v0.12.0b4,TODO Share some logic with non-discrete version,1
v0.12.0b4,TODO: support freq_weight and sample_var in debiased lasso,1
v0.12.0b4,"TODO: consider whether we need more care around stateful featurizers,",1
v0.12.0b4,TODO: support freq_weight and sample_var in debiased lasso,1
v0.12.0b4,"TODO: consider whether we need more care around stateful featurizers,",1
v0.12.0b4,"TODO: consider whether we need more care around stateful featurizers,",1
v0.12.0b4,TODO: Add a __dir__ implementation?,1
v0.12.0b4,TODO: studentized bootstrap? this would be more accurate in most cases but can we avoid,1
v0.12.0b4,TODO: support freq_weight and sample_var,1
v0.12.0b4,Doesn't work with CNTK backend as of 2018-07-17 - see https://github.com/keras-team/keras/issues/10715,1
v0.12.0b4,For some reason this doesn't work at all when run against the CNTK backend...,1
v0.12.0b4,TODO: test that the estimated effect is usually within the bounds,1
v0.12.0b4,TODO: test that the estimated effect is usually within the bounds,1
v0.12.0b4,TODO: set up proper flag for this,1
v0.12.0b4,TODO: test something rather than just print...,1
v0.12.0b4,TODO: this tests that we can run the method; how do we test that the results are reasonable?,1
v0.12.0b4,TODO: add tests for extra properties like coef_ where they exist,1
v0.12.0b4,TODO: add tests for extra properties like coef_ where they exist,1
v0.12.0b4,TODO: ideally we could also test whether Z and X are jointly okay when both discrete,1
v0.12.0b4,HACK: work around bug in assertWarns (https://bugs.python.org/issue29620),1
v0.12.0b4,does not work well with parallelism.,1
v0.12.0b4,does not work well with parallelism.,1
v0.12.0b4,TODO: does this imply we should change some defaults to make this more likely to succeed?,1
v0.12.0b4,TODO: does this imply we should change some defaults to make this more likely to succeed?,1
v0.12.0b4,"fourth column: 5 ones, also not enough but barely works even with forest heterogeneity",1
v0.12.0b4,"TODO There is a matrix dimension mismatch between multiple outcome and single outcome, should solve that",1
v0.12.0b4,"TODO: make sure that the public API consistently uses ""T"" instead of ""P"" for the treatment",1
v0.12.0b4,TODO: does the numeric stability actually make any difference?,1
v0.12.0b4,"CNTK backend can't randomize across batches and doesn't implement cumsum (at least as of June 2018,",1
v0.12.0b4,TODO: is there a more robust way to do this?,1
v0.12.0b4,TODO: do we need to give the user more control over other arguments to fit?,1
v0.12.0b4,TODO: do we need to give the user more control over other arguments to fit?,1
v0.12.0b4,"TODO: it seems like we need to sum over the batch because we can only apply gradient to a scalar,",1
v0.12.0b4,TODO: any way to get this to work on batches of arbitrary size?,1
v0.12.0b4,"TODO: if opt_reweighted is False, we could change the logic to support multidimensional treatments,",1
v0.12.0b4,TODO: do correct adjustment for sample_var,1
v0.12.0b4,TODO: allow the final model to actually use X? Then we'd need to rename the class,1
v0.12.0b4,TODO: allow the final model to actually use X?,1
v0.12.0b4,TODO: allow the final model to actually use X?,1
v0.12.0b4,TODO: would it be useful to extend to handle controls ala vanilla DML?,1
v0.12.0b4,TODO: is it right that the effective number of intruments is the,1
v0.12.0b4,TODO: this utility is documented but internal; reimplement?,1
v0.12.0b4,TODO: this utility is even less public...,1
v0.12.0b4,TODO: we can't currently handle unseen values of the feature column when getting the effect;,1
v0.12.0b4,TODO: check compatibility of X and Y lengths,1
v0.12.0b4,"TODO: bail out also if categorical columns, classification, random_state changed?",1
v0.12.0b4,TODO: should we also train a new model_y under any circumstances when warm_start is True?,1
v0.12.0b4,"don't train a model, but suggest workaround since there are enough instances of least",1
v0.12.0b4,TODO: enrich outcome logic for multi-class classification when that is supported,1
v0.12.0b4,TODO: Note that there's no column metadata for the sample number - should there be?,1
v0.12.0b4,TODO: it seems like it would be better to just return the tree itself rather than plot it;,1
v0.12.0b4,TODO: it seems like it would be better to just return the tree itself rather than plot it;,1
v0.12.0b4,TODO: conisder working around relying on sklearn implementation details,1
v0.12.0b4,TODO: consider working around relying on sklearn implementation details,1
v0.12.0b4,"TODO: Add other types of covariance estimation (e.g. Newey-West (HAC), HC2, HC3)",1
v0.12.0b4,TODO: consider working around relying on sklearn implementation details,1
v0.12.0b4,TODO: Check performance,1
v0.12.0b4,Need to redefine fit here for auto inference to work due to a quirk in how,1
v0.12.0b3,-- Options for todo extension ----------------------------------------------,1
v0.12.0b3,"If true, `todo` and `todoList` produce output, else they produce nothing.",1
v0.12.0b3,TODO. Deal with multi-class instrument,1
v0.12.0b3,TODO. Deal with multi-class instrument,1
v0.12.0b3,TODO. The solution below is not really a valid cross-fitting,1
v0.12.0b3,"HACK: DRIV doesn't expect a treatment dimension, so pretend we got a vector even if we really had a one-column array",1
v0.12.0b3,"Once multiple treatments are supported, we'll need to fix this",1
v0.12.0b3,TODO. Deal with multi-class instrument/treatment,1
v0.12.0b3,TODO: any way to avoid creating a copy if the array was already dense?,1
v0.12.0b3,TODO: might be faster to break into connected components first,1
v0.12.0b3,TODO: Consider investigating other performance ideas for these cases,1
v0.12.0b3,TODO: would using einsum's paths to optimize the order of merging help?,1
v0.12.0b3,HACK: We're relying on some of sklearn's non-public classes which are not completely stable.,1
v0.12.0b3,"HACK: it's not optimal to use a regex like this, but the base class's node_to_str doesn't expose any",1
v0.12.0b3,TODO. Create our own color pallete for multiple treatments. The one below is for binary treatments.,1
v0.12.0b3,"TODO: what if input is sparse? - there's no equivalent to einsum,",1
v0.12.0b3,TODO Share some logic with non-discrete version,1
v0.12.0b3,TODO: support freq_weight and sample_var in debiased lasso,1
v0.12.0b3,"TODO: consider whether we need more care around stateful featurizers,",1
v0.12.0b3,TODO: support freq_weight and sample_var in debiased lasso,1
v0.12.0b3,"TODO: consider whether we need more care around stateful featurizers,",1
v0.12.0b3,"TODO: consider whether we need more care around stateful featurizers,",1
v0.12.0b3,TODO: Add a __dir__ implementation?,1
v0.12.0b3,TODO: studentized bootstrap? this would be more accurate in most cases but can we avoid,1
v0.12.0b3,TODO: support freq_weight and sample_var,1
v0.12.0b3,Doesn't work with CNTK backend as of 2018-07-17 - see https://github.com/keras-team/keras/issues/10715,1
v0.12.0b3,For some reason this doesn't work at all when run against the CNTK backend...,1
v0.12.0b3,TODO: test that the estimated effect is usually within the bounds,1
v0.12.0b3,TODO: test that the estimated effect is usually within the bounds,1
v0.12.0b3,TODO: set up proper flag for this,1
v0.12.0b3,TODO: test something rather than just print...,1
v0.12.0b3,TODO: this tests that we can run the method; how do we test that the results are reasonable?,1
v0.12.0b3,TODO: add tests for extra properties like coef_ where they exist,1
v0.12.0b3,TODO: add tests for extra properties like coef_ where they exist,1
v0.12.0b3,TODO: ideally we could also test whether Z and X are jointly okay when both discrete,1
v0.12.0b3,HACK: work around bug in assertWarns (https://bugs.python.org/issue29620),1
v0.12.0b3,does not work well with parallelism.,1
v0.12.0b3,does not work well with parallelism.,1
v0.12.0b3,TODO: does this imply we should change some defaults to make this more likely to succeed?,1
v0.12.0b3,TODO: does this imply we should change some defaults to make this more likely to succeed?,1
v0.12.0b3,"fourth column: 5 ones, also not enough but barely works even with forest heterogeneity",1
v0.12.0b3,"TODO There is a matrix dimension mismatch between multiple outcome and single outcome, should solve that",1
v0.12.0b3,"TODO: make sure that the public API consistently uses ""T"" instead of ""P"" for the treatment",1
v0.12.0b3,TODO: does the numeric stability actually make any difference?,1
v0.12.0b3,"CNTK backend can't randomize across batches and doesn't implement cumsum (at least as of June 2018,",1
v0.12.0b3,TODO: is there a more robust way to do this?,1
v0.12.0b3,TODO: do we need to give the user more control over other arguments to fit?,1
v0.12.0b3,TODO: do we need to give the user more control over other arguments to fit?,1
v0.12.0b3,"TODO: it seems like we need to sum over the batch because we can only apply gradient to a scalar,",1
v0.12.0b3,TODO: any way to get this to work on batches of arbitrary size?,1
v0.12.0b3,"TODO: if opt_reweighted is False, we could change the logic to support multidimensional treatments,",1
v0.12.0b3,TODO: do correct adjustment for sample_var,1
v0.12.0b3,TODO: allow the final model to actually use X? Then we'd need to rename the class,1
v0.12.0b3,TODO: allow the final model to actually use X?,1
v0.12.0b3,TODO: allow the final model to actually use X?,1
v0.12.0b3,TODO: would it be useful to extend to handle controls ala vanilla DML?,1
v0.12.0b3,TODO: is it right that the effective number of intruments is the,1
v0.12.0b3,TODO: this utility is documented but internal; reimplement?,1
v0.12.0b3,TODO: this utility is even less public...,1
v0.12.0b3,TODO: we can't currently handle unseen values of the feature column when getting the effect;,1
v0.12.0b3,TODO: check compatibility of X and Y lengths,1
v0.12.0b3,"TODO: bail out also if categorical columns, classification, random_state changed?",1
v0.12.0b3,TODO: should we also train a new model_y under any circumstances when warm_start is True?,1
v0.12.0b3,"don't train a model, but suggest workaround since there are enough instances of least",1
v0.12.0b3,TODO: enrich outcome logic for multi-class classification when that is supported,1
v0.12.0b3,TODO: it seems like it would be better to just return the tree itself rather than plot it;,1
v0.12.0b3,TODO: it seems like it would be better to just return the tree itself rather than plot it;,1
v0.12.0b3,TODO: conisder working around relying on sklearn implementation details,1
v0.12.0b3,TODO: consider working around relying on sklearn implementation details,1
v0.12.0b3,"TODO: Add other types of covariance estimation (e.g. Newey-West (HAC), HC2, HC3)",1
v0.12.0b3,TODO: consider working around relying on sklearn implementation details,1
v0.12.0b3,TODO: Check performance,1
v0.12.0b3,Need to redefine fit here for auto inference to work due to a quirk in how,1
v0.12.0b2,-- Options for todo extension ----------------------------------------------,1
v0.12.0b2,"If true, `todo` and `todoList` produce output, else they produce nothing.",1
v0.12.0b2,TODO. Deal with multi-class instrument,1
v0.12.0b2,TODO. Deal with multi-class instrument,1
v0.12.0b2,TODO. The solution below is not really a valid cross-fitting,1
v0.12.0b2,"HACK: DRIV doesn't expect a treatment dimension, so pretend we got a vector even if we really had a one-column array",1
v0.12.0b2,"Once multiple treatments are supported, we'll need to fix this",1
v0.12.0b2,TODO. Deal with multi-class instrument/treatment,1
v0.12.0b2,TODO: any way to avoid creating a copy if the array was already dense?,1
v0.12.0b2,TODO: might be faster to break into connected components first,1
v0.12.0b2,TODO: Consider investigating other performance ideas for these cases,1
v0.12.0b2,TODO: would using einsum's paths to optimize the order of merging help?,1
v0.12.0b2,HACK: We're relying on some of sklearn's non-public classes which are not completely stable.,1
v0.12.0b2,"HACK: it's not optimal to use a regex like this, but the base class's node_to_str doesn't expose any",1
v0.12.0b2,TODO. Create our own color pallete for multiple treatments. The one below is for binary treatments.,1
v0.12.0b2,"TODO: what if input is sparse? - there's no equivalent to einsum,",1
v0.12.0b2,TODO Share some logic with non-discrete version,1
v0.12.0b2,TODO: support freq_weight and sample_var in debiased lasso,1
v0.12.0b2,"TODO: consider whether we need more care around stateful featurizers,",1
v0.12.0b2,TODO: support freq_weight and sample_var in debiased lasso,1
v0.12.0b2,"TODO: consider whether we need more care around stateful featurizers,",1
v0.12.0b2,"TODO: consider whether we need more care around stateful featurizers,",1
v0.12.0b2,TODO: Add a __dir__ implementation?,1
v0.12.0b2,TODO: studentized bootstrap? this would be more accurate in most cases but can we avoid,1
v0.12.0b2,TODO: support freq_weight and sample_var,1
v0.12.0b2,Doesn't work with CNTK backend as of 2018-07-17 - see https://github.com/keras-team/keras/issues/10715,1
v0.12.0b2,For some reason this doesn't work at all when run against the CNTK backend...,1
v0.12.0b2,TODO: test that the estimated effect is usually within the bounds,1
v0.12.0b2,TODO: test that the estimated effect is usually within the bounds,1
v0.12.0b2,TODO: set up proper flag for this,1
v0.12.0b2,TODO: test something rather than just print...,1
v0.12.0b2,TODO: this tests that we can run the method; how do we test that the results are reasonable?,1
v0.12.0b2,TODO: add tests for extra properties like coef_ where they exist,1
v0.12.0b2,TODO: add tests for extra properties like coef_ where they exist,1
v0.12.0b2,TODO: ideally we could also test whether Z and X are jointly okay when both discrete,1
v0.12.0b2,HACK: work around bug in assertWarns (https://bugs.python.org/issue29620),1
v0.12.0b2,does not work well with parallelism.,1
v0.12.0b2,does not work well with parallelism.,1
v0.12.0b2,TODO: does this imply we should change some defaults to make this more likely to succeed?,1
v0.12.0b2,TODO: does this imply we should change some defaults to make this more likely to succeed?,1
v0.12.0b2,"fourth column: 5 ones, also not enough but barely works even with forest heterogeneity",1
v0.12.0b2,"TODO There is a matrix dimension mismatch between multiple outcome and single outcome, should solve that",1
v0.12.0b2,"TODO: make sure that the public API consistently uses ""T"" instead of ""P"" for the treatment",1
v0.12.0b2,TODO: does the numeric stability actually make any difference?,1
v0.12.0b2,"CNTK backend can't randomize across batches and doesn't implement cumsum (at least as of June 2018,",1
v0.12.0b2,TODO: is there a more robust way to do this?,1
v0.12.0b2,TODO: do we need to give the user more control over other arguments to fit?,1
v0.12.0b2,TODO: do we need to give the user more control over other arguments to fit?,1
v0.12.0b2,"TODO: it seems like we need to sum over the batch because we can only apply gradient to a scalar,",1
v0.12.0b2,TODO: any way to get this to work on batches of arbitrary size?,1
v0.12.0b2,"TODO: if opt_reweighted is False, we could change the logic to support multidimensional treatments,",1
v0.12.0b2,TODO: do correct adjustment for sample_var,1
v0.12.0b2,TODO: allow the final model to actually use X? Then we'd need to rename the class,1
v0.12.0b2,TODO: allow the final model to actually use X?,1
v0.12.0b2,TODO: allow the final model to actually use X?,1
v0.12.0b2,TODO: would it be useful to extend to handle controls ala vanilla DML?,1
v0.12.0b2,TODO: is it right that the effective number of intruments is the,1
v0.12.0b2,TODO: this utility is documented but internal; reimplement?,1
v0.12.0b2,TODO: this utility is even less public...,1
v0.12.0b2,TODO: we can't currently handle unseen values of the feature column when getting the effect;,1
v0.12.0b2,TODO: check compatibility of X and Y lengths,1
v0.12.0b2,"TODO: bail out also if categorical columns, classification, random_state changed?",1
v0.12.0b2,TODO: should we also train a new model_y under any circumstances when warm_start is True?,1
v0.12.0b2,"don't train a model, but suggest workaround since there are enough instances of least",1
v0.12.0b2,TODO: enrich outcome logic for multi-class classification when that is supported,1
v0.12.0b2,TODO: it seems like it would be better to just return the tree itself rather than plot it;,1
v0.12.0b2,TODO: it seems like it would be better to just return the tree itself rather than plot it;,1
v0.12.0b2,TODO: conisder working around relying on sklearn implementation details,1
v0.12.0b2,TODO: consider working around relying on sklearn implementation details,1
v0.12.0b2,"TODO: Add other types of covariance estimation (e.g. Newey-West (HAC), HC2, HC3)",1
v0.12.0b2,TODO: consider working around relying on sklearn implementation details,1
v0.12.0b2,TODO: Check performance,1
v0.12.0b2,Need to redefine fit here for auto inference to work due to a quirk in how,1
v0.12.0b1,-- Options for todo extension ----------------------------------------------,1
v0.12.0b1,"If true, `todo` and `todoList` produce output, else they produce nothing.",1
v0.12.0b1,TODO. Deal with multi-class instrument,1
v0.12.0b1,TODO. Deal with multi-class instrument,1
v0.12.0b1,TODO. The solution below is not really a valid cross-fitting,1
v0.12.0b1,"HACK: DRIV doesn't expect a treatment dimension, so pretend we got a vector even if we really had a one-column array",1
v0.12.0b1,"Once multiple treatments are supported, we'll need to fix this",1
v0.12.0b1,TODO. Deal with multi-class instrument/treatment,1
v0.12.0b1,TODO: any way to avoid creating a copy if the array was already dense?,1
v0.12.0b1,TODO: might be faster to break into connected components first,1
v0.12.0b1,TODO: Consider investigating other performance ideas for these cases,1
v0.12.0b1,TODO: would using einsum's paths to optimize the order of merging help?,1
v0.12.0b1,HACK: We're relying on some of sklearn's non-public classes which are not completely stable.,1
v0.12.0b1,"HACK: it's not optimal to use a regex like this, but the base class's node_to_str doesn't expose any",1
v0.12.0b1,TODO. Create our own color pallete for multiple treatments. The one below is for binary treatments.,1
v0.12.0b1,"TODO: what if input is sparse? - there's no equivalent to einsum,",1
v0.12.0b1,TODO Share some logic with non-discrete version,1
v0.12.0b1,TODO: support freq_weight and sample_var in debiased lasso,1
v0.12.0b1,"TODO: consider whether we need more care around stateful featurizers,",1
v0.12.0b1,TODO: support freq_weight and sample_var in debiased lasso,1
v0.12.0b1,"TODO: consider whether we need more care around stateful featurizers,",1
v0.12.0b1,"TODO: consider whether we need more care around stateful featurizers,",1
v0.12.0b1,TODO: Add a __dir__ implementation?,1
v0.12.0b1,TODO: studentized bootstrap? this would be more accurate in most cases but can we avoid,1
v0.12.0b1,TODO: support freq_weight and sample_var,1
v0.12.0b1,Doesn't work with CNTK backend as of 2018-07-17 - see https://github.com/keras-team/keras/issues/10715,1
v0.12.0b1,For some reason this doesn't work at all when run against the CNTK backend...,1
v0.12.0b1,TODO: test that the estimated effect is usually within the bounds,1
v0.12.0b1,TODO: test that the estimated effect is usually within the bounds,1
v0.12.0b1,TODO: set up proper flag for this,1
v0.12.0b1,TODO: test something rather than just print...,1
v0.12.0b1,TODO: this tests that we can run the method; how do we test that the results are reasonable?,1
v0.12.0b1,TODO: add tests for extra properties like coef_ where they exist,1
v0.12.0b1,TODO: add tests for extra properties like coef_ where they exist,1
v0.12.0b1,TODO: ideally we could also test whether Z and X are jointly okay when both discrete,1
v0.12.0b1,HACK: work around bug in assertWarns (https://bugs.python.org/issue29620),1
v0.12.0b1,does not work well with parallelism.,1
v0.12.0b1,does not work well with parallelism.,1
v0.12.0b1,TODO: does this imply we should change some defaults to make this more likely to succeed?,1
v0.12.0b1,TODO: does this imply we should change some defaults to make this more likely to succeed?,1
v0.12.0b1,"TODO There is a matrix dimension mismatch between multiple outcome and single outcome, should solve that",1
v0.12.0b1,"TODO: make sure that the public API consistently uses ""T"" instead of ""P"" for the treatment",1
v0.12.0b1,TODO: does the numeric stability actually make any difference?,1
v0.12.0b1,"CNTK backend can't randomize across batches and doesn't implement cumsum (at least as of June 2018,",1
v0.12.0b1,TODO: is there a more robust way to do this?,1
v0.12.0b1,TODO: do we need to give the user more control over other arguments to fit?,1
v0.12.0b1,TODO: do we need to give the user more control over other arguments to fit?,1
v0.12.0b1,"TODO: it seems like we need to sum over the batch because we can only apply gradient to a scalar,",1
v0.12.0b1,TODO: any way to get this to work on batches of arbitrary size?,1
v0.12.0b1,"TODO: if opt_reweighted is False, we could change the logic to support multidimensional treatments,",1
v0.12.0b1,TODO: do correct adjustment for sample_var,1
v0.12.0b1,TODO: allow the final model to actually use X? Then we'd need to rename the class,1
v0.12.0b1,TODO: allow the final model to actually use X?,1
v0.12.0b1,TODO: allow the final model to actually use X?,1
v0.12.0b1,TODO: would it be useful to extend to handle controls ala vanilla DML?,1
v0.12.0b1,TODO: is it right that the effective number of intruments is the,1
v0.12.0b1,TODO: this utility is documented but internal; reimplement?,1
v0.12.0b1,TODO: this utility is even less public...,1
v0.12.0b1,TODO: we can't currently handle unseen values of the feature column when getting the effect;,1
v0.12.0b1,TODO: check compatibility of X and Y lengths,1
v0.12.0b1,"TODO: bail out also if categorical columns, classification, random_state changed?",1
v0.12.0b1,TODO: should we also train a new model_y under any circumstances when warm_start is True?,1
v0.12.0b1,TODO: enrich outcome logic for multi-class classification when that is supported,1
v0.12.0b1,TODO: it seems like it would be better to just return the tree itself rather than plot it;,1
v0.12.0b1,TODO: it seems like it would be better to just return the tree itself rather than plot it;,1
v0.12.0b1,TODO: conisder working around relying on sklearn implementation details,1
v0.12.0b1,TODO: consider working around relying on sklearn implementation details,1
v0.12.0b1,"TODO: Add other types of covariance estimation (e.g. Newey-West (HAC), HC2, HC3)",1
v0.12.0b1,TODO: consider working around relying on sklearn implementation details,1
v0.12.0b1,TODO: Check performance,1
v0.12.0b1,Need to redefine fit here for auto inference to work due to a quirk in how,1
v0.11.1,-- Options for todo extension ----------------------------------------------,1
v0.11.1,"If true, `todo` and `todoList` produce output, else they produce nothing.",1
v0.11.1,TODO. Deal with multi-class instrument,1
v0.11.1,TODO. Deal with multi-class instrument,1
v0.11.1,TODO. The solution below is not really a valid cross-fitting,1
v0.11.1,"HACK: DRIV doesn't expect a treatment dimension, so pretend we got a vector even if we really had a one-column array",1
v0.11.1,"Once multiple treatments are supported, we'll need to fix this",1
v0.11.1,TODO. Deal with multi-class instrument/treatment,1
v0.11.1,TODO: any way to avoid creating a copy if the array was already dense?,1
v0.11.1,TODO: might be faster to break into connected components first,1
v0.11.1,TODO: Consider investigating other performance ideas for these cases,1
v0.11.1,TODO: would using einsum's paths to optimize the order of merging help?,1
v0.11.1,HACK: We're relying on some of sklearn's non-public classes which are not completely stable.,1
v0.11.1,"HACK: it's not optimal to use a regex like this, but the base class's node_to_str doesn't expose any",1
v0.11.1,TODO. Create our own color pallete for multiple treatments. The one below is for binary treatments.,1
v0.11.1,"TODO: what if input is sparse? - there's no equivalent to einsum,",1
v0.11.1,TODO Share some logic with non-discrete version,1
v0.11.1,TODO: support freq_weight and sample_var in debiased lasso,1
v0.11.1,"TODO: consider whether we need more care around stateful featurizers,",1
v0.11.1,TODO: support freq_weight and sample_var in debiased lasso,1
v0.11.1,"TODO: consider whether we need more care around stateful featurizers,",1
v0.11.1,"TODO: consider whether we need more care around stateful featurizers,",1
v0.11.1,TODO: Add a __dir__ implementation?,1
v0.11.1,TODO: studentized bootstrap? this would be more accurate in most cases but can we avoid,1
v0.11.1,TODO: support freq_weight and sample_var,1
v0.11.1,Doesn't work with CNTK backend as of 2018-07-17 - see https://github.com/keras-team/keras/issues/10715,1
v0.11.1,For some reason this doesn't work at all when run against the CNTK backend...,1
v0.11.1,TODO: test that the estimated effect is usually within the bounds,1
v0.11.1,TODO: test that the estimated effect is usually within the bounds,1
v0.11.1,TODO: set up proper flag for this,1
v0.11.1,TODO: test something rather than just print...,1
v0.11.1,TODO: this tests that we can run the method; how do we test that the results are reasonable?,1
v0.11.1,TODO: add tests for extra properties like coef_ where they exist,1
v0.11.1,TODO: add tests for extra properties like coef_ where they exist,1
v0.11.1,TODO: ideally we could also test whether Z and X are jointly okay when both discrete,1
v0.11.1,HACK: work around bug in assertWarns (https://bugs.python.org/issue29620),1
v0.11.1,does not work well with parallelism.,1
v0.11.1,does not work well with parallelism.,1
v0.11.1,TODO: does this imply we should change some defaults to make this more likely to succeed?,1
v0.11.1,TODO: does this imply we should change some defaults to make this more likely to succeed?,1
v0.11.1,"TODO There is a matrix dimension mismatch between multiple outcome and single outcome, should solve that",1
v0.11.1,"TODO: make sure that the public API consistently uses ""T"" instead of ""P"" for the treatment",1
v0.11.1,TODO: does the numeric stability actually make any difference?,1
v0.11.1,"CNTK backend can't randomize across batches and doesn't implement cumsum (at least as of June 2018,",1
v0.11.1,TODO: is there a more robust way to do this?,1
v0.11.1,TODO: do we need to give the user more control over other arguments to fit?,1
v0.11.1,TODO: do we need to give the user more control over other arguments to fit?,1
v0.11.1,"TODO: it seems like we need to sum over the batch because we can only apply gradient to a scalar,",1
v0.11.1,TODO: any way to get this to work on batches of arbitrary size?,1
v0.11.1,"TODO: if opt_reweighted is False, we could change the logic to support multidimensional treatments,",1
v0.11.1,TODO: do correct adjustment for sample_var,1
v0.11.1,TODO: allow the final model to actually use X? Then we'd need to rename the class,1
v0.11.1,TODO: allow the final model to actually use X?,1
v0.11.1,TODO: allow the final model to actually use X?,1
v0.11.1,TODO: would it be useful to extend to handle controls ala vanilla DML?,1
v0.11.1,TODO: is it right that the effective number of intruments is the,1
v0.11.1,TODO: this utility is documented but internal; reimplement?,1
v0.11.1,TODO: this utility is even less public...,1
v0.11.1,TODO: check compatibility of X and Y lengths,1
v0.11.1,"TODO: bail out also if categorical columns, classification changed?",1
v0.11.1,TODO: should we also train a new model_y under any circumstances when warm_start is True?,1
v0.11.1,TODO: enrich outcome logic for multi-class classification when that is supported,1
v0.11.1,TODO: it seems like it would be better to just return the tree itself rather than plot it;,1
v0.11.1,TODO: it seems like it would be better to just return the tree itself rather than plot it;,1
v0.11.1,TODO: conisder working around relying on sklearn implementation details,1
v0.11.1,TODO: consider working around relying on sklearn implementation details,1
v0.11.1,"TODO: Add other types of covariance estimation (e.g. Newey-West (HAC), HC2, HC3)",1
v0.11.1,TODO: consider working around relying on sklearn implementation details,1
v0.11.1,TODO: Check performance,1
v0.11.1,Need to redefine fit here for auto inference to work due to a quirk in how,1
v0.11.0,-- Options for todo extension ----------------------------------------------,1
v0.11.0,"If true, `todo` and `todoList` produce output, else they produce nothing.",1
v0.11.0,TODO. Deal with multi-class instrument,1
v0.11.0,TODO. Deal with multi-class instrument,1
v0.11.0,TODO. The solution below is not really a valid cross-fitting,1
v0.11.0,"HACK: DRIV doesn't expect a treatment dimension, so pretend we got a vector even if we really had a one-column array",1
v0.11.0,"Once multiple treatments are supported, we'll need to fix this",1
v0.11.0,TODO. Deal with multi-class instrument/treatment,1
v0.11.0,TODO: any way to avoid creating a copy if the array was already dense?,1
v0.11.0,TODO: might be faster to break into connected components first,1
v0.11.0,TODO: Consider investigating other performance ideas for these cases,1
v0.11.0,TODO: would using einsum's paths to optimize the order of merging help?,1
v0.11.0,HACK: We're relying on some of sklearn's non-public classes which are not completely stable.,1
v0.11.0,"HACK: it's not optimal to use a regex like this, but the base class's node_to_str doesn't expose any",1
v0.11.0,TODO. Create our own color pallete for multiple treatments. The one below is for binary treatments.,1
v0.11.0,"TODO: what if input is sparse? - there's no equivalent to einsum,",1
v0.11.0,TODO Share some logic with non-discrete version,1
v0.11.0,TODO: support freq_weight and sample_var in debiased lasso,1
v0.11.0,"TODO: consider whether we need more care around stateful featurizers,",1
v0.11.0,TODO: support freq_weight and sample_var in debiased lasso,1
v0.11.0,"TODO: consider whether we need more care around stateful featurizers,",1
v0.11.0,"TODO: consider whether we need more care around stateful featurizers,",1
v0.11.0,TODO: Add a __dir__ implementation?,1
v0.11.0,TODO: studentized bootstrap? this would be more accurate in most cases but can we avoid,1
v0.11.0,TODO: support freq_weight and sample_var,1
v0.11.0,Doesn't work with CNTK backend as of 2018-07-17 - see https://github.com/keras-team/keras/issues/10715,1
v0.11.0,For some reason this doesn't work at all when run against the CNTK backend...,1
v0.11.0,TODO: test that the estimated effect is usually within the bounds,1
v0.11.0,TODO: test that the estimated effect is usually within the bounds,1
v0.11.0,TODO: set up proper flag for this,1
v0.11.0,TODO: test something rather than just print...,1
v0.11.0,TODO: this tests that we can run the method; how do we test that the results are reasonable?,1
v0.11.0,TODO: add tests for extra properties like coef_ where they exist,1
v0.11.0,TODO: add tests for extra properties like coef_ where they exist,1
v0.11.0,TODO: ideally we could also test whether Z and X are jointly okay when both discrete,1
v0.11.0,HACK: work around bug in assertWarns (https://bugs.python.org/issue29620),1
v0.11.0,does not work well with parallelism.,1
v0.11.0,does not work well with parallelism.,1
v0.11.0,TODO: does this imply we should change some defaults to make this more likely to succeed?,1
v0.11.0,TODO: does this imply we should change some defaults to make this more likely to succeed?,1
v0.11.0,"TODO There is a matrix dimension mismatch between multiple outcome and single outcome, should solve that",1
v0.11.0,"TODO: make sure that the public API consistently uses ""T"" instead of ""P"" for the treatment",1
v0.11.0,TODO: does the numeric stability actually make any difference?,1
v0.11.0,"CNTK backend can't randomize across batches and doesn't implement cumsum (at least as of June 2018,",1
v0.11.0,TODO: is there a more robust way to do this?,1
v0.11.0,TODO: do we need to give the user more control over other arguments to fit?,1
v0.11.0,TODO: do we need to give the user more control over other arguments to fit?,1
v0.11.0,"TODO: it seems like we need to sum over the batch because we can only apply gradient to a scalar,",1
v0.11.0,TODO: any way to get this to work on batches of arbitrary size?,1
v0.11.0,"TODO: if opt_reweighted is False, we could change the logic to support multidimensional treatments,",1
v0.11.0,TODO: do correct adjustment for sample_var,1
v0.11.0,TODO: allow the final model to actually use X? Then we'd need to rename the class,1
v0.11.0,TODO: allow the final model to actually use X?,1
v0.11.0,TODO: allow the final model to actually use X?,1
v0.11.0,TODO: would it be useful to extend to handle controls ala vanilla DML?,1
v0.11.0,TODO: is it right that the effective number of intruments is the,1
v0.11.0,TODO: this utility is documented but internal; reimplement?,1
v0.11.0,TODO: this utility is even less public...,1
v0.11.0,TODO: check compatibility of X and Y lengths,1
v0.11.0,TODO: implement check for upper bound on categoricals,1
v0.11.0,"TODO: bail out also if categorical columns, classification changed?",1
v0.11.0,TODO: should we also train a new model_y under any circumstances when warm_start is True?,1
v0.11.0,TODO: enrich outcome logic for multi-class classification when that is supported,1
v0.11.0,TODO: it seems like it would be better to just return the tree itself rather than plot it;,1
v0.11.0,TODO: it seems like it would be better to just return the tree itself rather than plot it;,1
v0.11.0,TODO: conisder working around relying on sklearn implementation details,1
v0.11.0,TODO: consider working around relying on sklearn implementation details,1
v0.11.0,"TODO: Add other types of covariance estimation (e.g. Newey-West (HAC), HC2, HC3)",1
v0.11.0,TODO: consider working around relying on sklearn implementation details,1
v0.11.0,TODO: Check performance,1
v0.11.0,Need to redefine fit here for auto inference to work due to a quirk in how,1
v0.10.0,-- Options for todo extension ----------------------------------------------,1
v0.10.0,"If true, `todo` and `todoList` produce output, else they produce nothing.",1
v0.10.0,TODO. Deal with multi-class instrument,1
v0.10.0,TODO. Deal with multi-class instrument,1
v0.10.0,TODO. The solution below is not really a valid cross-fitting,1
v0.10.0,"HACK: DRIV doesn't expect a treatment dimension, so pretend we got a vector even if we really had a one-column array",1
v0.10.0,"Once multiple treatments are supported, we'll need to fix this",1
v0.10.0,TODO. Deal with multi-class instrument/treatment,1
v0.10.0,TODO: any way to avoid creating a copy if the array was already dense?,1
v0.10.0,TODO: might be faster to break into connected components first,1
v0.10.0,TODO: Consider investigating other performance ideas for these cases,1
v0.10.0,TODO: would using einsum's paths to optimize the order of merging help?,1
v0.10.0,HACK: We're relying on some of sklearn's non-public classes which are not completely stable.,1
v0.10.0,"HACK: it's not optimal to use a regex like this, but the base class's node_to_str doesn't expose any",1
v0.10.0,TODO. Create our own color pallete for multiple treatments. The one below is for binary treatments.,1
v0.10.0,"TODO: what if input is sparse? - there's no equivalent to einsum,",1
v0.10.0,TODO Share some logic with non-discrete version,1
v0.10.0,TODO: support sample_var,1
v0.10.0,"TODO: consider whether we need more care around stateful featurizers,",1
v0.10.0,TODO: support sample_var,1
v0.10.0,"TODO: consider whether we need more care around stateful featurizers,",1
v0.10.0,"TODO: consider whether we need more care around stateful featurizers,",1
v0.10.0,TODO: Add a __dir__ implementation?,1
v0.10.0,TODO: studentized bootstrap? this would be more accurate in most cases but can we avoid,1
v0.10.0,Doesn't work with CNTK backend as of 2018-07-17 - see https://github.com/keras-team/keras/issues/10715,1
v0.10.0,For some reason this doesn't work at all when run against the CNTK backend...,1
v0.10.0,TODO: test that the estimated effect is usually within the bounds,1
v0.10.0,TODO: test that the estimated effect is usually within the bounds,1
v0.10.0,TODO: set up proper flag for this,1
v0.10.0,TODO: test something rather than just print...,1
v0.10.0,TODO: this tests that we can run the method; how do we test that the results are reasonable?,1
v0.10.0,TODO: add tests for extra properties like coef_ where they exist,1
v0.10.0,TODO: add tests for extra properties like coef_ where they exist,1
v0.10.0,TODO: ideally we could also test whether Z and X are jointly okay when both discrete,1
v0.10.0,HACK: work around bug in assertWarns (https://bugs.python.org/issue29620),1
v0.10.0,does not work well with parallelism.,1
v0.10.0,does not work well with parallelism.,1
v0.10.0,TODO: does this imply we should change some defaults to make this more likely to succeed?,1
v0.10.0,TODO: does this imply we should change some defaults to make this more likely to succeed?,1
v0.10.0,"TODO There is a matrix dimension mismatch between multiple outcome and single outcome, should solve that",1
v0.10.0,"TODO: make sure that the public API consistently uses ""T"" instead of ""P"" for the treatment",1
v0.10.0,TODO: does the numeric stability actually make any difference?,1
v0.10.0,"CNTK backend can't randomize across batches and doesn't implement cumsum (at least as of June 2018,",1
v0.10.0,TODO: is there a more robust way to do this?,1
v0.10.0,TODO: do we need to give the user more control over other arguments to fit?,1
v0.10.0,TODO: do we need to give the user more control over other arguments to fit?,1
v0.10.0,"TODO: it seems like we need to sum over the batch because we can only apply gradient to a scalar,",1
v0.10.0,TODO: any way to get this to work on batches of arbitrary size?,1
v0.10.0,"TODO: if opt_reweighted is False, we could change the logic to support multidimensional treatments,",1
v0.10.0,TODO: allow the final model to actually use X? Then we'd need to rename the class,1
v0.10.0,TODO: allow the final model to actually use X?,1
v0.10.0,TODO: allow the final model to actually use X?,1
v0.10.0,TODO: would it be useful to extend to handle controls ala vanilla DML?,1
v0.10.0,TODO: is it right that the effective number of intruments is the,1
v0.10.0,TODO: conisder working around relying on sklearn implementation details,1
v0.10.0,TODO: consider working around relying on sklearn implementation details,1
v0.10.0,"TODO: Add other types of covariance estimation (e.g. Newey-West (HAC), HC2, HC3)",1
v0.10.0,TODO: consider working around relying on sklearn implementation details,1
v0.10.0,TODO: Check performance,1
v0.10.0,Need to redefine fit here for auto inference to work due to a quirk in how,1
v0.9.2,-- Options for todo extension ----------------------------------------------,1
v0.9.2,"If true, `todo` and `todoList` produce output, else they produce nothing.",1
v0.9.2,TODO. Deal with multi-class instrument,1
v0.9.2,TODO. Deal with multi-class instrument,1
v0.9.2,TODO. The solution below is not really a valid cross-fitting,1
v0.9.2,"HACK: DRIV doesn't expect a treatment dimension, so pretend we got a vector even if we really had a one-column array",1
v0.9.2,"Once multiple treatments are supported, we'll need to fix this",1
v0.9.2,TODO. Deal with multi-class instrument/treatment,1
v0.9.2,TODO: any way to avoid creating a copy if the array was already dense?,1
v0.9.2,TODO: might be faster to break into connected components first,1
v0.9.2,TODO: Consider investigating other performance ideas for these cases,1
v0.9.2,TODO: would using einsum's paths to optimize the order of merging help?,1
v0.9.2,"TODO: what if input is sparse? - there's no equivalent to einsum,",1
v0.9.2,TODO Share some logic with non-discrete version,1
v0.9.2,TODO: support sample_var,1
v0.9.2,"TODO: consider whether we need more care around stateful featurizers,",1
v0.9.2,TODO: support sample_var,1
v0.9.2,"TODO: consider whether we need more care around stateful featurizers,",1
v0.9.2,"TODO: consider whether we need more care around stateful featurizers,",1
v0.9.2,TODO: Add a __dir__ implementation?,1
v0.9.2,TODO: studentized bootstrap? this would be more accurate in most cases but can we avoid,1
v0.9.2,Doesn't work with CNTK backend as of 2018-07-17 - see https://github.com/keras-team/keras/issues/10715,1
v0.9.2,For some reason this doesn't work at all when run against the CNTK backend...,1
v0.9.2,TODO: test that the estimated effect is usually within the bounds,1
v0.9.2,TODO: test that the estimated effect is usually within the bounds,1
v0.9.2,TODO: set up proper flag for this,1
v0.9.2,TODO: test something rather than just print...,1
v0.9.2,TODO: this tests that we can run the method; how do we test that the results are reasonable?,1
v0.9.2,TODO: add tests for extra properties like coef_ where they exist,1
v0.9.2,TODO: add tests for extra properties like coef_ where they exist,1
v0.9.2,TODO: ideally we could also test whether Z and X are jointly okay when both discrete,1
v0.9.2,HACK: work around bug in assertWarns (https://bugs.python.org/issue29620),1
v0.9.2,does not work well with parallelism.,1
v0.9.2,does not work well with parallelism.,1
v0.9.2,TODO: does this imply we should change some defaults to make this more likely to succeed?,1
v0.9.2,TODO: does this imply we should change some defaults to make this more likely to succeed?,1
v0.9.2,"TODO There is a matrix dimension mismatch between multiple outcome and single outcome, should solve that",1
v0.9.2,"TODO: make sure that the public API consistently uses ""T"" instead of ""P"" for the treatment",1
v0.9.2,TODO: does the numeric stability actually make any difference?,1
v0.9.2,"CNTK backend can't randomize across batches and doesn't implement cumsum (at least as of June 2018,",1
v0.9.2,TODO: is there a more robust way to do this?,1
v0.9.2,TODO: do we need to give the user more control over other arguments to fit?,1
v0.9.2,TODO: do we need to give the user more control over other arguments to fit?,1
v0.9.2,"TODO: it seems like we need to sum over the batch because we can only apply gradient to a scalar,",1
v0.9.2,TODO: any way to get this to work on batches of arbitrary size?,1
v0.9.2,"TODO: if opt_reweighted is False, we could change the logic to support multidimensional treatments,",1
v0.9.2,TODO: allow the final model to actually use X? Then we'd need to rename the class,1
v0.9.2,TODO: allow the final model to actually use X?,1
v0.9.2,TODO: allow the final model to actually use X?,1
v0.9.2,TODO: would it be useful to extend to handle controls ala vanilla DML?,1
v0.9.2,TODO: is it right that the effective number of intruments is the,1
v0.9.2,TODO: conisder working around relying on sklearn implementation details,1
v0.9.2,TODO: consider working around relying on sklearn implementation details,1
v0.9.2,"TODO: Add other types of covariance estimation (e.g. Newey-West (HAC), HC2, HC3)",1
v0.9.2,TODO: generalize to multiple treatment case?,1
v0.9.2,HACK: We're relying on some of sklearn's non-public classes which are not completely stable.,1
v0.9.2,"HACK: it's not optimal to use a regex like this, but the base class's node_to_str doesn't expose any",1
v0.9.2,TODO: consider working around relying on sklearn implementation details,1
v0.9.2,TODO: Check performance,1
v0.9.2,Need to redefine fit here for auto inference to work due to a quirk in how,1
v0.9.1,-- Options for todo extension ----------------------------------------------,1
v0.9.1,"If true, `todo` and `todoList` produce output, else they produce nothing.",1
v0.9.1,TODO. Deal with multi-class instrument,1
v0.9.1,TODO. Deal with multi-class instrument,1
v0.9.1,TODO. The solution below is not really a valid cross-fitting,1
v0.9.1,"HACK: DRIV doesn't expect a treatment dimension, so pretend we got a vector even if we really had a one-column array",1
v0.9.1,"Once multiple treatments are supported, we'll need to fix this",1
v0.9.1,TODO. Deal with multi-class instrument/treatment,1
v0.9.1,TODO: any way to avoid creating a copy if the array was already dense?,1
v0.9.1,TODO: might be faster to break into connected components first,1
v0.9.1,TODO: Consider investigating other performance ideas for these cases,1
v0.9.1,TODO: would using einsum's paths to optimize the order of merging help?,1
v0.9.1,"TODO: what if input is sparse? - there's no equivalent to einsum,",1
v0.9.1,TODO Share some logic with non-discrete version,1
v0.9.1,TODO: support sample_var,1
v0.9.1,"TODO: consider whether we need more care around stateful featurizers,",1
v0.9.1,TODO: support sample_var,1
v0.9.1,"TODO: consider whether we need more care around stateful featurizers,",1
v0.9.1,"TODO: consider whether we need more care around stateful featurizers,",1
v0.9.1,TODO: Add a __dir__ implementation?,1
v0.9.1,TODO: studentized bootstrap? this would be more accurate in most cases but can we avoid,1
v0.9.1,Doesn't work with CNTK backend as of 2018-07-17 - see https://github.com/keras-team/keras/issues/10715,1
v0.9.1,For some reason this doesn't work at all when run against the CNTK backend...,1
v0.9.1,TODO: test that the estimated effect is usually within the bounds,1
v0.9.1,TODO: test that the estimated effect is usually within the bounds,1
v0.9.1,TODO: set up proper flag for this,1
v0.9.1,TODO: test something rather than just print...,1
v0.9.1,TODO: this tests that we can run the method; how do we test that the results are reasonable?,1
v0.9.1,TODO: add tests for extra properties like coef_ where they exist,1
v0.9.1,TODO: add tests for extra properties like coef_ where they exist,1
v0.9.1,TODO: ideally we could also test whether Z and X are jointly okay when both discrete,1
v0.9.1,HACK: work around bug in assertWarns (https://bugs.python.org/issue29620),1
v0.9.1,does not work well with parallelism.,1
v0.9.1,does not work well with parallelism.,1
v0.9.1,TODO: does this imply we should change some defaults to make this more likely to succeed?,1
v0.9.1,TODO: does this imply we should change some defaults to make this more likely to succeed?,1
v0.9.1,"TODO There is a matrix dimension mismatch between multiple outcome and single outcome, should solve that",1
v0.9.1,"TODO: make sure that the public API consistently uses ""T"" instead of ""P"" for the treatment",1
v0.9.1,TODO: does the numeric stability actually make any difference?,1
v0.9.1,"CNTK backend can't randomize across batches and doesn't implement cumsum (at least as of June 2018,",1
v0.9.1,TODO: is there a more robust way to do this?,1
v0.9.1,TODO: do we need to give the user more control over other arguments to fit?,1
v0.9.1,TODO: do we need to give the user more control over other arguments to fit?,1
v0.9.1,"TODO: it seems like we need to sum over the batch because we can only apply gradient to a scalar,",1
v0.9.1,TODO: any way to get this to work on batches of arbitrary size?,1
v0.9.1,"TODO: if opt_reweighted is False, we could change the logic to support multidimensional treatments,",1
v0.9.1,TODO: allow the final model to actually use X? Then we'd need to rename the class,1
v0.9.1,TODO: allow the final model to actually use X?,1
v0.9.1,TODO: allow the final model to actually use X?,1
v0.9.1,TODO: would it be useful to extend to handle controls ala vanilla DML?,1
v0.9.1,TODO: is it right that the effective number of intruments is the,1
v0.9.1,TODO: conisder working around relying on sklearn implementation details,1
v0.9.1,TODO: consider working around relying on sklearn implementation details,1
v0.9.1,"TODO: Add other types of covariance estimation (e.g. Newey-West (HAC), HC2, HC3)",1
v0.9.1,TODO: generalize to multiple treatment case?,1
v0.9.1,HACK: We're relying on some of sklearn's non-public classes which are not completely stable.,1
v0.9.1,"HACK: it's not optimal to use a regex like this, but the base class's node_to_str doesn't expose any",1
v0.9.1,TODO: consider working around relying on sklearn implementation details,1
v0.9.1,TODO: Check performance,1
v0.9.1,Need to redefine fit here for auto inference to work due to a quirk in how,1
v0.9.0,-- Options for todo extension ----------------------------------------------,1
v0.9.0,"If true, `todo` and `todoList` produce output, else they produce nothing.",1
v0.9.0,TODO. Deal with multi-class instrument,1
v0.9.0,TODO. Deal with multi-class instrument,1
v0.9.0,TODO. The solution below is not really a valid cross-fitting,1
v0.9.0,"HACK: DRIV doesn't expect a treatment dimension, so pretend we got a vector even if we really had a one-column array",1
v0.9.0,"Once multiple treatments are supported, we'll need to fix this",1
v0.9.0,TODO. Deal with multi-class instrument/treatment,1
v0.9.0,TODO: any way to avoid creating a copy if the array was already dense?,1
v0.9.0,TODO: might be faster to break into connected components first,1
v0.9.0,TODO: Consider investigating other performance ideas for these cases,1
v0.9.0,TODO: would using einsum's paths to optimize the order of merging help?,1
v0.9.0,"TODO: what if input is sparse? - there's no equivalent to einsum,",1
v0.9.0,TODO Share some logic with non-discrete version,1
v0.9.0,TODO: support sample_var,1
v0.9.0,"TODO: consider whether we need more care around stateful featurizers,",1
v0.9.0,TODO: support sample_var,1
v0.9.0,"TODO: consider whether we need more care around stateful featurizers,",1
v0.9.0,"TODO: consider whether we need more care around stateful featurizers,",1
v0.9.0,TODO: Add a __dir__ implementation?,1
v0.9.0,TODO: studentized bootstrap? this would be more accurate in most cases but can we avoid,1
v0.9.0,Doesn't work with CNTK backend as of 2018-07-17 - see https://github.com/keras-team/keras/issues/10715,1
v0.9.0,For some reason this doesn't work at all when run against the CNTK backend...,1
v0.9.0,TODO: test that the estimated effect is usually within the bounds,1
v0.9.0,TODO: test that the estimated effect is usually within the bounds,1
v0.9.0,TODO: set up proper flag for this,1
v0.9.0,TODO: test something rather than just print...,1
v0.9.0,TODO: this tests that we can run the method; how do we test that the results are reasonable?,1
v0.9.0,TODO: add tests for extra properties like coef_ where they exist,1
v0.9.0,TODO: add tests for extra properties like coef_ where they exist,1
v0.9.0,TODO: ideally we could also test whether Z and X are jointly okay when both discrete,1
v0.9.0,HACK: work around bug in assertWarns (https://bugs.python.org/issue29620),1
v0.9.0,does not work well with parallelism.,1
v0.9.0,does not work well with parallelism.,1
v0.9.0,TODO: does this imply we should change some defaults to make this more likely to succeed?,1
v0.9.0,TODO: does this imply we should change some defaults to make this more likely to succeed?,1
v0.9.0,"TODO There is a matrix dimension mismatch between multiple outcome and single outcome, should solve that",1
v0.9.0,"TODO: make sure that the public API consistently uses ""T"" instead of ""P"" for the treatment",1
v0.9.0,TODO: does the numeric stability actually make any difference?,1
v0.9.0,"CNTK backend can't randomize across batches and doesn't implement cumsum (at least as of June 2018,",1
v0.9.0,TODO: is there a more robust way to do this?,1
v0.9.0,TODO: do we need to give the user more control over other arguments to fit?,1
v0.9.0,TODO: do we need to give the user more control over other arguments to fit?,1
v0.9.0,"TODO: it seems like we need to sum over the batch because we can only apply gradient to a scalar,",1
v0.9.0,TODO: any way to get this to work on batches of arbitrary size?,1
v0.9.0,"TODO: if opt_reweighted is False, we could change the logic to support multidimensional treatments,",1
v0.9.0,TODO: allow the final model to actually use X? Then we'd need to rename the class,1
v0.9.0,TODO: allow the final model to actually use X?,1
v0.9.0,TODO: allow the final model to actually use X?,1
v0.9.0,TODO: would it be useful to extend to handle controls ala vanilla DML?,1
v0.9.0,TODO: is it right that the effective number of intruments is the,1
v0.9.0,TODO: conisder working around relying on sklearn implementation details,1
v0.9.0,TODO: consider working around relying on sklearn implementation details,1
v0.9.0,"TODO: Add other types of covariance estimation (e.g. Newey-West (HAC), HC2, HC3)",1
v0.9.0,TODO: generalize to multiple treatment case?,1
v0.9.0,HACK: We're relying on some of sklearn's non-public classes which are not completely stable.,1
v0.9.0,"HACK: it's not optimal to use a regex like this, but the base class's node_to_str doesn't expose any",1
v0.9.0,TODO: consider working around relying on sklearn implementation details,1
v0.9.0,TODO: Check performance,1
v0.9.0,Need to redefine fit here for auto inference to work due to a quirk in how,1
v0.9.0b1,-- Options for todo extension ----------------------------------------------,1
v0.9.0b1,"If true, `todo` and `todoList` produce output, else they produce nothing.",1
v0.9.0b1,TODO. Deal with multi-class instrument,1
v0.9.0b1,TODO. Deal with multi-class instrument,1
v0.9.0b1,TODO. The solution below is not really a valid cross-fitting,1
v0.9.0b1,"HACK: DRIV doesn't expect a treatment dimension, so pretend we got a vector even if we really had a one-column array",1
v0.9.0b1,"Once multiple treatments are supported, we'll need to fix this",1
v0.9.0b1,TODO. Deal with multi-class instrument/treatment,1
v0.9.0b1,TODO: any way to avoid creating a copy if the array was already dense?,1
v0.9.0b1,TODO: might be faster to break into connected components first,1
v0.9.0b1,TODO: Consider investigating other performance ideas for these cases,1
v0.9.0b1,TODO: would using einsum's paths to optimize the order of merging help?,1
v0.9.0b1,"TODO: what if input is sparse? - there's no equivalent to einsum,",1
v0.9.0b1,TODO Share some logic with non-discrete version,1
v0.9.0b1,TODO: support sample_var,1
v0.9.0b1,"TODO: consider whether we need more care around stateful featurizers,",1
v0.9.0b1,TODO: support sample_var,1
v0.9.0b1,"TODO: consider whether we need more care around stateful featurizers,",1
v0.9.0b1,"TODO: consider whether we need more care around stateful featurizers,",1
v0.9.0b1,TODO: Add a __dir__ implementation?,1
v0.9.0b1,TODO: studentized bootstrap? this would be more accurate in most cases but can we avoid,1
v0.9.0b1,Doesn't work with CNTK backend as of 2018-07-17 - see https://github.com/keras-team/keras/issues/10715,1
v0.9.0b1,For some reason this doesn't work at all when run against the CNTK backend...,1
v0.9.0b1,TODO: test that the estimated effect is usually within the bounds,1
v0.9.0b1,TODO: test that the estimated effect is usually within the bounds,1
v0.9.0b1,TODO: set up proper flag for this,1
v0.9.0b1,TODO: test something rather than just print...,1
v0.9.0b1,TODO: this tests that we can run the method; how do we test that the results are reasonable?,1
v0.9.0b1,TODO: add tests for extra properties like coef_ where they exist,1
v0.9.0b1,TODO: add tests for extra properties like coef_ where they exist,1
v0.9.0b1,TODO: ideally we could also test whether Z and X are jointly okay when both discrete,1
v0.9.0b1,HACK: work around bug in assertWarns (https://bugs.python.org/issue29620),1
v0.9.0b1,does not work well with parallelism.,1
v0.9.0b1,does not work well with parallelism.,1
v0.9.0b1,TODO: does this imply we should change some defaults to make this more likely to succeed?,1
v0.9.0b1,TODO: does this imply we should change some defaults to make this more likely to succeed?,1
v0.9.0b1,"TODO There is a matrix dimension mismatch between multiple outcome and single outcome, should solve that",1
v0.9.0b1,"TODO: make sure that the public API consistently uses ""T"" instead of ""P"" for the treatment",1
v0.9.0b1,TODO: does the numeric stability actually make any difference?,1
v0.9.0b1,"CNTK backend can't randomize across batches and doesn't implement cumsum (at least as of June 2018,",1
v0.9.0b1,TODO: is there a more robust way to do this?,1
v0.9.0b1,TODO: do we need to give the user more control over other arguments to fit?,1
v0.9.0b1,TODO: do we need to give the user more control over other arguments to fit?,1
v0.9.0b1,"TODO: it seems like we need to sum over the batch because we can only apply gradient to a scalar,",1
v0.9.0b1,TODO: any way to get this to work on batches of arbitrary size?,1
v0.9.0b1,"TODO: if opt_reweighted is False, we could change the logic to support multidimensional treatments,",1
v0.9.0b1,TODO: allow the final model to actually use X? Then we'd need to rename the class,1
v0.9.0b1,TODO: allow the final model to actually use X?,1
v0.9.0b1,TODO: allow the final model to actually use X?,1
v0.9.0b1,TODO: would it be useful to extend to handle controls ala vanilla DML?,1
v0.9.0b1,TODO: is it right that the effective number of intruments is the,1
v0.9.0b1,TODO: conisder working around relying on sklearn implementation details,1
v0.9.0b1,TODO. The API of the private scikit-learn `_fit_and_predict` has changed,1
v0.9.0b1,TODO: consider working around relying on sklearn implementation details,1
v0.9.0b1,"TODO: Add other types of covariance estimation (e.g. Newey-West (HAC), HC2, HC3)",1
v0.9.0b1,TODO: generalize to multiple treatment case?,1
v0.9.0b1,HACK: We're relying on some of sklearn's non-public classes which are not completely stable.,1
v0.9.0b1,"HACK: it's not optimal to use a regex like this, but the base class's node_to_str doesn't expose any",1
v0.9.0b1,TODO: consider working around relying on sklearn implementation details,1
v0.9.0b1,TODO: Check performance,1
v0.9.0b1,Need to redefine fit here for auto inference to work due to a quirk in how,1
v0.8.1,-- Options for todo extension ----------------------------------------------,1
v0.8.1,"If true, `todo` and `todoList` produce output, else they produce nothing.",1
v0.8.1,TODO. Deal with multi-class instrument,1
v0.8.1,TODO. Deal with multi-class instrument,1
v0.8.1,TODO. The solution below is not really a valid cross-fitting,1
v0.8.1,"HACK: DRIV doesn't expect a treatment dimension, so pretend we got a vector even if we really had a one-column array",1
v0.8.1,"Once multiple treatments are supported, we'll need to fix this",1
v0.8.1,TODO. Deal with multi-class instrument/treatment,1
v0.8.1,TODO: consider working around relying on sklearn implementation details,1
v0.8.1,TODO: Check performance,1
v0.8.1,Need to redefine fit here for auto inference to work due to a quirk in how,1
v0.8.1,"TODO: consider whether we need more care around stateful featurizers,",1
v0.8.1,TODO: support sample_var,1
v0.8.1,"TODO: consider whether we need more care around stateful featurizers,",1
v0.8.1,TODO: is it right that the effective number of intruments is the,1
v0.8.1,TODO: allow the final model to actually use X? Then we'd need to rename the class,1
v0.8.1,TODO: allow the final model to actually use X?,1
v0.8.1,TODO: allow the final model to actually use X?,1
v0.8.1,TODO: would it be useful to extend to handle controls ala vanilla DML?,1
v0.8.1,"TODO: if opt_reweighted is False, we could change the logic to support multidimensional treatments,",1
v0.8.1,TODO: support sample_var,1
v0.8.1,"TODO: make sure that the public API consistently uses ""T"" instead of ""P"" for the treatment",1
v0.8.1,TODO: does the numeric stability actually make any difference?,1
v0.8.1,"CNTK backend can't randomize across batches and doesn't implement cumsum (at least as of June 2018,",1
v0.8.1,TODO: is there a more robust way to do this?,1
v0.8.1,TODO: do we need to give the user more control over other arguments to fit?,1
v0.8.1,TODO: do we need to give the user more control over other arguments to fit?,1
v0.8.1,"TODO: it seems like we need to sum over the batch because we can only apply gradient to a scalar,",1
v0.8.1,TODO: any way to get this to work on batches of arbitrary size?,1
v0.8.1,TODO: generalize to multiple treatment case?,1
v0.8.1,"TODO: what if input is sparse? - there's no equivalent to einsum,",1
v0.8.1,TODO Share some logic with non-discrete version,1
v0.8.1,TODO: any way to avoid creating a copy if the array was already dense?,1
v0.8.1,TODO: might be faster to break into connected components first,1
v0.8.1,TODO: Consider investigating other performance ideas for these cases,1
v0.8.1,TODO: would using einsum's paths to optimize the order of merging help?,1
v0.8.1,HACK: We're relying on some of sklearn's non-public classes which are not completely stable.,1
v0.8.1,"HACK: it's not optimal to use a regex like this, but the base class's node_to_str doesn't expose any",1
v0.8.1,TODO: Add a __dir__ implementation?,1
v0.8.1,TODO: studentized bootstrap? this would be more accurate in most cases but can we avoid,1
v0.8.1,Doesn't work with CNTK backend as of 2018-07-17 - see https://github.com/keras-team/keras/issues/10715,1
v0.8.1,For some reason this doesn't work at all when run against the CNTK backend...,1
v0.8.1,TODO: test that the estimated effect is usually within the bounds,1
v0.8.1,TODO: test that the estimated effect is usually within the bounds,1
v0.8.1,TODO: set up proper flag for this,1
v0.8.1,TODO: test something rather than just print...,1
v0.8.1,TODO: this tests that we can run the method; how do we test that the results are reasonable?,1
v0.8.1,TODO: add tests for extra properties like coef_ where they exist,1
v0.8.1,TODO: add tests for extra properties like coef_ where they exist,1
v0.8.1,TODO: ideally we could also test whether Z and X are jointly okay when both discrete,1
v0.8.1,HACK: work around bug in assertWarns (https://bugs.python.org/issue29620),1
v0.8.1,does not work well with parallelism.,1
v0.8.1,does not work well with parallelism.,1
v0.8.1,does not work well with parallelism.,1
v0.8.1,TODO: does this imply we should change some defaults to make this more likely to succeed?,1
v0.8.1,TODO: does this imply we should change some defaults to make this more likely to succeed?,1
v0.8.1,TODO: conisder working around relying on sklearn implementation details,1
v0.8.1,TODO: consider working around relying on sklearn implementation details,1
v0.8.1,"TODO: Add other types of covariance estimation (e.g. Newey-West (HAC), HC2, HC3)",1
v0.8.1,TODO. This slicing should ultimately be done inside the parallel function,1
v0.8.0,-- Options for todo extension ----------------------------------------------,1
v0.8.0,"If true, `todo` and `todoList` produce output, else they produce nothing.",1
v0.8.0,TODO. Deal with multi-class instrument,1
v0.8.0,TODO. Deal with multi-class instrument,1
v0.8.0,TODO. The solution below is not really a valid cross-fitting,1
v0.8.0,"HACK: DRIV doesn't expect a treatment dimension, so pretend we got a vector even if we really had a one-column array",1
v0.8.0,"Once multiple treatments are supported, we'll need to fix this",1
v0.8.0,TODO. Deal with multi-class instrument/treatment,1
v0.8.0,TODO: Check performance,1
v0.8.0,Need to redefine fit here for auto inference to work due to a quirk in how,1
v0.8.0,"TODO: consider whether we need more care around stateful featurizers,",1
v0.8.0,TODO: support sample_var,1
v0.8.0,"TODO: consider whether we need more care around stateful featurizers,",1
v0.8.0,TODO: is it right that the effective number of intruments is the,1
v0.8.0,TODO: allow the final model to actually use X? Then we'd need to rename the class,1
v0.8.0,TODO: allow the final model to actually use X?,1
v0.8.0,TODO: allow the final model to actually use X?,1
v0.8.0,TODO: would it be useful to extend to handle controls ala vanilla DML?,1
v0.8.0,"TODO: if opt_reweighted is False, we could change the logic to support multidimensional treatments,",1
v0.8.0,TODO: is there a good way to incorporate the other nuisance terms in the score?,1
v0.8.0,TODO: support sample_var,1
v0.8.0,"TODO: make sure that the public API consistently uses ""T"" instead of ""P"" for the treatment",1
v0.8.0,TODO: does the numeric stability actually make any difference?,1
v0.8.0,"CNTK backend can't randomize across batches and doesn't implement cumsum (at least as of June 2018,",1
v0.8.0,TODO: is there a more robust way to do this?,1
v0.8.0,TODO: do we need to give the user more control over other arguments to fit?,1
v0.8.0,TODO: do we need to give the user more control over other arguments to fit?,1
v0.8.0,"TODO: it seems like we need to sum over the batch because we can only apply gradient to a scalar,",1
v0.8.0,TODO: any way to get this to work on batches of arbitrary size?,1
v0.8.0,TODO: generalize to multiple treatment case?,1
v0.8.0,"TODO: what if input is sparse? - there's no equivalent to einsum,",1
v0.8.0,TODO Share some logic with non-discrete version,1
v0.8.0,TODO: any way to avoid creating a copy if the array was already dense?,1
v0.8.0,TODO: might be faster to break into connected components first,1
v0.8.0,TODO: Consider investigating other performance ideas for these cases,1
v0.8.0,TODO: would using einsum's paths to optimize the order of merging help?,1
v0.8.0,HACK: We're relying on some of sklearn's non-public classes which are not completely stable.,1
v0.8.0,"HACK: it's not optimal to use a regex like this, but the base class's node_to_str doesn't expose any",1
v0.8.0,TODO: Add a __dir__ implementation?,1
v0.8.0,TODO: studentized bootstrap? this would be more accurate in most cases but can we avoid,1
v0.8.0,Doesn't work with CNTK backend as of 2018-07-17 - see https://github.com/keras-team/keras/issues/10715,1
v0.8.0,For some reason this doesn't work at all when run against the CNTK backend...,1
v0.8.0,TODO: test that the estimated effect is usually within the bounds,1
v0.8.0,TODO: test that the estimated effect is usually within the bounds,1
v0.8.0,TODO: set up proper flag for this,1
v0.8.0,TODO: test something rather than just print...,1
v0.8.0,TODO: this tests that we can run the method; how do we test that the results are reasonable?,1
v0.8.0,TODO: add tests for extra properties like coef_ where they exist,1
v0.8.0,TODO: add tests for extra properties like coef_ where they exist,1
v0.8.0,TODO: ideally we could also test whether Z and X are jointly okay when both discrete,1
v0.8.0,HACK: work around bug in assertWarns (https://bugs.python.org/issue29620),1
v0.8.0,does not work well with parallelism.,1
v0.8.0,does not work well with parallelism.,1
v0.8.0,does not work well with parallelism.,1
v0.8.0,TODO: does this imply we should change some defaults to make this more likely to succeed?,1
v0.8.0,TODO: does this imply we should change some defaults to make this more likely to succeed?,1
v0.8.0,"TODO: Add other types of covariance estimation (e.g. Newey-West (HAC), HC2, HC3)",1
v0.8.0,TODO. This slicing should ultimately be done inside the parallel function,1
v0.8.0b1,-- Options for todo extension ----------------------------------------------,1
v0.8.0b1,"If true, `todo` and `todoList` produce output, else they produce nothing.",1
v0.8.0b1,TODO. Deal with multi-class instrument,1
v0.8.0b1,TODO. Deal with multi-class instrument,1
v0.8.0b1,TODO. The solution below is not really a valid cross-fitting,1
v0.8.0b1,"HACK: DRIV doesn't expect a treatment dimension, so pretend we got a vector even if we really had a one-column array",1
v0.8.0b1,"Once multiple treatments are supported, we'll need to fix this",1
v0.8.0b1,TODO. Deal with multi-class instrument/treatment,1
v0.8.0b1,TODO: Check performance,1
v0.8.0b1,"TODO: consider whether we need more care around stateful featurizers,",1
v0.8.0b1,TODO: support sample_var,1
v0.8.0b1,"TODO: consider whether we need more care around stateful featurizers,",1
v0.8.0b1,TODO: support vector T and Y,1
v0.8.0b1,TODO: is it right that the effective number of intruments is the,1
v0.8.0b1,TODO: allow the final model to actually use X? Then we'd need to rename the class,1
v0.8.0b1,TODO: allow the final model to actually use X?,1
v0.8.0b1,TODO: allow the final model to actually use X?,1
v0.8.0b1,TODO: would it be useful to extend to handle controls ala vanilla DML?,1
v0.8.0b1,"TODO: if opt_reweighted is False, we could change the logic to support multidimensional treatments,",1
v0.8.0b1,TODO: is there a good way to incorporate the other nuisance terms in the score?,1
v0.8.0b1,TODO: support sample_var,1
v0.8.0b1,"TODO: make sure that the public API consistently uses ""T"" instead of ""P"" for the treatment",1
v0.8.0b1,TODO: does the numeric stability actually make any difference?,1
v0.8.0b1,"CNTK backend can't randomize across batches and doesn't implement cumsum (at least as of June 2018,",1
v0.8.0b1,TODO: is there a more robust way to do this?,1
v0.8.0b1,TODO: do we need to give the user more control over other arguments to fit?,1
v0.8.0b1,TODO: do we need to give the user more control over other arguments to fit?,1
v0.8.0b1,"TODO: it seems like we need to sum over the batch because we can only apply gradient to a scalar,",1
v0.8.0b1,TODO: any way to get this to work on batches of arbitrary size?,1
v0.8.0b1,TODO: generalize to multiple treatment case?,1
v0.8.0b1,"TODO: what if input is sparse? - there's no equivalent to einsum,",1
v0.8.0b1,TODO Share some logic with non-discrete version,1
v0.8.0b1,TODO: any way to avoid creating a copy if the array was already dense?,1
v0.8.0b1,TODO: might be faster to break into connected components first,1
v0.8.0b1,TODO: Consider investigating other performance ideas for these cases,1
v0.8.0b1,TODO: would using einsum's paths to optimize the order of merging help?,1
v0.8.0b1,"TODO: Add other types of covariance estimation (e.g. Newey-West (HAC), HC2, HC3)",1
v0.8.0b1,HACK: We're relying on some of sklearn's non-public classes which are not completely stable.,1
v0.8.0b1,"HACK: it's not optimal to use a regex like this, but the base class's node_to_str doesn't expose any",1
v0.8.0b1,TODO: Add a __dir__ implementation?,1
v0.8.0b1,Doesn't work with CNTK backend as of 2018-07-17 - see https://github.com/keras-team/keras/issues/10715,1
v0.8.0b1,For some reason this doesn't work at all when run against the CNTK backend...,1
v0.8.0b1,TODO: test that the estimated effect is usually within the bounds,1
v0.8.0b1,TODO: test that the estimated effect is usually within the bounds,1
v0.8.0b1,TODO: set up proper flag for this,1
v0.8.0b1,TODO: test something rather than just print...,1
v0.8.0b1,TODO: this tests that we can run the method; how do we test that the results are reasonable?,1
v0.8.0b1,TODO: add tests for extra properties like coef_ where they exist,1
v0.8.0b1,TODO: add tests for extra properties like coef_ where they exist,1
v0.8.0b1,TODO: ideally we could also test whether Z and X are jointly okay when both discrete,1
v0.8.0b1,HACK: work around bug in assertWarns (https://bugs.python.org/issue29620),1
v0.8.0b1,does not work well with parallelism.,1
v0.8.0b1,does not work well with parallelism.,1
v0.8.0b1,"TODO Add bootstrap inference, once discrete treatment issue is fixed",1
v0.8.0b1,TODO: add stratification to bootstrap so that we can use it even with discrete treatments,1
v0.8.0b1,TODO. This slicing should ultimately be done inside the parallel function,1
v0.7.0,-- Options for todo extension ----------------------------------------------,1
v0.7.0,"If true, `todo` and `todoList` produce output, else they produce nothing.",1
v0.7.0,TODO. Deal with multi-class instrument,1
v0.7.0,TODO. Deal with multi-class instrument,1
v0.7.0,TODO. The solution below is not really a valid cross-fitting,1
v0.7.0,"HACK: DRIV doesn't expect a treatment dimension, so pretend we got a vector even if we really had a one-column array",1
v0.7.0,"Once multiple treatments are supported, we'll need to fix this",1
v0.7.0,TODO. Deal with multi-class instrument/treatment,1
v0.7.0,TODO: Check performance,1
v0.7.0,"TODO: consider whether we need more care around stateful featurizers,",1
v0.7.0,TODO: support sample_var,1
v0.7.0,"TODO: consider whether we need more care around stateful featurizers,",1
v0.7.0,TODO: support vector T and Y,1
v0.7.0,TODO: is it right that the effective number of intruments is the,1
v0.7.0,TODO: allow the final model to actually use X? Then we'd need to rename the class,1
v0.7.0,TODO: allow the final model to actually use X?,1
v0.7.0,TODO: allow the final model to actually use X?,1
v0.7.0,TODO: would it be useful to extend to handle controls ala vanilla DML?,1
v0.7.0,"TODO: if opt_reweighted is False, we could change the logic to support multidimensional treatments,",1
v0.7.0,TODO: is there a good way to incorporate the other nuisance terms in the score?,1
v0.7.0,TODO: support sample_var,1
v0.7.0,"TODO: make sure that the public API consistently uses ""T"" instead of ""P"" for the treatment",1
v0.7.0,TODO: does the numeric stability actually make any difference?,1
v0.7.0,"CNTK backend can't randomize across batches and doesn't implement cumsum (at least as of June 2018,",1
v0.7.0,TODO: is there a more robust way to do this?,1
v0.7.0,TODO: do we need to give the user more control over other arguments to fit?,1
v0.7.0,TODO: do we need to give the user more control over other arguments to fit?,1
v0.7.0,"TODO: it seems like we need to sum over the batch because we can only apply gradient to a scalar,",1
v0.7.0,TODO: any way to get this to work on batches of arbitrary size?,1
v0.7.0,TODO: generalize to multiple treatment case?,1
v0.7.0,"TODO: what if input is sparse? - there's no equivalent to einsum,",1
v0.7.0,TODO Share some logic with non-discrete version,1
v0.7.0,TODO: any way to avoid creating a copy if the array was already dense?,1
v0.7.0,TODO: might be faster to break into connected components first,1
v0.7.0,TODO: Consider investigating other performance ideas for these cases,1
v0.7.0,TODO: would using einsum's paths to optimize the order of merging help?,1
v0.7.0,"TODO: Add other types of covariance estimation (e.g. Newey-West (HAC), HC2, HC3)",1
v0.7.0,HACK: We're relying on some of sklearn's non-public classes which are not completely stable.,1
v0.7.0,"HACK: it's not optimal to use a regex like this, but the base class's node_to_str doesn't expose any",1
v0.7.0,TODO: Add a __dir__ implementation?,1
v0.7.0,Doesn't work with CNTK backend as of 2018-07-17 - see https://github.com/keras-team/keras/issues/10715,1
v0.7.0,For some reason this doesn't work at all when run against the CNTK backend...,1
v0.7.0,TODO: test that the estimated effect is usually within the bounds,1
v0.7.0,TODO: test that the estimated effect is usually within the bounds,1
v0.7.0,TODO: set up proper flag for this,1
v0.7.0,TODO: test something rather than just print...,1
v0.7.0,TODO: this tests that we can run the method; how do we test that the results are reasonable?,1
v0.7.0,TODO: add tests for extra properties like coef_ where they exist,1
v0.7.0,TODO: add tests for extra properties like coef_ where they exist,1
v0.7.0,TODO: ideally we could also test whether Z and X are jointly okay when both discrete,1
v0.7.0,HACK: work around bug in assertWarns (https://bugs.python.org/issue29620),1
v0.7.0,does not work well with parallelism.,1
v0.7.0,does not work well with parallelism.,1
v0.7.0,"TODO Add bootstrap inference, once discrete treatment issue is fixed",1
v0.7.0,TODO: add stratification to bootstrap so that we can use it even with discrete treatments,1
v0.7.0,TODO. This slicing should ultimately be done inside the parallel function,1
v0.7.0b1,-- Options for todo extension ----------------------------------------------,1
v0.7.0b1,"If true, `todo` and `todoList` produce output, else they produce nothing.",1
v0.7.0b1,TODO. Deal with multi-class instrument,1
v0.7.0b1,TODO. Deal with multi-class instrument,1
v0.7.0b1,TODO. The solution below is not really a valid cross-fitting,1
v0.7.0b1,"HACK: DRIV doesn't expect a treatment dimension, so pretend we got a vector even if we really had a one-column array",1
v0.7.0b1,"Once multiple treatments are supported, we'll need to fix this",1
v0.7.0b1,TODO. Deal with multi-class instrument/treatment,1
v0.7.0b1,TODO: Check performance,1
v0.7.0b1,"TODO: consider whether we need more care around stateful featurizers,",1
v0.7.0b1,TODO: support sample_var,1
v0.7.0b1,"TODO: consider whether we need more care around stateful featurizers,",1
v0.7.0b1,TODO: support vector T and Y,1
v0.7.0b1,TODO: is it right that the effective number of intruments is the,1
v0.7.0b1,TODO: allow the final model to actually use X? Then we'd need to rename the class,1
v0.7.0b1,TODO: allow the final model to actually use X?,1
v0.7.0b1,TODO: allow the final model to actually use X?,1
v0.7.0b1,TODO: would it be useful to extend to handle controls ala vanilla DML?,1
v0.7.0b1,"TODO: if opt_reweighted is False, we could change the logic to support multidimensional treatments,",1
v0.7.0b1,TODO: is there a good way to incorporate the other nuisance terms in the score?,1
v0.7.0b1,TODO: support sample_var,1
v0.7.0b1,"TODO: make sure that the public API consistently uses ""T"" instead of ""P"" for the treatment",1
v0.7.0b1,TODO: does the numeric stability actually make any difference?,1
v0.7.0b1,"CNTK backend can't randomize across batches and doesn't implement cumsum (at least as of June 2018,",1
v0.7.0b1,TODO: is there a more robust way to do this?,1
v0.7.0b1,TODO: do we need to give the user more control over other arguments to fit?,1
v0.7.0b1,TODO: do we need to give the user more control over other arguments to fit?,1
v0.7.0b1,"TODO: it seems like we need to sum over the batch because we can only apply gradient to a scalar,",1
v0.7.0b1,TODO: any way to get this to work on batches of arbitrary size?,1
v0.7.0b1,"TODO: what if input is sparse? - there's no equivalent to einsum,",1
v0.7.0b1,TODO Share some logic with non-discrete version,1
v0.7.0b1,TODO: any way to avoid creating a copy if the array was already dense?,1
v0.7.0b1,TODO: might be faster to break into connected components first,1
v0.7.0b1,TODO: Consider investigating other performance ideas for these cases,1
v0.7.0b1,TODO: would using einsum's paths to optimize the order of merging help?,1
v0.7.0b1,"TODO: Add other types of covariance estimation (e.g. Newey-West (HAC), HC2, HC3)",1
v0.7.0b1,HACK: We're relying on some of sklearn's non-public classes which are not completely stable.,1
v0.7.0b1,"HACK: it's not optimal to use a regex like this, but the base class's node_to_str doesn't expose any",1
v0.7.0b1,TODO: Add a __dir__ implementation?,1
v0.7.0b1,Doesn't work with CNTK backend as of 2018-07-17 - see https://github.com/keras-team/keras/issues/10715,1
v0.7.0b1,For some reason this doesn't work at all when run against the CNTK backend...,1
v0.7.0b1,TODO: test that the estimated effect is usually within the bounds,1
v0.7.0b1,TODO: test that the estimated effect is usually within the bounds,1
v0.7.0b1,TODO: set up proper flag for this,1
v0.7.0b1,TODO: test something rather than just print...,1
v0.7.0b1,TODO: this tests that we can run the method; how do we test that the results are reasonable?,1
v0.7.0b1,TODO: add tests for extra properties like coef_ where they exist,1
v0.7.0b1,TODO: add tests for extra properties like coef_ where they exist,1
v0.7.0b1,TODO: ideally we could also test whether Z and X are jointly okay when both discrete,1
v0.7.0b1,TODO: make IV related,1
v0.7.0b1,HACK: work around bug in assertWarns (https://bugs.python.org/issue29620),1
v0.7.0b1,does not work well with parallelism.,1
v0.7.0b1,does not work well with parallelism.,1
v0.7.0b1,"TODO Add bootstrap inference, once discrete treatment issue is fixed",1
v0.7.0b1,TODO: add stratification to bootstrap so that we can use it even with discrete treatments,1
v0.7.0b1,TODO. This slicing should ultimately be done inside the parallel function,1
v0.6.1,-- Options for todo extension ----------------------------------------------,1
v0.6.1,"If true, `todo` and `todoList` produce output, else they produce nothing.",1
v0.6.1,TODO. Deal with multi-class instrument,1
v0.6.1,TODO. Deal with multi-class instrument,1
v0.6.1,TODO. The solution below is not really a valid cross-fitting,1
v0.6.1,"HACK: DRIV doesn't expect a treatment dimension, so pretend we got a vector even if we really had a one-column array",1
v0.6.1,"Once multiple treatments are supported, we'll need to fix this",1
v0.6.1,TODO. Deal with multi-class instrument/treatment,1
v0.6.1,TODO: Check performance,1
v0.6.1,"TODO: consider whether we need more care around stateful featurizers,",1
v0.6.1,TODO: support sample_var,1
v0.6.1,"TODO: consider whether we need more care around stateful featurizers,",1
v0.6.1,TODO: support sample_var,1
v0.6.1,"TODO: make sure that the public API consistently uses ""T"" instead of ""P"" for the treatment",1
v0.6.1,TODO: does the numeric stability actually make any difference?,1
v0.6.1,"CNTK backend can't randomize across batches and doesn't implement cumsum (at least as of June 2018,",1
v0.6.1,TODO: is there a more robust way to do this?,1
v0.6.1,TODO: do we need to give the user more control over other arguments to fit?,1
v0.6.1,TODO: do we need to give the user more control over other arguments to fit?,1
v0.6.1,"TODO: it seems like we need to sum over the batch because we can only apply gradient to a scalar,",1
v0.6.1,TODO: any way to get this to work on batches of arbitrary size?,1
v0.6.1,"TODO: what if input is sparse? - there's no equivalent to einsum,",1
v0.6.1,TODO Share some logic with non-discrete version,1
v0.6.1,TODO: any way to avoid creating a copy if the array was already dense?,1
v0.6.1,TODO: might be faster to break into connected components first,1
v0.6.1,TODO: Consider investigating other performance ideas for these cases,1
v0.6.1,TODO: would using einsum's paths to optimize the order of merging help?,1
v0.6.1,"TODO: Add other types of covariance estimation (e.g. Newey-West (HAC), HC2, HC3)",1
v0.6.1,HACK: We're relying on some of sklearn's non-public classes which are not completely stable.,1
v0.6.1,"HACK: it's not optimal to use a regex like this, but the base class's node_to_str doesn't expose any",1
v0.6.1,TODO: Add a __dir__ implementation?,1
v0.6.1,Doesn't work with CNTK backend as of 2018-07-17 - see https://github.com/keras-team/keras/issues/10715,1
v0.6.1,For some reason this doesn't work at all when run against the CNTK backend...,1
v0.6.1,TODO: test that the estimated effect is usually within the bounds,1
v0.6.1,TODO: test that the estimated effect is usually within the bounds,1
v0.6.1,TODO: set up proper flag for this,1
v0.6.1,TODO: this tests that we can run the method; how do we test that the results are reasonable?,1
v0.6.1,HACK: work around bug in assertWarns (https://bugs.python.org/issue29620),1
v0.6.1,does not work well with parallelism.,1
v0.6.1,does not work well with parallelism.,1
v0.6.1,"TODO Add bootstrap inference, once discrete treatment issue is fixed",1
v0.6.1,TODO: add stratification to bootstrap so that we can use it even with discrete treatments,1
v0.6.1,TODO. This slicing should ultimately be done inside the parallel function,1
v0.6,-- Options for todo extension ----------------------------------------------,1
v0.6,"If true, `todo` and `todoList` produce output, else they produce nothing.",1
v0.6,TODO. Deal with multi-class instrument,1
v0.6,TODO. Deal with multi-class instrument,1
v0.6,TODO. The solution below is not really a valid cross-fitting,1
v0.6,"HACK: DRIV doesn't expect a treatment dimension, so pretend we got a vector even if we really had a one-column array",1
v0.6,"Once multiple treatments are supported, we'll need to fix this",1
v0.6,TODO. Deal with multi-class instrument/treatment,1
v0.6,TODO: Check performance,1
v0.6,"TODO: consider whether we need more care around stateful featurizers,",1
v0.6,TODO: support sample_var,1
v0.6,"TODO: consider whether we need more care around stateful featurizers,",1
v0.6,TODO: support sample_var,1
v0.6,"TODO: make sure that the public API consistently uses ""T"" instead of ""P"" for the treatment",1
v0.6,TODO: does the numeric stability actually make any difference?,1
v0.6,"CNTK backend can't randomize across batches and doesn't implement cumsum (at least as of June 2018,",1
v0.6,TODO: is there a more robust way to do this?,1
v0.6,TODO: do we need to give the user more control over other arguments to fit?,1
v0.6,TODO: do we need to give the user more control over other arguments to fit?,1
v0.6,"TODO: it seems like we need to sum over the batch because we can only apply gradient to a scalar,",1
v0.6,TODO: any way to get this to work on batches of arbitrary size?,1
v0.6,"TODO: what if input is sparse? - there's no equivalent to einsum,",1
v0.6,TODO Share some logic with non-discrete version,1
v0.6,TODO: any way to avoid creating a copy if the array was already dense?,1
v0.6,TODO: might be faster to break into connected components first,1
v0.6,TODO: Consider investigating other performance ideas for these cases,1
v0.6,TODO: would using einsum's paths to optimize the order of merging help?,1
v0.6,"TODO: Add other types of covariance estimation (e.g. Newey-West (HAC), HC2, HC3)",1
v0.6,HACK: We're relying on some of sklearn's non-public classes which are not completely stable.,1
v0.6,"HACK: it's not optimal to use a regex like this, but the base class's node_to_str doesn't expose any",1
v0.6,TODO: Add a __dir__ implementation?,1
v0.6,Doesn't work with CNTK backend as of 2018-07-17 - see https://github.com/keras-team/keras/issues/10715,1
v0.6,For some reason this doesn't work at all when run against the CNTK backend...,1
v0.6,TODO: test that the estimated effect is usually within the bounds,1
v0.6,TODO: test that the estimated effect is usually within the bounds,1
v0.6,TODO: set up proper flag for this,1
v0.6,TODO: this tests that we can run the method; how do we test that the results are reasonable?,1
v0.6,HACK: work around bug in assertWarns (https://bugs.python.org/issue29620),1
v0.6,does not work well with parallelism.,1
v0.6,does not work well with parallelism.,1
v0.6,"TODO Add bootstrap inference, once discrete treatment issue is fixed",1
v0.6,TODO: add stratification to bootstrap so that we can use it even with discrete treatments,1
v0.6,TODO. This slicing should ultimately be done inside the parallel function,1
v0.5,-- Options for todo extension ----------------------------------------------,1
v0.5,"If true, `todo` and `todoList` produce output, else they produce nothing.",1
v0.5,TODO. Deal with multi-class instrument,1
v0.5,TODO. Deal with multi-class instrument,1
v0.5,TODO. The solution below is not really a valid cross-fitting,1
v0.5,"HACK: DRIV doesn't expect a treatment dimension, so pretend we got a vector even if we really had a one-column array",1
v0.5,"Once multiple treatments are supported, we'll need to fix this",1
v0.5,TODO. Deal with multi-class instrument/treatment,1
v0.5,TODO: Check performance,1
v0.5,"TODO: consider whether we need more care around stateful featurizers,",1
v0.5,TODO: support sample_var,1
v0.5,TODO: support sample_var,1
v0.5,"TODO: make sure that the public API consistently uses ""T"" instead of ""P"" for the treatment",1
v0.5,TODO: does the numeric stability actually make any difference?,1
v0.5,"CNTK backend can't randomize across batches and doesn't implement cumsum (at least as of June 2018,",1
v0.5,TODO: is there a more robust way to do this?,1
v0.5,TODO: do we need to give the user more control over other arguments to fit?,1
v0.5,TODO: do we need to give the user more control over other arguments to fit?,1
v0.5,"TODO: it seems like we need to sum over the batch because we can only apply gradient to a scalar,",1
v0.5,TODO: any way to get this to work on batches of arbitrary size?,1
v0.5,"TODO: what if input is sparse? - there's no equivalent to einsum,",1
v0.5,TODO Share some logic with non-discrete version,1
v0.5,TODO: any way to avoid creating a copy if the array was already dense?,1
v0.5,TODO: might be faster to break into connected components first,1
v0.5,TODO: Consider investigating other performance ideas for these cases,1
v0.5,TODO: would using einsum's paths to optimize the order of merging help?,1
v0.5,"TODO: Add other types of covariance estimation (e.g. Newey-West (HAC), HC2, HC3)",1
v0.5,TODO: allow different subsets for L1 and L2 regularization?,1
v0.5,TODO: any better way to deal with sparsity?,1
v0.5,TODO: any better way to deal with sparsity?,1
v0.5,TODO: would be nice to relax this somehow,1
v0.5,TODO: Add a __dir__ implementation?,1
v0.5,TODO: make this test actually test something instead of generating images,1
v0.5,Doesn't work with CNTK backend as of 2018-07-17 - see https://github.com/keras-team/keras/issues/10715,1
v0.5,For some reason this doesn't work at all when run against the CNTK backend...,1
v0.5,TODO: test that the estimated effect is usually within the bounds,1
v0.5,TODO: test that the estimated effect is usually within the bounds,1
v0.5,TODO: set up proper flag for this,1
v0.5,TODO: this tests that we can run the method; how do we test that the results are reasonable?,1
v0.5,TODO: add stratification to bootstrap so that we can use it even with discrete treatments,1
v0.5,TODO: add stratification to bootstrap so that we can use it even with discrete treatments,1
v0.4,-- Options for todo extension ----------------------------------------------,1
v0.4,"If true, `todo` and `todoList` produce output, else they produce nothing.",1
v0.4,TODO. Deal with multi-class instrument,1
v0.4,TODO. Deal with multi-class instrument,1
v0.4,TODO. The solution below is not really a valid cross-fitting,1
v0.4,"HACK: DRIV doesn't expect a treatment dimension, so pretend we got a vector even if we really had a one-column array",1
v0.4,"Once multiple treatments are supported, we'll need to fix this",1
v0.4,TODO. Deal with multi-class instrument/treatment,1
v0.4,TODO: Check performance,1
v0.4,"TODO: If T is a vector rather than a 2-D array, then the model's fit must accept a vector...",1
v0.4,TODO: should this logic be moved up to the LinearCateEstimator class and,1
v0.4,TODO: Doing this kronecker/reshaping/transposing stuff so that predict can be called,1
v0.4,TODO: handle case where final model doesn't directly expose coef_?,1
v0.4,"TODO: make sure that the public API consistently uses ""T"" instead of ""P"" for the treatment",1
v0.4,TODO: does the numeric stability actually make any difference?,1
v0.4,"CNTK backend can't randomize across batches and doesn't implement cumsum (at least as of June 2018,",1
v0.4,TODO: is there a more robust way to do this?,1
v0.4,TODO: do we need to give the user more control over other arguments to fit?,1
v0.4,TODO: do we need to give the user more control over other arguments to fit?,1
v0.4,"TODO: it seems like we need to sum over the batch because we can only apply gradient to a scalar,",1
v0.4,TODO: any way to get this to work on batches of arbitrary size?,1
v0.4,"TODO: what if input is sparse? - there's no equivalent to einsum,",1
v0.4,"TODO: if T0 or T1 are scalars, we'll promote them to vectors;",1
v0.4,TODO: any way to avoid creating a copy if the array was already dense?,1
v0.4,TODO: wouldn't making X1 vary more slowly than X2 be more intuitive?,1
v0.4,TODO: might be faster to break into connected components first,1
v0.4,TODO: Consider investigating other performance ideas for these cases,1
v0.4,TODO: would using einsum's paths to optimize the order of merging help?,1
v0.4,TODO: allow different subsets for L1 and L2 regularization?,1
v0.4,TODO: any better way to deal with sparsity?,1
v0.4,TODO: any better way to deal with sparsity?,1
v0.4,TODO: would be nice to relax this somehow,1
v0.4,TODO: Add a __dir__ implementation?,1
v0.4,TODO: what if some args can be None?,1
v0.4,TODO: make this test actually test something instead of generating images,1
v0.4,Doesn't work with CNTK backend as of 2018-07-17 - see https://github.com/keras-team/keras/issues/10715,1
v0.4,For some reason this doesn't work at all when run against the CNTK backend...,1
v0.4,TODO: set up proper flag for this,1
v0.4,TODO: this tests that we can run the method; how do we test that the results are reasonable?,1
v0.4,TODO: it seems like roughly 20% of the calls to _test_sparse are failing - find out what's going wrong,1
v0.3,-- Options for todo extension ----------------------------------------------,1
v0.3,"If true, `todo` and `todoList` produce output, else they produce nothing.",1
v0.3,TODO: Check performance,1
v0.3,"TODO: If T is a vector rather than a 2-D array, then the model's fit must accept a vector...",1
v0.3,TODO: should this logic be moved up to the LinearCateEstimator class and,1
v0.3,TODO: Doing this kronecker/reshaping/transposing stuff so that predict can be called,1
v0.3,TODO: handle case where final model doesn't directly expose coef_?,1
v0.3,"TODO: make sure that the public API consistently uses ""T"" instead of ""P"" for the treatment",1
v0.3,TODO: does the numeric stability actually make any difference?,1
v0.3,"CNTK backend can't randomize across batches and doesn't implement cumsum (at least as of June 2018,",1
v0.3,TODO: allow 1D arguments for Y and T,1
v0.3,TODO: is there a more robust way to do this?,1
v0.3,TODO: do we need to give the user more control over other arguments to fit?,1
v0.3,TODO: do we need to give the user more control over other arguments to fit?,1
v0.3,"TODO: it seems like we need to sum over the batch because we can only apply gradient to a scalar,",1
v0.3,TODO: any way to get this to work on batches of arbitrary size?,1
v0.3,"TODO: what if input is sparse? - there's no equivalent to einsum,",1
v0.3,"TODO: if T0 or T1 are scalars, we'll promote them to vectors;",1
v0.3,TODO: any way to avoid creating a copy if the array was already dense?,1
v0.3,TODO: wouldn't making X1 vary more slowly than X2 be more intuitive?,1
v0.3,TODO: might be faster to break into connected components first,1
v0.3,TODO: Consider investigating other performance ideas for these cases,1
v0.3,TODO: would using einsum's paths to optimize the order of merging help?,1
v0.3,TODO: allow different subsets for L1 and L2 regularization?,1
v0.3,TODO: any better way to deal with sparsity?,1
v0.3,TODO: any better way to deal with sparsity?,1
v0.3,TODO: would be nice to relax this somehow,1
v0.3,TODO: Add a __dir__ implementation?,1
v0.3,TODO: what if some args can be None?,1
v0.3,TODO: make this test actually test something instead of generating images,1
v0.3,Doesn't work with CNTK backend as of 2018-07-17 - see https://github.com/keras-team/keras/issues/10715,1
v0.3,For some reason this doesn't work at all when run against the CNTK backend...,1
v0.3,TODO: set up proper flag for this,1
v0.3,TODO: this tests that we can run the method; how do we test that the results are reasonable?,1
v0.3,TODO: it seems like roughly 20% of the calls to _test_sparse are failing - find out what's going wrong,1
v0.2,-- Options for todo extension ----------------------------------------------,1
v0.2,"If true, `todo` and `todoList` produce output, else they produce nothing.",1
v0.2,TODO: Check performance,1
v0.2,"TODO: If T is a vector rather than a 2-D array, then the model's fit must accept a vector...",1
v0.2,TODO: should this logic be moved up to the LinearCateEstimator class and,1
v0.2,TODO: Doing this kronecker/reshaping/transposing stuff so that predict can be called,1
v0.2,TODO: handle case where final model doesn't directly expose coef_?,1
v0.2,"TODO: make sure that the public API consistently uses ""T"" instead of ""P"" for the treatment",1
v0.2,TODO: does the numeric stability actually make any difference?,1
v0.2,"CNTK backend can't randomize across batches and doesn't implement cumsum (at least as of June 2018,",1
v0.2,TODO: allow 1D arguments for Y and T,1
v0.2,TODO: is there a more robust way to do this?,1
v0.2,TODO: do we need to give the user more control over other arguments to fit?,1
v0.2,TODO: do we need to give the user more control over other arguments to fit?,1
v0.2,"TODO: it seems like we need to sum over the batch because we can only apply gradient to a scalar,",1
v0.2,TODO: any way to get this to work on batches of arbitrary size?,1
v0.2,"TODO: what if input is sparse? - there's no equivalent to einsum,",1
v0.2,TODO: any way to avoid creating a copy if the array was already dense?,1
v0.2,TODO: wouldn't making X1 vary more slowly than X2 be more intuitive?,1
v0.2,TODO: might be faster to break into connected components first,1
v0.2,TODO: Consider investigating other performance ideas for these cases,1
v0.2,TODO: would using einsum's paths to optimize the order of merging help?,1
v0.2,TODO: allow different subsets for L1 and L2 regularization?,1
v0.2,TODO: any better way to deal with sparsity?,1
v0.2,TODO: any better way to deal with sparsity?,1
v0.2,TODO: would be nice to relax this somehow,1
v0.2,TODO: Add a __dir__ implementation?,1
v0.2,TODO: what if some args can be None?,1
v0.2,TODO: make this test actually test something instead of generating images,1
v0.2,Doesn't work with CNTK backend as of 2018-07-17 - see https://github.com/keras-team/keras/issues/10715,1
v0.2,For some reason this doesn't work at all when run against the CNTK backend...,1
v0.2,TODO: set up proper flag for this,1
v0.2,TODO: this tests that we can run the method; how do we test that the results are reasonable?,1
v0.2,TODO: it seems like roughly 20% of the calls to _test_sparse are failing - find out what's going wrong,1
v0.1,-- Options for todo extension ----------------------------------------------,1
v0.1,"If true, `todo` and `todoList` produce output, else they produce nothing.",1
v0.1,TODO: Check performance,1
v0.1,"TODO: If T is a vector rather than a 2-D array, then the model's fit must accept a vector...",1
v0.1,TODO: Doing this kronecker/reshaping/transposing stuff so that predict can be called,1
v0.1,TODO: handle case where final model doesn't directly expose coef_?,1
v0.1,"TODO: yuck, is there any way to avoid having to know the structure of XW",1
v0.1,"TODO: make sure that the public API consistently uses ""T"" instead of ""P"" for the treatment",1
v0.1,TODO: does the numeric stability actually make any difference?,1
v0.1,"CNTK backend can't randomize across batches and doesn't implement cumsum (at least as of June 2018,",1
v0.1,TODO: allow 1D arguments for Y and T,1
v0.1,TODO: is there a more robust way to do this?,1
v0.1,TODO: do we need to give the user more control over other arguments to fit?,1
v0.1,TODO: do we need to give the user more control over other arguments to fit?,1
v0.1,"TODO: it seems like we need to sum over the batch because we can only apply gradient to a scalar,",1
v0.1,TODO: any way to get this to work on batches of arbitrary size?,1
v0.1,"TODO: what if input is sparse? - there's no equivalent to einsum,",1
v0.1,TODO: any way to avoid creating a copy if the array was already dense?,1
v0.1,TODO: wouldn't making X1 vary more slowly than X2 be more intuitive?,1
v0.1,TODO: might be faster to break into connected components first,1
v0.1,TODO: Consider investigating other performance ideas for these cases,1
v0.1,TODO: would using einsum's paths to optimize the order of merging help?,1
v0.1,TODO: allow different subsets for L1 and L2 regularization?,1
v0.1,TODO: any better way to deal with sparsity?,1
v0.1,TODO: any better way to deal with sparsity?,1
v0.1,TODO: would be nice to relax this somehow,1
v0.1,TODO: Add a __dir__ implementation?,1
v0.1,TODO: what if some args can be None?,1
v0.1,TODO: make this test actually test something instead of generating images,1
v0.1,Doesn't work with CNTK backend as of 2018-07-17 - see https://github.com/keras-team/keras/issues/10715,1
v0.1,For some reason this doesn't work at all when run against the CNTK backend...,1
v0.1,TODO: set up proper flag for this,1
v0.1,TODO: this tests that we can run the method; how do we test that the results are reasonable?,1
v0.1,TODO: it seems like roughly 20% of the calls to _test_sparse are failing - find out what's going wrong,1
