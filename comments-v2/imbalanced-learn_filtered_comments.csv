Version,Commit Message
0.13.0,This file is here so that when running from the root folder
0.13.0,./imblearn is added to sys.path by pytest.
0.13.0,See https://docs.pytest.org/en/latest/pythonpath.html for more details.
0.13.0,"For example, this allows to build extensions in place and run pytest"
0.13.0,doc/modules/clustering.rst and use imblearn from the local folder
0.13.0,rather than the one from site-packages.
0.13.0,use legacy numpy print options to avoid failures due to NumPy 2.+ scalar
0.13.0,representation
0.13.0,-*- coding: utf-8 -*-
0.13.0,
0.13.0,"imbalanced-learn documentation build configuration file, created by"
0.13.0,sphinx-quickstart on Mon Jan 18 14:44:12 2016.
0.13.0,
0.13.0,This file is execfile()d with the current directory set to its
0.13.0,containing dir.
0.13.0,
0.13.0,Note that not all possible configuration values are present in this
0.13.0,autogenerated file.
0.13.0,
0.13.0,All configuration values have a default; values that are commented out
0.13.0,serve to show the default.
0.13.0,"If extensions (or modules to document with autodoc) are in another directory,"
0.13.0,add these directories to sys.path here. If the directory is relative to the
0.13.0,"documentation root, use os.path.abspath to make it absolute, like shown here."
0.13.0,-- General configuration ------------------------------------------------
0.13.0,"If your documentation needs a minimal Sphinx version, state it here."
0.13.0,needs_sphinx = '1.0'
0.13.0,"Add any Sphinx extension module names here, as strings. They can be"
0.13.0,extensions coming with Sphinx (named 'sphinx.ext.*') or your custom
0.13.0,ones.
0.13.0,Specify how to identify the prompt when copying code snippets
0.13.0,"Add any paths that contain templates here, relative to this directory."
0.13.0,The suffix of source filenames.
0.13.0,The master toctree document.
0.13.0,General information about the project.
0.13.0,"The version info for the project you're documenting, acts as replacement for"
0.13.0,"|version| and |release|, also used in various other places throughout the"
0.13.0,built documents.
0.13.0,
0.13.0,The short X.Y version.
0.13.0,"The full version, including alpha/beta/rc tags."
0.13.0,"List of patterns, relative to source directory, that match files and"
0.13.0,directories to ignore when looking for source files.
0.13.0,The reST default role (used for this markup: `text`) to use for all
0.13.0,documents.
0.13.0,"If true, '()' will be appended to :func: etc. cross-reference text."
0.13.0,The name of the Pygments (syntax highlighting) style to use.
0.13.0,-- Options for HTML output ----------------------------------------------
0.13.0,The theme to use for HTML and HTML Help pages.  See the documentation for
0.13.0,a list of builtin themes.
0.13.0,"""navbar_align"": ""right"",  # For testing that the navbar items align properly"
0.13.0,"Add any paths that contain custom static files (such as style sheets) here,"
0.13.0,"relative to this directory. They are copied after the builtin static files,"
0.13.0,"so a file named ""default.css"" will overwrite the builtin ""default.css""."
0.13.0,Output file base name for HTML help builder.
0.13.0,-- Options for autodoc ------------------------------------------------------
0.13.0,generate autosummary even if no references
0.13.0,-- Options for numpydoc -----------------------------------------------------
0.13.0,this is needed for some reason...
0.13.0,see https://github.com/numpy/numpydoc/issues/69
0.13.0,-- Options for sphinxcontrib-bibtex -----------------------------------------
0.13.0,bibtex file
0.13.0,-- Options for intersphinx --------------------------------------------------
0.13.0,intersphinx configuration
0.13.0,-- Options for sphinx-gallery -----------------------------------------------
0.13.0,Generate the plot for the gallery
0.13.0,sphinx-gallery configuration
0.13.0,-- Options for github link for what's new -----------------------------------
0.13.0,Config for sphinx_issues
0.13.0,The following is used by sphinx.ext.linkcode to provide links to github
0.13.0,-- Options for LaTeX output ---------------------------------------------
0.13.0,The paper size ('letterpaper' or 'a4paper').
0.13.0,"'papersize': 'letterpaper',"
0.13.0,"The font size ('10pt', '11pt' or '12pt')."
0.13.0,"'pointsize': '10pt',"
0.13.0,Additional stuff for the LaTeX preamble.
0.13.0,"'preamble': '',"
0.13.0,Grouping the document tree into LaTeX files. List of tuples
0.13.0,"(source start file, target name, title,"
0.13.0,"author, documentclass [howto, manual, or own class])."
0.13.0,-- Options for manual page output ---------------------------------------
0.13.0,"If false, no module index is generated."
0.13.0,latex_domain_indices = True
0.13.0,One entry per manual page. List of tuples
0.13.0,"(source start file, name, description, authors, manual section)."
0.13.0,"If true, show URL addresses after external links."
0.13.0,man_show_urls = False
0.13.0,-- Options for Texinfo output -------------------------------------------
0.13.0,Grouping the document tree into Texinfo files. List of tuples
0.13.0,"(source start file, target name, title, author,"
0.13.0,"dir menu entry, description, category)"
0.13.0,-- Dependencies generation ----------------------------------------------
0.13.0,get length of header
0.13.0,-- Additional temporary hacks -----------------------------------------------
0.13.0,get the styles from the current theme
0.13.0,create and add the button to all the code blocks that contain >>>
0.13.0,tracebacks (.gt) contain bare text elements that need to be
0.13.0,wrapped in a span to work with .nextUntil() (see later)
0.13.0,define the behavior of the button when it's clicked
0.13.0,hide the code output
0.13.0,show the code output
0.13.0,-*- coding: utf-8 -*-
0.13.0,Format template for issues URI
0.13.0,e.g. 'https://github.com/sloria/marshmallow/issues/{issue}
0.13.0,Format template for PR URI
0.13.0,e.g. 'https://github.com/sloria/marshmallow/pull/{issue}
0.13.0,Format template for commit URI
0.13.0,e.g. 'https://github.com/sloria/marshmallow/commits/{commit}
0.13.0,"Shortcut for Github, e.g. 'sloria/marshmallow'"
0.13.0,Format template for user profile URI
0.13.0,e.g. 'https://github.com/{user}'
0.13.0,Unwrap the object to get the correct source
0.13.0,file in case that is wrapped by a decorator
0.13.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.13.0,License: MIT
0.13.0,%%
0.13.0,%%
0.13.0,"First, we will generate a toy classification dataset with only few samples."
0.13.0,The ratio between the classes will be imbalanced.
0.13.0,%%
0.13.0,%%
0.13.0,"Now, we will use a :class:`~imblearn.over_sampling.RandomOverSampler` to"
0.13.0,generate a bootstrap for the minority class with as many samples as in the
0.13.0,majority class.
0.13.0,%%
0.13.0,%%
0.13.0,We observe that the minority samples are less transparent than the samples
0.13.0,"from the majority class. Indeed, it is due to the fact that these samples"
0.13.0,of the minority class are repeated during the bootstrap generation.
0.13.0,
0.13.0,We can set `shrinkage` to a floating value to add a small perturbation to the
0.13.0,samples created and therefore create a smoothed bootstrap.
0.13.0,%%
0.13.0,%%
0.13.0,"In this case, we see that the samples in the minority class are not"
0.13.0,overlapping anymore due to the added noise.
0.13.0,
0.13.0,The parameter `shrinkage` allows to add more or less perturbation. Let's
0.13.0,add more perturbation when generating the smoothed bootstrap.
0.13.0,%%
0.13.0,%%
0.13.0,Increasing the value of `shrinkage` will disperse the new samples. Forcing
0.13.0,the shrinkage to 0 will be equivalent to generating a normal bootstrap.
0.13.0,%%
0.13.0,%%
0.13.0,"Therefore, the `shrinkage` is handy to manually tune the dispersion of the"
0.13.0,new samples.
0.13.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.13.0,License: MIT
0.13.0,%%
0.13.0,generate some data points
0.13.0,plot the majority and minority samples
0.13.0,draw the circle in which the new sample will generated
0.13.0,plot the line on which the sample will be generated
0.13.0,create and plot the new sample
0.13.0,make the plot nicer with legend and label
0.13.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.13.0,License: MIT
0.13.0,%%
0.13.0,%% [markdown]
0.13.0,The following function will be used to create toy dataset. It uses the
0.13.0,:func:`~sklearn.datasets.make_classification` from scikit-learn but fixing
0.13.0,some parameters.
0.13.0,%%
0.13.0,%% [markdown]
0.13.0,The following function will be used to plot the sample space after resampling
0.13.0,to illustrate the specificities of an algorithm.
0.13.0,%%
0.13.0,%% [markdown]
0.13.0,The following function will be used to plot the decision function of a
0.13.0,classifier given some data.
0.13.0,%%
0.13.0,%% [markdown]
0.13.0,Illustration of the influence of the balancing ratio
0.13.0,----------------------------------------------------
0.13.0,
0.13.0,We will first illustrate the influence of the balancing ratio on some toy
0.13.0,data using a logistic regression classifier which is a linear model.
0.13.0,%%
0.13.0,%% [markdown]
0.13.0,We will fit and show the decision boundary model to illustrate the impact of
0.13.0,dealing with imbalanced classes.
0.13.0,%%
0.13.0,%% [markdown]
0.13.0,"Greater is the difference between the number of samples in each class, poorer"
0.13.0,are the classification results.
0.13.0,
0.13.0,Random over-sampling to balance the data set
0.13.0,--------------------------------------------
0.13.0,
0.13.0,Random over-sampling can be used to repeat some samples and balance the
0.13.0,number of samples between the dataset. It can be seen that with this trivial
0.13.0,approach the boundary decision is already less biased toward the majority
0.13.0,class. The class :class:`~imblearn.over_sampling.RandomOverSampler`
0.13.0,implements such of a strategy.
0.13.0,%%
0.13.0,%% [markdown]
0.13.0,"By default, random over-sampling generates a bootstrap. The parameter"
0.13.0,`shrinkage` allows adding a small perturbation to the generated data
0.13.0,to generate a smoothed bootstrap instead. The plot below shows the difference
0.13.0,between the two data generation strategies.
0.13.0,%%
0.13.0,%% [markdown]
0.13.0,It looks like more samples are generated with smoothed bootstrap. This is due
0.13.0,to the fact that the samples generated are not superimposing with the
0.13.0,original samples.
0.13.0,
0.13.0,More advanced over-sampling using ADASYN and SMOTE
0.13.0,--------------------------------------------------
0.13.0,
0.13.0,Instead of repeating the same samples when over-sampling or perturbating the
0.13.0,"generated bootstrap samples, one can use some specific heuristic instead."
0.13.0,:class:`~imblearn.over_sampling.ADASYN` and
0.13.0,:class:`~imblearn.over_sampling.SMOTE` can be used in this case.
0.13.0,%%
0.13.0,%% [markdown]
0.13.0,The following plot illustrates the difference between
0.13.0,:class:`~imblearn.over_sampling.ADASYN` and
0.13.0,:class:`~imblearn.over_sampling.SMOTE`.
0.13.0,:class:`~imblearn.over_sampling.ADASYN` will focus on the samples which are
0.13.0,difficult to classify with a nearest-neighbors rule while regular
0.13.0,:class:`~imblearn.over_sampling.SMOTE` will not make any distinction.
0.13.0,"Therefore, the decision function depending of the algorithm."
0.13.0,%% [markdown]
0.13.0,"Due to those sampling particularities, it can give rise to some specific"
0.13.0,issues as illustrated below.
0.13.0,%%
0.13.0,%% [markdown]
0.13.0,SMOTE proposes several variants by identifying specific samples to consider
0.13.0,during the resampling. The borderline version
0.13.0,(:class:`~imblearn.over_sampling.BorderlineSMOTE`) will detect which point to
0.13.0,select which are in the border between two classes. The SVM version
0.13.0,(:class:`~imblearn.over_sampling.SVMSMOTE`) will use the support vectors
0.13.0,found using an SVM algorithm to create new sample while the KMeans version
0.13.0,(:class:`~imblearn.over_sampling.KMeansSMOTE`) will make a clustering before
0.13.0,to generate samples in each cluster independently depending each cluster
0.13.0,density.
0.13.0,%%
0.13.0,%% [markdown]
0.13.0,"When dealing with a mixed of continuous and categorical features,"
0.13.0,:class:`~imblearn.over_sampling.SMOTENC` is the only method which can handle
0.13.0,this case.
0.13.0,%%
0.13.0,Create a dataset of a mix of numerical and categorical data
0.13.0,%% [markdown]
0.13.0,"However, if the dataset is composed of only categorical features then one"
0.13.0,should use :class:`~imblearn.over_sampling.SMOTEN`.
0.13.0,%%
0.13.0,Generate only categorical data
0.13.0,Authors: Christos Aridas
0.13.0,Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.13.0,License: MIT
0.13.0,%%
0.13.0,%% [markdown]
0.13.0,Let's first generate a dataset with imbalanced class distribution.
0.13.0,%%
0.13.0,%% [markdown]
0.13.0,We will use an over-sampler :class:`~imblearn.over_sampling.SMOTE` followed
0.13.0,by a :class:`~sklearn.tree.DecisionTreeClassifier`. The aim will be to
0.13.0,search which `k_neighbors` parameter is the most adequate with the dataset
0.13.0,that we generated.
0.13.0,%%
0.13.0,%% [markdown]
0.13.0,We can use the :class:`~sklearn.model_selection.validation_curve` to inspect
0.13.0,"the impact of varying the parameter `k_neighbors`. In this case, we need"
0.13.0,to use a score to evaluate the generalization score during the
0.13.0,cross-validation.
0.13.0,%%
0.13.0,%%
0.13.0,%% [markdown]
0.13.0,We can now plot the results of the cross-validation for the different
0.13.0,parameter values that we tried.
0.13.0,%%
0.13.0,make nice plotting
0.13.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.13.0,License: MIT
0.13.0,Generate a dataset
0.13.0,Split the data
0.13.0,Train the classifier with balancing
0.13.0,Test the classifier and get the prediction
0.13.0,Show the classification report
0.13.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.13.0,License: MIT
0.13.0,%%
0.13.0,%% [markdown]
0.13.0,"First, we will generate some imbalanced dataset."
0.13.0,%%
0.13.0,%% [markdown]
0.13.0,We will split the data into a training and testing set.
0.13.0,%%
0.13.0,%% [markdown]
0.13.0,We will create a pipeline made of a :class:`~imblearn.over_sampling.SMOTE`
0.13.0,over-sampler followed by a :class:`~sklearn.linear_model.LogisticRegression`
0.13.0,classifier.
0.13.0,%%
0.13.0,%% [markdown]
0.13.0,"Now, we will train the model on the training set and get the prediction"
0.13.0,associated with the testing set. Be aware that the resampling will happen
0.13.0,only when calling `fit`: the number of samples in `y_pred` is the same than
0.13.0,in `y_test`.
0.13.0,%%
0.13.0,%% [markdown]
0.13.0,The geometric mean corresponds to the square root of the product of the
0.13.0,sensitivity and specificity. Combining the two metrics should account for
0.13.0,the balancing of the dataset.
0.13.0,%%
0.13.0,%% [markdown]
0.13.0,The index balanced accuracy can transform any metric to be used in
0.13.0,imbalanced learning problems.
0.13.0,%%
0.13.0,%%
0.13.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.13.0,License: MIT
0.13.0,%%
0.13.0,%% [markdown]
0.13.0,Dataset generation
0.13.0,------------------
0.13.0,
0.13.0,We will create an imbalanced dataset with a couple of samples. We will use
0.13.0,:func:`~sklearn.datasets.make_classification` to generate this dataset.
0.13.0,%%
0.13.0,%%
0.13.0,%% [markdown]
0.13.0,The following function will be used to plot the sample space after resampling
0.13.0,to illustrate the characteristic of an algorithm.
0.13.0,%%
0.13.0,%% [markdown]
0.13.0,The following function will be used to plot the decision function of a
0.13.0,classifier given some data.
0.13.0,%%
0.13.0,%% [markdown]
0.13.0,":class:`~imblearn.over_sampling.SMOTE` allows to generate samples. However,"
0.13.0,this method of over-sampling does not have any knowledge regarding the
0.13.0,"underlying distribution. Therefore, some noisy samples can be generated, e.g."
0.13.0,"when the different classes cannot be well separated. Hence, it can be"
0.13.0,beneficial to apply an under-sampling algorithm to clean the noisy samples.
0.13.0,Two methods are usually used in the literature: (i) Tomek's link and (ii)
0.13.0,edited nearest neighbours cleaning methods. Imbalanced-learn provides two
0.13.0,ready-to-use samplers :class:`~imblearn.combine.SMOTETomek` and
0.13.0,":class:`~imblearn.combine.SMOTEENN`. In general,"
0.13.0,:class:`~imblearn.combine.SMOTEENN` cleans more noisy data than
0.13.0,:class:`~imblearn.combine.SMOTETomek`.
0.13.0,%%
0.13.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.13.0,License: MIT
0.13.0,%%
0.13.0,%% [markdown]
0.13.0,Load an imbalanced dataset
0.13.0,--------------------------
0.13.0,
0.13.0,We will load the UCI SatImage dataset which has an imbalanced ratio of 9.3:1
0.13.0,(number of majority sample for a minority sample). The data are then split
0.13.0,into training and testing.
0.13.0,%%
0.13.0,%% [markdown]
0.13.0,Classification using a single decision tree
0.13.0,-------------------------------------------
0.13.0,
0.13.0,We train a decision tree classifier which will be used as a baseline for the
0.13.0,rest of this example.
0.13.0,
0.13.0,The results are reported in terms of balanced accuracy and geometric mean
0.13.0,which are metrics widely used in the literature to validate model trained on
0.13.0,imbalanced set.
0.13.0,%%
0.13.0,%%
0.13.0,%%
0.13.0,%% [markdown]
0.13.0,Classification using bagging classifier with and without sampling
0.13.0,-----------------------------------------------------------------
0.13.0,
0.13.0,"Instead of using a single tree, we will check if an ensemble of decision tree"
0.13.0,"can actually alleviate the issue induced by the class imbalancing. First, we"
0.13.0,will use a bagging classifier and its counter part which internally uses a
0.13.0,random under-sampling to balanced each bootstrap sample.
0.13.0,%%
0.13.0,%% [markdown]
0.13.0,Balancing each bootstrap sample allows to increase significantly the balanced
0.13.0,accuracy and the geometric mean.
0.13.0,%%
0.13.0,%%
0.13.0,%% [markdown]
0.13.0,Classification using random forest classifier with and without sampling
0.13.0,-----------------------------------------------------------------------
0.13.0,
0.13.0,Random forest is another popular ensemble method and it is usually
0.13.0,"outperforming bagging. Here, we used a vanilla random forest and its balanced"
0.13.0,counterpart in which each bootstrap sample is balanced.
0.13.0,%%
0.13.0,%% [markdown]
0.13.0,"Similarly to the previous experiment, the balanced classifier outperform the"
0.13.0,"classifier which learn from imbalanced bootstrap samples. In addition, random"
0.13.0,forest outperforms the bagging classifier.
0.13.0,%%
0.13.0,%%
0.13.0,%% [markdown]
0.13.0,Boosting classifier
0.13.0,-------------------
0.13.0,
0.13.0,"In the same manner, easy ensemble classifier is a bag of balanced AdaBoost"
0.13.0,"classifier. However, it will be slower to train than random forest and will"
0.13.0,achieve worse performance.
0.13.0,%%
0.13.0,%%
0.13.0,%%
0.13.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.13.0,License: MIT
0.13.0,%%
0.13.0,%% [markdown]
0.13.0,Generate an imbalanced dataset
0.13.0,------------------------------
0.13.0,
0.13.0,"For this example, we will create a synthetic dataset using the function"
0.13.0,:func:`~sklearn.datasets.make_classification`. The problem will be a toy
0.13.0,classification problem with a ratio of 1:9 between the two classes.
0.13.0,%%
0.13.0,%%
0.13.0,%% [markdown]
0.13.0,"In the following sections, we will show a couple of algorithms that have"
0.13.0,been proposed over the years. We intend to illustrate how one can reuse the
0.13.0,:class:`~imblearn.ensemble.BalancedBaggingClassifier` by passing different
0.13.0,sampler.
0.13.0,%%
0.13.0,%% [markdown]
0.13.0,Exactly Balanced Bagging and Over-Bagging
0.13.0,-----------------------------------------
0.13.0,
0.13.0,The :class:`~imblearn.ensemble.BalancedBaggingClassifier` can use in
0.13.0,conjunction with a :class:`~imblearn.under_sampling.RandomUnderSampler` or
0.13.0,:class:`~imblearn.over_sampling.RandomOverSampler`. These methods are
0.13.0,"referred as Exactly Balanced Bagging and Over-Bagging, respectively and have"
0.13.0,been proposed first in [1]_.
0.13.0,%%
0.13.0,Exactly Balanced Bagging
0.13.0,%%
0.13.0,Over-bagging
0.13.0,%% [markdown]
0.13.0,SMOTE-Bagging
0.13.0,-------------
0.13.0,
0.13.0,Instead of using a :class:`~imblearn.over_sampling.RandomOverSampler` that
0.13.0,"make a bootstrap, an alternative is to use"
0.13.0,:class:`~imblearn.over_sampling.SMOTE` as an over-sampler. This is known as
0.13.0,SMOTE-Bagging [2]_.
0.13.0,%%
0.13.0,SMOTE-Bagging
0.13.0,%% [markdown]
0.13.0,Roughly Balanced Bagging
0.13.0,------------------------
0.13.0,While using a :class:`~imblearn.under_sampling.RandomUnderSampler` or
0.13.0,:class:`~imblearn.over_sampling.RandomOverSampler` will create exactly the
0.13.0,"desired number of samples, it does not follow the statistical spirit wanted"
0.13.0,in the bagging framework. The authors in [3]_ proposes to use a negative
0.13.0,binomial distribution to compute the number of samples of the majority
0.13.0,class to be selected and then perform a random under-sampling.
0.13.0,
0.13.0,"Here, we illustrate this method by implementing a function in charge of"
0.13.0,resampling and use the :class:`~imblearn.FunctionSampler` to integrate it
0.13.0,within a :class:`~imblearn.pipeline.Pipeline` and
0.13.0,:class:`~sklearn.model_selection.cross_validate`.
0.13.0,%%
0.13.0,find the minority and majority classes
0.13.0,compute the number of sample to draw from the majority class using
0.13.0,a negative binomial distribution
0.13.0,draw randomly with or without replacement
0.13.0,Roughly Balanced Bagging
0.13.0,%% [markdown]
0.13.0,.. topic:: References:
0.13.0,
0.13.0,".. [1] R. Maclin, and D. Opitz. ""An empirical evaluation of bagging and"
0.13.0,"boosting."" AAAI/IAAI 1997 (1997): 546-551."
0.13.0,
0.13.0,".. [2] S. Wang, and X. Yao. ""Diversity analysis on imbalanced data sets by"
0.13.0,"using ensemble models."" 2009 IEEE symposium on computational"
0.13.0,"intelligence and data mining. IEEE, 2009."
0.13.0,
0.13.0,".. [3] S. Hido, H. Kashima, and Y. Takahashi. ""Roughly balanced bagging"
0.13.0,"for imbalanced data."" Statistical Analysis and Data Mining: The ASA"
0.13.0,Data Science Journal 2.5‚Äê6 (2009): 412-426.
0.13.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.13.0,License: MIT
0.13.0,%%
0.13.0,%% [markdown]
0.13.0,The following function will be used to create toy dataset. It uses the
0.13.0,:func:`~sklearn.datasets.make_classification` from scikit-learn but fixing
0.13.0,some parameters.
0.13.0,%%
0.13.0,%% [markdown]
0.13.0,The following function will be used to plot the sample space after resampling
0.13.0,to illustrate the specificities of an algorithm.
0.13.0,%%
0.13.0,%% [markdown]
0.13.0,The following function will be used to plot the decision function of a
0.13.0,classifier given some data.
0.13.0,%%
0.13.0,%%
0.13.0,%% [markdown]
0.13.0,Prototype generation: under-sampling by generating new samples
0.13.0,--------------------------------------------------------------
0.13.0,
0.13.0,:class:`~imblearn.under_sampling.ClusterCentroids` under-samples by replacing
0.13.0,the original samples by the centroids of the cluster found.
0.13.0,%%
0.13.0,%% [markdown]
0.13.0,Prototype selection: under-sampling by selecting existing samples
0.13.0,-----------------------------------------------------------------
0.13.0,
0.13.0,The algorithm performing prototype selection can be subdivided into two
0.13.0,groups: (i) the controlled under-sampling methods and (ii) the cleaning
0.13.0,under-sampling methods.
0.13.0,
0.13.0,"With the controlled under-sampling methods, the number of samples to be"
0.13.0,selected can be specified.
0.13.0,:class:`~imblearn.under_sampling.RandomUnderSampler` is the most naive way of
0.13.0,performing such selection by randomly selecting a given number of samples by
0.13.0,the targeted class.
0.13.0,%%
0.13.0,%% [markdown]
0.13.0,:class:`~imblearn.under_sampling.NearMiss` algorithms implement some
0.13.0,heuristic rules in order to select samples. NearMiss-1 selects samples from
0.13.0,the majority class for which the average distance of the :math:`k`` nearest
0.13.0,samples of the minority class is the smallest. NearMiss-2 selects the samples
0.13.0,from the majority class for which the average distance to the farthest
0.13.0,samples of the negative class is the smallest. NearMiss-3 is a 2-step
0.13.0,"algorithm: first, for each minority sample, their :math:`m`"
0.13.0,"nearest-neighbors will be kept; then, the majority samples selected are the"
0.13.0,on for which the average distance to the :math:`k` nearest neighbors is the
0.13.0,largest.
0.13.0,%%
0.13.0,%% [markdown]
0.13.0,:class:`~imblearn.under_sampling.EditedNearestNeighbours` removes samples of
0.13.0,the majority class for which their class differ from the one of their
0.13.0,nearest-neighbors. This sieve can be repeated which is the principle of the
0.13.0,:class:`~imblearn.under_sampling.RepeatedEditedNearestNeighbours`.
0.13.0,:class:`~imblearn.under_sampling.AllKNN` is slightly different from the
0.13.0,:class:`~imblearn.under_sampling.RepeatedEditedNearestNeighbours` by changing
0.13.0,"the :math:`k` parameter of the internal nearest neighors algorithm,"
0.13.0,increasing it at each iteration.
0.13.0,%%
0.13.0,%% [markdown]
0.13.0,:class:`~imblearn.under_sampling.CondensedNearestNeighbour` makes use of a
0.13.0,1-NN to iteratively decide if a sample should be kept in a dataset or not.
0.13.0,The issue is that :class:`~imblearn.under_sampling.CondensedNearestNeighbour`
0.13.0,is sensitive to noise by preserving the noisy samples.
0.13.0,:class:`~imblearn.under_sampling.OneSidedSelection` also used the 1-NN and
0.13.0,use :class:`~imblearn.under_sampling.TomekLinks` to remove the samples
0.13.0,considered noisy. The
0.13.0,:class:`~imblearn.under_sampling.NeighbourhoodCleaningRule` use a
0.13.0,:class:`~imblearn.under_sampling.EditedNearestNeighbours` to remove some
0.13.0,"sample. Additionally, they use a 3 nearest-neighbors to remove samples which"
0.13.0,do not agree with this rule.
0.13.0,%%
0.13.0,%% [markdown]
0.13.0,:class:`~imblearn.under_sampling.InstanceHardnessThreshold` uses the
0.13.0,prediction of classifier to exclude samples. All samples which are classified
0.13.0,with a low probability will be removed.
0.13.0,%%
0.13.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.13.0,License: MIT
0.13.0,%%
0.13.0,%% [markdown]
0.13.0,This function allows to make nice plotting
0.13.0,%%
0.13.0,%% [markdown]
0.13.0,We will generate some toy data that illustrates how
0.13.0,:class:`~imblearn.under_sampling.TomekLinks` is used to clean a dataset.
0.13.0,%%
0.13.0,%% [markdown]
0.13.0,"In the figure above, the samples highlighted in green form a Tomek link since"
0.13.0,they are of different classes and are nearest neighbors of each other.
0.13.0,highlight the samples of interest
0.13.0,%% [markdown]
0.13.0,We can run the :class:`~imblearn.under_sampling.TomekLinks` sampling to
0.13.0,remove the corresponding samples. If `sampling_strategy='auto'` only the
0.13.0,sample from the majority class will be removed. If `sampling_strategy='all'`
0.13.0,both samples will be removed.
0.13.0,%%
0.13.0,highlight the samples of interest
0.13.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.13.0,License: MIT
0.13.0,%%
0.13.0,%% [markdown]
0.13.0,We define a function allowing to make some nice decoration on the plot.
0.13.0,%%
0.13.0,%% [markdown]
0.13.0,We can start by generating some data to later illustrate the principle of
0.13.0,each :class:`~imblearn.under_sampling.NearMiss` heuristic rules.
0.13.0,%%
0.13.0,%% [mardown]
0.13.0,NearMiss-1
0.13.0,----------
0.13.0,
0.13.0,NearMiss-1 selects samples from the majority class for which the average
0.13.0,distance to some nearest neighbours is the smallest. In the following
0.13.0,"example, we use a 3-NN to compute the average distance on 2 specific samples"
0.13.0,"of the majority class. Therefore, in this case the point linked by the"
0.13.0,green-dashed line will be selected since the average distance is smaller.
0.13.0,%%
0.13.0,%% [mardown]
0.13.0,NearMiss-2
0.13.0,----------
0.13.0,
0.13.0,NearMiss-2 selects samples from the majority class for which the average
0.13.0,distance to the farthest neighbors is the smallest. With the same
0.13.0,"configuration as previously presented, the sample linked to the green-dashed"
0.13.0,line will be selected since its distance the 3 farthest neighbors is the
0.13.0,smallest.
0.13.0,%%
0.13.0,%% [mardown]
0.13.0,NearMiss-3
0.13.0,----------
0.13.0,
0.13.0,"NearMiss-3 can be divided into 2 steps. First, a nearest-neighbors is used to"
0.13.0,short-list samples from the majority class (i.e. correspond to the
0.13.0,"highlighted samples in the following plot). Then, the sample with the largest"
0.13.0,average distance to the *k* nearest-neighbors are selected.
0.13.0,%%
0.13.0,select only the majority point of interest
0.13.0,Authors: Christos Aridas
0.13.0,Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.13.0,License: MIT
0.13.0,%%
0.13.0,%% [markdown]
0.13.0,Let's first create an imbalanced dataset and split in to two sets.
0.13.0,%%
0.13.0,%% [markdown]
0.13.0,"Now, we will create each individual steps that we would like later to combine"
0.13.0,%%
0.13.0,%% [markdown]
0.13.0,"Now, we can finally create a pipeline to specify in which order the different"
0.13.0,transformers and samplers should be executed before to provide the data to
0.13.0,the final classifier.
0.13.0,%%
0.13.0,%% [markdown]
0.13.0,We can now use the pipeline created as a normal classifier where resampling
0.13.0,"will happen when calling `fit` and disabled when calling `decision_function`,"
0.13.0,"`predict_proba`, or `predict`."
0.13.0,%%
0.13.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.13.0,License: MIT
0.13.0,##############################################################################
0.13.0,Data loading
0.13.0,##############################################################################
0.13.0,##############################################################################
0.13.0,"First, you should download the Porto Seguro data set from Kaggle. See the"
0.13.0,link in the introduction.
0.13.0,##############################################################################
0.13.0,The data set is imbalanced and it will have an effect on the fitting.
0.13.0,##############################################################################
0.13.0,Define the pre-processing pipeline
0.13.0,##############################################################################
0.13.0,##############################################################################
0.13.0,We want to standard scale the numerical features while we want to one-hot
0.13.0,"encode the categorical features. In this regard, we make use of the"
0.13.0,:class:`~sklearn.compose.ColumnTransformer`.
0.13.0,Create an environment variable to avoid using the GPU. This can be changed.
0.13.0,##############################################################################
0.13.0,Create a neural-network
0.13.0,##############################################################################
0.13.0,##############################################################################
0.13.0,We create a decorator to report the computation time
0.13.0,##############################################################################
0.13.0,The first model will be trained using the ``fit`` method and with imbalanced
0.13.0,mini-batches.
0.13.0,predict_proba was removed in tensorflow 2.6
0.13.0,##############################################################################
0.13.0,"In the contrary, we will use imbalanced-learn to create a generator of"
0.13.0,mini-batches which will yield balanced mini-batches.
0.13.0,##############################################################################
0.13.0,Classification loop
0.13.0,##############################################################################
0.13.0,##############################################################################
0.13.0,We will perform a 10-fold cross-validation and train the neural-network with
0.13.0,the two different strategies previously presented.
0.13.0,##############################################################################
0.13.0,Plot of the results and computation time
0.13.0,##############################################################################
0.13.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.13.0,License: MIT
0.13.0,%%
0.13.0,%% [markdown]
0.13.0,Problem definition
0.13.0,------------------
0.13.0,
0.13.0,We are dropping the following features:
0.13.0,
0.13.0,"- ""fnlwgt"": this feature was created while studying the ""adult"" dataset."
0.13.0,"Thus, we will not use this feature which is not acquired during the survey."
0.13.0,"- ""education-num"": it is encoding the same information than ""education""."
0.13.0,"Thus, we are removing one of these 2 features."
0.13.0,%%
0.13.0,%% [markdown]
0.13.0,"The ""adult"" dataset as a class ratio of about 3:1"
0.13.0,%%
0.13.0,%% [markdown]
0.13.0,This dataset is only slightly imbalanced. To better highlight the effect of
0.13.0,"learning from an imbalanced dataset, we will increase its ratio to 30:1"
0.13.0,%%
0.13.0,%% [markdown]
0.13.0,We will perform a cross-validation evaluation to get an estimate of the test
0.13.0,score.
0.13.0,
0.13.0,"As a baseline, we could use a classifier which will always predict the"
0.13.0,majority class independently of the features provided.
0.13.0,%%
0.13.0,%% [markdown]
0.13.0,"Instead of using the accuracy, we can use the balanced accuracy which will"
0.13.0,take into account the balancing issue.
0.13.0,%%
0.13.0,%% [markdown]
0.13.0,Strategies to learn from an imbalanced dataset
0.13.0,----------------------------------------------
0.13.0,We will use a dictionary and a list to continuously store the results of
0.13.0,our experiments and show them as a pandas dataframe.
0.13.0,%%
0.13.0,%% [markdown]
0.13.0,Dummy baseline
0.13.0,..............
0.13.0,
0.13.0,"Before to train a real machine learning model, we can store the results"
0.13.0,obtained with our :class:`~sklearn.dummy.DummyClassifier`.
0.13.0,%%
0.13.0,%% [markdown]
0.13.0,Linear classifier baseline
0.13.0,..........................
0.13.0,
0.13.0,We will create a machine learning pipeline using a
0.13.0,":class:`~sklearn.linear_model.LogisticRegression` classifier. In this regard,"
0.13.0,we will need to one-hot encode the categorical columns and standardized the
0.13.0,numerical columns before to inject the data into the
0.13.0,:class:`~sklearn.linear_model.LogisticRegression` classifier.
0.13.0,
0.13.0,"First, we define our numerical and categorical pipelines."
0.13.0,%%
0.13.0,%% [markdown]
0.13.0,"Then, we can create a preprocessor which will dispatch the categorical"
0.13.0,columns to the categorical pipeline and the numerical columns to the
0.13.0,numerical pipeline
0.13.0,%%
0.13.0,%% [markdown]
0.13.0,"Finally, we connect our preprocessor with our"
0.13.0,:class:`~sklearn.linear_model.LogisticRegression`. We can then evaluate our
0.13.0,model.
0.13.0,%%
0.13.0,%%
0.13.0,%% [markdown]
0.13.0,We can see that our linear model is learning slightly better than our dummy
0.13.0,"baseline. However, it is impacted by the class imbalance."
0.13.0,
0.13.0,We can verify that something similar is happening with a tree-based model
0.13.0,such as :class:`~sklearn.ensemble.RandomForestClassifier`. With this type of
0.13.0,"classifier, we will not need to scale the numerical data, and we will only"
0.13.0,need to ordinal encode the categorical data.
0.13.0,%%
0.13.0,%%
0.13.0,%% [markdown]
0.13.0,The :class:`~sklearn.ensemble.RandomForestClassifier` is as well affected by
0.13.0,"the class imbalanced, slightly less than the linear model. Now, we will"
0.13.0,present different approach to improve the performance of these 2 models.
0.13.0,
0.13.0,Use `class_weight`
0.13.0,..................
0.13.0,
0.13.0,Most of the models in `scikit-learn` have a parameter `class_weight`. This
0.13.0,parameter will affect the computation of the loss in linear model or the
0.13.0,criterion in the tree-based model to penalize differently a false
0.13.0,classification from the minority and majority class. We can set
0.13.0,"`class_weight=""balanced""` such that the weight applied is inversely"
0.13.0,proportional to the class frequency. We test this parametrization in both
0.13.0,linear model and tree-based model.
0.13.0,%%
0.13.0,%%
0.13.0,%% [markdown]
0.13.0,We can see that using `class_weight` was really effective for the linear
0.13.0,"model, alleviating the issue of learning from imbalanced classes. However,"
0.13.0,the :class:`~sklearn.ensemble.RandomForestClassifier` is still biased toward
0.13.0,"the majority class, mainly due to the criterion which is not suited enough to"
0.13.0,fight the class imbalance.
0.13.0,
0.13.0,Resample the training set during learning
0.13.0,.........................................
0.13.0,
0.13.0,Another way is to resample the training set by under-sampling or
0.13.0,over-sampling some of the samples. `imbalanced-learn` provides some samplers
0.13.0,to do such processing.
0.13.0,%%
0.13.0,%%
0.13.0,%%
0.13.0,%%
0.13.0,%% [markdown]
0.13.0,Applying a random under-sampler before the training of the linear model or
0.13.0,"random forest, allows to not focus on the majority class at the cost of"
0.13.0,making more mistake for samples in the majority class (i.e. decreased
0.13.0,accuracy).
0.13.0,
0.13.0,We could apply any type of samplers and find which sampler is working best
0.13.0,on the current dataset.
0.13.0,
0.13.0,"Instead, we will present another way by using classifiers which will apply"
0.13.0,sampling internally.
0.13.0,
0.13.0,Use of specific balanced algorithms from imbalanced-learn
0.13.0,.........................................................
0.13.0,
0.13.0,We already showed that random under-sampling can be effective on decision
0.13.0,"tree. However, instead of under-sampling once the dataset, one could"
0.13.0,under-sample the original dataset before to take a bootstrap sample. This is
0.13.0,the base of the :class:`imblearn.ensemble.BalancedRandomForestClassifier` and
0.13.0,:class:`~imblearn.ensemble.BalancedBaggingClassifier`.
0.13.0,%%
0.13.0,%%
0.13.0,%% [markdown]
0.13.0,The performance with the
0.13.0,:class:`~imblearn.ensemble.BalancedRandomForestClassifier` is better than
0.13.0,applying a single random under-sampling. We will use a gradient-boosting
0.13.0,classifier within a :class:`~imblearn.ensemble.BalancedBaggingClassifier`.
0.13.0,%% [markdown]
0.13.0,This last approach is the most effective. The different under-sampling allows
0.13.0,to bring some diversity for the different GBDT to learn and not focus on a
0.13.0,portion of the majority class.
0.13.0,Authors: Christos Aridas
0.13.0,Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.13.0,License: MIT
0.13.0,%%
0.13.0,%% [markdown]
0.13.0,Load the dataset
0.13.0,----------------
0.13.0,
0.13.0,We will use a dataset containing image from know person where we will
0.13.0,build a model to recognize the person on the image. We will make this problem
0.13.0,a binary problem by taking picture of only George W. Bush and Bill Clinton.
0.13.0,%%
0.13.0,%%
0.13.0,%% [markdown]
0.13.0,We can check the ratio between the two classes.
0.13.0,%%
0.13.0,%% [markdown]
0.13.0,We see that we have an imbalanced classification problem with ~95% of the
0.13.0,data belonging to the class G.W. Bush.
0.13.0,
0.13.0,Compare over-sampling approaches
0.13.0,--------------------------------
0.13.0,
0.13.0,We will use different over-sampling approaches and use a kNN classifier
0.13.0,to check if we can recognize the 2 presidents. The evaluation will be
0.13.0,performed through cross-validation and we will plot the mean ROC curve.
0.13.0,
0.13.0,We will create different pipelines and evaluate them.
0.13.0,%%
0.13.0,%% [markdown]
0.13.0,We will compute the mean ROC curve for each pipeline using a different splits
0.13.0,provided by the :class:`~sklearn.model_selection.StratifiedKFold`
0.13.0,cross-validation.
0.13.0,%%
0.13.0,compute the mean fpr/tpr to get the mean ROC curve
0.13.0,Create a display that we will reuse to make the aggregated plots for
0.13.0,all methods
0.13.0,%% [markdown]
0.13.0,"In the previous cell, we created the different mean ROC curve and we can plot"
0.13.0,them on the same plot.
0.13.0,%%
0.13.0,%% [markdown]
0.13.0,"We see that for this task, methods that are generating new samples with some"
0.13.0,interpolation (i.e. ADASYN and SMOTE) perform better than random
0.13.0,over-sampling or no resampling.
0.13.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.13.0,License: MIT
0.13.0,Create a folder to fetch the dataset
0.13.0,Create a pipeline
0.13.0,Classify and report the results
0.13.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.13.0,License: MIT
0.13.0,%%
0.13.0,%% [markdown]
0.13.0,Setting the data set
0.13.0,--------------------
0.13.0,
0.13.0,We use a part of the 20 newsgroups data set by loading 4 topics. Using the
0.13.0,"scikit-learn loader, the data are split into a training and a testing set."
0.13.0,
0.13.0,Note the class \#3 is the minority class and has almost twice less samples
0.13.0,than the majority class.
0.13.0,%%
0.13.0,%%
0.13.0,%% [markdown]
0.13.0,The usual scikit-learn pipeline
0.13.0,-------------------------------
0.13.0,
0.13.0,You might usually use scikit-learn pipeline by combining the TF-IDF
0.13.0,vectorizer to feed a multinomial naive bayes classifier. A classification
0.13.0,report summarized the results on the testing set.
0.13.0,
0.13.0,"As expected, the recall of the class \#3 is low mainly due to the class"
0.13.0,imbalanced.
0.13.0,%%
0.13.0,%%
0.13.0,%% [markdown]
0.13.0,Balancing the class before classification
0.13.0,-----------------------------------------
0.13.0,
0.13.0,"To improve the prediction of the class \#3, it could be interesting to apply"
0.13.0,"a balancing before to train the naive bayes classifier. Therefore, we will"
0.13.0,use a :class:`~imblearn.under_sampling.RandomUnderSampler` to equalize the
0.13.0,number of samples in all the classes before the training.
0.13.0,
0.13.0,It is also important to note that we are using the
0.13.0,:class:`~imblearn.pipeline.make_pipeline` function implemented in
0.13.0,imbalanced-learn to properly handle the samplers.
0.13.0,%%
0.13.0,%% [markdown]
0.13.0,"Although the results are almost identical, it can be seen that the resampling"
0.13.0,allowed to correct the poor recall of the class \#3 at the cost of reducing
0.13.0,"the other metrics for the other classes. However, the overall results are"
0.13.0,slightly better.
0.13.0,%%
0.13.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.13.0,License: MIT
0.13.0,#############################################################################
0.13.0,Toy data generation
0.13.0,#############################################################################
0.13.0,#############################################################################
0.13.0,We are generating some non Gaussian data set contaminated with some unform
0.13.0,noise.
0.13.0,#############################################################################
0.13.0,We will generate some cleaned test data without outliers.
0.13.0,#############################################################################
0.13.0,How to use the :class:`~imblearn.FunctionSampler`
0.13.0,#############################################################################
0.13.0,#############################################################################
0.13.0,We first define a function which will use
0.13.0,:class:`~sklearn.ensemble.IsolationForest` to eliminate some outliers from
0.13.0,our dataset during training. The function passed to the
0.13.0,:class:`~imblearn.FunctionSampler` will be called when using the method
0.13.0,``fit_resample``.
0.13.0,#############################################################################
0.13.0,Integrate it within a pipeline
0.13.0,#############################################################################
0.13.0,#############################################################################
0.13.0,"By elimnating outliers before the training, the classifier will be less"
0.13.0,affected during the prediction.
0.13.0,Authors: Dayvid Oliveira
0.13.0,Christos Aridas
0.13.0,Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.13.0,License: MIT
0.13.0,%%
0.13.0,%% [markdown]
0.13.0,Generate the dataset
0.13.0,--------------------
0.13.0,
0.13.0,"First, we will generate a dataset and convert it to a"
0.13.0,:class:`~pandas.DataFrame` with arbitrary column names. We will plot the
0.13.0,original dataset.
0.13.0,%%
0.13.0,%% [markdown]
0.13.0,Make a dataset imbalanced
0.13.0,-------------------------
0.13.0,
0.13.0,"Now, we will show the helpers :func:`~imblearn.datasets.make_imbalance`"
0.13.0,that is useful to random select a subset of samples. It will impact the
0.13.0,class distribution as specified by the parameters.
0.13.0,%%
0.13.0,%%
0.13.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.13.0,License: MIT
0.13.0,%%
0.13.0,%% [markdown]
0.13.0,Create an imbalanced dataset
0.13.0,----------------------------
0.13.0,
0.13.0,"First, we will create an imbalanced data set from a the iris data set."
0.13.0,%%
0.13.0,%%
0.13.0,%% [markdown]
0.13.0,Using ``sampling_strategy`` in resampling algorithms
0.13.0,====================================================
0.13.0,
0.13.0,`sampling_strategy` as a `float`
0.13.0,--------------------------------
0.13.0,
0.13.0,`sampling_strategy` can be given a `float`. For **under-sampling
0.13.0,"methods**, it corresponds to the ratio :math:`\alpha_{us}` defined by"
0.13.0,:math:`N_{rM} = \alpha_{us} \times N_{m}` where :math:`N_{rM}` and
0.13.0,:math:`N_{m}` are the number of samples in the majority class after
0.13.0,"resampling and the number of samples in the minority class, respectively."
0.13.0,%%
0.13.0,select only 2 classes since the ratio make sense in this case
0.13.0,%%
0.13.0,%% [markdown]
0.13.0,"For **over-sampling methods**, it correspond to the ratio"
0.13.0,:math:`\alpha_{os}` defined by :math:`N_{rm} = \alpha_{os} \times N_{M}`
0.13.0,where :math:`N_{rm}` and :math:`N_{M}` are the number of samples in the
0.13.0,minority class after resampling and the number of samples in the majority
0.13.0,"class, respectively."
0.13.0,%%
0.13.0,%% [markdown]
0.13.0,`sampling_strategy` as a `str`
0.13.0,-------------------------------
0.13.0,
0.13.0,`sampling_strategy` can be given as a string which specify the class
0.13.0,"targeted by the resampling. With under- and over-sampling, the number of"
0.13.0,samples will be equalized.
0.13.0,
0.13.0,Note that we are using multiple classes from now on.
0.13.0,%%
0.13.0,%% [markdown]
0.13.0,"With **cleaning method**, the number of samples in each class will not be"
0.13.0,equalized even if targeted.
0.13.0,%%
0.13.0,%% [markdown]
0.13.0,`sampling_strategy` as a `dict`
0.13.0,-------------------------------
0.13.0,
0.13.0,"When `sampling_strategy` is a `dict`, the keys correspond to the targeted"
0.13.0,classes. The values correspond to the desired number of samples for each
0.13.0,targeted class. This is working for both **under- and over-sampling**
0.13.0,algorithms but not for the **cleaning algorithms**. Use a `list` instead.
0.13.0,%%
0.13.0,%% [markdown]
0.13.0,`sampling_strategy` as a `list`
0.13.0,-------------------------------
0.13.0,
0.13.0,"When `sampling_strategy` is a `list`, the list contains the targeted"
0.13.0,classes. It is used only for **cleaning methods** and raise an error
0.13.0,otherwise.
0.13.0,%%
0.13.0,%% [markdown]
0.13.0,`sampling_strategy` as a callable
0.13.0,---------------------------------
0.13.0,
0.13.0,"When callable, function taking `y` and returns a `dict`. The keys"
0.13.0,correspond to the targeted classes. The values correspond to the desired
0.13.0,number of samples for each class.
0.13.0,%%
0.13.0,List of whitelisted modules and methods; regexp are supported.
0.13.0,These docstrings will fail because they are inheriting from scikit-learn
0.13.0,skip private classes
0.13.0,"We ignore following error code,"
0.13.0,- RT02: The first line of the Returns section
0.13.0,"should contain only the type, .."
0.13.0,(as we may need refer to the name of the returned
0.13.0,object)
0.13.0,- GL01: Docstring text (summary) should start in the line
0.13.0,"immediately after the opening quotes (not in the same line,"
0.13.0,or leaving a blank line in between)
0.13.0,"- GL02: If there's a blank line, it should be before the"
0.13.0,"first line of the Returns section, not after (it allows to have"
0.13.0,short docstrings for properties).
0.13.0,Ignore PR02: Unknown parameters for properties. We sometimes use
0.13.0,"properties for ducktyping, i.e. SGDClassifier.predict_proba"
0.13.0,Following codes are only taken into account for the
0.13.0,top level class docstrings:
0.13.0,- ES01: No extended summary found
0.13.0,- SA01: See Also section not found
0.13.0,- EX01: No examples section found
0.13.0,In particular we can't parse the signature of properties
0.13.0,"When applied to classes, detect class method. For functions"
0.13.0,method = None.
0.13.0,TODO: this detection can be improved. Currently we assume that we have
0.13.0,class # methods if the second path element before last is in camel case.
0.13.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.13.0,License: MIT
0.13.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.13.0,Christos Aridas
0.13.0,License: MIT
0.13.0,"in older versions of scikit-learn, only METHODS is used"
0.13.0,we need to overwrite SamplerMixin.fit to bypass the validation
0.13.0,Adapted from scikit-learn
0.13.0,Author: Edouard Duchesnay
0.13.0,Gael Varoquaux
0.13.0,Virgile Fritsch
0.13.0,Alexandre Gramfort
0.13.0,Lars Buitinck
0.13.0,Christos Aridas
0.13.0,Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.13.0,License: BSD
0.13.0,we only get here if the above didn't raise
0.13.0,"If the parameter is a tuple, transform each element of the"
0.13.0,tuple. This is needed to support the pattern present in
0.13.0,`lightgbm` and `xgboost` where users can pass multiple
0.13.0,validation sets.
0.13.0,BaseEstimator interface
0.13.0,validate names
0.13.0,validate estimators
0.13.0,We allow last estimator to be None as an identity transformation
0.13.0,we only need to process step_params if transform_input is set
0.13.0,and metadata is given by the user.
0.13.0,here we get the metadata required by sub_pipeline.transform
0.13.0,"`step_params` is the output of `process_routing`, so it has a dict for each"
0.13.0,"method (e.g. fit, transform, predict), which are the args to be passed to"
0.13.0,those methods. We need to transform the parameters which are in the
0.13.0,"`transform_input`, before returning these dicts."
0.13.0,"An example of `(param_name, param_value)` is"
0.13.0,"`('sample_weight', array([0.5, 0.5, ...]))`"
0.13.0,"This parameter now needs to be transformed by the sub_pipeline, to"
0.13.0,this step. We cache these computations to avoid repeating them.
0.13.0,Estimator interface
0.13.0,"def _fit(self, X, y=None, **fit_params_steps):"
0.13.0,Setup the memory
0.13.0,we do not clone when caching is disabled to
0.13.0,preserve backward compatibility
0.13.0,Fit or load from cache the current transformer
0.13.0,Replace the transformer of the step with the fitted
0.13.0,transformer. This is necessary when loading the transformer
0.13.0,from the cache.
0.13.0,The `fit_*` methods need to be overridden to support the samplers.
0.13.0,estimators in Pipeline.steps are not validated yet
0.13.0,estimators in Pipeline.steps are not validated yet
0.13.0,TODO(0.15): Remove the context manager and use check_is_fitted(self)
0.13.0,metadata routing enabled
0.13.0,estimators in Pipeline.steps are not validated yet
0.13.0,estimators in Pipeline.steps are not validated yet
0.13.0,TODO: remove the following methods when the minimum scikit-learn >= 1.4
0.13.0,They do not depend on resampling but we need to redefine them for the
0.13.0,compatibility with the metadata routing framework.
0.13.0,TODO(0.15): Remove the context manager and use check_is_fitted(self)
0.13.0,metadata routing enabled
0.13.0,TODO(0.15): Remove the context manager and use check_is_fitted(self)
0.13.0,not branching here since params is only available if
0.13.0,enable_metadata_routing=True
0.13.0,TODO(0.15): Remove the context manager and use check_is_fitted(self)
0.13.0,TODO(0.15): Remove the context manager and use check_is_fitted(self)
0.13.0,metadata routing enabled
0.13.0,TODO(0.15): Remove the context manager and use check_is_fitted(self)
0.13.0,not branching here since params is only available if
0.13.0,enable_metadata_routing=True
0.13.0,TODO(0.15): Remove the context manager and use check_is_fitted(self)
0.13.0,"we don't have to branch here, since params is only non-empty if"
0.13.0,enable_metadata_routing=True.
0.13.0,TODO(0.15): Remove the context manager and use check_is_fitted(self)
0.13.0,metadata routing is enabled.
0.13.0,"TODO: once scikit-learn >= 1.4, the following function should be simplified by"
0.13.0,calling `super().get_metadata_routing()`
0.13.0,first we add all steps except the last one
0.13.0,"fit, fit_predict, and fit_transform call fit_transform if it"
0.13.0,"exists, or else fit and transform"
0.13.0,handling sampler if the fit_* stage
0.13.0,then we add the last step
0.13.0,"without metadata routing, fit_transform and fit_predict"
0.13.0,get all the same params and pass it to the last fit.
0.13.0,First find the last step that is not 'passthrough'
0.13.0,"All steps are 'passthrough', so the pipeline is considered fitted"
0.13.0,check if the last step of the pipeline is fitted
0.13.0,"we only check the last step since if the last step is fit, it"
0.13.0,means the previous steps should also be fit. This is faster than
0.13.0,checking if every step of the pipeline is fit.
0.13.0,"This happens when the `steps` is not a list of (name, estimator)"
0.13.0,tuples and `fit` is not called yet to validate the steps.
0.13.0,"This happens when the `steps` is not a list of (name, estimator)"
0.13.0,tuples and `fit` is not called yet to validate the steps.
0.13.0,"if we have a weight for this transformer, multiply output"
0.13.0,This variable is injected in the __builtins__ by the build
0.13.0,process. It is used to enable importing subpackages of sklearn when
0.13.0,the binaries are not built
0.13.0,mypy error: Cannot determine type of '__SKLEARN_SETUP__'
0.13.0,We are not importing the rest of scikit-learn during the build
0.13.0,"process, as it may not be compiled yet"
0.13.0,"FIXME: When we get Python 3.7 as minimal version, we will need to switch to"
0.13.0,the following solution:
0.13.0,https://snarky.ca/lazy-importing-in-python-3-7/
0.13.0,Import the target module and insert it into the parent's namespace
0.13.0,Update this object's dict so that if someone keeps a reference to the
0.13.0,"LazyLoader, lookups are efficient (__getattr__ is only called on"
0.13.0,lookups that fail).
0.13.0,delay the import of keras since we are going to import either tensorflow
0.13.0,or keras
0.13.0,Based on NiLearn package
0.13.0,License: simplified BSD
0.13.0,"PEP0440 compatible formatted version, see:"
0.13.0,https://www.python.org/dev/peps/pep-0440/
0.13.0,
0.13.0,Generic release markers:
0.13.0,X.Y
0.13.0,X.Y.Z # For bugfix releases
0.13.0,
0.13.0,Admissible pre-release markers:
0.13.0,X.YaN # Alpha release
0.13.0,X.YbN # Beta release
0.13.0,X.YrcN # Release Candidate
0.13.0,X.Y # Final release
0.13.0,
0.13.0,Dev branch marker is: 'X.Y.dev' or 'X.Y.devN' where N is an integer.
0.13.0,'X.Y.dev0' is the canonical version of 'X.Y.dev'
0.13.0,coding: utf-8
0.13.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.13.0,Dariusz Brzezinski
0.13.0,License: MIT
0.13.0,Only negative labels
0.13.0,"Calculate tp_sum, pred_sum, true_sum ###"
0.13.0,labels are now from 0 to len(labels) - 1 -> use bincount
0.13.0,Pathological case
0.13.0,Compute the true negative
0.13.0,Retain only selected labels
0.13.0,"Finally, we have all our sufficient statistics. Divide! #"
0.13.0,"Divide, and on zero-division, set scores to 0 and warn:"
0.13.0,"Oddly, we may get an ""invalid"" rather than a ""divide"" error"
0.13.0,here.
0.13.0,Average the results
0.13.0,labels are now from 0 to len(labels) - 1 -> use bincount
0.13.0,Pathological case
0.13.0,Retain only selected labels
0.13.0,old version of scipy return MaskedConstant instead of 0.0
0.13.0,check that the scoring function does not need a score
0.13.0,and only a prediction
0.13.0,We do not support multilabel so the only average supported
0.13.0,is binary
0.13.0,Compute the different metrics
0.13.0,Precision/recall/f1
0.13.0,Specificity
0.13.0,Geometric mean
0.13.0,Index balanced accuracy
0.13.0,compute averages
0.13.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.13.0,License: MIT
0.13.0,categories are expected to be encoded from 0 to n_categories - 1
0.13.0,"list of length n_features of ndarray (n_categories, n_classes)"
0.13.0,compute the counts
0.13.0,normalize by the summing over the classes
0.13.0,silence potential warning due to in-place division by zero
0.13.0,coding: utf-8
0.13.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.13.0,Christos Aridas
0.13.0,License: MIT
0.13.0,##############################################################################
0.13.0,Utilities for testing
0.13.0,import some data to play with
0.13.0,restrict to a binary classification task
0.13.0,add noisy features to make the problem harder and avoid perfect results
0.13.0,"run classifier, get class probabilities and label predictions"
0.13.0,only interested in probabilities of the positive case
0.13.0,XXX: do we really want a special API for the binary case?
0.13.0,##############################################################################
0.13.0,Tests
0.13.0,detailed measures for each class
0.13.0,individual scoring function that can be used for grid search: in the
0.13.0,binary class case the score is the value of the measure for the positive
0.13.0,class (e.g. label == 1). This is deprecated for average != 'binary'.
0.13.0,Such a case may occur with non-stratified cross-validation
0.13.0,ensure the above were meaningful tests:
0.13.0,Bad pos_label
0.13.0,Bad average option
0.13.0,but average != 'binary'; even if data is binary
0.13.0,compute the geometric mean for the binary problem
0.13.0,print classification report with class names
0.13.0,print classification report with label detection
0.13.0,print classification report with class names
0.13.0,print classification report with label detection
0.13.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.13.0,License: MIT
0.13.0,Check basic feature of the metric:
0.13.0,"* the shape of the distance matrix is (n_samples, n_samples)"
0.13.0,* computing pairwise distance of X is the same than explicitely between
0.13.0,X and X.
0.13.0,Check the property of the vdm distance. Let's check the property
0.13.0,"described in ""Improved Heterogeneous Distance Functions"", D.R. Wilson and"
0.13.0,"T.R. Martinez, Journal of Artificial Intelligence Research 6 (1997) 1-34"
0.13.0,https://arxiv.org/pdf/cs/9701101.pdf
0.13.0,
0.13.0,"""if an attribute color has three values red, green and blue, and the"
0.13.0,"application is to identify whether or not an object is an apple, red and"
0.13.0,green would be considered closer than red and blue because the former two
0.13.0,"both have similar correlations with the output class apple."""
0.13.0,defined our feature
0.13.0,0 - not an apple / 1 - an apple
0.13.0,computing the distance between a sample of the same category should
0.13.0,give a null distance
0.13.0,check the property explained in the introduction example
0.13.0,green and red are very close
0.13.0,blue is closer to red than green
0.13.0,"Check that ""auto"" is equivalent to provide the number categories"
0.13.0,beforehand
0.13.0,Check that we raise an error if n_categories is inconsistent with the
0.13.0,number of features in X
0.13.0,Check that we don't get issue when a category is missing between 0
0.13.0,n_categories - 1
0.13.0,remove a categories that could be between 0 and n_categories
0.13.0,Check that we raise a NotFittedError when `fit` is not not called before
0.13.0,pairwise.
0.13.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.13.0,Christos Aridas
0.13.0,License: MIT
0.13.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.13.0,Christos Aridas
0.13.0,License: MIT
0.13.0,The ratio is computed using a one-vs-rest manner. Using majority
0.13.0,in multi-class would lead to slightly different results at the
0.13.0,cost of introducing a new parameter.
0.13.0,rounding may cause new amount for n_samples
0.13.0,the nearest neighbors need to be fitted only on the current class
0.13.0,to find the class NN to generate new samples
0.13.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.13.0,Christos Aridas
0.13.0,License: MIT
0.13.0,smoothed bootstrap imposes to make numerical operation; we need
0.13.0,to be sure to have only numerical data in X
0.13.0,generate a smoothed bootstrap with a perturbation
0.13.0,generate a bootstrap
0.13.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.13.0,Christos Aridas
0.13.0,License: MIT
0.13.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.13.0,Fernando Nogueira
0.13.0,Christos Aridas
0.13.0,License: MIT
0.13.0,negate diagonal elements
0.13.0,identify cluster which are answering the requirements
0.13.0,empty cluster
0.13.0,the cluster is already considered balanced
0.13.0,not enough samples to apply SMOTE
0.13.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.13.0,Fernando Nogueira
0.13.0,Christos Aridas
0.13.0,Dzianis Dudnik
0.13.0,License: MIT
0.13.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.13.0,Fernando Nogueira
0.13.0,Christos Aridas
0.13.0,Dzianis Dudnik
0.13.0,License: MIT
0.13.0,np.newaxis for backwards compatability with random_state
0.13.0,Samples are in danger for m/2 <= m' < m
0.13.0,Samples are noise for m = m'
0.13.0,the input of the OneHotEncoder needs to be dense
0.13.0,SMOTE resampling starts here
0.13.0,"In the edge case where the median of the std is equal to 0, the 1s"
0.13.0,"entries will be also nullified. In this case, we store the original"
0.13.0,categorical encoding which will be later used for inverting the OHE
0.13.0,This variable will be used when generating data
0.13.0,we can replace the 1 entries of the categorical features with the
0.13.0,median of the standard deviation. It will ensure that whenever
0.13.0,"distance is computed between 2 samples, the difference will be equal"
0.13.0,to the median of the standard deviation as in the original paper.
0.13.0,"With one-hot encoding, the median will be repeated twice. We need"
0.13.0,to divide by sqrt(2) such that we only have one median value
0.13.0,contributing to the Euclidean distance
0.13.0,SMOTE resampling ends here
0.13.0,reverse the encoding of the categorical features
0.13.0,the matrix is supposed to be in the CSR format after the stacking
0.13.0,change in sparsity structure more efficient with LIL than CSR
0.13.0,convert to dense array since scipy.sparse doesn't handle 3D
0.13.0,"In the case that the median std was equal to zeros, we have to"
0.13.0,create non-null entry based on the encoded of OHE
0.13.0,tie breaking argmax
0.13.0,generate sample indices that will be used to generate new samples
0.13.0,"for each drawn samples, select its k-neighbors and generate a sample"
0.13.0,"where for each feature individually, each category generated is the"
0.13.0,most common category
0.13.0,the kneigbors search will include the sample itself which is
0.13.0,expected from the original algorithm
0.13.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.13.0,Christos Aridas
0.13.0,Dzianis Dudnik
0.13.0,License: MIT
0.13.0,create 2 random continuous feature
0.13.0,create a categorical feature using some string
0.13.0,create a categorical feature using some integer
0.13.0,return the categories
0.13.0,create 2 random continuous feature
0.13.0,create a categorical feature using some string
0.13.0,create a categorical feature using some integer
0.13.0,return the categories
0.13.0,create 2 random continuous feature
0.13.0,create a categorical feature using some string
0.13.0,create a categorical feature using some integer
0.13.0,return the categories
0.13.0,create 2 random continuous feature
0.13.0,create a categorical feature using some string
0.13.0,create a categorical feature using some integer
0.13.0,return the categories
0.13.0,create 2 random continuous feature
0.13.0,create a categorical feature using some string
0.13.0,create a categorical feature using some integer
0.13.0,part of the common test which apply to SMOTE-NC even if it is not default
0.13.0,constructible
0.13.0,Check that the samplers handle pandas dataframe and pandas series
0.13.0,Cast X and y to not default dtype
0.13.0,Non-regression test for #662
0.13.0,https://github.com/scikit-learn-contrib/imbalanced-learn/issues/662
0.13.0,check that the categorical feature is not random but correspond to the
0.13.0,categories seen in the minority class samples
0.13.0,overall check for SMOTEN
0.13.0,check if the SMOTEN resample data as expected
0.13.0,"we generate data such that ""not apple"" will be the minority class and"
0.13.0,"samples from this class will be generated. We will force the ""blue"""
0.13.0,"category to be associated with this class. Therefore, the new generated"
0.13.0,"samples should as well be from the ""blue"" category."
0.13.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.13.0,Christos Aridas
0.13.0,License: MIT
0.13.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.13.0,Christos Aridas
0.13.0,License: MIT
0.13.0,check the random over-sampling with a multiclass problem
0.13.0,check that resampling with heterogeneous dtype is working with basic
0.13.0,resampling
0.13.0,check that we can oversample even with missing or infinite data
0.13.0,regression tests for #605
0.13.0,check that we raise an error when heterogeneous dtype data are given
0.13.0,and a smoothed bootstrap is requested
0.13.0,check that smoothed bootstrap is working for numerical array
0.13.0,check that a shrinkage factor of 0 is equivalent to not create a smoothed
0.13.0,bootstrap
0.13.0,check the behaviour of the shrinkage parameter
0.13.0,the covariance of the data generated with the larger shrinkage factor
0.13.0,should also be larger.
0.13.0,check the validation of the shrinkage parameter
0.13.0,check that m_neighbors is properly set. Regression test for:
0.13.0,https://github.com/scikit-learn-contrib/imbalanced-learn/issues/568
0.13.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.13.0,Christos Aridas
0.13.0,License: MIT
0.13.0,shuffle the indices since the sampler are packing them by class
0.13.0,helper functions
0.13.0,input and output
0.13.0,build the model and weights
0.13.0,"build the loss, predict, and train operator"
0.13.0,Initialization of all variables in the graph
0.13.0,"For each epoch, run accuracy on train and test"
0.13.0,helper functions
0.13.0,input and output
0.13.0,build the model and weights
0.13.0,"build the loss, predict, and train operator"
0.13.0,Initialization of all variables in the graph
0.13.0,"For each epoch, run accuracy on train and test"
0.13.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.13.0,License: MIT
0.13.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.13.0,Fernando Nogueira
0.13.0,Christos Aridas
0.13.0,License: MIT
0.13.0,find which class to not consider
0.13.0,there is a Tomek link between two samples if they are both nearest
0.13.0,neighbors of each others.
0.13.0,Find the nearest neighbour of every point
0.13.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.13.0,Christos Aridas
0.13.0,License: MIT
0.13.0,Randomly get one sample from the majority class
0.13.0,Generate the index to select
0.13.0,Create the set C - One majority samples and all minority
0.13.0,Create the set S - all majority samples
0.13.0,fit knn on C
0.13.0,Check each sample in S if we keep it or drop it
0.13.0,Do not select sample which are already well classified
0.13.0,Classify on S
0.13.0,If the prediction do not agree with the true label
0.13.0,append it in C_x
0.13.0,Keep the index for later
0.13.0,Update C
0.13.0,fit a knn on C
0.13.0,This experimental to speed up the search
0.13.0,Classify all the element in S and avoid to test the
0.13.0,well classified elements
0.13.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.13.0,Dayvid Oliveira
0.13.0,Christos Aridas
0.13.0,License: MIT
0.13.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.13.0,Christos Aridas
0.13.0,License: MIT
0.13.0,Compute the distance considering the farthest neighbour
0.13.0,Sort the list of distance and get the index
0.13.0,Throw a warning to tell the user that we did not have enough samples
0.13.0,to select and that we just select everything
0.13.0,Select the desired number of samples
0.13.0,idx_tmp is relative to the feature selected in the
0.13.0,previous step and we need to find the indirection
0.13.0,fmt: off
0.13.0,fmt: on
0.13.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.13.0,Christos Aridas
0.13.0,License: MIT
0.13.0,select a sample from the current class
0.13.0,create the set composed of all minority samples and one
0.13.0,sample from the current class.
0.13.0,create the set S with removing the seed from S
0.13.0,since that it will be added anyway
0.13.0,apply Tomek cleaning
0.13.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.13.0,Dayvid Oliveira
0.13.0,Christos Aridas
0.13.0,License: MIT
0.13.0,Check the stopping criterion
0.13.0,1. If there is no changes for the vector y
0.13.0,2. If the number of samples in the other class become inferior to
0.13.0,the number of samples in the majority class
0.13.0,3. If one of the class is disappearing
0.13.0,Case 1
0.13.0,Case 2
0.13.0,Case 3
0.13.0,Check the stopping criterion
0.13.0,1. If the number of samples in the other class become inferior to
0.13.0,the number of samples in the majority class
0.13.0,2. If one of the class is disappearing
0.13.0,Case 1else:
0.13.0,overwrite b_min_bec_maj
0.13.0,Case 2
0.13.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.13.0,Christos Aridas
0.13.0,License: MIT
0.13.0,backward compatibility when passing a NearestNeighbors object
0.13.0,clean the neighborhood
0.13.0,compute which classes to consider for cleaning for the A2 group
0.13.0,add an additional sample since the query points contains the original dataset
0.13.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.13.0,Christos Aridas
0.13.0,License: MIT
0.13.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.13.0,Christos Aridas
0.13.0,License: MIT
0.13.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.13.0,Christos Aridas
0.13.0,License: MIT
0.13.0,"with a large `threshold_cleaning`, the algorithm is equivalent to ENN"
0.13.0,set a threshold that we should consider only the class #2
0.13.0,making the threshold slightly smaller to take into account class #1
0.13.0,we should have a more aggressive cleaning with n_neighbors is larger
0.13.0,TODO: remove in 0.14
0.13.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.13.0,Christos Aridas
0.13.0,License: MIT
0.13.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.13.0,Christos Aridas
0.13.0,License: MIT
0.13.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.13.0,Christos Aridas
0.13.0,License: MIT
0.13.0,check that we can undersample even with missing or infinite data
0.13.0,regression tests for #605
0.13.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.13.0,Christos Aridas
0.13.0,License: MIT
0.13.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.13.0,Christos Aridas
0.13.0,License: MIT
0.13.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.13.0,Christos Aridas
0.13.0,License: MIT
0.13.0,TODO: remove in 0.14
0.13.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.13.0,Christos Aridas
0.13.0,License: MIT
0.13.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.13.0,Christos Aridas
0.13.0,License: MIT
0.13.0,TODO: remove in 0.14
0.13.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.13.0,Fernando Nogueira
0.13.0,Christos Aridas
0.13.0,License: MIT
0.13.0,check that the samples selecting by the hard voting corresponds to the
0.13.0,targeted class
0.13.0,non-regression test for:
0.13.0,https://github.com/scikit-learn-contrib/imbalanced-learn/issues/738
0.13.0,Generate valid values for the required parameters
0.13.0,The parameters `*args` and `**kwargs` are ignored since we cannot generate
0.13.0,constraints.
0.13.0,check that there is a constraint for each parameter
0.13.0,this object does not have a valid type for sure for all params
0.13.0,This parameter is not validated
0.13.0,"First, check that the error is raised if param doesn't match any valid type."
0.13.0,"Then, for constraints that are more than a type constraint, check that the"
0.13.0,error is raised if param does match a valid type but does not match any valid
0.13.0,value for this type.
0.13.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.13.0,Christos Aridas
0.13.0,License: MIT
0.13.0,test that all_estimators doesn't find abstract classes.
0.13.0,Common tests for estimator instances
0.13.0,Authors: Alexandre Gramfort <alexandre.gramfort@inria.fr>
0.13.0,Raghav RV <rvraghav93@gmail.com>
0.13.0,License: BSD 3 clause
0.13.0,"walk_packages() ignores DeprecationWarnings, now we need to ignore"
0.13.0,FutureWarnings
0.13.0,"mypy error: Module has no attribute ""__path__"""
0.13.0,functions to ignore args / docstring of
0.13.0,Methods where y param should be ignored if y=None by default
0.13.0,numpydoc 0.8.0's docscrape tool raises because of collections.abc under
0.13.0,Python 3.7
0.13.0,Test module docstring formatting
0.13.0,Skip test if numpydoc is not found
0.13.0,XXX unreached code as of v0.22
0.13.0,"pytest tooling, not part of the scikit-learn API"
0.13.0,Exclude non-scikit-learn classes
0.13.0,Now skip docstring test for y when y is None
0.13.0,by default for API reason
0.13.0,Exclude imported functions
0.13.0,Don't test private methods / functions
0.13.0,Test that there are no tabs in our source files
0.13.0,because we don't import
0.13.0,"As certain attributes are present ""only"" if a certain parameter is"
0.13.0,"provided, this checks if the word ""only"" is present in the attribute"
0.13.0,"description, and if not the attribute is required to be present."
0.13.0,ignore deprecation warnings
0.13.0,attributes
0.13.0,properties
0.13.0,ignore properties that raises an AttributeError and deprecated
0.13.0,properties
0.13.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.13.0,License: MIT
0.13.0,check that we can let a pass a regression variable by turning down the
0.13.0,validation
0.13.0,Check that the validation is bypass when calling `fit`
0.13.0,Non-regression test for:
0.13.0,https://github.com/scikit-learn-contrib/imbalanced-learn/issues/782
0.13.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.13.0,Christos Aridas
0.13.0,License: MIT
0.13.0,store timestamp to figure out whether the result of 'fit' has been
0.13.0,cached or not
0.13.0,store timestamp to figure out whether the result of 'fit' has been
0.13.0,cached or not
0.13.0,Pipeline accepts steps as tuple
0.13.0,Test the various init parameters of the pipeline.
0.13.0,Check that we can't instantiate pipelines with objects without fit
0.13.0,method
0.13.0,Smoke test with only an estimator
0.13.0,Check that params are set
0.13.0,Smoke test the repr:
0.13.0,Test with two objects
0.13.0,Check that we can't instantiate with non-transformers on the way
0.13.0,"Note that NoTrans implements fit, but not transform"
0.13.0,Check that params are set
0.13.0,Smoke test the repr:
0.13.0,Check that params are not set when naming them wrong
0.13.0,Test clone
0.13.0,"Check that apart from estimators, the parameters are the same"
0.13.0,Remove estimators that where copied
0.13.0,Test the various methods of the pipeline (anova).
0.13.0,Test with Anova + LogisticRegression
0.13.0,Test that the pipeline can take fit parameters
0.13.0,classifier should return True
0.13.0,and transformer params should not be changed
0.13.0,invalid parameters should raise an error message
0.13.0,Pipeline should pass sample_weight
0.13.0,When sample_weight is None it shouldn't be passed
0.13.0,Test pipeline raises set params error message for nested models.
0.13.0,nested model check
0.13.0,Test the various methods of the pipeline (pca + svm).
0.13.0,Test with PCA + SVC
0.13.0,Test the various methods of the pipeline (preprocessing + svm).
0.13.0,check shapes of various prediction functions
0.13.0,test that the fit_predict method is implemented on a pipeline
0.13.0,test that the fit_predict on pipeline yields same results as applying
0.13.0,transform and clustering steps separately
0.13.0,"As pipeline doesn't clone estimators on construction,"
0.13.0,it must have its own estimators
0.13.0,first compute the transform and clustering step separately
0.13.0,use a pipeline to do the transform and clustering in one step
0.13.0,tests that a pipeline does not have fit_predict method when final
0.13.0,step of pipeline does not have fit_predict defined
0.13.0,tests that Pipeline passes fit_params to intermediate steps
0.13.0,when fit_predict is invoked
0.13.0,Test whether pipeline works with a transformer at the end.
0.13.0,Also test pipeline.transform and pipeline.inverse_transform
0.13.0,test transform and fit_transform:
0.13.0,Test whether pipeline works with a transformer missing fit_transform
0.13.0,test fit_transform:
0.13.0,Directly setting attr
0.13.0,Using set_params
0.13.0,Using set_params to replace single step
0.13.0,With invalid data
0.13.0,Test setting Pipeline steps to None
0.13.0,"for other methods, ensure no AttributeErrors on None:"
0.13.0,mult2 and mult3 are active
0.13.0,Check 'passthrough' step at construction time
0.13.0,Test with Transformer + SVC
0.13.0,Memoize the transformer at the first fit
0.13.0,Get the time stamp of the tranformer in the cached pipeline
0.13.0,Check that cached_pipe and pipe yield identical results
0.13.0,Check that we are reading the cache while fitting
0.13.0,a second time
0.13.0,Check that cached_pipe and pipe yield identical results
0.13.0,Create a new pipeline with cloned estimators
0.13.0,Check that even changing the name step does not affect the cache hit
0.13.0,Check that cached_pipe and pipe yield identical results
0.13.0,Test with Transformer + SVC
0.13.0,Memoize the transformer at the first fit
0.13.0,Get the time stamp of the tranformer in the cached pipeline
0.13.0,Check that cached_pipe and pipe yield identical results
0.13.0,Check that we are reading the cache while fitting
0.13.0,a second time
0.13.0,Check that cached_pipe and pipe yield identical results
0.13.0,Create a new pipeline with cloned estimators
0.13.0,Check that even changing the name step does not affect the cache hit
0.13.0,Check that cached_pipe and pipe yield identical results
0.13.0,Test the various methods of the pipeline (pca + svm).
0.13.0,Test with PCA + SVC
0.13.0,Test the various methods of the pipeline (pca + svm).
0.13.0,Test with PCA + SVC
0.13.0,Test whether pipeline works with a sampler at the end.
0.13.0,Also test pipeline.sampler
0.13.0,test transform and fit_transform:
0.13.0,We round the value near to zero. It seems that PCA has some issue
0.13.0,with that
0.13.0,Test whether pipeline works with a sampler at the end.
0.13.0,Also test pipeline.sampler
0.13.0,Test pipeline using None as preprocessing step and a classifier
0.13.0,"Test pipeline using None, RUS and a classifier"
0.13.0,"Test pipeline using RUS, None and a classifier"
0.13.0,Test pipeline using None step and a sampler
0.13.0,Test pipeline using None and a transformer that implements transform and
0.13.0,inverse_transform
0.13.0,Test the various methods of the pipeline (anova).
0.13.0,Test with RandomUnderSampling + Anova + LogisticRegression
0.13.0,Test the various methods of the pipeline (anova).
0.13.0,Test the various methods of the pipeline (anova).
0.13.0,Test with RandomUnderSampling + Anova + LogisticRegression
0.13.0,tests that Pipeline passes predict_params to the final estimator
0.13.0,when predict is invoked
0.13.0,Test that the score_samples method is implemented on a pipeline.
0.13.0,Test that the score_samples method on pipeline yields same results as
0.13.0,applying transform and score_samples steps separately.
0.13.0,Check the shapes
0.13.0,Check the values
0.13.0,Test that a pipeline does not have score_samples method when the final
0.13.0,step of the pipeline does not have score_samples defined.
0.13.0,Test that the score_samples method is implemented on a pipeline.
0.13.0,Test that the score_samples method on pipeline yields same results as
0.13.0,applying transform and score_samples steps separately.
0.13.0,Check the shapes
0.13.0,Check the values
0.13.0,transformer will not change `y` and sampler will always preserve the type of `y`
0.13.0,transformer will not change `y` and sampler will always preserve the type of `y`
0.13.0,TODO(0.15): change warning to checking for NotFittedError
0.13.0,transform_input tests
0.13.0,=====================
0.13.0,end of transform_input tests
0.13.0,=============================
0.13.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.13.0,Christos Aridas
0.13.0,License: MIT
0.13.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.13.0,License: MIT
0.13.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.13.0,License: MIT
0.13.0,Adapated from scikit-learn
0.13.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.13.0,License: MIT
0.13.0,we don't filter samplers based on their tag here because we want to make
0.13.0,sure that the fitted attribute does not exist if the tag is not
0.13.0,stipulated
0.13.0,trigger our checks if this is a SamplerMixin
0.13.0,should raise warning if the target is continuous (we cannot raise error)
0.13.0,if the target is multilabel then we should raise an error
0.13.0,IHT does not enforce the number of samples but provide a number
0.13.0,of samples the closest to the desired target.
0.13.0,in this test we will force all samplers to not change the class 1
0.13.0,check that sparse matrices can be passed through the sampler leading to
0.13.0,the same results than dense
0.13.0,Check that the samplers handle pandas dataframe and pandas series
0.13.0,check that we return the same type for dataframes or series types
0.13.0,Check that the samplers handle pandas dataframe and pandas series
0.13.0,check that we return the same type for dataframes or series types
0.13.0,Check that the can samplers handle simple lists
0.13.0,Check that multiclass target lead to the same results than OVA encoding
0.13.0,Cast X and y to not default dtype
0.13.0,Non-regression test for #709
0.13.0,https://github.com/scikit-learn-contrib/imbalanced-learn/issues/709
0.13.0,Check that an informative error is raised when the value of a constructor
0.13.0,parameter does not have an appropriate type or value.
0.13.0,check that there is a constraint for each parameter
0.13.0,this object does not have a valid type for sure for all params
0.13.0,This parameter is not validated
0.13.0,"First, check that the error is raised if param doesn't match any valid type."
0.13.0,the method is not accessible with the current set of parameters
0.13.0,"Then, for constraints that are more than a type constraint, check that the"
0.13.0,error is raised if param does match a valid type but does not match any valid
0.13.0,value for this type.
0.13.0,the method is not accessible with the current set of parameters
0.13.0,Check that calling `fit` does not raise any warnings about feature names.
0.13.0,Only check imblearn estimators for feature_names_in_ in docstring
0.13.0,partial_fit checks on second call
0.13.0,Do not call partial fit if early_stopping is on
0.13.0,input_features names is not the same length as n_features_in_
0.13.0,error is raised when `input_features` do not match feature_names_in
0.13.0,Adapted from scikit-learn
0.13.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.13.0,License: MIT
0.13.0,Ignore deprecation warnings triggered at import time and from walking
0.13.0,packages
0.13.0,get rid of abstract base classes
0.13.0,get rid of sklearn estimators which have been imported in some classes
0.13.0,"drop duplicates, sort for reproducibility"
0.13.0,itemgetter is used to ensure the sort does not extend to the 2nd item of
0.13.0,the tuple
0.13.0,#######################################################################################
0.13.0,The following code does not depend on the sklearn version
0.13.0,#######################################################################################
0.13.0,tags infrastructure
0.13.0,"Array-API was introduced in 1.3, we need to default to False if not inside"
0.13.0,the old-tags.
0.13.0,#######################################################################################
0.13.0,Upgrading for scikit-learn 1.3
0.13.0,#######################################################################################
0.13.0,parameter validation
0.13.0,parameter validation
0.13.0,#######################################################################################
0.13.0,Upgrading for scikit-learn 1.4
0.13.0,#######################################################################################
0.13.0,#######################################################################################
0.13.0,Upgrading for scikit-learn 1.5
0.13.0,#######################################################################################
0.13.0,chunking
0.13.0,extmath
0.13.0,fixes
0.13.0,indexing
0.13.0,mask
0.13.0,missing
0.13.0,optional dependencies
0.13.0,user interface
0.13.0,validation
0.13.0,chunking
0.13.0,indexing
0.13.0,mask
0.13.0,missing
0.13.0,optional dependencies
0.13.0,user interface
0.13.0,extmath
0.13.0,fixes
0.13.0,validation
0.13.0,#######################################################################################
0.13.0,Upgrading for scikit-learn 1.6
0.13.0,#######################################################################################
0.13.0,test_common
0.13.0,fix for raise_unknown which is introduced in scikit-learn 1.6
0.13.0,validation
0.13.0,tags infrastructure
0.13.0,Get tags from class-level _more_tags
0.13.0,Update with the xfail checks
0.13.0,Patch both class and instance level
0.13.0,"legacy, on_skip, on_fail, and callback are not supported and ignored"
0.13.0,legacy is not supported and ignored
0.13.0,test_common
0.13.0,tags infrastructure
0.13.0,validation
0.13.0,Author: Alexander L. Hayes <hayesall@iu.edu>
0.13.0,License: MIT
0.13.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.13.0,License: MIT
0.13.0,We lost the y.index during resampling. We can safely use X.index to align
0.13.0,them.
0.13.0,We special case the following error:
0.13.0,https://github.com/scikit-learn-contrib/imbalanced-learn/issues/1055
0.13.0,"There is no easy way to have a generic workaround. Here, we detect"
0.13.0,that we have a column with only null values that is datetime64
0.13.0,(resulting from the np.vstack of the resampling).
0.13.0,try again
0.13.0,_is_neighbors_object(nn_object)
0.13.0,check that all keys in sampling_strategy are also in y
0.13.0,check that there is no negative number
0.13.0,check that all keys in sampling_strategy are also in y
0.13.0,ignore first 'self' argument for instance methods
0.13.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.13.0,Christos Aridas
0.13.0,License: MIT
0.13.0,The following dictionary is to indicate constructor arguments suitable for the test
0.13.0,"suite, which uses very small datasets, and is intended to run rather quickly."
0.13.0,estimator
0.13.0,AdaBoostClassifier does not allow nan values
0.13.0,DecisionTreeClassifier allows nan values
0.13.0,over-sampling
0.13.0,under-sampling
0.13.0,combination
0.13.0,This dictionary stores parameters for specific checks. It also enables running the
0.13.0,same check with multiple instances of the same estimator with different parameters.
0.13.0,"The special key ""*"" allows to apply the parameters to all checks."
0.13.0,TODO(devtools): allow third-party developers to pass test specific params to checks
0.13.0,raise additional warning to be shown by pytest
0.13.0,TODO(devtools): enable this behavior for third party estimators as well
0.13.0,partial tests
0.13.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.13.0,Christos Aridas
0.13.0,License: MIT
0.13.0,this function could create an equal number of samples
0.13.0,We pass on purpose a non sorted dictionary and check that the resulting
0.13.0,dictionary is sorted. Refer to issue #428.
0.13.0,DataFrame and DataFrame case
0.13.0,DataFrames and Series case
0.13.0,The * is place before a keyword only argument without a default value
0.13.0,local import to not import the file with Python < 3.11
0.13.0,Test that the minimum dependencies in the README.rst file are
0.13.0,consistent with the minimum dependencies defined at the file:
0.13.0,pyproject.toml
0.13.0,Skip the test if the README.rst file is not available.
0.13.0,"For instance, when installing scikit-learn from wheels"
0.13.0,Author: Alexander L. Hayes <hayesall@iu.edu>
0.13.0,License: MIT
0.13.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.13.0,License: MIT
0.13.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.13.0,Christos Aridas
0.13.0,License: MIT
0.13.0,check if the filtering is working with a list or a single string
0.13.0,check that all estimators are sampler
0.13.0,check that an error is raised when the type is unknown
0.13.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.13.0,License: MIT
0.13.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.13.0,Christos Aridas
0.13.0,License: MIT
0.13.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.13.0,Christos Aridas
0.13.0,License: MIT
0.13.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.13.0,Christos Aridas
0.13.0,License: MIT
0.13.0,Check if default job count is None
0.13.0,Check if job count is set
0.13.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.13.0,Christos Aridas
0.13.0,License: MIT
0.13.0,Check if default job count is none
0.13.0,Check if job count is set
0.13.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.13.0,License: MIT
0.13.0,resample before to fit the tree
0.13.0,TODO: remove when the minimum supported version of scikit-learn will be 1.4
0.13.0,support for missing values
0.13.0,make a deepcopy to not modify the original dictionary
0.13.0,TODO: remove when the minimum supported version of scikit-learn will be 1.4
0.13.0,use scikit-learn support for monotonic constraints
0.13.0,create an attribute for compatibility with other scikit-learn tools such
0.13.0,as HTML representation.
0.13.0,Validate or convert input data
0.13.0,TODO: remove when the minimum supported version of scipy will be 1.4
0.13.0,Support for missing values
0.13.0,TODO: remove when the minimum supported version of scikit-learn will be 1.4
0.13.0,_compute_missing_values_in_feature_mask checks if X has missing values and
0.13.0,will raise an error if the underlying tree base estimator can't handle
0.13.0,missing values. Only the criterion is required to determine if the tree
0.13.0,supports missing values.
0.13.0,Pre-sort indices to avoid that each individual tree of the
0.13.0,ensemble sorts the indices.
0.13.0,reshape is necessary to preserve the data contiguity against vs
0.13.0,"[:, np.newaxis] that does not."
0.13.0,Get bootstrap sample size
0.13.0,Check parameters
0.13.0,"Free allocated memory, if any"
0.13.0,We draw from the random state to get the random state we
0.13.0,would have got if we hadn't used a warm_start.
0.13.0,Parallel loop: we prefer the threading backend as the Cython code
0.13.0,for fitting the trees is internally releasing the Python GIL
0.13.0,making threading more efficient than multiprocessing in
0.13.0,"that case. However, we respect any parallel_backend contexts set"
0.13.0,"at a higher level, since correctness does not rely on using"
0.13.0,threads.
0.13.0,Collect newly grown trees
0.13.0,Create pipeline with the fitted samplers and trees
0.13.0,FIXME: we could consider to support multiclass-multioutput if
0.13.0,we introduce or reuse a constructor parameter (e.g.
0.13.0,oob_score) allowing our user to pass a callable defining the
0.13.0,scoring strategy on OOB sample.
0.13.0,Decapsulate classes_ attributes
0.13.0,drop the n_outputs axis if there is a single output
0.13.0,Prediction requires X to be in CSR format
0.13.0,n_classes_ is a ndarray at this stage
0.13.0,all the supported type of target will have the same number of
0.13.0,classes in all outputs
0.13.0,"for regression, n_classes_ does not exist and we create an empty"
0.13.0,axis to be consistent with the classification case and make
0.13.0,the array operations compatible with the 2 settings
0.13.0,make a deepcopy to not modify the original dictionary
0.13.0,TODO: remove when minimum supported version of scikit-learn is 1.4
0.13.0,SAMME-R requires predict_proba-enabled estimators
0.13.0,Instances incorrectly classified
0.13.0,Error fraction
0.13.0,Stop if classification is perfect
0.13.0,Construct y coding as described in Zhu et al [2]:
0.13.0,
0.13.0,y_k = 1 if c == k else -1 / (K - 1)
0.13.0,
0.13.0,"where K == n_classes_ and c, k in [0, K) are indices along the second"
0.13.0,axis of the y coding with c being the index corresponding to the true
0.13.0,class label.
0.13.0,Displace zero probabilities so the log is defined.
0.13.0,Also fix negative elements which may occur with
0.13.0,negative sample weights.
0.13.0,Boost weight using multi-class AdaBoost SAMME.R alg
0.13.0,Only boost the weights if it will fit again
0.13.0,Only boost positive weights
0.13.0,Instances incorrectly classified
0.13.0,Error fraction
0.13.0,Stop if classification is perfect
0.13.0,Stop if the error is at least as bad as random guessing
0.13.0,Boost weight using multi-class AdaBoost SAMME alg
0.13.0,Only boost the weights if I will fit again
0.13.0,Only boost positive weights
0.13.0,TODO(0.14): remove this method because algorithm is deprecated.
0.13.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.13.0,Christos Aridas
0.13.0,License: MIT
0.13.0,make a deepcopy to not modify the original dictionary
0.13.0,TODO: remove when minimum supported version of scikit-learn is 1.4
0.13.0,overwrite the base class method by disallowing `sample_weight`
0.13.0,RandomUnderSampler is not supporting sample_weight. We need to pass
0.13.0,None.
0.13.0,TODO: remove when minimum supported version of scikit-learn is 1.1
0.13.0,Check data
0.13.0,Parallel loop
0.13.0,Reduce
0.13.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.13.0,Christos Aridas
0.13.0,License: MIT
0.13.0,make a deepcopy to not modify the original dictionary
0.13.0,TODO: remove when minimum supported version of scikit-learn is 1.4
0.13.0,overwrite the base class method by disallowing `sample_weight`
0.13.0,the sampler needs to be validated before to call _fit because
0.13.0,_validate_y is called before _validate_estimator and would require
0.13.0,to know which type of sampler we are using.
0.13.0,RandomUnderSampler is not supporting sample_weight. We need to pass
0.13.0,None.
0.13.0,check that we have an ensemble of samplers and estimators with a
0.13.0,consistent size
0.13.0,each sampler in the ensemble should have different random state
0.13.0,each estimator in the ensemble should have different random state
0.13.0,check the consistency of the feature importances
0.13.0,check the consistency of the prediction outpus
0.13.0,Predictions should be the same when sample_weight are all ones
0.13.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.13.0,Christos Aridas
0.13.0,License: MIT
0.13.0,Check classification for various parameter settings.
0.13.0,Test that bootstrapping samples generate non-perfect base estimators.
0.13.0,"without bootstrap, all trees are perfect on the training set"
0.13.0,disable the resampling by passing an empty dictionary.
0.13.0,"with bootstrap, trees are no longer perfect on the training set"
0.13.0,Test that bootstrapping features may generate duplicate features.
0.13.0,Predict probabilities.
0.13.0,Normal case
0.13.0,"Degenerate case, where some classes are missing"
0.13.0,Check that oob prediction is a good estimation of the generalization
0.13.0,error.
0.13.0,Test with few estimators
0.13.0,Check singleton ensembles.
0.13.0,Check that bagging ensembles can be grid-searched.
0.13.0,Transform iris into a binary classification task
0.13.0,Grid search with scoring based on decision_function
0.13.0,Check estimator and its default values.
0.13.0,Test if fitting incrementally with warm start gives a forest of the
0.13.0,right size and the same results as a normal fit.
0.13.0,Test if warm start'ed second fit with smaller n_estimators raises error.
0.13.0,Test that nothing happens when fitting without increasing n_estimators
0.13.0,"modify X to nonsense values, this should not change anything"
0.13.0,warm started classifier with 5+5 estimators should be equivalent to
0.13.0,one classifier with 10 estimators
0.13.0,Check using oob_score and warm_start simultaneously fails
0.13.0,"Make sure OOB scores are identical when random_state, estimator, and"
0.13.0,training data are fixed and fitting is done twice
0.13.0,Check that format of estimators_samples_ is correct and that results
0.13.0,generated at fit time can be identically reproduced at a later time
0.13.0,using data saved in object attributes.
0.13.0,remap the y outside of the BalancedBaggingclassifier
0.13.0,"_, y = np.unique(y, return_inverse=True)"
0.13.0,Get relevant attributes
0.13.0,Test for correct formatting
0.13.0,Re-fit single estimator to test for consistent sampling
0.13.0,Make sure validated max_samples and original max_samples are identical
0.13.0,when valid integer max_samples supplied by user
0.13.0,check that we can pass any kind of sampler to a bagging classifier
0.13.0,check that we have balanced class with the right counts of class
0.13.0,sample depending on the sampling strategy
0.13.0,check that we can provide a FunctionSampler in BalancedBaggingClassifier
0.13.0,find the minority and majority classes
0.13.0,compute the number of sample to draw from the majority class using
0.13.0,a negative binomial distribution
0.13.0,draw randomly with or without replacement
0.13.0,Roughly Balanced Bagging
0.13.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.13.0,Christos Aridas
0.13.0,License: MIT
0.13.0,Generate a global dataset to use
0.13.0,Check classification for various parameter settings.
0.13.0,test the different prediction function
0.13.0,Check estimator and its default values.
0.13.0,Test if fitting incrementally with warm start gives a forest of the
0.13.0,right size and the same results as a normal fit.
0.13.0,Test if warm start'ed second fit with smaller n_estimators raises error.
0.13.0,Test that nothing happens when fitting without increasing n_estimators
0.13.0,"modify X to nonsense values, this should not change anything"
0.13.0,warm started classifier with 5+5 estimators should be equivalent to
0.13.0,one classifier with 10 estimators
0.13.0,Check warning if not enough estimators
0.13.0,First fit with no restriction on max samples
0.13.0,Second fit with max samples restricted to just 2
0.13.0,Regression test for #655: check that the oob score is closed to 0.5
0.13.0,a binomial experiment.
0.13.0,Create dataset with missing values
0.13.0,Train forest with missing values
0.13.0,Train forest without missing values
0.13.0,Score is still 80 percent of the forest's score that had no missing values
0.13.0,Create a predictive feature using `y` and with some noise
0.13.0,Author: Guillaume Lemaitre
0.13.0,License: BSD 3 clause
0.13.0,"The index start at one, then we need to remove one"
0.13.0,to not have issue with the indexing.
0.13.0,go through the list and check if the data are available
0.13.0,Authors: Dayvid Oliveira
0.13.0,Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.13.0,Christos Aridas
0.13.0,License: MIT
0.13.0,restrict ratio to be a dict or a callable
0.13.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.13.0,Christos Aridas
0.13.0,License: MIT
0.13.0,"we are reusing part of utils.check_sampling_strategy, however this is not"
0.13.0,cover in the common tests so we will repeat it here
0.13.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.13.0,Christos Aridas
0.13.0,License: MIT
0.13.0,This is a trick to avoid an error during tests collection with pytest. We
0.13.0,avoid the error when importing the package raise the error at the moment of
0.13.0,creating the instance.
0.13.0,This is a trick to avoid an error during tests collection with pytest. We
0.13.0,avoid the error when importing the package raise the error at the moment of
0.13.0,creating the instance.
0.13.0,flag for keras sequence duck-typing
0.13.0,shuffle the indices since the sampler are packing them by class
0.12.4,This file is here so that when running from the root folder
0.12.4,./imblearn is added to sys.path by pytest.
0.12.4,See https://docs.pytest.org/en/latest/pythonpath.html for more details.
0.12.4,"For example, this allows to build extensions in place and run pytest"
0.12.4,doc/modules/clustering.rst and use imblearn from the local folder
0.12.4,rather than the one from site-packages.
0.12.4,use legacy numpy print options to avoid failures due to NumPy 2.+ scalar
0.12.4,representation
0.12.4,! /usr/bin/env python
0.12.4,Python 2 compat: just to be able to declare that Python >=3.7 is needed.
0.12.4,This is a bit (!) hackish: we are setting a global variable so that the
0.12.4,main imblearn __init__ can detect if it is being loaded by the setup
0.12.4,"routine, to avoid attempting to load components that aren't built yet:"
0.12.4,the numpy distutils extensions that are used by imbalanced-learn to
0.12.4,recursively build the compiled extensions in sub-packages is based on the
0.12.4,Python import machinery.
0.12.4,get __version__ from _version.py
0.12.4,-*- coding: utf-8 -*-
0.12.4,
0.12.4,"imbalanced-learn documentation build configuration file, created by"
0.12.4,sphinx-quickstart on Mon Jan 18 14:44:12 2016.
0.12.4,
0.12.4,This file is execfile()d with the current directory set to its
0.12.4,containing dir.
0.12.4,
0.12.4,Note that not all possible configuration values are present in this
0.12.4,autogenerated file.
0.12.4,
0.12.4,All configuration values have a default; values that are commented out
0.12.4,serve to show the default.
0.12.4,"If extensions (or modules to document with autodoc) are in another directory,"
0.12.4,add these directories to sys.path here. If the directory is relative to the
0.12.4,"documentation root, use os.path.abspath to make it absolute, like shown here."
0.12.4,-- General configuration ------------------------------------------------
0.12.4,"If your documentation needs a minimal Sphinx version, state it here."
0.12.4,needs_sphinx = '1.0'
0.12.4,"Add any Sphinx extension module names here, as strings. They can be"
0.12.4,extensions coming with Sphinx (named 'sphinx.ext.*') or your custom
0.12.4,ones.
0.12.4,Specify how to identify the prompt when copying code snippets
0.12.4,"Add any paths that contain templates here, relative to this directory."
0.12.4,The suffix of source filenames.
0.12.4,The master toctree document.
0.12.4,General information about the project.
0.12.4,"The version info for the project you're documenting, acts as replacement for"
0.12.4,"|version| and |release|, also used in various other places throughout the"
0.12.4,built documents.
0.12.4,
0.12.4,The short X.Y version.
0.12.4,"The full version, including alpha/beta/rc tags."
0.12.4,"List of patterns, relative to source directory, that match files and"
0.12.4,directories to ignore when looking for source files.
0.12.4,The reST default role (used for this markup: `text`) to use for all
0.12.4,documents.
0.12.4,"If true, '()' will be appended to :func: etc. cross-reference text."
0.12.4,The name of the Pygments (syntax highlighting) style to use.
0.12.4,-- Options for HTML output ----------------------------------------------
0.12.4,The theme to use for HTML and HTML Help pages.  See the documentation for
0.12.4,a list of builtin themes.
0.12.4,"""navbar_align"": ""right"",  # For testing that the navbar items align properly"
0.12.4,"Add any paths that contain custom static files (such as style sheets) here,"
0.12.4,"relative to this directory. They are copied after the builtin static files,"
0.12.4,"so a file named ""default.css"" will overwrite the builtin ""default.css""."
0.12.4,Output file base name for HTML help builder.
0.12.4,-- Options for autodoc ------------------------------------------------------
0.12.4,generate autosummary even if no references
0.12.4,-- Options for numpydoc -----------------------------------------------------
0.12.4,this is needed for some reason...
0.12.4,see https://github.com/numpy/numpydoc/issues/69
0.12.4,-- Options for sphinxcontrib-bibtex -----------------------------------------
0.12.4,bibtex file
0.12.4,-- Options for intersphinx --------------------------------------------------
0.12.4,intersphinx configuration
0.12.4,-- Options for sphinx-gallery -----------------------------------------------
0.12.4,Generate the plot for the gallery
0.12.4,sphinx-gallery configuration
0.12.4,-- Options for github link for what's new -----------------------------------
0.12.4,Config for sphinx_issues
0.12.4,The following is used by sphinx.ext.linkcode to provide links to github
0.12.4,-- Options for LaTeX output ---------------------------------------------
0.12.4,The paper size ('letterpaper' or 'a4paper').
0.12.4,"'papersize': 'letterpaper',"
0.12.4,"The font size ('10pt', '11pt' or '12pt')."
0.12.4,"'pointsize': '10pt',"
0.12.4,Additional stuff for the LaTeX preamble.
0.12.4,"'preamble': '',"
0.12.4,Grouping the document tree into LaTeX files. List of tuples
0.12.4,"(source start file, target name, title,"
0.12.4,"author, documentclass [howto, manual, or own class])."
0.12.4,-- Options for manual page output ---------------------------------------
0.12.4,"If false, no module index is generated."
0.12.4,latex_domain_indices = True
0.12.4,One entry per manual page. List of tuples
0.12.4,"(source start file, name, description, authors, manual section)."
0.12.4,"If true, show URL addresses after external links."
0.12.4,man_show_urls = False
0.12.4,-- Options for Texinfo output -------------------------------------------
0.12.4,Grouping the document tree into Texinfo files. List of tuples
0.12.4,"(source start file, target name, title, author,"
0.12.4,"dir menu entry, description, category)"
0.12.4,-- Dependencies generation ----------------------------------------------
0.12.4,get length of header
0.12.4,-- Additional temporary hacks -----------------------------------------------
0.12.4,get the styles from the current theme
0.12.4,create and add the button to all the code blocks that contain >>>
0.12.4,tracebacks (.gt) contain bare text elements that need to be
0.12.4,wrapped in a span to work with .nextUntil() (see later)
0.12.4,define the behavior of the button when it's clicked
0.12.4,hide the code output
0.12.4,show the code output
0.12.4,-*- coding: utf-8 -*-
0.12.4,Format template for issues URI
0.12.4,e.g. 'https://github.com/sloria/marshmallow/issues/{issue}
0.12.4,Format template for PR URI
0.12.4,e.g. 'https://github.com/sloria/marshmallow/pull/{issue}
0.12.4,Format template for commit URI
0.12.4,e.g. 'https://github.com/sloria/marshmallow/commits/{commit}
0.12.4,"Shortcut for Github, e.g. 'sloria/marshmallow'"
0.12.4,Format template for user profile URI
0.12.4,e.g. 'https://github.com/{user}'
0.12.4,Python 2 only
0.12.4,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.4,License: MIT
0.12.4,%%
0.12.4,%%
0.12.4,"First, we will generate a toy classification dataset with only few samples."
0.12.4,The ratio between the classes will be imbalanced.
0.12.4,%%
0.12.4,%%
0.12.4,"Now, we will use a :class:`~imblearn.over_sampling.RandomOverSampler` to"
0.12.4,generate a bootstrap for the minority class with as many samples as in the
0.12.4,majority class.
0.12.4,%%
0.12.4,%%
0.12.4,We observe that the minority samples are less transparent than the samples
0.12.4,"from the majority class. Indeed, it is due to the fact that these samples"
0.12.4,of the minority class are repeated during the bootstrap generation.
0.12.4,
0.12.4,We can set `shrinkage` to a floating value to add a small perturbation to the
0.12.4,samples created and therefore create a smoothed bootstrap.
0.12.4,%%
0.12.4,%%
0.12.4,"In this case, we see that the samples in the minority class are not"
0.12.4,overlapping anymore due to the added noise.
0.12.4,
0.12.4,The parameter `shrinkage` allows to add more or less perturbation. Let's
0.12.4,add more perturbation when generating the smoothed bootstrap.
0.12.4,%%
0.12.4,%%
0.12.4,Increasing the value of `shrinkage` will disperse the new samples. Forcing
0.12.4,the shrinkage to 0 will be equivalent to generating a normal bootstrap.
0.12.4,%%
0.12.4,%%
0.12.4,"Therefore, the `shrinkage` is handy to manually tune the dispersion of the"
0.12.4,new samples.
0.12.4,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.4,License: MIT
0.12.4,%%
0.12.4,generate some data points
0.12.4,plot the majority and minority samples
0.12.4,draw the circle in which the new sample will generated
0.12.4,plot the line on which the sample will be generated
0.12.4,create and plot the new sample
0.12.4,make the plot nicer with legend and label
0.12.4,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.4,License: MIT
0.12.4,%%
0.12.4,%% [markdown]
0.12.4,The following function will be used to create toy dataset. It uses the
0.12.4,:func:`~sklearn.datasets.make_classification` from scikit-learn but fixing
0.12.4,some parameters.
0.12.4,%%
0.12.4,%% [markdown]
0.12.4,The following function will be used to plot the sample space after resampling
0.12.4,to illustrate the specificities of an algorithm.
0.12.4,%%
0.12.4,%% [markdown]
0.12.4,The following function will be used to plot the decision function of a
0.12.4,classifier given some data.
0.12.4,%%
0.12.4,%% [markdown]
0.12.4,Illustration of the influence of the balancing ratio
0.12.4,----------------------------------------------------
0.12.4,
0.12.4,We will first illustrate the influence of the balancing ratio on some toy
0.12.4,data using a logistic regression classifier which is a linear model.
0.12.4,%%
0.12.4,%% [markdown]
0.12.4,We will fit and show the decision boundary model to illustrate the impact of
0.12.4,dealing with imbalanced classes.
0.12.4,%%
0.12.4,%% [markdown]
0.12.4,"Greater is the difference between the number of samples in each class, poorer"
0.12.4,are the classification results.
0.12.4,
0.12.4,Random over-sampling to balance the data set
0.12.4,--------------------------------------------
0.12.4,
0.12.4,Random over-sampling can be used to repeat some samples and balance the
0.12.4,number of samples between the dataset. It can be seen that with this trivial
0.12.4,approach the boundary decision is already less biased toward the majority
0.12.4,class. The class :class:`~imblearn.over_sampling.RandomOverSampler`
0.12.4,implements such of a strategy.
0.12.4,%%
0.12.4,%% [markdown]
0.12.4,"By default, random over-sampling generates a bootstrap. The parameter"
0.12.4,`shrinkage` allows adding a small perturbation to the generated data
0.12.4,to generate a smoothed bootstrap instead. The plot below shows the difference
0.12.4,between the two data generation strategies.
0.12.4,%%
0.12.4,%% [markdown]
0.12.4,It looks like more samples are generated with smoothed bootstrap. This is due
0.12.4,to the fact that the samples generated are not superimposing with the
0.12.4,original samples.
0.12.4,
0.12.4,More advanced over-sampling using ADASYN and SMOTE
0.12.4,--------------------------------------------------
0.12.4,
0.12.4,Instead of repeating the same samples when over-sampling or perturbating the
0.12.4,"generated bootstrap samples, one can use some specific heuristic instead."
0.12.4,:class:`~imblearn.over_sampling.ADASYN` and
0.12.4,:class:`~imblearn.over_sampling.SMOTE` can be used in this case.
0.12.4,%%
0.12.4,%% [markdown]
0.12.4,The following plot illustrates the difference between
0.12.4,:class:`~imblearn.over_sampling.ADASYN` and
0.12.4,:class:`~imblearn.over_sampling.SMOTE`.
0.12.4,:class:`~imblearn.over_sampling.ADASYN` will focus on the samples which are
0.12.4,difficult to classify with a nearest-neighbors rule while regular
0.12.4,:class:`~imblearn.over_sampling.SMOTE` will not make any distinction.
0.12.4,"Therefore, the decision function depending of the algorithm."
0.12.4,%% [markdown]
0.12.4,"Due to those sampling particularities, it can give rise to some specific"
0.12.4,issues as illustrated below.
0.12.4,%%
0.12.4,%% [markdown]
0.12.4,SMOTE proposes several variants by identifying specific samples to consider
0.12.4,during the resampling. The borderline version
0.12.4,(:class:`~imblearn.over_sampling.BorderlineSMOTE`) will detect which point to
0.12.4,select which are in the border between two classes. The SVM version
0.12.4,(:class:`~imblearn.over_sampling.SVMSMOTE`) will use the support vectors
0.12.4,found using an SVM algorithm to create new sample while the KMeans version
0.12.4,(:class:`~imblearn.over_sampling.KMeansSMOTE`) will make a clustering before
0.12.4,to generate samples in each cluster independently depending each cluster
0.12.4,density.
0.12.4,%%
0.12.4,%% [markdown]
0.12.4,"When dealing with a mixed of continuous and categorical features,"
0.12.4,:class:`~imblearn.over_sampling.SMOTENC` is the only method which can handle
0.12.4,this case.
0.12.4,%%
0.12.4,Create a dataset of a mix of numerical and categorical data
0.12.4,%% [markdown]
0.12.4,"However, if the dataset is composed of only categorical features then one"
0.12.4,should use :class:`~imblearn.over_sampling.SMOTEN`.
0.12.4,%%
0.12.4,Generate only categorical data
0.12.4,Authors: Christos Aridas
0.12.4,Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.4,License: MIT
0.12.4,%%
0.12.4,%% [markdown]
0.12.4,Let's first generate a dataset with imbalanced class distribution.
0.12.4,%%
0.12.4,%% [markdown]
0.12.4,We will use an over-sampler :class:`~imblearn.over_sampling.SMOTE` followed
0.12.4,by a :class:`~sklearn.tree.DecisionTreeClassifier`. The aim will be to
0.12.4,search which `k_neighbors` parameter is the most adequate with the dataset
0.12.4,that we generated.
0.12.4,%%
0.12.4,%% [markdown]
0.12.4,We can use the :class:`~sklearn.model_selection.validation_curve` to inspect
0.12.4,"the impact of varying the parameter `k_neighbors`. In this case, we need"
0.12.4,to use a score to evaluate the generalization score during the
0.12.4,cross-validation.
0.12.4,%%
0.12.4,%%
0.12.4,%% [markdown]
0.12.4,We can now plot the results of the cross-validation for the different
0.12.4,parameter values that we tried.
0.12.4,%%
0.12.4,make nice plotting
0.12.4,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.4,License: MIT
0.12.4,Generate a dataset
0.12.4,Split the data
0.12.4,Train the classifier with balancing
0.12.4,Test the classifier and get the prediction
0.12.4,Show the classification report
0.12.4,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.4,License: MIT
0.12.4,%%
0.12.4,%% [markdown]
0.12.4,"First, we will generate some imbalanced dataset."
0.12.4,%%
0.12.4,%% [markdown]
0.12.4,We will split the data into a training and testing set.
0.12.4,%%
0.12.4,%% [markdown]
0.12.4,We will create a pipeline made of a :class:`~imblearn.over_sampling.SMOTE`
0.12.4,over-sampler followed by a :class:`~sklearn.linear_model.LogisticRegression`
0.12.4,classifier.
0.12.4,%%
0.12.4,%% [markdown]
0.12.4,"Now, we will train the model on the training set and get the prediction"
0.12.4,associated with the testing set. Be aware that the resampling will happen
0.12.4,only when calling `fit`: the number of samples in `y_pred` is the same than
0.12.4,in `y_test`.
0.12.4,%%
0.12.4,%% [markdown]
0.12.4,The geometric mean corresponds to the square root of the product of the
0.12.4,sensitivity and specificity. Combining the two metrics should account for
0.12.4,the balancing of the dataset.
0.12.4,%%
0.12.4,%% [markdown]
0.12.4,The index balanced accuracy can transform any metric to be used in
0.12.4,imbalanced learning problems.
0.12.4,%%
0.12.4,%%
0.12.4,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.4,License: MIT
0.12.4,%%
0.12.4,%% [markdown]
0.12.4,Dataset generation
0.12.4,------------------
0.12.4,
0.12.4,We will create an imbalanced dataset with a couple of samples. We will use
0.12.4,:func:`~sklearn.datasets.make_classification` to generate this dataset.
0.12.4,%%
0.12.4,%%
0.12.4,%% [markdown]
0.12.4,The following function will be used to plot the sample space after resampling
0.12.4,to illustrate the characteristic of an algorithm.
0.12.4,%%
0.12.4,%% [markdown]
0.12.4,The following function will be used to plot the decision function of a
0.12.4,classifier given some data.
0.12.4,%%
0.12.4,%% [markdown]
0.12.4,":class:`~imblearn.over_sampling.SMOTE` allows to generate samples. However,"
0.12.4,this method of over-sampling does not have any knowledge regarding the
0.12.4,"underlying distribution. Therefore, some noisy samples can be generated, e.g."
0.12.4,"when the different classes cannot be well separated. Hence, it can be"
0.12.4,beneficial to apply an under-sampling algorithm to clean the noisy samples.
0.12.4,Two methods are usually used in the literature: (i) Tomek's link and (ii)
0.12.4,edited nearest neighbours cleaning methods. Imbalanced-learn provides two
0.12.4,ready-to-use samplers :class:`~imblearn.combine.SMOTETomek` and
0.12.4,":class:`~imblearn.combine.SMOTEENN`. In general,"
0.12.4,:class:`~imblearn.combine.SMOTEENN` cleans more noisy data than
0.12.4,:class:`~imblearn.combine.SMOTETomek`.
0.12.4,%%
0.12.4,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.4,License: MIT
0.12.4,%%
0.12.4,%% [markdown]
0.12.4,Load an imbalanced dataset
0.12.4,--------------------------
0.12.4,
0.12.4,We will load the UCI SatImage dataset which has an imbalanced ratio of 9.3:1
0.12.4,(number of majority sample for a minority sample). The data are then split
0.12.4,into training and testing.
0.12.4,%%
0.12.4,%% [markdown]
0.12.4,Classification using a single decision tree
0.12.4,-------------------------------------------
0.12.4,
0.12.4,We train a decision tree classifier which will be used as a baseline for the
0.12.4,rest of this example.
0.12.4,
0.12.4,The results are reported in terms of balanced accuracy and geometric mean
0.12.4,which are metrics widely used in the literature to validate model trained on
0.12.4,imbalanced set.
0.12.4,%%
0.12.4,%%
0.12.4,%%
0.12.4,%% [markdown]
0.12.4,Classification using bagging classifier with and without sampling
0.12.4,-----------------------------------------------------------------
0.12.4,
0.12.4,"Instead of using a single tree, we will check if an ensemble of decision tree"
0.12.4,"can actually alleviate the issue induced by the class imbalancing. First, we"
0.12.4,will use a bagging classifier and its counter part which internally uses a
0.12.4,random under-sampling to balanced each bootstrap sample.
0.12.4,%%
0.12.4,%% [markdown]
0.12.4,Balancing each bootstrap sample allows to increase significantly the balanced
0.12.4,accuracy and the geometric mean.
0.12.4,%%
0.12.4,%%
0.12.4,%% [markdown]
0.12.4,Classification using random forest classifier with and without sampling
0.12.4,-----------------------------------------------------------------------
0.12.4,
0.12.4,Random forest is another popular ensemble method and it is usually
0.12.4,"outperforming bagging. Here, we used a vanilla random forest and its balanced"
0.12.4,counterpart in which each bootstrap sample is balanced.
0.12.4,%%
0.12.4,%% [markdown]
0.12.4,"Similarly to the previous experiment, the balanced classifier outperform the"
0.12.4,"classifier which learn from imbalanced bootstrap samples. In addition, random"
0.12.4,forest outperforms the bagging classifier.
0.12.4,%%
0.12.4,%%
0.12.4,%% [markdown]
0.12.4,Boosting classifier
0.12.4,-------------------
0.12.4,
0.12.4,"In the same manner, easy ensemble classifier is a bag of balanced AdaBoost"
0.12.4,"classifier. However, it will be slower to train than random forest and will"
0.12.4,achieve worse performance.
0.12.4,%%
0.12.4,%%
0.12.4,%%
0.12.4,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.4,License: MIT
0.12.4,%%
0.12.4,%% [markdown]
0.12.4,Generate an imbalanced dataset
0.12.4,------------------------------
0.12.4,
0.12.4,"For this example, we will create a synthetic dataset using the function"
0.12.4,:func:`~sklearn.datasets.make_classification`. The problem will be a toy
0.12.4,classification problem with a ratio of 1:9 between the two classes.
0.12.4,%%
0.12.4,%%
0.12.4,%% [markdown]
0.12.4,"In the following sections, we will show a couple of algorithms that have"
0.12.4,been proposed over the years. We intend to illustrate how one can reuse the
0.12.4,:class:`~imblearn.ensemble.BalancedBaggingClassifier` by passing different
0.12.4,sampler.
0.12.4,%%
0.12.4,%% [markdown]
0.12.4,Exactly Balanced Bagging and Over-Bagging
0.12.4,-----------------------------------------
0.12.4,
0.12.4,The :class:`~imblearn.ensemble.BalancedBaggingClassifier` can use in
0.12.4,conjunction with a :class:`~imblearn.under_sampling.RandomUnderSampler` or
0.12.4,:class:`~imblearn.over_sampling.RandomOverSampler`. These methods are
0.12.4,"referred as Exactly Balanced Bagging and Over-Bagging, respectively and have"
0.12.4,been proposed first in [1]_.
0.12.4,%%
0.12.4,Exactly Balanced Bagging
0.12.4,%%
0.12.4,Over-bagging
0.12.4,%% [markdown]
0.12.4,SMOTE-Bagging
0.12.4,-------------
0.12.4,
0.12.4,Instead of using a :class:`~imblearn.over_sampling.RandomOverSampler` that
0.12.4,"make a bootstrap, an alternative is to use"
0.12.4,:class:`~imblearn.over_sampling.SMOTE` as an over-sampler. This is known as
0.12.4,SMOTE-Bagging [2]_.
0.12.4,%%
0.12.4,SMOTE-Bagging
0.12.4,%% [markdown]
0.12.4,Roughly Balanced Bagging
0.12.4,------------------------
0.12.4,While using a :class:`~imblearn.under_sampling.RandomUnderSampler` or
0.12.4,:class:`~imblearn.over_sampling.RandomOverSampler` will create exactly the
0.12.4,"desired number of samples, it does not follow the statistical spirit wanted"
0.12.4,in the bagging framework. The authors in [3]_ proposes to use a negative
0.12.4,binomial distribution to compute the number of samples of the majority
0.12.4,class to be selected and then perform a random under-sampling.
0.12.4,
0.12.4,"Here, we illustrate this method by implementing a function in charge of"
0.12.4,resampling and use the :class:`~imblearn.FunctionSampler` to integrate it
0.12.4,within a :class:`~imblearn.pipeline.Pipeline` and
0.12.4,:class:`~sklearn.model_selection.cross_validate`.
0.12.4,%%
0.12.4,find the minority and majority classes
0.12.4,compute the number of sample to draw from the majority class using
0.12.4,a negative binomial distribution
0.12.4,draw randomly with or without replacement
0.12.4,Roughly Balanced Bagging
0.12.4,%% [markdown]
0.12.4,.. topic:: References:
0.12.4,
0.12.4,".. [1] R. Maclin, and D. Opitz. ""An empirical evaluation of bagging and"
0.12.4,"boosting."" AAAI/IAAI 1997 (1997): 546-551."
0.12.4,
0.12.4,".. [2] S. Wang, and X. Yao. ""Diversity analysis on imbalanced data sets by"
0.12.4,"using ensemble models."" 2009 IEEE symposium on computational"
0.12.4,"intelligence and data mining. IEEE, 2009."
0.12.4,
0.12.4,".. [3] S. Hido, H. Kashima, and Y. Takahashi. ""Roughly balanced bagging"
0.12.4,"for imbalanced data."" Statistical Analysis and Data Mining: The ASA"
0.12.4,Data Science Journal 2.5‚Äê6 (2009): 412-426.
0.12.4,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.4,License: MIT
0.12.4,%%
0.12.4,%% [markdown]
0.12.4,The following function will be used to create toy dataset. It uses the
0.12.4,:func:`~sklearn.datasets.make_classification` from scikit-learn but fixing
0.12.4,some parameters.
0.12.4,%%
0.12.4,%% [markdown]
0.12.4,The following function will be used to plot the sample space after resampling
0.12.4,to illustrate the specificities of an algorithm.
0.12.4,%%
0.12.4,%% [markdown]
0.12.4,The following function will be used to plot the decision function of a
0.12.4,classifier given some data.
0.12.4,%%
0.12.4,%%
0.12.4,%% [markdown]
0.12.4,Prototype generation: under-sampling by generating new samples
0.12.4,--------------------------------------------------------------
0.12.4,
0.12.4,:class:`~imblearn.under_sampling.ClusterCentroids` under-samples by replacing
0.12.4,the original samples by the centroids of the cluster found.
0.12.4,%%
0.12.4,%% [markdown]
0.12.4,Prototype selection: under-sampling by selecting existing samples
0.12.4,-----------------------------------------------------------------
0.12.4,
0.12.4,The algorithm performing prototype selection can be subdivided into two
0.12.4,groups: (i) the controlled under-sampling methods and (ii) the cleaning
0.12.4,under-sampling methods.
0.12.4,
0.12.4,"With the controlled under-sampling methods, the number of samples to be"
0.12.4,selected can be specified.
0.12.4,:class:`~imblearn.under_sampling.RandomUnderSampler` is the most naive way of
0.12.4,performing such selection by randomly selecting a given number of samples by
0.12.4,the targeted class.
0.12.4,%%
0.12.4,%% [markdown]
0.12.4,:class:`~imblearn.under_sampling.NearMiss` algorithms implement some
0.12.4,heuristic rules in order to select samples. NearMiss-1 selects samples from
0.12.4,the majority class for which the average distance of the :math:`k`` nearest
0.12.4,samples of the minority class is the smallest. NearMiss-2 selects the samples
0.12.4,from the majority class for which the average distance to the farthest
0.12.4,samples of the negative class is the smallest. NearMiss-3 is a 2-step
0.12.4,"algorithm: first, for each minority sample, their :math:`m`"
0.12.4,"nearest-neighbors will be kept; then, the majority samples selected are the"
0.12.4,on for which the average distance to the :math:`k` nearest neighbors is the
0.12.4,largest.
0.12.4,%%
0.12.4,%% [markdown]
0.12.4,:class:`~imblearn.under_sampling.EditedNearestNeighbours` removes samples of
0.12.4,the majority class for which their class differ from the one of their
0.12.4,nearest-neighbors. This sieve can be repeated which is the principle of the
0.12.4,:class:`~imblearn.under_sampling.RepeatedEditedNearestNeighbours`.
0.12.4,:class:`~imblearn.under_sampling.AllKNN` is slightly different from the
0.12.4,:class:`~imblearn.under_sampling.RepeatedEditedNearestNeighbours` by changing
0.12.4,"the :math:`k` parameter of the internal nearest neighors algorithm,"
0.12.4,increasing it at each iteration.
0.12.4,%%
0.12.4,%% [markdown]
0.12.4,:class:`~imblearn.under_sampling.CondensedNearestNeighbour` makes use of a
0.12.4,1-NN to iteratively decide if a sample should be kept in a dataset or not.
0.12.4,The issue is that :class:`~imblearn.under_sampling.CondensedNearestNeighbour`
0.12.4,is sensitive to noise by preserving the noisy samples.
0.12.4,:class:`~imblearn.under_sampling.OneSidedSelection` also used the 1-NN and
0.12.4,use :class:`~imblearn.under_sampling.TomekLinks` to remove the samples
0.12.4,considered noisy. The
0.12.4,:class:`~imblearn.under_sampling.NeighbourhoodCleaningRule` use a
0.12.4,:class:`~imblearn.under_sampling.EditedNearestNeighbours` to remove some
0.12.4,"sample. Additionally, they use a 3 nearest-neighbors to remove samples which"
0.12.4,do not agree with this rule.
0.12.4,%%
0.12.4,%% [markdown]
0.12.4,:class:`~imblearn.under_sampling.InstanceHardnessThreshold` uses the
0.12.4,prediction of classifier to exclude samples. All samples which are classified
0.12.4,with a low probability will be removed.
0.12.4,%%
0.12.4,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.4,License: MIT
0.12.4,%%
0.12.4,%% [markdown]
0.12.4,This function allows to make nice plotting
0.12.4,%%
0.12.4,%% [markdown]
0.12.4,We will generate some toy data that illustrates how
0.12.4,:class:`~imblearn.under_sampling.TomekLinks` is used to clean a dataset.
0.12.4,%%
0.12.4,%% [markdown]
0.12.4,"In the figure above, the samples highlighted in green form a Tomek link since"
0.12.4,they are of different classes and are nearest neighbors of each other.
0.12.4,highlight the samples of interest
0.12.4,%% [markdown]
0.12.4,We can run the :class:`~imblearn.under_sampling.TomekLinks` sampling to
0.12.4,remove the corresponding samples. If `sampling_strategy='auto'` only the
0.12.4,sample from the majority class will be removed. If `sampling_strategy='all'`
0.12.4,both samples will be removed.
0.12.4,%%
0.12.4,highlight the samples of interest
0.12.4,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.4,License: MIT
0.12.4,%%
0.12.4,%% [markdown]
0.12.4,We define a function allowing to make some nice decoration on the plot.
0.12.4,%%
0.12.4,%% [markdown]
0.12.4,We can start by generating some data to later illustrate the principle of
0.12.4,each :class:`~imblearn.under_sampling.NearMiss` heuristic rules.
0.12.4,%%
0.12.4,%% [mardown]
0.12.4,NearMiss-1
0.12.4,----------
0.12.4,
0.12.4,NearMiss-1 selects samples from the majority class for which the average
0.12.4,distance to some nearest neighbours is the smallest. In the following
0.12.4,"example, we use a 3-NN to compute the average distance on 2 specific samples"
0.12.4,"of the majority class. Therefore, in this case the point linked by the"
0.12.4,green-dashed line will be selected since the average distance is smaller.
0.12.4,%%
0.12.4,%% [mardown]
0.12.4,NearMiss-2
0.12.4,----------
0.12.4,
0.12.4,NearMiss-2 selects samples from the majority class for which the average
0.12.4,distance to the farthest neighbors is the smallest. With the same
0.12.4,"configuration as previously presented, the sample linked to the green-dashed"
0.12.4,line will be selected since its distance the 3 farthest neighbors is the
0.12.4,smallest.
0.12.4,%%
0.12.4,%% [mardown]
0.12.4,NearMiss-3
0.12.4,----------
0.12.4,
0.12.4,"NearMiss-3 can be divided into 2 steps. First, a nearest-neighbors is used to"
0.12.4,short-list samples from the majority class (i.e. correspond to the
0.12.4,"highlighted samples in the following plot). Then, the sample with the largest"
0.12.4,average distance to the *k* nearest-neighbors are selected.
0.12.4,%%
0.12.4,select only the majority point of interest
0.12.4,Authors: Christos Aridas
0.12.4,Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.4,License: MIT
0.12.4,%%
0.12.4,%% [markdown]
0.12.4,Let's first create an imbalanced dataset and split in to two sets.
0.12.4,%%
0.12.4,%% [markdown]
0.12.4,"Now, we will create each individual steps that we would like later to combine"
0.12.4,%%
0.12.4,%% [markdown]
0.12.4,"Now, we can finally create a pipeline to specify in which order the different"
0.12.4,transformers and samplers should be executed before to provide the data to
0.12.4,the final classifier.
0.12.4,%%
0.12.4,%% [markdown]
0.12.4,We can now use the pipeline created as a normal classifier where resampling
0.12.4,"will happen when calling `fit` and disabled when calling `decision_function`,"
0.12.4,"`predict_proba`, or `predict`."
0.12.4,%%
0.12.4,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.4,License: MIT
0.12.4,##############################################################################
0.12.4,Data loading
0.12.4,##############################################################################
0.12.4,##############################################################################
0.12.4,"First, you should download the Porto Seguro data set from Kaggle. See the"
0.12.4,link in the introduction.
0.12.4,##############################################################################
0.12.4,The data set is imbalanced and it will have an effect on the fitting.
0.12.4,##############################################################################
0.12.4,Define the pre-processing pipeline
0.12.4,##############################################################################
0.12.4,##############################################################################
0.12.4,We want to standard scale the numerical features while we want to one-hot
0.12.4,"encode the categorical features. In this regard, we make use of the"
0.12.4,:class:`~sklearn.compose.ColumnTransformer`.
0.12.4,Create an environment variable to avoid using the GPU. This can be changed.
0.12.4,##############################################################################
0.12.4,Create a neural-network
0.12.4,##############################################################################
0.12.4,##############################################################################
0.12.4,We create a decorator to report the computation time
0.12.4,##############################################################################
0.12.4,The first model will be trained using the ``fit`` method and with imbalanced
0.12.4,mini-batches.
0.12.4,predict_proba was removed in tensorflow 2.6
0.12.4,##############################################################################
0.12.4,"In the contrary, we will use imbalanced-learn to create a generator of"
0.12.4,mini-batches which will yield balanced mini-batches.
0.12.4,##############################################################################
0.12.4,Classification loop
0.12.4,##############################################################################
0.12.4,##############################################################################
0.12.4,We will perform a 10-fold cross-validation and train the neural-network with
0.12.4,the two different strategies previously presented.
0.12.4,##############################################################################
0.12.4,Plot of the results and computation time
0.12.4,##############################################################################
0.12.4,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.4,License: MIT
0.12.4,%%
0.12.4,%% [markdown]
0.12.4,Problem definition
0.12.4,------------------
0.12.4,
0.12.4,We are dropping the following features:
0.12.4,
0.12.4,"- ""fnlwgt"": this feature was created while studying the ""adult"" dataset."
0.12.4,"Thus, we will not use this feature which is not acquired during the survey."
0.12.4,"- ""education-num"": it is encoding the same information than ""education""."
0.12.4,"Thus, we are removing one of these 2 features."
0.12.4,%%
0.12.4,%% [markdown]
0.12.4,"The ""adult"" dataset as a class ratio of about 3:1"
0.12.4,%%
0.12.4,%% [markdown]
0.12.4,This dataset is only slightly imbalanced. To better highlight the effect of
0.12.4,"learning from an imbalanced dataset, we will increase its ratio to 30:1"
0.12.4,%%
0.12.4,%% [markdown]
0.12.4,We will perform a cross-validation evaluation to get an estimate of the test
0.12.4,score.
0.12.4,
0.12.4,"As a baseline, we could use a classifier which will always predict the"
0.12.4,majority class independently of the features provided.
0.12.4,%%
0.12.4,%% [markdown]
0.12.4,"Instead of using the accuracy, we can use the balanced accuracy which will"
0.12.4,take into account the balancing issue.
0.12.4,%%
0.12.4,%% [markdown]
0.12.4,Strategies to learn from an imbalanced dataset
0.12.4,----------------------------------------------
0.12.4,We will use a dictionary and a list to continuously store the results of
0.12.4,our experiments and show them as a pandas dataframe.
0.12.4,%%
0.12.4,%% [markdown]
0.12.4,Dummy baseline
0.12.4,..............
0.12.4,
0.12.4,"Before to train a real machine learning model, we can store the results"
0.12.4,obtained with our :class:`~sklearn.dummy.DummyClassifier`.
0.12.4,%%
0.12.4,%% [markdown]
0.12.4,Linear classifier baseline
0.12.4,..........................
0.12.4,
0.12.4,We will create a machine learning pipeline using a
0.12.4,":class:`~sklearn.linear_model.LogisticRegression` classifier. In this regard,"
0.12.4,we will need to one-hot encode the categorical columns and standardized the
0.12.4,numerical columns before to inject the data into the
0.12.4,:class:`~sklearn.linear_model.LogisticRegression` classifier.
0.12.4,
0.12.4,"First, we define our numerical and categorical pipelines."
0.12.4,%%
0.12.4,%% [markdown]
0.12.4,"Then, we can create a preprocessor which will dispatch the categorical"
0.12.4,columns to the categorical pipeline and the numerical columns to the
0.12.4,numerical pipeline
0.12.4,%%
0.12.4,%% [markdown]
0.12.4,"Finally, we connect our preprocessor with our"
0.12.4,:class:`~sklearn.linear_model.LogisticRegression`. We can then evaluate our
0.12.4,model.
0.12.4,%%
0.12.4,%%
0.12.4,%% [markdown]
0.12.4,We can see that our linear model is learning slightly better than our dummy
0.12.4,"baseline. However, it is impacted by the class imbalance."
0.12.4,
0.12.4,We can verify that something similar is happening with a tree-based model
0.12.4,such as :class:`~sklearn.ensemble.RandomForestClassifier`. With this type of
0.12.4,"classifier, we will not need to scale the numerical data, and we will only"
0.12.4,need to ordinal encode the categorical data.
0.12.4,%%
0.12.4,%%
0.12.4,%% [markdown]
0.12.4,The :class:`~sklearn.ensemble.RandomForestClassifier` is as well affected by
0.12.4,"the class imbalanced, slightly less than the linear model. Now, we will"
0.12.4,present different approach to improve the performance of these 2 models.
0.12.4,
0.12.4,Use `class_weight`
0.12.4,..................
0.12.4,
0.12.4,Most of the models in `scikit-learn` have a parameter `class_weight`. This
0.12.4,parameter will affect the computation of the loss in linear model or the
0.12.4,criterion in the tree-based model to penalize differently a false
0.12.4,classification from the minority and majority class. We can set
0.12.4,"`class_weight=""balanced""` such that the weight applied is inversely"
0.12.4,proportional to the class frequency. We test this parametrization in both
0.12.4,linear model and tree-based model.
0.12.4,%%
0.12.4,%%
0.12.4,%% [markdown]
0.12.4,We can see that using `class_weight` was really effective for the linear
0.12.4,"model, alleviating the issue of learning from imbalanced classes. However,"
0.12.4,the :class:`~sklearn.ensemble.RandomForestClassifier` is still biased toward
0.12.4,"the majority class, mainly due to the criterion which is not suited enough to"
0.12.4,fight the class imbalance.
0.12.4,
0.12.4,Resample the training set during learning
0.12.4,.........................................
0.12.4,
0.12.4,Another way is to resample the training set by under-sampling or
0.12.4,over-sampling some of the samples. `imbalanced-learn` provides some samplers
0.12.4,to do such processing.
0.12.4,%%
0.12.4,%%
0.12.4,%%
0.12.4,%%
0.12.4,%% [markdown]
0.12.4,Applying a random under-sampler before the training of the linear model or
0.12.4,"random forest, allows to not focus on the majority class at the cost of"
0.12.4,making more mistake for samples in the majority class (i.e. decreased
0.12.4,accuracy).
0.12.4,
0.12.4,We could apply any type of samplers and find which sampler is working best
0.12.4,on the current dataset.
0.12.4,
0.12.4,"Instead, we will present another way by using classifiers which will apply"
0.12.4,sampling internally.
0.12.4,
0.12.4,Use of specific balanced algorithms from imbalanced-learn
0.12.4,.........................................................
0.12.4,
0.12.4,We already showed that random under-sampling can be effective on decision
0.12.4,"tree. However, instead of under-sampling once the dataset, one could"
0.12.4,under-sample the original dataset before to take a bootstrap sample. This is
0.12.4,the base of the :class:`imblearn.ensemble.BalancedRandomForestClassifier` and
0.12.4,:class:`~imblearn.ensemble.BalancedBaggingClassifier`.
0.12.4,%%
0.12.4,%%
0.12.4,%% [markdown]
0.12.4,The performance with the
0.12.4,:class:`~imblearn.ensemble.BalancedRandomForestClassifier` is better than
0.12.4,applying a single random under-sampling. We will use a gradient-boosting
0.12.4,classifier within a :class:`~imblearn.ensemble.BalancedBaggingClassifier`.
0.12.4,%% [markdown]
0.12.4,This last approach is the most effective. The different under-sampling allows
0.12.4,to bring some diversity for the different GBDT to learn and not focus on a
0.12.4,portion of the majority class.
0.12.4,Authors: Christos Aridas
0.12.4,Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.4,License: MIT
0.12.4,%%
0.12.4,%% [markdown]
0.12.4,Load the dataset
0.12.4,----------------
0.12.4,
0.12.4,We will use a dataset containing image from know person where we will
0.12.4,build a model to recognize the person on the image. We will make this problem
0.12.4,a binary problem by taking picture of only George W. Bush and Bill Clinton.
0.12.4,%%
0.12.4,%%
0.12.4,%% [markdown]
0.12.4,We can check the ratio between the two classes.
0.12.4,%%
0.12.4,%% [markdown]
0.12.4,We see that we have an imbalanced classification problem with ~95% of the
0.12.4,data belonging to the class G.W. Bush.
0.12.4,
0.12.4,Compare over-sampling approaches
0.12.4,--------------------------------
0.12.4,
0.12.4,We will use different over-sampling approaches and use a kNN classifier
0.12.4,to check if we can recognize the 2 presidents. The evaluation will be
0.12.4,performed through cross-validation and we will plot the mean ROC curve.
0.12.4,
0.12.4,We will create different pipelines and evaluate them.
0.12.4,%%
0.12.4,%% [markdown]
0.12.4,We will compute the mean ROC curve for each pipeline using a different splits
0.12.4,provided by the :class:`~sklearn.model_selection.StratifiedKFold`
0.12.4,cross-validation.
0.12.4,%%
0.12.4,compute the mean fpr/tpr to get the mean ROC curve
0.12.4,Create a display that we will reuse to make the aggregated plots for
0.12.4,all methods
0.12.4,%% [markdown]
0.12.4,"In the previous cell, we created the different mean ROC curve and we can plot"
0.12.4,them on the same plot.
0.12.4,%%
0.12.4,%% [markdown]
0.12.4,"We see that for this task, methods that are generating new samples with some"
0.12.4,interpolation (i.e. ADASYN and SMOTE) perform better than random
0.12.4,over-sampling or no resampling.
0.12.4,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.4,License: MIT
0.12.4,Create a folder to fetch the dataset
0.12.4,Create a pipeline
0.12.4,Classify and report the results
0.12.4,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.4,License: MIT
0.12.4,%%
0.12.4,%% [markdown]
0.12.4,Setting the data set
0.12.4,--------------------
0.12.4,
0.12.4,We use a part of the 20 newsgroups data set by loading 4 topics. Using the
0.12.4,"scikit-learn loader, the data are split into a training and a testing set."
0.12.4,
0.12.4,Note the class \#3 is the minority class and has almost twice less samples
0.12.4,than the majority class.
0.12.4,%%
0.12.4,%%
0.12.4,%% [markdown]
0.12.4,The usual scikit-learn pipeline
0.12.4,-------------------------------
0.12.4,
0.12.4,You might usually use scikit-learn pipeline by combining the TF-IDF
0.12.4,vectorizer to feed a multinomial naive bayes classifier. A classification
0.12.4,report summarized the results on the testing set.
0.12.4,
0.12.4,"As expected, the recall of the class \#3 is low mainly due to the class"
0.12.4,imbalanced.
0.12.4,%%
0.12.4,%%
0.12.4,%% [markdown]
0.12.4,Balancing the class before classification
0.12.4,-----------------------------------------
0.12.4,
0.12.4,"To improve the prediction of the class \#3, it could be interesting to apply"
0.12.4,"a balancing before to train the naive bayes classifier. Therefore, we will"
0.12.4,use a :class:`~imblearn.under_sampling.RandomUnderSampler` to equalize the
0.12.4,number of samples in all the classes before the training.
0.12.4,
0.12.4,It is also important to note that we are using the
0.12.4,:class:`~imblearn.pipeline.make_pipeline` function implemented in
0.12.4,imbalanced-learn to properly handle the samplers.
0.12.4,%%
0.12.4,%% [markdown]
0.12.4,"Although the results are almost identical, it can be seen that the resampling"
0.12.4,allowed to correct the poor recall of the class \#3 at the cost of reducing
0.12.4,"the other metrics for the other classes. However, the overall results are"
0.12.4,slightly better.
0.12.4,%%
0.12.4,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.4,License: MIT
0.12.4,#############################################################################
0.12.4,Toy data generation
0.12.4,#############################################################################
0.12.4,#############################################################################
0.12.4,We are generating some non Gaussian data set contaminated with some unform
0.12.4,noise.
0.12.4,#############################################################################
0.12.4,We will generate some cleaned test data without outliers.
0.12.4,#############################################################################
0.12.4,How to use the :class:`~imblearn.FunctionSampler`
0.12.4,#############################################################################
0.12.4,#############################################################################
0.12.4,We first define a function which will use
0.12.4,:class:`~sklearn.ensemble.IsolationForest` to eliminate some outliers from
0.12.4,our dataset during training. The function passed to the
0.12.4,:class:`~imblearn.FunctionSampler` will be called when using the method
0.12.4,``fit_resample``.
0.12.4,#############################################################################
0.12.4,Integrate it within a pipeline
0.12.4,#############################################################################
0.12.4,#############################################################################
0.12.4,"By elimnating outliers before the training, the classifier will be less"
0.12.4,affected during the prediction.
0.12.4,Authors: Dayvid Oliveira
0.12.4,Christos Aridas
0.12.4,Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.4,License: MIT
0.12.4,%%
0.12.4,%% [markdown]
0.12.4,Generate the dataset
0.12.4,--------------------
0.12.4,
0.12.4,"First, we will generate a dataset and convert it to a"
0.12.4,:class:`~pandas.DataFrame` with arbitrary column names. We will plot the
0.12.4,original dataset.
0.12.4,%%
0.12.4,%% [markdown]
0.12.4,Make a dataset imbalanced
0.12.4,-------------------------
0.12.4,
0.12.4,"Now, we will show the helpers :func:`~imblearn.datasets.make_imbalance`"
0.12.4,that is useful to random select a subset of samples. It will impact the
0.12.4,class distribution as specified by the parameters.
0.12.4,%%
0.12.4,%%
0.12.4,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.4,License: MIT
0.12.4,%%
0.12.4,%% [markdown]
0.12.4,Create an imbalanced dataset
0.12.4,----------------------------
0.12.4,
0.12.4,"First, we will create an imbalanced data set from a the iris data set."
0.12.4,%%
0.12.4,%%
0.12.4,%% [markdown]
0.12.4,Using ``sampling_strategy`` in resampling algorithms
0.12.4,====================================================
0.12.4,
0.12.4,`sampling_strategy` as a `float`
0.12.4,--------------------------------
0.12.4,
0.12.4,`sampling_strategy` can be given a `float`. For **under-sampling
0.12.4,"methods**, it corresponds to the ratio :math:`\alpha_{us}` defined by"
0.12.4,:math:`N_{rM} = \alpha_{us} \times N_{m}` where :math:`N_{rM}` and
0.12.4,:math:`N_{m}` are the number of samples in the majority class after
0.12.4,"resampling and the number of samples in the minority class, respectively."
0.12.4,%%
0.12.4,select only 2 classes since the ratio make sense in this case
0.12.4,%%
0.12.4,%% [markdown]
0.12.4,"For **over-sampling methods**, it correspond to the ratio"
0.12.4,:math:`\alpha_{os}` defined by :math:`N_{rm} = \alpha_{os} \times N_{M}`
0.12.4,where :math:`N_{rm}` and :math:`N_{M}` are the number of samples in the
0.12.4,minority class after resampling and the number of samples in the majority
0.12.4,"class, respectively."
0.12.4,%%
0.12.4,%% [markdown]
0.12.4,`sampling_strategy` as a `str`
0.12.4,-------------------------------
0.12.4,
0.12.4,`sampling_strategy` can be given as a string which specify the class
0.12.4,"targeted by the resampling. With under- and over-sampling, the number of"
0.12.4,samples will be equalized.
0.12.4,
0.12.4,Note that we are using multiple classes from now on.
0.12.4,%%
0.12.4,%% [markdown]
0.12.4,"With **cleaning method**, the number of samples in each class will not be"
0.12.4,equalized even if targeted.
0.12.4,%%
0.12.4,%% [markdown]
0.12.4,`sampling_strategy` as a `dict`
0.12.4,-------------------------------
0.12.4,
0.12.4,"When `sampling_strategy` is a `dict`, the keys correspond to the targeted"
0.12.4,classes. The values correspond to the desired number of samples for each
0.12.4,targeted class. This is working for both **under- and over-sampling**
0.12.4,algorithms but not for the **cleaning algorithms**. Use a `list` instead.
0.12.4,%%
0.12.4,%% [markdown]
0.12.4,`sampling_strategy` as a `list`
0.12.4,-------------------------------
0.12.4,
0.12.4,"When `sampling_strategy` is a `list`, the list contains the targeted"
0.12.4,classes. It is used only for **cleaning methods** and raise an error
0.12.4,otherwise.
0.12.4,%%
0.12.4,%% [markdown]
0.12.4,`sampling_strategy` as a callable
0.12.4,---------------------------------
0.12.4,
0.12.4,"When callable, function taking `y` and returns a `dict`. The keys"
0.12.4,correspond to the targeted classes. The values correspond to the desired
0.12.4,number of samples for each class.
0.12.4,%%
0.12.4,List of whitelisted modules and methods; regexp are supported.
0.12.4,These docstrings will fail because they are inheriting from scikit-learn
0.12.4,skip private classes
0.12.4,"We ignore following error code,"
0.12.4,- RT02: The first line of the Returns section
0.12.4,"should contain only the type, .."
0.12.4,(as we may need refer to the name of the returned
0.12.4,object)
0.12.4,- GL01: Docstring text (summary) should start in the line
0.12.4,"immediately after the opening quotes (not in the same line,"
0.12.4,or leaving a blank line in between)
0.12.4,"- GL02: If there's a blank line, it should be before the"
0.12.4,"first line of the Returns section, not after (it allows to have"
0.12.4,short docstrings for properties).
0.12.4,Ignore PR02: Unknown parameters for properties. We sometimes use
0.12.4,"properties for ducktyping, i.e. SGDClassifier.predict_proba"
0.12.4,Following codes are only taken into account for the
0.12.4,top level class docstrings:
0.12.4,- ES01: No extended summary found
0.12.4,- SA01: See Also section not found
0.12.4,- EX01: No examples section found
0.12.4,In particular we can't parse the signature of properties
0.12.4,"When applied to classes, detect class method. For functions"
0.12.4,method = None.
0.12.4,TODO: this detection can be improved. Currently we assume that we have
0.12.4,class # methods if the second path element before last is in camel case.
0.12.4,'build' and 'install' is included to have structured metadata for CI.
0.12.4,It will NOT be included in setup's extras_require
0.12.4,"The values are (version_spec, comma separated tags)"
0.12.4,create inverse mapping for setuptools
0.12.4,Used by CI to get the min dependencies
0.12.4,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.4,License: MIT
0.12.4,TODO: remove this file when scikit-learn minimum version is 1.3
0.12.4,Return a copy of the threadlocal configuration so that users will
0.12.4,not be able to modify the configuration with the returned dict.
0.12.4,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.4,Christos Aridas
0.12.4,License: MIT
0.12.4,scikit-learn >= 1.2
0.12.4,we need to overwrite SamplerMixin.fit to bypass the validation
0.12.4,Adapted from scikit-learn
0.12.4,Author: Edouard Duchesnay
0.12.4,Gael Varoquaux
0.12.4,Virgile Fritsch
0.12.4,Alexandre Gramfort
0.12.4,Lars Buitinck
0.12.4,Christos Aridas
0.12.4,Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.4,License: BSD
0.12.4,BaseEstimator interface
0.12.4,validate names
0.12.4,validate estimators
0.12.4,We allow last estimator to be None as an identity transformation
0.12.4,Estimator interface
0.12.4,"def _fit(self, X, y=None, **fit_params_steps):"
0.12.4,Setup the memory
0.12.4,we do not clone when caching is disabled to
0.12.4,preserve backward compatibility
0.12.4,Fit or load from cache the current transformer
0.12.4,Replace the transformer of the step with the fitted
0.12.4,transformer. This is necessary when loading the transformer
0.12.4,from the cache.
0.12.4,The `fit_*` methods need to be overridden to support the samplers.
0.12.4,estimators in Pipeline.steps are not validated yet
0.12.4,estimators in Pipeline.steps are not validated yet
0.12.4,metadata routing enabled
0.12.4,estimators in Pipeline.steps are not validated yet
0.12.4,estimators in Pipeline.steps are not validated yet
0.12.4,TODO: remove the following methods when the minimum scikit-learn >= 1.4
0.12.4,They do not depend on resampling but we need to redefine them for the
0.12.4,compatibility with the metadata routing framework.
0.12.4,metadata routing enabled
0.12.4,not branching here since params is only available if
0.12.4,enable_metadata_routing=True
0.12.4,metadata routing enabled
0.12.4,not branching here since params is only available if
0.12.4,enable_metadata_routing=True
0.12.4,"we don't have to branch here, since params is only non-empty if"
0.12.4,enable_metadata_routing=True.
0.12.4,metadata routing is enabled.
0.12.4,"TODO: once scikit-learn >= 1.4, the following function should be simplified by"
0.12.4,calling `super().get_metadata_routing()`
0.12.4,first we add all steps except the last one
0.12.4,"fit, fit_predict, and fit_transform call fit_transform if it"
0.12.4,"exists, or else fit and transform"
0.12.4,then we add the last step
0.12.4,"without metadata routing, fit_transform and fit_predict"
0.12.4,get all the same params and pass it to the last fit.
0.12.4,"if we have a weight for this transformer, multiply output"
0.12.4,This variable is injected in the __builtins__ by the build
0.12.4,process. It is used to enable importing subpackages of sklearn when
0.12.4,the binaries are not built
0.12.4,mypy error: Cannot determine type of '__SKLEARN_SETUP__'
0.12.4,We are not importing the rest of scikit-learn during the build
0.12.4,"process, as it may not be compiled yet"
0.12.4,"FIXME: When we get Python 3.7 as minimal version, we will need to switch to"
0.12.4,the following solution:
0.12.4,https://snarky.ca/lazy-importing-in-python-3-7/
0.12.4,Import the target module and insert it into the parent's namespace
0.12.4,Update this object's dict so that if someone keeps a reference to the
0.12.4,"LazyLoader, lookups are efficient (__getattr__ is only called on"
0.12.4,lookups that fail).
0.12.4,delay the import of keras since we are going to import either tensorflow
0.12.4,or keras
0.12.4,Based on NiLearn package
0.12.4,License: simplified BSD
0.12.4,"PEP0440 compatible formatted version, see:"
0.12.4,https://www.python.org/dev/peps/pep-0440/
0.12.4,
0.12.4,Generic release markers:
0.12.4,X.Y
0.12.4,X.Y.Z # For bugfix releases
0.12.4,
0.12.4,Admissible pre-release markers:
0.12.4,X.YaN # Alpha release
0.12.4,X.YbN # Beta release
0.12.4,X.YrcN # Release Candidate
0.12.4,X.Y # Final release
0.12.4,
0.12.4,Dev branch marker is: 'X.Y.dev' or 'X.Y.devN' where N is an integer.
0.12.4,'X.Y.dev0' is the canonical version of 'X.Y.dev'
0.12.4,
0.12.4,coding: utf-8
0.12.4,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.4,Dariusz Brzezinski
0.12.4,License: MIT
0.12.4,Only negative labels
0.12.4,"Calculate tp_sum, pred_sum, true_sum ###"
0.12.4,labels are now from 0 to len(labels) - 1 -> use bincount
0.12.4,Pathological case
0.12.4,Compute the true negative
0.12.4,Retain only selected labels
0.12.4,"Finally, we have all our sufficient statistics. Divide! #"
0.12.4,"Divide, and on zero-division, set scores to 0 and warn:"
0.12.4,"Oddly, we may get an ""invalid"" rather than a ""divide"" error"
0.12.4,here.
0.12.4,Average the results
0.12.4,labels are now from 0 to len(labels) - 1 -> use bincount
0.12.4,Pathological case
0.12.4,Retain only selected labels
0.12.4,old version of scipy return MaskedConstant instead of 0.0
0.12.4,check that the scoring function does not need a score
0.12.4,and only a prediction
0.12.4,We do not support multilabel so the only average supported
0.12.4,is binary
0.12.4,Compute the different metrics
0.12.4,Precision/recall/f1
0.12.4,Specificity
0.12.4,Geometric mean
0.12.4,Index balanced accuracy
0.12.4,compute averages
0.12.4,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.4,License: MIT
0.12.4,categories are expected to be encoded from 0 to n_categories - 1
0.12.4,"list of length n_features of ndarray (n_categories, n_classes)"
0.12.4,compute the counts
0.12.4,normalize by the summing over the classes
0.12.4,silence potential warning due to in-place division by zero
0.12.4,coding: utf-8
0.12.4,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.4,Christos Aridas
0.12.4,License: MIT
0.12.4,##############################################################################
0.12.4,Utilities for testing
0.12.4,import some data to play with
0.12.4,restrict to a binary classification task
0.12.4,add noisy features to make the problem harder and avoid perfect results
0.12.4,"run classifier, get class probabilities and label predictions"
0.12.4,only interested in probabilities of the positive case
0.12.4,XXX: do we really want a special API for the binary case?
0.12.4,##############################################################################
0.12.4,Tests
0.12.4,detailed measures for each class
0.12.4,individual scoring function that can be used for grid search: in the
0.12.4,binary class case the score is the value of the measure for the positive
0.12.4,class (e.g. label == 1). This is deprecated for average != 'binary'.
0.12.4,Such a case may occur with non-stratified cross-validation
0.12.4,ensure the above were meaningful tests:
0.12.4,Bad pos_label
0.12.4,Bad average option
0.12.4,but average != 'binary'; even if data is binary
0.12.4,compute the geometric mean for the binary problem
0.12.4,print classification report with class names
0.12.4,print classification report with label detection
0.12.4,print classification report with class names
0.12.4,print classification report with label detection
0.12.4,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.4,License: MIT
0.12.4,Check basic feature of the metric:
0.12.4,"* the shape of the distance matrix is (n_samples, n_samples)"
0.12.4,* computing pairwise distance of X is the same than explicitely between
0.12.4,X and X.
0.12.4,Check the property of the vdm distance. Let's check the property
0.12.4,"described in ""Improved Heterogeneous Distance Functions"", D.R. Wilson and"
0.12.4,"T.R. Martinez, Journal of Artificial Intelligence Research 6 (1997) 1-34"
0.12.4,https://arxiv.org/pdf/cs/9701101.pdf
0.12.4,
0.12.4,"""if an attribute color has three values red, green and blue, and the"
0.12.4,"application is to identify whether or not an object is an apple, red and"
0.12.4,green would be considered closer than red and blue because the former two
0.12.4,"both have similar correlations with the output class apple."""
0.12.4,defined our feature
0.12.4,0 - not an apple / 1 - an apple
0.12.4,computing the distance between a sample of the same category should
0.12.4,give a null distance
0.12.4,check the property explained in the introduction example
0.12.4,green and red are very close
0.12.4,blue is closer to red than green
0.12.4,"Check that ""auto"" is equivalent to provide the number categories"
0.12.4,beforehand
0.12.4,Check that we raise an error if n_categories is inconsistent with the
0.12.4,number of features in X
0.12.4,Check that we don't get issue when a category is missing between 0
0.12.4,n_categories - 1
0.12.4,remove a categories that could be between 0 and n_categories
0.12.4,Check that we raise a NotFittedError when `fit` is not not called before
0.12.4,pairwise.
0.12.4,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.4,Christos Aridas
0.12.4,License: MIT
0.12.4,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.4,Christos Aridas
0.12.4,License: MIT
0.12.4,FIXME: to be removed in 0.12
0.12.4,The ratio is computed using a one-vs-rest manner. Using majority
0.12.4,in multi-class would lead to slightly different results at the
0.12.4,cost of introducing a new parameter.
0.12.4,rounding may cause new amount for n_samples
0.12.4,the nearest neighbors need to be fitted only on the current class
0.12.4,to find the class NN to generate new samples
0.12.4,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.4,Christos Aridas
0.12.4,License: MIT
0.12.4,smoothed bootstrap imposes to make numerical operation; we need
0.12.4,to be sure to have only numerical data in X
0.12.4,generate a smoothed bootstrap with a perturbation
0.12.4,generate a bootstrap
0.12.4,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.4,Christos Aridas
0.12.4,License: MIT
0.12.4,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.4,Fernando Nogueira
0.12.4,Christos Aridas
0.12.4,License: MIT
0.12.4,negate diagonal elements
0.12.4,identify cluster which are answering the requirements
0.12.4,empty cluster
0.12.4,the cluster is already considered balanced
0.12.4,not enough samples to apply SMOTE
0.12.4,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.4,Fernando Nogueira
0.12.4,Christos Aridas
0.12.4,Dzianis Dudnik
0.12.4,License: MIT
0.12.4,FIXME: to be removed in 0.12
0.12.4,FIXME: to be removed in 0.12
0.12.4,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.4,Fernando Nogueira
0.12.4,Christos Aridas
0.12.4,Dzianis Dudnik
0.12.4,License: MIT
0.12.4,np.newaxis for backwards compatability with random_state
0.12.4,Samples are in danger for m/2 <= m' < m
0.12.4,Samples are noise for m = m'
0.12.4,FIXME: to be removed in 0.12
0.12.4,FIXME: to be removed in 0.12
0.12.4,the input of the OneHotEncoder needs to be dense
0.12.4,SMOTE resampling starts here
0.12.4,"In the edge case where the median of the std is equal to 0, the 1s"
0.12.4,"entries will be also nullified. In this case, we store the original"
0.12.4,categorical encoding which will be later used for inverting the OHE
0.12.4,This variable will be used when generating data
0.12.4,we can replace the 1 entries of the categorical features with the
0.12.4,median of the standard deviation. It will ensure that whenever
0.12.4,"distance is computed between 2 samples, the difference will be equal"
0.12.4,to the median of the standard deviation as in the original paper.
0.12.4,"With one-hot encoding, the median will be repeated twice. We need"
0.12.4,to divide by sqrt(2) such that we only have one median value
0.12.4,contributing to the Euclidean distance
0.12.4,SMOTE resampling ends here
0.12.4,reverse the encoding of the categorical features
0.12.4,the matrix is supposed to be in the CSR format after the stacking
0.12.4,change in sparsity structure more efficient with LIL than CSR
0.12.4,convert to dense array since scipy.sparse doesn't handle 3D
0.12.4,"In the case that the median std was equal to zeros, we have to"
0.12.4,create non-null entry based on the encoded of OHE
0.12.4,tie breaking argmax
0.12.4,generate sample indices that will be used to generate new samples
0.12.4,"for each drawn samples, select its k-neighbors and generate a sample"
0.12.4,"where for each feature individually, each category generated is the"
0.12.4,most common category
0.12.4,FIXME: to be removed in 0.12
0.12.4,the kneigbors search will include the sample itself which is
0.12.4,expected from the original algorithm
0.12.4,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.4,Christos Aridas
0.12.4,Dzianis Dudnik
0.12.4,License: MIT
0.12.4,create 2 random continuous feature
0.12.4,create a categorical feature using some string
0.12.4,create a categorical feature using some integer
0.12.4,return the categories
0.12.4,create 2 random continuous feature
0.12.4,create a categorical feature using some string
0.12.4,create a categorical feature using some integer
0.12.4,return the categories
0.12.4,create 2 random continuous feature
0.12.4,create a categorical feature using some string
0.12.4,create a categorical feature using some integer
0.12.4,return the categories
0.12.4,create 2 random continuous feature
0.12.4,create a categorical feature using some string
0.12.4,create a categorical feature using some integer
0.12.4,return the categories
0.12.4,create 2 random continuous feature
0.12.4,create a categorical feature using some string
0.12.4,create a categorical feature using some integer
0.12.4,part of the common test which apply to SMOTE-NC even if it is not default
0.12.4,constructible
0.12.4,Check that the samplers handle pandas dataframe and pandas series
0.12.4,Cast X and y to not default dtype
0.12.4,Non-regression test for #662
0.12.4,https://github.com/scikit-learn-contrib/imbalanced-learn/issues/662
0.12.4,check that the categorical feature is not random but correspond to the
0.12.4,categories seen in the minority class samples
0.12.4,TODO: only use `sparse_output` when sklearn >= 1.2
0.12.4,TODO(0.13): remove this test
0.12.4,overall check for SMOTEN
0.12.4,check if the SMOTEN resample data as expected
0.12.4,"we generate data such that ""not apple"" will be the minority class and"
0.12.4,"samples from this class will be generated. We will force the ""blue"""
0.12.4,"category to be associated with this class. Therefore, the new generated"
0.12.4,"samples should as well be from the ""blue"" category."
0.12.4,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.4,Christos Aridas
0.12.4,License: MIT
0.12.4,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.4,Christos Aridas
0.12.4,License: MIT
0.12.4,FIXME: we should use to_numpy with pandas >= 0.25
0.12.4,check the random over-sampling with a multiclass problem
0.12.4,check that resampling with heterogeneous dtype is working with basic
0.12.4,resampling
0.12.4,check that we can oversample even with missing or infinite data
0.12.4,regression tests for #605
0.12.4,check that we raise an error when heterogeneous dtype data are given
0.12.4,and a smoothed bootstrap is requested
0.12.4,check that smoothed bootstrap is working for numerical array
0.12.4,check that a shrinkage factor of 0 is equivalent to not create a smoothed
0.12.4,bootstrap
0.12.4,check the behaviour of the shrinkage parameter
0.12.4,the covariance of the data generated with the larger shrinkage factor
0.12.4,should also be larger.
0.12.4,check the validation of the shrinkage parameter
0.12.4,check that m_neighbors is properly set. Regression test for:
0.12.4,https://github.com/scikit-learn-contrib/imbalanced-learn/issues/568
0.12.4,FIXME: to be removed in 0.12
0.12.4,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.4,Christos Aridas
0.12.4,License: MIT
0.12.4,shuffle the indices since the sampler are packing them by class
0.12.4,helper functions
0.12.4,input and output
0.12.4,build the model and weights
0.12.4,"build the loss, predict, and train operator"
0.12.4,Initialization of all variables in the graph
0.12.4,"For each epoch, run accuracy on train and test"
0.12.4,helper functions
0.12.4,input and output
0.12.4,build the model and weights
0.12.4,"build the loss, predict, and train operator"
0.12.4,Initialization of all variables in the graph
0.12.4,"For each epoch, run accuracy on train and test"
0.12.4,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.4,License: MIT
0.12.4,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.4,Fernando Nogueira
0.12.4,Christos Aridas
0.12.4,License: MIT
0.12.4,find which class to not consider
0.12.4,there is a Tomek link between two samples if they are both nearest
0.12.4,neighbors of each others.
0.12.4,Find the nearest neighbour of every point
0.12.4,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.4,Christos Aridas
0.12.4,License: MIT
0.12.4,Randomly get one sample from the majority class
0.12.4,Generate the index to select
0.12.4,Create the set C - One majority samples and all minority
0.12.4,Create the set S - all majority samples
0.12.4,fit knn on C
0.12.4,Check each sample in S if we keep it or drop it
0.12.4,Do not select sample which are already well classified
0.12.4,Classify on S
0.12.4,If the prediction do not agree with the true label
0.12.4,append it in C_x
0.12.4,Keep the index for later
0.12.4,Update C
0.12.4,fit a knn on C
0.12.4,This experimental to speed up the search
0.12.4,Classify all the element in S and avoid to test the
0.12.4,well classified elements
0.12.4,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.4,Dayvid Oliveira
0.12.4,Christos Aridas
0.12.4,License: MIT
0.12.4,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.4,Christos Aridas
0.12.4,License: MIT
0.12.4,Compute the distance considering the farthest neighbour
0.12.4,Sort the list of distance and get the index
0.12.4,Throw a warning to tell the user that we did not have enough samples
0.12.4,to select and that we just select everything
0.12.4,Select the desired number of samples
0.12.4,idx_tmp is relative to the feature selected in the
0.12.4,previous step and we need to find the indirection
0.12.4,fmt: off
0.12.4,fmt: on
0.12.4,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.4,Christos Aridas
0.12.4,License: MIT
0.12.4,select a sample from the current class
0.12.4,create the set composed of all minority samples and one
0.12.4,sample from the current class.
0.12.4,create the set S with removing the seed from S
0.12.4,since that it will be added anyway
0.12.4,apply Tomek cleaning
0.12.4,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.4,Dayvid Oliveira
0.12.4,Christos Aridas
0.12.4,License: MIT
0.12.4,Check the stopping criterion
0.12.4,1. If there is no changes for the vector y
0.12.4,2. If the number of samples in the other class become inferior to
0.12.4,the number of samples in the majority class
0.12.4,3. If one of the class is disappearing
0.12.4,Case 1
0.12.4,Case 2
0.12.4,Case 3
0.12.4,Check the stopping criterion
0.12.4,1. If the number of samples in the other class become inferior to
0.12.4,the number of samples in the majority class
0.12.4,2. If one of the class is disappearing
0.12.4,Case 1else:
0.12.4,overwrite b_min_bec_maj
0.12.4,Case 2
0.12.4,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.4,Christos Aridas
0.12.4,License: MIT
0.12.4,backward compatibility when passing a NearestNeighbors object
0.12.4,clean the neighborhood
0.12.4,compute which classes to consider for cleaning for the A2 group
0.12.4,add an additional sample since the query points contains the original dataset
0.12.4,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.4,Christos Aridas
0.12.4,License: MIT
0.12.4,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.4,Christos Aridas
0.12.4,License: MIT
0.12.4,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.4,Christos Aridas
0.12.4,License: MIT
0.12.4,"with a large `threshold_cleaning`, the algorithm is equivalent to ENN"
0.12.4,set a threshold that we should consider only the class #2
0.12.4,making the threshold slightly smaller to take into account class #1
0.12.4,we should have a more aggressive cleaning with n_neighbors is larger
0.12.4,TODO: remove in 0.14
0.12.4,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.4,Christos Aridas
0.12.4,License: MIT
0.12.4,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.4,Christos Aridas
0.12.4,License: MIT
0.12.4,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.4,Christos Aridas
0.12.4,License: MIT
0.12.4,FIXME: we should use to_numpy with pandas >= 0.25
0.12.4,check that we can undersample even with missing or infinite data
0.12.4,regression tests for #605
0.12.4,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.4,Christos Aridas
0.12.4,License: MIT
0.12.4,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.4,Christos Aridas
0.12.4,License: MIT
0.12.4,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.4,Christos Aridas
0.12.4,License: MIT
0.12.4,TODO: remove in 0.14
0.12.4,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.4,Christos Aridas
0.12.4,License: MIT
0.12.4,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.4,Christos Aridas
0.12.4,License: MIT
0.12.4,TODO: remove in 0.14
0.12.4,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.4,Fernando Nogueira
0.12.4,Christos Aridas
0.12.4,License: MIT
0.12.4,check that the samples selecting by the hard voting corresponds to the
0.12.4,targeted class
0.12.4,non-regression test for:
0.12.4,https://github.com/scikit-learn-contrib/imbalanced-learn/issues/738
0.12.4,Generate valid values for the required parameters
0.12.4,The parameters `*args` and `**kwargs` are ignored since we cannot generate
0.12.4,constraints.
0.12.4,check that there is a constraint for each parameter
0.12.4,this object does not have a valid type for sure for all params
0.12.4,This parameter is not validated
0.12.4,"First, check that the error is raised if param doesn't match any valid type."
0.12.4,"Then, for constraints that are more than a type constraint, check that the"
0.12.4,error is raised if param does match a valid type but does not match any valid
0.12.4,value for this type.
0.12.4,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.4,Christos Aridas
0.12.4,License: MIT
0.12.4,test that all_estimators doesn't find abstract classes.
0.12.4,"For NearMiss, let's check the three algorithms"
0.12.4,Common tests for estimator instances
0.12.4,Authors: Alexandre Gramfort <alexandre.gramfort@inria.fr>
0.12.4,Raghav RV <rvraghav93@gmail.com>
0.12.4,License: BSD 3 clause
0.12.4,scikit-learn >= 1.2
0.12.4,"walk_packages() ignores DeprecationWarnings, now we need to ignore"
0.12.4,FutureWarnings
0.12.4,"mypy error: Module has no attribute ""__path__"""
0.12.4,functions to ignore args / docstring of
0.12.4,Methods where y param should be ignored if y=None by default
0.12.4,numpydoc 0.8.0's docscrape tool raises because of collections.abc under
0.12.4,Python 3.7
0.12.4,Test module docstring formatting
0.12.4,Skip test if numpydoc is not found
0.12.4,XXX unreached code as of v0.22
0.12.4,"pytest tooling, not part of the scikit-learn API"
0.12.4,Exclude non-scikit-learn classes
0.12.4,Now skip docstring test for y when y is None
0.12.4,by default for API reason
0.12.4,Exclude imported functions
0.12.4,Don't test private methods / functions
0.12.4,Test that there are no tabs in our source files
0.12.4,because we don't import
0.12.4,Minimal / degenerate instances: only useful to test the docstrings.
0.12.4,"As certain attributes are present ""only"" if a certain parameter is"
0.12.4,"provided, this checks if the word ""only"" is present in the attribute"
0.12.4,"description, and if not the attribute is required to be present."
0.12.4,ignore deprecation warnings
0.12.4,attributes
0.12.4,properties
0.12.4,ignore properties that raises an AttributeError and deprecated
0.12.4,properties
0.12.4,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.4,License: MIT
0.12.4,check that we can let a pass a regression variable by turning down the
0.12.4,validation
0.12.4,Check that the validation is bypass when calling `fit`
0.12.4,Non-regression test for:
0.12.4,https://github.com/scikit-learn-contrib/imbalanced-learn/issues/782
0.12.4,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.4,Christos Aridas
0.12.4,License: MIT
0.12.4,store timestamp to figure out whether the result of 'fit' has been
0.12.4,cached or not
0.12.4,store timestamp to figure out whether the result of 'fit' has been
0.12.4,cached or not
0.12.4,Pipeline accepts steps as tuple
0.12.4,Test the various init parameters of the pipeline.
0.12.4,Check that we can't instantiate pipelines with objects without fit
0.12.4,method
0.12.4,Smoke test with only an estimator
0.12.4,Check that params are set
0.12.4,Smoke test the repr:
0.12.4,Test with two objects
0.12.4,Check that we can't instantiate with non-transformers on the way
0.12.4,"Note that NoTrans implements fit, but not transform"
0.12.4,Check that params are set
0.12.4,Smoke test the repr:
0.12.4,Check that params are not set when naming them wrong
0.12.4,Test clone
0.12.4,"Check that apart from estimators, the parameters are the same"
0.12.4,Remove estimators that where copied
0.12.4,Test the various methods of the pipeline (anova).
0.12.4,Test with Anova + LogisticRegression
0.12.4,Test that the pipeline can take fit parameters
0.12.4,classifier should return True
0.12.4,and transformer params should not be changed
0.12.4,invalid parameters should raise an error message
0.12.4,Pipeline should pass sample_weight
0.12.4,When sample_weight is None it shouldn't be passed
0.12.4,Test pipeline raises set params error message for nested models.
0.12.4,nested model check
0.12.4,Test the various methods of the pipeline (pca + svm).
0.12.4,Test with PCA + SVC
0.12.4,Test the various methods of the pipeline (preprocessing + svm).
0.12.4,check shapes of various prediction functions
0.12.4,test that the fit_predict method is implemented on a pipeline
0.12.4,test that the fit_predict on pipeline yields same results as applying
0.12.4,transform and clustering steps separately
0.12.4,"As pipeline doesn't clone estimators on construction,"
0.12.4,it must have its own estimators
0.12.4,first compute the transform and clustering step separately
0.12.4,use a pipeline to do the transform and clustering in one step
0.12.4,tests that a pipeline does not have fit_predict method when final
0.12.4,step of pipeline does not have fit_predict defined
0.12.4,tests that Pipeline passes fit_params to intermediate steps
0.12.4,when fit_predict is invoked
0.12.4,Test whether pipeline works with a transformer at the end.
0.12.4,Also test pipeline.transform and pipeline.inverse_transform
0.12.4,test transform and fit_transform:
0.12.4,Test whether pipeline works with a transformer missing fit_transform
0.12.4,test fit_transform:
0.12.4,Directly setting attr
0.12.4,Using set_params
0.12.4,Using set_params to replace single step
0.12.4,With invalid data
0.12.4,Test setting Pipeline steps to None
0.12.4,"for other methods, ensure no AttributeErrors on None:"
0.12.4,mult2 and mult3 are active
0.12.4,Check 'passthrough' step at construction time
0.12.4,Test with Transformer + SVC
0.12.4,Memoize the transformer at the first fit
0.12.4,Get the time stamp of the tranformer in the cached pipeline
0.12.4,Check that cached_pipe and pipe yield identical results
0.12.4,Check that we are reading the cache while fitting
0.12.4,a second time
0.12.4,Check that cached_pipe and pipe yield identical results
0.12.4,Create a new pipeline with cloned estimators
0.12.4,Check that even changing the name step does not affect the cache hit
0.12.4,Check that cached_pipe and pipe yield identical results
0.12.4,Test with Transformer + SVC
0.12.4,Memoize the transformer at the first fit
0.12.4,Get the time stamp of the tranformer in the cached pipeline
0.12.4,Check that cached_pipe and pipe yield identical results
0.12.4,Check that we are reading the cache while fitting
0.12.4,a second time
0.12.4,Check that cached_pipe and pipe yield identical results
0.12.4,Create a new pipeline with cloned estimators
0.12.4,Check that even changing the name step does not affect the cache hit
0.12.4,Check that cached_pipe and pipe yield identical results
0.12.4,Test the various methods of the pipeline (pca + svm).
0.12.4,Test with PCA + SVC
0.12.4,Test the various methods of the pipeline (pca + svm).
0.12.4,Test with PCA + SVC
0.12.4,Test whether pipeline works with a sampler at the end.
0.12.4,Also test pipeline.sampler
0.12.4,test transform and fit_transform:
0.12.4,We round the value near to zero. It seems that PCA has some issue
0.12.4,with that
0.12.4,Test whether pipeline works with a sampler at the end.
0.12.4,Also test pipeline.sampler
0.12.4,Test pipeline using None as preprocessing step and a classifier
0.12.4,"Test pipeline using None, RUS and a classifier"
0.12.4,"Test pipeline using RUS, None and a classifier"
0.12.4,Test pipeline using None step and a sampler
0.12.4,Test pipeline using None and a transformer that implements transform and
0.12.4,inverse_transform
0.12.4,Test the various methods of the pipeline (anova).
0.12.4,Test with RandomUnderSampling + Anova + LogisticRegression
0.12.4,Test the various methods of the pipeline (anova).
0.12.4,Test the various methods of the pipeline (anova).
0.12.4,Test with RandomUnderSampling + Anova + LogisticRegression
0.12.4,tests that Pipeline passes predict_params to the final estimator
0.12.4,when predict is invoked
0.12.4,Test that the score_samples method is implemented on a pipeline.
0.12.4,Test that the score_samples method on pipeline yields same results as
0.12.4,applying transform and score_samples steps separately.
0.12.4,Check the shapes
0.12.4,Check the values
0.12.4,Test that a pipeline does not have score_samples method when the final
0.12.4,step of the pipeline does not have score_samples defined.
0.12.4,Test that the score_samples method is implemented on a pipeline.
0.12.4,Test that the score_samples method on pipeline yields same results as
0.12.4,applying transform and score_samples steps separately.
0.12.4,Check the shapes
0.12.4,Check the values
0.12.4,transformer will not change `y` and sampler will always preserve the type of `y`
0.12.4,transformer will not change `y` and sampler will always preserve the type of `y`
0.12.4,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.4,Christos Aridas
0.12.4,License: MIT
0.12.4,TODO: Remove when SciPy 1.9 is the minimum supported version
0.12.4,TODO: Remove when scikit-learn 1.1 is the minimum supported version
0.12.4,TODO: remove when scikit-learn minimum version is 1.3
0.12.4,we don't want to validate again for each call to partial_fit
0.12.4,TODO: remove when scikit-learn minimum version is 1.3
0.12.4,"Likely a pandas DataFrame, we explicitly check the type to confirm."
0.12.4,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.4,License: MIT
0.12.4,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.4,License: MIT
0.12.4,Adapated from scikit-learn
0.12.4,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.4,License: MIT
0.12.4,scikit-learn >= 1.2
0.12.4,TODO: remove in 0.13
0.12.4,future default in 0.13
0.12.4,we don't filter samplers based on their tag here because we want to make
0.12.4,sure that the fitted attribute does not exist if the tag is not
0.12.4,stipulated
0.12.4,trigger our checks if this is a SamplerMixin
0.12.4,should raise warning if the target is continuous (we cannot raise error)
0.12.4,if the target is multilabel then we should raise an error
0.12.4,IHT does not enforce the number of samples but provide a number
0.12.4,of samples the closest to the desired target.
0.12.4,in this test we will force all samplers to not change the class 1
0.12.4,check that sparse matrices can be passed through the sampler leading to
0.12.4,the same results than dense
0.12.4,Check that the samplers handle pandas dataframe and pandas series
0.12.4,check that we return the same type for dataframes or series types
0.12.4,FIXME: we should use to_numpy with pandas >= 0.25
0.12.4,Check that the samplers handle pandas dataframe and pandas series
0.12.4,check that we return the same type for dataframes or series types
0.12.4,FIXME: we should use to_numpy with pandas >= 0.25
0.12.4,Check that the can samplers handle simple lists
0.12.4,Check that multiclass target lead to the same results than OVA encoding
0.12.4,Cast X and y to not default dtype
0.12.4,Non-regression test for #709
0.12.4,https://github.com/scikit-learn-contrib/imbalanced-learn/issues/709
0.12.4,Check that an informative error is raised when the value of a constructor
0.12.4,parameter does not have an appropriate type or value.
0.12.4,check that there is a constraint for each parameter
0.12.4,this object does not have a valid type for sure for all params
0.12.4,This parameter is not validated
0.12.4,"First, check that the error is raised if param doesn't match any valid type."
0.12.4,the method is not accessible with the current set of parameters
0.12.4,The estimator is a label transformer and take only `y`
0.12.4,"Then, for constraints that are more than a type constraint, check that the"
0.12.4,error is raised if param does match a valid type but does not match any valid
0.12.4,value for this type.
0.12.4,the method is not accessible with the current set of parameters
0.12.4,The estimator is a label transformer and take only `y`
0.12.4,Check that calling `fit` does not raise any warnings about feature names.
0.12.4,Only check imblearn estimators for feature_names_in_ in docstring
0.12.4,partial_fit checks on second call
0.12.4,Do not call partial fit if early_stopping is on
0.12.4,input_features names is not the same length as n_features_in_
0.12.4,error is raised when `input_features` do not match feature_names_in
0.12.4,Adapted from scikit-learn
0.12.4,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.4,License: MIT
0.12.4,Ignore deprecation warnings triggered at import time and from walking
0.12.4,packages
0.12.4,get rid of abstract base classes
0.12.4,get rid of sklearn estimators which have been imported in some classes
0.12.4,"drop duplicates, sort for reproducibility"
0.12.4,itemgetter is used to ensure the sort does not extend to the 2nd item of
0.12.4,the tuple
0.12.4,Author: Adrin Jalali <adrin.jalali@gmail.com>
0.12.4,License: BSD 3 clause
0.12.4,Only the following methods are supported in the routing mechanism. Adding new
0.12.4,methods at the moment involves monkeypatching this list.
0.12.4,"Note that if this list is changed or monkeypatched, the corresponding method"
0.12.4,needs to be added under a TYPE_CHECKING condition like the one done here in
0.12.4,_MetadataRequester
0.12.4,These methods are a composite of other methods and one cannot set their
0.12.4,requests directly. Instead they should be set by setting the requests of the
0.12.4,simple methods which make the composite ones.
0.12.4,Request values
0.12.4,==============
0.12.4,"Each request value needs to be one of the following values, or an alias."
0.12.4,this is used in `__metadata_request__*` attributes to indicate that a
0.12.4,metadata is not present even though it may be present in the
0.12.4,corresponding method's signature.
0.12.4,"this is used whenever a default value is changed, and therefore the user"
0.12.4,"should explicitly set the value, otherwise a warning is shown. An example"
0.12.4,"is when a meta-estimator is only a router, but then becomes also a"
0.12.4,consumer in a new release.
0.12.4,this is the default used in `set_{method}_request` methods to indicate no
0.12.4,change requested by the user.
0.12.4,item is only an alias if it's a valid identifier
0.12.4,Metadata Request for Simple Consumers
0.12.4,=====================================
0.12.4,This section includes MethodMetadataRequest and MetadataRequest which are
0.12.4,used in simple consumers.
0.12.4,this is here for us to use this attribute's value instead of doing
0.12.4,"`isinstance` in our checks, so that we avoid issues when people vendor"
0.12.4,this file instead of using it directly from scikit-learn.
0.12.4,Called when the default attribute access fails with an AttributeError
0.12.4,(either __getattribute__() raises an AttributeError because name is
0.12.4,not an instance attribute or an attribute in the class tree for self;
0.12.4,or __get__() of a name property raises AttributeError). This method
0.12.4,should either return the (computed) attribute value or raise an
0.12.4,AttributeError exception.
0.12.4,https://docs.python.org/3/reference/datamodel.html#object.__getattr__
0.12.4,Metadata Request for Routers
0.12.4,============================
0.12.4,This section includes all objects required for MetadataRouter which is used
0.12.4,"in routers, returned by their ``get_metadata_routing``."
0.12.4,"This namedtuple is used to store a (mapping, routing) pair. Mapping is a"
0.12.4,"MethodMapping object, and routing is the output of `get_metadata_routing`."
0.12.4,MetadataRouter stores a collection of these namedtuples.
0.12.4,A namedtuple storing a single method route. A collection of these namedtuples
0.12.4,is stored in a MetadataRouter.
0.12.4,this is here for us to use this attribute's value instead of doing
0.12.4,"`isinstance`` in our checks, so that we avoid issues when people vendor"
0.12.4,this file instead of using it directly from scikit-learn.
0.12.4,`_self_request` is used if the router is also a consumer.
0.12.4,"_self_request, (added using `add_self_request()`) is treated"
0.12.4,differently from the other objects which are stored in
0.12.4,_route_mappings.
0.12.4,"conflicts are okay if the passed objects are the same, but it's"
0.12.4,an issue if they're different objects.
0.12.4,doing this instead of a try/except since an AttributeError could be raised
0.12.4,for other reasons.
0.12.4,Request method
0.12.4,==============
0.12.4,This section includes what's needed for the request method descriptor and
0.12.4,their dynamic generation in a meta class.
0.12.4,These strings are used to dynamically generate the docstrings for
0.12.4,set_{method}_request methods.
0.12.4,we would want to have a method which accepts only the expected args
0.12.4,This makes it possible to use the decorated method as an unbound
0.12.4,"method, for instance when monkeypatching."
0.12.4,https://github.com/scikit-learn/scikit-learn/issues/28632
0.12.4,Replicating python's behavior when positional args are given other
0.12.4,"than `self`, and `self` is only allowed if this method is unbound."
0.12.4,Now we set the relevant attributes of the function so that it seems
0.12.4,"like a normal method to the end user, with known expected arguments."
0.12.4,"This code is never run in runtime, but it's here for type checking."
0.12.4,Type checkers fail to understand that the `set_{method}_request`
0.12.4,"methods are dynamically generated, and they complain that they are"
0.12.4,not defined. We define them here to make type checkers happy.
0.12.4,During type checking analyzers assume this to be True.
0.12.4,The following list of defined methods mirrors the list of methods
0.12.4,in SIMPLE_METHODS.
0.12.4,fmt: off
0.12.4,fmt: on
0.12.4,"if there are any issues in the default values, it will be raised"
0.12.4,when ``get_metadata_routing`` is called. Here we are going to
0.12.4,ignore all the issues such as bad defaults etc.
0.12.4,set ``set_{method}_request``` methods
0.12.4,Here we use `isfunction` instead of `ismethod` because calling `getattr`
0.12.4,on a class instead of an instance returns an unbound function.
0.12.4,"ignore the first parameter of the method, which is usually ""self"""
0.12.4,Then overwrite those defaults with the ones provided in
0.12.4,__metadata_request__* attributes. Defaults set in
0.12.4,__metadata_request__* attributes take precedence over signature
0.12.4,sniffing.
0.12.4,need to go through the MRO since this is a class attribute and
0.12.4,``vars`` doesn't report the parent class attributes. We go through
0.12.4,the reverse of the MRO so that child classes have precedence over
0.12.4,their parents.
0.12.4,we don't check for attr.startswith() since python prefixes attrs
0.12.4,starting with __ with the `_ClassName`.
0.12.4,Process Routing in Routers
0.12.4,==========================
0.12.4,This is almost always the only method used in routers to process and route
0.12.4,given metadata. This is to minimize the boilerplate required in routers.
0.12.4,Here the first two arguments are positional only which makes everything
0.12.4,passed as keyword argument a metadata. The first two args also have an `_`
0.12.4,"prefix to reduce the chances of name collisions with the passed metadata, and"
0.12.4,"since they're positional only, users will never type those underscores."
0.12.4,"If routing is not enabled and kwargs are empty, then we don't have to"
0.12.4,"try doing any routing, we can simply return a structure which returns"
0.12.4,an empty dict on routed_params.ANYTHING.ANY_METHOD.
0.12.4,mypy: ignore-errors
0.12.4,update the docstring of the descriptor
0.12.4,"delegate only on instances, not the classes."
0.12.4,this is to allow access to the docstrings.
0.12.4,This makes it possible to use the decorated method as an
0.12.4,"unbound method, for instance when monkeypatching."
0.12.4,mypy: ignore-errors
0.12.4,Inherits from ValueError and TypeError to keep backward compatibility.
0.12.4,We allow parameters to not have a constraint so that third party
0.12.4,estimators can inherit from sklearn estimators without having to
0.12.4,necessarily use the validation tools.
0.12.4,"this constraint is satisfied, no need to check further."
0.12.4,"No constraint is satisfied, raise with an informative message."
0.12.4,Ignore constraints that we don't want to expose in the error
0.12.4,"message, i.e. options that are for internal purpose or not"
0.12.4,officially supported.
0.12.4,The dict of parameter constraints is set as an attribute of the function
0.12.4,to make it possible to dynamically introspect the constraints for
0.12.4,automatic testing.
0.12.4,Map *args/**kwargs to the function signature
0.12.4,ignore self/cls and positional/keyword markers
0.12.4,"When the function is just a wrapper around an estimator, we allow"
0.12.4,"the function to delegate validation to the estimator, but we"
0.12.4,replace the name of the estimator by the name of the function in
0.12.4,the error message to avoid confusion.
0.12.4,better repr if the bounds were given as integers
0.12.4,we use an interval of Real to ignore np.nan that has its own
0.12.4,constraint
0.12.4,"There's no integer outside (-inf, +inf)"
0.12.4,"bounds are -inf, +inf"
0.12.4,"interval is [-inf, +inf]"
0.12.4,special case for ndarray since it can't be instantiated without
0.12.4,arguments
0.12.4,special case for Integral and Real since they are abstract classes
0.12.4,Author: Alexander L. Hayes <hayesall@iu.edu>
0.12.4,License: MIT
0.12.4,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.4,License: MIT
0.12.4,We lost the y.index during resampling. We can safely use X.index to align
0.12.4,them.
0.12.4,We special case the following error:
0.12.4,https://github.com/scikit-learn-contrib/imbalanced-learn/issues/1055
0.12.4,"There is no easy way to have a generic workaround. Here, we detect"
0.12.4,that we have a column with only null values that is datetime64
0.12.4,(resulting from the np.vstack of the resampling).
0.12.4,try again
0.12.4,_is_neighbors_object(nn_object)
0.12.4,check that all keys in sampling_strategy are also in y
0.12.4,check that there is no negative number
0.12.4,check that all keys in sampling_strategy are also in y
0.12.4,ignore first 'self' argument for instance methods
0.12.4,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.4,Christos Aridas
0.12.4,License: MIT
0.12.4,this function could create an equal number of samples
0.12.4,We pass on purpose a non sorted dictionary and check that the resulting
0.12.4,dictionary is sorted. Refer to issue #428.
0.12.4,DataFrame and DataFrame case
0.12.4,DataFrames and Series case
0.12.4,The * is place before a keyword only argument without a default value
0.12.4,Test that the minimum dependencies in the README.rst file are
0.12.4,consistent with the minimum dependencies defined at the file:
0.12.4,imblearn/_min_dependencies.py
0.12.4,Skip the test if the README.rst file is not available.
0.12.4,"For instance, when installing scikit-learn from wheels"
0.12.4,Author: Alexander L. Hayes <hayesall@iu.edu>
0.12.4,License: MIT
0.12.4,Some helpers for the tests
0.12.4,check in the presence of extra positional and keyword args
0.12.4,outer decorator does not interfere with validation
0.12.4,validated method can be decorated
0.12.4,no validation in init
0.12.4,list and dict are valid params
0.12.4,the list option is not exposed in the error message
0.12.4,"""auto"" and ""warn"" are valid params"
0.12.4,"the ""warn"" option is not exposed in the error message"
0.12.4,True/False and np.bool_(True/False) are valid params
0.12.4,param1 is validated
0.12.4,param2 is not validated: any type is valid.
0.12.4,"does not raise, even though ""b"" is not in the constraints dict and ""a"" is not"
0.12.4,a parameter of the estimator.
0.12.4,does not raise
0.12.4,calls f with a bad parameter type
0.12.4,Validation for g is never skipped.
0.12.4,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.4,License: MIT
0.12.4,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.4,Christos Aridas
0.12.4,License: MIT
0.12.4,check if the filtering is working with a list or a single string
0.12.4,check that all estimators are sampler
0.12.4,check that an error is raised when the type is unknown
0.12.4,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.4,License: MIT
0.12.4,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.4,Christos Aridas
0.12.4,License: MIT
0.12.4,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.4,Christos Aridas
0.12.4,License: MIT
0.12.4,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.4,Christos Aridas
0.12.4,License: MIT
0.12.4,Check if default job count is None
0.12.4,Check if job count is set
0.12.4,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.4,Christos Aridas
0.12.4,License: MIT
0.12.4,Check if default job count is none
0.12.4,Check if job count is set
0.12.4,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.4,License: MIT
0.12.4,scikit-learn >= 1.2
0.12.4,resample before to fit the tree
0.12.4,TODO: remove when the minimum supported version of scikit-learn will be 1.4
0.12.4,support for missing values
0.12.4,TODO: remove when the minimum supported version of scikit-learn will be 1.1
0.12.4,change of signature in scikit-learn 1.1
0.12.4,make a deepcopy to not modify the original dictionary
0.12.4,TODO: remove when the minimum supported version of scikit-learn will be 1.4
0.12.4,use scikit-learn support for monotonic constraints
0.12.4,create an attribute for compatibility with other scikit-learn tools such
0.12.4,as HTML representation.
0.12.4,TODO: remove in 0.13
0.12.4,Validate or convert input data
0.12.4,TODO: remove when the minimum supported version of scipy will be 1.4
0.12.4,Support for missing values
0.12.4,TODO: remove when the minimum supported version of scikit-learn will be 1.4
0.12.4,_compute_missing_values_in_feature_mask checks if X has missing values and
0.12.4,will raise an error if the underlying tree base estimator can't handle
0.12.4,missing values. Only the criterion is required to determine if the tree
0.12.4,supports missing values.
0.12.4,Pre-sort indices to avoid that each individual tree of the
0.12.4,ensemble sorts the indices.
0.12.4,reshape is necessary to preserve the data contiguity against vs
0.12.4,"[:, np.newaxis] that does not."
0.12.4,Get bootstrap sample size
0.12.4,Check parameters
0.12.4,"Free allocated memory, if any"
0.12.4,We draw from the random state to get the random state we
0.12.4,would have got if we hadn't used a warm_start.
0.12.4,Parallel loop: we prefer the threading backend as the Cython code
0.12.4,for fitting the trees is internally releasing the Python GIL
0.12.4,making threading more efficient than multiprocessing in
0.12.4,"that case. However, we respect any parallel_backend contexts set"
0.12.4,"at a higher level, since correctness does not rely on using"
0.12.4,threads.
0.12.4,Collect newly grown trees
0.12.4,Create pipeline with the fitted samplers and trees
0.12.4,FIXME: we could consider to support multiclass-multioutput if
0.12.4,we introduce or reuse a constructor parameter (e.g.
0.12.4,oob_score) allowing our user to pass a callable defining the
0.12.4,scoring strategy on OOB sample.
0.12.4,Decapsulate classes_ attributes
0.12.4,drop the n_outputs axis if there is a single output
0.12.4,Prediction requires X to be in CSR format
0.12.4,n_classes_ is a ndarray at this stage
0.12.4,all the supported type of target will have the same number of
0.12.4,classes in all outputs
0.12.4,"for regression, n_classes_ does not exist and we create an empty"
0.12.4,axis to be consistent with the classification case and make
0.12.4,the array operations compatible with the 2 settings
0.12.4,TODO: remove when supporting scikit-learn>=1.2
0.12.4,make a deepcopy to not modify the original dictionary
0.12.4,TODO: remove when minimum supported version of scikit-learn is 1.4
0.12.4,SAMME-R requires predict_proba-enabled estimators
0.12.4,Instances incorrectly classified
0.12.4,Error fraction
0.12.4,Stop if classification is perfect
0.12.4,Construct y coding as described in Zhu et al [2]:
0.12.4,
0.12.4,y_k = 1 if c == k else -1 / (K - 1)
0.12.4,
0.12.4,"where K == n_classes_ and c, k in [0, K) are indices along the second"
0.12.4,axis of the y coding with c being the index corresponding to the true
0.12.4,class label.
0.12.4,Displace zero probabilities so the log is defined.
0.12.4,Also fix negative elements which may occur with
0.12.4,negative sample weights.
0.12.4,Boost weight using multi-class AdaBoost SAMME.R alg
0.12.4,Only boost the weights if it will fit again
0.12.4,Only boost positive weights
0.12.4,Instances incorrectly classified
0.12.4,Error fraction
0.12.4,Stop if classification is perfect
0.12.4,Stop if the error is at least as bad as random guessing
0.12.4,Boost weight using multi-class AdaBoost SAMME alg
0.12.4,Only boost the weights if I will fit again
0.12.4,Only boost positive weights
0.12.4,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.4,Christos Aridas
0.12.4,License: MIT
0.12.4,scikit-learn >= 1.2
0.12.4,make a deepcopy to not modify the original dictionary
0.12.4,TODO: remove when minimum supported version of scikit-learn is 1.4
0.12.4,TODO: remove when supporting scikit-learn>=1.2
0.12.4,overwrite the base class method by disallowing `sample_weight`
0.12.4,RandomUnderSampler is not supporting sample_weight. We need to pass
0.12.4,None.
0.12.4,TODO: remove when minimum supported version of scikit-learn is 1.1
0.12.4,Check data
0.12.4,Parallel loop
0.12.4,Reduce
0.12.4,The base class require to have the attribute defined. For scikit-learn
0.12.4,"> 1.2, we are going to raise an error."
0.12.4,TODO: remove when minimum supported version of scikit-learn is 1.5
0.12.4,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.4,Christos Aridas
0.12.4,License: MIT
0.12.4,scikit-learn >= 1.2
0.12.4,make a deepcopy to not modify the original dictionary
0.12.4,TODO: remove when minimum supported version of scikit-learn is 1.4
0.12.4,TODO: remove when supporting scikit-learn>=1.2
0.12.4,overwrite the base class method by disallowing `sample_weight`
0.12.4,the sampler needs to be validated before to call _fit because
0.12.4,_validate_y is called before _validate_estimator and would require
0.12.4,to know which type of sampler we are using.
0.12.4,RandomUnderSampler is not supporting sample_weight. We need to pass
0.12.4,None.
0.12.4,TODO: remove when minimum supported version of scikit-learn is 1.1
0.12.4,Check data
0.12.4,Parallel loop
0.12.4,Reduce
0.12.4,The base class require to have the attribute defined. For scikit-learn
0.12.4,"> 1.2, we are going to raise an error."
0.12.4,check that we have an ensemble of samplers and estimators with a
0.12.4,consistent size
0.12.4,each sampler in the ensemble should have different random state
0.12.4,each estimator in the ensemble should have different random state
0.12.4,check the consistency of the feature importances
0.12.4,check the consistency of the prediction outpus
0.12.4,Predictions should be the same when sample_weight are all ones
0.12.4,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.4,Christos Aridas
0.12.4,License: MIT
0.12.4,Check classification for various parameter settings.
0.12.4,Test that bootstrapping samples generate non-perfect base estimators.
0.12.4,"without bootstrap, all trees are perfect on the training set"
0.12.4,disable the resampling by passing an empty dictionary.
0.12.4,"with bootstrap, trees are no longer perfect on the training set"
0.12.4,Test that bootstrapping features may generate duplicate features.
0.12.4,Predict probabilities.
0.12.4,Normal case
0.12.4,"Degenerate case, where some classes are missing"
0.12.4,Check that oob prediction is a good estimation of the generalization
0.12.4,error.
0.12.4,Test with few estimators
0.12.4,Check singleton ensembles.
0.12.4,Check that bagging ensembles can be grid-searched.
0.12.4,Transform iris into a binary classification task
0.12.4,Grid search with scoring based on decision_function
0.12.4,Check estimator and its default values.
0.12.4,Test if fitting incrementally with warm start gives a forest of the
0.12.4,right size and the same results as a normal fit.
0.12.4,Test if warm start'ed second fit with smaller n_estimators raises error.
0.12.4,Test that nothing happens when fitting without increasing n_estimators
0.12.4,"modify X to nonsense values, this should not change anything"
0.12.4,warm started classifier with 5+5 estimators should be equivalent to
0.12.4,one classifier with 10 estimators
0.12.4,Check using oob_score and warm_start simultaneously fails
0.12.4,"Make sure OOB scores are identical when random_state, estimator, and"
0.12.4,training data are fixed and fitting is done twice
0.12.4,Check that format of estimators_samples_ is correct and that results
0.12.4,generated at fit time can be identically reproduced at a later time
0.12.4,using data saved in object attributes.
0.12.4,remap the y outside of the BalancedBaggingclassifier
0.12.4,"_, y = np.unique(y, return_inverse=True)"
0.12.4,Get relevant attributes
0.12.4,Test for correct formatting
0.12.4,Re-fit single estimator to test for consistent sampling
0.12.4,Make sure validated max_samples and original max_samples are identical
0.12.4,when valid integer max_samples supplied by user
0.12.4,check that we can pass any kind of sampler to a bagging classifier
0.12.4,check that we have balanced class with the right counts of class
0.12.4,sample depending on the sampling strategy
0.12.4,check that we can provide a FunctionSampler in BalancedBaggingClassifier
0.12.4,find the minority and majority classes
0.12.4,compute the number of sample to draw from the majority class using
0.12.4,a negative binomial distribution
0.12.4,draw randomly with or without replacement
0.12.4,Roughly Balanced Bagging
0.12.4,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.4,Christos Aridas
0.12.4,License: MIT
0.12.4,Generate a global dataset to use
0.12.4,Check classification for various parameter settings.
0.12.4,test the different prediction function
0.12.4,Check estimator and its default values.
0.12.4,Test if fitting incrementally with warm start gives a forest of the
0.12.4,right size and the same results as a normal fit.
0.12.4,Test if warm start'ed second fit with smaller n_estimators raises error.
0.12.4,Test that nothing happens when fitting without increasing n_estimators
0.12.4,"modify X to nonsense values, this should not change anything"
0.12.4,warm started classifier with 5+5 estimators should be equivalent to
0.12.4,one classifier with 10 estimators
0.12.4,Check warning if not enough estimators
0.12.4,First fit with no restriction on max samples
0.12.4,Second fit with max samples restricted to just 2
0.12.4,Regression test for #655: check that the oob score is closed to 0.5
0.12.4,a binomial experiment.
0.12.4,TODO: remove in 0.13
0.12.4,Create dataset with missing values
0.12.4,Train forest with missing values
0.12.4,Train forest without missing values
0.12.4,Score is still 80 percent of the forest's score that had no missing values
0.12.4,Create a predictive feature using `y` and with some noise
0.12.4,Author: Guillaume Lemaitre
0.12.4,License: BSD 3 clause
0.12.4,"The index start at one, then we need to remove one"
0.12.4,to not have issue with the indexing.
0.12.4,go through the list and check if the data are available
0.12.4,Authors: Dayvid Oliveira
0.12.4,Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.4,Christos Aridas
0.12.4,License: MIT
0.12.4,restrict ratio to be a dict or a callable
0.12.4,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.4,Christos Aridas
0.12.4,License: MIT
0.12.4,"we are reusing part of utils.check_sampling_strategy, however this is not"
0.12.4,cover in the common tests so we will repeat it here
0.12.4,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.4,Christos Aridas
0.12.4,License: MIT
0.12.4,This is a trick to avoid an error during tests collection with pytest. We
0.12.4,avoid the error when importing the package raise the error at the moment of
0.12.4,creating the instance.
0.12.4,This is a trick to avoid an error during tests collection with pytest. We
0.12.4,avoid the error when importing the package raise the error at the moment of
0.12.4,creating the instance.
0.12.4,flag for keras sequence duck-typing
0.12.4,shuffle the indices since the sampler are packing them by class
0.12.3,This file is here so that when running from the root folder
0.12.3,./imblearn is added to sys.path by pytest.
0.12.3,See https://docs.pytest.org/en/latest/pythonpath.html for more details.
0.12.3,"For example, this allows to build extensions in place and run pytest"
0.12.3,doc/modules/clustering.rst and use imblearn from the local folder
0.12.3,rather than the one from site-packages.
0.12.3,! /usr/bin/env python
0.12.3,Python 2 compat: just to be able to declare that Python >=3.7 is needed.
0.12.3,This is a bit (!) hackish: we are setting a global variable so that the
0.12.3,main imblearn __init__ can detect if it is being loaded by the setup
0.12.3,"routine, to avoid attempting to load components that aren't built yet:"
0.12.3,the numpy distutils extensions that are used by imbalanced-learn to
0.12.3,recursively build the compiled extensions in sub-packages is based on the
0.12.3,Python import machinery.
0.12.3,get __version__ from _version.py
0.12.3,-*- coding: utf-8 -*-
0.12.3,
0.12.3,"imbalanced-learn documentation build configuration file, created by"
0.12.3,sphinx-quickstart on Mon Jan 18 14:44:12 2016.
0.12.3,
0.12.3,This file is execfile()d with the current directory set to its
0.12.3,containing dir.
0.12.3,
0.12.3,Note that not all possible configuration values are present in this
0.12.3,autogenerated file.
0.12.3,
0.12.3,All configuration values have a default; values that are commented out
0.12.3,serve to show the default.
0.12.3,"If extensions (or modules to document with autodoc) are in another directory,"
0.12.3,add these directories to sys.path here. If the directory is relative to the
0.12.3,"documentation root, use os.path.abspath to make it absolute, like shown here."
0.12.3,-- General configuration ------------------------------------------------
0.12.3,"If your documentation needs a minimal Sphinx version, state it here."
0.12.3,needs_sphinx = '1.0'
0.12.3,"Add any Sphinx extension module names here, as strings. They can be"
0.12.3,extensions coming with Sphinx (named 'sphinx.ext.*') or your custom
0.12.3,ones.
0.12.3,Specify how to identify the prompt when copying code snippets
0.12.3,"Add any paths that contain templates here, relative to this directory."
0.12.3,The suffix of source filenames.
0.12.3,The master toctree document.
0.12.3,General information about the project.
0.12.3,"The version info for the project you're documenting, acts as replacement for"
0.12.3,"|version| and |release|, also used in various other places throughout the"
0.12.3,built documents.
0.12.3,
0.12.3,The short X.Y version.
0.12.3,"The full version, including alpha/beta/rc tags."
0.12.3,"List of patterns, relative to source directory, that match files and"
0.12.3,directories to ignore when looking for source files.
0.12.3,The reST default role (used for this markup: `text`) to use for all
0.12.3,documents.
0.12.3,"If true, '()' will be appended to :func: etc. cross-reference text."
0.12.3,The name of the Pygments (syntax highlighting) style to use.
0.12.3,-- Options for HTML output ----------------------------------------------
0.12.3,The theme to use for HTML and HTML Help pages.  See the documentation for
0.12.3,a list of builtin themes.
0.12.3,"""navbar_align"": ""right"",  # For testing that the navbar items align properly"
0.12.3,"Add any paths that contain custom static files (such as style sheets) here,"
0.12.3,"relative to this directory. They are copied after the builtin static files,"
0.12.3,"so a file named ""default.css"" will overwrite the builtin ""default.css""."
0.12.3,Output file base name for HTML help builder.
0.12.3,-- Options for autodoc ------------------------------------------------------
0.12.3,generate autosummary even if no references
0.12.3,-- Options for numpydoc -----------------------------------------------------
0.12.3,this is needed for some reason...
0.12.3,see https://github.com/numpy/numpydoc/issues/69
0.12.3,-- Options for sphinxcontrib-bibtex -----------------------------------------
0.12.3,bibtex file
0.12.3,-- Options for intersphinx --------------------------------------------------
0.12.3,intersphinx configuration
0.12.3,-- Options for sphinx-gallery -----------------------------------------------
0.12.3,Generate the plot for the gallery
0.12.3,sphinx-gallery configuration
0.12.3,-- Options for github link for what's new -----------------------------------
0.12.3,Config for sphinx_issues
0.12.3,The following is used by sphinx.ext.linkcode to provide links to github
0.12.3,-- Options for LaTeX output ---------------------------------------------
0.12.3,The paper size ('letterpaper' or 'a4paper').
0.12.3,"'papersize': 'letterpaper',"
0.12.3,"The font size ('10pt', '11pt' or '12pt')."
0.12.3,"'pointsize': '10pt',"
0.12.3,Additional stuff for the LaTeX preamble.
0.12.3,"'preamble': '',"
0.12.3,Grouping the document tree into LaTeX files. List of tuples
0.12.3,"(source start file, target name, title,"
0.12.3,"author, documentclass [howto, manual, or own class])."
0.12.3,-- Options for manual page output ---------------------------------------
0.12.3,"If false, no module index is generated."
0.12.3,latex_domain_indices = True
0.12.3,One entry per manual page. List of tuples
0.12.3,"(source start file, name, description, authors, manual section)."
0.12.3,"If true, show URL addresses after external links."
0.12.3,man_show_urls = False
0.12.3,-- Options for Texinfo output -------------------------------------------
0.12.3,Grouping the document tree into Texinfo files. List of tuples
0.12.3,"(source start file, target name, title, author,"
0.12.3,"dir menu entry, description, category)"
0.12.3,-- Dependencies generation ----------------------------------------------
0.12.3,get length of header
0.12.3,-- Additional temporary hacks -----------------------------------------------
0.12.3,get the styles from the current theme
0.12.3,create and add the button to all the code blocks that contain >>>
0.12.3,tracebacks (.gt) contain bare text elements that need to be
0.12.3,wrapped in a span to work with .nextUntil() (see later)
0.12.3,define the behavior of the button when it's clicked
0.12.3,hide the code output
0.12.3,show the code output
0.12.3,-*- coding: utf-8 -*-
0.12.3,Format template for issues URI
0.12.3,e.g. 'https://github.com/sloria/marshmallow/issues/{issue}
0.12.3,Format template for PR URI
0.12.3,e.g. 'https://github.com/sloria/marshmallow/pull/{issue}
0.12.3,Format template for commit URI
0.12.3,e.g. 'https://github.com/sloria/marshmallow/commits/{commit}
0.12.3,"Shortcut for Github, e.g. 'sloria/marshmallow'"
0.12.3,Format template for user profile URI
0.12.3,e.g. 'https://github.com/{user}'
0.12.3,Python 2 only
0.12.3,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.3,License: MIT
0.12.3,%%
0.12.3,%%
0.12.3,"First, we will generate a toy classification dataset with only few samples."
0.12.3,The ratio between the classes will be imbalanced.
0.12.3,%%
0.12.3,%%
0.12.3,"Now, we will use a :class:`~imblearn.over_sampling.RandomOverSampler` to"
0.12.3,generate a bootstrap for the minority class with as many samples as in the
0.12.3,majority class.
0.12.3,%%
0.12.3,%%
0.12.3,We observe that the minority samples are less transparent than the samples
0.12.3,"from the majority class. Indeed, it is due to the fact that these samples"
0.12.3,of the minority class are repeated during the bootstrap generation.
0.12.3,
0.12.3,We can set `shrinkage` to a floating value to add a small perturbation to the
0.12.3,samples created and therefore create a smoothed bootstrap.
0.12.3,%%
0.12.3,%%
0.12.3,"In this case, we see that the samples in the minority class are not"
0.12.3,overlapping anymore due to the added noise.
0.12.3,
0.12.3,The parameter `shrinkage` allows to add more or less perturbation. Let's
0.12.3,add more perturbation when generating the smoothed bootstrap.
0.12.3,%%
0.12.3,%%
0.12.3,Increasing the value of `shrinkage` will disperse the new samples. Forcing
0.12.3,the shrinkage to 0 will be equivalent to generating a normal bootstrap.
0.12.3,%%
0.12.3,%%
0.12.3,"Therefore, the `shrinkage` is handy to manually tune the dispersion of the"
0.12.3,new samples.
0.12.3,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.3,License: MIT
0.12.3,%%
0.12.3,generate some data points
0.12.3,plot the majority and minority samples
0.12.3,draw the circle in which the new sample will generated
0.12.3,plot the line on which the sample will be generated
0.12.3,create and plot the new sample
0.12.3,make the plot nicer with legend and label
0.12.3,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.3,License: MIT
0.12.3,%%
0.12.3,%% [markdown]
0.12.3,The following function will be used to create toy dataset. It uses the
0.12.3,:func:`~sklearn.datasets.make_classification` from scikit-learn but fixing
0.12.3,some parameters.
0.12.3,%%
0.12.3,%% [markdown]
0.12.3,The following function will be used to plot the sample space after resampling
0.12.3,to illustrate the specificities of an algorithm.
0.12.3,%%
0.12.3,%% [markdown]
0.12.3,The following function will be used to plot the decision function of a
0.12.3,classifier given some data.
0.12.3,%%
0.12.3,%% [markdown]
0.12.3,Illustration of the influence of the balancing ratio
0.12.3,----------------------------------------------------
0.12.3,
0.12.3,We will first illustrate the influence of the balancing ratio on some toy
0.12.3,data using a logistic regression classifier which is a linear model.
0.12.3,%%
0.12.3,%% [markdown]
0.12.3,We will fit and show the decision boundary model to illustrate the impact of
0.12.3,dealing with imbalanced classes.
0.12.3,%%
0.12.3,%% [markdown]
0.12.3,"Greater is the difference between the number of samples in each class, poorer"
0.12.3,are the classification results.
0.12.3,
0.12.3,Random over-sampling to balance the data set
0.12.3,--------------------------------------------
0.12.3,
0.12.3,Random over-sampling can be used to repeat some samples and balance the
0.12.3,number of samples between the dataset. It can be seen that with this trivial
0.12.3,approach the boundary decision is already less biased toward the majority
0.12.3,class. The class :class:`~imblearn.over_sampling.RandomOverSampler`
0.12.3,implements such of a strategy.
0.12.3,%%
0.12.3,%% [markdown]
0.12.3,"By default, random over-sampling generates a bootstrap. The parameter"
0.12.3,`shrinkage` allows adding a small perturbation to the generated data
0.12.3,to generate a smoothed bootstrap instead. The plot below shows the difference
0.12.3,between the two data generation strategies.
0.12.3,%%
0.12.3,%% [markdown]
0.12.3,It looks like more samples are generated with smoothed bootstrap. This is due
0.12.3,to the fact that the samples generated are not superimposing with the
0.12.3,original samples.
0.12.3,
0.12.3,More advanced over-sampling using ADASYN and SMOTE
0.12.3,--------------------------------------------------
0.12.3,
0.12.3,Instead of repeating the same samples when over-sampling or perturbating the
0.12.3,"generated bootstrap samples, one can use some specific heuristic instead."
0.12.3,:class:`~imblearn.over_sampling.ADASYN` and
0.12.3,:class:`~imblearn.over_sampling.SMOTE` can be used in this case.
0.12.3,%%
0.12.3,%% [markdown]
0.12.3,The following plot illustrates the difference between
0.12.3,:class:`~imblearn.over_sampling.ADASYN` and
0.12.3,:class:`~imblearn.over_sampling.SMOTE`.
0.12.3,:class:`~imblearn.over_sampling.ADASYN` will focus on the samples which are
0.12.3,difficult to classify with a nearest-neighbors rule while regular
0.12.3,:class:`~imblearn.over_sampling.SMOTE` will not make any distinction.
0.12.3,"Therefore, the decision function depending of the algorithm."
0.12.3,%% [markdown]
0.12.3,"Due to those sampling particularities, it can give rise to some specific"
0.12.3,issues as illustrated below.
0.12.3,%%
0.12.3,%% [markdown]
0.12.3,SMOTE proposes several variants by identifying specific samples to consider
0.12.3,during the resampling. The borderline version
0.12.3,(:class:`~imblearn.over_sampling.BorderlineSMOTE`) will detect which point to
0.12.3,select which are in the border between two classes. The SVM version
0.12.3,(:class:`~imblearn.over_sampling.SVMSMOTE`) will use the support vectors
0.12.3,found using an SVM algorithm to create new sample while the KMeans version
0.12.3,(:class:`~imblearn.over_sampling.KMeansSMOTE`) will make a clustering before
0.12.3,to generate samples in each cluster independently depending each cluster
0.12.3,density.
0.12.3,%%
0.12.3,%% [markdown]
0.12.3,"When dealing with a mixed of continuous and categorical features,"
0.12.3,:class:`~imblearn.over_sampling.SMOTENC` is the only method which can handle
0.12.3,this case.
0.12.3,%%
0.12.3,Create a dataset of a mix of numerical and categorical data
0.12.3,%% [markdown]
0.12.3,"However, if the dataset is composed of only categorical features then one"
0.12.3,should use :class:`~imblearn.over_sampling.SMOTEN`.
0.12.3,%%
0.12.3,Generate only categorical data
0.12.3,Authors: Christos Aridas
0.12.3,Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.3,License: MIT
0.12.3,%%
0.12.3,%% [markdown]
0.12.3,Let's first generate a dataset with imbalanced class distribution.
0.12.3,%%
0.12.3,%% [markdown]
0.12.3,We will use an over-sampler :class:`~imblearn.over_sampling.SMOTE` followed
0.12.3,by a :class:`~sklearn.tree.DecisionTreeClassifier`. The aim will be to
0.12.3,search which `k_neighbors` parameter is the most adequate with the dataset
0.12.3,that we generated.
0.12.3,%%
0.12.3,%% [markdown]
0.12.3,We can use the :class:`~sklearn.model_selection.validation_curve` to inspect
0.12.3,"the impact of varying the parameter `k_neighbors`. In this case, we need"
0.12.3,to use a score to evaluate the generalization score during the
0.12.3,cross-validation.
0.12.3,%%
0.12.3,%%
0.12.3,%% [markdown]
0.12.3,We can now plot the results of the cross-validation for the different
0.12.3,parameter values that we tried.
0.12.3,%%
0.12.3,make nice plotting
0.12.3,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.3,License: MIT
0.12.3,Generate a dataset
0.12.3,Split the data
0.12.3,Train the classifier with balancing
0.12.3,Test the classifier and get the prediction
0.12.3,Show the classification report
0.12.3,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.3,License: MIT
0.12.3,%%
0.12.3,%% [markdown]
0.12.3,"First, we will generate some imbalanced dataset."
0.12.3,%%
0.12.3,%% [markdown]
0.12.3,We will split the data into a training and testing set.
0.12.3,%%
0.12.3,%% [markdown]
0.12.3,We will create a pipeline made of a :class:`~imblearn.over_sampling.SMOTE`
0.12.3,over-sampler followed by a :class:`~sklearn.linear_model.LogisticRegression`
0.12.3,classifier.
0.12.3,%%
0.12.3,%% [markdown]
0.12.3,"Now, we will train the model on the training set and get the prediction"
0.12.3,associated with the testing set. Be aware that the resampling will happen
0.12.3,only when calling `fit`: the number of samples in `y_pred` is the same than
0.12.3,in `y_test`.
0.12.3,%%
0.12.3,%% [markdown]
0.12.3,The geometric mean corresponds to the square root of the product of the
0.12.3,sensitivity and specificity. Combining the two metrics should account for
0.12.3,the balancing of the dataset.
0.12.3,%%
0.12.3,%% [markdown]
0.12.3,The index balanced accuracy can transform any metric to be used in
0.12.3,imbalanced learning problems.
0.12.3,%%
0.12.3,%%
0.12.3,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.3,License: MIT
0.12.3,%%
0.12.3,%% [markdown]
0.12.3,Dataset generation
0.12.3,------------------
0.12.3,
0.12.3,We will create an imbalanced dataset with a couple of samples. We will use
0.12.3,:func:`~sklearn.datasets.make_classification` to generate this dataset.
0.12.3,%%
0.12.3,%%
0.12.3,%% [markdown]
0.12.3,The following function will be used to plot the sample space after resampling
0.12.3,to illustrate the characteristic of an algorithm.
0.12.3,%%
0.12.3,%% [markdown]
0.12.3,The following function will be used to plot the decision function of a
0.12.3,classifier given some data.
0.12.3,%%
0.12.3,%% [markdown]
0.12.3,":class:`~imblearn.over_sampling.SMOTE` allows to generate samples. However,"
0.12.3,this method of over-sampling does not have any knowledge regarding the
0.12.3,"underlying distribution. Therefore, some noisy samples can be generated, e.g."
0.12.3,"when the different classes cannot be well separated. Hence, it can be"
0.12.3,beneficial to apply an under-sampling algorithm to clean the noisy samples.
0.12.3,Two methods are usually used in the literature: (i) Tomek's link and (ii)
0.12.3,edited nearest neighbours cleaning methods. Imbalanced-learn provides two
0.12.3,ready-to-use samplers :class:`~imblearn.combine.SMOTETomek` and
0.12.3,":class:`~imblearn.combine.SMOTEENN`. In general,"
0.12.3,:class:`~imblearn.combine.SMOTEENN` cleans more noisy data than
0.12.3,:class:`~imblearn.combine.SMOTETomek`.
0.12.3,%%
0.12.3,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.3,License: MIT
0.12.3,%%
0.12.3,%% [markdown]
0.12.3,Load an imbalanced dataset
0.12.3,--------------------------
0.12.3,
0.12.3,We will load the UCI SatImage dataset which has an imbalanced ratio of 9.3:1
0.12.3,(number of majority sample for a minority sample). The data are then split
0.12.3,into training and testing.
0.12.3,%%
0.12.3,%% [markdown]
0.12.3,Classification using a single decision tree
0.12.3,-------------------------------------------
0.12.3,
0.12.3,We train a decision tree classifier which will be used as a baseline for the
0.12.3,rest of this example.
0.12.3,
0.12.3,The results are reported in terms of balanced accuracy and geometric mean
0.12.3,which are metrics widely used in the literature to validate model trained on
0.12.3,imbalanced set.
0.12.3,%%
0.12.3,%%
0.12.3,%%
0.12.3,%% [markdown]
0.12.3,Classification using bagging classifier with and without sampling
0.12.3,-----------------------------------------------------------------
0.12.3,
0.12.3,"Instead of using a single tree, we will check if an ensemble of decision tree"
0.12.3,"can actually alleviate the issue induced by the class imbalancing. First, we"
0.12.3,will use a bagging classifier and its counter part which internally uses a
0.12.3,random under-sampling to balanced each bootstrap sample.
0.12.3,%%
0.12.3,%% [markdown]
0.12.3,Balancing each bootstrap sample allows to increase significantly the balanced
0.12.3,accuracy and the geometric mean.
0.12.3,%%
0.12.3,%%
0.12.3,%% [markdown]
0.12.3,Classification using random forest classifier with and without sampling
0.12.3,-----------------------------------------------------------------------
0.12.3,
0.12.3,Random forest is another popular ensemble method and it is usually
0.12.3,"outperforming bagging. Here, we used a vanilla random forest and its balanced"
0.12.3,counterpart in which each bootstrap sample is balanced.
0.12.3,%%
0.12.3,%% [markdown]
0.12.3,"Similarly to the previous experiment, the balanced classifier outperform the"
0.12.3,"classifier which learn from imbalanced bootstrap samples. In addition, random"
0.12.3,forest outperforms the bagging classifier.
0.12.3,%%
0.12.3,%%
0.12.3,%% [markdown]
0.12.3,Boosting classifier
0.12.3,-------------------
0.12.3,
0.12.3,"In the same manner, easy ensemble classifier is a bag of balanced AdaBoost"
0.12.3,"classifier. However, it will be slower to train than random forest and will"
0.12.3,achieve worse performance.
0.12.3,%%
0.12.3,%%
0.12.3,%%
0.12.3,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.3,License: MIT
0.12.3,%%
0.12.3,%% [markdown]
0.12.3,Generate an imbalanced dataset
0.12.3,------------------------------
0.12.3,
0.12.3,"For this example, we will create a synthetic dataset using the function"
0.12.3,:func:`~sklearn.datasets.make_classification`. The problem will be a toy
0.12.3,classification problem with a ratio of 1:9 between the two classes.
0.12.3,%%
0.12.3,%%
0.12.3,%% [markdown]
0.12.3,"In the following sections, we will show a couple of algorithms that have"
0.12.3,been proposed over the years. We intend to illustrate how one can reuse the
0.12.3,:class:`~imblearn.ensemble.BalancedBaggingClassifier` by passing different
0.12.3,sampler.
0.12.3,%%
0.12.3,%% [markdown]
0.12.3,Exactly Balanced Bagging and Over-Bagging
0.12.3,-----------------------------------------
0.12.3,
0.12.3,The :class:`~imblearn.ensemble.BalancedBaggingClassifier` can use in
0.12.3,conjunction with a :class:`~imblearn.under_sampling.RandomUnderSampler` or
0.12.3,:class:`~imblearn.over_sampling.RandomOverSampler`. These methods are
0.12.3,"referred as Exactly Balanced Bagging and Over-Bagging, respectively and have"
0.12.3,been proposed first in [1]_.
0.12.3,%%
0.12.3,Exactly Balanced Bagging
0.12.3,%%
0.12.3,Over-bagging
0.12.3,%% [markdown]
0.12.3,SMOTE-Bagging
0.12.3,-------------
0.12.3,
0.12.3,Instead of using a :class:`~imblearn.over_sampling.RandomOverSampler` that
0.12.3,"make a bootstrap, an alternative is to use"
0.12.3,:class:`~imblearn.over_sampling.SMOTE` as an over-sampler. This is known as
0.12.3,SMOTE-Bagging [2]_.
0.12.3,%%
0.12.3,SMOTE-Bagging
0.12.3,%% [markdown]
0.12.3,Roughly Balanced Bagging
0.12.3,------------------------
0.12.3,While using a :class:`~imblearn.under_sampling.RandomUnderSampler` or
0.12.3,:class:`~imblearn.over_sampling.RandomOverSampler` will create exactly the
0.12.3,"desired number of samples, it does not follow the statistical spirit wanted"
0.12.3,in the bagging framework. The authors in [3]_ proposes to use a negative
0.12.3,binomial distribution to compute the number of samples of the majority
0.12.3,class to be selected and then perform a random under-sampling.
0.12.3,
0.12.3,"Here, we illustrate this method by implementing a function in charge of"
0.12.3,resampling and use the :class:`~imblearn.FunctionSampler` to integrate it
0.12.3,within a :class:`~imblearn.pipeline.Pipeline` and
0.12.3,:class:`~sklearn.model_selection.cross_validate`.
0.12.3,%%
0.12.3,find the minority and majority classes
0.12.3,compute the number of sample to draw from the majority class using
0.12.3,a negative binomial distribution
0.12.3,draw randomly with or without replacement
0.12.3,Roughly Balanced Bagging
0.12.3,%% [markdown]
0.12.3,.. topic:: References:
0.12.3,
0.12.3,".. [1] R. Maclin, and D. Opitz. ""An empirical evaluation of bagging and"
0.12.3,"boosting."" AAAI/IAAI 1997 (1997): 546-551."
0.12.3,
0.12.3,".. [2] S. Wang, and X. Yao. ""Diversity analysis on imbalanced data sets by"
0.12.3,"using ensemble models."" 2009 IEEE symposium on computational"
0.12.3,"intelligence and data mining. IEEE, 2009."
0.12.3,
0.12.3,".. [3] S. Hido, H. Kashima, and Y. Takahashi. ""Roughly balanced bagging"
0.12.3,"for imbalanced data."" Statistical Analysis and Data Mining: The ASA"
0.12.3,Data Science Journal 2.5‚Äê6 (2009): 412-426.
0.12.3,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.3,License: MIT
0.12.3,%%
0.12.3,%% [markdown]
0.12.3,The following function will be used to create toy dataset. It uses the
0.12.3,:func:`~sklearn.datasets.make_classification` from scikit-learn but fixing
0.12.3,some parameters.
0.12.3,%%
0.12.3,%% [markdown]
0.12.3,The following function will be used to plot the sample space after resampling
0.12.3,to illustrate the specificities of an algorithm.
0.12.3,%%
0.12.3,%% [markdown]
0.12.3,The following function will be used to plot the decision function of a
0.12.3,classifier given some data.
0.12.3,%%
0.12.3,%%
0.12.3,%% [markdown]
0.12.3,Prototype generation: under-sampling by generating new samples
0.12.3,--------------------------------------------------------------
0.12.3,
0.12.3,:class:`~imblearn.under_sampling.ClusterCentroids` under-samples by replacing
0.12.3,the original samples by the centroids of the cluster found.
0.12.3,%%
0.12.3,%% [markdown]
0.12.3,Prototype selection: under-sampling by selecting existing samples
0.12.3,-----------------------------------------------------------------
0.12.3,
0.12.3,The algorithm performing prototype selection can be subdivided into two
0.12.3,groups: (i) the controlled under-sampling methods and (ii) the cleaning
0.12.3,under-sampling methods.
0.12.3,
0.12.3,"With the controlled under-sampling methods, the number of samples to be"
0.12.3,selected can be specified.
0.12.3,:class:`~imblearn.under_sampling.RandomUnderSampler` is the most naive way of
0.12.3,performing such selection by randomly selecting a given number of samples by
0.12.3,the targeted class.
0.12.3,%%
0.12.3,%% [markdown]
0.12.3,:class:`~imblearn.under_sampling.NearMiss` algorithms implement some
0.12.3,heuristic rules in order to select samples. NearMiss-1 selects samples from
0.12.3,the majority class for which the average distance of the :math:`k`` nearest
0.12.3,samples of the minority class is the smallest. NearMiss-2 selects the samples
0.12.3,from the majority class for which the average distance to the farthest
0.12.3,samples of the negative class is the smallest. NearMiss-3 is a 2-step
0.12.3,"algorithm: first, for each minority sample, their :math:`m`"
0.12.3,"nearest-neighbors will be kept; then, the majority samples selected are the"
0.12.3,on for which the average distance to the :math:`k` nearest neighbors is the
0.12.3,largest.
0.12.3,%%
0.12.3,%% [markdown]
0.12.3,:class:`~imblearn.under_sampling.EditedNearestNeighbours` removes samples of
0.12.3,the majority class for which their class differ from the one of their
0.12.3,nearest-neighbors. This sieve can be repeated which is the principle of the
0.12.3,:class:`~imblearn.under_sampling.RepeatedEditedNearestNeighbours`.
0.12.3,:class:`~imblearn.under_sampling.AllKNN` is slightly different from the
0.12.3,:class:`~imblearn.under_sampling.RepeatedEditedNearestNeighbours` by changing
0.12.3,"the :math:`k` parameter of the internal nearest neighors algorithm,"
0.12.3,increasing it at each iteration.
0.12.3,%%
0.12.3,%% [markdown]
0.12.3,:class:`~imblearn.under_sampling.CondensedNearestNeighbour` makes use of a
0.12.3,1-NN to iteratively decide if a sample should be kept in a dataset or not.
0.12.3,The issue is that :class:`~imblearn.under_sampling.CondensedNearestNeighbour`
0.12.3,is sensitive to noise by preserving the noisy samples.
0.12.3,:class:`~imblearn.under_sampling.OneSidedSelection` also used the 1-NN and
0.12.3,use :class:`~imblearn.under_sampling.TomekLinks` to remove the samples
0.12.3,considered noisy. The
0.12.3,:class:`~imblearn.under_sampling.NeighbourhoodCleaningRule` use a
0.12.3,:class:`~imblearn.under_sampling.EditedNearestNeighbours` to remove some
0.12.3,"sample. Additionally, they use a 3 nearest-neighbors to remove samples which"
0.12.3,do not agree with this rule.
0.12.3,%%
0.12.3,%% [markdown]
0.12.3,:class:`~imblearn.under_sampling.InstanceHardnessThreshold` uses the
0.12.3,prediction of classifier to exclude samples. All samples which are classified
0.12.3,with a low probability will be removed.
0.12.3,%%
0.12.3,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.3,License: MIT
0.12.3,%%
0.12.3,%% [markdown]
0.12.3,This function allows to make nice plotting
0.12.3,%%
0.12.3,%% [markdown]
0.12.3,We will generate some toy data that illustrates how
0.12.3,:class:`~imblearn.under_sampling.TomekLinks` is used to clean a dataset.
0.12.3,%%
0.12.3,%% [markdown]
0.12.3,"In the figure above, the samples highlighted in green form a Tomek link since"
0.12.3,they are of different classes and are nearest neighbors of each other.
0.12.3,highlight the samples of interest
0.12.3,%% [markdown]
0.12.3,We can run the :class:`~imblearn.under_sampling.TomekLinks` sampling to
0.12.3,remove the corresponding samples. If `sampling_strategy='auto'` only the
0.12.3,sample from the majority class will be removed. If `sampling_strategy='all'`
0.12.3,both samples will be removed.
0.12.3,%%
0.12.3,highlight the samples of interest
0.12.3,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.3,License: MIT
0.12.3,%%
0.12.3,%% [markdown]
0.12.3,We define a function allowing to make some nice decoration on the plot.
0.12.3,%%
0.12.3,%% [markdown]
0.12.3,We can start by generating some data to later illustrate the principle of
0.12.3,each :class:`~imblearn.under_sampling.NearMiss` heuristic rules.
0.12.3,%%
0.12.3,%% [mardown]
0.12.3,NearMiss-1
0.12.3,----------
0.12.3,
0.12.3,NearMiss-1 selects samples from the majority class for which the average
0.12.3,distance to some nearest neighbours is the smallest. In the following
0.12.3,"example, we use a 3-NN to compute the average distance on 2 specific samples"
0.12.3,"of the majority class. Therefore, in this case the point linked by the"
0.12.3,green-dashed line will be selected since the average distance is smaller.
0.12.3,%%
0.12.3,%% [mardown]
0.12.3,NearMiss-2
0.12.3,----------
0.12.3,
0.12.3,NearMiss-2 selects samples from the majority class for which the average
0.12.3,distance to the farthest neighbors is the smallest. With the same
0.12.3,"configuration as previously presented, the sample linked to the green-dashed"
0.12.3,line will be selected since its distance the 3 farthest neighbors is the
0.12.3,smallest.
0.12.3,%%
0.12.3,%% [mardown]
0.12.3,NearMiss-3
0.12.3,----------
0.12.3,
0.12.3,"NearMiss-3 can be divided into 2 steps. First, a nearest-neighbors is used to"
0.12.3,short-list samples from the majority class (i.e. correspond to the
0.12.3,"highlighted samples in the following plot). Then, the sample with the largest"
0.12.3,average distance to the *k* nearest-neighbors are selected.
0.12.3,%%
0.12.3,select only the majority point of interest
0.12.3,Authors: Christos Aridas
0.12.3,Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.3,License: MIT
0.12.3,%%
0.12.3,%% [markdown]
0.12.3,Let's first create an imbalanced dataset and split in to two sets.
0.12.3,%%
0.12.3,%% [markdown]
0.12.3,"Now, we will create each individual steps that we would like later to combine"
0.12.3,%%
0.12.3,%% [markdown]
0.12.3,"Now, we can finally create a pipeline to specify in which order the different"
0.12.3,transformers and samplers should be executed before to provide the data to
0.12.3,the final classifier.
0.12.3,%%
0.12.3,%% [markdown]
0.12.3,We can now use the pipeline created as a normal classifier where resampling
0.12.3,"will happen when calling `fit` and disabled when calling `decision_function`,"
0.12.3,"`predict_proba`, or `predict`."
0.12.3,%%
0.12.3,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.3,License: MIT
0.12.3,##############################################################################
0.12.3,Data loading
0.12.3,##############################################################################
0.12.3,##############################################################################
0.12.3,"First, you should download the Porto Seguro data set from Kaggle. See the"
0.12.3,link in the introduction.
0.12.3,##############################################################################
0.12.3,The data set is imbalanced and it will have an effect on the fitting.
0.12.3,##############################################################################
0.12.3,Define the pre-processing pipeline
0.12.3,##############################################################################
0.12.3,##############################################################################
0.12.3,We want to standard scale the numerical features while we want to one-hot
0.12.3,"encode the categorical features. In this regard, we make use of the"
0.12.3,:class:`~sklearn.compose.ColumnTransformer`.
0.12.3,Create an environment variable to avoid using the GPU. This can be changed.
0.12.3,##############################################################################
0.12.3,Create a neural-network
0.12.3,##############################################################################
0.12.3,##############################################################################
0.12.3,We create a decorator to report the computation time
0.12.3,##############################################################################
0.12.3,The first model will be trained using the ``fit`` method and with imbalanced
0.12.3,mini-batches.
0.12.3,predict_proba was removed in tensorflow 2.6
0.12.3,##############################################################################
0.12.3,"In the contrary, we will use imbalanced-learn to create a generator of"
0.12.3,mini-batches which will yield balanced mini-batches.
0.12.3,##############################################################################
0.12.3,Classification loop
0.12.3,##############################################################################
0.12.3,##############################################################################
0.12.3,We will perform a 10-fold cross-validation and train the neural-network with
0.12.3,the two different strategies previously presented.
0.12.3,##############################################################################
0.12.3,Plot of the results and computation time
0.12.3,##############################################################################
0.12.3,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.3,License: MIT
0.12.3,%%
0.12.3,%% [markdown]
0.12.3,Problem definition
0.12.3,------------------
0.12.3,
0.12.3,We are dropping the following features:
0.12.3,
0.12.3,"- ""fnlwgt"": this feature was created while studying the ""adult"" dataset."
0.12.3,"Thus, we will not use this feature which is not acquired during the survey."
0.12.3,"- ""education-num"": it is encoding the same information than ""education""."
0.12.3,"Thus, we are removing one of these 2 features."
0.12.3,%%
0.12.3,%% [markdown]
0.12.3,"The ""adult"" dataset as a class ratio of about 3:1"
0.12.3,%%
0.12.3,%% [markdown]
0.12.3,This dataset is only slightly imbalanced. To better highlight the effect of
0.12.3,"learning from an imbalanced dataset, we will increase its ratio to 30:1"
0.12.3,%%
0.12.3,%% [markdown]
0.12.3,We will perform a cross-validation evaluation to get an estimate of the test
0.12.3,score.
0.12.3,
0.12.3,"As a baseline, we could use a classifier which will always predict the"
0.12.3,majority class independently of the features provided.
0.12.3,%%
0.12.3,%% [markdown]
0.12.3,"Instead of using the accuracy, we can use the balanced accuracy which will"
0.12.3,take into account the balancing issue.
0.12.3,%%
0.12.3,%% [markdown]
0.12.3,Strategies to learn from an imbalanced dataset
0.12.3,----------------------------------------------
0.12.3,We will use a dictionary and a list to continuously store the results of
0.12.3,our experiments and show them as a pandas dataframe.
0.12.3,%%
0.12.3,%% [markdown]
0.12.3,Dummy baseline
0.12.3,..............
0.12.3,
0.12.3,"Before to train a real machine learning model, we can store the results"
0.12.3,obtained with our :class:`~sklearn.dummy.DummyClassifier`.
0.12.3,%%
0.12.3,%% [markdown]
0.12.3,Linear classifier baseline
0.12.3,..........................
0.12.3,
0.12.3,We will create a machine learning pipeline using a
0.12.3,":class:`~sklearn.linear_model.LogisticRegression` classifier. In this regard,"
0.12.3,we will need to one-hot encode the categorical columns and standardized the
0.12.3,numerical columns before to inject the data into the
0.12.3,:class:`~sklearn.linear_model.LogisticRegression` classifier.
0.12.3,
0.12.3,"First, we define our numerical and categorical pipelines."
0.12.3,%%
0.12.3,%% [markdown]
0.12.3,"Then, we can create a preprocessor which will dispatch the categorical"
0.12.3,columns to the categorical pipeline and the numerical columns to the
0.12.3,numerical pipeline
0.12.3,%%
0.12.3,%% [markdown]
0.12.3,"Finally, we connect our preprocessor with our"
0.12.3,:class:`~sklearn.linear_model.LogisticRegression`. We can then evaluate our
0.12.3,model.
0.12.3,%%
0.12.3,%%
0.12.3,%% [markdown]
0.12.3,We can see that our linear model is learning slightly better than our dummy
0.12.3,"baseline. However, it is impacted by the class imbalance."
0.12.3,
0.12.3,We can verify that something similar is happening with a tree-based model
0.12.3,such as :class:`~sklearn.ensemble.RandomForestClassifier`. With this type of
0.12.3,"classifier, we will not need to scale the numerical data, and we will only"
0.12.3,need to ordinal encode the categorical data.
0.12.3,%%
0.12.3,%%
0.12.3,%% [markdown]
0.12.3,The :class:`~sklearn.ensemble.RandomForestClassifier` is as well affected by
0.12.3,"the class imbalanced, slightly less than the linear model. Now, we will"
0.12.3,present different approach to improve the performance of these 2 models.
0.12.3,
0.12.3,Use `class_weight`
0.12.3,..................
0.12.3,
0.12.3,Most of the models in `scikit-learn` have a parameter `class_weight`. This
0.12.3,parameter will affect the computation of the loss in linear model or the
0.12.3,criterion in the tree-based model to penalize differently a false
0.12.3,classification from the minority and majority class. We can set
0.12.3,"`class_weight=""balanced""` such that the weight applied is inversely"
0.12.3,proportional to the class frequency. We test this parametrization in both
0.12.3,linear model and tree-based model.
0.12.3,%%
0.12.3,%%
0.12.3,%% [markdown]
0.12.3,We can see that using `class_weight` was really effective for the linear
0.12.3,"model, alleviating the issue of learning from imbalanced classes. However,"
0.12.3,the :class:`~sklearn.ensemble.RandomForestClassifier` is still biased toward
0.12.3,"the majority class, mainly due to the criterion which is not suited enough to"
0.12.3,fight the class imbalance.
0.12.3,
0.12.3,Resample the training set during learning
0.12.3,.........................................
0.12.3,
0.12.3,Another way is to resample the training set by under-sampling or
0.12.3,over-sampling some of the samples. `imbalanced-learn` provides some samplers
0.12.3,to do such processing.
0.12.3,%%
0.12.3,%%
0.12.3,%%
0.12.3,%%
0.12.3,%% [markdown]
0.12.3,Applying a random under-sampler before the training of the linear model or
0.12.3,"random forest, allows to not focus on the majority class at the cost of"
0.12.3,making more mistake for samples in the majority class (i.e. decreased
0.12.3,accuracy).
0.12.3,
0.12.3,We could apply any type of samplers and find which sampler is working best
0.12.3,on the current dataset.
0.12.3,
0.12.3,"Instead, we will present another way by using classifiers which will apply"
0.12.3,sampling internally.
0.12.3,
0.12.3,Use of specific balanced algorithms from imbalanced-learn
0.12.3,.........................................................
0.12.3,
0.12.3,We already showed that random under-sampling can be effective on decision
0.12.3,"tree. However, instead of under-sampling once the dataset, one could"
0.12.3,under-sample the original dataset before to take a bootstrap sample. This is
0.12.3,the base of the :class:`imblearn.ensemble.BalancedRandomForestClassifier` and
0.12.3,:class:`~imblearn.ensemble.BalancedBaggingClassifier`.
0.12.3,%%
0.12.3,%%
0.12.3,%% [markdown]
0.12.3,The performance with the
0.12.3,:class:`~imblearn.ensemble.BalancedRandomForestClassifier` is better than
0.12.3,applying a single random under-sampling. We will use a gradient-boosting
0.12.3,classifier within a :class:`~imblearn.ensemble.BalancedBaggingClassifier`.
0.12.3,%% [markdown]
0.12.3,This last approach is the most effective. The different under-sampling allows
0.12.3,to bring some diversity for the different GBDT to learn and not focus on a
0.12.3,portion of the majority class.
0.12.3,Authors: Christos Aridas
0.12.3,Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.3,License: MIT
0.12.3,%%
0.12.3,%% [markdown]
0.12.3,Load the dataset
0.12.3,----------------
0.12.3,
0.12.3,We will use a dataset containing image from know person where we will
0.12.3,build a model to recognize the person on the image. We will make this problem
0.12.3,a binary problem by taking picture of only George W. Bush and Bill Clinton.
0.12.3,%%
0.12.3,%%
0.12.3,%% [markdown]
0.12.3,We can check the ratio between the two classes.
0.12.3,%%
0.12.3,%% [markdown]
0.12.3,We see that we have an imbalanced classification problem with ~95% of the
0.12.3,data belonging to the class G.W. Bush.
0.12.3,
0.12.3,Compare over-sampling approaches
0.12.3,--------------------------------
0.12.3,
0.12.3,We will use different over-sampling approaches and use a kNN classifier
0.12.3,to check if we can recognize the 2 presidents. The evaluation will be
0.12.3,performed through cross-validation and we will plot the mean ROC curve.
0.12.3,
0.12.3,We will create different pipelines and evaluate them.
0.12.3,%%
0.12.3,%% [markdown]
0.12.3,We will compute the mean ROC curve for each pipeline using a different splits
0.12.3,provided by the :class:`~sklearn.model_selection.StratifiedKFold`
0.12.3,cross-validation.
0.12.3,%%
0.12.3,compute the mean fpr/tpr to get the mean ROC curve
0.12.3,Create a display that we will reuse to make the aggregated plots for
0.12.3,all methods
0.12.3,%% [markdown]
0.12.3,"In the previous cell, we created the different mean ROC curve and we can plot"
0.12.3,them on the same plot.
0.12.3,%%
0.12.3,%% [markdown]
0.12.3,"We see that for this task, methods that are generating new samples with some"
0.12.3,interpolation (i.e. ADASYN and SMOTE) perform better than random
0.12.3,over-sampling or no resampling.
0.12.3,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.3,License: MIT
0.12.3,Create a folder to fetch the dataset
0.12.3,Create a pipeline
0.12.3,Classify and report the results
0.12.3,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.3,License: MIT
0.12.3,%%
0.12.3,%% [markdown]
0.12.3,Setting the data set
0.12.3,--------------------
0.12.3,
0.12.3,We use a part of the 20 newsgroups data set by loading 4 topics. Using the
0.12.3,"scikit-learn loader, the data are split into a training and a testing set."
0.12.3,
0.12.3,Note the class \#3 is the minority class and has almost twice less samples
0.12.3,than the majority class.
0.12.3,%%
0.12.3,%%
0.12.3,%% [markdown]
0.12.3,The usual scikit-learn pipeline
0.12.3,-------------------------------
0.12.3,
0.12.3,You might usually use scikit-learn pipeline by combining the TF-IDF
0.12.3,vectorizer to feed a multinomial naive bayes classifier. A classification
0.12.3,report summarized the results on the testing set.
0.12.3,
0.12.3,"As expected, the recall of the class \#3 is low mainly due to the class"
0.12.3,imbalanced.
0.12.3,%%
0.12.3,%%
0.12.3,%% [markdown]
0.12.3,Balancing the class before classification
0.12.3,-----------------------------------------
0.12.3,
0.12.3,"To improve the prediction of the class \#3, it could be interesting to apply"
0.12.3,"a balancing before to train the naive bayes classifier. Therefore, we will"
0.12.3,use a :class:`~imblearn.under_sampling.RandomUnderSampler` to equalize the
0.12.3,number of samples in all the classes before the training.
0.12.3,
0.12.3,It is also important to note that we are using the
0.12.3,:class:`~imblearn.pipeline.make_pipeline` function implemented in
0.12.3,imbalanced-learn to properly handle the samplers.
0.12.3,%%
0.12.3,%% [markdown]
0.12.3,"Although the results are almost identical, it can be seen that the resampling"
0.12.3,allowed to correct the poor recall of the class \#3 at the cost of reducing
0.12.3,"the other metrics for the other classes. However, the overall results are"
0.12.3,slightly better.
0.12.3,%%
0.12.3,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.3,License: MIT
0.12.3,#############################################################################
0.12.3,Toy data generation
0.12.3,#############################################################################
0.12.3,#############################################################################
0.12.3,We are generating some non Gaussian data set contaminated with some unform
0.12.3,noise.
0.12.3,#############################################################################
0.12.3,We will generate some cleaned test data without outliers.
0.12.3,#############################################################################
0.12.3,How to use the :class:`~imblearn.FunctionSampler`
0.12.3,#############################################################################
0.12.3,#############################################################################
0.12.3,We first define a function which will use
0.12.3,:class:`~sklearn.ensemble.IsolationForest` to eliminate some outliers from
0.12.3,our dataset during training. The function passed to the
0.12.3,:class:`~imblearn.FunctionSampler` will be called when using the method
0.12.3,``fit_resample``.
0.12.3,#############################################################################
0.12.3,Integrate it within a pipeline
0.12.3,#############################################################################
0.12.3,#############################################################################
0.12.3,"By elimnating outliers before the training, the classifier will be less"
0.12.3,affected during the prediction.
0.12.3,Authors: Dayvid Oliveira
0.12.3,Christos Aridas
0.12.3,Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.3,License: MIT
0.12.3,%%
0.12.3,%% [markdown]
0.12.3,Generate the dataset
0.12.3,--------------------
0.12.3,
0.12.3,"First, we will generate a dataset and convert it to a"
0.12.3,:class:`~pandas.DataFrame` with arbitrary column names. We will plot the
0.12.3,original dataset.
0.12.3,%%
0.12.3,%% [markdown]
0.12.3,Make a dataset imbalanced
0.12.3,-------------------------
0.12.3,
0.12.3,"Now, we will show the helpers :func:`~imblearn.datasets.make_imbalance`"
0.12.3,that is useful to random select a subset of samples. It will impact the
0.12.3,class distribution as specified by the parameters.
0.12.3,%%
0.12.3,%%
0.12.3,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.3,License: MIT
0.12.3,%%
0.12.3,%% [markdown]
0.12.3,Create an imbalanced dataset
0.12.3,----------------------------
0.12.3,
0.12.3,"First, we will create an imbalanced data set from a the iris data set."
0.12.3,%%
0.12.3,%%
0.12.3,%% [markdown]
0.12.3,Using ``sampling_strategy`` in resampling algorithms
0.12.3,====================================================
0.12.3,
0.12.3,`sampling_strategy` as a `float`
0.12.3,--------------------------------
0.12.3,
0.12.3,`sampling_strategy` can be given a `float`. For **under-sampling
0.12.3,"methods**, it corresponds to the ratio :math:`\alpha_{us}` defined by"
0.12.3,:math:`N_{rM} = \alpha_{us} \times N_{m}` where :math:`N_{rM}` and
0.12.3,:math:`N_{m}` are the number of samples in the majority class after
0.12.3,"resampling and the number of samples in the minority class, respectively."
0.12.3,%%
0.12.3,select only 2 classes since the ratio make sense in this case
0.12.3,%%
0.12.3,%% [markdown]
0.12.3,"For **over-sampling methods**, it correspond to the ratio"
0.12.3,:math:`\alpha_{os}` defined by :math:`N_{rm} = \alpha_{os} \times N_{M}`
0.12.3,where :math:`N_{rm}` and :math:`N_{M}` are the number of samples in the
0.12.3,minority class after resampling and the number of samples in the majority
0.12.3,"class, respectively."
0.12.3,%%
0.12.3,%% [markdown]
0.12.3,`sampling_strategy` as a `str`
0.12.3,-------------------------------
0.12.3,
0.12.3,`sampling_strategy` can be given as a string which specify the class
0.12.3,"targeted by the resampling. With under- and over-sampling, the number of"
0.12.3,samples will be equalized.
0.12.3,
0.12.3,Note that we are using multiple classes from now on.
0.12.3,%%
0.12.3,%% [markdown]
0.12.3,"With **cleaning method**, the number of samples in each class will not be"
0.12.3,equalized even if targeted.
0.12.3,%%
0.12.3,%% [markdown]
0.12.3,`sampling_strategy` as a `dict`
0.12.3,-------------------------------
0.12.3,
0.12.3,"When `sampling_strategy` is a `dict`, the keys correspond to the targeted"
0.12.3,classes. The values correspond to the desired number of samples for each
0.12.3,targeted class. This is working for both **under- and over-sampling**
0.12.3,algorithms but not for the **cleaning algorithms**. Use a `list` instead.
0.12.3,%%
0.12.3,%% [markdown]
0.12.3,`sampling_strategy` as a `list`
0.12.3,-------------------------------
0.12.3,
0.12.3,"When `sampling_strategy` is a `list`, the list contains the targeted"
0.12.3,classes. It is used only for **cleaning methods** and raise an error
0.12.3,otherwise.
0.12.3,%%
0.12.3,%% [markdown]
0.12.3,`sampling_strategy` as a callable
0.12.3,---------------------------------
0.12.3,
0.12.3,"When callable, function taking `y` and returns a `dict`. The keys"
0.12.3,correspond to the targeted classes. The values correspond to the desired
0.12.3,number of samples for each class.
0.12.3,%%
0.12.3,List of whitelisted modules and methods; regexp are supported.
0.12.3,These docstrings will fail because they are inheriting from scikit-learn
0.12.3,skip private classes
0.12.3,"We ignore following error code,"
0.12.3,- RT02: The first line of the Returns section
0.12.3,"should contain only the type, .."
0.12.3,(as we may need refer to the name of the returned
0.12.3,object)
0.12.3,- GL01: Docstring text (summary) should start in the line
0.12.3,"immediately after the opening quotes (not in the same line,"
0.12.3,or leaving a blank line in between)
0.12.3,"- GL02: If there's a blank line, it should be before the"
0.12.3,"first line of the Returns section, not after (it allows to have"
0.12.3,short docstrings for properties).
0.12.3,Ignore PR02: Unknown parameters for properties. We sometimes use
0.12.3,"properties for ducktyping, i.e. SGDClassifier.predict_proba"
0.12.3,Following codes are only taken into account for the
0.12.3,top level class docstrings:
0.12.3,- ES01: No extended summary found
0.12.3,- SA01: See Also section not found
0.12.3,- EX01: No examples section found
0.12.3,In particular we can't parse the signature of properties
0.12.3,"When applied to classes, detect class method. For functions"
0.12.3,method = None.
0.12.3,TODO: this detection can be improved. Currently we assume that we have
0.12.3,class # methods if the second path element before last is in camel case.
0.12.3,'build' and 'install' is included to have structured metadata for CI.
0.12.3,It will NOT be included in setup's extras_require
0.12.3,"The values are (version_spec, comma separated tags)"
0.12.3,create inverse mapping for setuptools
0.12.3,Used by CI to get the min dependencies
0.12.3,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.3,License: MIT
0.12.3,TODO: remove this file when scikit-learn minimum version is 1.3
0.12.3,Return a copy of the threadlocal configuration so that users will
0.12.3,not be able to modify the configuration with the returned dict.
0.12.3,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.3,Christos Aridas
0.12.3,License: MIT
0.12.3,scikit-learn >= 1.2
0.12.3,we need to overwrite SamplerMixin.fit to bypass the validation
0.12.3,Adapted from scikit-learn
0.12.3,Author: Edouard Duchesnay
0.12.3,Gael Varoquaux
0.12.3,Virgile Fritsch
0.12.3,Alexandre Gramfort
0.12.3,Lars Buitinck
0.12.3,Christos Aridas
0.12.3,Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.3,License: BSD
0.12.3,BaseEstimator interface
0.12.3,validate names
0.12.3,validate estimators
0.12.3,We allow last estimator to be None as an identity transformation
0.12.3,Estimator interface
0.12.3,"def _fit(self, X, y=None, **fit_params_steps):"
0.12.3,Setup the memory
0.12.3,we do not clone when caching is disabled to
0.12.3,preserve backward compatibility
0.12.3,Fit or load from cache the current transformer
0.12.3,Replace the transformer of the step with the fitted
0.12.3,transformer. This is necessary when loading the transformer
0.12.3,from the cache.
0.12.3,The `fit_*` methods need to be overridden to support the samplers.
0.12.3,estimators in Pipeline.steps are not validated yet
0.12.3,estimators in Pipeline.steps are not validated yet
0.12.3,metadata routing enabled
0.12.3,estimators in Pipeline.steps are not validated yet
0.12.3,estimators in Pipeline.steps are not validated yet
0.12.3,TODO: remove the following methods when the minimum scikit-learn >= 1.4
0.12.3,They do not depend on resampling but we need to redefine them for the
0.12.3,compatibility with the metadata routing framework.
0.12.3,metadata routing enabled
0.12.3,not branching here since params is only available if
0.12.3,enable_metadata_routing=True
0.12.3,metadata routing enabled
0.12.3,not branching here since params is only available if
0.12.3,enable_metadata_routing=True
0.12.3,"we don't have to branch here, since params is only non-empty if"
0.12.3,enable_metadata_routing=True.
0.12.3,metadata routing is enabled.
0.12.3,"TODO: once scikit-learn >= 1.4, the following function should be simplified by"
0.12.3,calling `super().get_metadata_routing()`
0.12.3,first we add all steps except the last one
0.12.3,"fit, fit_predict, and fit_transform call fit_transform if it"
0.12.3,"exists, or else fit and transform"
0.12.3,then we add the last step
0.12.3,"without metadata routing, fit_transform and fit_predict"
0.12.3,get all the same params and pass it to the last fit.
0.12.3,"if we have a weight for this transformer, multiply output"
0.12.3,This variable is injected in the __builtins__ by the build
0.12.3,process. It is used to enable importing subpackages of sklearn when
0.12.3,the binaries are not built
0.12.3,mypy error: Cannot determine type of '__SKLEARN_SETUP__'
0.12.3,We are not importing the rest of scikit-learn during the build
0.12.3,"process, as it may not be compiled yet"
0.12.3,"FIXME: When we get Python 3.7 as minimal version, we will need to switch to"
0.12.3,the following solution:
0.12.3,https://snarky.ca/lazy-importing-in-python-3-7/
0.12.3,Import the target module and insert it into the parent's namespace
0.12.3,Update this object's dict so that if someone keeps a reference to the
0.12.3,"LazyLoader, lookups are efficient (__getattr__ is only called on"
0.12.3,lookups that fail).
0.12.3,delay the import of keras since we are going to import either tensorflow
0.12.3,or keras
0.12.3,Based on NiLearn package
0.12.3,License: simplified BSD
0.12.3,"PEP0440 compatible formatted version, see:"
0.12.3,https://www.python.org/dev/peps/pep-0440/
0.12.3,
0.12.3,Generic release markers:
0.12.3,X.Y
0.12.3,X.Y.Z # For bugfix releases
0.12.3,
0.12.3,Admissible pre-release markers:
0.12.3,X.YaN # Alpha release
0.12.3,X.YbN # Beta release
0.12.3,X.YrcN # Release Candidate
0.12.3,X.Y # Final release
0.12.3,
0.12.3,Dev branch marker is: 'X.Y.dev' or 'X.Y.devN' where N is an integer.
0.12.3,'X.Y.dev0' is the canonical version of 'X.Y.dev'
0.12.3,
0.12.3,coding: utf-8
0.12.3,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.3,Dariusz Brzezinski
0.12.3,License: MIT
0.12.3,Only negative labels
0.12.3,"Calculate tp_sum, pred_sum, true_sum ###"
0.12.3,labels are now from 0 to len(labels) - 1 -> use bincount
0.12.3,Pathological case
0.12.3,Compute the true negative
0.12.3,Retain only selected labels
0.12.3,"Finally, we have all our sufficient statistics. Divide! #"
0.12.3,"Divide, and on zero-division, set scores to 0 and warn:"
0.12.3,"Oddly, we may get an ""invalid"" rather than a ""divide"" error"
0.12.3,here.
0.12.3,Average the results
0.12.3,labels are now from 0 to len(labels) - 1 -> use bincount
0.12.3,Pathological case
0.12.3,Retain only selected labels
0.12.3,old version of scipy return MaskedConstant instead of 0.0
0.12.3,check that the scoring function does not need a score
0.12.3,and only a prediction
0.12.3,We do not support multilabel so the only average supported
0.12.3,is binary
0.12.3,Compute the different metrics
0.12.3,Precision/recall/f1
0.12.3,Specificity
0.12.3,Geometric mean
0.12.3,Index balanced accuracy
0.12.3,compute averages
0.12.3,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.3,License: MIT
0.12.3,categories are expected to be encoded from 0 to n_categories - 1
0.12.3,"list of length n_features of ndarray (n_categories, n_classes)"
0.12.3,compute the counts
0.12.3,normalize by the summing over the classes
0.12.3,silence potential warning due to in-place division by zero
0.12.3,coding: utf-8
0.12.3,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.3,Christos Aridas
0.12.3,License: MIT
0.12.3,##############################################################################
0.12.3,Utilities for testing
0.12.3,import some data to play with
0.12.3,restrict to a binary classification task
0.12.3,add noisy features to make the problem harder and avoid perfect results
0.12.3,"run classifier, get class probabilities and label predictions"
0.12.3,only interested in probabilities of the positive case
0.12.3,XXX: do we really want a special API for the binary case?
0.12.3,##############################################################################
0.12.3,Tests
0.12.3,detailed measures for each class
0.12.3,individual scoring function that can be used for grid search: in the
0.12.3,binary class case the score is the value of the measure for the positive
0.12.3,class (e.g. label == 1). This is deprecated for average != 'binary'.
0.12.3,Such a case may occur with non-stratified cross-validation
0.12.3,ensure the above were meaningful tests:
0.12.3,Bad pos_label
0.12.3,Bad average option
0.12.3,but average != 'binary'; even if data is binary
0.12.3,compute the geometric mean for the binary problem
0.12.3,print classification report with class names
0.12.3,print classification report with label detection
0.12.3,print classification report with class names
0.12.3,print classification report with label detection
0.12.3,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.3,License: MIT
0.12.3,Check basic feature of the metric:
0.12.3,"* the shape of the distance matrix is (n_samples, n_samples)"
0.12.3,* computing pairwise distance of X is the same than explicitely between
0.12.3,X and X.
0.12.3,Check the property of the vdm distance. Let's check the property
0.12.3,"described in ""Improved Heterogeneous Distance Functions"", D.R. Wilson and"
0.12.3,"T.R. Martinez, Journal of Artificial Intelligence Research 6 (1997) 1-34"
0.12.3,https://arxiv.org/pdf/cs/9701101.pdf
0.12.3,
0.12.3,"""if an attribute color has three values red, green and blue, and the"
0.12.3,"application is to identify whether or not an object is an apple, red and"
0.12.3,green would be considered closer than red and blue because the former two
0.12.3,"both have similar correlations with the output class apple."""
0.12.3,defined our feature
0.12.3,0 - not an apple / 1 - an apple
0.12.3,computing the distance between a sample of the same category should
0.12.3,give a null distance
0.12.3,check the property explained in the introduction example
0.12.3,green and red are very close
0.12.3,blue is closer to red than green
0.12.3,"Check that ""auto"" is equivalent to provide the number categories"
0.12.3,beforehand
0.12.3,Check that we raise an error if n_categories is inconsistent with the
0.12.3,number of features in X
0.12.3,Check that we don't get issue when a category is missing between 0
0.12.3,n_categories - 1
0.12.3,remove a categories that could be between 0 and n_categories
0.12.3,Check that we raise a NotFittedError when `fit` is not not called before
0.12.3,pairwise.
0.12.3,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.3,Christos Aridas
0.12.3,License: MIT
0.12.3,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.3,Christos Aridas
0.12.3,License: MIT
0.12.3,FIXME: to be removed in 0.12
0.12.3,The ratio is computed using a one-vs-rest manner. Using majority
0.12.3,in multi-class would lead to slightly different results at the
0.12.3,cost of introducing a new parameter.
0.12.3,rounding may cause new amount for n_samples
0.12.3,the nearest neighbors need to be fitted only on the current class
0.12.3,to find the class NN to generate new samples
0.12.3,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.3,Christos Aridas
0.12.3,License: MIT
0.12.3,smoothed bootstrap imposes to make numerical operation; we need
0.12.3,to be sure to have only numerical data in X
0.12.3,generate a smoothed bootstrap with a perturbation
0.12.3,generate a bootstrap
0.12.3,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.3,Christos Aridas
0.12.3,License: MIT
0.12.3,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.3,Fernando Nogueira
0.12.3,Christos Aridas
0.12.3,License: MIT
0.12.3,negate diagonal elements
0.12.3,identify cluster which are answering the requirements
0.12.3,empty cluster
0.12.3,the cluster is already considered balanced
0.12.3,not enough samples to apply SMOTE
0.12.3,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.3,Fernando Nogueira
0.12.3,Christos Aridas
0.12.3,Dzianis Dudnik
0.12.3,License: MIT
0.12.3,FIXME: to be removed in 0.12
0.12.3,FIXME: to be removed in 0.12
0.12.3,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.3,Fernando Nogueira
0.12.3,Christos Aridas
0.12.3,Dzianis Dudnik
0.12.3,License: MIT
0.12.3,np.newaxis for backwards compatability with random_state
0.12.3,Samples are in danger for m/2 <= m' < m
0.12.3,Samples are noise for m = m'
0.12.3,FIXME: to be removed in 0.12
0.12.3,FIXME: to be removed in 0.12
0.12.3,the input of the OneHotEncoder needs to be dense
0.12.3,SMOTE resampling starts here
0.12.3,"In the edge case where the median of the std is equal to 0, the 1s"
0.12.3,"entries will be also nullified. In this case, we store the original"
0.12.3,categorical encoding which will be later used for inverting the OHE
0.12.3,This variable will be used when generating data
0.12.3,we can replace the 1 entries of the categorical features with the
0.12.3,median of the standard deviation. It will ensure that whenever
0.12.3,"distance is computed between 2 samples, the difference will be equal"
0.12.3,to the median of the standard deviation as in the original paper.
0.12.3,"With one-hot encoding, the median will be repeated twice. We need"
0.12.3,to divide by sqrt(2) such that we only have one median value
0.12.3,contributing to the Euclidean distance
0.12.3,SMOTE resampling ends here
0.12.3,reverse the encoding of the categorical features
0.12.3,the matrix is supposed to be in the CSR format after the stacking
0.12.3,change in sparsity structure more efficient with LIL than CSR
0.12.3,convert to dense array since scipy.sparse doesn't handle 3D
0.12.3,"In the case that the median std was equal to zeros, we have to"
0.12.3,create non-null entry based on the encoded of OHE
0.12.3,tie breaking argmax
0.12.3,generate sample indices that will be used to generate new samples
0.12.3,"for each drawn samples, select its k-neighbors and generate a sample"
0.12.3,"where for each feature individually, each category generated is the"
0.12.3,most common category
0.12.3,FIXME: to be removed in 0.12
0.12.3,the kneigbors search will include the sample itself which is
0.12.3,expected from the original algorithm
0.12.3,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.3,Christos Aridas
0.12.3,Dzianis Dudnik
0.12.3,License: MIT
0.12.3,create 2 random continuous feature
0.12.3,create a categorical feature using some string
0.12.3,create a categorical feature using some integer
0.12.3,return the categories
0.12.3,create 2 random continuous feature
0.12.3,create a categorical feature using some string
0.12.3,create a categorical feature using some integer
0.12.3,return the categories
0.12.3,create 2 random continuous feature
0.12.3,create a categorical feature using some string
0.12.3,create a categorical feature using some integer
0.12.3,return the categories
0.12.3,create 2 random continuous feature
0.12.3,create a categorical feature using some string
0.12.3,create a categorical feature using some integer
0.12.3,return the categories
0.12.3,create 2 random continuous feature
0.12.3,create a categorical feature using some string
0.12.3,create a categorical feature using some integer
0.12.3,part of the common test which apply to SMOTE-NC even if it is not default
0.12.3,constructible
0.12.3,Check that the samplers handle pandas dataframe and pandas series
0.12.3,Cast X and y to not default dtype
0.12.3,Non-regression test for #662
0.12.3,https://github.com/scikit-learn-contrib/imbalanced-learn/issues/662
0.12.3,check that the categorical feature is not random but correspond to the
0.12.3,categories seen in the minority class samples
0.12.3,TODO: only use `sparse_output` when sklearn >= 1.2
0.12.3,TODO(0.13): remove this test
0.12.3,overall check for SMOTEN
0.12.3,check if the SMOTEN resample data as expected
0.12.3,"we generate data such that ""not apple"" will be the minority class and"
0.12.3,"samples from this class will be generated. We will force the ""blue"""
0.12.3,"category to be associated with this class. Therefore, the new generated"
0.12.3,"samples should as well be from the ""blue"" category."
0.12.3,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.3,Christos Aridas
0.12.3,License: MIT
0.12.3,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.3,Christos Aridas
0.12.3,License: MIT
0.12.3,FIXME: we should use to_numpy with pandas >= 0.25
0.12.3,check the random over-sampling with a multiclass problem
0.12.3,check that resampling with heterogeneous dtype is working with basic
0.12.3,resampling
0.12.3,check that we can oversample even with missing or infinite data
0.12.3,regression tests for #605
0.12.3,check that we raise an error when heterogeneous dtype data are given
0.12.3,and a smoothed bootstrap is requested
0.12.3,check that smoothed bootstrap is working for numerical array
0.12.3,check that a shrinkage factor of 0 is equivalent to not create a smoothed
0.12.3,bootstrap
0.12.3,check the behaviour of the shrinkage parameter
0.12.3,the covariance of the data generated with the larger shrinkage factor
0.12.3,should also be larger.
0.12.3,check the validation of the shrinkage parameter
0.12.3,check that m_neighbors is properly set. Regression test for:
0.12.3,https://github.com/scikit-learn-contrib/imbalanced-learn/issues/568
0.12.3,FIXME: to be removed in 0.12
0.12.3,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.3,Christos Aridas
0.12.3,License: MIT
0.12.3,shuffle the indices since the sampler are packing them by class
0.12.3,helper functions
0.12.3,input and output
0.12.3,build the model and weights
0.12.3,"build the loss, predict, and train operator"
0.12.3,Initialization of all variables in the graph
0.12.3,"For each epoch, run accuracy on train and test"
0.12.3,helper functions
0.12.3,input and output
0.12.3,build the model and weights
0.12.3,"build the loss, predict, and train operator"
0.12.3,Initialization of all variables in the graph
0.12.3,"For each epoch, run accuracy on train and test"
0.12.3,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.3,License: MIT
0.12.3,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.3,Fernando Nogueira
0.12.3,Christos Aridas
0.12.3,License: MIT
0.12.3,find which class to not consider
0.12.3,there is a Tomek link between two samples if they are both nearest
0.12.3,neighbors of each others.
0.12.3,Find the nearest neighbour of every point
0.12.3,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.3,Christos Aridas
0.12.3,License: MIT
0.12.3,Randomly get one sample from the majority class
0.12.3,Generate the index to select
0.12.3,Create the set C - One majority samples and all minority
0.12.3,Create the set S - all majority samples
0.12.3,fit knn on C
0.12.3,Check each sample in S if we keep it or drop it
0.12.3,Do not select sample which are already well classified
0.12.3,Classify on S
0.12.3,If the prediction do not agree with the true label
0.12.3,append it in C_x
0.12.3,Keep the index for later
0.12.3,Update C
0.12.3,fit a knn on C
0.12.3,This experimental to speed up the search
0.12.3,Classify all the element in S and avoid to test the
0.12.3,well classified elements
0.12.3,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.3,Dayvid Oliveira
0.12.3,Christos Aridas
0.12.3,License: MIT
0.12.3,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.3,Christos Aridas
0.12.3,License: MIT
0.12.3,Compute the distance considering the farthest neighbour
0.12.3,Sort the list of distance and get the index
0.12.3,Throw a warning to tell the user that we did not have enough samples
0.12.3,to select and that we just select everything
0.12.3,Select the desired number of samples
0.12.3,idx_tmp is relative to the feature selected in the
0.12.3,previous step and we need to find the indirection
0.12.3,fmt: off
0.12.3,fmt: on
0.12.3,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.3,Christos Aridas
0.12.3,License: MIT
0.12.3,select a sample from the current class
0.12.3,create the set composed of all minority samples and one
0.12.3,sample from the current class.
0.12.3,create the set S with removing the seed from S
0.12.3,since that it will be added anyway
0.12.3,apply Tomek cleaning
0.12.3,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.3,Dayvid Oliveira
0.12.3,Christos Aridas
0.12.3,License: MIT
0.12.3,Check the stopping criterion
0.12.3,1. If there is no changes for the vector y
0.12.3,2. If the number of samples in the other class become inferior to
0.12.3,the number of samples in the majority class
0.12.3,3. If one of the class is disappearing
0.12.3,Case 1
0.12.3,Case 2
0.12.3,Case 3
0.12.3,Check the stopping criterion
0.12.3,1. If the number of samples in the other class become inferior to
0.12.3,the number of samples in the majority class
0.12.3,2. If one of the class is disappearing
0.12.3,Case 1else:
0.12.3,overwrite b_min_bec_maj
0.12.3,Case 2
0.12.3,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.3,Christos Aridas
0.12.3,License: MIT
0.12.3,backward compatibility when passing a NearestNeighbors object
0.12.3,clean the neighborhood
0.12.3,compute which classes to consider for cleaning for the A2 group
0.12.3,add an additional sample since the query points contains the original dataset
0.12.3,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.3,Christos Aridas
0.12.3,License: MIT
0.12.3,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.3,Christos Aridas
0.12.3,License: MIT
0.12.3,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.3,Christos Aridas
0.12.3,License: MIT
0.12.3,"with a large `threshold_cleaning`, the algorithm is equivalent to ENN"
0.12.3,set a threshold that we should consider only the class #2
0.12.3,making the threshold slightly smaller to take into account class #1
0.12.3,we should have a more aggressive cleaning with n_neighbors is larger
0.12.3,TODO: remove in 0.14
0.12.3,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.3,Christos Aridas
0.12.3,License: MIT
0.12.3,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.3,Christos Aridas
0.12.3,License: MIT
0.12.3,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.3,Christos Aridas
0.12.3,License: MIT
0.12.3,FIXME: we should use to_numpy with pandas >= 0.25
0.12.3,check that we can undersample even with missing or infinite data
0.12.3,regression tests for #605
0.12.3,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.3,Christos Aridas
0.12.3,License: MIT
0.12.3,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.3,Christos Aridas
0.12.3,License: MIT
0.12.3,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.3,Christos Aridas
0.12.3,License: MIT
0.12.3,TODO: remove in 0.14
0.12.3,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.3,Christos Aridas
0.12.3,License: MIT
0.12.3,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.3,Christos Aridas
0.12.3,License: MIT
0.12.3,TODO: remove in 0.14
0.12.3,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.3,Fernando Nogueira
0.12.3,Christos Aridas
0.12.3,License: MIT
0.12.3,check that the samples selecting by the hard voting corresponds to the
0.12.3,targeted class
0.12.3,non-regression test for:
0.12.3,https://github.com/scikit-learn-contrib/imbalanced-learn/issues/738
0.12.3,Generate valid values for the required parameters
0.12.3,The parameters `*args` and `**kwargs` are ignored since we cannot generate
0.12.3,constraints.
0.12.3,check that there is a constraint for each parameter
0.12.3,this object does not have a valid type for sure for all params
0.12.3,This parameter is not validated
0.12.3,"First, check that the error is raised if param doesn't match any valid type."
0.12.3,"Then, for constraints that are more than a type constraint, check that the"
0.12.3,error is raised if param does match a valid type but does not match any valid
0.12.3,value for this type.
0.12.3,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.3,Christos Aridas
0.12.3,License: MIT
0.12.3,test that all_estimators doesn't find abstract classes.
0.12.3,"For NearMiss, let's check the three algorithms"
0.12.3,Common tests for estimator instances
0.12.3,Authors: Alexandre Gramfort <alexandre.gramfort@inria.fr>
0.12.3,Raghav RV <rvraghav93@gmail.com>
0.12.3,License: BSD 3 clause
0.12.3,scikit-learn >= 1.2
0.12.3,"walk_packages() ignores DeprecationWarnings, now we need to ignore"
0.12.3,FutureWarnings
0.12.3,"mypy error: Module has no attribute ""__path__"""
0.12.3,functions to ignore args / docstring of
0.12.3,Methods where y param should be ignored if y=None by default
0.12.3,numpydoc 0.8.0's docscrape tool raises because of collections.abc under
0.12.3,Python 3.7
0.12.3,Test module docstring formatting
0.12.3,Skip test if numpydoc is not found
0.12.3,XXX unreached code as of v0.22
0.12.3,"pytest tooling, not part of the scikit-learn API"
0.12.3,Exclude non-scikit-learn classes
0.12.3,Now skip docstring test for y when y is None
0.12.3,by default for API reason
0.12.3,Exclude imported functions
0.12.3,Don't test private methods / functions
0.12.3,Test that there are no tabs in our source files
0.12.3,because we don't import
0.12.3,Minimal / degenerate instances: only useful to test the docstrings.
0.12.3,"As certain attributes are present ""only"" if a certain parameter is"
0.12.3,"provided, this checks if the word ""only"" is present in the attribute"
0.12.3,"description, and if not the attribute is required to be present."
0.12.3,ignore deprecation warnings
0.12.3,attributes
0.12.3,properties
0.12.3,ignore properties that raises an AttributeError and deprecated
0.12.3,properties
0.12.3,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.3,License: MIT
0.12.3,check that we can let a pass a regression variable by turning down the
0.12.3,validation
0.12.3,Check that the validation is bypass when calling `fit`
0.12.3,Non-regression test for:
0.12.3,https://github.com/scikit-learn-contrib/imbalanced-learn/issues/782
0.12.3,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.3,Christos Aridas
0.12.3,License: MIT
0.12.3,store timestamp to figure out whether the result of 'fit' has been
0.12.3,cached or not
0.12.3,store timestamp to figure out whether the result of 'fit' has been
0.12.3,cached or not
0.12.3,Pipeline accepts steps as tuple
0.12.3,Test the various init parameters of the pipeline.
0.12.3,Check that we can't instantiate pipelines with objects without fit
0.12.3,method
0.12.3,Smoke test with only an estimator
0.12.3,Check that params are set
0.12.3,Smoke test the repr:
0.12.3,Test with two objects
0.12.3,Check that we can't instantiate with non-transformers on the way
0.12.3,"Note that NoTrans implements fit, but not transform"
0.12.3,Check that params are set
0.12.3,Smoke test the repr:
0.12.3,Check that params are not set when naming them wrong
0.12.3,Test clone
0.12.3,"Check that apart from estimators, the parameters are the same"
0.12.3,Remove estimators that where copied
0.12.3,Test the various methods of the pipeline (anova).
0.12.3,Test with Anova + LogisticRegression
0.12.3,Test that the pipeline can take fit parameters
0.12.3,classifier should return True
0.12.3,and transformer params should not be changed
0.12.3,invalid parameters should raise an error message
0.12.3,Pipeline should pass sample_weight
0.12.3,When sample_weight is None it shouldn't be passed
0.12.3,Test pipeline raises set params error message for nested models.
0.12.3,nested model check
0.12.3,Test the various methods of the pipeline (pca + svm).
0.12.3,Test with PCA + SVC
0.12.3,Test the various methods of the pipeline (preprocessing + svm).
0.12.3,check shapes of various prediction functions
0.12.3,test that the fit_predict method is implemented on a pipeline
0.12.3,test that the fit_predict on pipeline yields same results as applying
0.12.3,transform and clustering steps separately
0.12.3,"As pipeline doesn't clone estimators on construction,"
0.12.3,it must have its own estimators
0.12.3,first compute the transform and clustering step separately
0.12.3,use a pipeline to do the transform and clustering in one step
0.12.3,tests that a pipeline does not have fit_predict method when final
0.12.3,step of pipeline does not have fit_predict defined
0.12.3,tests that Pipeline passes fit_params to intermediate steps
0.12.3,when fit_predict is invoked
0.12.3,Test whether pipeline works with a transformer at the end.
0.12.3,Also test pipeline.transform and pipeline.inverse_transform
0.12.3,test transform and fit_transform:
0.12.3,Test whether pipeline works with a transformer missing fit_transform
0.12.3,test fit_transform:
0.12.3,Directly setting attr
0.12.3,Using set_params
0.12.3,Using set_params to replace single step
0.12.3,With invalid data
0.12.3,Test setting Pipeline steps to None
0.12.3,"for other methods, ensure no AttributeErrors on None:"
0.12.3,mult2 and mult3 are active
0.12.3,Check 'passthrough' step at construction time
0.12.3,Test with Transformer + SVC
0.12.3,Memoize the transformer at the first fit
0.12.3,Get the time stamp of the tranformer in the cached pipeline
0.12.3,Check that cached_pipe and pipe yield identical results
0.12.3,Check that we are reading the cache while fitting
0.12.3,a second time
0.12.3,Check that cached_pipe and pipe yield identical results
0.12.3,Create a new pipeline with cloned estimators
0.12.3,Check that even changing the name step does not affect the cache hit
0.12.3,Check that cached_pipe and pipe yield identical results
0.12.3,Test with Transformer + SVC
0.12.3,Memoize the transformer at the first fit
0.12.3,Get the time stamp of the tranformer in the cached pipeline
0.12.3,Check that cached_pipe and pipe yield identical results
0.12.3,Check that we are reading the cache while fitting
0.12.3,a second time
0.12.3,Check that cached_pipe and pipe yield identical results
0.12.3,Create a new pipeline with cloned estimators
0.12.3,Check that even changing the name step does not affect the cache hit
0.12.3,Check that cached_pipe and pipe yield identical results
0.12.3,Test the various methods of the pipeline (pca + svm).
0.12.3,Test with PCA + SVC
0.12.3,Test the various methods of the pipeline (pca + svm).
0.12.3,Test with PCA + SVC
0.12.3,Test whether pipeline works with a sampler at the end.
0.12.3,Also test pipeline.sampler
0.12.3,test transform and fit_transform:
0.12.3,We round the value near to zero. It seems that PCA has some issue
0.12.3,with that
0.12.3,Test whether pipeline works with a sampler at the end.
0.12.3,Also test pipeline.sampler
0.12.3,Test pipeline using None as preprocessing step and a classifier
0.12.3,"Test pipeline using None, RUS and a classifier"
0.12.3,"Test pipeline using RUS, None and a classifier"
0.12.3,Test pipeline using None step and a sampler
0.12.3,Test pipeline using None and a transformer that implements transform and
0.12.3,inverse_transform
0.12.3,Test the various methods of the pipeline (anova).
0.12.3,Test with RandomUnderSampling + Anova + LogisticRegression
0.12.3,Test the various methods of the pipeline (anova).
0.12.3,Test the various methods of the pipeline (anova).
0.12.3,Test with RandomUnderSampling + Anova + LogisticRegression
0.12.3,tests that Pipeline passes predict_params to the final estimator
0.12.3,when predict is invoked
0.12.3,Test that the score_samples method is implemented on a pipeline.
0.12.3,Test that the score_samples method on pipeline yields same results as
0.12.3,applying transform and score_samples steps separately.
0.12.3,Check the shapes
0.12.3,Check the values
0.12.3,Test that a pipeline does not have score_samples method when the final
0.12.3,step of the pipeline does not have score_samples defined.
0.12.3,Test that the score_samples method is implemented on a pipeline.
0.12.3,Test that the score_samples method on pipeline yields same results as
0.12.3,applying transform and score_samples steps separately.
0.12.3,Check the shapes
0.12.3,Check the values
0.12.3,transformer will not change `y` and sampler will always preserve the type of `y`
0.12.3,transformer will not change `y` and sampler will always preserve the type of `y`
0.12.3,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.3,Christos Aridas
0.12.3,License: MIT
0.12.3,TODO: Remove when SciPy 1.9 is the minimum supported version
0.12.3,TODO: Remove when scikit-learn 1.1 is the minimum supported version
0.12.3,TODO: remove when scikit-learn minimum version is 1.3
0.12.3,we don't want to validate again for each call to partial_fit
0.12.3,TODO: remove when scikit-learn minimum version is 1.3
0.12.3,"Likely a pandas DataFrame, we explicitly check the type to confirm."
0.12.3,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.3,License: MIT
0.12.3,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.3,License: MIT
0.12.3,Adapated from scikit-learn
0.12.3,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.3,License: MIT
0.12.3,scikit-learn >= 1.2
0.12.3,TODO: remove in 0.13
0.12.3,future default in 0.13
0.12.3,we don't filter samplers based on their tag here because we want to make
0.12.3,sure that the fitted attribute does not exist if the tag is not
0.12.3,stipulated
0.12.3,trigger our checks if this is a SamplerMixin
0.12.3,should raise warning if the target is continuous (we cannot raise error)
0.12.3,if the target is multilabel then we should raise an error
0.12.3,IHT does not enforce the number of samples but provide a number
0.12.3,of samples the closest to the desired target.
0.12.3,in this test we will force all samplers to not change the class 1
0.12.3,check that sparse matrices can be passed through the sampler leading to
0.12.3,the same results than dense
0.12.3,Check that the samplers handle pandas dataframe and pandas series
0.12.3,check that we return the same type for dataframes or series types
0.12.3,FIXME: we should use to_numpy with pandas >= 0.25
0.12.3,Check that the samplers handle pandas dataframe and pandas series
0.12.3,check that we return the same type for dataframes or series types
0.12.3,FIXME: we should use to_numpy with pandas >= 0.25
0.12.3,Check that the can samplers handle simple lists
0.12.3,Check that multiclass target lead to the same results than OVA encoding
0.12.3,Cast X and y to not default dtype
0.12.3,Non-regression test for #709
0.12.3,https://github.com/scikit-learn-contrib/imbalanced-learn/issues/709
0.12.3,Check that an informative error is raised when the value of a constructor
0.12.3,parameter does not have an appropriate type or value.
0.12.3,check that there is a constraint for each parameter
0.12.3,this object does not have a valid type for sure for all params
0.12.3,This parameter is not validated
0.12.3,"First, check that the error is raised if param doesn't match any valid type."
0.12.3,the method is not accessible with the current set of parameters
0.12.3,The estimator is a label transformer and take only `y`
0.12.3,"Then, for constraints that are more than a type constraint, check that the"
0.12.3,error is raised if param does match a valid type but does not match any valid
0.12.3,value for this type.
0.12.3,the method is not accessible with the current set of parameters
0.12.3,The estimator is a label transformer and take only `y`
0.12.3,Check that calling `fit` does not raise any warnings about feature names.
0.12.3,Only check imblearn estimators for feature_names_in_ in docstring
0.12.3,partial_fit checks on second call
0.12.3,Do not call partial fit if early_stopping is on
0.12.3,input_features names is not the same length as n_features_in_
0.12.3,error is raised when `input_features` do not match feature_names_in
0.12.3,Adapted from scikit-learn
0.12.3,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.3,License: MIT
0.12.3,Ignore deprecation warnings triggered at import time and from walking
0.12.3,packages
0.12.3,get rid of abstract base classes
0.12.3,get rid of sklearn estimators which have been imported in some classes
0.12.3,"drop duplicates, sort for reproducibility"
0.12.3,itemgetter is used to ensure the sort does not extend to the 2nd item of
0.12.3,the tuple
0.12.3,Author: Adrin Jalali <adrin.jalali@gmail.com>
0.12.3,License: BSD 3 clause
0.12.3,Only the following methods are supported in the routing mechanism. Adding new
0.12.3,methods at the moment involves monkeypatching this list.
0.12.3,"Note that if this list is changed or monkeypatched, the corresponding method"
0.12.3,needs to be added under a TYPE_CHECKING condition like the one done here in
0.12.3,_MetadataRequester
0.12.3,These methods are a composite of other methods and one cannot set their
0.12.3,requests directly. Instead they should be set by setting the requests of the
0.12.3,simple methods which make the composite ones.
0.12.3,Request values
0.12.3,==============
0.12.3,"Each request value needs to be one of the following values, or an alias."
0.12.3,this is used in `__metadata_request__*` attributes to indicate that a
0.12.3,metadata is not present even though it may be present in the
0.12.3,corresponding method's signature.
0.12.3,"this is used whenever a default value is changed, and therefore the user"
0.12.3,"should explicitly set the value, otherwise a warning is shown. An example"
0.12.3,"is when a meta-estimator is only a router, but then becomes also a"
0.12.3,consumer in a new release.
0.12.3,this is the default used in `set_{method}_request` methods to indicate no
0.12.3,change requested by the user.
0.12.3,item is only an alias if it's a valid identifier
0.12.3,Metadata Request for Simple Consumers
0.12.3,=====================================
0.12.3,This section includes MethodMetadataRequest and MetadataRequest which are
0.12.3,used in simple consumers.
0.12.3,this is here for us to use this attribute's value instead of doing
0.12.3,"`isinstance` in our checks, so that we avoid issues when people vendor"
0.12.3,this file instead of using it directly from scikit-learn.
0.12.3,Called when the default attribute access fails with an AttributeError
0.12.3,(either __getattribute__() raises an AttributeError because name is
0.12.3,not an instance attribute or an attribute in the class tree for self;
0.12.3,or __get__() of a name property raises AttributeError). This method
0.12.3,should either return the (computed) attribute value or raise an
0.12.3,AttributeError exception.
0.12.3,https://docs.python.org/3/reference/datamodel.html#object.__getattr__
0.12.3,Metadata Request for Routers
0.12.3,============================
0.12.3,This section includes all objects required for MetadataRouter which is used
0.12.3,"in routers, returned by their ``get_metadata_routing``."
0.12.3,"This namedtuple is used to store a (mapping, routing) pair. Mapping is a"
0.12.3,"MethodMapping object, and routing is the output of `get_metadata_routing`."
0.12.3,MetadataRouter stores a collection of these namedtuples.
0.12.3,A namedtuple storing a single method route. A collection of these namedtuples
0.12.3,is stored in a MetadataRouter.
0.12.3,this is here for us to use this attribute's value instead of doing
0.12.3,"`isinstance`` in our checks, so that we avoid issues when people vendor"
0.12.3,this file instead of using it directly from scikit-learn.
0.12.3,`_self_request` is used if the router is also a consumer.
0.12.3,"_self_request, (added using `add_self_request()`) is treated"
0.12.3,differently from the other objects which are stored in
0.12.3,_route_mappings.
0.12.3,"conflicts are okay if the passed objects are the same, but it's"
0.12.3,an issue if they're different objects.
0.12.3,doing this instead of a try/except since an AttributeError could be raised
0.12.3,for other reasons.
0.12.3,Request method
0.12.3,==============
0.12.3,This section includes what's needed for the request method descriptor and
0.12.3,their dynamic generation in a meta class.
0.12.3,These strings are used to dynamically generate the docstrings for
0.12.3,set_{method}_request methods.
0.12.3,we would want to have a method which accepts only the expected args
0.12.3,This makes it possible to use the decorated method as an unbound
0.12.3,"method, for instance when monkeypatching."
0.12.3,https://github.com/scikit-learn/scikit-learn/issues/28632
0.12.3,Replicating python's behavior when positional args are given other
0.12.3,"than `self`, and `self` is only allowed if this method is unbound."
0.12.3,Now we set the relevant attributes of the function so that it seems
0.12.3,"like a normal method to the end user, with known expected arguments."
0.12.3,"This code is never run in runtime, but it's here for type checking."
0.12.3,Type checkers fail to understand that the `set_{method}_request`
0.12.3,"methods are dynamically generated, and they complain that they are"
0.12.3,not defined. We define them here to make type checkers happy.
0.12.3,During type checking analyzers assume this to be True.
0.12.3,The following list of defined methods mirrors the list of methods
0.12.3,in SIMPLE_METHODS.
0.12.3,fmt: off
0.12.3,fmt: on
0.12.3,"if there are any issues in the default values, it will be raised"
0.12.3,when ``get_metadata_routing`` is called. Here we are going to
0.12.3,ignore all the issues such as bad defaults etc.
0.12.3,set ``set_{method}_request``` methods
0.12.3,Here we use `isfunction` instead of `ismethod` because calling `getattr`
0.12.3,on a class instead of an instance returns an unbound function.
0.12.3,"ignore the first parameter of the method, which is usually ""self"""
0.12.3,Then overwrite those defaults with the ones provided in
0.12.3,__metadata_request__* attributes. Defaults set in
0.12.3,__metadata_request__* attributes take precedence over signature
0.12.3,sniffing.
0.12.3,need to go through the MRO since this is a class attribute and
0.12.3,``vars`` doesn't report the parent class attributes. We go through
0.12.3,the reverse of the MRO so that child classes have precedence over
0.12.3,their parents.
0.12.3,we don't check for attr.startswith() since python prefixes attrs
0.12.3,starting with __ with the `_ClassName`.
0.12.3,Process Routing in Routers
0.12.3,==========================
0.12.3,This is almost always the only method used in routers to process and route
0.12.3,given metadata. This is to minimize the boilerplate required in routers.
0.12.3,Here the first two arguments are positional only which makes everything
0.12.3,passed as keyword argument a metadata. The first two args also have an `_`
0.12.3,"prefix to reduce the chances of name collisions with the passed metadata, and"
0.12.3,"since they're positional only, users will never type those underscores."
0.12.3,"If routing is not enabled and kwargs are empty, then we don't have to"
0.12.3,"try doing any routing, we can simply return a structure which returns"
0.12.3,an empty dict on routed_params.ANYTHING.ANY_METHOD.
0.12.3,mypy: ignore-errors
0.12.3,update the docstring of the descriptor
0.12.3,"delegate only on instances, not the classes."
0.12.3,this is to allow access to the docstrings.
0.12.3,This makes it possible to use the decorated method as an
0.12.3,"unbound method, for instance when monkeypatching."
0.12.3,mypy: ignore-errors
0.12.3,Inherits from ValueError and TypeError to keep backward compatibility.
0.12.3,We allow parameters to not have a constraint so that third party
0.12.3,estimators can inherit from sklearn estimators without having to
0.12.3,necessarily use the validation tools.
0.12.3,"this constraint is satisfied, no need to check further."
0.12.3,"No constraint is satisfied, raise with an informative message."
0.12.3,Ignore constraints that we don't want to expose in the error
0.12.3,"message, i.e. options that are for internal purpose or not"
0.12.3,officially supported.
0.12.3,The dict of parameter constraints is set as an attribute of the function
0.12.3,to make it possible to dynamically introspect the constraints for
0.12.3,automatic testing.
0.12.3,Map *args/**kwargs to the function signature
0.12.3,ignore self/cls and positional/keyword markers
0.12.3,"When the function is just a wrapper around an estimator, we allow"
0.12.3,"the function to delegate validation to the estimator, but we"
0.12.3,replace the name of the estimator by the name of the function in
0.12.3,the error message to avoid confusion.
0.12.3,better repr if the bounds were given as integers
0.12.3,we use an interval of Real to ignore np.nan that has its own
0.12.3,constraint
0.12.3,"There's no integer outside (-inf, +inf)"
0.12.3,"bounds are -inf, +inf"
0.12.3,"interval is [-inf, +inf]"
0.12.3,special case for ndarray since it can't be instantiated without
0.12.3,arguments
0.12.3,special case for Integral and Real since they are abstract classes
0.12.3,Author: Alexander L. Hayes <hayesall@iu.edu>
0.12.3,License: MIT
0.12.3,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.3,License: MIT
0.12.3,We lost the y.index during resampling. We can safely use X.index to align
0.12.3,them.
0.12.3,We special case the following error:
0.12.3,https://github.com/scikit-learn-contrib/imbalanced-learn/issues/1055
0.12.3,"There is no easy way to have a generic workaround. Here, we detect"
0.12.3,that we have a column with only null values that is datetime64
0.12.3,(resulting from the np.vstack of the resampling).
0.12.3,try again
0.12.3,_is_neighbors_object(nn_object)
0.12.3,check that all keys in sampling_strategy are also in y
0.12.3,check that there is no negative number
0.12.3,check that all keys in sampling_strategy are also in y
0.12.3,ignore first 'self' argument for instance methods
0.12.3,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.3,Christos Aridas
0.12.3,License: MIT
0.12.3,this function could create an equal number of samples
0.12.3,We pass on purpose a non sorted dictionary and check that the resulting
0.12.3,dictionary is sorted. Refer to issue #428.
0.12.3,DataFrame and DataFrame case
0.12.3,DataFrames and Series case
0.12.3,The * is place before a keyword only argument without a default value
0.12.3,Test that the minimum dependencies in the README.rst file are
0.12.3,consistent with the minimum dependencies defined at the file:
0.12.3,imblearn/_min_dependencies.py
0.12.3,Skip the test if the README.rst file is not available.
0.12.3,"For instance, when installing scikit-learn from wheels"
0.12.3,Author: Alexander L. Hayes <hayesall@iu.edu>
0.12.3,License: MIT
0.12.3,Some helpers for the tests
0.12.3,check in the presence of extra positional and keyword args
0.12.3,outer decorator does not interfere with validation
0.12.3,validated method can be decorated
0.12.3,no validation in init
0.12.3,list and dict are valid params
0.12.3,the list option is not exposed in the error message
0.12.3,"""auto"" and ""warn"" are valid params"
0.12.3,"the ""warn"" option is not exposed in the error message"
0.12.3,True/False and np.bool_(True/False) are valid params
0.12.3,param1 is validated
0.12.3,param2 is not validated: any type is valid.
0.12.3,"does not raise, even though ""b"" is not in the constraints dict and ""a"" is not"
0.12.3,a parameter of the estimator.
0.12.3,does not raise
0.12.3,calls f with a bad parameter type
0.12.3,Validation for g is never skipped.
0.12.3,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.3,License: MIT
0.12.3,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.3,Christos Aridas
0.12.3,License: MIT
0.12.3,check if the filtering is working with a list or a single string
0.12.3,check that all estimators are sampler
0.12.3,check that an error is raised when the type is unknown
0.12.3,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.3,License: MIT
0.12.3,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.3,Christos Aridas
0.12.3,License: MIT
0.12.3,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.3,Christos Aridas
0.12.3,License: MIT
0.12.3,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.3,Christos Aridas
0.12.3,License: MIT
0.12.3,Check if default job count is None
0.12.3,Check if job count is set
0.12.3,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.3,Christos Aridas
0.12.3,License: MIT
0.12.3,Check if default job count is none
0.12.3,Check if job count is set
0.12.3,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.3,License: MIT
0.12.3,scikit-learn >= 1.2
0.12.3,resample before to fit the tree
0.12.3,TODO: remove when the minimum supported version of scikit-learn will be 1.4
0.12.3,support for missing values
0.12.3,TODO: remove when the minimum supported version of scikit-learn will be 1.1
0.12.3,change of signature in scikit-learn 1.1
0.12.3,make a deepcopy to not modify the original dictionary
0.12.3,TODO: remove when the minimum supported version of scikit-learn will be 1.4
0.12.3,use scikit-learn support for monotonic constraints
0.12.3,create an attribute for compatibility with other scikit-learn tools such
0.12.3,as HTML representation.
0.12.3,TODO: remove in 0.13
0.12.3,Validate or convert input data
0.12.3,TODO: remove when the minimum supported version of scipy will be 1.4
0.12.3,Support for missing values
0.12.3,TODO: remove when the minimum supported version of scikit-learn will be 1.4
0.12.3,_compute_missing_values_in_feature_mask checks if X has missing values and
0.12.3,will raise an error if the underlying tree base estimator can't handle
0.12.3,missing values. Only the criterion is required to determine if the tree
0.12.3,supports missing values.
0.12.3,Pre-sort indices to avoid that each individual tree of the
0.12.3,ensemble sorts the indices.
0.12.3,reshape is necessary to preserve the data contiguity against vs
0.12.3,"[:, np.newaxis] that does not."
0.12.3,Get bootstrap sample size
0.12.3,Check parameters
0.12.3,"Free allocated memory, if any"
0.12.3,We draw from the random state to get the random state we
0.12.3,would have got if we hadn't used a warm_start.
0.12.3,Parallel loop: we prefer the threading backend as the Cython code
0.12.3,for fitting the trees is internally releasing the Python GIL
0.12.3,making threading more efficient than multiprocessing in
0.12.3,"that case. However, we respect any parallel_backend contexts set"
0.12.3,"at a higher level, since correctness does not rely on using"
0.12.3,threads.
0.12.3,Collect newly grown trees
0.12.3,Create pipeline with the fitted samplers and trees
0.12.3,FIXME: we could consider to support multiclass-multioutput if
0.12.3,we introduce or reuse a constructor parameter (e.g.
0.12.3,oob_score) allowing our user to pass a callable defining the
0.12.3,scoring strategy on OOB sample.
0.12.3,Decapsulate classes_ attributes
0.12.3,drop the n_outputs axis if there is a single output
0.12.3,Prediction requires X to be in CSR format
0.12.3,n_classes_ is a ndarray at this stage
0.12.3,all the supported type of target will have the same number of
0.12.3,classes in all outputs
0.12.3,"for regression, n_classes_ does not exist and we create an empty"
0.12.3,axis to be consistent with the classification case and make
0.12.3,the array operations compatible with the 2 settings
0.12.3,TODO: remove when supporting scikit-learn>=1.2
0.12.3,make a deepcopy to not modify the original dictionary
0.12.3,TODO: remove when minimum supported version of scikit-learn is 1.4
0.12.3,SAMME-R requires predict_proba-enabled estimators
0.12.3,Instances incorrectly classified
0.12.3,Error fraction
0.12.3,Stop if classification is perfect
0.12.3,Construct y coding as described in Zhu et al [2]:
0.12.3,
0.12.3,y_k = 1 if c == k else -1 / (K - 1)
0.12.3,
0.12.3,"where K == n_classes_ and c, k in [0, K) are indices along the second"
0.12.3,axis of the y coding with c being the index corresponding to the true
0.12.3,class label.
0.12.3,Displace zero probabilities so the log is defined.
0.12.3,Also fix negative elements which may occur with
0.12.3,negative sample weights.
0.12.3,Boost weight using multi-class AdaBoost SAMME.R alg
0.12.3,Only boost the weights if it will fit again
0.12.3,Only boost positive weights
0.12.3,Instances incorrectly classified
0.12.3,Error fraction
0.12.3,Stop if classification is perfect
0.12.3,Stop if the error is at least as bad as random guessing
0.12.3,Boost weight using multi-class AdaBoost SAMME alg
0.12.3,Only boost the weights if I will fit again
0.12.3,Only boost positive weights
0.12.3,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.3,Christos Aridas
0.12.3,License: MIT
0.12.3,scikit-learn >= 1.2
0.12.3,make a deepcopy to not modify the original dictionary
0.12.3,TODO: remove when minimum supported version of scikit-learn is 1.4
0.12.3,TODO: remove when supporting scikit-learn>=1.2
0.12.3,overwrite the base class method by disallowing `sample_weight`
0.12.3,RandomUnderSampler is not supporting sample_weight. We need to pass
0.12.3,None.
0.12.3,TODO: remove when minimum supported version of scikit-learn is 1.1
0.12.3,Check data
0.12.3,Parallel loop
0.12.3,Reduce
0.12.3,The base class require to have the attribute defined. For scikit-learn
0.12.3,"> 1.2, we are going to raise an error."
0.12.3,TODO: remove when minimum supported version of scikit-learn is 1.5
0.12.3,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.3,Christos Aridas
0.12.3,License: MIT
0.12.3,scikit-learn >= 1.2
0.12.3,make a deepcopy to not modify the original dictionary
0.12.3,TODO: remove when minimum supported version of scikit-learn is 1.4
0.12.3,TODO: remove when supporting scikit-learn>=1.2
0.12.3,overwrite the base class method by disallowing `sample_weight`
0.12.3,the sampler needs to be validated before to call _fit because
0.12.3,_validate_y is called before _validate_estimator and would require
0.12.3,to know which type of sampler we are using.
0.12.3,RandomUnderSampler is not supporting sample_weight. We need to pass
0.12.3,None.
0.12.3,TODO: remove when minimum supported version of scikit-learn is 1.1
0.12.3,Check data
0.12.3,Parallel loop
0.12.3,Reduce
0.12.3,The base class require to have the attribute defined. For scikit-learn
0.12.3,"> 1.2, we are going to raise an error."
0.12.3,check that we have an ensemble of samplers and estimators with a
0.12.3,consistent size
0.12.3,each sampler in the ensemble should have different random state
0.12.3,each estimator in the ensemble should have different random state
0.12.3,check the consistency of the feature importances
0.12.3,check the consistency of the prediction outpus
0.12.3,Predictions should be the same when sample_weight are all ones
0.12.3,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.3,Christos Aridas
0.12.3,License: MIT
0.12.3,Check classification for various parameter settings.
0.12.3,Test that bootstrapping samples generate non-perfect base estimators.
0.12.3,"without bootstrap, all trees are perfect on the training set"
0.12.3,disable the resampling by passing an empty dictionary.
0.12.3,"with bootstrap, trees are no longer perfect on the training set"
0.12.3,Test that bootstrapping features may generate duplicate features.
0.12.3,Predict probabilities.
0.12.3,Normal case
0.12.3,"Degenerate case, where some classes are missing"
0.12.3,Check that oob prediction is a good estimation of the generalization
0.12.3,error.
0.12.3,Test with few estimators
0.12.3,Check singleton ensembles.
0.12.3,Check that bagging ensembles can be grid-searched.
0.12.3,Transform iris into a binary classification task
0.12.3,Grid search with scoring based on decision_function
0.12.3,Check estimator and its default values.
0.12.3,Test if fitting incrementally with warm start gives a forest of the
0.12.3,right size and the same results as a normal fit.
0.12.3,Test if warm start'ed second fit with smaller n_estimators raises error.
0.12.3,Test that nothing happens when fitting without increasing n_estimators
0.12.3,"modify X to nonsense values, this should not change anything"
0.12.3,warm started classifier with 5+5 estimators should be equivalent to
0.12.3,one classifier with 10 estimators
0.12.3,Check using oob_score and warm_start simultaneously fails
0.12.3,"Make sure OOB scores are identical when random_state, estimator, and"
0.12.3,training data are fixed and fitting is done twice
0.12.3,Check that format of estimators_samples_ is correct and that results
0.12.3,generated at fit time can be identically reproduced at a later time
0.12.3,using data saved in object attributes.
0.12.3,remap the y outside of the BalancedBaggingclassifier
0.12.3,"_, y = np.unique(y, return_inverse=True)"
0.12.3,Get relevant attributes
0.12.3,Test for correct formatting
0.12.3,Re-fit single estimator to test for consistent sampling
0.12.3,Make sure validated max_samples and original max_samples are identical
0.12.3,when valid integer max_samples supplied by user
0.12.3,check that we can pass any kind of sampler to a bagging classifier
0.12.3,check that we have balanced class with the right counts of class
0.12.3,sample depending on the sampling strategy
0.12.3,check that we can provide a FunctionSampler in BalancedBaggingClassifier
0.12.3,find the minority and majority classes
0.12.3,compute the number of sample to draw from the majority class using
0.12.3,a negative binomial distribution
0.12.3,draw randomly with or without replacement
0.12.3,Roughly Balanced Bagging
0.12.3,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.3,Christos Aridas
0.12.3,License: MIT
0.12.3,Generate a global dataset to use
0.12.3,Check classification for various parameter settings.
0.12.3,test the different prediction function
0.12.3,Check estimator and its default values.
0.12.3,Test if fitting incrementally with warm start gives a forest of the
0.12.3,right size and the same results as a normal fit.
0.12.3,Test if warm start'ed second fit with smaller n_estimators raises error.
0.12.3,Test that nothing happens when fitting without increasing n_estimators
0.12.3,"modify X to nonsense values, this should not change anything"
0.12.3,warm started classifier with 5+5 estimators should be equivalent to
0.12.3,one classifier with 10 estimators
0.12.3,Check warning if not enough estimators
0.12.3,First fit with no restriction on max samples
0.12.3,Second fit with max samples restricted to just 2
0.12.3,Regression test for #655: check that the oob score is closed to 0.5
0.12.3,a binomial experiment.
0.12.3,TODO: remove in 0.13
0.12.3,Create dataset with missing values
0.12.3,Train forest with missing values
0.12.3,Train forest without missing values
0.12.3,Score is still 80 percent of the forest's score that had no missing values
0.12.3,Create a predictive feature using `y` and with some noise
0.12.3,Author: Guillaume Lemaitre
0.12.3,License: BSD 3 clause
0.12.3,"The index start at one, then we need to remove one"
0.12.3,to not have issue with the indexing.
0.12.3,go through the list and check if the data are available
0.12.3,Authors: Dayvid Oliveira
0.12.3,Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.3,Christos Aridas
0.12.3,License: MIT
0.12.3,restrict ratio to be a dict or a callable
0.12.3,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.3,Christos Aridas
0.12.3,License: MIT
0.12.3,"we are reusing part of utils.check_sampling_strategy, however this is not"
0.12.3,cover in the common tests so we will repeat it here
0.12.3,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.3,Christos Aridas
0.12.3,License: MIT
0.12.3,This is a trick to avoid an error during tests collection with pytest. We
0.12.3,avoid the error when importing the package raise the error at the moment of
0.12.3,creating the instance.
0.12.3,This is a trick to avoid an error during tests collection with pytest. We
0.12.3,avoid the error when importing the package raise the error at the moment of
0.12.3,creating the instance.
0.12.3,flag for keras sequence duck-typing
0.12.3,shuffle the indices since the sampler are packing them by class
0.12.2,This file is here so that when running from the root folder
0.12.2,./imblearn is added to sys.path by pytest.
0.12.2,See https://docs.pytest.org/en/latest/pythonpath.html for more details.
0.12.2,"For example, this allows to build extensions in place and run pytest"
0.12.2,doc/modules/clustering.rst and use imblearn from the local folder
0.12.2,rather than the one from site-packages.
0.12.2,! /usr/bin/env python
0.12.2,Python 2 compat: just to be able to declare that Python >=3.7 is needed.
0.12.2,This is a bit (!) hackish: we are setting a global variable so that the
0.12.2,main imblearn __init__ can detect if it is being loaded by the setup
0.12.2,"routine, to avoid attempting to load components that aren't built yet:"
0.12.2,the numpy distutils extensions that are used by imbalanced-learn to
0.12.2,recursively build the compiled extensions in sub-packages is based on the
0.12.2,Python import machinery.
0.12.2,get __version__ from _version.py
0.12.2,-*- coding: utf-8 -*-
0.12.2,
0.12.2,"imbalanced-learn documentation build configuration file, created by"
0.12.2,sphinx-quickstart on Mon Jan 18 14:44:12 2016.
0.12.2,
0.12.2,This file is execfile()d with the current directory set to its
0.12.2,containing dir.
0.12.2,
0.12.2,Note that not all possible configuration values are present in this
0.12.2,autogenerated file.
0.12.2,
0.12.2,All configuration values have a default; values that are commented out
0.12.2,serve to show the default.
0.12.2,"If extensions (or modules to document with autodoc) are in another directory,"
0.12.2,add these directories to sys.path here. If the directory is relative to the
0.12.2,"documentation root, use os.path.abspath to make it absolute, like shown here."
0.12.2,-- General configuration ------------------------------------------------
0.12.2,"If your documentation needs a minimal Sphinx version, state it here."
0.12.2,needs_sphinx = '1.0'
0.12.2,"Add any Sphinx extension module names here, as strings. They can be"
0.12.2,extensions coming with Sphinx (named 'sphinx.ext.*') or your custom
0.12.2,ones.
0.12.2,Specify how to identify the prompt when copying code snippets
0.12.2,"Add any paths that contain templates here, relative to this directory."
0.12.2,The suffix of source filenames.
0.12.2,The master toctree document.
0.12.2,General information about the project.
0.12.2,"The version info for the project you're documenting, acts as replacement for"
0.12.2,"|version| and |release|, also used in various other places throughout the"
0.12.2,built documents.
0.12.2,
0.12.2,The short X.Y version.
0.12.2,"The full version, including alpha/beta/rc tags."
0.12.2,"List of patterns, relative to source directory, that match files and"
0.12.2,directories to ignore when looking for source files.
0.12.2,The reST default role (used for this markup: `text`) to use for all
0.12.2,documents.
0.12.2,"If true, '()' will be appended to :func: etc. cross-reference text."
0.12.2,The name of the Pygments (syntax highlighting) style to use.
0.12.2,-- Options for HTML output ----------------------------------------------
0.12.2,The theme to use for HTML and HTML Help pages.  See the documentation for
0.12.2,a list of builtin themes.
0.12.2,"""twitter_url"": ""https://twitter.com/pandas_dev"","
0.12.2,"""navbar_align"": ""right"",  # For testing that the navbar items align properly"
0.12.2,"Add any paths that contain custom static files (such as style sheets) here,"
0.12.2,"relative to this directory. They are copied after the builtin static files,"
0.12.2,"so a file named ""default.css"" will overwrite the builtin ""default.css""."
0.12.2,Output file base name for HTML help builder.
0.12.2,-- Options for autodoc ------------------------------------------------------
0.12.2,generate autosummary even if no references
0.12.2,-- Options for numpydoc -----------------------------------------------------
0.12.2,this is needed for some reason...
0.12.2,see https://github.com/numpy/numpydoc/issues/69
0.12.2,-- Options for sphinxcontrib-bibtex -----------------------------------------
0.12.2,bibtex file
0.12.2,-- Options for intersphinx --------------------------------------------------
0.12.2,intersphinx configuration
0.12.2,-- Options for sphinx-gallery -----------------------------------------------
0.12.2,Generate the plot for the gallery
0.12.2,sphinx-gallery configuration
0.12.2,-- Options for github link for what's new -----------------------------------
0.12.2,Config for sphinx_issues
0.12.2,The following is used by sphinx.ext.linkcode to provide links to github
0.12.2,-- Options for LaTeX output ---------------------------------------------
0.12.2,The paper size ('letterpaper' or 'a4paper').
0.12.2,"'papersize': 'letterpaper',"
0.12.2,"The font size ('10pt', '11pt' or '12pt')."
0.12.2,"'pointsize': '10pt',"
0.12.2,Additional stuff for the LaTeX preamble.
0.12.2,"'preamble': '',"
0.12.2,Grouping the document tree into LaTeX files. List of tuples
0.12.2,"(source start file, target name, title,"
0.12.2,"author, documentclass [howto, manual, or own class])."
0.12.2,-- Options for manual page output ---------------------------------------
0.12.2,"If false, no module index is generated."
0.12.2,latex_domain_indices = True
0.12.2,One entry per manual page. List of tuples
0.12.2,"(source start file, name, description, authors, manual section)."
0.12.2,"If true, show URL addresses after external links."
0.12.2,man_show_urls = False
0.12.2,-- Options for Texinfo output -------------------------------------------
0.12.2,Grouping the document tree into Texinfo files. List of tuples
0.12.2,"(source start file, target name, title, author,"
0.12.2,"dir menu entry, description, category)"
0.12.2,-- Dependencies generation ----------------------------------------------
0.12.2,get length of header
0.12.2,-- Additional temporary hacks -----------------------------------------------
0.12.2,Temporary work-around for spacing problem between parameter and parameter
0.12.2,"type in the doc, see https://github.com/numpy/numpydoc/issues/215. The bug"
0.12.2,has been fixed in sphinx (https://github.com/sphinx-doc/sphinx/pull/5976) but
0.12.2,through a change in sphinx basic.css except rtd_theme does not use basic.css.
0.12.2,"In an ideal world, this would get fixed in this PR:"
0.12.2,https://github.com/readthedocs/sphinx_rtd_theme/pull/747/files
0.12.2,get the styles from the current theme
0.12.2,create and add the button to all the code blocks that contain >>>
0.12.2,tracebacks (.gt) contain bare text elements that need to be
0.12.2,wrapped in a span to work with .nextUntil() (see later)
0.12.2,define the behavior of the button when it's clicked
0.12.2,hide the code output
0.12.2,show the code output
0.12.2,-*- coding: utf-8 -*-
0.12.2,Format template for issues URI
0.12.2,e.g. 'https://github.com/sloria/marshmallow/issues/{issue}
0.12.2,Format template for PR URI
0.12.2,e.g. 'https://github.com/sloria/marshmallow/pull/{issue}
0.12.2,Format template for commit URI
0.12.2,e.g. 'https://github.com/sloria/marshmallow/commits/{commit}
0.12.2,"Shortcut for Github, e.g. 'sloria/marshmallow'"
0.12.2,Format template for user profile URI
0.12.2,e.g. 'https://github.com/{user}'
0.12.2,Python 2 only
0.12.2,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.2,License: MIT
0.12.2,%%
0.12.2,%%
0.12.2,"First, we will generate a toy classification dataset with only few samples."
0.12.2,The ratio between the classes will be imbalanced.
0.12.2,%%
0.12.2,%%
0.12.2,"Now, we will use a :class:`~imblearn.over_sampling.RandomOverSampler` to"
0.12.2,generate a bootstrap for the minority class with as many samples as in the
0.12.2,majority class.
0.12.2,%%
0.12.2,%%
0.12.2,We observe that the minority samples are less transparent than the samples
0.12.2,"from the majority class. Indeed, it is due to the fact that these samples"
0.12.2,of the minority class are repeated during the bootstrap generation.
0.12.2,
0.12.2,We can set `shrinkage` to a floating value to add a small perturbation to the
0.12.2,samples created and therefore create a smoothed bootstrap.
0.12.2,%%
0.12.2,%%
0.12.2,"In this case, we see that the samples in the minority class are not"
0.12.2,overlapping anymore due to the added noise.
0.12.2,
0.12.2,The parameter `shrinkage` allows to add more or less perturbation. Let's
0.12.2,add more perturbation when generating the smoothed bootstrap.
0.12.2,%%
0.12.2,%%
0.12.2,Increasing the value of `shrinkage` will disperse the new samples. Forcing
0.12.2,the shrinkage to 0 will be equivalent to generating a normal bootstrap.
0.12.2,%%
0.12.2,%%
0.12.2,"Therefore, the `shrinkage` is handy to manually tune the dispersion of the"
0.12.2,new samples.
0.12.2,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.2,License: MIT
0.12.2,%%
0.12.2,generate some data points
0.12.2,plot the majority and minority samples
0.12.2,draw the circle in which the new sample will generated
0.12.2,plot the line on which the sample will be generated
0.12.2,create and plot the new sample
0.12.2,make the plot nicer with legend and label
0.12.2,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.2,License: MIT
0.12.2,%%
0.12.2,%% [markdown]
0.12.2,The following function will be used to create toy dataset. It uses the
0.12.2,:func:`~sklearn.datasets.make_classification` from scikit-learn but fixing
0.12.2,some parameters.
0.12.2,%%
0.12.2,%% [markdown]
0.12.2,The following function will be used to plot the sample space after resampling
0.12.2,to illustrate the specificities of an algorithm.
0.12.2,%%
0.12.2,%% [markdown]
0.12.2,The following function will be used to plot the decision function of a
0.12.2,classifier given some data.
0.12.2,%%
0.12.2,%% [markdown]
0.12.2,Illustration of the influence of the balancing ratio
0.12.2,----------------------------------------------------
0.12.2,
0.12.2,We will first illustrate the influence of the balancing ratio on some toy
0.12.2,data using a logistic regression classifier which is a linear model.
0.12.2,%%
0.12.2,%% [markdown]
0.12.2,We will fit and show the decision boundary model to illustrate the impact of
0.12.2,dealing with imbalanced classes.
0.12.2,%%
0.12.2,%% [markdown]
0.12.2,"Greater is the difference between the number of samples in each class, poorer"
0.12.2,are the classification results.
0.12.2,
0.12.2,Random over-sampling to balance the data set
0.12.2,--------------------------------------------
0.12.2,
0.12.2,Random over-sampling can be used to repeat some samples and balance the
0.12.2,number of samples between the dataset. It can be seen that with this trivial
0.12.2,approach the boundary decision is already less biased toward the majority
0.12.2,class. The class :class:`~imblearn.over_sampling.RandomOverSampler`
0.12.2,implements such of a strategy.
0.12.2,%%
0.12.2,%% [markdown]
0.12.2,"By default, random over-sampling generates a bootstrap. The parameter"
0.12.2,`shrinkage` allows adding a small perturbation to the generated data
0.12.2,to generate a smoothed bootstrap instead. The plot below shows the difference
0.12.2,between the two data generation strategies.
0.12.2,%%
0.12.2,%% [markdown]
0.12.2,It looks like more samples are generated with smoothed bootstrap. This is due
0.12.2,to the fact that the samples generated are not superimposing with the
0.12.2,original samples.
0.12.2,
0.12.2,More advanced over-sampling using ADASYN and SMOTE
0.12.2,--------------------------------------------------
0.12.2,
0.12.2,Instead of repeating the same samples when over-sampling or perturbating the
0.12.2,"generated bootstrap samples, one can use some specific heuristic instead."
0.12.2,:class:`~imblearn.over_sampling.ADASYN` and
0.12.2,:class:`~imblearn.over_sampling.SMOTE` can be used in this case.
0.12.2,%%
0.12.2,%% [markdown]
0.12.2,The following plot illustrates the difference between
0.12.2,:class:`~imblearn.over_sampling.ADASYN` and
0.12.2,:class:`~imblearn.over_sampling.SMOTE`.
0.12.2,:class:`~imblearn.over_sampling.ADASYN` will focus on the samples which are
0.12.2,difficult to classify with a nearest-neighbors rule while regular
0.12.2,:class:`~imblearn.over_sampling.SMOTE` will not make any distinction.
0.12.2,"Therefore, the decision function depending of the algorithm."
0.12.2,%% [markdown]
0.12.2,"Due to those sampling particularities, it can give rise to some specific"
0.12.2,issues as illustrated below.
0.12.2,%%
0.12.2,%% [markdown]
0.12.2,SMOTE proposes several variants by identifying specific samples to consider
0.12.2,during the resampling. The borderline version
0.12.2,(:class:`~imblearn.over_sampling.BorderlineSMOTE`) will detect which point to
0.12.2,select which are in the border between two classes. The SVM version
0.12.2,(:class:`~imblearn.over_sampling.SVMSMOTE`) will use the support vectors
0.12.2,found using an SVM algorithm to create new sample while the KMeans version
0.12.2,(:class:`~imblearn.over_sampling.KMeansSMOTE`) will make a clustering before
0.12.2,to generate samples in each cluster independently depending each cluster
0.12.2,density.
0.12.2,%%
0.12.2,%% [markdown]
0.12.2,"When dealing with a mixed of continuous and categorical features,"
0.12.2,:class:`~imblearn.over_sampling.SMOTENC` is the only method which can handle
0.12.2,this case.
0.12.2,%%
0.12.2,Create a dataset of a mix of numerical and categorical data
0.12.2,%% [markdown]
0.12.2,"However, if the dataset is composed of only categorical features then one"
0.12.2,should use :class:`~imblearn.over_sampling.SMOTEN`.
0.12.2,%%
0.12.2,Generate only categorical data
0.12.2,Authors: Christos Aridas
0.12.2,Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.2,License: MIT
0.12.2,%%
0.12.2,%% [markdown]
0.12.2,Let's first generate a dataset with imbalanced class distribution.
0.12.2,%%
0.12.2,%% [markdown]
0.12.2,We will use an over-sampler :class:`~imblearn.over_sampling.SMOTE` followed
0.12.2,by a :class:`~sklearn.tree.DecisionTreeClassifier`. The aim will be to
0.12.2,search which `k_neighbors` parameter is the most adequate with the dataset
0.12.2,that we generated.
0.12.2,%%
0.12.2,%% [markdown]
0.12.2,We can use the :class:`~sklearn.model_selection.validation_curve` to inspect
0.12.2,"the impact of varying the parameter `k_neighbors`. In this case, we need"
0.12.2,to use a score to evaluate the generalization score during the
0.12.2,cross-validation.
0.12.2,%%
0.12.2,%%
0.12.2,%% [markdown]
0.12.2,We can now plot the results of the cross-validation for the different
0.12.2,parameter values that we tried.
0.12.2,%%
0.12.2,make nice plotting
0.12.2,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.2,License: MIT
0.12.2,Generate a dataset
0.12.2,Split the data
0.12.2,Train the classifier with balancing
0.12.2,Test the classifier and get the prediction
0.12.2,Show the classification report
0.12.2,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.2,License: MIT
0.12.2,%%
0.12.2,%% [markdown]
0.12.2,"First, we will generate some imbalanced dataset."
0.12.2,%%
0.12.2,%% [markdown]
0.12.2,We will split the data into a training and testing set.
0.12.2,%%
0.12.2,%% [markdown]
0.12.2,We will create a pipeline made of a :class:`~imblearn.over_sampling.SMOTE`
0.12.2,over-sampler followed by a :class:`~sklearn.linear_model.LogisticRegression`
0.12.2,classifier.
0.12.2,%%
0.12.2,%% [markdown]
0.12.2,"Now, we will train the model on the training set and get the prediction"
0.12.2,associated with the testing set. Be aware that the resampling will happen
0.12.2,only when calling `fit`: the number of samples in `y_pred` is the same than
0.12.2,in `y_test`.
0.12.2,%%
0.12.2,%% [markdown]
0.12.2,The geometric mean corresponds to the square root of the product of the
0.12.2,sensitivity and specificity. Combining the two metrics should account for
0.12.2,the balancing of the dataset.
0.12.2,%%
0.12.2,%% [markdown]
0.12.2,The index balanced accuracy can transform any metric to be used in
0.12.2,imbalanced learning problems.
0.12.2,%%
0.12.2,%%
0.12.2,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.2,License: MIT
0.12.2,%%
0.12.2,%% [markdown]
0.12.2,Dataset generation
0.12.2,------------------
0.12.2,
0.12.2,We will create an imbalanced dataset with a couple of samples. We will use
0.12.2,:func:`~sklearn.datasets.make_classification` to generate this dataset.
0.12.2,%%
0.12.2,%%
0.12.2,%% [markdown]
0.12.2,The following function will be used to plot the sample space after resampling
0.12.2,to illustrate the characteristic of an algorithm.
0.12.2,%%
0.12.2,%% [markdown]
0.12.2,The following function will be used to plot the decision function of a
0.12.2,classifier given some data.
0.12.2,%%
0.12.2,%% [markdown]
0.12.2,":class:`~imblearn.over_sampling.SMOTE` allows to generate samples. However,"
0.12.2,this method of over-sampling does not have any knowledge regarding the
0.12.2,"underlying distribution. Therefore, some noisy samples can be generated, e.g."
0.12.2,"when the different classes cannot be well separated. Hence, it can be"
0.12.2,beneficial to apply an under-sampling algorithm to clean the noisy samples.
0.12.2,Two methods are usually used in the literature: (i) Tomek's link and (ii)
0.12.2,edited nearest neighbours cleaning methods. Imbalanced-learn provides two
0.12.2,ready-to-use samplers :class:`~imblearn.combine.SMOTETomek` and
0.12.2,":class:`~imblearn.combine.SMOTEENN`. In general,"
0.12.2,:class:`~imblearn.combine.SMOTEENN` cleans more noisy data than
0.12.2,:class:`~imblearn.combine.SMOTETomek`.
0.12.2,%%
0.12.2,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.2,License: MIT
0.12.2,%%
0.12.2,%% [markdown]
0.12.2,Load an imbalanced dataset
0.12.2,--------------------------
0.12.2,
0.12.2,We will load the UCI SatImage dataset which has an imbalanced ratio of 9.3:1
0.12.2,(number of majority sample for a minority sample). The data are then split
0.12.2,into training and testing.
0.12.2,%%
0.12.2,%% [markdown]
0.12.2,Classification using a single decision tree
0.12.2,-------------------------------------------
0.12.2,
0.12.2,We train a decision tree classifier which will be used as a baseline for the
0.12.2,rest of this example.
0.12.2,
0.12.2,The results are reported in terms of balanced accuracy and geometric mean
0.12.2,which are metrics widely used in the literature to validate model trained on
0.12.2,imbalanced set.
0.12.2,%%
0.12.2,%%
0.12.2,%%
0.12.2,%% [markdown]
0.12.2,Classification using bagging classifier with and without sampling
0.12.2,-----------------------------------------------------------------
0.12.2,
0.12.2,"Instead of using a single tree, we will check if an ensemble of decision tree"
0.12.2,"can actually alleviate the issue induced by the class imbalancing. First, we"
0.12.2,will use a bagging classifier and its counter part which internally uses a
0.12.2,random under-sampling to balanced each bootstrap sample.
0.12.2,%%
0.12.2,%% [markdown]
0.12.2,Balancing each bootstrap sample allows to increase significantly the balanced
0.12.2,accuracy and the geometric mean.
0.12.2,%%
0.12.2,%%
0.12.2,%% [markdown]
0.12.2,Classification using random forest classifier with and without sampling
0.12.2,-----------------------------------------------------------------------
0.12.2,
0.12.2,Random forest is another popular ensemble method and it is usually
0.12.2,"outperforming bagging. Here, we used a vanilla random forest and its balanced"
0.12.2,counterpart in which each bootstrap sample is balanced.
0.12.2,%%
0.12.2,%% [markdown]
0.12.2,"Similarly to the previous experiment, the balanced classifier outperform the"
0.12.2,"classifier which learn from imbalanced bootstrap samples. In addition, random"
0.12.2,forest outperforms the bagging classifier.
0.12.2,%%
0.12.2,%%
0.12.2,%% [markdown]
0.12.2,Boosting classifier
0.12.2,-------------------
0.12.2,
0.12.2,"In the same manner, easy ensemble classifier is a bag of balanced AdaBoost"
0.12.2,"classifier. However, it will be slower to train than random forest and will"
0.12.2,achieve worse performance.
0.12.2,%%
0.12.2,%%
0.12.2,%%
0.12.2,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.2,License: MIT
0.12.2,%%
0.12.2,%% [markdown]
0.12.2,Generate an imbalanced dataset
0.12.2,------------------------------
0.12.2,
0.12.2,"For this example, we will create a synthetic dataset using the function"
0.12.2,:func:`~sklearn.datasets.make_classification`. The problem will be a toy
0.12.2,classification problem with a ratio of 1:9 between the two classes.
0.12.2,%%
0.12.2,%%
0.12.2,%% [markdown]
0.12.2,"In the following sections, we will show a couple of algorithms that have"
0.12.2,been proposed over the years. We intend to illustrate how one can reuse the
0.12.2,:class:`~imblearn.ensemble.BalancedBaggingClassifier` by passing different
0.12.2,sampler.
0.12.2,%%
0.12.2,%% [markdown]
0.12.2,Exactly Balanced Bagging and Over-Bagging
0.12.2,-----------------------------------------
0.12.2,
0.12.2,The :class:`~imblearn.ensemble.BalancedBaggingClassifier` can use in
0.12.2,conjunction with a :class:`~imblearn.under_sampling.RandomUnderSampler` or
0.12.2,:class:`~imblearn.over_sampling.RandomOverSampler`. These methods are
0.12.2,"referred as Exactly Balanced Bagging and Over-Bagging, respectively and have"
0.12.2,been proposed first in [1]_.
0.12.2,%%
0.12.2,Exactly Balanced Bagging
0.12.2,%%
0.12.2,Over-bagging
0.12.2,%% [markdown]
0.12.2,SMOTE-Bagging
0.12.2,-------------
0.12.2,
0.12.2,Instead of using a :class:`~imblearn.over_sampling.RandomOverSampler` that
0.12.2,"make a bootstrap, an alternative is to use"
0.12.2,:class:`~imblearn.over_sampling.SMOTE` as an over-sampler. This is known as
0.12.2,SMOTE-Bagging [2]_.
0.12.2,%%
0.12.2,SMOTE-Bagging
0.12.2,%% [markdown]
0.12.2,Roughly Balanced Bagging
0.12.2,------------------------
0.12.2,While using a :class:`~imblearn.under_sampling.RandomUnderSampler` or
0.12.2,:class:`~imblearn.over_sampling.RandomOverSampler` will create exactly the
0.12.2,"desired number of samples, it does not follow the statistical spirit wanted"
0.12.2,in the bagging framework. The authors in [3]_ proposes to use a negative
0.12.2,binomial distribution to compute the number of samples of the majority
0.12.2,class to be selected and then perform a random under-sampling.
0.12.2,
0.12.2,"Here, we illustrate this method by implementing a function in charge of"
0.12.2,resampling and use the :class:`~imblearn.FunctionSampler` to integrate it
0.12.2,within a :class:`~imblearn.pipeline.Pipeline` and
0.12.2,:class:`~sklearn.model_selection.cross_validate`.
0.12.2,%%
0.12.2,find the minority and majority classes
0.12.2,compute the number of sample to draw from the majority class using
0.12.2,a negative binomial distribution
0.12.2,draw randomly with or without replacement
0.12.2,Roughly Balanced Bagging
0.12.2,%% [markdown]
0.12.2,.. topic:: References:
0.12.2,
0.12.2,".. [1] R. Maclin, and D. Opitz. ""An empirical evaluation of bagging and"
0.12.2,"boosting."" AAAI/IAAI 1997 (1997): 546-551."
0.12.2,
0.12.2,".. [2] S. Wang, and X. Yao. ""Diversity analysis on imbalanced data sets by"
0.12.2,"using ensemble models."" 2009 IEEE symposium on computational"
0.12.2,"intelligence and data mining. IEEE, 2009."
0.12.2,
0.12.2,".. [3] S. Hido, H. Kashima, and Y. Takahashi. ""Roughly balanced bagging"
0.12.2,"for imbalanced data."" Statistical Analysis and Data Mining: The ASA"
0.12.2,Data Science Journal 2.5‚Äê6 (2009): 412-426.
0.12.2,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.2,License: MIT
0.12.2,%%
0.12.2,%% [markdown]
0.12.2,The following function will be used to create toy dataset. It uses the
0.12.2,:func:`~sklearn.datasets.make_classification` from scikit-learn but fixing
0.12.2,some parameters.
0.12.2,%%
0.12.2,%% [markdown]
0.12.2,The following function will be used to plot the sample space after resampling
0.12.2,to illustrate the specificities of an algorithm.
0.12.2,%%
0.12.2,%% [markdown]
0.12.2,The following function will be used to plot the decision function of a
0.12.2,classifier given some data.
0.12.2,%%
0.12.2,%%
0.12.2,%% [markdown]
0.12.2,Prototype generation: under-sampling by generating new samples
0.12.2,--------------------------------------------------------------
0.12.2,
0.12.2,:class:`~imblearn.under_sampling.ClusterCentroids` under-samples by replacing
0.12.2,the original samples by the centroids of the cluster found.
0.12.2,%%
0.12.2,%% [markdown]
0.12.2,Prototype selection: under-sampling by selecting existing samples
0.12.2,-----------------------------------------------------------------
0.12.2,
0.12.2,The algorithm performing prototype selection can be subdivided into two
0.12.2,groups: (i) the controlled under-sampling methods and (ii) the cleaning
0.12.2,under-sampling methods.
0.12.2,
0.12.2,"With the controlled under-sampling methods, the number of samples to be"
0.12.2,selected can be specified.
0.12.2,:class:`~imblearn.under_sampling.RandomUnderSampler` is the most naive way of
0.12.2,performing such selection by randomly selecting a given number of samples by
0.12.2,the targeted class.
0.12.2,%%
0.12.2,%% [markdown]
0.12.2,:class:`~imblearn.under_sampling.NearMiss` algorithms implement some
0.12.2,heuristic rules in order to select samples. NearMiss-1 selects samples from
0.12.2,the majority class for which the average distance of the :math:`k`` nearest
0.12.2,samples of the minority class is the smallest. NearMiss-2 selects the samples
0.12.2,from the majority class for which the average distance to the farthest
0.12.2,samples of the negative class is the smallest. NearMiss-3 is a 2-step
0.12.2,"algorithm: first, for each minority sample, their :math:`m`"
0.12.2,"nearest-neighbors will be kept; then, the majority samples selected are the"
0.12.2,on for which the average distance to the :math:`k` nearest neighbors is the
0.12.2,largest.
0.12.2,%%
0.12.2,%% [markdown]
0.12.2,:class:`~imblearn.under_sampling.EditedNearestNeighbours` removes samples of
0.12.2,the majority class for which their class differ from the one of their
0.12.2,nearest-neighbors. This sieve can be repeated which is the principle of the
0.12.2,:class:`~imblearn.under_sampling.RepeatedEditedNearestNeighbours`.
0.12.2,:class:`~imblearn.under_sampling.AllKNN` is slightly different from the
0.12.2,:class:`~imblearn.under_sampling.RepeatedEditedNearestNeighbours` by changing
0.12.2,"the :math:`k` parameter of the internal nearest neighors algorithm,"
0.12.2,increasing it at each iteration.
0.12.2,%%
0.12.2,%% [markdown]
0.12.2,:class:`~imblearn.under_sampling.CondensedNearestNeighbour` makes use of a
0.12.2,1-NN to iteratively decide if a sample should be kept in a dataset or not.
0.12.2,The issue is that :class:`~imblearn.under_sampling.CondensedNearestNeighbour`
0.12.2,is sensitive to noise by preserving the noisy samples.
0.12.2,:class:`~imblearn.under_sampling.OneSidedSelection` also used the 1-NN and
0.12.2,use :class:`~imblearn.under_sampling.TomekLinks` to remove the samples
0.12.2,considered noisy. The
0.12.2,:class:`~imblearn.under_sampling.NeighbourhoodCleaningRule` use a
0.12.2,:class:`~imblearn.under_sampling.EditedNearestNeighbours` to remove some
0.12.2,"sample. Additionally, they use a 3 nearest-neighbors to remove samples which"
0.12.2,do not agree with this rule.
0.12.2,%%
0.12.2,%% [markdown]
0.12.2,:class:`~imblearn.under_sampling.InstanceHardnessThreshold` uses the
0.12.2,prediction of classifier to exclude samples. All samples which are classified
0.12.2,with a low probability will be removed.
0.12.2,%%
0.12.2,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.2,License: MIT
0.12.2,%%
0.12.2,%% [markdown]
0.12.2,This function allows to make nice plotting
0.12.2,%%
0.12.2,%% [markdown]
0.12.2,We will generate some toy data that illustrates how
0.12.2,:class:`~imblearn.under_sampling.TomekLinks` is used to clean a dataset.
0.12.2,%%
0.12.2,%% [markdown]
0.12.2,"In the figure above, the samples highlighted in green form a Tomek link since"
0.12.2,they are of different classes and are nearest neighbors of each other.
0.12.2,highlight the samples of interest
0.12.2,%% [markdown]
0.12.2,We can run the :class:`~imblearn.under_sampling.TomekLinks` sampling to
0.12.2,remove the corresponding samples. If `sampling_strategy='auto'` only the
0.12.2,sample from the majority class will be removed. If `sampling_strategy='all'`
0.12.2,both samples will be removed.
0.12.2,%%
0.12.2,highlight the samples of interest
0.12.2,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.2,License: MIT
0.12.2,%%
0.12.2,%% [markdown]
0.12.2,We define a function allowing to make some nice decoration on the plot.
0.12.2,%%
0.12.2,%% [markdown]
0.12.2,We can start by generating some data to later illustrate the principle of
0.12.2,each :class:`~imblearn.under_sampling.NearMiss` heuristic rules.
0.12.2,%%
0.12.2,%% [mardown]
0.12.2,NearMiss-1
0.12.2,----------
0.12.2,
0.12.2,NearMiss-1 selects samples from the majority class for which the average
0.12.2,distance to some nearest neighbours is the smallest. In the following
0.12.2,"example, we use a 3-NN to compute the average distance on 2 specific samples"
0.12.2,"of the majority class. Therefore, in this case the point linked by the"
0.12.2,green-dashed line will be selected since the average distance is smaller.
0.12.2,%%
0.12.2,%% [mardown]
0.12.2,NearMiss-2
0.12.2,----------
0.12.2,
0.12.2,NearMiss-2 selects samples from the majority class for which the average
0.12.2,distance to the farthest neighbors is the smallest. With the same
0.12.2,"configuration as previously presented, the sample linked to the green-dashed"
0.12.2,line will be selected since its distance the 3 farthest neighbors is the
0.12.2,smallest.
0.12.2,%%
0.12.2,%% [mardown]
0.12.2,NearMiss-3
0.12.2,----------
0.12.2,
0.12.2,"NearMiss-3 can be divided into 2 steps. First, a nearest-neighbors is used to"
0.12.2,short-list samples from the majority class (i.e. correspond to the
0.12.2,"highlighted samples in the following plot). Then, the sample with the largest"
0.12.2,average distance to the *k* nearest-neighbors are selected.
0.12.2,%%
0.12.2,select only the majority point of interest
0.12.2,Authors: Christos Aridas
0.12.2,Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.2,License: MIT
0.12.2,%%
0.12.2,%% [markdown]
0.12.2,Let's first create an imbalanced dataset and split in to two sets.
0.12.2,%%
0.12.2,%% [markdown]
0.12.2,"Now, we will create each individual steps that we would like later to combine"
0.12.2,%%
0.12.2,%% [markdown]
0.12.2,"Now, we can finally create a pipeline to specify in which order the different"
0.12.2,transformers and samplers should be executed before to provide the data to
0.12.2,the final classifier.
0.12.2,%%
0.12.2,%% [markdown]
0.12.2,We can now use the pipeline created as a normal classifier where resampling
0.12.2,"will happen when calling `fit` and disabled when calling `decision_function`,"
0.12.2,"`predict_proba`, or `predict`."
0.12.2,%%
0.12.2,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.2,License: MIT
0.12.2,##############################################################################
0.12.2,Data loading
0.12.2,##############################################################################
0.12.2,##############################################################################
0.12.2,"First, you should download the Porto Seguro data set from Kaggle. See the"
0.12.2,link in the introduction.
0.12.2,##############################################################################
0.12.2,The data set is imbalanced and it will have an effect on the fitting.
0.12.2,##############################################################################
0.12.2,Define the pre-processing pipeline
0.12.2,##############################################################################
0.12.2,##############################################################################
0.12.2,We want to standard scale the numerical features while we want to one-hot
0.12.2,"encode the categorical features. In this regard, we make use of the"
0.12.2,:class:`~sklearn.compose.ColumnTransformer`.
0.12.2,Create an environment variable to avoid using the GPU. This can be changed.
0.12.2,##############################################################################
0.12.2,Create a neural-network
0.12.2,##############################################################################
0.12.2,##############################################################################
0.12.2,We create a decorator to report the computation time
0.12.2,##############################################################################
0.12.2,The first model will be trained using the ``fit`` method and with imbalanced
0.12.2,mini-batches.
0.12.2,predict_proba was removed in tensorflow 2.6
0.12.2,##############################################################################
0.12.2,"In the contrary, we will use imbalanced-learn to create a generator of"
0.12.2,mini-batches which will yield balanced mini-batches.
0.12.2,##############################################################################
0.12.2,Classification loop
0.12.2,##############################################################################
0.12.2,##############################################################################
0.12.2,We will perform a 10-fold cross-validation and train the neural-network with
0.12.2,the two different strategies previously presented.
0.12.2,##############################################################################
0.12.2,Plot of the results and computation time
0.12.2,##############################################################################
0.12.2,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.2,License: MIT
0.12.2,%%
0.12.2,%% [markdown]
0.12.2,Problem definition
0.12.2,------------------
0.12.2,
0.12.2,We are dropping the following features:
0.12.2,
0.12.2,"- ""fnlwgt"": this feature was created while studying the ""adult"" dataset."
0.12.2,"Thus, we will not use this feature which is not acquired during the survey."
0.12.2,"- ""education-num"": it is encoding the same information than ""education""."
0.12.2,"Thus, we are removing one of these 2 features."
0.12.2,%%
0.12.2,%% [markdown]
0.12.2,"The ""adult"" dataset as a class ratio of about 3:1"
0.12.2,%%
0.12.2,%% [markdown]
0.12.2,This dataset is only slightly imbalanced. To better highlight the effect of
0.12.2,"learning from an imbalanced dataset, we will increase its ratio to 30:1"
0.12.2,%%
0.12.2,%% [markdown]
0.12.2,We will perform a cross-validation evaluation to get an estimate of the test
0.12.2,score.
0.12.2,
0.12.2,"As a baseline, we could use a classifier which will always predict the"
0.12.2,majority class independently of the features provided.
0.12.2,%%
0.12.2,%% [markdown]
0.12.2,"Instead of using the accuracy, we can use the balanced accuracy which will"
0.12.2,take into account the balancing issue.
0.12.2,%%
0.12.2,%% [markdown]
0.12.2,Strategies to learn from an imbalanced dataset
0.12.2,----------------------------------------------
0.12.2,We will use a dictionary and a list to continuously store the results of
0.12.2,our experiments and show them as a pandas dataframe.
0.12.2,%%
0.12.2,%% [markdown]
0.12.2,Dummy baseline
0.12.2,..............
0.12.2,
0.12.2,"Before to train a real machine learning model, we can store the results"
0.12.2,obtained with our :class:`~sklearn.dummy.DummyClassifier`.
0.12.2,%%
0.12.2,%% [markdown]
0.12.2,Linear classifier baseline
0.12.2,..........................
0.12.2,
0.12.2,We will create a machine learning pipeline using a
0.12.2,":class:`~sklearn.linear_model.LogisticRegression` classifier. In this regard,"
0.12.2,we will need to one-hot encode the categorical columns and standardized the
0.12.2,numerical columns before to inject the data into the
0.12.2,:class:`~sklearn.linear_model.LogisticRegression` classifier.
0.12.2,
0.12.2,"First, we define our numerical and categorical pipelines."
0.12.2,%%
0.12.2,%% [markdown]
0.12.2,"Then, we can create a preprocessor which will dispatch the categorical"
0.12.2,columns to the categorical pipeline and the numerical columns to the
0.12.2,numerical pipeline
0.12.2,%%
0.12.2,%% [markdown]
0.12.2,"Finally, we connect our preprocessor with our"
0.12.2,:class:`~sklearn.linear_model.LogisticRegression`. We can then evaluate our
0.12.2,model.
0.12.2,%%
0.12.2,%%
0.12.2,%% [markdown]
0.12.2,We can see that our linear model is learning slightly better than our dummy
0.12.2,"baseline. However, it is impacted by the class imbalance."
0.12.2,
0.12.2,We can verify that something similar is happening with a tree-based model
0.12.2,such as :class:`~sklearn.ensemble.RandomForestClassifier`. With this type of
0.12.2,"classifier, we will not need to scale the numerical data, and we will only"
0.12.2,need to ordinal encode the categorical data.
0.12.2,%%
0.12.2,%%
0.12.2,%% [markdown]
0.12.2,The :class:`~sklearn.ensemble.RandomForestClassifier` is as well affected by
0.12.2,"the class imbalanced, slightly less than the linear model. Now, we will"
0.12.2,present different approach to improve the performance of these 2 models.
0.12.2,
0.12.2,Use `class_weight`
0.12.2,..................
0.12.2,
0.12.2,Most of the models in `scikit-learn` have a parameter `class_weight`. This
0.12.2,parameter will affect the computation of the loss in linear model or the
0.12.2,criterion in the tree-based model to penalize differently a false
0.12.2,classification from the minority and majority class. We can set
0.12.2,"`class_weight=""balanced""` such that the weight applied is inversely"
0.12.2,proportional to the class frequency. We test this parametrization in both
0.12.2,linear model and tree-based model.
0.12.2,%%
0.12.2,%%
0.12.2,%% [markdown]
0.12.2,We can see that using `class_weight` was really effective for the linear
0.12.2,"model, alleviating the issue of learning from imbalanced classes. However,"
0.12.2,the :class:`~sklearn.ensemble.RandomForestClassifier` is still biased toward
0.12.2,"the majority class, mainly due to the criterion which is not suited enough to"
0.12.2,fight the class imbalance.
0.12.2,
0.12.2,Resample the training set during learning
0.12.2,.........................................
0.12.2,
0.12.2,Another way is to resample the training set by under-sampling or
0.12.2,over-sampling some of the samples. `imbalanced-learn` provides some samplers
0.12.2,to do such processing.
0.12.2,%%
0.12.2,%%
0.12.2,%%
0.12.2,%%
0.12.2,%% [markdown]
0.12.2,Applying a random under-sampler before the training of the linear model or
0.12.2,"random forest, allows to not focus on the majority class at the cost of"
0.12.2,making more mistake for samples in the majority class (i.e. decreased
0.12.2,accuracy).
0.12.2,
0.12.2,We could apply any type of samplers and find which sampler is working best
0.12.2,on the current dataset.
0.12.2,
0.12.2,"Instead, we will present another way by using classifiers which will apply"
0.12.2,sampling internally.
0.12.2,
0.12.2,Use of specific balanced algorithms from imbalanced-learn
0.12.2,.........................................................
0.12.2,
0.12.2,We already showed that random under-sampling can be effective on decision
0.12.2,"tree. However, instead of under-sampling once the dataset, one could"
0.12.2,under-sample the original dataset before to take a bootstrap sample. This is
0.12.2,the base of the :class:`imblearn.ensemble.BalancedRandomForestClassifier` and
0.12.2,:class:`~imblearn.ensemble.BalancedBaggingClassifier`.
0.12.2,%%
0.12.2,%%
0.12.2,%% [markdown]
0.12.2,The performance with the
0.12.2,:class:`~imblearn.ensemble.BalancedRandomForestClassifier` is better than
0.12.2,applying a single random under-sampling. We will use a gradient-boosting
0.12.2,classifier within a :class:`~imblearn.ensemble.BalancedBaggingClassifier`.
0.12.2,%% [markdown]
0.12.2,This last approach is the most effective. The different under-sampling allows
0.12.2,to bring some diversity for the different GBDT to learn and not focus on a
0.12.2,portion of the majority class.
0.12.2,Authors: Christos Aridas
0.12.2,Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.2,License: MIT
0.12.2,%%
0.12.2,%% [markdown]
0.12.2,Load the dataset
0.12.2,----------------
0.12.2,
0.12.2,We will use a dataset containing image from know person where we will
0.12.2,build a model to recognize the person on the image. We will make this problem
0.12.2,a binary problem by taking picture of only George W. Bush and Bill Clinton.
0.12.2,%%
0.12.2,%%
0.12.2,%% [markdown]
0.12.2,We can check the ratio between the two classes.
0.12.2,%%
0.12.2,%% [markdown]
0.12.2,We see that we have an imbalanced classification problem with ~95% of the
0.12.2,data belonging to the class G.W. Bush.
0.12.2,
0.12.2,Compare over-sampling approaches
0.12.2,--------------------------------
0.12.2,
0.12.2,We will use different over-sampling approaches and use a kNN classifier
0.12.2,to check if we can recognize the 2 presidents. The evaluation will be
0.12.2,performed through cross-validation and we will plot the mean ROC curve.
0.12.2,
0.12.2,We will create different pipelines and evaluate them.
0.12.2,%%
0.12.2,%% [markdown]
0.12.2,We will compute the mean ROC curve for each pipeline using a different splits
0.12.2,provided by the :class:`~sklearn.model_selection.StratifiedKFold`
0.12.2,cross-validation.
0.12.2,%%
0.12.2,compute the mean fpr/tpr to get the mean ROC curve
0.12.2,Create a display that we will reuse to make the aggregated plots for
0.12.2,all methods
0.12.2,%% [markdown]
0.12.2,"In the previous cell, we created the different mean ROC curve and we can plot"
0.12.2,them on the same plot.
0.12.2,%%
0.12.2,%% [markdown]
0.12.2,"We see that for this task, methods that are generating new samples with some"
0.12.2,interpolation (i.e. ADASYN and SMOTE) perform better than random
0.12.2,over-sampling or no resampling.
0.12.2,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.2,License: MIT
0.12.2,Create a folder to fetch the dataset
0.12.2,Create a pipeline
0.12.2,Classify and report the results
0.12.2,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.2,License: MIT
0.12.2,%%
0.12.2,%% [markdown]
0.12.2,Setting the data set
0.12.2,--------------------
0.12.2,
0.12.2,We use a part of the 20 newsgroups data set by loading 4 topics. Using the
0.12.2,"scikit-learn loader, the data are split into a training and a testing set."
0.12.2,
0.12.2,Note the class \#3 is the minority class and has almost twice less samples
0.12.2,than the majority class.
0.12.2,%%
0.12.2,%%
0.12.2,%% [markdown]
0.12.2,The usual scikit-learn pipeline
0.12.2,-------------------------------
0.12.2,
0.12.2,You might usually use scikit-learn pipeline by combining the TF-IDF
0.12.2,vectorizer to feed a multinomial naive bayes classifier. A classification
0.12.2,report summarized the results on the testing set.
0.12.2,
0.12.2,"As expected, the recall of the class \#3 is low mainly due to the class"
0.12.2,imbalanced.
0.12.2,%%
0.12.2,%%
0.12.2,%% [markdown]
0.12.2,Balancing the class before classification
0.12.2,-----------------------------------------
0.12.2,
0.12.2,"To improve the prediction of the class \#3, it could be interesting to apply"
0.12.2,"a balancing before to train the naive bayes classifier. Therefore, we will"
0.12.2,use a :class:`~imblearn.under_sampling.RandomUnderSampler` to equalize the
0.12.2,number of samples in all the classes before the training.
0.12.2,
0.12.2,It is also important to note that we are using the
0.12.2,:class:`~imblearn.pipeline.make_pipeline` function implemented in
0.12.2,imbalanced-learn to properly handle the samplers.
0.12.2,%%
0.12.2,%% [markdown]
0.12.2,"Although the results are almost identical, it can be seen that the resampling"
0.12.2,allowed to correct the poor recall of the class \#3 at the cost of reducing
0.12.2,"the other metrics for the other classes. However, the overall results are"
0.12.2,slightly better.
0.12.2,%%
0.12.2,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.2,License: MIT
0.12.2,#############################################################################
0.12.2,Toy data generation
0.12.2,#############################################################################
0.12.2,#############################################################################
0.12.2,We are generating some non Gaussian data set contaminated with some unform
0.12.2,noise.
0.12.2,#############################################################################
0.12.2,We will generate some cleaned test data without outliers.
0.12.2,#############################################################################
0.12.2,How to use the :class:`~imblearn.FunctionSampler`
0.12.2,#############################################################################
0.12.2,#############################################################################
0.12.2,We first define a function which will use
0.12.2,:class:`~sklearn.ensemble.IsolationForest` to eliminate some outliers from
0.12.2,our dataset during training. The function passed to the
0.12.2,:class:`~imblearn.FunctionSampler` will be called when using the method
0.12.2,``fit_resample``.
0.12.2,#############################################################################
0.12.2,Integrate it within a pipeline
0.12.2,#############################################################################
0.12.2,#############################################################################
0.12.2,"By elimnating outliers before the training, the classifier will be less"
0.12.2,affected during the prediction.
0.12.2,Authors: Dayvid Oliveira
0.12.2,Christos Aridas
0.12.2,Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.2,License: MIT
0.12.2,%%
0.12.2,%% [markdown]
0.12.2,Generate the dataset
0.12.2,--------------------
0.12.2,
0.12.2,"First, we will generate a dataset and convert it to a"
0.12.2,:class:`~pandas.DataFrame` with arbitrary column names. We will plot the
0.12.2,original dataset.
0.12.2,%%
0.12.2,%% [markdown]
0.12.2,Make a dataset imbalanced
0.12.2,-------------------------
0.12.2,
0.12.2,"Now, we will show the helpers :func:`~imblearn.datasets.make_imbalance`"
0.12.2,that is useful to random select a subset of samples. It will impact the
0.12.2,class distribution as specified by the parameters.
0.12.2,%%
0.12.2,%%
0.12.2,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.2,License: MIT
0.12.2,%%
0.12.2,%% [markdown]
0.12.2,Create an imbalanced dataset
0.12.2,----------------------------
0.12.2,
0.12.2,"First, we will create an imbalanced data set from a the iris data set."
0.12.2,%%
0.12.2,%%
0.12.2,%% [markdown]
0.12.2,Using ``sampling_strategy`` in resampling algorithms
0.12.2,====================================================
0.12.2,
0.12.2,`sampling_strategy` as a `float`
0.12.2,--------------------------------
0.12.2,
0.12.2,`sampling_strategy` can be given a `float`. For **under-sampling
0.12.2,"methods**, it corresponds to the ratio :math:`\alpha_{us}` defined by"
0.12.2,:math:`N_{rM} = \alpha_{us} \times N_{m}` where :math:`N_{rM}` and
0.12.2,:math:`N_{m}` are the number of samples in the majority class after
0.12.2,"resampling and the number of samples in the minority class, respectively."
0.12.2,%%
0.12.2,select only 2 classes since the ratio make sense in this case
0.12.2,%%
0.12.2,%% [markdown]
0.12.2,"For **over-sampling methods**, it correspond to the ratio"
0.12.2,:math:`\alpha_{os}` defined by :math:`N_{rm} = \alpha_{os} \times N_{M}`
0.12.2,where :math:`N_{rm}` and :math:`N_{M}` are the number of samples in the
0.12.2,minority class after resampling and the number of samples in the majority
0.12.2,"class, respectively."
0.12.2,%%
0.12.2,%% [markdown]
0.12.2,`sampling_strategy` as a `str`
0.12.2,-------------------------------
0.12.2,
0.12.2,`sampling_strategy` can be given as a string which specify the class
0.12.2,"targeted by the resampling. With under- and over-sampling, the number of"
0.12.2,samples will be equalized.
0.12.2,
0.12.2,Note that we are using multiple classes from now on.
0.12.2,%%
0.12.2,%% [markdown]
0.12.2,"With **cleaning method**, the number of samples in each class will not be"
0.12.2,equalized even if targeted.
0.12.2,%%
0.12.2,%% [markdown]
0.12.2,`sampling_strategy` as a `dict`
0.12.2,------------------------------
0.12.2,
0.12.2,"When `sampling_strategy` is a `dict`, the keys correspond to the targeted"
0.12.2,classes. The values correspond to the desired number of samples for each
0.12.2,targeted class. This is working for both **under- and over-sampling**
0.12.2,algorithms but not for the **cleaning algorithms**. Use a `list` instead.
0.12.2,%%
0.12.2,%% [markdown]
0.12.2,`sampling_strategy` as a `list`
0.12.2,-------------------------------
0.12.2,
0.12.2,"When `sampling_strategy` is a `list`, the list contains the targeted"
0.12.2,classes. It is used only for **cleaning methods** and raise an error
0.12.2,otherwise.
0.12.2,%%
0.12.2,%% [markdown]
0.12.2,`sampling_strategy` as a callable
0.12.2,---------------------------------
0.12.2,
0.12.2,"When callable, function taking `y` and returns a `dict`. The keys"
0.12.2,correspond to the targeted classes. The values correspond to the desired
0.12.2,number of samples for each class.
0.12.2,%%
0.12.2,List of whitelisted modules and methods; regexp are supported.
0.12.2,These docstrings will fail because they are inheriting from scikit-learn
0.12.2,skip private classes
0.12.2,"We ignore following error code,"
0.12.2,- RT02: The first line of the Returns section
0.12.2,"should contain only the type, .."
0.12.2,(as we may need refer to the name of the returned
0.12.2,object)
0.12.2,- GL01: Docstring text (summary) should start in the line
0.12.2,"immediately after the opening quotes (not in the same line,"
0.12.2,or leaving a blank line in between)
0.12.2,"- GL02: If there's a blank line, it should be before the"
0.12.2,"first line of the Returns section, not after (it allows to have"
0.12.2,short docstrings for properties).
0.12.2,Ignore PR02: Unknown parameters for properties. We sometimes use
0.12.2,"properties for ducktyping, i.e. SGDClassifier.predict_proba"
0.12.2,Following codes are only taken into account for the
0.12.2,top level class docstrings:
0.12.2,- ES01: No extended summary found
0.12.2,- SA01: See Also section not found
0.12.2,- EX01: No examples section found
0.12.2,In particular we can't parse the signature of properties
0.12.2,"When applied to classes, detect class method. For functions"
0.12.2,method = None.
0.12.2,TODO: this detection can be improved. Currently we assume that we have
0.12.2,class # methods if the second path element before last is in camel case.
0.12.2,'build' and 'install' is included to have structured metadata for CI.
0.12.2,It will NOT be included in setup's extras_require
0.12.2,"The values are (version_spec, comma separated tags)"
0.12.2,create inverse mapping for setuptools
0.12.2,Used by CI to get the min dependencies
0.12.2,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.2,License: MIT
0.12.2,TODO: remove this file when scikit-learn minimum version is 1.3
0.12.2,Return a copy of the threadlocal configuration so that users will
0.12.2,not be able to modify the configuration with the returned dict.
0.12.2,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.2,Christos Aridas
0.12.2,License: MIT
0.12.2,scikit-learn >= 1.2
0.12.2,we need to overwrite SamplerMixin.fit to bypass the validation
0.12.2,Adapted from scikit-learn
0.12.2,Author: Edouard Duchesnay
0.12.2,Gael Varoquaux
0.12.2,Virgile Fritsch
0.12.2,Alexandre Gramfort
0.12.2,Lars Buitinck
0.12.2,Christos Aridas
0.12.2,Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.2,License: BSD
0.12.2,BaseEstimator interface
0.12.2,validate names
0.12.2,validate estimators
0.12.2,We allow last estimator to be None as an identity transformation
0.12.2,Estimator interface
0.12.2,"def _fit(self, X, y=None, **fit_params_steps):"
0.12.2,Setup the memory
0.12.2,we do not clone when caching is disabled to
0.12.2,preserve backward compatibility
0.12.2,Fit or load from cache the current transformer
0.12.2,Replace the transformer of the step with the fitted
0.12.2,transformer. This is necessary when loading the transformer
0.12.2,from the cache.
0.12.2,The `fit_*` methods need to be overridden to support the samplers.
0.12.2,estimators in Pipeline.steps are not validated yet
0.12.2,estimators in Pipeline.steps are not validated yet
0.12.2,metadata routing enabled
0.12.2,estimators in Pipeline.steps are not validated yet
0.12.2,estimators in Pipeline.steps are not validated yet
0.12.2,TODO: remove the following methods when the minimum scikit-learn >= 1.4
0.12.2,They do not depend on resampling but we need to redefine them for the
0.12.2,compatibility with the metadata routing framework.
0.12.2,metadata routing enabled
0.12.2,not branching here since params is only available if
0.12.2,enable_metadata_routing=True
0.12.2,metadata routing enabled
0.12.2,not branching here since params is only available if
0.12.2,enable_metadata_routing=True
0.12.2,"we don't have to branch here, since params is only non-empty if"
0.12.2,enable_metadata_routing=True.
0.12.2,metadata routing is enabled.
0.12.2,"TODO: once scikit-learn >= 1.4, the following function should be simplified by"
0.12.2,calling `super().get_metadata_routing()`
0.12.2,first we add all steps except the last one
0.12.2,"fit, fit_predict, and fit_transform call fit_transform if it"
0.12.2,"exists, or else fit and transform"
0.12.2,then we add the last step
0.12.2,"without metadata routing, fit_transform and fit_predict"
0.12.2,get all the same params and pass it to the last fit.
0.12.2,"if we have a weight for this transformer, multiply output"
0.12.2,This variable is injected in the __builtins__ by the build
0.12.2,process. It is used to enable importing subpackages of sklearn when
0.12.2,the binaries are not built
0.12.2,mypy error: Cannot determine type of '__SKLEARN_SETUP__'
0.12.2,We are not importing the rest of scikit-learn during the build
0.12.2,"process, as it may not be compiled yet"
0.12.2,"FIXME: When we get Python 3.7 as minimal version, we will need to switch to"
0.12.2,the following solution:
0.12.2,https://snarky.ca/lazy-importing-in-python-3-7/
0.12.2,Import the target module and insert it into the parent's namespace
0.12.2,Update this object's dict so that if someone keeps a reference to the
0.12.2,"LazyLoader, lookups are efficient (__getattr__ is only called on"
0.12.2,lookups that fail).
0.12.2,delay the import of keras since we are going to import either tensorflow
0.12.2,or keras
0.12.2,Based on NiLearn package
0.12.2,License: simplified BSD
0.12.2,"PEP0440 compatible formatted version, see:"
0.12.2,https://www.python.org/dev/peps/pep-0440/
0.12.2,
0.12.2,Generic release markers:
0.12.2,X.Y
0.12.2,X.Y.Z # For bugfix releases
0.12.2,
0.12.2,Admissible pre-release markers:
0.12.2,X.YaN # Alpha release
0.12.2,X.YbN # Beta release
0.12.2,X.YrcN # Release Candidate
0.12.2,X.Y # Final release
0.12.2,
0.12.2,Dev branch marker is: 'X.Y.dev' or 'X.Y.devN' where N is an integer.
0.12.2,'X.Y.dev0' is the canonical version of 'X.Y.dev'
0.12.2,
0.12.2,coding: utf-8
0.12.2,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.2,Dariusz Brzezinski
0.12.2,License: MIT
0.12.2,Only negative labels
0.12.2,"Calculate tp_sum, pred_sum, true_sum ###"
0.12.2,labels are now from 0 to len(labels) - 1 -> use bincount
0.12.2,Pathological case
0.12.2,Compute the true negative
0.12.2,Retain only selected labels
0.12.2,"Finally, we have all our sufficient statistics. Divide! #"
0.12.2,"Divide, and on zero-division, set scores to 0 and warn:"
0.12.2,"Oddly, we may get an ""invalid"" rather than a ""divide"" error"
0.12.2,here.
0.12.2,Average the results
0.12.2,labels are now from 0 to len(labels) - 1 -> use bincount
0.12.2,Pathological case
0.12.2,Retain only selected labels
0.12.2,old version of scipy return MaskedConstant instead of 0.0
0.12.2,check that the scoring function does not need a score
0.12.2,and only a prediction
0.12.2,We do not support multilabel so the only average supported
0.12.2,is binary
0.12.2,Compute the different metrics
0.12.2,Precision/recall/f1
0.12.2,Specificity
0.12.2,Geometric mean
0.12.2,Index balanced accuracy
0.12.2,compute averages
0.12.2,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.2,License: MIT
0.12.2,categories are expected to be encoded from 0 to n_categories - 1
0.12.2,"list of length n_features of ndarray (n_categories, n_classes)"
0.12.2,compute the counts
0.12.2,normalize by the summing over the classes
0.12.2,silence potential warning due to in-place division by zero
0.12.2,coding: utf-8
0.12.2,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.2,Christos Aridas
0.12.2,License: MIT
0.12.2,##############################################################################
0.12.2,Utilities for testing
0.12.2,import some data to play with
0.12.2,restrict to a binary classification task
0.12.2,add noisy features to make the problem harder and avoid perfect results
0.12.2,"run classifier, get class probabilities and label predictions"
0.12.2,only interested in probabilities of the positive case
0.12.2,XXX: do we really want a special API for the binary case?
0.12.2,##############################################################################
0.12.2,Tests
0.12.2,detailed measures for each class
0.12.2,individual scoring function that can be used for grid search: in the
0.12.2,binary class case the score is the value of the measure for the positive
0.12.2,class (e.g. label == 1). This is deprecated for average != 'binary'.
0.12.2,Such a case may occur with non-stratified cross-validation
0.12.2,ensure the above were meaningful tests:
0.12.2,Bad pos_label
0.12.2,Bad average option
0.12.2,but average != 'binary'; even if data is binary
0.12.2,compute the geometric mean for the binary problem
0.12.2,print classification report with class names
0.12.2,print classification report with label detection
0.12.2,print classification report with class names
0.12.2,print classification report with label detection
0.12.2,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.2,License: MIT
0.12.2,Check basic feature of the metric:
0.12.2,"* the shape of the distance matrix is (n_samples, n_samples)"
0.12.2,* computing pairwise distance of X is the same than explicitely between
0.12.2,X and X.
0.12.2,Check the property of the vdm distance. Let's check the property
0.12.2,"described in ""Improved Heterogeneous Distance Functions"", D.R. Wilson and"
0.12.2,"T.R. Martinez, Journal of Artificial Intelligence Research 6 (1997) 1-34"
0.12.2,https://arxiv.org/pdf/cs/9701101.pdf
0.12.2,
0.12.2,"""if an attribute color has three values red, green and blue, and the"
0.12.2,"application is to identify whether or not an object is an apple, red and"
0.12.2,green would be considered closer than red and blue because the former two
0.12.2,"both have similar correlations with the output class apple."""
0.12.2,defined our feature
0.12.2,0 - not an apple / 1 - an apple
0.12.2,computing the distance between a sample of the same category should
0.12.2,give a null distance
0.12.2,check the property explained in the introduction example
0.12.2,green and red are very close
0.12.2,blue is closer to red than green
0.12.2,"Check that ""auto"" is equivalent to provide the number categories"
0.12.2,beforehand
0.12.2,Check that we raise an error if n_categories is inconsistent with the
0.12.2,number of features in X
0.12.2,Check that we don't get issue when a category is missing between 0
0.12.2,n_categories - 1
0.12.2,remove a categories that could be between 0 and n_categories
0.12.2,Check that we raise a NotFittedError when `fit` is not not called before
0.12.2,pairwise.
0.12.2,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.2,Christos Aridas
0.12.2,License: MIT
0.12.2,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.2,Christos Aridas
0.12.2,License: MIT
0.12.2,FIXME: to be removed in 0.12
0.12.2,The ratio is computed using a one-vs-rest manner. Using majority
0.12.2,in multi-class would lead to slightly different results at the
0.12.2,cost of introducing a new parameter.
0.12.2,rounding may cause new amount for n_samples
0.12.2,the nearest neighbors need to be fitted only on the current class
0.12.2,to find the class NN to generate new samples
0.12.2,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.2,Christos Aridas
0.12.2,License: MIT
0.12.2,smoothed bootstrap imposes to make numerical operation; we need
0.12.2,to be sure to have only numerical data in X
0.12.2,generate a smoothed bootstrap with a perturbation
0.12.2,generate a bootstrap
0.12.2,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.2,Christos Aridas
0.12.2,License: MIT
0.12.2,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.2,Fernando Nogueira
0.12.2,Christos Aridas
0.12.2,License: MIT
0.12.2,negate diagonal elements
0.12.2,identify cluster which are answering the requirements
0.12.2,empty cluster
0.12.2,the cluster is already considered balanced
0.12.2,not enough samples to apply SMOTE
0.12.2,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.2,Fernando Nogueira
0.12.2,Christos Aridas
0.12.2,Dzianis Dudnik
0.12.2,License: MIT
0.12.2,FIXME: to be removed in 0.12
0.12.2,FIXME: to be removed in 0.12
0.12.2,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.2,Fernando Nogueira
0.12.2,Christos Aridas
0.12.2,Dzianis Dudnik
0.12.2,License: MIT
0.12.2,np.newaxis for backwards compatability with random_state
0.12.2,Samples are in danger for m/2 <= m' < m
0.12.2,Samples are noise for m = m'
0.12.2,FIXME: to be removed in 0.12
0.12.2,FIXME: to be removed in 0.12
0.12.2,the input of the OneHotEncoder needs to be dense
0.12.2,SMOTE resampling starts here
0.12.2,"In the edge case where the median of the std is equal to 0, the 1s"
0.12.2,"entries will be also nullified. In this case, we store the original"
0.12.2,categorical encoding which will be later used for inverting the OHE
0.12.2,This variable will be used when generating data
0.12.2,we can replace the 1 entries of the categorical features with the
0.12.2,median of the standard deviation. It will ensure that whenever
0.12.2,"distance is computed between 2 samples, the difference will be equal"
0.12.2,to the median of the standard deviation as in the original paper.
0.12.2,"With one-hot encoding, the median will be repeated twice. We need"
0.12.2,to divide by sqrt(2) such that we only have one median value
0.12.2,contributing to the Euclidean distance
0.12.2,SMOTE resampling ends here
0.12.2,reverse the encoding of the categorical features
0.12.2,the matrix is supposed to be in the CSR format after the stacking
0.12.2,change in sparsity structure more efficient with LIL than CSR
0.12.2,convert to dense array since scipy.sparse doesn't handle 3D
0.12.2,"In the case that the median std was equal to zeros, we have to"
0.12.2,create non-null entry based on the encoded of OHE
0.12.2,tie breaking argmax
0.12.2,generate sample indices that will be used to generate new samples
0.12.2,"for each drawn samples, select its k-neighbors and generate a sample"
0.12.2,"where for each feature individually, each category generated is the"
0.12.2,most common category
0.12.2,FIXME: to be removed in 0.12
0.12.2,the kneigbors search will include the sample itself which is
0.12.2,expected from the original algorithm
0.12.2,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.2,Christos Aridas
0.12.2,Dzianis Dudnik
0.12.2,License: MIT
0.12.2,create 2 random continuous feature
0.12.2,create a categorical feature using some string
0.12.2,create a categorical feature using some integer
0.12.2,return the categories
0.12.2,create 2 random continuous feature
0.12.2,create a categorical feature using some string
0.12.2,create a categorical feature using some integer
0.12.2,return the categories
0.12.2,create 2 random continuous feature
0.12.2,create a categorical feature using some string
0.12.2,create a categorical feature using some integer
0.12.2,return the categories
0.12.2,create 2 random continuous feature
0.12.2,create a categorical feature using some string
0.12.2,create a categorical feature using some integer
0.12.2,return the categories
0.12.2,create 2 random continuous feature
0.12.2,create a categorical feature using some string
0.12.2,create a categorical feature using some integer
0.12.2,part of the common test which apply to SMOTE-NC even if it is not default
0.12.2,constructible
0.12.2,Check that the samplers handle pandas dataframe and pandas series
0.12.2,Cast X and y to not default dtype
0.12.2,Non-regression test for #662
0.12.2,https://github.com/scikit-learn-contrib/imbalanced-learn/issues/662
0.12.2,check that the categorical feature is not random but correspond to the
0.12.2,categories seen in the minority class samples
0.12.2,TODO: only use `sparse_output` when sklearn >= 1.2
0.12.2,TODO(0.13): remove this test
0.12.2,overall check for SMOTEN
0.12.2,check if the SMOTEN resample data as expected
0.12.2,"we generate data such that ""not apple"" will be the minority class and"
0.12.2,"samples from this class will be generated. We will force the ""blue"""
0.12.2,"category to be associated with this class. Therefore, the new generated"
0.12.2,"samples should as well be from the ""blue"" category."
0.12.2,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.2,Christos Aridas
0.12.2,License: MIT
0.12.2,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.2,Christos Aridas
0.12.2,License: MIT
0.12.2,FIXME: we should use to_numpy with pandas >= 0.25
0.12.2,check the random over-sampling with a multiclass problem
0.12.2,check that resampling with heterogeneous dtype is working with basic
0.12.2,resampling
0.12.2,check that we can oversample even with missing or infinite data
0.12.2,regression tests for #605
0.12.2,check that we raise an error when heterogeneous dtype data are given
0.12.2,and a smoothed bootstrap is requested
0.12.2,check that smoothed bootstrap is working for numerical array
0.12.2,check that a shrinkage factor of 0 is equivalent to not create a smoothed
0.12.2,bootstrap
0.12.2,check the behaviour of the shrinkage parameter
0.12.2,the covariance of the data generated with the larger shrinkage factor
0.12.2,should also be larger.
0.12.2,check the validation of the shrinkage parameter
0.12.2,check that m_neighbors is properly set. Regression test for:
0.12.2,https://github.com/scikit-learn-contrib/imbalanced-learn/issues/568
0.12.2,FIXME: to be removed in 0.12
0.12.2,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.2,Christos Aridas
0.12.2,License: MIT
0.12.2,shuffle the indices since the sampler are packing them by class
0.12.2,helper functions
0.12.2,input and output
0.12.2,build the model and weights
0.12.2,"build the loss, predict, and train operator"
0.12.2,Initialization of all variables in the graph
0.12.2,"For each epoch, run accuracy on train and test"
0.12.2,helper functions
0.12.2,input and output
0.12.2,build the model and weights
0.12.2,"build the loss, predict, and train operator"
0.12.2,Initialization of all variables in the graph
0.12.2,"For each epoch, run accuracy on train and test"
0.12.2,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.2,License: MIT
0.12.2,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.2,Fernando Nogueira
0.12.2,Christos Aridas
0.12.2,License: MIT
0.12.2,find which class to not consider
0.12.2,there is a Tomek link between two samples if they are both nearest
0.12.2,neighbors of each others.
0.12.2,Find the nearest neighbour of every point
0.12.2,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.2,Christos Aridas
0.12.2,License: MIT
0.12.2,Randomly get one sample from the majority class
0.12.2,Generate the index to select
0.12.2,Create the set C - One majority samples and all minority
0.12.2,Create the set S - all majority samples
0.12.2,fit knn on C
0.12.2,Check each sample in S if we keep it or drop it
0.12.2,Do not select sample which are already well classified
0.12.2,Classify on S
0.12.2,If the prediction do not agree with the true label
0.12.2,append it in C_x
0.12.2,Keep the index for later
0.12.2,Update C
0.12.2,fit a knn on C
0.12.2,This experimental to speed up the search
0.12.2,Classify all the element in S and avoid to test the
0.12.2,well classified elements
0.12.2,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.2,Dayvid Oliveira
0.12.2,Christos Aridas
0.12.2,License: MIT
0.12.2,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.2,Christos Aridas
0.12.2,License: MIT
0.12.2,Compute the distance considering the farthest neighbour
0.12.2,Sort the list of distance and get the index
0.12.2,Throw a warning to tell the user that we did not have enough samples
0.12.2,to select and that we just select everything
0.12.2,Select the desired number of samples
0.12.2,idx_tmp is relative to the feature selected in the
0.12.2,previous step and we need to find the indirection
0.12.2,fmt: off
0.12.2,fmt: on
0.12.2,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.2,Christos Aridas
0.12.2,License: MIT
0.12.2,select a sample from the current class
0.12.2,create the set composed of all minority samples and one
0.12.2,sample from the current class.
0.12.2,create the set S with removing the seed from S
0.12.2,since that it will be added anyway
0.12.2,apply Tomek cleaning
0.12.2,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.2,Dayvid Oliveira
0.12.2,Christos Aridas
0.12.2,License: MIT
0.12.2,Check the stopping criterion
0.12.2,1. If there is no changes for the vector y
0.12.2,2. If the number of samples in the other class become inferior to
0.12.2,the number of samples in the majority class
0.12.2,3. If one of the class is disappearing
0.12.2,Case 1
0.12.2,Case 2
0.12.2,Case 3
0.12.2,Check the stopping criterion
0.12.2,1. If the number of samples in the other class become inferior to
0.12.2,the number of samples in the majority class
0.12.2,2. If one of the class is disappearing
0.12.2,Case 1else:
0.12.2,overwrite b_min_bec_maj
0.12.2,Case 2
0.12.2,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.2,Christos Aridas
0.12.2,License: MIT
0.12.2,backward compatibility when passing a NearestNeighbors object
0.12.2,clean the neighborhood
0.12.2,compute which classes to consider for cleaning for the A2 group
0.12.2,add an additional sample since the query points contains the original dataset
0.12.2,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.2,Christos Aridas
0.12.2,License: MIT
0.12.2,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.2,Christos Aridas
0.12.2,License: MIT
0.12.2,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.2,Christos Aridas
0.12.2,License: MIT
0.12.2,"with a large `threshold_cleaning`, the algorithm is equivalent to ENN"
0.12.2,set a threshold that we should consider only the class #2
0.12.2,making the threshold slightly smaller to take into account class #1
0.12.2,we should have a more aggressive cleaning with n_neighbors is larger
0.12.2,TODO: remove in 0.14
0.12.2,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.2,Christos Aridas
0.12.2,License: MIT
0.12.2,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.2,Christos Aridas
0.12.2,License: MIT
0.12.2,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.2,Christos Aridas
0.12.2,License: MIT
0.12.2,FIXME: we should use to_numpy with pandas >= 0.25
0.12.2,check that we can undersample even with missing or infinite data
0.12.2,regression tests for #605
0.12.2,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.2,Christos Aridas
0.12.2,License: MIT
0.12.2,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.2,Christos Aridas
0.12.2,License: MIT
0.12.2,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.2,Christos Aridas
0.12.2,License: MIT
0.12.2,TODO: remove in 0.14
0.12.2,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.2,Christos Aridas
0.12.2,License: MIT
0.12.2,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.2,Christos Aridas
0.12.2,License: MIT
0.12.2,TODO: remove in 0.14
0.12.2,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.2,Fernando Nogueira
0.12.2,Christos Aridas
0.12.2,License: MIT
0.12.2,check that the samples selecting by the hard voting corresponds to the
0.12.2,targeted class
0.12.2,non-regression test for:
0.12.2,https://github.com/scikit-learn-contrib/imbalanced-learn/issues/738
0.12.2,Generate valid values for the required parameters
0.12.2,The parameters `*args` and `**kwargs` are ignored since we cannot generate
0.12.2,constraints.
0.12.2,check that there is a constraint for each parameter
0.12.2,this object does not have a valid type for sure for all params
0.12.2,This parameter is not validated
0.12.2,"First, check that the error is raised if param doesn't match any valid type."
0.12.2,"Then, for constraints that are more than a type constraint, check that the"
0.12.2,error is raised if param does match a valid type but does not match any valid
0.12.2,value for this type.
0.12.2,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.2,Christos Aridas
0.12.2,License: MIT
0.12.2,test that all_estimators doesn't find abstract classes.
0.12.2,"For NearMiss, let's check the three algorithms"
0.12.2,Common tests for estimator instances
0.12.2,Authors: Alexandre Gramfort <alexandre.gramfort@inria.fr>
0.12.2,Raghav RV <rvraghav93@gmail.com>
0.12.2,License: BSD 3 clause
0.12.2,scikit-learn >= 1.2
0.12.2,"walk_packages() ignores DeprecationWarnings, now we need to ignore"
0.12.2,FutureWarnings
0.12.2,"mypy error: Module has no attribute ""__path__"""
0.12.2,functions to ignore args / docstring of
0.12.2,Methods where y param should be ignored if y=None by default
0.12.2,numpydoc 0.8.0's docscrape tool raises because of collections.abc under
0.12.2,Python 3.7
0.12.2,Test module docstring formatting
0.12.2,Skip test if numpydoc is not found
0.12.2,XXX unreached code as of v0.22
0.12.2,"pytest tooling, not part of the scikit-learn API"
0.12.2,Exclude non-scikit-learn classes
0.12.2,Now skip docstring test for y when y is None
0.12.2,by default for API reason
0.12.2,Exclude imported functions
0.12.2,Don't test private methods / functions
0.12.2,Test that there are no tabs in our source files
0.12.2,because we don't import
0.12.2,Minimal / degenerate instances: only useful to test the docstrings.
0.12.2,"As certain attributes are present ""only"" if a certain parameter is"
0.12.2,"provided, this checks if the word ""only"" is present in the attribute"
0.12.2,"description, and if not the attribute is required to be present."
0.12.2,ignore deprecation warnings
0.12.2,attributes
0.12.2,properties
0.12.2,ignore properties that raises an AttributeError and deprecated
0.12.2,properties
0.12.2,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.2,License: MIT
0.12.2,check that we can let a pass a regression variable by turning down the
0.12.2,validation
0.12.2,Check that the validation is bypass when calling `fit`
0.12.2,Non-regression test for:
0.12.2,https://github.com/scikit-learn-contrib/imbalanced-learn/issues/782
0.12.2,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.2,Christos Aridas
0.12.2,License: MIT
0.12.2,store timestamp to figure out whether the result of 'fit' has been
0.12.2,cached or not
0.12.2,store timestamp to figure out whether the result of 'fit' has been
0.12.2,cached or not
0.12.2,Pipeline accepts steps as tuple
0.12.2,Test the various init parameters of the pipeline.
0.12.2,Check that we can't instantiate pipelines with objects without fit
0.12.2,method
0.12.2,Smoke test with only an estimator
0.12.2,Check that params are set
0.12.2,Smoke test the repr:
0.12.2,Test with two objects
0.12.2,Check that we can't instantiate with non-transformers on the way
0.12.2,"Note that NoTrans implements fit, but not transform"
0.12.2,Check that params are set
0.12.2,Smoke test the repr:
0.12.2,Check that params are not set when naming them wrong
0.12.2,Test clone
0.12.2,"Check that apart from estimators, the parameters are the same"
0.12.2,Remove estimators that where copied
0.12.2,Test the various methods of the pipeline (anova).
0.12.2,Test with Anova + LogisticRegression
0.12.2,Test that the pipeline can take fit parameters
0.12.2,classifier should return True
0.12.2,and transformer params should not be changed
0.12.2,invalid parameters should raise an error message
0.12.2,Pipeline should pass sample_weight
0.12.2,When sample_weight is None it shouldn't be passed
0.12.2,Test pipeline raises set params error message for nested models.
0.12.2,nested model check
0.12.2,Test the various methods of the pipeline (pca + svm).
0.12.2,Test with PCA + SVC
0.12.2,Test the various methods of the pipeline (preprocessing + svm).
0.12.2,check shapes of various prediction functions
0.12.2,test that the fit_predict method is implemented on a pipeline
0.12.2,test that the fit_predict on pipeline yields same results as applying
0.12.2,transform and clustering steps separately
0.12.2,"As pipeline doesn't clone estimators on construction,"
0.12.2,it must have its own estimators
0.12.2,first compute the transform and clustering step separately
0.12.2,use a pipeline to do the transform and clustering in one step
0.12.2,tests that a pipeline does not have fit_predict method when final
0.12.2,step of pipeline does not have fit_predict defined
0.12.2,tests that Pipeline passes fit_params to intermediate steps
0.12.2,when fit_predict is invoked
0.12.2,Test whether pipeline works with a transformer at the end.
0.12.2,Also test pipeline.transform and pipeline.inverse_transform
0.12.2,test transform and fit_transform:
0.12.2,Test whether pipeline works with a transformer missing fit_transform
0.12.2,test fit_transform:
0.12.2,Directly setting attr
0.12.2,Using set_params
0.12.2,Using set_params to replace single step
0.12.2,With invalid data
0.12.2,Test setting Pipeline steps to None
0.12.2,"for other methods, ensure no AttributeErrors on None:"
0.12.2,mult2 and mult3 are active
0.12.2,Check 'passthrough' step at construction time
0.12.2,Test with Transformer + SVC
0.12.2,Memoize the transformer at the first fit
0.12.2,Get the time stamp of the tranformer in the cached pipeline
0.12.2,Check that cached_pipe and pipe yield identical results
0.12.2,Check that we are reading the cache while fitting
0.12.2,a second time
0.12.2,Check that cached_pipe and pipe yield identical results
0.12.2,Create a new pipeline with cloned estimators
0.12.2,Check that even changing the name step does not affect the cache hit
0.12.2,Check that cached_pipe and pipe yield identical results
0.12.2,Test with Transformer + SVC
0.12.2,Memoize the transformer at the first fit
0.12.2,Get the time stamp of the tranformer in the cached pipeline
0.12.2,Check that cached_pipe and pipe yield identical results
0.12.2,Check that we are reading the cache while fitting
0.12.2,a second time
0.12.2,Check that cached_pipe and pipe yield identical results
0.12.2,Create a new pipeline with cloned estimators
0.12.2,Check that even changing the name step does not affect the cache hit
0.12.2,Check that cached_pipe and pipe yield identical results
0.12.2,Test the various methods of the pipeline (pca + svm).
0.12.2,Test with PCA + SVC
0.12.2,Test the various methods of the pipeline (pca + svm).
0.12.2,Test with PCA + SVC
0.12.2,Test whether pipeline works with a sampler at the end.
0.12.2,Also test pipeline.sampler
0.12.2,test transform and fit_transform:
0.12.2,We round the value near to zero. It seems that PCA has some issue
0.12.2,with that
0.12.2,Test whether pipeline works with a sampler at the end.
0.12.2,Also test pipeline.sampler
0.12.2,Test pipeline using None as preprocessing step and a classifier
0.12.2,"Test pipeline using None, RUS and a classifier"
0.12.2,"Test pipeline using RUS, None and a classifier"
0.12.2,Test pipeline using None step and a sampler
0.12.2,Test pipeline using None and a transformer that implements transform and
0.12.2,inverse_transform
0.12.2,Test the various methods of the pipeline (anova).
0.12.2,Test with RandomUnderSampling + Anova + LogisticRegression
0.12.2,Test the various methods of the pipeline (anova).
0.12.2,Test the various methods of the pipeline (anova).
0.12.2,Test with RandomUnderSampling + Anova + LogisticRegression
0.12.2,tests that Pipeline passes predict_params to the final estimator
0.12.2,when predict is invoked
0.12.2,Test that the score_samples method is implemented on a pipeline.
0.12.2,Test that the score_samples method on pipeline yields same results as
0.12.2,applying transform and score_samples steps separately.
0.12.2,Check the shapes
0.12.2,Check the values
0.12.2,Test that a pipeline does not have score_samples method when the final
0.12.2,step of the pipeline does not have score_samples defined.
0.12.2,Test that the score_samples method is implemented on a pipeline.
0.12.2,Test that the score_samples method on pipeline yields same results as
0.12.2,applying transform and score_samples steps separately.
0.12.2,Check the shapes
0.12.2,Check the values
0.12.2,transformer will not change `y` and sampler will always preserve the type of `y`
0.12.2,transformer will not change `y` and sampler will always preserve the type of `y`
0.12.2,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.2,Christos Aridas
0.12.2,License: MIT
0.12.2,TODO: Remove when SciPy 1.9 is the minimum supported version
0.12.2,TODO: Remove when scikit-learn 1.1 is the minimum supported version
0.12.2,TODO: remove when scikit-learn minimum version is 1.3
0.12.2,we don't want to validate again for each call to partial_fit
0.12.2,TODO: remove when scikit-learn minimum version is 1.3
0.12.2,"Likely a pandas DataFrame, we explicitly check the type to confirm."
0.12.2,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.2,License: MIT
0.12.2,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.2,License: MIT
0.12.2,Adapated from scikit-learn
0.12.2,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.2,License: MIT
0.12.2,scikit-learn >= 1.2
0.12.2,TODO: remove in 0.13
0.12.2,future default in 0.13
0.12.2,we don't filter samplers based on their tag here because we want to make
0.12.2,sure that the fitted attribute does not exist if the tag is not
0.12.2,stipulated
0.12.2,trigger our checks if this is a SamplerMixin
0.12.2,should raise warning if the target is continuous (we cannot raise error)
0.12.2,if the target is multilabel then we should raise an error
0.12.2,IHT does not enforce the number of samples but provide a number
0.12.2,of samples the closest to the desired target.
0.12.2,in this test we will force all samplers to not change the class 1
0.12.2,check that sparse matrices can be passed through the sampler leading to
0.12.2,the same results than dense
0.12.2,Check that the samplers handle pandas dataframe and pandas series
0.12.2,check that we return the same type for dataframes or series types
0.12.2,FIXME: we should use to_numpy with pandas >= 0.25
0.12.2,Check that the samplers handle pandas dataframe and pandas series
0.12.2,check that we return the same type for dataframes or series types
0.12.2,FIXME: we should use to_numpy with pandas >= 0.25
0.12.2,Check that the can samplers handle simple lists
0.12.2,Check that multiclass target lead to the same results than OVA encoding
0.12.2,Cast X and y to not default dtype
0.12.2,Non-regression test for #709
0.12.2,https://github.com/scikit-learn-contrib/imbalanced-learn/issues/709
0.12.2,Check that an informative error is raised when the value of a constructor
0.12.2,parameter does not have an appropriate type or value.
0.12.2,check that there is a constraint for each parameter
0.12.2,this object does not have a valid type for sure for all params
0.12.2,This parameter is not validated
0.12.2,"First, check that the error is raised if param doesn't match any valid type."
0.12.2,the method is not accessible with the current set of parameters
0.12.2,The estimator is a label transformer and take only `y`
0.12.2,"Then, for constraints that are more than a type constraint, check that the"
0.12.2,error is raised if param does match a valid type but does not match any valid
0.12.2,value for this type.
0.12.2,the method is not accessible with the current set of parameters
0.12.2,The estimator is a label transformer and take only `y`
0.12.2,Check that calling `fit` does not raise any warnings about feature names.
0.12.2,Only check imblearn estimators for feature_names_in_ in docstring
0.12.2,partial_fit checks on second call
0.12.2,Do not call partial fit if early_stopping is on
0.12.2,input_features names is not the same length as n_features_in_
0.12.2,error is raised when `input_features` do not match feature_names_in
0.12.2,Adapted from scikit-learn
0.12.2,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.2,License: MIT
0.12.2,Ignore deprecation warnings triggered at import time and from walking
0.12.2,packages
0.12.2,get rid of abstract base classes
0.12.2,get rid of sklearn estimators which have been imported in some classes
0.12.2,"drop duplicates, sort for reproducibility"
0.12.2,itemgetter is used to ensure the sort does not extend to the 2nd item of
0.12.2,the tuple
0.12.2,Author: Adrin Jalali <adrin.jalali@gmail.com>
0.12.2,License: BSD 3 clause
0.12.2,Only the following methods are supported in the routing mechanism. Adding new
0.12.2,methods at the moment involves monkeypatching this list.
0.12.2,"Note that if this list is changed or monkeypatched, the corresponding method"
0.12.2,needs to be added under a TYPE_CHECKING condition like the one done here in
0.12.2,_MetadataRequester
0.12.2,These methods are a composite of other methods and one cannot set their
0.12.2,requests directly. Instead they should be set by setting the requests of the
0.12.2,simple methods which make the composite ones.
0.12.2,Request values
0.12.2,==============
0.12.2,"Each request value needs to be one of the following values, or an alias."
0.12.2,this is used in `__metadata_request__*` attributes to indicate that a
0.12.2,metadata is not present even though it may be present in the
0.12.2,corresponding method's signature.
0.12.2,"this is used whenever a default value is changed, and therefore the user"
0.12.2,"should explicitly set the value, otherwise a warning is shown. An example"
0.12.2,"is when a meta-estimator is only a router, but then becomes also a"
0.12.2,consumer in a new release.
0.12.2,this is the default used in `set_{method}_request` methods to indicate no
0.12.2,change requested by the user.
0.12.2,item is only an alias if it's a valid identifier
0.12.2,Metadata Request for Simple Consumers
0.12.2,=====================================
0.12.2,This section includes MethodMetadataRequest and MetadataRequest which are
0.12.2,used in simple consumers.
0.12.2,this is here for us to use this attribute's value instead of doing
0.12.2,"`isinstance` in our checks, so that we avoid issues when people vendor"
0.12.2,this file instead of using it directly from scikit-learn.
0.12.2,Called when the default attribute access fails with an AttributeError
0.12.2,(either __getattribute__() raises an AttributeError because name is
0.12.2,not an instance attribute or an attribute in the class tree for self;
0.12.2,or __get__() of a name property raises AttributeError). This method
0.12.2,should either return the (computed) attribute value or raise an
0.12.2,AttributeError exception.
0.12.2,https://docs.python.org/3/reference/datamodel.html#object.__getattr__
0.12.2,Metadata Request for Routers
0.12.2,============================
0.12.2,This section includes all objects required for MetadataRouter which is used
0.12.2,"in routers, returned by their ``get_metadata_routing``."
0.12.2,"This namedtuple is used to store a (mapping, routing) pair. Mapping is a"
0.12.2,"MethodMapping object, and routing is the output of `get_metadata_routing`."
0.12.2,MetadataRouter stores a collection of these namedtuples.
0.12.2,A namedtuple storing a single method route. A collection of these namedtuples
0.12.2,is stored in a MetadataRouter.
0.12.2,this is here for us to use this attribute's value instead of doing
0.12.2,"`isinstance`` in our checks, so that we avoid issues when people vendor"
0.12.2,this file instead of using it directly from scikit-learn.
0.12.2,`_self_request` is used if the router is also a consumer.
0.12.2,"_self_request, (added using `add_self_request()`) is treated"
0.12.2,differently from the other objects which are stored in
0.12.2,_route_mappings.
0.12.2,"conflicts are okay if the passed objects are the same, but it's"
0.12.2,an issue if they're different objects.
0.12.2,doing this instead of a try/except since an AttributeError could be raised
0.12.2,for other reasons.
0.12.2,Request method
0.12.2,==============
0.12.2,This section includes what's needed for the request method descriptor and
0.12.2,their dynamic generation in a meta class.
0.12.2,These strings are used to dynamically generate the docstrings for
0.12.2,set_{method}_request methods.
0.12.2,we would want to have a method which accepts only the expected args
0.12.2,Now we set the relevant attributes of the function so that it seems
0.12.2,"like a normal method to the end user, with known expected arguments."
0.12.2,"This code is never run in runtime, but it's here for type checking."
0.12.2,Type checkers fail to understand that the `set_{method}_request`
0.12.2,"methods are dynamically generated, and they complain that they are"
0.12.2,not defined. We define them here to make type checkers happy.
0.12.2,During type checking analyzers assume this to be True.
0.12.2,The following list of defined methods mirrors the list of methods
0.12.2,in SIMPLE_METHODS.
0.12.2,fmt: off
0.12.2,fmt: on
0.12.2,"if there are any issues in the default values, it will be raised"
0.12.2,when ``get_metadata_routing`` is called. Here we are going to
0.12.2,ignore all the issues such as bad defaults etc.
0.12.2,set ``set_{method}_request``` methods
0.12.2,Here we use `isfunction` instead of `ismethod` because calling `getattr`
0.12.2,on a class instead of an instance returns an unbound function.
0.12.2,"ignore the first parameter of the method, which is usually ""self"""
0.12.2,Then overwrite those defaults with the ones provided in
0.12.2,__metadata_request__* attributes. Defaults set in
0.12.2,__metadata_request__* attributes take precedence over signature
0.12.2,sniffing.
0.12.2,need to go through the MRO since this is a class attribute and
0.12.2,``vars`` doesn't report the parent class attributes. We go through
0.12.2,the reverse of the MRO so that child classes have precedence over
0.12.2,their parents.
0.12.2,we don't check for attr.startswith() since python prefixes attrs
0.12.2,starting with __ with the `_ClassName`.
0.12.2,Process Routing in Routers
0.12.2,==========================
0.12.2,This is almost always the only method used in routers to process and route
0.12.2,given metadata. This is to minimize the boilerplate required in routers.
0.12.2,Here the first two arguments are positional only which makes everything
0.12.2,passed as keyword argument a metadata. The first two args also have an `_`
0.12.2,"prefix to reduce the chances of name collisions with the passed metadata, and"
0.12.2,"since they're positional only, users will never type those underscores."
0.12.2,"If routing is not enabled and kwargs are empty, then we don't have to"
0.12.2,"try doing any routing, we can simply return a structure which returns"
0.12.2,an empty dict on routed_params.ANYTHING.ANY_METHOD.
0.12.2,mypy: ignore-errors
0.12.2,update the docstring of the descriptor
0.12.2,"delegate only on instances, not the classes."
0.12.2,this is to allow access to the docstrings.
0.12.2,This makes it possible to use the decorated method as an
0.12.2,"unbound method, for instance when monkeypatching."
0.12.2,mypy: ignore-errors
0.12.2,Inherits from ValueError and TypeError to keep backward compatibility.
0.12.2,We allow parameters to not have a constraint so that third party
0.12.2,estimators can inherit from sklearn estimators without having to
0.12.2,necessarily use the validation tools.
0.12.2,"this constraint is satisfied, no need to check further."
0.12.2,"No constraint is satisfied, raise with an informative message."
0.12.2,Ignore constraints that we don't want to expose in the error
0.12.2,"message, i.e. options that are for internal purpose or not"
0.12.2,officially supported.
0.12.2,The dict of parameter constraints is set as an attribute of the function
0.12.2,to make it possible to dynamically introspect the constraints for
0.12.2,automatic testing.
0.12.2,Map *args/**kwargs to the function signature
0.12.2,ignore self/cls and positional/keyword markers
0.12.2,"When the function is just a wrapper around an estimator, we allow"
0.12.2,"the function to delegate validation to the estimator, but we"
0.12.2,replace the name of the estimator by the name of the function in
0.12.2,the error message to avoid confusion.
0.12.2,better repr if the bounds were given as integers
0.12.2,we use an interval of Real to ignore np.nan that has its own
0.12.2,constraint
0.12.2,"There's no integer outside (-inf, +inf)"
0.12.2,"bounds are -inf, +inf"
0.12.2,"interval is [-inf, +inf]"
0.12.2,special case for ndarray since it can't be instantiated without
0.12.2,arguments
0.12.2,special case for Integral and Real since they are abstract classes
0.12.2,Author: Alexander L. Hayes <hayesall@iu.edu>
0.12.2,License: MIT
0.12.2,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.2,License: MIT
0.12.2,We lost the y.index during resampling. We can safely use X.index to align
0.12.2,them.
0.12.2,We special case the following error:
0.12.2,https://github.com/scikit-learn-contrib/imbalanced-learn/issues/1055
0.12.2,"There is no easy way to have a generic workaround. Here, we detect"
0.12.2,that we have a column with only null values that is datetime64
0.12.2,(resulting from the np.vstack of the resampling).
0.12.2,try again
0.12.2,_is_neighbors_object(nn_object)
0.12.2,check that all keys in sampling_strategy are also in y
0.12.2,check that there is no negative number
0.12.2,check that all keys in sampling_strategy are also in y
0.12.2,ignore first 'self' argument for instance methods
0.12.2,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.2,Christos Aridas
0.12.2,License: MIT
0.12.2,this function could create an equal number of samples
0.12.2,We pass on purpose a non sorted dictionary and check that the resulting
0.12.2,dictionary is sorted. Refer to issue #428.
0.12.2,DataFrame and DataFrame case
0.12.2,DataFrames and Series case
0.12.2,The * is place before a keyword only argument without a default value
0.12.2,Test that the minimum dependencies in the README.rst file are
0.12.2,consistent with the minimum dependencies defined at the file:
0.12.2,imblearn/_min_dependencies.py
0.12.2,Skip the test if the README.rst file is not available.
0.12.2,"For instance, when installing scikit-learn from wheels"
0.12.2,Author: Alexander L. Hayes <hayesall@iu.edu>
0.12.2,License: MIT
0.12.2,Some helpers for the tests
0.12.2,check in the presence of extra positional and keyword args
0.12.2,outer decorator does not interfere with validation
0.12.2,validated method can be decorated
0.12.2,no validation in init
0.12.2,list and dict are valid params
0.12.2,the list option is not exposed in the error message
0.12.2,"""auto"" and ""warn"" are valid params"
0.12.2,"the ""warn"" option is not exposed in the error message"
0.12.2,True/False and np.bool_(True/False) are valid params
0.12.2,param1 is validated
0.12.2,param2 is not validated: any type is valid.
0.12.2,"does not raise, even though ""b"" is not in the constraints dict and ""a"" is not"
0.12.2,a parameter of the estimator.
0.12.2,does not raise
0.12.2,calls f with a bad parameter type
0.12.2,Validation for g is never skipped.
0.12.2,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.2,License: MIT
0.12.2,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.2,Christos Aridas
0.12.2,License: MIT
0.12.2,check if the filtering is working with a list or a single string
0.12.2,check that all estimators are sampler
0.12.2,check that an error is raised when the type is unknown
0.12.2,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.2,License: MIT
0.12.2,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.2,Christos Aridas
0.12.2,License: MIT
0.12.2,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.2,Christos Aridas
0.12.2,License: MIT
0.12.2,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.2,Christos Aridas
0.12.2,License: MIT
0.12.2,Check if default job count is None
0.12.2,Check if job count is set
0.12.2,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.2,Christos Aridas
0.12.2,License: MIT
0.12.2,Check if default job count is none
0.12.2,Check if job count is set
0.12.2,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.2,License: MIT
0.12.2,scikit-learn >= 1.2
0.12.2,resample before to fit the tree
0.12.2,TODO: remove when the minimum supported version of scikit-learn will be 1.4
0.12.2,support for missing values
0.12.2,TODO: remove when the minimum supported version of scikit-learn will be 1.1
0.12.2,change of signature in scikit-learn 1.1
0.12.2,make a deepcopy to not modify the original dictionary
0.12.2,TODO: remove when the minimum supported version of scikit-learn will be 1.4
0.12.2,use scikit-learn support for monotonic constraints
0.12.2,create an attribute for compatibility with other scikit-learn tools such
0.12.2,as HTML representation.
0.12.2,TODO: remove in 0.13
0.12.2,Validate or convert input data
0.12.2,TODO: remove when the minimum supported version of scipy will be 1.4
0.12.2,Support for missing values
0.12.2,TODO: remove when the minimum supported version of scikit-learn will be 1.4
0.12.2,_compute_missing_values_in_feature_mask checks if X has missing values and
0.12.2,will raise an error if the underlying tree base estimator can't handle
0.12.2,missing values. Only the criterion is required to determine if the tree
0.12.2,supports missing values.
0.12.2,Pre-sort indices to avoid that each individual tree of the
0.12.2,ensemble sorts the indices.
0.12.2,reshape is necessary to preserve the data contiguity against vs
0.12.2,"[:, np.newaxis] that does not."
0.12.2,Get bootstrap sample size
0.12.2,Check parameters
0.12.2,"Free allocated memory, if any"
0.12.2,We draw from the random state to get the random state we
0.12.2,would have got if we hadn't used a warm_start.
0.12.2,Parallel loop: we prefer the threading backend as the Cython code
0.12.2,for fitting the trees is internally releasing the Python GIL
0.12.2,making threading more efficient than multiprocessing in
0.12.2,"that case. However, we respect any parallel_backend contexts set"
0.12.2,"at a higher level, since correctness does not rely on using"
0.12.2,threads.
0.12.2,Collect newly grown trees
0.12.2,Create pipeline with the fitted samplers and trees
0.12.2,FIXME: we could consider to support multiclass-multioutput if
0.12.2,we introduce or reuse a constructor parameter (e.g.
0.12.2,oob_score) allowing our user to pass a callable defining the
0.12.2,scoring strategy on OOB sample.
0.12.2,Decapsulate classes_ attributes
0.12.2,drop the n_outputs axis if there is a single output
0.12.2,Prediction requires X to be in CSR format
0.12.2,n_classes_ is a ndarray at this stage
0.12.2,all the supported type of target will have the same number of
0.12.2,classes in all outputs
0.12.2,"for regression, n_classes_ does not exist and we create an empty"
0.12.2,axis to be consistent with the classification case and make
0.12.2,the array operations compatible with the 2 settings
0.12.2,TODO: remove when supporting scikit-learn>=1.2
0.12.2,make a deepcopy to not modify the original dictionary
0.12.2,TODO: remove when minimum supported version of scikit-learn is 1.4
0.12.2,SAMME-R requires predict_proba-enabled estimators
0.12.2,Instances incorrectly classified
0.12.2,Error fraction
0.12.2,Stop if classification is perfect
0.12.2,Construct y coding as described in Zhu et al [2]:
0.12.2,
0.12.2,y_k = 1 if c == k else -1 / (K - 1)
0.12.2,
0.12.2,"where K == n_classes_ and c, k in [0, K) are indices along the second"
0.12.2,axis of the y coding with c being the index corresponding to the true
0.12.2,class label.
0.12.2,Displace zero probabilities so the log is defined.
0.12.2,Also fix negative elements which may occur with
0.12.2,negative sample weights.
0.12.2,Boost weight using multi-class AdaBoost SAMME.R alg
0.12.2,Only boost the weights if it will fit again
0.12.2,Only boost positive weights
0.12.2,Instances incorrectly classified
0.12.2,Error fraction
0.12.2,Stop if classification is perfect
0.12.2,Stop if the error is at least as bad as random guessing
0.12.2,Boost weight using multi-class AdaBoost SAMME alg
0.12.2,Only boost the weights if I will fit again
0.12.2,Only boost positive weights
0.12.2,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.2,Christos Aridas
0.12.2,License: MIT
0.12.2,scikit-learn >= 1.2
0.12.2,make a deepcopy to not modify the original dictionary
0.12.2,TODO: remove when minimum supported version of scikit-learn is 1.4
0.12.2,TODO: remove when supporting scikit-learn>=1.2
0.12.2,overwrite the base class method by disallowing `sample_weight`
0.12.2,RandomUnderSampler is not supporting sample_weight. We need to pass
0.12.2,None.
0.12.2,TODO: remove when minimum supported version of scikit-learn is 1.1
0.12.2,Check data
0.12.2,Parallel loop
0.12.2,Reduce
0.12.2,The base class require to have the attribute defined. For scikit-learn
0.12.2,"> 1.2, we are going to raise an error."
0.12.2,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.2,Christos Aridas
0.12.2,License: MIT
0.12.2,scikit-learn >= 1.2
0.12.2,make a deepcopy to not modify the original dictionary
0.12.2,TODO: remove when minimum supported version of scikit-learn is 1.4
0.12.2,TODO: remove when supporting scikit-learn>=1.2
0.12.2,overwrite the base class method by disallowing `sample_weight`
0.12.2,the sampler needs to be validated before to call _fit because
0.12.2,_validate_y is called before _validate_estimator and would require
0.12.2,to know which type of sampler we are using.
0.12.2,RandomUnderSampler is not supporting sample_weight. We need to pass
0.12.2,None.
0.12.2,TODO: remove when minimum supported version of scikit-learn is 1.1
0.12.2,Check data
0.12.2,Parallel loop
0.12.2,Reduce
0.12.2,The base class require to have the attribute defined. For scikit-learn
0.12.2,"> 1.2, we are going to raise an error."
0.12.2,check that we have an ensemble of samplers and estimators with a
0.12.2,consistent size
0.12.2,each sampler in the ensemble should have different random state
0.12.2,each estimator in the ensemble should have different random state
0.12.2,check the consistency of the feature importances
0.12.2,check the consistency of the prediction outpus
0.12.2,Predictions should be the same when sample_weight are all ones
0.12.2,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.2,Christos Aridas
0.12.2,License: MIT
0.12.2,Check classification for various parameter settings.
0.12.2,Test that bootstrapping samples generate non-perfect base estimators.
0.12.2,"without bootstrap, all trees are perfect on the training set"
0.12.2,disable the resampling by passing an empty dictionary.
0.12.2,"with bootstrap, trees are no longer perfect on the training set"
0.12.2,Test that bootstrapping features may generate duplicate features.
0.12.2,Predict probabilities.
0.12.2,Normal case
0.12.2,"Degenerate case, where some classes are missing"
0.12.2,Check that oob prediction is a good estimation of the generalization
0.12.2,error.
0.12.2,Test with few estimators
0.12.2,Check singleton ensembles.
0.12.2,Check that bagging ensembles can be grid-searched.
0.12.2,Transform iris into a binary classification task
0.12.2,Grid search with scoring based on decision_function
0.12.2,Check estimator and its default values.
0.12.2,Test if fitting incrementally with warm start gives a forest of the
0.12.2,right size and the same results as a normal fit.
0.12.2,Test if warm start'ed second fit with smaller n_estimators raises error.
0.12.2,Test that nothing happens when fitting without increasing n_estimators
0.12.2,"modify X to nonsense values, this should not change anything"
0.12.2,warm started classifier with 5+5 estimators should be equivalent to
0.12.2,one classifier with 10 estimators
0.12.2,Check using oob_score and warm_start simultaneously fails
0.12.2,"Make sure OOB scores are identical when random_state, estimator, and"
0.12.2,training data are fixed and fitting is done twice
0.12.2,Check that format of estimators_samples_ is correct and that results
0.12.2,generated at fit time can be identically reproduced at a later time
0.12.2,using data saved in object attributes.
0.12.2,remap the y outside of the BalancedBaggingclassifier
0.12.2,"_, y = np.unique(y, return_inverse=True)"
0.12.2,Get relevant attributes
0.12.2,Test for correct formatting
0.12.2,Re-fit single estimator to test for consistent sampling
0.12.2,Make sure validated max_samples and original max_samples are identical
0.12.2,when valid integer max_samples supplied by user
0.12.2,check that we can pass any kind of sampler to a bagging classifier
0.12.2,check that we have balanced class with the right counts of class
0.12.2,sample depending on the sampling strategy
0.12.2,check that we can provide a FunctionSampler in BalancedBaggingClassifier
0.12.2,find the minority and majority classes
0.12.2,compute the number of sample to draw from the majority class using
0.12.2,a negative binomial distribution
0.12.2,draw randomly with or without replacement
0.12.2,Roughly Balanced Bagging
0.12.2,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.2,Christos Aridas
0.12.2,License: MIT
0.12.2,Generate a global dataset to use
0.12.2,Check classification for various parameter settings.
0.12.2,test the different prediction function
0.12.2,Check estimator and its default values.
0.12.2,Test if fitting incrementally with warm start gives a forest of the
0.12.2,right size and the same results as a normal fit.
0.12.2,Test if warm start'ed second fit with smaller n_estimators raises error.
0.12.2,Test that nothing happens when fitting without increasing n_estimators
0.12.2,"modify X to nonsense values, this should not change anything"
0.12.2,warm started classifier with 5+5 estimators should be equivalent to
0.12.2,one classifier with 10 estimators
0.12.2,Check warning if not enough estimators
0.12.2,First fit with no restriction on max samples
0.12.2,Second fit with max samples restricted to just 2
0.12.2,Regression test for #655: check that the oob score is closed to 0.5
0.12.2,a binomial experiment.
0.12.2,TODO: remove in 0.13
0.12.2,Create dataset with missing values
0.12.2,Train forest with missing values
0.12.2,Train forest without missing values
0.12.2,Score is still 80 percent of the forest's score that had no missing values
0.12.2,Create a predictive feature using `y` and with some noise
0.12.2,Author: Guillaume Lemaitre
0.12.2,License: BSD 3 clause
0.12.2,"The index start at one, then we need to remove one"
0.12.2,to not have issue with the indexing.
0.12.2,go through the list and check if the data are available
0.12.2,Authors: Dayvid Oliveira
0.12.2,Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.2,Christos Aridas
0.12.2,License: MIT
0.12.2,restrict ratio to be a dict or a callable
0.12.2,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.2,Christos Aridas
0.12.2,License: MIT
0.12.2,"we are reusing part of utils.check_sampling_strategy, however this is not"
0.12.2,cover in the common tests so we will repeat it here
0.12.2,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.2,Christos Aridas
0.12.2,License: MIT
0.12.2,This is a trick to avoid an error during tests collection with pytest. We
0.12.2,avoid the error when importing the package raise the error at the moment of
0.12.2,creating the instance.
0.12.2,This is a trick to avoid an error during tests collection with pytest. We
0.12.2,avoid the error when importing the package raise the error at the moment of
0.12.2,creating the instance.
0.12.2,flag for keras sequence duck-typing
0.12.2,shuffle the indices since the sampler are packing them by class
0.12.1,This file is here so that when running from the root folder
0.12.1,./imblearn is added to sys.path by pytest.
0.12.1,See https://docs.pytest.org/en/latest/pythonpath.html for more details.
0.12.1,"For example, this allows to build extensions in place and run pytest"
0.12.1,doc/modules/clustering.rst and use imblearn from the local folder
0.12.1,rather than the one from site-packages.
0.12.1,! /usr/bin/env python
0.12.1,Python 2 compat: just to be able to declare that Python >=3.7 is needed.
0.12.1,This is a bit (!) hackish: we are setting a global variable so that the
0.12.1,main imblearn __init__ can detect if it is being loaded by the setup
0.12.1,"routine, to avoid attempting to load components that aren't built yet:"
0.12.1,the numpy distutils extensions that are used by imbalanced-learn to
0.12.1,recursively build the compiled extensions in sub-packages is based on the
0.12.1,Python import machinery.
0.12.1,get __version__ from _version.py
0.12.1,-*- coding: utf-8 -*-
0.12.1,
0.12.1,"imbalanced-learn documentation build configuration file, created by"
0.12.1,sphinx-quickstart on Mon Jan 18 14:44:12 2016.
0.12.1,
0.12.1,This file is execfile()d with the current directory set to its
0.12.1,containing dir.
0.12.1,
0.12.1,Note that not all possible configuration values are present in this
0.12.1,autogenerated file.
0.12.1,
0.12.1,All configuration values have a default; values that are commented out
0.12.1,serve to show the default.
0.12.1,"If extensions (or modules to document with autodoc) are in another directory,"
0.12.1,add these directories to sys.path here. If the directory is relative to the
0.12.1,"documentation root, use os.path.abspath to make it absolute, like shown here."
0.12.1,-- General configuration ------------------------------------------------
0.12.1,"If your documentation needs a minimal Sphinx version, state it here."
0.12.1,needs_sphinx = '1.0'
0.12.1,"Add any Sphinx extension module names here, as strings. They can be"
0.12.1,extensions coming with Sphinx (named 'sphinx.ext.*') or your custom
0.12.1,ones.
0.12.1,Specify how to identify the prompt when copying code snippets
0.12.1,"Add any paths that contain templates here, relative to this directory."
0.12.1,The suffix of source filenames.
0.12.1,The master toctree document.
0.12.1,General information about the project.
0.12.1,"The version info for the project you're documenting, acts as replacement for"
0.12.1,"|version| and |release|, also used in various other places throughout the"
0.12.1,built documents.
0.12.1,
0.12.1,The short X.Y version.
0.12.1,"The full version, including alpha/beta/rc tags."
0.12.1,"List of patterns, relative to source directory, that match files and"
0.12.1,directories to ignore when looking for source files.
0.12.1,The reST default role (used for this markup: `text`) to use for all
0.12.1,documents.
0.12.1,"If true, '()' will be appended to :func: etc. cross-reference text."
0.12.1,The name of the Pygments (syntax highlighting) style to use.
0.12.1,-- Options for HTML output ----------------------------------------------
0.12.1,The theme to use for HTML and HTML Help pages.  See the documentation for
0.12.1,a list of builtin themes.
0.12.1,"""twitter_url"": ""https://twitter.com/pandas_dev"","
0.12.1,"""navbar_align"": ""right"",  # For testing that the navbar items align properly"
0.12.1,"Add any paths that contain custom static files (such as style sheets) here,"
0.12.1,"relative to this directory. They are copied after the builtin static files,"
0.12.1,"so a file named ""default.css"" will overwrite the builtin ""default.css""."
0.12.1,Output file base name for HTML help builder.
0.12.1,-- Options for autodoc ------------------------------------------------------
0.12.1,generate autosummary even if no references
0.12.1,-- Options for numpydoc -----------------------------------------------------
0.12.1,this is needed for some reason...
0.12.1,see https://github.com/numpy/numpydoc/issues/69
0.12.1,-- Options for sphinxcontrib-bibtex -----------------------------------------
0.12.1,bibtex file
0.12.1,-- Options for intersphinx --------------------------------------------------
0.12.1,intersphinx configuration
0.12.1,-- Options for sphinx-gallery -----------------------------------------------
0.12.1,Generate the plot for the gallery
0.12.1,sphinx-gallery configuration
0.12.1,-- Options for github link for what's new -----------------------------------
0.12.1,Config for sphinx_issues
0.12.1,The following is used by sphinx.ext.linkcode to provide links to github
0.12.1,-- Options for LaTeX output ---------------------------------------------
0.12.1,The paper size ('letterpaper' or 'a4paper').
0.12.1,"'papersize': 'letterpaper',"
0.12.1,"The font size ('10pt', '11pt' or '12pt')."
0.12.1,"'pointsize': '10pt',"
0.12.1,Additional stuff for the LaTeX preamble.
0.12.1,"'preamble': '',"
0.12.1,Grouping the document tree into LaTeX files. List of tuples
0.12.1,"(source start file, target name, title,"
0.12.1,"author, documentclass [howto, manual, or own class])."
0.12.1,-- Options for manual page output ---------------------------------------
0.12.1,"If false, no module index is generated."
0.12.1,latex_domain_indices = True
0.12.1,One entry per manual page. List of tuples
0.12.1,"(source start file, name, description, authors, manual section)."
0.12.1,"If true, show URL addresses after external links."
0.12.1,man_show_urls = False
0.12.1,-- Options for Texinfo output -------------------------------------------
0.12.1,Grouping the document tree into Texinfo files. List of tuples
0.12.1,"(source start file, target name, title, author,"
0.12.1,"dir menu entry, description, category)"
0.12.1,-- Dependencies generation ----------------------------------------------
0.12.1,get length of header
0.12.1,-- Additional temporary hacks -----------------------------------------------
0.12.1,Temporary work-around for spacing problem between parameter and parameter
0.12.1,"type in the doc, see https://github.com/numpy/numpydoc/issues/215. The bug"
0.12.1,has been fixed in sphinx (https://github.com/sphinx-doc/sphinx/pull/5976) but
0.12.1,through a change in sphinx basic.css except rtd_theme does not use basic.css.
0.12.1,"In an ideal world, this would get fixed in this PR:"
0.12.1,https://github.com/readthedocs/sphinx_rtd_theme/pull/747/files
0.12.1,get the styles from the current theme
0.12.1,create and add the button to all the code blocks that contain >>>
0.12.1,tracebacks (.gt) contain bare text elements that need to be
0.12.1,wrapped in a span to work with .nextUntil() (see later)
0.12.1,define the behavior of the button when it's clicked
0.12.1,hide the code output
0.12.1,show the code output
0.12.1,-*- coding: utf-8 -*-
0.12.1,Format template for issues URI
0.12.1,e.g. 'https://github.com/sloria/marshmallow/issues/{issue}
0.12.1,Format template for PR URI
0.12.1,e.g. 'https://github.com/sloria/marshmallow/pull/{issue}
0.12.1,Format template for commit URI
0.12.1,e.g. 'https://github.com/sloria/marshmallow/commits/{commit}
0.12.1,"Shortcut for Github, e.g. 'sloria/marshmallow'"
0.12.1,Format template for user profile URI
0.12.1,e.g. 'https://github.com/{user}'
0.12.1,Python 2 only
0.12.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.1,License: MIT
0.12.1,%%
0.12.1,%%
0.12.1,"First, we will generate a toy classification dataset with only few samples."
0.12.1,The ratio between the classes will be imbalanced.
0.12.1,%%
0.12.1,%%
0.12.1,"Now, we will use a :class:`~imblearn.over_sampling.RandomOverSampler` to"
0.12.1,generate a bootstrap for the minority class with as many samples as in the
0.12.1,majority class.
0.12.1,%%
0.12.1,%%
0.12.1,We observe that the minority samples are less transparent than the samples
0.12.1,"from the majority class. Indeed, it is due to the fact that these samples"
0.12.1,of the minority class are repeated during the bootstrap generation.
0.12.1,
0.12.1,We can set `shrinkage` to a floating value to add a small perturbation to the
0.12.1,samples created and therefore create a smoothed bootstrap.
0.12.1,%%
0.12.1,%%
0.12.1,"In this case, we see that the samples in the minority class are not"
0.12.1,overlapping anymore due to the added noise.
0.12.1,
0.12.1,The parameter `shrinkage` allows to add more or less perturbation. Let's
0.12.1,add more perturbation when generating the smoothed bootstrap.
0.12.1,%%
0.12.1,%%
0.12.1,Increasing the value of `shrinkage` will disperse the new samples. Forcing
0.12.1,the shrinkage to 0 will be equivalent to generating a normal bootstrap.
0.12.1,%%
0.12.1,%%
0.12.1,"Therefore, the `shrinkage` is handy to manually tune the dispersion of the"
0.12.1,new samples.
0.12.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.1,License: MIT
0.12.1,%%
0.12.1,generate some data points
0.12.1,plot the majority and minority samples
0.12.1,draw the circle in which the new sample will generated
0.12.1,plot the line on which the sample will be generated
0.12.1,create and plot the new sample
0.12.1,make the plot nicer with legend and label
0.12.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.1,License: MIT
0.12.1,%%
0.12.1,%% [markdown]
0.12.1,The following function will be used to create toy dataset. It uses the
0.12.1,:func:`~sklearn.datasets.make_classification` from scikit-learn but fixing
0.12.1,some parameters.
0.12.1,%%
0.12.1,%% [markdown]
0.12.1,The following function will be used to plot the sample space after resampling
0.12.1,to illustrate the specificities of an algorithm.
0.12.1,%%
0.12.1,%% [markdown]
0.12.1,The following function will be used to plot the decision function of a
0.12.1,classifier given some data.
0.12.1,%%
0.12.1,%% [markdown]
0.12.1,Illustration of the influence of the balancing ratio
0.12.1,----------------------------------------------------
0.12.1,
0.12.1,We will first illustrate the influence of the balancing ratio on some toy
0.12.1,data using a logistic regression classifier which is a linear model.
0.12.1,%%
0.12.1,%% [markdown]
0.12.1,We will fit and show the decision boundary model to illustrate the impact of
0.12.1,dealing with imbalanced classes.
0.12.1,%%
0.12.1,%% [markdown]
0.12.1,"Greater is the difference between the number of samples in each class, poorer"
0.12.1,are the classification results.
0.12.1,
0.12.1,Random over-sampling to balance the data set
0.12.1,--------------------------------------------
0.12.1,
0.12.1,Random over-sampling can be used to repeat some samples and balance the
0.12.1,number of samples between the dataset. It can be seen that with this trivial
0.12.1,approach the boundary decision is already less biased toward the majority
0.12.1,class. The class :class:`~imblearn.over_sampling.RandomOverSampler`
0.12.1,implements such of a strategy.
0.12.1,%%
0.12.1,%% [markdown]
0.12.1,"By default, random over-sampling generates a bootstrap. The parameter"
0.12.1,`shrinkage` allows adding a small perturbation to the generated data
0.12.1,to generate a smoothed bootstrap instead. The plot below shows the difference
0.12.1,between the two data generation strategies.
0.12.1,%%
0.12.1,%% [markdown]
0.12.1,It looks like more samples are generated with smoothed bootstrap. This is due
0.12.1,to the fact that the samples generated are not superimposing with the
0.12.1,original samples.
0.12.1,
0.12.1,More advanced over-sampling using ADASYN and SMOTE
0.12.1,--------------------------------------------------
0.12.1,
0.12.1,Instead of repeating the same samples when over-sampling or perturbating the
0.12.1,"generated bootstrap samples, one can use some specific heuristic instead."
0.12.1,:class:`~imblearn.over_sampling.ADASYN` and
0.12.1,:class:`~imblearn.over_sampling.SMOTE` can be used in this case.
0.12.1,%%
0.12.1,%% [markdown]
0.12.1,The following plot illustrates the difference between
0.12.1,:class:`~imblearn.over_sampling.ADASYN` and
0.12.1,:class:`~imblearn.over_sampling.SMOTE`.
0.12.1,:class:`~imblearn.over_sampling.ADASYN` will focus on the samples which are
0.12.1,difficult to classify with a nearest-neighbors rule while regular
0.12.1,:class:`~imblearn.over_sampling.SMOTE` will not make any distinction.
0.12.1,"Therefore, the decision function depending of the algorithm."
0.12.1,%% [markdown]
0.12.1,"Due to those sampling particularities, it can give rise to some specific"
0.12.1,issues as illustrated below.
0.12.1,%%
0.12.1,%% [markdown]
0.12.1,SMOTE proposes several variants by identifying specific samples to consider
0.12.1,during the resampling. The borderline version
0.12.1,(:class:`~imblearn.over_sampling.BorderlineSMOTE`) will detect which point to
0.12.1,select which are in the border between two classes. The SVM version
0.12.1,(:class:`~imblearn.over_sampling.SVMSMOTE`) will use the support vectors
0.12.1,found using an SVM algorithm to create new sample while the KMeans version
0.12.1,(:class:`~imblearn.over_sampling.KMeansSMOTE`) will make a clustering before
0.12.1,to generate samples in each cluster independently depending each cluster
0.12.1,density.
0.12.1,%%
0.12.1,%% [markdown]
0.12.1,"When dealing with a mixed of continuous and categorical features,"
0.12.1,:class:`~imblearn.over_sampling.SMOTENC` is the only method which can handle
0.12.1,this case.
0.12.1,%%
0.12.1,Create a dataset of a mix of numerical and categorical data
0.12.1,%% [markdown]
0.12.1,"However, if the dataset is composed of only categorical features then one"
0.12.1,should use :class:`~imblearn.over_sampling.SMOTEN`.
0.12.1,%%
0.12.1,Generate only categorical data
0.12.1,Authors: Christos Aridas
0.12.1,Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.1,License: MIT
0.12.1,%%
0.12.1,%% [markdown]
0.12.1,Let's first generate a dataset with imbalanced class distribution.
0.12.1,%%
0.12.1,%% [markdown]
0.12.1,We will use an over-sampler :class:`~imblearn.over_sampling.SMOTE` followed
0.12.1,by a :class:`~sklearn.tree.DecisionTreeClassifier`. The aim will be to
0.12.1,search which `k_neighbors` parameter is the most adequate with the dataset
0.12.1,that we generated.
0.12.1,%%
0.12.1,%% [markdown]
0.12.1,We can use the :class:`~sklearn.model_selection.validation_curve` to inspect
0.12.1,"the impact of varying the parameter `k_neighbors`. In this case, we need"
0.12.1,to use a score to evaluate the generalization score during the
0.12.1,cross-validation.
0.12.1,%%
0.12.1,%%
0.12.1,%% [markdown]
0.12.1,We can now plot the results of the cross-validation for the different
0.12.1,parameter values that we tried.
0.12.1,%%
0.12.1,make nice plotting
0.12.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.1,License: MIT
0.12.1,Generate a dataset
0.12.1,Split the data
0.12.1,Train the classifier with balancing
0.12.1,Test the classifier and get the prediction
0.12.1,Show the classification report
0.12.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.1,License: MIT
0.12.1,%%
0.12.1,%% [markdown]
0.12.1,"First, we will generate some imbalanced dataset."
0.12.1,%%
0.12.1,%% [markdown]
0.12.1,We will split the data into a training and testing set.
0.12.1,%%
0.12.1,%% [markdown]
0.12.1,We will create a pipeline made of a :class:`~imblearn.over_sampling.SMOTE`
0.12.1,over-sampler followed by a :class:`~sklearn.linear_model.LogisticRegression`
0.12.1,classifier.
0.12.1,%%
0.12.1,%% [markdown]
0.12.1,"Now, we will train the model on the training set and get the prediction"
0.12.1,associated with the testing set. Be aware that the resampling will happen
0.12.1,only when calling `fit`: the number of samples in `y_pred` is the same than
0.12.1,in `y_test`.
0.12.1,%%
0.12.1,%% [markdown]
0.12.1,The geometric mean corresponds to the square root of the product of the
0.12.1,sensitivity and specificity. Combining the two metrics should account for
0.12.1,the balancing of the dataset.
0.12.1,%%
0.12.1,%% [markdown]
0.12.1,The index balanced accuracy can transform any metric to be used in
0.12.1,imbalanced learning problems.
0.12.1,%%
0.12.1,%%
0.12.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.1,License: MIT
0.12.1,%%
0.12.1,%% [markdown]
0.12.1,Dataset generation
0.12.1,------------------
0.12.1,
0.12.1,We will create an imbalanced dataset with a couple of samples. We will use
0.12.1,:func:`~sklearn.datasets.make_classification` to generate this dataset.
0.12.1,%%
0.12.1,%%
0.12.1,%% [markdown]
0.12.1,The following function will be used to plot the sample space after resampling
0.12.1,to illustrate the characteristic of an algorithm.
0.12.1,%%
0.12.1,%% [markdown]
0.12.1,The following function will be used to plot the decision function of a
0.12.1,classifier given some data.
0.12.1,%%
0.12.1,%% [markdown]
0.12.1,":class:`~imblearn.over_sampling.SMOTE` allows to generate samples. However,"
0.12.1,this method of over-sampling does not have any knowledge regarding the
0.12.1,"underlying distribution. Therefore, some noisy samples can be generated, e.g."
0.12.1,"when the different classes cannot be well separated. Hence, it can be"
0.12.1,beneficial to apply an under-sampling algorithm to clean the noisy samples.
0.12.1,Two methods are usually used in the literature: (i) Tomek's link and (ii)
0.12.1,edited nearest neighbours cleaning methods. Imbalanced-learn provides two
0.12.1,ready-to-use samplers :class:`~imblearn.combine.SMOTETomek` and
0.12.1,":class:`~imblearn.combine.SMOTEENN`. In general,"
0.12.1,:class:`~imblearn.combine.SMOTEENN` cleans more noisy data than
0.12.1,:class:`~imblearn.combine.SMOTETomek`.
0.12.1,%%
0.12.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.1,License: MIT
0.12.1,%%
0.12.1,%% [markdown]
0.12.1,Load an imbalanced dataset
0.12.1,--------------------------
0.12.1,
0.12.1,We will load the UCI SatImage dataset which has an imbalanced ratio of 9.3:1
0.12.1,(number of majority sample for a minority sample). The data are then split
0.12.1,into training and testing.
0.12.1,%%
0.12.1,%% [markdown]
0.12.1,Classification using a single decision tree
0.12.1,-------------------------------------------
0.12.1,
0.12.1,We train a decision tree classifier which will be used as a baseline for the
0.12.1,rest of this example.
0.12.1,
0.12.1,The results are reported in terms of balanced accuracy and geometric mean
0.12.1,which are metrics widely used in the literature to validate model trained on
0.12.1,imbalanced set.
0.12.1,%%
0.12.1,%%
0.12.1,%%
0.12.1,%% [markdown]
0.12.1,Classification using bagging classifier with and without sampling
0.12.1,-----------------------------------------------------------------
0.12.1,
0.12.1,"Instead of using a single tree, we will check if an ensemble of decision tree"
0.12.1,"can actually alleviate the issue induced by the class imbalancing. First, we"
0.12.1,will use a bagging classifier and its counter part which internally uses a
0.12.1,random under-sampling to balanced each bootstrap sample.
0.12.1,%%
0.12.1,%% [markdown]
0.12.1,Balancing each bootstrap sample allows to increase significantly the balanced
0.12.1,accuracy and the geometric mean.
0.12.1,%%
0.12.1,%%
0.12.1,%% [markdown]
0.12.1,Classification using random forest classifier with and without sampling
0.12.1,-----------------------------------------------------------------------
0.12.1,
0.12.1,Random forest is another popular ensemble method and it is usually
0.12.1,"outperforming bagging. Here, we used a vanilla random forest and its balanced"
0.12.1,counterpart in which each bootstrap sample is balanced.
0.12.1,%%
0.12.1,%% [markdown]
0.12.1,"Similarly to the previous experiment, the balanced classifier outperform the"
0.12.1,"classifier which learn from imbalanced bootstrap samples. In addition, random"
0.12.1,forest outperforms the bagging classifier.
0.12.1,%%
0.12.1,%%
0.12.1,%% [markdown]
0.12.1,Boosting classifier
0.12.1,-------------------
0.12.1,
0.12.1,"In the same manner, easy ensemble classifier is a bag of balanced AdaBoost"
0.12.1,"classifier. However, it will be slower to train than random forest and will"
0.12.1,achieve worse performance.
0.12.1,%%
0.12.1,%%
0.12.1,%%
0.12.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.1,License: MIT
0.12.1,%%
0.12.1,%% [markdown]
0.12.1,Generate an imbalanced dataset
0.12.1,------------------------------
0.12.1,
0.12.1,"For this example, we will create a synthetic dataset using the function"
0.12.1,:func:`~sklearn.datasets.make_classification`. The problem will be a toy
0.12.1,classification problem with a ratio of 1:9 between the two classes.
0.12.1,%%
0.12.1,%%
0.12.1,%% [markdown]
0.12.1,"In the following sections, we will show a couple of algorithms that have"
0.12.1,been proposed over the years. We intend to illustrate how one can reuse the
0.12.1,:class:`~imblearn.ensemble.BalancedBaggingClassifier` by passing different
0.12.1,sampler.
0.12.1,%%
0.12.1,%% [markdown]
0.12.1,Exactly Balanced Bagging and Over-Bagging
0.12.1,-----------------------------------------
0.12.1,
0.12.1,The :class:`~imblearn.ensemble.BalancedBaggingClassifier` can use in
0.12.1,conjunction with a :class:`~imblearn.under_sampling.RandomUnderSampler` or
0.12.1,:class:`~imblearn.over_sampling.RandomOverSampler`. These methods are
0.12.1,"referred as Exactly Balanced Bagging and Over-Bagging, respectively and have"
0.12.1,been proposed first in [1]_.
0.12.1,%%
0.12.1,Exactly Balanced Bagging
0.12.1,%%
0.12.1,Over-bagging
0.12.1,%% [markdown]
0.12.1,SMOTE-Bagging
0.12.1,-------------
0.12.1,
0.12.1,Instead of using a :class:`~imblearn.over_sampling.RandomOverSampler` that
0.12.1,"make a bootstrap, an alternative is to use"
0.12.1,:class:`~imblearn.over_sampling.SMOTE` as an over-sampler. This is known as
0.12.1,SMOTE-Bagging [2]_.
0.12.1,%%
0.12.1,SMOTE-Bagging
0.12.1,%% [markdown]
0.12.1,Roughly Balanced Bagging
0.12.1,------------------------
0.12.1,While using a :class:`~imblearn.under_sampling.RandomUnderSampler` or
0.12.1,:class:`~imblearn.over_sampling.RandomOverSampler` will create exactly the
0.12.1,"desired number of samples, it does not follow the statistical spirit wanted"
0.12.1,in the bagging framework. The authors in [3]_ proposes to use a negative
0.12.1,binomial distribution to compute the number of samples of the majority
0.12.1,class to be selected and then perform a random under-sampling.
0.12.1,
0.12.1,"Here, we illustrate this method by implementing a function in charge of"
0.12.1,resampling and use the :class:`~imblearn.FunctionSampler` to integrate it
0.12.1,within a :class:`~imblearn.pipeline.Pipeline` and
0.12.1,:class:`~sklearn.model_selection.cross_validate`.
0.12.1,%%
0.12.1,find the minority and majority classes
0.12.1,compute the number of sample to draw from the majority class using
0.12.1,a negative binomial distribution
0.12.1,draw randomly with or without replacement
0.12.1,Roughly Balanced Bagging
0.12.1,%% [markdown]
0.12.1,.. topic:: References:
0.12.1,
0.12.1,".. [1] R. Maclin, and D. Opitz. ""An empirical evaluation of bagging and"
0.12.1,"boosting."" AAAI/IAAI 1997 (1997): 546-551."
0.12.1,
0.12.1,".. [2] S. Wang, and X. Yao. ""Diversity analysis on imbalanced data sets by"
0.12.1,"using ensemble models."" 2009 IEEE symposium on computational"
0.12.1,"intelligence and data mining. IEEE, 2009."
0.12.1,
0.12.1,".. [3] S. Hido, H. Kashima, and Y. Takahashi. ""Roughly balanced bagging"
0.12.1,"for imbalanced data."" Statistical Analysis and Data Mining: The ASA"
0.12.1,Data Science Journal 2.5‚Äê6 (2009): 412-426.
0.12.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.1,License: MIT
0.12.1,%%
0.12.1,%% [markdown]
0.12.1,The following function will be used to create toy dataset. It uses the
0.12.1,:func:`~sklearn.datasets.make_classification` from scikit-learn but fixing
0.12.1,some parameters.
0.12.1,%%
0.12.1,%% [markdown]
0.12.1,The following function will be used to plot the sample space after resampling
0.12.1,to illustrate the specificities of an algorithm.
0.12.1,%%
0.12.1,%% [markdown]
0.12.1,The following function will be used to plot the decision function of a
0.12.1,classifier given some data.
0.12.1,%%
0.12.1,%%
0.12.1,%% [markdown]
0.12.1,Prototype generation: under-sampling by generating new samples
0.12.1,--------------------------------------------------------------
0.12.1,
0.12.1,:class:`~imblearn.under_sampling.ClusterCentroids` under-samples by replacing
0.12.1,the original samples by the centroids of the cluster found.
0.12.1,%%
0.12.1,%% [markdown]
0.12.1,Prototype selection: under-sampling by selecting existing samples
0.12.1,-----------------------------------------------------------------
0.12.1,
0.12.1,The algorithm performing prototype selection can be subdivided into two
0.12.1,groups: (i) the controlled under-sampling methods and (ii) the cleaning
0.12.1,under-sampling methods.
0.12.1,
0.12.1,"With the controlled under-sampling methods, the number of samples to be"
0.12.1,selected can be specified.
0.12.1,:class:`~imblearn.under_sampling.RandomUnderSampler` is the most naive way of
0.12.1,performing such selection by randomly selecting a given number of samples by
0.12.1,the targeted class.
0.12.1,%%
0.12.1,%% [markdown]
0.12.1,:class:`~imblearn.under_sampling.NearMiss` algorithms implement some
0.12.1,heuristic rules in order to select samples. NearMiss-1 selects samples from
0.12.1,the majority class for which the average distance of the :math:`k`` nearest
0.12.1,samples of the minority class is the smallest. NearMiss-2 selects the samples
0.12.1,from the majority class for which the average distance to the farthest
0.12.1,samples of the negative class is the smallest. NearMiss-3 is a 2-step
0.12.1,"algorithm: first, for each minority sample, their :math:`m`"
0.12.1,"nearest-neighbors will be kept; then, the majority samples selected are the"
0.12.1,on for which the average distance to the :math:`k` nearest neighbors is the
0.12.1,largest.
0.12.1,%%
0.12.1,%% [markdown]
0.12.1,:class:`~imblearn.under_sampling.EditedNearestNeighbours` removes samples of
0.12.1,the majority class for which their class differ from the one of their
0.12.1,nearest-neighbors. This sieve can be repeated which is the principle of the
0.12.1,:class:`~imblearn.under_sampling.RepeatedEditedNearestNeighbours`.
0.12.1,:class:`~imblearn.under_sampling.AllKNN` is slightly different from the
0.12.1,:class:`~imblearn.under_sampling.RepeatedEditedNearestNeighbours` by changing
0.12.1,"the :math:`k` parameter of the internal nearest neighors algorithm,"
0.12.1,increasing it at each iteration.
0.12.1,%%
0.12.1,%% [markdown]
0.12.1,:class:`~imblearn.under_sampling.CondensedNearestNeighbour` makes use of a
0.12.1,1-NN to iteratively decide if a sample should be kept in a dataset or not.
0.12.1,The issue is that :class:`~imblearn.under_sampling.CondensedNearestNeighbour`
0.12.1,is sensitive to noise by preserving the noisy samples.
0.12.1,:class:`~imblearn.under_sampling.OneSidedSelection` also used the 1-NN and
0.12.1,use :class:`~imblearn.under_sampling.TomekLinks` to remove the samples
0.12.1,considered noisy. The
0.12.1,:class:`~imblearn.under_sampling.NeighbourhoodCleaningRule` use a
0.12.1,:class:`~imblearn.under_sampling.EditedNearestNeighbours` to remove some
0.12.1,"sample. Additionally, they use a 3 nearest-neighbors to remove samples which"
0.12.1,do not agree with this rule.
0.12.1,%%
0.12.1,%% [markdown]
0.12.1,:class:`~imblearn.under_sampling.InstanceHardnessThreshold` uses the
0.12.1,prediction of classifier to exclude samples. All samples which are classified
0.12.1,with a low probability will be removed.
0.12.1,%%
0.12.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.1,License: MIT
0.12.1,%%
0.12.1,%% [markdown]
0.12.1,This function allows to make nice plotting
0.12.1,%%
0.12.1,%% [markdown]
0.12.1,We will generate some toy data that illustrates how
0.12.1,:class:`~imblearn.under_sampling.TomekLinks` is used to clean a dataset.
0.12.1,%%
0.12.1,%% [markdown]
0.12.1,"In the figure above, the samples highlighted in green form a Tomek link since"
0.12.1,they are of different classes and are nearest neighbors of each other.
0.12.1,highlight the samples of interest
0.12.1,%% [markdown]
0.12.1,We can run the :class:`~imblearn.under_sampling.TomekLinks` sampling to
0.12.1,remove the corresponding samples. If `sampling_strategy='auto'` only the
0.12.1,sample from the majority class will be removed. If `sampling_strategy='all'`
0.12.1,both samples will be removed.
0.12.1,%%
0.12.1,highlight the samples of interest
0.12.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.1,License: MIT
0.12.1,%%
0.12.1,%% [markdown]
0.12.1,We define a function allowing to make some nice decoration on the plot.
0.12.1,%%
0.12.1,%% [markdown]
0.12.1,We can start by generating some data to later illustrate the principle of
0.12.1,each :class:`~imblearn.under_sampling.NearMiss` heuristic rules.
0.12.1,%%
0.12.1,%% [mardown]
0.12.1,NearMiss-1
0.12.1,----------
0.12.1,
0.12.1,NearMiss-1 selects samples from the majority class for which the average
0.12.1,distance to some nearest neighbours is the smallest. In the following
0.12.1,"example, we use a 3-NN to compute the average distance on 2 specific samples"
0.12.1,"of the majority class. Therefore, in this case the point linked by the"
0.12.1,green-dashed line will be selected since the average distance is smaller.
0.12.1,%%
0.12.1,%% [mardown]
0.12.1,NearMiss-2
0.12.1,----------
0.12.1,
0.12.1,NearMiss-2 selects samples from the majority class for which the average
0.12.1,distance to the farthest neighbors is the smallest. With the same
0.12.1,"configuration as previously presented, the sample linked to the green-dashed"
0.12.1,line will be selected since its distance the 3 farthest neighbors is the
0.12.1,smallest.
0.12.1,%%
0.12.1,%% [mardown]
0.12.1,NearMiss-3
0.12.1,----------
0.12.1,
0.12.1,"NearMiss-3 can be divided into 2 steps. First, a nearest-neighbors is used to"
0.12.1,short-list samples from the majority class (i.e. correspond to the
0.12.1,"highlighted samples in the following plot). Then, the sample with the largest"
0.12.1,average distance to the *k* nearest-neighbors are selected.
0.12.1,%%
0.12.1,select only the majority point of interest
0.12.1,Authors: Christos Aridas
0.12.1,Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.1,License: MIT
0.12.1,%%
0.12.1,%% [markdown]
0.12.1,Let's first create an imbalanced dataset and split in to two sets.
0.12.1,%%
0.12.1,%% [markdown]
0.12.1,"Now, we will create each individual steps that we would like later to combine"
0.12.1,%%
0.12.1,%% [markdown]
0.12.1,"Now, we can finally create a pipeline to specify in which order the different"
0.12.1,transformers and samplers should be executed before to provide the data to
0.12.1,the final classifier.
0.12.1,%%
0.12.1,%% [markdown]
0.12.1,We can now use the pipeline created as a normal classifier where resampling
0.12.1,"will happen when calling `fit` and disabled when calling `decision_function`,"
0.12.1,"`predict_proba`, or `predict`."
0.12.1,%%
0.12.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.1,License: MIT
0.12.1,##############################################################################
0.12.1,Data loading
0.12.1,##############################################################################
0.12.1,##############################################################################
0.12.1,"First, you should download the Porto Seguro data set from Kaggle. See the"
0.12.1,link in the introduction.
0.12.1,##############################################################################
0.12.1,The data set is imbalanced and it will have an effect on the fitting.
0.12.1,##############################################################################
0.12.1,Define the pre-processing pipeline
0.12.1,##############################################################################
0.12.1,##############################################################################
0.12.1,We want to standard scale the numerical features while we want to one-hot
0.12.1,"encode the categorical features. In this regard, we make use of the"
0.12.1,:class:`~sklearn.compose.ColumnTransformer`.
0.12.1,Create an environment variable to avoid using the GPU. This can be changed.
0.12.1,##############################################################################
0.12.1,Create a neural-network
0.12.1,##############################################################################
0.12.1,##############################################################################
0.12.1,We create a decorator to report the computation time
0.12.1,##############################################################################
0.12.1,The first model will be trained using the ``fit`` method and with imbalanced
0.12.1,mini-batches.
0.12.1,predict_proba was removed in tensorflow 2.6
0.12.1,##############################################################################
0.12.1,"In the contrary, we will use imbalanced-learn to create a generator of"
0.12.1,mini-batches which will yield balanced mini-batches.
0.12.1,##############################################################################
0.12.1,Classification loop
0.12.1,##############################################################################
0.12.1,##############################################################################
0.12.1,We will perform a 10-fold cross-validation and train the neural-network with
0.12.1,the two different strategies previously presented.
0.12.1,##############################################################################
0.12.1,Plot of the results and computation time
0.12.1,##############################################################################
0.12.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.1,License: MIT
0.12.1,%%
0.12.1,%% [markdown]
0.12.1,Problem definition
0.12.1,------------------
0.12.1,
0.12.1,We are dropping the following features:
0.12.1,
0.12.1,"- ""fnlwgt"": this feature was created while studying the ""adult"" dataset."
0.12.1,"Thus, we will not use this feature which is not acquired during the survey."
0.12.1,"- ""education-num"": it is encoding the same information than ""education""."
0.12.1,"Thus, we are removing one of these 2 features."
0.12.1,%%
0.12.1,%% [markdown]
0.12.1,"The ""adult"" dataset as a class ratio of about 3:1"
0.12.1,%%
0.12.1,%% [markdown]
0.12.1,This dataset is only slightly imbalanced. To better highlight the effect of
0.12.1,"learning from an imbalanced dataset, we will increase its ratio to 30:1"
0.12.1,%%
0.12.1,%% [markdown]
0.12.1,We will perform a cross-validation evaluation to get an estimate of the test
0.12.1,score.
0.12.1,
0.12.1,"As a baseline, we could use a classifier which will always predict the"
0.12.1,majority class independently of the features provided.
0.12.1,%%
0.12.1,%% [markdown]
0.12.1,"Instead of using the accuracy, we can use the balanced accuracy which will"
0.12.1,take into account the balancing issue.
0.12.1,%%
0.12.1,%% [markdown]
0.12.1,Strategies to learn from an imbalanced dataset
0.12.1,----------------------------------------------
0.12.1,We will use a dictionary and a list to continuously store the results of
0.12.1,our experiments and show them as a pandas dataframe.
0.12.1,%%
0.12.1,%% [markdown]
0.12.1,Dummy baseline
0.12.1,..............
0.12.1,
0.12.1,"Before to train a real machine learning model, we can store the results"
0.12.1,obtained with our :class:`~sklearn.dummy.DummyClassifier`.
0.12.1,%%
0.12.1,%% [markdown]
0.12.1,Linear classifier baseline
0.12.1,..........................
0.12.1,
0.12.1,We will create a machine learning pipeline using a
0.12.1,":class:`~sklearn.linear_model.LogisticRegression` classifier. In this regard,"
0.12.1,we will need to one-hot encode the categorical columns and standardized the
0.12.1,numerical columns before to inject the data into the
0.12.1,:class:`~sklearn.linear_model.LogisticRegression` classifier.
0.12.1,
0.12.1,"First, we define our numerical and categorical pipelines."
0.12.1,%%
0.12.1,%% [markdown]
0.12.1,"Then, we can create a preprocessor which will dispatch the categorical"
0.12.1,columns to the categorical pipeline and the numerical columns to the
0.12.1,numerical pipeline
0.12.1,%%
0.12.1,%% [markdown]
0.12.1,"Finally, we connect our preprocessor with our"
0.12.1,:class:`~sklearn.linear_model.LogisticRegression`. We can then evaluate our
0.12.1,model.
0.12.1,%%
0.12.1,%%
0.12.1,%% [markdown]
0.12.1,We can see that our linear model is learning slightly better than our dummy
0.12.1,"baseline. However, it is impacted by the class imbalance."
0.12.1,
0.12.1,We can verify that something similar is happening with a tree-based model
0.12.1,such as :class:`~sklearn.ensemble.RandomForestClassifier`. With this type of
0.12.1,"classifier, we will not need to scale the numerical data, and we will only"
0.12.1,need to ordinal encode the categorical data.
0.12.1,%%
0.12.1,%%
0.12.1,%% [markdown]
0.12.1,The :class:`~sklearn.ensemble.RandomForestClassifier` is as well affected by
0.12.1,"the class imbalanced, slightly less than the linear model. Now, we will"
0.12.1,present different approach to improve the performance of these 2 models.
0.12.1,
0.12.1,Use `class_weight`
0.12.1,..................
0.12.1,
0.12.1,Most of the models in `scikit-learn` have a parameter `class_weight`. This
0.12.1,parameter will affect the computation of the loss in linear model or the
0.12.1,criterion in the tree-based model to penalize differently a false
0.12.1,classification from the minority and majority class. We can set
0.12.1,"`class_weight=""balanced""` such that the weight applied is inversely"
0.12.1,proportional to the class frequency. We test this parametrization in both
0.12.1,linear model and tree-based model.
0.12.1,%%
0.12.1,%%
0.12.1,%% [markdown]
0.12.1,We can see that using `class_weight` was really effective for the linear
0.12.1,"model, alleviating the issue of learning from imbalanced classes. However,"
0.12.1,the :class:`~sklearn.ensemble.RandomForestClassifier` is still biased toward
0.12.1,"the majority class, mainly due to the criterion which is not suited enough to"
0.12.1,fight the class imbalance.
0.12.1,
0.12.1,Resample the training set during learning
0.12.1,.........................................
0.12.1,
0.12.1,Another way is to resample the training set by under-sampling or
0.12.1,over-sampling some of the samples. `imbalanced-learn` provides some samplers
0.12.1,to do such processing.
0.12.1,%%
0.12.1,%%
0.12.1,%%
0.12.1,%%
0.12.1,%% [markdown]
0.12.1,Applying a random under-sampler before the training of the linear model or
0.12.1,"random forest, allows to not focus on the majority class at the cost of"
0.12.1,making more mistake for samples in the majority class (i.e. decreased
0.12.1,accuracy).
0.12.1,
0.12.1,We could apply any type of samplers and find which sampler is working best
0.12.1,on the current dataset.
0.12.1,
0.12.1,"Instead, we will present another way by using classifiers which will apply"
0.12.1,sampling internally.
0.12.1,
0.12.1,Use of specific balanced algorithms from imbalanced-learn
0.12.1,.........................................................
0.12.1,
0.12.1,We already showed that random under-sampling can be effective on decision
0.12.1,"tree. However, instead of under-sampling once the dataset, one could"
0.12.1,under-sample the original dataset before to take a bootstrap sample. This is
0.12.1,the base of the :class:`imblearn.ensemble.BalancedRandomForestClassifier` and
0.12.1,:class:`~imblearn.ensemble.BalancedBaggingClassifier`.
0.12.1,%%
0.12.1,%%
0.12.1,%% [markdown]
0.12.1,The performance with the
0.12.1,:class:`~imblearn.ensemble.BalancedRandomForestClassifier` is better than
0.12.1,applying a single random under-sampling. We will use a gradient-boosting
0.12.1,classifier within a :class:`~imblearn.ensemble.BalancedBaggingClassifier`.
0.12.1,%% [markdown]
0.12.1,This last approach is the most effective. The different under-sampling allows
0.12.1,to bring some diversity for the different GBDT to learn and not focus on a
0.12.1,portion of the majority class.
0.12.1,Authors: Christos Aridas
0.12.1,Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.1,License: MIT
0.12.1,%%
0.12.1,%% [markdown]
0.12.1,Load the dataset
0.12.1,----------------
0.12.1,
0.12.1,We will use a dataset containing image from know person where we will
0.12.1,build a model to recognize the person on the image. We will make this problem
0.12.1,a binary problem by taking picture of only George W. Bush and Bill Clinton.
0.12.1,%%
0.12.1,%%
0.12.1,%% [markdown]
0.12.1,We can check the ratio between the two classes.
0.12.1,%%
0.12.1,%% [markdown]
0.12.1,We see that we have an imbalanced classification problem with ~95% of the
0.12.1,data belonging to the class G.W. Bush.
0.12.1,
0.12.1,Compare over-sampling approaches
0.12.1,--------------------------------
0.12.1,
0.12.1,We will use different over-sampling approaches and use a kNN classifier
0.12.1,to check if we can recognize the 2 presidents. The evaluation will be
0.12.1,performed through cross-validation and we will plot the mean ROC curve.
0.12.1,
0.12.1,We will create different pipelines and evaluate them.
0.12.1,%%
0.12.1,%% [markdown]
0.12.1,We will compute the mean ROC curve for each pipeline using a different splits
0.12.1,provided by the :class:`~sklearn.model_selection.StratifiedKFold`
0.12.1,cross-validation.
0.12.1,%%
0.12.1,compute the mean fpr/tpr to get the mean ROC curve
0.12.1,Create a display that we will reuse to make the aggregated plots for
0.12.1,all methods
0.12.1,%% [markdown]
0.12.1,"In the previous cell, we created the different mean ROC curve and we can plot"
0.12.1,them on the same plot.
0.12.1,%%
0.12.1,%% [markdown]
0.12.1,"We see that for this task, methods that are generating new samples with some"
0.12.1,interpolation (i.e. ADASYN and SMOTE) perform better than random
0.12.1,over-sampling or no resampling.
0.12.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.1,License: MIT
0.12.1,Create a folder to fetch the dataset
0.12.1,Create a pipeline
0.12.1,Classify and report the results
0.12.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.1,License: MIT
0.12.1,%%
0.12.1,%% [markdown]
0.12.1,Setting the data set
0.12.1,--------------------
0.12.1,
0.12.1,We use a part of the 20 newsgroups data set by loading 4 topics. Using the
0.12.1,"scikit-learn loader, the data are split into a training and a testing set."
0.12.1,
0.12.1,Note the class \#3 is the minority class and has almost twice less samples
0.12.1,than the majority class.
0.12.1,%%
0.12.1,%%
0.12.1,%% [markdown]
0.12.1,The usual scikit-learn pipeline
0.12.1,-------------------------------
0.12.1,
0.12.1,You might usually use scikit-learn pipeline by combining the TF-IDF
0.12.1,vectorizer to feed a multinomial naive bayes classifier. A classification
0.12.1,report summarized the results on the testing set.
0.12.1,
0.12.1,"As expected, the recall of the class \#3 is low mainly due to the class"
0.12.1,imbalanced.
0.12.1,%%
0.12.1,%%
0.12.1,%% [markdown]
0.12.1,Balancing the class before classification
0.12.1,-----------------------------------------
0.12.1,
0.12.1,"To improve the prediction of the class \#3, it could be interesting to apply"
0.12.1,"a balancing before to train the naive bayes classifier. Therefore, we will"
0.12.1,use a :class:`~imblearn.under_sampling.RandomUnderSampler` to equalize the
0.12.1,number of samples in all the classes before the training.
0.12.1,
0.12.1,It is also important to note that we are using the
0.12.1,:class:`~imblearn.pipeline.make_pipeline` function implemented in
0.12.1,imbalanced-learn to properly handle the samplers.
0.12.1,%%
0.12.1,%% [markdown]
0.12.1,"Although the results are almost identical, it can be seen that the resampling"
0.12.1,allowed to correct the poor recall of the class \#3 at the cost of reducing
0.12.1,"the other metrics for the other classes. However, the overall results are"
0.12.1,slightly better.
0.12.1,%%
0.12.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.1,License: MIT
0.12.1,#############################################################################
0.12.1,Toy data generation
0.12.1,#############################################################################
0.12.1,#############################################################################
0.12.1,We are generating some non Gaussian data set contaminated with some unform
0.12.1,noise.
0.12.1,#############################################################################
0.12.1,We will generate some cleaned test data without outliers.
0.12.1,#############################################################################
0.12.1,How to use the :class:`~imblearn.FunctionSampler`
0.12.1,#############################################################################
0.12.1,#############################################################################
0.12.1,We first define a function which will use
0.12.1,:class:`~sklearn.ensemble.IsolationForest` to eliminate some outliers from
0.12.1,our dataset during training. The function passed to the
0.12.1,:class:`~imblearn.FunctionSampler` will be called when using the method
0.12.1,``fit_resample``.
0.12.1,#############################################################################
0.12.1,Integrate it within a pipeline
0.12.1,#############################################################################
0.12.1,#############################################################################
0.12.1,"By elimnating outliers before the training, the classifier will be less"
0.12.1,affected during the prediction.
0.12.1,Authors: Dayvid Oliveira
0.12.1,Christos Aridas
0.12.1,Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.1,License: MIT
0.12.1,%%
0.12.1,%% [markdown]
0.12.1,Generate the dataset
0.12.1,--------------------
0.12.1,
0.12.1,"First, we will generate a dataset and convert it to a"
0.12.1,:class:`~pandas.DataFrame` with arbitrary column names. We will plot the
0.12.1,original dataset.
0.12.1,%%
0.12.1,%% [markdown]
0.12.1,Make a dataset imbalanced
0.12.1,-------------------------
0.12.1,
0.12.1,"Now, we will show the helpers :func:`~imblearn.datasets.make_imbalance`"
0.12.1,that is useful to random select a subset of samples. It will impact the
0.12.1,class distribution as specified by the parameters.
0.12.1,%%
0.12.1,%%
0.12.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.1,License: MIT
0.12.1,%%
0.12.1,%% [markdown]
0.12.1,Create an imbalanced dataset
0.12.1,----------------------------
0.12.1,
0.12.1,"First, we will create an imbalanced data set from a the iris data set."
0.12.1,%%
0.12.1,%%
0.12.1,%% [markdown]
0.12.1,Using ``sampling_strategy`` in resampling algorithms
0.12.1,====================================================
0.12.1,
0.12.1,`sampling_strategy` as a `float`
0.12.1,--------------------------------
0.12.1,
0.12.1,`sampling_strategy` can be given a `float`. For **under-sampling
0.12.1,"methods**, it corresponds to the ratio :math:`\alpha_{us}` defined by"
0.12.1,:math:`N_{rM} = \alpha_{us} \times N_{m}` where :math:`N_{rM}` and
0.12.1,:math:`N_{m}` are the number of samples in the majority class after
0.12.1,"resampling and the number of samples in the minority class, respectively."
0.12.1,%%
0.12.1,select only 2 classes since the ratio make sense in this case
0.12.1,%%
0.12.1,%% [markdown]
0.12.1,"For **over-sampling methods**, it correspond to the ratio"
0.12.1,:math:`\alpha_{os}` defined by :math:`N_{rm} = \alpha_{os} \times N_{M}`
0.12.1,where :math:`N_{rm}` and :math:`N_{M}` are the number of samples in the
0.12.1,minority class after resampling and the number of samples in the majority
0.12.1,"class, respectively."
0.12.1,%%
0.12.1,%% [markdown]
0.12.1,`sampling_strategy` as a `str`
0.12.1,-------------------------------
0.12.1,
0.12.1,`sampling_strategy` can be given as a string which specify the class
0.12.1,"targeted by the resampling. With under- and over-sampling, the number of"
0.12.1,samples will be equalized.
0.12.1,
0.12.1,Note that we are using multiple classes from now on.
0.12.1,%%
0.12.1,%% [markdown]
0.12.1,"With **cleaning method**, the number of samples in each class will not be"
0.12.1,equalized even if targeted.
0.12.1,%%
0.12.1,%% [markdown]
0.12.1,`sampling_strategy` as a `dict`
0.12.1,------------------------------
0.12.1,
0.12.1,"When `sampling_strategy` is a `dict`, the keys correspond to the targeted"
0.12.1,classes. The values correspond to the desired number of samples for each
0.12.1,targeted class. This is working for both **under- and over-sampling**
0.12.1,algorithms but not for the **cleaning algorithms**. Use a `list` instead.
0.12.1,%%
0.12.1,%% [markdown]
0.12.1,`sampling_strategy` as a `list`
0.12.1,-------------------------------
0.12.1,
0.12.1,"When `sampling_strategy` is a `list`, the list contains the targeted"
0.12.1,classes. It is used only for **cleaning methods** and raise an error
0.12.1,otherwise.
0.12.1,%%
0.12.1,%% [markdown]
0.12.1,`sampling_strategy` as a callable
0.12.1,---------------------------------
0.12.1,
0.12.1,"When callable, function taking `y` and returns a `dict`. The keys"
0.12.1,correspond to the targeted classes. The values correspond to the desired
0.12.1,number of samples for each class.
0.12.1,%%
0.12.1,List of whitelisted modules and methods; regexp are supported.
0.12.1,These docstrings will fail because they are inheriting from scikit-learn
0.12.1,skip private classes
0.12.1,"We ignore following error code,"
0.12.1,- RT02: The first line of the Returns section
0.12.1,"should contain only the type, .."
0.12.1,(as we may need refer to the name of the returned
0.12.1,object)
0.12.1,- GL01: Docstring text (summary) should start in the line
0.12.1,"immediately after the opening quotes (not in the same line,"
0.12.1,or leaving a blank line in between)
0.12.1,"- GL02: If there's a blank line, it should be before the"
0.12.1,"first line of the Returns section, not after (it allows to have"
0.12.1,short docstrings for properties).
0.12.1,Ignore PR02: Unknown parameters for properties. We sometimes use
0.12.1,"properties for ducktyping, i.e. SGDClassifier.predict_proba"
0.12.1,Following codes are only taken into account for the
0.12.1,top level class docstrings:
0.12.1,- ES01: No extended summary found
0.12.1,- SA01: See Also section not found
0.12.1,- EX01: No examples section found
0.12.1,In particular we can't parse the signature of properties
0.12.1,"When applied to classes, detect class method. For functions"
0.12.1,method = None.
0.12.1,TODO: this detection can be improved. Currently we assume that we have
0.12.1,class # methods if the second path element before last is in camel case.
0.12.1,'build' and 'install' is included to have structured metadata for CI.
0.12.1,It will NOT be included in setup's extras_require
0.12.1,"The values are (version_spec, comma separated tags)"
0.12.1,create inverse mapping for setuptools
0.12.1,Used by CI to get the min dependencies
0.12.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.1,License: MIT
0.12.1,TODO: remove this file when scikit-learn minimum version is 1.3
0.12.1,Return a copy of the threadlocal configuration so that users will
0.12.1,not be able to modify the configuration with the returned dict.
0.12.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.1,Christos Aridas
0.12.1,License: MIT
0.12.1,scikit-learn >= 1.2
0.12.1,we need to overwrite SamplerMixin.fit to bypass the validation
0.12.1,Adapted from scikit-learn
0.12.1,Author: Edouard Duchesnay
0.12.1,Gael Varoquaux
0.12.1,Virgile Fritsch
0.12.1,Alexandre Gramfort
0.12.1,Lars Buitinck
0.12.1,Christos Aridas
0.12.1,Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.1,License: BSD
0.12.1,BaseEstimator interface
0.12.1,validate names
0.12.1,validate estimators
0.12.1,We allow last estimator to be None as an identity transformation
0.12.1,Estimator interface
0.12.1,"def _fit(self, X, y=None, **fit_params_steps):"
0.12.1,Setup the memory
0.12.1,we do not clone when caching is disabled to
0.12.1,preserve backward compatibility
0.12.1,Fit or load from cache the current transformer
0.12.1,Replace the transformer of the step with the fitted
0.12.1,transformer. This is necessary when loading the transformer
0.12.1,from the cache.
0.12.1,The `fit_*` methods need to be overridden to support the samplers.
0.12.1,estimators in Pipeline.steps are not validated yet
0.12.1,estimators in Pipeline.steps are not validated yet
0.12.1,metadata routing enabled
0.12.1,estimators in Pipeline.steps are not validated yet
0.12.1,estimators in Pipeline.steps are not validated yet
0.12.1,TODO: remove the following methods when the minimum scikit-learn >= 1.4
0.12.1,They do not depend on resampling but we need to redefine them for the
0.12.1,compatibility with the metadata routing framework.
0.12.1,metadata routing enabled
0.12.1,not branching here since params is only available if
0.12.1,enable_metadata_routing=True
0.12.1,metadata routing enabled
0.12.1,not branching here since params is only available if
0.12.1,enable_metadata_routing=True
0.12.1,"we don't have to branch here, since params is only non-empty if"
0.12.1,enable_metadata_routing=True.
0.12.1,metadata routing is enabled.
0.12.1,"TODO: once scikit-learn >= 1.4, the following function should be simplified by"
0.12.1,calling `super().get_metadata_routing()`
0.12.1,first we add all steps except the last one
0.12.1,"fit, fit_predict, and fit_transform call fit_transform if it"
0.12.1,"exists, or else fit and transform"
0.12.1,then we add the last step
0.12.1,"without metadata routing, fit_transform and fit_predict"
0.12.1,get all the same params and pass it to the last fit.
0.12.1,"if we have a weight for this transformer, multiply output"
0.12.1,This variable is injected in the __builtins__ by the build
0.12.1,process. It is used to enable importing subpackages of sklearn when
0.12.1,the binaries are not built
0.12.1,mypy error: Cannot determine type of '__SKLEARN_SETUP__'
0.12.1,We are not importing the rest of scikit-learn during the build
0.12.1,"process, as it may not be compiled yet"
0.12.1,"FIXME: When we get Python 3.7 as minimal version, we will need to switch to"
0.12.1,the following solution:
0.12.1,https://snarky.ca/lazy-importing-in-python-3-7/
0.12.1,Import the target module and insert it into the parent's namespace
0.12.1,Update this object's dict so that if someone keeps a reference to the
0.12.1,"LazyLoader, lookups are efficient (__getattr__ is only called on"
0.12.1,lookups that fail).
0.12.1,delay the import of keras since we are going to import either tensorflow
0.12.1,or keras
0.12.1,Based on NiLearn package
0.12.1,License: simplified BSD
0.12.1,"PEP0440 compatible formatted version, see:"
0.12.1,https://www.python.org/dev/peps/pep-0440/
0.12.1,
0.12.1,Generic release markers:
0.12.1,X.Y
0.12.1,X.Y.Z # For bugfix releases
0.12.1,
0.12.1,Admissible pre-release markers:
0.12.1,X.YaN # Alpha release
0.12.1,X.YbN # Beta release
0.12.1,X.YrcN # Release Candidate
0.12.1,X.Y # Final release
0.12.1,
0.12.1,Dev branch marker is: 'X.Y.dev' or 'X.Y.devN' where N is an integer.
0.12.1,'X.Y.dev0' is the canonical version of 'X.Y.dev'
0.12.1,
0.12.1,coding: utf-8
0.12.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.1,Dariusz Brzezinski
0.12.1,License: MIT
0.12.1,Only negative labels
0.12.1,"Calculate tp_sum, pred_sum, true_sum ###"
0.12.1,labels are now from 0 to len(labels) - 1 -> use bincount
0.12.1,Pathological case
0.12.1,Compute the true negative
0.12.1,Retain only selected labels
0.12.1,"Finally, we have all our sufficient statistics. Divide! #"
0.12.1,"Divide, and on zero-division, set scores to 0 and warn:"
0.12.1,"Oddly, we may get an ""invalid"" rather than a ""divide"" error"
0.12.1,here.
0.12.1,Average the results
0.12.1,labels are now from 0 to len(labels) - 1 -> use bincount
0.12.1,Pathological case
0.12.1,Retain only selected labels
0.12.1,old version of scipy return MaskedConstant instead of 0.0
0.12.1,check that the scoring function does not need a score
0.12.1,and only a prediction
0.12.1,We do not support multilabel so the only average supported
0.12.1,is binary
0.12.1,Compute the different metrics
0.12.1,Precision/recall/f1
0.12.1,Specificity
0.12.1,Geometric mean
0.12.1,Index balanced accuracy
0.12.1,compute averages
0.12.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.1,License: MIT
0.12.1,categories are expected to be encoded from 0 to n_categories - 1
0.12.1,"list of length n_features of ndarray (n_categories, n_classes)"
0.12.1,compute the counts
0.12.1,normalize by the summing over the classes
0.12.1,silence potential warning due to in-place division by zero
0.12.1,coding: utf-8
0.12.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.1,Christos Aridas
0.12.1,License: MIT
0.12.1,##############################################################################
0.12.1,Utilities for testing
0.12.1,import some data to play with
0.12.1,restrict to a binary classification task
0.12.1,add noisy features to make the problem harder and avoid perfect results
0.12.1,"run classifier, get class probabilities and label predictions"
0.12.1,only interested in probabilities of the positive case
0.12.1,XXX: do we really want a special API for the binary case?
0.12.1,##############################################################################
0.12.1,Tests
0.12.1,detailed measures for each class
0.12.1,individual scoring function that can be used for grid search: in the
0.12.1,binary class case the score is the value of the measure for the positive
0.12.1,class (e.g. label == 1). This is deprecated for average != 'binary'.
0.12.1,Such a case may occur with non-stratified cross-validation
0.12.1,ensure the above were meaningful tests:
0.12.1,Bad pos_label
0.12.1,Bad average option
0.12.1,but average != 'binary'; even if data is binary
0.12.1,compute the geometric mean for the binary problem
0.12.1,print classification report with class names
0.12.1,print classification report with label detection
0.12.1,print classification report with class names
0.12.1,print classification report with label detection
0.12.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.1,License: MIT
0.12.1,Check basic feature of the metric:
0.12.1,"* the shape of the distance matrix is (n_samples, n_samples)"
0.12.1,* computing pairwise distance of X is the same than explicitely between
0.12.1,X and X.
0.12.1,Check the property of the vdm distance. Let's check the property
0.12.1,"described in ""Improved Heterogeneous Distance Functions"", D.R. Wilson and"
0.12.1,"T.R. Martinez, Journal of Artificial Intelligence Research 6 (1997) 1-34"
0.12.1,https://arxiv.org/pdf/cs/9701101.pdf
0.12.1,
0.12.1,"""if an attribute color has three values red, green and blue, and the"
0.12.1,"application is to identify whether or not an object is an apple, red and"
0.12.1,green would be considered closer than red and blue because the former two
0.12.1,"both have similar correlations with the output class apple."""
0.12.1,defined our feature
0.12.1,0 - not an apple / 1 - an apple
0.12.1,computing the distance between a sample of the same category should
0.12.1,give a null distance
0.12.1,check the property explained in the introduction example
0.12.1,green and red are very close
0.12.1,blue is closer to red than green
0.12.1,"Check that ""auto"" is equivalent to provide the number categories"
0.12.1,beforehand
0.12.1,Check that we raise an error if n_categories is inconsistent with the
0.12.1,number of features in X
0.12.1,Check that we don't get issue when a category is missing between 0
0.12.1,n_categories - 1
0.12.1,remove a categories that could be between 0 and n_categories
0.12.1,Check that we raise a NotFittedError when `fit` is not not called before
0.12.1,pairwise.
0.12.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.1,Christos Aridas
0.12.1,License: MIT
0.12.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.1,Christos Aridas
0.12.1,License: MIT
0.12.1,FIXME: to be removed in 0.12
0.12.1,The ratio is computed using a one-vs-rest manner. Using majority
0.12.1,in multi-class would lead to slightly different results at the
0.12.1,cost of introducing a new parameter.
0.12.1,rounding may cause new amount for n_samples
0.12.1,the nearest neighbors need to be fitted only on the current class
0.12.1,to find the class NN to generate new samples
0.12.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.1,Christos Aridas
0.12.1,License: MIT
0.12.1,smoothed bootstrap imposes to make numerical operation; we need
0.12.1,to be sure to have only numerical data in X
0.12.1,generate a smoothed bootstrap with a perturbation
0.12.1,generate a bootstrap
0.12.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.1,Christos Aridas
0.12.1,License: MIT
0.12.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.1,Fernando Nogueira
0.12.1,Christos Aridas
0.12.1,License: MIT
0.12.1,negate diagonal elements
0.12.1,identify cluster which are answering the requirements
0.12.1,empty cluster
0.12.1,the cluster is already considered balanced
0.12.1,not enough samples to apply SMOTE
0.12.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.1,Fernando Nogueira
0.12.1,Christos Aridas
0.12.1,Dzianis Dudnik
0.12.1,License: MIT
0.12.1,FIXME: to be removed in 0.12
0.12.1,FIXME: to be removed in 0.12
0.12.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.1,Fernando Nogueira
0.12.1,Christos Aridas
0.12.1,Dzianis Dudnik
0.12.1,License: MIT
0.12.1,np.newaxis for backwards compatability with random_state
0.12.1,Samples are in danger for m/2 <= m' < m
0.12.1,Samples are noise for m = m'
0.12.1,FIXME: to be removed in 0.12
0.12.1,FIXME: to be removed in 0.12
0.12.1,the input of the OneHotEncoder needs to be dense
0.12.1,SMOTE resampling starts here
0.12.1,"In the edge case where the median of the std is equal to 0, the 1s"
0.12.1,"entries will be also nullified. In this case, we store the original"
0.12.1,categorical encoding which will be later used for inverting the OHE
0.12.1,This variable will be used when generating data
0.12.1,we can replace the 1 entries of the categorical features with the
0.12.1,median of the standard deviation. It will ensure that whenever
0.12.1,"distance is computed between 2 samples, the difference will be equal"
0.12.1,to the median of the standard deviation as in the original paper.
0.12.1,"With one-hot encoding, the median will be repeated twice. We need"
0.12.1,to divide by sqrt(2) such that we only have one median value
0.12.1,contributing to the Euclidean distance
0.12.1,SMOTE resampling ends here
0.12.1,reverse the encoding of the categorical features
0.12.1,the matrix is supposed to be in the CSR format after the stacking
0.12.1,change in sparsity structure more efficient with LIL than CSR
0.12.1,convert to dense array since scipy.sparse doesn't handle 3D
0.12.1,"In the case that the median std was equal to zeros, we have to"
0.12.1,create non-null entry based on the encoded of OHE
0.12.1,tie breaking argmax
0.12.1,generate sample indices that will be used to generate new samples
0.12.1,"for each drawn samples, select its k-neighbors and generate a sample"
0.12.1,"where for each feature individually, each category generated is the"
0.12.1,most common category
0.12.1,FIXME: to be removed in 0.12
0.12.1,the kneigbors search will include the sample itself which is
0.12.1,expected from the original algorithm
0.12.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.1,Christos Aridas
0.12.1,Dzianis Dudnik
0.12.1,License: MIT
0.12.1,create 2 random continuous feature
0.12.1,create a categorical feature using some string
0.12.1,create a categorical feature using some integer
0.12.1,return the categories
0.12.1,create 2 random continuous feature
0.12.1,create a categorical feature using some string
0.12.1,create a categorical feature using some integer
0.12.1,return the categories
0.12.1,create 2 random continuous feature
0.12.1,create a categorical feature using some string
0.12.1,create a categorical feature using some integer
0.12.1,return the categories
0.12.1,create 2 random continuous feature
0.12.1,create a categorical feature using some string
0.12.1,create a categorical feature using some integer
0.12.1,return the categories
0.12.1,create 2 random continuous feature
0.12.1,create a categorical feature using some string
0.12.1,create a categorical feature using some integer
0.12.1,part of the common test which apply to SMOTE-NC even if it is not default
0.12.1,constructible
0.12.1,Check that the samplers handle pandas dataframe and pandas series
0.12.1,Cast X and y to not default dtype
0.12.1,Non-regression test for #662
0.12.1,https://github.com/scikit-learn-contrib/imbalanced-learn/issues/662
0.12.1,check that the categorical feature is not random but correspond to the
0.12.1,categories seen in the minority class samples
0.12.1,TODO: only use `sparse_output` when sklearn >= 1.2
0.12.1,TODO(0.13): remove this test
0.12.1,overall check for SMOTEN
0.12.1,check if the SMOTEN resample data as expected
0.12.1,"we generate data such that ""not apple"" will be the minority class and"
0.12.1,"samples from this class will be generated. We will force the ""blue"""
0.12.1,"category to be associated with this class. Therefore, the new generated"
0.12.1,"samples should as well be from the ""blue"" category."
0.12.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.1,Christos Aridas
0.12.1,License: MIT
0.12.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.1,Christos Aridas
0.12.1,License: MIT
0.12.1,FIXME: we should use to_numpy with pandas >= 0.25
0.12.1,check the random over-sampling with a multiclass problem
0.12.1,check that resampling with heterogeneous dtype is working with basic
0.12.1,resampling
0.12.1,check that we can oversample even with missing or infinite data
0.12.1,regression tests for #605
0.12.1,check that we raise an error when heterogeneous dtype data are given
0.12.1,and a smoothed bootstrap is requested
0.12.1,check that smoothed bootstrap is working for numerical array
0.12.1,check that a shrinkage factor of 0 is equivalent to not create a smoothed
0.12.1,bootstrap
0.12.1,check the behaviour of the shrinkage parameter
0.12.1,the covariance of the data generated with the larger shrinkage factor
0.12.1,should also be larger.
0.12.1,check the validation of the shrinkage parameter
0.12.1,check that m_neighbors is properly set. Regression test for:
0.12.1,https://github.com/scikit-learn-contrib/imbalanced-learn/issues/568
0.12.1,FIXME: to be removed in 0.12
0.12.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.1,Christos Aridas
0.12.1,License: MIT
0.12.1,shuffle the indices since the sampler are packing them by class
0.12.1,helper functions
0.12.1,input and output
0.12.1,build the model and weights
0.12.1,"build the loss, predict, and train operator"
0.12.1,Initialization of all variables in the graph
0.12.1,"For each epoch, run accuracy on train and test"
0.12.1,helper functions
0.12.1,input and output
0.12.1,build the model and weights
0.12.1,"build the loss, predict, and train operator"
0.12.1,Initialization of all variables in the graph
0.12.1,"For each epoch, run accuracy on train and test"
0.12.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.1,License: MIT
0.12.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.1,Fernando Nogueira
0.12.1,Christos Aridas
0.12.1,License: MIT
0.12.1,find which class to not consider
0.12.1,there is a Tomek link between two samples if they are both nearest
0.12.1,neighbors of each others.
0.12.1,Find the nearest neighbour of every point
0.12.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.1,Christos Aridas
0.12.1,License: MIT
0.12.1,Randomly get one sample from the majority class
0.12.1,Generate the index to select
0.12.1,Create the set C - One majority samples and all minority
0.12.1,Create the set S - all majority samples
0.12.1,fit knn on C
0.12.1,Check each sample in S if we keep it or drop it
0.12.1,Do not select sample which are already well classified
0.12.1,Classify on S
0.12.1,If the prediction do not agree with the true label
0.12.1,append it in C_x
0.12.1,Keep the index for later
0.12.1,Update C
0.12.1,fit a knn on C
0.12.1,This experimental to speed up the search
0.12.1,Classify all the element in S and avoid to test the
0.12.1,well classified elements
0.12.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.1,Dayvid Oliveira
0.12.1,Christos Aridas
0.12.1,License: MIT
0.12.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.1,Christos Aridas
0.12.1,License: MIT
0.12.1,Compute the distance considering the farthest neighbour
0.12.1,Sort the list of distance and get the index
0.12.1,Throw a warning to tell the user that we did not have enough samples
0.12.1,to select and that we just select everything
0.12.1,Select the desired number of samples
0.12.1,idx_tmp is relative to the feature selected in the
0.12.1,previous step and we need to find the indirection
0.12.1,fmt: off
0.12.1,fmt: on
0.12.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.1,Christos Aridas
0.12.1,License: MIT
0.12.1,select a sample from the current class
0.12.1,create the set composed of all minority samples and one
0.12.1,sample from the current class.
0.12.1,create the set S with removing the seed from S
0.12.1,since that it will be added anyway
0.12.1,apply Tomek cleaning
0.12.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.1,Dayvid Oliveira
0.12.1,Christos Aridas
0.12.1,License: MIT
0.12.1,Check the stopping criterion
0.12.1,1. If there is no changes for the vector y
0.12.1,2. If the number of samples in the other class become inferior to
0.12.1,the number of samples in the majority class
0.12.1,3. If one of the class is disappearing
0.12.1,Case 1
0.12.1,Case 2
0.12.1,Case 3
0.12.1,Check the stopping criterion
0.12.1,1. If the number of samples in the other class become inferior to
0.12.1,the number of samples in the majority class
0.12.1,2. If one of the class is disappearing
0.12.1,Case 1else:
0.12.1,overwrite b_min_bec_maj
0.12.1,Case 2
0.12.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.1,Christos Aridas
0.12.1,License: MIT
0.12.1,backward compatibility when passing a NearestNeighbors object
0.12.1,clean the neighborhood
0.12.1,compute which classes to consider for cleaning for the A2 group
0.12.1,add an additional sample since the query points contains the original dataset
0.12.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.1,Christos Aridas
0.12.1,License: MIT
0.12.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.1,Christos Aridas
0.12.1,License: MIT
0.12.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.1,Christos Aridas
0.12.1,License: MIT
0.12.1,"with a large `threshold_cleaning`, the algorithm is equivalent to ENN"
0.12.1,set a threshold that we should consider only the class #2
0.12.1,making the threshold slightly smaller to take into account class #1
0.12.1,we should have a more aggressive cleaning with n_neighbors is larger
0.12.1,TODO: remove in 0.14
0.12.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.1,Christos Aridas
0.12.1,License: MIT
0.12.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.1,Christos Aridas
0.12.1,License: MIT
0.12.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.1,Christos Aridas
0.12.1,License: MIT
0.12.1,FIXME: we should use to_numpy with pandas >= 0.25
0.12.1,check that we can undersample even with missing or infinite data
0.12.1,regression tests for #605
0.12.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.1,Christos Aridas
0.12.1,License: MIT
0.12.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.1,Christos Aridas
0.12.1,License: MIT
0.12.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.1,Christos Aridas
0.12.1,License: MIT
0.12.1,TODO: remove in 0.14
0.12.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.1,Christos Aridas
0.12.1,License: MIT
0.12.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.1,Christos Aridas
0.12.1,License: MIT
0.12.1,TODO: remove in 0.14
0.12.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.1,Fernando Nogueira
0.12.1,Christos Aridas
0.12.1,License: MIT
0.12.1,check that the samples selecting by the hard voting corresponds to the
0.12.1,targeted class
0.12.1,non-regression test for:
0.12.1,https://github.com/scikit-learn-contrib/imbalanced-learn/issues/738
0.12.1,Generate valid values for the required parameters
0.12.1,The parameters `*args` and `**kwargs` are ignored since we cannot generate
0.12.1,constraints.
0.12.1,check that there is a constraint for each parameter
0.12.1,this object does not have a valid type for sure for all params
0.12.1,This parameter is not validated
0.12.1,"First, check that the error is raised if param doesn't match any valid type."
0.12.1,"Then, for constraints that are more than a type constraint, check that the"
0.12.1,error is raised if param does match a valid type but does not match any valid
0.12.1,value for this type.
0.12.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.1,Christos Aridas
0.12.1,License: MIT
0.12.1,test that all_estimators doesn't find abstract classes.
0.12.1,"For NearMiss, let's check the three algorithms"
0.12.1,Common tests for estimator instances
0.12.1,Authors: Alexandre Gramfort <alexandre.gramfort@inria.fr>
0.12.1,Raghav RV <rvraghav93@gmail.com>
0.12.1,License: BSD 3 clause
0.12.1,scikit-learn >= 1.2
0.12.1,"walk_packages() ignores DeprecationWarnings, now we need to ignore"
0.12.1,FutureWarnings
0.12.1,"mypy error: Module has no attribute ""__path__"""
0.12.1,functions to ignore args / docstring of
0.12.1,Methods where y param should be ignored if y=None by default
0.12.1,numpydoc 0.8.0's docscrape tool raises because of collections.abc under
0.12.1,Python 3.7
0.12.1,Test module docstring formatting
0.12.1,Skip test if numpydoc is not found
0.12.1,XXX unreached code as of v0.22
0.12.1,"pytest tooling, not part of the scikit-learn API"
0.12.1,Exclude non-scikit-learn classes
0.12.1,Now skip docstring test for y when y is None
0.12.1,by default for API reason
0.12.1,Exclude imported functions
0.12.1,Don't test private methods / functions
0.12.1,Test that there are no tabs in our source files
0.12.1,because we don't import
0.12.1,Minimal / degenerate instances: only useful to test the docstrings.
0.12.1,"As certain attributes are present ""only"" if a certain parameter is"
0.12.1,"provided, this checks if the word ""only"" is present in the attribute"
0.12.1,"description, and if not the attribute is required to be present."
0.12.1,ignore deprecation warnings
0.12.1,attributes
0.12.1,properties
0.12.1,ignore properties that raises an AttributeError and deprecated
0.12.1,properties
0.12.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.1,License: MIT
0.12.1,check that we can let a pass a regression variable by turning down the
0.12.1,validation
0.12.1,Check that the validation is bypass when calling `fit`
0.12.1,Non-regression test for:
0.12.1,https://github.com/scikit-learn-contrib/imbalanced-learn/issues/782
0.12.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.1,Christos Aridas
0.12.1,License: MIT
0.12.1,store timestamp to figure out whether the result of 'fit' has been
0.12.1,cached or not
0.12.1,store timestamp to figure out whether the result of 'fit' has been
0.12.1,cached or not
0.12.1,Pipeline accepts steps as tuple
0.12.1,Test the various init parameters of the pipeline.
0.12.1,Check that we can't instantiate pipelines with objects without fit
0.12.1,method
0.12.1,Smoke test with only an estimator
0.12.1,Check that params are set
0.12.1,Smoke test the repr:
0.12.1,Test with two objects
0.12.1,Check that we can't instantiate with non-transformers on the way
0.12.1,"Note that NoTrans implements fit, but not transform"
0.12.1,Check that params are set
0.12.1,Smoke test the repr:
0.12.1,Check that params are not set when naming them wrong
0.12.1,Test clone
0.12.1,"Check that apart from estimators, the parameters are the same"
0.12.1,Remove estimators that where copied
0.12.1,Test the various methods of the pipeline (anova).
0.12.1,Test with Anova + LogisticRegression
0.12.1,Test that the pipeline can take fit parameters
0.12.1,classifier should return True
0.12.1,and transformer params should not be changed
0.12.1,invalid parameters should raise an error message
0.12.1,Pipeline should pass sample_weight
0.12.1,When sample_weight is None it shouldn't be passed
0.12.1,Test pipeline raises set params error message for nested models.
0.12.1,nested model check
0.12.1,Test the various methods of the pipeline (pca + svm).
0.12.1,Test with PCA + SVC
0.12.1,Test the various methods of the pipeline (preprocessing + svm).
0.12.1,check shapes of various prediction functions
0.12.1,test that the fit_predict method is implemented on a pipeline
0.12.1,test that the fit_predict on pipeline yields same results as applying
0.12.1,transform and clustering steps separately
0.12.1,"As pipeline doesn't clone estimators on construction,"
0.12.1,it must have its own estimators
0.12.1,first compute the transform and clustering step separately
0.12.1,use a pipeline to do the transform and clustering in one step
0.12.1,tests that a pipeline does not have fit_predict method when final
0.12.1,step of pipeline does not have fit_predict defined
0.12.1,tests that Pipeline passes fit_params to intermediate steps
0.12.1,when fit_predict is invoked
0.12.1,Test whether pipeline works with a transformer at the end.
0.12.1,Also test pipeline.transform and pipeline.inverse_transform
0.12.1,test transform and fit_transform:
0.12.1,Test whether pipeline works with a transformer missing fit_transform
0.12.1,test fit_transform:
0.12.1,Directly setting attr
0.12.1,Using set_params
0.12.1,Using set_params to replace single step
0.12.1,With invalid data
0.12.1,Test setting Pipeline steps to None
0.12.1,"for other methods, ensure no AttributeErrors on None:"
0.12.1,mult2 and mult3 are active
0.12.1,Check 'passthrough' step at construction time
0.12.1,Test with Transformer + SVC
0.12.1,Memoize the transformer at the first fit
0.12.1,Get the time stamp of the tranformer in the cached pipeline
0.12.1,Check that cached_pipe and pipe yield identical results
0.12.1,Check that we are reading the cache while fitting
0.12.1,a second time
0.12.1,Check that cached_pipe and pipe yield identical results
0.12.1,Create a new pipeline with cloned estimators
0.12.1,Check that even changing the name step does not affect the cache hit
0.12.1,Check that cached_pipe and pipe yield identical results
0.12.1,Test with Transformer + SVC
0.12.1,Memoize the transformer at the first fit
0.12.1,Get the time stamp of the tranformer in the cached pipeline
0.12.1,Check that cached_pipe and pipe yield identical results
0.12.1,Check that we are reading the cache while fitting
0.12.1,a second time
0.12.1,Check that cached_pipe and pipe yield identical results
0.12.1,Create a new pipeline with cloned estimators
0.12.1,Check that even changing the name step does not affect the cache hit
0.12.1,Check that cached_pipe and pipe yield identical results
0.12.1,Test the various methods of the pipeline (pca + svm).
0.12.1,Test with PCA + SVC
0.12.1,Test the various methods of the pipeline (pca + svm).
0.12.1,Test with PCA + SVC
0.12.1,Test whether pipeline works with a sampler at the end.
0.12.1,Also test pipeline.sampler
0.12.1,test transform and fit_transform:
0.12.1,We round the value near to zero. It seems that PCA has some issue
0.12.1,with that
0.12.1,Test whether pipeline works with a sampler at the end.
0.12.1,Also test pipeline.sampler
0.12.1,Test pipeline using None as preprocessing step and a classifier
0.12.1,"Test pipeline using None, RUS and a classifier"
0.12.1,"Test pipeline using RUS, None and a classifier"
0.12.1,Test pipeline using None step and a sampler
0.12.1,Test pipeline using None and a transformer that implements transform and
0.12.1,inverse_transform
0.12.1,Test the various methods of the pipeline (anova).
0.12.1,Test with RandomUnderSampling + Anova + LogisticRegression
0.12.1,Test the various methods of the pipeline (anova).
0.12.1,Test the various methods of the pipeline (anova).
0.12.1,Test with RandomUnderSampling + Anova + LogisticRegression
0.12.1,tests that Pipeline passes predict_params to the final estimator
0.12.1,when predict is invoked
0.12.1,Test that the score_samples method is implemented on a pipeline.
0.12.1,Test that the score_samples method on pipeline yields same results as
0.12.1,applying transform and score_samples steps separately.
0.12.1,Check the shapes
0.12.1,Check the values
0.12.1,Test that a pipeline does not have score_samples method when the final
0.12.1,step of the pipeline does not have score_samples defined.
0.12.1,Test that the score_samples method is implemented on a pipeline.
0.12.1,Test that the score_samples method on pipeline yields same results as
0.12.1,applying transform and score_samples steps separately.
0.12.1,Check the shapes
0.12.1,Check the values
0.12.1,transformer will not change `y` and sampler will always preserve the type of `y`
0.12.1,transformer will not change `y` and sampler will always preserve the type of `y`
0.12.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.1,Christos Aridas
0.12.1,License: MIT
0.12.1,TODO: Remove when SciPy 1.9 is the minimum supported version
0.12.1,TODO: Remove when scikit-learn 1.1 is the minimum supported version
0.12.1,TODO: remove when scikit-learn minimum version is 1.3
0.12.1,we don't want to validate again for each call to partial_fit
0.12.1,TODO: remove when scikit-learn minimum version is 1.3
0.12.1,"Likely a pandas DataFrame, we explicitly check the type to confirm."
0.12.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.1,License: MIT
0.12.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.1,License: MIT
0.12.1,Adapated from scikit-learn
0.12.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.1,License: MIT
0.12.1,scikit-learn >= 1.2
0.12.1,TODO: remove in 0.13
0.12.1,future default in 0.13
0.12.1,we don't filter samplers based on their tag here because we want to make
0.12.1,sure that the fitted attribute does not exist if the tag is not
0.12.1,stipulated
0.12.1,trigger our checks if this is a SamplerMixin
0.12.1,should raise warning if the target is continuous (we cannot raise error)
0.12.1,if the target is multilabel then we should raise an error
0.12.1,IHT does not enforce the number of samples but provide a number
0.12.1,of samples the closest to the desired target.
0.12.1,in this test we will force all samplers to not change the class 1
0.12.1,check that sparse matrices can be passed through the sampler leading to
0.12.1,the same results than dense
0.12.1,Check that the samplers handle pandas dataframe and pandas series
0.12.1,check that we return the same type for dataframes or series types
0.12.1,FIXME: we should use to_numpy with pandas >= 0.25
0.12.1,Check that the samplers handle pandas dataframe and pandas series
0.12.1,check that we return the same type for dataframes or series types
0.12.1,FIXME: we should use to_numpy with pandas >= 0.25
0.12.1,Check that the can samplers handle simple lists
0.12.1,Check that multiclass target lead to the same results than OVA encoding
0.12.1,Cast X and y to not default dtype
0.12.1,Non-regression test for #709
0.12.1,https://github.com/scikit-learn-contrib/imbalanced-learn/issues/709
0.12.1,Check that an informative error is raised when the value of a constructor
0.12.1,parameter does not have an appropriate type or value.
0.12.1,check that there is a constraint for each parameter
0.12.1,this object does not have a valid type for sure for all params
0.12.1,This parameter is not validated
0.12.1,"First, check that the error is raised if param doesn't match any valid type."
0.12.1,the method is not accessible with the current set of parameters
0.12.1,The estimator is a label transformer and take only `y`
0.12.1,"Then, for constraints that are more than a type constraint, check that the"
0.12.1,error is raised if param does match a valid type but does not match any valid
0.12.1,value for this type.
0.12.1,the method is not accessible with the current set of parameters
0.12.1,The estimator is a label transformer and take only `y`
0.12.1,Check that calling `fit` does not raise any warnings about feature names.
0.12.1,Only check imblearn estimators for feature_names_in_ in docstring
0.12.1,partial_fit checks on second call
0.12.1,Do not call partial fit if early_stopping is on
0.12.1,input_features names is not the same length as n_features_in_
0.12.1,error is raised when `input_features` do not match feature_names_in
0.12.1,Adapted from scikit-learn
0.12.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.1,License: MIT
0.12.1,Ignore deprecation warnings triggered at import time and from walking
0.12.1,packages
0.12.1,get rid of abstract base classes
0.12.1,get rid of sklearn estimators which have been imported in some classes
0.12.1,"drop duplicates, sort for reproducibility"
0.12.1,itemgetter is used to ensure the sort does not extend to the 2nd item of
0.12.1,the tuple
0.12.1,Author: Adrin Jalali <adrin.jalali@gmail.com>
0.12.1,License: BSD 3 clause
0.12.1,Only the following methods are supported in the routing mechanism. Adding new
0.12.1,methods at the moment involves monkeypatching this list.
0.12.1,"Note that if this list is changed or monkeypatched, the corresponding method"
0.12.1,needs to be added under a TYPE_CHECKING condition like the one done here in
0.12.1,_MetadataRequester
0.12.1,These methods are a composite of other methods and one cannot set their
0.12.1,requests directly. Instead they should be set by setting the requests of the
0.12.1,simple methods which make the composite ones.
0.12.1,Request values
0.12.1,==============
0.12.1,"Each request value needs to be one of the following values, or an alias."
0.12.1,this is used in `__metadata_request__*` attributes to indicate that a
0.12.1,metadata is not present even though it may be present in the
0.12.1,corresponding method's signature.
0.12.1,"this is used whenever a default value is changed, and therefore the user"
0.12.1,"should explicitly set the value, otherwise a warning is shown. An example"
0.12.1,"is when a meta-estimator is only a router, but then becomes also a"
0.12.1,consumer in a new release.
0.12.1,this is the default used in `set_{method}_request` methods to indicate no
0.12.1,change requested by the user.
0.12.1,item is only an alias if it's a valid identifier
0.12.1,Metadata Request for Simple Consumers
0.12.1,=====================================
0.12.1,This section includes MethodMetadataRequest and MetadataRequest which are
0.12.1,used in simple consumers.
0.12.1,this is here for us to use this attribute's value instead of doing
0.12.1,"`isinstance` in our checks, so that we avoid issues when people vendor"
0.12.1,this file instead of using it directly from scikit-learn.
0.12.1,Called when the default attribute access fails with an AttributeError
0.12.1,(either __getattribute__() raises an AttributeError because name is
0.12.1,not an instance attribute or an attribute in the class tree for self;
0.12.1,or __get__() of a name property raises AttributeError). This method
0.12.1,should either return the (computed) attribute value or raise an
0.12.1,AttributeError exception.
0.12.1,https://docs.python.org/3/reference/datamodel.html#object.__getattr__
0.12.1,Metadata Request for Routers
0.12.1,============================
0.12.1,This section includes all objects required for MetadataRouter which is used
0.12.1,"in routers, returned by their ``get_metadata_routing``."
0.12.1,"This namedtuple is used to store a (mapping, routing) pair. Mapping is a"
0.12.1,"MethodMapping object, and routing is the output of `get_metadata_routing`."
0.12.1,MetadataRouter stores a collection of these namedtuples.
0.12.1,A namedtuple storing a single method route. A collection of these namedtuples
0.12.1,is stored in a MetadataRouter.
0.12.1,this is here for us to use this attribute's value instead of doing
0.12.1,"`isinstance`` in our checks, so that we avoid issues when people vendor"
0.12.1,this file instead of using it directly from scikit-learn.
0.12.1,`_self_request` is used if the router is also a consumer.
0.12.1,"_self_request, (added using `add_self_request()`) is treated"
0.12.1,differently from the other objects which are stored in
0.12.1,_route_mappings.
0.12.1,"conflicts are okay if the passed objects are the same, but it's"
0.12.1,an issue if they're different objects.
0.12.1,doing this instead of a try/except since an AttributeError could be raised
0.12.1,for other reasons.
0.12.1,Request method
0.12.1,==============
0.12.1,This section includes what's needed for the request method descriptor and
0.12.1,their dynamic generation in a meta class.
0.12.1,These strings are used to dynamically generate the docstrings for
0.12.1,set_{method}_request methods.
0.12.1,we would want to have a method which accepts only the expected args
0.12.1,Now we set the relevant attributes of the function so that it seems
0.12.1,"like a normal method to the end user, with known expected arguments."
0.12.1,"This code is never run in runtime, but it's here for type checking."
0.12.1,Type checkers fail to understand that the `set_{method}_request`
0.12.1,"methods are dynamically generated, and they complain that they are"
0.12.1,not defined. We define them here to make type checkers happy.
0.12.1,During type checking analyzers assume this to be True.
0.12.1,The following list of defined methods mirrors the list of methods
0.12.1,in SIMPLE_METHODS.
0.12.1,fmt: off
0.12.1,fmt: on
0.12.1,"if there are any issues in the default values, it will be raised"
0.12.1,when ``get_metadata_routing`` is called. Here we are going to
0.12.1,ignore all the issues such as bad defaults etc.
0.12.1,set ``set_{method}_request``` methods
0.12.1,Here we use `isfunction` instead of `ismethod` because calling `getattr`
0.12.1,on a class instead of an instance returns an unbound function.
0.12.1,"ignore the first parameter of the method, which is usually ""self"""
0.12.1,Then overwrite those defaults with the ones provided in
0.12.1,__metadata_request__* attributes. Defaults set in
0.12.1,__metadata_request__* attributes take precedence over signature
0.12.1,sniffing.
0.12.1,need to go through the MRO since this is a class attribute and
0.12.1,``vars`` doesn't report the parent class attributes. We go through
0.12.1,the reverse of the MRO so that child classes have precedence over
0.12.1,their parents.
0.12.1,we don't check for attr.startswith() since python prefixes attrs
0.12.1,starting with __ with the `_ClassName`.
0.12.1,Process Routing in Routers
0.12.1,==========================
0.12.1,This is almost always the only method used in routers to process and route
0.12.1,given metadata. This is to minimize the boilerplate required in routers.
0.12.1,Here the first two arguments are positional only which makes everything
0.12.1,passed as keyword argument a metadata. The first two args also have an `_`
0.12.1,"prefix to reduce the chances of name collisions with the passed metadata, and"
0.12.1,"since they're positional only, users will never type those underscores."
0.12.1,"If routing is not enabled and kwargs are empty, then we don't have to"
0.12.1,"try doing any routing, we can simply return a structure which returns"
0.12.1,an empty dict on routed_params.ANYTHING.ANY_METHOD.
0.12.1,mypy: ignore-errors
0.12.1,update the docstring of the descriptor
0.12.1,"delegate only on instances, not the classes."
0.12.1,this is to allow access to the docstrings.
0.12.1,This makes it possible to use the decorated method as an
0.12.1,"unbound method, for instance when monkeypatching."
0.12.1,mypy: ignore-errors
0.12.1,Inherits from ValueError and TypeError to keep backward compatibility.
0.12.1,We allow parameters to not have a constraint so that third party
0.12.1,estimators can inherit from sklearn estimators without having to
0.12.1,necessarily use the validation tools.
0.12.1,"this constraint is satisfied, no need to check further."
0.12.1,"No constraint is satisfied, raise with an informative message."
0.12.1,Ignore constraints that we don't want to expose in the error
0.12.1,"message, i.e. options that are for internal purpose or not"
0.12.1,officially supported.
0.12.1,The dict of parameter constraints is set as an attribute of the function
0.12.1,to make it possible to dynamically introspect the constraints for
0.12.1,automatic testing.
0.12.1,Map *args/**kwargs to the function signature
0.12.1,ignore self/cls and positional/keyword markers
0.12.1,"When the function is just a wrapper around an estimator, we allow"
0.12.1,"the function to delegate validation to the estimator, but we"
0.12.1,replace the name of the estimator by the name of the function in
0.12.1,the error message to avoid confusion.
0.12.1,better repr if the bounds were given as integers
0.12.1,we use an interval of Real to ignore np.nan that has its own
0.12.1,constraint
0.12.1,"There's no integer outside (-inf, +inf)"
0.12.1,"bounds are -inf, +inf"
0.12.1,"interval is [-inf, +inf]"
0.12.1,special case for ndarray since it can't be instantiated without
0.12.1,arguments
0.12.1,special case for Integral and Real since they are abstract classes
0.12.1,Author: Alexander L. Hayes <hayesall@iu.edu>
0.12.1,License: MIT
0.12.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.1,License: MIT
0.12.1,We lost the y.index during resampling. We can safely use X.index to align
0.12.1,them.
0.12.1,We special case the following error:
0.12.1,https://github.com/scikit-learn-contrib/imbalanced-learn/issues/1055
0.12.1,"There is no easy way to have a generic workaround. Here, we detect"
0.12.1,that we have a column with only null values that is datetime64
0.12.1,(resulting from the np.vstack of the resampling).
0.12.1,try again
0.12.1,_is_neighbors_object(nn_object)
0.12.1,check that all keys in sampling_strategy are also in y
0.12.1,check that there is no negative number
0.12.1,check that all keys in sampling_strategy are also in y
0.12.1,ignore first 'self' argument for instance methods
0.12.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.1,Christos Aridas
0.12.1,License: MIT
0.12.1,this function could create an equal number of samples
0.12.1,We pass on purpose a non sorted dictionary and check that the resulting
0.12.1,dictionary is sorted. Refer to issue #428.
0.12.1,DataFrame and DataFrame case
0.12.1,DataFrames and Series case
0.12.1,The * is place before a keyword only argument without a default value
0.12.1,Test that the minimum dependencies in the README.rst file are
0.12.1,consistent with the minimum dependencies defined at the file:
0.12.1,imblearn/_min_dependencies.py
0.12.1,Skip the test if the README.rst file is not available.
0.12.1,"For instance, when installing scikit-learn from wheels"
0.12.1,Author: Alexander L. Hayes <hayesall@iu.edu>
0.12.1,License: MIT
0.12.1,Some helpers for the tests
0.12.1,check in the presence of extra positional and keyword args
0.12.1,outer decorator does not interfere with validation
0.12.1,validated method can be decorated
0.12.1,no validation in init
0.12.1,list and dict are valid params
0.12.1,the list option is not exposed in the error message
0.12.1,"""auto"" and ""warn"" are valid params"
0.12.1,"the ""warn"" option is not exposed in the error message"
0.12.1,True/False and np.bool_(True/False) are valid params
0.12.1,param1 is validated
0.12.1,param2 is not validated: any type is valid.
0.12.1,"does not raise, even though ""b"" is not in the constraints dict and ""a"" is not"
0.12.1,a parameter of the estimator.
0.12.1,does not raise
0.12.1,calls f with a bad parameter type
0.12.1,Validation for g is never skipped.
0.12.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.1,License: MIT
0.12.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.1,Christos Aridas
0.12.1,License: MIT
0.12.1,check if the filtering is working with a list or a single string
0.12.1,check that all estimators are sampler
0.12.1,check that an error is raised when the type is unknown
0.12.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.1,License: MIT
0.12.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.1,Christos Aridas
0.12.1,License: MIT
0.12.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.1,Christos Aridas
0.12.1,License: MIT
0.12.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.1,Christos Aridas
0.12.1,License: MIT
0.12.1,Check if default job count is None
0.12.1,Check if job count is set
0.12.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.1,Christos Aridas
0.12.1,License: MIT
0.12.1,Check if default job count is none
0.12.1,Check if job count is set
0.12.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.1,License: MIT
0.12.1,scikit-learn >= 1.2
0.12.1,resample before to fit the tree
0.12.1,TODO: remove when the minimum supported version of scikit-learn will be 1.4
0.12.1,support for missing values
0.12.1,TODO: remove when the minimum supported version of scikit-learn will be 1.1
0.12.1,change of signature in scikit-learn 1.1
0.12.1,make a deepcopy to not modify the original dictionary
0.12.1,TODO: remove when the minimum supported version of scikit-learn will be 1.4
0.12.1,use scikit-learn support for monotonic constraints
0.12.1,create an attribute for compatibility with other scikit-learn tools such
0.12.1,as HTML representation.
0.12.1,TODO: remove in 0.13
0.12.1,Validate or convert input data
0.12.1,TODO: remove when the minimum supported version of scipy will be 1.4
0.12.1,Support for missing values
0.12.1,TODO: remove when the minimum supported version of scikit-learn will be 1.4
0.12.1,_compute_missing_values_in_feature_mask checks if X has missing values and
0.12.1,will raise an error if the underlying tree base estimator can't handle
0.12.1,missing values. Only the criterion is required to determine if the tree
0.12.1,supports missing values.
0.12.1,Pre-sort indices to avoid that each individual tree of the
0.12.1,ensemble sorts the indices.
0.12.1,reshape is necessary to preserve the data contiguity against vs
0.12.1,"[:, np.newaxis] that does not."
0.12.1,Get bootstrap sample size
0.12.1,Check parameters
0.12.1,"Free allocated memory, if any"
0.12.1,We draw from the random state to get the random state we
0.12.1,would have got if we hadn't used a warm_start.
0.12.1,Parallel loop: we prefer the threading backend as the Cython code
0.12.1,for fitting the trees is internally releasing the Python GIL
0.12.1,making threading more efficient than multiprocessing in
0.12.1,"that case. However, we respect any parallel_backend contexts set"
0.12.1,"at a higher level, since correctness does not rely on using"
0.12.1,threads.
0.12.1,Collect newly grown trees
0.12.1,Create pipeline with the fitted samplers and trees
0.12.1,FIXME: we could consider to support multiclass-multioutput if
0.12.1,we introduce or reuse a constructor parameter (e.g.
0.12.1,oob_score) allowing our user to pass a callable defining the
0.12.1,scoring strategy on OOB sample.
0.12.1,Decapsulate classes_ attributes
0.12.1,drop the n_outputs axis if there is a single output
0.12.1,Prediction requires X to be in CSR format
0.12.1,n_classes_ is a ndarray at this stage
0.12.1,all the supported type of target will have the same number of
0.12.1,classes in all outputs
0.12.1,"for regression, n_classes_ does not exist and we create an empty"
0.12.1,axis to be consistent with the classification case and make
0.12.1,the array operations compatible with the 2 settings
0.12.1,TODO: remove when supporting scikit-learn>=1.2
0.12.1,make a deepcopy to not modify the original dictionary
0.12.1,TODO: remove when minimum supported version of scikit-learn is 1.4
0.12.1,SAMME-R requires predict_proba-enabled estimators
0.12.1,Instances incorrectly classified
0.12.1,Error fraction
0.12.1,Stop if classification is perfect
0.12.1,Construct y coding as described in Zhu et al [2]:
0.12.1,
0.12.1,y_k = 1 if c == k else -1 / (K - 1)
0.12.1,
0.12.1,"where K == n_classes_ and c, k in [0, K) are indices along the second"
0.12.1,axis of the y coding with c being the index corresponding to the true
0.12.1,class label.
0.12.1,Displace zero probabilities so the log is defined.
0.12.1,Also fix negative elements which may occur with
0.12.1,negative sample weights.
0.12.1,Boost weight using multi-class AdaBoost SAMME.R alg
0.12.1,Only boost the weights if it will fit again
0.12.1,Only boost positive weights
0.12.1,Instances incorrectly classified
0.12.1,Error fraction
0.12.1,Stop if classification is perfect
0.12.1,Stop if the error is at least as bad as random guessing
0.12.1,Boost weight using multi-class AdaBoost SAMME alg
0.12.1,Only boost the weights if I will fit again
0.12.1,Only boost positive weights
0.12.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.1,Christos Aridas
0.12.1,License: MIT
0.12.1,scikit-learn >= 1.2
0.12.1,make a deepcopy to not modify the original dictionary
0.12.1,TODO: remove when minimum supported version of scikit-learn is 1.4
0.12.1,TODO: remove when supporting scikit-learn>=1.2
0.12.1,overwrite the base class method by disallowing `sample_weight`
0.12.1,RandomUnderSampler is not supporting sample_weight. We need to pass
0.12.1,None.
0.12.1,TODO: remove when minimum supported version of scikit-learn is 1.1
0.12.1,Check data
0.12.1,Parallel loop
0.12.1,Reduce
0.12.1,The base class require to have the attribute defined. For scikit-learn
0.12.1,"> 1.2, we are going to raise an error."
0.12.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.1,Christos Aridas
0.12.1,License: MIT
0.12.1,scikit-learn >= 1.2
0.12.1,make a deepcopy to not modify the original dictionary
0.12.1,TODO: remove when minimum supported version of scikit-learn is 1.4
0.12.1,TODO: remove when supporting scikit-learn>=1.2
0.12.1,overwrite the base class method by disallowing `sample_weight`
0.12.1,the sampler needs to be validated before to call _fit because
0.12.1,_validate_y is called before _validate_estimator and would require
0.12.1,to know which type of sampler we are using.
0.12.1,RandomUnderSampler is not supporting sample_weight. We need to pass
0.12.1,None.
0.12.1,TODO: remove when minimum supported version of scikit-learn is 1.1
0.12.1,Check data
0.12.1,Parallel loop
0.12.1,Reduce
0.12.1,The base class require to have the attribute defined. For scikit-learn
0.12.1,"> 1.2, we are going to raise an error."
0.12.1,check that we have an ensemble of samplers and estimators with a
0.12.1,consistent size
0.12.1,each sampler in the ensemble should have different random state
0.12.1,each estimator in the ensemble should have different random state
0.12.1,check the consistency of the feature importances
0.12.1,check the consistency of the prediction outpus
0.12.1,Predictions should be the same when sample_weight are all ones
0.12.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.1,Christos Aridas
0.12.1,License: MIT
0.12.1,Check classification for various parameter settings.
0.12.1,Test that bootstrapping samples generate non-perfect base estimators.
0.12.1,"without bootstrap, all trees are perfect on the training set"
0.12.1,disable the resampling by passing an empty dictionary.
0.12.1,"with bootstrap, trees are no longer perfect on the training set"
0.12.1,Test that bootstrapping features may generate duplicate features.
0.12.1,Predict probabilities.
0.12.1,Normal case
0.12.1,"Degenerate case, where some classes are missing"
0.12.1,Check that oob prediction is a good estimation of the generalization
0.12.1,error.
0.12.1,Test with few estimators
0.12.1,Check singleton ensembles.
0.12.1,Check that bagging ensembles can be grid-searched.
0.12.1,Transform iris into a binary classification task
0.12.1,Grid search with scoring based on decision_function
0.12.1,Check estimator and its default values.
0.12.1,Test if fitting incrementally with warm start gives a forest of the
0.12.1,right size and the same results as a normal fit.
0.12.1,Test if warm start'ed second fit with smaller n_estimators raises error.
0.12.1,Test that nothing happens when fitting without increasing n_estimators
0.12.1,"modify X to nonsense values, this should not change anything"
0.12.1,warm started classifier with 5+5 estimators should be equivalent to
0.12.1,one classifier with 10 estimators
0.12.1,Check using oob_score and warm_start simultaneously fails
0.12.1,"Make sure OOB scores are identical when random_state, estimator, and"
0.12.1,training data are fixed and fitting is done twice
0.12.1,Check that format of estimators_samples_ is correct and that results
0.12.1,generated at fit time can be identically reproduced at a later time
0.12.1,using data saved in object attributes.
0.12.1,remap the y outside of the BalancedBaggingclassifier
0.12.1,"_, y = np.unique(y, return_inverse=True)"
0.12.1,Get relevant attributes
0.12.1,Test for correct formatting
0.12.1,Re-fit single estimator to test for consistent sampling
0.12.1,Make sure validated max_samples and original max_samples are identical
0.12.1,when valid integer max_samples supplied by user
0.12.1,check that we can pass any kind of sampler to a bagging classifier
0.12.1,check that we have balanced class with the right counts of class
0.12.1,sample depending on the sampling strategy
0.12.1,check that we can provide a FunctionSampler in BalancedBaggingClassifier
0.12.1,find the minority and majority classes
0.12.1,compute the number of sample to draw from the majority class using
0.12.1,a negative binomial distribution
0.12.1,draw randomly with or without replacement
0.12.1,Roughly Balanced Bagging
0.12.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.1,Christos Aridas
0.12.1,License: MIT
0.12.1,Generate a global dataset to use
0.12.1,Check classification for various parameter settings.
0.12.1,test the different prediction function
0.12.1,Check estimator and its default values.
0.12.1,Test if fitting incrementally with warm start gives a forest of the
0.12.1,right size and the same results as a normal fit.
0.12.1,Test if warm start'ed second fit with smaller n_estimators raises error.
0.12.1,Test that nothing happens when fitting without increasing n_estimators
0.12.1,"modify X to nonsense values, this should not change anything"
0.12.1,warm started classifier with 5+5 estimators should be equivalent to
0.12.1,one classifier with 10 estimators
0.12.1,Check warning if not enough estimators
0.12.1,First fit with no restriction on max samples
0.12.1,Second fit with max samples restricted to just 2
0.12.1,Regression test for #655: check that the oob score is closed to 0.5
0.12.1,a binomial experiment.
0.12.1,TODO: remove in 0.13
0.12.1,Create dataset with missing values
0.12.1,Train forest with missing values
0.12.1,Train forest without missing values
0.12.1,Score is still 80 percent of the forest's score that had no missing values
0.12.1,Create a predictive feature using `y` and with some noise
0.12.1,Author: Guillaume Lemaitre
0.12.1,License: BSD 3 clause
0.12.1,"The index start at one, then we need to remove one"
0.12.1,to not have issue with the indexing.
0.12.1,go through the list and check if the data are available
0.12.1,Authors: Dayvid Oliveira
0.12.1,Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.1,Christos Aridas
0.12.1,License: MIT
0.12.1,restrict ratio to be a dict or a callable
0.12.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.1,Christos Aridas
0.12.1,License: MIT
0.12.1,"we are reusing part of utils.check_sampling_strategy, however this is not"
0.12.1,cover in the common tests so we will repeat it here
0.12.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.1,Christos Aridas
0.12.1,License: MIT
0.12.1,This is a trick to avoid an error during tests collection with pytest. We
0.12.1,avoid the error when importing the package raise the error at the moment of
0.12.1,creating the instance.
0.12.1,This is a trick to avoid an error during tests collection with pytest. We
0.12.1,avoid the error when importing the package raise the error at the moment of
0.12.1,creating the instance.
0.12.1,flag for keras sequence duck-typing
0.12.1,shuffle the indices since the sampler are packing them by class
0.12.0,This file is here so that when running from the root folder
0.12.0,./imblearn is added to sys.path by pytest.
0.12.0,See https://docs.pytest.org/en/latest/pythonpath.html for more details.
0.12.0,"For example, this allows to build extensions in place and run pytest"
0.12.0,doc/modules/clustering.rst and use imblearn from the local folder
0.12.0,rather than the one from site-packages.
0.12.0,! /usr/bin/env python
0.12.0,Python 2 compat: just to be able to declare that Python >=3.7 is needed.
0.12.0,This is a bit (!) hackish: we are setting a global variable so that the
0.12.0,main imblearn __init__ can detect if it is being loaded by the setup
0.12.0,"routine, to avoid attempting to load components that aren't built yet:"
0.12.0,the numpy distutils extensions that are used by imbalanced-learn to
0.12.0,recursively build the compiled extensions in sub-packages is based on the
0.12.0,Python import machinery.
0.12.0,get __version__ from _version.py
0.12.0,-*- coding: utf-8 -*-
0.12.0,
0.12.0,"imbalanced-learn documentation build configuration file, created by"
0.12.0,sphinx-quickstart on Mon Jan 18 14:44:12 2016.
0.12.0,
0.12.0,This file is execfile()d with the current directory set to its
0.12.0,containing dir.
0.12.0,
0.12.0,Note that not all possible configuration values are present in this
0.12.0,autogenerated file.
0.12.0,
0.12.0,All configuration values have a default; values that are commented out
0.12.0,serve to show the default.
0.12.0,"If extensions (or modules to document with autodoc) are in another directory,"
0.12.0,add these directories to sys.path here. If the directory is relative to the
0.12.0,"documentation root, use os.path.abspath to make it absolute, like shown here."
0.12.0,-- General configuration ------------------------------------------------
0.12.0,"If your documentation needs a minimal Sphinx version, state it here."
0.12.0,needs_sphinx = '1.0'
0.12.0,"Add any Sphinx extension module names here, as strings. They can be"
0.12.0,extensions coming with Sphinx (named 'sphinx.ext.*') or your custom
0.12.0,ones.
0.12.0,Specify how to identify the prompt when copying code snippets
0.12.0,"Add any paths that contain templates here, relative to this directory."
0.12.0,The suffix of source filenames.
0.12.0,The master toctree document.
0.12.0,General information about the project.
0.12.0,"The version info for the project you're documenting, acts as replacement for"
0.12.0,"|version| and |release|, also used in various other places throughout the"
0.12.0,built documents.
0.12.0,
0.12.0,The short X.Y version.
0.12.0,"The full version, including alpha/beta/rc tags."
0.12.0,"List of patterns, relative to source directory, that match files and"
0.12.0,directories to ignore when looking for source files.
0.12.0,The reST default role (used for this markup: `text`) to use for all
0.12.0,documents.
0.12.0,"If true, '()' will be appended to :func: etc. cross-reference text."
0.12.0,The name of the Pygments (syntax highlighting) style to use.
0.12.0,-- Options for HTML output ----------------------------------------------
0.12.0,The theme to use for HTML and HTML Help pages.  See the documentation for
0.12.0,a list of builtin themes.
0.12.0,"""twitter_url"": ""https://twitter.com/pandas_dev"","
0.12.0,"""navbar_align"": ""right"",  # For testing that the navbar items align properly"
0.12.0,"Add any paths that contain custom static files (such as style sheets) here,"
0.12.0,"relative to this directory. They are copied after the builtin static files,"
0.12.0,"so a file named ""default.css"" will overwrite the builtin ""default.css""."
0.12.0,Output file base name for HTML help builder.
0.12.0,-- Options for autodoc ------------------------------------------------------
0.12.0,generate autosummary even if no references
0.12.0,-- Options for numpydoc -----------------------------------------------------
0.12.0,this is needed for some reason...
0.12.0,see https://github.com/numpy/numpydoc/issues/69
0.12.0,-- Options for sphinxcontrib-bibtex -----------------------------------------
0.12.0,bibtex file
0.12.0,-- Options for intersphinx --------------------------------------------------
0.12.0,intersphinx configuration
0.12.0,-- Options for sphinx-gallery -----------------------------------------------
0.12.0,Generate the plot for the gallery
0.12.0,sphinx-gallery configuration
0.12.0,-- Options for github link for what's new -----------------------------------
0.12.0,Config for sphinx_issues
0.12.0,The following is used by sphinx.ext.linkcode to provide links to github
0.12.0,-- Options for LaTeX output ---------------------------------------------
0.12.0,The paper size ('letterpaper' or 'a4paper').
0.12.0,"'papersize': 'letterpaper',"
0.12.0,"The font size ('10pt', '11pt' or '12pt')."
0.12.0,"'pointsize': '10pt',"
0.12.0,Additional stuff for the LaTeX preamble.
0.12.0,"'preamble': '',"
0.12.0,Grouping the document tree into LaTeX files. List of tuples
0.12.0,"(source start file, target name, title,"
0.12.0,"author, documentclass [howto, manual, or own class])."
0.12.0,-- Options for manual page output ---------------------------------------
0.12.0,"If false, no module index is generated."
0.12.0,latex_domain_indices = True
0.12.0,One entry per manual page. List of tuples
0.12.0,"(source start file, name, description, authors, manual section)."
0.12.0,"If true, show URL addresses after external links."
0.12.0,man_show_urls = False
0.12.0,-- Options for Texinfo output -------------------------------------------
0.12.0,Grouping the document tree into Texinfo files. List of tuples
0.12.0,"(source start file, target name, title, author,"
0.12.0,"dir menu entry, description, category)"
0.12.0,-- Dependencies generation ----------------------------------------------
0.12.0,get length of header
0.12.0,-- Additional temporary hacks -----------------------------------------------
0.12.0,Temporary work-around for spacing problem between parameter and parameter
0.12.0,"type in the doc, see https://github.com/numpy/numpydoc/issues/215. The bug"
0.12.0,has been fixed in sphinx (https://github.com/sphinx-doc/sphinx/pull/5976) but
0.12.0,through a change in sphinx basic.css except rtd_theme does not use basic.css.
0.12.0,"In an ideal world, this would get fixed in this PR:"
0.12.0,https://github.com/readthedocs/sphinx_rtd_theme/pull/747/files
0.12.0,get the styles from the current theme
0.12.0,create and add the button to all the code blocks that contain >>>
0.12.0,tracebacks (.gt) contain bare text elements that need to be
0.12.0,wrapped in a span to work with .nextUntil() (see later)
0.12.0,define the behavior of the button when it's clicked
0.12.0,hide the code output
0.12.0,show the code output
0.12.0,-*- coding: utf-8 -*-
0.12.0,Format template for issues URI
0.12.0,e.g. 'https://github.com/sloria/marshmallow/issues/{issue}
0.12.0,Format template for PR URI
0.12.0,e.g. 'https://github.com/sloria/marshmallow/pull/{issue}
0.12.0,Format template for commit URI
0.12.0,e.g. 'https://github.com/sloria/marshmallow/commits/{commit}
0.12.0,"Shortcut for Github, e.g. 'sloria/marshmallow'"
0.12.0,Format template for user profile URI
0.12.0,e.g. 'https://github.com/{user}'
0.12.0,Python 2 only
0.12.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.0,License: MIT
0.12.0,%%
0.12.0,%%
0.12.0,"First, we will generate a toy classification dataset with only few samples."
0.12.0,The ratio between the classes will be imbalanced.
0.12.0,%%
0.12.0,%%
0.12.0,"Now, we will use a :class:`~imblearn.over_sampling.RandomOverSampler` to"
0.12.0,generate a bootstrap for the minority class with as many samples as in the
0.12.0,majority class.
0.12.0,%%
0.12.0,%%
0.12.0,We observe that the minority samples are less transparent than the samples
0.12.0,"from the majority class. Indeed, it is due to the fact that these samples"
0.12.0,of the minority class are repeated during the bootstrap generation.
0.12.0,
0.12.0,We can set `shrinkage` to a floating value to add a small perturbation to the
0.12.0,samples created and therefore create a smoothed bootstrap.
0.12.0,%%
0.12.0,%%
0.12.0,"In this case, we see that the samples in the minority class are not"
0.12.0,overlapping anymore due to the added noise.
0.12.0,
0.12.0,The parameter `shrinkage` allows to add more or less perturbation. Let's
0.12.0,add more perturbation when generating the smoothed bootstrap.
0.12.0,%%
0.12.0,%%
0.12.0,Increasing the value of `shrinkage` will disperse the new samples. Forcing
0.12.0,the shrinkage to 0 will be equivalent to generating a normal bootstrap.
0.12.0,%%
0.12.0,%%
0.12.0,"Therefore, the `shrinkage` is handy to manually tune the dispersion of the"
0.12.0,new samples.
0.12.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.0,License: MIT
0.12.0,%%
0.12.0,generate some data points
0.12.0,plot the majority and minority samples
0.12.0,draw the circle in which the new sample will generated
0.12.0,plot the line on which the sample will be generated
0.12.0,create and plot the new sample
0.12.0,make the plot nicer with legend and label
0.12.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.0,License: MIT
0.12.0,%%
0.12.0,%% [markdown]
0.12.0,The following function will be used to create toy dataset. It uses the
0.12.0,:func:`~sklearn.datasets.make_classification` from scikit-learn but fixing
0.12.0,some parameters.
0.12.0,%%
0.12.0,%% [markdown]
0.12.0,The following function will be used to plot the sample space after resampling
0.12.0,to illustrate the specificities of an algorithm.
0.12.0,%%
0.12.0,%% [markdown]
0.12.0,The following function will be used to plot the decision function of a
0.12.0,classifier given some data.
0.12.0,%%
0.12.0,%% [markdown]
0.12.0,Illustration of the influence of the balancing ratio
0.12.0,----------------------------------------------------
0.12.0,
0.12.0,We will first illustrate the influence of the balancing ratio on some toy
0.12.0,data using a logistic regression classifier which is a linear model.
0.12.0,%%
0.12.0,%% [markdown]
0.12.0,We will fit and show the decision boundary model to illustrate the impact of
0.12.0,dealing with imbalanced classes.
0.12.0,%%
0.12.0,%% [markdown]
0.12.0,"Greater is the difference between the number of samples in each class, poorer"
0.12.0,are the classification results.
0.12.0,
0.12.0,Random over-sampling to balance the data set
0.12.0,--------------------------------------------
0.12.0,
0.12.0,Random over-sampling can be used to repeat some samples and balance the
0.12.0,number of samples between the dataset. It can be seen that with this trivial
0.12.0,approach the boundary decision is already less biased toward the majority
0.12.0,class. The class :class:`~imblearn.over_sampling.RandomOverSampler`
0.12.0,implements such of a strategy.
0.12.0,%%
0.12.0,%% [markdown]
0.12.0,"By default, random over-sampling generates a bootstrap. The parameter"
0.12.0,`shrinkage` allows adding a small perturbation to the generated data
0.12.0,to generate a smoothed bootstrap instead. The plot below shows the difference
0.12.0,between the two data generation strategies.
0.12.0,%%
0.12.0,%% [markdown]
0.12.0,It looks like more samples are generated with smoothed bootstrap. This is due
0.12.0,to the fact that the samples generated are not superimposing with the
0.12.0,original samples.
0.12.0,
0.12.0,More advanced over-sampling using ADASYN and SMOTE
0.12.0,--------------------------------------------------
0.12.0,
0.12.0,Instead of repeating the same samples when over-sampling or perturbating the
0.12.0,"generated bootstrap samples, one can use some specific heuristic instead."
0.12.0,:class:`~imblearn.over_sampling.ADASYN` and
0.12.0,:class:`~imblearn.over_sampling.SMOTE` can be used in this case.
0.12.0,%%
0.12.0,%% [markdown]
0.12.0,The following plot illustrates the difference between
0.12.0,:class:`~imblearn.over_sampling.ADASYN` and
0.12.0,:class:`~imblearn.over_sampling.SMOTE`.
0.12.0,:class:`~imblearn.over_sampling.ADASYN` will focus on the samples which are
0.12.0,difficult to classify with a nearest-neighbors rule while regular
0.12.0,:class:`~imblearn.over_sampling.SMOTE` will not make any distinction.
0.12.0,"Therefore, the decision function depending of the algorithm."
0.12.0,%% [markdown]
0.12.0,"Due to those sampling particularities, it can give rise to some specific"
0.12.0,issues as illustrated below.
0.12.0,%%
0.12.0,%% [markdown]
0.12.0,SMOTE proposes several variants by identifying specific samples to consider
0.12.0,during the resampling. The borderline version
0.12.0,(:class:`~imblearn.over_sampling.BorderlineSMOTE`) will detect which point to
0.12.0,select which are in the border between two classes. The SVM version
0.12.0,(:class:`~imblearn.over_sampling.SVMSMOTE`) will use the support vectors
0.12.0,found using an SVM algorithm to create new sample while the KMeans version
0.12.0,(:class:`~imblearn.over_sampling.KMeansSMOTE`) will make a clustering before
0.12.0,to generate samples in each cluster independently depending each cluster
0.12.0,density.
0.12.0,%%
0.12.0,%% [markdown]
0.12.0,"When dealing with a mixed of continuous and categorical features,"
0.12.0,:class:`~imblearn.over_sampling.SMOTENC` is the only method which can handle
0.12.0,this case.
0.12.0,%%
0.12.0,Create a dataset of a mix of numerical and categorical data
0.12.0,%% [markdown]
0.12.0,"However, if the dataset is composed of only categorical features then one"
0.12.0,should use :class:`~imblearn.over_sampling.SMOTEN`.
0.12.0,%%
0.12.0,Generate only categorical data
0.12.0,Authors: Christos Aridas
0.12.0,Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.0,License: MIT
0.12.0,%%
0.12.0,%% [markdown]
0.12.0,Let's first generate a dataset with imbalanced class distribution.
0.12.0,%%
0.12.0,%% [markdown]
0.12.0,We will use an over-sampler :class:`~imblearn.over_sampling.SMOTE` followed
0.12.0,by a :class:`~sklearn.tree.DecisionTreeClassifier`. The aim will be to
0.12.0,search which `k_neighbors` parameter is the most adequate with the dataset
0.12.0,that we generated.
0.12.0,%%
0.12.0,%% [markdown]
0.12.0,We can use the :class:`~sklearn.model_selection.validation_curve` to inspect
0.12.0,"the impact of varying the parameter `k_neighbors`. In this case, we need"
0.12.0,to use a score to evaluate the generalization score during the
0.12.0,cross-validation.
0.12.0,%%
0.12.0,%%
0.12.0,%% [markdown]
0.12.0,We can now plot the results of the cross-validation for the different
0.12.0,parameter values that we tried.
0.12.0,%%
0.12.0,make nice plotting
0.12.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.0,License: MIT
0.12.0,Generate a dataset
0.12.0,Split the data
0.12.0,Train the classifier with balancing
0.12.0,Test the classifier and get the prediction
0.12.0,Show the classification report
0.12.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.0,License: MIT
0.12.0,%%
0.12.0,%% [markdown]
0.12.0,"First, we will generate some imbalanced dataset."
0.12.0,%%
0.12.0,%% [markdown]
0.12.0,We will split the data into a training and testing set.
0.12.0,%%
0.12.0,%% [markdown]
0.12.0,We will create a pipeline made of a :class:`~imblearn.over_sampling.SMOTE`
0.12.0,over-sampler followed by a :class:`~sklearn.linear_model.LogisticRegression`
0.12.0,classifier.
0.12.0,%%
0.12.0,%% [markdown]
0.12.0,"Now, we will train the model on the training set and get the prediction"
0.12.0,associated with the testing set. Be aware that the resampling will happen
0.12.0,only when calling `fit`: the number of samples in `y_pred` is the same than
0.12.0,in `y_test`.
0.12.0,%%
0.12.0,%% [markdown]
0.12.0,The geometric mean corresponds to the square root of the product of the
0.12.0,sensitivity and specificity. Combining the two metrics should account for
0.12.0,the balancing of the dataset.
0.12.0,%%
0.12.0,%% [markdown]
0.12.0,The index balanced accuracy can transform any metric to be used in
0.12.0,imbalanced learning problems.
0.12.0,%%
0.12.0,%%
0.12.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.0,License: MIT
0.12.0,%%
0.12.0,%% [markdown]
0.12.0,Dataset generation
0.12.0,------------------
0.12.0,
0.12.0,We will create an imbalanced dataset with a couple of samples. We will use
0.12.0,:func:`~sklearn.datasets.make_classification` to generate this dataset.
0.12.0,%%
0.12.0,%%
0.12.0,%% [markdown]
0.12.0,The following function will be used to plot the sample space after resampling
0.12.0,to illustrate the characteristic of an algorithm.
0.12.0,%%
0.12.0,%% [markdown]
0.12.0,The following function will be used to plot the decision function of a
0.12.0,classifier given some data.
0.12.0,%%
0.12.0,%% [markdown]
0.12.0,":class:`~imblearn.over_sampling.SMOTE` allows to generate samples. However,"
0.12.0,this method of over-sampling does not have any knowledge regarding the
0.12.0,"underlying distribution. Therefore, some noisy samples can be generated, e.g."
0.12.0,"when the different classes cannot be well separated. Hence, it can be"
0.12.0,beneficial to apply an under-sampling algorithm to clean the noisy samples.
0.12.0,Two methods are usually used in the literature: (i) Tomek's link and (ii)
0.12.0,edited nearest neighbours cleaning methods. Imbalanced-learn provides two
0.12.0,ready-to-use samplers :class:`~imblearn.combine.SMOTETomek` and
0.12.0,":class:`~imblearn.combine.SMOTEENN`. In general,"
0.12.0,:class:`~imblearn.combine.SMOTEENN` cleans more noisy data than
0.12.0,:class:`~imblearn.combine.SMOTETomek`.
0.12.0,%%
0.12.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.0,License: MIT
0.12.0,%%
0.12.0,%% [markdown]
0.12.0,Load an imbalanced dataset
0.12.0,--------------------------
0.12.0,
0.12.0,We will load the UCI SatImage dataset which has an imbalanced ratio of 9.3:1
0.12.0,(number of majority sample for a minority sample). The data are then split
0.12.0,into training and testing.
0.12.0,%%
0.12.0,%% [markdown]
0.12.0,Classification using a single decision tree
0.12.0,-------------------------------------------
0.12.0,
0.12.0,We train a decision tree classifier which will be used as a baseline for the
0.12.0,rest of this example.
0.12.0,
0.12.0,The results are reported in terms of balanced accuracy and geometric mean
0.12.0,which are metrics widely used in the literature to validate model trained on
0.12.0,imbalanced set.
0.12.0,%%
0.12.0,%%
0.12.0,%%
0.12.0,%% [markdown]
0.12.0,Classification using bagging classifier with and without sampling
0.12.0,-----------------------------------------------------------------
0.12.0,
0.12.0,"Instead of using a single tree, we will check if an ensemble of decision tree"
0.12.0,"can actually alleviate the issue induced by the class imbalancing. First, we"
0.12.0,will use a bagging classifier and its counter part which internally uses a
0.12.0,random under-sampling to balanced each bootstrap sample.
0.12.0,%%
0.12.0,%% [markdown]
0.12.0,Balancing each bootstrap sample allows to increase significantly the balanced
0.12.0,accuracy and the geometric mean.
0.12.0,%%
0.12.0,%%
0.12.0,%% [markdown]
0.12.0,Classification using random forest classifier with and without sampling
0.12.0,-----------------------------------------------------------------------
0.12.0,
0.12.0,Random forest is another popular ensemble method and it is usually
0.12.0,"outperforming bagging. Here, we used a vanilla random forest and its balanced"
0.12.0,counterpart in which each bootstrap sample is balanced.
0.12.0,%%
0.12.0,%% [markdown]
0.12.0,"Similarly to the previous experiment, the balanced classifier outperform the"
0.12.0,"classifier which learn from imbalanced bootstrap samples. In addition, random"
0.12.0,forest outperforms the bagging classifier.
0.12.0,%%
0.12.0,%%
0.12.0,%% [markdown]
0.12.0,Boosting classifier
0.12.0,-------------------
0.12.0,
0.12.0,"In the same manner, easy ensemble classifier is a bag of balanced AdaBoost"
0.12.0,"classifier. However, it will be slower to train than random forest and will"
0.12.0,achieve worse performance.
0.12.0,%%
0.12.0,%%
0.12.0,%%
0.12.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.0,License: MIT
0.12.0,%%
0.12.0,%% [markdown]
0.12.0,Generate an imbalanced dataset
0.12.0,------------------------------
0.12.0,
0.12.0,"For this example, we will create a synthetic dataset using the function"
0.12.0,:func:`~sklearn.datasets.make_classification`. The problem will be a toy
0.12.0,classification problem with a ratio of 1:9 between the two classes.
0.12.0,%%
0.12.0,%%
0.12.0,%% [markdown]
0.12.0,"In the following sections, we will show a couple of algorithms that have"
0.12.0,been proposed over the years. We intend to illustrate how one can reuse the
0.12.0,:class:`~imblearn.ensemble.BalancedBaggingClassifier` by passing different
0.12.0,sampler.
0.12.0,%%
0.12.0,%% [markdown]
0.12.0,Exactly Balanced Bagging and Over-Bagging
0.12.0,-----------------------------------------
0.12.0,
0.12.0,The :class:`~imblearn.ensemble.BalancedBaggingClassifier` can use in
0.12.0,conjunction with a :class:`~imblearn.under_sampling.RandomUnderSampler` or
0.12.0,:class:`~imblearn.over_sampling.RandomOverSampler`. These methods are
0.12.0,"referred as Exactly Balanced Bagging and Over-Bagging, respectively and have"
0.12.0,been proposed first in [1]_.
0.12.0,%%
0.12.0,Exactly Balanced Bagging
0.12.0,%%
0.12.0,Over-bagging
0.12.0,%% [markdown]
0.12.0,SMOTE-Bagging
0.12.0,-------------
0.12.0,
0.12.0,Instead of using a :class:`~imblearn.over_sampling.RandomOverSampler` that
0.12.0,"make a bootstrap, an alternative is to use"
0.12.0,:class:`~imblearn.over_sampling.SMOTE` as an over-sampler. This is known as
0.12.0,SMOTE-Bagging [2]_.
0.12.0,%%
0.12.0,SMOTE-Bagging
0.12.0,%% [markdown]
0.12.0,Roughly Balanced Bagging
0.12.0,------------------------
0.12.0,While using a :class:`~imblearn.under_sampling.RandomUnderSampler` or
0.12.0,:class:`~imblearn.over_sampling.RandomOverSampler` will create exactly the
0.12.0,"desired number of samples, it does not follow the statistical spirit wanted"
0.12.0,in the bagging framework. The authors in [3]_ proposes to use a negative
0.12.0,binomial distribution to compute the number of samples of the majority
0.12.0,class to be selected and then perform a random under-sampling.
0.12.0,
0.12.0,"Here, we illustrate this method by implementing a function in charge of"
0.12.0,resampling and use the :class:`~imblearn.FunctionSampler` to integrate it
0.12.0,within a :class:`~imblearn.pipeline.Pipeline` and
0.12.0,:class:`~sklearn.model_selection.cross_validate`.
0.12.0,%%
0.12.0,find the minority and majority classes
0.12.0,compute the number of sample to draw from the majority class using
0.12.0,a negative binomial distribution
0.12.0,draw randomly with or without replacement
0.12.0,Roughly Balanced Bagging
0.12.0,%% [markdown]
0.12.0,.. topic:: References:
0.12.0,
0.12.0,".. [1] R. Maclin, and D. Opitz. ""An empirical evaluation of bagging and"
0.12.0,"boosting."" AAAI/IAAI 1997 (1997): 546-551."
0.12.0,
0.12.0,".. [2] S. Wang, and X. Yao. ""Diversity analysis on imbalanced data sets by"
0.12.0,"using ensemble models."" 2009 IEEE symposium on computational"
0.12.0,"intelligence and data mining. IEEE, 2009."
0.12.0,
0.12.0,".. [3] S. Hido, H. Kashima, and Y. Takahashi. ""Roughly balanced bagging"
0.12.0,"for imbalanced data."" Statistical Analysis and Data Mining: The ASA"
0.12.0,Data Science Journal 2.5‚Äê6 (2009): 412-426.
0.12.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.0,License: MIT
0.12.0,%%
0.12.0,%% [markdown]
0.12.0,The following function will be used to create toy dataset. It uses the
0.12.0,:func:`~sklearn.datasets.make_classification` from scikit-learn but fixing
0.12.0,some parameters.
0.12.0,%%
0.12.0,%% [markdown]
0.12.0,The following function will be used to plot the sample space after resampling
0.12.0,to illustrate the specificities of an algorithm.
0.12.0,%%
0.12.0,%% [markdown]
0.12.0,The following function will be used to plot the decision function of a
0.12.0,classifier given some data.
0.12.0,%%
0.12.0,%%
0.12.0,%% [markdown]
0.12.0,Prototype generation: under-sampling by generating new samples
0.12.0,--------------------------------------------------------------
0.12.0,
0.12.0,:class:`~imblearn.under_sampling.ClusterCentroids` under-samples by replacing
0.12.0,the original samples by the centroids of the cluster found.
0.12.0,%%
0.12.0,%% [markdown]
0.12.0,Prototype selection: under-sampling by selecting existing samples
0.12.0,-----------------------------------------------------------------
0.12.0,
0.12.0,The algorithm performing prototype selection can be subdivided into two
0.12.0,groups: (i) the controlled under-sampling methods and (ii) the cleaning
0.12.0,under-sampling methods.
0.12.0,
0.12.0,"With the controlled under-sampling methods, the number of samples to be"
0.12.0,selected can be specified.
0.12.0,:class:`~imblearn.under_sampling.RandomUnderSampler` is the most naive way of
0.12.0,performing such selection by randomly selecting a given number of samples by
0.12.0,the targeted class.
0.12.0,%%
0.12.0,%% [markdown]
0.12.0,:class:`~imblearn.under_sampling.NearMiss` algorithms implement some
0.12.0,heuristic rules in order to select samples. NearMiss-1 selects samples from
0.12.0,the majority class for which the average distance of the :math:`k`` nearest
0.12.0,samples of the minority class is the smallest. NearMiss-2 selects the samples
0.12.0,from the majority class for which the average distance to the farthest
0.12.0,samples of the negative class is the smallest. NearMiss-3 is a 2-step
0.12.0,"algorithm: first, for each minority sample, their :math:`m`"
0.12.0,"nearest-neighbors will be kept; then, the majority samples selected are the"
0.12.0,on for which the average distance to the :math:`k` nearest neighbors is the
0.12.0,largest.
0.12.0,%%
0.12.0,%% [markdown]
0.12.0,:class:`~imblearn.under_sampling.EditedNearestNeighbours` removes samples of
0.12.0,the majority class for which their class differ from the one of their
0.12.0,nearest-neighbors. This sieve can be repeated which is the principle of the
0.12.0,:class:`~imblearn.under_sampling.RepeatedEditedNearestNeighbours`.
0.12.0,:class:`~imblearn.under_sampling.AllKNN` is slightly different from the
0.12.0,:class:`~imblearn.under_sampling.RepeatedEditedNearestNeighbours` by changing
0.12.0,"the :math:`k` parameter of the internal nearest neighors algorithm,"
0.12.0,increasing it at each iteration.
0.12.0,%%
0.12.0,%% [markdown]
0.12.0,:class:`~imblearn.under_sampling.CondensedNearestNeighbour` makes use of a
0.12.0,1-NN to iteratively decide if a sample should be kept in a dataset or not.
0.12.0,The issue is that :class:`~imblearn.under_sampling.CondensedNearestNeighbour`
0.12.0,is sensitive to noise by preserving the noisy samples.
0.12.0,:class:`~imblearn.under_sampling.OneSidedSelection` also used the 1-NN and
0.12.0,use :class:`~imblearn.under_sampling.TomekLinks` to remove the samples
0.12.0,considered noisy. The
0.12.0,:class:`~imblearn.under_sampling.NeighbourhoodCleaningRule` use a
0.12.0,:class:`~imblearn.under_sampling.EditedNearestNeighbours` to remove some
0.12.0,"sample. Additionally, they use a 3 nearest-neighbors to remove samples which"
0.12.0,do not agree with this rule.
0.12.0,%%
0.12.0,%% [markdown]
0.12.0,:class:`~imblearn.under_sampling.InstanceHardnessThreshold` uses the
0.12.0,prediction of classifier to exclude samples. All samples which are classified
0.12.0,with a low probability will be removed.
0.12.0,%%
0.12.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.0,License: MIT
0.12.0,%%
0.12.0,%% [markdown]
0.12.0,This function allows to make nice plotting
0.12.0,%%
0.12.0,%% [markdown]
0.12.0,We will generate some toy data that illustrates how
0.12.0,:class:`~imblearn.under_sampling.TomekLinks` is used to clean a dataset.
0.12.0,%%
0.12.0,%% [markdown]
0.12.0,"In the figure above, the samples highlighted in green form a Tomek link since"
0.12.0,they are of different classes and are nearest neighbors of each other.
0.12.0,highlight the samples of interest
0.12.0,%% [markdown]
0.12.0,We can run the :class:`~imblearn.under_sampling.TomekLinks` sampling to
0.12.0,remove the corresponding samples. If `sampling_strategy='auto'` only the
0.12.0,sample from the majority class will be removed. If `sampling_strategy='all'`
0.12.0,both samples will be removed.
0.12.0,%%
0.12.0,highlight the samples of interest
0.12.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.0,License: MIT
0.12.0,%%
0.12.0,%% [markdown]
0.12.0,We define a function allowing to make some nice decoration on the plot.
0.12.0,%%
0.12.0,%% [markdown]
0.12.0,We can start by generating some data to later illustrate the principle of
0.12.0,each :class:`~imblearn.under_sampling.NearMiss` heuristic rules.
0.12.0,%%
0.12.0,%% [mardown]
0.12.0,NearMiss-1
0.12.0,----------
0.12.0,
0.12.0,NearMiss-1 selects samples from the majority class for which the average
0.12.0,distance to some nearest neighbours is the smallest. In the following
0.12.0,"example, we use a 3-NN to compute the average distance on 2 specific samples"
0.12.0,"of the majority class. Therefore, in this case the point linked by the"
0.12.0,green-dashed line will be selected since the average distance is smaller.
0.12.0,%%
0.12.0,%% [mardown]
0.12.0,NearMiss-2
0.12.0,----------
0.12.0,
0.12.0,NearMiss-2 selects samples from the majority class for which the average
0.12.0,distance to the farthest neighbors is the smallest. With the same
0.12.0,"configuration as previously presented, the sample linked to the green-dashed"
0.12.0,line will be selected since its distance the 3 farthest neighbors is the
0.12.0,smallest.
0.12.0,%%
0.12.0,%% [mardown]
0.12.0,NearMiss-3
0.12.0,----------
0.12.0,
0.12.0,"NearMiss-3 can be divided into 2 steps. First, a nearest-neighbors is used to"
0.12.0,short-list samples from the majority class (i.e. correspond to the
0.12.0,"highlighted samples in the following plot). Then, the sample with the largest"
0.12.0,average distance to the *k* nearest-neighbors are selected.
0.12.0,%%
0.12.0,select only the majority point of interest
0.12.0,Authors: Christos Aridas
0.12.0,Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.0,License: MIT
0.12.0,%%
0.12.0,%% [markdown]
0.12.0,Let's first create an imbalanced dataset and split in to two sets.
0.12.0,%%
0.12.0,%% [markdown]
0.12.0,"Now, we will create each individual steps that we would like later to combine"
0.12.0,%%
0.12.0,%% [markdown]
0.12.0,"Now, we can finally create a pipeline to specify in which order the different"
0.12.0,transformers and samplers should be executed before to provide the data to
0.12.0,the final classifier.
0.12.0,%%
0.12.0,%% [markdown]
0.12.0,We can now use the pipeline created as a normal classifier where resampling
0.12.0,"will happen when calling `fit` and disabled when calling `decision_function`,"
0.12.0,"`predict_proba`, or `predict`."
0.12.0,%%
0.12.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.0,License: MIT
0.12.0,##############################################################################
0.12.0,Data loading
0.12.0,##############################################################################
0.12.0,##############################################################################
0.12.0,"First, you should download the Porto Seguro data set from Kaggle. See the"
0.12.0,link in the introduction.
0.12.0,##############################################################################
0.12.0,The data set is imbalanced and it will have an effect on the fitting.
0.12.0,##############################################################################
0.12.0,Define the pre-processing pipeline
0.12.0,##############################################################################
0.12.0,##############################################################################
0.12.0,We want to standard scale the numerical features while we want to one-hot
0.12.0,"encode the categorical features. In this regard, we make use of the"
0.12.0,:class:`~sklearn.compose.ColumnTransformer`.
0.12.0,Create an environment variable to avoid using the GPU. This can be changed.
0.12.0,##############################################################################
0.12.0,Create a neural-network
0.12.0,##############################################################################
0.12.0,##############################################################################
0.12.0,We create a decorator to report the computation time
0.12.0,##############################################################################
0.12.0,The first model will be trained using the ``fit`` method and with imbalanced
0.12.0,mini-batches.
0.12.0,predict_proba was removed in tensorflow 2.6
0.12.0,##############################################################################
0.12.0,"In the contrary, we will use imbalanced-learn to create a generator of"
0.12.0,mini-batches which will yield balanced mini-batches.
0.12.0,##############################################################################
0.12.0,Classification loop
0.12.0,##############################################################################
0.12.0,##############################################################################
0.12.0,We will perform a 10-fold cross-validation and train the neural-network with
0.12.0,the two different strategies previously presented.
0.12.0,##############################################################################
0.12.0,Plot of the results and computation time
0.12.0,##############################################################################
0.12.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.0,License: MIT
0.12.0,%%
0.12.0,%% [markdown]
0.12.0,Problem definition
0.12.0,------------------
0.12.0,
0.12.0,We are dropping the following features:
0.12.0,
0.12.0,"- ""fnlwgt"": this feature was created while studying the ""adult"" dataset."
0.12.0,"Thus, we will not use this feature which is not acquired during the survey."
0.12.0,"- ""education-num"": it is encoding the same information than ""education""."
0.12.0,"Thus, we are removing one of these 2 features."
0.12.0,%%
0.12.0,%% [markdown]
0.12.0,"The ""adult"" dataset as a class ratio of about 3:1"
0.12.0,%%
0.12.0,%% [markdown]
0.12.0,This dataset is only slightly imbalanced. To better highlight the effect of
0.12.0,"learning from an imbalanced dataset, we will increase its ratio to 30:1"
0.12.0,%%
0.12.0,%% [markdown]
0.12.0,We will perform a cross-validation evaluation to get an estimate of the test
0.12.0,score.
0.12.0,
0.12.0,"As a baseline, we could use a classifier which will always predict the"
0.12.0,majority class independently of the features provided.
0.12.0,%%
0.12.0,%% [markdown]
0.12.0,"Instead of using the accuracy, we can use the balanced accuracy which will"
0.12.0,take into account the balancing issue.
0.12.0,%%
0.12.0,%% [markdown]
0.12.0,Strategies to learn from an imbalanced dataset
0.12.0,----------------------------------------------
0.12.0,We will use a dictionary and a list to continuously store the results of
0.12.0,our experiments and show them as a pandas dataframe.
0.12.0,%%
0.12.0,%% [markdown]
0.12.0,Dummy baseline
0.12.0,..............
0.12.0,
0.12.0,"Before to train a real machine learning model, we can store the results"
0.12.0,obtained with our :class:`~sklearn.dummy.DummyClassifier`.
0.12.0,%%
0.12.0,%% [markdown]
0.12.0,Linear classifier baseline
0.12.0,..........................
0.12.0,
0.12.0,We will create a machine learning pipeline using a
0.12.0,":class:`~sklearn.linear_model.LogisticRegression` classifier. In this regard,"
0.12.0,we will need to one-hot encode the categorical columns and standardized the
0.12.0,numerical columns before to inject the data into the
0.12.0,:class:`~sklearn.linear_model.LogisticRegression` classifier.
0.12.0,
0.12.0,"First, we define our numerical and categorical pipelines."
0.12.0,%%
0.12.0,%% [markdown]
0.12.0,"Then, we can create a preprocessor which will dispatch the categorical"
0.12.0,columns to the categorical pipeline and the numerical columns to the
0.12.0,numerical pipeline
0.12.0,%%
0.12.0,%% [markdown]
0.12.0,"Finally, we connect our preprocessor with our"
0.12.0,:class:`~sklearn.linear_model.LogisticRegression`. We can then evaluate our
0.12.0,model.
0.12.0,%%
0.12.0,%%
0.12.0,%% [markdown]
0.12.0,We can see that our linear model is learning slightly better than our dummy
0.12.0,"baseline. However, it is impacted by the class imbalance."
0.12.0,
0.12.0,We can verify that something similar is happening with a tree-based model
0.12.0,such as :class:`~sklearn.ensemble.RandomForestClassifier`. With this type of
0.12.0,"classifier, we will not need to scale the numerical data, and we will only"
0.12.0,need to ordinal encode the categorical data.
0.12.0,%%
0.12.0,%%
0.12.0,%% [markdown]
0.12.0,The :class:`~sklearn.ensemble.RandomForestClassifier` is as well affected by
0.12.0,"the class imbalanced, slightly less than the linear model. Now, we will"
0.12.0,present different approach to improve the performance of these 2 models.
0.12.0,
0.12.0,Use `class_weight`
0.12.0,..................
0.12.0,
0.12.0,Most of the models in `scikit-learn` have a parameter `class_weight`. This
0.12.0,parameter will affect the computation of the loss in linear model or the
0.12.0,criterion in the tree-based model to penalize differently a false
0.12.0,classification from the minority and majority class. We can set
0.12.0,"`class_weight=""balanced""` such that the weight applied is inversely"
0.12.0,proportional to the class frequency. We test this parametrization in both
0.12.0,linear model and tree-based model.
0.12.0,%%
0.12.0,%%
0.12.0,%% [markdown]
0.12.0,We can see that using `class_weight` was really effective for the linear
0.12.0,"model, alleviating the issue of learning from imbalanced classes. However,"
0.12.0,the :class:`~sklearn.ensemble.RandomForestClassifier` is still biased toward
0.12.0,"the majority class, mainly due to the criterion which is not suited enough to"
0.12.0,fight the class imbalance.
0.12.0,
0.12.0,Resample the training set during learning
0.12.0,.........................................
0.12.0,
0.12.0,Another way is to resample the training set by under-sampling or
0.12.0,over-sampling some of the samples. `imbalanced-learn` provides some samplers
0.12.0,to do such processing.
0.12.0,%%
0.12.0,%%
0.12.0,%%
0.12.0,%%
0.12.0,%% [markdown]
0.12.0,Applying a random under-sampler before the training of the linear model or
0.12.0,"random forest, allows to not focus on the majority class at the cost of"
0.12.0,making more mistake for samples in the majority class (i.e. decreased
0.12.0,accuracy).
0.12.0,
0.12.0,We could apply any type of samplers and find which sampler is working best
0.12.0,on the current dataset.
0.12.0,
0.12.0,"Instead, we will present another way by using classifiers which will apply"
0.12.0,sampling internally.
0.12.0,
0.12.0,Use of specific balanced algorithms from imbalanced-learn
0.12.0,.........................................................
0.12.0,
0.12.0,We already showed that random under-sampling can be effective on decision
0.12.0,"tree. However, instead of under-sampling once the dataset, one could"
0.12.0,under-sample the original dataset before to take a bootstrap sample. This is
0.12.0,the base of the :class:`imblearn.ensemble.BalancedRandomForestClassifier` and
0.12.0,:class:`~imblearn.ensemble.BalancedBaggingClassifier`.
0.12.0,%%
0.12.0,%%
0.12.0,%% [markdown]
0.12.0,The performance with the
0.12.0,:class:`~imblearn.ensemble.BalancedRandomForestClassifier` is better than
0.12.0,applying a single random under-sampling. We will use a gradient-boosting
0.12.0,classifier within a :class:`~imblearn.ensemble.BalancedBaggingClassifier`.
0.12.0,%% [markdown]
0.12.0,This last approach is the most effective. The different under-sampling allows
0.12.0,to bring some diversity for the different GBDT to learn and not focus on a
0.12.0,portion of the majority class.
0.12.0,Authors: Christos Aridas
0.12.0,Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.0,License: MIT
0.12.0,%%
0.12.0,%% [markdown]
0.12.0,Load the dataset
0.12.0,----------------
0.12.0,
0.12.0,We will use a dataset containing image from know person where we will
0.12.0,build a model to recognize the person on the image. We will make this problem
0.12.0,a binary problem by taking picture of only George W. Bush and Bill Clinton.
0.12.0,%%
0.12.0,%%
0.12.0,%% [markdown]
0.12.0,We can check the ratio between the two classes.
0.12.0,%%
0.12.0,%% [markdown]
0.12.0,We see that we have an imbalanced classification problem with ~95% of the
0.12.0,data belonging to the class G.W. Bush.
0.12.0,
0.12.0,Compare over-sampling approaches
0.12.0,--------------------------------
0.12.0,
0.12.0,We will use different over-sampling approaches and use a kNN classifier
0.12.0,to check if we can recognize the 2 presidents. The evaluation will be
0.12.0,performed through cross-validation and we will plot the mean ROC curve.
0.12.0,
0.12.0,We will create different pipelines and evaluate them.
0.12.0,%%
0.12.0,%% [markdown]
0.12.0,We will compute the mean ROC curve for each pipeline using a different splits
0.12.0,provided by the :class:`~sklearn.model_selection.StratifiedKFold`
0.12.0,cross-validation.
0.12.0,%%
0.12.0,compute the mean fpr/tpr to get the mean ROC curve
0.12.0,Create a display that we will reuse to make the aggregated plots for
0.12.0,all methods
0.12.0,%% [markdown]
0.12.0,"In the previous cell, we created the different mean ROC curve and we can plot"
0.12.0,them on the same plot.
0.12.0,%%
0.12.0,%% [markdown]
0.12.0,"We see that for this task, methods that are generating new samples with some"
0.12.0,interpolation (i.e. ADASYN and SMOTE) perform better than random
0.12.0,over-sampling or no resampling.
0.12.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.0,License: MIT
0.12.0,Create a folder to fetch the dataset
0.12.0,Create a pipeline
0.12.0,Classify and report the results
0.12.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.0,License: MIT
0.12.0,%%
0.12.0,%% [markdown]
0.12.0,Setting the data set
0.12.0,--------------------
0.12.0,
0.12.0,We use a part of the 20 newsgroups data set by loading 4 topics. Using the
0.12.0,"scikit-learn loader, the data are split into a training and a testing set."
0.12.0,
0.12.0,Note the class \#3 is the minority class and has almost twice less samples
0.12.0,than the majority class.
0.12.0,%%
0.12.0,%%
0.12.0,%% [markdown]
0.12.0,The usual scikit-learn pipeline
0.12.0,-------------------------------
0.12.0,
0.12.0,You might usually use scikit-learn pipeline by combining the TF-IDF
0.12.0,vectorizer to feed a multinomial naive bayes classifier. A classification
0.12.0,report summarized the results on the testing set.
0.12.0,
0.12.0,"As expected, the recall of the class \#3 is low mainly due to the class"
0.12.0,imbalanced.
0.12.0,%%
0.12.0,%%
0.12.0,%% [markdown]
0.12.0,Balancing the class before classification
0.12.0,-----------------------------------------
0.12.0,
0.12.0,"To improve the prediction of the class \#3, it could be interesting to apply"
0.12.0,"a balancing before to train the naive bayes classifier. Therefore, we will"
0.12.0,use a :class:`~imblearn.under_sampling.RandomUnderSampler` to equalize the
0.12.0,number of samples in all the classes before the training.
0.12.0,
0.12.0,It is also important to note that we are using the
0.12.0,:class:`~imblearn.pipeline.make_pipeline` function implemented in
0.12.0,imbalanced-learn to properly handle the samplers.
0.12.0,%%
0.12.0,%% [markdown]
0.12.0,"Although the results are almost identical, it can be seen that the resampling"
0.12.0,allowed to correct the poor recall of the class \#3 at the cost of reducing
0.12.0,"the other metrics for the other classes. However, the overall results are"
0.12.0,slightly better.
0.12.0,%%
0.12.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.0,License: MIT
0.12.0,#############################################################################
0.12.0,Toy data generation
0.12.0,#############################################################################
0.12.0,#############################################################################
0.12.0,We are generating some non Gaussian data set contaminated with some unform
0.12.0,noise.
0.12.0,#############################################################################
0.12.0,We will generate some cleaned test data without outliers.
0.12.0,#############################################################################
0.12.0,How to use the :class:`~imblearn.FunctionSampler`
0.12.0,#############################################################################
0.12.0,#############################################################################
0.12.0,We first define a function which will use
0.12.0,:class:`~sklearn.ensemble.IsolationForest` to eliminate some outliers from
0.12.0,our dataset during training. The function passed to the
0.12.0,:class:`~imblearn.FunctionSampler` will be called when using the method
0.12.0,``fit_resample``.
0.12.0,#############################################################################
0.12.0,Integrate it within a pipeline
0.12.0,#############################################################################
0.12.0,#############################################################################
0.12.0,"By elimnating outliers before the training, the classifier will be less"
0.12.0,affected during the prediction.
0.12.0,Authors: Dayvid Oliveira
0.12.0,Christos Aridas
0.12.0,Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.0,License: MIT
0.12.0,%%
0.12.0,%% [markdown]
0.12.0,Generate the dataset
0.12.0,--------------------
0.12.0,
0.12.0,"First, we will generate a dataset and convert it to a"
0.12.0,:class:`~pandas.DataFrame` with arbitrary column names. We will plot the
0.12.0,original dataset.
0.12.0,%%
0.12.0,%% [markdown]
0.12.0,Make a dataset imbalanced
0.12.0,-------------------------
0.12.0,
0.12.0,"Now, we will show the helpers :func:`~imblearn.datasets.make_imbalance`"
0.12.0,that is useful to random select a subset of samples. It will impact the
0.12.0,class distribution as specified by the parameters.
0.12.0,%%
0.12.0,%%
0.12.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.0,License: MIT
0.12.0,%%
0.12.0,%% [markdown]
0.12.0,Create an imbalanced dataset
0.12.0,----------------------------
0.12.0,
0.12.0,"First, we will create an imbalanced data set from a the iris data set."
0.12.0,%%
0.12.0,%%
0.12.0,%% [markdown]
0.12.0,Using ``sampling_strategy`` in resampling algorithms
0.12.0,====================================================
0.12.0,
0.12.0,`sampling_strategy` as a `float`
0.12.0,--------------------------------
0.12.0,
0.12.0,`sampling_strategy` can be given a `float`. For **under-sampling
0.12.0,"methods**, it corresponds to the ratio :math:`\alpha_{us}` defined by"
0.12.0,:math:`N_{rM} = \alpha_{us} \times N_{m}` where :math:`N_{rM}` and
0.12.0,:math:`N_{m}` are the number of samples in the majority class after
0.12.0,"resampling and the number of samples in the minority class, respectively."
0.12.0,%%
0.12.0,select only 2 classes since the ratio make sense in this case
0.12.0,%%
0.12.0,%% [markdown]
0.12.0,"For **over-sampling methods**, it correspond to the ratio"
0.12.0,:math:`\alpha_{os}` defined by :math:`N_{rm} = \alpha_{os} \times N_{M}`
0.12.0,where :math:`N_{rm}` and :math:`N_{M}` are the number of samples in the
0.12.0,minority class after resampling and the number of samples in the majority
0.12.0,"class, respectively."
0.12.0,%%
0.12.0,%% [markdown]
0.12.0,`sampling_strategy` as a `str`
0.12.0,-------------------------------
0.12.0,
0.12.0,`sampling_strategy` can be given as a string which specify the class
0.12.0,"targeted by the resampling. With under- and over-sampling, the number of"
0.12.0,samples will be equalized.
0.12.0,
0.12.0,Note that we are using multiple classes from now on.
0.12.0,%%
0.12.0,%% [markdown]
0.12.0,"With **cleaning method**, the number of samples in each class will not be"
0.12.0,equalized even if targeted.
0.12.0,%%
0.12.0,%% [markdown]
0.12.0,`sampling_strategy` as a `dict`
0.12.0,------------------------------
0.12.0,
0.12.0,"When `sampling_strategy` is a `dict`, the keys correspond to the targeted"
0.12.0,classes. The values correspond to the desired number of samples for each
0.12.0,targeted class. This is working for both **under- and over-sampling**
0.12.0,algorithms but not for the **cleaning algorithms**. Use a `list` instead.
0.12.0,%%
0.12.0,%% [markdown]
0.12.0,`sampling_strategy` as a `list`
0.12.0,-------------------------------
0.12.0,
0.12.0,"When `sampling_strategy` is a `list`, the list contains the targeted"
0.12.0,classes. It is used only for **cleaning methods** and raise an error
0.12.0,otherwise.
0.12.0,%%
0.12.0,%% [markdown]
0.12.0,`sampling_strategy` as a callable
0.12.0,---------------------------------
0.12.0,
0.12.0,"When callable, function taking `y` and returns a `dict`. The keys"
0.12.0,correspond to the targeted classes. The values correspond to the desired
0.12.0,number of samples for each class.
0.12.0,%%
0.12.0,List of whitelisted modules and methods; regexp are supported.
0.12.0,These docstrings will fail because they are inheriting from scikit-learn
0.12.0,skip private classes
0.12.0,"We ignore following error code,"
0.12.0,- RT02: The first line of the Returns section
0.12.0,"should contain only the type, .."
0.12.0,(as we may need refer to the name of the returned
0.12.0,object)
0.12.0,- GL01: Docstring text (summary) should start in the line
0.12.0,"immediately after the opening quotes (not in the same line,"
0.12.0,or leaving a blank line in between)
0.12.0,"- GL02: If there's a blank line, it should be before the"
0.12.0,"first line of the Returns section, not after (it allows to have"
0.12.0,short docstrings for properties).
0.12.0,Ignore PR02: Unknown parameters for properties. We sometimes use
0.12.0,"properties for ducktyping, i.e. SGDClassifier.predict_proba"
0.12.0,Following codes are only taken into account for the
0.12.0,top level class docstrings:
0.12.0,- ES01: No extended summary found
0.12.0,- SA01: See Also section not found
0.12.0,- EX01: No examples section found
0.12.0,In particular we can't parse the signature of properties
0.12.0,"When applied to classes, detect class method. For functions"
0.12.0,method = None.
0.12.0,TODO: this detection can be improved. Currently we assume that we have
0.12.0,class # methods if the second path element before last is in camel case.
0.12.0,'build' and 'install' is included to have structured metadata for CI.
0.12.0,It will NOT be included in setup's extras_require
0.12.0,"The values are (version_spec, comma separated tags)"
0.12.0,create inverse mapping for setuptools
0.12.0,Used by CI to get the min dependencies
0.12.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.0,License: MIT
0.12.0,TODO: remove this file when scikit-learn minimum version is 1.3
0.12.0,Return a copy of the threadlocal configuration so that users will
0.12.0,not be able to modify the configuration with the returned dict.
0.12.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.0,Christos Aridas
0.12.0,License: MIT
0.12.0,scikit-learn >= 1.2
0.12.0,we need to overwrite SamplerMixin.fit to bypass the validation
0.12.0,Adapted from scikit-learn
0.12.0,Author: Edouard Duchesnay
0.12.0,Gael Varoquaux
0.12.0,Virgile Fritsch
0.12.0,Alexandre Gramfort
0.12.0,Lars Buitinck
0.12.0,Christos Aridas
0.12.0,Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.0,License: BSD
0.12.0,BaseEstimator interface
0.12.0,validate names
0.12.0,validate estimators
0.12.0,We allow last estimator to be None as an identity transformation
0.12.0,Estimator interface
0.12.0,"def _fit(self, X, y=None, **fit_params_steps):"
0.12.0,Setup the memory
0.12.0,we do not clone when caching is disabled to
0.12.0,preserve backward compatibility
0.12.0,Fit or load from cache the current transformer
0.12.0,Replace the transformer of the step with the fitted
0.12.0,transformer. This is necessary when loading the transformer
0.12.0,from the cache.
0.12.0,The `fit_*` methods need to be overridden to support the samplers.
0.12.0,estimators in Pipeline.steps are not validated yet
0.12.0,estimators in Pipeline.steps are not validated yet
0.12.0,metadata routing enabled
0.12.0,estimators in Pipeline.steps are not validated yet
0.12.0,estimators in Pipeline.steps are not validated yet
0.12.0,TODO: remove the following methods when the minimum scikit-learn >= 1.4
0.12.0,They do not depend on resampling but we need to redefine them for the
0.12.0,compatibility with the metadata routing framework.
0.12.0,metadata routing enabled
0.12.0,not branching here since params is only available if
0.12.0,enable_metadata_routing=True
0.12.0,metadata routing enabled
0.12.0,not branching here since params is only available if
0.12.0,enable_metadata_routing=True
0.12.0,"we don't have to branch here, since params is only non-empty if"
0.12.0,enable_metadata_routing=True.
0.12.0,metadata routing is enabled.
0.12.0,"TODO: once scikit-learn >= 1.4, the following function should be simplified by"
0.12.0,calling `super().get_metadata_routing()`
0.12.0,first we add all steps except the last one
0.12.0,"fit, fit_predict, and fit_transform call fit_transform if it"
0.12.0,"exists, or else fit and transform"
0.12.0,then we add the last step
0.12.0,"without metadata routing, fit_transform and fit_predict"
0.12.0,get all the same params and pass it to the last fit.
0.12.0,"if we have a weight for this transformer, multiply output"
0.12.0,This variable is injected in the __builtins__ by the build
0.12.0,process. It is used to enable importing subpackages of sklearn when
0.12.0,the binaries are not built
0.12.0,mypy error: Cannot determine type of '__SKLEARN_SETUP__'
0.12.0,We are not importing the rest of scikit-learn during the build
0.12.0,"process, as it may not be compiled yet"
0.12.0,"FIXME: When we get Python 3.7 as minimal version, we will need to switch to"
0.12.0,the following solution:
0.12.0,https://snarky.ca/lazy-importing-in-python-3-7/
0.12.0,Import the target module and insert it into the parent's namespace
0.12.0,Update this object's dict so that if someone keeps a reference to the
0.12.0,"LazyLoader, lookups are efficient (__getattr__ is only called on"
0.12.0,lookups that fail).
0.12.0,delay the import of keras since we are going to import either tensorflow
0.12.0,or keras
0.12.0,Based on NiLearn package
0.12.0,License: simplified BSD
0.12.0,"PEP0440 compatible formatted version, see:"
0.12.0,https://www.python.org/dev/peps/pep-0440/
0.12.0,
0.12.0,Generic release markers:
0.12.0,X.Y
0.12.0,X.Y.Z # For bugfix releases
0.12.0,
0.12.0,Admissible pre-release markers:
0.12.0,X.YaN # Alpha release
0.12.0,X.YbN # Beta release
0.12.0,X.YrcN # Release Candidate
0.12.0,X.Y # Final release
0.12.0,
0.12.0,Dev branch marker is: 'X.Y.dev' or 'X.Y.devN' where N is an integer.
0.12.0,'X.Y.dev0' is the canonical version of 'X.Y.dev'
0.12.0,
0.12.0,coding: utf-8
0.12.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.0,Dariusz Brzezinski
0.12.0,License: MIT
0.12.0,Only negative labels
0.12.0,"Calculate tp_sum, pred_sum, true_sum ###"
0.12.0,labels are now from 0 to len(labels) - 1 -> use bincount
0.12.0,Pathological case
0.12.0,Compute the true negative
0.12.0,Retain only selected labels
0.12.0,"Finally, we have all our sufficient statistics. Divide! #"
0.12.0,"Divide, and on zero-division, set scores to 0 and warn:"
0.12.0,"Oddly, we may get an ""invalid"" rather than a ""divide"" error"
0.12.0,here.
0.12.0,Average the results
0.12.0,labels are now from 0 to len(labels) - 1 -> use bincount
0.12.0,Pathological case
0.12.0,Retain only selected labels
0.12.0,old version of scipy return MaskedConstant instead of 0.0
0.12.0,check that the scoring function does not need a score
0.12.0,and only a prediction
0.12.0,We do not support multilabel so the only average supported
0.12.0,is binary
0.12.0,Compute the different metrics
0.12.0,Precision/recall/f1
0.12.0,Specificity
0.12.0,Geometric mean
0.12.0,Index balanced accuracy
0.12.0,compute averages
0.12.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.0,License: MIT
0.12.0,categories are expected to be encoded from 0 to n_categories - 1
0.12.0,"list of length n_features of ndarray (n_categories, n_classes)"
0.12.0,compute the counts
0.12.0,normalize by the summing over the classes
0.12.0,silence potential warning due to in-place division by zero
0.12.0,coding: utf-8
0.12.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.0,Christos Aridas
0.12.0,License: MIT
0.12.0,##############################################################################
0.12.0,Utilities for testing
0.12.0,import some data to play with
0.12.0,restrict to a binary classification task
0.12.0,add noisy features to make the problem harder and avoid perfect results
0.12.0,"run classifier, get class probabilities and label predictions"
0.12.0,only interested in probabilities of the positive case
0.12.0,XXX: do we really want a special API for the binary case?
0.12.0,##############################################################################
0.12.0,Tests
0.12.0,detailed measures for each class
0.12.0,individual scoring function that can be used for grid search: in the
0.12.0,binary class case the score is the value of the measure for the positive
0.12.0,class (e.g. label == 1). This is deprecated for average != 'binary'.
0.12.0,Such a case may occur with non-stratified cross-validation
0.12.0,ensure the above were meaningful tests:
0.12.0,Bad pos_label
0.12.0,Bad average option
0.12.0,but average != 'binary'; even if data is binary
0.12.0,compute the geometric mean for the binary problem
0.12.0,print classification report with class names
0.12.0,print classification report with label detection
0.12.0,print classification report with class names
0.12.0,print classification report with label detection
0.12.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.0,License: MIT
0.12.0,Check basic feature of the metric:
0.12.0,"* the shape of the distance matrix is (n_samples, n_samples)"
0.12.0,* computing pairwise distance of X is the same than explicitely between
0.12.0,X and X.
0.12.0,Check the property of the vdm distance. Let's check the property
0.12.0,"described in ""Improved Heterogeneous Distance Functions"", D.R. Wilson and"
0.12.0,"T.R. Martinez, Journal of Artificial Intelligence Research 6 (1997) 1-34"
0.12.0,https://arxiv.org/pdf/cs/9701101.pdf
0.12.0,
0.12.0,"""if an attribute color has three values red, green and blue, and the"
0.12.0,"application is to identify whether or not an object is an apple, red and"
0.12.0,green would be considered closer than red and blue because the former two
0.12.0,"both have similar correlations with the output class apple."""
0.12.0,defined our feature
0.12.0,0 - not an apple / 1 - an apple
0.12.0,computing the distance between a sample of the same category should
0.12.0,give a null distance
0.12.0,check the property explained in the introduction example
0.12.0,green and red are very close
0.12.0,blue is closer to red than green
0.12.0,"Check that ""auto"" is equivalent to provide the number categories"
0.12.0,beforehand
0.12.0,Check that we raise an error if n_categories is inconsistent with the
0.12.0,number of features in X
0.12.0,Check that we don't get issue when a category is missing between 0
0.12.0,n_categories - 1
0.12.0,remove a categories that could be between 0 and n_categories
0.12.0,Check that we raise a NotFittedError when `fit` is not not called before
0.12.0,pairwise.
0.12.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.0,Christos Aridas
0.12.0,License: MIT
0.12.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.0,Christos Aridas
0.12.0,License: MIT
0.12.0,FIXME: to be removed in 0.12
0.12.0,The ratio is computed using a one-vs-rest manner. Using majority
0.12.0,in multi-class would lead to slightly different results at the
0.12.0,cost of introducing a new parameter.
0.12.0,rounding may cause new amount for n_samples
0.12.0,the nearest neighbors need to be fitted only on the current class
0.12.0,to find the class NN to generate new samples
0.12.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.0,Christos Aridas
0.12.0,License: MIT
0.12.0,smoothed bootstrap imposes to make numerical operation; we need
0.12.0,to be sure to have only numerical data in X
0.12.0,generate a smoothed bootstrap with a perturbation
0.12.0,generate a bootstrap
0.12.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.0,Christos Aridas
0.12.0,License: MIT
0.12.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.0,Fernando Nogueira
0.12.0,Christos Aridas
0.12.0,License: MIT
0.12.0,negate diagonal elements
0.12.0,identify cluster which are answering the requirements
0.12.0,empty cluster
0.12.0,the cluster is already considered balanced
0.12.0,not enough samples to apply SMOTE
0.12.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.0,Fernando Nogueira
0.12.0,Christos Aridas
0.12.0,Dzianis Dudnik
0.12.0,License: MIT
0.12.0,FIXME: to be removed in 0.12
0.12.0,FIXME: to be removed in 0.12
0.12.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.0,Fernando Nogueira
0.12.0,Christos Aridas
0.12.0,Dzianis Dudnik
0.12.0,License: MIT
0.12.0,np.newaxis for backwards compatability with random_state
0.12.0,Samples are in danger for m/2 <= m' < m
0.12.0,Samples are noise for m = m'
0.12.0,FIXME: to be removed in 0.12
0.12.0,FIXME: to be removed in 0.12
0.12.0,the input of the OneHotEncoder needs to be dense
0.12.0,SMOTE resampling starts here
0.12.0,"In the edge case where the median of the std is equal to 0, the 1s"
0.12.0,"entries will be also nullified. In this case, we store the original"
0.12.0,categorical encoding which will be later used for inverting the OHE
0.12.0,This variable will be used when generating data
0.12.0,we can replace the 1 entries of the categorical features with the
0.12.0,median of the standard deviation. It will ensure that whenever
0.12.0,"distance is computed between 2 samples, the difference will be equal"
0.12.0,to the median of the standard deviation as in the original paper.
0.12.0,"With one-hot encoding, the median will be repeated twice. We need"
0.12.0,to divide by sqrt(2) such that we only have one median value
0.12.0,contributing to the Euclidean distance
0.12.0,SMOTE resampling ends here
0.12.0,reverse the encoding of the categorical features
0.12.0,the matrix is supposed to be in the CSR format after the stacking
0.12.0,change in sparsity structure more efficient with LIL than CSR
0.12.0,convert to dense array since scipy.sparse doesn't handle 3D
0.12.0,"In the case that the median std was equal to zeros, we have to"
0.12.0,create non-null entry based on the encoded of OHE
0.12.0,tie breaking argmax
0.12.0,generate sample indices that will be used to generate new samples
0.12.0,"for each drawn samples, select its k-neighbors and generate a sample"
0.12.0,"where for each feature individually, each category generated is the"
0.12.0,most common category
0.12.0,FIXME: to be removed in 0.12
0.12.0,the kneigbors search will include the sample itself which is
0.12.0,expected from the original algorithm
0.12.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.0,Christos Aridas
0.12.0,Dzianis Dudnik
0.12.0,License: MIT
0.12.0,create 2 random continuous feature
0.12.0,create a categorical feature using some string
0.12.0,create a categorical feature using some integer
0.12.0,return the categories
0.12.0,create 2 random continuous feature
0.12.0,create a categorical feature using some string
0.12.0,create a categorical feature using some integer
0.12.0,return the categories
0.12.0,create 2 random continuous feature
0.12.0,create a categorical feature using some string
0.12.0,create a categorical feature using some integer
0.12.0,return the categories
0.12.0,create 2 random continuous feature
0.12.0,create a categorical feature using some string
0.12.0,create a categorical feature using some integer
0.12.0,return the categories
0.12.0,create 2 random continuous feature
0.12.0,create a categorical feature using some string
0.12.0,create a categorical feature using some integer
0.12.0,part of the common test which apply to SMOTE-NC even if it is not default
0.12.0,constructible
0.12.0,Check that the samplers handle pandas dataframe and pandas series
0.12.0,Cast X and y to not default dtype
0.12.0,Non-regression test for #662
0.12.0,https://github.com/scikit-learn-contrib/imbalanced-learn/issues/662
0.12.0,check that the categorical feature is not random but correspond to the
0.12.0,categories seen in the minority class samples
0.12.0,TODO: only use `sparse_output` when sklearn >= 1.2
0.12.0,TODO(0.13): remove this test
0.12.0,overall check for SMOTEN
0.12.0,check if the SMOTEN resample data as expected
0.12.0,"we generate data such that ""not apple"" will be the minority class and"
0.12.0,"samples from this class will be generated. We will force the ""blue"""
0.12.0,"category to be associated with this class. Therefore, the new generated"
0.12.0,"samples should as well be from the ""blue"" category."
0.12.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.0,Christos Aridas
0.12.0,License: MIT
0.12.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.0,Christos Aridas
0.12.0,License: MIT
0.12.0,FIXME: we should use to_numpy with pandas >= 0.25
0.12.0,check the random over-sampling with a multiclass problem
0.12.0,check that resampling with heterogeneous dtype is working with basic
0.12.0,resampling
0.12.0,check that we can oversample even with missing or infinite data
0.12.0,regression tests for #605
0.12.0,check that we raise an error when heterogeneous dtype data are given
0.12.0,and a smoothed bootstrap is requested
0.12.0,check that smoothed bootstrap is working for numerical array
0.12.0,check that a shrinkage factor of 0 is equivalent to not create a smoothed
0.12.0,bootstrap
0.12.0,check the behaviour of the shrinkage parameter
0.12.0,the covariance of the data generated with the larger shrinkage factor
0.12.0,should also be larger.
0.12.0,check the validation of the shrinkage parameter
0.12.0,check that m_neighbors is properly set. Regression test for:
0.12.0,https://github.com/scikit-learn-contrib/imbalanced-learn/issues/568
0.12.0,FIXME: to be removed in 0.12
0.12.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.0,Christos Aridas
0.12.0,License: MIT
0.12.0,shuffle the indices since the sampler are packing them by class
0.12.0,helper functions
0.12.0,input and output
0.12.0,build the model and weights
0.12.0,"build the loss, predict, and train operator"
0.12.0,Initialization of all variables in the graph
0.12.0,"For each epoch, run accuracy on train and test"
0.12.0,helper functions
0.12.0,input and output
0.12.0,build the model and weights
0.12.0,"build the loss, predict, and train operator"
0.12.0,Initialization of all variables in the graph
0.12.0,"For each epoch, run accuracy on train and test"
0.12.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.0,License: MIT
0.12.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.0,Fernando Nogueira
0.12.0,Christos Aridas
0.12.0,License: MIT
0.12.0,find which class to not consider
0.12.0,there is a Tomek link between two samples if they are both nearest
0.12.0,neighbors of each others.
0.12.0,Find the nearest neighbour of every point
0.12.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.0,Christos Aridas
0.12.0,License: MIT
0.12.0,Randomly get one sample from the majority class
0.12.0,Generate the index to select
0.12.0,Create the set C - One majority samples and all minority
0.12.0,Create the set S - all majority samples
0.12.0,fit knn on C
0.12.0,Check each sample in S if we keep it or drop it
0.12.0,Do not select sample which are already well classified
0.12.0,Classify on S
0.12.0,If the prediction do not agree with the true label
0.12.0,append it in C_x
0.12.0,Keep the index for later
0.12.0,Update C
0.12.0,fit a knn on C
0.12.0,This experimental to speed up the search
0.12.0,Classify all the element in S and avoid to test the
0.12.0,well classified elements
0.12.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.0,Dayvid Oliveira
0.12.0,Christos Aridas
0.12.0,License: MIT
0.12.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.0,Christos Aridas
0.12.0,License: MIT
0.12.0,Compute the distance considering the farthest neighbour
0.12.0,Sort the list of distance and get the index
0.12.0,Throw a warning to tell the user that we did not have enough samples
0.12.0,to select and that we just select everything
0.12.0,Select the desired number of samples
0.12.0,idx_tmp is relative to the feature selected in the
0.12.0,previous step and we need to find the indirection
0.12.0,fmt: off
0.12.0,fmt: on
0.12.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.0,Christos Aridas
0.12.0,License: MIT
0.12.0,select a sample from the current class
0.12.0,create the set composed of all minority samples and one
0.12.0,sample from the current class.
0.12.0,create the set S with removing the seed from S
0.12.0,since that it will be added anyway
0.12.0,apply Tomek cleaning
0.12.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.0,Dayvid Oliveira
0.12.0,Christos Aridas
0.12.0,License: MIT
0.12.0,Check the stopping criterion
0.12.0,1. If there is no changes for the vector y
0.12.0,2. If the number of samples in the other class become inferior to
0.12.0,the number of samples in the majority class
0.12.0,3. If one of the class is disappearing
0.12.0,Case 1
0.12.0,Case 2
0.12.0,Case 3
0.12.0,Check the stopping criterion
0.12.0,1. If the number of samples in the other class become inferior to
0.12.0,the number of samples in the majority class
0.12.0,2. If one of the class is disappearing
0.12.0,Case 1else:
0.12.0,overwrite b_min_bec_maj
0.12.0,Case 2
0.12.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.0,Christos Aridas
0.12.0,License: MIT
0.12.0,backward compatibility when passing a NearestNeighbors object
0.12.0,clean the neighborhood
0.12.0,compute which classes to consider for cleaning for the A2 group
0.12.0,add an additional sample since the query points contains the original dataset
0.12.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.0,Christos Aridas
0.12.0,License: MIT
0.12.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.0,Christos Aridas
0.12.0,License: MIT
0.12.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.0,Christos Aridas
0.12.0,License: MIT
0.12.0,"with a large `threshold_cleaning`, the algorithm is equivalent to ENN"
0.12.0,set a threshold that we should consider only the class #2
0.12.0,making the threshold slightly smaller to take into account class #1
0.12.0,we should have a more aggressive cleaning with n_neighbors is larger
0.12.0,TODO: remove in 0.14
0.12.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.0,Christos Aridas
0.12.0,License: MIT
0.12.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.0,Christos Aridas
0.12.0,License: MIT
0.12.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.0,Christos Aridas
0.12.0,License: MIT
0.12.0,FIXME: we should use to_numpy with pandas >= 0.25
0.12.0,check that we can undersample even with missing or infinite data
0.12.0,regression tests for #605
0.12.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.0,Christos Aridas
0.12.0,License: MIT
0.12.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.0,Christos Aridas
0.12.0,License: MIT
0.12.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.0,Christos Aridas
0.12.0,License: MIT
0.12.0,TODO: remove in 0.14
0.12.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.0,Christos Aridas
0.12.0,License: MIT
0.12.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.0,Christos Aridas
0.12.0,License: MIT
0.12.0,TODO: remove in 0.14
0.12.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.0,Fernando Nogueira
0.12.0,Christos Aridas
0.12.0,License: MIT
0.12.0,check that the samples selecting by the hard voting corresponds to the
0.12.0,targeted class
0.12.0,non-regression test for:
0.12.0,https://github.com/scikit-learn-contrib/imbalanced-learn/issues/738
0.12.0,Generate valid values for the required parameters
0.12.0,The parameters `*args` and `**kwargs` are ignored since we cannot generate
0.12.0,constraints.
0.12.0,check that there is a constraint for each parameter
0.12.0,this object does not have a valid type for sure for all params
0.12.0,This parameter is not validated
0.12.0,"First, check that the error is raised if param doesn't match any valid type."
0.12.0,"Then, for constraints that are more than a type constraint, check that the"
0.12.0,error is raised if param does match a valid type but does not match any valid
0.12.0,value for this type.
0.12.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.0,Christos Aridas
0.12.0,License: MIT
0.12.0,test that all_estimators doesn't find abstract classes.
0.12.0,"For NearMiss, let's check the three algorithms"
0.12.0,Common tests for estimator instances
0.12.0,Authors: Alexandre Gramfort <alexandre.gramfort@inria.fr>
0.12.0,Raghav RV <rvraghav93@gmail.com>
0.12.0,License: BSD 3 clause
0.12.0,scikit-learn >= 1.2
0.12.0,"walk_packages() ignores DeprecationWarnings, now we need to ignore"
0.12.0,FutureWarnings
0.12.0,"mypy error: Module has no attribute ""__path__"""
0.12.0,functions to ignore args / docstring of
0.12.0,Methods where y param should be ignored if y=None by default
0.12.0,numpydoc 0.8.0's docscrape tool raises because of collections.abc under
0.12.0,Python 3.7
0.12.0,Test module docstring formatting
0.12.0,Skip test if numpydoc is not found
0.12.0,XXX unreached code as of v0.22
0.12.0,"pytest tooling, not part of the scikit-learn API"
0.12.0,Exclude non-scikit-learn classes
0.12.0,Now skip docstring test for y when y is None
0.12.0,by default for API reason
0.12.0,Exclude imported functions
0.12.0,Don't test private methods / functions
0.12.0,Test that there are no tabs in our source files
0.12.0,because we don't import
0.12.0,Minimal / degenerate instances: only useful to test the docstrings.
0.12.0,"As certain attributes are present ""only"" if a certain parameter is"
0.12.0,"provided, this checks if the word ""only"" is present in the attribute"
0.12.0,"description, and if not the attribute is required to be present."
0.12.0,ignore deprecation warnings
0.12.0,attributes
0.12.0,properties
0.12.0,ignore properties that raises an AttributeError and deprecated
0.12.0,properties
0.12.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.0,License: MIT
0.12.0,check that we can let a pass a regression variable by turning down the
0.12.0,validation
0.12.0,Check that the validation is bypass when calling `fit`
0.12.0,Non-regression test for:
0.12.0,https://github.com/scikit-learn-contrib/imbalanced-learn/issues/782
0.12.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.0,Christos Aridas
0.12.0,License: MIT
0.12.0,store timestamp to figure out whether the result of 'fit' has been
0.12.0,cached or not
0.12.0,store timestamp to figure out whether the result of 'fit' has been
0.12.0,cached or not
0.12.0,Pipeline accepts steps as tuple
0.12.0,Test the various init parameters of the pipeline.
0.12.0,Check that we can't instantiate pipelines with objects without fit
0.12.0,method
0.12.0,Smoke test with only an estimator
0.12.0,Check that params are set
0.12.0,Smoke test the repr:
0.12.0,Test with two objects
0.12.0,Check that we can't instantiate with non-transformers on the way
0.12.0,"Note that NoTrans implements fit, but not transform"
0.12.0,Check that params are set
0.12.0,Smoke test the repr:
0.12.0,Check that params are not set when naming them wrong
0.12.0,Test clone
0.12.0,"Check that apart from estimators, the parameters are the same"
0.12.0,Remove estimators that where copied
0.12.0,Test the various methods of the pipeline (anova).
0.12.0,Test with Anova + LogisticRegression
0.12.0,Test that the pipeline can take fit parameters
0.12.0,classifier should return True
0.12.0,and transformer params should not be changed
0.12.0,invalid parameters should raise an error message
0.12.0,Pipeline should pass sample_weight
0.12.0,When sample_weight is None it shouldn't be passed
0.12.0,Test pipeline raises set params error message for nested models.
0.12.0,nested model check
0.12.0,Test the various methods of the pipeline (pca + svm).
0.12.0,Test with PCA + SVC
0.12.0,Test the various methods of the pipeline (preprocessing + svm).
0.12.0,check shapes of various prediction functions
0.12.0,test that the fit_predict method is implemented on a pipeline
0.12.0,test that the fit_predict on pipeline yields same results as applying
0.12.0,transform and clustering steps separately
0.12.0,"As pipeline doesn't clone estimators on construction,"
0.12.0,it must have its own estimators
0.12.0,first compute the transform and clustering step separately
0.12.0,use a pipeline to do the transform and clustering in one step
0.12.0,tests that a pipeline does not have fit_predict method when final
0.12.0,step of pipeline does not have fit_predict defined
0.12.0,tests that Pipeline passes fit_params to intermediate steps
0.12.0,when fit_predict is invoked
0.12.0,Test whether pipeline works with a transformer at the end.
0.12.0,Also test pipeline.transform and pipeline.inverse_transform
0.12.0,test transform and fit_transform:
0.12.0,Test whether pipeline works with a transformer missing fit_transform
0.12.0,test fit_transform:
0.12.0,Directly setting attr
0.12.0,Using set_params
0.12.0,Using set_params to replace single step
0.12.0,With invalid data
0.12.0,Test setting Pipeline steps to None
0.12.0,"for other methods, ensure no AttributeErrors on None:"
0.12.0,mult2 and mult3 are active
0.12.0,Check 'passthrough' step at construction time
0.12.0,Test with Transformer + SVC
0.12.0,Memoize the transformer at the first fit
0.12.0,Get the time stamp of the tranformer in the cached pipeline
0.12.0,Check that cached_pipe and pipe yield identical results
0.12.0,Check that we are reading the cache while fitting
0.12.0,a second time
0.12.0,Check that cached_pipe and pipe yield identical results
0.12.0,Create a new pipeline with cloned estimators
0.12.0,Check that even changing the name step does not affect the cache hit
0.12.0,Check that cached_pipe and pipe yield identical results
0.12.0,Test with Transformer + SVC
0.12.0,Memoize the transformer at the first fit
0.12.0,Get the time stamp of the tranformer in the cached pipeline
0.12.0,Check that cached_pipe and pipe yield identical results
0.12.0,Check that we are reading the cache while fitting
0.12.0,a second time
0.12.0,Check that cached_pipe and pipe yield identical results
0.12.0,Create a new pipeline with cloned estimators
0.12.0,Check that even changing the name step does not affect the cache hit
0.12.0,Check that cached_pipe and pipe yield identical results
0.12.0,Test the various methods of the pipeline (pca + svm).
0.12.0,Test with PCA + SVC
0.12.0,Test the various methods of the pipeline (pca + svm).
0.12.0,Test with PCA + SVC
0.12.0,Test whether pipeline works with a sampler at the end.
0.12.0,Also test pipeline.sampler
0.12.0,test transform and fit_transform:
0.12.0,We round the value near to zero. It seems that PCA has some issue
0.12.0,with that
0.12.0,Test whether pipeline works with a sampler at the end.
0.12.0,Also test pipeline.sampler
0.12.0,Test pipeline using None as preprocessing step and a classifier
0.12.0,"Test pipeline using None, RUS and a classifier"
0.12.0,"Test pipeline using RUS, None and a classifier"
0.12.0,Test pipeline using None step and a sampler
0.12.0,Test pipeline using None and a transformer that implements transform and
0.12.0,inverse_transform
0.12.0,Test the various methods of the pipeline (anova).
0.12.0,Test with RandomUnderSampling + Anova + LogisticRegression
0.12.0,Test the various methods of the pipeline (anova).
0.12.0,Test the various methods of the pipeline (anova).
0.12.0,Test with RandomUnderSampling + Anova + LogisticRegression
0.12.0,tests that Pipeline passes predict_params to the final estimator
0.12.0,when predict is invoked
0.12.0,Test that the score_samples method is implemented on a pipeline.
0.12.0,Test that the score_samples method on pipeline yields same results as
0.12.0,applying transform and score_samples steps separately.
0.12.0,Check the shapes
0.12.0,Check the values
0.12.0,Test that a pipeline does not have score_samples method when the final
0.12.0,step of the pipeline does not have score_samples defined.
0.12.0,Test that the score_samples method is implemented on a pipeline.
0.12.0,Test that the score_samples method on pipeline yields same results as
0.12.0,applying transform and score_samples steps separately.
0.12.0,Check the shapes
0.12.0,Check the values
0.12.0,transformer will not change `y` and sampler will always preserve the type of `y`
0.12.0,transformer will not change `y` and sampler will always preserve the type of `y`
0.12.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.0,Christos Aridas
0.12.0,License: MIT
0.12.0,TODO: Remove when SciPy 1.9 is the minimum supported version
0.12.0,TODO: Remove when scikit-learn 1.1 is the minimum supported version
0.12.0,TODO: remove when scikit-learn minimum version is 1.3
0.12.0,we don't want to validate again for each call to partial_fit
0.12.0,TODO: remove when scikit-learn minimum version is 1.3
0.12.0,"Likely a pandas DataFrame, we explicitly check the type to confirm."
0.12.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.0,License: MIT
0.12.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.0,License: MIT
0.12.0,Adapated from scikit-learn
0.12.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.0,License: MIT
0.12.0,scikit-learn >= 1.2
0.12.0,TODO: remove in 0.13
0.12.0,future default in 0.13
0.12.0,we don't filter samplers based on their tag here because we want to make
0.12.0,sure that the fitted attribute does not exist if the tag is not
0.12.0,stipulated
0.12.0,trigger our checks if this is a SamplerMixin
0.12.0,should raise warning if the target is continuous (we cannot raise error)
0.12.0,if the target is multilabel then we should raise an error
0.12.0,IHT does not enforce the number of samples but provide a number
0.12.0,of samples the closest to the desired target.
0.12.0,in this test we will force all samplers to not change the class 1
0.12.0,check that sparse matrices can be passed through the sampler leading to
0.12.0,the same results than dense
0.12.0,Check that the samplers handle pandas dataframe and pandas series
0.12.0,check that we return the same type for dataframes or series types
0.12.0,FIXME: we should use to_numpy with pandas >= 0.25
0.12.0,Check that the samplers handle pandas dataframe and pandas series
0.12.0,check that we return the same type for dataframes or series types
0.12.0,FIXME: we should use to_numpy with pandas >= 0.25
0.12.0,Check that the can samplers handle simple lists
0.12.0,Check that multiclass target lead to the same results than OVA encoding
0.12.0,Cast X and y to not default dtype
0.12.0,Non-regression test for #709
0.12.0,https://github.com/scikit-learn-contrib/imbalanced-learn/issues/709
0.12.0,Check that an informative error is raised when the value of a constructor
0.12.0,parameter does not have an appropriate type or value.
0.12.0,check that there is a constraint for each parameter
0.12.0,this object does not have a valid type for sure for all params
0.12.0,This parameter is not validated
0.12.0,"First, check that the error is raised if param doesn't match any valid type."
0.12.0,the method is not accessible with the current set of parameters
0.12.0,The estimator is a label transformer and take only `y`
0.12.0,"Then, for constraints that are more than a type constraint, check that the"
0.12.0,error is raised if param does match a valid type but does not match any valid
0.12.0,value for this type.
0.12.0,the method is not accessible with the current set of parameters
0.12.0,The estimator is a label transformer and take only `y`
0.12.0,Check that calling `fit` does not raise any warnings about feature names.
0.12.0,Only check imblearn estimators for feature_names_in_ in docstring
0.12.0,partial_fit checks on second call
0.12.0,Do not call partial fit if early_stopping is on
0.12.0,input_features names is not the same length as n_features_in_
0.12.0,error is raised when `input_features` do not match feature_names_in
0.12.0,Adapted from scikit-learn
0.12.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.0,License: MIT
0.12.0,Ignore deprecation warnings triggered at import time and from walking
0.12.0,packages
0.12.0,get rid of abstract base classes
0.12.0,get rid of sklearn estimators which have been imported in some classes
0.12.0,"drop duplicates, sort for reproducibility"
0.12.0,itemgetter is used to ensure the sort does not extend to the 2nd item of
0.12.0,the tuple
0.12.0,Author: Adrin Jalali <adrin.jalali@gmail.com>
0.12.0,License: BSD 3 clause
0.12.0,Only the following methods are supported in the routing mechanism. Adding new
0.12.0,methods at the moment involves monkeypatching this list.
0.12.0,"Note that if this list is changed or monkeypatched, the corresponding method"
0.12.0,needs to be added under a TYPE_CHECKING condition like the one done here in
0.12.0,_MetadataRequester
0.12.0,These methods are a composite of other methods and one cannot set their
0.12.0,requests directly. Instead they should be set by setting the requests of the
0.12.0,simple methods which make the composite ones.
0.12.0,Request values
0.12.0,==============
0.12.0,"Each request value needs to be one of the following values, or an alias."
0.12.0,this is used in `__metadata_request__*` attributes to indicate that a
0.12.0,metadata is not present even though it may be present in the
0.12.0,corresponding method's signature.
0.12.0,"this is used whenever a default value is changed, and therefore the user"
0.12.0,"should explicitly set the value, otherwise a warning is shown. An example"
0.12.0,"is when a meta-estimator is only a router, but then becomes also a"
0.12.0,consumer in a new release.
0.12.0,this is the default used in `set_{method}_request` methods to indicate no
0.12.0,change requested by the user.
0.12.0,item is only an alias if it's a valid identifier
0.12.0,Metadata Request for Simple Consumers
0.12.0,=====================================
0.12.0,This section includes MethodMetadataRequest and MetadataRequest which are
0.12.0,used in simple consumers.
0.12.0,this is here for us to use this attribute's value instead of doing
0.12.0,"`isinstance` in our checks, so that we avoid issues when people vendor"
0.12.0,this file instead of using it directly from scikit-learn.
0.12.0,Called when the default attribute access fails with an AttributeError
0.12.0,(either __getattribute__() raises an AttributeError because name is
0.12.0,not an instance attribute or an attribute in the class tree for self;
0.12.0,or __get__() of a name property raises AttributeError). This method
0.12.0,should either return the (computed) attribute value or raise an
0.12.0,AttributeError exception.
0.12.0,https://docs.python.org/3/reference/datamodel.html#object.__getattr__
0.12.0,Metadata Request for Routers
0.12.0,============================
0.12.0,This section includes all objects required for MetadataRouter which is used
0.12.0,"in routers, returned by their ``get_metadata_routing``."
0.12.0,"This namedtuple is used to store a (mapping, routing) pair. Mapping is a"
0.12.0,"MethodMapping object, and routing is the output of `get_metadata_routing`."
0.12.0,MetadataRouter stores a collection of these namedtuples.
0.12.0,A namedtuple storing a single method route. A collection of these namedtuples
0.12.0,is stored in a MetadataRouter.
0.12.0,this is here for us to use this attribute's value instead of doing
0.12.0,"`isinstance`` in our checks, so that we avoid issues when people vendor"
0.12.0,this file instead of using it directly from scikit-learn.
0.12.0,`_self_request` is used if the router is also a consumer.
0.12.0,"_self_request, (added using `add_self_request()`) is treated"
0.12.0,differently from the other objects which are stored in
0.12.0,_route_mappings.
0.12.0,"conflicts are okay if the passed objects are the same, but it's"
0.12.0,an issue if they're different objects.
0.12.0,doing this instead of a try/except since an AttributeError could be raised
0.12.0,for other reasons.
0.12.0,Request method
0.12.0,==============
0.12.0,This section includes what's needed for the request method descriptor and
0.12.0,their dynamic generation in a meta class.
0.12.0,These strings are used to dynamically generate the docstrings for
0.12.0,set_{method}_request methods.
0.12.0,we would want to have a method which accepts only the expected args
0.12.0,Now we set the relevant attributes of the function so that it seems
0.12.0,"like a normal method to the end user, with known expected arguments."
0.12.0,"This code is never run in runtime, but it's here for type checking."
0.12.0,Type checkers fail to understand that the `set_{method}_request`
0.12.0,"methods are dynamically generated, and they complain that they are"
0.12.0,not defined. We define them here to make type checkers happy.
0.12.0,During type checking analyzers assume this to be True.
0.12.0,The following list of defined methods mirrors the list of methods
0.12.0,in SIMPLE_METHODS.
0.12.0,fmt: off
0.12.0,fmt: on
0.12.0,"if there are any issues in the default values, it will be raised"
0.12.0,when ``get_metadata_routing`` is called. Here we are going to
0.12.0,ignore all the issues such as bad defaults etc.
0.12.0,set ``set_{method}_request``` methods
0.12.0,Here we use `isfunction` instead of `ismethod` because calling `getattr`
0.12.0,on a class instead of an instance returns an unbound function.
0.12.0,"ignore the first parameter of the method, which is usually ""self"""
0.12.0,Then overwrite those defaults with the ones provided in
0.12.0,__metadata_request__* attributes. Defaults set in
0.12.0,__metadata_request__* attributes take precedence over signature
0.12.0,sniffing.
0.12.0,need to go through the MRO since this is a class attribute and
0.12.0,``vars`` doesn't report the parent class attributes. We go through
0.12.0,the reverse of the MRO so that child classes have precedence over
0.12.0,their parents.
0.12.0,we don't check for attr.startswith() since python prefixes attrs
0.12.0,starting with __ with the `_ClassName`.
0.12.0,Process Routing in Routers
0.12.0,==========================
0.12.0,This is almost always the only method used in routers to process and route
0.12.0,given metadata. This is to minimize the boilerplate required in routers.
0.12.0,Here the first two arguments are positional only which makes everything
0.12.0,passed as keyword argument a metadata. The first two args also have an `_`
0.12.0,"prefix to reduce the chances of name collisions with the passed metadata, and"
0.12.0,"since they're positional only, users will never type those underscores."
0.12.0,"If routing is not enabled and kwargs are empty, then we don't have to"
0.12.0,"try doing any routing, we can simply return a structure which returns"
0.12.0,an empty dict on routed_params.ANYTHING.ANY_METHOD.
0.12.0,mypy: ignore-errors
0.12.0,update the docstring of the descriptor
0.12.0,"delegate only on instances, not the classes."
0.12.0,this is to allow access to the docstrings.
0.12.0,This makes it possible to use the decorated method as an
0.12.0,"unbound method, for instance when monkeypatching."
0.12.0,mypy: ignore-errors
0.12.0,Inherits from ValueError and TypeError to keep backward compatibility.
0.12.0,We allow parameters to not have a constraint so that third party
0.12.0,estimators can inherit from sklearn estimators without having to
0.12.0,necessarily use the validation tools.
0.12.0,"this constraint is satisfied, no need to check further."
0.12.0,"No constraint is satisfied, raise with an informative message."
0.12.0,Ignore constraints that we don't want to expose in the error
0.12.0,"message, i.e. options that are for internal purpose or not"
0.12.0,officially supported.
0.12.0,The dict of parameter constraints is set as an attribute of the function
0.12.0,to make it possible to dynamically introspect the constraints for
0.12.0,automatic testing.
0.12.0,Map *args/**kwargs to the function signature
0.12.0,ignore self/cls and positional/keyword markers
0.12.0,"When the function is just a wrapper around an estimator, we allow"
0.12.0,"the function to delegate validation to the estimator, but we"
0.12.0,replace the name of the estimator by the name of the function in
0.12.0,the error message to avoid confusion.
0.12.0,better repr if the bounds were given as integers
0.12.0,we use an interval of Real to ignore np.nan that has its own
0.12.0,constraint
0.12.0,"There's no integer outside (-inf, +inf)"
0.12.0,"bounds are -inf, +inf"
0.12.0,"interval is [-inf, +inf]"
0.12.0,special case for ndarray since it can't be instantiated without
0.12.0,arguments
0.12.0,special case for Integral and Real since they are abstract classes
0.12.0,Author: Alexander L. Hayes <hayesall@iu.edu>
0.12.0,License: MIT
0.12.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.0,License: MIT
0.12.0,We lost the y.index during resampling. We can safely use X.index to align
0.12.0,them.
0.12.0,We special case the following error:
0.12.0,https://github.com/scikit-learn-contrib/imbalanced-learn/issues/1055
0.12.0,"There is no easy way to have a generic workaround. Here, we detect"
0.12.0,that we have a column with only null values that is datetime64
0.12.0,(resulting from the np.vstack of the resampling).
0.12.0,try again
0.12.0,_is_neighbors_object(nn_object)
0.12.0,check that all keys in sampling_strategy are also in y
0.12.0,check that there is no negative number
0.12.0,check that all keys in sampling_strategy are also in y
0.12.0,ignore first 'self' argument for instance methods
0.12.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.0,Christos Aridas
0.12.0,License: MIT
0.12.0,this function could create an equal number of samples
0.12.0,We pass on purpose a non sorted dictionary and check that the resulting
0.12.0,dictionary is sorted. Refer to issue #428.
0.12.0,DataFrame and DataFrame case
0.12.0,DataFrames and Series case
0.12.0,The * is place before a keyword only argument without a default value
0.12.0,Test that the minimum dependencies in the README.rst file are
0.12.0,consistent with the minimum dependencies defined at the file:
0.12.0,imblearn/_min_dependencies.py
0.12.0,Skip the test if the README.rst file is not available.
0.12.0,"For instance, when installing scikit-learn from wheels"
0.12.0,Author: Alexander L. Hayes <hayesall@iu.edu>
0.12.0,License: MIT
0.12.0,Some helpers for the tests
0.12.0,check in the presence of extra positional and keyword args
0.12.0,outer decorator does not interfere with validation
0.12.0,validated method can be decorated
0.12.0,no validation in init
0.12.0,list and dict are valid params
0.12.0,the list option is not exposed in the error message
0.12.0,"""auto"" and ""warn"" are valid params"
0.12.0,"the ""warn"" option is not exposed in the error message"
0.12.0,True/False and np.bool_(True/False) are valid params
0.12.0,param1 is validated
0.12.0,param2 is not validated: any type is valid.
0.12.0,"does not raise, even though ""b"" is not in the constraints dict and ""a"" is not"
0.12.0,a parameter of the estimator.
0.12.0,does not raise
0.12.0,calls f with a bad parameter type
0.12.0,Validation for g is never skipped.
0.12.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.0,License: MIT
0.12.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.0,Christos Aridas
0.12.0,License: MIT
0.12.0,check if the filtering is working with a list or a single string
0.12.0,check that all estimators are sampler
0.12.0,check that an error is raised when the type is unknown
0.12.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.0,License: MIT
0.12.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.0,Christos Aridas
0.12.0,License: MIT
0.12.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.0,Christos Aridas
0.12.0,License: MIT
0.12.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.0,Christos Aridas
0.12.0,License: MIT
0.12.0,Check if default job count is None
0.12.0,Check if job count is set
0.12.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.0,Christos Aridas
0.12.0,License: MIT
0.12.0,Check if default job count is none
0.12.0,Check if job count is set
0.12.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.0,License: MIT
0.12.0,scikit-learn >= 1.2
0.12.0,resample before to fit the tree
0.12.0,TODO: remove when the minimum supported version of scikit-learn will be 1.4
0.12.0,support for missing values
0.12.0,TODO: remove when the minimum supported version of scikit-learn will be 1.1
0.12.0,change of signature in scikit-learn 1.1
0.12.0,make a deepcopy to not modify the original dictionary
0.12.0,TODO: remove when the minimum supported version of scikit-learn will be 1.4
0.12.0,use scikit-learn support for monotonic constraints
0.12.0,create an attribute for compatibility with other scikit-learn tools such
0.12.0,as HTML representation.
0.12.0,TODO: remove in 0.13
0.12.0,Validate or convert input data
0.12.0,TODO: remove when the minimum supported version of scipy will be 1.4
0.12.0,Support for missing values
0.12.0,TODO: remove when the minimum supported version of scikit-learn will be 1.4
0.12.0,_compute_missing_values_in_feature_mask checks if X has missing values and
0.12.0,will raise an error if the underlying tree base estimator can't handle
0.12.0,missing values. Only the criterion is required to determine if the tree
0.12.0,supports missing values.
0.12.0,Pre-sort indices to avoid that each individual tree of the
0.12.0,ensemble sorts the indices.
0.12.0,reshape is necessary to preserve the data contiguity against vs
0.12.0,"[:, np.newaxis] that does not."
0.12.0,Get bootstrap sample size
0.12.0,Check parameters
0.12.0,"Free allocated memory, if any"
0.12.0,We draw from the random state to get the random state we
0.12.0,would have got if we hadn't used a warm_start.
0.12.0,Parallel loop: we prefer the threading backend as the Cython code
0.12.0,for fitting the trees is internally releasing the Python GIL
0.12.0,making threading more efficient than multiprocessing in
0.12.0,"that case. However, we respect any parallel_backend contexts set"
0.12.0,"at a higher level, since correctness does not rely on using"
0.12.0,threads.
0.12.0,Collect newly grown trees
0.12.0,Create pipeline with the fitted samplers and trees
0.12.0,FIXME: we could consider to support multiclass-multioutput if
0.12.0,we introduce or reuse a constructor parameter (e.g.
0.12.0,oob_score) allowing our user to pass a callable defining the
0.12.0,scoring strategy on OOB sample.
0.12.0,Decapsulate classes_ attributes
0.12.0,drop the n_outputs axis if there is a single output
0.12.0,Prediction requires X to be in CSR format
0.12.0,n_classes_ is a ndarray at this stage
0.12.0,all the supported type of target will have the same number of
0.12.0,classes in all outputs
0.12.0,"for regression, n_classes_ does not exist and we create an empty"
0.12.0,axis to be consistent with the classification case and make
0.12.0,the array operations compatible with the 2 settings
0.12.0,TODO: remove when supporting scikit-learn>=1.2
0.12.0,make a deepcopy to not modify the original dictionary
0.12.0,TODO: remove when minimum supported version of scikit-learn is 1.4
0.12.0,SAMME-R requires predict_proba-enabled estimators
0.12.0,Instances incorrectly classified
0.12.0,Error fraction
0.12.0,Stop if classification is perfect
0.12.0,Construct y coding as described in Zhu et al [2]:
0.12.0,
0.12.0,y_k = 1 if c == k else -1 / (K - 1)
0.12.0,
0.12.0,"where K == n_classes_ and c, k in [0, K) are indices along the second"
0.12.0,axis of the y coding with c being the index corresponding to the true
0.12.0,class label.
0.12.0,Displace zero probabilities so the log is defined.
0.12.0,Also fix negative elements which may occur with
0.12.0,negative sample weights.
0.12.0,Boost weight using multi-class AdaBoost SAMME.R alg
0.12.0,Only boost the weights if it will fit again
0.12.0,Only boost positive weights
0.12.0,Instances incorrectly classified
0.12.0,Error fraction
0.12.0,Stop if classification is perfect
0.12.0,Stop if the error is at least as bad as random guessing
0.12.0,Boost weight using multi-class AdaBoost SAMME alg
0.12.0,Only boost the weights if I will fit again
0.12.0,Only boost positive weights
0.12.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.0,Christos Aridas
0.12.0,License: MIT
0.12.0,scikit-learn >= 1.2
0.12.0,make a deepcopy to not modify the original dictionary
0.12.0,TODO: remove when minimum supported version of scikit-learn is 1.4
0.12.0,TODO: remove when supporting scikit-learn>=1.2
0.12.0,overwrite the base class method by disallowing `sample_weight`
0.12.0,RandomUnderSampler is not supporting sample_weight. We need to pass
0.12.0,None.
0.12.0,TODO: remove when minimum supported version of scikit-learn is 1.1
0.12.0,Check data
0.12.0,Parallel loop
0.12.0,Reduce
0.12.0,The base class require to have the attribute defined. For scikit-learn
0.12.0,"> 1.2, we are going to raise an error."
0.12.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.0,Christos Aridas
0.12.0,License: MIT
0.12.0,scikit-learn >= 1.2
0.12.0,make a deepcopy to not modify the original dictionary
0.12.0,TODO: remove when minimum supported version of scikit-learn is 1.4
0.12.0,TODO: remove when supporting scikit-learn>=1.2
0.12.0,overwrite the base class method by disallowing `sample_weight`
0.12.0,the sampler needs to be validated before to call _fit because
0.12.0,_validate_y is called before _validate_estimator and would require
0.12.0,to know which type of sampler we are using.
0.12.0,RandomUnderSampler is not supporting sample_weight. We need to pass
0.12.0,None.
0.12.0,TODO: remove when minimum supported version of scikit-learn is 1.1
0.12.0,Check data
0.12.0,Parallel loop
0.12.0,Reduce
0.12.0,The base class require to have the attribute defined. For scikit-learn
0.12.0,"> 1.2, we are going to raise an error."
0.12.0,check that we have an ensemble of samplers and estimators with a
0.12.0,consistent size
0.12.0,each sampler in the ensemble should have different random state
0.12.0,each estimator in the ensemble should have different random state
0.12.0,check the consistency of the feature importances
0.12.0,check the consistency of the prediction outpus
0.12.0,Predictions should be the same when sample_weight are all ones
0.12.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.0,Christos Aridas
0.12.0,License: MIT
0.12.0,Check classification for various parameter settings.
0.12.0,Test that bootstrapping samples generate non-perfect base estimators.
0.12.0,"without bootstrap, all trees are perfect on the training set"
0.12.0,disable the resampling by passing an empty dictionary.
0.12.0,"with bootstrap, trees are no longer perfect on the training set"
0.12.0,Test that bootstrapping features may generate duplicate features.
0.12.0,Predict probabilities.
0.12.0,Normal case
0.12.0,"Degenerate case, where some classes are missing"
0.12.0,Check that oob prediction is a good estimation of the generalization
0.12.0,error.
0.12.0,Test with few estimators
0.12.0,Check singleton ensembles.
0.12.0,Check that bagging ensembles can be grid-searched.
0.12.0,Transform iris into a binary classification task
0.12.0,Grid search with scoring based on decision_function
0.12.0,Check estimator and its default values.
0.12.0,Test if fitting incrementally with warm start gives a forest of the
0.12.0,right size and the same results as a normal fit.
0.12.0,Test if warm start'ed second fit with smaller n_estimators raises error.
0.12.0,Test that nothing happens when fitting without increasing n_estimators
0.12.0,"modify X to nonsense values, this should not change anything"
0.12.0,warm started classifier with 5+5 estimators should be equivalent to
0.12.0,one classifier with 10 estimators
0.12.0,Check using oob_score and warm_start simultaneously fails
0.12.0,"Make sure OOB scores are identical when random_state, estimator, and"
0.12.0,training data are fixed and fitting is done twice
0.12.0,Check that format of estimators_samples_ is correct and that results
0.12.0,generated at fit time can be identically reproduced at a later time
0.12.0,using data saved in object attributes.
0.12.0,remap the y outside of the BalancedBaggingclassifier
0.12.0,"_, y = np.unique(y, return_inverse=True)"
0.12.0,Get relevant attributes
0.12.0,Test for correct formatting
0.12.0,Re-fit single estimator to test for consistent sampling
0.12.0,Make sure validated max_samples and original max_samples are identical
0.12.0,when valid integer max_samples supplied by user
0.12.0,check that we can pass any kind of sampler to a bagging classifier
0.12.0,check that we have balanced class with the right counts of class
0.12.0,sample depending on the sampling strategy
0.12.0,check that we can provide a FunctionSampler in BalancedBaggingClassifier
0.12.0,find the minority and majority classes
0.12.0,compute the number of sample to draw from the majority class using
0.12.0,a negative binomial distribution
0.12.0,draw randomly with or without replacement
0.12.0,Roughly Balanced Bagging
0.12.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.0,Christos Aridas
0.12.0,License: MIT
0.12.0,Generate a global dataset to use
0.12.0,Check classification for various parameter settings.
0.12.0,test the different prediction function
0.12.0,Check estimator and its default values.
0.12.0,Test if fitting incrementally with warm start gives a forest of the
0.12.0,right size and the same results as a normal fit.
0.12.0,Test if warm start'ed second fit with smaller n_estimators raises error.
0.12.0,Test that nothing happens when fitting without increasing n_estimators
0.12.0,"modify X to nonsense values, this should not change anything"
0.12.0,warm started classifier with 5+5 estimators should be equivalent to
0.12.0,one classifier with 10 estimators
0.12.0,Check warning if not enough estimators
0.12.0,First fit with no restriction on max samples
0.12.0,Second fit with max samples restricted to just 2
0.12.0,Regression test for #655: check that the oob score is closed to 0.5
0.12.0,a binomial experiment.
0.12.0,TODO: remove in 0.13
0.12.0,Create dataset with missing values
0.12.0,Train forest with missing values
0.12.0,Train forest without missing values
0.12.0,Score is still 80 percent of the forest's score that had no missing values
0.12.0,Create a predictive feature using `y` and with some noise
0.12.0,Author: Guillaume Lemaitre
0.12.0,License: BSD 3 clause
0.12.0,"The index start at one, then we need to remove one"
0.12.0,to not have issue with the indexing.
0.12.0,go through the list and check if the data are available
0.12.0,Authors: Dayvid Oliveira
0.12.0,Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.0,Christos Aridas
0.12.0,License: MIT
0.12.0,restrict ratio to be a dict or a callable
0.12.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.0,Christos Aridas
0.12.0,License: MIT
0.12.0,"we are reusing part of utils.check_sampling_strategy, however this is not"
0.12.0,cover in the common tests so we will repeat it here
0.12.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.12.0,Christos Aridas
0.12.0,License: MIT
0.12.0,This is a trick to avoid an error during tests collection with pytest. We
0.12.0,avoid the error when importing the package raise the error at the moment of
0.12.0,creating the instance.
0.12.0,This is a trick to avoid an error during tests collection with pytest. We
0.12.0,avoid the error when importing the package raise the error at the moment of
0.12.0,creating the instance.
0.12.0,flag for keras sequence duck-typing
0.12.0,shuffle the indices since the sampler are packing them by class
0.11.0,This file is here so that when running from the root folder
0.11.0,./imblearn is added to sys.path by pytest.
0.11.0,See https://docs.pytest.org/en/latest/pythonpath.html for more details.
0.11.0,"For example, this allows to build extensions in place and run pytest"
0.11.0,doc/modules/clustering.rst and use imblearn from the local folder
0.11.0,rather than the one from site-packages.
0.11.0,! /usr/bin/env python
0.11.0,Python 2 compat: just to be able to declare that Python >=3.7 is needed.
0.11.0,This is a bit (!) hackish: we are setting a global variable so that the
0.11.0,main imblearn __init__ can detect if it is being loaded by the setup
0.11.0,"routine, to avoid attempting to load components that aren't built yet:"
0.11.0,the numpy distutils extensions that are used by imbalanced-learn to
0.11.0,recursively build the compiled extensions in sub-packages is based on the
0.11.0,Python import machinery.
0.11.0,get __version__ from _version.py
0.11.0,-*- coding: utf-8 -*-
0.11.0,
0.11.0,"imbalanced-learn documentation build configuration file, created by"
0.11.0,sphinx-quickstart on Mon Jan 18 14:44:12 2016.
0.11.0,
0.11.0,This file is execfile()d with the current directory set to its
0.11.0,containing dir.
0.11.0,
0.11.0,Note that not all possible configuration values are present in this
0.11.0,autogenerated file.
0.11.0,
0.11.0,All configuration values have a default; values that are commented out
0.11.0,serve to show the default.
0.11.0,"If extensions (or modules to document with autodoc) are in another directory,"
0.11.0,add these directories to sys.path here. If the directory is relative to the
0.11.0,"documentation root, use os.path.abspath to make it absolute, like shown here."
0.11.0,-- General configuration ------------------------------------------------
0.11.0,"If your documentation needs a minimal Sphinx version, state it here."
0.11.0,needs_sphinx = '1.0'
0.11.0,"Add any Sphinx extension module names here, as strings. They can be"
0.11.0,extensions coming with Sphinx (named 'sphinx.ext.*') or your custom
0.11.0,ones.
0.11.0,Specify how to identify the prompt when copying code snippets
0.11.0,"Add any paths that contain templates here, relative to this directory."
0.11.0,The suffix of source filenames.
0.11.0,The master toctree document.
0.11.0,General information about the project.
0.11.0,"The version info for the project you're documenting, acts as replacement for"
0.11.0,"|version| and |release|, also used in various other places throughout the"
0.11.0,built documents.
0.11.0,
0.11.0,The short X.Y version.
0.11.0,"The full version, including alpha/beta/rc tags."
0.11.0,"List of patterns, relative to source directory, that match files and"
0.11.0,directories to ignore when looking for source files.
0.11.0,The reST default role (used for this markup: `text`) to use for all
0.11.0,documents.
0.11.0,"If true, '()' will be appended to :func: etc. cross-reference text."
0.11.0,The name of the Pygments (syntax highlighting) style to use.
0.11.0,-- Options for HTML output ----------------------------------------------
0.11.0,The theme to use for HTML and HTML Help pages.  See the documentation for
0.11.0,a list of builtin themes.
0.11.0,"""twitter_url"": ""https://twitter.com/pandas_dev"","
0.11.0,"""navbar_align"": ""right"",  # For testing that the navbar items align properly"
0.11.0,"Add any paths that contain custom static files (such as style sheets) here,"
0.11.0,"relative to this directory. They are copied after the builtin static files,"
0.11.0,"so a file named ""default.css"" will overwrite the builtin ""default.css""."
0.11.0,Output file base name for HTML help builder.
0.11.0,-- Options for autodoc ------------------------------------------------------
0.11.0,generate autosummary even if no references
0.11.0,-- Options for numpydoc -----------------------------------------------------
0.11.0,this is needed for some reason...
0.11.0,see https://github.com/numpy/numpydoc/issues/69
0.11.0,-- Options for sphinxcontrib-bibtex -----------------------------------------
0.11.0,bibtex file
0.11.0,-- Options for intersphinx --------------------------------------------------
0.11.0,intersphinx configuration
0.11.0,-- Options for sphinx-gallery -----------------------------------------------
0.11.0,Generate the plot for the gallery
0.11.0,sphinx-gallery configuration
0.11.0,-- Options for github link for what's new -----------------------------------
0.11.0,Config for sphinx_issues
0.11.0,The following is used by sphinx.ext.linkcode to provide links to github
0.11.0,-- Options for LaTeX output ---------------------------------------------
0.11.0,The paper size ('letterpaper' or 'a4paper').
0.11.0,"'papersize': 'letterpaper',"
0.11.0,"The font size ('10pt', '11pt' or '12pt')."
0.11.0,"'pointsize': '10pt',"
0.11.0,Additional stuff for the LaTeX preamble.
0.11.0,"'preamble': '',"
0.11.0,Grouping the document tree into LaTeX files. List of tuples
0.11.0,"(source start file, target name, title,"
0.11.0,"author, documentclass [howto, manual, or own class])."
0.11.0,-- Options for manual page output ---------------------------------------
0.11.0,"If false, no module index is generated."
0.11.0,latex_domain_indices = True
0.11.0,One entry per manual page. List of tuples
0.11.0,"(source start file, name, description, authors, manual section)."
0.11.0,"If true, show URL addresses after external links."
0.11.0,man_show_urls = False
0.11.0,-- Options for Texinfo output -------------------------------------------
0.11.0,Grouping the document tree into Texinfo files. List of tuples
0.11.0,"(source start file, target name, title, author,"
0.11.0,"dir menu entry, description, category)"
0.11.0,-- Dependencies generation ----------------------------------------------
0.11.0,get length of header
0.11.0,-- Additional temporary hacks -----------------------------------------------
0.11.0,Temporary work-around for spacing problem between parameter and parameter
0.11.0,"type in the doc, see https://github.com/numpy/numpydoc/issues/215. The bug"
0.11.0,has been fixed in sphinx (https://github.com/sphinx-doc/sphinx/pull/5976) but
0.11.0,through a change in sphinx basic.css except rtd_theme does not use basic.css.
0.11.0,"In an ideal world, this would get fixed in this PR:"
0.11.0,https://github.com/readthedocs/sphinx_rtd_theme/pull/747/files
0.11.0,get the styles from the current theme
0.11.0,create and add the button to all the code blocks that contain >>>
0.11.0,tracebacks (.gt) contain bare text elements that need to be
0.11.0,wrapped in a span to work with .nextUntil() (see later)
0.11.0,define the behavior of the button when it's clicked
0.11.0,hide the code output
0.11.0,show the code output
0.11.0,-*- coding: utf-8 -*-
0.11.0,Format template for issues URI
0.11.0,e.g. 'https://github.com/sloria/marshmallow/issues/{issue}
0.11.0,Format template for PR URI
0.11.0,e.g. 'https://github.com/sloria/marshmallow/pull/{issue}
0.11.0,Format template for commit URI
0.11.0,e.g. 'https://github.com/sloria/marshmallow/commits/{commit}
0.11.0,"Shortcut for Github, e.g. 'sloria/marshmallow'"
0.11.0,Format template for user profile URI
0.11.0,e.g. 'https://github.com/{user}'
0.11.0,Python 2 only
0.11.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.11.0,License: MIT
0.11.0,%%
0.11.0,%%
0.11.0,"First, we will generate a toy classification dataset with only few samples."
0.11.0,The ratio between the classes will be imbalanced.
0.11.0,%%
0.11.0,%%
0.11.0,"Now, we will use a :class:`~imblearn.over_sampling.RandomOverSampler` to"
0.11.0,generate a bootstrap for the minority class with as many samples as in the
0.11.0,majority class.
0.11.0,%%
0.11.0,%%
0.11.0,We observe that the minority samples are less transparent than the samples
0.11.0,"from the majority class. Indeed, it is due to the fact that these samples"
0.11.0,of the minority class are repeated during the bootstrap generation.
0.11.0,
0.11.0,We can set `shrinkage` to a floating value to add a small perturbation to the
0.11.0,samples created and therefore create a smoothed bootstrap.
0.11.0,%%
0.11.0,%%
0.11.0,"In this case, we see that the samples in the minority class are not"
0.11.0,overlapping anymore due to the added noise.
0.11.0,
0.11.0,The parameter `shrinkage` allows to add more or less perturbation. Let's
0.11.0,add more perturbation when generating the smoothed bootstrap.
0.11.0,%%
0.11.0,%%
0.11.0,Increasing the value of `shrinkage` will disperse the new samples. Forcing
0.11.0,the shrinkage to 0 will be equivalent to generating a normal bootstrap.
0.11.0,%%
0.11.0,%%
0.11.0,"Therefore, the `shrinkage` is handy to manually tune the dispersion of the"
0.11.0,new samples.
0.11.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.11.0,License: MIT
0.11.0,%%
0.11.0,generate some data points
0.11.0,plot the majority and minority samples
0.11.0,draw the circle in which the new sample will generated
0.11.0,plot the line on which the sample will be generated
0.11.0,create and plot the new sample
0.11.0,make the plot nicer with legend and label
0.11.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.11.0,License: MIT
0.11.0,%%
0.11.0,%% [markdown]
0.11.0,The following function will be used to create toy dataset. It uses the
0.11.0,:func:`~sklearn.datasets.make_classification` from scikit-learn but fixing
0.11.0,some parameters.
0.11.0,%%
0.11.0,%% [markdown]
0.11.0,The following function will be used to plot the sample space after resampling
0.11.0,to illustrate the specificities of an algorithm.
0.11.0,%%
0.11.0,%% [markdown]
0.11.0,The following function will be used to plot the decision function of a
0.11.0,classifier given some data.
0.11.0,%%
0.11.0,%% [markdown]
0.11.0,Illustration of the influence of the balancing ratio
0.11.0,----------------------------------------------------
0.11.0,
0.11.0,We will first illustrate the influence of the balancing ratio on some toy
0.11.0,data using a logistic regression classifier which is a linear model.
0.11.0,%%
0.11.0,%% [markdown]
0.11.0,We will fit and show the decision boundary model to illustrate the impact of
0.11.0,dealing with imbalanced classes.
0.11.0,%%
0.11.0,%% [markdown]
0.11.0,"Greater is the difference between the number of samples in each class, poorer"
0.11.0,are the classification results.
0.11.0,
0.11.0,Random over-sampling to balance the data set
0.11.0,--------------------------------------------
0.11.0,
0.11.0,Random over-sampling can be used to repeat some samples and balance the
0.11.0,number of samples between the dataset. It can be seen that with this trivial
0.11.0,approach the boundary decision is already less biased toward the majority
0.11.0,class. The class :class:`~imblearn.over_sampling.RandomOverSampler`
0.11.0,implements such of a strategy.
0.11.0,%%
0.11.0,%% [markdown]
0.11.0,"By default, random over-sampling generates a bootstrap. The parameter"
0.11.0,`shrinkage` allows adding a small perturbation to the generated data
0.11.0,to generate a smoothed bootstrap instead. The plot below shows the difference
0.11.0,between the two data generation strategies.
0.11.0,%%
0.11.0,%% [markdown]
0.11.0,It looks like more samples are generated with smoothed bootstrap. This is due
0.11.0,to the fact that the samples generated are not superimposing with the
0.11.0,original samples.
0.11.0,
0.11.0,More advanced over-sampling using ADASYN and SMOTE
0.11.0,--------------------------------------------------
0.11.0,
0.11.0,Instead of repeating the same samples when over-sampling or perturbating the
0.11.0,"generated bootstrap samples, one can use some specific heuristic instead."
0.11.0,:class:`~imblearn.over_sampling.ADASYN` and
0.11.0,:class:`~imblearn.over_sampling.SMOTE` can be used in this case.
0.11.0,%%
0.11.0,%% [markdown]
0.11.0,The following plot illustrates the difference between
0.11.0,:class:`~imblearn.over_sampling.ADASYN` and
0.11.0,:class:`~imblearn.over_sampling.SMOTE`.
0.11.0,:class:`~imblearn.over_sampling.ADASYN` will focus on the samples which are
0.11.0,difficult to classify with a nearest-neighbors rule while regular
0.11.0,:class:`~imblearn.over_sampling.SMOTE` will not make any distinction.
0.11.0,"Therefore, the decision function depending of the algorithm."
0.11.0,%% [markdown]
0.11.0,"Due to those sampling particularities, it can give rise to some specific"
0.11.0,issues as illustrated below.
0.11.0,%%
0.11.0,%% [markdown]
0.11.0,SMOTE proposes several variants by identifying specific samples to consider
0.11.0,during the resampling. The borderline version
0.11.0,(:class:`~imblearn.over_sampling.BorderlineSMOTE`) will detect which point to
0.11.0,select which are in the border between two classes. The SVM version
0.11.0,(:class:`~imblearn.over_sampling.SVMSMOTE`) will use the support vectors
0.11.0,found using an SVM algorithm to create new sample while the KMeans version
0.11.0,(:class:`~imblearn.over_sampling.KMeansSMOTE`) will make a clustering before
0.11.0,to generate samples in each cluster independently depending each cluster
0.11.0,density.
0.11.0,%%
0.11.0,%% [markdown]
0.11.0,"When dealing with a mixed of continuous and categorical features,"
0.11.0,:class:`~imblearn.over_sampling.SMOTENC` is the only method which can handle
0.11.0,this case.
0.11.0,%%
0.11.0,Create a dataset of a mix of numerical and categorical data
0.11.0,%% [markdown]
0.11.0,"However, if the dataset is composed of only categorical features then one"
0.11.0,should use :class:`~imblearn.over_sampling.SMOTEN`.
0.11.0,%%
0.11.0,Generate only categorical data
0.11.0,Authors: Christos Aridas
0.11.0,Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.11.0,License: MIT
0.11.0,%%
0.11.0,%% [markdown]
0.11.0,Let's first generate a dataset with imbalanced class distribution.
0.11.0,%%
0.11.0,%% [markdown]
0.11.0,We will use an over-sampler :class:`~imblearn.over_sampling.SMOTE` followed
0.11.0,by a :class:`~sklearn.tree.DecisionTreeClassifier`. The aim will be to
0.11.0,search which `k_neighbors` parameter is the most adequate with the dataset
0.11.0,that we generated.
0.11.0,%%
0.11.0,%% [markdown]
0.11.0,We can use the :class:`~sklearn.model_selection.validation_curve` to inspect
0.11.0,"the impact of varying the parameter `k_neighbors`. In this case, we need"
0.11.0,to use a score to evaluate the generalization score during the
0.11.0,cross-validation.
0.11.0,%%
0.11.0,%%
0.11.0,%% [markdown]
0.11.0,We can now plot the results of the cross-validation for the different
0.11.0,parameter values that we tried.
0.11.0,%%
0.11.0,make nice plotting
0.11.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.11.0,License: MIT
0.11.0,Generate a dataset
0.11.0,Split the data
0.11.0,Train the classifier with balancing
0.11.0,Test the classifier and get the prediction
0.11.0,Show the classification report
0.11.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.11.0,License: MIT
0.11.0,%%
0.11.0,%% [markdown]
0.11.0,"First, we will generate some imbalanced dataset."
0.11.0,%%
0.11.0,%% [markdown]
0.11.0,We will split the data into a training and testing set.
0.11.0,%%
0.11.0,%% [markdown]
0.11.0,We will create a pipeline made of a :class:`~imblearn.over_sampling.SMOTE`
0.11.0,over-sampler followed by a :class:`~sklearn.linear_model.LogisticRegression`
0.11.0,classifier.
0.11.0,%%
0.11.0,%% [markdown]
0.11.0,"Now, we will train the model on the training set and get the prediction"
0.11.0,associated with the testing set. Be aware that the resampling will happen
0.11.0,only when calling `fit`: the number of samples in `y_pred` is the same than
0.11.0,in `y_test`.
0.11.0,%%
0.11.0,%% [markdown]
0.11.0,The geometric mean corresponds to the square root of the product of the
0.11.0,sensitivity and specificity. Combining the two metrics should account for
0.11.0,the balancing of the dataset.
0.11.0,%%
0.11.0,%% [markdown]
0.11.0,The index balanced accuracy can transform any metric to be used in
0.11.0,imbalanced learning problems.
0.11.0,%%
0.11.0,%%
0.11.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.11.0,License: MIT
0.11.0,%%
0.11.0,%% [markdown]
0.11.0,Dataset generation
0.11.0,------------------
0.11.0,
0.11.0,We will create an imbalanced dataset with a couple of samples. We will use
0.11.0,:func:`~sklearn.datasets.make_classification` to generate this dataset.
0.11.0,%%
0.11.0,%%
0.11.0,%% [markdown]
0.11.0,The following function will be used to plot the sample space after resampling
0.11.0,to illustrate the characteristic of an algorithm.
0.11.0,%%
0.11.0,%% [markdown]
0.11.0,The following function will be used to plot the decision function of a
0.11.0,classifier given some data.
0.11.0,%%
0.11.0,%% [markdown]
0.11.0,":class:`~imblearn.over_sampling.SMOTE` allows to generate samples. However,"
0.11.0,this method of over-sampling does not have any knowledge regarding the
0.11.0,"underlying distribution. Therefore, some noisy samples can be generated, e.g."
0.11.0,"when the different classes cannot be well separated. Hence, it can be"
0.11.0,beneficial to apply an under-sampling algorithm to clean the noisy samples.
0.11.0,Two methods are usually used in the literature: (i) Tomek's link and (ii)
0.11.0,edited nearest neighbours cleaning methods. Imbalanced-learn provides two
0.11.0,ready-to-use samplers :class:`~imblearn.combine.SMOTETomek` and
0.11.0,":class:`~imblearn.combine.SMOTEENN`. In general,"
0.11.0,:class:`~imblearn.combine.SMOTEENN` cleans more noisy data than
0.11.0,:class:`~imblearn.combine.SMOTETomek`.
0.11.0,%%
0.11.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.11.0,License: MIT
0.11.0,%%
0.11.0,%% [markdown]
0.11.0,Load an imbalanced dataset
0.11.0,--------------------------
0.11.0,
0.11.0,We will load the UCI SatImage dataset which has an imbalanced ratio of 9.3:1
0.11.0,(number of majority sample for a minority sample). The data are then split
0.11.0,into training and testing.
0.11.0,%%
0.11.0,%% [markdown]
0.11.0,Classification using a single decision tree
0.11.0,-------------------------------------------
0.11.0,
0.11.0,We train a decision tree classifier which will be used as a baseline for the
0.11.0,rest of this example.
0.11.0,
0.11.0,The results are reported in terms of balanced accuracy and geometric mean
0.11.0,which are metrics widely used in the literature to validate model trained on
0.11.0,imbalanced set.
0.11.0,%%
0.11.0,%%
0.11.0,%%
0.11.0,%% [markdown]
0.11.0,Classification using bagging classifier with and without sampling
0.11.0,-----------------------------------------------------------------
0.11.0,
0.11.0,"Instead of using a single tree, we will check if an ensemble of decision tree"
0.11.0,"can actually alleviate the issue induced by the class imbalancing. First, we"
0.11.0,will use a bagging classifier and its counter part which internally uses a
0.11.0,random under-sampling to balanced each bootstrap sample.
0.11.0,%%
0.11.0,%% [markdown]
0.11.0,Balancing each bootstrap sample allows to increase significantly the balanced
0.11.0,accuracy and the geometric mean.
0.11.0,%%
0.11.0,%%
0.11.0,%% [markdown]
0.11.0,Classification using random forest classifier with and without sampling
0.11.0,-----------------------------------------------------------------------
0.11.0,
0.11.0,Random forest is another popular ensemble method and it is usually
0.11.0,"outperforming bagging. Here, we used a vanilla random forest and its balanced"
0.11.0,counterpart in which each bootstrap sample is balanced.
0.11.0,%%
0.11.0,%% [markdown]
0.11.0,"Similarly to the previous experiment, the balanced classifier outperform the"
0.11.0,"classifier which learn from imbalanced bootstrap samples. In addition, random"
0.11.0,forest outperforms the bagging classifier.
0.11.0,%%
0.11.0,%%
0.11.0,%% [markdown]
0.11.0,Boosting classifier
0.11.0,-------------------
0.11.0,
0.11.0,"In the same manner, easy ensemble classifier is a bag of balanced AdaBoost"
0.11.0,"classifier. However, it will be slower to train than random forest and will"
0.11.0,achieve worse performance.
0.11.0,%%
0.11.0,%%
0.11.0,%%
0.11.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.11.0,License: MIT
0.11.0,%%
0.11.0,%% [markdown]
0.11.0,Generate an imbalanced dataset
0.11.0,------------------------------
0.11.0,
0.11.0,"For this example, we will create a synthetic dataset using the function"
0.11.0,:func:`~sklearn.datasets.make_classification`. The problem will be a toy
0.11.0,classification problem with a ratio of 1:9 between the two classes.
0.11.0,%%
0.11.0,%%
0.11.0,%% [markdown]
0.11.0,"In the following sections, we will show a couple of algorithms that have"
0.11.0,been proposed over the years. We intend to illustrate how one can reuse the
0.11.0,:class:`~imblearn.ensemble.BalancedBaggingClassifier` by passing different
0.11.0,sampler.
0.11.0,%%
0.11.0,%% [markdown]
0.11.0,Exactly Balanced Bagging and Over-Bagging
0.11.0,-----------------------------------------
0.11.0,
0.11.0,The :class:`~imblearn.ensemble.BalancedBaggingClassifier` can use in
0.11.0,conjunction with a :class:`~imblearn.under_sampling.RandomUnderSampler` or
0.11.0,:class:`~imblearn.over_sampling.RandomOverSampler`. These methods are
0.11.0,"referred as Exactly Balanced Bagging and Over-Bagging, respectively and have"
0.11.0,been proposed first in [1]_.
0.11.0,%%
0.11.0,Exactly Balanced Bagging
0.11.0,%%
0.11.0,Over-bagging
0.11.0,%% [markdown]
0.11.0,SMOTE-Bagging
0.11.0,-------------
0.11.0,
0.11.0,Instead of using a :class:`~imblearn.over_sampling.RandomOverSampler` that
0.11.0,"make a bootstrap, an alternative is to use"
0.11.0,:class:`~imblearn.over_sampling.SMOTE` as an over-sampler. This is known as
0.11.0,SMOTE-Bagging [2]_.
0.11.0,%%
0.11.0,SMOTE-Bagging
0.11.0,%% [markdown]
0.11.0,Roughly Balanced Bagging
0.11.0,------------------------
0.11.0,While using a :class:`~imblearn.under_sampling.RandomUnderSampler` or
0.11.0,:class:`~imblearn.over_sampling.RandomOverSampler` will create exactly the
0.11.0,"desired number of samples, it does not follow the statistical spirit wanted"
0.11.0,in the bagging framework. The authors in [3]_ proposes to use a negative
0.11.0,binomial distribution to compute the number of samples of the majority
0.11.0,class to be selected and then perform a random under-sampling.
0.11.0,
0.11.0,"Here, we illustrate this method by implementing a function in charge of"
0.11.0,resampling and use the :class:`~imblearn.FunctionSampler` to integrate it
0.11.0,within a :class:`~imblearn.pipeline.Pipeline` and
0.11.0,:class:`~sklearn.model_selection.cross_validate`.
0.11.0,%%
0.11.0,find the minority and majority classes
0.11.0,compute the number of sample to draw from the majority class using
0.11.0,a negative binomial distribution
0.11.0,draw randomly with or without replacement
0.11.0,Roughly Balanced Bagging
0.11.0,%% [markdown]
0.11.0,.. topic:: References:
0.11.0,
0.11.0,".. [1] R. Maclin, and D. Opitz. ""An empirical evaluation of bagging and"
0.11.0,"boosting."" AAAI/IAAI 1997 (1997): 546-551."
0.11.0,
0.11.0,".. [2] S. Wang, and X. Yao. ""Diversity analysis on imbalanced data sets by"
0.11.0,"using ensemble models."" 2009 IEEE symposium on computational"
0.11.0,"intelligence and data mining. IEEE, 2009."
0.11.0,
0.11.0,".. [3] S. Hido, H. Kashima, and Y. Takahashi. ""Roughly balanced bagging"
0.11.0,"for imbalanced data."" Statistical Analysis and Data Mining: The ASA"
0.11.0,Data Science Journal 2.5‚Äê6 (2009): 412-426.
0.11.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.11.0,License: MIT
0.11.0,%%
0.11.0,%% [markdown]
0.11.0,The following function will be used to create toy dataset. It uses the
0.11.0,:func:`~sklearn.datasets.make_classification` from scikit-learn but fixing
0.11.0,some parameters.
0.11.0,%%
0.11.0,%% [markdown]
0.11.0,The following function will be used to plot the sample space after resampling
0.11.0,to illustrate the specificities of an algorithm.
0.11.0,%%
0.11.0,%% [markdown]
0.11.0,The following function will be used to plot the decision function of a
0.11.0,classifier given some data.
0.11.0,%%
0.11.0,%%
0.11.0,%% [markdown]
0.11.0,Prototype generation: under-sampling by generating new samples
0.11.0,--------------------------------------------------------------
0.11.0,
0.11.0,:class:`~imblearn.under_sampling.ClusterCentroids` under-samples by replacing
0.11.0,the original samples by the centroids of the cluster found.
0.11.0,%%
0.11.0,%% [markdown]
0.11.0,Prototype selection: under-sampling by selecting existing samples
0.11.0,-----------------------------------------------------------------
0.11.0,
0.11.0,The algorithm performing prototype selection can be subdivided into two
0.11.0,groups: (i) the controlled under-sampling methods and (ii) the cleaning
0.11.0,under-sampling methods.
0.11.0,
0.11.0,"With the controlled under-sampling methods, the number of samples to be"
0.11.0,selected can be specified.
0.11.0,:class:`~imblearn.under_sampling.RandomUnderSampler` is the most naive way of
0.11.0,performing such selection by randomly selecting a given number of samples by
0.11.0,the targetted class.
0.11.0,%%
0.11.0,%% [markdown]
0.11.0,:class:`~imblearn.under_sampling.NearMiss` algorithms implement some
0.11.0,heuristic rules in order to select samples. NearMiss-1 selects samples from
0.11.0,the majority class for which the average distance of the :math:`k`` nearest
0.11.0,samples of the minority class is the smallest. NearMiss-2 selects the samples
0.11.0,from the majority class for which the average distance to the farthest
0.11.0,samples of the negative class is the smallest. NearMiss-3 is a 2-step
0.11.0,"algorithm: first, for each minority sample, their :math:`m`"
0.11.0,"nearest-neighbors will be kept; then, the majority samples selected are the"
0.11.0,on for which the average distance to the :math:`k` nearest neighbors is the
0.11.0,largest.
0.11.0,%%
0.11.0,%% [markdown]
0.11.0,:class:`~imblearn.under_sampling.EditedNearestNeighbours` removes samples of
0.11.0,the majority class for which their class differ from the one of their
0.11.0,nearest-neighbors. This sieve can be repeated which is the principle of the
0.11.0,:class:`~imblearn.under_sampling.RepeatedEditedNearestNeighbours`.
0.11.0,:class:`~imblearn.under_sampling.AllKNN` is slightly different from the
0.11.0,:class:`~imblearn.under_sampling.RepeatedEditedNearestNeighbours` by changing
0.11.0,"the :math:`k` parameter of the internal nearest neighors algorithm,"
0.11.0,increasing it at each iteration.
0.11.0,%%
0.11.0,%% [markdown]
0.11.0,:class:`~imblearn.under_sampling.CondensedNearestNeighbour` makes use of a
0.11.0,1-NN to iteratively decide if a sample should be kept in a dataset or not.
0.11.0,The issue is that :class:`~imblearn.under_sampling.CondensedNearestNeighbour`
0.11.0,is sensitive to noise by preserving the noisy samples.
0.11.0,:class:`~imblearn.under_sampling.OneSidedSelection` also used the 1-NN and
0.11.0,use :class:`~imblearn.under_sampling.TomekLinks` to remove the samples
0.11.0,considered noisy. The
0.11.0,:class:`~imblearn.under_sampling.NeighbourhoodCleaningRule` use a
0.11.0,:class:`~imblearn.under_sampling.EditedNearestNeighbours` to remove some
0.11.0,"sample. Additionally, they use a 3 nearest-neighbors to remove samples which"
0.11.0,do not agree with this rule.
0.11.0,%%
0.11.0,%% [markdown]
0.11.0,:class:`~imblearn.under_sampling.InstanceHardnessThreshold` uses the
0.11.0,prediction of classifier to exclude samples. All samples which are classified
0.11.0,with a low probability will be removed.
0.11.0,%%
0.11.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.11.0,License: MIT
0.11.0,%%
0.11.0,%% [markdown]
0.11.0,This function allows to make nice plotting
0.11.0,%%
0.11.0,%% [markdown]
0.11.0,We will generate some toy data that illustrates how
0.11.0,:class:`~imblearn.under_sampling.TomekLinks` is used to clean a dataset.
0.11.0,%%
0.11.0,%% [markdown]
0.11.0,"In the figure above, the samples highlighted in green form a Tomek link since"
0.11.0,they are of different classes and are nearest neighbors of each other.
0.11.0,highlight the samples of interest
0.11.0,%% [markdown]
0.11.0,We can run the :class:`~imblearn.under_sampling.TomekLinks` sampling to
0.11.0,remove the corresponding samples. If `sampling_strategy='auto'` only the
0.11.0,sample from the majority class will be removed. If `sampling_strategy='all'`
0.11.0,both samples will be removed.
0.11.0,%%
0.11.0,highlight the samples of interest
0.11.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.11.0,License: MIT
0.11.0,%%
0.11.0,%% [markdown]
0.11.0,We define a function allowing to make some nice decoration on the plot.
0.11.0,%%
0.11.0,%% [markdown]
0.11.0,We can start by generating some data to later illustrate the principle of
0.11.0,each :class:`~imblearn.under_sampling.NearMiss` heuristic rules.
0.11.0,%%
0.11.0,%% [mardown]
0.11.0,NearMiss-1
0.11.0,----------
0.11.0,
0.11.0,NearMiss-1 selects samples from the majority class for which the average
0.11.0,distance to some nearest neighbours is the smallest. In the following
0.11.0,"example, we use a 3-NN to compute the average distance on 2 specific samples"
0.11.0,"of the majority class. Therefore, in this case the point linked by the"
0.11.0,green-dashed line will be selected since the average distance is smaller.
0.11.0,%%
0.11.0,%% [mardown]
0.11.0,NearMiss-2
0.11.0,----------
0.11.0,
0.11.0,NearMiss-2 selects samples from the majority class for which the average
0.11.0,distance to the farthest neighbors is the smallest. With the same
0.11.0,"configuration as previously presented, the sample linked to the green-dashed"
0.11.0,line will be selected since its distance the 3 farthest neighbors is the
0.11.0,smallest.
0.11.0,%%
0.11.0,%% [mardown]
0.11.0,NearMiss-3
0.11.0,----------
0.11.0,
0.11.0,"NearMiss-3 can be divided into 2 steps. First, a nearest-neighbors is used to"
0.11.0,short-list samples from the majority class (i.e. correspond to the
0.11.0,"highlighted samples in the following plot). Then, the sample with the largest"
0.11.0,average distance to the *k* nearest-neighbors are selected.
0.11.0,%%
0.11.0,select only the majority point of interest
0.11.0,Authors: Christos Aridas
0.11.0,Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.11.0,License: MIT
0.11.0,%%
0.11.0,%% [markdown]
0.11.0,Let's first create an imbalanced dataset and split in to two sets.
0.11.0,%%
0.11.0,%% [markdown]
0.11.0,"Now, we will create each individual steps that we would like later to combine"
0.11.0,%%
0.11.0,%% [markdown]
0.11.0,"Now, we can finally create a pipeline to specify in which order the different"
0.11.0,transformers and samplers should be executed before to provide the data to
0.11.0,the final classifier.
0.11.0,%%
0.11.0,%% [markdown]
0.11.0,We can now use the pipeline created as a normal classifier where resampling
0.11.0,"will happen when calling `fit` and disabled when calling `decision_function`,"
0.11.0,"`predict_proba`, or `predict`."
0.11.0,%%
0.11.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.11.0,License: MIT
0.11.0,##############################################################################
0.11.0,Data loading
0.11.0,##############################################################################
0.11.0,##############################################################################
0.11.0,"First, you should download the Porto Seguro data set from Kaggle. See the"
0.11.0,link in the introduction.
0.11.0,##############################################################################
0.11.0,The data set is imbalanced and it will have an effect on the fitting.
0.11.0,##############################################################################
0.11.0,Define the pre-processing pipeline
0.11.0,##############################################################################
0.11.0,##############################################################################
0.11.0,We want to standard scale the numerical features while we want to one-hot
0.11.0,"encode the categorical features. In this regard, we make use of the"
0.11.0,:class:`~sklearn.compose.ColumnTransformer`.
0.11.0,Create an environment variable to avoid using the GPU. This can be changed.
0.11.0,##############################################################################
0.11.0,Create a neural-network
0.11.0,##############################################################################
0.11.0,##############################################################################
0.11.0,We create a decorator to report the computation time
0.11.0,##############################################################################
0.11.0,The first model will be trained using the ``fit`` method and with imbalanced
0.11.0,mini-batches.
0.11.0,predict_proba was removed in tensorflow 2.6
0.11.0,##############################################################################
0.11.0,"In the contrary, we will use imbalanced-learn to create a generator of"
0.11.0,mini-batches which will yield balanced mini-batches.
0.11.0,##############################################################################
0.11.0,Classification loop
0.11.0,##############################################################################
0.11.0,##############################################################################
0.11.0,We will perform a 10-fold cross-validation and train the neural-network with
0.11.0,the two different strategies previously presented.
0.11.0,##############################################################################
0.11.0,Plot of the results and computation time
0.11.0,##############################################################################
0.11.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.11.0,License: MIT
0.11.0,%%
0.11.0,%% [markdown]
0.11.0,Problem definition
0.11.0,------------------
0.11.0,
0.11.0,We are dropping the following features:
0.11.0,
0.11.0,"- ""fnlwgt"": this feature was created while studying the ""adult"" dataset."
0.11.0,"Thus, we will not use this feature which is not acquired during the survey."
0.11.0,"- ""education-num"": it is encoding the same information than ""education""."
0.11.0,"Thus, we are removing one of these 2 features."
0.11.0,%%
0.11.0,%% [markdown]
0.11.0,"The ""adult"" dataset as a class ratio of about 3:1"
0.11.0,%%
0.11.0,%% [markdown]
0.11.0,This dataset is only slightly imbalanced. To better highlight the effect of
0.11.0,"learning from an imbalanced dataset, we will increase its ratio to 30:1"
0.11.0,%%
0.11.0,%% [markdown]
0.11.0,We will perform a cross-validation evaluation to get an estimate of the test
0.11.0,score.
0.11.0,
0.11.0,"As a baseline, we could use a classifier which will always predict the"
0.11.0,majority class independently of the features provided.
0.11.0,%%
0.11.0,%% [markdown]
0.11.0,"Instead of using the accuracy, we can use the balanced accuracy which will"
0.11.0,take into account the balancing issue.
0.11.0,%%
0.11.0,%% [markdown]
0.11.0,Strategies to learn from an imbalanced dataset
0.11.0,----------------------------------------------
0.11.0,We will use a dictionary and a list to continuously store the results of
0.11.0,our experiments and show them as a pandas dataframe.
0.11.0,%%
0.11.0,%% [markdown]
0.11.0,Dummy baseline
0.11.0,..............
0.11.0,
0.11.0,"Before to train a real machine learning model, we can store the results"
0.11.0,obtained with our :class:`~sklearn.dummy.DummyClassifier`.
0.11.0,%%
0.11.0,%% [markdown]
0.11.0,Linear classifier baseline
0.11.0,..........................
0.11.0,
0.11.0,We will create a machine learning pipeline using a
0.11.0,":class:`~sklearn.linear_model.LogisticRegression` classifier. In this regard,"
0.11.0,we will need to one-hot encode the categorical columns and standardized the
0.11.0,numerical columns before to inject the data into the
0.11.0,:class:`~sklearn.linear_model.LogisticRegression` classifier.
0.11.0,
0.11.0,"First, we define our numerical and categorical pipelines."
0.11.0,%%
0.11.0,%% [markdown]
0.11.0,"Then, we can create a preprocessor which will dispatch the categorical"
0.11.0,columns to the categorical pipeline and the numerical columns to the
0.11.0,numerical pipeline
0.11.0,%%
0.11.0,%% [markdown]
0.11.0,"Finally, we connect our preprocessor with our"
0.11.0,:class:`~sklearn.linear_model.LogisticRegression`. We can then evaluate our
0.11.0,model.
0.11.0,%%
0.11.0,%%
0.11.0,%% [markdown]
0.11.0,We can see that our linear model is learning slightly better than our dummy
0.11.0,"baseline. However, it is impacted by the class imbalance."
0.11.0,
0.11.0,We can verify that something similar is happening with a tree-based model
0.11.0,such as :class:`~sklearn.ensemble.RandomForestClassifier`. With this type of
0.11.0,"classifier, we will not need to scale the numerical data, and we will only"
0.11.0,need to ordinal encode the categorical data.
0.11.0,%%
0.11.0,%%
0.11.0,%% [markdown]
0.11.0,The :class:`~sklearn.ensemble.RandomForestClassifier` is as well affected by
0.11.0,"the class imbalanced, slightly less than the linear model. Now, we will"
0.11.0,present different approach to improve the performance of these 2 models.
0.11.0,
0.11.0,Use `class_weight`
0.11.0,..................
0.11.0,
0.11.0,Most of the models in `scikit-learn` have a parameter `class_weight`. This
0.11.0,parameter will affect the computation of the loss in linear model or the
0.11.0,criterion in the tree-based model to penalize differently a false
0.11.0,classification from the minority and majority class. We can set
0.11.0,"`class_weight=""balanced""` such that the weight applied is inversely"
0.11.0,proportional to the class frequency. We test this parametrization in both
0.11.0,linear model and tree-based model.
0.11.0,%%
0.11.0,%%
0.11.0,%% [markdown]
0.11.0,We can see that using `class_weight` was really effective for the linear
0.11.0,"model, alleviating the issue of learning from imbalanced classes. However,"
0.11.0,the :class:`~sklearn.ensemble.RandomForestClassifier` is still biased toward
0.11.0,"the majority class, mainly due to the criterion which is not suited enough to"
0.11.0,fight the class imbalance.
0.11.0,
0.11.0,Resample the training set during learning
0.11.0,.........................................
0.11.0,
0.11.0,Another way is to resample the training set by under-sampling or
0.11.0,over-sampling some of the samples. `imbalanced-learn` provides some samplers
0.11.0,to do such processing.
0.11.0,%%
0.11.0,%%
0.11.0,%%
0.11.0,%%
0.11.0,%% [markdown]
0.11.0,Applying a random under-sampler before the training of the linear model or
0.11.0,"random forest, allows to not focus on the majority class at the cost of"
0.11.0,making more mistake for samples in the majority class (i.e. decreased
0.11.0,accuracy).
0.11.0,
0.11.0,We could apply any type of samplers and find which sampler is working best
0.11.0,on the current dataset.
0.11.0,
0.11.0,"Instead, we will present another way by using classifiers which will apply"
0.11.0,sampling internally.
0.11.0,
0.11.0,Use of specific balanced algorithms from imbalanced-learn
0.11.0,.........................................................
0.11.0,
0.11.0,We already showed that random under-sampling can be effective on decision
0.11.0,"tree. However, instead of under-sampling once the dataset, one could"
0.11.0,under-sample the original dataset before to take a bootstrap sample. This is
0.11.0,the base of the :class:`imblearn.ensemble.BalancedRandomForestClassifier` and
0.11.0,:class:`~imblearn.ensemble.BalancedBaggingClassifier`.
0.11.0,%%
0.11.0,%%
0.11.0,%% [markdown]
0.11.0,The performance with the
0.11.0,:class:`~imblearn.ensemble.BalancedRandomForestClassifier` is better than
0.11.0,applying a single random under-sampling. We will use a gradient-boosting
0.11.0,classifier within a :class:`~imblearn.ensemble.BalancedBaggingClassifier`.
0.11.0,%% [markdown]
0.11.0,This last approach is the most effective. The different under-sampling allows
0.11.0,to bring some diversity for the different GBDT to learn and not focus on a
0.11.0,portion of the majority class.
0.11.0,Authors: Christos Aridas
0.11.0,Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.11.0,License: MIT
0.11.0,%%
0.11.0,%% [markdown]
0.11.0,Load the dataset
0.11.0,----------------
0.11.0,
0.11.0,We will use a dataset containing image from know person where we will
0.11.0,build a model to recognize the person on the image. We will make this problem
0.11.0,a binary problem by taking picture of only George W. Bush and Bill Clinton.
0.11.0,%%
0.11.0,%%
0.11.0,%% [markdown]
0.11.0,We can check the ratio between the two classes.
0.11.0,%%
0.11.0,%% [markdown]
0.11.0,We see that we have an imbalanced classification problem with ~95% of the
0.11.0,data belonging to the class G.W. Bush.
0.11.0,
0.11.0,Compare over-sampling approaches
0.11.0,--------------------------------
0.11.0,
0.11.0,We will use different over-sampling approaches and use a kNN classifier
0.11.0,to check if we can recognize the 2 presidents. The evaluation will be
0.11.0,performed through cross-validation and we will plot the mean ROC curve.
0.11.0,
0.11.0,We will create different pipelines and evaluate them.
0.11.0,%%
0.11.0,%% [markdown]
0.11.0,We will compute the mean ROC curve for each pipeline using a different splits
0.11.0,provided by the :class:`~sklearn.model_selection.StratifiedKFold`
0.11.0,cross-validation.
0.11.0,%%
0.11.0,compute the mean fpr/tpr to get the mean ROC curve
0.11.0,Create a display that we will reuse to make the aggregated plots for
0.11.0,all methods
0.11.0,%% [markdown]
0.11.0,"In the previous cell, we created the different mean ROC curve and we can plot"
0.11.0,them on the same plot.
0.11.0,%%
0.11.0,%% [markdown]
0.11.0,"We see that for this task, methods that are generating new samples with some"
0.11.0,interpolation (i.e. ADASYN and SMOTE) perform better than random
0.11.0,over-sampling or no resampling.
0.11.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.11.0,License: MIT
0.11.0,Create a folder to fetch the dataset
0.11.0,Create a pipeline
0.11.0,Classify and report the results
0.11.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.11.0,License: MIT
0.11.0,%%
0.11.0,%% [markdown]
0.11.0,Setting the data set
0.11.0,--------------------
0.11.0,
0.11.0,We use a part of the 20 newsgroups data set by loading 4 topics. Using the
0.11.0,"scikit-learn loader, the data are split into a training and a testing set."
0.11.0,
0.11.0,Note the class \#3 is the minority class and has almost twice less samples
0.11.0,than the majority class.
0.11.0,%%
0.11.0,%%
0.11.0,%% [markdown]
0.11.0,The usual scikit-learn pipeline
0.11.0,-------------------------------
0.11.0,
0.11.0,You might usually use scikit-learn pipeline by combining the TF-IDF
0.11.0,vectorizer to feed a multinomial naive bayes classifier. A classification
0.11.0,report summarized the results on the testing set.
0.11.0,
0.11.0,"As expected, the recall of the class \#3 is low mainly due to the class"
0.11.0,imbalanced.
0.11.0,%%
0.11.0,%%
0.11.0,%% [markdown]
0.11.0,Balancing the class before classification
0.11.0,-----------------------------------------
0.11.0,
0.11.0,"To improve the prediction of the class \#3, it could be interesting to apply"
0.11.0,"a balancing before to train the naive bayes classifier. Therefore, we will"
0.11.0,use a :class:`~imblearn.under_sampling.RandomUnderSampler` to equalize the
0.11.0,number of samples in all the classes before the training.
0.11.0,
0.11.0,It is also important to note that we are using the
0.11.0,:class:`~imblearn.pipeline.make_pipeline` function implemented in
0.11.0,imbalanced-learn to properly handle the samplers.
0.11.0,%%
0.11.0,%% [markdown]
0.11.0,"Although the results are almost identical, it can be seen that the resampling"
0.11.0,allowed to correct the poor recall of the class \#3 at the cost of reducing
0.11.0,"the other metrics for the other classes. However, the overall results are"
0.11.0,slightly better.
0.11.0,%%
0.11.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.11.0,License: MIT
0.11.0,#############################################################################
0.11.0,Toy data generation
0.11.0,#############################################################################
0.11.0,#############################################################################
0.11.0,We are generating some non Gaussian data set contaminated with some unform
0.11.0,noise.
0.11.0,#############################################################################
0.11.0,We will generate some cleaned test data without outliers.
0.11.0,#############################################################################
0.11.0,How to use the :class:`~imblearn.FunctionSampler`
0.11.0,#############################################################################
0.11.0,#############################################################################
0.11.0,We first define a function which will use
0.11.0,:class:`~sklearn.ensemble.IsolationForest` to eliminate some outliers from
0.11.0,our dataset during training. The function passed to the
0.11.0,:class:`~imblearn.FunctionSampler` will be called when using the method
0.11.0,``fit_resample``.
0.11.0,#############################################################################
0.11.0,Integrate it within a pipeline
0.11.0,#############################################################################
0.11.0,#############################################################################
0.11.0,"By elimnating outliers before the training, the classifier will be less"
0.11.0,affected during the prediction.
0.11.0,Authors: Dayvid Oliveira
0.11.0,Christos Aridas
0.11.0,Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.11.0,License: MIT
0.11.0,%%
0.11.0,%% [markdown]
0.11.0,Generate the dataset
0.11.0,--------------------
0.11.0,
0.11.0,"First, we will generate a dataset and convert it to a"
0.11.0,:class:`~pandas.DataFrame` with arbitrary column names. We will plot the
0.11.0,original dataset.
0.11.0,%%
0.11.0,%% [markdown]
0.11.0,Make a dataset imbalanced
0.11.0,-------------------------
0.11.0,
0.11.0,"Now, we will show the helpers :func:`~imblearn.datasets.make_imbalance`"
0.11.0,that is useful to random select a subset of samples. It will impact the
0.11.0,class distribution as specified by the parameters.
0.11.0,%%
0.11.0,%%
0.11.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.11.0,License: MIT
0.11.0,%%
0.11.0,%% [markdown]
0.11.0,Create an imbalanced dataset
0.11.0,----------------------------
0.11.0,
0.11.0,"First, we will create an imbalanced data set from a the iris data set."
0.11.0,%%
0.11.0,%%
0.11.0,%% [markdown]
0.11.0,Using ``sampling_strategy`` in resampling algorithms
0.11.0,====================================================
0.11.0,
0.11.0,`sampling_strategy` as a `float`
0.11.0,--------------------------------
0.11.0,
0.11.0,`sampling_strategy` can be given a `float`. For **under-sampling
0.11.0,"methods**, it corresponds to the ratio :math:`\alpha_{us}` defined by"
0.11.0,:math:`N_{rM} = \alpha_{us} \times N_{m}` where :math:`N_{rM}` and
0.11.0,:math:`N_{m}` are the number of samples in the majority class after
0.11.0,"resampling and the number of samples in the minority class, respectively."
0.11.0,%%
0.11.0,select only 2 classes since the ratio make sense in this case
0.11.0,%%
0.11.0,%% [markdown]
0.11.0,"For **over-sampling methods**, it correspond to the ratio"
0.11.0,:math:`\alpha_{os}` defined by :math:`N_{rm} = \alpha_{os} \times N_{M}`
0.11.0,where :math:`N_{rm}` and :math:`N_{M}` are the number of samples in the
0.11.0,minority class after resampling and the number of samples in the majority
0.11.0,"class, respectively."
0.11.0,%%
0.11.0,%% [markdown]
0.11.0,`sampling_strategy` as a `str`
0.11.0,-------------------------------
0.11.0,
0.11.0,`sampling_strategy` can be given as a string which specify the class
0.11.0,"targeted by the resampling. With under- and over-sampling, the number of"
0.11.0,samples will be equalized.
0.11.0,
0.11.0,Note that we are using multiple classes from now on.
0.11.0,%%
0.11.0,%% [markdown]
0.11.0,"With **cleaning method**, the number of samples in each class will not be"
0.11.0,equalized even if targeted.
0.11.0,%%
0.11.0,%% [markdown]
0.11.0,`sampling_strategy` as a `dict`
0.11.0,------------------------------
0.11.0,
0.11.0,"When `sampling_strategy` is a `dict`, the keys correspond to the targeted"
0.11.0,classes. The values correspond to the desired number of samples for each
0.11.0,targeted class. This is working for both **under- and over-sampling**
0.11.0,algorithms but not for the **cleaning algorithms**. Use a `list` instead.
0.11.0,%%
0.11.0,%% [markdown]
0.11.0,`sampling_strategy` as a `list`
0.11.0,-------------------------------
0.11.0,
0.11.0,"When `sampling_strategy` is a `list`, the list contains the targeted"
0.11.0,classes. It is used only for **cleaning methods** and raise an error
0.11.0,otherwise.
0.11.0,%%
0.11.0,%% [markdown]
0.11.0,`sampling_strategy` as a callable
0.11.0,---------------------------------
0.11.0,
0.11.0,"When callable, function taking `y` and returns a `dict`. The keys"
0.11.0,correspond to the targeted classes. The values correspond to the desired
0.11.0,number of samples for each class.
0.11.0,%%
0.11.0,List of whitelisted modules and methods; regexp are supported.
0.11.0,These docstrings will fail because they are inheriting from scikit-learn
0.11.0,skip private classes
0.11.0,"We ignore following error code,"
0.11.0,- RT02: The first line of the Returns section
0.11.0,"should contain only the type, .."
0.11.0,(as we may need refer to the name of the returned
0.11.0,object)
0.11.0,- GL01: Docstring text (summary) should start in the line
0.11.0,"immediately after the opening quotes (not in the same line,"
0.11.0,or leaving a blank line in between)
0.11.0,"- GL02: If there's a blank line, it should be before the"
0.11.0,"first line of the Returns section, not after (it allows to have"
0.11.0,short docstrings for properties).
0.11.0,Ignore PR02: Unknown parameters for properties. We sometimes use
0.11.0,"properties for ducktyping, i.e. SGDClassifier.predict_proba"
0.11.0,Following codes are only taken into account for the
0.11.0,top level class docstrings:
0.11.0,- ES01: No extended summary found
0.11.0,- SA01: See Also section not found
0.11.0,- EX01: No examples section found
0.11.0,In particular we can't parse the signature of properties
0.11.0,"When applied to classes, detect class method. For functions"
0.11.0,method = None.
0.11.0,TODO: this detection can be improved. Currently we assume that we have
0.11.0,class # methods if the second path element before last is in camel case.
0.11.0,'build' and 'install' is included to have structured metadata for CI.
0.11.0,It will NOT be included in setup's extras_require
0.11.0,"The values are (version_spec, comma separated tags)"
0.11.0,create inverse mapping for setuptools
0.11.0,Used by CI to get the min dependencies
0.11.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.11.0,License: MIT
0.11.0,TODO: remove this file when scikit-learn minimum version is 1.3
0.11.0,Return a copy of the threadlocal configuration so that users will
0.11.0,not be able to modify the configuration with the returned dict.
0.11.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.11.0,Christos Aridas
0.11.0,License: MIT
0.11.0,scikit-learn >= 1.2
0.11.0,we need to overwrite SamplerMixin.fit to bypass the validation
0.11.0,Adapted from scikit-learn
0.11.0,Author: Edouard Duchesnay
0.11.0,Gael Varoquaux
0.11.0,Virgile Fritsch
0.11.0,Alexandre Gramfort
0.11.0,Lars Buitinck
0.11.0,Christos Aridas
0.11.0,Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.11.0,License: BSD
0.11.0,BaseEstimator interface
0.11.0,validate names
0.11.0,validate estimators
0.11.0,We allow last estimator to be None as an identity transformation
0.11.0,Estimator interface
0.11.0,Setup the memory
0.11.0,joblib >= 0.12
0.11.0,Fit or load from cache the current transformer
0.11.0,Replace the transformer of the step with the fitted
0.11.0,transformer. This is necessary when loading the transformer
0.11.0,from the cache.
0.11.0,This variable is injected in the __builtins__ by the build
0.11.0,process. It is used to enable importing subpackages of sklearn when
0.11.0,the binaries are not built
0.11.0,mypy error: Cannot determine type of '__SKLEARN_SETUP__'
0.11.0,We are not importing the rest of scikit-learn during the build
0.11.0,"process, as it may not be compiled yet"
0.11.0,"FIXME: When we get Python 3.7 as minimal version, we will need to switch to"
0.11.0,the following solution:
0.11.0,https://snarky.ca/lazy-importing-in-python-3-7/
0.11.0,Import the target module and insert it into the parent's namespace
0.11.0,Update this object's dict so that if someone keeps a reference to the
0.11.0,"LazyLoader, lookups are efficient (__getattr__ is only called on"
0.11.0,lookups that fail).
0.11.0,delay the import of keras since we are going to import either tensorflow
0.11.0,or keras
0.11.0,Based on NiLearn package
0.11.0,License: simplified BSD
0.11.0,"PEP0440 compatible formatted version, see:"
0.11.0,https://www.python.org/dev/peps/pep-0440/
0.11.0,
0.11.0,Generic release markers:
0.11.0,X.Y
0.11.0,X.Y.Z # For bugfix releases
0.11.0,
0.11.0,Admissible pre-release markers:
0.11.0,X.YaN # Alpha release
0.11.0,X.YbN # Beta release
0.11.0,X.YrcN # Release Candidate
0.11.0,X.Y # Final release
0.11.0,
0.11.0,Dev branch marker is: 'X.Y.dev' or 'X.Y.devN' where N is an integer.
0.11.0,'X.Y.dev0' is the canonical version of 'X.Y.dev'
0.11.0,
0.11.0,coding: utf-8
0.11.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.11.0,Dariusz Brzezinski
0.11.0,License: MIT
0.11.0,Only negative labels
0.11.0,"Calculate tp_sum, pred_sum, true_sum ###"
0.11.0,labels are now from 0 to len(labels) - 1 -> use bincount
0.11.0,Pathological case
0.11.0,Compute the true negative
0.11.0,Retain only selected labels
0.11.0,"Finally, we have all our sufficient statistics. Divide! #"
0.11.0,"Divide, and on zero-division, set scores to 0 and warn:"
0.11.0,"Oddly, we may get an ""invalid"" rather than a ""divide"" error"
0.11.0,here.
0.11.0,Average the results
0.11.0,labels are now from 0 to len(labels) - 1 -> use bincount
0.11.0,Pathological case
0.11.0,Retain only selected labels
0.11.0,old version of scipy return MaskedConstant instead of 0.0
0.11.0,check that the scoring function does not need a score
0.11.0,and only a prediction
0.11.0,We do not support multilabel so the only average supported
0.11.0,is binary
0.11.0,Compute the different metrics
0.11.0,Precision/recall/f1
0.11.0,Specificity
0.11.0,Geometric mean
0.11.0,Index balanced accuracy
0.11.0,compute averages
0.11.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.11.0,License: MIT
0.11.0,categories are expected to be encoded from 0 to n_categories - 1
0.11.0,"list of length n_features of ndarray (n_categories, n_classes)"
0.11.0,compute the counts
0.11.0,normalize by the summing over the classes
0.11.0,silence potential warning due to in-place division by zero
0.11.0,coding: utf-8
0.11.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.11.0,Christos Aridas
0.11.0,License: MIT
0.11.0,##############################################################################
0.11.0,Utilities for testing
0.11.0,import some data to play with
0.11.0,restrict to a binary classification task
0.11.0,add noisy features to make the problem harder and avoid perfect results
0.11.0,"run classifier, get class probabilities and label predictions"
0.11.0,only interested in probabilities of the positive case
0.11.0,XXX: do we really want a special API for the binary case?
0.11.0,##############################################################################
0.11.0,Tests
0.11.0,detailed measures for each class
0.11.0,individual scoring function that can be used for grid search: in the
0.11.0,binary class case the score is the value of the measure for the positive
0.11.0,class (e.g. label == 1). This is deprecated for average != 'binary'.
0.11.0,Such a case may occur with non-stratified cross-validation
0.11.0,ensure the above were meaningful tests:
0.11.0,Bad pos_label
0.11.0,Bad average option
0.11.0,but average != 'binary'; even if data is binary
0.11.0,compute the geometric mean for the binary problem
0.11.0,print classification report with class names
0.11.0,print classification report with label detection
0.11.0,print classification report with class names
0.11.0,print classification report with label detection
0.11.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.11.0,License: MIT
0.11.0,Check basic feature of the metric:
0.11.0,"* the shape of the distance matrix is (n_samples, n_samples)"
0.11.0,* computing pairwise distance of X is the same than explicitely between
0.11.0,X and X.
0.11.0,Check the property of the vdm distance. Let's check the property
0.11.0,"described in ""Improved Heterogeneous Distance Functions"", D.R. Wilson and"
0.11.0,"T.R. Martinez, Journal of Artificial Intelligence Research 6 (1997) 1-34"
0.11.0,https://arxiv.org/pdf/cs/9701101.pdf
0.11.0,
0.11.0,"""if an attribute color has three values red, green and blue, and the"
0.11.0,"application is to identify whether or not an object is an apple, red and"
0.11.0,green would be considered closer than red and blue because the former two
0.11.0,"both have similar correlations with the output class apple."""
0.11.0,defined our feature
0.11.0,0 - not an apple / 1 - an apple
0.11.0,computing the distance between a sample of the same category should
0.11.0,give a null distance
0.11.0,check the property explained in the introduction example
0.11.0,green and red are very close
0.11.0,blue is closer to red than green
0.11.0,"Check that ""auto"" is equivalent to provide the number categories"
0.11.0,beforehand
0.11.0,Check that we raise an error if n_categories is inconsistent with the
0.11.0,number of features in X
0.11.0,Check that we don't get issue when a category is missing between 0
0.11.0,n_categories - 1
0.11.0,remove a categories that could be between 0 and n_categories
0.11.0,Check that we raise a NotFittedError when `fit` is not not called before
0.11.0,pairwise.
0.11.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.11.0,Christos Aridas
0.11.0,License: MIT
0.11.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.11.0,Christos Aridas
0.11.0,License: MIT
0.11.0,FIXME: to be removed in 0.12
0.11.0,The ratio is computed using a one-vs-rest manner. Using majority
0.11.0,in multi-class would lead to slightly different results at the
0.11.0,cost of introducing a new parameter.
0.11.0,rounding may cause new amount for n_samples
0.11.0,the nearest neighbors need to be fitted only on the current class
0.11.0,to find the class NN to generate new samples
0.11.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.11.0,Christos Aridas
0.11.0,License: MIT
0.11.0,smoothed bootstrap imposes to make numerical operation; we need
0.11.0,to be sure to have only numerical data in X
0.11.0,generate a smoothed bootstrap with a perturbation
0.11.0,generate a bootstrap
0.11.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.11.0,Christos Aridas
0.11.0,License: MIT
0.11.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.11.0,Fernando Nogueira
0.11.0,Christos Aridas
0.11.0,License: MIT
0.11.0,negate diagonal elements
0.11.0,identify cluster which are answering the requirements
0.11.0,empty cluster
0.11.0,the cluster is already considered balanced
0.11.0,not enough samples to apply SMOTE
0.11.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.11.0,Fernando Nogueira
0.11.0,Christos Aridas
0.11.0,Dzianis Dudnik
0.11.0,License: MIT
0.11.0,FIXME: to be removed in 0.12
0.11.0,divergence between borderline-1 and borderline-2
0.11.0,Create synthetic samples for borderline points.
0.11.0,only minority
0.11.0,we use a one-vs-rest policy to handle the multiclass in which
0.11.0,new samples will be created considering not only the majority
0.11.0,class but all over classes.
0.11.0,FIXME: to be removed in 0.12
0.11.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.11.0,Fernando Nogueira
0.11.0,Christos Aridas
0.11.0,Dzianis Dudnik
0.11.0,License: MIT
0.11.0,np.newaxis for backwards compatability with random_state
0.11.0,Samples are in danger for m/2 <= m' < m
0.11.0,Samples are noise for m = m'
0.11.0,FIXME: to be removed in 0.12
0.11.0,FIXME: to be removed in 0.12
0.11.0,compute the median of the standard deviation of the minority class
0.11.0,the input of the OneHotEncoder needs to be dense
0.11.0,we can replace the 1 entries of the categorical features with the
0.11.0,median of the standard deviation. It will ensure that whenever
0.11.0,"distance is computed between 2 samples, the difference will be equal"
0.11.0,to the median of the standard deviation as in the original paper.
0.11.0,"In the edge case where the median of the std is equal to 0, the 1s"
0.11.0,"entries will be also nullified. In this case, we store the original"
0.11.0,categorical encoding which will be later used for inversing the OHE
0.11.0,reverse the encoding of the categorical features
0.11.0,the matrix is supposed to be in the CSR format after the stacking
0.11.0,change in sparsity structure more efficient with LIL than CSR
0.11.0,convert to dense array since scipy.sparse doesn't handle 3D
0.11.0,"In the case that the median std was equal to zeros, we have to"
0.11.0,create non-null entry based on the encoded of OHE
0.11.0,tie breaking argmax
0.11.0,generate sample indices that will be used to generate new samples
0.11.0,"for each drawn samples, select its k-neighbors and generate a sample"
0.11.0,"where for each feature individually, each category generated is the"
0.11.0,most common category
0.11.0,FIXME: to be removed in 0.12
0.11.0,the kneigbors search will include the sample itself which is
0.11.0,expected from the original algorithm
0.11.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.11.0,Christos Aridas
0.11.0,Dzianis Dudnik
0.11.0,License: MIT
0.11.0,create 2 random continuous feature
0.11.0,create a categorical feature using some string
0.11.0,create a categorical feature using some integer
0.11.0,return the categories
0.11.0,create 2 random continuous feature
0.11.0,create a categorical feature using some string
0.11.0,create a categorical feature using some integer
0.11.0,return the categories
0.11.0,create 2 random continuous feature
0.11.0,create a categorical feature using some string
0.11.0,create a categorical feature using some integer
0.11.0,return the categories
0.11.0,create 2 random continuous feature
0.11.0,create a categorical feature using some string
0.11.0,create a categorical feature using some integer
0.11.0,return the categories
0.11.0,create 2 random continuous feature
0.11.0,create a categorical feature using some string
0.11.0,create a categorical feature using some integer
0.11.0,part of the common test which apply to SMOTE-NC even if it is not default
0.11.0,constructible
0.11.0,Check that the samplers handle pandas dataframe and pandas series
0.11.0,Cast X and y to not default dtype
0.11.0,Non-regression test for #662
0.11.0,https://github.com/scikit-learn-contrib/imbalanced-learn/issues/662
0.11.0,check that the categorical feature is not random but correspond to the
0.11.0,categories seen in the minority class samples
0.11.0,TODO: only use `sparse_output` when sklearn >= 1.2
0.11.0,TODO(0.13): remove this test
0.11.0,overall check for SMOTEN
0.11.0,check if the SMOTEN resample data as expected
0.11.0,"we generate data such that ""not apple"" will be the minority class and"
0.11.0,"samples from this class will be generated. We will force the ""blue"""
0.11.0,"category to be associated with this class. Therefore, the new generated"
0.11.0,"samples should as well be from the ""blue"" category."
0.11.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.11.0,Christos Aridas
0.11.0,License: MIT
0.11.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.11.0,Christos Aridas
0.11.0,License: MIT
0.11.0,FIXME: we should use to_numpy with pandas >= 0.25
0.11.0,check the random over-sampling with a multiclass problem
0.11.0,check that resampling with heterogeneous dtype is working with basic
0.11.0,resampling
0.11.0,check that we can oversample even with missing or infinite data
0.11.0,regression tests for #605
0.11.0,check that we raise an error when heterogeneous dtype data are given
0.11.0,and a smoothed bootstrap is requested
0.11.0,check that smoothed bootstrap is working for numerical array
0.11.0,check that a shrinkage factor of 0 is equivalent to not create a smoothed
0.11.0,bootstrap
0.11.0,check the behaviour of the shrinkage parameter
0.11.0,the covariance of the data generated with the larger shrinkage factor
0.11.0,should also be larger.
0.11.0,check the validation of the shrinkage parameter
0.11.0,check that m_neighbors is properly set. Regression test for:
0.11.0,https://github.com/scikit-learn-contrib/imbalanced-learn/issues/568
0.11.0,FIXME: to be removed in 0.12
0.11.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.11.0,Christos Aridas
0.11.0,License: MIT
0.11.0,shuffle the indices since the sampler are packing them by class
0.11.0,helper functions
0.11.0,input and output
0.11.0,build the model and weights
0.11.0,"build the loss, predict, and train operator"
0.11.0,Initialization of all variables in the graph
0.11.0,"For each epoch, run accuracy on train and test"
0.11.0,helper functions
0.11.0,input and output
0.11.0,build the model and weights
0.11.0,"build the loss, predict, and train operator"
0.11.0,Initialization of all variables in the graph
0.11.0,"For each epoch, run accuracy on train and test"
0.11.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.11.0,License: MIT
0.11.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.11.0,Fernando Nogueira
0.11.0,Christos Aridas
0.11.0,License: MIT
0.11.0,find which class to not consider
0.11.0,there is a Tomek link between two samples if they are both nearest
0.11.0,neighbors of each others.
0.11.0,Find the nearest neighbour of every point
0.11.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.11.0,Christos Aridas
0.11.0,License: MIT
0.11.0,Randomly get one sample from the majority class
0.11.0,Generate the index to select
0.11.0,Create the set C - One majority samples and all minority
0.11.0,Create the set S - all majority samples
0.11.0,fit knn on C
0.11.0,Check each sample in S if we keep it or drop it
0.11.0,Do not select sample which are already well classified
0.11.0,Classify on S
0.11.0,If the prediction do not agree with the true label
0.11.0,append it in C_x
0.11.0,Keep the index for later
0.11.0,Update C
0.11.0,fit a knn on C
0.11.0,This experimental to speed up the search
0.11.0,Classify all the element in S and avoid to test the
0.11.0,well classified elements
0.11.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.11.0,Dayvid Oliveira
0.11.0,Christos Aridas
0.11.0,License: MIT
0.11.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.11.0,Christos Aridas
0.11.0,License: MIT
0.11.0,Compute the distance considering the farthest neighbour
0.11.0,Sort the list of distance and get the index
0.11.0,Throw a warning to tell the user that we did not have enough samples
0.11.0,to select and that we just select everything
0.11.0,Select the desired number of samples
0.11.0,idx_tmp is relative to the feature selected in the
0.11.0,previous step and we need to find the indirection
0.11.0,fmt: off
0.11.0,fmt: on
0.11.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.11.0,Christos Aridas
0.11.0,License: MIT
0.11.0,select a sample from the current class
0.11.0,create the set composed of all minority samples and one
0.11.0,sample from the current class.
0.11.0,create the set S with removing the seed from S
0.11.0,since that it will be added anyway
0.11.0,apply Tomek cleaning
0.11.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.11.0,Dayvid Oliveira
0.11.0,Christos Aridas
0.11.0,License: MIT
0.11.0,Check the stopping criterion
0.11.0,1. If there is no changes for the vector y
0.11.0,2. If the number of samples in the other class become inferior to
0.11.0,the number of samples in the majority class
0.11.0,3. If one of the class is disappearing
0.11.0,Case 1
0.11.0,Case 2
0.11.0,Case 3
0.11.0,Check the stopping criterion
0.11.0,1. If the number of samples in the other class become inferior to
0.11.0,the number of samples in the majority class
0.11.0,2. If one of the class is disappearing
0.11.0,Case 1else:
0.11.0,overwrite b_min_bec_maj
0.11.0,Case 2
0.11.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.11.0,Christos Aridas
0.11.0,License: MIT
0.11.0,clean the neighborhood
0.11.0,compute which classes to consider for cleaning for the A2 group
0.11.0,compute a2 group
0.11.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.11.0,Christos Aridas
0.11.0,License: MIT
0.11.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.11.0,Christos Aridas
0.11.0,License: MIT
0.11.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.11.0,Christos Aridas
0.11.0,License: MIT
0.11.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.11.0,Christos Aridas
0.11.0,License: MIT
0.11.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.11.0,Christos Aridas
0.11.0,License: MIT
0.11.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.11.0,Christos Aridas
0.11.0,License: MIT
0.11.0,FIXME: we should use to_numpy with pandas >= 0.25
0.11.0,check that we can undersample even with missing or infinite data
0.11.0,regression tests for #605
0.11.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.11.0,Christos Aridas
0.11.0,License: MIT
0.11.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.11.0,Christos Aridas
0.11.0,License: MIT
0.11.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.11.0,Christos Aridas
0.11.0,License: MIT
0.11.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.11.0,Christos Aridas
0.11.0,License: MIT
0.11.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.11.0,Christos Aridas
0.11.0,License: MIT
0.11.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.11.0,Fernando Nogueira
0.11.0,Christos Aridas
0.11.0,License: MIT
0.11.0,check that the samples selecting by the hard voting corresponds to the
0.11.0,targeted class
0.11.0,non-regression test for:
0.11.0,https://github.com/scikit-learn-contrib/imbalanced-learn/issues/738
0.11.0,Generate valid values for the required parameters
0.11.0,The parameters `*args` and `**kwargs` are ignored since we cannot generate
0.11.0,constraints.
0.11.0,check that there is a constraint for each parameter
0.11.0,this object does not have a valid type for sure for all params
0.11.0,This parameter is not validated
0.11.0,"First, check that the error is raised if param doesn't match any valid type."
0.11.0,"Then, for constraints that are more than a type constraint, check that the"
0.11.0,error is raised if param does match a valid type but does not match any valid
0.11.0,value for this type.
0.11.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.11.0,Christos Aridas
0.11.0,License: MIT
0.11.0,test that all_estimators doesn't find abstract classes.
0.11.0,"For NearMiss, let's check the three algorithms"
0.11.0,Common tests for estimator instances
0.11.0,Authors: Alexandre Gramfort <alexandre.gramfort@inria.fr>
0.11.0,Raghav RV <rvraghav93@gmail.com>
0.11.0,License: BSD 3 clause
0.11.0,scikit-learn >= 1.2
0.11.0,"walk_packages() ignores DeprecationWarnings, now we need to ignore"
0.11.0,FutureWarnings
0.11.0,"mypy error: Module has no attribute ""__path__"""
0.11.0,functions to ignore args / docstring of
0.11.0,Methods where y param should be ignored if y=None by default
0.11.0,numpydoc 0.8.0's docscrape tool raises because of collections.abc under
0.11.0,Python 3.7
0.11.0,Test module docstring formatting
0.11.0,Skip test if numpydoc is not found
0.11.0,XXX unreached code as of v0.22
0.11.0,"pytest tooling, not part of the scikit-learn API"
0.11.0,Exclude non-scikit-learn classes
0.11.0,Now skip docstring test for y when y is None
0.11.0,by default for API reason
0.11.0,Exclude imported functions
0.11.0,Don't test private methods / functions
0.11.0,Test that there are no tabs in our source files
0.11.0,because we don't import
0.11.0,Minimal / degenerate instances: only useful to test the docstrings.
0.11.0,"As certain attributes are present ""only"" if a certain parameter is"
0.11.0,"provided, this checks if the word ""only"" is present in the attribute"
0.11.0,"description, and if not the attribute is required to be present."
0.11.0,ignore deprecation warnings
0.11.0,attributes
0.11.0,properties
0.11.0,ignore properties that raises an AttributeError and deprecated
0.11.0,properties
0.11.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.11.0,License: MIT
0.11.0,check that we can let a pass a regression variable by turning down the
0.11.0,validation
0.11.0,Check that the validation is bypass when calling `fit`
0.11.0,Non-regression test for:
0.11.0,https://github.com/scikit-learn-contrib/imbalanced-learn/issues/782
0.11.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.11.0,Christos Aridas
0.11.0,License: MIT
0.11.0,store timestamp to figure out whether the result of 'fit' has been
0.11.0,cached or not
0.11.0,store timestamp to figure out whether the result of 'fit' has been
0.11.0,cached or not
0.11.0,Pipeline accepts steps as tuple
0.11.0,Test the various init parameters of the pipeline.
0.11.0,Check that we can't instantiate pipelines with objects without fit
0.11.0,method
0.11.0,Smoke test with only an estimator
0.11.0,Check that params are set
0.11.0,Smoke test the repr:
0.11.0,Test with two objects
0.11.0,Check that we can't instantiate with non-transformers on the way
0.11.0,"Note that NoTrans implements fit, but not transform"
0.11.0,Check that params are set
0.11.0,Smoke test the repr:
0.11.0,Check that params are not set when naming them wrong
0.11.0,Test clone
0.11.0,"Check that apart from estimators, the parameters are the same"
0.11.0,Remove estimators that where copied
0.11.0,Test the various methods of the pipeline (anova).
0.11.0,Test with Anova + LogisticRegression
0.11.0,Test that the pipeline can take fit parameters
0.11.0,classifier should return True
0.11.0,and transformer params should not be changed
0.11.0,invalid parameters should raise an error message
0.11.0,Pipeline should pass sample_weight
0.11.0,When sample_weight is None it shouldn't be passed
0.11.0,Test pipeline raises set params error message for nested models.
0.11.0,nested model check
0.11.0,Test the various methods of the pipeline (pca + svm).
0.11.0,Test with PCA + SVC
0.11.0,Test the various methods of the pipeline (preprocessing + svm).
0.11.0,check shapes of various prediction functions
0.11.0,test that the fit_predict method is implemented on a pipeline
0.11.0,test that the fit_predict on pipeline yields same results as applying
0.11.0,transform and clustering steps separately
0.11.0,"As pipeline doesn't clone estimators on construction,"
0.11.0,it must have its own estimators
0.11.0,first compute the transform and clustering step separately
0.11.0,use a pipeline to do the transform and clustering in one step
0.11.0,tests that a pipeline does not have fit_predict method when final
0.11.0,step of pipeline does not have fit_predict defined
0.11.0,tests that Pipeline passes fit_params to intermediate steps
0.11.0,when fit_predict is invoked
0.11.0,Test whether pipeline works with a transformer at the end.
0.11.0,Also test pipeline.transform and pipeline.inverse_transform
0.11.0,test transform and fit_transform:
0.11.0,Test whether pipeline works with a transformer missing fit_transform
0.11.0,test fit_transform:
0.11.0,Directly setting attr
0.11.0,Using set_params
0.11.0,Using set_params to replace single step
0.11.0,With invalid data
0.11.0,Test setting Pipeline steps to None
0.11.0,"for other methods, ensure no AttributeErrors on None:"
0.11.0,mult2 and mult3 are active
0.11.0,Check 'passthrough' step at construction time
0.11.0,Test with Transformer + SVC
0.11.0,Memoize the transformer at the first fit
0.11.0,Get the time stamp of the tranformer in the cached pipeline
0.11.0,Check that cached_pipe and pipe yield identical results
0.11.0,Check that we are reading the cache while fitting
0.11.0,a second time
0.11.0,Check that cached_pipe and pipe yield identical results
0.11.0,Create a new pipeline with cloned estimators
0.11.0,Check that even changing the name step does not affect the cache hit
0.11.0,Check that cached_pipe and pipe yield identical results
0.11.0,Test with Transformer + SVC
0.11.0,Memoize the transformer at the first fit
0.11.0,Get the time stamp of the tranformer in the cached pipeline
0.11.0,Check that cached_pipe and pipe yield identical results
0.11.0,Check that we are reading the cache while fitting
0.11.0,a second time
0.11.0,Check that cached_pipe and pipe yield identical results
0.11.0,Create a new pipeline with cloned estimators
0.11.0,Check that even changing the name step does not affect the cache hit
0.11.0,Check that cached_pipe and pipe yield identical results
0.11.0,Test the various methods of the pipeline (pca + svm).
0.11.0,Test with PCA + SVC
0.11.0,Test the various methods of the pipeline (pca + svm).
0.11.0,Test with PCA + SVC
0.11.0,Test whether pipeline works with a sampler at the end.
0.11.0,Also test pipeline.sampler
0.11.0,test transform and fit_transform:
0.11.0,We round the value near to zero. It seems that PCA has some issue
0.11.0,with that
0.11.0,Test whether pipeline works with a sampler at the end.
0.11.0,Also test pipeline.sampler
0.11.0,Test pipeline using None as preprocessing step and a classifier
0.11.0,"Test pipeline using None, RUS and a classifier"
0.11.0,"Test pipeline using RUS, None and a classifier"
0.11.0,Test pipeline using None step and a sampler
0.11.0,Test pipeline using None and a transformer that implements transform and
0.11.0,inverse_transform
0.11.0,Test the various methods of the pipeline (anova).
0.11.0,Test with RandomUnderSampling + Anova + LogisticRegression
0.11.0,Test the various methods of the pipeline (anova).
0.11.0,Test the various methods of the pipeline (anova).
0.11.0,Test with RandomUnderSampling + Anova + LogisticRegression
0.11.0,tests that Pipeline passes predict_params to the final estimator
0.11.0,when predict is invoked
0.11.0,Test that the score_samples method is implemented on a pipeline.
0.11.0,Test that the score_samples method on pipeline yields same results as
0.11.0,applying transform and score_samples steps separately.
0.11.0,Check the shapes
0.11.0,Check the values
0.11.0,Test that a pipeline does not have score_samples method when the final
0.11.0,step of the pipeline does not have score_samples defined.
0.11.0,Test that the score_samples method is implemented on a pipeline.
0.11.0,Test that the score_samples method on pipeline yields same results as
0.11.0,applying transform and score_samples steps separately.
0.11.0,Check the shapes
0.11.0,Check the values
0.11.0,transformer will not change `y` and sampler will always preserve the type of `y`
0.11.0,transformer will not change `y` and sampler will always preserve the type of `y`
0.11.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.11.0,Christos Aridas
0.11.0,License: MIT
0.11.0,TODO: Remove when SciPy 1.9 is the minimum supported version
0.11.0,TODO: Remove when scikit-learn 1.1 is the minimum supported version
0.11.0,TODO: remove when scikit-learn minimum version is 1.3
0.11.0,we don't want to validate again for each call to partial_fit
0.11.0,TODO: remove when scikit-learn minimum version is 1.3
0.11.0,"Likely a pandas DataFrame, we explicitly check the type to confirm."
0.11.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.11.0,License: MIT
0.11.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.11.0,License: MIT
0.11.0,Adapated from scikit-learn
0.11.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.11.0,License: MIT
0.11.0,scikit-learn >= 1.2
0.11.0,TODO: remove in 0.13
0.11.0,future default in 0.13
0.11.0,we don't filter samplers based on their tag here because we want to make
0.11.0,sure that the fitted attribute does not exist if the tag is not
0.11.0,stipulated
0.11.0,trigger our checks if this is a SamplerMixin
0.11.0,should raise warning if the target is continuous (we cannot raise error)
0.11.0,if the target is multilabel then we should raise an error
0.11.0,IHT does not enforce the number of samples but provide a number
0.11.0,of samples the closest to the desired target.
0.11.0,in this test we will force all samplers to not change the class 1
0.11.0,check that sparse matrices can be passed through the sampler leading to
0.11.0,the same results than dense
0.11.0,Check that the samplers handle pandas dataframe and pandas series
0.11.0,check that we return the same type for dataframes or series types
0.11.0,FIXME: we should use to_numpy with pandas >= 0.25
0.11.0,Check that the can samplers handle simple lists
0.11.0,Check that multiclass target lead to the same results than OVA encoding
0.11.0,Cast X and y to not default dtype
0.11.0,Non-regression test for #709
0.11.0,https://github.com/scikit-learn-contrib/imbalanced-learn/issues/709
0.11.0,Check that an informative error is raised when the value of a constructor
0.11.0,parameter does not have an appropriate type or value.
0.11.0,check that there is a constraint for each parameter
0.11.0,this object does not have a valid type for sure for all params
0.11.0,This parameter is not validated
0.11.0,"First, check that the error is raised if param doesn't match any valid type."
0.11.0,the method is not accessible with the current set of parameters
0.11.0,The estimator is a label transformer and take only `y`
0.11.0,"Then, for constraints that are more than a type constraint, check that the"
0.11.0,error is raised if param does match a valid type but does not match any valid
0.11.0,value for this type.
0.11.0,the method is not accessible with the current set of parameters
0.11.0,The estimator is a label transformer and take only `y`
0.11.0,Check that calling `fit` does not raise any warnings about feature names.
0.11.0,Only check imblearn estimators for feature_names_in_ in docstring
0.11.0,partial_fit checks on second call
0.11.0,Do not call partial fit if early_stopping is on
0.11.0,input_features names is not the same length as n_features_in_
0.11.0,error is raised when `input_features` do not match feature_names_in
0.11.0,Adapted from scikit-learn
0.11.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.11.0,License: MIT
0.11.0,Ignore deprecation warnings triggered at import time and from walking
0.11.0,packages
0.11.0,get rid of abstract base classes
0.11.0,get rid of sklearn estimators which have been imported in some classes
0.11.0,"drop duplicates, sort for reproducibility"
0.11.0,itemgetter is used to ensure the sort does not extend to the 2nd item of
0.11.0,the tuple
0.11.0,mypy: ignore-errors
0.11.0,update the docstring of the descriptor
0.11.0,"delegate only on instances, not the classes."
0.11.0,this is to allow access to the docstrings.
0.11.0,This makes it possible to use the decorated method as an
0.11.0,"unbound method, for instance when monkeypatching."
0.11.0,mypy: ignore-errors
0.11.0,TODO: remove `if True` when we have clear support for:
0.11.0,- ignoring `*args` and `**kwargs` in the signature
0.11.0,Inherits from ValueError and TypeError to keep backward compatibility.
0.11.0,We allow parameters to not have a constraint so that third party
0.11.0,estimators can inherit from sklearn estimators without having to
0.11.0,necessarily use the validation tools.
0.11.0,"this constraint is satisfied, no need to check further."
0.11.0,"No constraint is satisfied, raise with an informative message."
0.11.0,Ignore constraints that we don't want to expose in the error
0.11.0,"message, i.e. options that are for internal purpose or not"
0.11.0,officially supported.
0.11.0,The dict of parameter constraints is set as an attribute of the function
0.11.0,to make it possible to dynamically introspect the constraints for
0.11.0,automatic testing.
0.11.0,Map *args/**kwargs to the function signature
0.11.0,ignore self/cls and positional/keyword markers
0.11.0,"When the function is just a wrapper around an estimator,"
0.11.0,we allow the function to delegate validation to the
0.11.0,"estimator, but we replace the name of the estimator by"
0.11.0,the name of the function in the error message to avoid
0.11.0,confusion.
0.11.0,better repr if the bounds were given as integers
0.11.0,TODO(1.4) remove support for Integral.
0.11.0,we use an interval of Real to ignore np.nan that has its own
0.11.0,constraint
0.11.0,"There's no integer outside (-inf, +inf)"
0.11.0,"bounds are -inf, +inf"
0.11.0,"interval is [-inf, +inf]"
0.11.0,special case for ndarray since it can't be instantiated without
0.11.0,arguments
0.11.0,special case for Integral and Real since they are abstract classes
0.11.0,Author: Alexander L. Hayes <hayesall@iu.edu>
0.11.0,License: MIT
0.11.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.11.0,License: MIT
0.11.0,We lost the y.index during resampling. We can safely use X.index to align
0.11.0,them.
0.11.0,_is_neighbors_object(nn_object)
0.11.0,check that all keys in sampling_strategy are also in y
0.11.0,check that there is no negative number
0.11.0,check that all keys in sampling_strategy are also in y
0.11.0,ignore first 'self' argument for instance methods
0.11.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.11.0,Christos Aridas
0.11.0,License: MIT
0.11.0,this function could create an equal number of samples
0.11.0,We pass on purpose a non sorted dictionary and check that the resulting
0.11.0,dictionary is sorted. Refer to issue #428.
0.11.0,DataFrame and DataFrame case
0.11.0,DataFrames and Series case
0.11.0,The * is place before a keyword only argument without a default value
0.11.0,Test that the minimum dependencies in the README.rst file are
0.11.0,consistent with the minimum dependencies defined at the file:
0.11.0,imblearn/_min_dependencies.py
0.11.0,Skip the test if the README.rst file is not available.
0.11.0,"For instance, when installing scikit-learn from wheels"
0.11.0,Author: Alexander L. Hayes <hayesall@iu.edu>
0.11.0,License: MIT
0.11.0,Some helpers for the tests
0.11.0,check in the presence of extra positional and keyword args
0.11.0,outer decorator does not interfere with validation
0.11.0,validated method can be decorated
0.11.0,no validation in init
0.11.0,list and dict are valid params
0.11.0,the list option is not exposed in the error message
0.11.0,"""auto"" and ""warn"" are valid params"
0.11.0,"the ""warn"" option is not exposed in the error message"
0.11.0,True/False and np.bool_(True/False) are valid params
0.11.0,an int is also valid but deprecated
0.11.0,param1 is validated
0.11.0,param2 is not validated: any type is valid.
0.11.0,"does not raise, even though ""b"" is not in the constraints dict and ""a"" is not"
0.11.0,a parameter of the estimator.
0.11.0,does not raise
0.11.0,calls f with a bad parameter type
0.11.0,Validation for g is never skipped.
0.11.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.11.0,License: MIT
0.11.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.11.0,Christos Aridas
0.11.0,License: MIT
0.11.0,check if the filtering is working with a list or a single string
0.11.0,check that all estimators are sampler
0.11.0,check that an error is raised when the type is unknown
0.11.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.11.0,License: MIT
0.11.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.11.0,Christos Aridas
0.11.0,License: MIT
0.11.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.11.0,Christos Aridas
0.11.0,License: MIT
0.11.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.11.0,Christos Aridas
0.11.0,License: MIT
0.11.0,Check if default job count is None
0.11.0,Check if job count is set
0.11.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.11.0,Christos Aridas
0.11.0,License: MIT
0.11.0,Check if default job count is none
0.11.0,Check if job count is set
0.11.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.11.0,License: MIT
0.11.0,scikit-learn >= 1.2
0.11.0,resample before to fit the tree
0.11.0,TODO: remove when the minimum version of scikit-learn supported is 1.1
0.11.0,make a deepcopy to not modify the original dictionary
0.11.0,scikit-learn < 1.2
0.11.0,TODO: remove in 0.13
0.11.0,Validate or convert input data
0.11.0,Pre-sort indices to avoid that each individual tree of the
0.11.0,ensemble sorts the indices.
0.11.0,reshape is necessary to preserve the data contiguity against vs
0.11.0,"[:, np.newaxis] that does not."
0.11.0,Get bootstrap sample size
0.11.0,Check parameters
0.11.0,"Free allocated memory, if any"
0.11.0,We draw from the random state to get the random state we
0.11.0,would have got if we hadn't used a warm_start.
0.11.0,Parallel loop: we prefer the threading backend as the Cython code
0.11.0,for fitting the trees is internally releasing the Python GIL
0.11.0,making threading more efficient than multiprocessing in
0.11.0,"that case. However, we respect any parallel_backend contexts set"
0.11.0,"at a higher level, since correctness does not rely on using"
0.11.0,threads.
0.11.0,Collect newly grown trees
0.11.0,Create pipeline with the fitted samplers and trees
0.11.0,FIXME: we could consider to support multiclass-multioutput if
0.11.0,we introduce or reuse a constructor parameter (e.g.
0.11.0,oob_score) allowing our user to pass a callable defining the
0.11.0,scoring strategy on OOB sample.
0.11.0,Decapsulate classes_ attributes
0.11.0,drop the n_outputs axis if there is a single output
0.11.0,Prediction requires X to be in CSR format
0.11.0,n_classes_ is a ndarray at this stage
0.11.0,all the supported type of target will have the same number of
0.11.0,classes in all outputs
0.11.0,"for regression, n_classes_ does not exist and we create an empty"
0.11.0,axis to be consistent with the classification case and make
0.11.0,the array operations compatible with the 2 settings
0.11.0,TODO: remove when supporting scikit-learn>=1.4
0.11.0,TODO: remove when supporting scikit-learn>=1.2
0.11.0,make a deepcopy to not modify the original dictionary
0.11.0,TODO: remove when supporting scikit-learn>=1.2
0.11.0,scikit-learn < 1.2
0.11.0,SAMME-R requires predict_proba-enabled estimators
0.11.0,Instances incorrectly classified
0.11.0,Error fraction
0.11.0,Stop if classification is perfect
0.11.0,Construct y coding as described in Zhu et al [2]:
0.11.0,
0.11.0,y_k = 1 if c == k else -1 / (K - 1)
0.11.0,
0.11.0,"where K == n_classes_ and c, k in [0, K) are indices along the second"
0.11.0,axis of the y coding with c being the index corresponding to the true
0.11.0,class label.
0.11.0,Displace zero probabilities so the log is defined.
0.11.0,Also fix negative elements which may occur with
0.11.0,negative sample weights.
0.11.0,Boost weight using multi-class AdaBoost SAMME.R alg
0.11.0,Only boost the weights if it will fit again
0.11.0,Only boost positive weights
0.11.0,Instances incorrectly classified
0.11.0,Error fraction
0.11.0,Stop if classification is perfect
0.11.0,Stop if the error is at least as bad as random guessing
0.11.0,Boost weight using multi-class AdaBoost SAMME alg
0.11.0,Only boost the weights if I will fit again
0.11.0,Only boost positive weights
0.11.0,TODO: remove when supporting scikit-learn>=1.4
0.11.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.11.0,Christos Aridas
0.11.0,License: MIT
0.11.0,scikit-learn >= 1.2
0.11.0,make a deepcopy to not modify the original dictionary
0.11.0,TODO: remove when supporting scikit-learn>=1.2
0.11.0,scikit-learn < 1.2
0.11.0,TODO: remove when supporting scikit-learn>=1.4
0.11.0,TODO: remove when supporting scikit-learn>=1.2
0.11.0,overwrite the base class method by disallowing `sample_weight`
0.11.0,RandomUnderSampler is not supporting sample_weight. We need to pass
0.11.0,None.
0.11.0,TODO: remove when minimum supported version of scikit-learn is 1.1
0.11.0,Check data
0.11.0,Parallel loop
0.11.0,Reduce
0.11.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.11.0,Christos Aridas
0.11.0,License: MIT
0.11.0,scikit-learn >= 1.2
0.11.0,make a deepcopy to not modify the original dictionary
0.11.0,TODO: remove when supporting scikit-learn>=1.2
0.11.0,scikit-learn < 1.2
0.11.0,TODO: remove when supporting scikit-learn>=1.4
0.11.0,TODO: remove when supporting scikit-learn>=1.2
0.11.0,overwrite the base class method by disallowing `sample_weight`
0.11.0,the sampler needs to be validated before to call _fit because
0.11.0,_validate_y is called before _validate_estimator and would require
0.11.0,to know which type of sampler we are using.
0.11.0,RandomUnderSampler is not supporting sample_weight. We need to pass
0.11.0,None.
0.11.0,TODO: remove when minimum supported version of scikit-learn is 1.1
0.11.0,Check data
0.11.0,Parallel loop
0.11.0,Reduce
0.11.0,check that we have an ensemble of samplers and estimators with a
0.11.0,consistent size
0.11.0,each sampler in the ensemble should have different random state
0.11.0,each estimator in the ensemble should have different random state
0.11.0,check the consistency of the feature importances
0.11.0,check the consistency of the prediction outpus
0.11.0,Predictions should be the same when sample_weight are all ones
0.11.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.11.0,Christos Aridas
0.11.0,License: MIT
0.11.0,Check classification for various parameter settings.
0.11.0,Test that bootstrapping samples generate non-perfect base estimators.
0.11.0,"without bootstrap, all trees are perfect on the training set"
0.11.0,disable the resampling by passing an empty dictionary.
0.11.0,"with bootstrap, trees are no longer perfect on the training set"
0.11.0,Test that bootstrapping features may generate duplicate features.
0.11.0,Predict probabilities.
0.11.0,Normal case
0.11.0,"Degenerate case, where some classes are missing"
0.11.0,Check that oob prediction is a good estimation of the generalization
0.11.0,error.
0.11.0,Test with few estimators
0.11.0,Check singleton ensembles.
0.11.0,Check that bagging ensembles can be grid-searched.
0.11.0,Transform iris into a binary classification task
0.11.0,Grid search with scoring based on decision_function
0.11.0,Check estimator and its default values.
0.11.0,Test if fitting incrementally with warm start gives a forest of the
0.11.0,right size and the same results as a normal fit.
0.11.0,Test if warm start'ed second fit with smaller n_estimators raises error.
0.11.0,Test that nothing happens when fitting without increasing n_estimators
0.11.0,"modify X to nonsense values, this should not change anything"
0.11.0,warm started classifier with 5+5 estimators should be equivalent to
0.11.0,one classifier with 10 estimators
0.11.0,Check using oob_score and warm_start simultaneously fails
0.11.0,"Make sure OOB scores are identical when random_state, estimator, and"
0.11.0,training data are fixed and fitting is done twice
0.11.0,Check that format of estimators_samples_ is correct and that results
0.11.0,generated at fit time can be identically reproduced at a later time
0.11.0,using data saved in object attributes.
0.11.0,remap the y outside of the BalancedBaggingclassifier
0.11.0,"_, y = np.unique(y, return_inverse=True)"
0.11.0,Get relevant attributes
0.11.0,Test for correct formatting
0.11.0,Re-fit single estimator to test for consistent sampling
0.11.0,Make sure validated max_samples and original max_samples are identical
0.11.0,when valid integer max_samples supplied by user
0.11.0,check that we can pass any kind of sampler to a bagging classifier
0.11.0,check that we have balanced class with the right counts of class
0.11.0,sample depending on the sampling strategy
0.11.0,check that we can provide a FunctionSampler in BalancedBaggingClassifier
0.11.0,find the minority and majority classes
0.11.0,compute the number of sample to draw from the majority class using
0.11.0,a negative binomial distribution
0.11.0,draw randomly with or without replacement
0.11.0,Roughly Balanced Bagging
0.11.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.11.0,Christos Aridas
0.11.0,License: MIT
0.11.0,Generate a global dataset to use
0.11.0,Check classification for various parameter settings.
0.11.0,test the different prediction function
0.11.0,Check estimator and its default values.
0.11.0,Test if fitting incrementally with warm start gives a forest of the
0.11.0,right size and the same results as a normal fit.
0.11.0,Test if warm start'ed second fit with smaller n_estimators raises error.
0.11.0,Test that nothing happens when fitting without increasing n_estimators
0.11.0,"modify X to nonsense values, this should not change anything"
0.11.0,warm started classifier with 5+5 estimators should be equivalent to
0.11.0,one classifier with 10 estimators
0.11.0,Check warning if not enough estimators
0.11.0,First fit with no restriction on max samples
0.11.0,Second fit with max samples restricted to just 2
0.11.0,Regression test for #655: check that the oob score is closed to 0.5
0.11.0,a binomial experiment.
0.11.0,TODO: remove in 0.13
0.11.0,Author: Guillaume Lemaitre
0.11.0,License: BSD 3 clause
0.11.0,"The index start at one, then we need to remove one"
0.11.0,to not have issue with the indexing.
0.11.0,go through the list and check if the data are available
0.11.0,Authors: Dayvid Oliveira
0.11.0,Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.11.0,Christos Aridas
0.11.0,License: MIT
0.11.0,restrict ratio to be a dict or a callable
0.11.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.11.0,Christos Aridas
0.11.0,License: MIT
0.11.0,"we are reusing part of utils.check_sampling_strategy, however this is not"
0.11.0,cover in the common tests so we will repeat it here
0.11.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.11.0,Christos Aridas
0.11.0,License: MIT
0.11.0,This is a trick to avoid an error during tests collection with pytest. We
0.11.0,avoid the error when importing the package raise the error at the moment of
0.11.0,creating the instance.
0.11.0,This is a trick to avoid an error during tests collection with pytest. We
0.11.0,avoid the error when importing the package raise the error at the moment of
0.11.0,creating the instance.
0.11.0,flag for keras sequence duck-typing
0.11.0,shuffle the indices since the sampler are packing them by class
0.10.1,This file is here so that when running from the root folder
0.10.1,./imblearn is added to sys.path by pytest.
0.10.1,See https://docs.pytest.org/en/latest/pythonpath.html for more details.
0.10.1,"For example, this allows to build extensions in place and run pytest"
0.10.1,doc/modules/clustering.rst and use imblearn from the local folder
0.10.1,rather than the one from site-packages.
0.10.1,! /usr/bin/env python
0.10.1,Python 2 compat: just to be able to declare that Python >=3.7 is needed.
0.10.1,This is a bit (!) hackish: we are setting a global variable so that the
0.10.1,main imblearn __init__ can detect if it is being loaded by the setup
0.10.1,"routine, to avoid attempting to load components that aren't built yet:"
0.10.1,the numpy distutils extensions that are used by imbalanced-learn to
0.10.1,recursively build the compiled extensions in sub-packages is based on the
0.10.1,Python import machinery.
0.10.1,get __version__ from _version.py
0.10.1,-*- coding: utf-8 -*-
0.10.1,
0.10.1,"imbalanced-learn documentation build configuration file, created by"
0.10.1,sphinx-quickstart on Mon Jan 18 14:44:12 2016.
0.10.1,
0.10.1,This file is execfile()d with the current directory set to its
0.10.1,containing dir.
0.10.1,
0.10.1,Note that not all possible configuration values are present in this
0.10.1,autogenerated file.
0.10.1,
0.10.1,All configuration values have a default; values that are commented out
0.10.1,serve to show the default.
0.10.1,"If extensions (or modules to document with autodoc) are in another directory,"
0.10.1,add these directories to sys.path here. If the directory is relative to the
0.10.1,"documentation root, use os.path.abspath to make it absolute, like shown here."
0.10.1,-- General configuration ------------------------------------------------
0.10.1,"If your documentation needs a minimal Sphinx version, state it here."
0.10.1,needs_sphinx = '1.0'
0.10.1,"Add any Sphinx extension module names here, as strings. They can be"
0.10.1,extensions coming with Sphinx (named 'sphinx.ext.*') or your custom
0.10.1,ones.
0.10.1,"Add any paths that contain templates here, relative to this directory."
0.10.1,The suffix of source filenames.
0.10.1,The master toctree document.
0.10.1,General information about the project.
0.10.1,"The version info for the project you're documenting, acts as replacement for"
0.10.1,"|version| and |release|, also used in various other places throughout the"
0.10.1,built documents.
0.10.1,
0.10.1,The short X.Y version.
0.10.1,"The full version, including alpha/beta/rc tags."
0.10.1,"List of patterns, relative to source directory, that match files and"
0.10.1,directories to ignore when looking for source files.
0.10.1,The reST default role (used for this markup: `text`) to use for all
0.10.1,documents.
0.10.1,"If true, '()' will be appended to :func: etc. cross-reference text."
0.10.1,The name of the Pygments (syntax highlighting) style to use.
0.10.1,-- Options for HTML output ----------------------------------------------
0.10.1,The theme to use for HTML and HTML Help pages.  See the documentation for
0.10.1,a list of builtin themes.
0.10.1,"""twitter_url"": ""https://twitter.com/pandas_dev"","
0.10.1,"""navbar_align"": ""right"",  # For testing that the navbar items align properly"
0.10.1,"Add any paths that contain custom static files (such as style sheets) here,"
0.10.1,"relative to this directory. They are copied after the builtin static files,"
0.10.1,"so a file named ""default.css"" will overwrite the builtin ""default.css""."
0.10.1,Output file base name for HTML help builder.
0.10.1,-- Options for autodoc ------------------------------------------------------
0.10.1,generate autosummary even if no references
0.10.1,-- Options for numpydoc -----------------------------------------------------
0.10.1,this is needed for some reason...
0.10.1,see https://github.com/numpy/numpydoc/issues/69
0.10.1,-- Options for sphinxcontrib-bibtex -----------------------------------------
0.10.1,bibtex file
0.10.1,-- Options for intersphinx --------------------------------------------------
0.10.1,intersphinx configuration
0.10.1,-- Options for sphinx-gallery -----------------------------------------------
0.10.1,Generate the plot for the gallery
0.10.1,sphinx-gallery configuration
0.10.1,-- Options for github link for what's new -----------------------------------
0.10.1,Config for sphinx_issues
0.10.1,The following is used by sphinx.ext.linkcode to provide links to github
0.10.1,-- Options for LaTeX output ---------------------------------------------
0.10.1,The paper size ('letterpaper' or 'a4paper').
0.10.1,"'papersize': 'letterpaper',"
0.10.1,"The font size ('10pt', '11pt' or '12pt')."
0.10.1,"'pointsize': '10pt',"
0.10.1,Additional stuff for the LaTeX preamble.
0.10.1,"'preamble': '',"
0.10.1,Grouping the document tree into LaTeX files. List of tuples
0.10.1,"(source start file, target name, title,"
0.10.1,"author, documentclass [howto, manual, or own class])."
0.10.1,-- Options for manual page output ---------------------------------------
0.10.1,"If false, no module index is generated."
0.10.1,latex_domain_indices = True
0.10.1,One entry per manual page. List of tuples
0.10.1,"(source start file, name, description, authors, manual section)."
0.10.1,"If true, show URL addresses after external links."
0.10.1,man_show_urls = False
0.10.1,-- Options for Texinfo output -------------------------------------------
0.10.1,Grouping the document tree into Texinfo files. List of tuples
0.10.1,"(source start file, target name, title, author,"
0.10.1,"dir menu entry, description, category)"
0.10.1,-- Dependencies generation ----------------------------------------------
0.10.1,get length of header
0.10.1,-- Additional temporary hacks -----------------------------------------------
0.10.1,Temporary work-around for spacing problem between parameter and parameter
0.10.1,"type in the doc, see https://github.com/numpy/numpydoc/issues/215. The bug"
0.10.1,has been fixed in sphinx (https://github.com/sphinx-doc/sphinx/pull/5976) but
0.10.1,through a change in sphinx basic.css except rtd_theme does not use basic.css.
0.10.1,"In an ideal world, this would get fixed in this PR:"
0.10.1,https://github.com/readthedocs/sphinx_rtd_theme/pull/747/files
0.10.1,get the styles from the current theme
0.10.1,create and add the button to all the code blocks that contain >>>
0.10.1,tracebacks (.gt) contain bare text elements that need to be
0.10.1,wrapped in a span to work with .nextUntil() (see later)
0.10.1,define the behavior of the button when it's clicked
0.10.1,hide the code output
0.10.1,show the code output
0.10.1,-*- coding: utf-8 -*-
0.10.1,Format template for issues URI
0.10.1,e.g. 'https://github.com/sloria/marshmallow/issues/{issue}
0.10.1,Format template for PR URI
0.10.1,e.g. 'https://github.com/sloria/marshmallow/pull/{issue}
0.10.1,Format template for commit URI
0.10.1,e.g. 'https://github.com/sloria/marshmallow/commits/{commit}
0.10.1,"Shortcut for Github, e.g. 'sloria/marshmallow'"
0.10.1,Format template for user profile URI
0.10.1,e.g. 'https://github.com/{user}'
0.10.1,Python 2 only
0.10.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.10.1,License: MIT
0.10.1,%%
0.10.1,%%
0.10.1,"First, we will generate a toy classification dataset with only few samples."
0.10.1,The ratio between the classes will be imbalanced.
0.10.1,%%
0.10.1,%%
0.10.1,"Now, we will use a :class:`~imblearn.over_sampling.RandomOverSampler` to"
0.10.1,generate a bootstrap for the minority class with as many samples as in the
0.10.1,majority class.
0.10.1,%%
0.10.1,%%
0.10.1,We observe that the minority samples are less transparent than the samples
0.10.1,"from the majority class. Indeed, it is due to the fact that these samples"
0.10.1,of the minority class are repeated during the bootstrap generation.
0.10.1,
0.10.1,We can set `shrinkage` to a floating value to add a small perturbation to the
0.10.1,samples created and therefore create a smoothed bootstrap.
0.10.1,%%
0.10.1,%%
0.10.1,"In this case, we see that the samples in the minority class are not"
0.10.1,overlapping anymore due to the added noise.
0.10.1,
0.10.1,The parameter `shrinkage` allows to add more or less perturbation. Let's
0.10.1,add more perturbation when generating the smoothed bootstrap.
0.10.1,%%
0.10.1,%%
0.10.1,Increasing the value of `shrinkage` will disperse the new samples. Forcing
0.10.1,the shrinkage to 0 will be equivalent to generating a normal bootstrap.
0.10.1,%%
0.10.1,%%
0.10.1,"Therefore, the `shrinkage` is handy to manually tune the dispersion of the"
0.10.1,new samples.
0.10.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.10.1,License: MIT
0.10.1,%%
0.10.1,generate some data points
0.10.1,plot the majority and minority samples
0.10.1,draw the circle in which the new sample will generated
0.10.1,plot the line on which the sample will be generated
0.10.1,create and plot the new sample
0.10.1,make the plot nicer with legend and label
0.10.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.10.1,License: MIT
0.10.1,%%
0.10.1,%% [markdown]
0.10.1,The following function will be used to create toy dataset. It uses the
0.10.1,:func:`~sklearn.datasets.make_classification` from scikit-learn but fixing
0.10.1,some parameters.
0.10.1,%%
0.10.1,%% [markdown]
0.10.1,The following function will be used to plot the sample space after resampling
0.10.1,to illustrate the specificities of an algorithm.
0.10.1,%%
0.10.1,%% [markdown]
0.10.1,The following function will be used to plot the decision function of a
0.10.1,classifier given some data.
0.10.1,%%
0.10.1,%% [markdown]
0.10.1,Illustration of the influence of the balancing ratio
0.10.1,----------------------------------------------------
0.10.1,
0.10.1,We will first illustrate the influence of the balancing ratio on some toy
0.10.1,data using a logistic regression classifier which is a linear model.
0.10.1,%%
0.10.1,%% [markdown]
0.10.1,We will fit and show the decision boundary model to illustrate the impact of
0.10.1,dealing with imbalanced classes.
0.10.1,%%
0.10.1,%% [markdown]
0.10.1,"Greater is the difference between the number of samples in each class, poorer"
0.10.1,are the classification results.
0.10.1,
0.10.1,Random over-sampling to balance the data set
0.10.1,--------------------------------------------
0.10.1,
0.10.1,Random over-sampling can be used to repeat some samples and balance the
0.10.1,number of samples between the dataset. It can be seen that with this trivial
0.10.1,approach the boundary decision is already less biased toward the majority
0.10.1,class. The class :class:`~imblearn.over_sampling.RandomOverSampler`
0.10.1,implements such of a strategy.
0.10.1,%%
0.10.1,%% [markdown]
0.10.1,"By default, random over-sampling generates a bootstrap. The parameter"
0.10.1,`shrinkage` allows adding a small perturbation to the generated data
0.10.1,to generate a smoothed bootstrap instead. The plot below shows the difference
0.10.1,between the two data generation strategies.
0.10.1,%%
0.10.1,%% [markdown]
0.10.1,It looks like more samples are generated with smoothed bootstrap. This is due
0.10.1,to the fact that the samples generated are not superimposing with the
0.10.1,original samples.
0.10.1,
0.10.1,More advanced over-sampling using ADASYN and SMOTE
0.10.1,--------------------------------------------------
0.10.1,
0.10.1,Instead of repeating the same samples when over-sampling or perturbating the
0.10.1,"generated bootstrap samples, one can use some specific heuristic instead."
0.10.1,:class:`~imblearn.over_sampling.ADASYN` and
0.10.1,:class:`~imblearn.over_sampling.SMOTE` can be used in this case.
0.10.1,%%
0.10.1,%% [markdown]
0.10.1,The following plot illustrates the difference between
0.10.1,:class:`~imblearn.over_sampling.ADASYN` and
0.10.1,:class:`~imblearn.over_sampling.SMOTE`.
0.10.1,:class:`~imblearn.over_sampling.ADASYN` will focus on the samples which are
0.10.1,difficult to classify with a nearest-neighbors rule while regular
0.10.1,:class:`~imblearn.over_sampling.SMOTE` will not make any distinction.
0.10.1,"Therefore, the decision function depending of the algorithm."
0.10.1,%% [markdown]
0.10.1,"Due to those sampling particularities, it can give rise to some specific"
0.10.1,issues as illustrated below.
0.10.1,%%
0.10.1,%% [markdown]
0.10.1,SMOTE proposes several variants by identifying specific samples to consider
0.10.1,during the resampling. The borderline version
0.10.1,(:class:`~imblearn.over_sampling.BorderlineSMOTE`) will detect which point to
0.10.1,select which are in the border between two classes. The SVM version
0.10.1,(:class:`~imblearn.over_sampling.SVMSMOTE`) will use the support vectors
0.10.1,found using an SVM algorithm to create new sample while the KMeans version
0.10.1,(:class:`~imblearn.over_sampling.KMeansSMOTE`) will make a clustering before
0.10.1,to generate samples in each cluster independently depending each cluster
0.10.1,density.
0.10.1,%%
0.10.1,%% [markdown]
0.10.1,"When dealing with a mixed of continuous and categorical features,"
0.10.1,:class:`~imblearn.over_sampling.SMOTENC` is the only method which can handle
0.10.1,this case.
0.10.1,%%
0.10.1,Create a dataset of a mix of numerical and categorical data
0.10.1,%% [markdown]
0.10.1,"However, if the dataset is composed of only categorical features then one"
0.10.1,should use :class:`~imblearn.over_sampling.SMOTEN`.
0.10.1,%%
0.10.1,Generate only categorical data
0.10.1,Authors: Christos Aridas
0.10.1,Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.10.1,License: MIT
0.10.1,%%
0.10.1,%% [markdown]
0.10.1,Let's first generate a dataset with imbalanced class distribution.
0.10.1,%%
0.10.1,%% [markdown]
0.10.1,We will use an over-sampler :class:`~imblearn.over_sampling.SMOTE` followed
0.10.1,by a :class:`~sklearn.tree.DecisionTreeClassifier`. The aim will be to
0.10.1,search which `k_neighbors` parameter is the most adequate with the dataset
0.10.1,that we generated.
0.10.1,%%
0.10.1,%% [markdown]
0.10.1,We can use the :class:`~sklearn.model_selection.validation_curve` to inspect
0.10.1,"the impact of varying the parameter `k_neighbors`. In this case, we need"
0.10.1,to use a score to evaluate the generalization score during the
0.10.1,cross-validation.
0.10.1,%%
0.10.1,%%
0.10.1,%% [markdown]
0.10.1,We can now plot the results of the cross-validation for the different
0.10.1,parameter values that we tried.
0.10.1,%%
0.10.1,make nice plotting
0.10.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.10.1,License: MIT
0.10.1,Generate a dataset
0.10.1,Split the data
0.10.1,Train the classifier with balancing
0.10.1,Test the classifier and get the prediction
0.10.1,Show the classification report
0.10.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.10.1,License: MIT
0.10.1,%%
0.10.1,%% [markdown]
0.10.1,"First, we will generate some imbalanced dataset."
0.10.1,%%
0.10.1,%% [markdown]
0.10.1,We will split the data into a training and testing set.
0.10.1,%%
0.10.1,%% [markdown]
0.10.1,We will create a pipeline made of a :class:`~imblearn.over_sampling.SMOTE`
0.10.1,over-sampler followed by a :class:`~sklearn.svm.LinearSVC` classifier.
0.10.1,%%
0.10.1,%% [markdown]
0.10.1,"Now, we will train the model on the training set and get the prediction"
0.10.1,associated with the testing set. Be aware that the resampling will happen
0.10.1,only when calling `fit`: the number of samples in `y_pred` is the same than
0.10.1,in `y_test`.
0.10.1,%%
0.10.1,%% [markdown]
0.10.1,The geometric mean corresponds to the square root of the product of the
0.10.1,sensitivity and specificity. Combining the two metrics should account for
0.10.1,the balancing of the dataset.
0.10.1,%%
0.10.1,%% [markdown]
0.10.1,The index balanced accuracy can transform any metric to be used in
0.10.1,imbalanced learning problems.
0.10.1,%%
0.10.1,%%
0.10.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.10.1,License: MIT
0.10.1,%%
0.10.1,%% [markdown]
0.10.1,Dataset generation
0.10.1,------------------
0.10.1,
0.10.1,We will create an imbalanced dataset with a couple of samples. We will use
0.10.1,:func:`~sklearn.datasets.make_classification` to generate this dataset.
0.10.1,%%
0.10.1,%%
0.10.1,%% [markdown]
0.10.1,The following function will be used to plot the sample space after resampling
0.10.1,to illustrate the characteristic of an algorithm.
0.10.1,%%
0.10.1,%% [markdown]
0.10.1,The following function will be used to plot the decision function of a
0.10.1,classifier given some data.
0.10.1,%%
0.10.1,%% [markdown]
0.10.1,":class:`~imblearn.over_sampling.SMOTE` allows to generate samples. However,"
0.10.1,this method of over-sampling does not have any knowledge regarding the
0.10.1,"underlying distribution. Therefore, some noisy samples can be generated, e.g."
0.10.1,"when the different classes cannot be well separated. Hence, it can be"
0.10.1,beneficial to apply an under-sampling algorithm to clean the noisy samples.
0.10.1,Two methods are usually used in the literature: (i) Tomek's link and (ii)
0.10.1,edited nearest neighbours cleaning methods. Imbalanced-learn provides two
0.10.1,ready-to-use samplers :class:`~imblearn.combine.SMOTETomek` and
0.10.1,":class:`~imblearn.combine.SMOTEENN`. In general,"
0.10.1,:class:`~imblearn.combine.SMOTEENN` cleans more noisy data than
0.10.1,:class:`~imblearn.combine.SMOTETomek`.
0.10.1,%%
0.10.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.10.1,License: MIT
0.10.1,%%
0.10.1,%% [markdown]
0.10.1,Load an imbalanced dataset
0.10.1,--------------------------
0.10.1,
0.10.1,We will load the UCI SatImage dataset which has an imbalanced ratio of 9.3:1
0.10.1,(number of majority sample for a minority sample). The data are then split
0.10.1,into training and testing.
0.10.1,%%
0.10.1,%% [markdown]
0.10.1,Classification using a single decision tree
0.10.1,-------------------------------------------
0.10.1,
0.10.1,We train a decision tree classifier which will be used as a baseline for the
0.10.1,rest of this example.
0.10.1,
0.10.1,The results are reported in terms of balanced accuracy and geometric mean
0.10.1,which are metrics widely used in the literature to validate model trained on
0.10.1,imbalanced set.
0.10.1,%%
0.10.1,%%
0.10.1,%%
0.10.1,%% [markdown]
0.10.1,Classification using bagging classifier with and without sampling
0.10.1,-----------------------------------------------------------------
0.10.1,
0.10.1,"Instead of using a single tree, we will check if an ensemble of decision tree"
0.10.1,"can actually alleviate the issue induced by the class imbalancing. First, we"
0.10.1,will use a bagging classifier and its counter part which internally uses a
0.10.1,random under-sampling to balanced each bootstrap sample.
0.10.1,%%
0.10.1,%% [markdown]
0.10.1,Balancing each bootstrap sample allows to increase significantly the balanced
0.10.1,accuracy and the geometric mean.
0.10.1,%%
0.10.1,%%
0.10.1,%% [markdown]
0.10.1,Classification using random forest classifier with and without sampling
0.10.1,-----------------------------------------------------------------------
0.10.1,
0.10.1,Random forest is another popular ensemble method and it is usually
0.10.1,"outperforming bagging. Here, we used a vanilla random forest and its balanced"
0.10.1,counterpart in which each bootstrap sample is balanced.
0.10.1,%%
0.10.1,%% [markdown]
0.10.1,"Similarly to the previous experiment, the balanced classifier outperform the"
0.10.1,"classifier which learn from imbalanced bootstrap samples. In addition, random"
0.10.1,forest outperforms the bagging classifier.
0.10.1,%%
0.10.1,%%
0.10.1,%% [markdown]
0.10.1,Boosting classifier
0.10.1,-------------------
0.10.1,
0.10.1,"In the same manner, easy ensemble classifier is a bag of balanced AdaBoost"
0.10.1,"classifier. However, it will be slower to train than random forest and will"
0.10.1,achieve worse performance.
0.10.1,%%
0.10.1,%%
0.10.1,%%
0.10.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.10.1,License: MIT
0.10.1,%%
0.10.1,%% [markdown]
0.10.1,Generate an imbalanced dataset
0.10.1,------------------------------
0.10.1,
0.10.1,"For this example, we will create a synthetic dataset using the function"
0.10.1,:func:`~sklearn.datasets.make_classification`. The problem will be a toy
0.10.1,classification problem with a ratio of 1:9 between the two classes.
0.10.1,%%
0.10.1,%%
0.10.1,%% [markdown]
0.10.1,"In the following sections, we will show a couple of algorithms that have"
0.10.1,been proposed over the years. We intend to illustrate how one can reuse the
0.10.1,:class:`~imblearn.ensemble.BalancedBaggingClassifier` by passing different
0.10.1,sampler.
0.10.1,%%
0.10.1,%% [markdown]
0.10.1,Exactly Balanced Bagging and Over-Bagging
0.10.1,-----------------------------------------
0.10.1,
0.10.1,The :class:`~imblearn.ensemble.BalancedBaggingClassifier` can use in
0.10.1,conjunction with a :class:`~imblearn.under_sampling.RandomUnderSampler` or
0.10.1,:class:`~imblearn.over_sampling.RandomOverSampler`. These methods are
0.10.1,"referred as Exactly Balanced Bagging and Over-Bagging, respectively and have"
0.10.1,been proposed first in [1]_.
0.10.1,%%
0.10.1,Exactly Balanced Bagging
0.10.1,%%
0.10.1,Over-bagging
0.10.1,%% [markdown]
0.10.1,SMOTE-Bagging
0.10.1,-------------
0.10.1,
0.10.1,Instead of using a :class:`~imblearn.over_sampling.RandomOverSampler` that
0.10.1,"make a bootstrap, an alternative is to use"
0.10.1,:class:`~imblearn.over_sampling.SMOTE` as an over-sampler. This is known as
0.10.1,SMOTE-Bagging [2]_.
0.10.1,%%
0.10.1,SMOTE-Bagging
0.10.1,%% [markdown]
0.10.1,Roughly Balanced Bagging
0.10.1,------------------------
0.10.1,While using a :class:`~imblearn.under_sampling.RandomUnderSampler` or
0.10.1,:class:`~imblearn.over_sampling.RandomOverSampler` will create exactly the
0.10.1,"desired number of samples, it does not follow the statistical spirit wanted"
0.10.1,in the bagging framework. The authors in [3]_ proposes to use a negative
0.10.1,binomial distribution to compute the number of samples of the majority
0.10.1,class to be selected and then perform a random under-sampling.
0.10.1,
0.10.1,"Here, we illustrate this method by implementing a function in charge of"
0.10.1,resampling and use the :class:`~imblearn.FunctionSampler` to integrate it
0.10.1,within a :class:`~imblearn.pipeline.Pipeline` and
0.10.1,:class:`~sklearn.model_selection.cross_validate`.
0.10.1,%%
0.10.1,find the minority and majority classes
0.10.1,compute the number of sample to draw from the majority class using
0.10.1,a negative binomial distribution
0.10.1,draw randomly with or without replacement
0.10.1,Roughly Balanced Bagging
0.10.1,%% [markdown]
0.10.1,.. topic:: References:
0.10.1,
0.10.1,".. [1] R. Maclin, and D. Opitz. ""An empirical evaluation of bagging and"
0.10.1,"boosting."" AAAI/IAAI 1997 (1997): 546-551."
0.10.1,
0.10.1,".. [2] S. Wang, and X. Yao. ""Diversity analysis on imbalanced data sets by"
0.10.1,"using ensemble models."" 2009 IEEE symposium on computational"
0.10.1,"intelligence and data mining. IEEE, 2009."
0.10.1,
0.10.1,".. [3] S. Hido, H. Kashima, and Y. Takahashi. ""Roughly balanced bagging"
0.10.1,"for imbalanced data."" Statistical Analysis and Data Mining: The ASA"
0.10.1,Data Science Journal 2.5‚Äê6 (2009): 412-426.
0.10.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.10.1,License: MIT
0.10.1,%%
0.10.1,%% [markdown]
0.10.1,The following function will be used to create toy dataset. It uses the
0.10.1,:func:`~sklearn.datasets.make_classification` from scikit-learn but fixing
0.10.1,some parameters.
0.10.1,%%
0.10.1,%% [markdown]
0.10.1,The following function will be used to plot the sample space after resampling
0.10.1,to illustrate the specificities of an algorithm.
0.10.1,%%
0.10.1,%% [markdown]
0.10.1,The following function will be used to plot the decision function of a
0.10.1,classifier given some data.
0.10.1,%%
0.10.1,%%
0.10.1,%% [markdown]
0.10.1,Prototype generation: under-sampling by generating new samples
0.10.1,--------------------------------------------------------------
0.10.1,
0.10.1,:class:`~imblearn.under_sampling.ClusterCentroids` under-samples by replacing
0.10.1,the original samples by the centroids of the cluster found.
0.10.1,%%
0.10.1,%% [markdown]
0.10.1,Prototype selection: under-sampling by selecting existing samples
0.10.1,-----------------------------------------------------------------
0.10.1,
0.10.1,The algorithm performing prototype selection can be subdivided into two
0.10.1,groups: (i) the controlled under-sampling methods and (ii) the cleaning
0.10.1,under-sampling methods.
0.10.1,
0.10.1,"With the controlled under-sampling methods, the number of samples to be"
0.10.1,selected can be specified.
0.10.1,:class:`~imblearn.under_sampling.RandomUnderSampler` is the most naive way of
0.10.1,performing such selection by randomly selecting a given number of samples by
0.10.1,the targetted class.
0.10.1,%%
0.10.1,%% [markdown]
0.10.1,:class:`~imblearn.under_sampling.NearMiss` algorithms implement some
0.10.1,heuristic rules in order to select samples. NearMiss-1 selects samples from
0.10.1,the majority class for which the average distance of the :math:`k`` nearest
0.10.1,samples of the minority class is the smallest. NearMiss-2 selects the samples
0.10.1,from the majority class for which the average distance to the farthest
0.10.1,samples of the negative class is the smallest. NearMiss-3 is a 2-step
0.10.1,"algorithm: first, for each minority sample, their :math:`m`"
0.10.1,"nearest-neighbors will be kept; then, the majority samples selected are the"
0.10.1,on for which the average distance to the :math:`k` nearest neighbors is the
0.10.1,largest.
0.10.1,%%
0.10.1,%% [markdown]
0.10.1,:class:`~imblearn.under_sampling.EditedNearestNeighbours` removes samples of
0.10.1,the majority class for which their class differ from the one of their
0.10.1,nearest-neighbors. This sieve can be repeated which is the principle of the
0.10.1,:class:`~imblearn.under_sampling.RepeatedEditedNearestNeighbours`.
0.10.1,:class:`~imblearn.under_sampling.AllKNN` is slightly different from the
0.10.1,:class:`~imblearn.under_sampling.RepeatedEditedNearestNeighbours` by changing
0.10.1,"the :math:`k` parameter of the internal nearest neighors algorithm,"
0.10.1,increasing it at each iteration.
0.10.1,%%
0.10.1,%% [markdown]
0.10.1,:class:`~imblearn.under_sampling.CondensedNearestNeighbour` makes use of a
0.10.1,1-NN to iteratively decide if a sample should be kept in a dataset or not.
0.10.1,The issue is that :class:`~imblearn.under_sampling.CondensedNearestNeighbour`
0.10.1,is sensitive to noise by preserving the noisy samples.
0.10.1,:class:`~imblearn.under_sampling.OneSidedSelection` also used the 1-NN and
0.10.1,use :class:`~imblearn.under_sampling.TomekLinks` to remove the samples
0.10.1,considered noisy. The
0.10.1,:class:`~imblearn.under_sampling.NeighbourhoodCleaningRule` use a
0.10.1,:class:`~imblearn.under_sampling.EditedNearestNeighbours` to remove some
0.10.1,"sample. Additionally, they use a 3 nearest-neighbors to remove samples which"
0.10.1,do not agree with this rule.
0.10.1,%%
0.10.1,%% [markdown]
0.10.1,:class:`~imblearn.under_sampling.InstanceHardnessThreshold` uses the
0.10.1,prediction of classifier to exclude samples. All samples which are classified
0.10.1,with a low probability will be removed.
0.10.1,%%
0.10.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.10.1,License: MIT
0.10.1,%%
0.10.1,%% [markdown]
0.10.1,This function allows to make nice plotting
0.10.1,%%
0.10.1,%% [markdown]
0.10.1,We will generate some toy data that illustrates how
0.10.1,:class:`~imblearn.under_sampling.TomekLinks` is used to clean a dataset.
0.10.1,%%
0.10.1,%% [markdown]
0.10.1,"In the figure above, the samples highlighted in green form a Tomek link since"
0.10.1,they are of different classes and are nearest neighbors of each other.
0.10.1,highlight the samples of interest
0.10.1,%% [markdown]
0.10.1,We can run the :class:`~imblearn.under_sampling.TomekLinks` sampling to
0.10.1,remove the corresponding samples. If `sampling_strategy='auto'` only the
0.10.1,sample from the majority class will be removed. If `sampling_strategy='all'`
0.10.1,both samples will be removed.
0.10.1,%%
0.10.1,highlight the samples of interest
0.10.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.10.1,License: MIT
0.10.1,%%
0.10.1,%% [markdown]
0.10.1,We define a function allowing to make some nice decoration on the plot.
0.10.1,%%
0.10.1,%% [markdown]
0.10.1,We can start by generating some data to later illustrate the principle of
0.10.1,each :class:`~imblearn.under_sampling.NearMiss` heuristic rules.
0.10.1,%%
0.10.1,%% [mardown]
0.10.1,NearMiss-1
0.10.1,----------
0.10.1,
0.10.1,NearMiss-1 selects samples from the majority class for which the average
0.10.1,distance to some nearest neighbours is the smallest. In the following
0.10.1,"example, we use a 3-NN to compute the average distance on 2 specific samples"
0.10.1,"of the majority class. Therefore, in this case the point linked by the"
0.10.1,green-dashed line will be selected since the average distance is smaller.
0.10.1,%%
0.10.1,%% [mardown]
0.10.1,NearMiss-2
0.10.1,----------
0.10.1,
0.10.1,NearMiss-2 selects samples from the majority class for which the average
0.10.1,distance to the farthest neighbors is the smallest. With the same
0.10.1,"configuration as previously presented, the sample linked to the green-dashed"
0.10.1,line will be selected since its distance the 3 farthest neighbors is the
0.10.1,smallest.
0.10.1,%%
0.10.1,%% [mardown]
0.10.1,NearMiss-3
0.10.1,----------
0.10.1,
0.10.1,"NearMiss-3 can be divided into 2 steps. First, a nearest-neighbors is used to"
0.10.1,short-list samples from the majority class (i.e. correspond to the
0.10.1,"highlighted samples in the following plot). Then, the sample with the largest"
0.10.1,average distance to the *k* nearest-neighbors are selected.
0.10.1,%%
0.10.1,select only the majority point of interest
0.10.1,Authors: Christos Aridas
0.10.1,Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.10.1,License: MIT
0.10.1,%%
0.10.1,%% [markdown]
0.10.1,Let's first create an imbalanced dataset and split in to two sets.
0.10.1,%%
0.10.1,%% [markdown]
0.10.1,"Now, we will create each individual steps that we would like later to combine"
0.10.1,%%
0.10.1,%% [markdown]
0.10.1,"Now, we can finally create a pipeline to specify in which order the different"
0.10.1,transformers and samplers should be executed before to provide the data to
0.10.1,the final classifier.
0.10.1,%%
0.10.1,%% [markdown]
0.10.1,We can now use the pipeline created as a normal classifier where resampling
0.10.1,"will happen when calling `fit` and disabled when calling `decision_function`,"
0.10.1,"`predict_proba`, or `predict`."
0.10.1,%%
0.10.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.10.1,License: MIT
0.10.1,##############################################################################
0.10.1,Data loading
0.10.1,##############################################################################
0.10.1,##############################################################################
0.10.1,"First, you should download the Porto Seguro data set from Kaggle. See the"
0.10.1,link in the introduction.
0.10.1,##############################################################################
0.10.1,The data set is imbalanced and it will have an effect on the fitting.
0.10.1,##############################################################################
0.10.1,Define the pre-processing pipeline
0.10.1,##############################################################################
0.10.1,##############################################################################
0.10.1,We want to standard scale the numerical features while we want to one-hot
0.10.1,"encode the categorical features. In this regard, we make use of the"
0.10.1,:class:`~sklearn.compose.ColumnTransformer`.
0.10.1,Create an environment variable to avoid using the GPU. This can be changed.
0.10.1,##############################################################################
0.10.1,Create a neural-network
0.10.1,##############################################################################
0.10.1,##############################################################################
0.10.1,We create a decorator to report the computation time
0.10.1,##############################################################################
0.10.1,The first model will be trained using the ``fit`` method and with imbalanced
0.10.1,mini-batches.
0.10.1,##############################################################################
0.10.1,"In the contrary, we will use imbalanced-learn to create a generator of"
0.10.1,mini-batches which will yield balanced mini-batches.
0.10.1,##############################################################################
0.10.1,Classification loop
0.10.1,##############################################################################
0.10.1,##############################################################################
0.10.1,We will perform a 10-fold cross-validation and train the neural-network with
0.10.1,the two different strategies previously presented.
0.10.1,##############################################################################
0.10.1,Plot of the results and computation time
0.10.1,##############################################################################
0.10.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.10.1,License: MIT
0.10.1,%%
0.10.1,%% [markdown]
0.10.1,Problem definition
0.10.1,------------------
0.10.1,
0.10.1,We are dropping the following features:
0.10.1,
0.10.1,"- ""fnlwgt"": this feature was created while studying the ""adult"" dataset."
0.10.1,"Thus, we will not use this feature which is not acquired during the survey."
0.10.1,"- ""education-num"": it is encoding the same information than ""education""."
0.10.1,"Thus, we are removing one of these 2 features."
0.10.1,%%
0.10.1,%% [markdown]
0.10.1,"The ""adult"" dataset as a class ratio of about 3:1"
0.10.1,%%
0.10.1,%% [markdown]
0.10.1,This dataset is only slightly imbalanced. To better highlight the effect of
0.10.1,"learning from an imbalanced dataset, we will increase its ratio to 30:1"
0.10.1,%%
0.10.1,%% [markdown]
0.10.1,We will perform a cross-validation evaluation to get an estimate of the test
0.10.1,score.
0.10.1,
0.10.1,"As a baseline, we could use a classifier which will always predict the"
0.10.1,majority class independently of the features provided.
0.10.1,%%
0.10.1,%% [markdown]
0.10.1,"Instead of using the accuracy, we can use the balanced accuracy which will"
0.10.1,take into account the balancing issue.
0.10.1,%%
0.10.1,%% [markdown]
0.10.1,Strategies to learn from an imbalanced dataset
0.10.1,----------------------------------------------
0.10.1,We will use a dictionary and a list to continuously store the results of
0.10.1,our experiments and show them as a pandas dataframe.
0.10.1,%%
0.10.1,%% [markdown]
0.10.1,Dummy baseline
0.10.1,..............
0.10.1,
0.10.1,"Before to train a real machine learning model, we can store the results"
0.10.1,obtained with our :class:`~sklearn.dummy.DummyClassifier`.
0.10.1,%%
0.10.1,%% [markdown]
0.10.1,Linear classifier baseline
0.10.1,..........................
0.10.1,
0.10.1,We will create a machine learning pipeline using a
0.10.1,":class:`~sklearn.linear_model.LogisticRegression` classifier. In this regard,"
0.10.1,we will need to one-hot encode the categorical columns and standardized the
0.10.1,numerical columns before to inject the data into the
0.10.1,:class:`~sklearn.linear_model.LogisticRegression` classifier.
0.10.1,
0.10.1,"First, we define our numerical and categorical pipelines."
0.10.1,%%
0.10.1,%% [markdown]
0.10.1,"Then, we can create a preprocessor which will dispatch the categorical"
0.10.1,columns to the categorical pipeline and the numerical columns to the
0.10.1,numerical pipeline
0.10.1,%%
0.10.1,%% [markdown]
0.10.1,"Finally, we connect our preprocessor with our"
0.10.1,:class:`~sklearn.linear_model.LogisticRegression`. We can then evaluate our
0.10.1,model.
0.10.1,%%
0.10.1,%%
0.10.1,%% [markdown]
0.10.1,We can see that our linear model is learning slightly better than our dummy
0.10.1,"baseline. However, it is impacted by the class imbalance."
0.10.1,
0.10.1,We can verify that something similar is happening with a tree-based model
0.10.1,such as :class:`~sklearn.ensemble.RandomForestClassifier`. With this type of
0.10.1,"classifier, we will not need to scale the numerical data, and we will only"
0.10.1,need to ordinal encode the categorical data.
0.10.1,%%
0.10.1,%%
0.10.1,%% [markdown]
0.10.1,The :class:`~sklearn.ensemble.RandomForestClassifier` is as well affected by
0.10.1,"the class imbalanced, slightly less than the linear model. Now, we will"
0.10.1,present different approach to improve the performance of these 2 models.
0.10.1,
0.10.1,Use `class_weight`
0.10.1,..................
0.10.1,
0.10.1,Most of the models in `scikit-learn` have a parameter `class_weight`. This
0.10.1,parameter will affect the computation of the loss in linear model or the
0.10.1,criterion in the tree-based model to penalize differently a false
0.10.1,classification from the minority and majority class. We can set
0.10.1,"`class_weight=""balanced""` such that the weight applied is inversely"
0.10.1,proportional to the class frequency. We test this parametrization in both
0.10.1,linear model and tree-based model.
0.10.1,%%
0.10.1,%%
0.10.1,%% [markdown]
0.10.1,We can see that using `class_weight` was really effective for the linear
0.10.1,"model, alleviating the issue of learning from imbalanced classes. However,"
0.10.1,the :class:`~sklearn.ensemble.RandomForestClassifier` is still biased toward
0.10.1,"the majority class, mainly due to the criterion which is not suited enough to"
0.10.1,fight the class imbalance.
0.10.1,
0.10.1,Resample the training set during learning
0.10.1,.........................................
0.10.1,
0.10.1,Another way is to resample the training set by under-sampling or
0.10.1,over-sampling some of the samples. `imbalanced-learn` provides some samplers
0.10.1,to do such processing.
0.10.1,%%
0.10.1,%%
0.10.1,%%
0.10.1,%%
0.10.1,%% [markdown]
0.10.1,Applying a random under-sampler before the training of the linear model or
0.10.1,"random forest, allows to not focus on the majority class at the cost of"
0.10.1,making more mistake for samples in the majority class (i.e. decreased
0.10.1,accuracy).
0.10.1,
0.10.1,We could apply any type of samplers and find which sampler is working best
0.10.1,on the current dataset.
0.10.1,
0.10.1,"Instead, we will present another way by using classifiers which will apply"
0.10.1,sampling internally.
0.10.1,
0.10.1,Use of specific balanced algorithms from imbalanced-learn
0.10.1,.........................................................
0.10.1,
0.10.1,We already showed that random under-sampling can be effective on decision
0.10.1,"tree. However, instead of under-sampling once the dataset, one could"
0.10.1,under-sample the original dataset before to take a bootstrap sample. This is
0.10.1,the base of the :class:`imblearn.ensemble.BalancedRandomForestClassifier` and
0.10.1,:class:`~imblearn.ensemble.BalancedBaggingClassifier`.
0.10.1,%%
0.10.1,%%
0.10.1,%% [markdown]
0.10.1,The performance with the
0.10.1,:class:`~imblearn.ensemble.BalancedRandomForestClassifier` is better than
0.10.1,applying a single random under-sampling. We will use a gradient-boosting
0.10.1,classifier within a :class:`~imblearn.ensemble.BalancedBaggingClassifier`.
0.10.1,%% [markdown]
0.10.1,This last approach is the most effective. The different under-sampling allows
0.10.1,to bring some diversity for the different GBDT to learn and not focus on a
0.10.1,portion of the majority class.
0.10.1,Authors: Christos Aridas
0.10.1,Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.10.1,License: MIT
0.10.1,%%
0.10.1,%% [markdown]
0.10.1,Load the dataset
0.10.1,----------------
0.10.1,
0.10.1,We will use a dataset containing image from know person where we will
0.10.1,build a model to recognize the person on the image. We will make this problem
0.10.1,a binary problem by taking picture of only George W. Bush and Bill Clinton.
0.10.1,%%
0.10.1,%%
0.10.1,%% [markdown]
0.10.1,We can check the ratio between the two classes.
0.10.1,%%
0.10.1,%% [markdown]
0.10.1,We see that we have an imbalanced classification problem with ~95% of the
0.10.1,data belonging to the class G.W. Bush.
0.10.1,
0.10.1,Compare over-sampling approaches
0.10.1,--------------------------------
0.10.1,
0.10.1,We will use different over-sampling approaches and use a kNN classifier
0.10.1,to check if we can recognize the 2 presidents. The evaluation will be
0.10.1,performed through cross-validation and we will plot the mean ROC curve.
0.10.1,
0.10.1,We will create different pipelines and evaluate them.
0.10.1,%%
0.10.1,%% [markdown]
0.10.1,We will compute the mean ROC curve for each pipeline using a different splits
0.10.1,provided by the :class:`~sklearn.model_selection.StratifiedKFold`
0.10.1,cross-validation.
0.10.1,%%
0.10.1,compute the mean fpr/tpr to get the mean ROC curve
0.10.1,Create a display that we will reuse to make the aggregated plots for
0.10.1,all methods
0.10.1,%% [markdown]
0.10.1,"In the previous cell, we created the different mean ROC curve and we can plot"
0.10.1,them on the same plot.
0.10.1,%%
0.10.1,%% [markdown]
0.10.1,"We see that for this task, methods that are generating new samples with some"
0.10.1,interpolation (i.e. ADASYN and SMOTE) perform better than random
0.10.1,over-sampling or no resampling.
0.10.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.10.1,License: MIT
0.10.1,Create a folder to fetch the dataset
0.10.1,Create a pipeline
0.10.1,Classify and report the results
0.10.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.10.1,License: MIT
0.10.1,%%
0.10.1,%% [markdown]
0.10.1,Setting the data set
0.10.1,--------------------
0.10.1,
0.10.1,We use a part of the 20 newsgroups data set by loading 4 topics. Using the
0.10.1,"scikit-learn loader, the data are split into a training and a testing set."
0.10.1,
0.10.1,Note the class \#3 is the minority class and has almost twice less samples
0.10.1,than the majority class.
0.10.1,%%
0.10.1,%%
0.10.1,%% [markdown]
0.10.1,The usual scikit-learn pipeline
0.10.1,-------------------------------
0.10.1,
0.10.1,You might usually use scikit-learn pipeline by combining the TF-IDF
0.10.1,vectorizer to feed a multinomial naive bayes classifier. A classification
0.10.1,report summarized the results on the testing set.
0.10.1,
0.10.1,"As expected, the recall of the class \#3 is low mainly due to the class"
0.10.1,imbalanced.
0.10.1,%%
0.10.1,%%
0.10.1,%% [markdown]
0.10.1,Balancing the class before classification
0.10.1,-----------------------------------------
0.10.1,
0.10.1,"To improve the prediction of the class \#3, it could be interesting to apply"
0.10.1,"a balancing before to train the naive bayes classifier. Therefore, we will"
0.10.1,use a :class:`~imblearn.under_sampling.RandomUnderSampler` to equalize the
0.10.1,number of samples in all the classes before the training.
0.10.1,
0.10.1,It is also important to note that we are using the
0.10.1,:class:`~imblearn.pipeline.make_pipeline` function implemented in
0.10.1,imbalanced-learn to properly handle the samplers.
0.10.1,%%
0.10.1,%% [markdown]
0.10.1,"Although the results are almost identical, it can be seen that the resampling"
0.10.1,allowed to correct the poor recall of the class \#3 at the cost of reducing
0.10.1,"the other metrics for the other classes. However, the overall results are"
0.10.1,slightly better.
0.10.1,%%
0.10.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.10.1,License: MIT
0.10.1,#############################################################################
0.10.1,Toy data generation
0.10.1,#############################################################################
0.10.1,#############################################################################
0.10.1,We are generating some non Gaussian data set contaminated with some unform
0.10.1,noise.
0.10.1,#############################################################################
0.10.1,We will generate some cleaned test data without outliers.
0.10.1,#############################################################################
0.10.1,How to use the :class:`~imblearn.FunctionSampler`
0.10.1,#############################################################################
0.10.1,#############################################################################
0.10.1,We first define a function which will use
0.10.1,:class:`~sklearn.ensemble.IsolationForest` to eliminate some outliers from
0.10.1,our dataset during training. The function passed to the
0.10.1,:class:`~imblearn.FunctionSampler` will be called when using the method
0.10.1,``fit_resample``.
0.10.1,#############################################################################
0.10.1,Integrate it within a pipeline
0.10.1,#############################################################################
0.10.1,#############################################################################
0.10.1,"By elimnating outliers before the training, the classifier will be less"
0.10.1,affected during the prediction.
0.10.1,Authors: Dayvid Oliveira
0.10.1,Christos Aridas
0.10.1,Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.10.1,License: MIT
0.10.1,%%
0.10.1,%% [markdown]
0.10.1,Generate the dataset
0.10.1,--------------------
0.10.1,
0.10.1,"First, we will generate a dataset and convert it to a"
0.10.1,:class:`~pandas.DataFrame` with arbitrary column names. We will plot the
0.10.1,original dataset.
0.10.1,%%
0.10.1,%% [markdown]
0.10.1,Make a dataset imbalanced
0.10.1,-------------------------
0.10.1,
0.10.1,"Now, we will show the helpers :func:`~imblearn.datasets.make_imbalance`"
0.10.1,that is useful to random select a subset of samples. It will impact the
0.10.1,class distribution as specified by the parameters.
0.10.1,%%
0.10.1,%%
0.10.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.10.1,License: MIT
0.10.1,%%
0.10.1,%% [markdown]
0.10.1,Create an imbalanced dataset
0.10.1,----------------------------
0.10.1,
0.10.1,"First, we will create an imbalanced data set from a the iris data set."
0.10.1,%%
0.10.1,%%
0.10.1,%% [markdown]
0.10.1,Using ``sampling_strategy`` in resampling algorithms
0.10.1,====================================================
0.10.1,
0.10.1,`sampling_strategy` as a `float`
0.10.1,--------------------------------
0.10.1,
0.10.1,`sampling_strategy` can be given a `float`. For **under-sampling
0.10.1,"methods**, it corresponds to the ratio :math:`\alpha_{us}` defined by"
0.10.1,:math:`N_{rM} = \alpha_{us} \times N_{m}` where :math:`N_{rM}` and
0.10.1,:math:`N_{m}` are the number of samples in the majority class after
0.10.1,"resampling and the number of samples in the minority class, respectively."
0.10.1,%%
0.10.1,select only 2 classes since the ratio make sense in this case
0.10.1,%%
0.10.1,%% [markdown]
0.10.1,"For **over-sampling methods**, it correspond to the ratio"
0.10.1,:math:`\alpha_{os}` defined by :math:`N_{rm} = \alpha_{os} \times N_{M}`
0.10.1,where :math:`N_{rm}` and :math:`N_{M}` are the number of samples in the
0.10.1,minority class after resampling and the number of samples in the majority
0.10.1,"class, respectively."
0.10.1,%%
0.10.1,%% [markdown]
0.10.1,`sampling_strategy` as a `str`
0.10.1,-------------------------------
0.10.1,
0.10.1,`sampling_strategy` can be given as a string which specify the class
0.10.1,"targeted by the resampling. With under- and over-sampling, the number of"
0.10.1,samples will be equalized.
0.10.1,
0.10.1,Note that we are using multiple classes from now on.
0.10.1,%%
0.10.1,%% [markdown]
0.10.1,"With **cleaning method**, the number of samples in each class will not be"
0.10.1,equalized even if targeted.
0.10.1,%%
0.10.1,%% [markdown]
0.10.1,`sampling_strategy` as a `dict`
0.10.1,------------------------------
0.10.1,
0.10.1,"When `sampling_strategy` is a `dict`, the keys correspond to the targeted"
0.10.1,classes. The values correspond to the desired number of samples for each
0.10.1,targeted class. This is working for both **under- and over-sampling**
0.10.1,algorithms but not for the **cleaning algorithms**. Use a `list` instead.
0.10.1,%%
0.10.1,%% [markdown]
0.10.1,`sampling_strategy` as a `list`
0.10.1,-------------------------------
0.10.1,
0.10.1,"When `sampling_strategy` is a `list`, the list contains the targeted"
0.10.1,classes. It is used only for **cleaning methods** and raise an error
0.10.1,otherwise.
0.10.1,%%
0.10.1,%% [markdown]
0.10.1,`sampling_strategy` as a callable
0.10.1,---------------------------------
0.10.1,
0.10.1,"When callable, function taking `y` and returns a `dict`. The keys"
0.10.1,correspond to the targeted classes. The values correspond to the desired
0.10.1,number of samples for each class.
0.10.1,%%
0.10.1,List of whitelisted modules and methods; regexp are supported.
0.10.1,These docstrings will fail because they are inheriting from scikit-learn
0.10.1,skip private classes
0.10.1,"We ignore following error code,"
0.10.1,- RT02: The first line of the Returns section
0.10.1,"should contain only the type, .."
0.10.1,(as we may need refer to the name of the returned
0.10.1,object)
0.10.1,- GL01: Docstring text (summary) should start in the line
0.10.1,"immediately after the opening quotes (not in the same line,"
0.10.1,or leaving a blank line in between)
0.10.1,"- GL02: If there's a blank line, it should be before the"
0.10.1,"first line of the Returns section, not after (it allows to have"
0.10.1,short docstrings for properties).
0.10.1,Ignore PR02: Unknown parameters for properties. We sometimes use
0.10.1,"properties for ducktyping, i.e. SGDClassifier.predict_proba"
0.10.1,Following codes are only taken into account for the
0.10.1,top level class docstrings:
0.10.1,- ES01: No extended summary found
0.10.1,- SA01: See Also section not found
0.10.1,- EX01: No examples section found
0.10.1,In particular we can't parse the signature of properties
0.10.1,"When applied to classes, detect class method. For functions"
0.10.1,method = None.
0.10.1,TODO: this detection can be improved. Currently we assume that we have
0.10.1,class # methods if the second path element before last is in camel case.
0.10.1,'build' and 'install' is included to have structured metadata for CI.
0.10.1,It will NOT be included in setup's extras_require
0.10.1,"The values are (version_spec, comma separated tags)"
0.10.1,create inverse mapping for setuptools
0.10.1,Used by CI to get the min dependencies
0.10.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.10.1,License: MIT
0.10.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.10.1,Christos Aridas
0.10.1,License: MIT
0.10.1,scikit-learn >= 1.2
0.10.1,we need to overwrite SamplerMixin.fit to bypass the validation
0.10.1,Adapted from scikit-learn
0.10.1,Author: Edouard Duchesnay
0.10.1,Gael Varoquaux
0.10.1,Virgile Fritsch
0.10.1,Alexandre Gramfort
0.10.1,Lars Buitinck
0.10.1,Christos Aridas
0.10.1,Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.10.1,License: BSD
0.10.1,BaseEstimator interface
0.10.1,validate names
0.10.1,validate estimators
0.10.1,We allow last estimator to be None as an identity transformation
0.10.1,Estimator interface
0.10.1,Setup the memory
0.10.1,joblib >= 0.12
0.10.1,Fit or load from cache the current transformer
0.10.1,Replace the transformer of the step with the fitted
0.10.1,transformer. This is necessary when loading the transformer
0.10.1,from the cache.
0.10.1,This variable is injected in the __builtins__ by the build
0.10.1,process. It is used to enable importing subpackages of sklearn when
0.10.1,the binaries are not built
0.10.1,mypy error: Cannot determine type of '__SKLEARN_SETUP__'
0.10.1,We are not importing the rest of scikit-learn during the build
0.10.1,"process, as it may not be compiled yet"
0.10.1,"FIXME: When we get Python 3.7 as minimal version, we will need to switch to"
0.10.1,the following solution:
0.10.1,https://snarky.ca/lazy-importing-in-python-3-7/
0.10.1,Import the target module and insert it into the parent's namespace
0.10.1,Update this object's dict so that if someone keeps a reference to the
0.10.1,"LazyLoader, lookups are efficient (__getattr__ is only called on"
0.10.1,lookups that fail).
0.10.1,delay the import of keras since we are going to import either tensorflow
0.10.1,or keras
0.10.1,Based on NiLearn package
0.10.1,License: simplified BSD
0.10.1,"PEP0440 compatible formatted version, see:"
0.10.1,https://www.python.org/dev/peps/pep-0440/
0.10.1,
0.10.1,Generic release markers:
0.10.1,X.Y
0.10.1,X.Y.Z # For bugfix releases
0.10.1,
0.10.1,Admissible pre-release markers:
0.10.1,X.YaN # Alpha release
0.10.1,X.YbN # Beta release
0.10.1,X.YrcN # Release Candidate
0.10.1,X.Y # Final release
0.10.1,
0.10.1,Dev branch marker is: 'X.Y.dev' or 'X.Y.devN' where N is an integer.
0.10.1,'X.Y.dev0' is the canonical version of 'X.Y.dev'
0.10.1,
0.10.1,coding: utf-8
0.10.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.10.1,Dariusz Brzezinski
0.10.1,License: MIT
0.10.1,Only negative labels
0.10.1,"Calculate tp_sum, pred_sum, true_sum ###"
0.10.1,labels are now from 0 to len(labels) - 1 -> use bincount
0.10.1,Pathological case
0.10.1,Compute the true negative
0.10.1,Retain only selected labels
0.10.1,"Finally, we have all our sufficient statistics. Divide! #"
0.10.1,"Divide, and on zero-division, set scores to 0 and warn:"
0.10.1,"Oddly, we may get an ""invalid"" rather than a ""divide"" error"
0.10.1,here.
0.10.1,Average the results
0.10.1,labels are now from 0 to len(labels) - 1 -> use bincount
0.10.1,Pathological case
0.10.1,Retain only selected labels
0.10.1,old version of scipy return MaskedConstant instead of 0.0
0.10.1,check that the scoring function does not need a score
0.10.1,and only a prediction
0.10.1,We do not support multilabel so the only average supported
0.10.1,is binary
0.10.1,Compute the different metrics
0.10.1,Precision/recall/f1
0.10.1,Specificity
0.10.1,Geometric mean
0.10.1,Index balanced accuracy
0.10.1,compute averages
0.10.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.10.1,License: MIT
0.10.1,categories are expected to be encoded from 0 to n_categories - 1
0.10.1,"list of length n_features of ndarray (n_categories, n_classes)"
0.10.1,compute the counts
0.10.1,normalize by the summing over the classes
0.10.1,silence potential warning due to in-place division by zero
0.10.1,coding: utf-8
0.10.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.10.1,Christos Aridas
0.10.1,License: MIT
0.10.1,##############################################################################
0.10.1,Utilities for testing
0.10.1,import some data to play with
0.10.1,restrict to a binary classification task
0.10.1,add noisy features to make the problem harder and avoid perfect results
0.10.1,"run classifier, get class probabilities and label predictions"
0.10.1,only interested in probabilities of the positive case
0.10.1,XXX: do we really want a special API for the binary case?
0.10.1,##############################################################################
0.10.1,Tests
0.10.1,detailed measures for each class
0.10.1,individual scoring function that can be used for grid search: in the
0.10.1,binary class case the score is the value of the measure for the positive
0.10.1,class (e.g. label == 1). This is deprecated for average != 'binary'.
0.10.1,Such a case may occur with non-stratified cross-validation
0.10.1,ensure the above were meaningful tests:
0.10.1,Bad pos_label
0.10.1,Bad average option
0.10.1,but average != 'binary'; even if data is binary
0.10.1,compute the geometric mean for the binary problem
0.10.1,print classification report with class names
0.10.1,print classification report with label detection
0.10.1,print classification report with class names
0.10.1,print classification report with label detection
0.10.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.10.1,License: MIT
0.10.1,Check basic feature of the metric:
0.10.1,"* the shape of the distance matrix is (n_samples, n_samples)"
0.10.1,* computing pairwise distance of X is the same than explicitely between
0.10.1,X and X.
0.10.1,Check the property of the vdm distance. Let's check the property
0.10.1,"described in ""Improved Heterogeneous Distance Functions"", D.R. Wilson and"
0.10.1,"T.R. Martinez, Journal of Artificial Intelligence Research 6 (1997) 1-34"
0.10.1,https://arxiv.org/pdf/cs/9701101.pdf
0.10.1,
0.10.1,"""if an attribute color has three values red, green and blue, and the"
0.10.1,"application is to identify whether or not an object is an apple, red and"
0.10.1,green would be considered closer than red and blue because the former two
0.10.1,"both have similar correlations with the output class apple."""
0.10.1,defined our feature
0.10.1,0 - not an apple / 1 - an apple
0.10.1,computing the distance between a sample of the same category should
0.10.1,give a null distance
0.10.1,check the property explained in the introduction example
0.10.1,green and red are very close
0.10.1,blue is closer to red than green
0.10.1,"Check that ""auto"" is equivalent to provide the number categories"
0.10.1,beforehand
0.10.1,Check that we raise an error if n_categories is inconsistent with the
0.10.1,number of features in X
0.10.1,Check that we don't get issue when a category is missing between 0
0.10.1,n_categories - 1
0.10.1,remove a categories that could be between 0 and n_categories
0.10.1,Check that we raise a NotFittedError when `fit` is not not called before
0.10.1,pairwise.
0.10.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.10.1,Christos Aridas
0.10.1,License: MIT
0.10.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.10.1,Christos Aridas
0.10.1,License: MIT
0.10.1,FIXME: to be removed in 0.12
0.10.1,The ratio is computed using a one-vs-rest manner. Using majority
0.10.1,in multi-class would lead to slightly different results at the
0.10.1,cost of introducing a new parameter.
0.10.1,rounding may cause new amount for n_samples
0.10.1,the nearest neighbors need to be fitted only on the current class
0.10.1,to find the class NN to generate new samples
0.10.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.10.1,Christos Aridas
0.10.1,License: MIT
0.10.1,smoothed bootstrap imposes to make numerical operation; we need
0.10.1,to be sure to have only numerical data in X
0.10.1,generate a smoothed bootstrap with a perturbation
0.10.1,generate a bootstrap
0.10.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.10.1,Christos Aridas
0.10.1,License: MIT
0.10.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.10.1,Fernando Nogueira
0.10.1,Christos Aridas
0.10.1,License: MIT
0.10.1,negate diagonal elements
0.10.1,identify cluster which are answering the requirements
0.10.1,the cluster is already considered balanced
0.10.1,not enough samples to apply SMOTE
0.10.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.10.1,Fernando Nogueira
0.10.1,Christos Aridas
0.10.1,Dzianis Dudnik
0.10.1,License: MIT
0.10.1,FIXME: to be removed in 0.12
0.10.1,divergence between borderline-1 and borderline-2
0.10.1,Create synthetic samples for borderline points.
0.10.1,only minority
0.10.1,we use a one-vs-rest policy to handle the multiclass in which
0.10.1,new samples will be created considering not only the majority
0.10.1,class but all over classes.
0.10.1,FIXME: to be removed in 0.12
0.10.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.10.1,Fernando Nogueira
0.10.1,Christos Aridas
0.10.1,Dzianis Dudnik
0.10.1,License: MIT
0.10.1,np.newaxis for backwards compatability with random_state
0.10.1,Samples are in danger for m/2 <= m' < m
0.10.1,Samples are noise for m = m'
0.10.1,FIXME: to be removed in 0.12
0.10.1,FIXME: to be removed in 0.12
0.10.1,compute the median of the standard deviation of the minority class
0.10.1,scikit-learn >= 1.2
0.10.1,the input of the OneHotEncoder needs to be dense
0.10.1,we can replace the 1 entries of the categorical features with the
0.10.1,median of the standard deviation. It will ensure that whenever
0.10.1,"distance is computed between 2 samples, the difference will be equal"
0.10.1,to the median of the standard deviation as in the original paper.
0.10.1,"In the edge case where the median of the std is equal to 0, the 1s"
0.10.1,"entries will be also nullified. In this case, we store the original"
0.10.1,categorical encoding which will be later used for inversing the OHE
0.10.1,reverse the encoding of the categorical features
0.10.1,the matrix is supposed to be in the CSR format after the stacking
0.10.1,change in sparsity structure more efficient with LIL than CSR
0.10.1,convert to dense array since scipy.sparse doesn't handle 3D
0.10.1,"In the case that the median std was equal to zeros, we have to"
0.10.1,create non-null entry based on the encoded of OHE
0.10.1,tie breaking argmax
0.10.1,generate sample indices that will be used to generate new samples
0.10.1,"for each drawn samples, select its k-neighbors and generate a sample"
0.10.1,"where for each feature individually, each category generated is the"
0.10.1,most common category
0.10.1,FIXME: to be removed in 0.12
0.10.1,the kneigbors search will include the sample itself which is
0.10.1,expected from the original algorithm
0.10.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.10.1,Christos Aridas
0.10.1,Dzianis Dudnik
0.10.1,License: MIT
0.10.1,create 2 random continuous feature
0.10.1,create a categorical feature using some string
0.10.1,create a categorical feature using some integer
0.10.1,return the categories
0.10.1,create 2 random continuous feature
0.10.1,create a categorical feature using some string
0.10.1,create a categorical feature using some integer
0.10.1,return the categories
0.10.1,create 2 random continuous feature
0.10.1,create a categorical feature using some string
0.10.1,create a categorical feature using some integer
0.10.1,return the categories
0.10.1,create 2 random continuous feature
0.10.1,create a categorical feature using some string
0.10.1,create a categorical feature using some integer
0.10.1,return the categories
0.10.1,create 2 random continuous feature
0.10.1,create a categorical feature using some string
0.10.1,create a categorical feature using some integer
0.10.1,part of the common test which apply to SMOTE-NC even if it is not default
0.10.1,constructible
0.10.1,Check that the samplers handle pandas dataframe and pandas series
0.10.1,FIXME: we should use to_numpy with pandas >= 0.25
0.10.1,Cast X and y to not default dtype
0.10.1,Non-regression test for #662
0.10.1,https://github.com/scikit-learn-contrib/imbalanced-learn/issues/662
0.10.1,check that the categorical feature is not random but correspond to the
0.10.1,categories seen in the minority class samples
0.10.1,overall check for SMOTEN
0.10.1,check if the SMOTEN resample data as expected
0.10.1,"we generate data such that ""not apple"" will be the minority class and"
0.10.1,"samples from this class will be generated. We will force the ""blue"""
0.10.1,"category to be associated with this class. Therefore, the new generated"
0.10.1,"samples should as well be from the ""blue"" category."
0.10.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.10.1,Christos Aridas
0.10.1,License: MIT
0.10.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.10.1,Christos Aridas
0.10.1,License: MIT
0.10.1,FIXME: we should use to_numpy with pandas >= 0.25
0.10.1,check the random over-sampling with a multiclass problem
0.10.1,check that resampling with heterogeneous dtype is working with basic
0.10.1,resampling
0.10.1,check that we can oversample even with missing or infinite data
0.10.1,regression tests for #605
0.10.1,check that we raise an error when heterogeneous dtype data are given
0.10.1,and a smoothed bootstrap is requested
0.10.1,check that smoothed bootstrap is working for numerical array
0.10.1,check that a shrinkage factor of 0 is equivalent to not create a smoothed
0.10.1,bootstrap
0.10.1,check the behaviour of the shrinkage parameter
0.10.1,the covariance of the data generated with the larger shrinkage factor
0.10.1,should also be larger.
0.10.1,check the validation of the shrinkage parameter
0.10.1,check that m_neighbors is properly set. Regression test for:
0.10.1,https://github.com/scikit-learn-contrib/imbalanced-learn/issues/568
0.10.1,FIXME: to be removed in 0.12
0.10.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.10.1,Christos Aridas
0.10.1,License: MIT
0.10.1,shuffle the indices since the sampler are packing them by class
0.10.1,helper functions
0.10.1,input and output
0.10.1,build the model and weights
0.10.1,"build the loss, predict, and train operator"
0.10.1,Initialization of all variables in the graph
0.10.1,"For each epoch, run accuracy on train and test"
0.10.1,helper functions
0.10.1,input and output
0.10.1,build the model and weights
0.10.1,"build the loss, predict, and train operator"
0.10.1,Initialization of all variables in the graph
0.10.1,"For each epoch, run accuracy on train and test"
0.10.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.10.1,License: MIT
0.10.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.10.1,Fernando Nogueira
0.10.1,Christos Aridas
0.10.1,License: MIT
0.10.1,find which class to not consider
0.10.1,there is a Tomek link between two samples if they are both nearest
0.10.1,neighbors of each others.
0.10.1,Find the nearest neighbour of every point
0.10.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.10.1,Christos Aridas
0.10.1,License: MIT
0.10.1,Randomly get one sample from the majority class
0.10.1,Generate the index to select
0.10.1,Create the set C - One majority samples and all minority
0.10.1,Create the set S - all majority samples
0.10.1,fit knn on C
0.10.1,Check each sample in S if we keep it or drop it
0.10.1,Do not select sample which are already well classified
0.10.1,Classify on S
0.10.1,If the prediction do not agree with the true label
0.10.1,append it in C_x
0.10.1,Keep the index for later
0.10.1,Update C
0.10.1,fit a knn on C
0.10.1,This experimental to speed up the search
0.10.1,Classify all the element in S and avoid to test the
0.10.1,well classified elements
0.10.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.10.1,Dayvid Oliveira
0.10.1,Christos Aridas
0.10.1,License: MIT
0.10.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.10.1,Christos Aridas
0.10.1,License: MIT
0.10.1,Compute the distance considering the farthest neighbour
0.10.1,Sort the list of distance and get the index
0.10.1,Throw a warning to tell the user that we did not have enough samples
0.10.1,to select and that we just select everything
0.10.1,Select the desired number of samples
0.10.1,idx_tmp is relative to the feature selected in the
0.10.1,previous step and we need to find the indirection
0.10.1,fmt: off
0.10.1,fmt: on
0.10.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.10.1,Christos Aridas
0.10.1,License: MIT
0.10.1,select a sample from the current class
0.10.1,create the set composed of all minority samples and one
0.10.1,sample from the current class.
0.10.1,create the set S with removing the seed from S
0.10.1,since that it will be added anyway
0.10.1,apply Tomek cleaning
0.10.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.10.1,Dayvid Oliveira
0.10.1,Christos Aridas
0.10.1,License: MIT
0.10.1,Check the stopping criterion
0.10.1,1. If there is no changes for the vector y
0.10.1,2. If the number of samples in the other class become inferior to
0.10.1,the number of samples in the majority class
0.10.1,3. If one of the class is disappearing
0.10.1,Case 1
0.10.1,Case 2
0.10.1,Case 3
0.10.1,Check the stopping criterion
0.10.1,1. If the number of samples in the other class become inferior to
0.10.1,the number of samples in the majority class
0.10.1,2. If one of the class is disappearing
0.10.1,Case 1else:
0.10.1,overwrite b_min_bec_maj
0.10.1,Case 2
0.10.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.10.1,Christos Aridas
0.10.1,License: MIT
0.10.1,clean the neighborhood
0.10.1,compute which classes to consider for cleaning for the A2 group
0.10.1,compute a2 group
0.10.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.10.1,Christos Aridas
0.10.1,License: MIT
0.10.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.10.1,Christos Aridas
0.10.1,License: MIT
0.10.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.10.1,Christos Aridas
0.10.1,License: MIT
0.10.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.10.1,Christos Aridas
0.10.1,License: MIT
0.10.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.10.1,Christos Aridas
0.10.1,License: MIT
0.10.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.10.1,Christos Aridas
0.10.1,License: MIT
0.10.1,FIXME: we should use to_numpy with pandas >= 0.25
0.10.1,check that we can undersample even with missing or infinite data
0.10.1,regression tests for #605
0.10.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.10.1,Christos Aridas
0.10.1,License: MIT
0.10.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.10.1,Christos Aridas
0.10.1,License: MIT
0.10.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.10.1,Christos Aridas
0.10.1,License: MIT
0.10.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.10.1,Christos Aridas
0.10.1,License: MIT
0.10.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.10.1,Christos Aridas
0.10.1,License: MIT
0.10.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.10.1,Fernando Nogueira
0.10.1,Christos Aridas
0.10.1,License: MIT
0.10.1,check that the samples selecting by the hard voting corresponds to the
0.10.1,targeted class
0.10.1,non-regression test for:
0.10.1,https://github.com/scikit-learn-contrib/imbalanced-learn/issues/738
0.10.1,Generate valid values for the required parameters
0.10.1,The parameters `*args` and `**kwargs` are ignored since we cannot generate
0.10.1,constraints.
0.10.1,check that there is a constraint for each parameter
0.10.1,this object does not have a valid type for sure for all params
0.10.1,This parameter is not validated
0.10.1,"First, check that the error is raised if param doesn't match any valid type."
0.10.1,"Then, for constraints that are more than a type constraint, check that the"
0.10.1,error is raised if param does match a valid type but does not match any valid
0.10.1,value for this type.
0.10.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.10.1,Christos Aridas
0.10.1,License: MIT
0.10.1,test that all_estimators doesn't find abstract classes.
0.10.1,"For NearMiss, let's check the three algorithms"
0.10.1,Common tests for estimator instances
0.10.1,Authors: Alexandre Gramfort <alexandre.gramfort@inria.fr>
0.10.1,Raghav RV <rvraghav93@gmail.com>
0.10.1,License: BSD 3 clause
0.10.1,scikit-learn >= 1.2
0.10.1,"walk_packages() ignores DeprecationWarnings, now we need to ignore"
0.10.1,FutureWarnings
0.10.1,"mypy error: Module has no attribute ""__path__"""
0.10.1,functions to ignore args / docstring of
0.10.1,Methods where y param should be ignored if y=None by default
0.10.1,numpydoc 0.8.0's docscrape tool raises because of collections.abc under
0.10.1,Python 3.7
0.10.1,Test module docstring formatting
0.10.1,Skip test if numpydoc is not found
0.10.1,XXX unreached code as of v0.22
0.10.1,"pytest tooling, not part of the scikit-learn API"
0.10.1,Exclude non-scikit-learn classes
0.10.1,Now skip docstring test for y when y is None
0.10.1,by default for API reason
0.10.1,Exclude imported functions
0.10.1,Don't test private methods / functions
0.10.1,Test that there are no tabs in our source files
0.10.1,because we don't import
0.10.1,Minimal / degenerate instances: only useful to test the docstrings.
0.10.1,"As certain attributes are present ""only"" if a certain parameter is"
0.10.1,"provided, this checks if the word ""only"" is present in the attribute"
0.10.1,"description, and if not the attribute is required to be present."
0.10.1,ignore deprecation warnings
0.10.1,attributes
0.10.1,properties
0.10.1,ignore properties that raises an AttributeError and deprecated
0.10.1,properties
0.10.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.10.1,License: MIT
0.10.1,check that we can let a pass a regression variable by turning down the
0.10.1,validation
0.10.1,Check that the validation is bypass when calling `fit`
0.10.1,Non-regression test for:
0.10.1,https://github.com/scikit-learn-contrib/imbalanced-learn/issues/782
0.10.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.10.1,Christos Aridas
0.10.1,License: MIT
0.10.1,store timestamp to figure out whether the result of 'fit' has been
0.10.1,cached or not
0.10.1,store timestamp to figure out whether the result of 'fit' has been
0.10.1,cached or not
0.10.1,Pipeline accepts steps as tuple
0.10.1,Test the various init parameters of the pipeline.
0.10.1,Check that we can't instantiate pipelines with objects without fit
0.10.1,method
0.10.1,Smoke test with only an estimator
0.10.1,Check that params are set
0.10.1,Smoke test the repr:
0.10.1,Test with two objects
0.10.1,Check that we can't instantiate with non-transformers on the way
0.10.1,"Note that NoTrans implements fit, but not transform"
0.10.1,Check that params are set
0.10.1,Smoke test the repr:
0.10.1,Check that params are not set when naming them wrong
0.10.1,Test clone
0.10.1,"Check that apart from estimators, the parameters are the same"
0.10.1,Remove estimators that where copied
0.10.1,Test the various methods of the pipeline (anova).
0.10.1,Test with Anova + LogisticRegression
0.10.1,Test that the pipeline can take fit parameters
0.10.1,classifier should return True
0.10.1,and transformer params should not be changed
0.10.1,invalid parameters should raise an error message
0.10.1,Pipeline should pass sample_weight
0.10.1,When sample_weight is None it shouldn't be passed
0.10.1,Test pipeline raises set params error message for nested models.
0.10.1,nested model check
0.10.1,Test the various methods of the pipeline (pca + svm).
0.10.1,Test with PCA + SVC
0.10.1,Test the various methods of the pipeline (preprocessing + svm).
0.10.1,check shapes of various prediction functions
0.10.1,test that the fit_predict method is implemented on a pipeline
0.10.1,test that the fit_predict on pipeline yields same results as applying
0.10.1,transform and clustering steps separately
0.10.1,"As pipeline doesn't clone estimators on construction,"
0.10.1,it must have its own estimators
0.10.1,first compute the transform and clustering step separately
0.10.1,use a pipeline to do the transform and clustering in one step
0.10.1,tests that a pipeline does not have fit_predict method when final
0.10.1,step of pipeline does not have fit_predict defined
0.10.1,tests that Pipeline passes fit_params to intermediate steps
0.10.1,when fit_predict is invoked
0.10.1,Test whether pipeline works with a transformer at the end.
0.10.1,Also test pipeline.transform and pipeline.inverse_transform
0.10.1,test transform and fit_transform:
0.10.1,Test whether pipeline works with a transformer missing fit_transform
0.10.1,test fit_transform:
0.10.1,Directly setting attr
0.10.1,Using set_params
0.10.1,Using set_params to replace single step
0.10.1,With invalid data
0.10.1,Test setting Pipeline steps to None
0.10.1,"for other methods, ensure no AttributeErrors on None:"
0.10.1,mult2 and mult3 are active
0.10.1,Check 'passthrough' step at construction time
0.10.1,Test with Transformer + SVC
0.10.1,Memoize the transformer at the first fit
0.10.1,Get the time stamp of the tranformer in the cached pipeline
0.10.1,Check that cached_pipe and pipe yield identical results
0.10.1,Check that we are reading the cache while fitting
0.10.1,a second time
0.10.1,Check that cached_pipe and pipe yield identical results
0.10.1,Create a new pipeline with cloned estimators
0.10.1,Check that even changing the name step does not affect the cache hit
0.10.1,Check that cached_pipe and pipe yield identical results
0.10.1,Test with Transformer + SVC
0.10.1,Memoize the transformer at the first fit
0.10.1,Get the time stamp of the tranformer in the cached pipeline
0.10.1,Check that cached_pipe and pipe yield identical results
0.10.1,Check that we are reading the cache while fitting
0.10.1,a second time
0.10.1,Check that cached_pipe and pipe yield identical results
0.10.1,Create a new pipeline with cloned estimators
0.10.1,Check that even changing the name step does not affect the cache hit
0.10.1,Check that cached_pipe and pipe yield identical results
0.10.1,Test the various methods of the pipeline (pca + svm).
0.10.1,Test with PCA + SVC
0.10.1,Test the various methods of the pipeline (pca + svm).
0.10.1,Test with PCA + SVC
0.10.1,Test whether pipeline works with a sampler at the end.
0.10.1,Also test pipeline.sampler
0.10.1,test transform and fit_transform:
0.10.1,We round the value near to zero. It seems that PCA has some issue
0.10.1,with that
0.10.1,Test whether pipeline works with a sampler at the end.
0.10.1,Also test pipeline.sampler
0.10.1,Test pipeline using None as preprocessing step and a classifier
0.10.1,"Test pipeline using None, RUS and a classifier"
0.10.1,"Test pipeline using RUS, None and a classifier"
0.10.1,Test pipeline using None step and a sampler
0.10.1,Test pipeline using None and a transformer that implements transform and
0.10.1,inverse_transform
0.10.1,Test the various methods of the pipeline (anova).
0.10.1,Test with RandomUnderSampling + Anova + LogisticRegression
0.10.1,Test the various methods of the pipeline (anova).
0.10.1,Test the various methods of the pipeline (anova).
0.10.1,Test with RandomUnderSampling + Anova + LogisticRegression
0.10.1,tests that Pipeline passes predict_params to the final estimator
0.10.1,when predict is invoked
0.10.1,Test that the score_samples method is implemented on a pipeline.
0.10.1,Test that the score_samples method on pipeline yields same results as
0.10.1,applying transform and score_samples steps separately.
0.10.1,Check the shapes
0.10.1,Check the values
0.10.1,Test that a pipeline does not have score_samples method when the final
0.10.1,step of the pipeline does not have score_samples defined.
0.10.1,Test that the score_samples method is implemented on a pipeline.
0.10.1,Test that the score_samples method on pipeline yields same results as
0.10.1,applying transform and score_samples steps separately.
0.10.1,Check the shapes
0.10.1,Check the values
0.10.1,transformer will not change `y` and sampler will always preserve the type of `y`
0.10.1,transformer will not change `y` and sampler will always preserve the type of `y`
0.10.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.10.1,Christos Aridas
0.10.1,License: MIT
0.10.1,TODO: Remove when SciPy 1.9 is the minimum supported version
0.10.1,TODO: Remove when scikit-learn 1.1 is the minimum supported version
0.10.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.10.1,License: MIT
0.10.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.10.1,License: MIT
0.10.1,Adapated from scikit-learn
0.10.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.10.1,License: MIT
0.10.1,scikit-learn >= 1.2
0.10.1,we don't filter samplers based on their tag here because we want to make
0.10.1,sure that the fitted attribute does not exist if the tag is not
0.10.1,stipulated
0.10.1,trigger our checks if this is a SamplerMixin
0.10.1,should raise warning if the target is continuous (we cannot raise error)
0.10.1,if the target is multilabel then we should raise an error
0.10.1,IHT does not enforce the number of samples but provide a number
0.10.1,of samples the closest to the desired target.
0.10.1,in this test we will force all samplers to not change the class 1
0.10.1,check that sparse matrices can be passed through the sampler leading to
0.10.1,the same results than dense
0.10.1,Check that the samplers handle pandas dataframe and pandas series
0.10.1,check that we return the same type for dataframes or series types
0.10.1,FIXME: we should use to_numpy with pandas >= 0.25
0.10.1,Check that the can samplers handle simple lists
0.10.1,Check that multiclass target lead to the same results than OVA encoding
0.10.1,Cast X and y to not default dtype
0.10.1,Non-regression test for #709
0.10.1,https://github.com/scikit-learn-contrib/imbalanced-learn/issues/709
0.10.1,Check that an informative error is raised when the value of a constructor
0.10.1,parameter does not have an appropriate type or value.
0.10.1,check that there is a constraint for each parameter
0.10.1,this object does not have a valid type for sure for all params
0.10.1,This parameter is not validated
0.10.1,"First, check that the error is raised if param doesn't match any valid type."
0.10.1,the method is not accessible with the current set of parameters
0.10.1,The estimator is a label transformer and take only `y`
0.10.1,"Then, for constraints that are more than a type constraint, check that the"
0.10.1,error is raised if param does match a valid type but does not match any valid
0.10.1,value for this type.
0.10.1,the method is not accessible with the current set of parameters
0.10.1,The estimator is a label transformer and take only `y`
0.10.1,Check that calling `fit` does not raise any warnings about feature names.
0.10.1,Only check imblearn estimators for feature_names_in_ in docstring
0.10.1,partial_fit checks on second call
0.10.1,Do not call partial fit if early_stopping is on
0.10.1,input_features names is not the same length as n_features_in_
0.10.1,error is raised when `input_features` do not match feature_names_in
0.10.1,Adapted from scikit-learn
0.10.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.10.1,License: MIT
0.10.1,Ignore deprecation warnings triggered at import time and from walking
0.10.1,packages
0.10.1,get rid of abstract base classes
0.10.1,get rid of sklearn estimators which have been imported in some classes
0.10.1,"drop duplicates, sort for reproducibility"
0.10.1,itemgetter is used to ensure the sort does not extend to the 2nd item of
0.10.1,the tuple
0.10.1,mypy: ignore-errors
0.10.1,update the docstring of the descriptor
0.10.1,"delegate only on instances, not the classes."
0.10.1,this is to allow access to the docstrings.
0.10.1,This makes it possible to use the decorated method as an
0.10.1,"unbound method, for instance when monkeypatching."
0.10.1,mypy: ignore-errors
0.10.1,TODO: remove `if True` when we have clear support for:
0.10.1,- ignoring `*args` and `**kwargs` in the signature
0.10.1,We allow parameters to not have a constraint so that third party
0.10.1,estimators can inherit from sklearn estimators without having to
0.10.1,necessarily use the validation tools.
0.10.1,"this constraint is satisfied, no need to check further."
0.10.1,"No constraint is satisfied, raise with an informative message."
0.10.1,"Ignore constraints that we don't want to expose in the error message,"
0.10.1,i.e. options that are for internal purpose or not officially
0.10.1,supported.
0.10.1,The dict of parameter constraints is set as an attribute of the function
0.10.1,to make it possible to dynamically introspect the constraints for
0.10.1,automatic testing.
0.10.1,Map *args/**kwargs to the function signature
0.10.1,ignore self/cls and positional/keyword markers
0.10.1,TODO(1.4) remove support for Integral.
0.10.1,we use an interval of Real to ignore np.nan that has its own
0.10.1,constraint
0.10.1,constraint is an interval
0.10.1,generate a non-integer value such that it can't be valid even if there's
0.10.1,also an integer interval constraint.
0.10.1,We need to check if there's also a real interval constraint to generate a
0.10.1,value that is not valid for any of the 2 interval constraints.
0.10.1,Only the integer interval constraint -> easy
0.10.1,There's also a real interval constraint. Try to find a value left to both
0.10.1,or right to both or in between them.
0.10.1,redefine left and right bounds to be smallest and largest valid integers
0.10.1,in both intervals.
0.10.1,there exists an int left to both intervals
0.10.1,there exists an int right to both intervals
0.10.1,there exists an int between the 2 intervals
0.10.1,there exists an int between the 2 intervals
0.10.1,Author: Alexander L. Hayes <hayesall@iu.edu>
0.10.1,License: MIT
0.10.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.10.1,License: MIT
0.10.1,_is_neighbors_object(nn_object)
0.10.1,check that all keys in sampling_strategy are also in y
0.10.1,check that there is no negative number
0.10.1,check that all keys in sampling_strategy are also in y
0.10.1,ignore first 'self' argument for instance methods
0.10.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.10.1,Christos Aridas
0.10.1,License: MIT
0.10.1,this function could create an equal number of samples
0.10.1,We pass on purpose a non sorted dictionary and check that the resulting
0.10.1,dictionary is sorted. Refer to issue #428.
0.10.1,DataFrame and DataFrame case
0.10.1,DataFrames and Series case
0.10.1,The * is place before a keyword only argument without a default value
0.10.1,Test that the minimum dependencies in the README.rst file are
0.10.1,consistent with the minimum dependencies defined at the file:
0.10.1,imblearn/_min_dependencies.py
0.10.1,Skip the test if the README.rst file is not available.
0.10.1,"For instance, when installing scikit-learn from wheels"
0.10.1,Author: Alexander L. Hayes <hayesall@iu.edu>
0.10.1,License: MIT
0.10.1,Some helpers for the tests
0.10.1,check in the presence of extra positional and keyword args
0.10.1,outer decorator does not interfer with validation
0.10.1,validated method can be decorated
0.10.1,no validation in init
0.10.1,list and dict are valid params
0.10.1,the list option is not exposed in the error message
0.10.1,"""auto"" and ""warn"" are valid params"
0.10.1,"the ""warn"" option is not exposed in the error message"
0.10.1,True/False and np.bool_(True/False) are valid params
0.10.1,an int is also valid but deprecated
0.10.1,param1 is validated
0.10.1,param2 is not validated: any type is valid.
0.10.1,"does not raise, even though ""b"" is not in the constraints dict and ""a"" is not"
0.10.1,a parameter of the estimator.
0.10.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.10.1,License: MIT
0.10.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.10.1,Christos Aridas
0.10.1,License: MIT
0.10.1,check if the filtering is working with a list or a single string
0.10.1,check that all estimators are sampler
0.10.1,check that an error is raised when the type is unknown
0.10.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.10.1,License: MIT
0.10.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.10.1,Christos Aridas
0.10.1,License: MIT
0.10.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.10.1,Christos Aridas
0.10.1,License: MIT
0.10.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.10.1,Christos Aridas
0.10.1,License: MIT
0.10.1,Check if default job count is None
0.10.1,Check if job count is set
0.10.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.10.1,Christos Aridas
0.10.1,License: MIT
0.10.1,Check if default job count is none
0.10.1,Check if job count is set
0.10.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.10.1,License: MIT
0.10.1,resample before to fit the tree
0.10.1,TODO: remove when the minimum version of scikit-learn supported is 1.1
0.10.1,make a deepcopy to not modify the original dictionary
0.10.1,scikit-learn >= 1.2
0.10.1,scikit-learn < 1.2
0.10.1,Validate or convert input data
0.10.1,Pre-sort indices to avoid that each individual tree of the
0.10.1,ensemble sorts the indices.
0.10.1,reshape is necessary to preserve the data contiguity against vs
0.10.1,"[:, np.newaxis] that does not."
0.10.1,Get bootstrap sample size
0.10.1,Check parameters
0.10.1,"Free allocated memory, if any"
0.10.1,We draw from the random state to get the random state we
0.10.1,would have got if we hadn't used a warm_start.
0.10.1,Parallel loop: we prefer the threading backend as the Cython code
0.10.1,for fitting the trees is internally releasing the Python GIL
0.10.1,making threading more efficient than multiprocessing in
0.10.1,"that case. However, we respect any parallel_backend contexts set"
0.10.1,"at a higher level, since correctness does not rely on using"
0.10.1,threads.
0.10.1,Collect newly grown trees
0.10.1,Create pipeline with the fitted samplers and trees
0.10.1,FIXME: we could consider to support multiclass-multioutput if
0.10.1,we introduce or reuse a constructor parameter (e.g.
0.10.1,oob_score) allowing our user to pass a callable defining the
0.10.1,scoring strategy on OOB sample.
0.10.1,Decapsulate classes_ attributes
0.10.1,drop the n_outputs axis if there is a single output
0.10.1,Prediction requires X to be in CSR format
0.10.1,n_classes_ is a ndarray at this stage
0.10.1,all the supported type of target will have the same number of
0.10.1,classes in all outputs
0.10.1,"for regression, n_classes_ does not exist and we create an empty"
0.10.1,axis to be consistent with the classification case and make
0.10.1,the array operations compatible with the 2 settings
0.10.1,TODO: remove when supporting scikit-learn>=1.4
0.10.1,TODO: remove when supporting scikit-learn>=1.2
0.10.1,make a deepcopy to not modify the original dictionary
0.10.1,scikit-learn >= 1.2
0.10.1,TODO: remove when supporting scikit-learn>=1.2
0.10.1,scikit-learn < 1.2
0.10.1,SAMME-R requires predict_proba-enabled estimators
0.10.1,Instances incorrectly classified
0.10.1,Error fraction
0.10.1,Stop if classification is perfect
0.10.1,Construct y coding as described in Zhu et al [2]:
0.10.1,
0.10.1,y_k = 1 if c == k else -1 / (K - 1)
0.10.1,
0.10.1,"where K == n_classes_ and c, k in [0, K) are indices along the second"
0.10.1,axis of the y coding with c being the index corresponding to the true
0.10.1,class label.
0.10.1,Displace zero probabilities so the log is defined.
0.10.1,Also fix negative elements which may occur with
0.10.1,negative sample weights.
0.10.1,Boost weight using multi-class AdaBoost SAMME.R alg
0.10.1,Only boost the weights if it will fit again
0.10.1,Only boost positive weights
0.10.1,Instances incorrectly classified
0.10.1,Error fraction
0.10.1,Stop if classification is perfect
0.10.1,Stop if the error is at least as bad as random guessing
0.10.1,Boost weight using multi-class AdaBoost SAMME alg
0.10.1,Only boost the weights if I will fit again
0.10.1,Only boost positive weights
0.10.1,TODO: remove when supporting scikit-learn>=1.4
0.10.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.10.1,Christos Aridas
0.10.1,License: MIT
0.10.1,make a deepcopy to not modify the original dictionary
0.10.1,scikit-learn >= 1.2
0.10.1,TODO: remove when supporting scikit-learn>=1.2
0.10.1,scikit-learn < 1.2
0.10.1,TODO: remove when supporting scikit-learn>=1.4
0.10.1,TODO: remove when supporting scikit-learn>=1.2
0.10.1,overwrite the base class method by disallowing `sample_weight`
0.10.1,RandomUnderSampler is not supporting sample_weight. We need to pass
0.10.1,None.
0.10.1,TODO: remove when minimum supported version of scikit-learn is 1.1
0.10.1,Check data
0.10.1,Parallel loop
0.10.1,Reduce
0.10.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.10.1,Christos Aridas
0.10.1,License: MIT
0.10.1,make a deepcopy to not modify the original dictionary
0.10.1,scikit-learn >= 1.2
0.10.1,TODO: remove when supporting scikit-learn>=1.2
0.10.1,scikit-learn < 1.2
0.10.1,TODO: remove when supporting scikit-learn>=1.4
0.10.1,TODO: remove when supporting scikit-learn>=1.2
0.10.1,overwrite the base class method by disallowing `sample_weight`
0.10.1,the sampler needs to be validated before to call _fit because
0.10.1,_validate_y is called before _validate_estimator and would require
0.10.1,to know which type of sampler we are using.
0.10.1,RandomUnderSampler is not supporting sample_weight. We need to pass
0.10.1,None.
0.10.1,TODO: remove when minimum supported version of scikit-learn is 1.1
0.10.1,Check data
0.10.1,Parallel loop
0.10.1,Reduce
0.10.1,check that we have an ensemble of samplers and estimators with a
0.10.1,consistent size
0.10.1,each sampler in the ensemble should have different random state
0.10.1,each estimator in the ensemble should have different random state
0.10.1,check the consistency of the feature importances
0.10.1,check the consistency of the prediction outpus
0.10.1,Predictions should be the same when sample_weight are all ones
0.10.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.10.1,Christos Aridas
0.10.1,License: MIT
0.10.1,Check classification for various parameter settings.
0.10.1,Test that bootstrapping samples generate non-perfect base estimators.
0.10.1,"without bootstrap, all trees are perfect on the training set"
0.10.1,disable the resampling by passing an empty dictionary.
0.10.1,"with bootstrap, trees are no longer perfect on the training set"
0.10.1,Test that bootstrapping features may generate duplicate features.
0.10.1,Predict probabilities.
0.10.1,Normal case
0.10.1,"Degenerate case, where some classes are missing"
0.10.1,Check that oob prediction is a good estimation of the generalization
0.10.1,error.
0.10.1,Test with few estimators
0.10.1,Check singleton ensembles.
0.10.1,Check that bagging ensembles can be grid-searched.
0.10.1,Transform iris into a binary classification task
0.10.1,Grid search with scoring based on decision_function
0.10.1,Check estimator and its default values.
0.10.1,Test if fitting incrementally with warm start gives a forest of the
0.10.1,right size and the same results as a normal fit.
0.10.1,Test if warm start'ed second fit with smaller n_estimators raises error.
0.10.1,Test that nothing happens when fitting without increasing n_estimators
0.10.1,"modify X to nonsense values, this should not change anything"
0.10.1,warm started classifier with 5+5 estimators should be equivalent to
0.10.1,one classifier with 10 estimators
0.10.1,Check using oob_score and warm_start simultaneously fails
0.10.1,"Make sure OOB scores are identical when random_state, estimator, and"
0.10.1,training data are fixed and fitting is done twice
0.10.1,Check that format of estimators_samples_ is correct and that results
0.10.1,generated at fit time can be identically reproduced at a later time
0.10.1,using data saved in object attributes.
0.10.1,remap the y outside of the BalancedBaggingclassifier
0.10.1,"_, y = np.unique(y, return_inverse=True)"
0.10.1,Get relevant attributes
0.10.1,Test for correct formatting
0.10.1,Re-fit single estimator to test for consistent sampling
0.10.1,Make sure validated max_samples and original max_samples are identical
0.10.1,when valid integer max_samples supplied by user
0.10.1,check that we can pass any kind of sampler to a bagging classifier
0.10.1,check that we have balanced class with the right counts of class
0.10.1,sample depending on the sampling strategy
0.10.1,check that we can provide a FunctionSampler in BalancedBaggingClassifier
0.10.1,find the minority and majority classes
0.10.1,compute the number of sample to draw from the majority class using
0.10.1,a negative binomial distribution
0.10.1,draw randomly with or without replacement
0.10.1,Roughly Balanced Bagging
0.10.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.10.1,Christos Aridas
0.10.1,License: MIT
0.10.1,Generate a global dataset to use
0.10.1,Check classification for various parameter settings.
0.10.1,test the different prediction function
0.10.1,Check estimator and its default values.
0.10.1,Test if fitting incrementally with warm start gives a forest of the
0.10.1,right size and the same results as a normal fit.
0.10.1,Test if warm start'ed second fit with smaller n_estimators raises error.
0.10.1,Test that nothing happens when fitting without increasing n_estimators
0.10.1,"modify X to nonsense values, this should not change anything"
0.10.1,warm started classifier with 5+5 estimators should be equivalent to
0.10.1,one classifier with 10 estimators
0.10.1,Check warning if not enough estimators
0.10.1,First fit with no restriction on max samples
0.10.1,Second fit with max samples restricted to just 2
0.10.1,Regression test for #655: check that the oob score is closed to 0.5
0.10.1,a binomial experiment.
0.10.1,Author: Guillaume Lemaitre
0.10.1,License: BSD 3 clause
0.10.1,"The index start at one, then we need to remove one"
0.10.1,to not have issue with the indexing.
0.10.1,go through the list and check if the data are available
0.10.1,Authors: Dayvid Oliveira
0.10.1,Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.10.1,Christos Aridas
0.10.1,License: MIT
0.10.1,restrict ratio to be a dict or a callable
0.10.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.10.1,Christos Aridas
0.10.1,License: MIT
0.10.1,"we are reusing part of utils.check_sampling_strategy, however this is not"
0.10.1,cover in the common tests so we will repeat it here
0.10.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.10.1,Christos Aridas
0.10.1,License: MIT
0.10.1,This is a trick to avoid an error during tests collection with pytest. We
0.10.1,avoid the error when importing the package raise the error at the moment of
0.10.1,creating the instance.
0.10.1,This is a trick to avoid an error during tests collection with pytest. We
0.10.1,avoid the error when importing the package raise the error at the moment of
0.10.1,creating the instance.
0.10.1,flag for keras sequence duck-typing
0.10.1,shuffle the indices since the sampler are packing them by class
0.10.0,This file is here so that when running from the root folder
0.10.0,./imblearn is added to sys.path by pytest.
0.10.0,See https://docs.pytest.org/en/latest/pythonpath.html for more details.
0.10.0,"For example, this allows to build extensions in place and run pytest"
0.10.0,doc/modules/clustering.rst and use imblearn from the local folder
0.10.0,rather than the one from site-packages.
0.10.0,! /usr/bin/env python
0.10.0,Python 2 compat: just to be able to declare that Python >=3.7 is needed.
0.10.0,This is a bit (!) hackish: we are setting a global variable so that the
0.10.0,main imblearn __init__ can detect if it is being loaded by the setup
0.10.0,"routine, to avoid attempting to load components that aren't built yet:"
0.10.0,the numpy distutils extensions that are used by imbalanced-learn to
0.10.0,recursively build the compiled extensions in sub-packages is based on the
0.10.0,Python import machinery.
0.10.0,get __version__ from _version.py
0.10.0,-*- coding: utf-8 -*-
0.10.0,
0.10.0,"imbalanced-learn documentation build configuration file, created by"
0.10.0,sphinx-quickstart on Mon Jan 18 14:44:12 2016.
0.10.0,
0.10.0,This file is execfile()d with the current directory set to its
0.10.0,containing dir.
0.10.0,
0.10.0,Note that not all possible configuration values are present in this
0.10.0,autogenerated file.
0.10.0,
0.10.0,All configuration values have a default; values that are commented out
0.10.0,serve to show the default.
0.10.0,"If extensions (or modules to document with autodoc) are in another directory,"
0.10.0,add these directories to sys.path here. If the directory is relative to the
0.10.0,"documentation root, use os.path.abspath to make it absolute, like shown here."
0.10.0,-- General configuration ------------------------------------------------
0.10.0,"If your documentation needs a minimal Sphinx version, state it here."
0.10.0,needs_sphinx = '1.0'
0.10.0,"Add any Sphinx extension module names here, as strings. They can be"
0.10.0,extensions coming with Sphinx (named 'sphinx.ext.*') or your custom
0.10.0,ones.
0.10.0,"Add any paths that contain templates here, relative to this directory."
0.10.0,The suffix of source filenames.
0.10.0,The master toctree document.
0.10.0,General information about the project.
0.10.0,"The version info for the project you're documenting, acts as replacement for"
0.10.0,"|version| and |release|, also used in various other places throughout the"
0.10.0,built documents.
0.10.0,
0.10.0,The short X.Y version.
0.10.0,"The full version, including alpha/beta/rc tags."
0.10.0,"List of patterns, relative to source directory, that match files and"
0.10.0,directories to ignore when looking for source files.
0.10.0,The reST default role (used for this markup: `text`) to use for all
0.10.0,documents.
0.10.0,"If true, '()' will be appended to :func: etc. cross-reference text."
0.10.0,The name of the Pygments (syntax highlighting) style to use.
0.10.0,-- Options for HTML output ----------------------------------------------
0.10.0,The theme to use for HTML and HTML Help pages.  See the documentation for
0.10.0,a list of builtin themes.
0.10.0,"""twitter_url"": ""https://twitter.com/pandas_dev"","
0.10.0,"""navbar_align"": ""right"",  # For testing that the navbar items align properly"
0.10.0,"Add any paths that contain custom static files (such as style sheets) here,"
0.10.0,"relative to this directory. They are copied after the builtin static files,"
0.10.0,"so a file named ""default.css"" will overwrite the builtin ""default.css""."
0.10.0,Output file base name for HTML help builder.
0.10.0,-- Options for autodoc ------------------------------------------------------
0.10.0,generate autosummary even if no references
0.10.0,-- Options for numpydoc -----------------------------------------------------
0.10.0,this is needed for some reason...
0.10.0,see https://github.com/numpy/numpydoc/issues/69
0.10.0,-- Options for sphinxcontrib-bibtex -----------------------------------------
0.10.0,bibtex file
0.10.0,-- Options for intersphinx --------------------------------------------------
0.10.0,intersphinx configuration
0.10.0,-- Options for sphinx-gallery -----------------------------------------------
0.10.0,Generate the plot for the gallery
0.10.0,sphinx-gallery configuration
0.10.0,-- Options for github link for what's new -----------------------------------
0.10.0,Config for sphinx_issues
0.10.0,The following is used by sphinx.ext.linkcode to provide links to github
0.10.0,-- Options for LaTeX output ---------------------------------------------
0.10.0,The paper size ('letterpaper' or 'a4paper').
0.10.0,"'papersize': 'letterpaper',"
0.10.0,"The font size ('10pt', '11pt' or '12pt')."
0.10.0,"'pointsize': '10pt',"
0.10.0,Additional stuff for the LaTeX preamble.
0.10.0,"'preamble': '',"
0.10.0,Grouping the document tree into LaTeX files. List of tuples
0.10.0,"(source start file, target name, title,"
0.10.0,"author, documentclass [howto, manual, or own class])."
0.10.0,-- Options for manual page output ---------------------------------------
0.10.0,"If false, no module index is generated."
0.10.0,latex_domain_indices = True
0.10.0,One entry per manual page. List of tuples
0.10.0,"(source start file, name, description, authors, manual section)."
0.10.0,"If true, show URL addresses after external links."
0.10.0,man_show_urls = False
0.10.0,-- Options for Texinfo output -------------------------------------------
0.10.0,Grouping the document tree into Texinfo files. List of tuples
0.10.0,"(source start file, target name, title, author,"
0.10.0,"dir menu entry, description, category)"
0.10.0,-- Dependencies generation ----------------------------------------------
0.10.0,get length of header
0.10.0,-- Additional temporary hacks -----------------------------------------------
0.10.0,Temporary work-around for spacing problem between parameter and parameter
0.10.0,"type in the doc, see https://github.com/numpy/numpydoc/issues/215. The bug"
0.10.0,has been fixed in sphinx (https://github.com/sphinx-doc/sphinx/pull/5976) but
0.10.0,through a change in sphinx basic.css except rtd_theme does not use basic.css.
0.10.0,"In an ideal world, this would get fixed in this PR:"
0.10.0,https://github.com/readthedocs/sphinx_rtd_theme/pull/747/files
0.10.0,get the styles from the current theme
0.10.0,create and add the button to all the code blocks that contain >>>
0.10.0,tracebacks (.gt) contain bare text elements that need to be
0.10.0,wrapped in a span to work with .nextUntil() (see later)
0.10.0,define the behavior of the button when it's clicked
0.10.0,hide the code output
0.10.0,show the code output
0.10.0,-*- coding: utf-8 -*-
0.10.0,Format template for issues URI
0.10.0,e.g. 'https://github.com/sloria/marshmallow/issues/{issue}
0.10.0,Format template for PR URI
0.10.0,e.g. 'https://github.com/sloria/marshmallow/pull/{issue}
0.10.0,Format template for commit URI
0.10.0,e.g. 'https://github.com/sloria/marshmallow/commits/{commit}
0.10.0,"Shortcut for Github, e.g. 'sloria/marshmallow'"
0.10.0,Format template for user profile URI
0.10.0,e.g. 'https://github.com/{user}'
0.10.0,Python 2 only
0.10.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.10.0,License: MIT
0.10.0,%%
0.10.0,%%
0.10.0,"First, we will generate a toy classification dataset with only few samples."
0.10.0,The ratio between the classes will be imbalanced.
0.10.0,%%
0.10.0,%%
0.10.0,"Now, we will use a :class:`~imblearn.over_sampling.RandomOverSampler` to"
0.10.0,generate a bootstrap for the minority class with as many samples as in the
0.10.0,majority class.
0.10.0,%%
0.10.0,%%
0.10.0,We observe that the minority samples are less transparent than the samples
0.10.0,"from the majority class. Indeed, it is due to the fact that these samples"
0.10.0,of the minority class are repeated during the bootstrap generation.
0.10.0,
0.10.0,We can set `shrinkage` to a floating value to add a small perturbation to the
0.10.0,samples created and therefore create a smoothed bootstrap.
0.10.0,%%
0.10.0,%%
0.10.0,"In this case, we see that the samples in the minority class are not"
0.10.0,overlapping anymore due to the added noise.
0.10.0,
0.10.0,The parameter `shrinkage` allows to add more or less perturbation. Let's
0.10.0,add more perturbation when generating the smoothed bootstrap.
0.10.0,%%
0.10.0,%%
0.10.0,Increasing the value of `shrinkage` will disperse the new samples. Forcing
0.10.0,the shrinkage to 0 will be equivalent to generating a normal bootstrap.
0.10.0,%%
0.10.0,%%
0.10.0,"Therefore, the `shrinkage` is handy to manually tune the dispersion of the"
0.10.0,new samples.
0.10.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.10.0,License: MIT
0.10.0,%%
0.10.0,generate some data points
0.10.0,plot the majority and minority samples
0.10.0,draw the circle in which the new sample will generated
0.10.0,plot the line on which the sample will be generated
0.10.0,create and plot the new sample
0.10.0,make the plot nicer with legend and label
0.10.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.10.0,License: MIT
0.10.0,%%
0.10.0,%% [markdown]
0.10.0,The following function will be used to create toy dataset. It uses the
0.10.0,:func:`~sklearn.datasets.make_classification` from scikit-learn but fixing
0.10.0,some parameters.
0.10.0,%%
0.10.0,%% [markdown]
0.10.0,The following function will be used to plot the sample space after resampling
0.10.0,to illustrate the specificities of an algorithm.
0.10.0,%%
0.10.0,%% [markdown]
0.10.0,The following function will be used to plot the decision function of a
0.10.0,classifier given some data.
0.10.0,%%
0.10.0,%% [markdown]
0.10.0,Illustration of the influence of the balancing ratio
0.10.0,----------------------------------------------------
0.10.0,
0.10.0,We will first illustrate the influence of the balancing ratio on some toy
0.10.0,data using a logistic regression classifier which is a linear model.
0.10.0,%%
0.10.0,%% [markdown]
0.10.0,We will fit and show the decision boundary model to illustrate the impact of
0.10.0,dealing with imbalanced classes.
0.10.0,%%
0.10.0,%% [markdown]
0.10.0,"Greater is the difference between the number of samples in each class, poorer"
0.10.0,are the classification results.
0.10.0,
0.10.0,Random over-sampling to balance the data set
0.10.0,--------------------------------------------
0.10.0,
0.10.0,Random over-sampling can be used to repeat some samples and balance the
0.10.0,number of samples between the dataset. It can be seen that with this trivial
0.10.0,approach the boundary decision is already less biased toward the majority
0.10.0,class. The class :class:`~imblearn.over_sampling.RandomOverSampler`
0.10.0,implements such of a strategy.
0.10.0,%%
0.10.0,%% [markdown]
0.10.0,"By default, random over-sampling generates a bootstrap. The parameter"
0.10.0,`shrinkage` allows adding a small perturbation to the generated data
0.10.0,to generate a smoothed bootstrap instead. The plot below shows the difference
0.10.0,between the two data generation strategies.
0.10.0,%%
0.10.0,%% [markdown]
0.10.0,It looks like more samples are generated with smoothed bootstrap. This is due
0.10.0,to the fact that the samples generated are not superimposing with the
0.10.0,original samples.
0.10.0,
0.10.0,More advanced over-sampling using ADASYN and SMOTE
0.10.0,--------------------------------------------------
0.10.0,
0.10.0,Instead of repeating the same samples when over-sampling or perturbating the
0.10.0,"generated bootstrap samples, one can use some specific heuristic instead."
0.10.0,:class:`~imblearn.over_sampling.ADASYN` and
0.10.0,:class:`~imblearn.over_sampling.SMOTE` can be used in this case.
0.10.0,%%
0.10.0,%% [markdown]
0.10.0,The following plot illustrates the difference between
0.10.0,:class:`~imblearn.over_sampling.ADASYN` and
0.10.0,:class:`~imblearn.over_sampling.SMOTE`.
0.10.0,:class:`~imblearn.over_sampling.ADASYN` will focus on the samples which are
0.10.0,difficult to classify with a nearest-neighbors rule while regular
0.10.0,:class:`~imblearn.over_sampling.SMOTE` will not make any distinction.
0.10.0,"Therefore, the decision function depending of the algorithm."
0.10.0,%% [markdown]
0.10.0,"Due to those sampling particularities, it can give rise to some specific"
0.10.0,issues as illustrated below.
0.10.0,%%
0.10.0,%% [markdown]
0.10.0,SMOTE proposes several variants by identifying specific samples to consider
0.10.0,during the resampling. The borderline version
0.10.0,(:class:`~imblearn.over_sampling.BorderlineSMOTE`) will detect which point to
0.10.0,select which are in the border between two classes. The SVM version
0.10.0,(:class:`~imblearn.over_sampling.SVMSMOTE`) will use the support vectors
0.10.0,found using an SVM algorithm to create new sample while the KMeans version
0.10.0,(:class:`~imblearn.over_sampling.KMeansSMOTE`) will make a clustering before
0.10.0,to generate samples in each cluster independently depending each cluster
0.10.0,density.
0.10.0,%%
0.10.0,%% [markdown]
0.10.0,"When dealing with a mixed of continuous and categorical features,"
0.10.0,:class:`~imblearn.over_sampling.SMOTENC` is the only method which can handle
0.10.0,this case.
0.10.0,%%
0.10.0,Create a dataset of a mix of numerical and categorical data
0.10.0,%% [markdown]
0.10.0,"However, if the dataset is composed of only categorical features then one"
0.10.0,should use :class:`~imblearn.over_sampling.SMOTEN`.
0.10.0,%%
0.10.0,Generate only categorical data
0.10.0,Authors: Christos Aridas
0.10.0,Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.10.0,License: MIT
0.10.0,%%
0.10.0,%% [markdown]
0.10.0,Let's first generate a dataset with imbalanced class distribution.
0.10.0,%%
0.10.0,%% [markdown]
0.10.0,We will use an over-sampler :class:`~imblearn.over_sampling.SMOTE` followed
0.10.0,by a :class:`~sklearn.tree.DecisionTreeClassifier`. The aim will be to
0.10.0,search which `k_neighbors` parameter is the most adequate with the dataset
0.10.0,that we generated.
0.10.0,%%
0.10.0,%% [markdown]
0.10.0,We can use the :class:`~sklearn.model_selection.validation_curve` to inspect
0.10.0,"the impact of varying the parameter `k_neighbors`. In this case, we need"
0.10.0,to use a score to evaluate the generalization score during the
0.10.0,cross-validation.
0.10.0,%%
0.10.0,%%
0.10.0,%% [markdown]
0.10.0,We can now plot the results of the cross-validation for the different
0.10.0,parameter values that we tried.
0.10.0,%%
0.10.0,make nice plotting
0.10.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.10.0,License: MIT
0.10.0,Generate a dataset
0.10.0,Split the data
0.10.0,Train the classifier with balancing
0.10.0,Test the classifier and get the prediction
0.10.0,Show the classification report
0.10.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.10.0,License: MIT
0.10.0,%%
0.10.0,%% [markdown]
0.10.0,"First, we will generate some imbalanced dataset."
0.10.0,%%
0.10.0,%% [markdown]
0.10.0,We will split the data into a training and testing set.
0.10.0,%%
0.10.0,%% [markdown]
0.10.0,We will create a pipeline made of a :class:`~imblearn.over_sampling.SMOTE`
0.10.0,over-sampler followed by a :class:`~sklearn.svm.LinearSVC` classifier.
0.10.0,%%
0.10.0,%% [markdown]
0.10.0,"Now, we will train the model on the training set and get the prediction"
0.10.0,associated with the testing set. Be aware that the resampling will happen
0.10.0,only when calling `fit`: the number of samples in `y_pred` is the same than
0.10.0,in `y_test`.
0.10.0,%%
0.10.0,%% [markdown]
0.10.0,The geometric mean corresponds to the square root of the product of the
0.10.0,sensitivity and specificity. Combining the two metrics should account for
0.10.0,the balancing of the dataset.
0.10.0,%%
0.10.0,%% [markdown]
0.10.0,The index balanced accuracy can transform any metric to be used in
0.10.0,imbalanced learning problems.
0.10.0,%%
0.10.0,%%
0.10.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.10.0,License: MIT
0.10.0,%%
0.10.0,%% [markdown]
0.10.0,Dataset generation
0.10.0,------------------
0.10.0,
0.10.0,We will create an imbalanced dataset with a couple of samples. We will use
0.10.0,:func:`~sklearn.datasets.make_classification` to generate this dataset.
0.10.0,%%
0.10.0,%%
0.10.0,%% [markdown]
0.10.0,The following function will be used to plot the sample space after resampling
0.10.0,to illustrate the characteristic of an algorithm.
0.10.0,%%
0.10.0,%% [markdown]
0.10.0,The following function will be used to plot the decision function of a
0.10.0,classifier given some data.
0.10.0,%%
0.10.0,%% [markdown]
0.10.0,":class:`~imblearn.over_sampling.SMOTE` allows to generate samples. However,"
0.10.0,this method of over-sampling does not have any knowledge regarding the
0.10.0,"underlying distribution. Therefore, some noisy samples can be generated, e.g."
0.10.0,"when the different classes cannot be well separated. Hence, it can be"
0.10.0,beneficial to apply an under-sampling algorithm to clean the noisy samples.
0.10.0,Two methods are usually used in the literature: (i) Tomek's link and (ii)
0.10.0,edited nearest neighbours cleaning methods. Imbalanced-learn provides two
0.10.0,ready-to-use samplers :class:`~imblearn.combine.SMOTETomek` and
0.10.0,":class:`~imblearn.combine.SMOTEENN`. In general,"
0.10.0,:class:`~imblearn.combine.SMOTEENN` cleans more noisy data than
0.10.0,:class:`~imblearn.combine.SMOTETomek`.
0.10.0,%%
0.10.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.10.0,License: MIT
0.10.0,%%
0.10.0,%% [markdown]
0.10.0,Load an imbalanced dataset
0.10.0,--------------------------
0.10.0,
0.10.0,We will load the UCI SatImage dataset which has an imbalanced ratio of 9.3:1
0.10.0,(number of majority sample for a minority sample). The data are then split
0.10.0,into training and testing.
0.10.0,%%
0.10.0,%% [markdown]
0.10.0,Classification using a single decision tree
0.10.0,-------------------------------------------
0.10.0,
0.10.0,We train a decision tree classifier which will be used as a baseline for the
0.10.0,rest of this example.
0.10.0,
0.10.0,The results are reported in terms of balanced accuracy and geometric mean
0.10.0,which are metrics widely used in the literature to validate model trained on
0.10.0,imbalanced set.
0.10.0,%%
0.10.0,%%
0.10.0,%%
0.10.0,%% [markdown]
0.10.0,Classification using bagging classifier with and without sampling
0.10.0,-----------------------------------------------------------------
0.10.0,
0.10.0,"Instead of using a single tree, we will check if an ensemble of decision tree"
0.10.0,"can actually alleviate the issue induced by the class imbalancing. First, we"
0.10.0,will use a bagging classifier and its counter part which internally uses a
0.10.0,random under-sampling to balanced each bootstrap sample.
0.10.0,%%
0.10.0,%% [markdown]
0.10.0,Balancing each bootstrap sample allows to increase significantly the balanced
0.10.0,accuracy and the geometric mean.
0.10.0,%%
0.10.0,%%
0.10.0,%% [markdown]
0.10.0,Classification using random forest classifier with and without sampling
0.10.0,-----------------------------------------------------------------------
0.10.0,
0.10.0,Random forest is another popular ensemble method and it is usually
0.10.0,"outperforming bagging. Here, we used a vanilla random forest and its balanced"
0.10.0,counterpart in which each bootstrap sample is balanced.
0.10.0,%%
0.10.0,%% [markdown]
0.10.0,"Similarly to the previous experiment, the balanced classifier outperform the"
0.10.0,"classifier which learn from imbalanced bootstrap samples. In addition, random"
0.10.0,forest outperforms the bagging classifier.
0.10.0,%%
0.10.0,%%
0.10.0,%% [markdown]
0.10.0,Boosting classifier
0.10.0,-------------------
0.10.0,
0.10.0,"In the same manner, easy ensemble classifier is a bag of balanced AdaBoost"
0.10.0,"classifier. However, it will be slower to train than random forest and will"
0.10.0,achieve worse performance.
0.10.0,%%
0.10.0,%%
0.10.0,%%
0.10.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.10.0,License: MIT
0.10.0,%%
0.10.0,%% [markdown]
0.10.0,Generate an imbalanced dataset
0.10.0,------------------------------
0.10.0,
0.10.0,"For this example, we will create a synthetic dataset using the function"
0.10.0,:func:`~sklearn.datasets.make_classification`. The problem will be a toy
0.10.0,classification problem with a ratio of 1:9 between the two classes.
0.10.0,%%
0.10.0,%%
0.10.0,%% [markdown]
0.10.0,"In the following sections, we will show a couple of algorithms that have"
0.10.0,been proposed over the years. We intend to illustrate how one can reuse the
0.10.0,:class:`~imblearn.ensemble.BalancedBaggingClassifier` by passing different
0.10.0,sampler.
0.10.0,%%
0.10.0,%% [markdown]
0.10.0,Exactly Balanced Bagging and Over-Bagging
0.10.0,-----------------------------------------
0.10.0,
0.10.0,The :class:`~imblearn.ensemble.BalancedBaggingClassifier` can use in
0.10.0,conjunction with a :class:`~imblearn.under_sampling.RandomUnderSampler` or
0.10.0,:class:`~imblearn.over_sampling.RandomOverSampler`. These methods are
0.10.0,"referred as Exactly Balanced Bagging and Over-Bagging, respectively and have"
0.10.0,been proposed first in [1]_.
0.10.0,%%
0.10.0,Exactly Balanced Bagging
0.10.0,%%
0.10.0,Over-bagging
0.10.0,%% [markdown]
0.10.0,SMOTE-Bagging
0.10.0,-------------
0.10.0,
0.10.0,Instead of using a :class:`~imblearn.over_sampling.RandomOverSampler` that
0.10.0,"make a bootstrap, an alternative is to use"
0.10.0,:class:`~imblearn.over_sampling.SMOTE` as an over-sampler. This is known as
0.10.0,SMOTE-Bagging [2]_.
0.10.0,%%
0.10.0,SMOTE-Bagging
0.10.0,%% [markdown]
0.10.0,Roughly Balanced Bagging
0.10.0,------------------------
0.10.0,While using a :class:`~imblearn.under_sampling.RandomUnderSampler` or
0.10.0,:class:`~imblearn.over_sampling.RandomOverSampler` will create exactly the
0.10.0,"desired number of samples, it does not follow the statistical spirit wanted"
0.10.0,in the bagging framework. The authors in [3]_ proposes to use a negative
0.10.0,binomial distribution to compute the number of samples of the majority
0.10.0,class to be selected and then perform a random under-sampling.
0.10.0,
0.10.0,"Here, we illustrate this method by implementing a function in charge of"
0.10.0,resampling and use the :class:`~imblearn.FunctionSampler` to integrate it
0.10.0,within a :class:`~imblearn.pipeline.Pipeline` and
0.10.0,:class:`~sklearn.model_selection.cross_validate`.
0.10.0,%%
0.10.0,find the minority and majority classes
0.10.0,compute the number of sample to draw from the majority class using
0.10.0,a negative binomial distribution
0.10.0,draw randomly with or without replacement
0.10.0,Roughly Balanced Bagging
0.10.0,%% [markdown]
0.10.0,.. topic:: References:
0.10.0,
0.10.0,".. [1] R. Maclin, and D. Opitz. ""An empirical evaluation of bagging and"
0.10.0,"boosting."" AAAI/IAAI 1997 (1997): 546-551."
0.10.0,
0.10.0,".. [2] S. Wang, and X. Yao. ""Diversity analysis on imbalanced data sets by"
0.10.0,"using ensemble models."" 2009 IEEE symposium on computational"
0.10.0,"intelligence and data mining. IEEE, 2009."
0.10.0,
0.10.0,".. [3] S. Hido, H. Kashima, and Y. Takahashi. ""Roughly balanced bagging"
0.10.0,"for imbalanced data."" Statistical Analysis and Data Mining: The ASA"
0.10.0,Data Science Journal 2.5‚Äê6 (2009): 412-426.
0.10.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.10.0,License: MIT
0.10.0,%%
0.10.0,%% [markdown]
0.10.0,The following function will be used to create toy dataset. It uses the
0.10.0,:func:`~sklearn.datasets.make_classification` from scikit-learn but fixing
0.10.0,some parameters.
0.10.0,%%
0.10.0,%% [markdown]
0.10.0,The following function will be used to plot the sample space after resampling
0.10.0,to illustrate the specificities of an algorithm.
0.10.0,%%
0.10.0,%% [markdown]
0.10.0,The following function will be used to plot the decision function of a
0.10.0,classifier given some data.
0.10.0,%%
0.10.0,%%
0.10.0,%% [markdown]
0.10.0,Prototype generation: under-sampling by generating new samples
0.10.0,--------------------------------------------------------------
0.10.0,
0.10.0,:class:`~imblearn.under_sampling.ClusterCentroids` under-samples by replacing
0.10.0,the original samples by the centroids of the cluster found.
0.10.0,%%
0.10.0,%% [markdown]
0.10.0,Prototype selection: under-sampling by selecting existing samples
0.10.0,-----------------------------------------------------------------
0.10.0,
0.10.0,The algorithm performing prototype selection can be subdivided into two
0.10.0,groups: (i) the controlled under-sampling methods and (ii) the cleaning
0.10.0,under-sampling methods.
0.10.0,
0.10.0,"With the controlled under-sampling methods, the number of samples to be"
0.10.0,selected can be specified.
0.10.0,:class:`~imblearn.under_sampling.RandomUnderSampler` is the most naive way of
0.10.0,performing such selection by randomly selecting a given number of samples by
0.10.0,the targetted class.
0.10.0,%%
0.10.0,%% [markdown]
0.10.0,:class:`~imblearn.under_sampling.NearMiss` algorithms implement some
0.10.0,heuristic rules in order to select samples. NearMiss-1 selects samples from
0.10.0,the majority class for which the average distance of the :math:`k`` nearest
0.10.0,samples of the minority class is the smallest. NearMiss-2 selects the samples
0.10.0,from the majority class for which the average distance to the farthest
0.10.0,samples of the negative class is the smallest. NearMiss-3 is a 2-step
0.10.0,"algorithm: first, for each minority sample, their :math:`m`"
0.10.0,"nearest-neighbors will be kept; then, the majority samples selected are the"
0.10.0,on for which the average distance to the :math:`k` nearest neighbors is the
0.10.0,largest.
0.10.0,%%
0.10.0,%% [markdown]
0.10.0,:class:`~imblearn.under_sampling.EditedNearestNeighbours` removes samples of
0.10.0,the majority class for which their class differ from the one of their
0.10.0,nearest-neighbors. This sieve can be repeated which is the principle of the
0.10.0,:class:`~imblearn.under_sampling.RepeatedEditedNearestNeighbours`.
0.10.0,:class:`~imblearn.under_sampling.AllKNN` is slightly different from the
0.10.0,:class:`~imblearn.under_sampling.RepeatedEditedNearestNeighbours` by changing
0.10.0,"the :math:`k` parameter of the internal nearest neighors algorithm,"
0.10.0,increasing it at each iteration.
0.10.0,%%
0.10.0,%% [markdown]
0.10.0,:class:`~imblearn.under_sampling.CondensedNearestNeighbour` makes use of a
0.10.0,1-NN to iteratively decide if a sample should be kept in a dataset or not.
0.10.0,The issue is that :class:`~imblearn.under_sampling.CondensedNearestNeighbour`
0.10.0,is sensitive to noise by preserving the noisy samples.
0.10.0,:class:`~imblearn.under_sampling.OneSidedSelection` also used the 1-NN and
0.10.0,use :class:`~imblearn.under_sampling.TomekLinks` to remove the samples
0.10.0,considered noisy. The
0.10.0,:class:`~imblearn.under_sampling.NeighbourhoodCleaningRule` use a
0.10.0,:class:`~imblearn.under_sampling.EditedNearestNeighbours` to remove some
0.10.0,"sample. Additionally, they use a 3 nearest-neighbors to remove samples which"
0.10.0,do not agree with this rule.
0.10.0,%%
0.10.0,%% [markdown]
0.10.0,:class:`~imblearn.under_sampling.InstanceHardnessThreshold` uses the
0.10.0,prediction of classifier to exclude samples. All samples which are classified
0.10.0,with a low probability will be removed.
0.10.0,%%
0.10.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.10.0,License: MIT
0.10.0,%%
0.10.0,%% [markdown]
0.10.0,This function allows to make nice plotting
0.10.0,%%
0.10.0,%% [markdown]
0.10.0,We will generate some toy data that illustrates how
0.10.0,:class:`~imblearn.under_sampling.TomekLinks` is used to clean a dataset.
0.10.0,%%
0.10.0,%% [markdown]
0.10.0,"In the figure above, the samples highlighted in green form a Tomek link since"
0.10.0,they are of different classes and are nearest neighbors of each other.
0.10.0,highlight the samples of interest
0.10.0,%% [markdown]
0.10.0,We can run the :class:`~imblearn.under_sampling.TomekLinks` sampling to
0.10.0,remove the corresponding samples. If `sampling_strategy='auto'` only the
0.10.0,sample from the majority class will be removed. If `sampling_strategy='all'`
0.10.0,both samples will be removed.
0.10.0,%%
0.10.0,highlight the samples of interest
0.10.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.10.0,License: MIT
0.10.0,%%
0.10.0,%% [markdown]
0.10.0,We define a function allowing to make some nice decoration on the plot.
0.10.0,%%
0.10.0,%% [markdown]
0.10.0,We can start by generating some data to later illustrate the principle of
0.10.0,each :class:`~imblearn.under_sampling.NearMiss` heuristic rules.
0.10.0,%%
0.10.0,%% [mardown]
0.10.0,NearMiss-1
0.10.0,----------
0.10.0,
0.10.0,NearMiss-1 selects samples from the majority class for which the average
0.10.0,distance to some nearest neighbours is the smallest. In the following
0.10.0,"example, we use a 3-NN to compute the average distance on 2 specific samples"
0.10.0,"of the majority class. Therefore, in this case the point linked by the"
0.10.0,green-dashed line will be selected since the average distance is smaller.
0.10.0,%%
0.10.0,%% [mardown]
0.10.0,NearMiss-2
0.10.0,----------
0.10.0,
0.10.0,NearMiss-2 selects samples from the majority class for which the average
0.10.0,distance to the farthest neighbors is the smallest. With the same
0.10.0,"configuration as previously presented, the sample linked to the green-dashed"
0.10.0,line will be selected since its distance the 3 farthest neighbors is the
0.10.0,smallest.
0.10.0,%%
0.10.0,%% [mardown]
0.10.0,NearMiss-3
0.10.0,----------
0.10.0,
0.10.0,"NearMiss-3 can be divided into 2 steps. First, a nearest-neighbors is used to"
0.10.0,short-list samples from the majority class (i.e. correspond to the
0.10.0,"highlighted samples in the following plot). Then, the sample with the largest"
0.10.0,average distance to the *k* nearest-neighbors are selected.
0.10.0,%%
0.10.0,select only the majority point of interest
0.10.0,Authors: Christos Aridas
0.10.0,Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.10.0,License: MIT
0.10.0,%%
0.10.0,%% [markdown]
0.10.0,Let's first create an imbalanced dataset and split in to two sets.
0.10.0,%%
0.10.0,%% [markdown]
0.10.0,"Now, we will create each individual steps that we would like later to combine"
0.10.0,%%
0.10.0,%% [markdown]
0.10.0,"Now, we can finally create a pipeline to specify in which order the different"
0.10.0,transformers and samplers should be executed before to provide the data to
0.10.0,the final classifier.
0.10.0,%%
0.10.0,%% [markdown]
0.10.0,We can now use the pipeline created as a normal classifier where resampling
0.10.0,"will happen when calling `fit` and disabled when calling `decision_function`,"
0.10.0,"`predict_proba`, or `predict`."
0.10.0,%%
0.10.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.10.0,License: MIT
0.10.0,##############################################################################
0.10.0,Data loading
0.10.0,##############################################################################
0.10.0,##############################################################################
0.10.0,"First, you should download the Porto Seguro data set from Kaggle. See the"
0.10.0,link in the introduction.
0.10.0,##############################################################################
0.10.0,The data set is imbalanced and it will have an effect on the fitting.
0.10.0,##############################################################################
0.10.0,Define the pre-processing pipeline
0.10.0,##############################################################################
0.10.0,##############################################################################
0.10.0,We want to standard scale the numerical features while we want to one-hot
0.10.0,"encode the categorical features. In this regard, we make use of the"
0.10.0,:class:`~sklearn.compose.ColumnTransformer`.
0.10.0,Create an environment variable to avoid using the GPU. This can be changed.
0.10.0,##############################################################################
0.10.0,Create a neural-network
0.10.0,##############################################################################
0.10.0,##############################################################################
0.10.0,We create a decorator to report the computation time
0.10.0,##############################################################################
0.10.0,The first model will be trained using the ``fit`` method and with imbalanced
0.10.0,mini-batches.
0.10.0,##############################################################################
0.10.0,"In the contrary, we will use imbalanced-learn to create a generator of"
0.10.0,mini-batches which will yield balanced mini-batches.
0.10.0,##############################################################################
0.10.0,Classification loop
0.10.0,##############################################################################
0.10.0,##############################################################################
0.10.0,We will perform a 10-fold cross-validation and train the neural-network with
0.10.0,the two different strategies previously presented.
0.10.0,##############################################################################
0.10.0,Plot of the results and computation time
0.10.0,##############################################################################
0.10.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.10.0,License: MIT
0.10.0,%%
0.10.0,%% [markdown]
0.10.0,Problem definition
0.10.0,------------------
0.10.0,
0.10.0,We are dropping the following features:
0.10.0,
0.10.0,"- ""fnlwgt"": this feature was created while studying the ""adult"" dataset."
0.10.0,"Thus, we will not use this feature which is not acquired during the survey."
0.10.0,"- ""education-num"": it is encoding the same information than ""education""."
0.10.0,"Thus, we are removing one of these 2 features."
0.10.0,%%
0.10.0,%% [markdown]
0.10.0,"The ""adult"" dataset as a class ratio of about 3:1"
0.10.0,%%
0.10.0,%% [markdown]
0.10.0,This dataset is only slightly imbalanced. To better highlight the effect of
0.10.0,"learning from an imbalanced dataset, we will increase its ratio to 30:1"
0.10.0,%%
0.10.0,%% [markdown]
0.10.0,We will perform a cross-validation evaluation to get an estimate of the test
0.10.0,score.
0.10.0,
0.10.0,"As a baseline, we could use a classifier which will always predict the"
0.10.0,majority class independently of the features provided.
0.10.0,%%
0.10.0,%% [markdown]
0.10.0,"Instead of using the accuracy, we can use the balanced accuracy which will"
0.10.0,take into account the balancing issue.
0.10.0,%%
0.10.0,%% [markdown]
0.10.0,Strategies to learn from an imbalanced dataset
0.10.0,----------------------------------------------
0.10.0,We will use a dictionary and a list to continuously store the results of
0.10.0,our experiments and show them as a pandas dataframe.
0.10.0,%%
0.10.0,%% [markdown]
0.10.0,Dummy baseline
0.10.0,..............
0.10.0,
0.10.0,"Before to train a real machine learning model, we can store the results"
0.10.0,obtained with our :class:`~sklearn.dummy.DummyClassifier`.
0.10.0,%%
0.10.0,%% [markdown]
0.10.0,Linear classifier baseline
0.10.0,..........................
0.10.0,
0.10.0,We will create a machine learning pipeline using a
0.10.0,":class:`~sklearn.linear_model.LogisticRegression` classifier. In this regard,"
0.10.0,we will need to one-hot encode the categorical columns and standardized the
0.10.0,numerical columns before to inject the data into the
0.10.0,:class:`~sklearn.linear_model.LogisticRegression` classifier.
0.10.0,
0.10.0,"First, we define our numerical and categorical pipelines."
0.10.0,%%
0.10.0,%% [markdown]
0.10.0,"Then, we can create a preprocessor which will dispatch the categorical"
0.10.0,columns to the categorical pipeline and the numerical columns to the
0.10.0,numerical pipeline
0.10.0,%%
0.10.0,%% [markdown]
0.10.0,"Finally, we connect our preprocessor with our"
0.10.0,:class:`~sklearn.linear_model.LogisticRegression`. We can then evaluate our
0.10.0,model.
0.10.0,%%
0.10.0,%%
0.10.0,%% [markdown]
0.10.0,We can see that our linear model is learning slightly better than our dummy
0.10.0,"baseline. However, it is impacted by the class imbalance."
0.10.0,
0.10.0,We can verify that something similar is happening with a tree-based model
0.10.0,such as :class:`~sklearn.ensemble.RandomForestClassifier`. With this type of
0.10.0,"classifier, we will not need to scale the numerical data, and we will only"
0.10.0,need to ordinal encode the categorical data.
0.10.0,%%
0.10.0,%%
0.10.0,%% [markdown]
0.10.0,The :class:`~sklearn.ensemble.RandomForestClassifier` is as well affected by
0.10.0,"the class imbalanced, slightly less than the linear model. Now, we will"
0.10.0,present different approach to improve the performance of these 2 models.
0.10.0,
0.10.0,Use `class_weight`
0.10.0,..................
0.10.0,
0.10.0,Most of the models in `scikit-learn` have a parameter `class_weight`. This
0.10.0,parameter will affect the computation of the loss in linear model or the
0.10.0,criterion in the tree-based model to penalize differently a false
0.10.0,classification from the minority and majority class. We can set
0.10.0,"`class_weight=""balanced""` such that the weight applied is inversely"
0.10.0,proportional to the class frequency. We test this parametrization in both
0.10.0,linear model and tree-based model.
0.10.0,%%
0.10.0,%%
0.10.0,%% [markdown]
0.10.0,We can see that using `class_weight` was really effective for the linear
0.10.0,"model, alleviating the issue of learning from imbalanced classes. However,"
0.10.0,the :class:`~sklearn.ensemble.RandomForestClassifier` is still biased toward
0.10.0,"the majority class, mainly due to the criterion which is not suited enough to"
0.10.0,fight the class imbalance.
0.10.0,
0.10.0,Resample the training set during learning
0.10.0,.........................................
0.10.0,
0.10.0,Another way is to resample the training set by under-sampling or
0.10.0,over-sampling some of the samples. `imbalanced-learn` provides some samplers
0.10.0,to do such processing.
0.10.0,%%
0.10.0,%%
0.10.0,%%
0.10.0,%%
0.10.0,%% [markdown]
0.10.0,Applying a random under-sampler before the training of the linear model or
0.10.0,"random forest, allows to not focus on the majority class at the cost of"
0.10.0,making more mistake for samples in the majority class (i.e. decreased
0.10.0,accuracy).
0.10.0,
0.10.0,We could apply any type of samplers and find which sampler is working best
0.10.0,on the current dataset.
0.10.0,
0.10.0,"Instead, we will present another way by using classifiers which will apply"
0.10.0,sampling internally.
0.10.0,
0.10.0,Use of specific balanced algorithms from imbalanced-learn
0.10.0,.........................................................
0.10.0,
0.10.0,We already showed that random under-sampling can be effective on decision
0.10.0,"tree. However, instead of under-sampling once the dataset, one could"
0.10.0,under-sample the original dataset before to take a bootstrap sample. This is
0.10.0,the base of the :class:`imblearn.ensemble.BalancedRandomForestClassifier` and
0.10.0,:class:`~imblearn.ensemble.BalancedBaggingClassifier`.
0.10.0,%%
0.10.0,%%
0.10.0,%% [markdown]
0.10.0,The performance with the
0.10.0,:class:`~imblearn.ensemble.BalancedRandomForestClassifier` is better than
0.10.0,applying a single random under-sampling. We will use a gradient-boosting
0.10.0,classifier within a :class:`~imblearn.ensemble.BalancedBaggingClassifier`.
0.10.0,%% [markdown]
0.10.0,This last approach is the most effective. The different under-sampling allows
0.10.0,to bring some diversity for the different GBDT to learn and not focus on a
0.10.0,portion of the majority class.
0.10.0,Authors: Christos Aridas
0.10.0,Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.10.0,License: MIT
0.10.0,%%
0.10.0,%% [markdown]
0.10.0,Load the dataset
0.10.0,----------------
0.10.0,
0.10.0,We will use a dataset containing image from know person where we will
0.10.0,build a model to recognize the person on the image. We will make this problem
0.10.0,a binary problem by taking picture of only George W. Bush and Bill Clinton.
0.10.0,%%
0.10.0,%%
0.10.0,%% [markdown]
0.10.0,We can check the ratio between the two classes.
0.10.0,%%
0.10.0,%% [markdown]
0.10.0,We see that we have an imbalanced classification problem with ~95% of the
0.10.0,data belonging to the class G.W. Bush.
0.10.0,
0.10.0,Compare over-sampling approaches
0.10.0,--------------------------------
0.10.0,
0.10.0,We will use different over-sampling approaches and use a kNN classifier
0.10.0,to check if we can recognize the 2 presidents. The evaluation will be
0.10.0,performed through cross-validation and we will plot the mean ROC curve.
0.10.0,
0.10.0,We will create different pipelines and evaluate them.
0.10.0,%%
0.10.0,%% [markdown]
0.10.0,We will compute the mean ROC curve for each pipeline using a different splits
0.10.0,provided by the :class:`~sklearn.model_selection.StratifiedKFold`
0.10.0,cross-validation.
0.10.0,%%
0.10.0,compute the mean fpr/tpr to get the mean ROC curve
0.10.0,Create a display that we will reuse to make the aggregated plots for
0.10.0,all methods
0.10.0,%% [markdown]
0.10.0,"In the previous cell, we created the different mean ROC curve and we can plot"
0.10.0,them on the same plot.
0.10.0,%%
0.10.0,%% [markdown]
0.10.0,"We see that for this task, methods that are generating new samples with some"
0.10.0,interpolation (i.e. ADASYN and SMOTE) perform better than random
0.10.0,over-sampling or no resampling.
0.10.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.10.0,License: MIT
0.10.0,Create a folder to fetch the dataset
0.10.0,Create a pipeline
0.10.0,Classify and report the results
0.10.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.10.0,License: MIT
0.10.0,%%
0.10.0,%% [markdown]
0.10.0,Setting the data set
0.10.0,--------------------
0.10.0,
0.10.0,We use a part of the 20 newsgroups data set by loading 4 topics. Using the
0.10.0,"scikit-learn loader, the data are split into a training and a testing set."
0.10.0,
0.10.0,Note the class \#3 is the minority class and has almost twice less samples
0.10.0,than the majority class.
0.10.0,%%
0.10.0,%%
0.10.0,%% [markdown]
0.10.0,The usual scikit-learn pipeline
0.10.0,-------------------------------
0.10.0,
0.10.0,You might usually use scikit-learn pipeline by combining the TF-IDF
0.10.0,vectorizer to feed a multinomial naive bayes classifier. A classification
0.10.0,report summarized the results on the testing set.
0.10.0,
0.10.0,"As expected, the recall of the class \#3 is low mainly due to the class"
0.10.0,imbalanced.
0.10.0,%%
0.10.0,%%
0.10.0,%% [markdown]
0.10.0,Balancing the class before classification
0.10.0,-----------------------------------------
0.10.0,
0.10.0,"To improve the prediction of the class \#3, it could be interesting to apply"
0.10.0,"a balancing before to train the naive bayes classifier. Therefore, we will"
0.10.0,use a :class:`~imblearn.under_sampling.RandomUnderSampler` to equalize the
0.10.0,number of samples in all the classes before the training.
0.10.0,
0.10.0,It is also important to note that we are using the
0.10.0,:class:`~imblearn.pipeline.make_pipeline` function implemented in
0.10.0,imbalanced-learn to properly handle the samplers.
0.10.0,%%
0.10.0,%% [markdown]
0.10.0,"Although the results are almost identical, it can be seen that the resampling"
0.10.0,allowed to correct the poor recall of the class \#3 at the cost of reducing
0.10.0,"the other metrics for the other classes. However, the overall results are"
0.10.0,slightly better.
0.10.0,%%
0.10.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.10.0,License: MIT
0.10.0,#############################################################################
0.10.0,Toy data generation
0.10.0,#############################################################################
0.10.0,#############################################################################
0.10.0,We are generating some non Gaussian data set contaminated with some unform
0.10.0,noise.
0.10.0,#############################################################################
0.10.0,We will generate some cleaned test data without outliers.
0.10.0,#############################################################################
0.10.0,How to use the :class:`~imblearn.FunctionSampler`
0.10.0,#############################################################################
0.10.0,#############################################################################
0.10.0,We first define a function which will use
0.10.0,:class:`~sklearn.ensemble.IsolationForest` to eliminate some outliers from
0.10.0,our dataset during training. The function passed to the
0.10.0,:class:`~imblearn.FunctionSampler` will be called when using the method
0.10.0,``fit_resample``.
0.10.0,#############################################################################
0.10.0,Integrate it within a pipeline
0.10.0,#############################################################################
0.10.0,#############################################################################
0.10.0,"By elimnating outliers before the training, the classifier will be less"
0.10.0,affected during the prediction.
0.10.0,Authors: Dayvid Oliveira
0.10.0,Christos Aridas
0.10.0,Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.10.0,License: MIT
0.10.0,%%
0.10.0,%% [markdown]
0.10.0,Generate the dataset
0.10.0,--------------------
0.10.0,
0.10.0,"First, we will generate a dataset and convert it to a"
0.10.0,:class:`~pandas.DataFrame` with arbitrary column names. We will plot the
0.10.0,original dataset.
0.10.0,%%
0.10.0,%% [markdown]
0.10.0,Make a dataset imbalanced
0.10.0,-------------------------
0.10.0,
0.10.0,"Now, we will show the helpers :func:`~imblearn.datasets.make_imbalance`"
0.10.0,that is useful to random select a subset of samples. It will impact the
0.10.0,class distribution as specified by the parameters.
0.10.0,%%
0.10.0,%%
0.10.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.10.0,License: MIT
0.10.0,%%
0.10.0,%% [markdown]
0.10.0,Create an imbalanced dataset
0.10.0,----------------------------
0.10.0,
0.10.0,"First, we will create an imbalanced data set from a the iris data set."
0.10.0,%%
0.10.0,%%
0.10.0,%% [markdown]
0.10.0,Using ``sampling_strategy`` in resampling algorithms
0.10.0,====================================================
0.10.0,
0.10.0,`sampling_strategy` as a `float`
0.10.0,--------------------------------
0.10.0,
0.10.0,`sampling_strategy` can be given a `float`. For **under-sampling
0.10.0,"methods**, it corresponds to the ratio :math:`\alpha_{us}` defined by"
0.10.0,:math:`N_{rM} = \alpha_{us} \times N_{m}` where :math:`N_{rM}` and
0.10.0,:math:`N_{m}` are the number of samples in the majority class after
0.10.0,"resampling and the number of samples in the minority class, respectively."
0.10.0,%%
0.10.0,select only 2 classes since the ratio make sense in this case
0.10.0,%%
0.10.0,%% [markdown]
0.10.0,"For **over-sampling methods**, it correspond to the ratio"
0.10.0,:math:`\alpha_{os}` defined by :math:`N_{rm} = \alpha_{os} \times N_{M}`
0.10.0,where :math:`N_{rm}` and :math:`N_{M}` are the number of samples in the
0.10.0,minority class after resampling and the number of samples in the majority
0.10.0,"class, respectively."
0.10.0,%%
0.10.0,%% [markdown]
0.10.0,`sampling_strategy` has a `str`
0.10.0,-------------------------------
0.10.0,
0.10.0,`sampling_strategy` can be given as a string which specify the class
0.10.0,"targeted by the resampling. With under- and over-sampling, the number of"
0.10.0,samples will be equalized.
0.10.0,
0.10.0,Note that we are using multiple classes from now on.
0.10.0,%%
0.10.0,%% [markdown]
0.10.0,"With **cleaning method**, the number of samples in each class will not be"
0.10.0,equalized even if targeted.
0.10.0,%%
0.10.0,%% [markdown]
0.10.0,`sampling_strategy as a `dict`
0.10.0,------------------------------
0.10.0,
0.10.0,"When `sampling_strategy` is a `dict`, the keys correspond to the targeted"
0.10.0,classes. The values correspond to the desired number of samples for each
0.10.0,targeted class. This is working for both **under- and over-sampling**
0.10.0,algorithms but not for the **cleaning algorithms**. Use a `list` instead.
0.10.0,%%
0.10.0,%% [markdown]
0.10.0,`sampling_strategy` as a `list`
0.10.0,-------------------------------
0.10.0,
0.10.0,"When `sampling_strategy` is a `list`, the list contains the targeted"
0.10.0,classes. It is used only for **cleaning methods** and raise an error
0.10.0,otherwise.
0.10.0,%%
0.10.0,%% [markdown]
0.10.0,`sampling_strategy` as a callable
0.10.0,---------------------------------
0.10.0,
0.10.0,"When callable, function taking `y` and returns a `dict`. The keys"
0.10.0,correspond to the targeted classes. The values correspond to the desired
0.10.0,number of samples for each class.
0.10.0,%%
0.10.0,List of whitelisted modules and methods; regexp are supported.
0.10.0,These docstrings will fail because they are inheriting from scikit-learn
0.10.0,skip private classes
0.10.0,"We ignore following error code,"
0.10.0,- RT02: The first line of the Returns section
0.10.0,"should contain only the type, .."
0.10.0,(as we may need refer to the name of the returned
0.10.0,object)
0.10.0,- GL01: Docstring text (summary) should start in the line
0.10.0,"immediately after the opening quotes (not in the same line,"
0.10.0,or leaving a blank line in between)
0.10.0,"- GL02: If there's a blank line, it should be before the"
0.10.0,"first line of the Returns section, not after (it allows to have"
0.10.0,short docstrings for properties).
0.10.0,Ignore PR02: Unknown parameters for properties. We sometimes use
0.10.0,"properties for ducktyping, i.e. SGDClassifier.predict_proba"
0.10.0,Following codes are only taken into account for the
0.10.0,top level class docstrings:
0.10.0,- ES01: No extended summary found
0.10.0,- SA01: See Also section not found
0.10.0,- EX01: No examples section found
0.10.0,In particular we can't parse the signature of properties
0.10.0,"When applied to classes, detect class method. For functions"
0.10.0,method = None.
0.10.0,TODO: this detection can be improved. Currently we assume that we have
0.10.0,class # methods if the second path element before last is in camel case.
0.10.0,'build' and 'install' is included to have structured metadata for CI.
0.10.0,It will NOT be included in setup's extras_require
0.10.0,"The values are (version_spec, comma separated tags)"
0.10.0,create inverse mapping for setuptools
0.10.0,Used by CI to get the min dependencies
0.10.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.10.0,License: MIT
0.10.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.10.0,Christos Aridas
0.10.0,License: MIT
0.10.0,scikit-learn >= 1.2
0.10.0,we need to overwrite SamplerMixin.fit to bypass the validation
0.10.0,Adapted from scikit-learn
0.10.0,Author: Edouard Duchesnay
0.10.0,Gael Varoquaux
0.10.0,Virgile Fritsch
0.10.0,Alexandre Gramfort
0.10.0,Lars Buitinck
0.10.0,Christos Aridas
0.10.0,Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.10.0,License: BSD
0.10.0,BaseEstimator interface
0.10.0,validate names
0.10.0,validate estimators
0.10.0,We allow last estimator to be None as an identity transformation
0.10.0,Estimator interface
0.10.0,Setup the memory
0.10.0,joblib >= 0.12
0.10.0,Fit or load from cache the current transformer
0.10.0,Replace the transformer of the step with the fitted
0.10.0,transformer. This is necessary when loading the transformer
0.10.0,from the cache.
0.10.0,This variable is injected in the __builtins__ by the build
0.10.0,process. It is used to enable importing subpackages of sklearn when
0.10.0,the binaries are not built
0.10.0,mypy error: Cannot determine type of '__SKLEARN_SETUP__'
0.10.0,We are not importing the rest of scikit-learn during the build
0.10.0,"process, as it may not be compiled yet"
0.10.0,"FIXME: When we get Python 3.7 as minimal version, we will need to switch to"
0.10.0,the following solution:
0.10.0,https://snarky.ca/lazy-importing-in-python-3-7/
0.10.0,Import the target module and insert it into the parent's namespace
0.10.0,Update this object's dict so that if someone keeps a reference to the
0.10.0,"LazyLoader, lookups are efficient (__getattr__ is only called on"
0.10.0,lookups that fail).
0.10.0,delay the import of keras since we are going to import either tensorflow
0.10.0,or keras
0.10.0,Based on NiLearn package
0.10.0,License: simplified BSD
0.10.0,"PEP0440 compatible formatted version, see:"
0.10.0,https://www.python.org/dev/peps/pep-0440/
0.10.0,
0.10.0,Generic release markers:
0.10.0,X.Y
0.10.0,X.Y.Z # For bugfix releases
0.10.0,
0.10.0,Admissible pre-release markers:
0.10.0,X.YaN # Alpha release
0.10.0,X.YbN # Beta release
0.10.0,X.YrcN # Release Candidate
0.10.0,X.Y # Final release
0.10.0,
0.10.0,Dev branch marker is: 'X.Y.dev' or 'X.Y.devN' where N is an integer.
0.10.0,'X.Y.dev0' is the canonical version of 'X.Y.dev'
0.10.0,
0.10.0,coding: utf-8
0.10.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.10.0,Dariusz Brzezinski
0.10.0,License: MIT
0.10.0,Only negative labels
0.10.0,"Calculate tp_sum, pred_sum, true_sum ###"
0.10.0,labels are now from 0 to len(labels) - 1 -> use bincount
0.10.0,Pathological case
0.10.0,Compute the true negative
0.10.0,Retain only selected labels
0.10.0,"Finally, we have all our sufficient statistics. Divide! #"
0.10.0,"Divide, and on zero-division, set scores to 0 and warn:"
0.10.0,"Oddly, we may get an ""invalid"" rather than a ""divide"" error"
0.10.0,here.
0.10.0,Average the results
0.10.0,labels are now from 0 to len(labels) - 1 -> use bincount
0.10.0,Pathological case
0.10.0,Retain only selected labels
0.10.0,old version of scipy return MaskedConstant instead of 0.0
0.10.0,check that the scoring function does not need a score
0.10.0,and only a prediction
0.10.0,We do not support multilabel so the only average supported
0.10.0,is binary
0.10.0,Compute the different metrics
0.10.0,Precision/recall/f1
0.10.0,Specificity
0.10.0,Geometric mean
0.10.0,Index balanced accuracy
0.10.0,compute averages
0.10.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.10.0,License: MIT
0.10.0,categories are expected to be encoded from 0 to n_categories - 1
0.10.0,"list of length n_features of ndarray (n_categories, n_classes)"
0.10.0,compute the counts
0.10.0,normalize by the summing over the classes
0.10.0,silence potential warning due to in-place division by zero
0.10.0,coding: utf-8
0.10.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.10.0,Christos Aridas
0.10.0,License: MIT
0.10.0,##############################################################################
0.10.0,Utilities for testing
0.10.0,import some data to play with
0.10.0,restrict to a binary classification task
0.10.0,add noisy features to make the problem harder and avoid perfect results
0.10.0,"run classifier, get class probabilities and label predictions"
0.10.0,only interested in probabilities of the positive case
0.10.0,XXX: do we really want a special API for the binary case?
0.10.0,##############################################################################
0.10.0,Tests
0.10.0,detailed measures for each class
0.10.0,individual scoring function that can be used for grid search: in the
0.10.0,binary class case the score is the value of the measure for the positive
0.10.0,class (e.g. label == 1). This is deprecated for average != 'binary'.
0.10.0,Such a case may occur with non-stratified cross-validation
0.10.0,ensure the above were meaningful tests:
0.10.0,Bad pos_label
0.10.0,Bad average option
0.10.0,but average != 'binary'; even if data is binary
0.10.0,compute the geometric mean for the binary problem
0.10.0,print classification report with class names
0.10.0,print classification report with label detection
0.10.0,print classification report with class names
0.10.0,print classification report with label detection
0.10.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.10.0,License: MIT
0.10.0,Check basic feature of the metric:
0.10.0,"* the shape of the distance matrix is (n_samples, n_samples)"
0.10.0,* computing pairwise distance of X is the same than explicitely between
0.10.0,X and X.
0.10.0,Check the property of the vdm distance. Let's check the property
0.10.0,"described in ""Improved Heterogeneous Distance Functions"", D.R. Wilson and"
0.10.0,"T.R. Martinez, Journal of Artificial Intelligence Research 6 (1997) 1-34"
0.10.0,https://arxiv.org/pdf/cs/9701101.pdf
0.10.0,
0.10.0,"""if an attribute color has three values red, green and blue, and the"
0.10.0,"application is to identify whether or not an object is an apple, red and"
0.10.0,green would be considered closer than red and blue because the former two
0.10.0,"both have similar correlations with the output class apple."""
0.10.0,defined our feature
0.10.0,0 - not an apple / 1 - an apple
0.10.0,computing the distance between a sample of the same category should
0.10.0,give a null distance
0.10.0,check the property explained in the introduction example
0.10.0,green and red are very close
0.10.0,blue is closer to red than green
0.10.0,"Check that ""auto"" is equivalent to provide the number categories"
0.10.0,beforehand
0.10.0,Check that we raise an error if n_categories is inconsistent with the
0.10.0,number of features in X
0.10.0,Check that we don't get issue when a category is missing between 0
0.10.0,n_categories - 1
0.10.0,remove a categories that could be between 0 and n_categories
0.10.0,Check that we raise a NotFittedError when `fit` is not not called before
0.10.0,pairwise.
0.10.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.10.0,Christos Aridas
0.10.0,License: MIT
0.10.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.10.0,Christos Aridas
0.10.0,License: MIT
0.10.0,FIXME: to be removed in 0.12
0.10.0,The ratio is computed using a one-vs-rest manner. Using majority
0.10.0,in multi-class would lead to slightly different results at the
0.10.0,cost of introducing a new parameter.
0.10.0,rounding may cause new amount for n_samples
0.10.0,the nearest neighbors need to be fitted only on the current class
0.10.0,to find the class NN to generate new samples
0.10.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.10.0,Christos Aridas
0.10.0,License: MIT
0.10.0,smoothed bootstrap imposes to make numerical operation; we need
0.10.0,to be sure to have only numerical data in X
0.10.0,generate a smoothed bootstrap with a perturbation
0.10.0,generate a bootstrap
0.10.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.10.0,Christos Aridas
0.10.0,License: MIT
0.10.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.10.0,Fernando Nogueira
0.10.0,Christos Aridas
0.10.0,License: MIT
0.10.0,negate diagonal elements
0.10.0,identify cluster which are answering the requirements
0.10.0,the cluster is already considered balanced
0.10.0,not enough samples to apply SMOTE
0.10.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.10.0,Fernando Nogueira
0.10.0,Christos Aridas
0.10.0,Dzianis Dudnik
0.10.0,License: MIT
0.10.0,FIXME: to be removed in 0.12
0.10.0,divergence between borderline-1 and borderline-2
0.10.0,Create synthetic samples for borderline points.
0.10.0,only minority
0.10.0,we use a one-vs-rest policy to handle the multiclass in which
0.10.0,new samples will be created considering not only the majority
0.10.0,class but all over classes.
0.10.0,FIXME: to be removed in 0.12
0.10.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.10.0,Fernando Nogueira
0.10.0,Christos Aridas
0.10.0,Dzianis Dudnik
0.10.0,License: MIT
0.10.0,np.newaxis for backwards compatability with random_state
0.10.0,Samples are in danger for m/2 <= m' < m
0.10.0,Samples are noise for m = m'
0.10.0,FIXME: to be removed in 0.12
0.10.0,FIXME: to be removed in 0.12
0.10.0,compute the median of the standard deviation of the minority class
0.10.0,scikit-learn >= 1.2
0.10.0,the input of the OneHotEncoder needs to be dense
0.10.0,we can replace the 1 entries of the categorical features with the
0.10.0,median of the standard deviation. It will ensure that whenever
0.10.0,"distance is computed between 2 samples, the difference will be equal"
0.10.0,to the median of the standard deviation as in the original paper.
0.10.0,"In the edge case where the median of the std is equal to 0, the 1s"
0.10.0,"entries will be also nullified. In this case, we store the original"
0.10.0,categorical encoding which will be later used for inversing the OHE
0.10.0,reverse the encoding of the categorical features
0.10.0,the matrix is supposed to be in the CSR format after the stacking
0.10.0,change in sparsity structure more efficient with LIL than CSR
0.10.0,convert to dense array since scipy.sparse doesn't handle 3D
0.10.0,"In the case that the median std was equal to zeros, we have to"
0.10.0,create non-null entry based on the encoded of OHE
0.10.0,tie breaking argmax
0.10.0,generate sample indices that will be used to generate new samples
0.10.0,"for each drawn samples, select its k-neighbors and generate a sample"
0.10.0,"where for each feature individually, each category generated is the"
0.10.0,most common category
0.10.0,FIXME: to be removed in 0.12
0.10.0,the kneigbors search will include the sample itself which is
0.10.0,expected from the original algorithm
0.10.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.10.0,Christos Aridas
0.10.0,Dzianis Dudnik
0.10.0,License: MIT
0.10.0,create 2 random continuous feature
0.10.0,create a categorical feature using some string
0.10.0,create a categorical feature using some integer
0.10.0,return the categories
0.10.0,create 2 random continuous feature
0.10.0,create a categorical feature using some string
0.10.0,create a categorical feature using some integer
0.10.0,return the categories
0.10.0,create 2 random continuous feature
0.10.0,create a categorical feature using some string
0.10.0,create a categorical feature using some integer
0.10.0,return the categories
0.10.0,create 2 random continuous feature
0.10.0,create a categorical feature using some string
0.10.0,create a categorical feature using some integer
0.10.0,return the categories
0.10.0,create 2 random continuous feature
0.10.0,create a categorical feature using some string
0.10.0,create a categorical feature using some integer
0.10.0,part of the common test which apply to SMOTE-NC even if it is not default
0.10.0,constructible
0.10.0,Check that the samplers handle pandas dataframe and pandas series
0.10.0,FIXME: we should use to_numpy with pandas >= 0.25
0.10.0,Cast X and y to not default dtype
0.10.0,Non-regression test for #662
0.10.0,https://github.com/scikit-learn-contrib/imbalanced-learn/issues/662
0.10.0,check that the categorical feature is not random but correspond to the
0.10.0,categories seen in the minority class samples
0.10.0,overall check for SMOTEN
0.10.0,check if the SMOTEN resample data as expected
0.10.0,"we generate data such that ""not apple"" will be the minority class and"
0.10.0,"samples from this class will be generated. We will force the ""blue"""
0.10.0,"category to be associated with this class. Therefore, the new generated"
0.10.0,"samples should as well be from the ""blue"" category."
0.10.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.10.0,Christos Aridas
0.10.0,License: MIT
0.10.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.10.0,Christos Aridas
0.10.0,License: MIT
0.10.0,FIXME: we should use to_numpy with pandas >= 0.25
0.10.0,check the random over-sampling with a multiclass problem
0.10.0,check that resampling with heterogeneous dtype is working with basic
0.10.0,resampling
0.10.0,check that we can oversample even with missing or infinite data
0.10.0,regression tests for #605
0.10.0,check that we raise an error when heterogeneous dtype data are given
0.10.0,and a smoothed bootstrap is requested
0.10.0,check that smoothed bootstrap is working for numerical array
0.10.0,check that a shrinkage factor of 0 is equivalent to not create a smoothed
0.10.0,bootstrap
0.10.0,check the behaviour of the shrinkage parameter
0.10.0,the covariance of the data generated with the larger shrinkage factor
0.10.0,should also be larger.
0.10.0,check the validation of the shrinkage parameter
0.10.0,check that m_neighbors is properly set. Regression test for:
0.10.0,https://github.com/scikit-learn-contrib/imbalanced-learn/issues/568
0.10.0,FIXME: to be removed in 0.12
0.10.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.10.0,Christos Aridas
0.10.0,License: MIT
0.10.0,shuffle the indices since the sampler are packing them by class
0.10.0,helper functions
0.10.0,input and output
0.10.0,build the model and weights
0.10.0,"build the loss, predict, and train operator"
0.10.0,Initialization of all variables in the graph
0.10.0,"For each epoch, run accuracy on train and test"
0.10.0,helper functions
0.10.0,input and output
0.10.0,build the model and weights
0.10.0,"build the loss, predict, and train operator"
0.10.0,Initialization of all variables in the graph
0.10.0,"For each epoch, run accuracy on train and test"
0.10.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.10.0,License: MIT
0.10.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.10.0,Fernando Nogueira
0.10.0,Christos Aridas
0.10.0,License: MIT
0.10.0,find which class to not consider
0.10.0,there is a Tomek link between two samples if they are both nearest
0.10.0,neighbors of each others.
0.10.0,Find the nearest neighbour of every point
0.10.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.10.0,Christos Aridas
0.10.0,License: MIT
0.10.0,Randomly get one sample from the majority class
0.10.0,Generate the index to select
0.10.0,Create the set C - One majority samples and all minority
0.10.0,Create the set S - all majority samples
0.10.0,fit knn on C
0.10.0,Check each sample in S if we keep it or drop it
0.10.0,Do not select sample which are already well classified
0.10.0,Classify on S
0.10.0,If the prediction do not agree with the true label
0.10.0,append it in C_x
0.10.0,Keep the index for later
0.10.0,Update C
0.10.0,fit a knn on C
0.10.0,This experimental to speed up the search
0.10.0,Classify all the element in S and avoid to test the
0.10.0,well classified elements
0.10.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.10.0,Dayvid Oliveira
0.10.0,Christos Aridas
0.10.0,License: MIT
0.10.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.10.0,Christos Aridas
0.10.0,License: MIT
0.10.0,Compute the distance considering the farthest neighbour
0.10.0,Sort the list of distance and get the index
0.10.0,Throw a warning to tell the user that we did not have enough samples
0.10.0,to select and that we just select everything
0.10.0,Select the desired number of samples
0.10.0,idx_tmp is relative to the feature selected in the
0.10.0,previous step and we need to find the indirection
0.10.0,fmt: off
0.10.0,fmt: on
0.10.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.10.0,Christos Aridas
0.10.0,License: MIT
0.10.0,select a sample from the current class
0.10.0,create the set composed of all minority samples and one
0.10.0,sample from the current class.
0.10.0,create the set S with removing the seed from S
0.10.0,since that it will be added anyway
0.10.0,apply Tomek cleaning
0.10.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.10.0,Dayvid Oliveira
0.10.0,Christos Aridas
0.10.0,License: MIT
0.10.0,Check the stopping criterion
0.10.0,1. If there is no changes for the vector y
0.10.0,2. If the number of samples in the other class become inferior to
0.10.0,the number of samples in the majority class
0.10.0,3. If one of the class is disappearing
0.10.0,Case 1
0.10.0,Case 2
0.10.0,Case 3
0.10.0,Check the stopping criterion
0.10.0,1. If the number of samples in the other class become inferior to
0.10.0,the number of samples in the majority class
0.10.0,2. If one of the class is disappearing
0.10.0,Case 1else:
0.10.0,overwrite b_min_bec_maj
0.10.0,Case 2
0.10.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.10.0,Christos Aridas
0.10.0,License: MIT
0.10.0,clean the neighborhood
0.10.0,compute which classes to consider for cleaning for the A2 group
0.10.0,compute a2 group
0.10.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.10.0,Christos Aridas
0.10.0,License: MIT
0.10.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.10.0,Christos Aridas
0.10.0,License: MIT
0.10.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.10.0,Christos Aridas
0.10.0,License: MIT
0.10.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.10.0,Christos Aridas
0.10.0,License: MIT
0.10.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.10.0,Christos Aridas
0.10.0,License: MIT
0.10.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.10.0,Christos Aridas
0.10.0,License: MIT
0.10.0,FIXME: we should use to_numpy with pandas >= 0.25
0.10.0,check that we can undersample even with missing or infinite data
0.10.0,regression tests for #605
0.10.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.10.0,Christos Aridas
0.10.0,License: MIT
0.10.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.10.0,Christos Aridas
0.10.0,License: MIT
0.10.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.10.0,Christos Aridas
0.10.0,License: MIT
0.10.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.10.0,Christos Aridas
0.10.0,License: MIT
0.10.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.10.0,Christos Aridas
0.10.0,License: MIT
0.10.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.10.0,Fernando Nogueira
0.10.0,Christos Aridas
0.10.0,License: MIT
0.10.0,check that the samples selecting by the hard voting corresponds to the
0.10.0,targeted class
0.10.0,non-regression test for:
0.10.0,https://github.com/scikit-learn-contrib/imbalanced-learn/issues/738
0.10.0,Generate valid values for the required parameters
0.10.0,The parameters `*args` and `**kwargs` are ignored since we cannot generate
0.10.0,constraints.
0.10.0,check that there is a constraint for each parameter
0.10.0,this object does not have a valid type for sure for all params
0.10.0,This parameter is not validated
0.10.0,"First, check that the error is raised if param doesn't match any valid type."
0.10.0,"Then, for constraints that are more than a type constraint, check that the"
0.10.0,error is raised if param does match a valid type but does not match any valid
0.10.0,value for this type.
0.10.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.10.0,Christos Aridas
0.10.0,License: MIT
0.10.0,test that all_estimators doesn't find abstract classes.
0.10.0,"For NearMiss, let's check the three algorithms"
0.10.0,Common tests for estimator instances
0.10.0,Authors: Alexandre Gramfort <alexandre.gramfort@inria.fr>
0.10.0,Raghav RV <rvraghav93@gmail.com>
0.10.0,License: BSD 3 clause
0.10.0,scikit-learn >= 1.2
0.10.0,"walk_packages() ignores DeprecationWarnings, now we need to ignore"
0.10.0,FutureWarnings
0.10.0,"mypy error: Module has no attribute ""__path__"""
0.10.0,functions to ignore args / docstring of
0.10.0,Methods where y param should be ignored if y=None by default
0.10.0,numpydoc 0.8.0's docscrape tool raises because of collections.abc under
0.10.0,Python 3.7
0.10.0,Test module docstring formatting
0.10.0,Skip test if numpydoc is not found
0.10.0,XXX unreached code as of v0.22
0.10.0,"pytest tooling, not part of the scikit-learn API"
0.10.0,Exclude non-scikit-learn classes
0.10.0,Now skip docstring test for y when y is None
0.10.0,by default for API reason
0.10.0,Exclude imported functions
0.10.0,Don't test private methods / functions
0.10.0,Test that there are no tabs in our source files
0.10.0,because we don't import
0.10.0,Minimal / degenerate instances: only useful to test the docstrings.
0.10.0,"As certain attributes are present ""only"" if a certain parameter is"
0.10.0,"provided, this checks if the word ""only"" is present in the attribute"
0.10.0,"description, and if not the attribute is required to be present."
0.10.0,ignore deprecation warnings
0.10.0,attributes
0.10.0,properties
0.10.0,ignore properties that raises an AttributeError and deprecated
0.10.0,properties
0.10.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.10.0,License: MIT
0.10.0,check that we can let a pass a regression variable by turning down the
0.10.0,validation
0.10.0,Check that the validation is bypass when calling `fit`
0.10.0,Non-regression test for:
0.10.0,https://github.com/scikit-learn-contrib/imbalanced-learn/issues/782
0.10.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.10.0,Christos Aridas
0.10.0,License: MIT
0.10.0,store timestamp to figure out whether the result of 'fit' has been
0.10.0,cached or not
0.10.0,store timestamp to figure out whether the result of 'fit' has been
0.10.0,cached or not
0.10.0,Pipeline accepts steps as tuple
0.10.0,Test the various init parameters of the pipeline.
0.10.0,Check that we can't instantiate pipelines with objects without fit
0.10.0,method
0.10.0,Smoke test with only an estimator
0.10.0,Check that params are set
0.10.0,Smoke test the repr:
0.10.0,Test with two objects
0.10.0,Check that we can't instantiate with non-transformers on the way
0.10.0,"Note that NoTrans implements fit, but not transform"
0.10.0,Check that params are set
0.10.0,Smoke test the repr:
0.10.0,Check that params are not set when naming them wrong
0.10.0,Test clone
0.10.0,"Check that apart from estimators, the parameters are the same"
0.10.0,Remove estimators that where copied
0.10.0,Test the various methods of the pipeline (anova).
0.10.0,Test with Anova + LogisticRegression
0.10.0,Test that the pipeline can take fit parameters
0.10.0,classifier should return True
0.10.0,and transformer params should not be changed
0.10.0,invalid parameters should raise an error message
0.10.0,Pipeline should pass sample_weight
0.10.0,When sample_weight is None it shouldn't be passed
0.10.0,Test pipeline raises set params error message for nested models.
0.10.0,nested model check
0.10.0,Test the various methods of the pipeline (pca + svm).
0.10.0,Test with PCA + SVC
0.10.0,Test the various methods of the pipeline (preprocessing + svm).
0.10.0,check shapes of various prediction functions
0.10.0,test that the fit_predict method is implemented on a pipeline
0.10.0,test that the fit_predict on pipeline yields same results as applying
0.10.0,transform and clustering steps separately
0.10.0,"As pipeline doesn't clone estimators on construction,"
0.10.0,it must have its own estimators
0.10.0,first compute the transform and clustering step separately
0.10.0,use a pipeline to do the transform and clustering in one step
0.10.0,tests that a pipeline does not have fit_predict method when final
0.10.0,step of pipeline does not have fit_predict defined
0.10.0,tests that Pipeline passes fit_params to intermediate steps
0.10.0,when fit_predict is invoked
0.10.0,Test whether pipeline works with a transformer at the end.
0.10.0,Also test pipeline.transform and pipeline.inverse_transform
0.10.0,test transform and fit_transform:
0.10.0,Test whether pipeline works with a transformer missing fit_transform
0.10.0,test fit_transform:
0.10.0,Directly setting attr
0.10.0,Using set_params
0.10.0,Using set_params to replace single step
0.10.0,With invalid data
0.10.0,Test setting Pipeline steps to None
0.10.0,"for other methods, ensure no AttributeErrors on None:"
0.10.0,mult2 and mult3 are active
0.10.0,Check 'passthrough' step at construction time
0.10.0,Test with Transformer + SVC
0.10.0,Memoize the transformer at the first fit
0.10.0,Get the time stamp of the tranformer in the cached pipeline
0.10.0,Check that cached_pipe and pipe yield identical results
0.10.0,Check that we are reading the cache while fitting
0.10.0,a second time
0.10.0,Check that cached_pipe and pipe yield identical results
0.10.0,Create a new pipeline with cloned estimators
0.10.0,Check that even changing the name step does not affect the cache hit
0.10.0,Check that cached_pipe and pipe yield identical results
0.10.0,Test with Transformer + SVC
0.10.0,Memoize the transformer at the first fit
0.10.0,Get the time stamp of the tranformer in the cached pipeline
0.10.0,Check that cached_pipe and pipe yield identical results
0.10.0,Check that we are reading the cache while fitting
0.10.0,a second time
0.10.0,Check that cached_pipe and pipe yield identical results
0.10.0,Create a new pipeline with cloned estimators
0.10.0,Check that even changing the name step does not affect the cache hit
0.10.0,Check that cached_pipe and pipe yield identical results
0.10.0,Test the various methods of the pipeline (pca + svm).
0.10.0,Test with PCA + SVC
0.10.0,Test the various methods of the pipeline (pca + svm).
0.10.0,Test with PCA + SVC
0.10.0,Test whether pipeline works with a sampler at the end.
0.10.0,Also test pipeline.sampler
0.10.0,test transform and fit_transform:
0.10.0,We round the value near to zero. It seems that PCA has some issue
0.10.0,with that
0.10.0,Test whether pipeline works with a sampler at the end.
0.10.0,Also test pipeline.sampler
0.10.0,Test pipeline using None as preprocessing step and a classifier
0.10.0,"Test pipeline using None, RUS and a classifier"
0.10.0,"Test pipeline using RUS, None and a classifier"
0.10.0,Test pipeline using None step and a sampler
0.10.0,Test pipeline using None and a transformer that implements transform and
0.10.0,inverse_transform
0.10.0,Test the various methods of the pipeline (anova).
0.10.0,Test with RandomUnderSampling + Anova + LogisticRegression
0.10.0,Test the various methods of the pipeline (anova).
0.10.0,Test the various methods of the pipeline (anova).
0.10.0,Test with RandomUnderSampling + Anova + LogisticRegression
0.10.0,tests that Pipeline passes predict_params to the final estimator
0.10.0,when predict is invoked
0.10.0,Test that the score_samples method is implemented on a pipeline.
0.10.0,Test that the score_samples method on pipeline yields same results as
0.10.0,applying transform and score_samples steps separately.
0.10.0,Check the shapes
0.10.0,Check the values
0.10.0,Test that a pipeline does not have score_samples method when the final
0.10.0,step of the pipeline does not have score_samples defined.
0.10.0,Test that the score_samples method is implemented on a pipeline.
0.10.0,Test that the score_samples method on pipeline yields same results as
0.10.0,applying transform and score_samples steps separately.
0.10.0,Check the shapes
0.10.0,Check the values
0.10.0,transformer will not change `y` and sampler will always preserve the type of `y`
0.10.0,transformer will not change `y` and sampler will always preserve the type of `y`
0.10.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.10.0,Christos Aridas
0.10.0,License: MIT
0.10.0,TODO: Remove when SciPy 1.9 is the minimum supported version
0.10.0,TODO: Remove when scikit-learn 1.1 is the minimum supported version
0.10.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.10.0,License: MIT
0.10.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.10.0,License: MIT
0.10.0,Adapated from scikit-learn
0.10.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.10.0,License: MIT
0.10.0,scikit-learn >= 1.2
0.10.0,we don't filter samplers based on their tag here because we want to make
0.10.0,sure that the fitted attribute does not exist if the tag is not
0.10.0,stipulated
0.10.0,trigger our checks if this is a SamplerMixin
0.10.0,should raise warning if the target is continuous (we cannot raise error)
0.10.0,if the target is multilabel then we should raise an error
0.10.0,IHT does not enforce the number of samples but provide a number
0.10.0,of samples the closest to the desired target.
0.10.0,in this test we will force all samplers to not change the class 1
0.10.0,check that sparse matrices can be passed through the sampler leading to
0.10.0,the same results than dense
0.10.0,Check that the samplers handle pandas dataframe and pandas series
0.10.0,check that we return the same type for dataframes or series types
0.10.0,FIXME: we should use to_numpy with pandas >= 0.25
0.10.0,Check that the can samplers handle simple lists
0.10.0,Check that multiclass target lead to the same results than OVA encoding
0.10.0,Cast X and y to not default dtype
0.10.0,Non-regression test for #709
0.10.0,https://github.com/scikit-learn-contrib/imbalanced-learn/issues/709
0.10.0,Check that an informative error is raised when the value of a constructor
0.10.0,parameter does not have an appropriate type or value.
0.10.0,check that there is a constraint for each parameter
0.10.0,this object does not have a valid type for sure for all params
0.10.0,This parameter is not validated
0.10.0,"First, check that the error is raised if param doesn't match any valid type."
0.10.0,the method is not accessible with the current set of parameters
0.10.0,The estimator is a label transformer and take only `y`
0.10.0,"Then, for constraints that are more than a type constraint, check that the"
0.10.0,error is raised if param does match a valid type but does not match any valid
0.10.0,value for this type.
0.10.0,the method is not accessible with the current set of parameters
0.10.0,The estimator is a label transformer and take only `y`
0.10.0,Check that calling `fit` does not raise any warnings about feature names.
0.10.0,Only check imblearn estimators for feature_names_in_ in docstring
0.10.0,partial_fit checks on second call
0.10.0,Do not call partial fit if early_stopping is on
0.10.0,input_features names is not the same length as n_features_in_
0.10.0,error is raised when `input_features` do not match feature_names_in
0.10.0,Adapted from scikit-learn
0.10.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.10.0,License: MIT
0.10.0,Ignore deprecation warnings triggered at import time and from walking
0.10.0,packages
0.10.0,get rid of abstract base classes
0.10.0,get rid of sklearn estimators which have been imported in some classes
0.10.0,"drop duplicates, sort for reproducibility"
0.10.0,itemgetter is used to ensure the sort does not extend to the 2nd item of
0.10.0,the tuple
0.10.0,mypy: ignore-errors
0.10.0,update the docstring of the descriptor
0.10.0,"delegate only on instances, not the classes."
0.10.0,this is to allow access to the docstrings.
0.10.0,This makes it possible to use the decorated method as an
0.10.0,"unbound method, for instance when monkeypatching."
0.10.0,mypy: ignore-errors
0.10.0,TODO: remove `if True` when we have clear support for:
0.10.0,- ignoring `*args` and `**kwargs` in the signature
0.10.0,We allow parameters to not have a constraint so that third party
0.10.0,estimators can inherit from sklearn estimators without having to
0.10.0,necessarily use the validation tools.
0.10.0,"this constraint is satisfied, no need to check further."
0.10.0,"No constraint is satisfied, raise with an informative message."
0.10.0,"Ignore constraints that we don't want to expose in the error message,"
0.10.0,i.e. options that are for internal purpose or not officially
0.10.0,supported.
0.10.0,The dict of parameter constraints is set as an attribute of the function
0.10.0,to make it possible to dynamically introspect the constraints for
0.10.0,automatic testing.
0.10.0,Map *args/**kwargs to the function signature
0.10.0,ignore self/cls and positional/keyword markers
0.10.0,TODO(1.4) remove support for Integral.
0.10.0,we use an interval of Real to ignore np.nan that has its own
0.10.0,constraint
0.10.0,constraint is an interval
0.10.0,generate a non-integer value such that it can't be valid even if there's
0.10.0,also an integer interval constraint.
0.10.0,We need to check if there's also a real interval constraint to generate a
0.10.0,value that is not valid for any of the 2 interval constraints.
0.10.0,Only the integer interval constraint -> easy
0.10.0,There's also a real interval constraint. Try to find a value left to both
0.10.0,or right to both or in between them.
0.10.0,redefine left and right bounds to be smallest and largest valid integers
0.10.0,in both intervals.
0.10.0,there exists an int left to both intervals
0.10.0,there exists an int right to both intervals
0.10.0,there exists an int between the 2 intervals
0.10.0,there exists an int between the 2 intervals
0.10.0,Author: Alexander L. Hayes <hayesall@iu.edu>
0.10.0,License: MIT
0.10.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.10.0,License: MIT
0.10.0,_is_neighbors_object(nn_object)
0.10.0,check that all keys in sampling_strategy are also in y
0.10.0,check that there is no negative number
0.10.0,check that all keys in sampling_strategy are also in y
0.10.0,ignore first 'self' argument for instance methods
0.10.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.10.0,Christos Aridas
0.10.0,License: MIT
0.10.0,this function could create an equal number of samples
0.10.0,We pass on purpose a non sorted dictionary and check that the resulting
0.10.0,dictionary is sorted. Refer to issue #428.
0.10.0,DataFrame and DataFrame case
0.10.0,DataFrames and Series case
0.10.0,The * is place before a keyword only argument without a default value
0.10.0,Test that the minimum dependencies in the README.rst file are
0.10.0,consistent with the minimum dependencies defined at the file:
0.10.0,imblearn/_min_dependencies.py
0.10.0,Skip the test if the README.rst file is not available.
0.10.0,"For instance, when installing scikit-learn from wheels"
0.10.0,Author: Alexander L. Hayes <hayesall@iu.edu>
0.10.0,License: MIT
0.10.0,Some helpers for the tests
0.10.0,check in the presence of extra positional and keyword args
0.10.0,outer decorator does not interfer with validation
0.10.0,validated method can be decorated
0.10.0,no validation in init
0.10.0,list and dict are valid params
0.10.0,the list option is not exposed in the error message
0.10.0,"""auto"" and ""warn"" are valid params"
0.10.0,"the ""warn"" option is not exposed in the error message"
0.10.0,True/False and np.bool_(True/False) are valid params
0.10.0,an int is also valid but deprecated
0.10.0,param1 is validated
0.10.0,param2 is not validated: any type is valid.
0.10.0,"does not raise, even though ""b"" is not in the constraints dict and ""a"" is not"
0.10.0,a parameter of the estimator.
0.10.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.10.0,License: MIT
0.10.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.10.0,Christos Aridas
0.10.0,License: MIT
0.10.0,check if the filtering is working with a list or a single string
0.10.0,check that all estimators are sampler
0.10.0,check that an error is raised when the type is unknown
0.10.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.10.0,License: MIT
0.10.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.10.0,Christos Aridas
0.10.0,License: MIT
0.10.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.10.0,Christos Aridas
0.10.0,License: MIT
0.10.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.10.0,Christos Aridas
0.10.0,License: MIT
0.10.0,Check if default job count is None
0.10.0,Check if job count is set
0.10.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.10.0,Christos Aridas
0.10.0,License: MIT
0.10.0,Check if default job count is none
0.10.0,Check if job count is set
0.10.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.10.0,License: MIT
0.10.0,resample before to fit the tree
0.10.0,TODO: remove when the minimum version of scikit-learn supported is 1.1
0.10.0,make a deepcopy to not modify the original dictionary
0.10.0,scikit-learn >= 1.2
0.10.0,scikit-learn < 1.2
0.10.0,Validate or convert input data
0.10.0,Pre-sort indices to avoid that each individual tree of the
0.10.0,ensemble sorts the indices.
0.10.0,reshape is necessary to preserve the data contiguity against vs
0.10.0,"[:, np.newaxis] that does not."
0.10.0,Get bootstrap sample size
0.10.0,Check parameters
0.10.0,"Free allocated memory, if any"
0.10.0,We draw from the random state to get the random state we
0.10.0,would have got if we hadn't used a warm_start.
0.10.0,Parallel loop: we prefer the threading backend as the Cython code
0.10.0,for fitting the trees is internally releasing the Python GIL
0.10.0,making threading more efficient than multiprocessing in
0.10.0,"that case. However, we respect any parallel_backend contexts set"
0.10.0,"at a higher level, since correctness does not rely on using"
0.10.0,threads.
0.10.0,Collect newly grown trees
0.10.0,Create pipeline with the fitted samplers and trees
0.10.0,FIXME: we could consider to support multiclass-multioutput if
0.10.0,we introduce or reuse a constructor parameter (e.g.
0.10.0,oob_score) allowing our user to pass a callable defining the
0.10.0,scoring strategy on OOB sample.
0.10.0,Decapsulate classes_ attributes
0.10.0,drop the n_outputs axis if there is a single output
0.10.0,Prediction requires X to be in CSR format
0.10.0,n_classes_ is a ndarray at this stage
0.10.0,all the supported type of target will have the same number of
0.10.0,classes in all outputs
0.10.0,"for regression, n_classes_ does not exist and we create an empty"
0.10.0,axis to be consistent with the classification case and make
0.10.0,the array operations compatible with the 2 settings
0.10.0,TODO: remove when supporting scikit-learn>=1.4
0.10.0,TODO: remove when supporting scikit-learn>=1.2
0.10.0,make a deepcopy to not modify the original dictionary
0.10.0,scikit-learn >= 1.2
0.10.0,TODO: remove when supporting scikit-learn>=1.2
0.10.0,scikit-learn < 1.2
0.10.0,SAMME-R requires predict_proba-enabled estimators
0.10.0,Instances incorrectly classified
0.10.0,Error fraction
0.10.0,Stop if classification is perfect
0.10.0,Construct y coding as described in Zhu et al [2]:
0.10.0,
0.10.0,y_k = 1 if c == k else -1 / (K - 1)
0.10.0,
0.10.0,"where K == n_classes_ and c, k in [0, K) are indices along the second"
0.10.0,axis of the y coding with c being the index corresponding to the true
0.10.0,class label.
0.10.0,Displace zero probabilities so the log is defined.
0.10.0,Also fix negative elements which may occur with
0.10.0,negative sample weights.
0.10.0,Boost weight using multi-class AdaBoost SAMME.R alg
0.10.0,Only boost the weights if it will fit again
0.10.0,Only boost positive weights
0.10.0,Instances incorrectly classified
0.10.0,Error fraction
0.10.0,Stop if classification is perfect
0.10.0,Stop if the error is at least as bad as random guessing
0.10.0,Boost weight using multi-class AdaBoost SAMME alg
0.10.0,Only boost the weights if I will fit again
0.10.0,Only boost positive weights
0.10.0,TODO: remove when supporting scikit-learn>=1.4
0.10.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.10.0,Christos Aridas
0.10.0,License: MIT
0.10.0,make a deepcopy to not modify the original dictionary
0.10.0,scikit-learn >= 1.2
0.10.0,TODO: remove when supporting scikit-learn>=1.2
0.10.0,scikit-learn < 1.2
0.10.0,TODO: remove when supporting scikit-learn>=1.4
0.10.0,TODO: remove when supporting scikit-learn>=1.2
0.10.0,overwrite the base class method by disallowing `sample_weight`
0.10.0,RandomUnderSampler is not supporting sample_weight. We need to pass
0.10.0,None.
0.10.0,TODO: remove when minimum supported version of scikit-learn is 1.1
0.10.0,Check data
0.10.0,Parallel loop
0.10.0,Reduce
0.10.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.10.0,Christos Aridas
0.10.0,License: MIT
0.10.0,make a deepcopy to not modify the original dictionary
0.10.0,scikit-learn >= 1.2
0.10.0,TODO: remove when supporting scikit-learn>=1.2
0.10.0,scikit-learn < 1.2
0.10.0,TODO: remove when supporting scikit-learn>=1.4
0.10.0,TODO: remove when supporting scikit-learn>=1.2
0.10.0,overwrite the base class method by disallowing `sample_weight`
0.10.0,the sampler needs to be validated before to call _fit because
0.10.0,_validate_y is called before _validate_estimator and would require
0.10.0,to know which type of sampler we are using.
0.10.0,RandomUnderSampler is not supporting sample_weight. We need to pass
0.10.0,None.
0.10.0,TODO: remove when minimum supported version of scikit-learn is 1.1
0.10.0,Check data
0.10.0,Parallel loop
0.10.0,Reduce
0.10.0,check that we have an ensemble of samplers and estimators with a
0.10.0,consistent size
0.10.0,each sampler in the ensemble should have different random state
0.10.0,each estimator in the ensemble should have different random state
0.10.0,check the consistency of the feature importances
0.10.0,check the consistency of the prediction outpus
0.10.0,Predictions should be the same when sample_weight are all ones
0.10.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.10.0,Christos Aridas
0.10.0,License: MIT
0.10.0,Check classification for various parameter settings.
0.10.0,Test that bootstrapping samples generate non-perfect base estimators.
0.10.0,"without bootstrap, all trees are perfect on the training set"
0.10.0,disable the resampling by passing an empty dictionary.
0.10.0,"with bootstrap, trees are no longer perfect on the training set"
0.10.0,Test that bootstrapping features may generate duplicate features.
0.10.0,Predict probabilities.
0.10.0,Normal case
0.10.0,"Degenerate case, where some classes are missing"
0.10.0,Check that oob prediction is a good estimation of the generalization
0.10.0,error.
0.10.0,Test with few estimators
0.10.0,Check singleton ensembles.
0.10.0,Check that bagging ensembles can be grid-searched.
0.10.0,Transform iris into a binary classification task
0.10.0,Grid search with scoring based on decision_function
0.10.0,Check estimator and its default values.
0.10.0,Test if fitting incrementally with warm start gives a forest of the
0.10.0,right size and the same results as a normal fit.
0.10.0,Test if warm start'ed second fit with smaller n_estimators raises error.
0.10.0,Test that nothing happens when fitting without increasing n_estimators
0.10.0,"modify X to nonsense values, this should not change anything"
0.10.0,warm started classifier with 5+5 estimators should be equivalent to
0.10.0,one classifier with 10 estimators
0.10.0,Check using oob_score and warm_start simultaneously fails
0.10.0,"Make sure OOB scores are identical when random_state, estimator, and"
0.10.0,training data are fixed and fitting is done twice
0.10.0,Check that format of estimators_samples_ is correct and that results
0.10.0,generated at fit time can be identically reproduced at a later time
0.10.0,using data saved in object attributes.
0.10.0,remap the y outside of the BalancedBaggingclassifier
0.10.0,"_, y = np.unique(y, return_inverse=True)"
0.10.0,Get relevant attributes
0.10.0,Test for correct formatting
0.10.0,Re-fit single estimator to test for consistent sampling
0.10.0,Make sure validated max_samples and original max_samples are identical
0.10.0,when valid integer max_samples supplied by user
0.10.0,check that we can pass any kind of sampler to a bagging classifier
0.10.0,check that we have balanced class with the right counts of class
0.10.0,sample depending on the sampling strategy
0.10.0,check that we can provide a FunctionSampler in BalancedBaggingClassifier
0.10.0,find the minority and majority classes
0.10.0,compute the number of sample to draw from the majority class using
0.10.0,a negative binomial distribution
0.10.0,draw randomly with or without replacement
0.10.0,Roughly Balanced Bagging
0.10.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.10.0,Christos Aridas
0.10.0,License: MIT
0.10.0,Generate a global dataset to use
0.10.0,Check classification for various parameter settings.
0.10.0,test the different prediction function
0.10.0,Check estimator and its default values.
0.10.0,Test if fitting incrementally with warm start gives a forest of the
0.10.0,right size and the same results as a normal fit.
0.10.0,Test if warm start'ed second fit with smaller n_estimators raises error.
0.10.0,Test that nothing happens when fitting without increasing n_estimators
0.10.0,"modify X to nonsense values, this should not change anything"
0.10.0,warm started classifier with 5+5 estimators should be equivalent to
0.10.0,one classifier with 10 estimators
0.10.0,Check warning if not enough estimators
0.10.0,First fit with no restriction on max samples
0.10.0,Second fit with max samples restricted to just 2
0.10.0,Regression test for #655: check that the oob score is closed to 0.5
0.10.0,a binomial experiment.
0.10.0,Author: Guillaume Lemaitre
0.10.0,License: BSD 3 clause
0.10.0,"The index start at one, then we need to remove one"
0.10.0,to not have issue with the indexing.
0.10.0,go through the list and check if the data are available
0.10.0,Authors: Dayvid Oliveira
0.10.0,Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.10.0,Christos Aridas
0.10.0,License: MIT
0.10.0,restrict ratio to be a dict or a callable
0.10.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.10.0,Christos Aridas
0.10.0,License: MIT
0.10.0,"we are reusing part of utils.check_sampling_strategy, however this is not"
0.10.0,cover in the common tests so we will repeat it here
0.10.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.10.0,Christos Aridas
0.10.0,License: MIT
0.10.0,This is a trick to avoid an error during tests collection with pytest. We
0.10.0,avoid the error when importing the package raise the error at the moment of
0.10.0,creating the instance.
0.10.0,This is a trick to avoid an error during tests collection with pytest. We
0.10.0,avoid the error when importing the package raise the error at the moment of
0.10.0,creating the instance.
0.10.0,flag for keras sequence duck-typing
0.10.0,shuffle the indices since the sampler are packing them by class
0.9.1,This file is here so that when running from the root folder
0.9.1,./imblearn is added to sys.path by pytest.
0.9.1,See https://docs.pytest.org/en/latest/pythonpath.html for more details.
0.9.1,"For example, this allows to build extensions in place and run pytest"
0.9.1,doc/modules/clustering.rst and use imblearn from the local folder
0.9.1,rather than the one from site-packages.
0.9.1,! /usr/bin/env python
0.9.1,Python 2 compat: just to be able to declare that Python >=3.7 is needed.
0.9.1,This is a bit (!) hackish: we are setting a global variable so that the
0.9.1,main imblearn __init__ can detect if it is being loaded by the setup
0.9.1,"routine, to avoid attempting to load components that aren't built yet:"
0.9.1,the numpy distutils extensions that are used by imbalanced-learn to
0.9.1,recursively build the compiled extensions in sub-packages is based on the
0.9.1,Python import machinery.
0.9.1,get __version__ from _version.py
0.9.1,-*- coding: utf-8 -*-
0.9.1,
0.9.1,"imbalanced-learn documentation build configuration file, created by"
0.9.1,sphinx-quickstart on Mon Jan 18 14:44:12 2016.
0.9.1,
0.9.1,This file is execfile()d with the current directory set to its
0.9.1,containing dir.
0.9.1,
0.9.1,Note that not all possible configuration values are present in this
0.9.1,autogenerated file.
0.9.1,
0.9.1,All configuration values have a default; values that are commented out
0.9.1,serve to show the default.
0.9.1,"If extensions (or modules to document with autodoc) are in another directory,"
0.9.1,add these directories to sys.path here. If the directory is relative to the
0.9.1,"documentation root, use os.path.abspath to make it absolute, like shown here."
0.9.1,-- General configuration ------------------------------------------------
0.9.1,"If your documentation needs a minimal Sphinx version, state it here."
0.9.1,needs_sphinx = '1.0'
0.9.1,"Add any Sphinx extension module names here, as strings. They can be"
0.9.1,extensions coming with Sphinx (named 'sphinx.ext.*') or your custom
0.9.1,ones.
0.9.1,"Add any paths that contain templates here, relative to this directory."
0.9.1,The suffix of source filenames.
0.9.1,The master toctree document.
0.9.1,General information about the project.
0.9.1,"The version info for the project you're documenting, acts as replacement for"
0.9.1,"|version| and |release|, also used in various other places throughout the"
0.9.1,built documents.
0.9.1,
0.9.1,The short X.Y version.
0.9.1,"The full version, including alpha/beta/rc tags."
0.9.1,"List of patterns, relative to source directory, that match files and"
0.9.1,directories to ignore when looking for source files.
0.9.1,The reST default role (used for this markup: `text`) to use for all
0.9.1,documents.
0.9.1,"If true, '()' will be appended to :func: etc. cross-reference text."
0.9.1,The name of the Pygments (syntax highlighting) style to use.
0.9.1,-- Options for math equations -----------------------------------------------
0.9.1,-- Options for HTML output ----------------------------------------------
0.9.1,The theme to use for HTML and HTML Help pages.  See the documentation for
0.9.1,a list of builtin themes.
0.9.1,"""twitter_url"": ""https://twitter.com/pandas_dev"","
0.9.1,"""navbar_align"": ""right"",  # For testing that the navbar items align properly"
0.9.1,"Add any paths that contain custom static files (such as style sheets) here,"
0.9.1,"relative to this directory. They are copied after the builtin static files,"
0.9.1,"so a file named ""default.css"" will overwrite the builtin ""default.css""."
0.9.1,Output file base name for HTML help builder.
0.9.1,-- Options for autodoc ------------------------------------------------------
0.9.1,generate autosummary even if no references
0.9.1,-- Options for numpydoc -----------------------------------------------------
0.9.1,this is needed for some reason...
0.9.1,see https://github.com/numpy/numpydoc/issues/69
0.9.1,-- Options for sphinxcontrib-bibtex -----------------------------------------
0.9.1,bibtex file
0.9.1,-- Options for intersphinx --------------------------------------------------
0.9.1,intersphinx configuration
0.9.1,-- Options for sphinx-gallery -----------------------------------------------
0.9.1,Generate the plot for the gallery
0.9.1,sphinx-gallery configuration
0.9.1,-- Options for github link for what's new -----------------------------------
0.9.1,Config for sphinx_issues
0.9.1,The following is used by sphinx.ext.linkcode to provide links to github
0.9.1,-- Options for LaTeX output ---------------------------------------------
0.9.1,The paper size ('letterpaper' or 'a4paper').
0.9.1,"'papersize': 'letterpaper',"
0.9.1,"The font size ('10pt', '11pt' or '12pt')."
0.9.1,"'pointsize': '10pt',"
0.9.1,Additional stuff for the LaTeX preamble.
0.9.1,"'preamble': '',"
0.9.1,Grouping the document tree into LaTeX files. List of tuples
0.9.1,"(source start file, target name, title,"
0.9.1,"author, documentclass [howto, manual, or own class])."
0.9.1,-- Options for manual page output ---------------------------------------
0.9.1,"If false, no module index is generated."
0.9.1,latex_domain_indices = True
0.9.1,One entry per manual page. List of tuples
0.9.1,"(source start file, name, description, authors, manual section)."
0.9.1,"If true, show URL addresses after external links."
0.9.1,man_show_urls = False
0.9.1,-- Options for Texinfo output -------------------------------------------
0.9.1,Grouping the document tree into Texinfo files. List of tuples
0.9.1,"(source start file, target name, title, author,"
0.9.1,"dir menu entry, description, category)"
0.9.1,-- Dependencies generation ----------------------------------------------
0.9.1,get length of header
0.9.1,-- Additional temporary hacks -----------------------------------------------
0.9.1,Temporary work-around for spacing problem between parameter and parameter
0.9.1,"type in the doc, see https://github.com/numpy/numpydoc/issues/215. The bug"
0.9.1,has been fixed in sphinx (https://github.com/sphinx-doc/sphinx/pull/5976) but
0.9.1,through a change in sphinx basic.css except rtd_theme does not use basic.css.
0.9.1,"In an ideal world, this would get fixed in this PR:"
0.9.1,https://github.com/readthedocs/sphinx_rtd_theme/pull/747/files
0.9.1,get the styles from the current theme
0.9.1,create and add the button to all the code blocks that contain >>>
0.9.1,tracebacks (.gt) contain bare text elements that need to be
0.9.1,wrapped in a span to work with .nextUntil() (see later)
0.9.1,define the behavior of the button when it's clicked
0.9.1,hide the code output
0.9.1,show the code output
0.9.1,-*- coding: utf-8 -*-
0.9.1,Format template for issues URI
0.9.1,e.g. 'https://github.com/sloria/marshmallow/issues/{issue}
0.9.1,Format template for PR URI
0.9.1,e.g. 'https://github.com/sloria/marshmallow/pull/{issue}
0.9.1,Format template for commit URI
0.9.1,e.g. 'https://github.com/sloria/marshmallow/commits/{commit}
0.9.1,"Shortcut for Github, e.g. 'sloria/marshmallow'"
0.9.1,Format template for user profile URI
0.9.1,e.g. 'https://github.com/{user}'
0.9.1,Python 2 only
0.9.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.9.1,License: MIT
0.9.1,%%
0.9.1,%%
0.9.1,"First, we will generate a toy classification dataset with only few samples."
0.9.1,The ratio between the classes will be imbalanced.
0.9.1,%%
0.9.1,%%
0.9.1,"Now, we will use a :class:`~imblearn.over_sampling.RandomOverSampler` to"
0.9.1,generate a bootstrap for the minority class with as many samples as in the
0.9.1,majority class.
0.9.1,%%
0.9.1,%%
0.9.1,We observe that the minority samples are less transparent than the samples
0.9.1,"from the majority class. Indeed, it is due to the fact that these samples"
0.9.1,of the minority class are repeated during the bootstrap generation.
0.9.1,
0.9.1,We can set `shrinkage` to a floating value to add a small perturbation to the
0.9.1,samples created and therefore create a smoothed bootstrap.
0.9.1,%%
0.9.1,%%
0.9.1,"In this case, we see that the samples in the minority class are not"
0.9.1,overlapping anymore due to the added noise.
0.9.1,
0.9.1,The parameter `shrinkage` allows to add more or less perturbation. Let's
0.9.1,add more perturbation when generating the smoothed bootstrap.
0.9.1,%%
0.9.1,%%
0.9.1,Increasing the value of `shrinkage` will disperse the new samples. Forcing
0.9.1,the shrinkage to 0 will be equivalent to generating a normal bootstrap.
0.9.1,%%
0.9.1,%%
0.9.1,"Therefore, the `shrinkage` is handy to manually tune the dispersion of the"
0.9.1,new samples.
0.9.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.9.1,License: MIT
0.9.1,%%
0.9.1,generate some data points
0.9.1,plot the majority and minority samples
0.9.1,draw the circle in which the new sample will generated
0.9.1,plot the line on which the sample will be generated
0.9.1,create and plot the new sample
0.9.1,make the plot nicer with legend and label
0.9.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.9.1,License: MIT
0.9.1,%%
0.9.1,%% [markdown]
0.9.1,The following function will be used to create toy dataset. It uses the
0.9.1,:func:`~sklearn.datasets.make_classification` from scikit-learn but fixing
0.9.1,some parameters.
0.9.1,%%
0.9.1,%% [markdown]
0.9.1,The following function will be used to plot the sample space after resampling
0.9.1,to illustrate the specificities of an algorithm.
0.9.1,%%
0.9.1,%% [markdown]
0.9.1,The following function will be used to plot the decision function of a
0.9.1,classifier given some data.
0.9.1,%%
0.9.1,%% [markdown]
0.9.1,Illustration of the influence of the balancing ratio
0.9.1,----------------------------------------------------
0.9.1,
0.9.1,We will first illustrate the influence of the balancing ratio on some toy
0.9.1,data using a logistic regression classifier which is a linear model.
0.9.1,%%
0.9.1,%% [markdown]
0.9.1,We will fit and show the decision boundary model to illustrate the impact of
0.9.1,dealing with imbalanced classes.
0.9.1,%%
0.9.1,%% [markdown]
0.9.1,"Greater is the difference between the number of samples in each class, poorer"
0.9.1,are the classification results.
0.9.1,
0.9.1,Random over-sampling to balance the data set
0.9.1,--------------------------------------------
0.9.1,
0.9.1,Random over-sampling can be used to repeat some samples and balance the
0.9.1,number of samples between the dataset. It can be seen that with this trivial
0.9.1,approach the boundary decision is already less biased toward the majority
0.9.1,class. The class :class:`~imblearn.over_sampling.RandomOverSampler`
0.9.1,implements such of a strategy.
0.9.1,%%
0.9.1,%% [markdown]
0.9.1,"By default, random over-sampling generates a bootstrap. The parameter"
0.9.1,`shrinkage` allows adding a small perturbation to the generated data
0.9.1,to generate a smoothed bootstrap instead. The plot below shows the difference
0.9.1,between the two data generation strategies.
0.9.1,%%
0.9.1,%% [markdown]
0.9.1,It looks like more samples are generated with smoothed bootstrap. This is due
0.9.1,to the fact that the samples generated are not superimposing with the
0.9.1,original samples.
0.9.1,
0.9.1,More advanced over-sampling using ADASYN and SMOTE
0.9.1,--------------------------------------------------
0.9.1,
0.9.1,Instead of repeating the same samples when over-sampling or perturbating the
0.9.1,"generated bootstrap samples, one can use some specific heuristic instead."
0.9.1,:class:`~imblearn.over_sampling.ADASYN` and
0.9.1,:class:`~imblearn.over_sampling.SMOTE` can be used in this case.
0.9.1,%%
0.9.1,%% [markdown]
0.9.1,The following plot illustrates the difference between
0.9.1,:class:`~imblearn.over_sampling.ADASYN` and
0.9.1,:class:`~imblearn.over_sampling.SMOTE`.
0.9.1,:class:`~imblearn.over_sampling.ADASYN` will focus on the samples which are
0.9.1,difficult to classify with a nearest-neighbors rule while regular
0.9.1,:class:`~imblearn.over_sampling.SMOTE` will not make any distinction.
0.9.1,"Therefore, the decision function depending of the algorithm."
0.9.1,%% [markdown]
0.9.1,"Due to those sampling particularities, it can give rise to some specific"
0.9.1,issues as illustrated below.
0.9.1,%%
0.9.1,%% [markdown]
0.9.1,SMOTE proposes several variants by identifying specific samples to consider
0.9.1,during the resampling. The borderline version
0.9.1,(:class:`~imblearn.over_sampling.BorderlineSMOTE`) will detect which point to
0.9.1,select which are in the border between two classes. The SVM version
0.9.1,(:class:`~imblearn.over_sampling.SVMSMOTE`) will use the support vectors
0.9.1,found using an SVM algorithm to create new sample while the KMeans version
0.9.1,(:class:`~imblearn.over_sampling.KMeansSMOTE`) will make a clustering before
0.9.1,to generate samples in each cluster independently depending each cluster
0.9.1,density.
0.9.1,%%
0.9.1,%% [markdown]
0.9.1,"When dealing with a mixed of continuous and categorical features,"
0.9.1,:class:`~imblearn.over_sampling.SMOTENC` is the only method which can handle
0.9.1,this case.
0.9.1,%%
0.9.1,Create a dataset of a mix of numerical and categorical data
0.9.1,%% [markdown]
0.9.1,"However, if the dataset is composed of only categorical features then one"
0.9.1,should use :class:`~imblearn.over_sampling.SMOTEN`.
0.9.1,%%
0.9.1,Generate only categorical data
0.9.1,Authors: Christos Aridas
0.9.1,Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.9.1,License: MIT
0.9.1,%%
0.9.1,%% [markdown]
0.9.1,Let's first generate a dataset with imbalanced class distribution.
0.9.1,%%
0.9.1,%% [markdown]
0.9.1,We will use an over-sampler :class:`~imblearn.over_sampling.SMOTE` followed
0.9.1,by a :class:`~sklearn.tree.DecisionTreeClassifier`. The aim will be to
0.9.1,search which `k_neighbors` parameter is the most adequate with the dataset
0.9.1,that we generated.
0.9.1,%%
0.9.1,%% [markdown]
0.9.1,We can use the :class:`~sklearn.model_selection.validation_curve` to inspect
0.9.1,"the impact of varying the parameter `k_neighbors`. In this case, we need"
0.9.1,to use a score to evaluate the generalization score during the
0.9.1,cross-validation.
0.9.1,%%
0.9.1,%%
0.9.1,%% [markdown]
0.9.1,We can now plot the results of the cross-validation for the different
0.9.1,parameter values that we tried.
0.9.1,%%
0.9.1,make nice plotting
0.9.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.9.1,License: MIT
0.9.1,Generate a dataset
0.9.1,Split the data
0.9.1,Train the classifier with balancing
0.9.1,Test the classifier and get the prediction
0.9.1,Show the classification report
0.9.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.9.1,License: MIT
0.9.1,%%
0.9.1,%% [markdown]
0.9.1,"First, we will generate some imbalanced dataset."
0.9.1,%%
0.9.1,%% [markdown]
0.9.1,We will split the data into a training and testing set.
0.9.1,%%
0.9.1,%% [markdown]
0.9.1,We will create a pipeline made of a :class:`~imblearn.over_sampling.SMOTE`
0.9.1,over-sampler followed by a :class:`~sklearn.svm.LinearSVC` classifier.
0.9.1,%%
0.9.1,%% [markdown]
0.9.1,"Now, we will train the model on the training set and get the prediction"
0.9.1,associated with the testing set. Be aware that the resampling will happen
0.9.1,only when calling `fit`: the number of samples in `y_pred` is the same than
0.9.1,in `y_test`.
0.9.1,%%
0.9.1,%% [markdown]
0.9.1,The geometric mean corresponds to the square root of the product of the
0.9.1,sensitivity and specificity. Combining the two metrics should account for
0.9.1,the balancing of the dataset.
0.9.1,%%
0.9.1,%% [markdown]
0.9.1,The index balanced accuracy can transform any metric to be used in
0.9.1,imbalanced learning problems.
0.9.1,%%
0.9.1,%%
0.9.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.9.1,License: MIT
0.9.1,%%
0.9.1,%% [markdown]
0.9.1,Dataset generation
0.9.1,------------------
0.9.1,
0.9.1,We will create an imbalanced dataset with a couple of samples. We will use
0.9.1,:func:`~sklearn.datasets.make_classification` to generate this dataset.
0.9.1,%%
0.9.1,%%
0.9.1,%% [markdown]
0.9.1,The following function will be used to plot the sample space after resampling
0.9.1,to illustrate the characteristic of an algorithm.
0.9.1,%%
0.9.1,%% [markdown]
0.9.1,The following function will be used to plot the decision function of a
0.9.1,classifier given some data.
0.9.1,%%
0.9.1,%% [markdown]
0.9.1,":class:`~imblearn.over_sampling.SMOTE` allows to generate samples. However,"
0.9.1,this method of over-sampling does not have any knowledge regarding the
0.9.1,"underlying distribution. Therefore, some noisy samples can be generated, e.g."
0.9.1,"when the different classes cannot be well separated. Hence, it can be"
0.9.1,beneficial to apply an under-sampling algorithm to clean the noisy samples.
0.9.1,Two methods are usually used in the literature: (i) Tomek's link and (ii)
0.9.1,edited nearest neighbours cleaning methods. Imbalanced-learn provides two
0.9.1,ready-to-use samplers :class:`~imblearn.combine.SMOTETomek` and
0.9.1,":class:`~imblearn.combine.SMOTEENN`. In general,"
0.9.1,:class:`~imblearn.combine.SMOTEENN` cleans more noisy data than
0.9.1,:class:`~imblearn.combine.SMOTETomek`.
0.9.1,%%
0.9.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.9.1,License: MIT
0.9.1,%%
0.9.1,%% [markdown]
0.9.1,Load an imbalanced dataset
0.9.1,--------------------------
0.9.1,
0.9.1,We will load the UCI SatImage dataset which has an imbalanced ratio of 9.3:1
0.9.1,(number of majority sample for a minority sample). The data are then split
0.9.1,into training and testing.
0.9.1,%%
0.9.1,%% [markdown]
0.9.1,Classification using a single decision tree
0.9.1,-------------------------------------------
0.9.1,
0.9.1,We train a decision tree classifier which will be used as a baseline for the
0.9.1,rest of this example.
0.9.1,
0.9.1,The results are reported in terms of balanced accuracy and geometric mean
0.9.1,which are metrics widely used in the literature to validate model trained on
0.9.1,imbalanced set.
0.9.1,%%
0.9.1,%%
0.9.1,%%
0.9.1,%% [markdown]
0.9.1,Classification using bagging classifier with and without sampling
0.9.1,-----------------------------------------------------------------
0.9.1,
0.9.1,"Instead of using a single tree, we will check if an ensemble of decsion tree"
0.9.1,"can actually alleviate the issue induced by the class imbalancing. First, we"
0.9.1,will use a bagging classifier and its counter part which internally uses a
0.9.1,random under-sampling to balanced each boostrap sample.
0.9.1,%%
0.9.1,%% [markdown]
0.9.1,Balancing each bootstrap sample allows to increase significantly the balanced
0.9.1,accuracy and the geometric mean.
0.9.1,%%
0.9.1,%%
0.9.1,%% [markdown]
0.9.1,Classification using random forest classifier with and without sampling
0.9.1,-----------------------------------------------------------------------
0.9.1,
0.9.1,Random forest is another popular ensemble method and it is usually
0.9.1,"outperforming bagging. Here, we used a vanilla random forest and its balanced"
0.9.1,counterpart in which each bootstrap sample is balanced.
0.9.1,%%
0.9.1,%% [markdown]
0.9.1,"Similarly to the previous experiment, the balanced classifier outperform the"
0.9.1,"classifier which learn from imbalanced bootstrap samples. In addition, random"
0.9.1,forest outsperforms the bagging classifier.
0.9.1,%%
0.9.1,%%
0.9.1,%% [markdown]
0.9.1,Boosting classifier
0.9.1,-------------------
0.9.1,
0.9.1,"In the same manner, easy ensemble classifier is a bag of balanced AdaBoost"
0.9.1,"classifier. However, it will be slower to train than random forest and will"
0.9.1,achieve worse performance.
0.9.1,%%
0.9.1,%%
0.9.1,%%
0.9.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.9.1,License: MIT
0.9.1,%%
0.9.1,%% [markdown]
0.9.1,Generate an imbalanced dataset
0.9.1,------------------------------
0.9.1,
0.9.1,"For this example, we will create a synthetic dataset using the function"
0.9.1,:func:`~sklearn.datasets.make_classification`. The problem will be a toy
0.9.1,classification problem with a ratio of 1:9 between the two classes.
0.9.1,%%
0.9.1,%%
0.9.1,%% [markdown]
0.9.1,"In the following sections, we will show a couple of algorithms that have"
0.9.1,been proposed over the years. We intend to illustrate how one can reuse the
0.9.1,:class:`~imblearn.ensemble.BalancedBaggingClassifier` by passing different
0.9.1,sampler.
0.9.1,%%
0.9.1,%% [markdown]
0.9.1,Exactly Balanced Bagging and Over-Bagging
0.9.1,-----------------------------------------
0.9.1,
0.9.1,The :class:`~imblearn.ensemble.BalancedBaggingClassifier` can use in
0.9.1,conjunction with a :class:`~imblearn.under_sampling.RandomUnderSampler` or
0.9.1,:class:`~imblearn.over_sampling.RandomOverSampler`. These methods are
0.9.1,"referred as Exactly Balanced Bagging and Over-Bagging, respectively and have"
0.9.1,been proposed first in [1]_.
0.9.1,%%
0.9.1,Exactly Balanced Bagging
0.9.1,%%
0.9.1,Over-bagging
0.9.1,%% [markdown]
0.9.1,SMOTE-Bagging
0.9.1,-------------
0.9.1,
0.9.1,Instead of using a :class:`~imblearn.over_sampling.RandomOverSampler` that
0.9.1,"make a bootstrap, an alternative is to use"
0.9.1,:class:`~imblearn.over_sampling.SMOTE` as an over-sampler. This is known as
0.9.1,SMOTE-Bagging [2]_.
0.9.1,%%
0.9.1,SMOTE-Bagging
0.9.1,%% [markdown]
0.9.1,Roughly Balanced Bagging
0.9.1,------------------------
0.9.1,While using a :class:`~imblearn.under_sampling.RandomUnderSampler` or
0.9.1,:class:`~imblearn.over_sampling.RandomOverSampler` will create exactly the
0.9.1,"desired number of samples, it does not follow the statistical spirit wanted"
0.9.1,in the bagging framework. The authors in [3]_ proposes to use a negative
0.9.1,binomial distribution to compute the number of samples of the majority
0.9.1,class to be selected and then perform a random under-sampling.
0.9.1,
0.9.1,"Here, we illustrate this method by implementing a function in charge of"
0.9.1,resampling and use the :class:`~imblearn.FunctionSampler` to integrate it
0.9.1,within a :class:`~imblearn.pipeline.Pipeline` and
0.9.1,:class:`~sklearn.model_selection.cross_validate`.
0.9.1,%%
0.9.1,find the minority and majority classes
0.9.1,compute the number of sample to draw from the majority class using
0.9.1,a negative binomial distribution
0.9.1,draw randomly with or without replacement
0.9.1,Roughly Balanced Bagging
0.9.1,%% [markdown]
0.9.1,.. topic:: References:
0.9.1,
0.9.1,".. [1] R. Maclin, and D. Opitz. ""An empirical evaluation of bagging and"
0.9.1,"boosting."" AAAI/IAAI 1997 (1997): 546-551."
0.9.1,
0.9.1,".. [2] S. Wang, and X. Yao. ""Diversity analysis on imbalanced data sets by"
0.9.1,"using ensemble models."" 2009 IEEE symposium on computational"
0.9.1,"intelligence and data mining. IEEE, 2009."
0.9.1,
0.9.1,".. [3] S. Hido, H. Kashima, and Y. Takahashi. ""Roughly balanced bagging"
0.9.1,"for imbalanced data."" Statistical Analysis and Data Mining: The ASA"
0.9.1,Data Science Journal 2.5‚Äê6 (2009): 412-426.
0.9.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.9.1,License: MIT
0.9.1,%%
0.9.1,%% [markdown]
0.9.1,The following function will be used to create toy dataset. It uses the
0.9.1,:func:`~sklearn.datasets.make_classification` from scikit-learn but fixing
0.9.1,some parameters.
0.9.1,%%
0.9.1,%% [markdown]
0.9.1,The following function will be used to plot the sample space after resampling
0.9.1,to illustrate the specificities of an algorithm.
0.9.1,%%
0.9.1,%% [markdown]
0.9.1,The following function will be used to plot the decision function of a
0.9.1,classifier given some data.
0.9.1,%%
0.9.1,%%
0.9.1,%% [markdown]
0.9.1,Prototype generation: under-sampling by generating new samples
0.9.1,--------------------------------------------------------------
0.9.1,
0.9.1,:class:`~imblearn.under_sampling.ClusterCentroids` under-samples by replacing
0.9.1,the original samples by the centroids of the cluster found.
0.9.1,%%
0.9.1,%% [markdown]
0.9.1,Prototype selection: under-sampling by selecting existing samples
0.9.1,-----------------------------------------------------------------
0.9.1,
0.9.1,The algorithm performing prototype selection can be subdivided into two
0.9.1,groups: (i) the controlled under-sampling methods and (ii) the cleaning
0.9.1,under-sampling methods.
0.9.1,
0.9.1,"With the controlled under-sampling methods, the number of samples to be"
0.9.1,selected can be specified.
0.9.1,:class:`~imblearn.under_sampling.RandomUnderSampler` is the most naive way of
0.9.1,performing such selection by randomly selecting a given number of samples by
0.9.1,the targetted class.
0.9.1,%%
0.9.1,%% [markdown]
0.9.1,:class:`~imblearn.under_sampling.NearMiss` algorithms implement some
0.9.1,heuristic rules in order to select samples. NearMiss-1 selects samples from
0.9.1,the majority class for which the average distance of the :math:`k`` nearest
0.9.1,samples of the minority class is the smallest. NearMiss-2 selects the samples
0.9.1,from the majority class for which the average distance to the farthest
0.9.1,samples of the negative class is the smallest. NearMiss-3 is a 2-step
0.9.1,"algorithm: first, for each minority sample, their :math:`m`"
0.9.1,"nearest-neighbors will be kept; then, the majority samples selected are the"
0.9.1,on for which the average distance to the :math:`k` nearest neighbors is the
0.9.1,largest.
0.9.1,%%
0.9.1,%% [markdown]
0.9.1,:class:`~imblearn.under_sampling.EditedNearestNeighbours` removes samples of
0.9.1,the majority class for which their class differ from the one of their
0.9.1,nearest-neighbors. This sieve can be repeated which is the principle of the
0.9.1,:class:`~imblearn.under_sampling.RepeatedEditedNearestNeighbours`.
0.9.1,:class:`~imblearn.under_sampling.AllKNN` is slightly different from the
0.9.1,:class:`~imblearn.under_sampling.RepeatedEditedNearestNeighbours` by changing
0.9.1,"the :math:`k` parameter of the internal nearest neighors algorithm,"
0.9.1,increasing it at each iteration.
0.9.1,%%
0.9.1,%% [markdown]
0.9.1,:class:`~imblearn.under_sampling.CondensedNearestNeighbour` makes use of a
0.9.1,1-NN to iteratively decide if a sample should be kept in a dataset or not.
0.9.1,The issue is that :class:`~imblearn.under_sampling.CondensedNearestNeighbour`
0.9.1,is sensitive to noise by preserving the noisy samples.
0.9.1,:class:`~imblearn.under_sampling.OneSidedSelection` also used the 1-NN and
0.9.1,use :class:`~imblearn.under_sampling.TomekLinks` to remove the samples
0.9.1,considered noisy. The
0.9.1,:class:`~imblearn.under_sampling.NeighbourhoodCleaningRule` use a
0.9.1,:class:`~imblearn.under_sampling.EditedNearestNeighbours` to remove some
0.9.1,"sample. Additionally, they use a 3 nearest-neighbors to remove samples which"
0.9.1,do not agree with this rule.
0.9.1,%%
0.9.1,%% [markdown]
0.9.1,:class:`~imblearn.under_sampling.InstanceHardnessThreshold` uses the
0.9.1,prediction of classifier to exclude samples. All samples which are classified
0.9.1,with a low probability will be removed.
0.9.1,%%
0.9.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.9.1,License: MIT
0.9.1,%%
0.9.1,%% [markdown]
0.9.1,This function allows to make nice plotting
0.9.1,%%
0.9.1,%% [markdown]
0.9.1,We will generate some toy data that illustrates how
0.9.1,:class:`~imblearn.under_sampling.TomekLinks` is used to clean a dataset.
0.9.1,%%
0.9.1,%% [markdown]
0.9.1,"In the figure above, the samples highlighted in green form a Tomek link since"
0.9.1,they are of different classes and are nearest neighbors of each other.
0.9.1,highlight the samples of interest
0.9.1,%% [markdown]
0.9.1,We can run the :class:`~imblearn.under_sampling.TomekLinks` sampling to
0.9.1,remove the corresponding samples. If `sampling_strategy='auto'` only the
0.9.1,sample from the majority class will be removed. If `sampling_strategy='all'`
0.9.1,both samples will be removed.
0.9.1,%%
0.9.1,highlight the samples of interest
0.9.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.9.1,License: MIT
0.9.1,%%
0.9.1,%% [markdown]
0.9.1,We define a function allowing to make some nice decoration on the plot.
0.9.1,%%
0.9.1,%% [markdown]
0.9.1,We can start by generating some data to later illustrate the principle of
0.9.1,each :class:`~imblearn.under_sampling.NearMiss` heuristic rules.
0.9.1,%%
0.9.1,%% [mardown]
0.9.1,NearMiss-1
0.9.1,----------
0.9.1,
0.9.1,NearMiss-1 selects samples from the majority class for which the average
0.9.1,distance to some nearest neighbours is the smallest. In the following
0.9.1,"example, we use a 3-NN to compute the average distance on 2 specific samples"
0.9.1,"of the majority class. Therefore, in this case the point linked by the"
0.9.1,green-dashed line will be selected since the average distance is smaller.
0.9.1,%%
0.9.1,%% [mardown]
0.9.1,NearMiss-2
0.9.1,----------
0.9.1,
0.9.1,NearMiss-2 selects samples from the majority class for which the average
0.9.1,distance to the farthest neighbors is the smallest. With the same
0.9.1,"configuration as previously presented, the sample linked to the green-dashed"
0.9.1,line will be selected since its distance the 3 farthest neighbors is the
0.9.1,smallest.
0.9.1,%%
0.9.1,%% [mardown]
0.9.1,NearMiss-3
0.9.1,----------
0.9.1,
0.9.1,"NearMiss-3 can be divided into 2 steps. First, a nearest-neighbors is used to"
0.9.1,short-list samples from the majority class (i.e. correspond to the
0.9.1,"highlighted samples in the following plot). Then, the sample with the largest"
0.9.1,average distance to the *k* nearest-neighbors are selected.
0.9.1,%%
0.9.1,select only the majority point of interest
0.9.1,Authors: Christos Aridas
0.9.1,Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.9.1,License: MIT
0.9.1,%%
0.9.1,%% [markdown]
0.9.1,Let's first create an imbalanced dataset and split in to two sets.
0.9.1,%%
0.9.1,%% [markdown]
0.9.1,"Now, we will create each individual steps that we would like later to combine"
0.9.1,%%
0.9.1,%% [markdown]
0.9.1,"Now, we can finally create a pipeline to specify in which order the different"
0.9.1,transformers and samplers should be executed before to provide the data to
0.9.1,the final classifier.
0.9.1,%%
0.9.1,%% [markdown]
0.9.1,We can now use the pipeline created as a normal classifier where resampling
0.9.1,"will happen when calling `fit` and disabled when calling `decision_function`,"
0.9.1,"`predict_proba`, or `predict`."
0.9.1,%%
0.9.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.9.1,License: MIT
0.9.1,##############################################################################
0.9.1,Data loading
0.9.1,##############################################################################
0.9.1,##############################################################################
0.9.1,"First, you should download the Porto Seguro data set from Kaggle. See the"
0.9.1,link in the introduction.
0.9.1,##############################################################################
0.9.1,The data set is imbalanced and it will have an effect on the fitting.
0.9.1,##############################################################################
0.9.1,Define the pre-processing pipeline
0.9.1,##############################################################################
0.9.1,##############################################################################
0.9.1,We want to standard scale the numerical features while we want to one-hot
0.9.1,"encode the categorical features. In this regard, we make use of the"
0.9.1,:class:`~sklearn.compose.ColumnTransformer`.
0.9.1,Create an environment variable to avoid using the GPU. This can be changed.
0.9.1,##############################################################################
0.9.1,Create a neural-network
0.9.1,##############################################################################
0.9.1,##############################################################################
0.9.1,We create a decorator to report the computation time
0.9.1,##############################################################################
0.9.1,The first model will be trained using the ``fit`` method and with imbalanced
0.9.1,mini-batches.
0.9.1,##############################################################################
0.9.1,"In the contrary, we will use imbalanced-learn to create a generator of"
0.9.1,mini-batches which will yield balanced mini-batches.
0.9.1,##############################################################################
0.9.1,Classification loop
0.9.1,##############################################################################
0.9.1,##############################################################################
0.9.1,We will perform a 10-fold cross-validation and train the neural-network with
0.9.1,the two different strategies previously presented.
0.9.1,##############################################################################
0.9.1,Plot of the results and computation time
0.9.1,##############################################################################
0.9.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.9.1,License: MIT
0.9.1,%%
0.9.1,%% [markdown]
0.9.1,Problem definition
0.9.1,------------------
0.9.1,
0.9.1,We are dropping the following features:
0.9.1,
0.9.1,"- ""fnlwgt"": this feature was created while studying the ""adult"" dataset."
0.9.1,"Thus, we will not use this feature which is not acquired during the survey."
0.9.1,"- ""education-num"": it is encoding the same information than ""education""."
0.9.1,"Thus, we are removing one of these 2 features."
0.9.1,%%
0.9.1,%% [markdown]
0.9.1,"The ""adult"" dataset as a class ratio of about 3:1"
0.9.1,%%
0.9.1,%% [markdown]
0.9.1,This dataset is only slightly imbalanced. To better highlight the effect of
0.9.1,"learning from an imbalanced dataset, we will increase its ratio to 30:1"
0.9.1,%%
0.9.1,%% [markdown]
0.9.1,We will perform a cross-validation evaluation to get an estimate of the test
0.9.1,score.
0.9.1,
0.9.1,"As a baseline, we could use a classifier which will always predict the"
0.9.1,majority class independently of the features provided.
0.9.1,%%
0.9.1,%% [markdown]
0.9.1,"Instead of using the accuracy, we can use the balanced accuracy which will"
0.9.1,take into account the balancing issue.
0.9.1,%%
0.9.1,%% [markdown]
0.9.1,Strategies to learn from an imbalanced dataset
0.9.1,----------------------------------------------
0.9.1,We will use a dictionary and a list to continuously store the results of
0.9.1,our experiments and show them as a pandas dataframe.
0.9.1,%%
0.9.1,%% [markdown]
0.9.1,Dummy baseline
0.9.1,..............
0.9.1,
0.9.1,"Before to train a real machine learning model, we can store the results"
0.9.1,obtained with our :class:`~sklearn.dummy.DummyClassifier`.
0.9.1,%%
0.9.1,%% [markdown]
0.9.1,Linear classifier baseline
0.9.1,..........................
0.9.1,
0.9.1,We will create a machine learning pipeline using a
0.9.1,":class:`~sklearn.linear_model.LogisticRegression` classifier. In this regard,"
0.9.1,we will need to one-hot encode the categorical columns and standardized the
0.9.1,numerical columns before to inject the data into the
0.9.1,:class:`~sklearn.linear_model.LogisticRegression` classifier.
0.9.1,
0.9.1,"First, we define our numerical and categorical pipelines."
0.9.1,%%
0.9.1,%% [markdown]
0.9.1,"Then, we can create a preprocessor which will dispatch the categorical"
0.9.1,columns to the categorical pipeline and the numerical columns to the
0.9.1,numerical pipeline
0.9.1,%%
0.9.1,%% [markdown]
0.9.1,"Finally, we connect our preprocessor with our"
0.9.1,:class:`~sklearn.linear_model.LogisticRegression`. We can then evaluate our
0.9.1,model.
0.9.1,%%
0.9.1,%%
0.9.1,%% [markdown]
0.9.1,We can see that our linear model is learning slightly better than our dummy
0.9.1,"baseline. However, it is impacted by the class imbalance."
0.9.1,
0.9.1,We can verify that something similar is happening with a tree-based model
0.9.1,such as :class:`~sklearn.ensemble.RandomForestClassifier`. With this type of
0.9.1,"classifier, we will not need to scale the numerical data, and we will only"
0.9.1,need to ordinal encode the categorical data.
0.9.1,%%
0.9.1,%%
0.9.1,%% [markdown]
0.9.1,The :class:`~sklearn.ensemble.RandomForestClassifier` is as well affected by
0.9.1,"the class imbalanced, slightly less than the linear model. Now, we will"
0.9.1,present different approach to improve the performance of these 2 models.
0.9.1,
0.9.1,Use `class_weight`
0.9.1,..................
0.9.1,
0.9.1,Most of the models in `scikit-learn` have a parameter `class_weight`. This
0.9.1,parameter will affect the computation of the loss in linear model or the
0.9.1,criterion in the tree-based model to penalize differently a false
0.9.1,classification from the minority and majority class. We can set
0.9.1,"`class_weight=""balanced""` such that the weight applied is inversely"
0.9.1,proportional to the class frequency. We test this parametrization in both
0.9.1,linear model and tree-based model.
0.9.1,%%
0.9.1,%%
0.9.1,%% [markdown]
0.9.1,We can see that using `class_weight` was really effective for the linear
0.9.1,"model, alleviating the issue of learning from imbalanced classes. However,"
0.9.1,the :class:`~sklearn.ensemble.RandomForestClassifier` is still biased toward
0.9.1,"the majority class, mainly due to the criterion which is not suited enough to"
0.9.1,fight the class imbalance.
0.9.1,
0.9.1,Resample the training set during learning
0.9.1,.........................................
0.9.1,
0.9.1,Another way is to resample the training set by under-sampling or
0.9.1,over-sampling some of the samples. `imbalanced-learn` provides some samplers
0.9.1,to do such processing.
0.9.1,%%
0.9.1,%%
0.9.1,%%
0.9.1,%%
0.9.1,%% [markdown]
0.9.1,Applying a random under-sampler before the training of the linear model or
0.9.1,"random forest, allows to not focus on the majority class at the cost of"
0.9.1,making more mistake for samples in the majority class (i.e. decreased
0.9.1,accuracy).
0.9.1,
0.9.1,We could apply any type of samplers and find which sampler is working best
0.9.1,on the current dataset.
0.9.1,
0.9.1,"Instead, we will present another way by using classifiers which will apply"
0.9.1,sampling internally.
0.9.1,
0.9.1,Use of specific balanced algorithms from imbalanced-learn
0.9.1,.........................................................
0.9.1,
0.9.1,We already showed that random under-sampling can be effective on decision
0.9.1,"tree. However, instead of under-sampling once the dataset, one could"
0.9.1,under-sample the original dataset before to take a bootstrap sample. This is
0.9.1,the base of the :class:`imblearn.ensemble.BalancedRandomForestClassifier` and
0.9.1,:class:`~imblearn.ensemble.BalancedBaggingClassifier`.
0.9.1,%%
0.9.1,%%
0.9.1,%% [markdown]
0.9.1,The performance with the
0.9.1,:class:`~imblearn.ensemble.BalancedRandomForestClassifier` is better than
0.9.1,applying a single random under-sampling. We will use a gradient-boosting
0.9.1,classifier within a :class:`~imblearn.ensemble.BalancedBaggingClassifier`.
0.9.1,%% [markdown]
0.9.1,This last approach is the most effective. The different under-sampling allows
0.9.1,to bring some diversity for the different GBDT to learn and not focus on a
0.9.1,portion of the majority class.
0.9.1,Authors: Christos Aridas
0.9.1,Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.9.1,License: MIT
0.9.1,%%
0.9.1,%% [markdown]
0.9.1,Load the dataset
0.9.1,----------------
0.9.1,
0.9.1,We will use a dataset containing image from know person where we will
0.9.1,build a model to recognize the person on the image. We will make this problem
0.9.1,a binary problem by taking picture of only George W. Bush and Bill Clinton.
0.9.1,%%
0.9.1,%%
0.9.1,%% [markdown]
0.9.1,We can check the ratio between the two classes.
0.9.1,%%
0.9.1,%% [markdown]
0.9.1,We see that we have an imbalanced classification problem with ~95% of the
0.9.1,data belonging to the class G.W. Bush.
0.9.1,
0.9.1,Compare over-sampling approaches
0.9.1,--------------------------------
0.9.1,
0.9.1,We will use different over-sampling approaches and use a kNN classifier
0.9.1,to check if we can recognize the 2 presidents. The evaluation will be
0.9.1,performed through cross-validation and we will plot the mean ROC curve.
0.9.1,
0.9.1,We will create different pipelines and evaluate them.
0.9.1,%%
0.9.1,%%
0.9.1,%% [markdown]
0.9.1,We will compute the mean ROC curve for each pipeline using a different splits
0.9.1,provided by the :class:`~sklearn.model_selection.StratifiedKFold`
0.9.1,cross-validation.
0.9.1,%%
0.9.1,compute the mean fpr/tpr to get the mean ROC curve
0.9.1,Create a display that we will reuse to make the aggregated plots for
0.9.1,all methods
0.9.1,%% [markdown]
0.9.1,"In the previous cell, we created the different mean ROC curve and we can plot"
0.9.1,them on the same plot.
0.9.1,%%
0.9.1,%% [markdown]
0.9.1,"We see that for this task, methods that are generating new samples with some"
0.9.1,interpolation (i.e. ADASYN and SMOTE) perform better than random
0.9.1,over-sampling or no resampling.
0.9.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.9.1,License: MIT
0.9.1,Create a folder to fetch the dataset
0.9.1,Create a pipeline
0.9.1,Classify and report the results
0.9.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.9.1,License: MIT
0.9.1,%%
0.9.1,%% [markdown]
0.9.1,Setting the data set
0.9.1,--------------------
0.9.1,
0.9.1,We use a part of the 20 newsgroups data set by loading 4 topics. Using the
0.9.1,"scikit-learn loader, the data are split into a training and a testing set."
0.9.1,
0.9.1,Note the class \#3 is the minority class and has almost twice less samples
0.9.1,than the majority class.
0.9.1,%%
0.9.1,%%
0.9.1,%% [markdown]
0.9.1,The usual scikit-learn pipeline
0.9.1,-------------------------------
0.9.1,
0.9.1,You might usually use scikit-learn pipeline by combining the TF-IDF
0.9.1,vectorizer to feed a multinomial naive bayes classifier. A classification
0.9.1,report summarized the results on the testing set.
0.9.1,
0.9.1,"As expected, the recall of the class \#3 is low mainly due to the class"
0.9.1,imbalanced.
0.9.1,%%
0.9.1,%%
0.9.1,%% [markdown]
0.9.1,Balancing the class before classification
0.9.1,-----------------------------------------
0.9.1,
0.9.1,"To improve the prediction of the class \#3, it could be interesting to apply"
0.9.1,"a balancing before to train the naive bayes classifier. Therefore, we will"
0.9.1,use a :class:`~imblearn.under_sampling.RandomUnderSampler` to equalize the
0.9.1,number of samples in all the classes before the training.
0.9.1,
0.9.1,It is also important to note that we are using the
0.9.1,:class:`~imblearn.pipeline.make_pipeline` function implemented in
0.9.1,imbalanced-learn to properly handle the samplers.
0.9.1,%%
0.9.1,%% [markdown]
0.9.1,"Although the results are almost identical, it can be seen that the resampling"
0.9.1,allowed to correct the poor recall of the class \#3 at the cost of reducing
0.9.1,"the other metrics for the other classes. However, the overall results are"
0.9.1,slightly better.
0.9.1,%%
0.9.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.9.1,License: MIT
0.9.1,#############################################################################
0.9.1,Toy data generation
0.9.1,#############################################################################
0.9.1,#############################################################################
0.9.1,We are generating some non Gaussian data set contaminated with some unform
0.9.1,noise.
0.9.1,#############################################################################
0.9.1,We will generate some cleaned test data without outliers.
0.9.1,#############################################################################
0.9.1,How to use the :class:`~imblearn.FunctionSampler`
0.9.1,#############################################################################
0.9.1,#############################################################################
0.9.1,We first define a function which will use
0.9.1,:class:`~sklearn.ensemble.IsolationForest` to eliminate some outliers from
0.9.1,our dataset during training. The function passed to the
0.9.1,:class:`~imblearn.FunctionSampler` will be called when using the method
0.9.1,``fit_resample``.
0.9.1,#############################################################################
0.9.1,Integrate it within a pipeline
0.9.1,#############################################################################
0.9.1,#############################################################################
0.9.1,"By elimnating outliers before the training, the classifier will be less"
0.9.1,affected during the prediction.
0.9.1,Authors: Dayvid Oliveira
0.9.1,Christos Aridas
0.9.1,Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.9.1,License: MIT
0.9.1,%%
0.9.1,%% [markdown]
0.9.1,Generate the dataset
0.9.1,--------------------
0.9.1,
0.9.1,"First, we will generate a dataset and convert it to a"
0.9.1,:class:`~pandas.DataFrame` with arbitrary column names. We will plot the
0.9.1,original dataset.
0.9.1,%%
0.9.1,%% [markdown]
0.9.1,Make a dataset imbalanced
0.9.1,-------------------------
0.9.1,
0.9.1,"Now, we will show the helpers :func:`~imblearn.datasets.make_imbalance`"
0.9.1,that is useful to random select a subset of samples. It will impact the
0.9.1,class distribution as specified by the parameters.
0.9.1,%%
0.9.1,%%
0.9.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.9.1,License: MIT
0.9.1,%%
0.9.1,%% [markdown]
0.9.1,Create an imbalanced dataset
0.9.1,----------------------------
0.9.1,
0.9.1,"First, we will create an imbalanced data set from a the iris data set."
0.9.1,%%
0.9.1,%%
0.9.1,%% [markdown]
0.9.1,Using ``sampling_strategy`` in resampling algorithms
0.9.1,====================================================
0.9.1,
0.9.1,`sampling_strategy` as a `float`
0.9.1,--------------------------------
0.9.1,
0.9.1,`sampling_strategy` can be given a `float`. For **under-sampling
0.9.1,"methods**, it corresponds to the ratio :math:`\\alpha_{us}` defined by"
0.9.1,:math:`N_{rM} = \\alpha_{us} \\times N_{m}` where :math:`N_{rM}` and
0.9.1,:math:`N_{m}` are the number of samples in the majority class after
0.9.1,"resampling and the number of samples in the minority class, respectively."
0.9.1,%%
0.9.1,select only 2 classes since the ratio make sense in this case
0.9.1,%%
0.9.1,%% [markdown]
0.9.1,"For **over-sampling methods**, it correspond to the ratio"
0.9.1,:math:`\\alpha_{os}` defined by :math:`N_{rm} = \\alpha_{os} \\times N_{M}`
0.9.1,where :math:`N_{rm}` and :math:`N_{M}` are the number of samples in the
0.9.1,minority class after resampling and the number of samples in the majority
0.9.1,"class, respectively."
0.9.1,%%
0.9.1,%% [markdown]
0.9.1,`sampling_strategy` has a `str`
0.9.1,-------------------------------
0.9.1,
0.9.1,`sampling_strategy` can be given as a string which specify the class
0.9.1,"targeted by the resampling. With under- and over-sampling, the number of"
0.9.1,samples will be equalized.
0.9.1,
0.9.1,Note that we are using multiple classes from now on.
0.9.1,%%
0.9.1,%% [markdown]
0.9.1,"With **cleaning method**, the number of samples in each class will not be"
0.9.1,equalized even if targeted.
0.9.1,%%
0.9.1,%% [markdown]
0.9.1,`sampling_strategy as a `dict`
0.9.1,------------------------------
0.9.1,
0.9.1,"When `sampling_strategy` is a `dict`, the keys correspond to the targeted"
0.9.1,classes. The values correspond to the desired number of samples for each
0.9.1,targeted class. This is working for both **under- and over-sampling**
0.9.1,algorithms but not for the **cleaning algorithms**. Use a `list` instead.
0.9.1,%%
0.9.1,%% [markdown]
0.9.1,`sampling_strategy` as a `list`
0.9.1,-------------------------------
0.9.1,
0.9.1,"When `sampling_strategy` is a `list`, the list contains the targeted"
0.9.1,classes. It is used only for **cleaning methods** and raise an error
0.9.1,otherwise.
0.9.1,%%
0.9.1,%% [markdown]
0.9.1,`sampling_strategy` as a callable
0.9.1,---------------------------------
0.9.1,
0.9.1,"When callable, function taking `y` and returns a `dict`. The keys"
0.9.1,correspond to the targeted classes. The values correspond to the desired
0.9.1,number of samples for each class.
0.9.1,%%
0.9.1,List of whitelisted modules and methods; regexp are supported.
0.9.1,These docstrings will fail because they are inheriting from scikit-learn
0.9.1,skip private classes
0.9.1,"We ignore following error code,"
0.9.1,- RT02: The first line of the Returns section
0.9.1,"should contain only the type, .."
0.9.1,(as we may need refer to the name of the returned
0.9.1,object)
0.9.1,- GL01: Docstring text (summary) should start in the line
0.9.1,"immediately after the opening quotes (not in the same line,"
0.9.1,or leaving a blank line in between)
0.9.1,"- GL02: If there's a blank line, it should be before the"
0.9.1,"first line of the Returns section, not after (it allows to have"
0.9.1,short docstrings for properties).
0.9.1,Ignore PR02: Unknown parameters for properties. We sometimes use
0.9.1,"properties for ducktyping, i.e. SGDClassifier.predict_proba"
0.9.1,Following codes are only taken into account for the
0.9.1,top level class docstrings:
0.9.1,- ES01: No extended summary found
0.9.1,- SA01: See Also section not found
0.9.1,- EX01: No examples section found
0.9.1,In particular we can't parse the signature of properties
0.9.1,"When applied to classes, detect class method. For functions"
0.9.1,method = None.
0.9.1,TODO: this detection can be improved. Currently we assume that we have
0.9.1,class # methods if the second path element before last is in camel case.
0.9.1,'build' and 'install' is included to have structured metadata for CI.
0.9.1,It will NOT be included in setup's extras_require
0.9.1,"The values are (version_spec, comma separated tags)"
0.9.1,create inverse mapping for setuptools
0.9.1,Used by CI to get the min dependencies
0.9.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.9.1,License: MIT
0.9.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.9.1,Christos Aridas
0.9.1,License: MIT
0.9.1,we need to overwrite SamplerMixin.fit to bypass the validation
0.9.1,Adapted from scikit-learn
0.9.1,Author: Edouard Duchesnay
0.9.1,Gael Varoquaux
0.9.1,Virgile Fritsch
0.9.1,Alexandre Gramfort
0.9.1,Lars Buitinck
0.9.1,Christos Aridas
0.9.1,Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.9.1,License: BSD
0.9.1,BaseEstimator interface
0.9.1,validate names
0.9.1,validate estimators
0.9.1,We allow last estimator to be None as an identity transformation
0.9.1,Estimator interface
0.9.1,Setup the memory
0.9.1,joblib >= 0.12
0.9.1,Fit or load from cache the current transformer
0.9.1,Replace the transformer of the step with the fitted
0.9.1,transformer. This is necessary when loading the transformer
0.9.1,from the cache.
0.9.1,This variable is injected in the __builtins__ by the build
0.9.1,process. It is used to enable importing subpackages of sklearn when
0.9.1,the binaries are not built
0.9.1,mypy error: Cannot determine type of '__SKLEARN_SETUP__'
0.9.1,We are not importing the rest of scikit-learn during the build
0.9.1,"process, as it may not be compiled yet"
0.9.1,"FIXME: When we get Python 3.7 as minimal version, we will need to switch to"
0.9.1,the following solution:
0.9.1,https://snarky.ca/lazy-importing-in-python-3-7/
0.9.1,Import the target module and insert it into the parent's namespace
0.9.1,Update this object's dict so that if someone keeps a reference to the
0.9.1,"LazyLoader, lookups are efficient (__getattr__ is only called on"
0.9.1,lookups that fail).
0.9.1,delay the import of keras since we are going to import either tensorflow
0.9.1,or keras
0.9.1,Based on NiLearn package
0.9.1,License: simplified BSD
0.9.1,"PEP0440 compatible formatted version, see:"
0.9.1,https://www.python.org/dev/peps/pep-0440/
0.9.1,
0.9.1,Generic release markers:
0.9.1,X.Y
0.9.1,X.Y.Z # For bugfix releases
0.9.1,
0.9.1,Admissible pre-release markers:
0.9.1,X.YaN # Alpha release
0.9.1,X.YbN # Beta release
0.9.1,X.YrcN # Release Candidate
0.9.1,X.Y # Final release
0.9.1,
0.9.1,Dev branch marker is: 'X.Y.dev' or 'X.Y.devN' where N is an integer.
0.9.1,'X.Y.dev0' is the canonical version of 'X.Y.dev'
0.9.1,
0.9.1,coding: utf-8
0.9.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.9.1,Dariusz Brzezinski
0.9.1,License: MIT
0.9.1,Only negative labels
0.9.1,"Calculate tp_sum, pred_sum, true_sum ###"
0.9.1,labels are now from 0 to len(labels) - 1 -> use bincount
0.9.1,Pathological case
0.9.1,Compute the true negative
0.9.1,Retain only selected labels
0.9.1,"Finally, we have all our sufficient statistics. Divide! #"
0.9.1,"Divide, and on zero-division, set scores to 0 and warn:"
0.9.1,"Oddly, we may get an ""invalid"" rather than a ""divide"" error"
0.9.1,here.
0.9.1,Average the results
0.9.1,labels are now from 0 to len(labels) - 1 -> use bincount
0.9.1,Pathological case
0.9.1,Retain only selected labels
0.9.1,old version of scipy return MaskedConstant instead of 0.0
0.9.1,check that the scoring function does not need a score
0.9.1,and only a prediction
0.9.1,We do not support multilabel so the only average supported
0.9.1,is binary
0.9.1,Compute the different metrics
0.9.1,Precision/recall/f1
0.9.1,Specificity
0.9.1,Geometric mean
0.9.1,Index balanced accuracy
0.9.1,compute averages
0.9.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.9.1,License: MIT
0.9.1,categories are expected to be encoded from 0 to n_categories - 1
0.9.1,"list of length n_features of ndarray (n_categories, n_classes)"
0.9.1,compute the counts
0.9.1,normalize by the summing over the classes
0.9.1,silence potential warning due to in-place division by zero
0.9.1,coding: utf-8
0.9.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.9.1,Christos Aridas
0.9.1,License: MIT
0.9.1,##############################################################################
0.9.1,Utilities for testing
0.9.1,import some data to play with
0.9.1,restrict to a binary classification task
0.9.1,add noisy features to make the problem harder and avoid perfect results
0.9.1,"run classifier, get class probabilities and label predictions"
0.9.1,only interested in probabilities of the positive case
0.9.1,XXX: do we really want a special API for the binary case?
0.9.1,##############################################################################
0.9.1,Tests
0.9.1,detailed measures for each class
0.9.1,individual scoring function that can be used for grid search: in the
0.9.1,binary class case the score is the value of the measure for the positive
0.9.1,class (e.g. label == 1). This is deprecated for average != 'binary'.
0.9.1,Such a case may occur with non-stratified cross-validation
0.9.1,ensure the above were meaningful tests:
0.9.1,Bad pos_label
0.9.1,Bad average option
0.9.1,but average != 'binary'; even if data is binary
0.9.1,compute the geometric mean for the binary problem
0.9.1,print classification report with class names
0.9.1,print classification report with label detection
0.9.1,print classification report with class names
0.9.1,print classification report with label detection
0.9.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.9.1,License: MIT
0.9.1,Check basic feature of the metric:
0.9.1,"* the shape of the distance matrix is (n_samples, n_samples)"
0.9.1,* computing pairwise distance of X is the same than explicitely between
0.9.1,X and X.
0.9.1,Check the property of the vdm distance. Let's check the property
0.9.1,"described in ""Improved Heterogeneous Distance Functions"", D.R. Wilson and"
0.9.1,"T.R. Martinez, Journal of Artificial Intelligence Research 6 (1997) 1-34"
0.9.1,https://arxiv.org/pdf/cs/9701101.pdf
0.9.1,
0.9.1,"""if an attribute color has three values red, green and blue, and the"
0.9.1,"application is to identify whether or not an object is an apple, red and"
0.9.1,green would be considered closer than red and blue because the former two
0.9.1,"both have similar correlations with the output class apple."""
0.9.1,defined our feature
0.9.1,0 - not an apple / 1 - an apple
0.9.1,computing the distance between a sample of the same category should
0.9.1,give a null distance
0.9.1,check the property explained in the introduction example
0.9.1,green and red are very close
0.9.1,blue is closer to red than green
0.9.1,"Check that ""auto"" is equivalent to provide the number categories"
0.9.1,beforehand
0.9.1,Check that we raise an error if n_categories is inconsistent with the
0.9.1,number of features in X
0.9.1,Check that we don't get issue when a category is missing between 0
0.9.1,n_categories - 1
0.9.1,remove a categories that could be between 0 and n_categories
0.9.1,Check that we raise a NotFittedError when `fit` is not not called before
0.9.1,pairwise.
0.9.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.9.1,Christos Aridas
0.9.1,License: MIT
0.9.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.9.1,Christos Aridas
0.9.1,License: MIT
0.9.1,The ratio is computed using a one-vs-rest manner. Using majority
0.9.1,in multi-class would lead to slightly different results at the
0.9.1,cost of introducing a new parameter.
0.9.1,rounding may cause new amount for n_samples
0.9.1,the nearest neighbors need to be fitted only on the current class
0.9.1,to find the class NN to generate new samples
0.9.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.9.1,Christos Aridas
0.9.1,License: MIT
0.9.1,smoothed bootstrap imposes to make numerical operation; we need
0.9.1,to be sure to have only numerical data in X
0.9.1,generate a smoothed bootstrap with a perturbation
0.9.1,generate a bootstrap
0.9.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.9.1,Christos Aridas
0.9.1,License: MIT
0.9.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.9.1,Fernando Nogueira
0.9.1,Christos Aridas
0.9.1,License: MIT
0.9.1,validate the parameters
0.9.1,negate diagonal elements
0.9.1,identify cluster which are answering the requirements
0.9.1,the cluster is already considered balanced
0.9.1,not enough samples to apply SMOTE
0.9.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.9.1,Fernando Nogueira
0.9.1,Christos Aridas
0.9.1,Dzianis Dudnik
0.9.1,License: MIT
0.9.1,divergence between borderline-1 and borderline-2
0.9.1,Create synthetic samples for borderline points.
0.9.1,only minority
0.9.1,we use a one-vs-rest policy to handle the multiclass in which
0.9.1,new samples will be created considering not only the majority
0.9.1,class but all over classes.
0.9.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.9.1,Fernando Nogueira
0.9.1,Christos Aridas
0.9.1,Dzianis Dudnik
0.9.1,License: MIT
0.9.1,np.newaxis for backwards compatability with random_state
0.9.1,Samples are in danger for m/2 <= m' < m
0.9.1,Samples are noise for m = m'
0.9.1,compute the median of the standard deviation of the minority class
0.9.1,the input of the OneHotEncoder needs to be dense
0.9.1,we can replace the 1 entries of the categorical features with the
0.9.1,median of the standard deviation. It will ensure that whenever
0.9.1,"distance is computed between 2 samples, the difference will be equal"
0.9.1,to the median of the standard deviation as in the original paper.
0.9.1,"In the edge case where the median of the std is equal to 0, the 1s"
0.9.1,"entries will be also nullified. In this case, we store the original"
0.9.1,categorical encoding which will be later used for inversing the OHE
0.9.1,reverse the encoding of the categorical features
0.9.1,the matrix is supposed to be in the CSR format after the stacking
0.9.1,change in sparsity structure more efficient with LIL than CSR
0.9.1,convert to dense array since scipy.sparse doesn't handle 3D
0.9.1,"In the case that the median std was equal to zeros, we have to"
0.9.1,create non-null entry based on the encoded of OHE
0.9.1,tie breaking argmax
0.9.1,generate sample indices that will be used to generate new samples
0.9.1,"for each drawn samples, select its k-neighbors and generate a sample"
0.9.1,"where for each feature individually, each category generated is the"
0.9.1,most common category
0.9.1,the kneigbors search will include the sample itself which is
0.9.1,expected from the original algorithm
0.9.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.9.1,Christos Aridas
0.9.1,Dzianis Dudnik
0.9.1,License: MIT
0.9.1,create 2 random continuous feature
0.9.1,create a categorical feature using some string
0.9.1,create a categorical feature using some integer
0.9.1,return the categories
0.9.1,create 2 random continuous feature
0.9.1,create a categorical feature using some string
0.9.1,create a categorical feature using some integer
0.9.1,return the categories
0.9.1,create 2 random continuous feature
0.9.1,create a categorical feature using some string
0.9.1,create a categorical feature using some integer
0.9.1,return the categories
0.9.1,create 2 random continuous feature
0.9.1,create a categorical feature using some string
0.9.1,create a categorical feature using some integer
0.9.1,return the categories
0.9.1,create 2 random continuous feature
0.9.1,create a categorical feature using some string
0.9.1,create a categorical feature using some integer
0.9.1,part of the common test which apply to SMOTE-NC even if it is not default
0.9.1,constructible
0.9.1,Check that the samplers handle pandas dataframe and pandas series
0.9.1,FIXME: we should use to_numpy with pandas >= 0.25
0.9.1,Cast X and y to not default dtype
0.9.1,Non-regression test for #662
0.9.1,https://github.com/scikit-learn-contrib/imbalanced-learn/issues/662
0.9.1,check that the categorical feature is not random but correspond to the
0.9.1,categories seen in the minority class samples
0.9.1,overall check for SMOTEN
0.9.1,check if the SMOTEN resample data as expected
0.9.1,"we generate data such that ""not apple"" will be the minority class and"
0.9.1,"samples from this class will be generated. We will force the ""blue"""
0.9.1,"category to be associated with this class. Therefore, the new generated"
0.9.1,"samples should as well be from the ""blue"" category."
0.9.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.9.1,Christos Aridas
0.9.1,License: MIT
0.9.1,check that m_neighbors is properly set. Regression test for:
0.9.1,https://github.com/scikit-learn-contrib/imbalanced-learn/issues/568
0.9.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.9.1,Christos Aridas
0.9.1,License: MIT
0.9.1,FIXME: we should use to_numpy with pandas >= 0.25
0.9.1,check the random over-sampling with a multiclass problem
0.9.1,check that resampling with heterogeneous dtype is working with basic
0.9.1,resampling
0.9.1,check that we can oversample even with missing or infinite data
0.9.1,regression tests for #605
0.9.1,check that we raise an error when heterogeneous dtype data are given
0.9.1,and a smoothed bootstrap is requested
0.9.1,check that smoothed bootstrap is working for numerical array
0.9.1,check that a shrinkage factor of 0 is equivalent to not create a smoothed
0.9.1,bootstrap
0.9.1,check the behaviour of the shrinkage parameter
0.9.1,the covariance of the data generated with the larger shrinkage factor
0.9.1,should also be larger.
0.9.1,check the validation of the shrinkage parameter
0.9.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.9.1,Christos Aridas
0.9.1,License: MIT
0.9.1,shuffle the indices since the sampler are packing them by class
0.9.1,helper functions
0.9.1,input and output
0.9.1,build the model and weights
0.9.1,"build the loss, predict, and train operator"
0.9.1,Initialization of all variables in the graph
0.9.1,"For each epoch, run accuracy on train and test"
0.9.1,helper functions
0.9.1,input and output
0.9.1,build the model and weights
0.9.1,"build the loss, predict, and train operator"
0.9.1,Initialization of all variables in the graph
0.9.1,"For each epoch, run accuracy on train and test"
0.9.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.9.1,License: MIT
0.9.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.9.1,Fernando Nogueira
0.9.1,Christos Aridas
0.9.1,License: MIT
0.9.1,find which class to not consider
0.9.1,there is a Tomek link between two samples if they are both nearest
0.9.1,neighbors of each others.
0.9.1,Find the nearest neighbour of every point
0.9.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.9.1,Christos Aridas
0.9.1,License: MIT
0.9.1,Randomly get one sample from the majority class
0.9.1,Generate the index to select
0.9.1,Create the set C - One majority samples and all minority
0.9.1,Create the set S - all majority samples
0.9.1,fit knn on C
0.9.1,Check each sample in S if we keep it or drop it
0.9.1,Do not select sample which are already well classified
0.9.1,Classify on S
0.9.1,If the prediction do not agree with the true label
0.9.1,append it in C_x
0.9.1,Keep the index for later
0.9.1,Update C
0.9.1,fit a knn on C
0.9.1,This experimental to speed up the search
0.9.1,Classify all the element in S and avoid to test the
0.9.1,well classified elements
0.9.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.9.1,Dayvid Oliveira
0.9.1,Christos Aridas
0.9.1,License: MIT
0.9.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.9.1,Christos Aridas
0.9.1,License: MIT
0.9.1,Compute the distance considering the farthest neighbour
0.9.1,Sort the list of distance and get the index
0.9.1,Throw a warning to tell the user that we did not have enough samples
0.9.1,to select and that we just select everything
0.9.1,Select the desired number of samples
0.9.1,idx_tmp is relative to the feature selected in the
0.9.1,previous step and we need to find the indirection
0.9.1,fmt: off
0.9.1,fmt: on
0.9.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.9.1,Christos Aridas
0.9.1,License: MIT
0.9.1,select a sample from the current class
0.9.1,create the set composed of all minority samples and one
0.9.1,sample from the current class.
0.9.1,create the set S with removing the seed from S
0.9.1,since that it will be added anyway
0.9.1,apply Tomek cleaning
0.9.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.9.1,Dayvid Oliveira
0.9.1,Christos Aridas
0.9.1,License: MIT
0.9.1,Check the stopping criterion
0.9.1,1. If there is no changes for the vector y
0.9.1,2. If the number of samples in the other class become inferior to
0.9.1,the number of samples in the majority class
0.9.1,3. If one of the class is disappearing
0.9.1,Case 1
0.9.1,Case 2
0.9.1,Case 3
0.9.1,Check the stopping criterion
0.9.1,1. If the number of samples in the other class become inferior to
0.9.1,the number of samples in the majority class
0.9.1,2. If one of the class is disappearing
0.9.1,Case 1else:
0.9.1,overwrite b_min_bec_maj
0.9.1,Case 2
0.9.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.9.1,Christos Aridas
0.9.1,License: MIT
0.9.1,clean the neighborhood
0.9.1,compute which classes to consider for cleaning for the A2 group
0.9.1,compute a2 group
0.9.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.9.1,Christos Aridas
0.9.1,License: MIT
0.9.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.9.1,Christos Aridas
0.9.1,License: MIT
0.9.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.9.1,Christos Aridas
0.9.1,License: MIT
0.9.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.9.1,Christos Aridas
0.9.1,License: MIT
0.9.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.9.1,Christos Aridas
0.9.1,License: MIT
0.9.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.9.1,Christos Aridas
0.9.1,License: MIT
0.9.1,FIXME: we should use to_numpy with pandas >= 0.25
0.9.1,check that we can undersample even with missing or infinite data
0.9.1,regression tests for #605
0.9.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.9.1,Christos Aridas
0.9.1,License: MIT
0.9.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.9.1,Christos Aridas
0.9.1,License: MIT
0.9.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.9.1,Christos Aridas
0.9.1,License: MIT
0.9.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.9.1,Christos Aridas
0.9.1,License: MIT
0.9.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.9.1,Christos Aridas
0.9.1,License: MIT
0.9.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.9.1,Fernando Nogueira
0.9.1,Christos Aridas
0.9.1,License: MIT
0.9.1,check that we deprecate the `n_jobs` parameter.
0.9.1,check that the samples selecting by the hard voting corresponds to the
0.9.1,targeted class
0.9.1,non-regression test for:
0.9.1,https://github.com/scikit-learn-contrib/imbalanced-learn/issues/738
0.9.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.9.1,Christos Aridas
0.9.1,License: MIT
0.9.1,test that all_estimators doesn't find abstract classes.
0.9.1,"For NearMiss, let's check the three algorithms"
0.9.1,Common tests for estimator instances
0.9.1,Authors: Alexandre Gramfort <alexandre.gramfort@inria.fr>
0.9.1,Raghav RV <rvraghav93@gmail.com>
0.9.1,License: BSD 3 clause
0.9.1,"walk_packages() ignores DeprecationWarnings, now we need to ignore"
0.9.1,FutureWarnings
0.9.1,"mypy error: Module has no attribute ""__path__"""
0.9.1,functions to ignore args / docstring of
0.9.1,Methods where y param should be ignored if y=None by default
0.9.1,numpydoc 0.8.0's docscrape tool raises because of collections.abc under
0.9.1,Python 3.7
0.9.1,Test module docstring formatting
0.9.1,Skip test if numpydoc is not found
0.9.1,XXX unreached code as of v0.22
0.9.1,"pytest tooling, not part of the scikit-learn API"
0.9.1,Exclude non-scikit-learn classes
0.9.1,Now skip docstring test for y when y is None
0.9.1,by default for API reason
0.9.1,Exclude imported functions
0.9.1,Don't test private methods / functions
0.9.1,Test that there are no tabs in our source files
0.9.1,because we don't import
0.9.1,Minimal / degenerate instances: only useful to test the docstrings.
0.9.1,"As certain attributes are present ""only"" if a certain parameter is"
0.9.1,"provided, this checks if the word ""only"" is present in the attribute"
0.9.1,"description, and if not the attribute is required to be present."
0.9.1,ignore deprecation warnings
0.9.1,attributes
0.9.1,properties
0.9.1,ignore properties that raises an AttributeError and deprecated
0.9.1,properties
0.9.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.9.1,License: MIT
0.9.1,check that we can let a pass a regression variable by turning down the
0.9.1,validation
0.9.1,Check that the validation is bypass when calling `fit`
0.9.1,Non-regression test for:
0.9.1,https://github.com/scikit-learn-contrib/imbalanced-learn/issues/782
0.9.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.9.1,Christos Aridas
0.9.1,License: MIT
0.9.1,store timestamp to figure out whether the result of 'fit' has been
0.9.1,cached or not
0.9.1,store timestamp to figure out whether the result of 'fit' has been
0.9.1,cached or not
0.9.1,Pipeline accepts steps as tuple
0.9.1,Test the various init parameters of the pipeline.
0.9.1,Check that we can't instantiate pipelines with objects without fit
0.9.1,method
0.9.1,Smoke test with only an estimator
0.9.1,Check that params are set
0.9.1,Smoke test the repr:
0.9.1,Test with two objects
0.9.1,Check that we can't instantiate with non-transformers on the way
0.9.1,"Note that NoTrans implements fit, but not transform"
0.9.1,Check that params are set
0.9.1,Smoke test the repr:
0.9.1,Check that params are not set when naming them wrong
0.9.1,Test clone
0.9.1,"Check that apart from estimators, the parameters are the same"
0.9.1,Remove estimators that where copied
0.9.1,Test the various methods of the pipeline (anova).
0.9.1,Test with Anova + LogisticRegression
0.9.1,Test that the pipeline can take fit parameters
0.9.1,classifier should return True
0.9.1,and transformer params should not be changed
0.9.1,invalid parameters should raise an error message
0.9.1,Pipeline should pass sample_weight
0.9.1,When sample_weight is None it shouldn't be passed
0.9.1,Test pipeline raises set params error message for nested models.
0.9.1,nested model check
0.9.1,Test the various methods of the pipeline (pca + svm).
0.9.1,Test with PCA + SVC
0.9.1,Test the various methods of the pipeline (preprocessing + svm).
0.9.1,check shapes of various prediction functions
0.9.1,test that the fit_predict method is implemented on a pipeline
0.9.1,test that the fit_predict on pipeline yields same results as applying
0.9.1,transform and clustering steps separately
0.9.1,"As pipeline doesn't clone estimators on construction,"
0.9.1,it must have its own estimators
0.9.1,first compute the transform and clustering step separately
0.9.1,use a pipeline to do the transform and clustering in one step
0.9.1,tests that a pipeline does not have fit_predict method when final
0.9.1,step of pipeline does not have fit_predict defined
0.9.1,tests that Pipeline passes fit_params to intermediate steps
0.9.1,when fit_predict is invoked
0.9.1,Test whether pipeline works with a transformer at the end.
0.9.1,Also test pipeline.transform and pipeline.inverse_transform
0.9.1,test transform and fit_transform:
0.9.1,Test whether pipeline works with a transformer missing fit_transform
0.9.1,test fit_transform:
0.9.1,Directly setting attr
0.9.1,Using set_params
0.9.1,Using set_params to replace single step
0.9.1,With invalid data
0.9.1,Test setting Pipeline steps to None
0.9.1,"for other methods, ensure no AttributeErrors on None:"
0.9.1,mult2 and mult3 are active
0.9.1,Check 'passthrough' step at construction time
0.9.1,Test that an error is raised when memory is not a string or a Memory
0.9.1,instance
0.9.1,Define memory as an integer
0.9.1,Test with Transformer + SVC
0.9.1,Memoize the transformer at the first fit
0.9.1,Get the time stamp of the tranformer in the cached pipeline
0.9.1,Check that cached_pipe and pipe yield identical results
0.9.1,Check that we are reading the cache while fitting
0.9.1,a second time
0.9.1,Check that cached_pipe and pipe yield identical results
0.9.1,Create a new pipeline with cloned estimators
0.9.1,Check that even changing the name step does not affect the cache hit
0.9.1,Check that cached_pipe and pipe yield identical results
0.9.1,Test with Transformer + SVC
0.9.1,Memoize the transformer at the first fit
0.9.1,Get the time stamp of the tranformer in the cached pipeline
0.9.1,Check that cached_pipe and pipe yield identical results
0.9.1,Check that we are reading the cache while fitting
0.9.1,a second time
0.9.1,Check that cached_pipe and pipe yield identical results
0.9.1,Create a new pipeline with cloned estimators
0.9.1,Check that even changing the name step does not affect the cache hit
0.9.1,Check that cached_pipe and pipe yield identical results
0.9.1,Test the various methods of the pipeline (pca + svm).
0.9.1,Test with PCA + SVC
0.9.1,Test the various methods of the pipeline (pca + svm).
0.9.1,Test with PCA + SVC
0.9.1,Test whether pipeline works with a sampler at the end.
0.9.1,Also test pipeline.sampler
0.9.1,test transform and fit_transform:
0.9.1,We round the value near to zero. It seems that PCA has some issue
0.9.1,with that
0.9.1,Test whether pipeline works with a sampler at the end.
0.9.1,Also test pipeline.sampler
0.9.1,Test pipeline using None as preprocessing step and a classifier
0.9.1,"Test pipeline using None, RUS and a classifier"
0.9.1,"Test pipeline using RUS, None and a classifier"
0.9.1,Test pipeline using None step and a sampler
0.9.1,Test pipeline using None and a transformer that implements transform and
0.9.1,inverse_transform
0.9.1,Test the various methods of the pipeline (anova).
0.9.1,Test with RandomUnderSampling + Anova + LogisticRegression
0.9.1,Test the various methods of the pipeline (anova).
0.9.1,Test the various methods of the pipeline (anova).
0.9.1,Test with RandomUnderSampling + Anova + LogisticRegression
0.9.1,tests that Pipeline passes predict_params to the final estimator
0.9.1,when predict is invoked
0.9.1,Test that the score_samples method is implemented on a pipeline.
0.9.1,Test that the score_samples method on pipeline yields same results as
0.9.1,applying transform and score_samples steps separately.
0.9.1,Check the shapes
0.9.1,Check the values
0.9.1,Test that a pipeline does not have score_samples method when the final
0.9.1,step of the pipeline does not have score_samples defined.
0.9.1,Test that the score_samples method is implemented on a pipeline.
0.9.1,Test that the score_samples method on pipeline yields same results as
0.9.1,applying transform and score_samples steps separately.
0.9.1,Check the shapes
0.9.1,Check the values
0.9.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.9.1,Christos Aridas
0.9.1,License: MIT
0.9.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.9.1,License: MIT
0.9.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.9.1,License: MIT
0.9.1,Adapated from scikit-learn
0.9.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.9.1,License: MIT
0.9.1,we don't filter samplers based on their tag here because we want to make
0.9.1,sure that the fitted attribute does not exist if the tag is not
0.9.1,stipulated
0.9.1,trigger our checks if this is a SamplerMixin
0.9.1,should raise warning if the target is continuous (we cannot raise error)
0.9.1,if the target is multilabel then we should raise an error
0.9.1,IHT does not enforce the number of samples but provide a number
0.9.1,of samples the closest to the desired target.
0.9.1,in this test we will force all samplers to not change the class 1
0.9.1,check that sparse matrices can be passed through the sampler leading to
0.9.1,the same results than dense
0.9.1,Check that the samplers handle pandas dataframe and pandas series
0.9.1,check that we return the same type for dataframes or series types
0.9.1,FIXME: we should use to_numpy with pandas >= 0.25
0.9.1,Check that the can samplers handle simple lists
0.9.1,Check that multiclass target lead to the same results than OVA encoding
0.9.1,Cast X and y to not default dtype
0.9.1,Non-regression test for #709
0.9.1,https://github.com/scikit-learn-contrib/imbalanced-learn/issues/709
0.9.1,Adapted from scikit-learn
0.9.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.9.1,License: MIT
0.9.1,Ignore deprecation warnings triggered at import time and from walking
0.9.1,packages
0.9.1,get rid of abstract base classes
0.9.1,get rid of sklearn estimators which have been imported in some classes
0.9.1,"drop duplicates, sort for reproducibility"
0.9.1,itemgetter is used to ensure the sort does not extend to the 2nd item of
0.9.1,the tuple
0.9.1,Author: Alexander L. Hayes <hayesall@iu.edu>
0.9.1,License: MIT
0.9.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.9.1,License: MIT
0.9.1,check that all keys in sampling_strategy are also in y
0.9.1,check that there is no negative number
0.9.1,check that all keys in sampling_strategy are also in y
0.9.1,ignore first 'self' argument for instance methods
0.9.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.9.1,Christos Aridas
0.9.1,License: MIT
0.9.1,this function could create an equal number of samples
0.9.1,We pass on purpose a non sorted dictionary and check that the resulting
0.9.1,dictionary is sorted. Refer to issue #428.
0.9.1,DataFrame and DataFrame case
0.9.1,DataFrames and Series case
0.9.1,The * is place before a keyword only argument without a default value
0.9.1,Test that the minimum dependencies in the README.rst file are
0.9.1,consistent with the minimum dependencies defined at the file:
0.9.1,imblearn/_min_dependencies.py
0.9.1,Skip the test if the README.rst file is not available.
0.9.1,"For instance, when installing scikit-learn from wheels"
0.9.1,Author: Alexander L. Hayes <hayesall@iu.edu>
0.9.1,License: MIT
0.9.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.9.1,License: MIT
0.9.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.9.1,Christos Aridas
0.9.1,License: MIT
0.9.1,check if the filtering is working with a list or a single string
0.9.1,check that all estimators are sampler
0.9.1,check that an error is raised when the type is unknown
0.9.1,TODO: remove in 0.9
0.9.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.9.1,License: MIT
0.9.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.9.1,Christos Aridas
0.9.1,License: MIT
0.9.1,Otherwise create a default SMOTE
0.9.1,Otherwise create a default TomekLinks
0.9.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.9.1,Christos Aridas
0.9.1,License: MIT
0.9.1,Otherwise create a default SMOTE
0.9.1,Otherwise create a default EditedNearestNeighbours
0.9.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.9.1,Christos Aridas
0.9.1,License: MIT
0.9.1,Check if default job count is None
0.9.1,Check if job count is set
0.9.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.9.1,Christos Aridas
0.9.1,License: MIT
0.9.1,Check if default job count is none
0.9.1,Check if job count is set
0.9.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.9.1,License: MIT
0.9.1,resample before to fit the tree
0.9.1,Validate or convert input data
0.9.1,Pre-sort indices to avoid that each individual tree of the
0.9.1,ensemble sorts the indices.
0.9.1,reshape is necessary to preserve the data contiguity against vs
0.9.1,"[:, np.newaxis] that does not."
0.9.1,Get bootstrap sample size
0.9.1,Check parameters
0.9.1,"Free allocated memory, if any"
0.9.1,We draw from the random state to get the random state we
0.9.1,would have got if we hadn't used a warm_start.
0.9.1,Parallel loop: we prefer the threading backend as the Cython code
0.9.1,for fitting the trees is internally releasing the Python GIL
0.9.1,making threading more efficient than multiprocessing in
0.9.1,"that case. However, we respect any parallel_backend contexts set"
0.9.1,"at a higher level, since correctness does not rely on using"
0.9.1,threads.
0.9.1,Collect newly grown trees
0.9.1,Create pipeline with the fitted samplers and trees
0.9.1,FIXME: we could consider to support multiclass-multioutput if
0.9.1,we introduce or reuse a constructor parameter (e.g.
0.9.1,oob_score) allowing our user to pass a callable defining the
0.9.1,scoring strategy on OOB sample.
0.9.1,Decapsulate classes_ attributes
0.9.1,drop the n_outputs axis if there is a single output
0.9.1,Prediction requires X to be in CSR format
0.9.1,n_classes_ is a ndarray at this stage
0.9.1,all the supported type of target will have the same number of
0.9.1,classes in all outputs
0.9.1,"for regression, n_classes_ does not exist and we create an empty"
0.9.1,axis to be consistent with the classification case and make
0.9.1,the array operations compatible with the 2 settings
0.9.1,Instances incorrectly classified
0.9.1,Error fraction
0.9.1,Stop if classification is perfect
0.9.1,Construct y coding as described in Zhu et al [2]:
0.9.1,
0.9.1,y_k = 1 if c == k else -1 / (K - 1)
0.9.1,
0.9.1,"where K == n_classes_ and c, k in [0, K) are indices along the second"
0.9.1,axis of the y coding with c being the index corresponding to the true
0.9.1,class label.
0.9.1,Displace zero probabilities so the log is defined.
0.9.1,Also fix negative elements which may occur with
0.9.1,negative sample weights.
0.9.1,Boost weight using multi-class AdaBoost SAMME.R alg
0.9.1,Only boost the weights if it will fit again
0.9.1,Only boost positive weights
0.9.1,Instances incorrectly classified
0.9.1,Error fraction
0.9.1,Stop if classification is perfect
0.9.1,Stop if the error is at least as bad as random guessing
0.9.1,Boost weight using multi-class AdaBoost SAMME alg
0.9.1,Only boost the weights if I will fit again
0.9.1,Only boost positive weights
0.9.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.9.1,Christos Aridas
0.9.1,License: MIT
0.9.1,overwrite the base class method by disallowing `sample_weight`
0.9.1,RandomUnderSampler is not supporting sample_weight. We need to pass
0.9.1,None.
0.9.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.9.1,Christos Aridas
0.9.1,License: MIT
0.9.1,overwrite the base class method by disallowing `sample_weight`
0.9.1,the sampler needs to be validated before to call _fit because
0.9.1,_validate_y is called before _validate_estimator and would require
0.9.1,to know which type of sampler we are using.
0.9.1,RandomUnderSampler is not supporting sample_weight. We need to pass
0.9.1,None.
0.9.1,check that we have an ensemble of samplers and estimators with a
0.9.1,consistent size
0.9.1,each sampler in the ensemble should have different random state
0.9.1,each estimator in the ensemble should have different random state
0.9.1,check the consistency of the feature importances
0.9.1,check the consistency of the prediction outpus
0.9.1,Predictions should be the same when sample_weight are all ones
0.9.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.9.1,Christos Aridas
0.9.1,License: MIT
0.9.1,Check classification for various parameter settings.
0.9.1,Test that bootstrapping samples generate non-perfect base estimators.
0.9.1,"without bootstrap, all trees are perfect on the training set"
0.9.1,disable the resampling by passing an empty dictionary.
0.9.1,"with bootstrap, trees are no longer perfect on the training set"
0.9.1,Test that bootstrapping features may generate duplicate features.
0.9.1,Predict probabilities.
0.9.1,Normal case
0.9.1,"Degenerate case, where some classes are missing"
0.9.1,Check that oob prediction is a good estimation of the generalization
0.9.1,error.
0.9.1,Test with few estimators
0.9.1,Check singleton ensembles.
0.9.1,Test that it gives proper exception on deficient input.
0.9.1,Test support of decision_function
0.9.1,Check that bagging ensembles can be grid-searched.
0.9.1,Transform iris into a binary classification task
0.9.1,Grid search with scoring based on decision_function
0.9.1,Check base_estimator and its default values.
0.9.1,Test if fitting incrementally with warm start gives a forest of the
0.9.1,right size and the same results as a normal fit.
0.9.1,Test if warm start'ed second fit with smaller n_estimators raises error.
0.9.1,Test that nothing happens when fitting without increasing n_estimators
0.9.1,"modify X to nonsense values, this should not change anything"
0.9.1,warm started classifier with 5+5 estimators should be equivalent to
0.9.1,one classifier with 10 estimators
0.9.1,Check using oob_score and warm_start simultaneously fails
0.9.1,"Make sure OOB scores are identical when random_state, estimator, and"
0.9.1,training data are fixed and fitting is done twice
0.9.1,Check that format of estimators_samples_ is correct and that results
0.9.1,generated at fit time can be identically reproduced at a later time
0.9.1,using data saved in object attributes.
0.9.1,remap the y outside of the BalancedBaggingclassifier
0.9.1,"_, y = np.unique(y, return_inverse=True)"
0.9.1,Get relevant attributes
0.9.1,Test for correct formatting
0.9.1,Re-fit single estimator to test for consistent sampling
0.9.1,Make sure validated max_samples and original max_samples are identical
0.9.1,when valid integer max_samples supplied by user
0.9.1,check that we can pass any kind of sampler to a bagging classifier
0.9.1,check that we have balanced class with the right counts of class
0.9.1,sample depending on the sampling strategy
0.9.1,check that we can provide a FunctionSampler in BalancedBaggingClassifier
0.9.1,find the minority and majority classes
0.9.1,compute the number of sample to draw from the majority class using
0.9.1,a negative binomial distribution
0.9.1,draw randomly with or without replacement
0.9.1,Roughly Balanced Bagging
0.9.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.9.1,Christos Aridas
0.9.1,License: MIT
0.9.1,Generate a global dataset to use
0.9.1,Check classification for various parameter settings.
0.9.1,test the different prediction function
0.9.1,Check base_estimator and its default values.
0.9.1,Test if fitting incrementally with warm start gives a forest of the
0.9.1,right size and the same results as a normal fit.
0.9.1,Test if warm start'ed second fit with smaller n_estimators raises error.
0.9.1,Test that nothing happens when fitting without increasing n_estimators
0.9.1,"modify X to nonsense values, this should not change anything"
0.9.1,warm started classifier with 5+5 estimators should be equivalent to
0.9.1,one classifier with 10 estimators
0.9.1,Check warning if not enough estimators
0.9.1,First fit with no restriction on max samples
0.9.1,Second fit with max samples restricted to just 2
0.9.1,Regression test for #655: check that the oob score is closed to 0.5
0.9.1,a binomial experiment.
0.9.1,Author: Guillaume Lemaitre
0.9.1,License: BSD 3 clause
0.9.1,"The index start at one, then we need to remove one"
0.9.1,to not have issue with the indexing.
0.9.1,go through the list and check if the data are available
0.9.1,Authors: Dayvid Oliveira
0.9.1,Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.9.1,Christos Aridas
0.9.1,License: MIT
0.9.1,restrict ratio to be a dict or a callable
0.9.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.9.1,Christos Aridas
0.9.1,License: MIT
0.9.1,"we are reusing part of utils.check_sampling_strategy, however this is not"
0.9.1,cover in the common tests so we will repeat it here
0.9.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.9.1,Christos Aridas
0.9.1,License: MIT
0.9.1,This is a trick to avoid an error during tests collection with pytest. We
0.9.1,avoid the error when importing the package raise the error at the moment of
0.9.1,creating the instance.
0.9.1,This is a trick to avoid an error during tests collection with pytest. We
0.9.1,avoid the error when importing the package raise the error at the moment of
0.9.1,creating the instance.
0.9.1,flag for keras sequence duck-typing
0.9.1,shuffle the indices since the sampler are packing them by class
0.9.0,This file is here so that when running from the root folder
0.9.0,./imblearn is added to sys.path by pytest.
0.9.0,See https://docs.pytest.org/en/latest/pythonpath.html for more details.
0.9.0,"For example, this allows to build extensions in place and run pytest"
0.9.0,doc/modules/clustering.rst and use imblearn from the local folder
0.9.0,rather than the one from site-packages.
0.9.0,! /usr/bin/env python
0.9.0,Python 2 compat: just to be able to declare that Python >=3.7 is needed.
0.9.0,This is a bit (!) hackish: we are setting a global variable so that the
0.9.0,main imblearn __init__ can detect if it is being loaded by the setup
0.9.0,"routine, to avoid attempting to load components that aren't built yet:"
0.9.0,the numpy distutils extensions that are used by imbalanced-learn to
0.9.0,recursively build the compiled extensions in sub-packages is based on the
0.9.0,Python import machinery.
0.9.0,get __version__ from _version.py
0.9.0,-*- coding: utf-8 -*-
0.9.0,
0.9.0,"imbalanced-learn documentation build configuration file, created by"
0.9.0,sphinx-quickstart on Mon Jan 18 14:44:12 2016.
0.9.0,
0.9.0,This file is execfile()d with the current directory set to its
0.9.0,containing dir.
0.9.0,
0.9.0,Note that not all possible configuration values are present in this
0.9.0,autogenerated file.
0.9.0,
0.9.0,All configuration values have a default; values that are commented out
0.9.0,serve to show the default.
0.9.0,"If extensions (or modules to document with autodoc) are in another directory,"
0.9.0,add these directories to sys.path here. If the directory is relative to the
0.9.0,"documentation root, use os.path.abspath to make it absolute, like shown here."
0.9.0,-- General configuration ------------------------------------------------
0.9.0,"If your documentation needs a minimal Sphinx version, state it here."
0.9.0,needs_sphinx = '1.0'
0.9.0,"Add any Sphinx extension module names here, as strings. They can be"
0.9.0,extensions coming with Sphinx (named 'sphinx.ext.*') or your custom
0.9.0,ones.
0.9.0,"Add any paths that contain templates here, relative to this directory."
0.9.0,The suffix of source filenames.
0.9.0,The master toctree document.
0.9.0,General information about the project.
0.9.0,"The version info for the project you're documenting, acts as replacement for"
0.9.0,"|version| and |release|, also used in various other places throughout the"
0.9.0,built documents.
0.9.0,
0.9.0,The short X.Y version.
0.9.0,"The full version, including alpha/beta/rc tags."
0.9.0,"List of patterns, relative to source directory, that match files and"
0.9.0,directories to ignore when looking for source files.
0.9.0,The reST default role (used for this markup: `text`) to use for all
0.9.0,documents.
0.9.0,"If true, '()' will be appended to :func: etc. cross-reference text."
0.9.0,The name of the Pygments (syntax highlighting) style to use.
0.9.0,-- Options for math equations -----------------------------------------------
0.9.0,-- Options for HTML output ----------------------------------------------
0.9.0,The theme to use for HTML and HTML Help pages.  See the documentation for
0.9.0,a list of builtin themes.
0.9.0,"""twitter_url"": ""https://twitter.com/pandas_dev"","
0.9.0,"""navbar_align"": ""right"",  # For testing that the navbar items align properly"
0.9.0,"Add any paths that contain custom static files (such as style sheets) here,"
0.9.0,"relative to this directory. They are copied after the builtin static files,"
0.9.0,"so a file named ""default.css"" will overwrite the builtin ""default.css""."
0.9.0,Output file base name for HTML help builder.
0.9.0,-- Options for autodoc ------------------------------------------------------
0.9.0,generate autosummary even if no references
0.9.0,-- Options for numpydoc -----------------------------------------------------
0.9.0,this is needed for some reason...
0.9.0,see https://github.com/numpy/numpydoc/issues/69
0.9.0,-- Options for sphinxcontrib-bibtex -----------------------------------------
0.9.0,bibtex file
0.9.0,-- Options for intersphinx --------------------------------------------------
0.9.0,intersphinx configuration
0.9.0,-- Options for sphinx-gallery -----------------------------------------------
0.9.0,Generate the plot for the gallery
0.9.0,sphinx-gallery configuration
0.9.0,-- Options for github link for what's new -----------------------------------
0.9.0,Config for sphinx_issues
0.9.0,The following is used by sphinx.ext.linkcode to provide links to github
0.9.0,-- Options for LaTeX output ---------------------------------------------
0.9.0,The paper size ('letterpaper' or 'a4paper').
0.9.0,"'papersize': 'letterpaper',"
0.9.0,"The font size ('10pt', '11pt' or '12pt')."
0.9.0,"'pointsize': '10pt',"
0.9.0,Additional stuff for the LaTeX preamble.
0.9.0,"'preamble': '',"
0.9.0,Grouping the document tree into LaTeX files. List of tuples
0.9.0,"(source start file, target name, title,"
0.9.0,"author, documentclass [howto, manual, or own class])."
0.9.0,-- Options for manual page output ---------------------------------------
0.9.0,"If false, no module index is generated."
0.9.0,latex_domain_indices = True
0.9.0,One entry per manual page. List of tuples
0.9.0,"(source start file, name, description, authors, manual section)."
0.9.0,"If true, show URL addresses after external links."
0.9.0,man_show_urls = False
0.9.0,-- Options for Texinfo output -------------------------------------------
0.9.0,Grouping the document tree into Texinfo files. List of tuples
0.9.0,"(source start file, target name, title, author,"
0.9.0,"dir menu entry, description, category)"
0.9.0,-- Dependencies generation ----------------------------------------------
0.9.0,get length of header
0.9.0,-- Additional temporary hacks -----------------------------------------------
0.9.0,Temporary work-around for spacing problem between parameter and parameter
0.9.0,"type in the doc, see https://github.com/numpy/numpydoc/issues/215. The bug"
0.9.0,has been fixed in sphinx (https://github.com/sphinx-doc/sphinx/pull/5976) but
0.9.0,through a change in sphinx basic.css except rtd_theme does not use basic.css.
0.9.0,"In an ideal world, this would get fixed in this PR:"
0.9.0,https://github.com/readthedocs/sphinx_rtd_theme/pull/747/files
0.9.0,get the styles from the current theme
0.9.0,create and add the button to all the code blocks that contain >>>
0.9.0,tracebacks (.gt) contain bare text elements that need to be
0.9.0,wrapped in a span to work with .nextUntil() (see later)
0.9.0,define the behavior of the button when it's clicked
0.9.0,hide the code output
0.9.0,show the code output
0.9.0,-*- coding: utf-8 -*-
0.9.0,Format template for issues URI
0.9.0,e.g. 'https://github.com/sloria/marshmallow/issues/{issue}
0.9.0,Format template for PR URI
0.9.0,e.g. 'https://github.com/sloria/marshmallow/pull/{issue}
0.9.0,Format template for commit URI
0.9.0,e.g. 'https://github.com/sloria/marshmallow/commits/{commit}
0.9.0,"Shortcut for Github, e.g. 'sloria/marshmallow'"
0.9.0,Format template for user profile URI
0.9.0,e.g. 'https://github.com/{user}'
0.9.0,Python 2 only
0.9.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.9.0,License: MIT
0.9.0,%%
0.9.0,%%
0.9.0,"First, we will generate a toy classification dataset with only few samples."
0.9.0,The ratio between the classes will be imbalanced.
0.9.0,%%
0.9.0,%%
0.9.0,"Now, we will use a :class:`~imblearn.over_sampling.RandomOverSampler` to"
0.9.0,generate a bootstrap for the minority class with as many samples as in the
0.9.0,majority class.
0.9.0,%%
0.9.0,%%
0.9.0,We observe that the minority samples are less transparent than the samples
0.9.0,"from the majority class. Indeed, it is due to the fact that these samples"
0.9.0,of the minority class are repeated during the bootstrap generation.
0.9.0,
0.9.0,We can set `shrinkage` to a floating value to add a small perturbation to the
0.9.0,samples created and therefore create a smoothed bootstrap.
0.9.0,%%
0.9.0,%%
0.9.0,"In this case, we see that the samples in the minority class are not"
0.9.0,overlapping anymore due to the added noise.
0.9.0,
0.9.0,The parameter `shrinkage` allows to add more or less perturbation. Let's
0.9.0,add more perturbation when generating the smoothed bootstrap.
0.9.0,%%
0.9.0,%%
0.9.0,Increasing the value of `shrinkage` will disperse the new samples. Forcing
0.9.0,the shrinkage to 0 will be equivalent to generating a normal bootstrap.
0.9.0,%%
0.9.0,%%
0.9.0,"Therefore, the `shrinkage` is handy to manually tune the dispersion of the"
0.9.0,new samples.
0.9.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.9.0,License: MIT
0.9.0,%%
0.9.0,generate some data points
0.9.0,plot the majority and minority samples
0.9.0,draw the circle in which the new sample will generated
0.9.0,plot the line on which the sample will be generated
0.9.0,create and plot the new sample
0.9.0,make the plot nicer with legend and label
0.9.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.9.0,License: MIT
0.9.0,%%
0.9.0,%% [markdown]
0.9.0,The following function will be used to create toy dataset. It uses the
0.9.0,:func:`~sklearn.datasets.make_classification` from scikit-learn but fixing
0.9.0,some parameters.
0.9.0,%%
0.9.0,%% [markdown]
0.9.0,The following function will be used to plot the sample space after resampling
0.9.0,to illustrate the specificities of an algorithm.
0.9.0,%%
0.9.0,%% [markdown]
0.9.0,The following function will be used to plot the decision function of a
0.9.0,classifier given some data.
0.9.0,%%
0.9.0,%% [markdown]
0.9.0,Illustration of the influence of the balancing ratio
0.9.0,----------------------------------------------------
0.9.0,
0.9.0,We will first illustrate the influence of the balancing ratio on some toy
0.9.0,data using a logistic regression classifier which is a linear model.
0.9.0,%%
0.9.0,%% [markdown]
0.9.0,We will fit and show the decision boundary model to illustrate the impact of
0.9.0,dealing with imbalanced classes.
0.9.0,%%
0.9.0,%% [markdown]
0.9.0,"Greater is the difference between the number of samples in each class, poorer"
0.9.0,are the classification results.
0.9.0,
0.9.0,Random over-sampling to balance the data set
0.9.0,--------------------------------------------
0.9.0,
0.9.0,Random over-sampling can be used to repeat some samples and balance the
0.9.0,number of samples between the dataset. It can be seen that with this trivial
0.9.0,approach the boundary decision is already less biased toward the majority
0.9.0,class. The class :class:`~imblearn.over_sampling.RandomOverSampler`
0.9.0,implements such of a strategy.
0.9.0,%%
0.9.0,%% [markdown]
0.9.0,"By default, random over-sampling generates a bootstrap. The parameter"
0.9.0,`shrinkage` allows adding a small perturbation to the generated data
0.9.0,to generate a smoothed bootstrap instead. The plot below shows the difference
0.9.0,between the two data generation strategies.
0.9.0,%%
0.9.0,%% [markdown]
0.9.0,It looks like more samples are generated with smoothed bootstrap. This is due
0.9.0,to the fact that the samples generated are not superimposing with the
0.9.0,original samples.
0.9.0,
0.9.0,More advanced over-sampling using ADASYN and SMOTE
0.9.0,--------------------------------------------------
0.9.0,
0.9.0,Instead of repeating the same samples when over-sampling or perturbating the
0.9.0,"generated bootstrap samples, one can use some specific heuristic instead."
0.9.0,:class:`~imblearn.over_sampling.ADASYN` and
0.9.0,:class:`~imblearn.over_sampling.SMOTE` can be used in this case.
0.9.0,%%
0.9.0,%% [markdown]
0.9.0,The following plot illustrates the difference between
0.9.0,:class:`~imblearn.over_sampling.ADASYN` and
0.9.0,:class:`~imblearn.over_sampling.SMOTE`.
0.9.0,:class:`~imblearn.over_sampling.ADASYN` will focus on the samples which are
0.9.0,difficult to classify with a nearest-neighbors rule while regular
0.9.0,:class:`~imblearn.over_sampling.SMOTE` will not make any distinction.
0.9.0,"Therefore, the decision function depending of the algorithm."
0.9.0,%% [markdown]
0.9.0,"Due to those sampling particularities, it can give rise to some specific"
0.9.0,issues as illustrated below.
0.9.0,%%
0.9.0,%% [markdown]
0.9.0,SMOTE proposes several variants by identifying specific samples to consider
0.9.0,during the resampling. The borderline version
0.9.0,(:class:`~imblearn.over_sampling.BorderlineSMOTE`) will detect which point to
0.9.0,select which are in the border between two classes. The SVM version
0.9.0,(:class:`~imblearn.over_sampling.SVMSMOTE`) will use the support vectors
0.9.0,found using an SVM algorithm to create new sample while the KMeans version
0.9.0,(:class:`~imblearn.over_sampling.KMeansSMOTE`) will make a clustering before
0.9.0,to generate samples in each cluster independently depending each cluster
0.9.0,density.
0.9.0,%%
0.9.0,%% [markdown]
0.9.0,"When dealing with a mixed of continuous and categorical features,"
0.9.0,:class:`~imblearn.over_sampling.SMOTENC` is the only method which can handle
0.9.0,this case.
0.9.0,%%
0.9.0,Create a dataset of a mix of numerical and categorical data
0.9.0,%% [markdown]
0.9.0,"However, if the dataset is composed of only categorical features then one"
0.9.0,should use :class:`~imblearn.over_sampling.SMOTEN`.
0.9.0,%%
0.9.0,Generate only categorical data
0.9.0,Authors: Christos Aridas
0.9.0,Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.9.0,License: MIT
0.9.0,%%
0.9.0,%% [markdown]
0.9.0,Let's first generate a dataset with imbalanced class distribution.
0.9.0,%%
0.9.0,%% [markdown]
0.9.0,We will use an over-sampler :class:`~imblearn.over_sampling.SMOTE` followed
0.9.0,by a :class:`~sklearn.tree.DecisionTreeClassifier`. The aim will be to
0.9.0,search which `k_neighbors` parameter is the most adequate with the dataset
0.9.0,that we generated.
0.9.0,%%
0.9.0,%% [markdown]
0.9.0,We can use the :class:`~sklearn.model_selection.validation_curve` to inspect
0.9.0,"the impact of varying the parameter `k_neighbors`. In this case, we need"
0.9.0,to use a score to evaluate the generalization score during the
0.9.0,cross-validation.
0.9.0,%%
0.9.0,%%
0.9.0,%% [markdown]
0.9.0,We can now plot the results of the cross-validation for the different
0.9.0,parameter values that we tried.
0.9.0,%%
0.9.0,make nice plotting
0.9.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.9.0,License: MIT
0.9.0,Generate a dataset
0.9.0,Split the data
0.9.0,Train the classifier with balancing
0.9.0,Test the classifier and get the prediction
0.9.0,Show the classification report
0.9.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.9.0,License: MIT
0.9.0,%%
0.9.0,%% [markdown]
0.9.0,"First, we will generate some imbalanced dataset."
0.9.0,%%
0.9.0,%% [markdown]
0.9.0,We will split the data into a training and testing set.
0.9.0,%%
0.9.0,%% [markdown]
0.9.0,We will create a pipeline made of a :class:`~imblearn.over_sampling.SMOTE`
0.9.0,over-sampler followed by a :class:`~sklearn.svm.LinearSVC` classifier.
0.9.0,%%
0.9.0,%% [markdown]
0.9.0,"Now, we will train the model on the training set and get the prediction"
0.9.0,associated with the testing set. Be aware that the resampling will happen
0.9.0,only when calling `fit`: the number of samples in `y_pred` is the same than
0.9.0,in `y_test`.
0.9.0,%%
0.9.0,%% [markdown]
0.9.0,The geometric mean corresponds to the square root of the product of the
0.9.0,sensitivity and specificity. Combining the two metrics should account for
0.9.0,the balancing of the dataset.
0.9.0,%%
0.9.0,%% [markdown]
0.9.0,The index balanced accuracy can transform any metric to be used in
0.9.0,imbalanced learning problems.
0.9.0,%%
0.9.0,%%
0.9.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.9.0,License: MIT
0.9.0,%%
0.9.0,%% [markdown]
0.9.0,Dataset generation
0.9.0,------------------
0.9.0,
0.9.0,We will create an imbalanced dataset with a couple of samples. We will use
0.9.0,:func:`~sklearn.datasets.make_classification` to generate this dataset.
0.9.0,%%
0.9.0,%%
0.9.0,%% [markdown]
0.9.0,The following function will be used to plot the sample space after resampling
0.9.0,to illustrate the characteristic of an algorithm.
0.9.0,%%
0.9.0,%% [markdown]
0.9.0,The following function will be used to plot the decision function of a
0.9.0,classifier given some data.
0.9.0,%%
0.9.0,%% [markdown]
0.9.0,":class:`~imblearn.over_sampling.SMOTE` allows to generate samples. However,"
0.9.0,this method of over-sampling does not have any knowledge regarding the
0.9.0,"underlying distribution. Therefore, some noisy samples can be generated, e.g."
0.9.0,"when the different classes cannot be well separated. Hence, it can be"
0.9.0,beneficial to apply an under-sampling algorithm to clean the noisy samples.
0.9.0,Two methods are usually used in the literature: (i) Tomek's link and (ii)
0.9.0,edited nearest neighbours cleaning methods. Imbalanced-learn provides two
0.9.0,ready-to-use samplers :class:`~imblearn.combine.SMOTETomek` and
0.9.0,":class:`~imblearn.combine.SMOTEENN`. In general,"
0.9.0,:class:`~imblearn.combine.SMOTEENN` cleans more noisy data than
0.9.0,:class:`~imblearn.combine.SMOTETomek`.
0.9.0,%%
0.9.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.9.0,License: MIT
0.9.0,%%
0.9.0,%% [markdown]
0.9.0,Load an imbalanced dataset
0.9.0,--------------------------
0.9.0,
0.9.0,We will load the UCI SatImage dataset which has an imbalanced ratio of 9.3:1
0.9.0,(number of majority sample for a minority sample). The data are then split
0.9.0,into training and testing.
0.9.0,%%
0.9.0,%% [markdown]
0.9.0,Classification using a single decision tree
0.9.0,-------------------------------------------
0.9.0,
0.9.0,We train a decision tree classifier which will be used as a baseline for the
0.9.0,rest of this example.
0.9.0,
0.9.0,The results are reported in terms of balanced accuracy and geometric mean
0.9.0,which are metrics widely used in the literature to validate model trained on
0.9.0,imbalanced set.
0.9.0,%%
0.9.0,%%
0.9.0,%%
0.9.0,%% [markdown]
0.9.0,Classification using bagging classifier with and without sampling
0.9.0,-----------------------------------------------------------------
0.9.0,
0.9.0,"Instead of using a single tree, we will check if an ensemble of decsion tree"
0.9.0,"can actually alleviate the issue induced by the class imbalancing. First, we"
0.9.0,will use a bagging classifier and its counter part which internally uses a
0.9.0,random under-sampling to balanced each boostrap sample.
0.9.0,%%
0.9.0,%% [markdown]
0.9.0,Balancing each bootstrap sample allows to increase significantly the balanced
0.9.0,accuracy and the geometric mean.
0.9.0,%%
0.9.0,%%
0.9.0,%% [markdown]
0.9.0,Classification using random forest classifier with and without sampling
0.9.0,-----------------------------------------------------------------------
0.9.0,
0.9.0,Random forest is another popular ensemble method and it is usually
0.9.0,"outperforming bagging. Here, we used a vanilla random forest and its balanced"
0.9.0,counterpart in which each bootstrap sample is balanced.
0.9.0,%%
0.9.0,%% [markdown]
0.9.0,"Similarly to the previous experiment, the balanced classifier outperform the"
0.9.0,"classifier which learn from imbalanced bootstrap samples. In addition, random"
0.9.0,forest outsperforms the bagging classifier.
0.9.0,%%
0.9.0,%%
0.9.0,%% [markdown]
0.9.0,Boosting classifier
0.9.0,-------------------
0.9.0,
0.9.0,"In the same manner, easy ensemble classifier is a bag of balanced AdaBoost"
0.9.0,"classifier. However, it will be slower to train than random forest and will"
0.9.0,achieve worse performance.
0.9.0,%%
0.9.0,%%
0.9.0,%%
0.9.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.9.0,License: MIT
0.9.0,%%
0.9.0,%% [markdown]
0.9.0,Generate an imbalanced dataset
0.9.0,------------------------------
0.9.0,
0.9.0,"For this example, we will create a synthetic dataset using the function"
0.9.0,:func:`~sklearn.datasets.make_classification`. The problem will be a toy
0.9.0,classification problem with a ratio of 1:9 between the two classes.
0.9.0,%%
0.9.0,%%
0.9.0,%% [markdown]
0.9.0,"In the following sections, we will show a couple of algorithms that have"
0.9.0,been proposed over the years. We intend to illustrate how one can reuse the
0.9.0,:class:`~imblearn.ensemble.BalancedBaggingClassifier` by passing different
0.9.0,sampler.
0.9.0,%%
0.9.0,%% [markdown]
0.9.0,Exactly Balanced Bagging and Over-Bagging
0.9.0,-----------------------------------------
0.9.0,
0.9.0,The :class:`~imblearn.ensemble.BalancedBaggingClassifier` can use in
0.9.0,conjunction with a :class:`~imblearn.under_sampling.RandomUnderSampler` or
0.9.0,:class:`~imblearn.over_sampling.RandomOverSampler`. These methods are
0.9.0,"referred as Exactly Balanced Bagging and Over-Bagging, respectively and have"
0.9.0,been proposed first in [1]_.
0.9.0,%%
0.9.0,Exactly Balanced Bagging
0.9.0,%%
0.9.0,Over-bagging
0.9.0,%% [markdown]
0.9.0,SMOTE-Bagging
0.9.0,-------------
0.9.0,
0.9.0,Instead of using a :class:`~imblearn.over_sampling.RandomOverSampler` that
0.9.0,"make a bootstrap, an alternative is to use"
0.9.0,:class:`~imblearn.over_sampling.SMOTE` as an over-sampler. This is known as
0.9.0,SMOTE-Bagging [2]_.
0.9.0,%%
0.9.0,SMOTE-Bagging
0.9.0,%% [markdown]
0.9.0,Roughly Balanced Bagging
0.9.0,------------------------
0.9.0,While using a :class:`~imblearn.under_sampling.RandomUnderSampler` or
0.9.0,:class:`~imblearn.over_sampling.RandomOverSampler` will create exactly the
0.9.0,"desired number of samples, it does not follow the statistical spirit wanted"
0.9.0,in the bagging framework. The authors in [3]_ proposes to use a negative
0.9.0,binomial distribution to compute the number of samples of the majority
0.9.0,class to be selected and then perform a random under-sampling.
0.9.0,
0.9.0,"Here, we illustrate this method by implementing a function in charge of"
0.9.0,resampling and use the :class:`~imblearn.FunctionSampler` to integrate it
0.9.0,within a :class:`~imblearn.pipeline.Pipeline` and
0.9.0,:class:`~sklearn.model_selection.cross_validate`.
0.9.0,%%
0.9.0,find the minority and majority classes
0.9.0,compute the number of sample to draw from the majority class using
0.9.0,a negative binomial distribution
0.9.0,draw randomly with or without replacement
0.9.0,Roughly Balanced Bagging
0.9.0,%% [markdown]
0.9.0,.. topic:: References:
0.9.0,
0.9.0,".. [1] R. Maclin, and D. Opitz. ""An empirical evaluation of bagging and"
0.9.0,"boosting."" AAAI/IAAI 1997 (1997): 546-551."
0.9.0,
0.9.0,".. [2] S. Wang, and X. Yao. ""Diversity analysis on imbalanced data sets by"
0.9.0,"using ensemble models."" 2009 IEEE symposium on computational"
0.9.0,"intelligence and data mining. IEEE, 2009."
0.9.0,
0.9.0,".. [3] S. Hido, H. Kashima, and Y. Takahashi. ""Roughly balanced bagging"
0.9.0,"for imbalanced data."" Statistical Analysis and Data Mining: The ASA"
0.9.0,Data Science Journal 2.5‚Äê6 (2009): 412-426.
0.9.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.9.0,License: MIT
0.9.0,%%
0.9.0,%% [markdown]
0.9.0,The following function will be used to create toy dataset. It uses the
0.9.0,:func:`~sklearn.datasets.make_classification` from scikit-learn but fixing
0.9.0,some parameters.
0.9.0,%%
0.9.0,%% [markdown]
0.9.0,The following function will be used to plot the sample space after resampling
0.9.0,to illustrate the specificities of an algorithm.
0.9.0,%%
0.9.0,%% [markdown]
0.9.0,The following function will be used to plot the decision function of a
0.9.0,classifier given some data.
0.9.0,%%
0.9.0,%%
0.9.0,%% [markdown]
0.9.0,Prototype generation: under-sampling by generating new samples
0.9.0,--------------------------------------------------------------
0.9.0,
0.9.0,:class:`~imblearn.under_sampling.ClusterCentroids` under-samples by replacing
0.9.0,the original samples by the centroids of the cluster found.
0.9.0,%%
0.9.0,%% [markdown]
0.9.0,Prototype selection: under-sampling by selecting existing samples
0.9.0,-----------------------------------------------------------------
0.9.0,
0.9.0,The algorithm performing prototype selection can be subdivided into two
0.9.0,groups: (i) the controlled under-sampling methods and (ii) the cleaning
0.9.0,under-sampling methods.
0.9.0,
0.9.0,"With the controlled under-sampling methods, the number of samples to be"
0.9.0,selected can be specified.
0.9.0,:class:`~imblearn.under_sampling.RandomUnderSampler` is the most naive way of
0.9.0,performing such selection by randomly selecting a given number of samples by
0.9.0,the targetted class.
0.9.0,%%
0.9.0,%% [markdown]
0.9.0,:class:`~imblearn.under_sampling.NearMiss` algorithms implement some
0.9.0,heuristic rules in order to select samples. NearMiss-1 selects samples from
0.9.0,the majority class for which the average distance of the :math:`k`` nearest
0.9.0,samples of the minority class is the smallest. NearMiss-2 selects the samples
0.9.0,from the majority class for which the average distance to the farthest
0.9.0,samples of the negative class is the smallest. NearMiss-3 is a 2-step
0.9.0,"algorithm: first, for each minority sample, their :math:`m`"
0.9.0,"nearest-neighbors will be kept; then, the majority samples selected are the"
0.9.0,on for which the average distance to the :math:`k` nearest neighbors is the
0.9.0,largest.
0.9.0,%%
0.9.0,%% [markdown]
0.9.0,:class:`~imblearn.under_sampling.EditedNearestNeighbours` removes samples of
0.9.0,the majority class for which their class differ from the one of their
0.9.0,nearest-neighbors. This sieve can be repeated which is the principle of the
0.9.0,:class:`~imblearn.under_sampling.RepeatedEditedNearestNeighbours`.
0.9.0,:class:`~imblearn.under_sampling.AllKNN` is slightly different from the
0.9.0,:class:`~imblearn.under_sampling.RepeatedEditedNearestNeighbours` by changing
0.9.0,"the :math:`k` parameter of the internal nearest neighors algorithm,"
0.9.0,increasing it at each iteration.
0.9.0,%%
0.9.0,%% [markdown]
0.9.0,:class:`~imblearn.under_sampling.CondensedNearestNeighbour` makes use of a
0.9.0,1-NN to iteratively decide if a sample should be kept in a dataset or not.
0.9.0,The issue is that :class:`~imblearn.under_sampling.CondensedNearestNeighbour`
0.9.0,is sensitive to noise by preserving the noisy samples.
0.9.0,:class:`~imblearn.under_sampling.OneSidedSelection` also used the 1-NN and
0.9.0,use :class:`~imblearn.under_sampling.TomekLinks` to remove the samples
0.9.0,considered noisy. The
0.9.0,:class:`~imblearn.under_sampling.NeighbourhoodCleaningRule` use a
0.9.0,:class:`~imblearn.under_sampling.EditedNearestNeighbours` to remove some
0.9.0,"sample. Additionally, they use a 3 nearest-neighbors to remove samples which"
0.9.0,do not agree with this rule.
0.9.0,%%
0.9.0,%% [markdown]
0.9.0,:class:`~imblearn.under_sampling.InstanceHardnessThreshold` uses the
0.9.0,prediction of classifier to exclude samples. All samples which are classified
0.9.0,with a low probability will be removed.
0.9.0,%%
0.9.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.9.0,License: MIT
0.9.0,%%
0.9.0,%% [markdown]
0.9.0,This function allows to make nice plotting
0.9.0,%%
0.9.0,%% [markdown]
0.9.0,We will generate some toy data that illustrates how
0.9.0,:class:`~imblearn.under_sampling.TomekLinks` is used to clean a dataset.
0.9.0,%%
0.9.0,%% [markdown]
0.9.0,"In the figure above, the samples highlighted in green form a Tomek link since"
0.9.0,they are of different classes and are nearest neighbors of each other.
0.9.0,highlight the samples of interest
0.9.0,%% [markdown]
0.9.0,We can run the :class:`~imblearn.under_sampling.TomekLinks` sampling to
0.9.0,remove the corresponding samples. If `sampling_strategy='auto'` only the
0.9.0,sample from the majority class will be removed. If `sampling_strategy='all'`
0.9.0,both samples will be removed.
0.9.0,%%
0.9.0,highlight the samples of interest
0.9.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.9.0,License: MIT
0.9.0,%%
0.9.0,%% [markdown]
0.9.0,We define a function allowing to make some nice decoration on the plot.
0.9.0,%%
0.9.0,%% [markdown]
0.9.0,We can start by generating some data to later illustrate the principle of
0.9.0,each :class:`~imblearn.under_sampling.NearMiss` heuristic rules.
0.9.0,%%
0.9.0,%% [mardown]
0.9.0,NearMiss-1
0.9.0,----------
0.9.0,
0.9.0,NearMiss-1 selects samples from the majority class for which the average
0.9.0,distance to some nearest neighbours is the smallest. In the following
0.9.0,"example, we use a 3-NN to compute the average distance on 2 specific samples"
0.9.0,"of the majority class. Therefore, in this case the point linked by the"
0.9.0,green-dashed line will be selected since the average distance is smaller.
0.9.0,%%
0.9.0,%% [mardown]
0.9.0,NearMiss-2
0.9.0,----------
0.9.0,
0.9.0,NearMiss-2 selects samples from the majority class for which the average
0.9.0,distance to the farthest neighbors is the smallest. With the same
0.9.0,"configuration as previously presented, the sample linked to the green-dashed"
0.9.0,line will be selected since its distance the 3 farthest neighbors is the
0.9.0,smallest.
0.9.0,%%
0.9.0,%% [mardown]
0.9.0,NearMiss-3
0.9.0,----------
0.9.0,
0.9.0,"NearMiss-3 can be divided into 2 steps. First, a nearest-neighbors is used to"
0.9.0,short-list samples from the majority class (i.e. correspond to the
0.9.0,"highlighted samples in the following plot). Then, the sample with the largest"
0.9.0,average distance to the *k* nearest-neighbors are selected.
0.9.0,%%
0.9.0,select only the majority point of interest
0.9.0,Authors: Christos Aridas
0.9.0,Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.9.0,License: MIT
0.9.0,%%
0.9.0,%% [markdown]
0.9.0,Let's first create an imbalanced dataset and split in to two sets.
0.9.0,%%
0.9.0,%% [markdown]
0.9.0,"Now, we will create each individual steps that we would like later to combine"
0.9.0,%%
0.9.0,%% [markdown]
0.9.0,"Now, we can finally create a pipeline to specify in which order the different"
0.9.0,transformers and samplers should be executed before to provide the data to
0.9.0,the final classifier.
0.9.0,%%
0.9.0,%% [markdown]
0.9.0,We can now use the pipeline created as a normal classifier where resampling
0.9.0,"will happen when calling `fit` and disabled when calling `decision_function`,"
0.9.0,"`predict_proba`, or `predict`."
0.9.0,%%
0.9.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.9.0,License: MIT
0.9.0,##############################################################################
0.9.0,Data loading
0.9.0,##############################################################################
0.9.0,##############################################################################
0.9.0,"First, you should download the Porto Seguro data set from Kaggle. See the"
0.9.0,link in the introduction.
0.9.0,##############################################################################
0.9.0,The data set is imbalanced and it will have an effect on the fitting.
0.9.0,##############################################################################
0.9.0,Define the pre-processing pipeline
0.9.0,##############################################################################
0.9.0,##############################################################################
0.9.0,We want to standard scale the numerical features while we want to one-hot
0.9.0,"encode the categorical features. In this regard, we make use of the"
0.9.0,:class:`~sklearn.compose.ColumnTransformer`.
0.9.0,Create an environment variable to avoid using the GPU. This can be changed.
0.9.0,##############################################################################
0.9.0,Create a neural-network
0.9.0,##############################################################################
0.9.0,##############################################################################
0.9.0,We create a decorator to report the computation time
0.9.0,##############################################################################
0.9.0,The first model will be trained using the ``fit`` method and with imbalanced
0.9.0,mini-batches.
0.9.0,##############################################################################
0.9.0,"In the contrary, we will use imbalanced-learn to create a generator of"
0.9.0,mini-batches which will yield balanced mini-batches.
0.9.0,##############################################################################
0.9.0,Classification loop
0.9.0,##############################################################################
0.9.0,##############################################################################
0.9.0,We will perform a 10-fold cross-validation and train the neural-network with
0.9.0,the two different strategies previously presented.
0.9.0,##############################################################################
0.9.0,Plot of the results and computation time
0.9.0,##############################################################################
0.9.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.9.0,License: MIT
0.9.0,%%
0.9.0,%% [markdown]
0.9.0,Problem definition
0.9.0,------------------
0.9.0,
0.9.0,We are dropping the following features:
0.9.0,
0.9.0,"- ""fnlwgt"": this feature was created while studying the ""adult"" dataset."
0.9.0,"Thus, we will not use this feature which is not acquired during the survey."
0.9.0,"- ""education-num"": it is encoding the same information than ""education""."
0.9.0,"Thus, we are removing one of these 2 features."
0.9.0,%%
0.9.0,%% [markdown]
0.9.0,"The ""adult"" dataset as a class ratio of about 3:1"
0.9.0,%%
0.9.0,%% [markdown]
0.9.0,This dataset is only slightly imbalanced. To better highlight the effect of
0.9.0,"learning from an imbalanced dataset, we will increase its ratio to 30:1"
0.9.0,%%
0.9.0,%% [markdown]
0.9.0,We will perform a cross-validation evaluation to get an estimate of the test
0.9.0,score.
0.9.0,
0.9.0,"As a baseline, we could use a classifier which will always predict the"
0.9.0,majority class independently of the features provided.
0.9.0,%%
0.9.0,%% [markdown]
0.9.0,"Instead of using the accuracy, we can use the balanced accuracy which will"
0.9.0,take into account the balancing issue.
0.9.0,%%
0.9.0,%% [markdown]
0.9.0,Strategies to learn from an imbalanced dataset
0.9.0,----------------------------------------------
0.9.0,We will use a dictionary and a list to continuously store the results of
0.9.0,our experiments and show them as a pandas dataframe.
0.9.0,%%
0.9.0,%% [markdown]
0.9.0,Dummy baseline
0.9.0,..............
0.9.0,
0.9.0,"Before to train a real machine learning model, we can store the results"
0.9.0,obtained with our :class:`~sklearn.dummy.DummyClassifier`.
0.9.0,%%
0.9.0,%% [markdown]
0.9.0,Linear classifier baseline
0.9.0,..........................
0.9.0,
0.9.0,We will create a machine learning pipeline using a
0.9.0,":class:`~sklearn.linear_model.LogisticRegression` classifier. In this regard,"
0.9.0,we will need to one-hot encode the categorical columns and standardized the
0.9.0,numerical columns before to inject the data into the
0.9.0,:class:`~sklearn.linear_model.LogisticRegression` classifier.
0.9.0,
0.9.0,"First, we define our numerical and categorical pipelines."
0.9.0,%%
0.9.0,%% [markdown]
0.9.0,"Then, we can create a preprocessor which will dispatch the categorical"
0.9.0,columns to the categorical pipeline and the numerical columns to the
0.9.0,numerical pipeline
0.9.0,%%
0.9.0,%% [markdown]
0.9.0,"Finally, we connect our preprocessor with our"
0.9.0,:class:`~sklearn.linear_model.LogisticRegression`. We can then evaluate our
0.9.0,model.
0.9.0,%%
0.9.0,%%
0.9.0,%% [markdown]
0.9.0,We can see that our linear model is learning slightly better than our dummy
0.9.0,"baseline. However, it is impacted by the class imbalance."
0.9.0,
0.9.0,We can verify that something similar is happening with a tree-based model
0.9.0,such as :class:`~sklearn.ensemble.RandomForestClassifier`. With this type of
0.9.0,"classifier, we will not need to scale the numerical data, and we will only"
0.9.0,need to ordinal encode the categorical data.
0.9.0,%%
0.9.0,%%
0.9.0,%% [markdown]
0.9.0,The :class:`~sklearn.ensemble.RandomForestClassifier` is as well affected by
0.9.0,"the class imbalanced, slightly less than the linear model. Now, we will"
0.9.0,present different approach to improve the performance of these 2 models.
0.9.0,
0.9.0,Use `class_weight`
0.9.0,..................
0.9.0,
0.9.0,Most of the models in `scikit-learn` have a parameter `class_weight`. This
0.9.0,parameter will affect the computation of the loss in linear model or the
0.9.0,criterion in the tree-based model to penalize differently a false
0.9.0,classification from the minority and majority class. We can set
0.9.0,"`class_weight=""balanced""` such that the weight applied is inversely"
0.9.0,proportional to the class frequency. We test this parametrization in both
0.9.0,linear model and tree-based model.
0.9.0,%%
0.9.0,%%
0.9.0,%% [markdown]
0.9.0,We can see that using `class_weight` was really effective for the linear
0.9.0,"model, alleviating the issue of learning from imbalanced classes. However,"
0.9.0,the :class:`~sklearn.ensemble.RandomForestClassifier` is still biased toward
0.9.0,"the majority class, mainly due to the criterion which is not suited enough to"
0.9.0,fight the class imbalance.
0.9.0,
0.9.0,Resample the training set during learning
0.9.0,.........................................
0.9.0,
0.9.0,Another way is to resample the training set by under-sampling or
0.9.0,over-sampling some of the samples. `imbalanced-learn` provides some samplers
0.9.0,to do such processing.
0.9.0,%%
0.9.0,%%
0.9.0,%%
0.9.0,%%
0.9.0,%% [markdown]
0.9.0,Applying a random under-sampler before the training of the linear model or
0.9.0,"random forest, allows to not focus on the majority class at the cost of"
0.9.0,making more mistake for samples in the majority class (i.e. decreased
0.9.0,accuracy).
0.9.0,
0.9.0,We could apply any type of samplers and find which sampler is working best
0.9.0,on the current dataset.
0.9.0,
0.9.0,"Instead, we will present another way by using classifiers which will apply"
0.9.0,sampling internally.
0.9.0,
0.9.0,Use of specific balanced algorithms from imbalanced-learn
0.9.0,.........................................................
0.9.0,
0.9.0,We already showed that random under-sampling can be effective on decision
0.9.0,"tree. However, instead of under-sampling once the dataset, one could"
0.9.0,under-sample the original dataset before to take a bootstrap sample. This is
0.9.0,the base of the :class:`imblearn.ensemble.BalancedRandomForestClassifier` and
0.9.0,:class:`~imblearn.ensemble.BalancedBaggingClassifier`.
0.9.0,%%
0.9.0,%%
0.9.0,%% [markdown]
0.9.0,The performance with the
0.9.0,:class:`~imblearn.ensemble.BalancedRandomForestClassifier` is better than
0.9.0,applying a single random under-sampling. We will use a gradient-boosting
0.9.0,classifier within a :class:`~imblearn.ensemble.BalancedBaggingClassifier`.
0.9.0,%% [markdown]
0.9.0,This last approach is the most effective. The different under-sampling allows
0.9.0,to bring some diversity for the different GBDT to learn and not focus on a
0.9.0,portion of the majority class.
0.9.0,Authors: Christos Aridas
0.9.0,Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.9.0,License: MIT
0.9.0,%%
0.9.0,%% [markdown]
0.9.0,Load the dataset
0.9.0,----------------
0.9.0,
0.9.0,We will use a dataset containing image from know person where we will
0.9.0,build a model to recognize the person on the image. We will make this problem
0.9.0,a binary problem by taking picture of only George W. Bush and Bill Clinton.
0.9.0,%%
0.9.0,%%
0.9.0,%% [markdown]
0.9.0,We can check the ratio between the two classes.
0.9.0,%%
0.9.0,%% [markdown]
0.9.0,We see that we have an imbalanced classification problem with ~95% of the
0.9.0,data belonging to the class G.W. Bush.
0.9.0,
0.9.0,Compare over-sampling approaches
0.9.0,--------------------------------
0.9.0,
0.9.0,We will use different over-sampling approaches and use a kNN classifier
0.9.0,to check if we can recognize the 2 presidents. The evaluation will be
0.9.0,performed through cross-validation and we will plot the mean ROC curve.
0.9.0,
0.9.0,We will create different pipelines and evaluate them.
0.9.0,%%
0.9.0,%%
0.9.0,%% [markdown]
0.9.0,We will compute the mean ROC curve for each pipeline using a different splits
0.9.0,provided by the :class:`~sklearn.model_selection.StratifiedKFold`
0.9.0,cross-validation.
0.9.0,%%
0.9.0,compute the mean fpr/tpr to get the mean ROC curve
0.9.0,Create a display that we will reuse to make the aggregated plots for
0.9.0,all methods
0.9.0,%% [markdown]
0.9.0,"In the previous cell, we created the different mean ROC curve and we can plot"
0.9.0,them on the same plot.
0.9.0,%%
0.9.0,%% [markdown]
0.9.0,"We see that for this task, methods that are generating new samples with some"
0.9.0,interpolation (i.e. ADASYN and SMOTE) perform better than random
0.9.0,over-sampling or no resampling.
0.9.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.9.0,License: MIT
0.9.0,Create a folder to fetch the dataset
0.9.0,Create a pipeline
0.9.0,Classify and report the results
0.9.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.9.0,License: MIT
0.9.0,%%
0.9.0,%% [markdown]
0.9.0,Setting the data set
0.9.0,--------------------
0.9.0,
0.9.0,We use a part of the 20 newsgroups data set by loading 4 topics. Using the
0.9.0,"scikit-learn loader, the data are split into a training and a testing set."
0.9.0,
0.9.0,Note the class \#3 is the minority class and has almost twice less samples
0.9.0,than the majority class.
0.9.0,%%
0.9.0,%%
0.9.0,%% [markdown]
0.9.0,The usual scikit-learn pipeline
0.9.0,-------------------------------
0.9.0,
0.9.0,You might usually use scikit-learn pipeline by combining the TF-IDF
0.9.0,vectorizer to feed a multinomial naive bayes classifier. A classification
0.9.0,report summarized the results on the testing set.
0.9.0,
0.9.0,"As expected, the recall of the class \#3 is low mainly due to the class"
0.9.0,imbalanced.
0.9.0,%%
0.9.0,%%
0.9.0,%% [markdown]
0.9.0,Balancing the class before classification
0.9.0,-----------------------------------------
0.9.0,
0.9.0,"To improve the prediction of the class \#3, it could be interesting to apply"
0.9.0,"a balancing before to train the naive bayes classifier. Therefore, we will"
0.9.0,use a :class:`~imblearn.under_sampling.RandomUnderSampler` to equalize the
0.9.0,number of samples in all the classes before the training.
0.9.0,
0.9.0,It is also important to note that we are using the
0.9.0,:class:`~imblearn.pipeline.make_pipeline` function implemented in
0.9.0,imbalanced-learn to properly handle the samplers.
0.9.0,%%
0.9.0,%% [markdown]
0.9.0,"Although the results are almost identical, it can be seen that the resampling"
0.9.0,allowed to correct the poor recall of the class \#3 at the cost of reducing
0.9.0,"the other metrics for the other classes. However, the overall results are"
0.9.0,slightly better.
0.9.0,%%
0.9.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.9.0,License: MIT
0.9.0,#############################################################################
0.9.0,Toy data generation
0.9.0,#############################################################################
0.9.0,#############################################################################
0.9.0,We are generating some non Gaussian data set contaminated with some unform
0.9.0,noise.
0.9.0,#############################################################################
0.9.0,We will generate some cleaned test data without outliers.
0.9.0,#############################################################################
0.9.0,How to use the :class:`~imblearn.FunctionSampler`
0.9.0,#############################################################################
0.9.0,#############################################################################
0.9.0,We first define a function which will use
0.9.0,:class:`~sklearn.ensemble.IsolationForest` to eliminate some outliers from
0.9.0,our dataset during training. The function passed to the
0.9.0,:class:`~imblearn.FunctionSampler` will be called when using the method
0.9.0,``fit_resample``.
0.9.0,#############################################################################
0.9.0,Integrate it within a pipeline
0.9.0,#############################################################################
0.9.0,#############################################################################
0.9.0,"By elimnating outliers before the training, the classifier will be less"
0.9.0,affected during the prediction.
0.9.0,Authors: Dayvid Oliveira
0.9.0,Christos Aridas
0.9.0,Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.9.0,License: MIT
0.9.0,%%
0.9.0,%% [markdown]
0.9.0,Generate the dataset
0.9.0,--------------------
0.9.0,
0.9.0,"First, we will generate a dataset and convert it to a"
0.9.0,:class:`~pandas.DataFrame` with arbitrary column names. We will plot the
0.9.0,original dataset.
0.9.0,%%
0.9.0,%% [markdown]
0.9.0,Make a dataset imbalanced
0.9.0,-------------------------
0.9.0,
0.9.0,"Now, we will show the helpers :func:`~imblearn.datasets.make_imbalance`"
0.9.0,that is useful to random select a subset of samples. It will impact the
0.9.0,class distribution as specified by the parameters.
0.9.0,%%
0.9.0,%%
0.9.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.9.0,License: MIT
0.9.0,%%
0.9.0,%% [markdown]
0.9.0,Create an imbalanced dataset
0.9.0,----------------------------
0.9.0,
0.9.0,"First, we will create an imbalanced data set from a the iris data set."
0.9.0,%%
0.9.0,%%
0.9.0,%% [markdown]
0.9.0,Using ``sampling_strategy`` in resampling algorithms
0.9.0,====================================================
0.9.0,
0.9.0,`sampling_strategy` as a `float`
0.9.0,--------------------------------
0.9.0,
0.9.0,`sampling_strategy` can be given a `float`. For **under-sampling
0.9.0,"methods**, it corresponds to the ratio :math:`\\alpha_{us}` defined by"
0.9.0,:math:`N_{rM} = \\alpha_{us} \\times N_{m}` where :math:`N_{rM}` and
0.9.0,:math:`N_{m}` are the number of samples in the majority class after
0.9.0,"resampling and the number of samples in the minority class, respectively."
0.9.0,%%
0.9.0,select only 2 classes since the ratio make sense in this case
0.9.0,%%
0.9.0,%% [markdown]
0.9.0,"For **over-sampling methods**, it correspond to the ratio"
0.9.0,:math:`\\alpha_{os}` defined by :math:`N_{rm} = \\alpha_{os} \\times N_{M}`
0.9.0,where :math:`N_{rm}` and :math:`N_{M}` are the number of samples in the
0.9.0,minority class after resampling and the number of samples in the majority
0.9.0,"class, respectively."
0.9.0,%%
0.9.0,%% [markdown]
0.9.0,`sampling_strategy` has a `str`
0.9.0,-------------------------------
0.9.0,
0.9.0,`sampling_strategy` can be given as a string which specify the class
0.9.0,"targeted by the resampling. With under- and over-sampling, the number of"
0.9.0,samples will be equalized.
0.9.0,
0.9.0,Note that we are using multiple classes from now on.
0.9.0,%%
0.9.0,%% [markdown]
0.9.0,"With **cleaning method**, the number of samples in each class will not be"
0.9.0,equalized even if targeted.
0.9.0,%%
0.9.0,%% [markdown]
0.9.0,`sampling_strategy as a `dict`
0.9.0,------------------------------
0.9.0,
0.9.0,"When `sampling_strategy` is a `dict`, the keys correspond to the targeted"
0.9.0,classes. The values correspond to the desired number of samples for each
0.9.0,targeted class. This is working for both **under- and over-sampling**
0.9.0,algorithms but not for the **cleaning algorithms**. Use a `list` instead.
0.9.0,%%
0.9.0,%% [markdown]
0.9.0,`sampling_strategy` as a `list`
0.9.0,-------------------------------
0.9.0,
0.9.0,"When `sampling_strategy` is a `list`, the list contains the targeted"
0.9.0,classes. It is used only for **cleaning methods** and raise an error
0.9.0,otherwise.
0.9.0,%%
0.9.0,%% [markdown]
0.9.0,`sampling_strategy` as a callable
0.9.0,---------------------------------
0.9.0,
0.9.0,"When callable, function taking `y` and returns a `dict`. The keys"
0.9.0,correspond to the targeted classes. The values correspond to the desired
0.9.0,number of samples for each class.
0.9.0,%%
0.9.0,List of whitelisted modules and methods; regexp are supported.
0.9.0,These docstrings will fail because they are inheriting from scikit-learn
0.9.0,skip private classes
0.9.0,"We ignore following error code,"
0.9.0,- RT02: The first line of the Returns section
0.9.0,"should contain only the type, .."
0.9.0,(as we may need refer to the name of the returned
0.9.0,object)
0.9.0,- GL01: Docstring text (summary) should start in the line
0.9.0,"immediately after the opening quotes (not in the same line,"
0.9.0,or leaving a blank line in between)
0.9.0,"- GL02: If there's a blank line, it should be before the"
0.9.0,"first line of the Returns section, not after (it allows to have"
0.9.0,short docstrings for properties).
0.9.0,Ignore PR02: Unknown parameters for properties. We sometimes use
0.9.0,"properties for ducktyping, i.e. SGDClassifier.predict_proba"
0.9.0,Following codes are only taken into account for the
0.9.0,top level class docstrings:
0.9.0,- ES01: No extended summary found
0.9.0,- SA01: See Also section not found
0.9.0,- EX01: No examples section found
0.9.0,In particular we can't parse the signature of properties
0.9.0,"When applied to classes, detect class method. For functions"
0.9.0,method = None.
0.9.0,TODO: this detection can be improved. Currently we assume that we have
0.9.0,class # methods if the second path element before last is in camel case.
0.9.0,numpy scipy and cython should by in sync with pyproject.toml
0.9.0,We pinned PyWavelet (a scikit-image dependence) to 1.1.1 in the minimum
0.9.0,documentation CI builds that is the latest version that support our
0.9.0,"minimum NumPy version required. If PyWavelets 1.2+ is installed, it would"
0.9.0,require NumPy 1.17+ that trigger a bug with Pandas 0.25:
0.9.0,https://github.com/numpy/numpy/issues/18355#issuecomment-774610226
0.9.0,"When upgrading NumPy, we can unpin PyWavelets but we need to update the"
0.9.0,minimum version of Pandas >= 1.0.5.
0.9.0,'build' and 'install' is included to have structured metadata for CI.
0.9.0,It will NOT be included in setup's extras_require
0.9.0,"The values are (version_spec, comma separated tags)"
0.9.0,create inverse mapping for setuptools
0.9.0,Used by CI to get the min dependencies
0.9.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.9.0,License: MIT
0.9.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.9.0,Christos Aridas
0.9.0,License: MIT
0.9.0,we need to overwrite SamplerMixin.fit to bypass the validation
0.9.0,Adapted from scikit-learn
0.9.0,Author: Edouard Duchesnay
0.9.0,Gael Varoquaux
0.9.0,Virgile Fritsch
0.9.0,Alexandre Gramfort
0.9.0,Lars Buitinck
0.9.0,Christos Aridas
0.9.0,Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.9.0,License: BSD
0.9.0,BaseEstimator interface
0.9.0,validate names
0.9.0,validate estimators
0.9.0,We allow last estimator to be None as an identity transformation
0.9.0,Estimator interface
0.9.0,Setup the memory
0.9.0,joblib >= 0.12
0.9.0,Fit or load from cache the current transformer
0.9.0,Replace the transformer of the step with the fitted
0.9.0,transformer. This is necessary when loading the transformer
0.9.0,from the cache.
0.9.0,This variable is injected in the __builtins__ by the build
0.9.0,process. It is used to enable importing subpackages of sklearn when
0.9.0,the binaries are not built
0.9.0,mypy error: Cannot determine type of '__SKLEARN_SETUP__'
0.9.0,We are not importing the rest of scikit-learn during the build
0.9.0,"process, as it may not be compiled yet"
0.9.0,"FIXME: When we get Python 3.7 as minimal version, we will need to switch to"
0.9.0,the following solution:
0.9.0,https://snarky.ca/lazy-importing-in-python-3-7/
0.9.0,Import the target module and insert it into the parent's namespace
0.9.0,Update this object's dict so that if someone keeps a reference to the
0.9.0,"LazyLoader, lookups are efficient (__getattr__ is only called on"
0.9.0,lookups that fail).
0.9.0,delay the import of keras since we are going to import either tensorflow
0.9.0,or keras
0.9.0,Based on NiLearn package
0.9.0,License: simplified BSD
0.9.0,"PEP0440 compatible formatted version, see:"
0.9.0,https://www.python.org/dev/peps/pep-0440/
0.9.0,
0.9.0,Generic release markers:
0.9.0,X.Y
0.9.0,X.Y.Z # For bugfix releases
0.9.0,
0.9.0,Admissible pre-release markers:
0.9.0,X.YaN # Alpha release
0.9.0,X.YbN # Beta release
0.9.0,X.YrcN # Release Candidate
0.9.0,X.Y # Final release
0.9.0,
0.9.0,Dev branch marker is: 'X.Y.dev' or 'X.Y.devN' where N is an integer.
0.9.0,'X.Y.dev0' is the canonical version of 'X.Y.dev'
0.9.0,
0.9.0,coding: utf-8
0.9.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.9.0,Dariusz Brzezinski
0.9.0,License: MIT
0.9.0,Only negative labels
0.9.0,"Calculate tp_sum, pred_sum, true_sum ###"
0.9.0,labels are now from 0 to len(labels) - 1 -> use bincount
0.9.0,Pathological case
0.9.0,Compute the true negative
0.9.0,Retain only selected labels
0.9.0,"Finally, we have all our sufficient statistics. Divide! #"
0.9.0,"Divide, and on zero-division, set scores to 0 and warn:"
0.9.0,"Oddly, we may get an ""invalid"" rather than a ""divide"" error"
0.9.0,here.
0.9.0,Average the results
0.9.0,labels are now from 0 to len(labels) - 1 -> use bincount
0.9.0,Pathological case
0.9.0,Retain only selected labels
0.9.0,old version of scipy return MaskedConstant instead of 0.0
0.9.0,check that the scoring function does not need a score
0.9.0,and only a prediction
0.9.0,We do not support multilabel so the only average supported
0.9.0,is binary
0.9.0,Compute the different metrics
0.9.0,Precision/recall/f1
0.9.0,Specificity
0.9.0,Geometric mean
0.9.0,Index balanced accuracy
0.9.0,compute averages
0.9.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.9.0,License: MIT
0.9.0,categories are expected to be encoded from 0 to n_categories - 1
0.9.0,"list of length n_features of ndarray (n_categories, n_classes)"
0.9.0,compute the counts
0.9.0,normalize by the summing over the classes
0.9.0,silence potential warning due to in-place division by zero
0.9.0,coding: utf-8
0.9.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.9.0,Christos Aridas
0.9.0,License: MIT
0.9.0,##############################################################################
0.9.0,Utilities for testing
0.9.0,import some data to play with
0.9.0,restrict to a binary classification task
0.9.0,add noisy features to make the problem harder and avoid perfect results
0.9.0,"run classifier, get class probabilities and label predictions"
0.9.0,only interested in probabilities of the positive case
0.9.0,XXX: do we really want a special API for the binary case?
0.9.0,##############################################################################
0.9.0,Tests
0.9.0,detailed measures for each class
0.9.0,individual scoring function that can be used for grid search: in the
0.9.0,binary class case the score is the value of the measure for the positive
0.9.0,class (e.g. label == 1). This is deprecated for average != 'binary'.
0.9.0,Such a case may occur with non-stratified cross-validation
0.9.0,ensure the above were meaningful tests:
0.9.0,Bad pos_label
0.9.0,Bad average option
0.9.0,but average != 'binary'; even if data is binary
0.9.0,compute the geometric mean for the binary problem
0.9.0,print classification report with class names
0.9.0,print classification report with label detection
0.9.0,print classification report with class names
0.9.0,print classification report with label detection
0.9.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.9.0,License: MIT
0.9.0,Check basic feature of the metric:
0.9.0,"* the shape of the distance matrix is (n_samples, n_samples)"
0.9.0,* computing pairwise distance of X is the same than explicitely between
0.9.0,X and X.
0.9.0,Check the property of the vdm distance. Let's check the property
0.9.0,"described in ""Improved Heterogeneous Distance Functions"", D.R. Wilson and"
0.9.0,"T.R. Martinez, Journal of Artificial Intelligence Research 6 (1997) 1-34"
0.9.0,https://arxiv.org/pdf/cs/9701101.pdf
0.9.0,
0.9.0,"""if an attribute color has three values red, green and blue, and the"
0.9.0,"application is to identify whether or not an object is an apple, red and"
0.9.0,green would be considered closer than red and blue because the former two
0.9.0,"both have similar correlations with the output class apple."""
0.9.0,defined our feature
0.9.0,0 - not an apple / 1 - an apple
0.9.0,computing the distance between a sample of the same category should
0.9.0,give a null distance
0.9.0,check the property explained in the introduction example
0.9.0,green and red are very close
0.9.0,blue is closer to red than green
0.9.0,"Check that ""auto"" is equivalent to provide the number categories"
0.9.0,beforehand
0.9.0,Check that we raise an error if n_categories is inconsistent with the
0.9.0,number of features in X
0.9.0,Check that we don't get issue when a category is missing between 0
0.9.0,n_categories - 1
0.9.0,remove a categories that could be between 0 and n_categories
0.9.0,Check that we raise a NotFittedError when `fit` is not not called before
0.9.0,pairwise.
0.9.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.9.0,Christos Aridas
0.9.0,License: MIT
0.9.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.9.0,Christos Aridas
0.9.0,License: MIT
0.9.0,The ratio is computed using a one-vs-rest manner. Using majority
0.9.0,in multi-class would lead to slightly different results at the
0.9.0,cost of introducing a new parameter.
0.9.0,rounding may cause new amount for n_samples
0.9.0,the nearest neighbors need to be fitted only on the current class
0.9.0,to find the class NN to generate new samples
0.9.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.9.0,Christos Aridas
0.9.0,License: MIT
0.9.0,smoothed bootstrap imposes to make numerical operation; we need
0.9.0,to be sure to have only numerical data in X
0.9.0,generate a smoothed bootstrap with a perturbation
0.9.0,generate a bootstrap
0.9.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.9.0,Christos Aridas
0.9.0,License: MIT
0.9.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.9.0,Fernando Nogueira
0.9.0,Christos Aridas
0.9.0,License: MIT
0.9.0,validate the parameters
0.9.0,negate diagonal elements
0.9.0,identify cluster which are answering the requirements
0.9.0,the cluster is already considered balanced
0.9.0,not enough samples to apply SMOTE
0.9.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.9.0,Fernando Nogueira
0.9.0,Christos Aridas
0.9.0,Dzianis Dudnik
0.9.0,License: MIT
0.9.0,divergence between borderline-1 and borderline-2
0.9.0,Create synthetic samples for borderline points.
0.9.0,only minority
0.9.0,we use a one-vs-rest policy to handle the multiclass in which
0.9.0,new samples will be created considering not only the majority
0.9.0,class but all over classes.
0.9.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.9.0,Fernando Nogueira
0.9.0,Christos Aridas
0.9.0,Dzianis Dudnik
0.9.0,License: MIT
0.9.0,np.newaxis for backwards compatability with random_state
0.9.0,Samples are in danger for m/2 <= m' < m
0.9.0,Samples are noise for m = m'
0.9.0,compute the median of the standard deviation of the minority class
0.9.0,the input of the OneHotEncoder needs to be dense
0.9.0,we can replace the 1 entries of the categorical features with the
0.9.0,median of the standard deviation. It will ensure that whenever
0.9.0,"distance is computed between 2 samples, the difference will be equal"
0.9.0,to the median of the standard deviation as in the original paper.
0.9.0,"In the edge case where the median of the std is equal to 0, the 1s"
0.9.0,"entries will be also nullified. In this case, we store the original"
0.9.0,categorical encoding which will be later used for inversing the OHE
0.9.0,reverse the encoding of the categorical features
0.9.0,the matrix is supposed to be in the CSR format after the stacking
0.9.0,change in sparsity structure more efficient with LIL than CSR
0.9.0,convert to dense array since scipy.sparse doesn't handle 3D
0.9.0,"In the case that the median std was equal to zeros, we have to"
0.9.0,create non-null entry based on the encoded of OHE
0.9.0,tie breaking argmax
0.9.0,generate sample indices that will be used to generate new samples
0.9.0,"for each drawn samples, select its k-neighbors and generate a sample"
0.9.0,"where for each feature individually, each category generated is the"
0.9.0,most common category
0.9.0,the kneigbors search will include the sample itself which is
0.9.0,expected from the original algorithm
0.9.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.9.0,Christos Aridas
0.9.0,Dzianis Dudnik
0.9.0,License: MIT
0.9.0,create 2 random continuous feature
0.9.0,create a categorical feature using some string
0.9.0,create a categorical feature using some integer
0.9.0,return the categories
0.9.0,create 2 random continuous feature
0.9.0,create a categorical feature using some string
0.9.0,create a categorical feature using some integer
0.9.0,return the categories
0.9.0,create 2 random continuous feature
0.9.0,create a categorical feature using some string
0.9.0,create a categorical feature using some integer
0.9.0,return the categories
0.9.0,create 2 random continuous feature
0.9.0,create a categorical feature using some string
0.9.0,create a categorical feature using some integer
0.9.0,return the categories
0.9.0,create 2 random continuous feature
0.9.0,create a categorical feature using some string
0.9.0,create a categorical feature using some integer
0.9.0,part of the common test which apply to SMOTE-NC even if it is not default
0.9.0,constructible
0.9.0,Check that the samplers handle pandas dataframe and pandas series
0.9.0,FIXME: we should use to_numpy with pandas >= 0.25
0.9.0,Cast X and y to not default dtype
0.9.0,Non-regression test for #662
0.9.0,https://github.com/scikit-learn-contrib/imbalanced-learn/issues/662
0.9.0,check that the categorical feature is not random but correspond to the
0.9.0,categories seen in the minority class samples
0.9.0,overall check for SMOTEN
0.9.0,check if the SMOTEN resample data as expected
0.9.0,"we generate data such that ""not apple"" will be the minority class and"
0.9.0,"samples from this class will be generated. We will force the ""blue"""
0.9.0,"category to be associated with this class. Therefore, the new generated"
0.9.0,"samples should as well be from the ""blue"" category."
0.9.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.9.0,Christos Aridas
0.9.0,License: MIT
0.9.0,check that m_neighbors is properly set. Regression test for:
0.9.0,https://github.com/scikit-learn-contrib/imbalanced-learn/issues/568
0.9.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.9.0,Christos Aridas
0.9.0,License: MIT
0.9.0,FIXME: we should use to_numpy with pandas >= 0.25
0.9.0,check the random over-sampling with a multiclass problem
0.9.0,check that resampling with heterogeneous dtype is working with basic
0.9.0,resampling
0.9.0,check that we can oversample even with missing or infinite data
0.9.0,regression tests for #605
0.9.0,check that we raise an error when heterogeneous dtype data are given
0.9.0,and a smoothed bootstrap is requested
0.9.0,check that smoothed bootstrap is working for numerical array
0.9.0,check that a shrinkage factor of 0 is equivalent to not create a smoothed
0.9.0,bootstrap
0.9.0,check the behaviour of the shrinkage parameter
0.9.0,the covariance of the data generated with the larger shrinkage factor
0.9.0,should also be larger.
0.9.0,check the validation of the shrinkage parameter
0.9.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.9.0,Christos Aridas
0.9.0,License: MIT
0.9.0,shuffle the indices since the sampler are packing them by class
0.9.0,helper functions
0.9.0,input and output
0.9.0,build the model and weights
0.9.0,"build the loss, predict, and train operator"
0.9.0,Initialization of all variables in the graph
0.9.0,"For each epoch, run accuracy on train and test"
0.9.0,helper functions
0.9.0,input and output
0.9.0,build the model and weights
0.9.0,"build the loss, predict, and train operator"
0.9.0,Initialization of all variables in the graph
0.9.0,"For each epoch, run accuracy on train and test"
0.9.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.9.0,License: MIT
0.9.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.9.0,Fernando Nogueira
0.9.0,Christos Aridas
0.9.0,License: MIT
0.9.0,find which class to not consider
0.9.0,there is a Tomek link between two samples if they are both nearest
0.9.0,neighbors of each others.
0.9.0,Find the nearest neighbour of every point
0.9.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.9.0,Christos Aridas
0.9.0,License: MIT
0.9.0,Randomly get one sample from the majority class
0.9.0,Generate the index to select
0.9.0,Create the set C - One majority samples and all minority
0.9.0,Create the set S - all majority samples
0.9.0,fit knn on C
0.9.0,Check each sample in S if we keep it or drop it
0.9.0,Do not select sample which are already well classified
0.9.0,Classify on S
0.9.0,If the prediction do not agree with the true label
0.9.0,append it in C_x
0.9.0,Keep the index for later
0.9.0,Update C
0.9.0,fit a knn on C
0.9.0,This experimental to speed up the search
0.9.0,Classify all the element in S and avoid to test the
0.9.0,well classified elements
0.9.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.9.0,Dayvid Oliveira
0.9.0,Christos Aridas
0.9.0,License: MIT
0.9.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.9.0,Christos Aridas
0.9.0,License: MIT
0.9.0,Compute the distance considering the farthest neighbour
0.9.0,Sort the list of distance and get the index
0.9.0,Throw a warning to tell the user that we did not have enough samples
0.9.0,to select and that we just select everything
0.9.0,Select the desired number of samples
0.9.0,idx_tmp is relative to the feature selected in the
0.9.0,previous step and we need to find the indirection
0.9.0,fmt: off
0.9.0,fmt: on
0.9.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.9.0,Christos Aridas
0.9.0,License: MIT
0.9.0,select a sample from the current class
0.9.0,create the set composed of all minority samples and one
0.9.0,sample from the current class.
0.9.0,create the set S with removing the seed from S
0.9.0,since that it will be added anyway
0.9.0,apply Tomek cleaning
0.9.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.9.0,Dayvid Oliveira
0.9.0,Christos Aridas
0.9.0,License: MIT
0.9.0,Check the stopping criterion
0.9.0,1. If there is no changes for the vector y
0.9.0,2. If the number of samples in the other class become inferior to
0.9.0,the number of samples in the majority class
0.9.0,3. If one of the class is disappearing
0.9.0,Case 1
0.9.0,Case 2
0.9.0,Case 3
0.9.0,Check the stopping criterion
0.9.0,1. If the number of samples in the other class become inferior to
0.9.0,the number of samples in the majority class
0.9.0,2. If one of the class is disappearing
0.9.0,Case 1else:
0.9.0,overwrite b_min_bec_maj
0.9.0,Case 2
0.9.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.9.0,Christos Aridas
0.9.0,License: MIT
0.9.0,clean the neighborhood
0.9.0,compute which classes to consider for cleaning for the A2 group
0.9.0,compute a2 group
0.9.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.9.0,Christos Aridas
0.9.0,License: MIT
0.9.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.9.0,Christos Aridas
0.9.0,License: MIT
0.9.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.9.0,Christos Aridas
0.9.0,License: MIT
0.9.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.9.0,Christos Aridas
0.9.0,License: MIT
0.9.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.9.0,Christos Aridas
0.9.0,License: MIT
0.9.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.9.0,Christos Aridas
0.9.0,License: MIT
0.9.0,FIXME: we should use to_numpy with pandas >= 0.25
0.9.0,check that we can undersample even with missing or infinite data
0.9.0,regression tests for #605
0.9.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.9.0,Christos Aridas
0.9.0,License: MIT
0.9.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.9.0,Christos Aridas
0.9.0,License: MIT
0.9.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.9.0,Christos Aridas
0.9.0,License: MIT
0.9.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.9.0,Christos Aridas
0.9.0,License: MIT
0.9.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.9.0,Christos Aridas
0.9.0,License: MIT
0.9.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.9.0,Fernando Nogueira
0.9.0,Christos Aridas
0.9.0,License: MIT
0.9.0,check that we deprecate the `n_jobs` parameter.
0.9.0,check that the samples selecting by the hard voting corresponds to the
0.9.0,targeted class
0.9.0,non-regression test for:
0.9.0,https://github.com/scikit-learn-contrib/imbalanced-learn/issues/738
0.9.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.9.0,Christos Aridas
0.9.0,License: MIT
0.9.0,test that all_estimators doesn't find abstract classes.
0.9.0,"For NearMiss, let's check the three algorithms"
0.9.0,Common tests for estimator instances
0.9.0,Authors: Alexandre Gramfort <alexandre.gramfort@inria.fr>
0.9.0,Raghav RV <rvraghav93@gmail.com>
0.9.0,License: BSD 3 clause
0.9.0,"walk_packages() ignores DeprecationWarnings, now we need to ignore"
0.9.0,FutureWarnings
0.9.0,"mypy error: Module has no attribute ""__path__"""
0.9.0,functions to ignore args / docstring of
0.9.0,Methods where y param should be ignored if y=None by default
0.9.0,numpydoc 0.8.0's docscrape tool raises because of collections.abc under
0.9.0,Python 3.7
0.9.0,Test module docstring formatting
0.9.0,Skip test if numpydoc is not found
0.9.0,XXX unreached code as of v0.22
0.9.0,"pytest tooling, not part of the scikit-learn API"
0.9.0,Exclude non-scikit-learn classes
0.9.0,Now skip docstring test for y when y is None
0.9.0,by default for API reason
0.9.0,Exclude imported functions
0.9.0,Don't test private methods / functions
0.9.0,Test that there are no tabs in our source files
0.9.0,because we don't import
0.9.0,Minimal / degenerate instances: only useful to test the docstrings.
0.9.0,"As certain attributes are present ""only"" if a certain parameter is"
0.9.0,"provided, this checks if the word ""only"" is present in the attribute"
0.9.0,"description, and if not the attribute is required to be present."
0.9.0,ignore deprecation warnings
0.9.0,attributes
0.9.0,properties
0.9.0,ignore properties that raises an AttributeError and deprecated
0.9.0,properties
0.9.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.9.0,License: MIT
0.9.0,check that we can let a pass a regression variable by turning down the
0.9.0,validation
0.9.0,Check that the validation is bypass when calling `fit`
0.9.0,Non-regression test for:
0.9.0,https://github.com/scikit-learn-contrib/imbalanced-learn/issues/782
0.9.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.9.0,Christos Aridas
0.9.0,License: MIT
0.9.0,store timestamp to figure out whether the result of 'fit' has been
0.9.0,cached or not
0.9.0,store timestamp to figure out whether the result of 'fit' has been
0.9.0,cached or not
0.9.0,Pipeline accepts steps as tuple
0.9.0,Test the various init parameters of the pipeline.
0.9.0,Check that we can't instantiate pipelines with objects without fit
0.9.0,method
0.9.0,Smoke test with only an estimator
0.9.0,Check that params are set
0.9.0,Smoke test the repr:
0.9.0,Test with two objects
0.9.0,Check that we can't instantiate with non-transformers on the way
0.9.0,"Note that NoTrans implements fit, but not transform"
0.9.0,Check that params are set
0.9.0,Smoke test the repr:
0.9.0,Check that params are not set when naming them wrong
0.9.0,Test clone
0.9.0,"Check that apart from estimators, the parameters are the same"
0.9.0,Remove estimators that where copied
0.9.0,Test the various methods of the pipeline (anova).
0.9.0,Test with Anova + LogisticRegression
0.9.0,Test that the pipeline can take fit parameters
0.9.0,classifier should return True
0.9.0,and transformer params should not be changed
0.9.0,invalid parameters should raise an error message
0.9.0,Pipeline should pass sample_weight
0.9.0,When sample_weight is None it shouldn't be passed
0.9.0,Test pipeline raises set params error message for nested models.
0.9.0,nested model check
0.9.0,Test the various methods of the pipeline (pca + svm).
0.9.0,Test with PCA + SVC
0.9.0,Test the various methods of the pipeline (preprocessing + svm).
0.9.0,check shapes of various prediction functions
0.9.0,test that the fit_predict method is implemented on a pipeline
0.9.0,test that the fit_predict on pipeline yields same results as applying
0.9.0,transform and clustering steps separately
0.9.0,"As pipeline doesn't clone estimators on construction,"
0.9.0,it must have its own estimators
0.9.0,first compute the transform and clustering step separately
0.9.0,use a pipeline to do the transform and clustering in one step
0.9.0,tests that a pipeline does not have fit_predict method when final
0.9.0,step of pipeline does not have fit_predict defined
0.9.0,tests that Pipeline passes fit_params to intermediate steps
0.9.0,when fit_predict is invoked
0.9.0,Test whether pipeline works with a transformer at the end.
0.9.0,Also test pipeline.transform and pipeline.inverse_transform
0.9.0,test transform and fit_transform:
0.9.0,Test whether pipeline works with a transformer missing fit_transform
0.9.0,test fit_transform:
0.9.0,Directly setting attr
0.9.0,Using set_params
0.9.0,Using set_params to replace single step
0.9.0,With invalid data
0.9.0,Test setting Pipeline steps to None
0.9.0,"for other methods, ensure no AttributeErrors on None:"
0.9.0,mult2 and mult3 are active
0.9.0,Check 'passthrough' step at construction time
0.9.0,Test that an error is raised when memory is not a string or a Memory
0.9.0,instance
0.9.0,Define memory as an integer
0.9.0,Test with Transformer + SVC
0.9.0,Memoize the transformer at the first fit
0.9.0,Get the time stamp of the tranformer in the cached pipeline
0.9.0,Check that cached_pipe and pipe yield identical results
0.9.0,Check that we are reading the cache while fitting
0.9.0,a second time
0.9.0,Check that cached_pipe and pipe yield identical results
0.9.0,Create a new pipeline with cloned estimators
0.9.0,Check that even changing the name step does not affect the cache hit
0.9.0,Check that cached_pipe and pipe yield identical results
0.9.0,Test with Transformer + SVC
0.9.0,Memoize the transformer at the first fit
0.9.0,Get the time stamp of the tranformer in the cached pipeline
0.9.0,Check that cached_pipe and pipe yield identical results
0.9.0,Check that we are reading the cache while fitting
0.9.0,a second time
0.9.0,Check that cached_pipe and pipe yield identical results
0.9.0,Create a new pipeline with cloned estimators
0.9.0,Check that even changing the name step does not affect the cache hit
0.9.0,Check that cached_pipe and pipe yield identical results
0.9.0,Test the various methods of the pipeline (pca + svm).
0.9.0,Test with PCA + SVC
0.9.0,Test the various methods of the pipeline (pca + svm).
0.9.0,Test with PCA + SVC
0.9.0,Test whether pipeline works with a sampler at the end.
0.9.0,Also test pipeline.sampler
0.9.0,test transform and fit_transform:
0.9.0,We round the value near to zero. It seems that PCA has some issue
0.9.0,with that
0.9.0,Test whether pipeline works with a sampler at the end.
0.9.0,Also test pipeline.sampler
0.9.0,Test pipeline using None as preprocessing step and a classifier
0.9.0,"Test pipeline using None, RUS and a classifier"
0.9.0,"Test pipeline using RUS, None and a classifier"
0.9.0,Test pipeline using None step and a sampler
0.9.0,Test pipeline using None and a transformer that implements transform and
0.9.0,inverse_transform
0.9.0,Test the various methods of the pipeline (anova).
0.9.0,Test with RandomUnderSampling + Anova + LogisticRegression
0.9.0,Test the various methods of the pipeline (anova).
0.9.0,Test the various methods of the pipeline (anova).
0.9.0,Test with RandomUnderSampling + Anova + LogisticRegression
0.9.0,tests that Pipeline passes predict_params to the final estimator
0.9.0,when predict is invoked
0.9.0,Test that the score_samples method is implemented on a pipeline.
0.9.0,Test that the score_samples method on pipeline yields same results as
0.9.0,applying transform and score_samples steps separately.
0.9.0,Check the shapes
0.9.0,Check the values
0.9.0,Test that a pipeline does not have score_samples method when the final
0.9.0,step of the pipeline does not have score_samples defined.
0.9.0,Test that the score_samples method is implemented on a pipeline.
0.9.0,Test that the score_samples method on pipeline yields same results as
0.9.0,applying transform and score_samples steps separately.
0.9.0,Check the shapes
0.9.0,Check the values
0.9.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.9.0,Christos Aridas
0.9.0,License: MIT
0.9.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.9.0,License: MIT
0.9.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.9.0,License: MIT
0.9.0,Adapated from scikit-learn
0.9.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.9.0,License: MIT
0.9.0,we don't filter samplers based on their tag here because we want to make
0.9.0,sure that the fitted attribute does not exist if the tag is not
0.9.0,stipulated
0.9.0,trigger our checks if this is a SamplerMixin
0.9.0,should raise warning if the target is continuous (we cannot raise error)
0.9.0,if the target is multilabel then we should raise an error
0.9.0,IHT does not enforce the number of samples but provide a number
0.9.0,of samples the closest to the desired target.
0.9.0,in this test we will force all samplers to not change the class 1
0.9.0,check that sparse matrices can be passed through the sampler leading to
0.9.0,the same results than dense
0.9.0,Check that the samplers handle pandas dataframe and pandas series
0.9.0,check that we return the same type for dataframes or series types
0.9.0,FIXME: we should use to_numpy with pandas >= 0.25
0.9.0,Check that the can samplers handle simple lists
0.9.0,Check that multiclass target lead to the same results than OVA encoding
0.9.0,Cast X and y to not default dtype
0.9.0,Non-regression test for #709
0.9.0,https://github.com/scikit-learn-contrib/imbalanced-learn/issues/709
0.9.0,Adapted from scikit-learn
0.9.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.9.0,License: MIT
0.9.0,Ignore deprecation warnings triggered at import time and from walking
0.9.0,packages
0.9.0,get rid of abstract base classes
0.9.0,get rid of sklearn estimators which have been imported in some classes
0.9.0,"drop duplicates, sort for reproducibility"
0.9.0,itemgetter is used to ensure the sort does not extend to the 2nd item of
0.9.0,the tuple
0.9.0,Author: Alexander L. Hayes <hayesall@iu.edu>
0.9.0,License: MIT
0.9.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.9.0,License: MIT
0.9.0,check that all keys in sampling_strategy are also in y
0.9.0,check that there is no negative number
0.9.0,check that all keys in sampling_strategy are also in y
0.9.0,ignore first 'self' argument for instance methods
0.9.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.9.0,Christos Aridas
0.9.0,License: MIT
0.9.0,this function could create an equal number of samples
0.9.0,We pass on purpose a non sorted dictionary and check that the resulting
0.9.0,dictionary is sorted. Refer to issue #428.
0.9.0,DataFrame and DataFrame case
0.9.0,DataFrames and Series case
0.9.0,The * is place before a keyword only argument without a default value
0.9.0,Test that the minimum dependencies in the README.rst file are
0.9.0,consistent with the minimum dependencies defined at the file:
0.9.0,imblearn/_min_dependencies.py
0.9.0,Skip the test if the README.rst file is not available.
0.9.0,"For instance, when installing scikit-learn from wheels"
0.9.0,Author: Alexander L. Hayes <hayesall@iu.edu>
0.9.0,License: MIT
0.9.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.9.0,License: MIT
0.9.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.9.0,Christos Aridas
0.9.0,License: MIT
0.9.0,check if the filtering is working with a list or a single string
0.9.0,check that all estimators are sampler
0.9.0,check that an error is raised when the type is unknown
0.9.0,TODO: remove in 0.9
0.9.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.9.0,License: MIT
0.9.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.9.0,Christos Aridas
0.9.0,License: MIT
0.9.0,Otherwise create a default SMOTE
0.9.0,Otherwise create a default TomekLinks
0.9.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.9.0,Christos Aridas
0.9.0,License: MIT
0.9.0,Otherwise create a default SMOTE
0.9.0,Otherwise create a default EditedNearestNeighbours
0.9.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.9.0,Christos Aridas
0.9.0,License: MIT
0.9.0,Check if default job count is None
0.9.0,Check if job count is set
0.9.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.9.0,Christos Aridas
0.9.0,License: MIT
0.9.0,Check if default job count is none
0.9.0,Check if job count is set
0.9.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.9.0,License: MIT
0.9.0,resample before to fit the tree
0.9.0,Validate or convert input data
0.9.0,Pre-sort indices to avoid that each individual tree of the
0.9.0,ensemble sorts the indices.
0.9.0,reshape is necessary to preserve the data contiguity against vs
0.9.0,"[:, np.newaxis] that does not."
0.9.0,Get bootstrap sample size
0.9.0,Check parameters
0.9.0,"Free allocated memory, if any"
0.9.0,We draw from the random state to get the random state we
0.9.0,would have got if we hadn't used a warm_start.
0.9.0,Parallel loop: we prefer the threading backend as the Cython code
0.9.0,for fitting the trees is internally releasing the Python GIL
0.9.0,making threading more efficient than multiprocessing in
0.9.0,"that case. However, we respect any parallel_backend contexts set"
0.9.0,"at a higher level, since correctness does not rely on using"
0.9.0,threads.
0.9.0,Collect newly grown trees
0.9.0,Create pipeline with the fitted samplers and trees
0.9.0,Decapsulate classes_ attributes
0.9.0,"with the resampling, we are likely to have rows not included"
0.9.0,for the OOB score leading to division by zero
0.9.0,Instances incorrectly classified
0.9.0,Error fraction
0.9.0,Stop if classification is perfect
0.9.0,Construct y coding as described in Zhu et al [2]:
0.9.0,
0.9.0,y_k = 1 if c == k else -1 / (K - 1)
0.9.0,
0.9.0,"where K == n_classes_ and c, k in [0, K) are indices along the second"
0.9.0,axis of the y coding with c being the index corresponding to the true
0.9.0,class label.
0.9.0,Displace zero probabilities so the log is defined.
0.9.0,Also fix negative elements which may occur with
0.9.0,negative sample weights.
0.9.0,Boost weight using multi-class AdaBoost SAMME.R alg
0.9.0,Only boost the weights if it will fit again
0.9.0,Only boost positive weights
0.9.0,Instances incorrectly classified
0.9.0,Error fraction
0.9.0,Stop if classification is perfect
0.9.0,Stop if the error is at least as bad as random guessing
0.9.0,Boost weight using multi-class AdaBoost SAMME alg
0.9.0,Only boost the weights if I will fit again
0.9.0,Only boost positive weights
0.9.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.9.0,Christos Aridas
0.9.0,License: MIT
0.9.0,overwrite the base class method by disallowing `sample_weight`
0.9.0,RandomUnderSampler is not supporting sample_weight. We need to pass
0.9.0,None.
0.9.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.9.0,Christos Aridas
0.9.0,License: MIT
0.9.0,overwrite the base class method by disallowing `sample_weight`
0.9.0,the sampler needs to be validated before to call _fit because
0.9.0,_validate_y is called before _validate_estimator and would require
0.9.0,to know which type of sampler we are using.
0.9.0,RandomUnderSampler is not supporting sample_weight. We need to pass
0.9.0,None.
0.9.0,check that we have an ensemble of samplers and estimators with a
0.9.0,consistent size
0.9.0,each sampler in the ensemble should have different random state
0.9.0,each estimator in the ensemble should have different random state
0.9.0,check the consistency of the feature importances
0.9.0,check the consistency of the prediction outpus
0.9.0,Predictions should be the same when sample_weight are all ones
0.9.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.9.0,Christos Aridas
0.9.0,License: MIT
0.9.0,Check classification for various parameter settings.
0.9.0,Test that bootstrapping samples generate non-perfect base estimators.
0.9.0,"without bootstrap, all trees are perfect on the training set"
0.9.0,disable the resampling by passing an empty dictionary.
0.9.0,"with bootstrap, trees are no longer perfect on the training set"
0.9.0,Test that bootstrapping features may generate duplicate features.
0.9.0,Predict probabilities.
0.9.0,Normal case
0.9.0,"Degenerate case, where some classes are missing"
0.9.0,Check that oob prediction is a good estimation of the generalization
0.9.0,error.
0.9.0,Test with few estimators
0.9.0,Check singleton ensembles.
0.9.0,Test that it gives proper exception on deficient input.
0.9.0,Test support of decision_function
0.9.0,Check that bagging ensembles can be grid-searched.
0.9.0,Transform iris into a binary classification task
0.9.0,Grid search with scoring based on decision_function
0.9.0,Check base_estimator and its default values.
0.9.0,Test if fitting incrementally with warm start gives a forest of the
0.9.0,right size and the same results as a normal fit.
0.9.0,Test if warm start'ed second fit with smaller n_estimators raises error.
0.9.0,Test that nothing happens when fitting without increasing n_estimators
0.9.0,"modify X to nonsense values, this should not change anything"
0.9.0,warm started classifier with 5+5 estimators should be equivalent to
0.9.0,one classifier with 10 estimators
0.9.0,Check using oob_score and warm_start simultaneously fails
0.9.0,"Make sure OOB scores are identical when random_state, estimator, and"
0.9.0,training data are fixed and fitting is done twice
0.9.0,Check that format of estimators_samples_ is correct and that results
0.9.0,generated at fit time can be identically reproduced at a later time
0.9.0,using data saved in object attributes.
0.9.0,remap the y outside of the BalancedBaggingclassifier
0.9.0,"_, y = np.unique(y, return_inverse=True)"
0.9.0,Get relevant attributes
0.9.0,Test for correct formatting
0.9.0,Re-fit single estimator to test for consistent sampling
0.9.0,Make sure validated max_samples and original max_samples are identical
0.9.0,when valid integer max_samples supplied by user
0.9.0,check that we can pass any kind of sampler to a bagging classifier
0.9.0,check that we have balanced class with the right counts of class
0.9.0,sample depending on the sampling strategy
0.9.0,check that we can provide a FunctionSampler in BalancedBaggingClassifier
0.9.0,find the minority and majority classes
0.9.0,compute the number of sample to draw from the majority class using
0.9.0,a negative binomial distribution
0.9.0,draw randomly with or without replacement
0.9.0,Roughly Balanced Bagging
0.9.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.9.0,Christos Aridas
0.9.0,License: MIT
0.9.0,Generate a global dataset to use
0.9.0,Check classification for various parameter settings.
0.9.0,test the different prediction function
0.9.0,Check base_estimator and its default values.
0.9.0,Test if fitting incrementally with warm start gives a forest of the
0.9.0,right size and the same results as a normal fit.
0.9.0,Test if warm start'ed second fit with smaller n_estimators raises error.
0.9.0,Test that nothing happens when fitting without increasing n_estimators
0.9.0,"modify X to nonsense values, this should not change anything"
0.9.0,warm started classifier with 5+5 estimators should be equivalent to
0.9.0,one classifier with 10 estimators
0.9.0,Check warning if not enough estimators
0.9.0,First fit with no restriction on max samples
0.9.0,Second fit with max samples restricted to just 2
0.9.0,Regression test for #655: check that the oob score is closed to 0.5
0.9.0,a binomial experiment.
0.9.0,Author: Guillaume Lemaitre
0.9.0,License: BSD 3 clause
0.9.0,"The index start at one, then we need to remove one"
0.9.0,to not have issue with the indexing.
0.9.0,go through the list and check if the data are available
0.9.0,Authors: Dayvid Oliveira
0.9.0,Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.9.0,Christos Aridas
0.9.0,License: MIT
0.9.0,restrict ratio to be a dict or a callable
0.9.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.9.0,Christos Aridas
0.9.0,License: MIT
0.9.0,"we are reusing part of utils.check_sampling_strategy, however this is not"
0.9.0,cover in the common tests so we will repeat it here
0.9.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.9.0,Christos Aridas
0.9.0,License: MIT
0.9.0,This is a trick to avoid an error during tests collection with pytest. We
0.9.0,avoid the error when importing the package raise the error at the moment of
0.9.0,creating the instance.
0.9.0,This is a trick to avoid an error during tests collection with pytest. We
0.9.0,avoid the error when importing the package raise the error at the moment of
0.9.0,creating the instance.
0.9.0,flag for keras sequence duck-typing
0.9.0,shuffle the indices since the sampler are packing them by class
0.8.1,This file is here so that when running from the root folder
0.8.1,./sklearn is added to sys.path by pytest.
0.8.1,See https://docs.pytest.org/en/latest/pythonpath.html for more details.
0.8.1,"For example, this allows to build extensions in place and run pytest"
0.8.1,doc/modules/clustering.rst and use sklearn from the local folder
0.8.1,rather than the one from site-packages.
0.8.1,Set numpy array str/repr to legacy behaviour on numpy > 1.13 to make
0.8.1,the doctests pass
0.8.1,! /usr/bin/env python
0.8.1,get __version__ from _version.py
0.8.1,-*- coding: utf-8 -*-
0.8.1,
0.8.1,"imbalanced-learn documentation build configuration file, created by"
0.8.1,sphinx-quickstart on Mon Jan 18 14:44:12 2016.
0.8.1,
0.8.1,This file is execfile()d with the current directory set to its
0.8.1,containing dir.
0.8.1,
0.8.1,Note that not all possible configuration values are present in this
0.8.1,autogenerated file.
0.8.1,
0.8.1,All configuration values have a default; values that are commented out
0.8.1,serve to show the default.
0.8.1,"If extensions (or modules to document with autodoc) are in another directory,"
0.8.1,add these directories to sys.path here. If the directory is relative to the
0.8.1,"documentation root, use os.path.abspath to make it absolute, like shown here."
0.8.1,-- General configuration ------------------------------------------------
0.8.1,"If your documentation needs a minimal Sphinx version, state it here."
0.8.1,needs_sphinx = '1.0'
0.8.1,"Add any Sphinx extension module names here, as strings. They can be"
0.8.1,extensions coming with Sphinx (named 'sphinx.ext.*') or your custom
0.8.1,ones.
0.8.1,"Add any paths that contain templates here, relative to this directory."
0.8.1,The suffix of source filenames.
0.8.1,The master toctree document.
0.8.1,General information about the project.
0.8.1,"The version info for the project you're documenting, acts as replacement for"
0.8.1,"|version| and |release|, also used in various other places throughout the"
0.8.1,built documents.
0.8.1,
0.8.1,The short X.Y version.
0.8.1,"The full version, including alpha/beta/rc tags."
0.8.1,"List of patterns, relative to source directory, that match files and"
0.8.1,directories to ignore when looking for source files.
0.8.1,The reST default role (used for this markup: `text`) to use for all
0.8.1,documents.
0.8.1,"If true, '()' will be appended to :func: etc. cross-reference text."
0.8.1,The name of the Pygments (syntax highlighting) style to use.
0.8.1,-- Options for math equations -----------------------------------------------
0.8.1,-- Options for HTML output ----------------------------------------------
0.8.1,The theme to use for HTML and HTML Help pages.  See the documentation for
0.8.1,a list of builtin themes.
0.8.1,"""twitter_url"": ""https://twitter.com/pandas_dev"","
0.8.1,"""navbar_align"": ""right"",  # For testing that the navbar items align properly"
0.8.1,"Add any paths that contain custom static files (such as style sheets) here,"
0.8.1,"relative to this directory. They are copied after the builtin static files,"
0.8.1,"so a file named ""default.css"" will overwrite the builtin ""default.css""."
0.8.1,Output file base name for HTML help builder.
0.8.1,-- Options for autodoc ------------------------------------------------------
0.8.1,generate autosummary even if no references
0.8.1,-- Options for numpydoc -----------------------------------------------------
0.8.1,this is needed for some reason...
0.8.1,see https://github.com/numpy/numpydoc/issues/69
0.8.1,-- Options for sphinxcontrib-bibtex -----------------------------------------
0.8.1,bibtex file
0.8.1,-- Options for intersphinx --------------------------------------------------
0.8.1,intersphinx configuration
0.8.1,-- Options for sphinx-gallery -----------------------------------------------
0.8.1,Generate the plot for the gallery
0.8.1,sphinx-gallery configuration
0.8.1,-- Options for github link for what's new -----------------------------------
0.8.1,Config for sphinx_issues
0.8.1,The following is used by sphinx.ext.linkcode to provide links to github
0.8.1,-- Options for LaTeX output ---------------------------------------------
0.8.1,The paper size ('letterpaper' or 'a4paper').
0.8.1,"'papersize': 'letterpaper',"
0.8.1,"The font size ('10pt', '11pt' or '12pt')."
0.8.1,"'pointsize': '10pt',"
0.8.1,Additional stuff for the LaTeX preamble.
0.8.1,"'preamble': '',"
0.8.1,Grouping the document tree into LaTeX files. List of tuples
0.8.1,"(source start file, target name, title,"
0.8.1,"author, documentclass [howto, manual, or own class])."
0.8.1,-- Options for manual page output ---------------------------------------
0.8.1,"If false, no module index is generated."
0.8.1,latex_domain_indices = True
0.8.1,One entry per manual page. List of tuples
0.8.1,"(source start file, name, description, authors, manual section)."
0.8.1,"If true, show URL addresses after external links."
0.8.1,man_show_urls = False
0.8.1,-- Options for Texinfo output -------------------------------------------
0.8.1,Grouping the document tree into Texinfo files. List of tuples
0.8.1,"(source start file, target name, title, author,"
0.8.1,"dir menu entry, description, category)"
0.8.1,-- Additional temporary hacks -----------------------------------------------
0.8.1,Temporary work-around for spacing problem between parameter and parameter
0.8.1,"type in the doc, see https://github.com/numpy/numpydoc/issues/215. The bug"
0.8.1,has been fixed in sphinx (https://github.com/sphinx-doc/sphinx/pull/5976) but
0.8.1,through a change in sphinx basic.css except rtd_theme does not use basic.css.
0.8.1,"In an ideal world, this would get fixed in this PR:"
0.8.1,https://github.com/readthedocs/sphinx_rtd_theme/pull/747/files
0.8.1,get the styles from the current theme
0.8.1,create and add the button to all the code blocks that contain >>>
0.8.1,tracebacks (.gt) contain bare text elements that need to be
0.8.1,wrapped in a span to work with .nextUntil() (see later)
0.8.1,define the behavior of the button when it's clicked
0.8.1,hide the code output
0.8.1,show the code output
0.8.1,-*- coding: utf-8 -*-
0.8.1,Format template for issues URI
0.8.1,e.g. 'https://github.com/sloria/marshmallow/issues/{issue}
0.8.1,Format template for PR URI
0.8.1,e.g. 'https://github.com/sloria/marshmallow/pull/{issue}
0.8.1,Format template for commit URI
0.8.1,e.g. 'https://github.com/sloria/marshmallow/commits/{commit}
0.8.1,"Shortcut for Github, e.g. 'sloria/marshmallow'"
0.8.1,Format template for user profile URI
0.8.1,e.g. 'https://github.com/{user}'
0.8.1,Python 2 only
0.8.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.8.1,License: MIT
0.8.1,%%
0.8.1,%%
0.8.1,"First, we will generate a toy classification dataset with only few samples."
0.8.1,The ratio between the classes will be imbalanced.
0.8.1,%%
0.8.1,%%
0.8.1,"Now, we will use a :class:`~imblearn.over_sampling.RandomOverSampler` to"
0.8.1,generate a bootstrap for the minority class with as many samples as in the
0.8.1,majority class.
0.8.1,%%
0.8.1,%%
0.8.1,We observe that the minority samples are less transparent than the samples
0.8.1,"from the majority class. Indeed, it is due to the fact that these samples"
0.8.1,of the minority class are repeated during the bootstrap generation.
0.8.1,
0.8.1,We can set `shrinkage` to a floating value to add a small perturbation to the
0.8.1,samples created and therefore create a smoothed bootstrap.
0.8.1,%%
0.8.1,%%
0.8.1,"In this case, we see that the samples in the minority class are not"
0.8.1,overlapping anymore due to the added noise.
0.8.1,
0.8.1,The parameter `shrinkage` allows to add more or less perturbation. Let's
0.8.1,add more perturbation when generating the smoothed bootstrap.
0.8.1,%%
0.8.1,%%
0.8.1,Increasing the value of `shrinkage` will disperse the new samples. Forcing
0.8.1,the shrinkage to 0 will be equivalent to generating a normal bootstrap.
0.8.1,%%
0.8.1,%%
0.8.1,"Therefore, the `shrinkage` is handy to manually tune the dispersion of the"
0.8.1,new samples.
0.8.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.8.1,License: MIT
0.8.1,%%
0.8.1,generate some data points
0.8.1,plot the majority and minority samples
0.8.1,draw the circle in which the new sample will generated
0.8.1,plot the line on which the sample will be generated
0.8.1,create and plot the new sample
0.8.1,make the plot nicer with legend and label
0.8.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.8.1,License: MIT
0.8.1,%%
0.8.1,%% [markdown]
0.8.1,The following function will be used to create toy dataset. It uses the
0.8.1,:func:`~sklearn.datasets.make_classification` from scikit-learn but fixing
0.8.1,some parameters.
0.8.1,%%
0.8.1,%% [markdown]
0.8.1,The following function will be used to plot the sample space after resampling
0.8.1,to illustrate the specificities of an algorithm.
0.8.1,%%
0.8.1,%% [markdown]
0.8.1,The following function will be used to plot the decision function of a
0.8.1,classifier given some data.
0.8.1,%%
0.8.1,%% [markdown]
0.8.1,Illustration of the influence of the balancing ratio
0.8.1,----------------------------------------------------
0.8.1,
0.8.1,We will first illustrate the influence of the balancing ratio on some toy
0.8.1,data using a logistic regression classifier which is a linear model.
0.8.1,%%
0.8.1,%% [markdown]
0.8.1,We will fit and show the decision boundary model to illustrate the impact of
0.8.1,dealing with imbalanced classes.
0.8.1,%%
0.8.1,%% [markdown]
0.8.1,"Greater is the difference between the number of samples in each class, poorer"
0.8.1,are the classification results.
0.8.1,
0.8.1,Random over-sampling to balance the data set
0.8.1,--------------------------------------------
0.8.1,
0.8.1,Random over-sampling can be used to repeat some samples and balance the
0.8.1,number of samples between the dataset. It can be seen that with this trivial
0.8.1,approach the boundary decision is already less biased toward the majority
0.8.1,class. The class :class:`~imblearn.over_sampling.RandomOverSampler`
0.8.1,implements such of a strategy.
0.8.1,%%
0.8.1,%% [markdown]
0.8.1,"By default, random over-sampling generates a bootstrap. The parameter"
0.8.1,`shrinkage` allows adding a small perturbation to the generated data
0.8.1,to generate a smoothed bootstrap instead. The plot below shows the difference
0.8.1,between the two data generation strategies.
0.8.1,%%
0.8.1,%% [markdown]
0.8.1,It looks like more samples are generated with smoothed bootstrap. This is due
0.8.1,to the fact that the samples generated are not superimposing with the
0.8.1,original samples.
0.8.1,
0.8.1,More advanced over-sampling using ADASYN and SMOTE
0.8.1,--------------------------------------------------
0.8.1,
0.8.1,Instead of repeating the same samples when over-sampling or perturbating the
0.8.1,"generated bootstrap samples, one can use some specific heuristic instead."
0.8.1,:class:`~imblearn.over_sampling.ADASYN` and
0.8.1,:class:`~imblearn.over_sampling.SMOTE` can be used in this case.
0.8.1,%%
0.8.1,%% [markdown]
0.8.1,The following plot illustrates the difference between
0.8.1,:class:`~imblearn.over_sampling.ADASYN` and
0.8.1,:class:`~imblearn.over_sampling.SMOTE`.
0.8.1,:class:`~imblearn.over_sampling.ADASYN` will focus on the samples which are
0.8.1,difficult to classify with a nearest-neighbors rule while regular
0.8.1,:class:`~imblearn.over_sampling.SMOTE` will not make any distinction.
0.8.1,"Therefore, the decision function depending of the algorithm."
0.8.1,%% [markdown]
0.8.1,"Due to those sampling particularities, it can give rise to some specific"
0.8.1,issues as illustrated below.
0.8.1,%%
0.8.1,%% [markdown]
0.8.1,SMOTE proposes several variants by identifying specific samples to consider
0.8.1,during the resampling. The borderline version
0.8.1,(:class:`~imblearn.over_sampling.BorderlineSMOTE`) will detect which point to
0.8.1,select which are in the border between two classes. The SVM version
0.8.1,(:class:`~imblearn.over_sampling.SVMSMOTE`) will use the support vectors
0.8.1,found using an SVM algorithm to create new sample while the KMeans version
0.8.1,(:class:`~imblearn.over_sampling.KMeansSMOTE`) will make a clustering before
0.8.1,to generate samples in each cluster independently depending each cluster
0.8.1,density.
0.8.1,%%
0.8.1,%% [markdown]
0.8.1,"When dealing with a mixed of continuous and categorical features,"
0.8.1,:class:`~imblearn.over_sampling.SMOTENC` is the only method which can handle
0.8.1,this case.
0.8.1,%%
0.8.1,Create a dataset of a mix of numerical and categorical data
0.8.1,%% [markdown]
0.8.1,"However, if the dataset is composed of only categorical features then one"
0.8.1,should use :class:`~imblearn.over_sampling.SMOTEN`.
0.8.1,%%
0.8.1,Generate only categorical data
0.8.1,Authors: Christos Aridas
0.8.1,Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.8.1,License: MIT
0.8.1,%%
0.8.1,%% [markdown]
0.8.1,Let's first generate a dataset with imbalanced class distribution.
0.8.1,%%
0.8.1,%% [markdown]
0.8.1,We will use an over-sampler :class:`~imblearn.over_sampling.SMOTE` followed
0.8.1,by a :class:`~sklearn.tree.DecisionTreeClassifier`. The aim will be to
0.8.1,search which `k_neighbors` parameter is the most adequate with the dataset
0.8.1,that we generated.
0.8.1,%%
0.8.1,%% [markdown]
0.8.1,We can use the :class:`~sklearn.model_selection.validation_curve` to inspect
0.8.1,"the impact of varying the parameter `k_neighbors`. In this case, we need"
0.8.1,to use a score to evaluate the generalization score during the
0.8.1,cross-validation.
0.8.1,%%
0.8.1,%%
0.8.1,%% [markdown]
0.8.1,We can now plot the results of the cross-validation for the different
0.8.1,parameter values that we tried.
0.8.1,%%
0.8.1,make nice plotting
0.8.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.8.1,License: MIT
0.8.1,Generate a dataset
0.8.1,Split the data
0.8.1,Train the classifier with balancing
0.8.1,Test the classifier and get the prediction
0.8.1,Show the classification report
0.8.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.8.1,License: MIT
0.8.1,%%
0.8.1,%% [markdown]
0.8.1,"First, we will generate some imbalanced dataset."
0.8.1,%%
0.8.1,%% [markdown]
0.8.1,We will split the data into a training and testing set.
0.8.1,%%
0.8.1,%% [markdown]
0.8.1,We will create a pipeline made of a :class:`~imblearn.over_sampling.SMOTE`
0.8.1,over-sampler followed by a :class:`~sklearn.svm.LinearSVC` classifier.
0.8.1,%%
0.8.1,%% [markdown]
0.8.1,"Now, we will train the model on the training set and get the prediction"
0.8.1,associated with the testing set. Be aware that the resampling will happen
0.8.1,only when calling `fit`: the number of samples in `y_pred` is the same than
0.8.1,in `y_test`.
0.8.1,%%
0.8.1,%% [markdown]
0.8.1,The geometric mean corresponds to the square root of the product of the
0.8.1,sensitivity and specificity. Combining the two metrics should account for
0.8.1,the balancing of the dataset.
0.8.1,%%
0.8.1,%% [markdown]
0.8.1,The index balanced accuracy can transform any metric to be used in
0.8.1,imbalanced learning problems.
0.8.1,%%
0.8.1,%%
0.8.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.8.1,License: MIT
0.8.1,%%
0.8.1,%% [markdown]
0.8.1,Dataset generation
0.8.1,------------------
0.8.1,
0.8.1,We will create an imbalanced dataset with a couple of samples. We will use
0.8.1,:func:`~sklearn.datasets.make_classification` to generate this dataset.
0.8.1,%%
0.8.1,%%
0.8.1,%% [markdown]
0.8.1,The following function will be used to plot the sample space after resampling
0.8.1,to illustrate the characteristic of an algorithm.
0.8.1,%%
0.8.1,%% [markdown]
0.8.1,The following function will be used to plot the decision function of a
0.8.1,classifier given some data.
0.8.1,%%
0.8.1,%% [markdown]
0.8.1,":class:`~imblearn.over_sampling.SMOTE` allows to generate samples. However,"
0.8.1,this method of over-sampling does not have any knowledge regarding the
0.8.1,"underlying distribution. Therefore, some noisy samples can be generated, e.g."
0.8.1,"when the different classes cannot be well separated. Hence, it can be"
0.8.1,beneficial to apply an under-sampling algorithm to clean the noisy samples.
0.8.1,Two methods are usually used in the literature: (i) Tomek's link and (ii)
0.8.1,edited nearest neighbours cleaning methods. Imbalanced-learn provides two
0.8.1,ready-to-use samplers :class:`~imblearn.combine.SMOTETomek` and
0.8.1,":class:`~imblearn.combine.SMOTEENN`. In general,"
0.8.1,:class:`~imblearn.combine.SMOTEENN` cleans more noisy data than
0.8.1,:class:`~imblearn.combine.SMOTETomek`.
0.8.1,%%
0.8.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.8.1,License: MIT
0.8.1,%%
0.8.1,%% [markdown]
0.8.1,Load an imbalanced dataset
0.8.1,--------------------------
0.8.1,
0.8.1,We will load the UCI SatImage dataset which has an imbalanced ratio of 9.3:1
0.8.1,(number of majority sample for a minority sample). The data are then split
0.8.1,into training and testing.
0.8.1,%%
0.8.1,%% [markdown]
0.8.1,Classification using a single decision tree
0.8.1,-------------------------------------------
0.8.1,
0.8.1,We train a decision tree classifier which will be used as a baseline for the
0.8.1,rest of this example.
0.8.1,
0.8.1,The results are reported in terms of balanced accuracy and geometric mean
0.8.1,which are metrics widely used in the literature to validate model trained on
0.8.1,imbalanced set.
0.8.1,%%
0.8.1,%%
0.8.1,%%
0.8.1,%% [markdown]
0.8.1,Classification using bagging classifier with and without sampling
0.8.1,-----------------------------------------------------------------
0.8.1,
0.8.1,"Instead of using a single tree, we will check if an ensemble of decsion tree"
0.8.1,"can actually alleviate the issue induced by the class imbalancing. First, we"
0.8.1,will use a bagging classifier and its counter part which internally uses a
0.8.1,random under-sampling to balanced each boostrap sample.
0.8.1,%%
0.8.1,%% [markdown]
0.8.1,Balancing each bootstrap sample allows to increase significantly the balanced
0.8.1,accuracy and the geometric mean.
0.8.1,%%
0.8.1,%%
0.8.1,%% [markdown]
0.8.1,Classification using random forest classifier with and without sampling
0.8.1,-----------------------------------------------------------------------
0.8.1,
0.8.1,Random forest is another popular ensemble method and it is usually
0.8.1,"outperforming bagging. Here, we used a vanilla random forest and its balanced"
0.8.1,counterpart in which each bootstrap sample is balanced.
0.8.1,%%
0.8.1,%% [markdown]
0.8.1,"Similarly to the previous experiment, the balanced classifier outperform the"
0.8.1,"classifier which learn from imbalanced bootstrap samples. In addition, random"
0.8.1,forest outsperforms the bagging classifier.
0.8.1,%%
0.8.1,%%
0.8.1,%% [markdown]
0.8.1,Boosting classifier
0.8.1,-------------------
0.8.1,
0.8.1,"In the same manner, easy ensemble classifier is a bag of balanced AdaBoost"
0.8.1,"classifier. However, it will be slower to train than random forest and will"
0.8.1,achieve worse performance.
0.8.1,%%
0.8.1,%%
0.8.1,%%
0.8.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.8.1,License: MIT
0.8.1,%%
0.8.1,%% [markdown]
0.8.1,Generate an imbalanced dataset
0.8.1,------------------------------
0.8.1,
0.8.1,"For this example, we will create a synthetic dataset using the function"
0.8.1,:func:`~sklearn.datasets.make_classification`. The problem will be a toy
0.8.1,classification problem with a ratio of 1:9 between the two classes.
0.8.1,%%
0.8.1,%%
0.8.1,%% [markdown]
0.8.1,"In the following sections, we will show a couple of algorithms that have"
0.8.1,been proposed over the years. We intend to illustrate how one can reuse the
0.8.1,:class:`~imblearn.ensemble.BalancedBaggingClassifier` by passing different
0.8.1,sampler.
0.8.1,%%
0.8.1,%% [markdown]
0.8.1,Exactly Balanced Bagging and Over-Bagging
0.8.1,-----------------------------------------
0.8.1,
0.8.1,The :class:`~imblearn.ensemble.BalancedBaggingClassifier` can use in
0.8.1,conjunction with a :class:`~imblearn.under_sampling.RandomUnderSampler` or
0.8.1,:class:`~imblearn.over_sampling.RandomOverSampler`. These methods are
0.8.1,"referred as Exactly Balanced Bagging and Over-Bagging, respectively and have"
0.8.1,been proposed first in [1]_.
0.8.1,%%
0.8.1,Exactly Balanced Bagging
0.8.1,%%
0.8.1,Over-bagging
0.8.1,%% [markdown]
0.8.1,SMOTE-Bagging
0.8.1,-------------
0.8.1,
0.8.1,Instead of using a :class:`~imblearn.over_sampling.RandomOverSampler` that
0.8.1,"make a bootstrap, an alternative is to use"
0.8.1,:class:`~imblearn.over_sampling.SMOTE` as an over-sampler. This is known as
0.8.1,SMOTE-Bagging [2]_.
0.8.1,%%
0.8.1,SMOTE-Bagging
0.8.1,%% [markdown]
0.8.1,Roughly Balanced Bagging
0.8.1,------------------------
0.8.1,While using a :class:`~imblearn.under_sampling.RandomUnderSampler` or
0.8.1,:class:`~imblearn.over_sampling.RandomOverSampler` will create exactly the
0.8.1,"desired number of samples, it does not follow the statistical spirit wanted"
0.8.1,in the bagging framework. The authors in [3]_ proposes to use a negative
0.8.1,binomial distribution to compute the number of samples of the majority
0.8.1,class to be selected and then perform a random under-sampling.
0.8.1,
0.8.1,"Here, we illustrate this method by implementing a function in charge of"
0.8.1,resampling and use the :class:`~imblearn.FunctionSampler` to integrate it
0.8.1,within a :class:`~imblearn.pipeline.Pipeline` and
0.8.1,:class:`~sklearn.model_selection.cross_validate`.
0.8.1,%%
0.8.1,find the minority and majority classes
0.8.1,compute the number of sample to draw from the majority class using
0.8.1,a negative binomial distribution
0.8.1,draw randomly with or without replacement
0.8.1,Roughly Balanced Bagging
0.8.1,%% [markdown]
0.8.1,.. topic:: References:
0.8.1,
0.8.1,".. [1] R. Maclin, and D. Opitz. ""An empirical evaluation of bagging and"
0.8.1,"boosting."" AAAI/IAAI 1997 (1997): 546-551."
0.8.1,
0.8.1,".. [2] S. Wang, and X. Yao. ""Diversity analysis on imbalanced data sets by"
0.8.1,"using ensemble models."" 2009 IEEE symposium on computational"
0.8.1,"intelligence and data mining. IEEE, 2009."
0.8.1,
0.8.1,".. [3] S. Hido, H. Kashima, and Y. Takahashi. ""Roughly balanced bagging"
0.8.1,"for imbalanced data."" Statistical Analysis and Data Mining: The ASA"
0.8.1,Data Science Journal 2.5‚Äê6 (2009): 412-426.
0.8.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.8.1,License: MIT
0.8.1,%%
0.8.1,%% [markdown]
0.8.1,The following function will be used to create toy dataset. It uses the
0.8.1,:func:`~sklearn.datasets.make_classification` from scikit-learn but fixing
0.8.1,some parameters.
0.8.1,%%
0.8.1,%% [markdown]
0.8.1,The following function will be used to plot the sample space after resampling
0.8.1,to illustrate the specificities of an algorithm.
0.8.1,%%
0.8.1,%% [markdown]
0.8.1,The following function will be used to plot the decision function of a
0.8.1,classifier given some data.
0.8.1,%%
0.8.1,%%
0.8.1,%% [markdown]
0.8.1,Prototype generation: under-sampling by generating new samples
0.8.1,--------------------------------------------------------------
0.8.1,
0.8.1,:class:`~imblearn.under_sampling.ClusterCentroids` under-samples by replacing
0.8.1,the original samples by the centroids of the cluster found.
0.8.1,%%
0.8.1,%% [markdown]
0.8.1,Prototype selection: under-sampling by selecting existing samples
0.8.1,-----------------------------------------------------------------
0.8.1,
0.8.1,The algorithm performing prototype selection can be subdivided into two
0.8.1,groups: (i) the controlled under-sampling methods and (ii) the cleaning
0.8.1,under-sampling methods.
0.8.1,
0.8.1,"With the controlled under-sampling methods, the number of samples to be"
0.8.1,selected can be specified.
0.8.1,:class:`~imblearn.under_sampling.RandomUnderSampler` is the most naive way of
0.8.1,performing such selection by randomly selecting a given number of samples by
0.8.1,the targetted class.
0.8.1,%%
0.8.1,%% [markdown]
0.8.1,:class:`~imblearn.under_sampling.NearMiss` algorithms implement some
0.8.1,heuristic rules in order to select samples. NearMiss-1 selects samples from
0.8.1,the majority class for which the average distance of the :math:`k`` nearest
0.8.1,samples of the minority class is the smallest. NearMiss-2 selects the samples
0.8.1,from the majority class for which the average distance to the farthest
0.8.1,samples of the negative class is the smallest. NearMiss-3 is a 2-step
0.8.1,"algorithm: first, for each minority sample, their :math:`m`"
0.8.1,"nearest-neighbors will be kept; then, the majority samples selected are the"
0.8.1,on for which the average distance to the :math:`k` nearest neighbors is the
0.8.1,largest.
0.8.1,%%
0.8.1,%% [markdown]
0.8.1,:class:`~imblearn.under_sampling.EditedNearestNeighbours` removes samples of
0.8.1,the majority class for which their class differ from the one of their
0.8.1,nearest-neighbors. This sieve can be repeated which is the principle of the
0.8.1,:class:`~imblearn.under_sampling.RepeatedEditedNearestNeighbours`.
0.8.1,:class:`~imblearn.under_sampling.AllKNN` is slightly different from the
0.8.1,:class:`~imblearn.under_sampling.RepeatedEditedNearestNeighbours` by changing
0.8.1,"the :math:`k` parameter of the internal nearest neighors algorithm,"
0.8.1,increasing it at each iteration.
0.8.1,%%
0.8.1,%% [markdown]
0.8.1,:class:`~imblearn.under_sampling.CondensedNearestNeighbour` makes use of a
0.8.1,1-NN to iteratively decide if a sample should be kept in a dataset or not.
0.8.1,The issue is that :class:`~imblearn.under_sampling.CondensedNearestNeighbour`
0.8.1,is sensitive to noise by preserving the noisy samples.
0.8.1,:class:`~imblearn.under_sampling.OneSidedSelection` also used the 1-NN and
0.8.1,use :class:`~imblearn.under_sampling.TomekLinks` to remove the samples
0.8.1,considered noisy. The
0.8.1,:class:`~imblearn.under_sampling.NeighbourhoodCleaningRule` use a
0.8.1,:class:`~imblearn.under_sampling.EditedNearestNeighbours` to remove some
0.8.1,"sample. Additionally, they use a 3 nearest-neighbors to remove samples which"
0.8.1,do not agree with this rule.
0.8.1,%%
0.8.1,%% [markdown]
0.8.1,:class:`~imblearn.under_sampling.InstanceHardnessThreshold` uses the
0.8.1,prediction of classifier to exclude samples. All samples which are classified
0.8.1,with a low probability will be removed.
0.8.1,%%
0.8.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.8.1,License: MIT
0.8.1,%%
0.8.1,%% [markdown]
0.8.1,This function allows to make nice plotting
0.8.1,%%
0.8.1,%% [markdown]
0.8.1,We will generate some toy data that illustrates how
0.8.1,:class:`~imblearn.under_sampling.TomekLinks` is used to clean a dataset.
0.8.1,%%
0.8.1,%% [markdown]
0.8.1,"In the figure above, the samples highlighted in green form a Tomek link since"
0.8.1,they are of different classes and are nearest neighbors of each other.
0.8.1,highlight the samples of interest
0.8.1,%% [markdown]
0.8.1,We can run the :class:`~imblearn.under_sampling.TomekLinks` sampling to
0.8.1,remove the corresponding samples. If `sampling_strategy='auto'` only the
0.8.1,sample from the majority class will be removed. If `sampling_strategy='all'`
0.8.1,both samples will be removed.
0.8.1,%%
0.8.1,highlight the samples of interest
0.8.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.8.1,License: MIT
0.8.1,%%
0.8.1,%% [markdown]
0.8.1,We define a function allowing to make some nice decoration on the plot.
0.8.1,%%
0.8.1,%% [markdown]
0.8.1,We can start by generating some data to later illustrate the principle of
0.8.1,each :class:`~imblearn.under_sampling.NearMiss` heuristic rules.
0.8.1,%%
0.8.1,%% [mardown]
0.8.1,NearMiss-1
0.8.1,----------
0.8.1,
0.8.1,NearMiss-1 selects samples from the majority class for which the average
0.8.1,distance to some nearest neighbours is the smallest. In the following
0.8.1,"example, we use a 3-NN to compute the average distance on 2 specific samples"
0.8.1,"of the majority class. Therefore, in this case the point linked by the"
0.8.1,green-dashed line will be selected since the average distance is smaller.
0.8.1,%%
0.8.1,%% [mardown]
0.8.1,NearMiss-2
0.8.1,----------
0.8.1,
0.8.1,NearMiss-2 selects samples from the majority class for which the average
0.8.1,distance to the farthest neighbors is the smallest. With the same
0.8.1,"configuration as previously presented, the sample linked to the green-dashed"
0.8.1,line will be selected since its distance the 3 farthest neighbors is the
0.8.1,smallest.
0.8.1,%%
0.8.1,%% [mardown]
0.8.1,NearMiss-3
0.8.1,----------
0.8.1,
0.8.1,"NearMiss-3 can be divided into 2 steps. First, a nearest-neighbors is used to"
0.8.1,short-list samples from the majority class (i.e. correspond to the
0.8.1,"highlighted samples in the following plot). Then, the sample with the largest"
0.8.1,average distance to the *k* nearest-neighbors are selected.
0.8.1,%%
0.8.1,select only the majority point of interest
0.8.1,Authors: Christos Aridas
0.8.1,Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.8.1,License: MIT
0.8.1,%%
0.8.1,%% [markdown]
0.8.1,Let's first create an imbalanced dataset and split in to two sets.
0.8.1,%%
0.8.1,%% [markdown]
0.8.1,"Now, we will create each individual steps that we would like later to combine"
0.8.1,%%
0.8.1,%% [markdown]
0.8.1,"Now, we can finally create a pipeline to specify in which order the different"
0.8.1,transformers and samplers should be executed before to provide the data to
0.8.1,the final classifier.
0.8.1,%%
0.8.1,%% [markdown]
0.8.1,We can now use the pipeline created as a normal classifier where resampling
0.8.1,"will happen when calling `fit` and disabled when calling `decision_function`,"
0.8.1,"`predict_proba`, or `predict`."
0.8.1,%%
0.8.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.8.1,License: MIT
0.8.1,##############################################################################
0.8.1,Data loading
0.8.1,##############################################################################
0.8.1,##############################################################################
0.8.1,"First, you should download the Porto Seguro data set from Kaggle. See the"
0.8.1,link in the introduction.
0.8.1,##############################################################################
0.8.1,The data set is imbalanced and it will have an effect on the fitting.
0.8.1,##############################################################################
0.8.1,Define the pre-processing pipeline
0.8.1,##############################################################################
0.8.1,##############################################################################
0.8.1,We want to standard scale the numerical features while we want to one-hot
0.8.1,"encode the categorical features. In this regard, we make use of the"
0.8.1,:class:`~sklearn.compose.ColumnTransformer`.
0.8.1,Create an environment variable to avoid using the GPU. This can be changed.
0.8.1,##############################################################################
0.8.1,Create a neural-network
0.8.1,##############################################################################
0.8.1,##############################################################################
0.8.1,We create a decorator to report the computation time
0.8.1,##############################################################################
0.8.1,The first model will be trained using the ``fit`` method and with imbalanced
0.8.1,mini-batches.
0.8.1,##############################################################################
0.8.1,"In the contrary, we will use imbalanced-learn to create a generator of"
0.8.1,mini-batches which will yield balanced mini-batches.
0.8.1,##############################################################################
0.8.1,Classification loop
0.8.1,##############################################################################
0.8.1,##############################################################################
0.8.1,We will perform a 10-fold cross-validation and train the neural-network with
0.8.1,the two different strategies previously presented.
0.8.1,##############################################################################
0.8.1,Plot of the results and computation time
0.8.1,##############################################################################
0.8.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.8.1,License: MIT
0.8.1,%%
0.8.1,%% [markdown]
0.8.1,Problem definition
0.8.1,------------------
0.8.1,
0.8.1,We are dropping the following features:
0.8.1,
0.8.1,"- ""fnlwgt"": this feature was created while studying the ""adult"" dataset."
0.8.1,"Thus, we will not use this feature which is not acquired during the survey."
0.8.1,"- ""education-num"": it is encoding the same information than ""education""."
0.8.1,"Thus, we are removing one of these 2 features."
0.8.1,%%
0.8.1,%% [markdown]
0.8.1,"The ""adult"" dataset as a class ratio of about 3:1"
0.8.1,%%
0.8.1,%% [markdown]
0.8.1,This dataset is only slightly imbalanced. To better highlight the effect of
0.8.1,"learning from an imbalanced dataset, we will increase its ratio to 30:1"
0.8.1,%%
0.8.1,%% [markdown]
0.8.1,We will perform a cross-validation evaluation to get an estimate of the test
0.8.1,score.
0.8.1,
0.8.1,"As a baseline, we could use a classifier which will always predict the"
0.8.1,majority class independently of the features provided.
0.8.1,%%
0.8.1,%% [markdown]
0.8.1,"Instead of using the accuracy, we can use the balanced accuracy which will"
0.8.1,take into account the balancing issue.
0.8.1,%%
0.8.1,%% [markdown]
0.8.1,Strategies to learn from an imbalanced dataset
0.8.1,----------------------------------------------
0.8.1,We will use a dictionary and a list to continuously store the results of
0.8.1,our experiments and show them as a pandas dataframe.
0.8.1,%%
0.8.1,%% [markdown]
0.8.1,Dummy baseline
0.8.1,..............
0.8.1,
0.8.1,"Before to train a real machine learning model, we can store the results"
0.8.1,obtained with our :class:`~sklearn.dummy.DummyClassifier`.
0.8.1,%%
0.8.1,%% [markdown]
0.8.1,Linear classifier baseline
0.8.1,..........................
0.8.1,
0.8.1,We will create a machine learning pipeline using a
0.8.1,":class:`~sklearn.linear_model.LogisticRegression` classifier. In this regard,"
0.8.1,we will need to one-hot encode the categorical columns and standardized the
0.8.1,numerical columns before to inject the data into the
0.8.1,:class:`~sklearn.linear_model.LogisticRegression` classifier.
0.8.1,
0.8.1,"First, we define our numerical and categorical pipelines."
0.8.1,%%
0.8.1,%% [markdown]
0.8.1,"Then, we can create a preprocessor which will dispatch the categorical"
0.8.1,columns to the categorical pipeline and the numerical columns to the
0.8.1,numerical pipeline
0.8.1,%%
0.8.1,%% [markdown]
0.8.1,"Finally, we connect our preprocessor with our"
0.8.1,:class:`~sklearn.linear_model.LogisticRegression`. We can then evaluate our
0.8.1,model.
0.8.1,%%
0.8.1,%%
0.8.1,%% [markdown]
0.8.1,We can see that our linear model is learning slightly better than our dummy
0.8.1,"baseline. However, it is impacted by the class imbalance."
0.8.1,
0.8.1,We can verify that something similar is happening with a tree-based model
0.8.1,such as :class:`~sklearn.ensemble.RandomForestClassifier`. With this type of
0.8.1,"classifier, we will not need to scale the numerical data, and we will only"
0.8.1,need to ordinal encode the categorical data.
0.8.1,%%
0.8.1,%%
0.8.1,%% [markdown]
0.8.1,The :class:`~sklearn.ensemble.RandomForestClassifier` is as well affected by
0.8.1,"the class imbalanced, slightly less than the linear model. Now, we will"
0.8.1,present different approach to improve the performance of these 2 models.
0.8.1,
0.8.1,Use `class_weight`
0.8.1,..................
0.8.1,
0.8.1,Most of the models in `scikit-learn` have a parameter `class_weight`. This
0.8.1,parameter will affect the computation of the loss in linear model or the
0.8.1,criterion in the tree-based model to penalize differently a false
0.8.1,classification from the minority and majority class. We can set
0.8.1,"`class_weight=""balanced""` such that the weight applied is inversely"
0.8.1,proportional to the class frequency. We test this parametrization in both
0.8.1,linear model and tree-based model.
0.8.1,%%
0.8.1,%%
0.8.1,%% [markdown]
0.8.1,We can see that using `class_weight` was really effective for the linear
0.8.1,"model, alleviating the issue of learning from imbalanced classes. However,"
0.8.1,the :class:`~sklearn.ensemble.RandomForestClassifier` is still biased toward
0.8.1,"the majority class, mainly due to the criterion which is not suited enough to"
0.8.1,fight the class imbalance.
0.8.1,
0.8.1,Resample the training set during learning
0.8.1,.........................................
0.8.1,
0.8.1,Another way is to resample the training set by under-sampling or
0.8.1,over-sampling some of the samples. `imbalanced-learn` provides some samplers
0.8.1,to do such processing.
0.8.1,%%
0.8.1,%%
0.8.1,%%
0.8.1,%%
0.8.1,%% [markdown]
0.8.1,Applying a random under-sampler before the training of the linear model or
0.8.1,"random forest, allows to not focus on the majority class at the cost of"
0.8.1,making more mistake for samples in the majority class (i.e. decreased
0.8.1,accuracy).
0.8.1,
0.8.1,We could apply any type of samplers and find which sampler is working best
0.8.1,on the current dataset.
0.8.1,
0.8.1,"Instead, we will present another way by using classifiers which will apply"
0.8.1,sampling internally.
0.8.1,
0.8.1,Use of specific balanced algorithms from imbalanced-learn
0.8.1,.........................................................
0.8.1,
0.8.1,We already showed that random under-sampling can be effective on decision
0.8.1,"tree. However, instead of under-sampling once the dataset, one could"
0.8.1,under-sample the original dataset before to take a bootstrap sample. This is
0.8.1,the base of the :class:`imblearn.ensemble.BalancedRandomForestClassifier` and
0.8.1,:class:`~imblearn.ensemble.BalancedBaggingClassifier`.
0.8.1,%%
0.8.1,%%
0.8.1,%% [markdown]
0.8.1,The performance with the
0.8.1,:class:`~imblearn.ensemble.BalancedRandomForestClassifier` is better than
0.8.1,applying a single random under-sampling. We will use a gradient-boosting
0.8.1,classifier within a :class:`~imblearn.ensemble.BalancedBaggingClassifier`.
0.8.1,%% [markdown]
0.8.1,This last approach is the most effective. The different under-sampling allows
0.8.1,to bring some diversity for the different GBDT to learn and not focus on a
0.8.1,portion of the majority class.
0.8.1,Authors: Christos Aridas
0.8.1,Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.8.1,License: MIT
0.8.1,%%
0.8.1,%% [markdown]
0.8.1,Load the dataset
0.8.1,----------------
0.8.1,
0.8.1,We will use a dataset containing image from know person where we will
0.8.1,build a model to recognize the person on the image. We will make this problem
0.8.1,a binary problem by taking picture of only George W. Bush and Bill Clinton.
0.8.1,%%
0.8.1,%%
0.8.1,%% [markdown]
0.8.1,We can check the ratio between the two classes.
0.8.1,%%
0.8.1,%% [markdown]
0.8.1,We see that we have an imbalanced classification problem with ~95% of the
0.8.1,data belonging to the class G.W. Bush.
0.8.1,
0.8.1,Compare over-sampling approaches
0.8.1,--------------------------------
0.8.1,
0.8.1,We will use different over-sampling approaches and use a kNN classifier
0.8.1,to check if we can recognize the 2 presidents. The evaluation will be
0.8.1,performed through cross-validation and we will plot the mean ROC curve.
0.8.1,
0.8.1,We will create different pipelines and evaluate them.
0.8.1,%%
0.8.1,%%
0.8.1,%% [markdown]
0.8.1,We will compute the mean ROC curve for each pipeline using a different splits
0.8.1,provided by the :class:`~sklearn.model_selection.StratifiedKFold`
0.8.1,cross-validation.
0.8.1,%%
0.8.1,compute the mean fpr/tpr to get the mean ROC curve
0.8.1,Create a display that we will reuse to make the aggregated plots for
0.8.1,all methods
0.8.1,%% [markdown]
0.8.1,"In the previous cell, we created the different mean ROC curve and we can plot"
0.8.1,them on the same plot.
0.8.1,%%
0.8.1,%% [markdown]
0.8.1,"We see that for this task, methods that are generating new samples with some"
0.8.1,interpolation (i.e. ADASYN and SMOTE) perform better than random
0.8.1,over-sampling or no resampling.
0.8.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.8.1,License: MIT
0.8.1,Create a folder to fetch the dataset
0.8.1,Create a pipeline
0.8.1,Classify and report the results
0.8.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.8.1,License: MIT
0.8.1,%%
0.8.1,%% [markdown]
0.8.1,Setting the data set
0.8.1,--------------------
0.8.1,
0.8.1,We use a part of the 20 newsgroups data set by loading 4 topics. Using the
0.8.1,"scikit-learn loader, the data are split into a training and a testing set."
0.8.1,
0.8.1,Note the class \#3 is the minority class and has almost twice less samples
0.8.1,than the majority class.
0.8.1,%%
0.8.1,%%
0.8.1,% [markdown]
0.8.1,The usual scikit-learn pipeline
0.8.1,-------------------------------
0.8.1,
0.8.1,You might usually use scikit-learn pipeline by combining the TF-IDF
0.8.1,vectorizer to feed a multinomial naive bayes classifier. A classification
0.8.1,report summarized the results on the testing set.
0.8.1,
0.8.1,"As expected, the recall of the class \#3 is low mainly due to the class"
0.8.1,imbalanced.
0.8.1,%%
0.8.1,%%
0.8.1,%% [markdown]
0.8.1,Balancing the class before classification
0.8.1,-----------------------------------------
0.8.1,
0.8.1,"To improve the prediction of the class \#3, it could be interesting to apply"
0.8.1,"a balancing before to train the naive bayes classifier. Therefore, we will"
0.8.1,use a :class:`~imblearn.under_sampling.RandomUnderSampler` to equalize the
0.8.1,number of samples in all the classes before the training.
0.8.1,
0.8.1,It is also important to note that we are using the
0.8.1,:class:`~imblearn.pipeline.make_pipeline` function implemented in
0.8.1,imbalanced-learn to properly handle the samplers.
0.8.1,%%
0.8.1,%% [markdown]
0.8.1,"Although the results are almost identical, it can be seen that the resampling"
0.8.1,allowed to correct the poor recall of the class \#3 at the cost of reducing
0.8.1,"the other metrics for the other classes. However, the overall results are"
0.8.1,slightly better.
0.8.1,%%
0.8.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.8.1,License: MIT
0.8.1,#############################################################################
0.8.1,Toy data generation
0.8.1,#############################################################################
0.8.1,#############################################################################
0.8.1,We are generating some non Gaussian data set contaminated with some unform
0.8.1,noise.
0.8.1,#############################################################################
0.8.1,We will generate some cleaned test data without outliers.
0.8.1,#############################################################################
0.8.1,How to use the :class:`~imblearn.FunctionSampler`
0.8.1,#############################################################################
0.8.1,#############################################################################
0.8.1,We first define a function which will use
0.8.1,:class:`~sklearn.ensemble.IsolationForest` to eliminate some outliers from
0.8.1,our dataset during training. The function passed to the
0.8.1,:class:`~imblearn.FunctionSampler` will be called when using the method
0.8.1,``fit_resample``.
0.8.1,#############################################################################
0.8.1,Integrate it within a pipeline
0.8.1,#############################################################################
0.8.1,#############################################################################
0.8.1,"By elimnating outliers before the training, the classifier will be less"
0.8.1,affected during the prediction.
0.8.1,Authors: Dayvid Oliveira
0.8.1,Christos Aridas
0.8.1,Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.8.1,License: MIT
0.8.1,%%
0.8.1,%% [markdown]
0.8.1,Generate the dataset
0.8.1,--------------------
0.8.1,
0.8.1,"First, we will generate a dataset and convert it to a"
0.8.1,:class:`~pandas.DataFrame` with arbitrary column names. We will plot the
0.8.1,original dataset.
0.8.1,%%
0.8.1,%% [markdown]
0.8.1,Make a dataset imbalanced
0.8.1,-------------------------
0.8.1,
0.8.1,"Now, we will show the helpers :func:`~imblearn.datasets.make_imbalance`"
0.8.1,that is useful to random select a subset of samples. It will impact the
0.8.1,class distribution as specified by the parameters.
0.8.1,%%
0.8.1,%%
0.8.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.8.1,License: MIT
0.8.1,%%
0.8.1,%% [markdown]
0.8.1,Create an imbalanced dataset
0.8.1,----------------------------
0.8.1,
0.8.1,"First, we will create an imbalanced data set from a the iris data set."
0.8.1,%%
0.8.1,%%
0.8.1,%% [markdown]
0.8.1,Using ``sampling_strategy`` in resampling algorithms
0.8.1,====================================================
0.8.1,
0.8.1,`sampling_strategy` as a `float`
0.8.1,--------------------------------
0.8.1,
0.8.1,`sampling_strategy` can be given a `float`. For **under-sampling
0.8.1,"methods**, it corresponds to the ratio :math:`\\alpha_{us}` defined by"
0.8.1,:math:`N_{rM} = \\alpha_{us} \\times N_{m}` where :math:`N_{rM}` and
0.8.1,:math:`N_{m}` are the number of samples in the majority class after
0.8.1,"resampling and the number of samples in the minority class, respectively."
0.8.1,%%
0.8.1,select only 2 classes since the ratio make sense in this case
0.8.1,%%
0.8.1,%% [markdown]
0.8.1,"For **over-sampling methods**, it correspond to the ratio"
0.8.1,:math:`\\alpha_{os}` defined by :math:`N_{rm} = \\alpha_{os} \\times N_{M}`
0.8.1,where :math:`N_{rm}` and :math:`N_{M}` are the number of samples in the
0.8.1,minority class after resampling and the number of samples in the majority
0.8.1,"class, respectively."
0.8.1,%%
0.8.1,%% [markdown]
0.8.1,`sampling_strategy` has a `str`
0.8.1,-------------------------------
0.8.1,
0.8.1,`sampling_strategy` can be given as a string which specify the class
0.8.1,"targeted by the resampling. With under- and over-sampling, the number of"
0.8.1,samples will be equalized.
0.8.1,
0.8.1,Note that we are using multiple classes from now on.
0.8.1,%%
0.8.1,%% [markdown]
0.8.1,"With **cleaning method**, the number of samples in each class will not be"
0.8.1,equalized even if targeted.
0.8.1,%%
0.8.1,%% [markdown]
0.8.1,`sampling_strategy as a `dict`
0.8.1,------------------------------
0.8.1,
0.8.1,"When `sampling_strategy` is a `dict`, the keys correspond to the targeted"
0.8.1,classes. The values correspond to the desired number of samples for each
0.8.1,targeted class. This is working for both **under- and over-sampling**
0.8.1,algorithms but not for the **cleaning algorithms**. Use a `list` instead.
0.8.1,%%
0.8.1,%% [markdown]
0.8.1,`sampling_strategy` as a `list`
0.8.1,-------------------------------
0.8.1,
0.8.1,"When `sampling_strategy` is a `list`, the list contains the targeted"
0.8.1,classes. It is used only for **cleaning methods** and raise an error
0.8.1,otherwise.
0.8.1,%%
0.8.1,%% [markdown]
0.8.1,`sampling_strategy` as a callable
0.8.1,---------------------------------
0.8.1,
0.8.1,"When callable, function taking `y` and returns a `dict`. The keys"
0.8.1,correspond to the targeted classes. The values correspond to the desired
0.8.1,number of samples for each class.
0.8.1,%%
0.8.1,List of whitelisted modules and methods; regexp are supported.
0.8.1,These docstrings will fail because they are inheriting from scikit-learn
0.8.1,skip private classes
0.8.1,"We ignore following error code,"
0.8.1,- RT02: The first line of the Returns section
0.8.1,"should contain only the type, .."
0.8.1,(as we may need refer to the name of the returned
0.8.1,object)
0.8.1,- GL01: Docstring text (summary) should start in the line
0.8.1,"immediately after the opening quotes (not in the same line,"
0.8.1,or leaving a blank line in between)
0.8.1,Following codes are only taken into account for the
0.8.1,top level class docstrings:
0.8.1,- ES01: No extended summary found
0.8.1,- SA01: See Also section not found
0.8.1,- EX01: No examples section found
0.8.1,In particular we can't parse the signature of properties
0.8.1,"When applied to classes, detect class method. For functions"
0.8.1,method = None.
0.8.1,TODO: this detection can be improved. Currently we assume that we have
0.8.1,class # methods if the second path element before last is in camel case.
0.8.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.8.1,License: MIT
0.8.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.8.1,Christos Aridas
0.8.1,License: MIT
0.8.1,we need to overwrite SamplerMixin.fit to bypass the validation
0.8.1,Adapted from scikit-learn
0.8.1,Author: Edouard Duchesnay
0.8.1,Gael Varoquaux
0.8.1,Virgile Fritsch
0.8.1,Alexandre Gramfort
0.8.1,Lars Buitinck
0.8.1,Christos Aridas
0.8.1,Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.8.1,License: BSD
0.8.1,BaseEstimator interface
0.8.1,validate names
0.8.1,validate estimators
0.8.1,We allow last estimator to be None as an identity transformation
0.8.1,Estimator interface
0.8.1,Setup the memory
0.8.1,joblib >= 0.12
0.8.1,Fit or load from cache the current transformer
0.8.1,Replace the transformer of the step with the fitted
0.8.1,transformer. This is necessary when loading the transformer
0.8.1,from the cache.
0.8.1,"# FIXME: When we get Python 3.7 as minimal version, we will need to switch to"
0.8.1,# the following solution:
0.8.1,# https://snarky.ca/lazy-importing-in-python-3-7/
0.8.1,Import the target module and insert it into the parent's namespace
0.8.1,Update this object's dict so that if someone keeps a reference to the
0.8.1,"LazyLoader, lookups are efficient (__getattr__ is only called on"
0.8.1,lookups that fail).
0.8.1,delay the import of keras since we are going to import either tensorflow
0.8.1,or keras
0.8.1,Based on NiLearn package
0.8.1,License: simplified BSD
0.8.1,"PEP0440 compatible formatted version, see:"
0.8.1,https://www.python.org/dev/peps/pep-0440/
0.8.1,
0.8.1,Generic release markers:
0.8.1,X.Y
0.8.1,X.Y.Z # For bugfix releases
0.8.1,
0.8.1,Admissible pre-release markers:
0.8.1,X.YaN # Alpha release
0.8.1,X.YbN # Beta release
0.8.1,X.YrcN # Release Candidate
0.8.1,X.Y # Final release
0.8.1,
0.8.1,Dev branch marker is: 'X.Y.dev' or 'X.Y.devN' where N is an integer.
0.8.1,'X.Y.dev0' is the canonical version of 'X.Y.dev'
0.8.1,
0.8.1,coding: utf-8
0.8.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.8.1,Dariusz Brzezinski
0.8.1,License: MIT
0.8.1,Only negative labels
0.8.1,"Calculate tp_sum, pred_sum, true_sum ###"
0.8.1,labels are now from 0 to len(labels) - 1 -> use bincount
0.8.1,Pathological case
0.8.1,Compute the true negative
0.8.1,Retain only selected labels
0.8.1,"Finally, we have all our sufficient statistics. Divide! #"
0.8.1,"Divide, and on zero-division, set scores to 0 and warn:"
0.8.1,"Oddly, we may get an ""invalid"" rather than a ""divide"" error"
0.8.1,here.
0.8.1,Average the results
0.8.1,labels are now from 0 to len(labels) - 1 -> use bincount
0.8.1,Pathological case
0.8.1,Retain only selected labels
0.8.1,old version of scipy return MaskedConstant instead of 0.0
0.8.1,check that the scoring function does not need a score
0.8.1,and only a prediction
0.8.1,We do not support multilabel so the only average supported
0.8.1,is binary
0.8.1,Compute the different metrics
0.8.1,Precision/recall/f1
0.8.1,Specificity
0.8.1,Geometric mean
0.8.1,Index balanced accuracy
0.8.1,compute averages
0.8.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.8.1,License: MIT
0.8.1,categories are expected to be encoded from 0 to n_categories - 1
0.8.1,"list of length n_features of ndarray (n_categories, n_classes)"
0.8.1,compute the counts
0.8.1,normalize by the summing over the classes
0.8.1,silence potential warning due to in-place division by zero
0.8.1,coding: utf-8
0.8.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.8.1,Christos Aridas
0.8.1,License: MIT
0.8.1,##############################################################################
0.8.1,Utilities for testing
0.8.1,import some data to play with
0.8.1,restrict to a binary classification task
0.8.1,add noisy features to make the problem harder and avoid perfect results
0.8.1,"run classifier, get class probabilities and label predictions"
0.8.1,only interested in probabilities of the positive case
0.8.1,XXX: do we really want a special API for the binary case?
0.8.1,##############################################################################
0.8.1,Tests
0.8.1,detailed measures for each class
0.8.1,individual scoring function that can be used for grid search: in the
0.8.1,binary class case the score is the value of the measure for the positive
0.8.1,class (e.g. label == 1). This is deprecated for average != 'binary'.
0.8.1,Such a case may occur with non-stratified cross-validation
0.8.1,ensure the above were meaningful tests:
0.8.1,Bad pos_label
0.8.1,Bad average option
0.8.1,but average != 'binary'; even if data is binary
0.8.1,compute the geometric mean for the binary problem
0.8.1,print classification report with class names
0.8.1,print classification report with label detection
0.8.1,print classification report with class names
0.8.1,print classification report with label detection
0.8.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.8.1,License: MIT
0.8.1,Check basic feature of the metric:
0.8.1,"* the shape of the distance matrix is (n_samples, n_samples)"
0.8.1,* computing pairwise distance of X is the same than explicitely between
0.8.1,X and X.
0.8.1,Check the property of the vdm distance. Let's check the property
0.8.1,"described in ""Improved Heterogeneous Distance Functions"", D.R. Wilson and"
0.8.1,"T.R. Martinez, Journal of Artificial Intelligence Research 6 (1997) 1-34"
0.8.1,https://arxiv.org/pdf/cs/9701101.pdf
0.8.1,
0.8.1,"""if an attribute color has three values red, green and blue, and the"
0.8.1,"application is to identify whether or not an object is an apple, red and"
0.8.1,green would be considered closer than red and blue because the former two
0.8.1,"both have similar correlations with the output class apple."""
0.8.1,defined our feature
0.8.1,0 - not an apple / 1 - an apple
0.8.1,computing the distance between a sample of the same category should
0.8.1,give a null distance
0.8.1,check the property explained in the introduction example
0.8.1,green and red are very close
0.8.1,blue is closer to red than green
0.8.1,"Check that ""auto"" is equivalent to provide the number categories"
0.8.1,beforehand
0.8.1,Check that we raise an error if n_categories is inconsistent with the
0.8.1,number of features in X
0.8.1,Check that we don't get issue when a category is missing between 0
0.8.1,n_categories - 1
0.8.1,remove a categories that could be between 0 and n_categories
0.8.1,Check that we raise a NotFittedError when `fit` is not not called before
0.8.1,pairwise.
0.8.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.8.1,Christos Aridas
0.8.1,License: MIT
0.8.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.8.1,Christos Aridas
0.8.1,License: MIT
0.8.1,The ratio is computed using a one-vs-rest manner. Using majority
0.8.1,in multi-class would lead to slightly different results at the
0.8.1,cost of introducing a new parameter.
0.8.1,rounding may cause new amount for n_samples
0.8.1,the nearest neighbors need to be fitted only on the current class
0.8.1,to find the class NN to generate new samples
0.8.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.8.1,Christos Aridas
0.8.1,License: MIT
0.8.1,smoothed bootstrap imposes to make numerical operation; we need
0.8.1,to be sure to have only numerical data in X
0.8.1,generate a smoothed bootstrap with a perturbation
0.8.1,generate a bootstrap
0.8.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.8.1,Christos Aridas
0.8.1,License: MIT
0.8.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.8.1,Fernando Nogueira
0.8.1,Christos Aridas
0.8.1,License: MIT
0.8.1,validate the parameters
0.8.1,negate diagonal elements
0.8.1,identify cluster which are answering the requirements
0.8.1,the cluster is already considered balanced
0.8.1,not enough samples to apply SMOTE
0.8.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.8.1,Fernando Nogueira
0.8.1,Christos Aridas
0.8.1,Dzianis Dudnik
0.8.1,License: MIT
0.8.1,divergence between borderline-1 and borderline-2
0.8.1,Create synthetic samples for borderline points.
0.8.1,only minority
0.8.1,we use a one-vs-rest policy to handle the multiclass in which
0.8.1,new samples will be created considering not only the majority
0.8.1,class but all over classes.
0.8.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.8.1,Fernando Nogueira
0.8.1,Christos Aridas
0.8.1,Dzianis Dudnik
0.8.1,License: MIT
0.8.1,np.newaxis for backwards compatability with random_state
0.8.1,Samples are in danger for m/2 <= m' < m
0.8.1,Samples are noise for m = m'
0.8.1,compute the median of the standard deviation of the minority class
0.8.1,the input of the OneHotEncoder needs to be dense
0.8.1,we can replace the 1 entries of the categorical features with the
0.8.1,median of the standard deviation. It will ensure that whenever
0.8.1,"distance is computed between 2 samples, the difference will be equal"
0.8.1,to the median of the standard deviation as in the original paper.
0.8.1,"In the edge case where the median of the std is equal to 0, the 1s"
0.8.1,"entries will be also nullified. In this case, we store the original"
0.8.1,categorical encoding which will be later used for inversing the OHE
0.8.1,reverse the encoding of the categorical features
0.8.1,the matrix is supposed to be in the CSR format after the stacking
0.8.1,change in sparsity structure more efficient with LIL than CSR
0.8.1,convert to dense array since scipy.sparse doesn't handle 3D
0.8.1,"In the case that the median std was equal to zeros, we have to"
0.8.1,create non-null entry based on the encoded of OHE
0.8.1,tie breaking argmax
0.8.1,generate sample indices that will be used to generate new samples
0.8.1,"for each drawn samples, select its k-neighbors and generate a sample"
0.8.1,"where for each feature individually, each category generated is the"
0.8.1,most common category
0.8.1,the kneigbors search will include the sample itself which is
0.8.1,expected from the original algorithm
0.8.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.8.1,Christos Aridas
0.8.1,Dzianis Dudnik
0.8.1,License: MIT
0.8.1,create 2 random continuous feature
0.8.1,create a categorical feature using some string
0.8.1,create a categorical feature using some integer
0.8.1,return the categories
0.8.1,create 2 random continuous feature
0.8.1,create a categorical feature using some string
0.8.1,create a categorical feature using some integer
0.8.1,return the categories
0.8.1,create 2 random continuous feature
0.8.1,create a categorical feature using some string
0.8.1,create a categorical feature using some integer
0.8.1,return the categories
0.8.1,create 2 random continuous feature
0.8.1,create a categorical feature using some string
0.8.1,create a categorical feature using some integer
0.8.1,return the categories
0.8.1,create 2 random continuous feature
0.8.1,create a categorical feature using some string
0.8.1,create a categorical feature using some integer
0.8.1,part of the common test which apply to SMOTE-NC even if it is not default
0.8.1,constructible
0.8.1,Check that the samplers handle pandas dataframe and pandas series
0.8.1,Cast X and y to not default dtype
0.8.1,Non-regression test for #662
0.8.1,https://github.com/scikit-learn-contrib/imbalanced-learn/issues/662
0.8.1,check that the categorical feature is not random but correspond to the
0.8.1,categories seen in the minority class samples
0.8.1,overall check for SMOTEN
0.8.1,check if the SMOTEN resample data as expected
0.8.1,"we generate data such that ""not apple"" will be the minority class and"
0.8.1,"samples from this class will be generated. We will force the ""blue"""
0.8.1,"category to be associated with this class. Therefore, the new generated"
0.8.1,"samples should as well be from the ""blue"" category."
0.8.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.8.1,Christos Aridas
0.8.1,License: MIT
0.8.1,check that m_neighbors is properly set. Regression test for:
0.8.1,https://github.com/scikit-learn-contrib/imbalanced-learn/issues/568
0.8.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.8.1,Christos Aridas
0.8.1,License: MIT
0.8.1,check the random over-sampling with a multiclass problem
0.8.1,check that resampling with heterogeneous dtype is working with basic
0.8.1,resampling
0.8.1,check that we can oversample even with missing or infinite data
0.8.1,regression tests for #605
0.8.1,check that we raise an error when heterogeneous dtype data are given
0.8.1,and a smoothed bootstrap is requested
0.8.1,check that smoothed bootstrap is working for numerical array
0.8.1,check that a shrinkage factor of 0 is equivalent to not create a smoothed
0.8.1,bootstrap
0.8.1,check the behaviour of the shrinkage parameter
0.8.1,the covariance of the data generated with the larger shrinkage factor
0.8.1,should also be larger.
0.8.1,check the validation of the shrinkage parameter
0.8.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.8.1,Christos Aridas
0.8.1,License: MIT
0.8.1,shuffle the indices since the sampler are packing them by class
0.8.1,helper functions
0.8.1,input and output
0.8.1,build the model and weights
0.8.1,"build the loss, predict, and train operator"
0.8.1,Initialization of all variables in the graph
0.8.1,"For each epoch, run accuracy on train and test"
0.8.1,helper functions
0.8.1,input and output
0.8.1,build the model and weights
0.8.1,"build the loss, predict, and train operator"
0.8.1,Initialization of all variables in the graph
0.8.1,"For each epoch, run accuracy on train and test"
0.8.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.8.1,License: MIT
0.8.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.8.1,Fernando Nogueira
0.8.1,Christos Aridas
0.8.1,License: MIT
0.8.1,find which class to not consider
0.8.1,there is a Tomek link between two samples if they are both nearest
0.8.1,neighbors of each others.
0.8.1,Find the nearest neighbour of every point
0.8.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.8.1,Christos Aridas
0.8.1,License: MIT
0.8.1,Randomly get one sample from the majority class
0.8.1,Generate the index to select
0.8.1,Create the set C - One majority samples and all minority
0.8.1,Create the set S - all majority samples
0.8.1,fit knn on C
0.8.1,Check each sample in S if we keep it or drop it
0.8.1,Do not select sample which are already well classified
0.8.1,Classify on S
0.8.1,If the prediction do not agree with the true label
0.8.1,append it in C_x
0.8.1,Keep the index for later
0.8.1,Update C
0.8.1,fit a knn on C
0.8.1,This experimental to speed up the search
0.8.1,Classify all the element in S and avoid to test the
0.8.1,well classified elements
0.8.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.8.1,Dayvid Oliveira
0.8.1,Christos Aridas
0.8.1,License: MIT
0.8.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.8.1,Christos Aridas
0.8.1,License: MIT
0.8.1,Compute the distance considering the farthest neighbour
0.8.1,Sort the list of distance and get the index
0.8.1,Throw a warning to tell the user that we did not have enough samples
0.8.1,to select and that we just select everything
0.8.1,Select the desired number of samples
0.8.1,idx_tmp is relative to the feature selected in the
0.8.1,previous step and we need to find the indirection
0.8.1,fmt: off
0.8.1,fmt: on
0.8.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.8.1,Christos Aridas
0.8.1,License: MIT
0.8.1,select a sample from the current class
0.8.1,create the set composed of all minority samples and one
0.8.1,sample from the current class.
0.8.1,create the set S with removing the seed from S
0.8.1,since that it will be added anyway
0.8.1,apply Tomek cleaning
0.8.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.8.1,Dayvid Oliveira
0.8.1,Christos Aridas
0.8.1,License: MIT
0.8.1,Check the stopping criterion
0.8.1,1. If there is no changes for the vector y
0.8.1,2. If the number of samples in the other class become inferior to
0.8.1,the number of samples in the majority class
0.8.1,3. If one of the class is disappearing
0.8.1,Case 1
0.8.1,Case 2
0.8.1,Case 3
0.8.1,Check the stopping criterion
0.8.1,1. If the number of samples in the other class become inferior to
0.8.1,the number of samples in the majority class
0.8.1,2. If one of the class is disappearing
0.8.1,Case 1else:
0.8.1,overwrite b_min_bec_maj
0.8.1,Case 2
0.8.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.8.1,Christos Aridas
0.8.1,License: MIT
0.8.1,clean the neighborhood
0.8.1,compute which classes to consider for cleaning for the A2 group
0.8.1,compute a2 group
0.8.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.8.1,Christos Aridas
0.8.1,License: MIT
0.8.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.8.1,Christos Aridas
0.8.1,License: MIT
0.8.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.8.1,Christos Aridas
0.8.1,License: MIT
0.8.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.8.1,Christos Aridas
0.8.1,License: MIT
0.8.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.8.1,Christos Aridas
0.8.1,License: MIT
0.8.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.8.1,Christos Aridas
0.8.1,License: MIT
0.8.1,check that we can undersample even with missing or infinite data
0.8.1,regression tests for #605
0.8.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.8.1,Christos Aridas
0.8.1,License: MIT
0.8.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.8.1,Christos Aridas
0.8.1,License: MIT
0.8.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.8.1,Christos Aridas
0.8.1,License: MIT
0.8.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.8.1,Christos Aridas
0.8.1,License: MIT
0.8.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.8.1,Christos Aridas
0.8.1,License: MIT
0.8.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.8.1,Fernando Nogueira
0.8.1,Christos Aridas
0.8.1,License: MIT
0.8.1,check that we deprecate the `n_jobs` parameter.
0.8.1,check that the samples selecting by the hard voting corresponds to the
0.8.1,targeted class
0.8.1,non-regression test for:
0.8.1,https://github.com/scikit-learn-contrib/imbalanced-learn/issues/738
0.8.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.8.1,Christos Aridas
0.8.1,License: MIT
0.8.1,test that all_estimators doesn't find abstract classes.
0.8.1,"For NearMiss, let's check the three algorithms"
0.8.1,Common tests for estimator instances
0.8.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.8.1,License: MIT
0.8.1,check that we can let a pass a regression variable by turning down the
0.8.1,validation
0.8.1,Check that the validation is bypass when calling `fit`
0.8.1,Non-regression test for:
0.8.1,https://github.com/scikit-learn-contrib/imbalanced-learn/issues/782
0.8.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.8.1,Christos Aridas
0.8.1,License: MIT
0.8.1,store timestamp to figure out whether the result of 'fit' has been
0.8.1,cached or not
0.8.1,store timestamp to figure out whether the result of 'fit' has been
0.8.1,cached or not
0.8.1,Pipeline accepts steps as tuple
0.8.1,Test the various init parameters of the pipeline.
0.8.1,Check that we can't instantiate pipelines with objects without fit
0.8.1,method
0.8.1,Smoke test with only an estimator
0.8.1,Check that params are set
0.8.1,Smoke test the repr:
0.8.1,Test with two objects
0.8.1,Check that we can't instantiate with non-transformers on the way
0.8.1,"Note that NoTrans implements fit, but not transform"
0.8.1,Check that params are set
0.8.1,Smoke test the repr:
0.8.1,Check that params are not set when naming them wrong
0.8.1,Test clone
0.8.1,"Check that apart from estimators, the parameters are the same"
0.8.1,Remove estimators that where copied
0.8.1,Test the various methods of the pipeline (anova).
0.8.1,Test with Anova + LogisticRegression
0.8.1,Test that the pipeline can take fit parameters
0.8.1,classifier should return True
0.8.1,and transformer params should not be changed
0.8.1,invalid parameters should raise an error message
0.8.1,Pipeline should pass sample_weight
0.8.1,When sample_weight is None it shouldn't be passed
0.8.1,Test pipeline raises set params error message for nested models.
0.8.1,nested model check
0.8.1,Test the various methods of the pipeline (pca + svm).
0.8.1,Test with PCA + SVC
0.8.1,Test the various methods of the pipeline (preprocessing + svm).
0.8.1,check shapes of various prediction functions
0.8.1,test that the fit_predict method is implemented on a pipeline
0.8.1,test that the fit_predict on pipeline yields same results as applying
0.8.1,transform and clustering steps separately
0.8.1,"As pipeline doesn't clone estimators on construction,"
0.8.1,it must have its own estimators
0.8.1,first compute the transform and clustering step separately
0.8.1,use a pipeline to do the transform and clustering in one step
0.8.1,tests that a pipeline does not have fit_predict method when final
0.8.1,step of pipeline does not have fit_predict defined
0.8.1,tests that Pipeline passes fit_params to intermediate steps
0.8.1,when fit_predict is invoked
0.8.1,Test whether pipeline works with a transformer at the end.
0.8.1,Also test pipeline.transform and pipeline.inverse_transform
0.8.1,test transform and fit_transform:
0.8.1,Test whether pipeline works with a transformer missing fit_transform
0.8.1,test fit_transform:
0.8.1,Directly setting attr
0.8.1,Using set_params
0.8.1,Using set_params to replace single step
0.8.1,With invalid data
0.8.1,Test setting Pipeline steps to None
0.8.1,"for other methods, ensure no AttributeErrors on None:"
0.8.1,mult2 and mult3 are active
0.8.1,Check 'passthrough' step at construction time
0.8.1,Test that an error is raised when memory is not a string or a Memory
0.8.1,instance
0.8.1,Define memory as an integer
0.8.1,Test with Transformer + SVC
0.8.1,Memoize the transformer at the first fit
0.8.1,Get the time stamp of the tranformer in the cached pipeline
0.8.1,Check that cached_pipe and pipe yield identical results
0.8.1,Check that we are reading the cache while fitting
0.8.1,a second time
0.8.1,Check that cached_pipe and pipe yield identical results
0.8.1,Create a new pipeline with cloned estimators
0.8.1,Check that even changing the name step does not affect the cache hit
0.8.1,Check that cached_pipe and pipe yield identical results
0.8.1,Test with Transformer + SVC
0.8.1,Memoize the transformer at the first fit
0.8.1,Get the time stamp of the tranformer in the cached pipeline
0.8.1,Check that cached_pipe and pipe yield identical results
0.8.1,Check that we are reading the cache while fitting
0.8.1,a second time
0.8.1,Check that cached_pipe and pipe yield identical results
0.8.1,Create a new pipeline with cloned estimators
0.8.1,Check that even changing the name step does not affect the cache hit
0.8.1,Check that cached_pipe and pipe yield identical results
0.8.1,Test the various methods of the pipeline (pca + svm).
0.8.1,Test with PCA + SVC
0.8.1,Test the various methods of the pipeline (pca + svm).
0.8.1,Test with PCA + SVC
0.8.1,Test whether pipeline works with a sampler at the end.
0.8.1,Also test pipeline.sampler
0.8.1,test transform and fit_transform:
0.8.1,We round the value near to zero. It seems that PCA has some issue
0.8.1,with that
0.8.1,Test whether pipeline works with a sampler at the end.
0.8.1,Also test pipeline.sampler
0.8.1,Test pipeline using None as preprocessing step and a classifier
0.8.1,"Test pipeline using None, RUS and a classifier"
0.8.1,"Test pipeline using RUS, None and a classifier"
0.8.1,Test pipeline using None step and a sampler
0.8.1,Test pipeline using None and a transformer that implements transform and
0.8.1,inverse_transform
0.8.1,Test the various methods of the pipeline (anova).
0.8.1,Test with RandomUnderSampling + Anova + LogisticRegression
0.8.1,Test the various methods of the pipeline (anova).
0.8.1,Test the various methods of the pipeline (anova).
0.8.1,Test with RandomUnderSampling + Anova + LogisticRegression
0.8.1,tests that Pipeline passes predict_params to the final estimator
0.8.1,when predict is invoked
0.8.1,Test that the score_samples method is implemented on a pipeline.
0.8.1,Test that the score_samples method on pipeline yields same results as
0.8.1,applying transform and score_samples steps separately.
0.8.1,Check the shapes
0.8.1,Check the values
0.8.1,Test that a pipeline does not have score_samples method when the final
0.8.1,step of the pipeline does not have score_samples defined.
0.8.1,Test that the score_samples method is implemented on a pipeline.
0.8.1,Test that the score_samples method on pipeline yields same results as
0.8.1,applying transform and score_samples steps separately.
0.8.1,Check the shapes
0.8.1,Check the values
0.8.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.8.1,Christos Aridas
0.8.1,License: MIT
0.8.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.8.1,License: MIT
0.8.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.8.1,License: MIT
0.8.1,Adapated from scikit-learn
0.8.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.8.1,License: MIT
0.8.1,we don't filter samplers based on their tag here because we want to make
0.8.1,sure that the fitted attribute does not exist if the tag is not
0.8.1,stipulated
0.8.1,trigger our checks if this is a SamplerMixin
0.8.1,should raise warning if the target is continuous (we cannot raise error)
0.8.1,if the target is multilabel then we should raise an error
0.8.1,IHT does not enforce the number of samples but provide a number
0.8.1,of samples the closest to the desired target.
0.8.1,in this test we will force all samplers to not change the class 1
0.8.1,check that sparse matrices can be passed through the sampler leading to
0.8.1,the same results than dense
0.8.1,Check that the samplers handle pandas dataframe and pandas series
0.8.1,check that we return the same type for dataframes or series types
0.8.1,Check that the can samplers handle simple lists
0.8.1,Check that multiclass target lead to the same results than OVA encoding
0.8.1,Cast X and y to not default dtype
0.8.1,Non-regression test for #709
0.8.1,https://github.com/scikit-learn-contrib/imbalanced-learn/issues/709
0.8.1,Adapted from scikit-learn
0.8.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.8.1,License: MIT
0.8.1,Ignore deprecation warnings triggered at import time and from walking
0.8.1,packages
0.8.1,get rid of abstract base classes
0.8.1,get rid of sklearn estimators which have been imported in some classes
0.8.1,"drop duplicates, sort for reproducibility"
0.8.1,itemgetter is used to ensure the sort does not extend to the 2nd item of
0.8.1,the tuple
0.8.1,Author: Alexander L. Hayes <hayesall@iu.edu>
0.8.1,License: MIT
0.8.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.8.1,License: MIT
0.8.1,check that all keys in sampling_strategy are also in y
0.8.1,check that there is no negative number
0.8.1,check that all keys in sampling_strategy are also in y
0.8.1,ignore first 'self' argument for instance methods
0.8.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.8.1,Christos Aridas
0.8.1,License: MIT
0.8.1,this function could create an equal number of samples
0.8.1,We pass on purpose a non sorted dictionary and check that the resulting
0.8.1,dictionary is sorted. Refer to issue #428.
0.8.1,DataFrame and DataFrame case
0.8.1,DataFrames and Series case
0.8.1,The * is place before a keyword only argument without a default value
0.8.1,Author: Alexander L. Hayes <hayesall@iu.edu>
0.8.1,License: MIT
0.8.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.8.1,License: MIT
0.8.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.8.1,Christos Aridas
0.8.1,License: MIT
0.8.1,check if the filtering is working with a list or a single string
0.8.1,check that all estimators are sampler
0.8.1,check that an error is raised when the type is unknown
0.8.1,TODO: remove in 0.9
0.8.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.8.1,License: MIT
0.8.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.8.1,Christos Aridas
0.8.1,License: MIT
0.8.1,Otherwise create a default SMOTE
0.8.1,Otherwise create a default TomekLinks
0.8.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.8.1,Christos Aridas
0.8.1,License: MIT
0.8.1,Otherwise create a default SMOTE
0.8.1,Otherwise create a default EditedNearestNeighbours
0.8.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.8.1,Christos Aridas
0.8.1,License: MIT
0.8.1,Check if default job count is None
0.8.1,Check if job count is set
0.8.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.8.1,Christos Aridas
0.8.1,License: MIT
0.8.1,Check if default job count is none
0.8.1,Check if job count is set
0.8.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.8.1,License: MIT
0.8.1,resample before to fit the tree
0.8.1,Validate or convert input data
0.8.1,Pre-sort indices to avoid that each individual tree of the
0.8.1,ensemble sorts the indices.
0.8.1,reshape is necessary to preserve the data contiguity against vs
0.8.1,"[:, np.newaxis] that does not."
0.8.1,Get bootstrap sample size
0.8.1,Check parameters
0.8.1,"Free allocated memory, if any"
0.8.1,We draw from the random state to get the random state we
0.8.1,would have got if we hadn't used a warm_start.
0.8.1,Parallel loop: we prefer the threading backend as the Cython code
0.8.1,for fitting the trees is internally releasing the Python GIL
0.8.1,making threading more efficient than multiprocessing in
0.8.1,"that case. However, we respect any parallel_backend contexts set"
0.8.1,"at a higher level, since correctness does not rely on using"
0.8.1,threads.
0.8.1,Collect newly grown trees
0.8.1,Create pipeline with the fitted samplers and trees
0.8.1,Decapsulate classes_ attributes
0.8.1,"with the resampling, we are likely to have rows not included"
0.8.1,for the OOB score leading to division by zero
0.8.1,Instances incorrectly classified
0.8.1,Error fraction
0.8.1,Stop if classification is perfect
0.8.1,Construct y coding as described in Zhu et al [2]:
0.8.1,
0.8.1,y_k = 1 if c == k else -1 / (K - 1)
0.8.1,
0.8.1,"where K == n_classes_ and c, k in [0, K) are indices along the second"
0.8.1,axis of the y coding with c being the index corresponding to the true
0.8.1,class label.
0.8.1,Displace zero probabilities so the log is defined.
0.8.1,Also fix negative elements which may occur with
0.8.1,negative sample weights.
0.8.1,Boost weight using multi-class AdaBoost SAMME.R alg
0.8.1,Only boost the weights if it will fit again
0.8.1,Only boost positive weights
0.8.1,Instances incorrectly classified
0.8.1,Error fraction
0.8.1,Stop if classification is perfect
0.8.1,Stop if the error is at least as bad as random guessing
0.8.1,Boost weight using multi-class AdaBoost SAMME alg
0.8.1,Only boost the weights if I will fit again
0.8.1,Only boost positive weights
0.8.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.8.1,Christos Aridas
0.8.1,License: MIT
0.8.1,RandomUnderSampler is not supporting sample_weight. We need to pass
0.8.1,None.
0.8.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.8.1,Christos Aridas
0.8.1,License: MIT
0.8.1,the sampler needs to be validated before to call _fit because
0.8.1,_validate_y is called before _validate_estimator and would require
0.8.1,to know which type of sampler we are using.
0.8.1,RandomUnderSampler is not supporting sample_weight. We need to pass
0.8.1,None.
0.8.1,check that we have an ensemble of samplers and estimators with a
0.8.1,consistent size
0.8.1,each sampler in the ensemble should have different random state
0.8.1,each estimator in the ensemble should have different random state
0.8.1,check the consistency of the feature importances
0.8.1,check the consistency of the prediction outpus
0.8.1,Predictions should be the same when sample_weight are all ones
0.8.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.8.1,Christos Aridas
0.8.1,License: MIT
0.8.1,Check classification for various parameter settings.
0.8.1,Test that bootstrapping samples generate non-perfect base estimators.
0.8.1,"without bootstrap, all trees are perfect on the training set"
0.8.1,disable the resampling by passing an empty dictionary.
0.8.1,"with bootstrap, trees are no longer perfect on the training set"
0.8.1,Test that bootstrapping features may generate duplicate features.
0.8.1,Predict probabilities.
0.8.1,Normal case
0.8.1,"Degenerate case, where some classes are missing"
0.8.1,Check that oob prediction is a good estimation of the generalization
0.8.1,error.
0.8.1,Test with few estimators
0.8.1,Check singleton ensembles.
0.8.1,Test that it gives proper exception on deficient input.
0.8.1,Test support of decision_function
0.8.1,Check that bagging ensembles can be grid-searched.
0.8.1,Transform iris into a binary classification task
0.8.1,Grid search with scoring based on decision_function
0.8.1,Check base_estimator and its default values.
0.8.1,Test if fitting incrementally with warm start gives a forest of the
0.8.1,right size and the same results as a normal fit.
0.8.1,Test if warm start'ed second fit with smaller n_estimators raises error.
0.8.1,Test that nothing happens when fitting without increasing n_estimators
0.8.1,"modify X to nonsense values, this should not change anything"
0.8.1,warm started classifier with 5+5 estimators should be equivalent to
0.8.1,one classifier with 10 estimators
0.8.1,Check using oob_score and warm_start simultaneously fails
0.8.1,"Make sure OOB scores are identical when random_state, estimator, and"
0.8.1,training data are fixed and fitting is done twice
0.8.1,Check that format of estimators_samples_ is correct and that results
0.8.1,generated at fit time can be identically reproduced at a later time
0.8.1,using data saved in object attributes.
0.8.1,remap the y outside of the BalancedBaggingclassifier
0.8.1,"_, y = np.unique(y, return_inverse=True)"
0.8.1,Get relevant attributes
0.8.1,Test for correct formatting
0.8.1,Re-fit single estimator to test for consistent sampling
0.8.1,Make sure validated max_samples and original max_samples are identical
0.8.1,when valid integer max_samples supplied by user
0.8.1,check that we can pass any kind of sampler to a bagging classifier
0.8.1,check that we have balanced class with the right counts of class
0.8.1,sample depending on the sampling strategy
0.8.1,check that we can provide a FunctionSampler in BalancedBaggingClassifier
0.8.1,find the minority and majority classes
0.8.1,compute the number of sample to draw from the majority class using
0.8.1,a negative binomial distribution
0.8.1,draw randomly with or without replacement
0.8.1,Roughly Balanced Bagging
0.8.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.8.1,Christos Aridas
0.8.1,License: MIT
0.8.1,Generate a global dataset to use
0.8.1,Check classification for various parameter settings.
0.8.1,test the different prediction function
0.8.1,Check base_estimator and its default values.
0.8.1,Test if fitting incrementally with warm start gives a forest of the
0.8.1,right size and the same results as a normal fit.
0.8.1,Test if warm start'ed second fit with smaller n_estimators raises error.
0.8.1,Test that nothing happens when fitting without increasing n_estimators
0.8.1,"modify X to nonsense values, this should not change anything"
0.8.1,warm started classifier with 5+5 estimators should be equivalent to
0.8.1,one classifier with 10 estimators
0.8.1,Check warning if not enough estimators
0.8.1,First fit with no restriction on max samples
0.8.1,Second fit with max samples restricted to just 2
0.8.1,Regression test for #655: check that the oob score is closed to 0.5
0.8.1,a binomial experiment.
0.8.1,Author: Guillaume Lemaitre
0.8.1,License: BSD 3 clause
0.8.1,"The index start at one, then we need to remove one"
0.8.1,to not have issue with the indexing.
0.8.1,go through the list and check if the data are available
0.8.1,Authors: Dayvid Oliveira
0.8.1,Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.8.1,Christos Aridas
0.8.1,License: MIT
0.8.1,restrict ratio to be a dict or a callable
0.8.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.8.1,Christos Aridas
0.8.1,License: MIT
0.8.1,"we are reusing part of utils.check_sampling_strategy, however this is not"
0.8.1,cover in the common tests so we will repeat it here
0.8.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.8.1,Christos Aridas
0.8.1,License: MIT
0.8.1,This is a trick to avoid an error during tests collection with pytest. We
0.8.1,avoid the error when importing the package raise the error at the moment of
0.8.1,creating the instance.
0.8.1,flag for keras sequence duck-typing
0.8.1,shuffle the indices since the sampler are packing them by class
0.8.0,This file is here so that when running from the root folder
0.8.0,./sklearn is added to sys.path by pytest.
0.8.0,See https://docs.pytest.org/en/latest/pythonpath.html for more details.
0.8.0,"For example, this allows to build extensions in place and run pytest"
0.8.0,doc/modules/clustering.rst and use sklearn from the local folder
0.8.0,rather than the one from site-packages.
0.8.0,Set numpy array str/repr to legacy behaviour on numpy > 1.13 to make
0.8.0,the doctests pass
0.8.0,! /usr/bin/env python
0.8.0,get __version__ from _version.py
0.8.0,-*- coding: utf-8 -*-
0.8.0,
0.8.0,"imbalanced-learn documentation build configuration file, created by"
0.8.0,sphinx-quickstart on Mon Jan 18 14:44:12 2016.
0.8.0,
0.8.0,This file is execfile()d with the current directory set to its
0.8.0,containing dir.
0.8.0,
0.8.0,Note that not all possible configuration values are present in this
0.8.0,autogenerated file.
0.8.0,
0.8.0,All configuration values have a default; values that are commented out
0.8.0,serve to show the default.
0.8.0,"If extensions (or modules to document with autodoc) are in another directory,"
0.8.0,add these directories to sys.path here. If the directory is relative to the
0.8.0,"documentation root, use os.path.abspath to make it absolute, like shown here."
0.8.0,-- General configuration ------------------------------------------------
0.8.0,"If your documentation needs a minimal Sphinx version, state it here."
0.8.0,needs_sphinx = '1.0'
0.8.0,"Add any Sphinx extension module names here, as strings. They can be"
0.8.0,extensions coming with Sphinx (named 'sphinx.ext.*') or your custom
0.8.0,ones.
0.8.0,"Add any paths that contain templates here, relative to this directory."
0.8.0,The suffix of source filenames.
0.8.0,The master toctree document.
0.8.0,General information about the project.
0.8.0,"The version info for the project you're documenting, acts as replacement for"
0.8.0,"|version| and |release|, also used in various other places throughout the"
0.8.0,built documents.
0.8.0,
0.8.0,The short X.Y version.
0.8.0,"The full version, including alpha/beta/rc tags."
0.8.0,"List of patterns, relative to source directory, that match files and"
0.8.0,directories to ignore when looking for source files.
0.8.0,The reST default role (used for this markup: `text`) to use for all
0.8.0,documents.
0.8.0,"If true, '()' will be appended to :func: etc. cross-reference text."
0.8.0,The name of the Pygments (syntax highlighting) style to use.
0.8.0,-- Options for math equations -----------------------------------------------
0.8.0,-- Options for HTML output ----------------------------------------------
0.8.0,The theme to use for HTML and HTML Help pages.  See the documentation for
0.8.0,a list of builtin themes.
0.8.0,"""twitter_url"": ""https://twitter.com/pandas_dev"","
0.8.0,"""navbar_align"": ""right"",  # For testing that the navbar items align properly"
0.8.0,"Add any paths that contain custom static files (such as style sheets) here,"
0.8.0,"relative to this directory. They are copied after the builtin static files,"
0.8.0,"so a file named ""default.css"" will overwrite the builtin ""default.css""."
0.8.0,Output file base name for HTML help builder.
0.8.0,-- Options for autodoc ------------------------------------------------------
0.8.0,generate autosummary even if no references
0.8.0,-- Options for numpydoc -----------------------------------------------------
0.8.0,this is needed for some reason...
0.8.0,see https://github.com/numpy/numpydoc/issues/69
0.8.0,-- Options for sphinxcontrib-bibtex -----------------------------------------
0.8.0,bibtex file
0.8.0,-- Options for intersphinx --------------------------------------------------
0.8.0,intersphinx configuration
0.8.0,-- Options for sphinx-gallery -----------------------------------------------
0.8.0,Generate the plot for the gallery
0.8.0,sphinx-gallery configuration
0.8.0,-- Options for github link for what's new -----------------------------------
0.8.0,Config for sphinx_issues
0.8.0,The following is used by sphinx.ext.linkcode to provide links to github
0.8.0,-- Options for LaTeX output ---------------------------------------------
0.8.0,The paper size ('letterpaper' or 'a4paper').
0.8.0,"'papersize': 'letterpaper',"
0.8.0,"The font size ('10pt', '11pt' or '12pt')."
0.8.0,"'pointsize': '10pt',"
0.8.0,Additional stuff for the LaTeX preamble.
0.8.0,"'preamble': '',"
0.8.0,Grouping the document tree into LaTeX files. List of tuples
0.8.0,"(source start file, target name, title,"
0.8.0,"author, documentclass [howto, manual, or own class])."
0.8.0,-- Options for manual page output ---------------------------------------
0.8.0,"If false, no module index is generated."
0.8.0,latex_domain_indices = True
0.8.0,One entry per manual page. List of tuples
0.8.0,"(source start file, name, description, authors, manual section)."
0.8.0,"If true, show URL addresses after external links."
0.8.0,man_show_urls = False
0.8.0,-- Options for Texinfo output -------------------------------------------
0.8.0,Grouping the document tree into Texinfo files. List of tuples
0.8.0,"(source start file, target name, title, author,"
0.8.0,"dir menu entry, description, category)"
0.8.0,-- Additional temporary hacks -----------------------------------------------
0.8.0,Temporary work-around for spacing problem between parameter and parameter
0.8.0,"type in the doc, see https://github.com/numpy/numpydoc/issues/215. The bug"
0.8.0,has been fixed in sphinx (https://github.com/sphinx-doc/sphinx/pull/5976) but
0.8.0,through a change in sphinx basic.css except rtd_theme does not use basic.css.
0.8.0,"In an ideal world, this would get fixed in this PR:"
0.8.0,https://github.com/readthedocs/sphinx_rtd_theme/pull/747/files
0.8.0,get the styles from the current theme
0.8.0,create and add the button to all the code blocks that contain >>>
0.8.0,tracebacks (.gt) contain bare text elements that need to be
0.8.0,wrapped in a span to work with .nextUntil() (see later)
0.8.0,define the behavior of the button when it's clicked
0.8.0,hide the code output
0.8.0,show the code output
0.8.0,-*- coding: utf-8 -*-
0.8.0,Format template for issues URI
0.8.0,e.g. 'https://github.com/sloria/marshmallow/issues/{issue}
0.8.0,Format template for PR URI
0.8.0,e.g. 'https://github.com/sloria/marshmallow/pull/{issue}
0.8.0,Format template for commit URI
0.8.0,e.g. 'https://github.com/sloria/marshmallow/commits/{commit}
0.8.0,"Shortcut for Github, e.g. 'sloria/marshmallow'"
0.8.0,Format template for user profile URI
0.8.0,e.g. 'https://github.com/{user}'
0.8.0,Python 2 only
0.8.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.8.0,License: MIT
0.8.0,%%
0.8.0,%%
0.8.0,"First, we will generate a toy classification dataset with only few samples."
0.8.0,The ratio between the classes will be imbalanced.
0.8.0,%%
0.8.0,%%
0.8.0,"Now, we will use a :class:`~imblearn.over_sampling.RandomOverSampler` to"
0.8.0,generate a bootstrap for the minority class with as many samples as in the
0.8.0,majority class.
0.8.0,%%
0.8.0,%%
0.8.0,We observe that the minority samples are less transparent than the samples
0.8.0,"from the majority class. Indeed, it is due to the fact that these samples"
0.8.0,of the minority class are repeated during the bootstrap generation.
0.8.0,
0.8.0,We can set `shrinkage` to a floating value to add a small perturbation to the
0.8.0,samples created and therefore create a smoothed bootstrap.
0.8.0,%%
0.8.0,%%
0.8.0,"In this case, we see that the samples in the minority class are not"
0.8.0,overlapping anymore due to the added noise.
0.8.0,
0.8.0,The parameter `shrinkage` allows to add more or less perturbation. Let's
0.8.0,add more perturbation when generating the smoothed bootstrap.
0.8.0,%%
0.8.0,%%
0.8.0,Increasing the value of `shrinkage` will disperse the new samples. Forcing
0.8.0,the shrinkage to 0 will be equivalent to generating a normal bootstrap.
0.8.0,%%
0.8.0,%%
0.8.0,"Therefore, the `shrinkage` is handy to manually tune the dispersion of the"
0.8.0,new samples.
0.8.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.8.0,License: MIT
0.8.0,%%
0.8.0,generate some data points
0.8.0,plot the majority and minority samples
0.8.0,draw the circle in which the new sample will generated
0.8.0,plot the line on which the sample will be generated
0.8.0,create and plot the new sample
0.8.0,make the plot nicer with legend and label
0.8.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.8.0,License: MIT
0.8.0,%%
0.8.0,%% [markdown]
0.8.0,The following function will be used to create toy dataset. It uses the
0.8.0,:func:`~sklearn.datasets.make_classification` from scikit-learn but fixing
0.8.0,some parameters.
0.8.0,%%
0.8.0,%% [markdown]
0.8.0,The following function will be used to plot the sample space after resampling
0.8.0,to illustrate the specificities of an algorithm.
0.8.0,%%
0.8.0,%% [markdown]
0.8.0,The following function will be used to plot the decision function of a
0.8.0,classifier given some data.
0.8.0,%%
0.8.0,%% [markdown]
0.8.0,Illustration of the influence of the balancing ratio
0.8.0,----------------------------------------------------
0.8.0,
0.8.0,We will first illustrate the influence of the balancing ratio on some toy
0.8.0,data using a logistic regression classifier which is a linear model.
0.8.0,%%
0.8.0,%% [markdown]
0.8.0,We will fit and show the decision boundary model to illustrate the impact of
0.8.0,dealing with imbalanced classes.
0.8.0,%%
0.8.0,%% [markdown]
0.8.0,"Greater is the difference between the number of samples in each class, poorer"
0.8.0,are the classification results.
0.8.0,
0.8.0,Random over-sampling to balance the data set
0.8.0,--------------------------------------------
0.8.0,
0.8.0,Random over-sampling can be used to repeat some samples and balance the
0.8.0,number of samples between the dataset. It can be seen that with this trivial
0.8.0,approach the boundary decision is already less biased toward the majority
0.8.0,class. The class :class:`~imblearn.over_sampling.RandomOverSampler`
0.8.0,implements such of a strategy.
0.8.0,%%
0.8.0,%% [markdown]
0.8.0,"By default, random over-sampling generates a bootstrap. The parameter"
0.8.0,`shrinkage` allows adding a small perturbation to the generated data
0.8.0,to generate a smoothed bootstrap instead. The plot below shows the difference
0.8.0,between the two data generation strategies.
0.8.0,%%
0.8.0,%% [markdown]
0.8.0,It looks like more samples are generated with smoothed bootstrap. This is due
0.8.0,to the fact that the samples generated are not superimposing with the
0.8.0,original samples.
0.8.0,
0.8.0,More advanced over-sampling using ADASYN and SMOTE
0.8.0,--------------------------------------------------
0.8.0,
0.8.0,Instead of repeating the same samples when over-sampling or perturbating the
0.8.0,"generated bootstrap samples, one can use some specific heuristic instead."
0.8.0,:class:`~imblearn.over_sampling.ADASYN` and
0.8.0,:class:`~imblearn.over_sampling.SMOTE` can be used in this case.
0.8.0,%%
0.8.0,%% [markdown]
0.8.0,The following plot illustrates the difference between
0.8.0,:class:`~imblearn.over_sampling.ADASYN` and
0.8.0,:class:`~imblearn.over_sampling.SMOTE`.
0.8.0,:class:`~imblearn.over_sampling.ADASYN` will focus on the samples which are
0.8.0,difficult to classify with a nearest-neighbors rule while regular
0.8.0,:class:`~imblearn.over_sampling.SMOTE` will not make any distinction.
0.8.0,"Therefore, the decision function depending of the algorithm."
0.8.0,%% [markdown]
0.8.0,"Due to those sampling particularities, it can give rise to some specific"
0.8.0,issues as illustrated below.
0.8.0,%%
0.8.0,%% [markdown]
0.8.0,SMOTE proposes several variants by identifying specific samples to consider
0.8.0,during the resampling. The borderline version
0.8.0,(:class:`~imblearn.over_sampling.BorderlineSMOTE`) will detect which point to
0.8.0,select which are in the border between two classes. The SVM version
0.8.0,(:class:`~imblearn.over_sampling.SVMSMOTE`) will use the support vectors
0.8.0,found using an SVM algorithm to create new sample while the KMeans version
0.8.0,(:class:`~imblearn.over_sampling.KMeansSMOTE`) will make a clustering before
0.8.0,to generate samples in each cluster independently depending each cluster
0.8.0,density.
0.8.0,%%
0.8.0,%% [markdown]
0.8.0,"When dealing with a mixed of continuous and categorical features,"
0.8.0,:class:`~imblearn.over_sampling.SMOTENC` is the only method which can handle
0.8.0,this case.
0.8.0,%%
0.8.0,Create a dataset of a mix of numerical and categorical data
0.8.0,%% [markdown]
0.8.0,"However, if the dataset is composed of only categorical features then one"
0.8.0,should use :class:`~imblearn.over_sampling.SMOTEN`.
0.8.0,%%
0.8.0,Generate only categorical data
0.8.0,Authors: Christos Aridas
0.8.0,Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.8.0,License: MIT
0.8.0,%%
0.8.0,%% [markdown]
0.8.0,Let's first generate a dataset with imbalanced class distribution.
0.8.0,%%
0.8.0,%% [markdown]
0.8.0,We will use an over-sampler :class:`~imblearn.over_sampling.SMOTE` followed
0.8.0,by a :class:`~sklearn.tree.DecisionTreeClassifier`. The aim will be to
0.8.0,search which `k_neighbors` parameter is the most adequate with the dataset
0.8.0,that we generated.
0.8.0,%%
0.8.0,%% [markdown]
0.8.0,We can use the :class:`~sklearn.model_selection.validation_curve` to inspect
0.8.0,"the impact of varying the parameter `k_neighbors`. In this case, we need"
0.8.0,to use a score to evaluate the generalization score during the
0.8.0,cross-validation.
0.8.0,%%
0.8.0,%%
0.8.0,%% [markdown]
0.8.0,We can now plot the results of the cross-validation for the different
0.8.0,parameter values that we tried.
0.8.0,%%
0.8.0,make nice plotting
0.8.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.8.0,License: MIT
0.8.0,Generate a dataset
0.8.0,Split the data
0.8.0,Train the classifier with balancing
0.8.0,Test the classifier and get the prediction
0.8.0,Show the classification report
0.8.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.8.0,License: MIT
0.8.0,%%
0.8.0,%% [markdown]
0.8.0,"First, we will generate some imbalanced dataset."
0.8.0,%%
0.8.0,%% [markdown]
0.8.0,We will split the data into a training and testing set.
0.8.0,%%
0.8.0,%% [markdown]
0.8.0,We will create a pipeline made of a :class:`~imblearn.over_sampling.SMOTE`
0.8.0,over-sampler followed by a :class:`~sklearn.svm.LinearSVC` classifier.
0.8.0,%%
0.8.0,%% [markdown]
0.8.0,"Now, we will train the model on the training set and get the prediction"
0.8.0,associated with the testing set. Be aware that the resampling will happen
0.8.0,only when calling `fit`: the number of samples in `y_pred` is the same than
0.8.0,in `y_test`.
0.8.0,%%
0.8.0,%% [markdown]
0.8.0,The geometric mean corresponds to the square root of the product of the
0.8.0,sensitivity and specificity. Combining the two metrics should account for
0.8.0,the balancing of the dataset.
0.8.0,%%
0.8.0,%% [markdown]
0.8.0,The index balanced accuracy can transform any metric to be used in
0.8.0,imbalanced learning problems.
0.8.0,%%
0.8.0,%%
0.8.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.8.0,License: MIT
0.8.0,%%
0.8.0,%% [markdown]
0.8.0,Dataset generation
0.8.0,------------------
0.8.0,
0.8.0,We will create an imbalanced dataset with a couple of samples. We will use
0.8.0,:func:`~sklearn.datasets.make_classification` to generate this dataset.
0.8.0,%%
0.8.0,%%
0.8.0,%% [markdown]
0.8.0,The following function will be used to plot the sample space after resampling
0.8.0,to illustrate the characteristic of an algorithm.
0.8.0,%%
0.8.0,%% [markdown]
0.8.0,The following function will be used to plot the decision function of a
0.8.0,classifier given some data.
0.8.0,%%
0.8.0,%% [markdown]
0.8.0,":class:`~imblearn.over_sampling.SMOTE` allows to generate samples. However,"
0.8.0,this method of over-sampling does not have any knowledge regarding the
0.8.0,"underlying distribution. Therefore, some noisy samples can be generated, e.g."
0.8.0,"when the different classes cannot be well separated. Hence, it can be"
0.8.0,beneficial to apply an under-sampling algorithm to clean the noisy samples.
0.8.0,Two methods are usually used in the literature: (i) Tomek's link and (ii)
0.8.0,edited nearest neighbours cleaning methods. Imbalanced-learn provides two
0.8.0,ready-to-use samplers :class:`~imblearn.combine.SMOTETomek` and
0.8.0,":class:`~imblearn.combine.SMOTEENN`. In general,"
0.8.0,:class:`~imblearn.combine.SMOTEENN` cleans more noisy data than
0.8.0,:class:`~imblearn.combine.SMOTETomek`.
0.8.0,%%
0.8.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.8.0,License: MIT
0.8.0,%%
0.8.0,%% [markdown]
0.8.0,Load an imbalanced dataset
0.8.0,--------------------------
0.8.0,
0.8.0,We will load the UCI SatImage dataset which has an imbalanced ratio of 9.3:1
0.8.0,(number of majority sample for a minority sample). The data are then split
0.8.0,into training and testing.
0.8.0,%%
0.8.0,%% [markdown]
0.8.0,Classification using a single decision tree
0.8.0,-------------------------------------------
0.8.0,
0.8.0,We train a decision tree classifier which will be used as a baseline for the
0.8.0,rest of this example.
0.8.0,
0.8.0,The results are reported in terms of balanced accuracy and geometric mean
0.8.0,which are metrics widely used in the literature to validate model trained on
0.8.0,imbalanced set.
0.8.0,%%
0.8.0,%%
0.8.0,%%
0.8.0,%% [markdown]
0.8.0,Classification using bagging classifier with and without sampling
0.8.0,-----------------------------------------------------------------
0.8.0,
0.8.0,"Instead of using a single tree, we will check if an ensemble of decsion tree"
0.8.0,"can actually alleviate the issue induced by the class imbalancing. First, we"
0.8.0,will use a bagging classifier and its counter part which internally uses a
0.8.0,random under-sampling to balanced each boostrap sample.
0.8.0,%%
0.8.0,%% [markdown]
0.8.0,Balancing each bootstrap sample allows to increase significantly the balanced
0.8.0,accuracy and the geometric mean.
0.8.0,%%
0.8.0,%%
0.8.0,%% [markdown]
0.8.0,Classification using random forest classifier with and without sampling
0.8.0,-----------------------------------------------------------------------
0.8.0,
0.8.0,Random forest is another popular ensemble method and it is usually
0.8.0,"outperforming bagging. Here, we used a vanilla random forest and its balanced"
0.8.0,counterpart in which each bootstrap sample is balanced.
0.8.0,%%
0.8.0,%% [markdown]
0.8.0,"Similarly to the previous experiment, the balanced classifier outperform the"
0.8.0,"classifier which learn from imbalanced bootstrap samples. In addition, random"
0.8.0,forest outsperforms the bagging classifier.
0.8.0,%%
0.8.0,%%
0.8.0,%% [markdown]
0.8.0,Boosting classifier
0.8.0,-------------------
0.8.0,
0.8.0,"In the same manner, easy ensemble classifier is a bag of balanced AdaBoost"
0.8.0,"classifier. However, it will be slower to train than random forest and will"
0.8.0,achieve worse performance.
0.8.0,%%
0.8.0,%%
0.8.0,%%
0.8.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.8.0,License: MIT
0.8.0,%%
0.8.0,%% [markdown]
0.8.0,Generate an imbalanced dataset
0.8.0,------------------------------
0.8.0,
0.8.0,"For this example, we will create a synthetic dataset using the function"
0.8.0,:func:`~sklearn.datasets.make_classification`. The problem will be a toy
0.8.0,classification problem with a ratio of 1:9 between the two classes.
0.8.0,%%
0.8.0,%%
0.8.0,%% [markdown]
0.8.0,"In the following sections, we will show a couple of algorithms that have"
0.8.0,been proposed over the years. We intend to illustrate how one can reuse the
0.8.0,:class:`~imblearn.ensemble.BalancedBaggingClassifier` by passing different
0.8.0,sampler.
0.8.0,%%
0.8.0,%% [markdown]
0.8.0,Exactly Balanced Bagging and Over-Bagging
0.8.0,-----------------------------------------
0.8.0,
0.8.0,The :class:`~imblearn.ensemble.BalancedBaggingClassifier` can use in
0.8.0,conjunction with a :class:`~imblearn.under_sampling.RandomUnderSampler` or
0.8.0,:class:`~imblearn.over_sampling.RandomOverSampler`. These methods are
0.8.0,"referred as Exactly Balanced Bagging and Over-Bagging, respectively and have"
0.8.0,been proposed first in [1]_.
0.8.0,%%
0.8.0,Exactly Balanced Bagging
0.8.0,%%
0.8.0,Over-bagging
0.8.0,%% [markdown]
0.8.0,SMOTE-Bagging
0.8.0,-------------
0.8.0,
0.8.0,Instead of using a :class:`~imblearn.over_sampling.RandomOverSampler` that
0.8.0,"make a bootstrap, an alternative is to use"
0.8.0,:class:`~imblearn.over_sampling.SMOTE` as an over-sampler. This is known as
0.8.0,SMOTE-Bagging [2]_.
0.8.0,%%
0.8.0,SMOTE-Bagging
0.8.0,%% [markdown]
0.8.0,Roughly Balanced Bagging
0.8.0,------------------------
0.8.0,While using a :class:`~imblearn.under_sampling.RandomUnderSampler` or
0.8.0,:class:`~imblearn.over_sampling.RandomOverSampler` will create exactly the
0.8.0,"desired number of samples, it does not follow the statistical spirit wanted"
0.8.0,in the bagging framework. The authors in [3]_ proposes to use a negative
0.8.0,binomial distribution to compute the number of samples of the majority
0.8.0,class to be selected and then perform a random under-sampling.
0.8.0,
0.8.0,"Here, we illustrate this method by implementing a function in charge of"
0.8.0,resampling and use the :class:`~imblearn.FunctionSampler` to integrate it
0.8.0,within a :class:`~imblearn.pipeline.Pipeline` and
0.8.0,:class:`~sklearn.model_selection.cross_validate`.
0.8.0,%%
0.8.0,find the minority and majority classes
0.8.0,compute the number of sample to draw from the majority class using
0.8.0,a negative binomial distribution
0.8.0,draw randomly with or without replacement
0.8.0,Roughly Balanced Bagging
0.8.0,%% [markdown]
0.8.0,.. topic:: References:
0.8.0,
0.8.0,".. [1] R. Maclin, and D. Opitz. ""An empirical evaluation of bagging and"
0.8.0,"boosting."" AAAI/IAAI 1997 (1997): 546-551."
0.8.0,
0.8.0,".. [2] S. Wang, and X. Yao. ""Diversity analysis on imbalanced data sets by"
0.8.0,"using ensemble models."" 2009 IEEE symposium on computational"
0.8.0,"intelligence and data mining. IEEE, 2009."
0.8.0,
0.8.0,".. [3] S. Hido, H. Kashima, and Y. Takahashi. ""Roughly balanced bagging"
0.8.0,"for imbalanced data."" Statistical Analysis and Data Mining: The ASA"
0.8.0,Data Science Journal 2.5‚Äê6 (2009): 412-426.
0.8.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.8.0,License: MIT
0.8.0,%%
0.8.0,%% [markdown]
0.8.0,The following function will be used to create toy dataset. It uses the
0.8.0,:func:`~sklearn.datasets.make_classification` from scikit-learn but fixing
0.8.0,some parameters.
0.8.0,%%
0.8.0,%% [markdown]
0.8.0,The following function will be used to plot the sample space after resampling
0.8.0,to illustrate the specificities of an algorithm.
0.8.0,%%
0.8.0,%% [markdown]
0.8.0,The following function will be used to plot the decision function of a
0.8.0,classifier given some data.
0.8.0,%%
0.8.0,%%
0.8.0,%% [markdown]
0.8.0,Prototype generation: under-sampling by generating new samples
0.8.0,--------------------------------------------------------------
0.8.0,
0.8.0,:class:`~imblearn.under_sampling.ClusterCentroids` under-samples by replacing
0.8.0,the original samples by the centroids of the cluster found.
0.8.0,%%
0.8.0,%% [markdown]
0.8.0,Prototype selection: under-sampling by selecting existing samples
0.8.0,-----------------------------------------------------------------
0.8.0,
0.8.0,The algorithm performing prototype selection can be subdivided into two
0.8.0,groups: (i) the controlled under-sampling methods and (ii) the cleaning
0.8.0,under-sampling methods.
0.8.0,
0.8.0,"With the controlled under-sampling methods, the number of samples to be"
0.8.0,selected can be specified.
0.8.0,:class:`~imblearn.under_sampling.RandomUnderSampler` is the most naive way of
0.8.0,performing such selection by randomly selecting a given number of samples by
0.8.0,the targetted class.
0.8.0,%%
0.8.0,%% [markdown]
0.8.0,:class:`~imblearn.under_sampling.NearMiss` algorithms implement some
0.8.0,heuristic rules in order to select samples. NearMiss-1 selects samples from
0.8.0,the majority class for which the average distance of the :math:`k`` nearest
0.8.0,samples of the minority class is the smallest. NearMiss-2 selects the samples
0.8.0,from the majority class for which the average distance to the farthest
0.8.0,samples of the negative class is the smallest. NearMiss-3 is a 2-step
0.8.0,"algorithm: first, for each minority sample, their :math:`m`"
0.8.0,"nearest-neighbors will be kept; then, the majority samples selected are the"
0.8.0,on for which the average distance to the :math:`k` nearest neighbors is the
0.8.0,largest.
0.8.0,%%
0.8.0,%% [markdown]
0.8.0,:class:`~imblearn.under_sampling.EditedNearestNeighbours` removes samples of
0.8.0,the majority class for which their class differ from the one of their
0.8.0,nearest-neighbors. This sieve can be repeated which is the principle of the
0.8.0,:class:`~imblearn.under_sampling.RepeatedEditedNearestNeighbours`.
0.8.0,:class:`~imblearn.under_sampling.AllKNN` is slightly different from the
0.8.0,:class:`~imblearn.under_sampling.RepeatedEditedNearestNeighbours` by changing
0.8.0,"the :math:`k` parameter of the internal nearest neighors algorithm,"
0.8.0,increasing it at each iteration.
0.8.0,%%
0.8.0,%% [markdown]
0.8.0,:class:`~imblearn.under_sampling.CondensedNearestNeighbour` makes use of a
0.8.0,1-NN to iteratively decide if a sample should be kept in a dataset or not.
0.8.0,The issue is that :class:`~imblearn.under_sampling.CondensedNearestNeighbour`
0.8.0,is sensitive to noise by preserving the noisy samples.
0.8.0,:class:`~imblearn.under_sampling.OneSidedSelection` also used the 1-NN and
0.8.0,use :class:`~imblearn.under_sampling.TomekLinks` to remove the samples
0.8.0,considered noisy. The
0.8.0,:class:`~imblearn.under_sampling.NeighbourhoodCleaningRule` use a
0.8.0,:class:`~imblearn.under_sampling.EditedNearestNeighbours` to remove some
0.8.0,"sample. Additionally, they use a 3 nearest-neighbors to remove samples which"
0.8.0,do not agree with this rule.
0.8.0,%%
0.8.0,%% [markdown]
0.8.0,:class:`~imblearn.under_sampling.InstanceHardnessThreshold` uses the
0.8.0,prediction of classifier to exclude samples. All samples which are classified
0.8.0,with a low probability will be removed.
0.8.0,%%
0.8.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.8.0,License: MIT
0.8.0,%%
0.8.0,%% [markdown]
0.8.0,This function allows to make nice plotting
0.8.0,%%
0.8.0,%% [markdown]
0.8.0,We will generate some toy data that illustrates how
0.8.0,:class:`~imblearn.under_sampling.TomekLinks` is used to clean a dataset.
0.8.0,%%
0.8.0,%% [markdown]
0.8.0,"In the figure above, the samples highlighted in green form a Tomek link since"
0.8.0,they are of different classes and are nearest neighbors of each other.
0.8.0,highlight the samples of interest
0.8.0,%% [markdown]
0.8.0,We can run the :class:`~imblearn.under_sampling.TomekLinks` sampling to
0.8.0,remove the corresponding samples. If `sampling_strategy='auto'` only the
0.8.0,sample from the majority class will be removed. If `sampling_strategy='all'`
0.8.0,both samples will be removed.
0.8.0,%%
0.8.0,highlight the samples of interest
0.8.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.8.0,License: MIT
0.8.0,%%
0.8.0,%% [markdown]
0.8.0,We define a function allowing to make some nice decoration on the plot.
0.8.0,%%
0.8.0,%% [markdown]
0.8.0,We can start by generating some data to later illustrate the principle of
0.8.0,each :class:`~imblearn.under_sampling.NearMiss` heuristic rules.
0.8.0,%%
0.8.0,%% [mardown]
0.8.0,NearMiss-1
0.8.0,----------
0.8.0,
0.8.0,NearMiss-1 selects samples from the majority class for which the average
0.8.0,distance to some nearest neighbours is the smallest. In the following
0.8.0,"example, we use a 3-NN to compute the average distance on 2 specific samples"
0.8.0,"of the majority class. Therefore, in this case the point linked by the"
0.8.0,green-dashed line will be selected since the average distance is smaller.
0.8.0,%%
0.8.0,%% [mardown]
0.8.0,NearMiss-2
0.8.0,----------
0.8.0,
0.8.0,NearMiss-2 selects samples from the majority class for which the average
0.8.0,distance to the farthest neighbors is the smallest. With the same
0.8.0,"configuration as previously presented, the sample linked to the green-dashed"
0.8.0,line will be selected since its distance the 3 farthest neighbors is the
0.8.0,smallest.
0.8.0,%%
0.8.0,%% [mardown]
0.8.0,NearMiss-3
0.8.0,----------
0.8.0,
0.8.0,"NearMiss-3 can be divided into 2 steps. First, a nearest-neighbors is used to"
0.8.0,short-list samples from the majority class (i.e. correspond to the
0.8.0,"highlighted samples in the following plot). Then, the sample with the largest"
0.8.0,average distance to the *k* nearest-neighbors are selected.
0.8.0,%%
0.8.0,select only the majority point of interest
0.8.0,Authors: Christos Aridas
0.8.0,Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.8.0,License: MIT
0.8.0,%%
0.8.0,%% [markdown]
0.8.0,Let's first create an imbalanced dataset and split in to two sets.
0.8.0,%%
0.8.0,%% [markdown]
0.8.0,"Now, we will create each individual steps that we would like later to combine"
0.8.0,%%
0.8.0,%% [markdown]
0.8.0,"Now, we can finally create a pipeline to specify in which order the different"
0.8.0,transformers and samplers should be executed before to provide the data to
0.8.0,the final classifier.
0.8.0,%%
0.8.0,%% [markdown]
0.8.0,We can now use the pipeline created as a normal classifier where resampling
0.8.0,"will happen when calling `fit` and disabled when calling `decision_function`,"
0.8.0,"`predict_proba`, or `predict`."
0.8.0,%%
0.8.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.8.0,License: MIT
0.8.0,##############################################################################
0.8.0,Data loading
0.8.0,##############################################################################
0.8.0,##############################################################################
0.8.0,"First, you should download the Porto Seguro data set from Kaggle. See the"
0.8.0,link in the introduction.
0.8.0,##############################################################################
0.8.0,The data set is imbalanced and it will have an effect on the fitting.
0.8.0,##############################################################################
0.8.0,Define the pre-processing pipeline
0.8.0,##############################################################################
0.8.0,##############################################################################
0.8.0,We want to standard scale the numerical features while we want to one-hot
0.8.0,"encode the categorical features. In this regard, we make use of the"
0.8.0,:class:`~sklearn.compose.ColumnTransformer`.
0.8.0,Create an environment variable to avoid using the GPU. This can be changed.
0.8.0,##############################################################################
0.8.0,Create a neural-network
0.8.0,##############################################################################
0.8.0,##############################################################################
0.8.0,We create a decorator to report the computation time
0.8.0,##############################################################################
0.8.0,The first model will be trained using the ``fit`` method and with imbalanced
0.8.0,mini-batches.
0.8.0,##############################################################################
0.8.0,"In the contrary, we will use imbalanced-learn to create a generator of"
0.8.0,mini-batches which will yield balanced mini-batches.
0.8.0,##############################################################################
0.8.0,Classification loop
0.8.0,##############################################################################
0.8.0,##############################################################################
0.8.0,We will perform a 10-fold cross-validation and train the neural-network with
0.8.0,the two different strategies previously presented.
0.8.0,##############################################################################
0.8.0,Plot of the results and computation time
0.8.0,##############################################################################
0.8.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.8.0,License: MIT
0.8.0,%%
0.8.0,%% [markdown]
0.8.0,Problem definition
0.8.0,------------------
0.8.0,
0.8.0,We are dropping the following features:
0.8.0,
0.8.0,"- ""fnlwgt"": this feature was created while studying the ""adult"" dataset."
0.8.0,"Thus, we will not use this feature which is not acquired during the survey."
0.8.0,"- ""education-num"": it is encoding the same information than ""education""."
0.8.0,"Thus, we are removing one of these 2 features."
0.8.0,%%
0.8.0,%% [markdown]
0.8.0,"The ""adult"" dataset as a class ratio of about 3:1"
0.8.0,%%
0.8.0,%% [markdown]
0.8.0,This dataset is only slightly imbalanced. To better highlight the effect of
0.8.0,"learning from an imbalanced dataset, we will increase its ratio to 30:1"
0.8.0,%%
0.8.0,%% [markdown]
0.8.0,We will perform a cross-validation evaluation to get an estimate of the test
0.8.0,score.
0.8.0,
0.8.0,"As a baseline, we could use a classifier which will always predict the"
0.8.0,majority class independently of the features provided.
0.8.0,%%
0.8.0,%% [markdown]
0.8.0,"Instead of using the accuracy, we can use the balanced accuracy which will"
0.8.0,take into account the balancing issue.
0.8.0,%%
0.8.0,%% [markdown]
0.8.0,Strategies to learn from an imbalanced dataset
0.8.0,----------------------------------------------
0.8.0,We will use a dictionary and a list to continuously store the results of
0.8.0,our experiments and show them as a pandas dataframe.
0.8.0,%%
0.8.0,%% [markdown]
0.8.0,Dummy baseline
0.8.0,..............
0.8.0,
0.8.0,"Before to train a real machine learning model, we can store the results"
0.8.0,obtained with our :class:`~sklearn.dummy.DummyClassifier`.
0.8.0,%%
0.8.0,%% [markdown]
0.8.0,Linear classifier baseline
0.8.0,..........................
0.8.0,
0.8.0,We will create a machine learning pipeline using a
0.8.0,":class:`~sklearn.linear_model.LogisticRegression` classifier. In this regard,"
0.8.0,we will need to one-hot encode the categorical columns and standardized the
0.8.0,numerical columns before to inject the data into the
0.8.0,:class:`~sklearn.linear_model.LogisticRegression` classifier.
0.8.0,
0.8.0,"First, we define our numerical and categorical pipelines."
0.8.0,%%
0.8.0,%% [markdown]
0.8.0,"Then, we can create a preprocessor which will dispatch the categorical"
0.8.0,columns to the categorical pipeline and the numerical columns to the
0.8.0,numerical pipeline
0.8.0,%%
0.8.0,%% [markdown]
0.8.0,"Finally, we connect our preprocessor with our"
0.8.0,:class:`~sklearn.linear_model.LogisticRegression`. We can then evaluate our
0.8.0,model.
0.8.0,%%
0.8.0,%%
0.8.0,%% [markdown]
0.8.0,We can see that our linear model is learning slightly better than our dummy
0.8.0,"baseline. However, it is impacted by the class imbalance."
0.8.0,
0.8.0,We can verify that something similar is happening with a tree-based model
0.8.0,such as :class:`~sklearn.ensemble.RandomForestClassifier`. With this type of
0.8.0,"classifier, we will not need to scale the numerical data, and we will only"
0.8.0,need to ordinal encode the categorical data.
0.8.0,%%
0.8.0,%%
0.8.0,%% [markdown]
0.8.0,The :class:`~sklearn.ensemble.RandomForestClassifier` is as well affected by
0.8.0,"the class imbalanced, slightly less than the linear model. Now, we will"
0.8.0,present different approach to improve the performance of these 2 models.
0.8.0,
0.8.0,Use `class_weight`
0.8.0,..................
0.8.0,
0.8.0,Most of the models in `scikit-learn` have a parameter `class_weight`. This
0.8.0,parameter will affect the computation of the loss in linear model or the
0.8.0,criterion in the tree-based model to penalize differently a false
0.8.0,classification from the minority and majority class. We can set
0.8.0,"`class_weight=""balanced""` such that the weight applied is inversely"
0.8.0,proportional to the class frequency. We test this parametrization in both
0.8.0,linear model and tree-based model.
0.8.0,%%
0.8.0,%%
0.8.0,%% [markdown]
0.8.0,We can see that using `class_weight` was really effective for the linear
0.8.0,"model, alleviating the issue of learning from imbalanced classes. However,"
0.8.0,the :class:`~sklearn.ensemble.RandomForestClassifier` is still biased toward
0.8.0,"the majority class, mainly due to the criterion which is not suited enough to"
0.8.0,fight the class imbalance.
0.8.0,
0.8.0,Resample the training set during learning
0.8.0,.........................................
0.8.0,
0.8.0,Another way is to resample the training set by under-sampling or
0.8.0,over-sampling some of the samples. `imbalanced-learn` provides some samplers
0.8.0,to do such processing.
0.8.0,%%
0.8.0,%%
0.8.0,%%
0.8.0,%%
0.8.0,%% [markdown]
0.8.0,Applying a random under-sampler before the training of the linear model or
0.8.0,"random forest, allows to not focus on the majority class at the cost of"
0.8.0,making more mistake for samples in the majority class (i.e. decreased
0.8.0,accuracy).
0.8.0,
0.8.0,We could apply any type of samplers and find which sampler is working best
0.8.0,on the current dataset.
0.8.0,
0.8.0,"Instead, we will present another way by using classifiers which will apply"
0.8.0,sampling internally.
0.8.0,
0.8.0,Use of specific balanced algorithms from imbalanced-learn
0.8.0,.........................................................
0.8.0,
0.8.0,We already showed that random under-sampling can be effective on decision
0.8.0,"tree. However, instead of under-sampling once the dataset, one could"
0.8.0,under-sample the original dataset before to take a bootstrap sample. This is
0.8.0,the base of the :class:`imblearn.ensemble.BalancedRandomForestClassifier` and
0.8.0,:class:`~imblearn.ensemble.BalancedBaggingClassifier`.
0.8.0,%%
0.8.0,%%
0.8.0,%% [markdown]
0.8.0,The performance with the
0.8.0,:class:`~imblearn.ensemble.BalancedRandomForestClassifier` is better than
0.8.0,applying a single random under-sampling. We will use a gradient-boosting
0.8.0,classifier within a :class:`~imblearn.ensemble.BalancedBaggingClassifier`.
0.8.0,%% [markdown]
0.8.0,This last approach is the most effective. The different under-sampling allows
0.8.0,to bring some diversity for the different GBDT to learn and not focus on a
0.8.0,portion of the majority class.
0.8.0,Authors: Christos Aridas
0.8.0,Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.8.0,License: MIT
0.8.0,%%
0.8.0,%% [markdown]
0.8.0,Load the dataset
0.8.0,----------------
0.8.0,
0.8.0,We will use a dataset containing image from know person where we will
0.8.0,build a model to recognize the person on the image. We will make this problem
0.8.0,a binary problem by taking picture of only George W. Bush and Bill Clinton.
0.8.0,%%
0.8.0,%%
0.8.0,%% [markdown]
0.8.0,We can check the ratio between the two classes.
0.8.0,%%
0.8.0,%% [markdown]
0.8.0,We see that we have an imbalanced classification problem with ~95% of the
0.8.0,data belonging to the class G.W. Bush.
0.8.0,
0.8.0,Compare over-sampling approaches
0.8.0,--------------------------------
0.8.0,
0.8.0,We will use different over-sampling approaches and use a kNN classifier
0.8.0,to check if we can recognize the 2 presidents. The evaluation will be
0.8.0,performed through cross-validation and we will plot the mean ROC curve.
0.8.0,
0.8.0,We will create different pipelines and evaluate them.
0.8.0,%%
0.8.0,%%
0.8.0,%% [markdown]
0.8.0,We will compute the mean ROC curve for each pipeline using a different splits
0.8.0,provided by the :class:`~sklearn.model_selection.StratifiedKFold`
0.8.0,cross-validation.
0.8.0,%%
0.8.0,compute the mean fpr/tpr to get the mean ROC curve
0.8.0,Create a display that we will reuse to make the aggregated plots for
0.8.0,all methods
0.8.0,%% [markdown]
0.8.0,"In the previous cell, we created the different mean ROC curve and we can plot"
0.8.0,them on the same plot.
0.8.0,%%
0.8.0,%% [markdown]
0.8.0,"We see that for this task, methods that are generating new samples with some"
0.8.0,interpolation (i.e. ADASYN and SMOTE) perform better than random
0.8.0,over-sampling or no resampling.
0.8.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.8.0,License: MIT
0.8.0,Create a folder to fetch the dataset
0.8.0,Create a pipeline
0.8.0,Classify and report the results
0.8.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.8.0,License: MIT
0.8.0,%%
0.8.0,%% [markdown]
0.8.0,Setting the data set
0.8.0,--------------------
0.8.0,
0.8.0,We use a part of the 20 newsgroups data set by loading 4 topics. Using the
0.8.0,"scikit-learn loader, the data are split into a training and a testing set."
0.8.0,
0.8.0,Note the class \#3 is the minority class and has almost twice less samples
0.8.0,than the majority class.
0.8.0,%%
0.8.0,%%
0.8.0,% [markdown]
0.8.0,The usual scikit-learn pipeline
0.8.0,-------------------------------
0.8.0,
0.8.0,You might usually use scikit-learn pipeline by combining the TF-IDF
0.8.0,vectorizer to feed a multinomial naive bayes classifier. A classification
0.8.0,report summarized the results on the testing set.
0.8.0,
0.8.0,"As expected, the recall of the class \#3 is low mainly due to the class"
0.8.0,imbalanced.
0.8.0,%%
0.8.0,%%
0.8.0,%% [markdown]
0.8.0,Balancing the class before classification
0.8.0,-----------------------------------------
0.8.0,
0.8.0,"To improve the prediction of the class \#3, it could be interesting to apply"
0.8.0,"a balancing before to train the naive bayes classifier. Therefore, we will"
0.8.0,use a :class:`~imblearn.under_sampling.RandomUnderSampler` to equalize the
0.8.0,number of samples in all the classes before the training.
0.8.0,
0.8.0,It is also important to note that we are using the
0.8.0,:class:`~imblearn.pipeline.make_pipeline` function implemented in
0.8.0,imbalanced-learn to properly handle the samplers.
0.8.0,%%
0.8.0,%% [markdown]
0.8.0,"Although the results are almost identical, it can be seen that the resampling"
0.8.0,allowed to correct the poor recall of the class \#3 at the cost of reducing
0.8.0,"the other metrics for the other classes. However, the overall results are"
0.8.0,slightly better.
0.8.0,%%
0.8.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.8.0,License: MIT
0.8.0,#############################################################################
0.8.0,Toy data generation
0.8.0,#############################################################################
0.8.0,#############################################################################
0.8.0,We are generating some non Gaussian data set contaminated with some unform
0.8.0,noise.
0.8.0,#############################################################################
0.8.0,We will generate some cleaned test data without outliers.
0.8.0,#############################################################################
0.8.0,How to use the :class:`~imblearn.FunctionSampler`
0.8.0,#############################################################################
0.8.0,#############################################################################
0.8.0,We first define a function which will use
0.8.0,:class:`~sklearn.ensemble.IsolationForest` to eliminate some outliers from
0.8.0,our dataset during training. The function passed to the
0.8.0,:class:`~imblearn.FunctionSampler` will be called when using the method
0.8.0,``fit_resample``.
0.8.0,#############################################################################
0.8.0,Integrate it within a pipeline
0.8.0,#############################################################################
0.8.0,#############################################################################
0.8.0,"By elimnating outliers before the training, the classifier will be less"
0.8.0,affected during the prediction.
0.8.0,Authors: Dayvid Oliveira
0.8.0,Christos Aridas
0.8.0,Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.8.0,License: MIT
0.8.0,%%
0.8.0,%% [markdown]
0.8.0,Generate the dataset
0.8.0,--------------------
0.8.0,
0.8.0,"First, we will generate a dataset and convert it to a"
0.8.0,:class:`~pandas.DataFrame` with arbitrary column names. We will plot the
0.8.0,original dataset.
0.8.0,%%
0.8.0,%% [markdown]
0.8.0,Make a dataset imbalanced
0.8.0,-------------------------
0.8.0,
0.8.0,"Now, we will show the helpers :func:`~imblearn.datasets.make_imbalance`"
0.8.0,that is useful to random select a subset of samples. It will impact the
0.8.0,class distribution as specified by the parameters.
0.8.0,%%
0.8.0,%%
0.8.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.8.0,License: MIT
0.8.0,%%
0.8.0,%% [markdown]
0.8.0,Create an imbalanced dataset
0.8.0,----------------------------
0.8.0,
0.8.0,"First, we will create an imbalanced data set from a the iris data set."
0.8.0,%%
0.8.0,%%
0.8.0,%% [markdown]
0.8.0,Using ``sampling_strategy`` in resampling algorithms
0.8.0,====================================================
0.8.0,
0.8.0,`sampling_strategy` as a `float`
0.8.0,--------------------------------
0.8.0,
0.8.0,`sampling_strategy` can be given a `float`. For **under-sampling
0.8.0,"methods**, it corresponds to the ratio :math:`\\alpha_{us}` defined by"
0.8.0,:math:`N_{rM} = \\alpha_{us} \\times N_{m}` where :math:`N_{rM}` and
0.8.0,:math:`N_{m}` are the number of samples in the majority class after
0.8.0,"resampling and the number of samples in the minority class, respectively."
0.8.0,%%
0.8.0,select only 2 classes since the ratio make sense in this case
0.8.0,%%
0.8.0,%% [markdown]
0.8.0,"For **over-sampling methods**, it correspond to the ratio"
0.8.0,:math:`\\alpha_{os}` defined by :math:`N_{rm} = \\alpha_{os} \\times N_{M}`
0.8.0,where :math:`N_{rm}` and :math:`N_{M}` are the number of samples in the
0.8.0,minority class after resampling and the number of samples in the majority
0.8.0,"class, respectively."
0.8.0,%%
0.8.0,%% [markdown]
0.8.0,`sampling_strategy` has a `str`
0.8.0,-------------------------------
0.8.0,
0.8.0,`sampling_strategy` can be given as a string which specify the class
0.8.0,"targeted by the resampling. With under- and over-sampling, the number of"
0.8.0,samples will be equalized.
0.8.0,
0.8.0,Note that we are using multiple classes from now on.
0.8.0,%%
0.8.0,%% [markdown]
0.8.0,"With **cleaning method**, the number of samples in each class will not be"
0.8.0,equalized even if targeted.
0.8.0,%%
0.8.0,%% [markdown]
0.8.0,`sampling_strategy as a `dict`
0.8.0,------------------------------
0.8.0,
0.8.0,"When `sampling_strategy` is a `dict`, the keys correspond to the targeted"
0.8.0,classes. The values correspond to the desired number of samples for each
0.8.0,targeted class. This is working for both **under- and over-sampling**
0.8.0,algorithms but not for the **cleaning algorithms**. Use a `list` instead.
0.8.0,%%
0.8.0,%% [markdown]
0.8.0,`sampling_strategy` as a `list`
0.8.0,-------------------------------
0.8.0,
0.8.0,"When `sampling_strategy` is a `list`, the list contains the targeted"
0.8.0,classes. It is used only for **cleaning methods** and raise an error
0.8.0,otherwise.
0.8.0,%%
0.8.0,%% [markdown]
0.8.0,`sampling_strategy` as a callable
0.8.0,---------------------------------
0.8.0,
0.8.0,"When callable, function taking `y` and returns a `dict`. The keys"
0.8.0,correspond to the targeted classes. The values correspond to the desired
0.8.0,number of samples for each class.
0.8.0,%%
0.8.0,List of whitelisted modules and methods; regexp are supported.
0.8.0,These docstrings will fail because they are inheriting from scikit-learn
0.8.0,skip private classes
0.8.0,"We ignore following error code,"
0.8.0,- RT02: The first line of the Returns section
0.8.0,"should contain only the type, .."
0.8.0,(as we may need refer to the name of the returned
0.8.0,object)
0.8.0,- GL01: Docstring text (summary) should start in the line
0.8.0,"immediately after the opening quotes (not in the same line,"
0.8.0,or leaving a blank line in between)
0.8.0,Following codes are only taken into account for the
0.8.0,top level class docstrings:
0.8.0,- ES01: No extended summary found
0.8.0,- SA01: See Also section not found
0.8.0,- EX01: No examples section found
0.8.0,In particular we can't parse the signature of properties
0.8.0,"When applied to classes, detect class method. For functions"
0.8.0,method = None.
0.8.0,TODO: this detection can be improved. Currently we assume that we have
0.8.0,class # methods if the second path element before last is in camel case.
0.8.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.8.0,License: MIT
0.8.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.8.0,Christos Aridas
0.8.0,License: MIT
0.8.0,we need to overwrite SamplerMixin.fit to bypass the validation
0.8.0,Adapted from scikit-learn
0.8.0,Author: Edouard Duchesnay
0.8.0,Gael Varoquaux
0.8.0,Virgile Fritsch
0.8.0,Alexandre Gramfort
0.8.0,Lars Buitinck
0.8.0,Christos Aridas
0.8.0,Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.8.0,License: BSD
0.8.0,BaseEstimator interface
0.8.0,validate names
0.8.0,validate estimators
0.8.0,We allow last estimator to be None as an identity transformation
0.8.0,Estimator interface
0.8.0,Setup the memory
0.8.0,joblib >= 0.12
0.8.0,Fit or load from cache the current transformer
0.8.0,Replace the transformer of the step with the fitted
0.8.0,transformer. This is necessary when loading the transformer
0.8.0,from the cache.
0.8.0,"# FIXME: When we get Python 3.7 as minimal version, we will need to switch to"
0.8.0,# the following solution:
0.8.0,# https://snarky.ca/lazy-importing-in-python-3-7/
0.8.0,Import the target module and insert it into the parent's namespace
0.8.0,Update this object's dict so that if someone keeps a reference to the
0.8.0,"LazyLoader, lookups are efficient (__getattr__ is only called on"
0.8.0,lookups that fail).
0.8.0,delay the import of keras since we are going to import either tensorflow
0.8.0,or keras
0.8.0,Based on NiLearn package
0.8.0,License: simplified BSD
0.8.0,"PEP0440 compatible formatted version, see:"
0.8.0,https://www.python.org/dev/peps/pep-0440/
0.8.0,
0.8.0,Generic release markers:
0.8.0,X.Y
0.8.0,X.Y.Z # For bugfix releases
0.8.0,
0.8.0,Admissible pre-release markers:
0.8.0,X.YaN # Alpha release
0.8.0,X.YbN # Beta release
0.8.0,X.YrcN # Release Candidate
0.8.0,X.Y # Final release
0.8.0,
0.8.0,Dev branch marker is: 'X.Y.dev' or 'X.Y.devN' where N is an integer.
0.8.0,'X.Y.dev0' is the canonical version of 'X.Y.dev'
0.8.0,
0.8.0,coding: utf-8
0.8.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.8.0,Dariusz Brzezinski
0.8.0,License: MIT
0.8.0,Only negative labels
0.8.0,"Calculate tp_sum, pred_sum, true_sum ###"
0.8.0,labels are now from 0 to len(labels) - 1 -> use bincount
0.8.0,Pathological case
0.8.0,Compute the true negative
0.8.0,Retain only selected labels
0.8.0,"Finally, we have all our sufficient statistics. Divide! #"
0.8.0,"Divide, and on zero-division, set scores to 0 and warn:"
0.8.0,"Oddly, we may get an ""invalid"" rather than a ""divide"" error"
0.8.0,here.
0.8.0,Average the results
0.8.0,labels are now from 0 to len(labels) - 1 -> use bincount
0.8.0,Pathological case
0.8.0,Retain only selected labels
0.8.0,old version of scipy return MaskedConstant instead of 0.0
0.8.0,check that the scoring function does not need a score
0.8.0,and only a prediction
0.8.0,We do not support multilabel so the only average supported
0.8.0,is binary
0.8.0,Compute the different metrics
0.8.0,Precision/recall/f1
0.8.0,Specificity
0.8.0,Geometric mean
0.8.0,Index balanced accuracy
0.8.0,compute averages
0.8.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.8.0,License: MIT
0.8.0,categories are expected to be encoded from 0 to n_categories - 1
0.8.0,"list of length n_features of ndarray (n_categories, n_classes)"
0.8.0,compute the counts
0.8.0,normalize by the summing over the classes
0.8.0,silence potential warning due to in-place division by zero
0.8.0,coding: utf-8
0.8.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.8.0,Christos Aridas
0.8.0,License: MIT
0.8.0,##############################################################################
0.8.0,Utilities for testing
0.8.0,import some data to play with
0.8.0,restrict to a binary classification task
0.8.0,add noisy features to make the problem harder and avoid perfect results
0.8.0,"run classifier, get class probabilities and label predictions"
0.8.0,only interested in probabilities of the positive case
0.8.0,XXX: do we really want a special API for the binary case?
0.8.0,##############################################################################
0.8.0,Tests
0.8.0,detailed measures for each class
0.8.0,individual scoring function that can be used for grid search: in the
0.8.0,binary class case the score is the value of the measure for the positive
0.8.0,class (e.g. label == 1). This is deprecated for average != 'binary'.
0.8.0,Such a case may occur with non-stratified cross-validation
0.8.0,ensure the above were meaningful tests:
0.8.0,Bad pos_label
0.8.0,Bad average option
0.8.0,but average != 'binary'; even if data is binary
0.8.0,compute the geometric mean for the binary problem
0.8.0,print classification report with class names
0.8.0,print classification report with label detection
0.8.0,print classification report with class names
0.8.0,print classification report with label detection
0.8.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.8.0,License: MIT
0.8.0,Check basic feature of the metric:
0.8.0,"* the shape of the distance matrix is (n_samples, n_samples)"
0.8.0,* computing pairwise distance of X is the same than explicitely between
0.8.0,X and X.
0.8.0,Check the property of the vdm distance. Let's check the property
0.8.0,"described in ""Improved Heterogeneous Distance Functions"", D.R. Wilson and"
0.8.0,"T.R. Martinez, Journal of Artificial Intelligence Research 6 (1997) 1-34"
0.8.0,https://arxiv.org/pdf/cs/9701101.pdf
0.8.0,
0.8.0,"""if an attribute color has three values red, green and blue, and the"
0.8.0,"application is to identify whether or not an object is an apple, red and"
0.8.0,green would be considered closer than red and blue because the former two
0.8.0,"both have similar correlations with the output class apple."""
0.8.0,defined our feature
0.8.0,0 - not an apple / 1 - an apple
0.8.0,computing the distance between a sample of the same category should
0.8.0,give a null distance
0.8.0,check the property explained in the introduction example
0.8.0,green and red are very close
0.8.0,blue is closer to red than green
0.8.0,"Check that ""auto"" is equivalent to provide the number categories"
0.8.0,beforehand
0.8.0,Check that we raise an error if n_categories is inconsistent with the
0.8.0,number of features in X
0.8.0,Check that we don't get issue when a category is missing between 0
0.8.0,n_categories - 1
0.8.0,remove a categories that could be between 0 and n_categories
0.8.0,Check that we raise a NotFittedError when `fit` is not not called before
0.8.0,pairwise.
0.8.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.8.0,Christos Aridas
0.8.0,License: MIT
0.8.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.8.0,Christos Aridas
0.8.0,License: MIT
0.8.0,The ratio is computed using a one-vs-rest manner. Using majority
0.8.0,in multi-class would lead to slightly different results at the
0.8.0,cost of introducing a new parameter.
0.8.0,rounding may cause new amount for n_samples
0.8.0,the nearest neighbors need to be fitted only on the current class
0.8.0,to find the class NN to generate new samples
0.8.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.8.0,Christos Aridas
0.8.0,License: MIT
0.8.0,smoothed bootstrap imposes to make numerical operation; we need
0.8.0,to be sure to have only numerical data in X
0.8.0,generate a smoothed bootstrap with a perturbation
0.8.0,generate a bootstrap
0.8.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.8.0,Christos Aridas
0.8.0,License: MIT
0.8.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.8.0,Fernando Nogueira
0.8.0,Christos Aridas
0.8.0,License: MIT
0.8.0,validate the parameters
0.8.0,negate diagonal elements
0.8.0,target_class_indices = np.flatnonzero(y == class_sample)
0.8.0,"X_class = _safe_indexing(X, target_class_indices)"
0.8.0,identify cluster which are answering the requirements
0.8.0,the cluster is already considered balanced
0.8.0,not enough samples to apply SMOTE
0.8.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.8.0,Fernando Nogueira
0.8.0,Christos Aridas
0.8.0,Dzianis Dudnik
0.8.0,License: MIT
0.8.0,divergence between borderline-1 and borderline-2
0.8.0,Create synthetic samples for borderline points.
0.8.0,only minority
0.8.0,we use a one-vs-rest policy to handle the multiclass in which
0.8.0,new samples will be created considering not only the majority
0.8.0,class but all over classes.
0.8.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.8.0,Fernando Nogueira
0.8.0,Christos Aridas
0.8.0,Dzianis Dudnik
0.8.0,License: MIT
0.8.0,np.newaxis for backwards compatability with random_state
0.8.0,Samples are in danger for m/2 <= m' < m
0.8.0,Samples are noise for m = m'
0.8.0,compute the median of the standard deviation of the minority class
0.8.0,the input of the OneHotEncoder needs to be dense
0.8.0,we can replace the 1 entries of the categorical features with the
0.8.0,median of the standard deviation. It will ensure that whenever
0.8.0,"distance is computed between 2 samples, the difference will be equal"
0.8.0,to the median of the standard deviation as in the original paper.
0.8.0,"In the edge case where the median of the std is equal to 0, the 1s"
0.8.0,"entries will be also nullified. In this case, we store the original"
0.8.0,categorical encoding which will be later used for inversing the OHE
0.8.0,reverse the encoding of the categorical features
0.8.0,the matrix is supposed to be in the CSR format after the stacking
0.8.0,change in sparsity structure more efficient with LIL than CSR
0.8.0,convert to dense array since scipy.sparse doesn't handle 3D
0.8.0,"In the case that the median std was equal to zeros, we have to"
0.8.0,create non-null entry based on the encoded of OHE
0.8.0,tie breaking argmax
0.8.0,generate sample indices that will be used to generate new samples
0.8.0,"for each drawn samples, select its k-neighbors and generate a sample"
0.8.0,"where for each feature individually, each category generated is the"
0.8.0,most common category
0.8.0,the kneigbors search will include the sample itself which is
0.8.0,expected from the original algorithm
0.8.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.8.0,Christos Aridas
0.8.0,Dzianis Dudnik
0.8.0,License: MIT
0.8.0,create 2 random continuous feature
0.8.0,create a categorical feature using some string
0.8.0,create a categorical feature using some integer
0.8.0,return the categories
0.8.0,create 2 random continuous feature
0.8.0,create a categorical feature using some string
0.8.0,create a categorical feature using some integer
0.8.0,return the categories
0.8.0,create 2 random continuous feature
0.8.0,create a categorical feature using some string
0.8.0,create a categorical feature using some integer
0.8.0,return the categories
0.8.0,create 2 random continuous feature
0.8.0,create a categorical feature using some string
0.8.0,create a categorical feature using some integer
0.8.0,return the categories
0.8.0,create 2 random continuous feature
0.8.0,create a categorical feature using some string
0.8.0,create a categorical feature using some integer
0.8.0,part of the common test which apply to SMOTE-NC even if it is not default
0.8.0,constructible
0.8.0,Check that the samplers handle pandas dataframe and pandas series
0.8.0,Cast X and y to not default dtype
0.8.0,Non-regression test for #662
0.8.0,https://github.com/scikit-learn-contrib/imbalanced-learn/issues/662
0.8.0,check that the categorical feature is not random but correspond to the
0.8.0,categories seen in the minority class samples
0.8.0,overall check for SMOTEN
0.8.0,check if the SMOTEN resample data as expected
0.8.0,"we generate data such that ""not apple"" will be the minority class and"
0.8.0,"samples from this class will be generated. We will force the ""blue"""
0.8.0,"category to be associated with this class. Therefore, the new generated"
0.8.0,"samples should as well be from the ""blue"" category."
0.8.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.8.0,Christos Aridas
0.8.0,License: MIT
0.8.0,check that m_neighbors is properly set. Regression test for:
0.8.0,https://github.com/scikit-learn-contrib/imbalanced-learn/issues/568
0.8.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.8.0,Christos Aridas
0.8.0,License: MIT
0.8.0,check the random over-sampling with a multiclass problem
0.8.0,check that resampling with heterogeneous dtype is working with basic
0.8.0,resampling
0.8.0,check that we can oversample even with missing or infinite data
0.8.0,regression tests for #605
0.8.0,check that we raise an error when heterogeneous dtype data are given
0.8.0,and a smoothed bootstrap is requested
0.8.0,check that smoothed bootstrap is working for numerical array
0.8.0,check that a shrinkage factor of 0 is equivalent to not create a smoothed
0.8.0,bootstrap
0.8.0,check the behaviour of the shrinkage parameter
0.8.0,the covariance of the data generated with the larger shrinkage factor
0.8.0,should also be larger.
0.8.0,check the validation of the shrinkage parameter
0.8.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.8.0,Christos Aridas
0.8.0,License: MIT
0.8.0,shuffle the indices since the sampler are packing them by class
0.8.0,helper functions
0.8.0,input and output
0.8.0,build the model and weights
0.8.0,"build the loss, predict, and train operator"
0.8.0,Initialization of all variables in the graph
0.8.0,"For each epoch, run accuracy on train and test"
0.8.0,helper functions
0.8.0,input and output
0.8.0,build the model and weights
0.8.0,"build the loss, predict, and train operator"
0.8.0,Initialization of all variables in the graph
0.8.0,"For each epoch, run accuracy on train and test"
0.8.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.8.0,License: MIT
0.8.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.8.0,Fernando Nogueira
0.8.0,Christos Aridas
0.8.0,License: MIT
0.8.0,find which class to not consider
0.8.0,there is a Tomek link between two samples if they are both nearest
0.8.0,neighbors of each others.
0.8.0,Find the nearest neighbour of every point
0.8.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.8.0,Christos Aridas
0.8.0,License: MIT
0.8.0,Randomly get one sample from the majority class
0.8.0,Generate the index to select
0.8.0,Create the set C - One majority samples and all minority
0.8.0,Create the set S - all majority samples
0.8.0,fit knn on C
0.8.0,Check each sample in S if we keep it or drop it
0.8.0,Do not select sample which are already well classified
0.8.0,Classify on S
0.8.0,If the prediction do not agree with the true label
0.8.0,append it in C_x
0.8.0,Keep the index for later
0.8.0,Update C
0.8.0,fit a knn on C
0.8.0,This experimental to speed up the search
0.8.0,Classify all the element in S and avoid to test the
0.8.0,well classified elements
0.8.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.8.0,Dayvid Oliveira
0.8.0,Christos Aridas
0.8.0,License: MIT
0.8.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.8.0,Christos Aridas
0.8.0,License: MIT
0.8.0,Compute the distance considering the farthest neighbour
0.8.0,Sort the list of distance and get the index
0.8.0,Throw a warning to tell the user that we did not have enough samples
0.8.0,to select and that we just select everything
0.8.0,Select the desired number of samples
0.8.0,idx_tmp is relative to the feature selected in the
0.8.0,previous step and we need to find the indirection
0.8.0,fmt: off
0.8.0,fmt: on
0.8.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.8.0,Christos Aridas
0.8.0,License: MIT
0.8.0,select a sample from the current class
0.8.0,create the set composed of all minority samples and one
0.8.0,sample from the current class.
0.8.0,create the set S with removing the seed from S
0.8.0,since that it will be added anyway
0.8.0,apply Tomek cleaning
0.8.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.8.0,Dayvid Oliveira
0.8.0,Christos Aridas
0.8.0,License: MIT
0.8.0,Check the stopping criterion
0.8.0,1. If there is no changes for the vector y
0.8.0,2. If the number of samples in the other class become inferior to
0.8.0,the number of samples in the majority class
0.8.0,3. If one of the class is disappearing
0.8.0,Case 1
0.8.0,Case 2
0.8.0,Case 3
0.8.0,Check the stopping criterion
0.8.0,1. If the number of samples in the other class become inferior to
0.8.0,the number of samples in the majority class
0.8.0,2. If one of the class is disappearing
0.8.0,Case 1else:
0.8.0,overwrite b_min_bec_maj
0.8.0,Case 2
0.8.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.8.0,Christos Aridas
0.8.0,License: MIT
0.8.0,clean the neighborhood
0.8.0,compute which classes to consider for cleaning for the A2 group
0.8.0,compute a2 group
0.8.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.8.0,Christos Aridas
0.8.0,License: MIT
0.8.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.8.0,Christos Aridas
0.8.0,License: MIT
0.8.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.8.0,Christos Aridas
0.8.0,License: MIT
0.8.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.8.0,Christos Aridas
0.8.0,License: MIT
0.8.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.8.0,Christos Aridas
0.8.0,License: MIT
0.8.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.8.0,Christos Aridas
0.8.0,License: MIT
0.8.0,check that we can undersample even with missing or infinite data
0.8.0,regression tests for #605
0.8.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.8.0,Christos Aridas
0.8.0,License: MIT
0.8.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.8.0,Christos Aridas
0.8.0,License: MIT
0.8.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.8.0,Christos Aridas
0.8.0,License: MIT
0.8.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.8.0,Christos Aridas
0.8.0,License: MIT
0.8.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.8.0,Christos Aridas
0.8.0,License: MIT
0.8.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.8.0,Fernando Nogueira
0.8.0,Christos Aridas
0.8.0,License: MIT
0.8.0,check that we deprecate the `n_jobs` parameter.
0.8.0,check that the samples selecting by the hard voting corresponds to the
0.8.0,targeted class
0.8.0,non-regression test for:
0.8.0,https://github.com/scikit-learn-contrib/imbalanced-learn/issues/738
0.8.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.8.0,Christos Aridas
0.8.0,License: MIT
0.8.0,test that all_estimators doesn't find abstract classes.
0.8.0,"For NearMiss, let's check the three algorithms"
0.8.0,Common tests for estimator instances
0.8.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.8.0,License: MIT
0.8.0,check that we can let a pass a regression variable by turning down the
0.8.0,validation
0.8.0,Check that the validation is bypass when calling `fit`
0.8.0,Non-regression test for:
0.8.0,https://github.com/scikit-learn-contrib/imbalanced-learn/issues/782
0.8.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.8.0,Christos Aridas
0.8.0,License: MIT
0.8.0,store timestamp to figure out whether the result of 'fit' has been
0.8.0,cached or not
0.8.0,store timestamp to figure out whether the result of 'fit' has been
0.8.0,cached or not
0.8.0,Pipeline accepts steps as tuple
0.8.0,Test the various init parameters of the pipeline.
0.8.0,Check that we can't instantiate pipelines with objects without fit
0.8.0,method
0.8.0,Smoke test with only an estimator
0.8.0,Check that params are set
0.8.0,Smoke test the repr:
0.8.0,Test with two objects
0.8.0,Check that we can't instantiate with non-transformers on the way
0.8.0,"Note that NoTrans implements fit, but not transform"
0.8.0,Check that params are set
0.8.0,Smoke test the repr:
0.8.0,Check that params are not set when naming them wrong
0.8.0,Test clone
0.8.0,"Check that apart from estimators, the parameters are the same"
0.8.0,Remove estimators that where copied
0.8.0,Test the various methods of the pipeline (anova).
0.8.0,Test with Anova + LogisticRegression
0.8.0,Test that the pipeline can take fit parameters
0.8.0,classifier should return True
0.8.0,and transformer params should not be changed
0.8.0,invalid parameters should raise an error message
0.8.0,Pipeline should pass sample_weight
0.8.0,When sample_weight is None it shouldn't be passed
0.8.0,Test pipeline raises set params error message for nested models.
0.8.0,nested model check
0.8.0,Test the various methods of the pipeline (pca + svm).
0.8.0,Test with PCA + SVC
0.8.0,Test the various methods of the pipeline (preprocessing + svm).
0.8.0,check shapes of various prediction functions
0.8.0,test that the fit_predict method is implemented on a pipeline
0.8.0,test that the fit_predict on pipeline yields same results as applying
0.8.0,transform and clustering steps separately
0.8.0,"As pipeline doesn't clone estimators on construction,"
0.8.0,it must have its own estimators
0.8.0,first compute the transform and clustering step separately
0.8.0,use a pipeline to do the transform and clustering in one step
0.8.0,tests that a pipeline does not have fit_predict method when final
0.8.0,step of pipeline does not have fit_predict defined
0.8.0,tests that Pipeline passes fit_params to intermediate steps
0.8.0,when fit_predict is invoked
0.8.0,Test whether pipeline works with a transformer at the end.
0.8.0,Also test pipeline.transform and pipeline.inverse_transform
0.8.0,test transform and fit_transform:
0.8.0,Test whether pipeline works with a transformer missing fit_transform
0.8.0,test fit_transform:
0.8.0,Directly setting attr
0.8.0,Using set_params
0.8.0,Using set_params to replace single step
0.8.0,With invalid data
0.8.0,Test setting Pipeline steps to None
0.8.0,"for other methods, ensure no AttributeErrors on None:"
0.8.0,mult2 and mult3 are active
0.8.0,Check 'passthrough' step at construction time
0.8.0,Test that an error is raised when memory is not a string or a Memory
0.8.0,instance
0.8.0,Define memory as an integer
0.8.0,Test with Transformer + SVC
0.8.0,Memoize the transformer at the first fit
0.8.0,Get the time stamp of the tranformer in the cached pipeline
0.8.0,Check that cached_pipe and pipe yield identical results
0.8.0,Check that we are reading the cache while fitting
0.8.0,a second time
0.8.0,Check that cached_pipe and pipe yield identical results
0.8.0,Create a new pipeline with cloned estimators
0.8.0,Check that even changing the name step does not affect the cache hit
0.8.0,Check that cached_pipe and pipe yield identical results
0.8.0,Test with Transformer + SVC
0.8.0,Memoize the transformer at the first fit
0.8.0,Get the time stamp of the tranformer in the cached pipeline
0.8.0,Check that cached_pipe and pipe yield identical results
0.8.0,Check that we are reading the cache while fitting
0.8.0,a second time
0.8.0,Check that cached_pipe and pipe yield identical results
0.8.0,Create a new pipeline with cloned estimators
0.8.0,Check that even changing the name step does not affect the cache hit
0.8.0,Check that cached_pipe and pipe yield identical results
0.8.0,Test the various methods of the pipeline (pca + svm).
0.8.0,Test with PCA + SVC
0.8.0,Test the various methods of the pipeline (pca + svm).
0.8.0,Test with PCA + SVC
0.8.0,Test whether pipeline works with a sampler at the end.
0.8.0,Also test pipeline.sampler
0.8.0,test transform and fit_transform:
0.8.0,We round the value near to zero. It seems that PCA has some issue
0.8.0,with that
0.8.0,Test whether pipeline works with a sampler at the end.
0.8.0,Also test pipeline.sampler
0.8.0,Test pipeline using None as preprocessing step and a classifier
0.8.0,"Test pipeline using None, RUS and a classifier"
0.8.0,"Test pipeline using RUS, None and a classifier"
0.8.0,Test pipeline using None step and a sampler
0.8.0,Test pipeline using None and a transformer that implements transform and
0.8.0,inverse_transform
0.8.0,Test the various methods of the pipeline (anova).
0.8.0,Test with RandomUnderSampling + Anova + LogisticRegression
0.8.0,Test the various methods of the pipeline (anova).
0.8.0,Test the various methods of the pipeline (anova).
0.8.0,Test with RandomUnderSampling + Anova + LogisticRegression
0.8.0,tests that Pipeline passes predict_params to the final estimator
0.8.0,when predict is invoked
0.8.0,Test that the score_samples method is implemented on a pipeline.
0.8.0,Test that the score_samples method on pipeline yields same results as
0.8.0,applying transform and score_samples steps separately.
0.8.0,Check the shapes
0.8.0,Check the values
0.8.0,Test that a pipeline does not have score_samples method when the final
0.8.0,step of the pipeline does not have score_samples defined.
0.8.0,Test that the score_samples method is implemented on a pipeline.
0.8.0,Test that the score_samples method on pipeline yields same results as
0.8.0,applying transform and score_samples steps separately.
0.8.0,Check the shapes
0.8.0,Check the values
0.8.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.8.0,Christos Aridas
0.8.0,License: MIT
0.8.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.8.0,License: MIT
0.8.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.8.0,License: MIT
0.8.0,Adapated from scikit-learn
0.8.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.8.0,License: MIT
0.8.0,we don't filter samplers based on their tag here because we want to make
0.8.0,sure that the fitted attribute does not exist if the tag is not
0.8.0,stipulated
0.8.0,trigger our checks if this is a SamplerMixin
0.8.0,should raise warning if the target is continuous (we cannot raise error)
0.8.0,if the target is multilabel then we should raise an error
0.8.0,IHT does not enforce the number of samples but provide a number
0.8.0,of samples the closest to the desired target.
0.8.0,in this test we will force all samplers to not change the class 1
0.8.0,check that sparse matrices can be passed through the sampler leading to
0.8.0,the same results than dense
0.8.0,Check that the samplers handle pandas dataframe and pandas series
0.8.0,check that we return the same type for dataframes or series types
0.8.0,Check that the can samplers handle simple lists
0.8.0,Check that multiclass target lead to the same results than OVA encoding
0.8.0,Cast X and y to not default dtype
0.8.0,Non-regression test for #709
0.8.0,https://github.com/scikit-learn-contrib/imbalanced-learn/issues/709
0.8.0,Adapted from scikit-learn
0.8.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.8.0,License: MIT
0.8.0,Ignore deprecation warnings triggered at import time and from walking
0.8.0,packages
0.8.0,get rid of abstract base classes
0.8.0,get rid of sklearn estimators which have been imported in some classes
0.8.0,"drop duplicates, sort for reproducibility"
0.8.0,itemgetter is used to ensure the sort does not extend to the 2nd item of
0.8.0,the tuple
0.8.0,Author: Alexander L. Hayes <hayesall@iu.edu>
0.8.0,License: MIT
0.8.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.8.0,License: MIT
0.8.0,check that all keys in sampling_strategy are also in y
0.8.0,check that there is no negative number
0.8.0,check that all keys in sampling_strategy are also in y
0.8.0,ignore first 'self' argument for instance methods
0.8.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.8.0,Christos Aridas
0.8.0,License: MIT
0.8.0,this function could create an equal number of samples
0.8.0,We pass on purpose a non sorted dictionary and check that the resulting
0.8.0,dictionary is sorted. Refer to issue #428.
0.8.0,DataFrame and DataFrame case
0.8.0,DataFrames and Series case
0.8.0,The * is place before a keyword only argument without a default value
0.8.0,Author: Alexander L. Hayes <hayesall@iu.edu>
0.8.0,License: MIT
0.8.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.8.0,License: MIT
0.8.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.8.0,Christos Aridas
0.8.0,License: MIT
0.8.0,check if the filtering is working with a list or a single string
0.8.0,check that all estimators are sampler
0.8.0,check that an error is raised when the type is unknown
0.8.0,TODO: remove in 0.9
0.8.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.8.0,License: MIT
0.8.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.8.0,Christos Aridas
0.8.0,License: MIT
0.8.0,Otherwise create a default SMOTE
0.8.0,Otherwise create a default TomekLinks
0.8.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.8.0,Christos Aridas
0.8.0,License: MIT
0.8.0,Otherwise create a default SMOTE
0.8.0,Otherwise create a default EditedNearestNeighbours
0.8.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.8.0,Christos Aridas
0.8.0,License: MIT
0.8.0,Check if default job count is None
0.8.0,Check if job count is set
0.8.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.8.0,Christos Aridas
0.8.0,License: MIT
0.8.0,Check if default job count is none
0.8.0,Check if job count is set
0.8.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.8.0,License: MIT
0.8.0,resample before to fit the tree
0.8.0,Validate or convert input data
0.8.0,Pre-sort indices to avoid that each individual tree of the
0.8.0,ensemble sorts the indices.
0.8.0,Remap output
0.8.0,reshape is necessary to preserve the data contiguity against vs
0.8.0,"[:, np.newaxis] that does not."
0.8.0,Get bootstrap sample size
0.8.0,Check parameters
0.8.0,"Free allocated memory, if any"
0.8.0,We draw from the random state to get the random state we
0.8.0,would have got if we hadn't used a warm_start.
0.8.0,Parallel loop: we prefer the threading backend as the Cython code
0.8.0,for fitting the trees is internally releasing the Python GIL
0.8.0,making threading more efficient than multiprocessing in
0.8.0,"that case. However, we respect any parallel_backend contexts set"
0.8.0,"at a higher level, since correctness does not rely on using"
0.8.0,threads.
0.8.0,Collect newly grown trees
0.8.0,Create pipeline with the fitted samplers and trees
0.8.0,Decapsulate classes_ attributes
0.8.0,"with the resampling, we are likely to have rows not included"
0.8.0,for the OOB score leading to division by zero
0.8.0,Instances incorrectly classified
0.8.0,Error fraction
0.8.0,Stop if classification is perfect
0.8.0,Construct y coding as described in Zhu et al [2]:
0.8.0,
0.8.0,y_k = 1 if c == k else -1 / (K - 1)
0.8.0,
0.8.0,"where K == n_classes_ and c, k in [0, K) are indices along the second"
0.8.0,axis of the y coding with c being the index corresponding to the true
0.8.0,class label.
0.8.0,Displace zero probabilities so the log is defined.
0.8.0,Also fix negative elements which may occur with
0.8.0,negative sample weights.
0.8.0,Boost weight using multi-class AdaBoost SAMME.R alg
0.8.0,Only boost the weights if it will fit again
0.8.0,Only boost positive weights
0.8.0,Instances incorrectly classified
0.8.0,Error fraction
0.8.0,Stop if classification is perfect
0.8.0,Stop if the error is at least as bad as random guessing
0.8.0,Boost weight using multi-class AdaBoost SAMME alg
0.8.0,Only boost the weights if I will fit again
0.8.0,Only boost positive weights
0.8.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.8.0,Christos Aridas
0.8.0,License: MIT
0.8.0,RandomUnderSampler is not supporting sample_weight. We need to pass
0.8.0,None.
0.8.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.8.0,Christos Aridas
0.8.0,License: MIT
0.8.0,the sampler needs to be validated before to call _fit because
0.8.0,_validate_y is called before _validate_estimator and would require
0.8.0,to know which type of sampler we are using.
0.8.0,RandomUnderSampler is not supporting sample_weight. We need to pass
0.8.0,None.
0.8.0,check that we have an ensemble of samplers and estimators with a
0.8.0,consistent size
0.8.0,each sampler in the ensemble should have different random state
0.8.0,each estimator in the ensemble should have different random state
0.8.0,check the consistency of the feature importances
0.8.0,check the consistency of the prediction outpus
0.8.0,Predictions should be the same when sample_weight are all ones
0.8.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.8.0,Christos Aridas
0.8.0,License: MIT
0.8.0,Check classification for various parameter settings.
0.8.0,Test that bootstrapping samples generate non-perfect base estimators.
0.8.0,"without bootstrap, all trees are perfect on the training set"
0.8.0,disable the resampling by passing an empty dictionary.
0.8.0,"with bootstrap, trees are no longer perfect on the training set"
0.8.0,Test that bootstrapping features may generate duplicate features.
0.8.0,Predict probabilities.
0.8.0,Normal case
0.8.0,"Degenerate case, where some classes are missing"
0.8.0,Check that oob prediction is a good estimation of the generalization
0.8.0,error.
0.8.0,Test with few estimators
0.8.0,Check singleton ensembles.
0.8.0,Test that it gives proper exception on deficient input.
0.8.0,Test support of decision_function
0.8.0,Check that bagging ensembles can be grid-searched.
0.8.0,Transform iris into a binary classification task
0.8.0,Grid search with scoring based on decision_function
0.8.0,Check base_estimator and its default values.
0.8.0,Test if fitting incrementally with warm start gives a forest of the
0.8.0,right size and the same results as a normal fit.
0.8.0,Test if warm start'ed second fit with smaller n_estimators raises error.
0.8.0,Test that nothing happens when fitting without increasing n_estimators
0.8.0,"modify X to nonsense values, this should not change anything"
0.8.0,warm started classifier with 5+5 estimators should be equivalent to
0.8.0,one classifier with 10 estimators
0.8.0,Check using oob_score and warm_start simultaneously fails
0.8.0,"Make sure OOB scores are identical when random_state, estimator, and"
0.8.0,training data are fixed and fitting is done twice
0.8.0,Check that format of estimators_samples_ is correct and that results
0.8.0,generated at fit time can be identically reproduced at a later time
0.8.0,using data saved in object attributes.
0.8.0,remap the y outside of the BalancedBaggingclassifier
0.8.0,"_, y = np.unique(y, return_inverse=True)"
0.8.0,Get relevant attributes
0.8.0,Test for correct formatting
0.8.0,Re-fit single estimator to test for consistent sampling
0.8.0,Make sure validated max_samples and original max_samples are identical
0.8.0,when valid integer max_samples supplied by user
0.8.0,check that we can pass any kind of sampler to a bagging classifier
0.8.0,check that we have balanced class with the right counts of class
0.8.0,sample depending on the sampling strategy
0.8.0,check that we can provide a FunctionSampler in BalancedBaggingClassifier
0.8.0,find the minority and majority classes
0.8.0,compute the number of sample to draw from the majority class using
0.8.0,a negative binomial distribution
0.8.0,draw randomly with or without replacement
0.8.0,Roughly Balanced Bagging
0.8.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.8.0,Christos Aridas
0.8.0,License: MIT
0.8.0,Generate a global dataset to use
0.8.0,Check classification for various parameter settings.
0.8.0,test the different prediction function
0.8.0,Check base_estimator and its default values.
0.8.0,Test if fitting incrementally with warm start gives a forest of the
0.8.0,right size and the same results as a normal fit.
0.8.0,Test if warm start'ed second fit with smaller n_estimators raises error.
0.8.0,Test that nothing happens when fitting without increasing n_estimators
0.8.0,"modify X to nonsense values, this should not change anything"
0.8.0,warm started classifier with 5+5 estimators should be equivalent to
0.8.0,one classifier with 10 estimators
0.8.0,Check warning if not enough estimators
0.8.0,First fit with no restriction on max samples
0.8.0,Second fit with max samples restricted to just 2
0.8.0,Regression test for #655: check that the oob score is closed to 0.5
0.8.0,a binomial experiment.
0.8.0,Author: Guillaume Lemaitre
0.8.0,License: BSD 3 clause
0.8.0,"The index start at one, then we need to remove one"
0.8.0,to not have issue with the indexing.
0.8.0,go through the list and check if the data are available
0.8.0,Authors: Dayvid Oliveira
0.8.0,Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.8.0,Christos Aridas
0.8.0,License: MIT
0.8.0,restrict ratio to be a dict or a callable
0.8.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.8.0,Christos Aridas
0.8.0,License: MIT
0.8.0,"we are reusing part of utils.check_sampling_strategy, however this is not"
0.8.0,cover in the common tests so we will repeat it here
0.8.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.8.0,Christos Aridas
0.8.0,License: MIT
0.8.0,This is a trick to avoid an error during tests collection with pytest. We
0.8.0,avoid the error when importing the package raise the error at the moment of
0.8.0,creating the instance.
0.8.0,flag for keras sequence duck-typing
0.8.0,shuffle the indices since the sampler are packing them by class
0.7.0,This file is here so that when running from the root folder
0.7.0,./sklearn is added to sys.path by pytest.
0.7.0,See https://docs.pytest.org/en/latest/pythonpath.html for more details.
0.7.0,"For example, this allows to build extensions in place and run pytest"
0.7.0,doc/modules/clustering.rst and use sklearn from the local folder
0.7.0,rather than the one from site-packages.
0.7.0,Set numpy array str/repr to legacy behaviour on numpy > 1.13 to make
0.7.0,the doctests pass
0.7.0,! /usr/bin/env python
0.7.0,get __version__ from _version.py
0.7.0,-*- coding: utf-8 -*-
0.7.0,
0.7.0,"imbalanced-learn documentation build configuration file, created by"
0.7.0,sphinx-quickstart on Mon Jan 18 14:44:12 2016.
0.7.0,
0.7.0,This file is execfile()d with the current directory set to its
0.7.0,containing dir.
0.7.0,
0.7.0,Note that not all possible configuration values are present in this
0.7.0,autogenerated file.
0.7.0,
0.7.0,All configuration values have a default; values that are commented out
0.7.0,serve to show the default.
0.7.0,"If extensions (or modules to document with autodoc) are in another directory,"
0.7.0,add these directories to sys.path here. If the directory is relative to the
0.7.0,"documentation root, use os.path.abspath to make it absolute, like shown here."
0.7.0,-- General configuration ------------------------------------------------
0.7.0,"If your documentation needs a minimal Sphinx version, state it here."
0.7.0,needs_sphinx = '1.0'
0.7.0,"Add any Sphinx extension module names here, as strings. They can be"
0.7.0,extensions coming with Sphinx (named 'sphinx.ext.*') or your custom
0.7.0,ones.
0.7.0,this is needed for some reason...
0.7.0,see https://github.com/numpy/numpydoc/issues/69
0.7.0,"Add any paths that contain templates here, relative to this directory."
0.7.0,generate autosummary even if no references
0.7.0,The suffix of source filenames.
0.7.0,The encoding of source files.
0.7.0,source_encoding = 'utf-8-sig'
0.7.0,Generate the plot for the gallery
0.7.0,The master toctree document.
0.7.0,General information about the project.
0.7.0,"The version info for the project you're documenting, acts as replacement for"
0.7.0,"|version| and |release|, also used in various other places throughout the"
0.7.0,built documents.
0.7.0,
0.7.0,The short X.Y version.
0.7.0,"The full version, including alpha/beta/rc tags."
0.7.0,The language for content autogenerated by Sphinx. Refer to documentation
0.7.0,for a list of supported languages.
0.7.0,language = None
0.7.0,"There are two options for replacing |today|: either, you set today to some"
0.7.0,"non-false value, then it is used:"
0.7.0,today = ''
0.7.0,"Else, today_fmt is used as the format for a strftime call."
0.7.0,"today_fmt = '%B %d, %Y'"
0.7.0,"List of patterns, relative to source directory, that match files and"
0.7.0,directories to ignore when looking for source files.
0.7.0,The reST default role (used for this markup: `text`) to use for all
0.7.0,documents.
0.7.0,"If true, '()' will be appended to :func: etc. cross-reference text."
0.7.0,"If true, the current module name will be prepended to all description"
0.7.0,unit titles (such as .. function::).
0.7.0,add_module_names = True
0.7.0,"If true, sectionauthor and moduleauthor directives will be shown in the"
0.7.0,output. They are ignored by default.
0.7.0,show_authors = False
0.7.0,The name of the Pygments (syntax highlighting) style to use.
0.7.0,Custom style
0.7.0,A list of ignored prefixes for module index sorting.
0.7.0,modindex_common_prefix = []
0.7.0,"If true, keep warnings as ""system message"" paragraphs in the built documents."
0.7.0,keep_warnings = False
0.7.0,-- Options for HTML output ----------------------------------------------
0.7.0,The theme to use for HTML and HTML Help pages.  See the documentation for
0.7.0,a list of builtin themes.
0.7.0,Theme options are theme-specific and customize the look and feel of a theme
0.7.0,"further.  For a list of options available for each theme, see the"
0.7.0,documentation.
0.7.0,html_theme_options = {}
0.7.0,"Add any paths that contain custom themes here, relative to this directory."
0.7.0,"The name for this set of Sphinx documents.  If None, it defaults to"
0.7.0,"""<project> v<release> documentation""."
0.7.0,html_title = None
0.7.0,A shorter title for the navigation bar.  Default is the same as html_title.
0.7.0,html_short_title = None
0.7.0,The name of an image file (relative to this directory) to place at the top
0.7.0,of the sidebar.
0.7.0,html_logo = None
0.7.0,The name of an image file (within the static path) to use as favicon of the
0.7.0,docs.  This file should be a Windows icon file (.ico) being 16x16 or 32x32
0.7.0,pixels large.
0.7.0,html_favicon = None
0.7.0,"Add any paths that contain custom static files (such as style sheets) here,"
0.7.0,"relative to this directory. They are copied after the builtin static files,"
0.7.0,"so a file named ""default.css"" will overwrite the builtin ""default.css""."
0.7.0,Add any extra paths that contain custom files (such as robots.txt or
0.7.0,".htaccess) here, relative to this directory. These files are copied"
0.7.0,directly to the root of the documentation.
0.7.0,html_extra_path = []
0.7.0,"If not '', a 'Last updated on:' timestamp is inserted at every page bottom,"
0.7.0,using the given strftime format.
0.7.0,"html_last_updated_fmt = '%b %d, %Y'"
0.7.0,"If true, SmartyPants will be used to convert quotes and dashes to"
0.7.0,typographically correct entities.
0.7.0,html_use_smartypants = True
0.7.0,"Custom sidebar templates, maps document names to template names."
0.7.0,html_sidebars = {}
0.7.0,"Additional templates that should be rendered to pages, maps page names to"
0.7.0,template names.
0.7.0,html_additional_pages = {}
0.7.0,"If false, no module index is generated."
0.7.0,html_domain_indices = True
0.7.0,"If false, no index is generated."
0.7.0,html_use_index = True
0.7.0,"If true, the index is split into individual pages for each letter."
0.7.0,html_split_index = False
0.7.0,"If true, links to the reST sources are added to the pages."
0.7.0,html_show_sourcelink = True
0.7.0,"If true, ""Created using Sphinx"" is shown in the HTML footer. Default is True."
0.7.0,html_show_sphinx = True
0.7.0,"If true, ""(C) Copyright ..."" is shown in the HTML footer. Default is True."
0.7.0,html_show_copyright = True
0.7.0,"If true, an OpenSearch description file will be output, and all pages will"
0.7.0,contain a <link> tag referring to it.  The value of this option must be the
0.7.0,base URL from which the finished HTML is served.
0.7.0,html_use_opensearch = ''
0.7.0,"This is the file name suffix for HTML files (e.g. "".xhtml"")."
0.7.0,html_file_suffix = None
0.7.0,Output file base name for HTML help builder.
0.7.0,-- Options for LaTeX output ---------------------------------------------
0.7.0,The paper size ('letterpaper' or 'a4paper').
0.7.0,"'papersize': 'letterpaper',"
0.7.0,"The font size ('10pt', '11pt' or '12pt')."
0.7.0,"'pointsize': '10pt',"
0.7.0,Additional stuff for the LaTeX preamble.
0.7.0,"'preamble': '',"
0.7.0,Grouping the document tree into LaTeX files. List of tuples
0.7.0,"(source start file, target name, title,"
0.7.0,"author, documentclass [howto, manual, or own class])."
0.7.0,The name of an image file (relative to this directory) to place at the top of
0.7.0,the title page.
0.7.0,latex_logo = None
0.7.0,"For ""manual"" documents, if this is true, then toplevel headings are parts,"
0.7.0,not chapters.
0.7.0,latex_use_parts = False
0.7.0,"If true, show page references after internal links."
0.7.0,latex_show_pagerefs = False
0.7.0,"If true, show URL addresses after external links."
0.7.0,latex_show_urls = False
0.7.0,Documents to append as an appendix to all manuals.
0.7.0,latex_appendices = []
0.7.0,intersphinx configuration
0.7.0,sphinx-gallery configuration
0.7.0,-- Options for manual page output ---------------------------------------
0.7.0,"If false, no module index is generated."
0.7.0,latex_domain_indices = True
0.7.0,One entry per manual page. List of tuples
0.7.0,"(source start file, name, description, authors, manual section)."
0.7.0,"If true, show URL addresses after external links."
0.7.0,man_show_urls = False
0.7.0,-- Options for Texinfo output -------------------------------------------
0.7.0,Grouping the document tree into Texinfo files. List of tuples
0.7.0,"(source start file, target name, title, author,"
0.7.0,"dir menu entry, description, category)"
0.7.0,"def generate_example_rst(app, what, name, obj, options, lines):"
0.7.0,"# generate empty examples files, so that we don't get"
0.7.0,# inclusion errors if there are no examples for a class / module
0.7.0,"examples_path = os.path.join(app.srcdir, ""generated"","
0.7.0,"""%s.examples"" % name)"
0.7.0,if not os.path.exists(examples_path):
0.7.0,# touch file
0.7.0,"open(examples_path, 'w').close()"
0.7.0,Config for sphinx_issues
0.7.0,Temporary work-around for spacing problem between parameter and parameter
0.7.0,"type in the doc, see https://github.com/numpy/numpydoc/issues/215. The bug"
0.7.0,has been fixed in sphinx (https://github.com/sphinx-doc/sphinx/pull/5976) but
0.7.0,through a change in sphinx basic.css except rtd_theme does not use basic.css.
0.7.0,"In an ideal world, this would get fixed in this PR:"
0.7.0,https://github.com/readthedocs/sphinx_rtd_theme/pull/747/files
0.7.0,"app.connect('autodoc-process-docstring', generate_example_rst)"
0.7.0,Documents to append as an appendix to all manuals.
0.7.0,texinfo_appendices = []
0.7.0,"If false, no module index is generated."
0.7.0,texinfo_domain_indices = True
0.7.0,"How to display URL addresses: 'footnote', 'no', or 'inline'."
0.7.0,texinfo_show_urls = 'footnote'
0.7.0,"If true, do not generate a @detailmenu in the ""Top"" node's menu."
0.7.0,texinfo_no_detailmenu = False
0.7.0,The following is used by sphinx.ext.linkcode to provide links to github
0.7.0,get the styles from the current theme
0.7.0,create and add the button to all the code blocks that contain >>>
0.7.0,tracebacks (.gt) contain bare text elements that need to be
0.7.0,wrapped in a span to work with .nextUntil() (see later)
0.7.0,define the behavior of the button when it's clicked
0.7.0,hide the code output
0.7.0,show the code output
0.7.0,-*- coding: utf-8 -*-
0.7.0,Format template for issues URI
0.7.0,e.g. 'https://github.com/sloria/marshmallow/issues/{issue}
0.7.0,Format template for PR URI
0.7.0,e.g. 'https://github.com/sloria/marshmallow/pull/{issue}
0.7.0,Format template for commit URI
0.7.0,e.g. 'https://github.com/sloria/marshmallow/commits/{commit}
0.7.0,"Shortcut for Github, e.g. 'sloria/marshmallow'"
0.7.0,Format template for user profile URI
0.7.0,e.g. 'https://github.com/{user}'
0.7.0,Python 2 only
0.7.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.7.0,License: MIT
0.7.0,##############################################################################
0.7.0,"First, we will create an imbalanced data set from a the iris data set."
0.7.0,##############################################################################
0.7.0,Using ``sampling_strategy`` in resampling algorithms
0.7.0,##############################################################################
0.7.0,##############################################################################
0.7.0,``sampling_strategy`` as a ``float``
0.7.0,....................................
0.7.0,
0.7.0,``sampling_strategy`` can be given a ``float``. For **under-sampling
0.7.0,"methods**, it corresponds to the ratio :math:`\\alpha_{us}` defined by"
0.7.0,:math:`N_{rM} = \\alpha_{us} \\times N_{m}` where :math:`N_{rM}` and
0.7.0,:math:`N_{m}` are the number of samples in the majority class after
0.7.0,"resampling and the number of samples in the minority class, respectively."
0.7.0,select only 2 classes since the ratio make sense in this case
0.7.0,##############################################################################
0.7.0,"For **over-sampling methods**, it correspond to the ratio"
0.7.0,:math:`\\alpha_{os}` defined by :math:`N_{rm} = \\alpha_{os} \\times N_{M}`
0.7.0,where :math:`N_{rm}` and :math:`N_{M}` are the number of samples in the
0.7.0,minority class after resampling and the number of samples in the majority
0.7.0,"class, respectively."
0.7.0,##############################################################################
0.7.0,``sampling_strategy`` has a ``str``
0.7.0,...................................
0.7.0,
0.7.0,``sampling_strategy`` can be given as a string which specify the class
0.7.0,"targeted by the resampling. With under- and over-sampling, the number of"
0.7.0,samples will be equalized.
0.7.0,
0.7.0,Note that we are using multiple classes from now on.
0.7.0,##############################################################################
0.7.0,"With **cleaning method**, the number of samples in each class will not be"
0.7.0,equalized even if targeted.
0.7.0,##############################################################################
0.7.0,``sampling_strategy`` as a ``dict``
0.7.0,...................................
0.7.0,
0.7.0,"When ``sampling_strategy`` is a ``dict``, the keys correspond to the targeted"
0.7.0,classes. The values correspond to the desired number of samples for each
0.7.0,targeted class. This is working for both **under- and over-sampling**
0.7.0,algorithms but not for the **cleaning algorithms**. Use a ``list`` instead.
0.7.0,##############################################################################
0.7.0,``sampling_strategy`` as a ``list``
0.7.0,...................................
0.7.0,
0.7.0,"When ``sampling_strategy`` is a ``list``, the list contains the targeted"
0.7.0,classes. It is used only for **cleaning methods** and raise an error
0.7.0,otherwise.
0.7.0,##############################################################################
0.7.0,``sampling_strategy`` as a callable
0.7.0,...................................
0.7.0,
0.7.0,"When callable, function taking ``y`` and returns a ``dict``. The keys"
0.7.0,correspond to the targeted classes. The values correspond to the desired
0.7.0,number of samples for each class.
0.7.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.7.0,License: MIT
0.7.0,#############################################################################
0.7.0,Toy data generation
0.7.0,#############################################################################
0.7.0,#############################################################################
0.7.0,We are generating some non Gaussian data set contaminated with some unform
0.7.0,noise.
0.7.0,#############################################################################
0.7.0,We will generate some cleaned test data without outliers.
0.7.0,#############################################################################
0.7.0,How to use the :class:`imblearn.FunctionSampler`
0.7.0,#############################################################################
0.7.0,#############################################################################
0.7.0,We first define a function which will use
0.7.0,:class:`sklearn.ensemble.IsolationForest` to eliminate some outliers from
0.7.0,our dataset during training. The function passed to the
0.7.0,:class:`imblearn.FunctionSampler` will be called when using the method
0.7.0,``fit_resample``.
0.7.0,#############################################################################
0.7.0,Integrate it within a pipeline
0.7.0,#############################################################################
0.7.0,#############################################################################
0.7.0,"By elimnating outliers before the training, the classifier will be less"
0.7.0,affected during the prediction.
0.7.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.7.0,License: MIT
0.7.0,generate some data points
0.7.0,plot the majority and minority samples
0.7.0,draw the circle in which the new sample will generated
0.7.0,plot the line on which the sample will be generated
0.7.0,create and plot the new sample
0.7.0,make the plot nicer with legend and label
0.7.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.7.0,License: MIT
0.7.0,##############################################################################
0.7.0,The following function will be used to create toy dataset. It using the
0.7.0,``make_classification`` from scikit-learn but fixing some parameters.
0.7.0,##############################################################################
0.7.0,The following function will be used to plot the sample space after resampling
0.7.0,to illustrate the characterisitic of an algorithm.
0.7.0,make nice plotting
0.7.0,##############################################################################
0.7.0,The following function will be used to plot the decision function of a
0.7.0,classifier given some data.
0.7.0,##############################################################################
0.7.0,Illustration of the influence of the balancing ratio
0.7.0,##############################################################################
0.7.0,##############################################################################
0.7.0,We will first illustrate the influence of the balancing ratio on some toy
0.7.0,data using a linear SVM classifier. Greater is the difference between the
0.7.0,"number of samples in each class, poorer are the classfication results."
0.7.0,##############################################################################
0.7.0,Random over-sampling to balance the data set
0.7.0,##############################################################################
0.7.0,##############################################################################
0.7.0,Random over-sampling can be used to repeat some samples and balance the
0.7.0,number of samples between the dataset. It can be seen that with this trivial
0.7.0,approach the boundary decision is already less biaised toward the majority
0.7.0,class.
0.7.0,##############################################################################
0.7.0,More advanced over-sampling using ADASYN and SMOTE
0.7.0,##############################################################################
0.7.0,##############################################################################
0.7.0,"Instead of repeating the same samples when over-sampling, we can use some"
0.7.0,specific heuristic instead. ADASYN and SMOTE can be used in this case.
0.7.0,Make an identity sampler
0.7.0,##############################################################################
0.7.0,The following plot illustrate the difference between ADASYN and SMOTE. ADASYN
0.7.0,will focus on the samples which are difficult to classify with a
0.7.0,nearest-neighbors rule while regular SMOTE will not make any distinction.
0.7.0,"Therefore, the decision function depending of the algorithm."
0.7.0,##############################################################################
0.7.0,"Due to those sampling particularities, it can give rise to some specific"
0.7.0,issues as illustrated below.
0.7.0,##############################################################################
0.7.0,SMOTE proposes several variants by identifying specific samples to consider
0.7.0,during the resampling. The borderline version will detect which point to
0.7.0,select which are in the border between two classes. The SVM version will use
0.7.0,the support vectors found using an SVM algorithm to create new sample while
0.7.0,the KMeans version will make a clustering before to generate samples in each
0.7.0,cluster independently depending each cluster density.
0.7.0,##############################################################################
0.7.0,"When dealing with a mixed of continuous and categorical features, SMOTE-NC"
0.7.0,is the only method which can handle this case.
0.7.0,create a synthetic data set with continuous and categorical features
0.7.0,Authors: Christos Aridas
0.7.0,Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.7.0,License: MIT
0.7.0,Generate the dataset
0.7.0,make nice plotting
0.7.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.7.0,License: MIT
0.7.0,Generate a dataset
0.7.0,Split the data
0.7.0,Train the classifier with balancing
0.7.0,Test the classifier and get the prediction
0.7.0,Show the classification report
0.7.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.7.0,License: MIT
0.7.0,Generate a dataset
0.7.0,Split the data
0.7.0,Train the classifier with balancing
0.7.0,Test the classifier and get the prediction
0.7.0,##############################################################################
0.7.0,The geometric mean corresponds to the square root of the product of the
0.7.0,sensitivity and specificity. Combining the two metrics should account for
0.7.0,the balancing of the dataset.
0.7.0,##############################################################################
0.7.0,The index balanced accuracy can transform any metric to be used in
0.7.0,imbalanced learning problems.
0.7.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.7.0,License: MIT
0.7.0,##############################################################################
0.7.0,The following function will be used to create toy dataset. It using the
0.7.0,``make_classification`` from scikit-learn but fixing some parameters.
0.7.0,##############################################################################
0.7.0,The following function will be used to plot the sample space after resampling
0.7.0,to illustrate the characteristic of an algorithm.
0.7.0,make nice plotting
0.7.0,##############################################################################
0.7.0,The following function will be used to plot the decision function of a
0.7.0,classifier given some data.
0.7.0,##############################################################################
0.7.0,"``SMOTE`` allows to generate samples. However, this method of over-sampling"
0.7.0,"does not have any knowledge regarding the underlying distribution. Therefore,"
0.7.0,"some noisy samples can be generated, e.g. when the different classes cannot"
0.7.0,"be well separated. Hence, it can be beneficial to apply an under-sampling"
0.7.0,algorithm to clean the noisy samples. Two methods are usually used in the
0.7.0,literature: (i) Tomek's link and (ii) edited nearest neighbours cleaning
0.7.0,methods. Imbalanced-learn provides two ready-to-use samplers ``SMOTETomek``
0.7.0,"and ``SMOTEENN``. In general, ``SMOTEENN`` cleans more noisy data than"
0.7.0,``SMOTETomek``.
0.7.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.7.0,License: MIT
0.7.0,##############################################################################
0.7.0,Load an imbalanced dataset
0.7.0,##############################################################################
0.7.0,We will load the UCI SatImage dataset which has an imbalanced ratio of 9.3:1
0.7.0,(number of majority sample for a minority sample). The data are then split
0.7.0,into training and testing.
0.7.0,##############################################################################
0.7.0,Classification using a single decision tree
0.7.0,##############################################################################
0.7.0,We train a decision tree classifier which will be used as a baseline for the
0.7.0,rest of this example.
0.7.0,##############################################################################
0.7.0,The results are reported in terms of balanced accuracy and geometric mean
0.7.0,which are metrics widely used in the literature to validate model trained on
0.7.0,imbalanced set.
0.7.0,##############################################################################
0.7.0,Classification using bagging classifier with and without sampling
0.7.0,##############################################################################
0.7.0,"Instead of using a single tree, we will check if an ensemble of decsion tree"
0.7.0,"can actually alleviate the issue induced by the class imbalancing. First, we"
0.7.0,will use a bagging classifier and its counter part which internally uses a
0.7.0,random under-sampling to balanced each boostrap sample.
0.7.0,##############################################################################
0.7.0,Balancing each bootstrap sample allows to increase significantly the balanced
0.7.0,accuracy and the geometric mean.
0.7.0,##############################################################################
0.7.0,Classification using random forest classifier with and without sampling
0.7.0,##############################################################################
0.7.0,Random forest is another popular ensemble method and it is usually
0.7.0,"outperforming bagging. Here, we used a vanilla random forest and its balanced"
0.7.0,counterpart in which each bootstrap sample is balanced.
0.7.0,"Similarly to the previous experiment, the balanced classifier outperform the"
0.7.0,"classifier which learn from imbalanced bootstrap samples. In addition, random"
0.7.0,forest outsperforms the bagging classifier.
0.7.0,##############################################################################
0.7.0,Boosting classifier
0.7.0,##############################################################################
0.7.0,"In the same manner, easy ensemble classifier is a bag of balanced AdaBoost"
0.7.0,"classifier. However, it will be slower to train than random forest and will"
0.7.0,achieve worse performance.
0.7.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.7.0,License: MIT
0.7.0,##############################################################################
0.7.0,The following function will be used to create toy dataset. It using the
0.7.0,``make_classification`` from scikit-learn but fixing some parameters.
0.7.0,##############################################################################
0.7.0,The following function will be used to plot the sample space after resampling
0.7.0,to illustrate the characteristic of an algorithm.
0.7.0,make nice plotting
0.7.0,##############################################################################
0.7.0,The following function will be used to plot the decision function of a
0.7.0,classifier given some data.
0.7.0,##############################################################################
0.7.0,Prototype generation: under-sampling by generating new samples
0.7.0,##############################################################################
0.7.0,##############################################################################
0.7.0,``ClusterCentroids`` under-samples by replacing the original samples by the
0.7.0,centroids of the cluster found.
0.7.0,##############################################################################
0.7.0,Prototype selection: under-sampling by selecting existing samples
0.7.0,##############################################################################
0.7.0,##############################################################################
0.7.0,The algorithm performing prototype selection can be subdivided into two
0.7.0,groups: (i) the controlled under-sampling methods and (ii) the cleaning
0.7.0,under-sampling methods.
0.7.0,##############################################################################
0.7.0,"With the controlled under-sampling methods, the number of samples to be"
0.7.0,selected can be specified. ``RandomUnderSampler`` is the most naive way of
0.7.0,performing such selection by randomly selecting a given number of samples by
0.7.0,the targetted class.
0.7.0,##############################################################################
0.7.0,``NearMiss`` algorithms implement some heuristic rules in order to select
0.7.0,samples. NearMiss-1 selects samples from the majority class for which the
0.7.0,average distance of the :math:`k`` nearest samples of the minority class is
0.7.0,the smallest. NearMiss-2 selects the samples from the majority class for
0.7.0,which the average distance to the farthest samples of the negative class is
0.7.0,"the smallest. NearMiss-3 is a 2-step algorithm: first, for each minority"
0.7.0,"sample, their ::math:`m` nearest-neighbors will be kept; then, the majority"
0.7.0,samples selected are the on for which the average distance to the :math:`k`
0.7.0,nearest neighbors is the largest.
0.7.0,##############################################################################
0.7.0,``EditedNearestNeighbours`` removes samples of the majority class for which
0.7.0,their class differ from the one of their nearest-neighbors. This sieve can be
0.7.0,repeated which is the principle of the
0.7.0,``RepeatedEditedNearestNeighbours``. ``AllKNN`` is slightly different from
0.7.0,the ``RepeatedEditedNearestNeighbours`` by changing the :math:`k` parameter
0.7.0,"of the internal nearest neighors algorithm, increasing it at each iteration."
0.7.0,##############################################################################
0.7.0,``CondensedNearestNeighbour`` makes use of a 1-NN to iteratively decide if a
0.7.0,sample should be kept in a dataset or not. The issue is that
0.7.0,``CondensedNearestNeighbour`` is sensitive to noise by preserving the noisy
0.7.0,samples. ``OneSidedSelection`` also used the 1-NN and use ``TomekLinks`` to
0.7.0,remove the samples considered noisy. The ``NeighbourhoodCleaningRule`` use a
0.7.0,"``EditedNearestNeighbours`` to remove some sample. Additionally, they use a 3"
0.7.0,nearest-neighbors to remove samples which do not agree with this rule.
0.7.0,##############################################################################
0.7.0,``InstanceHardnessThreshold`` uses the prediction of classifier to exclude
0.7.0,samples. All samples which are classified with a low probability will be
0.7.0,removed.
0.7.0,##############################################################################
0.7.0,This function allows to make nice plotting
0.7.0,##############################################################################
0.7.0,Generate some data with one Tomek link
0.7.0,minority class
0.7.0,majority class
0.7.0,##############################################################################
0.7.0,"In the figure above, the samples highlighted in green form a Tomek link since"
0.7.0,they are of different classes and are nearest neighbours of each other.
0.7.0,highlight the samples of interest
0.7.0,##############################################################################
0.7.0,We can run the ``TomekLinks`` sampling to remove the corresponding
0.7.0,samples. If ``sampling_strategy='auto'`` only the sample from the majority
0.7.0,class will be removed. If ``sampling_strategy='all'`` both samples will be
0.7.0,removed.
0.7.0,highlight the samples of interest
0.7.0,##############################################################################
0.7.0,This function allows to make nice plotting
0.7.0,##############################################################################
0.7.0,We can start by generating some data to later illustrate the principle of
0.7.0,each NearMiss heuritic rules.
0.7.0,minority class
0.7.0,majority class
0.7.0,##############################################################################
0.7.0,NearMiss-1
0.7.0,##############################################################################
0.7.0,##############################################################################
0.7.0,NearMiss-1 selects samples from the majority class for which the average
0.7.0,distance to some nearest neighbours is the smallest. In the following
0.7.0,"example, we use a 3-NN to compute the average distance on 2 specific samples"
0.7.0,"of the majority class. Therefore, in this case the point linked by the"
0.7.0,green-dashed line will be selected since the average distance is smaller.
0.7.0,##############################################################################
0.7.0,NearMiss-2
0.7.0,##############################################################################
0.7.0,##############################################################################
0.7.0,NearMiss-2 selects samples from the majority class for which the average
0.7.0,distance to the farthest neighbors is the smallest. With the same
0.7.0,"configuration as previously presented, the sample linked to the green-dashed"
0.7.0,line will be selected since its distance the 3 farthest neighbors is the
0.7.0,smallest.
0.7.0,##############################################################################
0.7.0,NearMiss-3
0.7.0,##############################################################################
0.7.0,##############################################################################
0.7.0,"NearMiss-3 can be divided into 2 steps. First, a nearest-neighbors is used to"
0.7.0,short-list samples from the majority class (i.e. correspond to the
0.7.0,"highlighted samples in the following plot). Then, the sample with the largest"
0.7.0,average distance to the *k* nearest-neighbors are selected.
0.7.0,select only the majority point of interest
0.7.0,Authors: Christos Aridas
0.7.0,Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.7.0,License: MIT
0.7.0,Generate the dataset
0.7.0,Instanciate a PCA object for the sake of easy visualisation
0.7.0,Create the samplers
0.7.0,Create the classifier
0.7.0,Make the splits
0.7.0,Add one transformers and two samplers in the pipeline object
0.7.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.7.0,License: MIT
0.7.0,##############################################################################
0.7.0,Data loading
0.7.0,##############################################################################
0.7.0,##############################################################################
0.7.0,"First, you should download the Porto Seguro data set from Kaggle. See the"
0.7.0,link in the introduction.
0.7.0,##############################################################################
0.7.0,The data set is imbalanced and it will have an effect on the fitting.
0.7.0,##############################################################################
0.7.0,Define the pre-processing pipeline
0.7.0,##############################################################################
0.7.0,##############################################################################
0.7.0,We want to standard scale the numerical features while we want to one-hot
0.7.0,"encode the categorical features. In this regard, we make use of the"
0.7.0,:class:`sklearn.compose.ColumnTransformer`.
0.7.0,Create an environment variable to avoid using the GPU. This can be changed.
0.7.0,##############################################################################
0.7.0,Create a neural-network
0.7.0,##############################################################################
0.7.0,##############################################################################
0.7.0,We create a decorator to report the computation time
0.7.0,##############################################################################
0.7.0,The first model will be trained using the ``fit`` method and with imbalanced
0.7.0,mini-batches.
0.7.0,##############################################################################
0.7.0,"In the contrary, we will use imbalanced-learn to create a generator of"
0.7.0,mini-batches which will yield balanced mini-batches.
0.7.0,##############################################################################
0.7.0,Classification loop
0.7.0,##############################################################################
0.7.0,##############################################################################
0.7.0,We will perform a 10-fold cross-validation and train the neural-network with
0.7.0,the two different strategies previously presented.
0.7.0,##############################################################################
0.7.0,Plot of the results and computation time
0.7.0,##############################################################################
0.7.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.7.0,License: MIT
0.7.0,##############################################################################
0.7.0,Problem definition
0.7.0,##############################################################################
0.7.0,we are dropping the following features:
0.7.0,"- ""fnlwgt"": this feature was created while studying the ""adult"" dataset."
0.7.0,"Thus, we will not use this feature which is not acquired during the survey."
0.7.0,"- ""education-num"": it is encoding the same information than ""education""."
0.7.0,"Thus, we are removing one of these 2 features."
0.7.0,##############################################################################
0.7.0,"The ""adult"" dataset as a class ratio of about 3:1"
0.7.0,##############################################################################
0.7.0,This dataset is only slightly imbalanced. To better highlight the effect of
0.7.0,"learning from an imbalanced dataset, we will increase its ratio to 30:1"
0.7.0,##############################################################################
0.7.0,"For the rest of the notebook, we will make a single split to get training"
0.7.0,and testing data. Note that you should use cross-validation to have an
0.7.0,estimate of the performance variation in practice.
0.7.0,##############################################################################
0.7.0,"As a baseline, we could use a classifier which will always predict the"
0.7.0,majority class independently of the features provided.
0.7.0,#############################################################################
0.7.0,"Instead of using the accuracy, we can use the balanced accuracy which will"
0.7.0,take into account the balancing issue.
0.7.0,##############################################################################
0.7.0,Strategies to learn from an imbalanced dataset
0.7.0,##############################################################################
0.7.0,##############################################################################
0.7.0,We will first define a helper function which will train a given model
0.7.0,and compute both accuracy and balanced accuracy. The results will be stored
0.7.0,in a dataframe
0.7.0,Let's define an empty dataframe to store the results
0.7.0,##############################################################################
0.7.0,Dummy baseline
0.7.0,..............
0.7.0,
0.7.0,"Before to train a real machine learning model, we can store the results"
0.7.0,obtained with our `DummyClassifier`.
0.7.0,##############################################################################
0.7.0,Linear classifier baseline
0.7.0,..........................
0.7.0,
0.7.0,We will create a machine learning pipeline using a `LogisticRegression`
0.7.0,"classifier. In this regard, we will need to one-hot encode the categorical"
0.7.0,columns and standardized the numerical columns before to inject the data into
0.7.0,the `LogisticRegression` classifier.
0.7.0,
0.7.0,"First, we define our numerical and categorical pipelines."
0.7.0,##############################################################################
0.7.0,"Then, we can create a preprocessor which will dispatch the categorical"
0.7.0,columns to the categorical pipeline and the numerical columns to the
0.7.0,numerical pipeline
0.7.0,##############################################################################
0.7.0,"Finally, we connect our preprocessor with our `LogisticRegression`. We can"
0.7.0,then evaluate our model.
0.7.0,##############################################################################
0.7.0,We can see that our linear model is learning slightly better than our dummy
0.7.0,"baseline. However, it is impacted by the class imbalance."
0.7.0,
0.7.0,We can verify that something similar is happening with a tree-based model
0.7.0,"such as `RandomForestClassifier`. With this type of classifier, we will not"
0.7.0,"need to scale the numerical data, and we will only need to ordinal encode the"
0.7.0,categorical data.
0.7.0,##############################################################################
0.7.0,"The `RandomForestClassifier` is as well affected by the class imbalanced,"
0.7.0,"slightly less than the linear model. Now, we will present different approach"
0.7.0,to improve the performance of these 2 models.
0.7.0,
0.7.0,Use `class_weight`
0.7.0,..................
0.7.0,
0.7.0,Most of the models in `scikit-learn` have a parameter `class_weight`. This
0.7.0,parameter will affect the computation of the loss in linear model or the
0.7.0,criterion in the tree-based model to penalize differently a false
0.7.0,classification from the minority and majority class. We can set
0.7.0,"`class_weight=""balanced""` such that the weight applied is inversely"
0.7.0,proportional to the class frequency. We test this parametrization in both
0.7.0,linear model and tree-based model.
0.7.0,##############################################################################
0.7.0,
0.7.0,##############################################################################
0.7.0,We can see that using `class_weight` was really effective for the linear
0.7.0,"model, alleviating the issue of learning from imbalanced classes. However,"
0.7.0,"the `RandomForestClassifier` is still biased toward the majority class,"
0.7.0,mainly due to the criterion which is not suited enough to fight the class
0.7.0,imbalance.
0.7.0,
0.7.0,Resample the training set during learning
0.7.0,.........................................
0.7.0,
0.7.0,Another way is to resample the training set by under-sampling or
0.7.0,over-sampling some of the samples. `imbalanced-learn` provides some samplers
0.7.0,to do such processing.
0.7.0,##############################################################################
0.7.0,
0.7.0,##############################################################################
0.7.0,Applying a random under-sampler before the training of the linear model or
0.7.0,"random forest, allows to not focus on the majority class at the cost of"
0.7.0,making more mistake for samples in the majority class (i.e. decreased
0.7.0,accuracy).
0.7.0,
0.7.0,We could apply any type of samplers and find which sampler is working best
0.7.0,on the current dataset.
0.7.0,
0.7.0,"Instead, we will present another way by using classifiers which will apply"
0.7.0,sampling internally.
0.7.0,
0.7.0,Use of `BalancedRandomForestClassifier` and `BalancedBaggingClassifier`
0.7.0,.......................................................................
0.7.0,
0.7.0,We already showed that random under-sampling can be effective on decision
0.7.0,"tree. However, instead of under-sampling once the dataset, one could"
0.7.0,under-sample the original dataset before to take a bootstrap sample. This is
0.7.0,the base of the `BalancedRandomForestClassifier` and
0.7.0,`BalancedBaggingClassifier`.
0.7.0,##############################################################################
0.7.0,The performance with the `BalancedRandomForestClassifier` is better than
0.7.0,applying a single random under-sampling. We will use a gradient-boosting
0.7.0,classifier within a `BalancedBaggingClassifier`.
0.7.0,##############################################################################
0.7.0,This last approach is the most effective. The different under-sampling allows
0.7.0,to bring some diversity for the different GBDT to learn and not focus on a
0.7.0,portion of the majority class.
0.7.0,
0.7.0,We will repeat the same experiment but with a ratio of 100:1 and make a
0.7.0,similar analysis.
0.7.0,##############################################################################
0.7.0,Increase imbalanced ratio
0.7.0,##############################################################################
0.7.0,##############################################################################
0.7.0,"When we analyse the results, we can draw similar conclusions than in the"
0.7.0,"previous discussion. However, we can observe that the strategy"
0.7.0,"`class_weight=""balanced""` does not improve the performance when using a"
0.7.0,`RandomForestClassifier`. A resampling is indeed required. The most effective
0.7.0,method remains the `BalancedBaggingClassifier` using a GBDT as a base
0.7.0,learner.
0.7.0,Authors: Christos Aridas
0.7.0,Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.7.0,License: MIT
0.7.0,Load the dataset
0.7.0,make nice plotting
0.7.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.7.0,License: MIT
0.7.0,Create a folder to fetch the dataset
0.7.0,Create a pipeline
0.7.0,Classify and report the results
0.7.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.7.0,License: MIT
0.7.0,##############################################################################
0.7.0,Setting the data set
0.7.0,##############################################################################
0.7.0,##############################################################################
0.7.0,We use a part of the 20 newsgroups data set by loading 4 topics. Using the
0.7.0,"scikit-learn loader, the data are split into a training and a testing set."
0.7.0,
0.7.0,Note the class \#3 is the minority class and has almost twice less samples
0.7.0,than the majority class.
0.7.0,##############################################################################
0.7.0,The usual scikit-learn pipeline
0.7.0,##############################################################################
0.7.0,##############################################################################
0.7.0,You might usually use scikit-learn pipeline by combining the TF-IDF
0.7.0,vectorizer to feed a multinomial naive bayes classifier. A classification
0.7.0,report summarized the results on the testing set.
0.7.0,
0.7.0,"As expected, the recall of the class \#3 is low mainly due to the class"
0.7.0,imbalanced.
0.7.0,##############################################################################
0.7.0,Balancing the class before classification
0.7.0,##############################################################################
0.7.0,##############################################################################
0.7.0,"To improve the prediction of the class \#3, it could be interesting to apply"
0.7.0,"a balancing before to train the naive bayes classifier. Therefore, we will"
0.7.0,use a ``RandomUnderSampler`` to equalize the number of samples in all the
0.7.0,classes before the training.
0.7.0,
0.7.0,It is also important to note that we are using the ``make_pipeline`` function
0.7.0,implemented in imbalanced-learn to properly handle the samplers.
0.7.0,##############################################################################
0.7.0,"Although the results are almost identical, it can be seen that the resampling"
0.7.0,allowed to correct the poor recall of the class \#3 at the cost of reducing
0.7.0,"the other metrics for the other classes. However, the overall results are"
0.7.0,slightly better.
0.7.0,Authors: Dayvid Oliveira
0.7.0,Christos Aridas
0.7.0,Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.7.0,License: MIT
0.7.0,Generate the dataset
0.7.0,"Two subplots, unpack the axes array immediately"
0.7.0,List of whitelisted modules and methods; regexp are supported.
0.7.0,These docstrings will fail because they are inheriting from scikit-learn
0.7.0,skip private classes
0.7.0,"We ignore following error code,"
0.7.0,- RT02: The first line of the Returns section
0.7.0,"should contain only the type, .."
0.7.0,(as we may need refer to the name of the returned
0.7.0,object)
0.7.0,- GL01: Docstring text (summary) should start in the line
0.7.0,"immediately after the opening quotes (not in the same line,"
0.7.0,or leaving a blank line in between)
0.7.0,Following codes are only taken into account for the
0.7.0,top level class docstrings:
0.7.0,- ES01: No extended summary found
0.7.0,- SA01: See Also section not found
0.7.0,- EX01: No examples section found
0.7.0,In particular we can't parse the signature of properties
0.7.0,"When applied to classes, detect class method. For functions"
0.7.0,method = None.
0.7.0,TODO: this detection can be improved. Currently we assume that we have
0.7.0,class # methods if the second path element before last is in camel case.
0.7.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.7.0,License: MIT
0.7.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.7.0,Christos Aridas
0.7.0,License: MIT
0.7.0,define an alias for back-compatibility
0.7.0,Adapted from scikit-learn
0.7.0,Author: Edouard Duchesnay
0.7.0,Gael Varoquaux
0.7.0,Virgile Fritsch
0.7.0,Alexandre Gramfort
0.7.0,Lars Buitinck
0.7.0,Christos Aridas
0.7.0,Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.7.0,License: BSD
0.7.0,BaseEstimator interface
0.7.0,validate names
0.7.0,validate estimators
0.7.0,We allow last estimator to be None as an identity transformation
0.7.0,Estimator interface
0.7.0,Setup the memory
0.7.0,joblib >= 0.12
0.7.0,Fit or load from cache the current transfomer
0.7.0,Replace the transformer of the step with the fitted
0.7.0,transformer. This is necessary when loading the transformer
0.7.0,from the cache.
0.7.0,"# FIXME: When we get Python 3.7 as minimal version, we will need to switch to"
0.7.0,# the following solution:
0.7.0,# https://snarky.ca/lazy-importing-in-python-3-7/
0.7.0,Import the target module and insert it into the parent's namespace
0.7.0,Update this object's dict so that if someone keeps a reference to the
0.7.0,"LazyLoader, lookups are efficient (__getattr__ is only called on"
0.7.0,lookups that fail).
0.7.0,delay the import of keras since we are going to import either tensorflow
0.7.0,or keras
0.7.0,Based on NiLearn package
0.7.0,License: simplified BSD
0.7.0,"PEP0440 compatible formatted version, see:"
0.7.0,https://www.python.org/dev/peps/pep-0440/
0.7.0,
0.7.0,Generic release markers:
0.7.0,X.Y
0.7.0,X.Y.Z # For bugfix releases
0.7.0,
0.7.0,Admissible pre-release markers:
0.7.0,X.YaN # Alpha release
0.7.0,X.YbN # Beta release
0.7.0,X.YrcN # Release Candidate
0.7.0,X.Y # Final release
0.7.0,
0.7.0,Dev branch marker is: 'X.Y.dev' or 'X.Y.devN' where N is an integer.
0.7.0,'X.Y.dev0' is the canonical version of 'X.Y.dev'
0.7.0,
0.7.0,coding: utf-8
0.7.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.7.0,Dariusz Brzezinski
0.7.0,License: MIT
0.7.0,Only negative labels
0.7.0,"Calculate tp_sum, pred_sum, true_sum ###"
0.7.0,labels are now from 0 to len(labels) - 1 -> use bincount
0.7.0,Pathological case
0.7.0,Compute the true negative
0.7.0,Retain only selected labels
0.7.0,"Finally, we have all our sufficient statistics. Divide! #"
0.7.0,"Divide, and on zero-division, set scores to 0 and warn:"
0.7.0,"Oddly, we may get an ""invalid"" rather than a ""divide"" error"
0.7.0,here.
0.7.0,Average the results
0.7.0,labels are now from 0 to len(labels) - 1 -> use bincount
0.7.0,Pathological case
0.7.0,Retain only selected labels
0.7.0,old version of scipy return MaskedConstant instead of 0.0
0.7.0,check that the scoring function does not need a score
0.7.0,and only a prediction
0.7.0,We do not support multilabel so the only average supported
0.7.0,is binary
0.7.0,Compute the different metrics
0.7.0,Precision/recall/f1
0.7.0,Specificity
0.7.0,Geometric mean
0.7.0,Index balanced accuracy
0.7.0,compute averages
0.7.0,coding: utf-8
0.7.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.7.0,Christos Aridas
0.7.0,License: MIT
0.7.0,##############################################################################
0.7.0,Utilities for testing
0.7.0,import some data to play with
0.7.0,restrict to a binary classification task
0.7.0,add noisy features to make the problem harder and avoid perfect results
0.7.0,"run classifier, get class probabilities and label predictions"
0.7.0,only interested in probabilities of the positive case
0.7.0,XXX: do we really want a special API for the binary case?
0.7.0,##############################################################################
0.7.0,Tests
0.7.0,detailed measures for each class
0.7.0,individual scoring function that can be used for grid search: in the
0.7.0,binary class case the score is the value of the measure for the positive
0.7.0,class (e.g. label == 1). This is deprecated for average != 'binary'.
0.7.0,Such a case may occur with non-stratified cross-validation
0.7.0,ensure the above were meaningful tests:
0.7.0,Bad pos_label
0.7.0,Bad average option
0.7.0,but average != 'binary'; even if data is binary
0.7.0,compute the geometric mean for the binary problem
0.7.0,print classification report with class names
0.7.0,print classification report with label detection
0.7.0,print classification report with class names
0.7.0,print classification report with label detection
0.7.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.7.0,Christos Aridas
0.7.0,License: MIT
0.7.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.7.0,Christos Aridas
0.7.0,License: MIT
0.7.0,The ratio is computed using a one-vs-rest manner. Using majority
0.7.0,in multi-class would lead to slightly different results at the
0.7.0,cost of introducing a new parameter.
0.7.0,rounding may cause new amount for n_samples
0.7.0,the nearest neighbors need to be fitted only on the current class
0.7.0,to find the class NN to generate new samples
0.7.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.7.0,Christos Aridas
0.7.0,License: MIT
0.7.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.7.0,Christos Aridas
0.7.0,License: MIT
0.7.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.7.0,Fernando Nogueira
0.7.0,Christos Aridas
0.7.0,Dzianis Dudnik
0.7.0,License: MIT
0.7.0,np.newaxis for backwards compatability with random_state
0.7.0,Samples are in danger for m/2 <= m' < m
0.7.0,Samples are noise for m = m'
0.7.0,divergence between borderline-1 and borderline-2
0.7.0,Create synthetic samples for borderline points.
0.7.0,only minority
0.7.0,we use a one-vs-rest policy to handle the multiclass in which
0.7.0,new samples will be created considering not only the majority
0.7.0,class but all over classes.
0.7.0,@Substitution(
0.7.0,"sampling_strategy=BaseOverSampler._sampling_strategy_docstring,"
0.7.0,random_state=_random_state_docstring)
0.7.0,compute the median of the standard deviation of the minority class
0.7.0,the input of the OneHotEncoder needs to be dense
0.7.0,we can replace the 1 entries of the categorical features with the
0.7.0,median of the standard deviation. It will ensure that whenever
0.7.0,"distance is computed between 2 samples, the difference will be equal"
0.7.0,to the median of the standard deviation as in the original paper.
0.7.0,"In the edge case where the median of the std is equal to 0, the 1s"
0.7.0,"entries will be also nullified. In this case, we store the original"
0.7.0,categorical encoding which will be later used for inversing the OHE
0.7.0,reverse the encoding of the categorical features
0.7.0,the matrix is supposed to be in the CSR format after the stacking
0.7.0,change in sparsity structure more efficient with LIL than CSR
0.7.0,convert to dense array since scipy.sparse doesn't handle 3D
0.7.0,"In the case that the median std was equal to zeros, we have to"
0.7.0,create non-null entry based on the encoded of OHE
0.7.0,tie breaking argmax
0.7.0,validate the parameters
0.7.0,negate diagonal elements
0.7.0,target_class_indices = np.flatnonzero(y == class_sample)
0.7.0,"X_class = _safe_indexing(X, target_class_indices)"
0.7.0,identify cluster which are answering the requirements
0.7.0,the cluster is already considered balanced
0.7.0,not enough samples to apply SMOTE
0.7.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.7.0,Christos Aridas
0.7.0,License: MIT
0.7.0,check that we can oversample even with missing or infinite data
0.7.0,regression tests for #605
0.7.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.7.0,Christos Aridas
0.7.0,Dzianis Dudnik
0.7.0,License: MIT
0.7.0,create 2 random continuous feature
0.7.0,create a categorical feature using some string
0.7.0,create a categorical feature using some integer
0.7.0,return the categories
0.7.0,create 2 random continuous feature
0.7.0,create a categorical feature using some string
0.7.0,create a categorical feature using some integer
0.7.0,return the categories
0.7.0,create 2 random continuous feature
0.7.0,create a categorical feature using some string
0.7.0,create a categorical feature using some integer
0.7.0,return the categories
0.7.0,create 2 random continuous feature
0.7.0,create a categorical feature using some string
0.7.0,create a categorical feature using some integer
0.7.0,return the categories
0.7.0,create 2 random continuous feature
0.7.0,create a categorical feature using some string
0.7.0,create a categorical feature using some integer
0.7.0,part of the common test which apply to SMOTE-NC even if it is not default
0.7.0,constructible
0.7.0,Check that the samplers handle pandas dataframe and pandas series
0.7.0,Cast X and y to not default dtype
0.7.0,Non-regression test for #662
0.7.0,https://github.com/scikit-learn-contrib/imbalanced-learn/issues/662
0.7.0,check that the categorical feature is not random but correspond to the
0.7.0,categories seen in the minority class samples
0.7.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.7.0,Christos Aridas
0.7.0,License: MIT
0.7.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.7.0,Christos Aridas
0.7.0,License: MIT
0.7.0,check that m_neighbors is properly set. Regression test for:
0.7.0,https://github.com/scikit-learn-contrib/imbalanced-learn/issues/568
0.7.0,shuffle the indices since the sampler are packing them by class
0.7.0,helper functions
0.7.0,input and output
0.7.0,build the model and weights
0.7.0,"build the loss, predict, and train operator"
0.7.0,Initialization of all variables in the graph
0.7.0,"For each epoch, run accuracy on train and test"
0.7.0,helper functions
0.7.0,input and output
0.7.0,build the model and weights
0.7.0,"build the loss, predict, and train operator"
0.7.0,Initialization of all variables in the graph
0.7.0,"For each epoch, run accuracy on train and test"
0.7.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.7.0,License: MIT
0.7.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.7.0,Fernando Nogueira
0.7.0,Christos Aridas
0.7.0,License: MIT
0.7.0,find which class to not consider
0.7.0,there is a Tomek link between two samples if they are both nearest
0.7.0,neighbors of each others.
0.7.0,Find the nearest neighbour of every point
0.7.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.7.0,Christos Aridas
0.7.0,License: MIT
0.7.0,Randomly get one sample from the majority class
0.7.0,Generate the index to select
0.7.0,Create the set C - One majority samples and all minority
0.7.0,Create the set S - all majority samples
0.7.0,fit knn on C
0.7.0,Check each sample in S if we keep it or drop it
0.7.0,Do not select sample which are already well classified
0.7.0,Classify on S
0.7.0,If the prediction do not agree with the true label
0.7.0,append it in C_x
0.7.0,Keep the index for later
0.7.0,Update C
0.7.0,fit a knn on C
0.7.0,This experimental to speed up the search
0.7.0,Classify all the element in S and avoid to test the
0.7.0,well classified elements
0.7.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.7.0,Dayvid Oliveira
0.7.0,Christos Aridas
0.7.0,License: MIT
0.7.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.7.0,Christos Aridas
0.7.0,License: MIT
0.7.0,Compute the distance considering the farthest neighbour
0.7.0,Sort the list of distance and get the index
0.7.0,Throw a warning to tell the user that we did not have enough samples
0.7.0,to select and that we just select everything
0.7.0,Select the desired number of samples
0.7.0,idx_tmp is relative to the feature selected in the
0.7.0,previous step and we need to find the indirection
0.7.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.7.0,Christos Aridas
0.7.0,License: MIT
0.7.0,select a sample from the current class
0.7.0,create the set composed of all minority samples and one
0.7.0,sample from the current class.
0.7.0,create the set S with removing the seed from S
0.7.0,since that it will be added anyway
0.7.0,apply Tomek cleaning
0.7.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.7.0,Dayvid Oliveira
0.7.0,Christos Aridas
0.7.0,License: MIT
0.7.0,Check the stopping criterion
0.7.0,1. If there is no changes for the vector y
0.7.0,2. If the number of samples in the other class become inferior to
0.7.0,the number of samples in the majority class
0.7.0,3. If one of the class is disappearing
0.7.0,Case 1
0.7.0,Case 2
0.7.0,Case 3
0.7.0,Check the stopping criterion
0.7.0,1. If the number of samples in the other class become inferior to
0.7.0,the number of samples in the majority class
0.7.0,2. If one of the class is disappearing
0.7.0,Case 1else:
0.7.0,overwrite b_min_bec_maj
0.7.0,Case 2
0.7.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.7.0,Christos Aridas
0.7.0,License: MIT
0.7.0,clean the neighborhood
0.7.0,compute which classes to consider for cleaning for the A2 group
0.7.0,compute a2 group
0.7.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.7.0,Christos Aridas
0.7.0,License: MIT
0.7.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.7.0,Christos Aridas
0.7.0,License: MIT
0.7.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.7.0,Christos Aridas
0.7.0,License: MIT
0.7.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.7.0,Christos Aridas
0.7.0,License: MIT
0.7.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.7.0,Christos Aridas
0.7.0,License: MIT
0.7.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.7.0,Christos Aridas
0.7.0,License: MIT
0.7.0,check that we can undersample even with missing or infinite data
0.7.0,regression tests for #605
0.7.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.7.0,Christos Aridas
0.7.0,License: MIT
0.7.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.7.0,Christos Aridas
0.7.0,License: MIT
0.7.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.7.0,Christos Aridas
0.7.0,License: MIT
0.7.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.7.0,Christos Aridas
0.7.0,License: MIT
0.7.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.7.0,Christos Aridas
0.7.0,License: MIT
0.7.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.7.0,Fernando Nogueira
0.7.0,Christos Aridas
0.7.0,License: MIT
0.7.0,check that we deprecate the `n_jobs` parameter.
0.7.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.7.0,Christos Aridas
0.7.0,License: MIT
0.7.0,test that all_estimators doesn't find abstract classes.
0.7.0,"For NearMiss, let's check the three algorithms"
0.7.0,Common tests for estimator instances
0.7.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.7.0,License: MIT
0.7.0,check that we can let a pass a regression variable by turning down the
0.7.0,validation
0.7.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.7.0,Christos Aridas
0.7.0,License: MIT
0.7.0,store timestamp to figure out whether the result of 'fit' has been
0.7.0,cached or not
0.7.0,store timestamp to figure out whether the result of 'fit' has been
0.7.0,cached or not
0.7.0,Pipeline accepts steps as tuple
0.7.0,Test the various init parameters of the pipeline.
0.7.0,Check that we can't instantiate pipelines with objects without fit
0.7.0,method
0.7.0,Smoke test with only an estimator
0.7.0,Check that params are set
0.7.0,Smoke test the repr:
0.7.0,Test with two objects
0.7.0,Check that we can't instantiate with non-transformers on the way
0.7.0,"Note that NoTrans implements fit, but not transform"
0.7.0,Check that params are set
0.7.0,Smoke test the repr:
0.7.0,Check that params are not set when naming them wrong
0.7.0,Test clone
0.7.0,"Check that apart from estimators, the parameters are the same"
0.7.0,Remove estimators that where copied
0.7.0,Test the various methods of the pipeline (anova).
0.7.0,Test with Anova + LogisticRegression
0.7.0,Test that the pipeline can take fit parameters
0.7.0,classifier should return True
0.7.0,and transformer params should not be changed
0.7.0,invalid parameters should raise an error message
0.7.0,Pipeline should pass sample_weight
0.7.0,When sample_weight is None it shouldn't be passed
0.7.0,Test pipeline raises set params error message for nested models.
0.7.0,nested model check
0.7.0,Test the various methods of the pipeline (pca + svm).
0.7.0,Test with PCA + SVC
0.7.0,Test the various methods of the pipeline (preprocessing + svm).
0.7.0,check shapes of various prediction functions
0.7.0,test that the fit_predict method is implemented on a pipeline
0.7.0,test that the fit_predict on pipeline yields same results as applying
0.7.0,transform and clustering steps separately
0.7.0,"As pipeline doesn't clone estimators on construction,"
0.7.0,it must have its own estimators
0.7.0,first compute the transform and clustering step separately
0.7.0,use a pipeline to do the transform and clustering in one step
0.7.0,tests that a pipeline does not have fit_predict method when final
0.7.0,step of pipeline does not have fit_predict defined
0.7.0,tests that Pipeline passes fit_params to intermediate steps
0.7.0,when fit_predict is invoked
0.7.0,Test whether pipeline works with a transformer at the end.
0.7.0,Also test pipeline.transform and pipeline.inverse_transform
0.7.0,test transform and fit_transform:
0.7.0,Test whether pipeline works with a transformer missing fit_transform
0.7.0,test fit_transform:
0.7.0,Directly setting attr
0.7.0,Using set_params
0.7.0,Using set_params to replace single step
0.7.0,With invalid data
0.7.0,Test setting Pipeline steps to None
0.7.0,"for other methods, ensure no AttributeErrors on None:"
0.7.0,mult2 and mult3 are active
0.7.0,Check 'passthrough' step at construction time
0.7.0,Test that an error is raised when memory is not a string or a Memory
0.7.0,instance
0.7.0,Define memory as an integer
0.7.0,Test with Transformer + SVC
0.7.0,Memoize the transformer at the first fit
0.7.0,Get the time stamp of the tranformer in the cached pipeline
0.7.0,Check that cached_pipe and pipe yield identical results
0.7.0,Check that we are reading the cache while fitting
0.7.0,a second time
0.7.0,Check that cached_pipe and pipe yield identical results
0.7.0,Create a new pipeline with cloned estimators
0.7.0,Check that even changing the name step does not affect the cache hit
0.7.0,Check that cached_pipe and pipe yield identical results
0.7.0,Test with Transformer + SVC
0.7.0,Memoize the transformer at the first fit
0.7.0,Get the time stamp of the tranformer in the cached pipeline
0.7.0,Check that cached_pipe and pipe yield identical results
0.7.0,Check that we are reading the cache while fitting
0.7.0,a second time
0.7.0,Check that cached_pipe and pipe yield identical results
0.7.0,Create a new pipeline with cloned estimators
0.7.0,Check that even changing the name step does not affect the cache hit
0.7.0,Check that cached_pipe and pipe yield identical results
0.7.0,Test the various methods of the pipeline (pca + svm).
0.7.0,Test with PCA + SVC
0.7.0,Test the various methods of the pipeline (pca + svm).
0.7.0,Test with PCA + SVC
0.7.0,Test whether pipeline works with a sampler at the end.
0.7.0,Also test pipeline.sampler
0.7.0,test transform and fit_transform:
0.7.0,We round the value near to zero. It seems that PCA has some issue
0.7.0,with that
0.7.0,Test whether pipeline works with a sampler at the end.
0.7.0,Also test pipeline.sampler
0.7.0,Test pipeline using None as preprocessing step and a classifier
0.7.0,"Test pipeline using None, RUS and a classifier"
0.7.0,"Test pipeline using RUS, None and a classifier"
0.7.0,Test pipeline using None step and a sampler
0.7.0,Test pipeline using None and a transformer that implements transform and
0.7.0,inverse_transform
0.7.0,Test the various methods of the pipeline (anova).
0.7.0,Test with RandomUnderSampling + Anova + LogisticRegression
0.7.0,Test the various methods of the pipeline (anova).
0.7.0,Test the various methods of the pipeline (anova).
0.7.0,Test with RandomUnderSampling + Anova + LogisticRegression
0.7.0,tests that Pipeline passes predict_params to the final estimator
0.7.0,when predict is invoked
0.7.0,Test that the score_samples method is implemented on a pipeline.
0.7.0,Test that the score_samples method on pipeline yields same results as
0.7.0,applying transform and score_samples steps separately.
0.7.0,Check the shapes
0.7.0,Check the values
0.7.0,Test that a pipeline does not have score_samples method when the final
0.7.0,step of the pipeline does not have score_samples defined.
0.7.0,Test that the score_samples method is implemented on a pipeline.
0.7.0,Test that the score_samples method on pipeline yields same results as
0.7.0,applying transform and score_samples steps separately.
0.7.0,Check the shapes
0.7.0,Check the values
0.7.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.7.0,Christos Aridas
0.7.0,License: MIT
0.7.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.7.0,License: MIT
0.7.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.7.0,License: MIT
0.7.0,Adapated from scikit-learn
0.7.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.7.0,License: MIT
0.7.0,trigger our checks if this is a SamplerMixin
0.7.0,should raise warning if the target is continuous (we cannot raise error)
0.7.0,if the target is multilabel then we should raise an error
0.7.0,IHT does not enforce the number of samples but provide a number
0.7.0,of samples the closest to the desired target.
0.7.0,in this test we will force all samplers to not change the class 1
0.7.0,check that sparse matrices can be passed through the sampler leading to
0.7.0,the same results than dense
0.7.0,Check that the samplers handle pandas dataframe and pandas series
0.7.0,check that we return the same type for dataframes or series types
0.7.0,Check that the can samplers handle simple lists
0.7.0,Check that multiclass target lead to the same results than OVA encoding
0.7.0,Cast X and y to not default dtype
0.7.0,Non-regression test for #709
0.7.0,https://github.com/scikit-learn-contrib/imbalanced-learn/issues/709
0.7.0,Adapted from scikit-learn
0.7.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.7.0,License: MIT
0.7.0,Ignore deprecation warnings triggered at import time and from walking
0.7.0,packages
0.7.0,get rid of abstract base classes
0.7.0,get rid of sklearn estimators which have been imported in some classes
0.7.0,"drop duplicates, sort for reproducibility"
0.7.0,itemgetter is used to ensure the sort does not extend to the 2nd item of
0.7.0,the tuple
0.7.0,Author: Alexander L. Hayes <hayesall@iu.edu>
0.7.0,License: MIT
0.7.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.7.0,License: MIT
0.7.0,check that all keys in sampling_strategy are also in y
0.7.0,check that there is no negative number
0.7.0,check that all keys in sampling_strategy are also in y
0.7.0,ignore first 'self' argument for instance methods
0.7.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.7.0,Christos Aridas
0.7.0,License: MIT
0.7.0,this function could create an equal number of samples
0.7.0,We pass on purpose a non sorted dictionary and check that the resulting
0.7.0,dictionary is sorted. Refer to issue #428.
0.7.0,DataFrame and DataFrame case
0.7.0,DataFrames and Series case
0.7.0,The * is place before a keyword only argument without a default value
0.7.0,Author: Alexander L. Hayes <hayesall@iu.edu>
0.7.0,License: MIT
0.7.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.7.0,License: MIT
0.7.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.7.0,Christos Aridas
0.7.0,License: MIT
0.7.0,check if the filtering is working with a list or a single string
0.7.0,check that all estimators are sampler
0.7.0,check that an error is raised when the type is unknown
0.7.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.7.0,License: MIT
0.7.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.7.0,Christos Aridas
0.7.0,License: MIT
0.7.0,Otherwise create a default SMOTE
0.7.0,Otherwise create a default TomekLinks
0.7.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.7.0,Christos Aridas
0.7.0,License: MIT
0.7.0,Otherwise create a default SMOTE
0.7.0,Otherwise create a default EditedNearestNeighbours
0.7.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.7.0,Christos Aridas
0.7.0,License: MIT
0.7.0,Check if default job count is None
0.7.0,Check if job count is set
0.7.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.7.0,Christos Aridas
0.7.0,License: MIT
0.7.0,Check if default job count is none
0.7.0,Check if job count is set
0.7.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.7.0,License: MIT
0.7.0,resample before to fit the tree
0.7.0,Validate or convert input data
0.7.0,Pre-sort indices to avoid that each individual tree of the
0.7.0,ensemble sorts the indices.
0.7.0,Remap output
0.7.0,reshape is necessary to preserve the data contiguity against vs
0.7.0,"[:, np.newaxis] that does not."
0.7.0,Get bootstrap sample size
0.7.0,Check parameters
0.7.0,"Free allocated memory, if any"
0.7.0,We draw from the random state to get the random state we
0.7.0,would have got if we hadn't used a warm_start.
0.7.0,Parallel loop: we prefer the threading backend as the Cython code
0.7.0,for fitting the trees is internally releasing the Python GIL
0.7.0,making threading more efficient than multiprocessing in
0.7.0,"that case. However, we respect any parallel_backend contexts set"
0.7.0,"at a higher level, since correctness does not rely on using"
0.7.0,threads.
0.7.0,Collect newly grown trees
0.7.0,Create pipeline with the fitted samplers and trees
0.7.0,Decapsulate classes_ attributes
0.7.0,"with the resampling, we are likely to have rows not included"
0.7.0,for the OOB score leading to division by zero
0.7.0,Instances incorrectly classified
0.7.0,Error fraction
0.7.0,Stop if classification is perfect
0.7.0,Construct y coding as described in Zhu et al [2]:
0.7.0,
0.7.0,y_k = 1 if c == k else -1 / (K - 1)
0.7.0,
0.7.0,"where K == n_classes_ and c, k in [0, K) are indices along the second"
0.7.0,axis of the y coding with c being the index corresponding to the true
0.7.0,class label.
0.7.0,Displace zero probabilities so the log is defined.
0.7.0,Also fix negative elements which may occur with
0.7.0,negative sample weights.
0.7.0,Boost weight using multi-class AdaBoost SAMME.R alg
0.7.0,Only boost the weights if it will fit again
0.7.0,Only boost positive weights
0.7.0,Instances incorrectly classified
0.7.0,Error fraction
0.7.0,Stop if classification is perfect
0.7.0,Stop if the error is at least as bad as random guessing
0.7.0,Boost weight using multi-class AdaBoost SAMME alg
0.7.0,Only boost the weights if I will fit again
0.7.0,Only boost positive weights
0.7.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.7.0,Christos Aridas
0.7.0,License: MIT
0.7.0,RandomUnderSampler is not supporting sample_weight. We need to pass
0.7.0,None.
0.7.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.7.0,Christos Aridas
0.7.0,License: MIT
0.7.0,RandomUnderSampler is not supporting sample_weight. We need to pass
0.7.0,None.
0.7.0,check that we have an ensemble of samplers and estimators with a
0.7.0,consistent size
0.7.0,each sampler in the ensemble should have different random state
0.7.0,each estimator in the ensemble should have different random state
0.7.0,check the consistency of the feature importances
0.7.0,check the consistency of the prediction outpus
0.7.0,Predictions should be the same when sample_weight are all ones
0.7.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.7.0,Christos Aridas
0.7.0,License: MIT
0.7.0,Check classification for various parameter settings.
0.7.0,Test that bootstrapping samples generate non-perfect base estimators.
0.7.0,"without bootstrap, all trees are perfect on the training set"
0.7.0,disable the resampling by passing an empty dictionary.
0.7.0,"with bootstrap, trees are no longer perfect on the training set"
0.7.0,Test that bootstrapping features may generate duplicate features.
0.7.0,Predict probabilities.
0.7.0,Normal case
0.7.0,"Degenerate case, where some classes are missing"
0.7.0,Check that oob prediction is a good estimation of the generalization
0.7.0,error.
0.7.0,Test with few estimators
0.7.0,Check singleton ensembles.
0.7.0,Test that it gives proper exception on deficient input.
0.7.0,Test n_estimators
0.7.0,Test max_samples
0.7.0,Test max_features
0.7.0,Test support of decision_function
0.7.0,Check that bagging ensembles can be grid-searched.
0.7.0,Transform iris into a binary classification task
0.7.0,Grid search with scoring based on decision_function
0.7.0,Check base_estimator and its default values.
0.7.0,Test if fitting incrementally with warm start gives a forest of the
0.7.0,right size and the same results as a normal fit.
0.7.0,Test if warm start'ed second fit with smaller n_estimators raises error.
0.7.0,Test that nothing happens when fitting without increasing n_estimators
0.7.0,"modify X to nonsense values, this should not change anything"
0.7.0,warm started classifier with 5+5 estimators should be equivalent to
0.7.0,one classifier with 10 estimators
0.7.0,Check using oob_score and warm_start simultaneously fails
0.7.0,"Make sure OOB scores are identical when random_state, estimator, and"
0.7.0,training data are fixed and fitting is done twice
0.7.0,Check that format of estimators_samples_ is correct and that results
0.7.0,generated at fit time can be identically reproduced at a later time
0.7.0,using data saved in object attributes.
0.7.0,remap the y outside of the BalancedBaggingclassifier
0.7.0,"_, y = np.unique(y, return_inverse=True)"
0.7.0,Get relevant attributes
0.7.0,Test for correct formatting
0.7.0,Re-fit single estimator to test for consistent sampling
0.7.0,Make sure validated max_samples and original max_samples are identical
0.7.0,when valid integer max_samples supplied by user
0.7.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.7.0,Christos Aridas
0.7.0,License: MIT
0.7.0,Generate a global dataset to use
0.7.0,Check classification for various parameter settings.
0.7.0,test the different prediction function
0.7.0,Check base_estimator and its default values.
0.7.0,Test if fitting incrementally with warm start gives a forest of the
0.7.0,right size and the same results as a normal fit.
0.7.0,Test if warm start'ed second fit with smaller n_estimators raises error.
0.7.0,Test that nothing happens when fitting without increasing n_estimators
0.7.0,"modify X to nonsense values, this should not change anything"
0.7.0,warm started classifier with 5+5 estimators should be equivalent to
0.7.0,one classifier with 10 estimators
0.7.0,Check warning if not enough estimators
0.7.0,First fit with no restriction on max samples
0.7.0,Second fit with max samples restricted to just 2
0.7.0,Regression test for #655: check that the oob score is closed to 0.5
0.7.0,a binomial experiment.
0.7.0,Author: Guillaume Lemaitre
0.7.0,License: BSD 3 clause
0.7.0,"The index start at one, then we need to remove one"
0.7.0,to not have issue with the indexing.
0.7.0,go through the list and check if the data are available
0.7.0,Authors: Dayvid Oliveira
0.7.0,Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.7.0,Christos Aridas
0.7.0,License: MIT
0.7.0,restrict ratio to be a dict or a callable
0.7.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.7.0,Christos Aridas
0.7.0,License: MIT
0.7.0,"we are reusing part of utils.check_sampling_strategy, however this is not"
0.7.0,cover in the common tests so we will repeat it here
0.7.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.7.0,Christos Aridas
0.7.0,License: MIT
0.7.0,This is a trick to avoid an error during tests collection with pytest. We
0.7.0,avoid the error when importing the package raise the error at the moment of
0.7.0,creating the instance.
0.7.0,flag for keras sequence duck-typing
0.7.0,shuffle the indices since the sampler are packing them by class
0.6.2,This file is here so that when running from the root folder
0.6.2,./sklearn is added to sys.path by pytest.
0.6.2,See https://docs.pytest.org/en/latest/pythonpath.html for more details.
0.6.2,"For example, this allows to build extensions in place and run pytest"
0.6.2,doc/modules/clustering.rst and use sklearn from the local folder
0.6.2,rather than the one from site-packages.
0.6.2,Set numpy array str/repr to legacy behaviour on numpy > 1.13 to make
0.6.2,the doctests pass
0.6.2,! /usr/bin/env python
0.6.2,get __version__ from _version.py
0.6.2,-*- coding: utf-8 -*-
0.6.2,
0.6.2,"imbalanced-learn documentation build configuration file, created by"
0.6.2,sphinx-quickstart on Mon Jan 18 14:44:12 2016.
0.6.2,
0.6.2,This file is execfile()d with the current directory set to its
0.6.2,containing dir.
0.6.2,
0.6.2,Note that not all possible configuration values are present in this
0.6.2,autogenerated file.
0.6.2,
0.6.2,All configuration values have a default; values that are commented out
0.6.2,serve to show the default.
0.6.2,"If extensions (or modules to document with autodoc) are in another directory,"
0.6.2,add these directories to sys.path here. If the directory is relative to the
0.6.2,"documentation root, use os.path.abspath to make it absolute, like shown here."
0.6.2,-- General configuration ------------------------------------------------
0.6.2,"If your documentation needs a minimal Sphinx version, state it here."
0.6.2,needs_sphinx = '1.0'
0.6.2,"Add any Sphinx extension module names here, as strings. They can be"
0.6.2,extensions coming with Sphinx (named 'sphinx.ext.*') or your custom
0.6.2,ones.
0.6.2,this is needed for some reason...
0.6.2,see https://github.com/numpy/numpydoc/issues/69
0.6.2,"Add any paths that contain templates here, relative to this directory."
0.6.2,generate autosummary even if no references
0.6.2,The suffix of source filenames.
0.6.2,The encoding of source files.
0.6.2,source_encoding = 'utf-8-sig'
0.6.2,Generate the plot for the gallery
0.6.2,The master toctree document.
0.6.2,General information about the project.
0.6.2,"The version info for the project you're documenting, acts as replacement for"
0.6.2,"|version| and |release|, also used in various other places throughout the"
0.6.2,built documents.
0.6.2,
0.6.2,The short X.Y version.
0.6.2,"The full version, including alpha/beta/rc tags."
0.6.2,The language for content autogenerated by Sphinx. Refer to documentation
0.6.2,for a list of supported languages.
0.6.2,language = None
0.6.2,"There are two options for replacing |today|: either, you set today to some"
0.6.2,"non-false value, then it is used:"
0.6.2,today = ''
0.6.2,"Else, today_fmt is used as the format for a strftime call."
0.6.2,"today_fmt = '%B %d, %Y'"
0.6.2,"List of patterns, relative to source directory, that match files and"
0.6.2,directories to ignore when looking for source files.
0.6.2,The reST default role (used for this markup: `text`) to use for all
0.6.2,documents.
0.6.2,"If true, '()' will be appended to :func: etc. cross-reference text."
0.6.2,"If true, the current module name will be prepended to all description"
0.6.2,unit titles (such as .. function::).
0.6.2,add_module_names = True
0.6.2,"If true, sectionauthor and moduleauthor directives will be shown in the"
0.6.2,output. They are ignored by default.
0.6.2,show_authors = False
0.6.2,The name of the Pygments (syntax highlighting) style to use.
0.6.2,Custom style
0.6.2,A list of ignored prefixes for module index sorting.
0.6.2,modindex_common_prefix = []
0.6.2,"If true, keep warnings as ""system message"" paragraphs in the built documents."
0.6.2,keep_warnings = False
0.6.2,-- Options for HTML output ----------------------------------------------
0.6.2,The theme to use for HTML and HTML Help pages.  See the documentation for
0.6.2,a list of builtin themes.
0.6.2,Theme options are theme-specific and customize the look and feel of a theme
0.6.2,"further.  For a list of options available for each theme, see the"
0.6.2,documentation.
0.6.2,html_theme_options = {}
0.6.2,"Add any paths that contain custom themes here, relative to this directory."
0.6.2,"The name for this set of Sphinx documents.  If None, it defaults to"
0.6.2,"""<project> v<release> documentation""."
0.6.2,html_title = None
0.6.2,A shorter title for the navigation bar.  Default is the same as html_title.
0.6.2,html_short_title = None
0.6.2,The name of an image file (relative to this directory) to place at the top
0.6.2,of the sidebar.
0.6.2,html_logo = None
0.6.2,The name of an image file (within the static path) to use as favicon of the
0.6.2,docs.  This file should be a Windows icon file (.ico) being 16x16 or 32x32
0.6.2,pixels large.
0.6.2,html_favicon = None
0.6.2,"Add any paths that contain custom static files (such as style sheets) here,"
0.6.2,"relative to this directory. They are copied after the builtin static files,"
0.6.2,"so a file named ""default.css"" will overwrite the builtin ""default.css""."
0.6.2,Add any extra paths that contain custom files (such as robots.txt or
0.6.2,".htaccess) here, relative to this directory. These files are copied"
0.6.2,directly to the root of the documentation.
0.6.2,html_extra_path = []
0.6.2,"If not '', a 'Last updated on:' timestamp is inserted at every page bottom,"
0.6.2,using the given strftime format.
0.6.2,"html_last_updated_fmt = '%b %d, %Y'"
0.6.2,"If true, SmartyPants will be used to convert quotes and dashes to"
0.6.2,typographically correct entities.
0.6.2,html_use_smartypants = True
0.6.2,"Custom sidebar templates, maps document names to template names."
0.6.2,html_sidebars = {}
0.6.2,"Additional templates that should be rendered to pages, maps page names to"
0.6.2,template names.
0.6.2,html_additional_pages = {}
0.6.2,"If false, no module index is generated."
0.6.2,html_domain_indices = True
0.6.2,"If false, no index is generated."
0.6.2,html_use_index = True
0.6.2,"If true, the index is split into individual pages for each letter."
0.6.2,html_split_index = False
0.6.2,"If true, links to the reST sources are added to the pages."
0.6.2,html_show_sourcelink = True
0.6.2,"If true, ""Created using Sphinx"" is shown in the HTML footer. Default is True."
0.6.2,html_show_sphinx = True
0.6.2,"If true, ""(C) Copyright ..."" is shown in the HTML footer. Default is True."
0.6.2,html_show_copyright = True
0.6.2,"If true, an OpenSearch description file will be output, and all pages will"
0.6.2,contain a <link> tag referring to it.  The value of this option must be the
0.6.2,base URL from which the finished HTML is served.
0.6.2,html_use_opensearch = ''
0.6.2,"This is the file name suffix for HTML files (e.g. "".xhtml"")."
0.6.2,html_file_suffix = None
0.6.2,Output file base name for HTML help builder.
0.6.2,-- Options for LaTeX output ---------------------------------------------
0.6.2,The paper size ('letterpaper' or 'a4paper').
0.6.2,"'papersize': 'letterpaper',"
0.6.2,"The font size ('10pt', '11pt' or '12pt')."
0.6.2,"'pointsize': '10pt',"
0.6.2,Additional stuff for the LaTeX preamble.
0.6.2,"'preamble': '',"
0.6.2,Grouping the document tree into LaTeX files. List of tuples
0.6.2,"(source start file, target name, title,"
0.6.2,"author, documentclass [howto, manual, or own class])."
0.6.2,The name of an image file (relative to this directory) to place at the top of
0.6.2,the title page.
0.6.2,latex_logo = None
0.6.2,"For ""manual"" documents, if this is true, then toplevel headings are parts,"
0.6.2,not chapters.
0.6.2,latex_use_parts = False
0.6.2,"If true, show page references after internal links."
0.6.2,latex_show_pagerefs = False
0.6.2,"If true, show URL addresses after external links."
0.6.2,latex_show_urls = False
0.6.2,Documents to append as an appendix to all manuals.
0.6.2,latex_appendices = []
0.6.2,intersphinx configuration
0.6.2,sphinx-gallery configuration
0.6.2,-- Options for manual page output ---------------------------------------
0.6.2,"If false, no module index is generated."
0.6.2,latex_domain_indices = True
0.6.2,One entry per manual page. List of tuples
0.6.2,"(source start file, name, description, authors, manual section)."
0.6.2,"If true, show URL addresses after external links."
0.6.2,man_show_urls = False
0.6.2,-- Options for Texinfo output -------------------------------------------
0.6.2,Grouping the document tree into Texinfo files. List of tuples
0.6.2,"(source start file, target name, title, author,"
0.6.2,"dir menu entry, description, category)"
0.6.2,"def generate_example_rst(app, what, name, obj, options, lines):"
0.6.2,"# generate empty examples files, so that we don't get"
0.6.2,# inclusion errors if there are no examples for a class / module
0.6.2,"examples_path = os.path.join(app.srcdir, ""generated"","
0.6.2,"""%s.examples"" % name)"
0.6.2,if not os.path.exists(examples_path):
0.6.2,# touch file
0.6.2,"open(examples_path, 'w').close()"
0.6.2,Config for sphinx_issues
0.6.2,Temporary work-around for spacing problem between parameter and parameter
0.6.2,"type in the doc, see https://github.com/numpy/numpydoc/issues/215. The bug"
0.6.2,has been fixed in sphinx (https://github.com/sphinx-doc/sphinx/pull/5976) but
0.6.2,through a change in sphinx basic.css except rtd_theme does not use basic.css.
0.6.2,"In an ideal world, this would get fixed in this PR:"
0.6.2,https://github.com/readthedocs/sphinx_rtd_theme/pull/747/files
0.6.2,"app.connect('autodoc-process-docstring', generate_example_rst)"
0.6.2,Documents to append as an appendix to all manuals.
0.6.2,texinfo_appendices = []
0.6.2,"If false, no module index is generated."
0.6.2,texinfo_domain_indices = True
0.6.2,"How to display URL addresses: 'footnote', 'no', or 'inline'."
0.6.2,texinfo_show_urls = 'footnote'
0.6.2,"If true, do not generate a @detailmenu in the ""Top"" node's menu."
0.6.2,texinfo_no_detailmenu = False
0.6.2,The following is used by sphinx.ext.linkcode to provide links to github
0.6.2,get the styles from the current theme
0.6.2,create and add the button to all the code blocks that contain >>>
0.6.2,tracebacks (.gt) contain bare text elements that need to be
0.6.2,wrapped in a span to work with .nextUntil() (see later)
0.6.2,define the behavior of the button when it's clicked
0.6.2,hide the code output
0.6.2,show the code output
0.6.2,-*- coding: utf-8 -*-
0.6.2,Format template for issues URI
0.6.2,e.g. 'https://github.com/sloria/marshmallow/issues/{issue}
0.6.2,Format template for PR URI
0.6.2,e.g. 'https://github.com/sloria/marshmallow/pull/{issue}
0.6.2,Format template for commit URI
0.6.2,e.g. 'https://github.com/sloria/marshmallow/commits/{commit}
0.6.2,"Shortcut for Github, e.g. 'sloria/marshmallow'"
0.6.2,Format template for user profile URI
0.6.2,e.g. 'https://github.com/{user}'
0.6.2,Python 2 only
0.6.2,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.6.2,License: MIT
0.6.2,##############################################################################
0.6.2,"First, we will create an imbalanced data set from a the iris data set."
0.6.2,##############################################################################
0.6.2,Using ``sampling_strategy`` in resampling algorithms
0.6.2,##############################################################################
0.6.2,##############################################################################
0.6.2,``sampling_strategy`` as a ``float``
0.6.2,....................................
0.6.2,
0.6.2,``sampling_strategy`` can be given a ``float``. For **under-sampling
0.6.2,"methods**, it corresponds to the ratio :math:`\\alpha_{us}` defined by"
0.6.2,:math:`N_{rM} = \\alpha_{us} \\times N_{m}` where :math:`N_{rM}` and
0.6.2,:math:`N_{m}` are the number of samples in the majority class after
0.6.2,"resampling and the number of samples in the minority class, respectively."
0.6.2,select only 2 classes since the ratio make sense in this case
0.6.2,##############################################################################
0.6.2,"For **over-sampling methods**, it correspond to the ratio"
0.6.2,:math:`\\alpha_{os}` defined by :math:`N_{rm} = \\alpha_{os} \\times N_{M}`
0.6.2,where :math:`N_{rm}` and :math:`N_{M}` are the number of samples in the
0.6.2,minority class after resampling and the number of samples in the majority
0.6.2,"class, respectively."
0.6.2,##############################################################################
0.6.2,``sampling_strategy`` has a ``str``
0.6.2,...................................
0.6.2,
0.6.2,``sampling_strategy`` can be given as a string which specify the class
0.6.2,"targeted by the resampling. With under- and over-sampling, the number of"
0.6.2,samples will be equalized.
0.6.2,
0.6.2,Note that we are using multiple classes from now on.
0.6.2,##############################################################################
0.6.2,"With **cleaning method**, the number of samples in each class will not be"
0.6.2,equalized even if targeted.
0.6.2,##############################################################################
0.6.2,``sampling_strategy`` as a ``dict``
0.6.2,...................................
0.6.2,
0.6.2,"When ``sampling_strategy`` is a ``dict``, the keys correspond to the targeted"
0.6.2,classes. The values correspond to the desired number of samples for each
0.6.2,targeted class. This is working for both **under- and over-sampling**
0.6.2,algorithms but not for the **cleaning algorithms**. Use a ``list`` instead.
0.6.2,##############################################################################
0.6.2,``sampling_strategy`` as a ``list``
0.6.2,...................................
0.6.2,
0.6.2,"When ``sampling_strategy`` is a ``list``, the list contains the targeted"
0.6.2,classes. It is used only for **cleaning methods** and raise an error
0.6.2,otherwise.
0.6.2,##############################################################################
0.6.2,``sampling_strategy`` as a callable
0.6.2,...................................
0.6.2,
0.6.2,"When callable, function taking ``y`` and returns a ``dict``. The keys"
0.6.2,correspond to the targeted classes. The values correspond to the desired
0.6.2,number of samples for each class.
0.6.2,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.6.2,License: MIT
0.6.2,#############################################################################
0.6.2,Toy data generation
0.6.2,#############################################################################
0.6.2,#############################################################################
0.6.2,We are generating some non Gaussian data set contaminated with some unform
0.6.2,noise.
0.6.2,#############################################################################
0.6.2,We will generate some cleaned test data without outliers.
0.6.2,#############################################################################
0.6.2,How to use the :class:`imblearn.FunctionSampler`
0.6.2,#############################################################################
0.6.2,#############################################################################
0.6.2,We first define a function which will use
0.6.2,:class:`sklearn.ensemble.IsolationForest` to eliminate some outliers from
0.6.2,our dataset during training. The function passed to the
0.6.2,:class:`imblearn.FunctionSampler` will be called when using the method
0.6.2,``fit_resample``.
0.6.2,#############################################################################
0.6.2,Integrate it within a pipeline
0.6.2,#############################################################################
0.6.2,#############################################################################
0.6.2,"By elimnating outliers before the training, the classifier will be less"
0.6.2,affected during the prediction.
0.6.2,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.6.2,License: MIT
0.6.2,generate some data points
0.6.2,plot the majority and minority samples
0.6.2,draw the circle in which the new sample will generated
0.6.2,plot the line on which the sample will be generated
0.6.2,create and plot the new sample
0.6.2,make the plot nicer with legend and label
0.6.2,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.6.2,License: MIT
0.6.2,##############################################################################
0.6.2,The following function will be used to create toy dataset. It using the
0.6.2,``make_classification`` from scikit-learn but fixing some parameters.
0.6.2,##############################################################################
0.6.2,The following function will be used to plot the sample space after resampling
0.6.2,to illustrate the characterisitic of an algorithm.
0.6.2,make nice plotting
0.6.2,##############################################################################
0.6.2,The following function will be used to plot the decision function of a
0.6.2,classifier given some data.
0.6.2,##############################################################################
0.6.2,Illustration of the influence of the balancing ratio
0.6.2,##############################################################################
0.6.2,##############################################################################
0.6.2,We will first illustrate the influence of the balancing ratio on some toy
0.6.2,data using a linear SVM classifier. Greater is the difference between the
0.6.2,"number of samples in each class, poorer are the classfication results."
0.6.2,##############################################################################
0.6.2,Random over-sampling to balance the data set
0.6.2,##############################################################################
0.6.2,##############################################################################
0.6.2,Random over-sampling can be used to repeat some samples and balance the
0.6.2,number of samples between the dataset. It can be seen that with this trivial
0.6.2,approach the boundary decision is already less biaised toward the majority
0.6.2,class.
0.6.2,##############################################################################
0.6.2,More advanced over-sampling using ADASYN and SMOTE
0.6.2,##############################################################################
0.6.2,##############################################################################
0.6.2,"Instead of repeating the same samples when over-sampling, we can use some"
0.6.2,specific heuristic instead. ADASYN and SMOTE can be used in this case.
0.6.2,Make an identity sampler
0.6.2,##############################################################################
0.6.2,The following plot illustrate the difference between ADASYN and SMOTE. ADASYN
0.6.2,will focus on the samples which are difficult to classify with a
0.6.2,nearest-neighbors rule while regular SMOTE will not make any distinction.
0.6.2,"Therefore, the decision function depending of the algorithm."
0.6.2,##############################################################################
0.6.2,"Due to those sampling particularities, it can give rise to some specific"
0.6.2,issues as illustrated below.
0.6.2,##############################################################################
0.6.2,SMOTE proposes several variants by identifying specific samples to consider
0.6.2,during the resampling. The borderline version will detect which point to
0.6.2,select which are in the border between two classes. The SVM version will use
0.6.2,the support vectors found using an SVM algorithm to create new sample while
0.6.2,the KMeans version will make a clustering before to generate samples in each
0.6.2,cluster independently depending each cluster density.
0.6.2,##############################################################################
0.6.2,"When dealing with a mixed of continuous and categorical features, SMOTE-NC"
0.6.2,is the only method which can handle this case.
0.6.2,create a synthetic data set with continuous and categorical features
0.6.2,Authors: Christos Aridas
0.6.2,Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.6.2,License: MIT
0.6.2,Generate the dataset
0.6.2,make nice plotting
0.6.2,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.6.2,License: MIT
0.6.2,Generate a dataset
0.6.2,Split the data
0.6.2,Train the classifier with balancing
0.6.2,Test the classifier and get the prediction
0.6.2,Show the classification report
0.6.2,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.6.2,License: MIT
0.6.2,Generate a dataset
0.6.2,Split the data
0.6.2,Train the classifier with balancing
0.6.2,Test the classifier and get the prediction
0.6.2,##############################################################################
0.6.2,The geometric mean corresponds to the square root of the product of the
0.6.2,sensitivity and specificity. Combining the two metrics should account for
0.6.2,the balancing of the dataset.
0.6.2,##############################################################################
0.6.2,The index balanced accuracy can transform any metric to be used in
0.6.2,imbalanced learning problems.
0.6.2,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.6.2,License: MIT
0.6.2,##############################################################################
0.6.2,The following function will be used to create toy dataset. It using the
0.6.2,``make_classification`` from scikit-learn but fixing some parameters.
0.6.2,##############################################################################
0.6.2,The following function will be used to plot the sample space after resampling
0.6.2,to illustrate the characteristic of an algorithm.
0.6.2,make nice plotting
0.6.2,##############################################################################
0.6.2,The following function will be used to plot the decision function of a
0.6.2,classifier given some data.
0.6.2,##############################################################################
0.6.2,"``SMOTE`` allows to generate samples. However, this method of over-sampling"
0.6.2,"does not have any knowledge regarding the underlying distribution. Therefore,"
0.6.2,"some noisy samples can be generated, e.g. when the different classes cannot"
0.6.2,"be well separated. Hence, it can be beneficial to apply an under-sampling"
0.6.2,algorithm to clean the noisy samples. Two methods are usually used in the
0.6.2,literature: (i) Tomek's link and (ii) edited nearest neighbours cleaning
0.6.2,methods. Imbalanced-learn provides two ready-to-use samplers ``SMOTETomek``
0.6.2,"and ``SMOTEENN``. In general, ``SMOTEENN`` cleans more noisy data than"
0.6.2,``SMOTETomek``.
0.6.2,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.6.2,License: MIT
0.6.2,##############################################################################
0.6.2,Load an imbalanced dataset
0.6.2,##############################################################################
0.6.2,We will load the UCI SatImage dataset which has an imbalanced ratio of 9.3:1
0.6.2,(number of majority sample for a minority sample). The data are then split
0.6.2,into training and testing.
0.6.2,##############################################################################
0.6.2,Classification using a single decision tree
0.6.2,##############################################################################
0.6.2,We train a decision tree classifier which will be used as a baseline for the
0.6.2,rest of this example.
0.6.2,##############################################################################
0.6.2,The results are reported in terms of balanced accuracy and geometric mean
0.6.2,which are metrics widely used in the literature to validate model trained on
0.6.2,imbalanced set.
0.6.2,##############################################################################
0.6.2,Classification using bagging classifier with and without sampling
0.6.2,##############################################################################
0.6.2,"Instead of using a single tree, we will check if an ensemble of decsion tree"
0.6.2,"can actually alleviate the issue induced by the class imbalancing. First, we"
0.6.2,will use a bagging classifier and its counter part which internally uses a
0.6.2,random under-sampling to balanced each boostrap sample.
0.6.2,##############################################################################
0.6.2,Balancing each bootstrap sample allows to increase significantly the balanced
0.6.2,accuracy and the geometric mean.
0.6.2,##############################################################################
0.6.2,Classification using random forest classifier with and without sampling
0.6.2,##############################################################################
0.6.2,Random forest is another popular ensemble method and it is usually
0.6.2,"outperforming bagging. Here, we used a vanilla random forest and its balanced"
0.6.2,counterpart in which each bootstrap sample is balanced.
0.6.2,"Similarly to the previous experiment, the balanced classifier outperform the"
0.6.2,"classifier which learn from imbalanced bootstrap samples. In addition, random"
0.6.2,forest outsperforms the bagging classifier.
0.6.2,##############################################################################
0.6.2,Boosting classifier
0.6.2,##############################################################################
0.6.2,"In the same manner, easy ensemble classifier is a bag of balanced AdaBoost"
0.6.2,"classifier. However, it will be slower to train than random forest and will"
0.6.2,achieve worse performance.
0.6.2,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.6.2,License: MIT
0.6.2,##############################################################################
0.6.2,The following function will be used to create toy dataset. It using the
0.6.2,``make_classification`` from scikit-learn but fixing some parameters.
0.6.2,##############################################################################
0.6.2,The following function will be used to plot the sample space after resampling
0.6.2,to illustrate the characteristic of an algorithm.
0.6.2,make nice plotting
0.6.2,##############################################################################
0.6.2,The following function will be used to plot the decision function of a
0.6.2,classifier given some data.
0.6.2,##############################################################################
0.6.2,Prototype generation: under-sampling by generating new samples
0.6.2,##############################################################################
0.6.2,##############################################################################
0.6.2,``ClusterCentroids`` under-samples by replacing the original samples by the
0.6.2,centroids of the cluster found.
0.6.2,##############################################################################
0.6.2,Prototype selection: under-sampling by selecting existing samples
0.6.2,##############################################################################
0.6.2,##############################################################################
0.6.2,The algorithm performing prototype selection can be subdivided into two
0.6.2,groups: (i) the controlled under-sampling methods and (ii) the cleaning
0.6.2,under-sampling methods.
0.6.2,##############################################################################
0.6.2,"With the controlled under-sampling methods, the number of samples to be"
0.6.2,selected can be specified. ``RandomUnderSampler`` is the most naive way of
0.6.2,performing such selection by randomly selecting a given number of samples by
0.6.2,the targetted class.
0.6.2,##############################################################################
0.6.2,``NearMiss`` algorithms implement some heuristic rules in order to select
0.6.2,samples. NearMiss-1 selects samples from the majority class for which the
0.6.2,average distance of the :math:`k`` nearest samples of the minority class is
0.6.2,the smallest. NearMiss-2 selects the samples from the majority class for
0.6.2,which the average distance to the farthest samples of the negative class is
0.6.2,"the smallest. NearMiss-3 is a 2-step algorithm: first, for each minority"
0.6.2,"sample, their ::math:`m` nearest-neighbors will be kept; then, the majority"
0.6.2,samples selected are the on for which the average distance to the :math:`k`
0.6.2,nearest neighbors is the largest.
0.6.2,##############################################################################
0.6.2,``EditedNearestNeighbours`` removes samples of the majority class for which
0.6.2,their class differ from the one of their nearest-neighbors. This sieve can be
0.6.2,repeated which is the principle of the
0.6.2,``RepeatedEditedNearestNeighbours``. ``AllKNN`` is slightly different from
0.6.2,the ``RepeatedEditedNearestNeighbours`` by changing the :math:`k` parameter
0.6.2,"of the internal nearest neighors algorithm, increasing it at each iteration."
0.6.2,##############################################################################
0.6.2,``CondensedNearestNeighbour`` makes use of a 1-NN to iteratively decide if a
0.6.2,sample should be kept in a dataset or not. The issue is that
0.6.2,``CondensedNearestNeighbour`` is sensitive to noise by preserving the noisy
0.6.2,samples. ``OneSidedSelection`` also used the 1-NN and use ``TomekLinks`` to
0.6.2,remove the samples considered noisy. The ``NeighbourhoodCleaningRule`` use a
0.6.2,"``EditedNearestNeighbours`` to remove some sample. Additionally, they use a 3"
0.6.2,nearest-neighbors to remove samples which do not agree with this rule.
0.6.2,##############################################################################
0.6.2,``InstanceHardnessThreshold`` uses the prediction of classifier to exclude
0.6.2,samples. All samples which are classified with a low probability will be
0.6.2,removed.
0.6.2,##############################################################################
0.6.2,This function allows to make nice plotting
0.6.2,##############################################################################
0.6.2,Generate some data with one Tomek link
0.6.2,minority class
0.6.2,majority class
0.6.2,##############################################################################
0.6.2,"In the figure above, the samples highlighted in green form a Tomek link since"
0.6.2,they are of different classes and are nearest neighbours of each other.
0.6.2,highlight the samples of interest
0.6.2,##############################################################################
0.6.2,We can run the ``TomekLinks`` sampling to remove the corresponding
0.6.2,samples. If ``sampling_strategy='auto'`` only the sample from the majority
0.6.2,class will be removed. If ``sampling_strategy='all'`` both samples will be
0.6.2,removed.
0.6.2,highlight the samples of interest
0.6.2,##############################################################################
0.6.2,This function allows to make nice plotting
0.6.2,##############################################################################
0.6.2,We can start by generating some data to later illustrate the principle of
0.6.2,each NearMiss heuritic rules.
0.6.2,minority class
0.6.2,majority class
0.6.2,##############################################################################
0.6.2,NearMiss-1
0.6.2,##############################################################################
0.6.2,##############################################################################
0.6.2,NearMiss-1 selects samples from the majority class for which the average
0.6.2,distance to some nearest neighbours is the smallest. In the following
0.6.2,"example, we use a 3-NN to compute the average distance on 2 specific samples"
0.6.2,"of the majority class. Therefore, in this case the point linked by the"
0.6.2,green-dashed line will be selected since the average distance is smaller.
0.6.2,##############################################################################
0.6.2,NearMiss-2
0.6.2,##############################################################################
0.6.2,##############################################################################
0.6.2,NearMiss-2 selects samples from the majority class for which the average
0.6.2,distance to the farthest neighbors is the smallest. With the same
0.6.2,"configuration as previously presented, the sample linked to the green-dashed"
0.6.2,line will be selected since its distance the 3 farthest neighbors is the
0.6.2,smallest.
0.6.2,##############################################################################
0.6.2,NearMiss-3
0.6.2,##############################################################################
0.6.2,##############################################################################
0.6.2,"NearMiss-3 can be divided into 2 steps. First, a nearest-neighbors is used to"
0.6.2,short-list samples from the majority class (i.e. correspond to the
0.6.2,"highlighted samples in the following plot). Then, the sample with the largest"
0.6.2,average distance to the *k* nearest-neighbors are selected.
0.6.2,select only the majority point of interest
0.6.2,Authors: Christos Aridas
0.6.2,Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.6.2,License: MIT
0.6.2,Generate the dataset
0.6.2,Instanciate a PCA object for the sake of easy visualisation
0.6.2,Create the samplers
0.6.2,Create the classifier
0.6.2,Make the splits
0.6.2,Add one transformers and two samplers in the pipeline object
0.6.2,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.6.2,License: MIT
0.6.2,##############################################################################
0.6.2,Data loading
0.6.2,##############################################################################
0.6.2,##############################################################################
0.6.2,"First, you should download the Porto Seguro data set from Kaggle. See the"
0.6.2,link in the introduction.
0.6.2,##############################################################################
0.6.2,The data set is imbalanced and it will have an effect on the fitting.
0.6.2,##############################################################################
0.6.2,Define the pre-processing pipeline
0.6.2,##############################################################################
0.6.2,##############################################################################
0.6.2,We want to standard scale the numerical features while we want to one-hot
0.6.2,"encode the categorical features. In this regard, we make use of the"
0.6.2,:class:`sklearn.compose.ColumnTransformer`.
0.6.2,Create an environment variable to avoid using the GPU. This can be changed.
0.6.2,##############################################################################
0.6.2,Create a neural-network
0.6.2,##############################################################################
0.6.2,##############################################################################
0.6.2,We create a decorator to report the computation time
0.6.2,##############################################################################
0.6.2,The first model will be trained using the ``fit`` method and with imbalanced
0.6.2,mini-batches.
0.6.2,##############################################################################
0.6.2,"In the contrary, we will use imbalanced-learn to create a generator of"
0.6.2,mini-batches which will yield balanced mini-batches.
0.6.2,##############################################################################
0.6.2,Classification loop
0.6.2,##############################################################################
0.6.2,##############################################################################
0.6.2,We will perform a 10-fold cross-validation and train the neural-network with
0.6.2,the two different strategies previously presented.
0.6.2,##############################################################################
0.6.2,Plot of the results and computation time
0.6.2,##############################################################################
0.6.2,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.6.2,License: MIT
0.6.2,##############################################################################
0.6.2,Problem definition
0.6.2,##############################################################################
0.6.2,we are dropping the following features:
0.6.2,"- ""fnlwgt"": this feature was created while studying the ""adult"" dataset."
0.6.2,"Thus, we will not use this feature which is not acquired during the survey."
0.6.2,"- ""education-num"": it is encoding the same information than ""education""."
0.6.2,"Thus, we are removing one of these 2 features."
0.6.2,##############################################################################
0.6.2,"The ""adult"" dataset as a class ratio of about 3:1"
0.6.2,##############################################################################
0.6.2,This dataset is only slightly imbalanced. To better highlight the effect of
0.6.2,"learning from an imbalanced dataset, we will increase its ratio to 30:1"
0.6.2,##############################################################################
0.6.2,"For the rest of the notebook, we will make a single split to get training"
0.6.2,and testing data. Note that you should use cross-validation to have an
0.6.2,estimate of the performance variation in practice.
0.6.2,##############################################################################
0.6.2,"As a baseline, we could use a classifier which will always predict the"
0.6.2,majority class independently of the features provided.
0.6.2,#############################################################################
0.6.2,"Instead of using the accuracy, we can use the balanced accuracy which will"
0.6.2,take into account the balancing issue.
0.6.2,##############################################################################
0.6.2,Strategies to learn from an imbalanced dataset
0.6.2,##############################################################################
0.6.2,##############################################################################
0.6.2,We will first define a helper function which will train a given model
0.6.2,and compute both accuracy and balanced accuracy. The results will be stored
0.6.2,in a dataframe
0.6.2,Let's define an empty dataframe to store the results
0.6.2,##############################################################################
0.6.2,Dummy baseline
0.6.2,..............
0.6.2,
0.6.2,"Before to train a real machine learning model, we can store the results"
0.6.2,obtained with our `DummyClassifier`.
0.6.2,##############################################################################
0.6.2,Linear classifier baseline
0.6.2,..........................
0.6.2,
0.6.2,We will create a machine learning pipeline using a `LogisticRegression`
0.6.2,"classifier. In this regard, we will need to one-hot encode the categorical"
0.6.2,columns and standardized the numerical columns before to inject the data into
0.6.2,the `LogisticRegression` classifier.
0.6.2,
0.6.2,"First, we define our numerical and categorical pipelines."
0.6.2,##############################################################################
0.6.2,"Then, we can create a preprocessor which will dispatch the categorical"
0.6.2,columns to the categorical pipeline and the numerical columns to the
0.6.2,numerical pipeline
0.6.2,##############################################################################
0.6.2,"Finally, we connect our preprocessor with our `LogisticRegression`. We can"
0.6.2,then evaluate our model.
0.6.2,##############################################################################
0.6.2,We can see that our linear model is learning slightly better than our dummy
0.6.2,"baseline. However, it is impacted by the class imbalance."
0.6.2,
0.6.2,We can verify that something similar is happening with a tree-based model
0.6.2,"such as `RandomForestClassifier`. With this type of classifier, we will not"
0.6.2,"need to scale the numerical data, and we will only need to ordinal encode the"
0.6.2,categorical data.
0.6.2,##############################################################################
0.6.2,"The `RandomForestClassifier` is as well affected by the class imbalanced,"
0.6.2,"slightly less than the linear model. Now, we will present different approach"
0.6.2,to improve the performance of these 2 models.
0.6.2,
0.6.2,Use `class_weight`
0.6.2,..................
0.6.2,
0.6.2,Most of the models in `scikit-learn` have a parameter `class_weight`. This
0.6.2,parameter will affect the computation of the loss in linear model or the
0.6.2,criterion in the tree-based model to penalize differently a false
0.6.2,classification from the minority and majority class. We can set
0.6.2,"`class_weight=""balanced""` such that the weight applied is inversely"
0.6.2,proportional to the class frequency. We test this parametrization in both
0.6.2,linear model and tree-based model.
0.6.2,##############################################################################
0.6.2,
0.6.2,##############################################################################
0.6.2,We can see that using `class_weight` was really effective for the linear
0.6.2,"model, alleviating the issue of learning from imbalanced classes. However,"
0.6.2,"the `RandomForestClassifier` is still biased toward the majority class,"
0.6.2,mainly due to the criterion which is not suited enough to fight the class
0.6.2,imbalance.
0.6.2,
0.6.2,Resample the training set during learning
0.6.2,.........................................
0.6.2,
0.6.2,Another way is to resample the training set by under-sampling or
0.6.2,over-sampling some of the samples. `imbalanced-learn` provides some samplers
0.6.2,to do such processing.
0.6.2,##############################################################################
0.6.2,
0.6.2,##############################################################################
0.6.2,Applying a random under-sampler before the training of the linear model or
0.6.2,"random forest, allows to not focus on the majority class at the cost of"
0.6.2,making more mistake for samples in the majority class (i.e. decreased
0.6.2,accuracy).
0.6.2,
0.6.2,We could apply any type of samplers and find which sampler is working best
0.6.2,on the current dataset.
0.6.2,
0.6.2,"Instead, we will present another way by using classifiers which will apply"
0.6.2,sampling internally.
0.6.2,
0.6.2,Use of `BalancedRandomForestClassifier` and `BalancedBaggingClassifier`
0.6.2,.......................................................................
0.6.2,
0.6.2,We already showed that random under-sampling can be effective on decision
0.6.2,"tree. However, instead of under-sampling once the dataset, one could"
0.6.2,under-sample the original dataset before to take a bootstrap sample. This is
0.6.2,the base of the `BalancedRandomForestClassifier` and
0.6.2,`BalancedBaggingClassifier`.
0.6.2,##############################################################################
0.6.2,The performance with the `BalancedRandomForestClassifier` is better than
0.6.2,applying a single random under-sampling. We will use a gradient-boosting
0.6.2,classifier within a `BalancedBaggingClassifier`.
0.6.2,##############################################################################
0.6.2,This last approach is the most effective. The different under-sampling allows
0.6.2,to bring some diversity for the different GBDT to learn and not focus on a
0.6.2,portion of the majority class.
0.6.2,
0.6.2,We will repeat the same experiment but with a ratio of 100:1 and make a
0.6.2,similar analysis.
0.6.2,##############################################################################
0.6.2,Increase imbalanced ratio
0.6.2,##############################################################################
0.6.2,##############################################################################
0.6.2,"When we analyse the results, we can draw similar conclusions than in the"
0.6.2,"previous discussion. However, we can observe that the strategy"
0.6.2,"`class_weight=""balanced""` does not improve the performance when using a"
0.6.2,`RandomForestClassifier`. A resampling is indeed required. The most effective
0.6.2,method remains the `BalancedBaggingClassifier` using a GBDT as a base
0.6.2,learner.
0.6.2,Authors: Christos Aridas
0.6.2,Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.6.2,License: MIT
0.6.2,Load the dataset
0.6.2,make nice plotting
0.6.2,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.6.2,License: MIT
0.6.2,Create a folder to fetch the dataset
0.6.2,Create a pipeline
0.6.2,Classify and report the results
0.6.2,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.6.2,License: MIT
0.6.2,##############################################################################
0.6.2,Setting the data set
0.6.2,##############################################################################
0.6.2,##############################################################################
0.6.2,We use a part of the 20 newsgroups data set by loading 4 topics. Using the
0.6.2,"scikit-learn loader, the data are split into a training and a testing set."
0.6.2,
0.6.2,Note the class \#3 is the minority class and has almost twice less samples
0.6.2,than the majority class.
0.6.2,##############################################################################
0.6.2,The usual scikit-learn pipeline
0.6.2,##############################################################################
0.6.2,##############################################################################
0.6.2,You might usually use scikit-learn pipeline by combining the TF-IDF
0.6.2,vectorizer to feed a multinomial naive bayes classifier. A classification
0.6.2,report summarized the results on the testing set.
0.6.2,
0.6.2,"As expected, the recall of the class \#3 is low mainly due to the class"
0.6.2,imbalanced.
0.6.2,##############################################################################
0.6.2,Balancing the class before classification
0.6.2,##############################################################################
0.6.2,##############################################################################
0.6.2,"To improve the prediction of the class \#3, it could be interesting to apply"
0.6.2,"a balancing before to train the naive bayes classifier. Therefore, we will"
0.6.2,use a ``RandomUnderSampler`` to equalize the number of samples in all the
0.6.2,classes before the training.
0.6.2,
0.6.2,It is also important to note that we are using the ``make_pipeline`` function
0.6.2,implemented in imbalanced-learn to properly handle the samplers.
0.6.2,##############################################################################
0.6.2,"Although the results are almost identical, it can be seen that the resampling"
0.6.2,allowed to correct the poor recall of the class \#3 at the cost of reducing
0.6.2,"the other metrics for the other classes. However, the overall results are"
0.6.2,slightly better.
0.6.2,Authors: Dayvid Oliveira
0.6.2,Christos Aridas
0.6.2,Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.6.2,License: MIT
0.6.2,Generate the dataset
0.6.2,"Two subplots, unpack the axes array immediately"
0.6.2,List of whitelisted modules and methods; regexp are supported.
0.6.2,These docstrings will fail because they are inheriting from scikit-learn
0.6.2,skip private classes
0.6.2,"We ignore following error code,"
0.6.2,- RT02: The first line of the Returns section
0.6.2,"should contain only the type, .."
0.6.2,(as we may need refer to the name of the returned
0.6.2,object)
0.6.2,- GL01: Docstring text (summary) should start in the line
0.6.2,"immediately after the opening quotes (not in the same line,"
0.6.2,or leaving a blank line in between)
0.6.2,Following codes are only taken into account for the
0.6.2,top level class docstrings:
0.6.2,- ES01: No extended summary found
0.6.2,- SA01: See Also section not found
0.6.2,- EX01: No examples section found
0.6.2,In particular we can't parse the signature of properties
0.6.2,"When applied to classes, detect class method. For functions"
0.6.2,method = None.
0.6.2,TODO: this detection can be improved. Currently we assume that we have
0.6.2,class # methods if the second path element before last is in camel case.
0.6.2,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.6.2,License: MIT
0.6.2,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.6.2,Christos Aridas
0.6.2,License: MIT
0.6.2,define an alias for back-compatibility
0.6.2,Adapted from scikit-learn
0.6.2,Author: Edouard Duchesnay
0.6.2,Gael Varoquaux
0.6.2,Virgile Fritsch
0.6.2,Alexandre Gramfort
0.6.2,Lars Buitinck
0.6.2,Christos Aridas
0.6.2,Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.6.2,License: BSD
0.6.2,BaseEstimator interface
0.6.2,validate names
0.6.2,validate estimators
0.6.2,We allow last estimator to be None as an identity transformation
0.6.2,Estimator interface
0.6.2,Setup the memory
0.6.2,joblib >= 0.12
0.6.2,we do not clone when caching is disabled to
0.6.2,preserve backward compatibility
0.6.2,joblib <= 0.11
0.6.2,we do not clone when caching is disabled to
0.6.2,preserve backward compatibility
0.6.2,Fit or load from cache the current transfomer
0.6.2,Replace the transformer of the step with the fitted
0.6.2,transformer. This is necessary when loading the transformer
0.6.2,from the cache.
0.6.2,Based on NiLearn package
0.6.2,License: simplified BSD
0.6.2,"PEP0440 compatible formatted version, see:"
0.6.2,https://www.python.org/dev/peps/pep-0440/
0.6.2,
0.6.2,Generic release markers:
0.6.2,X.Y
0.6.2,X.Y.Z # For bugfix releases
0.6.2,
0.6.2,Admissible pre-release markers:
0.6.2,X.YaN # Alpha release
0.6.2,X.YbN # Beta release
0.6.2,X.YrcN # Release Candidate
0.6.2,X.Y # Final release
0.6.2,
0.6.2,Dev branch marker is: 'X.Y.dev' or 'X.Y.devN' where N is an integer.
0.6.2,'X.Y.dev0' is the canonical version of 'X.Y.dev'
0.6.2,
0.6.2,coding: utf-8
0.6.2,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.6.2,Dariusz Brzezinski
0.6.2,License: MIT
0.6.2,Only negative labels
0.6.2,"Calculate tp_sum, pred_sum, true_sum ###"
0.6.2,labels are now from 0 to len(labels) - 1 -> use bincount
0.6.2,Pathological case
0.6.2,Compute the true negative
0.6.2,Retain only selected labels
0.6.2,"Finally, we have all our sufficient statistics. Divide! #"
0.6.2,"Divide, and on zero-division, set scores to 0 and warn:"
0.6.2,"Oddly, we may get an ""invalid"" rather than a ""divide"" error"
0.6.2,here.
0.6.2,Average the results
0.6.2,labels are now from 0 to len(labels) - 1 -> use bincount
0.6.2,Pathological case
0.6.2,Retain only selected labels
0.6.2,old version of scipy return MaskedConstant instead of 0.0
0.6.2,Create the list of tags
0.6.2,check that the scoring function does not need a score
0.6.2,and only a prediction
0.6.2,Compute the score from the scoring function
0.6.2,Square if desired
0.6.2,Get the signature of the sens/spec function
0.6.2,We need to extract from kwargs only the one needed by the
0.6.2,specificity and specificity
0.6.2,Make the intersection between the parameters
0.6.2,Create a sub dictionary
0.6.2,Check if the metric is the geometric mean
0.6.2,We do not support multilabel so the only average supported
0.6.2,is binary
0.6.2,Create the list of parameters through signature binding
0.6.2,Call the sens/spec function
0.6.2,Compute the dominance
0.6.2,Compute the different metrics
0.6.2,Precision/recall/f1
0.6.2,Specificity
0.6.2,Geometric mean
0.6.2,Index balanced accuracy
0.6.2,compute averages
0.6.2,coding: utf-8
0.6.2,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.6.2,Christos Aridas
0.6.2,License: MIT
0.6.2,##############################################################################
0.6.2,Utilities for testing
0.6.2,import some data to play with
0.6.2,restrict to a binary classification task
0.6.2,add noisy features to make the problem harder and avoid perfect results
0.6.2,"run classifier, get class probabilities and label predictions"
0.6.2,only interested in probabilities of the positive case
0.6.2,XXX: do we really want a special API for the binary case?
0.6.2,##############################################################################
0.6.2,Tests
0.6.2,detailed measures for each class
0.6.2,individual scoring function that can be used for grid search: in the
0.6.2,binary class case the score is the value of the measure for the positive
0.6.2,class (e.g. label == 1). This is deprecated for average != 'binary'.
0.6.2,Such a case may occur with non-stratified cross-validation
0.6.2,ensure the above were meaningful tests:
0.6.2,Bad pos_label
0.6.2,Bad average option
0.6.2,but average != 'binary'; even if data is binary
0.6.2,compute the geometric mean for the binary problem
0.6.2,print classification report with class names
0.6.2,print classification report with label detection
0.6.2,print classification report with class names
0.6.2,print classification report with label detection
0.6.2,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.6.2,Christos Aridas
0.6.2,License: MIT
0.6.2,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.6.2,Christos Aridas
0.6.2,License: MIT
0.6.2,The ratio is computed using a one-vs-rest manner. Using majority
0.6.2,in multi-class would lead to slightly different results at the
0.6.2,cost of introducing a new parameter.
0.6.2,rounding may cause new amount for n_samples
0.6.2,the nearest neighbors need to be fitted only on the current class
0.6.2,to find the class NN to generate new samples
0.6.2,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.6.2,Christos Aridas
0.6.2,License: MIT
0.6.2,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.6.2,Christos Aridas
0.6.2,License: MIT
0.6.2,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.6.2,Fernando Nogueira
0.6.2,Christos Aridas
0.6.2,Dzianis Dudnik
0.6.2,License: MIT
0.6.2,np.newaxis for backwards compatability with random_state
0.6.2,Samples are in danger for m/2 <= m' < m
0.6.2,Samples are noise for m = m'
0.6.2,divergence between borderline-1 and borderline-2
0.6.2,Create synthetic samples for borderline points.
0.6.2,only minority
0.6.2,we use a one-vs-rest policy to handle the multiclass in which
0.6.2,new samples will be created considering not only the majority
0.6.2,class but all over classes.
0.6.2,@Substitution(
0.6.2,"sampling_strategy=BaseOverSampler._sampling_strategy_docstring,"
0.6.2,random_state=_random_state_docstring)
0.6.2,compute the median of the standard deviation of the minority class
0.6.2,the input of the OneHotEncoder needs to be dense
0.6.2,we can replace the 1 entries of the categorical features with the
0.6.2,median of the standard deviation. It will ensure that whenever
0.6.2,"distance is computed between 2 samples, the difference will be equal"
0.6.2,to the median of the standard deviation as in the original paper.
0.6.2,reverse the encoding of the categorical features
0.6.2,the matrix is supposed to be in the CSR format after the stacking
0.6.2,change in sparsity structure more efficient with LIL than CSR
0.6.2,convert to dense array since scipy.sparse doesn't handle 3D
0.6.2,tie breaking argmax
0.6.2,validate the parameters
0.6.2,negate diagonal elements
0.6.2,target_class_indices = np.flatnonzero(y == class_sample)
0.6.2,"X_class = _safe_indexing(X, target_class_indices)"
0.6.2,identify cluster which are answering the requirements
0.6.2,the cluster is already considered balanced
0.6.2,not enough samples to apply SMOTE
0.6.2,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.6.2,Christos Aridas
0.6.2,License: MIT
0.6.2,check that we can oversample even with missing or infinite data
0.6.2,regression tests for #605
0.6.2,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.6.2,Christos Aridas
0.6.2,Dzianis Dudnik
0.6.2,License: MIT
0.6.2,create 2 random continuous feature
0.6.2,create a categorical feature using some string
0.6.2,create a categorical feature using some integer
0.6.2,return the categories
0.6.2,create 2 random continuous feature
0.6.2,create a categorical feature using some string
0.6.2,create a categorical feature using some integer
0.6.2,return the categories
0.6.2,create 2 random continuous feature
0.6.2,create a categorical feature using some string
0.6.2,create a categorical feature using some integer
0.6.2,return the categories
0.6.2,create 2 random continuous feature
0.6.2,create a categorical feature using some string
0.6.2,create a categorical feature using some integer
0.6.2,return the categories
0.6.2,create 2 random continuous feature
0.6.2,create a categorical feature using some string
0.6.2,create a categorical feature using some integer
0.6.2,part of the common test which apply to SMOTE-NC even if it is not default
0.6.2,constructible
0.6.2,Check that the samplers handle pandas dataframe and pandas series
0.6.2,Cast X and y to not default dtype
0.6.2,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.6.2,Christos Aridas
0.6.2,License: MIT
0.6.2,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.6.2,Christos Aridas
0.6.2,License: MIT
0.6.2,check that m_neighbors is properly set. Regression test for:
0.6.2,https://github.com/scikit-learn-contrib/imbalanced-learn/issues/568
0.6.2,shuffle the indices since the sampler are packing them by class
0.6.2,helper functions
0.6.2,input and output
0.6.2,build the model and weights
0.6.2,"build the loss, predict, and train operator"
0.6.2,Initialization of all variables in the graph
0.6.2,"For each epoch, run accuracy on train and test"
0.6.2,helper functions
0.6.2,input and output
0.6.2,build the model and weights
0.6.2,"build the loss, predict, and train operator"
0.6.2,Initialization of all variables in the graph
0.6.2,"For each epoch, run accuracy on train and test"
0.6.2,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.6.2,License: MIT
0.6.2,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.6.2,Fernando Nogueira
0.6.2,Christos Aridas
0.6.2,License: MIT
0.6.2,find which class to not consider
0.6.2,there is a Tomek link between two samples if they are both nearest
0.6.2,neighbors of each others.
0.6.2,Find the nearest neighbour of every point
0.6.2,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.6.2,Christos Aridas
0.6.2,License: MIT
0.6.2,Randomly get one sample from the majority class
0.6.2,Generate the index to select
0.6.2,Create the set C - One majority samples and all minority
0.6.2,Create the set S - all majority samples
0.6.2,fit knn on C
0.6.2,Check each sample in S if we keep it or drop it
0.6.2,Do not select sample which are already well classified
0.6.2,Classify on S
0.6.2,If the prediction do not agree with the true label
0.6.2,append it in C_x
0.6.2,Keep the index for later
0.6.2,Update C
0.6.2,fit a knn on C
0.6.2,This experimental to speed up the search
0.6.2,Classify all the element in S and avoid to test the
0.6.2,well classified elements
0.6.2,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.6.2,Dayvid Oliveira
0.6.2,Christos Aridas
0.6.2,License: MIT
0.6.2,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.6.2,Christos Aridas
0.6.2,License: MIT
0.6.2,Compute the distance considering the farthest neighbour
0.6.2,Sort the list of distance and get the index
0.6.2,Throw a warning to tell the user that we did not have enough samples
0.6.2,to select and that we just select everything
0.6.2,Select the desired number of samples
0.6.2,idx_tmp is relative to the feature selected in the
0.6.2,previous step and we need to find the indirection
0.6.2,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.6.2,Christos Aridas
0.6.2,License: MIT
0.6.2,select a sample from the current class
0.6.2,create the set composed of all minority samples and one
0.6.2,sample from the current class.
0.6.2,create the set S with removing the seed from S
0.6.2,since that it will be added anyway
0.6.2,apply Tomek cleaning
0.6.2,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.6.2,Dayvid Oliveira
0.6.2,Christos Aridas
0.6.2,License: MIT
0.6.2,Check the stopping criterion
0.6.2,1. If there is no changes for the vector y
0.6.2,2. If the number of samples in the other class become inferior to
0.6.2,the number of samples in the majority class
0.6.2,3. If one of the class is disappearing
0.6.2,Case 1
0.6.2,Case 2
0.6.2,Case 3
0.6.2,Check the stopping criterion
0.6.2,1. If the number of samples in the other class become inferior to
0.6.2,the number of samples in the majority class
0.6.2,2. If one of the class is disappearing
0.6.2,Case 1else:
0.6.2,overwrite b_min_bec_maj
0.6.2,Case 2
0.6.2,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.6.2,Christos Aridas
0.6.2,License: MIT
0.6.2,clean the neighborhood
0.6.2,compute which classes to consider for cleaning for the A2 group
0.6.2,compute a2 group
0.6.2,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.6.2,Christos Aridas
0.6.2,License: MIT
0.6.2,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.6.2,Christos Aridas
0.6.2,License: MIT
0.6.2,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.6.2,Christos Aridas
0.6.2,License: MIT
0.6.2,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.6.2,Christos Aridas
0.6.2,License: MIT
0.6.2,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.6.2,Christos Aridas
0.6.2,License: MIT
0.6.2,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.6.2,Christos Aridas
0.6.2,License: MIT
0.6.2,check that we can undersample even with missing or infinite data
0.6.2,regression tests for #605
0.6.2,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.6.2,Christos Aridas
0.6.2,License: MIT
0.6.2,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.6.2,Christos Aridas
0.6.2,License: MIT
0.6.2,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.6.2,Christos Aridas
0.6.2,License: MIT
0.6.2,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.6.2,Christos Aridas
0.6.2,License: MIT
0.6.2,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.6.2,Christos Aridas
0.6.2,License: MIT
0.6.2,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.6.2,Fernando Nogueira
0.6.2,Christos Aridas
0.6.2,License: MIT
0.6.2,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.6.2,Christos Aridas
0.6.2,License: MIT
0.6.2,test that all_estimators doesn't find abstract classes.
0.6.2,don't run twice the sampler tests. Meta-estimator do not have a
0.6.2,fit_resample method.
0.6.2,input validation etc for non-meta estimators
0.6.2,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.6.2,License: MIT
0.6.2,check that we can let a pass a regression variable by turning down the
0.6.2,validation
0.6.2,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.6.2,Christos Aridas
0.6.2,License: MIT
0.6.2,store timestamp to figure out whether the result of 'fit' has been
0.6.2,cached or not
0.6.2,store timestamp to figure out whether the result of 'fit' has been
0.6.2,cached or not
0.6.2,Pipeline accepts steps as tuple
0.6.2,Test the various init parameters of the pipeline.
0.6.2,Check that we can't instantiate pipelines with objects without fit
0.6.2,method
0.6.2,Smoke test with only an estimator
0.6.2,Check that params are set
0.6.2,Smoke test the repr:
0.6.2,Test with two objects
0.6.2,Check that we can't instantiate with non-transformers on the way
0.6.2,"Note that NoTrans implements fit, but not transform"
0.6.2,Check that params are set
0.6.2,Smoke test the repr:
0.6.2,Check that params are not set when naming them wrong
0.6.2,Test clone
0.6.2,"Check that apart from estimators, the parameters are the same"
0.6.2,Remove estimators that where copied
0.6.2,Test the various methods of the pipeline (anova).
0.6.2,Test with Anova + LogisticRegression
0.6.2,Test that the pipeline can take fit parameters
0.6.2,classifier should return True
0.6.2,and transformer params should not be changed
0.6.2,invalid parameters should raise an error message
0.6.2,Pipeline should pass sample_weight
0.6.2,When sample_weight is None it shouldn't be passed
0.6.2,Test pipeline raises set params error message for nested models.
0.6.2,nested model check
0.6.2,Test the various methods of the pipeline (pca + svm).
0.6.2,Test with PCA + SVC
0.6.2,Test the various methods of the pipeline (preprocessing + svm).
0.6.2,check shapes of various prediction functions
0.6.2,test that the fit_predict method is implemented on a pipeline
0.6.2,test that the fit_predict on pipeline yields same results as applying
0.6.2,transform and clustering steps separately
0.6.2,"As pipeline doesn't clone estimators on construction,"
0.6.2,it must have its own estimators
0.6.2,first compute the transform and clustering step separately
0.6.2,use a pipeline to do the transform and clustering in one step
0.6.2,tests that a pipeline does not have fit_predict method when final
0.6.2,step of pipeline does not have fit_predict defined
0.6.2,tests that Pipeline passes fit_params to intermediate steps
0.6.2,when fit_predict is invoked
0.6.2,Test whether pipeline works with a transformer at the end.
0.6.2,Also test pipeline.transform and pipeline.inverse_transform
0.6.2,test transform and fit_transform:
0.6.2,Test whether pipeline works with a transformer missing fit_transform
0.6.2,test fit_transform:
0.6.2,Directly setting attr
0.6.2,Using set_params
0.6.2,Using set_params to replace single step
0.6.2,With invalid data
0.6.2,Test setting Pipeline steps to None
0.6.2,"for other methods, ensure no AttributeErrors on None:"
0.6.2,mult2 and mult3 are active
0.6.2,Check 'passthrough' step at construction time
0.6.2,Test that an error is raised when memory is not a string or a Memory
0.6.2,instance
0.6.2,Define memory as an integer
0.6.2,Test with Transformer + SVC
0.6.2,Memoize the transformer at the first fit
0.6.2,Get the time stamp of the tranformer in the cached pipeline
0.6.2,Check that cached_pipe and pipe yield identical results
0.6.2,Check that we are reading the cache while fitting
0.6.2,a second time
0.6.2,Check that cached_pipe and pipe yield identical results
0.6.2,Create a new pipeline with cloned estimators
0.6.2,Check that even changing the name step does not affect the cache hit
0.6.2,Check that cached_pipe and pipe yield identical results
0.6.2,Test with Transformer + SVC
0.6.2,Memoize the transformer at the first fit
0.6.2,Get the time stamp of the tranformer in the cached pipeline
0.6.2,Check that cached_pipe and pipe yield identical results
0.6.2,Check that we are reading the cache while fitting
0.6.2,a second time
0.6.2,Check that cached_pipe and pipe yield identical results
0.6.2,Create a new pipeline with cloned estimators
0.6.2,Check that even changing the name step does not affect the cache hit
0.6.2,Check that cached_pipe and pipe yield identical results
0.6.2,Test the various methods of the pipeline (pca + svm).
0.6.2,Test with PCA + SVC
0.6.2,Test the various methods of the pipeline (pca + svm).
0.6.2,Test with PCA + SVC
0.6.2,Test whether pipeline works with a sampler at the end.
0.6.2,Also test pipeline.sampler
0.6.2,test transform and fit_transform:
0.6.2,We round the value near to zero. It seems that PCA has some issue
0.6.2,with that
0.6.2,Test whether pipeline works with a sampler at the end.
0.6.2,Also test pipeline.sampler
0.6.2,Test pipeline using None as preprocessing step and a classifier
0.6.2,"Test pipeline using None, RUS and a classifier"
0.6.2,"Test pipeline using RUS, None and a classifier"
0.6.2,Test pipeline using None step and a sampler
0.6.2,Test pipeline using None and a transformer that implements transform and
0.6.2,inverse_transform
0.6.2,Test the various methods of the pipeline (anova).
0.6.2,Test with RandomUnderSampling + Anova + LogisticRegression
0.6.2,Test the various methods of the pipeline (anova).
0.6.2,Test the various methods of the pipeline (anova).
0.6.2,Test with RandomUnderSampling + Anova + LogisticRegression
0.6.2,tests that Pipeline passes predict_params to the final estimator
0.6.2,when predict is invoked
0.6.2,Test that the score_samples method is implemented on a pipeline.
0.6.2,Test that the score_samples method on pipeline yields same results as
0.6.2,applying transform and score_samples steps separately.
0.6.2,Check the shapes
0.6.2,Check the values
0.6.2,Test that a pipeline does not have score_samples method when the final
0.6.2,step of the pipeline does not have score_samples defined.
0.6.2,Test that the score_samples method is implemented on a pipeline.
0.6.2,Test that the score_samples method on pipeline yields same results as
0.6.2,applying transform and score_samples steps separately.
0.6.2,Check the shapes
0.6.2,Check the values
0.6.2,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.6.2,Christos Aridas
0.6.2,License: MIT
0.6.2,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.6.2,License: MIT
0.6.2,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.6.2,License: MIT
0.6.2,Adapated from scikit-learn
0.6.2,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.6.2,License: MIT
0.6.2,trigger our checks if this is a SamplerMixin
0.6.2,scikit-learn common tests
0.6.2,should raise warning if the target is continuous (we cannot raise error)
0.6.2,if the target is multilabel then we should raise an error
0.6.2,IHT does not enforce the number of samples but provide a number
0.6.2,of samples the closest to the desired target.
0.6.2,in this test we will force all samplers to not change the class 1
0.6.2,check that sparse matrices can be passed through the sampler leading to
0.6.2,the same results than dense
0.6.2,set KMeans to full since it support sparse and dense
0.6.2,Check that the samplers handle pandas dataframe and pandas series
0.6.2,check that we return the same type for dataframes or series types
0.6.2,Check that the can samplers handle simple lists
0.6.2,Check that multiclass target lead to the same results than OVA encoding
0.6.2,Cast X and y to not default dtype
0.6.2,Adapted from scikit-learn
0.6.2,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.6.2,License: MIT
0.6.2,meta-estimators need another estimator to be instantiated.
0.6.2,estimators that there is no way to default-construct sensibly
0.6.2,some strange ones
0.6.2,get parent folder
0.6.2,get rid of abstract base classes
0.6.2,get rid of sklearn estimators which have been imported in some classes
0.6.2,possibly get rid of meta estimators
0.6.2,"drop duplicates, sort for reproducibility"
0.6.2,itemgetter is used to ensure the sort does not extend to the 2nd item of
0.6.2,the tuple
0.6.2,Author: Alexander L. Hayes <hayesall@iu.edu>
0.6.2,License: MIT
0.6.2,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.6.2,License: MIT
0.6.2,check that all keys in sampling_strategy are also in y
0.6.2,check that there is no negative number
0.6.2,check that all keys in sampling_strategy are also in y
0.6.2,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.6.2,Christos Aridas
0.6.2,License: MIT
0.6.2,this function could create an equal number of samples
0.6.2,We pass on purpose a non sorted dictionary and check that the resulting
0.6.2,dictionary is sorted. Refer to issue #428.
0.6.2,DataFrame and DataFrame case
0.6.2,DataFrames and Series case
0.6.2,Author: Alexander L. Hayes <hayesall@iu.edu>
0.6.2,License: MIT
0.6.2,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.6.2,License: MIT
0.6.2,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.6.2,Christos Aridas
0.6.2,License: MIT
0.6.2,check if the filtering is working with a list or a single string
0.6.2,check that all estimators are sampler
0.6.2,check that an error is raised when the type is unknown
0.6.2,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.6.2,License: MIT
0.6.2,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.6.2,Christos Aridas
0.6.2,License: MIT
0.6.2,Otherwise create a default SMOTE
0.6.2,Otherwise create a default TomekLinks
0.6.2,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.6.2,Christos Aridas
0.6.2,License: MIT
0.6.2,Otherwise create a default SMOTE
0.6.2,Otherwise create a default EditedNearestNeighbours
0.6.2,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.6.2,Christos Aridas
0.6.2,License: MIT
0.6.2,Check if default job count is None
0.6.2,Check if job count is set
0.6.2,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.6.2,Christos Aridas
0.6.2,License: MIT
0.6.2,Check if default job count is none
0.6.2,Check if job count is set
0.6.2,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.6.2,License: MIT
0.6.2,resample before to fit the tree
0.6.2,Validate or convert input data
0.6.2,Pre-sort indices to avoid that each individual tree of the
0.6.2,ensemble sorts the indices.
0.6.2,Remap output
0.6.2,reshape is necessary to preserve the data contiguity against vs
0.6.2,"[:, np.newaxis] that does not."
0.6.2,Get bootstrap sample size
0.6.2,Check parameters
0.6.2,"Free allocated memory, if any"
0.6.2,We draw from the random state to get the random state we
0.6.2,would have got if we hadn't used a warm_start.
0.6.2,Parallel loop: we prefer the threading backend as the Cython code
0.6.2,for fitting the trees is internally releasing the Python GIL
0.6.2,making threading more efficient than multiprocessing in
0.6.2,"that case. However, we respect any parallel_backend contexts set"
0.6.2,"at a higher level, since correctness does not rely on using"
0.6.2,threads.
0.6.2,Collect newly grown trees
0.6.2,Create pipeline with the fitted samplers and trees
0.6.2,Decapsulate classes_ attributes
0.6.2,"with the resampling, we are likely to have rows not included"
0.6.2,for the OOB score leading to division by zero
0.6.2,Instances incorrectly classified
0.6.2,Error fraction
0.6.2,Stop if classification is perfect
0.6.2,Construct y coding as described in Zhu et al [2]:
0.6.2,
0.6.2,y_k = 1 if c == k else -1 / (K - 1)
0.6.2,
0.6.2,"where K == n_classes_ and c, k in [0, K) are indices along the second"
0.6.2,axis of the y coding with c being the index corresponding to the true
0.6.2,class label.
0.6.2,Displace zero probabilities so the log is defined.
0.6.2,Also fix negative elements which may occur with
0.6.2,negative sample weights.
0.6.2,Boost weight using multi-class AdaBoost SAMME.R alg
0.6.2,Only boost the weights if it will fit again
0.6.2,Only boost positive weights
0.6.2,Instances incorrectly classified
0.6.2,Error fraction
0.6.2,Stop if classification is perfect
0.6.2,Stop if the error is at least as bad as random guessing
0.6.2,Boost weight using multi-class AdaBoost SAMME alg
0.6.2,Only boost the weights if I will fit again
0.6.2,Only boost positive weights
0.6.2,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.6.2,Christos Aridas
0.6.2,License: MIT
0.6.2,RandomUnderSampler is not supporting sample_weight. We need to pass
0.6.2,None.
0.6.2,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.6.2,Christos Aridas
0.6.2,License: MIT
0.6.2,RandomUnderSampler is not supporting sample_weight. We need to pass
0.6.2,None.
0.6.2,check that we have an ensemble of samplers and estimators with a
0.6.2,consistent size
0.6.2,each sampler in the ensemble should have different random state
0.6.2,each estimator in the ensemble should have different random state
0.6.2,check the consistency of the feature importances
0.6.2,check the consistency of the prediction outpus
0.6.2,Predictions should be the same when sample_weight are all ones
0.6.2,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.6.2,Christos Aridas
0.6.2,License: MIT
0.6.2,Check classification for various parameter settings.
0.6.2,Test that bootstrapping samples generate non-perfect base estimators.
0.6.2,"without bootstrap, all trees are perfect on the training set"
0.6.2,disable the resampling by passing an empty dictionary.
0.6.2,"with bootstrap, trees are no longer perfect on the training set"
0.6.2,Test that bootstrapping features may generate duplicate features.
0.6.2,Predict probabilities.
0.6.2,Normal case
0.6.2,"Degenerate case, where some classes are missing"
0.6.2,Check that oob prediction is a good estimation of the generalization
0.6.2,error.
0.6.2,Test with few estimators
0.6.2,Check singleton ensembles.
0.6.2,Test that it gives proper exception on deficient input.
0.6.2,Test n_estimators
0.6.2,Test max_samples
0.6.2,Test max_features
0.6.2,Test support of decision_function
0.6.2,Check that bagging ensembles can be grid-searched.
0.6.2,Transform iris into a binary classification task
0.6.2,Grid search with scoring based on decision_function
0.6.2,Check base_estimator and its default values.
0.6.2,Test if fitting incrementally with warm start gives a forest of the
0.6.2,right size and the same results as a normal fit.
0.6.2,Test if warm start'ed second fit with smaller n_estimators raises error.
0.6.2,Test that nothing happens when fitting without increasing n_estimators
0.6.2,"modify X to nonsense values, this should not change anything"
0.6.2,warm started classifier with 5+5 estimators should be equivalent to
0.6.2,one classifier with 10 estimators
0.6.2,Check using oob_score and warm_start simultaneously fails
0.6.2,"Make sure OOB scores are identical when random_state, estimator, and"
0.6.2,training data are fixed and fitting is done twice
0.6.2,Check that format of estimators_samples_ is correct and that results
0.6.2,generated at fit time can be identically reproduced at a later time
0.6.2,using data saved in object attributes.
0.6.2,remap the y outside of the BalancedBaggingclassifier
0.6.2,"_, y = np.unique(y, return_inverse=True)"
0.6.2,Get relevant attributes
0.6.2,Test for correct formatting
0.6.2,Re-fit single estimator to test for consistent sampling
0.6.2,Make sure validated max_samples and original max_samples are identical
0.6.2,when valid integer max_samples supplied by user
0.6.2,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.6.2,Christos Aridas
0.6.2,License: MIT
0.6.2,Generate a global dataset to use
0.6.2,Check classification for various parameter settings.
0.6.2,test the different prediction function
0.6.2,Check base_estimator and its default values.
0.6.2,Test if fitting incrementally with warm start gives a forest of the
0.6.2,right size and the same results as a normal fit.
0.6.2,Test if warm start'ed second fit with smaller n_estimators raises error.
0.6.2,Test that nothing happens when fitting without increasing n_estimators
0.6.2,"modify X to nonsense values, this should not change anything"
0.6.2,warm started classifier with 5+5 estimators should be equivalent to
0.6.2,one classifier with 10 estimators
0.6.2,Check warning if not enough estimators
0.6.2,First fit with no restriction on max samples
0.6.2,Second fit with max samples restricted to just 2
0.6.2,Regression test for #655: check that the oob score is closed to 0.5
0.6.2,a binomial experiment.
0.6.2,Author: Guillaume Lemaitre
0.6.2,License: BSD 3 clause
0.6.2,"The index start at one, then we need to remove one"
0.6.2,to not have issue with the indexing.
0.6.2,go through the list and check if the data are available
0.6.2,Authors: Dayvid Oliveira
0.6.2,Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.6.2,Christos Aridas
0.6.2,License: MIT
0.6.2,restrict ratio to be a dict or a callable
0.6.2,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.6.2,Christos Aridas
0.6.2,License: MIT
0.6.2,"we are reusing part of utils.check_sampling_strategy, however this is not"
0.6.2,cover in the common tests so we will repeat it here
0.6.2,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.6.2,Christos Aridas
0.6.2,License: MIT
0.6.2,This is a trick to avoid an error during tests collection with pytest. We
0.6.2,avoid the error when importing the package raise the error at the moment of
0.6.2,creating the instance.
0.6.2,flag for keras sequence duck-typing
0.6.2,shuffle the indices since the sampler are packing them by class
0.6.1,This file is here so that when running from the root folder
0.6.1,./sklearn is added to sys.path by pytest.
0.6.1,See https://docs.pytest.org/en/latest/pythonpath.html for more details.
0.6.1,"For example, this allows to build extensions in place and run pytest"
0.6.1,doc/modules/clustering.rst and use sklearn from the local folder
0.6.1,rather than the one from site-packages.
0.6.1,Set numpy array str/repr to legacy behaviour on numpy > 1.13 to make
0.6.1,the doctests pass
0.6.1,! /usr/bin/env python
0.6.1,get __version__ from _version.py
0.6.1,-*- coding: utf-8 -*-
0.6.1,
0.6.1,"imbalanced-learn documentation build configuration file, created by"
0.6.1,sphinx-quickstart on Mon Jan 18 14:44:12 2016.
0.6.1,
0.6.1,This file is execfile()d with the current directory set to its
0.6.1,containing dir.
0.6.1,
0.6.1,Note that not all possible configuration values are present in this
0.6.1,autogenerated file.
0.6.1,
0.6.1,All configuration values have a default; values that are commented out
0.6.1,serve to show the default.
0.6.1,"If extensions (or modules to document with autodoc) are in another directory,"
0.6.1,add these directories to sys.path here. If the directory is relative to the
0.6.1,"documentation root, use os.path.abspath to make it absolute, like shown here."
0.6.1,-- General configuration ------------------------------------------------
0.6.1,"If your documentation needs a minimal Sphinx version, state it here."
0.6.1,needs_sphinx = '1.0'
0.6.1,"Add any Sphinx extension module names here, as strings. They can be"
0.6.1,extensions coming with Sphinx (named 'sphinx.ext.*') or your custom
0.6.1,ones.
0.6.1,this is needed for some reason...
0.6.1,see https://github.com/numpy/numpydoc/issues/69
0.6.1,"Add any paths that contain templates here, relative to this directory."
0.6.1,generate autosummary even if no references
0.6.1,The suffix of source filenames.
0.6.1,The encoding of source files.
0.6.1,source_encoding = 'utf-8-sig'
0.6.1,Generate the plot for the gallery
0.6.1,The master toctree document.
0.6.1,General information about the project.
0.6.1,"The version info for the project you're documenting, acts as replacement for"
0.6.1,"|version| and |release|, also used in various other places throughout the"
0.6.1,built documents.
0.6.1,
0.6.1,The short X.Y version.
0.6.1,"The full version, including alpha/beta/rc tags."
0.6.1,The language for content autogenerated by Sphinx. Refer to documentation
0.6.1,for a list of supported languages.
0.6.1,language = None
0.6.1,"There are two options for replacing |today|: either, you set today to some"
0.6.1,"non-false value, then it is used:"
0.6.1,today = ''
0.6.1,"Else, today_fmt is used as the format for a strftime call."
0.6.1,"today_fmt = '%B %d, %Y'"
0.6.1,"List of patterns, relative to source directory, that match files and"
0.6.1,directories to ignore when looking for source files.
0.6.1,The reST default role (used for this markup: `text`) to use for all
0.6.1,documents.
0.6.1,default_role = None
0.6.1,"If true, '()' will be appended to :func: etc. cross-reference text."
0.6.1,"If true, the current module name will be prepended to all description"
0.6.1,unit titles (such as .. function::).
0.6.1,add_module_names = True
0.6.1,"If true, sectionauthor and moduleauthor directives will be shown in the"
0.6.1,output. They are ignored by default.
0.6.1,show_authors = False
0.6.1,The name of the Pygments (syntax highlighting) style to use.
0.6.1,Custom style
0.6.1,A list of ignored prefixes for module index sorting.
0.6.1,modindex_common_prefix = []
0.6.1,"If true, keep warnings as ""system message"" paragraphs in the built documents."
0.6.1,keep_warnings = False
0.6.1,-- Options for HTML output ----------------------------------------------
0.6.1,The theme to use for HTML and HTML Help pages.  See the documentation for
0.6.1,a list of builtin themes.
0.6.1,Theme options are theme-specific and customize the look and feel of a theme
0.6.1,"further.  For a list of options available for each theme, see the"
0.6.1,documentation.
0.6.1,html_theme_options = {}
0.6.1,"Add any paths that contain custom themes here, relative to this directory."
0.6.1,"The name for this set of Sphinx documents.  If None, it defaults to"
0.6.1,"""<project> v<release> documentation""."
0.6.1,html_title = None
0.6.1,A shorter title for the navigation bar.  Default is the same as html_title.
0.6.1,html_short_title = None
0.6.1,The name of an image file (relative to this directory) to place at the top
0.6.1,of the sidebar.
0.6.1,html_logo = None
0.6.1,The name of an image file (within the static path) to use as favicon of the
0.6.1,docs.  This file should be a Windows icon file (.ico) being 16x16 or 32x32
0.6.1,pixels large.
0.6.1,html_favicon = None
0.6.1,"Add any paths that contain custom static files (such as style sheets) here,"
0.6.1,"relative to this directory. They are copied after the builtin static files,"
0.6.1,"so a file named ""default.css"" will overwrite the builtin ""default.css""."
0.6.1,Add any extra paths that contain custom files (such as robots.txt or
0.6.1,".htaccess) here, relative to this directory. These files are copied"
0.6.1,directly to the root of the documentation.
0.6.1,html_extra_path = []
0.6.1,"If not '', a 'Last updated on:' timestamp is inserted at every page bottom,"
0.6.1,using the given strftime format.
0.6.1,"html_last_updated_fmt = '%b %d, %Y'"
0.6.1,"If true, SmartyPants will be used to convert quotes and dashes to"
0.6.1,typographically correct entities.
0.6.1,html_use_smartypants = True
0.6.1,"Custom sidebar templates, maps document names to template names."
0.6.1,html_sidebars = {}
0.6.1,"Additional templates that should be rendered to pages, maps page names to"
0.6.1,template names.
0.6.1,html_additional_pages = {}
0.6.1,"If false, no module index is generated."
0.6.1,html_domain_indices = True
0.6.1,"If false, no index is generated."
0.6.1,html_use_index = True
0.6.1,"If true, the index is split into individual pages for each letter."
0.6.1,html_split_index = False
0.6.1,"If true, links to the reST sources are added to the pages."
0.6.1,html_show_sourcelink = True
0.6.1,"If true, ""Created using Sphinx"" is shown in the HTML footer. Default is True."
0.6.1,html_show_sphinx = True
0.6.1,"If true, ""(C) Copyright ..."" is shown in the HTML footer. Default is True."
0.6.1,html_show_copyright = True
0.6.1,"If true, an OpenSearch description file will be output, and all pages will"
0.6.1,contain a <link> tag referring to it.  The value of this option must be the
0.6.1,base URL from which the finished HTML is served.
0.6.1,html_use_opensearch = ''
0.6.1,"This is the file name suffix for HTML files (e.g. "".xhtml"")."
0.6.1,html_file_suffix = None
0.6.1,Output file base name for HTML help builder.
0.6.1,-- Options for LaTeX output ---------------------------------------------
0.6.1,The paper size ('letterpaper' or 'a4paper').
0.6.1,"'papersize': 'letterpaper',"
0.6.1,"The font size ('10pt', '11pt' or '12pt')."
0.6.1,"'pointsize': '10pt',"
0.6.1,Additional stuff for the LaTeX preamble.
0.6.1,"'preamble': '',"
0.6.1,Grouping the document tree into LaTeX files. List of tuples
0.6.1,"(source start file, target name, title,"
0.6.1,"author, documentclass [howto, manual, or own class])."
0.6.1,The name of an image file (relative to this directory) to place at the top of
0.6.1,the title page.
0.6.1,latex_logo = None
0.6.1,"For ""manual"" documents, if this is true, then toplevel headings are parts,"
0.6.1,not chapters.
0.6.1,latex_use_parts = False
0.6.1,"If true, show page references after internal links."
0.6.1,latex_show_pagerefs = False
0.6.1,"If true, show URL addresses after external links."
0.6.1,latex_show_urls = False
0.6.1,Documents to append as an appendix to all manuals.
0.6.1,latex_appendices = []
0.6.1,intersphinx configuration
0.6.1,sphinx-gallery configuration
0.6.1,-- Options for manual page output ---------------------------------------
0.6.1,"If false, no module index is generated."
0.6.1,latex_domain_indices = True
0.6.1,One entry per manual page. List of tuples
0.6.1,"(source start file, name, description, authors, manual section)."
0.6.1,"If true, show URL addresses after external links."
0.6.1,man_show_urls = False
0.6.1,-- Options for Texinfo output -------------------------------------------
0.6.1,Grouping the document tree into Texinfo files. List of tuples
0.6.1,"(source start file, target name, title, author,"
0.6.1,"dir menu entry, description, category)"
0.6.1,"def generate_example_rst(app, what, name, obj, options, lines):"
0.6.1,"# generate empty examples files, so that we don't get"
0.6.1,# inclusion errors if there are no examples for a class / module
0.6.1,"examples_path = os.path.join(app.srcdir, ""generated"","
0.6.1,"""%s.examples"" % name)"
0.6.1,if not os.path.exists(examples_path):
0.6.1,# touch file
0.6.1,"open(examples_path, 'w').close()"
0.6.1,Config for sphinx_issues
0.6.1,Temporary work-around for spacing problem between parameter and parameter
0.6.1,"type in the doc, see https://github.com/numpy/numpydoc/issues/215. The bug"
0.6.1,has been fixed in sphinx (https://github.com/sphinx-doc/sphinx/pull/5976) but
0.6.1,through a change in sphinx basic.css except rtd_theme does not use basic.css.
0.6.1,"In an ideal world, this would get fixed in this PR:"
0.6.1,https://github.com/readthedocs/sphinx_rtd_theme/pull/747/files
0.6.1,"app.connect('autodoc-process-docstring', generate_example_rst)"
0.6.1,Documents to append as an appendix to all manuals.
0.6.1,texinfo_appendices = []
0.6.1,"If false, no module index is generated."
0.6.1,texinfo_domain_indices = True
0.6.1,"How to display URL addresses: 'footnote', 'no', or 'inline'."
0.6.1,texinfo_show_urls = 'footnote'
0.6.1,"If true, do not generate a @detailmenu in the ""Top"" node's menu."
0.6.1,texinfo_no_detailmenu = False
0.6.1,The following is used by sphinx.ext.linkcode to provide links to github
0.6.1,get the styles from the current theme
0.6.1,create and add the button to all the code blocks that contain >>>
0.6.1,tracebacks (.gt) contain bare text elements that need to be
0.6.1,wrapped in a span to work with .nextUntil() (see later)
0.6.1,define the behavior of the button when it's clicked
0.6.1,hide the code output
0.6.1,show the code output
0.6.1,-*- coding: utf-8 -*-
0.6.1,Format template for issues URI
0.6.1,e.g. 'https://github.com/sloria/marshmallow/issues/{issue}
0.6.1,Format template for PR URI
0.6.1,e.g. 'https://github.com/sloria/marshmallow/pull/{issue}
0.6.1,Format template for commit URI
0.6.1,e.g. 'https://github.com/sloria/marshmallow/commits/{commit}
0.6.1,"Shortcut for Github, e.g. 'sloria/marshmallow'"
0.6.1,Format template for user profile URI
0.6.1,e.g. 'https://github.com/{user}'
0.6.1,Python 2 only
0.6.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.6.1,License: MIT
0.6.1,##############################################################################
0.6.1,"First, we will create an imbalanced data set from a the iris data set."
0.6.1,##############################################################################
0.6.1,Using ``sampling_strategy`` in resampling algorithms
0.6.1,##############################################################################
0.6.1,##############################################################################
0.6.1,``sampling_strategy`` as a ``float``
0.6.1,....................................
0.6.1,
0.6.1,``sampling_strategy`` can be given a ``float``. For **under-sampling
0.6.1,"methods**, it corresponds to the ratio :math:`\\alpha_{us}` defined by"
0.6.1,:math:`N_{rM} = \\alpha_{us} \\times N_{m}` where :math:`N_{rM}` and
0.6.1,:math:`N_{m}` are the number of samples in the majority class after
0.6.1,"resampling and the number of samples in the minority class, respectively."
0.6.1,select only 2 classes since the ratio make sense in this case
0.6.1,##############################################################################
0.6.1,"For **over-sampling methods**, it correspond to the ratio"
0.6.1,:math:`\\alpha_{os}` defined by :math:`N_{rm} = \\alpha_{os} \\times N_{M}`
0.6.1,where :math:`N_{rm}` and :math:`N_{M}` are the number of samples in the
0.6.1,minority class after resampling and the number of samples in the majority
0.6.1,"class, respectively."
0.6.1,##############################################################################
0.6.1,``sampling_strategy`` has a ``str``
0.6.1,...................................
0.6.1,
0.6.1,``sampling_strategy`` can be given as a string which specify the class
0.6.1,"targeted by the resampling. With under- and over-sampling, the number of"
0.6.1,samples will be equalized.
0.6.1,
0.6.1,Note that we are using multiple classes from now on.
0.6.1,##############################################################################
0.6.1,"With **cleaning method**, the number of samples in each class will not be"
0.6.1,equalized even if targeted.
0.6.1,##############################################################################
0.6.1,``sampling_strategy`` as a ``dict``
0.6.1,...................................
0.6.1,
0.6.1,"When ``sampling_strategy`` is a ``dict``, the keys correspond to the targeted"
0.6.1,classes. The values correspond to the desired number of samples for each
0.6.1,targeted class. This is working for both **under- and over-sampling**
0.6.1,algorithms but not for the **cleaning algorithms**. Use a ``list`` instead.
0.6.1,##############################################################################
0.6.1,``sampling_strategy`` as a ``list``
0.6.1,...................................
0.6.1,
0.6.1,"When ``sampling_strategy`` is a ``list``, the list contains the targeted"
0.6.1,classes. It is used only for **cleaning methods** and raise an error
0.6.1,otherwise.
0.6.1,##############################################################################
0.6.1,``sampling_strategy`` as a callable
0.6.1,...................................
0.6.1,
0.6.1,"When callable, function taking ``y`` and returns a ``dict``. The keys"
0.6.1,correspond to the targeted classes. The values correspond to the desired
0.6.1,number of samples for each class.
0.6.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.6.1,License: MIT
0.6.1,#############################################################################
0.6.1,Toy data generation
0.6.1,#############################################################################
0.6.1,#############################################################################
0.6.1,We are generating some non Gaussian data set contaminated with some unform
0.6.1,noise.
0.6.1,#############################################################################
0.6.1,We will generate some cleaned test data without outliers.
0.6.1,#############################################################################
0.6.1,How to use the :class:`imblearn.FunctionSampler`
0.6.1,#############################################################################
0.6.1,#############################################################################
0.6.1,We first define a function which will use
0.6.1,:class:`sklearn.ensemble.IsolationForest` to eliminate some outliers from
0.6.1,our dataset during training. The function passed to the
0.6.1,:class:`imblearn.FunctionSampler` will be called when using the method
0.6.1,``fit_resample``.
0.6.1,#############################################################################
0.6.1,Integrate it within a pipeline
0.6.1,#############################################################################
0.6.1,#############################################################################
0.6.1,"By elimnating outliers before the training, the classifier will be less"
0.6.1,affected during the prediction.
0.6.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.6.1,License: MIT
0.6.1,generate some data points
0.6.1,plot the majority and minority samples
0.6.1,draw the circle in which the new sample will generated
0.6.1,plot the line on which the sample will be generated
0.6.1,create and plot the new sample
0.6.1,make the plot nicer with legend and label
0.6.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.6.1,License: MIT
0.6.1,##############################################################################
0.6.1,The following function will be used to create toy dataset. It using the
0.6.1,``make_classification`` from scikit-learn but fixing some parameters.
0.6.1,##############################################################################
0.6.1,The following function will be used to plot the sample space after resampling
0.6.1,to illustrate the characterisitic of an algorithm.
0.6.1,make nice plotting
0.6.1,##############################################################################
0.6.1,The following function will be used to plot the decision function of a
0.6.1,classifier given some data.
0.6.1,##############################################################################
0.6.1,Illustration of the influence of the balancing ratio
0.6.1,##############################################################################
0.6.1,##############################################################################
0.6.1,We will first illustrate the influence of the balancing ratio on some toy
0.6.1,data using a linear SVM classifier. Greater is the difference between the
0.6.1,"number of samples in each class, poorer are the classfication results."
0.6.1,##############################################################################
0.6.1,Random over-sampling to balance the data set
0.6.1,##############################################################################
0.6.1,##############################################################################
0.6.1,Random over-sampling can be used to repeat some samples and balance the
0.6.1,number of samples between the dataset. It can be seen that with this trivial
0.6.1,approach the boundary decision is already less biaised toward the majority
0.6.1,class.
0.6.1,##############################################################################
0.6.1,More advanced over-sampling using ADASYN and SMOTE
0.6.1,##############################################################################
0.6.1,##############################################################################
0.6.1,"Instead of repeating the same samples when over-sampling, we can use some"
0.6.1,specific heuristic instead. ADASYN and SMOTE can be used in this case.
0.6.1,Make an identity sampler
0.6.1,##############################################################################
0.6.1,The following plot illustrate the difference between ADASYN and SMOTE. ADASYN
0.6.1,will focus on the samples which are difficult to classify with a
0.6.1,nearest-neighbors rule while regular SMOTE will not make any distinction.
0.6.1,"Therefore, the decision function depending of the algorithm."
0.6.1,##############################################################################
0.6.1,"Due to those sampling particularities, it can give rise to some specific"
0.6.1,issues as illustrated below.
0.6.1,##############################################################################
0.6.1,SMOTE proposes several variants by identifying specific samples to consider
0.6.1,during the resampling. The borderline version will detect which point to
0.6.1,select which are in the border between two classes. The SVM version will use
0.6.1,the support vectors found using an SVM algorithm to create new sample while
0.6.1,the KMeans version will make a clustering before to generate samples in each
0.6.1,cluster independently depending each cluster density.
0.6.1,##############################################################################
0.6.1,"When dealing with a mixed of continuous and categorical features, SMOTE-NC"
0.6.1,is the only method which can handle this case.
0.6.1,create a synthetic data set with continuous and categorical features
0.6.1,Authors: Christos Aridas
0.6.1,Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.6.1,License: MIT
0.6.1,Generate the dataset
0.6.1,make nice plotting
0.6.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.6.1,License: MIT
0.6.1,Generate a dataset
0.6.1,Split the data
0.6.1,Train the classifier with balancing
0.6.1,Test the classifier and get the prediction
0.6.1,Show the classification report
0.6.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.6.1,License: MIT
0.6.1,Generate a dataset
0.6.1,Split the data
0.6.1,Train the classifier with balancing
0.6.1,Test the classifier and get the prediction
0.6.1,##############################################################################
0.6.1,The geometric mean corresponds to the square root of the product of the
0.6.1,sensitivity and specificity. Combining the two metrics should account for
0.6.1,the balancing of the dataset.
0.6.1,##############################################################################
0.6.1,The index balanced accuracy can transform any metric to be used in
0.6.1,imbalanced learning problems.
0.6.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.6.1,License: MIT
0.6.1,##############################################################################
0.6.1,The following function will be used to create toy dataset. It using the
0.6.1,``make_classification`` from scikit-learn but fixing some parameters.
0.6.1,##############################################################################
0.6.1,The following function will be used to plot the sample space after resampling
0.6.1,to illustrate the characteristic of an algorithm.
0.6.1,make nice plotting
0.6.1,##############################################################################
0.6.1,The following function will be used to plot the decision function of a
0.6.1,classifier given some data.
0.6.1,##############################################################################
0.6.1,"``SMOTE`` allows to generate samples. However, this method of over-sampling"
0.6.1,"does not have any knowledge regarding the underlying distribution. Therefore,"
0.6.1,"some noisy samples can be generated, e.g. when the different classes cannot"
0.6.1,"be well separated. Hence, it can be beneficial to apply an under-sampling"
0.6.1,algorithm to clean the noisy samples. Two methods are usually used in the
0.6.1,literature: (i) Tomek's link and (ii) edited nearest neighbours cleaning
0.6.1,methods. Imbalanced-learn provides two ready-to-use samplers ``SMOTETomek``
0.6.1,"and ``SMOTEENN``. In general, ``SMOTEENN`` cleans more noisy data than"
0.6.1,``SMOTETomek``.
0.6.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.6.1,License: MIT
0.6.1,##############################################################################
0.6.1,Load an imbalanced dataset
0.6.1,##############################################################################
0.6.1,We will load the UCI SatImage dataset which has an imbalanced ratio of 9.3:1
0.6.1,(number of majority sample for a minority sample). The data are then split
0.6.1,into training and testing.
0.6.1,##############################################################################
0.6.1,Classification using a single decision tree
0.6.1,##############################################################################
0.6.1,We train a decision tree classifier which will be used as a baseline for the
0.6.1,rest of this example.
0.6.1,##############################################################################
0.6.1,The results are reported in terms of balanced accuracy and geometric mean
0.6.1,which are metrics widely used in the literature to validate model trained on
0.6.1,imbalanced set.
0.6.1,##############################################################################
0.6.1,Classification using bagging classifier with and without sampling
0.6.1,##############################################################################
0.6.1,"Instead of using a single tree, we will check if an ensemble of decsion tree"
0.6.1,"can actually alleviate the issue induced by the class imbalancing. First, we"
0.6.1,will use a bagging classifier and its counter part which internally uses a
0.6.1,random under-sampling to balanced each boostrap sample.
0.6.1,##############################################################################
0.6.1,Balancing each bootstrap sample allows to increase significantly the balanced
0.6.1,accuracy and the geometric mean.
0.6.1,##############################################################################
0.6.1,Classification using random forest classifier with and without sampling
0.6.1,##############################################################################
0.6.1,Random forest is another popular ensemble method and it is usually
0.6.1,"outperforming bagging. Here, we used a vanilla random forest and its balanced"
0.6.1,counterpart in which each bootstrap sample is balanced.
0.6.1,"Similarly to the previous experiment, the balanced classifier outperform the"
0.6.1,"classifier which learn from imbalanced bootstrap samples. In addition, random"
0.6.1,forest outsperforms the bagging classifier.
0.6.1,##############################################################################
0.6.1,Boosting classifier
0.6.1,##############################################################################
0.6.1,"In the same manner, easy ensemble classifier is a bag of balanced AdaBoost"
0.6.1,"classifier. However, it will be slower to train than random forest and will"
0.6.1,achieve worse performance.
0.6.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.6.1,License: MIT
0.6.1,##############################################################################
0.6.1,The following function will be used to create toy dataset. It using the
0.6.1,``make_classification`` from scikit-learn but fixing some parameters.
0.6.1,##############################################################################
0.6.1,The following function will be used to plot the sample space after resampling
0.6.1,to illustrate the characteristic of an algorithm.
0.6.1,make nice plotting
0.6.1,##############################################################################
0.6.1,The following function will be used to plot the decision function of a
0.6.1,classifier given some data.
0.6.1,##############################################################################
0.6.1,Prototype generation: under-sampling by generating new samples
0.6.1,##############################################################################
0.6.1,##############################################################################
0.6.1,``ClusterCentroids`` under-samples by replacing the original samples by the
0.6.1,centroids of the cluster found.
0.6.1,##############################################################################
0.6.1,Prototype selection: under-sampling by selecting existing samples
0.6.1,##############################################################################
0.6.1,##############################################################################
0.6.1,The algorithm performing prototype selection can be subdivided into two
0.6.1,groups: (i) the controlled under-sampling methods and (ii) the cleaning
0.6.1,under-sampling methods.
0.6.1,##############################################################################
0.6.1,"With the controlled under-sampling methods, the number of samples to be"
0.6.1,selected can be specified. ``RandomUnderSampler`` is the most naive way of
0.6.1,performing such selection by randomly selecting a given number of samples by
0.6.1,the targetted class.
0.6.1,##############################################################################
0.6.1,``NearMiss`` algorithms implement some heuristic rules in order to select
0.6.1,samples. NearMiss-1 selects samples from the majority class for which the
0.6.1,average distance of the :math:`k`` nearest samples of the minority class is
0.6.1,the smallest. NearMiss-2 selects the samples from the majority class for
0.6.1,which the average distance to the farthest samples of the negative class is
0.6.1,"the smallest. NearMiss-3 is a 2-step algorithm: first, for each minority"
0.6.1,"sample, their ::math:`m` nearest-neighbors will be kept; then, the majority"
0.6.1,samples selected are the on for which the average distance to the :math:`k`
0.6.1,nearest neighbors is the largest.
0.6.1,##############################################################################
0.6.1,``EditedNearestNeighbours`` removes samples of the majority class for which
0.6.1,their class differ from the one of their nearest-neighbors. This sieve can be
0.6.1,repeated which is the principle of the
0.6.1,``RepeatedEditedNearestNeighbours``. ``AllKNN`` is slightly different from
0.6.1,the ``RepeatedEditedNearestNeighbours`` by changing the :math:`k` parameter
0.6.1,"of the internal nearest neighors algorithm, increasing it at each iteration."
0.6.1,##############################################################################
0.6.1,``CondensedNearestNeighbour`` makes use of a 1-NN to iteratively decide if a
0.6.1,sample should be kept in a dataset or not. The issue is that
0.6.1,``CondensedNearestNeighbour`` is sensitive to noise by preserving the noisy
0.6.1,samples. ``OneSidedSelection`` also used the 1-NN and use ``TomekLinks`` to
0.6.1,remove the samples considered noisy. The ``NeighbourhoodCleaningRule`` use a
0.6.1,"``EditedNearestNeighbours`` to remove some sample. Additionally, they use a 3"
0.6.1,nearest-neighbors to remove samples which do not agree with this rule.
0.6.1,##############################################################################
0.6.1,``InstanceHardnessThreshold`` uses the prediction of classifier to exclude
0.6.1,samples. All samples which are classified with a low probability will be
0.6.1,removed.
0.6.1,##############################################################################
0.6.1,This function allows to make nice plotting
0.6.1,##############################################################################
0.6.1,Generate some data with one Tomek link
0.6.1,minority class
0.6.1,majority class
0.6.1,##############################################################################
0.6.1,"In the figure above, the samples highlighted in green form a Tomek link since"
0.6.1,they are of different classes and are nearest neighbours of each other.
0.6.1,highlight the samples of interest
0.6.1,##############################################################################
0.6.1,We can run the ``TomekLinks`` sampling to remove the corresponding
0.6.1,samples. If ``sampling_strategy='auto'`` only the sample from the majority
0.6.1,class will be removed. If ``sampling_strategy='all'`` both samples will be
0.6.1,removed.
0.6.1,highlight the samples of interest
0.6.1,##############################################################################
0.6.1,This function allows to make nice plotting
0.6.1,##############################################################################
0.6.1,We can start by generating some data to later illustrate the principle of
0.6.1,each NearMiss heuritic rules.
0.6.1,minority class
0.6.1,majority class
0.6.1,##############################################################################
0.6.1,NearMiss-1
0.6.1,##############################################################################
0.6.1,##############################################################################
0.6.1,NearMiss-1 selects samples from the majority class for which the average
0.6.1,distance to some nearest neighbours is the smallest. In the following
0.6.1,"example, we use a 3-NN to compute the average distance on 2 specific samples"
0.6.1,"of the majority class. Therefore, in this case the point linked by the"
0.6.1,green-dashed line will be selected since the average distance is smaller.
0.6.1,##############################################################################
0.6.1,NearMiss-2
0.6.1,##############################################################################
0.6.1,##############################################################################
0.6.1,NearMiss-2 selects samples from the majority class for which the average
0.6.1,distance to the farthest neighbors is the smallest. With the same
0.6.1,"configuration as previously presented, the sample linked to the green-dashed"
0.6.1,line will be selected since its distance the 3 farthest neighbors is the
0.6.1,smallest.
0.6.1,##############################################################################
0.6.1,NearMiss-3
0.6.1,##############################################################################
0.6.1,##############################################################################
0.6.1,"NearMiss-3 can be divided into 2 steps. First, a nearest-neighbors is used to"
0.6.1,short-list samples from the majority class (i.e. correspond to the
0.6.1,"highlighted samples in the following plot). Then, the sample with the largest"
0.6.1,average distance to the *k* nearest-neighbors are selected.
0.6.1,select only the majority point of interest
0.6.1,Authors: Christos Aridas
0.6.1,Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.6.1,License: MIT
0.6.1,Generate the dataset
0.6.1,Instanciate a PCA object for the sake of easy visualisation
0.6.1,Create the samplers
0.6.1,Create the classifier
0.6.1,Make the splits
0.6.1,Add one transformers and two samplers in the pipeline object
0.6.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.6.1,License: MIT
0.6.1,##############################################################################
0.6.1,Data loading
0.6.1,##############################################################################
0.6.1,##############################################################################
0.6.1,"First, you should download the Porto Seguro data set from Kaggle. See the"
0.6.1,link in the introduction.
0.6.1,##############################################################################
0.6.1,The data set is imbalanced and it will have an effect on the fitting.
0.6.1,##############################################################################
0.6.1,Define the pre-processing pipeline
0.6.1,##############################################################################
0.6.1,##############################################################################
0.6.1,We want to standard scale the numerical features while we want to one-hot
0.6.1,"encode the categorical features. In this regard, we make use of the"
0.6.1,:class:`sklearn.compose.ColumnTransformer`.
0.6.1,Create an environment variable to avoid using the GPU. This can be changed.
0.6.1,##############################################################################
0.6.1,Create a neural-network
0.6.1,##############################################################################
0.6.1,##############################################################################
0.6.1,We create a decorator to report the computation time
0.6.1,##############################################################################
0.6.1,The first model will be trained using the ``fit`` method and with imbalanced
0.6.1,mini-batches.
0.6.1,##############################################################################
0.6.1,"In the contrary, we will use imbalanced-learn to create a generator of"
0.6.1,mini-batches which will yield balanced mini-batches.
0.6.1,##############################################################################
0.6.1,Classification loop
0.6.1,##############################################################################
0.6.1,##############################################################################
0.6.1,We will perform a 10-fold cross-validation and train the neural-network with
0.6.1,the two different strategies previously presented.
0.6.1,##############################################################################
0.6.1,Plot of the results and computation time
0.6.1,##############################################################################
0.6.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.6.1,License: MIT
0.6.1,##############################################################################
0.6.1,Problem definition
0.6.1,##############################################################################
0.6.1,we are dropping the following features:
0.6.1,"- ""fnlwgt"": this feature was created while studying the ""adult"" dataset."
0.6.1,"Thus, we will not use this feature which is not acquired during the survey."
0.6.1,"- ""education-num"": it is encoding the same information than ""education""."
0.6.1,"Thus, we are removing one of these 2 features."
0.6.1,##############################################################################
0.6.1,"The ""adult"" dataset as a class ratio of about 3:1"
0.6.1,##############################################################################
0.6.1,This dataset is only slightly imbalanced. To better highlight the effect of
0.6.1,"learning from an imbalanced dataset, we will increase its ratio to 30:1"
0.6.1,##############################################################################
0.6.1,"For the rest of the notebook, we will make a single split to get training"
0.6.1,and testing data. Note that you should use cross-validation to have an
0.6.1,estimate of the performance variation in practice.
0.6.1,##############################################################################
0.6.1,"As a baseline, we could use a classifier which will always predict the"
0.6.1,majority class independently of the features provided.
0.6.1,#############################################################################
0.6.1,"Instead of using the accuracy, we can use the balanced accuracy which will"
0.6.1,take into account the balancing issue.
0.6.1,##############################################################################
0.6.1,Strategies to learn from an imbalanced dataset
0.6.1,##############################################################################
0.6.1,##############################################################################
0.6.1,We will first define a helper function which will train a given model
0.6.1,and compute both accuracy and balanced accuracy. The results will be stored
0.6.1,in a dataframe
0.6.1,Let's define an empty dataframe to store the results
0.6.1,##############################################################################
0.6.1,Dummy baseline
0.6.1,..............
0.6.1,
0.6.1,"Before to train a real machine learning model, we can store the results"
0.6.1,obtained with our `DummyClassifier`.
0.6.1,##############################################################################
0.6.1,Linear classifier baseline
0.6.1,..........................
0.6.1,
0.6.1,We will create a machine learning pipeline using a `LogisticRegression`
0.6.1,"classifier. In this regard, we will need to one-hot encode the categorical"
0.6.1,columns and standardized the numerical columns before to inject the data into
0.6.1,the `LogisticRegression` classifier.
0.6.1,
0.6.1,"First, we define our numerical and categorical pipelines."
0.6.1,##############################################################################
0.6.1,"Then, we can create a preprocessor which will dispatch the categorical"
0.6.1,columns to the categorical pipeline and the numerical columns to the
0.6.1,numerical pipeline
0.6.1,##############################################################################
0.6.1,"Finally, we connect our preprocessor with our `LogisticRegression`. We can"
0.6.1,then evaluate our model.
0.6.1,##############################################################################
0.6.1,We can see that our linear model is learning slightly better than our dummy
0.6.1,"baseline. However, it is impacted by the class imbalance."
0.6.1,
0.6.1,We can verify that something similar is happening with a tree-based model
0.6.1,"such as `RandomForestClassifier`. With this type of classifier, we will not"
0.6.1,"need to scale the numerical data, and we will only need to ordinal encode the"
0.6.1,categorical data.
0.6.1,##############################################################################
0.6.1,"The `RandomForestClassifier` is as well affected by the class imbalanced,"
0.6.1,"slightly less than the linear model. Now, we will present different approach"
0.6.1,to improve the performance of these 2 models.
0.6.1,
0.6.1,Use `class_weight`
0.6.1,..................
0.6.1,
0.6.1,Most of the models in `scikit-learn` have a parameter `class_weight`. This
0.6.1,parameter will affect the computation of the loss in linear model or the
0.6.1,criterion in the tree-based model to penalize differently a false
0.6.1,classification from the minority and majority class. We can set
0.6.1,"`class_weight=""balanced""` such that the weight applied is inversely"
0.6.1,proportional to the class frequency. We test this parametrization in both
0.6.1,linear model and tree-based model.
0.6.1,##############################################################################
0.6.1,
0.6.1,##############################################################################
0.6.1,We can see that using `class_weight` was really effective for the linear
0.6.1,"model, alleviating the issue of learning from imbalanced classes. However,"
0.6.1,"the `RandomForestClassifier` is still biased toward the majority class,"
0.6.1,mainly due to the criterion which is not suited enough to fight the class
0.6.1,imbalance.
0.6.1,
0.6.1,Resample the training set during learning
0.6.1,.........................................
0.6.1,
0.6.1,Another way is to resample the training set by under-sampling or
0.6.1,over-sampling some of the samples. `imbalanced-learn` provides some samplers
0.6.1,to do such processing.
0.6.1,##############################################################################
0.6.1,
0.6.1,##############################################################################
0.6.1,Applying a random under-sampler before the training of the linear model or
0.6.1,"random forest, allows to not focus on the majority class at the cost of"
0.6.1,making more mistake for samples in the majority class (i.e. decreased
0.6.1,accuracy).
0.6.1,
0.6.1,We could apply any type of samplers and find which sampler is working best
0.6.1,on the current dataset.
0.6.1,
0.6.1,"Instead, we will present another way by using classifiers which will apply"
0.6.1,sampling internally.
0.6.1,
0.6.1,Use of `BalancedRandomForestClassifier` and `BalancedBaggingClassifier`
0.6.1,.......................................................................
0.6.1,
0.6.1,We already showed that random under-sampling can be effective on decision
0.6.1,"tree. However, instead of under-sampling once the dataset, one could"
0.6.1,under-sample the original dataset before to take a bootstrap sample. This is
0.6.1,the base of the `BalancedRandomForestClassifier` and
0.6.1,`BalancedBaggingClassifier`.
0.6.1,##############################################################################
0.6.1,The performance with the `BalancedRandomForestClassifier` is better than
0.6.1,applying a single random under-sampling. We will use a gradient-boosting
0.6.1,classifier within a `BalancedBaggingClassifier`.
0.6.1,##############################################################################
0.6.1,This last approach is the most effective. The different under-sampling allows
0.6.1,to bring some diversity for the different GBDT to learn and not focus on a
0.6.1,portion of the majority class.
0.6.1,
0.6.1,We will repeat the same experiment but with a ratio of 100:1 and make a
0.6.1,similar analysis.
0.6.1,##############################################################################
0.6.1,Increase imbalanced ratio
0.6.1,##############################################################################
0.6.1,##############################################################################
0.6.1,"When we analyse the results, we can draw similar conclusions than in the"
0.6.1,"previous discussion. However, we can observe that the strategy"
0.6.1,"`class_weight=""balanced""` does not improve the performance when using a"
0.6.1,`RandomForestClassifier`. A resampling is indeed required. The most effective
0.6.1,method remains the `BalancedBaggingClassifier` using a GBDT as a base
0.6.1,learner.
0.6.1,Authors: Christos Aridas
0.6.1,Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.6.1,License: MIT
0.6.1,Load the dataset
0.6.1,make nice plotting
0.6.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.6.1,License: MIT
0.6.1,Create a folder to fetch the dataset
0.6.1,Create a pipeline
0.6.1,Classify and report the results
0.6.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.6.1,License: MIT
0.6.1,##############################################################################
0.6.1,Setting the data set
0.6.1,##############################################################################
0.6.1,##############################################################################
0.6.1,We use a part of the 20 newsgroups data set by loading 4 topics. Using the
0.6.1,"scikit-learn loader, the data are split into a training and a testing set."
0.6.1,
0.6.1,Note the class \#3 is the minority class and has almost twice less samples
0.6.1,than the majority class.
0.6.1,##############################################################################
0.6.1,The usual scikit-learn pipeline
0.6.1,##############################################################################
0.6.1,##############################################################################
0.6.1,You might usually use scikit-learn pipeline by combining the TF-IDF
0.6.1,vectorizer to feed a multinomial naive bayes classifier. A classification
0.6.1,report summarized the results on the testing set.
0.6.1,
0.6.1,"As expected, the recall of the class \#3 is low mainly due to the class"
0.6.1,imbalanced.
0.6.1,##############################################################################
0.6.1,Balancing the class before classification
0.6.1,##############################################################################
0.6.1,##############################################################################
0.6.1,"To improve the prediction of the class \#3, it could be interesting to apply"
0.6.1,"a balancing before to train the naive bayes classifier. Therefore, we will"
0.6.1,use a ``RandomUnderSampler`` to equalize the number of samples in all the
0.6.1,classes before the training.
0.6.1,
0.6.1,It is also important to note that we are using the ``make_pipeline`` function
0.6.1,implemented in imbalanced-learn to properly handle the samplers.
0.6.1,##############################################################################
0.6.1,"Although the results are almost identical, it can be seen that the resampling"
0.6.1,allowed to correct the poor recall of the class \#3 at the cost of reducing
0.6.1,"the other metrics for the other classes. However, the overall results are"
0.6.1,slightly better.
0.6.1,Authors: Dayvid Oliveira
0.6.1,Christos Aridas
0.6.1,Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.6.1,License: MIT
0.6.1,Generate the dataset
0.6.1,"Two subplots, unpack the axes array immediately"
0.6.1,List of whitelisted modules and methods; regexp are supported.
0.6.1,These docstrings will fail because they are inheriting from scikit-learn
0.6.1,skip private classes
0.6.1,"We ignore following error code,"
0.6.1,- RT02: The first line of the Returns section
0.6.1,"should contain only the type, .."
0.6.1,(as we may need refer to the name of the returned
0.6.1,object)
0.6.1,- GL01: Docstring text (summary) should start in the line
0.6.1,"immediately after the opening quotes (not in the same line,"
0.6.1,or leaving a blank line in between)
0.6.1,Following codes are only taken into account for the
0.6.1,top level class docstrings:
0.6.1,- ES01: No extended summary found
0.6.1,- SA01: See Also section not found
0.6.1,- EX01: No examples section found
0.6.1,In particular we can't parse the signature of properties
0.6.1,"When applied to classes, detect class method. For functions"
0.6.1,method = None.
0.6.1,TODO: this detection can be improved. Currently we assume that we have
0.6.1,class # methods if the second path element before last is in camel case.
0.6.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.6.1,License: MIT
0.6.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.6.1,Christos Aridas
0.6.1,License: MIT
0.6.1,define an alias for back-compatibility
0.6.1,store information to build dataframe
0.6.1,store information to build a series
0.6.1,store the columns name to reconstruct a dataframe
0.6.1,Adapted from scikit-learn
0.6.1,Author: Edouard Duchesnay
0.6.1,Gael Varoquaux
0.6.1,Virgile Fritsch
0.6.1,Alexandre Gramfort
0.6.1,Lars Buitinck
0.6.1,Christos Aridas
0.6.1,Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.6.1,License: BSD
0.6.1,BaseEstimator interface
0.6.1,validate names
0.6.1,validate estimators
0.6.1,We allow last estimator to be None as an identity transformation
0.6.1,Estimator interface
0.6.1,Setup the memory
0.6.1,joblib >= 0.12
0.6.1,we do not clone when caching is disabled to
0.6.1,preserve backward compatibility
0.6.1,joblib <= 0.11
0.6.1,we do not clone when caching is disabled to
0.6.1,preserve backward compatibility
0.6.1,Fit or load from cache the current transfomer
0.6.1,Replace the transformer of the step with the fitted
0.6.1,transformer. This is necessary when loading the transformer
0.6.1,from the cache.
0.6.1,Based on NiLearn package
0.6.1,License: simplified BSD
0.6.1,"PEP0440 compatible formatted version, see:"
0.6.1,https://www.python.org/dev/peps/pep-0440/
0.6.1,
0.6.1,Generic release markers:
0.6.1,X.Y
0.6.1,X.Y.Z # For bugfix releases
0.6.1,
0.6.1,Admissible pre-release markers:
0.6.1,X.YaN # Alpha release
0.6.1,X.YbN # Beta release
0.6.1,X.YrcN # Release Candidate
0.6.1,X.Y # Final release
0.6.1,
0.6.1,Dev branch marker is: 'X.Y.dev' or 'X.Y.devN' where N is an integer.
0.6.1,'X.Y.dev0' is the canonical version of 'X.Y.dev'
0.6.1,
0.6.1,coding: utf-8
0.6.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.6.1,Dariusz Brzezinski
0.6.1,License: MIT
0.6.1,Only negative labels
0.6.1,"Calculate tp_sum, pred_sum, true_sum ###"
0.6.1,labels are now from 0 to len(labels) - 1 -> use bincount
0.6.1,Pathological case
0.6.1,Compute the true negative
0.6.1,Retain only selected labels
0.6.1,"Finally, we have all our sufficient statistics. Divide! #"
0.6.1,"Divide, and on zero-division, set scores to 0 and warn:"
0.6.1,"Oddly, we may get an ""invalid"" rather than a ""divide"" error"
0.6.1,here.
0.6.1,Average the results
0.6.1,labels are now from 0 to len(labels) - 1 -> use bincount
0.6.1,Pathological case
0.6.1,Retain only selected labels
0.6.1,old version of scipy return MaskedConstant instead of 0.0
0.6.1,Create the list of tags
0.6.1,check that the scoring function does not need a score
0.6.1,and only a prediction
0.6.1,Compute the score from the scoring function
0.6.1,Square if desired
0.6.1,Get the signature of the sens/spec function
0.6.1,We need to extract from kwargs only the one needed by the
0.6.1,specificity and specificity
0.6.1,Make the intersection between the parameters
0.6.1,Create a sub dictionary
0.6.1,Check if the metric is the geometric mean
0.6.1,We do not support multilabel so the only average supported
0.6.1,is binary
0.6.1,Create the list of parameters through signature binding
0.6.1,Call the sens/spec function
0.6.1,Compute the dominance
0.6.1,Compute the different metrics
0.6.1,Precision/recall/f1
0.6.1,Specificity
0.6.1,Geometric mean
0.6.1,Index balanced accuracy
0.6.1,compute averages
0.6.1,coding: utf-8
0.6.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.6.1,Christos Aridas
0.6.1,License: MIT
0.6.1,##############################################################################
0.6.1,Utilities for testing
0.6.1,import some data to play with
0.6.1,restrict to a binary classification task
0.6.1,add noisy features to make the problem harder and avoid perfect results
0.6.1,"run classifier, get class probabilities and label predictions"
0.6.1,only interested in probabilities of the positive case
0.6.1,XXX: do we really want a special API for the binary case?
0.6.1,##############################################################################
0.6.1,Tests
0.6.1,detailed measures for each class
0.6.1,individual scoring function that can be used for grid search: in the
0.6.1,binary class case the score is the value of the measure for the positive
0.6.1,class (e.g. label == 1). This is deprecated for average != 'binary'.
0.6.1,Such a case may occur with non-stratified cross-validation
0.6.1,ensure the above were meaningful tests:
0.6.1,Bad pos_label
0.6.1,Bad average option
0.6.1,but average != 'binary'; even if data is binary
0.6.1,compute the geometric mean for the binary problem
0.6.1,print classification report with class names
0.6.1,print classification report with label detection
0.6.1,print classification report with class names
0.6.1,print classification report with label detection
0.6.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.6.1,Christos Aridas
0.6.1,License: MIT
0.6.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.6.1,Christos Aridas
0.6.1,License: MIT
0.6.1,The ratio is computed using a one-vs-rest manner. Using majority
0.6.1,in multi-class would lead to slightly different results at the
0.6.1,cost of introducing a new parameter.
0.6.1,rounding may cause new amount for n_samples
0.6.1,the nearest neighbors need to be fitted only on the current class
0.6.1,to find the class NN to generate new samples
0.6.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.6.1,Christos Aridas
0.6.1,License: MIT
0.6.1,store information to build dataframe
0.6.1,store information to build a series
0.6.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.6.1,Christos Aridas
0.6.1,License: MIT
0.6.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.6.1,Fernando Nogueira
0.6.1,Christos Aridas
0.6.1,Dzianis Dudnik
0.6.1,License: MIT
0.6.1,np.newaxis for backwards compatability with random_state
0.6.1,Samples are in danger for m/2 <= m' < m
0.6.1,Samples are noise for m = m'
0.6.1,divergence between borderline-1 and borderline-2
0.6.1,Create synthetic samples for borderline points.
0.6.1,only minority
0.6.1,we use a one-vs-rest policy to handle the multiclass in which
0.6.1,new samples will be created considering not only the majority
0.6.1,class but all over classes.
0.6.1,@Substitution(
0.6.1,"sampling_strategy=BaseOverSampler._sampling_strategy_docstring,"
0.6.1,random_state=_random_state_docstring)
0.6.1,store information to build dataframe
0.6.1,store information to build a series
0.6.1,compute the median of the standard deviation of the minority class
0.6.1,the input of the OneHotEncoder needs to be dense
0.6.1,we can replace the 1 entries of the categorical features with the
0.6.1,median of the standard deviation. It will ensure that whenever
0.6.1,"distance is computed between 2 samples, the difference will be equal"
0.6.1,to the median of the standard deviation as in the original paper.
0.6.1,reverse the encoding of the categorical features
0.6.1,the matrix is supposed to be in the CSR format after the stacking
0.6.1,change in sparsity structure more efficient with LIL than CSR
0.6.1,convert to dense array since scipy.sparse doesn't handle 3D
0.6.1,tie breaking argmax
0.6.1,validate the parameters
0.6.1,negate diagonal elements
0.6.1,target_class_indices = np.flatnonzero(y == class_sample)
0.6.1,"X_class = _safe_indexing(X, target_class_indices)"
0.6.1,identify cluster which are answering the requirements
0.6.1,the cluster is already considered balanced
0.6.1,not enough samples to apply SMOTE
0.6.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.6.1,Christos Aridas
0.6.1,License: MIT
0.6.1,check that we can oversample even with missing or infinite data
0.6.1,regression tests for #605
0.6.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.6.1,Christos Aridas
0.6.1,Dzianis Dudnik
0.6.1,License: MIT
0.6.1,create 2 random continuous feature
0.6.1,create a categorical feature using some string
0.6.1,create a categorical feature using some integer
0.6.1,return the categories
0.6.1,create 2 random continuous feature
0.6.1,create a categorical feature using some string
0.6.1,create a categorical feature using some integer
0.6.1,return the categories
0.6.1,create 2 random continuous feature
0.6.1,create a categorical feature using some string
0.6.1,create a categorical feature using some integer
0.6.1,return the categories
0.6.1,create 2 random continuous feature
0.6.1,create a categorical feature using some string
0.6.1,create a categorical feature using some integer
0.6.1,return the categories
0.6.1,create 2 random continuous feature
0.6.1,create a categorical feature using some string
0.6.1,create a categorical feature using some integer
0.6.1,part of the common test which apply to SMOTE-NC even if it is not default
0.6.1,constructible
0.6.1,Check that the samplers handle pandas dataframe and pandas series
0.6.1,Cast X and y to not default dtype
0.6.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.6.1,Christos Aridas
0.6.1,License: MIT
0.6.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.6.1,Christos Aridas
0.6.1,License: MIT
0.6.1,check that m_neighbors is properly set. Regression test for:
0.6.1,https://github.com/scikit-learn-contrib/imbalanced-learn/issues/568
0.6.1,shuffle the indices since the sampler are packing them by class
0.6.1,helper functions
0.6.1,input and output
0.6.1,build the model and weights
0.6.1,"build the loss, predict, and train operator"
0.6.1,Initialization of all variables in the graph
0.6.1,"For each epoch, run accuracy on train and test"
0.6.1,helper functions
0.6.1,input and output
0.6.1,build the model and weights
0.6.1,"build the loss, predict, and train operator"
0.6.1,Initialization of all variables in the graph
0.6.1,"For each epoch, run accuracy on train and test"
0.6.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.6.1,License: MIT
0.6.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.6.1,Fernando Nogueira
0.6.1,Christos Aridas
0.6.1,License: MIT
0.6.1,find which class to not consider
0.6.1,there is a Tomek link between two samples if they are both nearest
0.6.1,neighbors of each others.
0.6.1,Find the nearest neighbour of every point
0.6.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.6.1,Christos Aridas
0.6.1,License: MIT
0.6.1,Randomly get one sample from the majority class
0.6.1,Generate the index to select
0.6.1,Create the set C - One majority samples and all minority
0.6.1,Create the set S - all majority samples
0.6.1,fit knn on C
0.6.1,Check each sample in S if we keep it or drop it
0.6.1,Do not select sample which are already well classified
0.6.1,Classify on S
0.6.1,If the prediction do not agree with the true label
0.6.1,append it in C_x
0.6.1,Keep the index for later
0.6.1,Update C
0.6.1,fit a knn on C
0.6.1,This experimental to speed up the search
0.6.1,Classify all the element in S and avoid to test the
0.6.1,well classified elements
0.6.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.6.1,Dayvid Oliveira
0.6.1,Christos Aridas
0.6.1,License: MIT
0.6.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.6.1,Christos Aridas
0.6.1,License: MIT
0.6.1,Compute the distance considering the farthest neighbour
0.6.1,Sort the list of distance and get the index
0.6.1,Throw a warning to tell the user that we did not have enough samples
0.6.1,to select and that we just select everything
0.6.1,Select the desired number of samples
0.6.1,idx_tmp is relative to the feature selected in the
0.6.1,previous step and we need to find the indirection
0.6.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.6.1,Christos Aridas
0.6.1,License: MIT
0.6.1,select a sample from the current class
0.6.1,create the set composed of all minority samples and one
0.6.1,sample from the current class.
0.6.1,create the set S with removing the seed from S
0.6.1,since that it will be added anyway
0.6.1,apply Tomek cleaning
0.6.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.6.1,Dayvid Oliveira
0.6.1,Christos Aridas
0.6.1,License: MIT
0.6.1,Check the stopping criterion
0.6.1,1. If there is no changes for the vector y
0.6.1,2. If the number of samples in the other class become inferior to
0.6.1,the number of samples in the majority class
0.6.1,3. If one of the class is disappearing
0.6.1,Case 1
0.6.1,Case 2
0.6.1,Case 3
0.6.1,Check the stopping criterion
0.6.1,1. If the number of samples in the other class become inferior to
0.6.1,the number of samples in the majority class
0.6.1,2. If one of the class is disappearing
0.6.1,Case 1else:
0.6.1,overwrite b_min_bec_maj
0.6.1,Case 2
0.6.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.6.1,Christos Aridas
0.6.1,License: MIT
0.6.1,clean the neighborhood
0.6.1,compute which classes to consider for cleaning for the A2 group
0.6.1,compute a2 group
0.6.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.6.1,Christos Aridas
0.6.1,License: MIT
0.6.1,store information to build dataframe
0.6.1,store information to build a series
0.6.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.6.1,Christos Aridas
0.6.1,License: MIT
0.6.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.6.1,Christos Aridas
0.6.1,License: MIT
0.6.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.6.1,Christos Aridas
0.6.1,License: MIT
0.6.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.6.1,Christos Aridas
0.6.1,License: MIT
0.6.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.6.1,Christos Aridas
0.6.1,License: MIT
0.6.1,check that we can undersample even with missing or infinite data
0.6.1,regression tests for #605
0.6.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.6.1,Christos Aridas
0.6.1,License: MIT
0.6.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.6.1,Christos Aridas
0.6.1,License: MIT
0.6.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.6.1,Christos Aridas
0.6.1,License: MIT
0.6.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.6.1,Christos Aridas
0.6.1,License: MIT
0.6.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.6.1,Christos Aridas
0.6.1,License: MIT
0.6.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.6.1,Fernando Nogueira
0.6.1,Christos Aridas
0.6.1,License: MIT
0.6.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.6.1,Christos Aridas
0.6.1,License: MIT
0.6.1,test that all_estimators doesn't find abstract classes.
0.6.1,don't run twice the sampler tests. Meta-estimator do not have a
0.6.1,fit_resample method.
0.6.1,input validation etc for non-meta estimators
0.6.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.6.1,License: MIT
0.6.1,check that we can let a pass a regression variable by turning down the
0.6.1,validation
0.6.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.6.1,Christos Aridas
0.6.1,License: MIT
0.6.1,store timestamp to figure out whether the result of 'fit' has been
0.6.1,cached or not
0.6.1,store timestamp to figure out whether the result of 'fit' has been
0.6.1,cached or not
0.6.1,Pipeline accepts steps as tuple
0.6.1,Test the various init parameters of the pipeline.
0.6.1,Check that we can't instantiate pipelines with objects without fit
0.6.1,method
0.6.1,Smoke test with only an estimator
0.6.1,Check that params are set
0.6.1,Smoke test the repr:
0.6.1,Test with two objects
0.6.1,Check that we can't instantiate with non-transformers on the way
0.6.1,"Note that NoTrans implements fit, but not transform"
0.6.1,Check that params are set
0.6.1,Smoke test the repr:
0.6.1,Check that params are not set when naming them wrong
0.6.1,Test clone
0.6.1,"Check that apart from estimators, the parameters are the same"
0.6.1,Remove estimators that where copied
0.6.1,Test the various methods of the pipeline (anova).
0.6.1,Test with Anova + LogisticRegression
0.6.1,Test that the pipeline can take fit parameters
0.6.1,classifier should return True
0.6.1,and transformer params should not be changed
0.6.1,invalid parameters should raise an error message
0.6.1,Pipeline should pass sample_weight
0.6.1,When sample_weight is None it shouldn't be passed
0.6.1,Test pipeline raises set params error message for nested models.
0.6.1,nested model check
0.6.1,Test the various methods of the pipeline (pca + svm).
0.6.1,Test with PCA + SVC
0.6.1,Test the various methods of the pipeline (preprocessing + svm).
0.6.1,check shapes of various prediction functions
0.6.1,test that the fit_predict method is implemented on a pipeline
0.6.1,test that the fit_predict on pipeline yields same results as applying
0.6.1,transform and clustering steps separately
0.6.1,"As pipeline doesn't clone estimators on construction,"
0.6.1,it must have its own estimators
0.6.1,first compute the transform and clustering step separately
0.6.1,use a pipeline to do the transform and clustering in one step
0.6.1,tests that a pipeline does not have fit_predict method when final
0.6.1,step of pipeline does not have fit_predict defined
0.6.1,tests that Pipeline passes fit_params to intermediate steps
0.6.1,when fit_predict is invoked
0.6.1,Test whether pipeline works with a transformer at the end.
0.6.1,Also test pipeline.transform and pipeline.inverse_transform
0.6.1,test transform and fit_transform:
0.6.1,Test whether pipeline works with a transformer missing fit_transform
0.6.1,test fit_transform:
0.6.1,Directly setting attr
0.6.1,Using set_params
0.6.1,Using set_params to replace single step
0.6.1,With invalid data
0.6.1,Test setting Pipeline steps to None
0.6.1,"for other methods, ensure no AttributeErrors on None:"
0.6.1,mult2 and mult3 are active
0.6.1,Check 'passthrough' step at construction time
0.6.1,Test that an error is raised when memory is not a string or a Memory
0.6.1,instance
0.6.1,Define memory as an integer
0.6.1,Test with Transformer + SVC
0.6.1,Memoize the transformer at the first fit
0.6.1,Get the time stamp of the tranformer in the cached pipeline
0.6.1,Check that cached_pipe and pipe yield identical results
0.6.1,Check that we are reading the cache while fitting
0.6.1,a second time
0.6.1,Check that cached_pipe and pipe yield identical results
0.6.1,Create a new pipeline with cloned estimators
0.6.1,Check that even changing the name step does not affect the cache hit
0.6.1,Check that cached_pipe and pipe yield identical results
0.6.1,Test with Transformer + SVC
0.6.1,Memoize the transformer at the first fit
0.6.1,Get the time stamp of the tranformer in the cached pipeline
0.6.1,Check that cached_pipe and pipe yield identical results
0.6.1,Check that we are reading the cache while fitting
0.6.1,a second time
0.6.1,Check that cached_pipe and pipe yield identical results
0.6.1,Create a new pipeline with cloned estimators
0.6.1,Check that even changing the name step does not affect the cache hit
0.6.1,Check that cached_pipe and pipe yield identical results
0.6.1,Test the various methods of the pipeline (pca + svm).
0.6.1,Test with PCA + SVC
0.6.1,Test the various methods of the pipeline (pca + svm).
0.6.1,Test with PCA + SVC
0.6.1,Test whether pipeline works with a sampler at the end.
0.6.1,Also test pipeline.sampler
0.6.1,test transform and fit_transform:
0.6.1,We round the value near to zero. It seems that PCA has some issue
0.6.1,with that
0.6.1,Test whether pipeline works with a sampler at the end.
0.6.1,Also test pipeline.sampler
0.6.1,Test pipeline using None as preprocessing step and a classifier
0.6.1,"Test pipeline using None, RUS and a classifier"
0.6.1,"Test pipeline using RUS, None and a classifier"
0.6.1,Test pipeline using None step and a sampler
0.6.1,Test pipeline using None and a transformer that implements transform and
0.6.1,inverse_transform
0.6.1,Test the various methods of the pipeline (anova).
0.6.1,Test with RandomUnderSampling + Anova + LogisticRegression
0.6.1,Test the various methods of the pipeline (anova).
0.6.1,Test the various methods of the pipeline (anova).
0.6.1,Test with RandomUnderSampling + Anova + LogisticRegression
0.6.1,tests that Pipeline passes predict_params to the final estimator
0.6.1,when predict is invoked
0.6.1,Test that the score_samples method is implemented on a pipeline.
0.6.1,Test that the score_samples method on pipeline yields same results as
0.6.1,applying transform and score_samples steps separately.
0.6.1,Check the shapes
0.6.1,Check the values
0.6.1,Test that a pipeline does not have score_samples method when the final
0.6.1,step of the pipeline does not have score_samples defined.
0.6.1,Test that the score_samples method is implemented on a pipeline.
0.6.1,Test that the score_samples method on pipeline yields same results as
0.6.1,applying transform and score_samples steps separately.
0.6.1,Check the shapes
0.6.1,Check the values
0.6.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.6.1,Christos Aridas
0.6.1,License: MIT
0.6.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.6.1,License: MIT
0.6.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.6.1,License: MIT
0.6.1,Adapated from scikit-learn
0.6.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.6.1,License: MIT
0.6.1,trigger our checks if this is a SamplerMixin
0.6.1,scikit-learn common tests
0.6.1,should raise warning if the target is continuous (we cannot raise error)
0.6.1,if the target is multilabel then we should raise an error
0.6.1,IHT does not enforce the number of samples but provide a number
0.6.1,of samples the closest to the desired target.
0.6.1,in this test we will force all samplers to not change the class 1
0.6.1,check that sparse matrices can be passed through the sampler leading to
0.6.1,the same results than dense
0.6.1,set KMeans to full since it support sparse and dense
0.6.1,Check that the samplers handle pandas dataframe and pandas series
0.6.1,check that we return a pandas dataframe if a dataframe was given in
0.6.1,Check that multiclass target lead to the same results than OVA encoding
0.6.1,Cast X and y to not default dtype
0.6.1,Adapted from scikit-learn
0.6.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.6.1,License: MIT
0.6.1,meta-estimators need another estimator to be instantiated.
0.6.1,estimators that there is no way to default-construct sensibly
0.6.1,some strange ones
0.6.1,get parent folder
0.6.1,get rid of abstract base classes
0.6.1,get rid of sklearn estimators which have been imported in some classes
0.6.1,possibly get rid of meta estimators
0.6.1,"drop duplicates, sort for reproducibility"
0.6.1,itemgetter is used to ensure the sort does not extend to the 2nd item of
0.6.1,the tuple
0.6.1,Author: Alexander L. Hayes <hayesall@iu.edu>
0.6.1,License: MIT
0.6.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.6.1,License: MIT
0.6.1,check that all keys in sampling_strategy are also in y
0.6.1,check that there is no negative number
0.6.1,check that all keys in sampling_strategy are also in y
0.6.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.6.1,Christos Aridas
0.6.1,License: MIT
0.6.1,this function could create an equal number of samples
0.6.1,We pass on purpose a non sorted dictionary and check that the resulting
0.6.1,dictionary is sorted. Refer to issue #428.
0.6.1,Author: Alexander L. Hayes <hayesall@iu.edu>
0.6.1,License: MIT
0.6.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.6.1,License: MIT
0.6.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.6.1,Christos Aridas
0.6.1,License: MIT
0.6.1,check if the filtering is working with a list or a single string
0.6.1,check that all estimators are sampler
0.6.1,check that an error is raised when the type is unknown
0.6.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.6.1,License: MIT
0.6.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.6.1,Christos Aridas
0.6.1,License: MIT
0.6.1,Otherwise create a default SMOTE
0.6.1,Otherwise create a default TomekLinks
0.6.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.6.1,Christos Aridas
0.6.1,License: MIT
0.6.1,Otherwise create a default SMOTE
0.6.1,Otherwise create a default EditedNearestNeighbours
0.6.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.6.1,Christos Aridas
0.6.1,License: MIT
0.6.1,Check if default job count is None
0.6.1,Check if job count is set
0.6.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.6.1,Christos Aridas
0.6.1,License: MIT
0.6.1,Check if default job count is none
0.6.1,Check if job count is set
0.6.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.6.1,License: MIT
0.6.1,resample before to fit the tree
0.6.1,Validate or convert input data
0.6.1,Pre-sort indices to avoid that each individual tree of the
0.6.1,ensemble sorts the indices.
0.6.1,Remap output
0.6.1,reshape is necessary to preserve the data contiguity against vs
0.6.1,"[:, np.newaxis] that does not."
0.6.1,Get bootstrap sample size
0.6.1,Check parameters
0.6.1,"Free allocated memory, if any"
0.6.1,We draw from the random state to get the random state we
0.6.1,would have got if we hadn't used a warm_start.
0.6.1,Parallel loop: we prefer the threading backend as the Cython code
0.6.1,for fitting the trees is internally releasing the Python GIL
0.6.1,making threading more efficient than multiprocessing in
0.6.1,"that case. However, we respect any parallel_backend contexts set"
0.6.1,"at a higher level, since correctness does not rely on using"
0.6.1,threads.
0.6.1,Collect newly grown trees
0.6.1,Create pipeline with the fitted samplers and trees
0.6.1,Decapsulate classes_ attributes
0.6.1,"with the resampling, we are likely to have rows not included"
0.6.1,for the OOB score leading to division by zero
0.6.1,Instances incorrectly classified
0.6.1,Error fraction
0.6.1,Stop if classification is perfect
0.6.1,Construct y coding as described in Zhu et al [2]:
0.6.1,
0.6.1,y_k = 1 if c == k else -1 / (K - 1)
0.6.1,
0.6.1,"where K == n_classes_ and c, k in [0, K) are indices along the second"
0.6.1,axis of the y coding with c being the index corresponding to the true
0.6.1,class label.
0.6.1,Displace zero probabilities so the log is defined.
0.6.1,Also fix negative elements which may occur with
0.6.1,negative sample weights.
0.6.1,Boost weight using multi-class AdaBoost SAMME.R alg
0.6.1,Only boost the weights if it will fit again
0.6.1,Only boost positive weights
0.6.1,Instances incorrectly classified
0.6.1,Error fraction
0.6.1,Stop if classification is perfect
0.6.1,Stop if the error is at least as bad as random guessing
0.6.1,Boost weight using multi-class AdaBoost SAMME alg
0.6.1,Only boost the weights if I will fit again
0.6.1,Only boost positive weights
0.6.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.6.1,Christos Aridas
0.6.1,License: MIT
0.6.1,RandomUnderSampler is not supporting sample_weight. We need to pass
0.6.1,None.
0.6.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.6.1,Christos Aridas
0.6.1,License: MIT
0.6.1,RandomUnderSampler is not supporting sample_weight. We need to pass
0.6.1,None.
0.6.1,check that we have an ensemble of samplers and estimators with a
0.6.1,consistent size
0.6.1,each sampler in the ensemble should have different random state
0.6.1,each estimator in the ensemble should have different random state
0.6.1,check the consistency of the feature importances
0.6.1,check the consistency of the prediction outpus
0.6.1,Predictions should be the same when sample_weight are all ones
0.6.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.6.1,Christos Aridas
0.6.1,License: MIT
0.6.1,Check classification for various parameter settings.
0.6.1,Test that bootstrapping samples generate non-perfect base estimators.
0.6.1,"without bootstrap, all trees are perfect on the training set"
0.6.1,disable the resampling by passing an empty dictionary.
0.6.1,"with bootstrap, trees are no longer perfect on the training set"
0.6.1,Test that bootstrapping features may generate duplicate features.
0.6.1,Predict probabilities.
0.6.1,Normal case
0.6.1,"Degenerate case, where some classes are missing"
0.6.1,Check that oob prediction is a good estimation of the generalization
0.6.1,error.
0.6.1,Test with few estimators
0.6.1,Check singleton ensembles.
0.6.1,Test that it gives proper exception on deficient input.
0.6.1,Test n_estimators
0.6.1,Test max_samples
0.6.1,Test max_features
0.6.1,Test support of decision_function
0.6.1,Check that bagging ensembles can be grid-searched.
0.6.1,Transform iris into a binary classification task
0.6.1,Grid search with scoring based on decision_function
0.6.1,Check base_estimator and its default values.
0.6.1,Test if fitting incrementally with warm start gives a forest of the
0.6.1,right size and the same results as a normal fit.
0.6.1,Test if warm start'ed second fit with smaller n_estimators raises error.
0.6.1,Test that nothing happens when fitting without increasing n_estimators
0.6.1,"modify X to nonsense values, this should not change anything"
0.6.1,warm started classifier with 5+5 estimators should be equivalent to
0.6.1,one classifier with 10 estimators
0.6.1,Check using oob_score and warm_start simultaneously fails
0.6.1,"Make sure OOB scores are identical when random_state, estimator, and"
0.6.1,training data are fixed and fitting is done twice
0.6.1,Check that format of estimators_samples_ is correct and that results
0.6.1,generated at fit time can be identically reproduced at a later time
0.6.1,using data saved in object attributes.
0.6.1,remap the y outside of the BalancedBaggingclassifier
0.6.1,"_, y = np.unique(y, return_inverse=True)"
0.6.1,Get relevant attributes
0.6.1,Test for correct formatting
0.6.1,Re-fit single estimator to test for consistent sampling
0.6.1,Make sure validated max_samples and original max_samples are identical
0.6.1,when valid integer max_samples supplied by user
0.6.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.6.1,Christos Aridas
0.6.1,License: MIT
0.6.1,Generate a global dataset to use
0.6.1,Check classification for various parameter settings.
0.6.1,test the different prediction function
0.6.1,Check base_estimator and its default values.
0.6.1,Test if fitting incrementally with warm start gives a forest of the
0.6.1,right size and the same results as a normal fit.
0.6.1,Test if warm start'ed second fit with smaller n_estimators raises error.
0.6.1,Test that nothing happens when fitting without increasing n_estimators
0.6.1,"modify X to nonsense values, this should not change anything"
0.6.1,warm started classifier with 5+5 estimators should be equivalent to
0.6.1,one classifier with 10 estimators
0.6.1,Check warning if not enough estimators
0.6.1,First fit with no restriction on max samples
0.6.1,Second fit with max samples restricted to just 2
0.6.1,Regression test for #655: check that the oob score is closed to 0.5
0.6.1,a binomial experiment.
0.6.1,Author: Guillaume Lemaitre
0.6.1,License: BSD 3 clause
0.6.1,"The index start at one, then we need to remove one"
0.6.1,to not have issue with the indexing.
0.6.1,go through the list and check if the data are available
0.6.1,Authors: Dayvid Oliveira
0.6.1,Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.6.1,Christos Aridas
0.6.1,License: MIT
0.6.1,restrict ratio to be a dict or a callable
0.6.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.6.1,Christos Aridas
0.6.1,License: MIT
0.6.1,"we are reusing part of utils.check_sampling_strategy, however this is not"
0.6.1,cover in the common tests so we will repeat it here
0.6.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.6.1,Christos Aridas
0.6.1,License: MIT
0.6.1,This is a trick to avoid an error during tests collection with pytest. We
0.6.1,avoid the error when importing the package raise the error at the moment of
0.6.1,creating the instance.
0.6.1,flag for keras sequence duck-typing
0.6.1,shuffle the indices since the sampler are packing them by class
0.6.0,This file is here so that when running from the root folder
0.6.0,./sklearn is added to sys.path by pytest.
0.6.0,See https://docs.pytest.org/en/latest/pythonpath.html for more details.
0.6.0,"For example, this allows to build extensions in place and run pytest"
0.6.0,doc/modules/clustering.rst and use sklearn from the local folder
0.6.0,rather than the one from site-packages.
0.6.0,Set numpy array str/repr to legacy behaviour on numpy > 1.13 to make
0.6.0,the doctests pass
0.6.0,! /usr/bin/env python
0.6.0,get __version__ from _version.py
0.6.0,-*- coding: utf-8 -*-
0.6.0,
0.6.0,"imbalanced-learn documentation build configuration file, created by"
0.6.0,sphinx-quickstart on Mon Jan 18 14:44:12 2016.
0.6.0,
0.6.0,This file is execfile()d with the current directory set to its
0.6.0,containing dir.
0.6.0,
0.6.0,Note that not all possible configuration values are present in this
0.6.0,autogenerated file.
0.6.0,
0.6.0,All configuration values have a default; values that are commented out
0.6.0,serve to show the default.
0.6.0,"If extensions (or modules to document with autodoc) are in another directory,"
0.6.0,add these directories to sys.path here. If the directory is relative to the
0.6.0,"documentation root, use os.path.abspath to make it absolute, like shown here."
0.6.0,-- General configuration ------------------------------------------------
0.6.0,"If your documentation needs a minimal Sphinx version, state it here."
0.6.0,needs_sphinx = '1.0'
0.6.0,"Add any Sphinx extension module names here, as strings. They can be"
0.6.0,extensions coming with Sphinx (named 'sphinx.ext.*') or your custom
0.6.0,ones.
0.6.0,this is needed for some reason...
0.6.0,see https://github.com/numpy/numpydoc/issues/69
0.6.0,"Add any paths that contain templates here, relative to this directory."
0.6.0,generate autosummary even if no references
0.6.0,The suffix of source filenames.
0.6.0,The encoding of source files.
0.6.0,source_encoding = 'utf-8-sig'
0.6.0,Generate the plot for the gallery
0.6.0,The master toctree document.
0.6.0,General information about the project.
0.6.0,"The version info for the project you're documenting, acts as replacement for"
0.6.0,"|version| and |release|, also used in various other places throughout the"
0.6.0,built documents.
0.6.0,
0.6.0,The short X.Y version.
0.6.0,"The full version, including alpha/beta/rc tags."
0.6.0,The language for content autogenerated by Sphinx. Refer to documentation
0.6.0,for a list of supported languages.
0.6.0,language = None
0.6.0,"There are two options for replacing |today|: either, you set today to some"
0.6.0,"non-false value, then it is used:"
0.6.0,today = ''
0.6.0,"Else, today_fmt is used as the format for a strftime call."
0.6.0,"today_fmt = '%B %d, %Y'"
0.6.0,"List of patterns, relative to source directory, that match files and"
0.6.0,directories to ignore when looking for source files.
0.6.0,The reST default role (used for this markup: `text`) to use for all
0.6.0,documents.
0.6.0,default_role = None
0.6.0,"If true, '()' will be appended to :func: etc. cross-reference text."
0.6.0,"If true, the current module name will be prepended to all description"
0.6.0,unit titles (such as .. function::).
0.6.0,add_module_names = True
0.6.0,"If true, sectionauthor and moduleauthor directives will be shown in the"
0.6.0,output. They are ignored by default.
0.6.0,show_authors = False
0.6.0,The name of the Pygments (syntax highlighting) style to use.
0.6.0,Custom style
0.6.0,A list of ignored prefixes for module index sorting.
0.6.0,modindex_common_prefix = []
0.6.0,"If true, keep warnings as ""system message"" paragraphs in the built documents."
0.6.0,keep_warnings = False
0.6.0,-- Options for HTML output ----------------------------------------------
0.6.0,The theme to use for HTML and HTML Help pages.  See the documentation for
0.6.0,a list of builtin themes.
0.6.0,Theme options are theme-specific and customize the look and feel of a theme
0.6.0,"further.  For a list of options available for each theme, see the"
0.6.0,documentation.
0.6.0,html_theme_options = {}
0.6.0,"Add any paths that contain custom themes here, relative to this directory."
0.6.0,"The name for this set of Sphinx documents.  If None, it defaults to"
0.6.0,"""<project> v<release> documentation""."
0.6.0,html_title = None
0.6.0,A shorter title for the navigation bar.  Default is the same as html_title.
0.6.0,html_short_title = None
0.6.0,The name of an image file (relative to this directory) to place at the top
0.6.0,of the sidebar.
0.6.0,html_logo = None
0.6.0,The name of an image file (within the static path) to use as favicon of the
0.6.0,docs.  This file should be a Windows icon file (.ico) being 16x16 or 32x32
0.6.0,pixels large.
0.6.0,html_favicon = None
0.6.0,"Add any paths that contain custom static files (such as style sheets) here,"
0.6.0,"relative to this directory. They are copied after the builtin static files,"
0.6.0,"so a file named ""default.css"" will overwrite the builtin ""default.css""."
0.6.0,Add any extra paths that contain custom files (such as robots.txt or
0.6.0,".htaccess) here, relative to this directory. These files are copied"
0.6.0,directly to the root of the documentation.
0.6.0,html_extra_path = []
0.6.0,"If not '', a 'Last updated on:' timestamp is inserted at every page bottom,"
0.6.0,using the given strftime format.
0.6.0,"html_last_updated_fmt = '%b %d, %Y'"
0.6.0,"If true, SmartyPants will be used to convert quotes and dashes to"
0.6.0,typographically correct entities.
0.6.0,html_use_smartypants = True
0.6.0,"Custom sidebar templates, maps document names to template names."
0.6.0,html_sidebars = {}
0.6.0,"Additional templates that should be rendered to pages, maps page names to"
0.6.0,template names.
0.6.0,html_additional_pages = {}
0.6.0,"If false, no module index is generated."
0.6.0,html_domain_indices = True
0.6.0,"If false, no index is generated."
0.6.0,html_use_index = True
0.6.0,"If true, the index is split into individual pages for each letter."
0.6.0,html_split_index = False
0.6.0,"If true, links to the reST sources are added to the pages."
0.6.0,html_show_sourcelink = True
0.6.0,"If true, ""Created using Sphinx"" is shown in the HTML footer. Default is True."
0.6.0,html_show_sphinx = True
0.6.0,"If true, ""(C) Copyright ..."" is shown in the HTML footer. Default is True."
0.6.0,html_show_copyright = True
0.6.0,"If true, an OpenSearch description file will be output, and all pages will"
0.6.0,contain a <link> tag referring to it.  The value of this option must be the
0.6.0,base URL from which the finished HTML is served.
0.6.0,html_use_opensearch = ''
0.6.0,"This is the file name suffix for HTML files (e.g. "".xhtml"")."
0.6.0,html_file_suffix = None
0.6.0,Output file base name for HTML help builder.
0.6.0,-- Options for LaTeX output ---------------------------------------------
0.6.0,The paper size ('letterpaper' or 'a4paper').
0.6.0,"'papersize': 'letterpaper',"
0.6.0,"The font size ('10pt', '11pt' or '12pt')."
0.6.0,"'pointsize': '10pt',"
0.6.0,Additional stuff for the LaTeX preamble.
0.6.0,"'preamble': '',"
0.6.0,Grouping the document tree into LaTeX files. List of tuples
0.6.0,"(source start file, target name, title,"
0.6.0,"author, documentclass [howto, manual, or own class])."
0.6.0,The name of an image file (relative to this directory) to place at the top of
0.6.0,the title page.
0.6.0,latex_logo = None
0.6.0,"For ""manual"" documents, if this is true, then toplevel headings are parts,"
0.6.0,not chapters.
0.6.0,latex_use_parts = False
0.6.0,"If true, show page references after internal links."
0.6.0,latex_show_pagerefs = False
0.6.0,"If true, show URL addresses after external links."
0.6.0,latex_show_urls = False
0.6.0,Documents to append as an appendix to all manuals.
0.6.0,latex_appendices = []
0.6.0,intersphinx configuration
0.6.0,sphinx-gallery configuration
0.6.0,-- Options for manual page output ---------------------------------------
0.6.0,"If false, no module index is generated."
0.6.0,latex_domain_indices = True
0.6.0,One entry per manual page. List of tuples
0.6.0,"(source start file, name, description, authors, manual section)."
0.6.0,"If true, show URL addresses after external links."
0.6.0,man_show_urls = False
0.6.0,-- Options for Texinfo output -------------------------------------------
0.6.0,Grouping the document tree into Texinfo files. List of tuples
0.6.0,"(source start file, target name, title, author,"
0.6.0,"dir menu entry, description, category)"
0.6.0,"def generate_example_rst(app, what, name, obj, options, lines):"
0.6.0,"# generate empty examples files, so that we don't get"
0.6.0,# inclusion errors if there are no examples for a class / module
0.6.0,"examples_path = os.path.join(app.srcdir, ""generated"","
0.6.0,"""%s.examples"" % name)"
0.6.0,if not os.path.exists(examples_path):
0.6.0,# touch file
0.6.0,"open(examples_path, 'w').close()"
0.6.0,Config for sphinx_issues
0.6.0,Temporary work-around for spacing problem between parameter and parameter
0.6.0,"type in the doc, see https://github.com/numpy/numpydoc/issues/215. The bug"
0.6.0,has been fixed in sphinx (https://github.com/sphinx-doc/sphinx/pull/5976) but
0.6.0,through a change in sphinx basic.css except rtd_theme does not use basic.css.
0.6.0,"In an ideal world, this would get fixed in this PR:"
0.6.0,https://github.com/readthedocs/sphinx_rtd_theme/pull/747/files
0.6.0,"app.connect('autodoc-process-docstring', generate_example_rst)"
0.6.0,Documents to append as an appendix to all manuals.
0.6.0,texinfo_appendices = []
0.6.0,"If false, no module index is generated."
0.6.0,texinfo_domain_indices = True
0.6.0,"How to display URL addresses: 'footnote', 'no', or 'inline'."
0.6.0,texinfo_show_urls = 'footnote'
0.6.0,"If true, do not generate a @detailmenu in the ""Top"" node's menu."
0.6.0,texinfo_no_detailmenu = False
0.6.0,The following is used by sphinx.ext.linkcode to provide links to github
0.6.0,get the styles from the current theme
0.6.0,create and add the button to all the code blocks that contain >>>
0.6.0,tracebacks (.gt) contain bare text elements that need to be
0.6.0,wrapped in a span to work with .nextUntil() (see later)
0.6.0,define the behavior of the button when it's clicked
0.6.0,hide the code output
0.6.0,show the code output
0.6.0,-*- coding: utf-8 -*-
0.6.0,Format template for issues URI
0.6.0,e.g. 'https://github.com/sloria/marshmallow/issues/{issue}
0.6.0,Format template for PR URI
0.6.0,e.g. 'https://github.com/sloria/marshmallow/pull/{issue}
0.6.0,Format template for commit URI
0.6.0,e.g. 'https://github.com/sloria/marshmallow/commits/{commit}
0.6.0,"Shortcut for Github, e.g. 'sloria/marshmallow'"
0.6.0,Format template for user profile URI
0.6.0,e.g. 'https://github.com/{user}'
0.6.0,Python 2 only
0.6.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.6.0,License: MIT
0.6.0,##############################################################################
0.6.0,"First, we will create an imbalanced data set from a the iris data set."
0.6.0,##############################################################################
0.6.0,Using ``sampling_strategy`` in resampling algorithms
0.6.0,##############################################################################
0.6.0,##############################################################################
0.6.0,``sampling_strategy`` as a ``float``
0.6.0,....................................
0.6.0,
0.6.0,``sampling_strategy`` can be given a ``float``. For **under-sampling
0.6.0,"methods**, it corresponds to the ratio :math:`\\alpha_{us}` defined by"
0.6.0,:math:`N_{rM} = \\alpha_{us} \\times N_{m}` where :math:`N_{rM}` and
0.6.0,:math:`N_{m}` are the number of samples in the majority class after
0.6.0,"resampling and the number of samples in the minority class, respectively."
0.6.0,select only 2 classes since the ratio make sense in this case
0.6.0,##############################################################################
0.6.0,"For **over-sampling methods**, it correspond to the ratio"
0.6.0,:math:`\\alpha_{os}` defined by :math:`N_{rm} = \\alpha_{os} \\times N_{M}`
0.6.0,where :math:`N_{rm}` and :math:`N_{M}` are the number of samples in the
0.6.0,minority class after resampling and the number of samples in the majority
0.6.0,"class, respectively."
0.6.0,##############################################################################
0.6.0,``sampling_strategy`` has a ``str``
0.6.0,...................................
0.6.0,
0.6.0,``sampling_strategy`` can be given as a string which specify the class
0.6.0,"targeted by the resampling. With under- and over-sampling, the number of"
0.6.0,samples will be equalized.
0.6.0,
0.6.0,Note that we are using multiple classes from now on.
0.6.0,##############################################################################
0.6.0,"With **cleaning method**, the number of samples in each class will not be"
0.6.0,equalized even if targeted.
0.6.0,##############################################################################
0.6.0,``sampling_strategy`` as a ``dict``
0.6.0,...................................
0.6.0,
0.6.0,"When ``sampling_strategy`` is a ``dict``, the keys correspond to the targeted"
0.6.0,classes. The values correspond to the desired number of samples for each
0.6.0,targeted class. This is working for both **under- and over-sampling**
0.6.0,algorithms but not for the **cleaning algorithms**. Use a ``list`` instead.
0.6.0,##############################################################################
0.6.0,``sampling_strategy`` as a ``list``
0.6.0,...................................
0.6.0,
0.6.0,"When ``sampling_strategy`` is a ``list``, the list contains the targeted"
0.6.0,classes. It is used only for **cleaning methods** and raise an error
0.6.0,otherwise.
0.6.0,##############################################################################
0.6.0,``sampling_strategy`` as a callable
0.6.0,...................................
0.6.0,
0.6.0,"When callable, function taking ``y`` and returns a ``dict``. The keys"
0.6.0,correspond to the targeted classes. The values correspond to the desired
0.6.0,number of samples for each class.
0.6.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.6.0,License: MIT
0.6.0,#############################################################################
0.6.0,Toy data generation
0.6.0,#############################################################################
0.6.0,#############################################################################
0.6.0,We are generating some non Gaussian data set contaminated with some unform
0.6.0,noise.
0.6.0,#############################################################################
0.6.0,We will generate some cleaned test data without outliers.
0.6.0,#############################################################################
0.6.0,How to use the :class:`imblearn.FunctionSampler`
0.6.0,#############################################################################
0.6.0,#############################################################################
0.6.0,We first define a function which will use
0.6.0,:class:`sklearn.ensemble.IsolationForest` to eliminate some outliers from
0.6.0,our dataset during training. The function passed to the
0.6.0,:class:`imblearn.FunctionSampler` will be called when using the method
0.6.0,``fit_resample``.
0.6.0,#############################################################################
0.6.0,Integrate it within a pipeline
0.6.0,#############################################################################
0.6.0,#############################################################################
0.6.0,"By elimnating outliers before the training, the classifier will be less"
0.6.0,affected during the prediction.
0.6.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.6.0,License: MIT
0.6.0,generate some data points
0.6.0,plot the majority and minority samples
0.6.0,draw the circle in which the new sample will generated
0.6.0,plot the line on which the sample will be generated
0.6.0,create and plot the new sample
0.6.0,make the plot nicer with legend and label
0.6.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.6.0,License: MIT
0.6.0,##############################################################################
0.6.0,The following function will be used to create toy dataset. It using the
0.6.0,``make_classification`` from scikit-learn but fixing some parameters.
0.6.0,##############################################################################
0.6.0,The following function will be used to plot the sample space after resampling
0.6.0,to illustrate the characterisitic of an algorithm.
0.6.0,make nice plotting
0.6.0,##############################################################################
0.6.0,The following function will be used to plot the decision function of a
0.6.0,classifier given some data.
0.6.0,##############################################################################
0.6.0,Illustration of the influence of the balancing ratio
0.6.0,##############################################################################
0.6.0,##############################################################################
0.6.0,We will first illustrate the influence of the balancing ratio on some toy
0.6.0,data using a linear SVM classifier. Greater is the difference between the
0.6.0,"number of samples in each class, poorer are the classfication results."
0.6.0,##############################################################################
0.6.0,Random over-sampling to balance the data set
0.6.0,##############################################################################
0.6.0,##############################################################################
0.6.0,Random over-sampling can be used to repeat some samples and balance the
0.6.0,number of samples between the dataset. It can be seen that with this trivial
0.6.0,approach the boundary decision is already less biaised toward the majority
0.6.0,class.
0.6.0,##############################################################################
0.6.0,More advanced over-sampling using ADASYN and SMOTE
0.6.0,##############################################################################
0.6.0,##############################################################################
0.6.0,"Instead of repeating the same samples when over-sampling, we can use some"
0.6.0,specific heuristic instead. ADASYN and SMOTE can be used in this case.
0.6.0,Make an identity sampler
0.6.0,##############################################################################
0.6.0,The following plot illustrate the difference between ADASYN and SMOTE. ADASYN
0.6.0,will focus on the samples which are difficult to classify with a
0.6.0,nearest-neighbors rule while regular SMOTE will not make any distinction.
0.6.0,"Therefore, the decision function depending of the algorithm."
0.6.0,##############################################################################
0.6.0,"Due to those sampling particularities, it can give rise to some specific"
0.6.0,issues as illustrated below.
0.6.0,##############################################################################
0.6.0,SMOTE proposes several variants by identifying specific samples to consider
0.6.0,during the resampling. The borderline version will detect which point to
0.6.0,select which are in the border between two classes. The SVM version will use
0.6.0,the support vectors found using an SVM algorithm to create new sample while
0.6.0,the KMeans version will make a clustering before to generate samples in each
0.6.0,cluster independently depending each cluster density.
0.6.0,##############################################################################
0.6.0,"When dealing with a mixed of continuous and categorical features, SMOTE-NC"
0.6.0,is the only method which can handle this case.
0.6.0,create a synthetic data set with continuous and categorical features
0.6.0,Authors: Christos Aridas
0.6.0,Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.6.0,License: MIT
0.6.0,Generate the dataset
0.6.0,make nice plotting
0.6.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.6.0,License: MIT
0.6.0,Generate a dataset
0.6.0,Split the data
0.6.0,Train the classifier with balancing
0.6.0,Test the classifier and get the prediction
0.6.0,Show the classification report
0.6.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.6.0,License: MIT
0.6.0,Generate a dataset
0.6.0,Split the data
0.6.0,Train the classifier with balancing
0.6.0,Test the classifier and get the prediction
0.6.0,##############################################################################
0.6.0,The geometric mean corresponds to the square root of the product of the
0.6.0,sensitivity and specificity. Combining the two metrics should account for
0.6.0,the balancing of the dataset.
0.6.0,##############################################################################
0.6.0,The index balanced accuracy can transform any metric to be used in
0.6.0,imbalanced learning problems.
0.6.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.6.0,License: MIT
0.6.0,##############################################################################
0.6.0,The following function will be used to create toy dataset. It using the
0.6.0,``make_classification`` from scikit-learn but fixing some parameters.
0.6.0,##############################################################################
0.6.0,The following function will be used to plot the sample space after resampling
0.6.0,to illustrate the characteristic of an algorithm.
0.6.0,make nice plotting
0.6.0,##############################################################################
0.6.0,The following function will be used to plot the decision function of a
0.6.0,classifier given some data.
0.6.0,##############################################################################
0.6.0,"``SMOTE`` allows to generate samples. However, this method of over-sampling"
0.6.0,"does not have any knowledge regarding the underlying distribution. Therefore,"
0.6.0,"some noisy samples can be generated, e.g. when the different classes cannot"
0.6.0,"be well separated. Hence, it can be beneficial to apply an under-sampling"
0.6.0,algorithm to clean the noisy samples. Two methods are usually used in the
0.6.0,literature: (i) Tomek's link and (ii) edited nearest neighbours cleaning
0.6.0,methods. Imbalanced-learn provides two ready-to-use samplers ``SMOTETomek``
0.6.0,"and ``SMOTEENN``. In general, ``SMOTEENN`` cleans more noisy data than"
0.6.0,``SMOTETomek``.
0.6.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.6.0,License: MIT
0.6.0,##############################################################################
0.6.0,Load an imbalanced dataset
0.6.0,##############################################################################
0.6.0,We will load the UCI SatImage dataset which has an imbalanced ratio of 9.3:1
0.6.0,(number of majority sample for a minority sample). The data are then split
0.6.0,into training and testing.
0.6.0,##############################################################################
0.6.0,Classification using a single decision tree
0.6.0,##############################################################################
0.6.0,We train a decision tree classifier which will be used as a baseline for the
0.6.0,rest of this example.
0.6.0,##############################################################################
0.6.0,The results are reported in terms of balanced accuracy and geometric mean
0.6.0,which are metrics widely used in the literature to validate model trained on
0.6.0,imbalanced set.
0.6.0,##############################################################################
0.6.0,Classification using bagging classifier with and without sampling
0.6.0,##############################################################################
0.6.0,"Instead of using a single tree, we will check if an ensemble of decsion tree"
0.6.0,"can actually alleviate the issue induced by the class imbalancing. First, we"
0.6.0,will use a bagging classifier and its counter part which internally uses a
0.6.0,random under-sampling to balanced each boostrap sample.
0.6.0,##############################################################################
0.6.0,Balancing each bootstrap sample allows to increase significantly the balanced
0.6.0,accuracy and the geometric mean.
0.6.0,##############################################################################
0.6.0,Classification using random forest classifier with and without sampling
0.6.0,##############################################################################
0.6.0,Random forest is another popular ensemble method and it is usually
0.6.0,"outperforming bagging. Here, we used a vanilla random forest and its balanced"
0.6.0,counterpart in which each bootstrap sample is balanced.
0.6.0,"Similarly to the previous experiment, the balanced classifier outperform the"
0.6.0,"classifier which learn from imbalanced bootstrap samples. In addition, random"
0.6.0,forest outsperforms the bagging classifier.
0.6.0,##############################################################################
0.6.0,Boosting classifier
0.6.0,##############################################################################
0.6.0,"In the same manner, easy ensemble classifier is a bag of balanced AdaBoost"
0.6.0,"classifier. However, it will be slower to train than random forest and will"
0.6.0,achieve worse performance.
0.6.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.6.0,License: MIT
0.6.0,##############################################################################
0.6.0,The following function will be used to create toy dataset. It using the
0.6.0,``make_classification`` from scikit-learn but fixing some parameters.
0.6.0,##############################################################################
0.6.0,The following function will be used to plot the sample space after resampling
0.6.0,to illustrate the characteristic of an algorithm.
0.6.0,make nice plotting
0.6.0,##############################################################################
0.6.0,The following function will be used to plot the decision function of a
0.6.0,classifier given some data.
0.6.0,##############################################################################
0.6.0,Prototype generation: under-sampling by generating new samples
0.6.0,##############################################################################
0.6.0,##############################################################################
0.6.0,``ClusterCentroids`` under-samples by replacing the original samples by the
0.6.0,centroids of the cluster found.
0.6.0,##############################################################################
0.6.0,Prototype selection: under-sampling by selecting existing samples
0.6.0,##############################################################################
0.6.0,##############################################################################
0.6.0,The algorithm performing prototype selection can be subdivided into two
0.6.0,groups: (i) the controlled under-sampling methods and (ii) the cleaning
0.6.0,under-sampling methods.
0.6.0,##############################################################################
0.6.0,"With the controlled under-sampling methods, the number of samples to be"
0.6.0,selected can be specified. ``RandomUnderSampler`` is the most naive way of
0.6.0,performing such selection by randomly selecting a given number of samples by
0.6.0,the targetted class.
0.6.0,##############################################################################
0.6.0,``NearMiss`` algorithms implement some heuristic rules in order to select
0.6.0,samples. NearMiss-1 selects samples from the majority class for which the
0.6.0,average distance of the :math:`k`` nearest samples of the minority class is
0.6.0,the smallest. NearMiss-2 selects the samples from the majority class for
0.6.0,which the average distance to the farthest samples of the negative class is
0.6.0,"the smallest. NearMiss-3 is a 2-step algorithm: first, for each minority"
0.6.0,"sample, their ::math:`m` nearest-neighbors will be kept; then, the majority"
0.6.0,samples selected are the on for which the average distance to the :math:`k`
0.6.0,nearest neighbors is the largest.
0.6.0,##############################################################################
0.6.0,``EditedNearestNeighbours`` removes samples of the majority class for which
0.6.0,their class differ from the one of their nearest-neighbors. This sieve can be
0.6.0,repeated which is the principle of the
0.6.0,``RepeatedEditedNearestNeighbours``. ``AllKNN`` is slightly different from
0.6.0,the ``RepeatedEditedNearestNeighbours`` by changing the :math:`k` parameter
0.6.0,"of the internal nearest neighors algorithm, increasing it at each iteration."
0.6.0,##############################################################################
0.6.0,``CondensedNearestNeighbour`` makes use of a 1-NN to iteratively decide if a
0.6.0,sample should be kept in a dataset or not. The issue is that
0.6.0,``CondensedNearestNeighbour`` is sensitive to noise by preserving the noisy
0.6.0,samples. ``OneSidedSelection`` also used the 1-NN and use ``TomekLinks`` to
0.6.0,remove the samples considered noisy. The ``NeighbourhoodCleaningRule`` use a
0.6.0,"``EditedNearestNeighbours`` to remove some sample. Additionally, they use a 3"
0.6.0,nearest-neighbors to remove samples which do not agree with this rule.
0.6.0,##############################################################################
0.6.0,``InstanceHardnessThreshold`` uses the prediction of classifier to exclude
0.6.0,samples. All samples which are classified with a low probability will be
0.6.0,removed.
0.6.0,##############################################################################
0.6.0,This function allows to make nice plotting
0.6.0,##############################################################################
0.6.0,Generate some data with one Tomek link
0.6.0,minority class
0.6.0,majority class
0.6.0,##############################################################################
0.6.0,"In the figure above, the samples highlighted in green form a Tomek link since"
0.6.0,they are of different classes and are nearest neighbours of each other.
0.6.0,highlight the samples of interest
0.6.0,##############################################################################
0.6.0,We can run the ``TomekLinks`` sampling to remove the corresponding
0.6.0,samples. If ``sampling_strategy='auto'`` only the sample from the majority
0.6.0,class will be removed. If ``sampling_strategy='all'`` both samples will be
0.6.0,removed.
0.6.0,highlight the samples of interest
0.6.0,##############################################################################
0.6.0,This function allows to make nice plotting
0.6.0,##############################################################################
0.6.0,We can start by generating some data to later illustrate the principle of
0.6.0,each NearMiss heuritic rules.
0.6.0,minority class
0.6.0,majority class
0.6.0,##############################################################################
0.6.0,NearMiss-1
0.6.0,##############################################################################
0.6.0,##############################################################################
0.6.0,NearMiss-1 selects samples from the majority class for which the average
0.6.0,distance to some nearest neighbours is the smallest. In the following
0.6.0,"example, we use a 3-NN to compute the average distance on 2 specific samples"
0.6.0,"of the majority class. Therefore, in this case the point linked by the"
0.6.0,green-dashed line will be selected since the average distance is smaller.
0.6.0,##############################################################################
0.6.0,NearMiss-2
0.6.0,##############################################################################
0.6.0,##############################################################################
0.6.0,NearMiss-2 selects samples from the majority class for which the average
0.6.0,distance to the farthest neighbors is the smallest. With the same
0.6.0,"configuration as previously presented, the sample linked to the green-dashed"
0.6.0,line will be selected since its distance the 3 farthest neighbors is the
0.6.0,smallest.
0.6.0,##############################################################################
0.6.0,NearMiss-3
0.6.0,##############################################################################
0.6.0,##############################################################################
0.6.0,"NearMiss-3 can be divided into 2 steps. First, a nearest-neighbors is used to"
0.6.0,short-list samples from the majority class (i.e. correspond to the
0.6.0,"highlighted samples in the following plot). Then, the sample with the largest"
0.6.0,average distance to the *k* nearest-neighbors are selected.
0.6.0,select only the majority point of interest
0.6.0,Authors: Christos Aridas
0.6.0,Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.6.0,License: MIT
0.6.0,Generate the dataset
0.6.0,Instanciate a PCA object for the sake of easy visualisation
0.6.0,Create the samplers
0.6.0,Create the classifier
0.6.0,Make the splits
0.6.0,Add one transformers and two samplers in the pipeline object
0.6.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.6.0,License: MIT
0.6.0,##############################################################################
0.6.0,Data loading
0.6.0,##############################################################################
0.6.0,##############################################################################
0.6.0,"First, you should download the Porto Seguro data set from Kaggle. See the"
0.6.0,link in the introduction.
0.6.0,##############################################################################
0.6.0,The data set is imbalanced and it will have an effect on the fitting.
0.6.0,##############################################################################
0.6.0,Define the pre-processing pipeline
0.6.0,##############################################################################
0.6.0,##############################################################################
0.6.0,We want to standard scale the numerical features while we want to one-hot
0.6.0,"encode the categorical features. In this regard, we make use of the"
0.6.0,:class:`sklearn.compose.ColumnTransformer`.
0.6.0,Create an environment variable to avoid using the GPU. This can be changed.
0.6.0,##############################################################################
0.6.0,Create a neural-network
0.6.0,##############################################################################
0.6.0,##############################################################################
0.6.0,We create a decorator to report the computation time
0.6.0,##############################################################################
0.6.0,The first model will be trained using the ``fit`` method and with imbalanced
0.6.0,mini-batches.
0.6.0,##############################################################################
0.6.0,"In the contrary, we will use imbalanced-learn to create a generator of"
0.6.0,mini-batches which will yield balanced mini-batches.
0.6.0,##############################################################################
0.6.0,Classification loop
0.6.0,##############################################################################
0.6.0,##############################################################################
0.6.0,We will perform a 10-fold cross-validation and train the neural-network with
0.6.0,the two different strategies previously presented.
0.6.0,##############################################################################
0.6.0,Plot of the results and computation time
0.6.0,##############################################################################
0.6.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.6.0,License: MIT
0.6.0,##############################################################################
0.6.0,Problem definition
0.6.0,##############################################################################
0.6.0,we are dropping the following features:
0.6.0,"- ""fnlwgt"": this feature was created while studying the ""adult"" dataset."
0.6.0,"Thus, we will not use this feature which is not acquired during the survey."
0.6.0,"- ""education-num"": it is encoding the same information than ""education""."
0.6.0,"Thus, we are removing one of these 2 features."
0.6.0,##############################################################################
0.6.0,"The ""adult"" dataset as a class ratio of about 3:1"
0.6.0,##############################################################################
0.6.0,This dataset is only slightly imbalanced. To better highlight the effect of
0.6.0,"learning from an imbalanced dataset, we will increase its ratio to 30:1"
0.6.0,##############################################################################
0.6.0,"For the rest of the notebook, we will make a single split to get training"
0.6.0,and testing data. Note that you should use cross-validation to have an
0.6.0,estimate of the performance variation in practice.
0.6.0,##############################################################################
0.6.0,"As a baseline, we could use a classifier which will always predict the"
0.6.0,majority class independently of the features provided.
0.6.0,#############################################################################
0.6.0,"Instead of using the accuracy, we can use the balanced accuracy which will"
0.6.0,take into account the balancing issue.
0.6.0,##############################################################################
0.6.0,Strategies to learn from an imbalanced dataset
0.6.0,##############################################################################
0.6.0,##############################################################################
0.6.0,We will first define a helper function which will train a given model
0.6.0,and compute both accuracy and balanced accuracy. The results will be stored
0.6.0,in a dataframe
0.6.0,Let's define an empty dataframe to store the results
0.6.0,##############################################################################
0.6.0,Dummy baseline
0.6.0,..............
0.6.0,
0.6.0,"Before to train a real machine learning model, we can store the results"
0.6.0,obtained with our `DummyClassifier`.
0.6.0,##############################################################################
0.6.0,Linear classifier baseline
0.6.0,..........................
0.6.0,
0.6.0,We will create a machine learning pipeline using a `LogisticRegression`
0.6.0,"classifier. In this regard, we will need to one-hot encode the categorical"
0.6.0,columns and standardized the numerical columns before to inject the data into
0.6.0,the `LogisticRegression` classifier.
0.6.0,
0.6.0,"First, we define our numerical and categorical pipelines."
0.6.0,##############################################################################
0.6.0,"Then, we can create a preprocessor which will dispatch the categorical"
0.6.0,columns to the categorical pipeline and the numerical columns to the
0.6.0,numerical pipeline
0.6.0,##############################################################################
0.6.0,"Finally, we connect our preprocessor with our `LogisticRegression`. We can"
0.6.0,then evaluate our model.
0.6.0,##############################################################################
0.6.0,We can see that our linear model is learning slightly better than our dummy
0.6.0,"baseline. However, it is impacted by the class imbalance."
0.6.0,
0.6.0,We can verify that something similar is happening with a tree-based model
0.6.0,"such as `RandomForestClassifier`. With this type of classifier, we will not"
0.6.0,"need to scale the numerical data, and we will only need to ordinal encode the"
0.6.0,categorical data.
0.6.0,##############################################################################
0.6.0,"The `RandomForestClassifier` is as well affected by the class imbalanced,"
0.6.0,"slightly less than the linear model. Now, we will present different approach"
0.6.0,to improve the performance of these 2 models.
0.6.0,
0.6.0,Use `class_weight`
0.6.0,..................
0.6.0,
0.6.0,Most of the models in `scikit-learn` have a parameter `class_weight`. This
0.6.0,parameter will affect the computation of the loss in linear model or the
0.6.0,criterion in the tree-based model to penalize differently a false
0.6.0,classification from the minority and majority class. We can set
0.6.0,"`class_weight=""balanced""` such that the weight applied is inversely"
0.6.0,proportional to the class frequency. We test this parametrization in both
0.6.0,linear model and tree-based model.
0.6.0,##############################################################################
0.6.0,
0.6.0,##############################################################################
0.6.0,We can see that using `class_weight` was really effective for the linear
0.6.0,"model, alleviating the issue of learning from imbalanced classes. However,"
0.6.0,"the `RandomForestClassifier` is still biased toward the majority class,"
0.6.0,mainly due to the criterion which is not suited enough to fight the class
0.6.0,imbalance.
0.6.0,
0.6.0,Resample the training set during learning
0.6.0,.........................................
0.6.0,
0.6.0,Another way is to resample the training set by under-sampling or
0.6.0,over-sampling some of the samples. `imbalanced-learn` provides some samplers
0.6.0,to do such processing.
0.6.0,##############################################################################
0.6.0,
0.6.0,##############################################################################
0.6.0,Applying a random under-sampler before the training of the linear model or
0.6.0,"random forest, allows to not focus on the majority class at the cost of"
0.6.0,making more mistake for samples in the majority class (i.e. decreased
0.6.0,accuracy).
0.6.0,
0.6.0,We could apply any type of samplers and find which sampler is working best
0.6.0,on the current dataset.
0.6.0,
0.6.0,"Instead, we will present another way by using classifiers which will apply"
0.6.0,sampling internally.
0.6.0,
0.6.0,Use of `BalancedRandomForestClassifier` and `BalancedBaggingClassifier`
0.6.0,.......................................................................
0.6.0,
0.6.0,We already showed that random under-sampling can be effective on decision
0.6.0,"tree. However, instead of under-sampling once the dataset, one could"
0.6.0,under-sample the original dataset before to take a bootstrap sample. This is
0.6.0,the base of the `BalancedRandomForestClassifier` and
0.6.0,`BalancedBaggingClassifier`.
0.6.0,##############################################################################
0.6.0,The performance with the `BalancedRandomForestClassifier` is better than
0.6.0,applying a single random under-sampling. We will use a gradient-boosting
0.6.0,classifier within a `BalancedBaggingClassifier`.
0.6.0,##############################################################################
0.6.0,This last approach is the most effective. The different under-sampling allows
0.6.0,to bring some diversity for the different GBDT to learn and not focus on a
0.6.0,portion of the majority class.
0.6.0,
0.6.0,We will repeat the same experiment but with a ratio of 100:1 and make a
0.6.0,similar analysis.
0.6.0,##############################################################################
0.6.0,Increase imbalanced ratio
0.6.0,##############################################################################
0.6.0,##############################################################################
0.6.0,"When we analyse the results, we can draw similar conclusions than in the"
0.6.0,"previous discussion. However, we can observe that the strategy"
0.6.0,"`class_weight=""balanced""` does not improve the performance when using a"
0.6.0,`RandomForestClassifier`. A resampling is indeed required. The most effective
0.6.0,method remains the `BalancedBaggingClassifier` using a GBDT as a base
0.6.0,learner.
0.6.0,Authors: Christos Aridas
0.6.0,Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.6.0,License: MIT
0.6.0,Load the dataset
0.6.0,make nice plotting
0.6.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.6.0,License: MIT
0.6.0,Create a folder to fetch the dataset
0.6.0,Create a pipeline
0.6.0,Classify and report the results
0.6.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.6.0,License: MIT
0.6.0,##############################################################################
0.6.0,Setting the data set
0.6.0,##############################################################################
0.6.0,##############################################################################
0.6.0,We use a part of the 20 newsgroups data set by loading 4 topics. Using the
0.6.0,"scikit-learn loader, the data are split into a training and a testing set."
0.6.0,
0.6.0,Note the class \#3 is the minority class and has almost twice less samples
0.6.0,than the majority class.
0.6.0,##############################################################################
0.6.0,The usual scikit-learn pipeline
0.6.0,##############################################################################
0.6.0,##############################################################################
0.6.0,You might usually use scikit-learn pipeline by combining the TF-IDF
0.6.0,vectorizer to feed a multinomial naive bayes classifier. A classification
0.6.0,report summarized the results on the testing set.
0.6.0,
0.6.0,"As expected, the recall of the class \#3 is low mainly due to the class"
0.6.0,imbalanced.
0.6.0,##############################################################################
0.6.0,Balancing the class before classification
0.6.0,##############################################################################
0.6.0,##############################################################################
0.6.0,"To improve the prediction of the class \#3, it could be interesting to apply"
0.6.0,"a balancing before to train the naive bayes classifier. Therefore, we will"
0.6.0,use a ``RandomUnderSampler`` to equalize the number of samples in all the
0.6.0,classes before the training.
0.6.0,
0.6.0,It is also important to note that we are using the ``make_pipeline`` function
0.6.0,implemented in imbalanced-learn to properly handle the samplers.
0.6.0,##############################################################################
0.6.0,"Although the results are almost identical, it can be seen that the resampling"
0.6.0,allowed to correct the poor recall of the class \#3 at the cost of reducing
0.6.0,"the other metrics for the other classes. However, the overall results are"
0.6.0,slightly better.
0.6.0,Authors: Dayvid Oliveira
0.6.0,Christos Aridas
0.6.0,Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.6.0,License: MIT
0.6.0,Generate the dataset
0.6.0,"Two subplots, unpack the axes array immediately"
0.6.0,List of whitelisted modules and methods; regexp are supported.
0.6.0,These docstrings will fail because they are inheriting from scikit-learn
0.6.0,skip private classes
0.6.0,"We ignore following error code,"
0.6.0,- RT02: The first line of the Returns section
0.6.0,"should contain only the type, .."
0.6.0,(as we may need refer to the name of the returned
0.6.0,object)
0.6.0,- GL01: Docstring text (summary) should start in the line
0.6.0,"immediately after the opening quotes (not in the same line,"
0.6.0,or leaving a blank line in between)
0.6.0,Following codes are only taken into account for the
0.6.0,top level class docstrings:
0.6.0,- ES01: No extended summary found
0.6.0,- SA01: See Also section not found
0.6.0,- EX01: No examples section found
0.6.0,In particular we can't parse the signature of properties
0.6.0,"When applied to classes, detect class method. For functions"
0.6.0,method = None.
0.6.0,TODO: this detection can be improved. Currently we assume that we have
0.6.0,class # methods if the second path element before last is in camel case.
0.6.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.6.0,License: MIT
0.6.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.6.0,Christos Aridas
0.6.0,License: MIT
0.6.0,define an alias for back-compatibility
0.6.0,store information to build dataframe
0.6.0,store information to build a series
0.6.0,store the columns name to reconstruct a dataframe
0.6.0,Adapted from scikit-learn
0.6.0,Author: Edouard Duchesnay
0.6.0,Gael Varoquaux
0.6.0,Virgile Fritsch
0.6.0,Alexandre Gramfort
0.6.0,Lars Buitinck
0.6.0,Christos Aridas
0.6.0,Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.6.0,License: BSD
0.6.0,BaseEstimator interface
0.6.0,validate names
0.6.0,validate estimators
0.6.0,We allow last estimator to be None as an identity transformation
0.6.0,Estimator interface
0.6.0,Setup the memory
0.6.0,joblib >= 0.12
0.6.0,we do not clone when caching is disabled to
0.6.0,preserve backward compatibility
0.6.0,joblib <= 0.11
0.6.0,we do not clone when caching is disabled to
0.6.0,preserve backward compatibility
0.6.0,Fit or load from cache the current transfomer
0.6.0,Replace the transformer of the step with the fitted
0.6.0,transformer. This is necessary when loading the transformer
0.6.0,from the cache.
0.6.0,Based on NiLearn package
0.6.0,License: simplified BSD
0.6.0,"PEP0440 compatible formatted version, see:"
0.6.0,https://www.python.org/dev/peps/pep-0440/
0.6.0,
0.6.0,Generic release markers:
0.6.0,X.Y
0.6.0,X.Y.Z # For bugfix releases
0.6.0,
0.6.0,Admissible pre-release markers:
0.6.0,X.YaN # Alpha release
0.6.0,X.YbN # Beta release
0.6.0,X.YrcN # Release Candidate
0.6.0,X.Y # Final release
0.6.0,
0.6.0,Dev branch marker is: 'X.Y.dev' or 'X.Y.devN' where N is an integer.
0.6.0,'X.Y.dev0' is the canonical version of 'X.Y.dev'
0.6.0,
0.6.0,coding: utf-8
0.6.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.6.0,Dariusz Brzezinski
0.6.0,License: MIT
0.6.0,Only negative labels
0.6.0,"Calculate tp_sum, pred_sum, true_sum ###"
0.6.0,labels are now from 0 to len(labels) - 1 -> use bincount
0.6.0,Pathological case
0.6.0,Compute the true negative
0.6.0,Retain only selected labels
0.6.0,"Finally, we have all our sufficient statistics. Divide! #"
0.6.0,"Divide, and on zero-division, set scores to 0 and warn:"
0.6.0,"Oddly, we may get an ""invalid"" rather than a ""divide"" error"
0.6.0,here.
0.6.0,Average the results
0.6.0,labels are now from 0 to len(labels) - 1 -> use bincount
0.6.0,Pathological case
0.6.0,Retain only selected labels
0.6.0,old version of scipy return MaskedConstant instead of 0.0
0.6.0,Create the list of tags
0.6.0,check that the scoring function does not need a score
0.6.0,and only a prediction
0.6.0,Compute the score from the scoring function
0.6.0,Square if desired
0.6.0,Get the signature of the sens/spec function
0.6.0,We need to extract from kwargs only the one needed by the
0.6.0,specificity and specificity
0.6.0,Make the intersection between the parameters
0.6.0,Create a sub dictionary
0.6.0,Check if the metric is the geometric mean
0.6.0,We do not support multilabel so the only average supported
0.6.0,is binary
0.6.0,Create the list of parameters through signature binding
0.6.0,Call the sens/spec function
0.6.0,Compute the dominance
0.6.0,Compute the different metrics
0.6.0,Precision/recall/f1
0.6.0,Specificity
0.6.0,Geometric mean
0.6.0,Index balanced accuracy
0.6.0,compute averages
0.6.0,coding: utf-8
0.6.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.6.0,Christos Aridas
0.6.0,License: MIT
0.6.0,##############################################################################
0.6.0,Utilities for testing
0.6.0,import some data to play with
0.6.0,restrict to a binary classification task
0.6.0,add noisy features to make the problem harder and avoid perfect results
0.6.0,"run classifier, get class probabilities and label predictions"
0.6.0,only interested in probabilities of the positive case
0.6.0,XXX: do we really want a special API for the binary case?
0.6.0,##############################################################################
0.6.0,Tests
0.6.0,detailed measures for each class
0.6.0,individual scoring function that can be used for grid search: in the
0.6.0,binary class case the score is the value of the measure for the positive
0.6.0,class (e.g. label == 1). This is deprecated for average != 'binary'.
0.6.0,Such a case may occur with non-stratified cross-validation
0.6.0,ensure the above were meaningful tests:
0.6.0,Bad pos_label
0.6.0,Bad average option
0.6.0,but average != 'binary'; even if data is binary
0.6.0,compute the geometric mean for the binary problem
0.6.0,print classification report with class names
0.6.0,print classification report with label detection
0.6.0,print classification report with class names
0.6.0,print classification report with label detection
0.6.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.6.0,Christos Aridas
0.6.0,License: MIT
0.6.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.6.0,Christos Aridas
0.6.0,License: MIT
0.6.0,The ratio is computed using a one-vs-rest manner. Using majority
0.6.0,in multi-class would lead to slightly different results at the
0.6.0,cost of introducing a new parameter.
0.6.0,rounding may cause new amount for n_samples
0.6.0,the nearest neighbors need to be fitted only on the current class
0.6.0,to find the class NN to generate new samples
0.6.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.6.0,Christos Aridas
0.6.0,License: MIT
0.6.0,store information to build dataframe
0.6.0,store information to build a series
0.6.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.6.0,Christos Aridas
0.6.0,License: MIT
0.6.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.6.0,Fernando Nogueira
0.6.0,Christos Aridas
0.6.0,Dzianis Dudnik
0.6.0,License: MIT
0.6.0,np.newaxis for backwards compatability with random_state
0.6.0,Samples are in danger for m/2 <= m' < m
0.6.0,Samples are noise for m = m'
0.6.0,divergence between borderline-1 and borderline-2
0.6.0,Create synthetic samples for borderline points.
0.6.0,only minority
0.6.0,we use a one-vs-rest policy to handle the multiclass in which
0.6.0,new samples will be created considering not only the majority
0.6.0,class but all over classes.
0.6.0,@Substitution(
0.6.0,"sampling_strategy=BaseOverSampler._sampling_strategy_docstring,"
0.6.0,random_state=_random_state_docstring)
0.6.0,store information to build dataframe
0.6.0,store information to build a series
0.6.0,compute the median of the standard deviation of the minority class
0.6.0,the input of the OneHotEncoder needs to be dense
0.6.0,we can replace the 1 entries of the categorical features with the
0.6.0,median of the standard deviation. It will ensure that whenever
0.6.0,"distance is computed between 2 samples, the difference will be equal"
0.6.0,to the median of the standard deviation as in the original paper.
0.6.0,reverse the encoding of the categorical features
0.6.0,the matrix is supposed to be in the CSR format after the stacking
0.6.0,change in sparsity structure more efficient with LIL than CSR
0.6.0,convert to dense array since scipy.sparse doesn't handle 3D
0.6.0,tie breaking argmax
0.6.0,validate the parameters
0.6.0,negate diagonal elements
0.6.0,target_class_indices = np.flatnonzero(y == class_sample)
0.6.0,"X_class = _safe_indexing(X, target_class_indices)"
0.6.0,identify cluster which are answering the requirements
0.6.0,the cluster is already considered balanced
0.6.0,not enough samples to apply SMOTE
0.6.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.6.0,Christos Aridas
0.6.0,License: MIT
0.6.0,check that we can oversample even with missing or infinite data
0.6.0,regression tests for #605
0.6.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.6.0,Christos Aridas
0.6.0,Dzianis Dudnik
0.6.0,License: MIT
0.6.0,create 2 random continuous feature
0.6.0,create a categorical feature using some string
0.6.0,create a categorical feature using some integer
0.6.0,return the categories
0.6.0,create 2 random continuous feature
0.6.0,create a categorical feature using some string
0.6.0,create a categorical feature using some integer
0.6.0,return the categories
0.6.0,create 2 random continuous feature
0.6.0,create a categorical feature using some string
0.6.0,create a categorical feature using some integer
0.6.0,return the categories
0.6.0,create 2 random continuous feature
0.6.0,create a categorical feature using some string
0.6.0,create a categorical feature using some integer
0.6.0,return the categories
0.6.0,create 2 random continuous feature
0.6.0,create a categorical feature using some string
0.6.0,create a categorical feature using some integer
0.6.0,part of the common test which apply to SMOTE-NC even if it is not default
0.6.0,constructible
0.6.0,Check that the samplers handle pandas dataframe and pandas series
0.6.0,Cast X and y to not default dtype
0.6.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.6.0,Christos Aridas
0.6.0,License: MIT
0.6.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.6.0,Christos Aridas
0.6.0,License: MIT
0.6.0,check that m_neighbors is properly set. Regression test for:
0.6.0,https://github.com/scikit-learn-contrib/imbalanced-learn/issues/568
0.6.0,shuffle the indices since the sampler are packing them by class
0.6.0,helper functions
0.6.0,input and output
0.6.0,build the model and weights
0.6.0,"build the loss, predict, and train operator"
0.6.0,Initialization of all variables in the graph
0.6.0,"For each epoch, run accuracy on train and test"
0.6.0,helper functions
0.6.0,input and output
0.6.0,build the model and weights
0.6.0,"build the loss, predict, and train operator"
0.6.0,Initialization of all variables in the graph
0.6.0,"For each epoch, run accuracy on train and test"
0.6.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.6.0,License: MIT
0.6.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.6.0,Fernando Nogueira
0.6.0,Christos Aridas
0.6.0,License: MIT
0.6.0,find which class to not consider
0.6.0,there is a Tomek link between two samples if they are both nearest
0.6.0,neighbors of each others.
0.6.0,Find the nearest neighbour of every point
0.6.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.6.0,Christos Aridas
0.6.0,License: MIT
0.6.0,Randomly get one sample from the majority class
0.6.0,Generate the index to select
0.6.0,Create the set C - One majority samples and all minority
0.6.0,Create the set S - all majority samples
0.6.0,fit knn on C
0.6.0,Check each sample in S if we keep it or drop it
0.6.0,Do not select sample which are already well classified
0.6.0,Classify on S
0.6.0,If the prediction do not agree with the true label
0.6.0,append it in C_x
0.6.0,Keep the index for later
0.6.0,Update C
0.6.0,fit a knn on C
0.6.0,This experimental to speed up the search
0.6.0,Classify all the element in S and avoid to test the
0.6.0,well classified elements
0.6.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.6.0,Dayvid Oliveira
0.6.0,Christos Aridas
0.6.0,License: MIT
0.6.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.6.0,Christos Aridas
0.6.0,License: MIT
0.6.0,Compute the distance considering the farthest neighbour
0.6.0,Sort the list of distance and get the index
0.6.0,Throw a warning to tell the user that we did not have enough samples
0.6.0,to select and that we just select everything
0.6.0,Select the desired number of samples
0.6.0,idx_tmp is relative to the feature selected in the
0.6.0,previous step and we need to find the indirection
0.6.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.6.0,Christos Aridas
0.6.0,License: MIT
0.6.0,select a sample from the current class
0.6.0,create the set composed of all minority samples and one
0.6.0,sample from the current class.
0.6.0,create the set S with removing the seed from S
0.6.0,since that it will be added anyway
0.6.0,apply Tomek cleaning
0.6.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.6.0,Dayvid Oliveira
0.6.0,Christos Aridas
0.6.0,License: MIT
0.6.0,Check the stopping criterion
0.6.0,1. If there is no changes for the vector y
0.6.0,2. If the number of samples in the other class become inferior to
0.6.0,the number of samples in the majority class
0.6.0,3. If one of the class is disappearing
0.6.0,Case 1
0.6.0,Case 2
0.6.0,Case 3
0.6.0,Check the stopping criterion
0.6.0,1. If the number of samples in the other class become inferior to
0.6.0,the number of samples in the majority class
0.6.0,2. If one of the class is disappearing
0.6.0,Case 1else:
0.6.0,overwrite b_min_bec_maj
0.6.0,Case 2
0.6.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.6.0,Christos Aridas
0.6.0,License: MIT
0.6.0,clean the neighborhood
0.6.0,compute which classes to consider for cleaning for the A2 group
0.6.0,compute a2 group
0.6.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.6.0,Christos Aridas
0.6.0,License: MIT
0.6.0,store information to build dataframe
0.6.0,store information to build a series
0.6.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.6.0,Christos Aridas
0.6.0,License: MIT
0.6.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.6.0,Christos Aridas
0.6.0,License: MIT
0.6.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.6.0,Christos Aridas
0.6.0,License: MIT
0.6.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.6.0,Christos Aridas
0.6.0,License: MIT
0.6.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.6.0,Christos Aridas
0.6.0,License: MIT
0.6.0,check that we can undersample even with missing or infinite data
0.6.0,regression tests for #605
0.6.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.6.0,Christos Aridas
0.6.0,License: MIT
0.6.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.6.0,Christos Aridas
0.6.0,License: MIT
0.6.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.6.0,Christos Aridas
0.6.0,License: MIT
0.6.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.6.0,Christos Aridas
0.6.0,License: MIT
0.6.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.6.0,Christos Aridas
0.6.0,License: MIT
0.6.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.6.0,Fernando Nogueira
0.6.0,Christos Aridas
0.6.0,License: MIT
0.6.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.6.0,Christos Aridas
0.6.0,License: MIT
0.6.0,test that all_estimators doesn't find abstract classes.
0.6.0,don't run twice the sampler tests. Meta-estimator do not have a
0.6.0,fit_resample method.
0.6.0,input validation etc for non-meta estimators
0.6.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.6.0,License: MIT
0.6.0,check that we can let a pass a regression variable by turning down the
0.6.0,validation
0.6.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.6.0,Christos Aridas
0.6.0,License: MIT
0.6.0,store timestamp to figure out whether the result of 'fit' has been
0.6.0,cached or not
0.6.0,store timestamp to figure out whether the result of 'fit' has been
0.6.0,cached or not
0.6.0,Pipeline accepts steps as tuple
0.6.0,Test the various init parameters of the pipeline.
0.6.0,Check that we can't instantiate pipelines with objects without fit
0.6.0,method
0.6.0,Smoke test with only an estimator
0.6.0,Check that params are set
0.6.0,Smoke test the repr:
0.6.0,Test with two objects
0.6.0,Check that we can't instantiate with non-transformers on the way
0.6.0,"Note that NoTrans implements fit, but not transform"
0.6.0,Check that params are set
0.6.0,Smoke test the repr:
0.6.0,Check that params are not set when naming them wrong
0.6.0,Test clone
0.6.0,"Check that apart from estimators, the parameters are the same"
0.6.0,Remove estimators that where copied
0.6.0,Test the various methods of the pipeline (anova).
0.6.0,Test with Anova + LogisticRegression
0.6.0,Test that the pipeline can take fit parameters
0.6.0,classifier should return True
0.6.0,and transformer params should not be changed
0.6.0,invalid parameters should raise an error message
0.6.0,Pipeline should pass sample_weight
0.6.0,When sample_weight is None it shouldn't be passed
0.6.0,Test pipeline raises set params error message for nested models.
0.6.0,nested model check
0.6.0,Test the various methods of the pipeline (pca + svm).
0.6.0,Test with PCA + SVC
0.6.0,Test the various methods of the pipeline (preprocessing + svm).
0.6.0,check shapes of various prediction functions
0.6.0,test that the fit_predict method is implemented on a pipeline
0.6.0,test that the fit_predict on pipeline yields same results as applying
0.6.0,transform and clustering steps separately
0.6.0,"As pipeline doesn't clone estimators on construction,"
0.6.0,it must have its own estimators
0.6.0,first compute the transform and clustering step separately
0.6.0,use a pipeline to do the transform and clustering in one step
0.6.0,tests that a pipeline does not have fit_predict method when final
0.6.0,step of pipeline does not have fit_predict defined
0.6.0,tests that Pipeline passes fit_params to intermediate steps
0.6.0,when fit_predict is invoked
0.6.0,Test whether pipeline works with a transformer at the end.
0.6.0,Also test pipeline.transform and pipeline.inverse_transform
0.6.0,test transform and fit_transform:
0.6.0,Test whether pipeline works with a transformer missing fit_transform
0.6.0,test fit_transform:
0.6.0,Directly setting attr
0.6.0,Using set_params
0.6.0,Using set_params to replace single step
0.6.0,With invalid data
0.6.0,Test setting Pipeline steps to None
0.6.0,"for other methods, ensure no AttributeErrors on None:"
0.6.0,mult2 and mult3 are active
0.6.0,Check 'passthrough' step at construction time
0.6.0,Test that an error is raised when memory is not a string or a Memory
0.6.0,instance
0.6.0,Define memory as an integer
0.6.0,Test with Transformer + SVC
0.6.0,Memoize the transformer at the first fit
0.6.0,Get the time stamp of the tranformer in the cached pipeline
0.6.0,Check that cached_pipe and pipe yield identical results
0.6.0,Check that we are reading the cache while fitting
0.6.0,a second time
0.6.0,Check that cached_pipe and pipe yield identical results
0.6.0,Create a new pipeline with cloned estimators
0.6.0,Check that even changing the name step does not affect the cache hit
0.6.0,Check that cached_pipe and pipe yield identical results
0.6.0,Test with Transformer + SVC
0.6.0,Memoize the transformer at the first fit
0.6.0,Get the time stamp of the tranformer in the cached pipeline
0.6.0,Check that cached_pipe and pipe yield identical results
0.6.0,Check that we are reading the cache while fitting
0.6.0,a second time
0.6.0,Check that cached_pipe and pipe yield identical results
0.6.0,Create a new pipeline with cloned estimators
0.6.0,Check that even changing the name step does not affect the cache hit
0.6.0,Check that cached_pipe and pipe yield identical results
0.6.0,Test the various methods of the pipeline (pca + svm).
0.6.0,Test with PCA + SVC
0.6.0,Test the various methods of the pipeline (pca + svm).
0.6.0,Test with PCA + SVC
0.6.0,Test whether pipeline works with a sampler at the end.
0.6.0,Also test pipeline.sampler
0.6.0,test transform and fit_transform:
0.6.0,We round the value near to zero. It seems that PCA has some issue
0.6.0,with that
0.6.0,Test whether pipeline works with a sampler at the end.
0.6.0,Also test pipeline.sampler
0.6.0,Test pipeline using None as preprocessing step and a classifier
0.6.0,"Test pipeline using None, RUS and a classifier"
0.6.0,"Test pipeline using RUS, None and a classifier"
0.6.0,Test pipeline using None step and a sampler
0.6.0,Test pipeline using None and a transformer that implements transform and
0.6.0,inverse_transform
0.6.0,Test the various methods of the pipeline (anova).
0.6.0,Test with RandomUnderSampling + Anova + LogisticRegression
0.6.0,Test the various methods of the pipeline (anova).
0.6.0,Test the various methods of the pipeline (anova).
0.6.0,Test with RandomUnderSampling + Anova + LogisticRegression
0.6.0,tests that Pipeline passes predict_params to the final estimator
0.6.0,when predict is invoked
0.6.0,Test that the score_samples method is implemented on a pipeline.
0.6.0,Test that the score_samples method on pipeline yields same results as
0.6.0,applying transform and score_samples steps separately.
0.6.0,Check the shapes
0.6.0,Check the values
0.6.0,Test that a pipeline does not have score_samples method when the final
0.6.0,step of the pipeline does not have score_samples defined.
0.6.0,Test that the score_samples method is implemented on a pipeline.
0.6.0,Test that the score_samples method on pipeline yields same results as
0.6.0,applying transform and score_samples steps separately.
0.6.0,Check the shapes
0.6.0,Check the values
0.6.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.6.0,Christos Aridas
0.6.0,License: MIT
0.6.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.6.0,License: MIT
0.6.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.6.0,License: MIT
0.6.0,Adapated from scikit-learn
0.6.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.6.0,License: MIT
0.6.0,trigger our checks if this is a SamplerMixin
0.6.0,scikit-learn common tests
0.6.0,should raise warning if the target is continuous (we cannot raise error)
0.6.0,if the target is multilabel then we should raise an error
0.6.0,IHT does not enforce the number of samples but provide a number
0.6.0,of samples the closest to the desired target.
0.6.0,in this test we will force all samplers to not change the class 1
0.6.0,check that sparse matrices can be passed through the sampler leading to
0.6.0,the same results than dense
0.6.0,set KMeans to full since it support sparse and dense
0.6.0,Check that the samplers handle pandas dataframe and pandas series
0.6.0,check that we return a pandas dataframe if a dataframe was given in
0.6.0,Check that multiclass target lead to the same results than OVA encoding
0.6.0,Cast X and y to not default dtype
0.6.0,Adapted from scikit-learn
0.6.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.6.0,License: MIT
0.6.0,meta-estimators need another estimator to be instantiated.
0.6.0,estimators that there is no way to default-construct sensibly
0.6.0,some strange ones
0.6.0,get parent folder
0.6.0,get rid of abstract base classes
0.6.0,get rid of sklearn estimators which have been imported in some classes
0.6.0,possibly get rid of meta estimators
0.6.0,"drop duplicates, sort for reproducibility"
0.6.0,itemgetter is used to ensure the sort does not extend to the 2nd item of
0.6.0,the tuple
0.6.0,Author: Alexander L. Hayes <hayesall@iu.edu>
0.6.0,License: MIT
0.6.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.6.0,License: MIT
0.6.0,check that all keys in sampling_strategy are also in y
0.6.0,check that there is no negative number
0.6.0,check that all keys in sampling_strategy are also in y
0.6.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.6.0,Christos Aridas
0.6.0,License: MIT
0.6.0,this function could create an equal number of samples
0.6.0,We pass on purpose a non sorted dictionary and check that the resulting
0.6.0,dictionary is sorted. Refer to issue #428.
0.6.0,Author: Alexander L. Hayes <hayesall@iu.edu>
0.6.0,License: MIT
0.6.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.6.0,License: MIT
0.6.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.6.0,Christos Aridas
0.6.0,License: MIT
0.6.0,check if the filtering is working with a list or a single string
0.6.0,check that all estimators are sampler
0.6.0,check that an error is raised when the type is unknown
0.6.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.6.0,License: MIT
0.6.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.6.0,Christos Aridas
0.6.0,License: MIT
0.6.0,Otherwise create a default SMOTE
0.6.0,Otherwise create a default TomekLinks
0.6.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.6.0,Christos Aridas
0.6.0,License: MIT
0.6.0,Otherwise create a default SMOTE
0.6.0,Otherwise create a default EditedNearestNeighbours
0.6.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.6.0,Christos Aridas
0.6.0,License: MIT
0.6.0,Check if default job count is None
0.6.0,Check if job count is set
0.6.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.6.0,Christos Aridas
0.6.0,License: MIT
0.6.0,Check if default job count is none
0.6.0,Check if job count is set
0.6.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.6.0,License: MIT
0.6.0,resample before to fit the tree
0.6.0,Validate or convert input data
0.6.0,Pre-sort indices to avoid that each individual tree of the
0.6.0,ensemble sorts the indices.
0.6.0,Remap output
0.6.0,reshape is necessary to preserve the data contiguity against vs
0.6.0,"[:, np.newaxis] that does not."
0.6.0,Get bootstrap sample size
0.6.0,Check parameters
0.6.0,"Free allocated memory, if any"
0.6.0,We draw from the random state to get the random state we
0.6.0,would have got if we hadn't used a warm_start.
0.6.0,Parallel loop: we prefer the threading backend as the Cython code
0.6.0,for fitting the trees is internally releasing the Python GIL
0.6.0,making threading more efficient than multiprocessing in
0.6.0,"that case. However, we respect any parallel_backend contexts set"
0.6.0,"at a higher level, since correctness does not rely on using"
0.6.0,threads.
0.6.0,Collect newly grown trees
0.6.0,Create pipeline with the fitted samplers and trees
0.6.0,Decapsulate classes_ attributes
0.6.0,"with the resampling, we are likely to have rows not included"
0.6.0,for the OOB score leading to division by zero
0.6.0,Instances incorrectly classified
0.6.0,Error fraction
0.6.0,Stop if classification is perfect
0.6.0,Construct y coding as described in Zhu et al [2]:
0.6.0,
0.6.0,y_k = 1 if c == k else -1 / (K - 1)
0.6.0,
0.6.0,"where K == n_classes_ and c, k in [0, K) are indices along the second"
0.6.0,axis of the y coding with c being the index corresponding to the true
0.6.0,class label.
0.6.0,Displace zero probabilities so the log is defined.
0.6.0,Also fix negative elements which may occur with
0.6.0,negative sample weights.
0.6.0,Boost weight using multi-class AdaBoost SAMME.R alg
0.6.0,Only boost the weights if it will fit again
0.6.0,Only boost positive weights
0.6.0,Instances incorrectly classified
0.6.0,Error fraction
0.6.0,Stop if classification is perfect
0.6.0,Stop if the error is at least as bad as random guessing
0.6.0,Boost weight using multi-class AdaBoost SAMME alg
0.6.0,Only boost the weights if I will fit again
0.6.0,Only boost positive weights
0.6.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.6.0,Christos Aridas
0.6.0,License: MIT
0.6.0,RandomUnderSampler is not supporting sample_weight. We need to pass
0.6.0,None.
0.6.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.6.0,Christos Aridas
0.6.0,License: MIT
0.6.0,RandomUnderSampler is not supporting sample_weight. We need to pass
0.6.0,None.
0.6.0,check that we have an ensemble of samplers and estimators with a
0.6.0,consistent size
0.6.0,each sampler in the ensemble should have different random state
0.6.0,each estimator in the ensemble should have different random state
0.6.0,check the consistency of the feature importances
0.6.0,check the consistency of the prediction outpus
0.6.0,Predictions should be the same when sample_weight are all ones
0.6.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.6.0,Christos Aridas
0.6.0,License: MIT
0.6.0,Check classification for various parameter settings.
0.6.0,Test that bootstrapping samples generate non-perfect base estimators.
0.6.0,"without bootstrap, all trees are perfect on the training set"
0.6.0,disable the resampling by passing an empty dictionary.
0.6.0,"with bootstrap, trees are no longer perfect on the training set"
0.6.0,Test that bootstrapping features may generate duplicate features.
0.6.0,Predict probabilities.
0.6.0,Normal case
0.6.0,"Degenerate case, where some classes are missing"
0.6.0,Check that oob prediction is a good estimation of the generalization
0.6.0,error.
0.6.0,Test with few estimators
0.6.0,Check singleton ensembles.
0.6.0,Test that it gives proper exception on deficient input.
0.6.0,Test n_estimators
0.6.0,Test max_samples
0.6.0,Test max_features
0.6.0,Test support of decision_function
0.6.0,Check that bagging ensembles can be grid-searched.
0.6.0,Transform iris into a binary classification task
0.6.0,Grid search with scoring based on decision_function
0.6.0,Check base_estimator and its default values.
0.6.0,Test if fitting incrementally with warm start gives a forest of the
0.6.0,right size and the same results as a normal fit.
0.6.0,Test if warm start'ed second fit with smaller n_estimators raises error.
0.6.0,Test that nothing happens when fitting without increasing n_estimators
0.6.0,"modify X to nonsense values, this should not change anything"
0.6.0,warm started classifier with 5+5 estimators should be equivalent to
0.6.0,one classifier with 10 estimators
0.6.0,Check using oob_score and warm_start simultaneously fails
0.6.0,"Make sure OOB scores are identical when random_state, estimator, and"
0.6.0,training data are fixed and fitting is done twice
0.6.0,Check that format of estimators_samples_ is correct and that results
0.6.0,generated at fit time can be identically reproduced at a later time
0.6.0,using data saved in object attributes.
0.6.0,remap the y outside of the BalancedBaggingclassifier
0.6.0,"_, y = np.unique(y, return_inverse=True)"
0.6.0,Get relevant attributes
0.6.0,Test for correct formatting
0.6.0,Re-fit single estimator to test for consistent sampling
0.6.0,Make sure validated max_samples and original max_samples are identical
0.6.0,when valid integer max_samples supplied by user
0.6.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.6.0,Christos Aridas
0.6.0,License: MIT
0.6.0,Generate a global dataset to use
0.6.0,Check classification for various parameter settings.
0.6.0,test the different prediction function
0.6.0,Check base_estimator and its default values.
0.6.0,Test if fitting incrementally with warm start gives a forest of the
0.6.0,right size and the same results as a normal fit.
0.6.0,Test if warm start'ed second fit with smaller n_estimators raises error.
0.6.0,Test that nothing happens when fitting without increasing n_estimators
0.6.0,"modify X to nonsense values, this should not change anything"
0.6.0,warm started classifier with 5+5 estimators should be equivalent to
0.6.0,one classifier with 10 estimators
0.6.0,Check warning if not enough estimators
0.6.0,First fit with no restriction on max samples
0.6.0,Second fit with max samples restricted to just 2
0.6.0,Regression test for #655: check that the oob score is closed to 0.5
0.6.0,a binomial experiment.
0.6.0,Author: Guillaume Lemaitre
0.6.0,License: BSD 3 clause
0.6.0,"The index start at one, then we need to remove one"
0.6.0,to not have issue with the indexing.
0.6.0,go through the list and check if the data are available
0.6.0,Authors: Dayvid Oliveira
0.6.0,Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.6.0,Christos Aridas
0.6.0,License: MIT
0.6.0,restrict ratio to be a dict or a callable
0.6.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.6.0,Christos Aridas
0.6.0,License: MIT
0.6.0,"we are reusing part of utils.check_sampling_strategy, however this is not"
0.6.0,cover in the common tests so we will repeat it here
0.6.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.6.0,Christos Aridas
0.6.0,License: MIT
0.6.0,This is a trick to avoid an error during tests collection with pytest. We
0.6.0,avoid the error when importing the package raise the error at the moment of
0.6.0,creating the instance.
0.6.0,flag for keras sequence duck-typing
0.6.0,shuffle the indices since the sampler are packing them by class
0.5.0,This file is here so that when running from the root folder
0.5.0,./sklearn is added to sys.path by pytest.
0.5.0,See https://docs.pytest.org/en/latest/pythonpath.html for more details.
0.5.0,"For example, this allows to build extensions in place and run pytest"
0.5.0,doc/modules/clustering.rst and use sklearn from the local folder
0.5.0,rather than the one from site-packages.
0.5.0,Set numpy array str/repr to legacy behaviour on numpy > 1.13 to make
0.5.0,the doctests pass
0.5.0,! /usr/bin/env python
0.5.0,get __version__ from _version.py
0.5.0,-*- coding: utf-8 -*-
0.5.0,
0.5.0,"imbalanced-learn documentation build configuration file, created by"
0.5.0,sphinx-quickstart on Mon Jan 18 14:44:12 2016.
0.5.0,
0.5.0,This file is execfile()d with the current directory set to its
0.5.0,containing dir.
0.5.0,
0.5.0,Note that not all possible configuration values are present in this
0.5.0,autogenerated file.
0.5.0,
0.5.0,All configuration values have a default; values that are commented out
0.5.0,serve to show the default.
0.5.0,"If extensions (or modules to document with autodoc) are in another directory,"
0.5.0,add these directories to sys.path here. If the directory is relative to the
0.5.0,"documentation root, use os.path.abspath to make it absolute, like shown here."
0.5.0,-- General configuration ------------------------------------------------
0.5.0,"If your documentation needs a minimal Sphinx version, state it here."
0.5.0,needs_sphinx = '1.0'
0.5.0,"Add any Sphinx extension module names here, as strings. They can be"
0.5.0,extensions coming with Sphinx (named 'sphinx.ext.*') or your custom
0.5.0,ones.
0.5.0,this is needed for some reason...
0.5.0,see https://github.com/numpy/numpydoc/issues/69
0.5.0,pngmath / imgmath compatibility layer for different sphinx versions
0.5.0,"Add any paths that contain templates here, relative to this directory."
0.5.0,generate autosummary even if no references
0.5.0,The suffix of source filenames.
0.5.0,The encoding of source files.
0.5.0,source_encoding = 'utf-8-sig'
0.5.0,Generate the plot for the gallery
0.5.0,The master toctree document.
0.5.0,General information about the project.
0.5.0,"The version info for the project you're documenting, acts as replacement for"
0.5.0,"|version| and |release|, also used in various other places throughout the"
0.5.0,built documents.
0.5.0,
0.5.0,The short X.Y version.
0.5.0,"The full version, including alpha/beta/rc tags."
0.5.0,The language for content autogenerated by Sphinx. Refer to documentation
0.5.0,for a list of supported languages.
0.5.0,language = None
0.5.0,"There are two options for replacing |today|: either, you set today to some"
0.5.0,"non-false value, then it is used:"
0.5.0,today = ''
0.5.0,"Else, today_fmt is used as the format for a strftime call."
0.5.0,"today_fmt = '%B %d, %Y'"
0.5.0,"List of patterns, relative to source directory, that match files and"
0.5.0,directories to ignore when looking for source files.
0.5.0,The reST default role (used for this markup: `text`) to use for all
0.5.0,documents.
0.5.0,default_role = None
0.5.0,"If true, '()' will be appended to :func: etc. cross-reference text."
0.5.0,"If true, the current module name will be prepended to all description"
0.5.0,unit titles (such as .. function::).
0.5.0,add_module_names = True
0.5.0,"If true, sectionauthor and moduleauthor directives will be shown in the"
0.5.0,output. They are ignored by default.
0.5.0,show_authors = False
0.5.0,The name of the Pygments (syntax highlighting) style to use.
0.5.0,Custom style
0.5.0,A list of ignored prefixes for module index sorting.
0.5.0,modindex_common_prefix = []
0.5.0,"If true, keep warnings as ""system message"" paragraphs in the built documents."
0.5.0,keep_warnings = False
0.5.0,-- Options for HTML output ----------------------------------------------
0.5.0,The theme to use for HTML and HTML Help pages.  See the documentation for
0.5.0,a list of builtin themes.
0.5.0,Theme options are theme-specific and customize the look and feel of a theme
0.5.0,"further.  For a list of options available for each theme, see the"
0.5.0,documentation.
0.5.0,html_theme_options = {'prev_next_buttons_location': None}
0.5.0,"Add any paths that contain custom themes here, relative to this directory."
0.5.0,"The name for this set of Sphinx documents.  If None, it defaults to"
0.5.0,"""<project> v<release> documentation""."
0.5.0,html_title = None
0.5.0,A shorter title for the navigation bar.  Default is the same as html_title.
0.5.0,html_short_title = None
0.5.0,The name of an image file (relative to this directory) to place at the top
0.5.0,of the sidebar.
0.5.0,html_logo = None
0.5.0,The name of an image file (within the static path) to use as favicon of the
0.5.0,docs.  This file should be a Windows icon file (.ico) being 16x16 or 32x32
0.5.0,pixels large.
0.5.0,html_favicon = None
0.5.0,"Add any paths that contain custom static files (such as style sheets) here,"
0.5.0,"relative to this directory. They are copied after the builtin static files,"
0.5.0,"so a file named ""default.css"" will overwrite the builtin ""default.css""."
0.5.0,Add any extra paths that contain custom files (such as robots.txt or
0.5.0,".htaccess) here, relative to this directory. These files are copied"
0.5.0,directly to the root of the documentation.
0.5.0,html_extra_path = []
0.5.0,"If not '', a 'Last updated on:' timestamp is inserted at every page bottom,"
0.5.0,using the given strftime format.
0.5.0,"html_last_updated_fmt = '%b %d, %Y'"
0.5.0,"If true, SmartyPants will be used to convert quotes and dashes to"
0.5.0,typographically correct entities.
0.5.0,html_use_smartypants = True
0.5.0,"Custom sidebar templates, maps document names to template names."
0.5.0,html_sidebars = {}
0.5.0,"Additional templates that should be rendered to pages, maps page names to"
0.5.0,template names.
0.5.0,html_additional_pages = {}
0.5.0,"If false, no module index is generated."
0.5.0,html_domain_indices = True
0.5.0,"If false, no index is generated."
0.5.0,html_use_index = True
0.5.0,"If true, the index is split into individual pages for each letter."
0.5.0,html_split_index = False
0.5.0,"If true, links to the reST sources are added to the pages."
0.5.0,html_show_sourcelink = True
0.5.0,"If true, ""Created using Sphinx"" is shown in the HTML footer. Default is True."
0.5.0,html_show_sphinx = True
0.5.0,"If true, ""(C) Copyright ..."" is shown in the HTML footer. Default is True."
0.5.0,html_show_copyright = True
0.5.0,"If true, an OpenSearch description file will be output, and all pages will"
0.5.0,contain a <link> tag referring to it.  The value of this option must be the
0.5.0,base URL from which the finished HTML is served.
0.5.0,html_use_opensearch = ''
0.5.0,"This is the file name suffix for HTML files (e.g. "".xhtml"")."
0.5.0,html_file_suffix = None
0.5.0,Output file base name for HTML help builder.
0.5.0,-- Options for LaTeX output ---------------------------------------------
0.5.0,The paper size ('letterpaper' or 'a4paper').
0.5.0,"'papersize': 'letterpaper',"
0.5.0,"The font size ('10pt', '11pt' or '12pt')."
0.5.0,"'pointsize': '10pt',"
0.5.0,Additional stuff for the LaTeX preamble.
0.5.0,"'preamble': '',"
0.5.0,Grouping the document tree into LaTeX files. List of tuples
0.5.0,"(source start file, target name, title,"
0.5.0,"author, documentclass [howto, manual, or own class])."
0.5.0,The name of an image file (relative to this directory) to place at the top of
0.5.0,the title page.
0.5.0,latex_logo = None
0.5.0,"For ""manual"" documents, if this is true, then toplevel headings are parts,"
0.5.0,not chapters.
0.5.0,latex_use_parts = False
0.5.0,"If true, show page references after internal links."
0.5.0,latex_show_pagerefs = False
0.5.0,"If true, show URL addresses after external links."
0.5.0,latex_show_urls = False
0.5.0,Documents to append as an appendix to all manuals.
0.5.0,latex_appendices = []
0.5.0,intersphinx configuration
0.5.0,sphinx-gallery configuration
0.5.0,-- Options for manual page output ---------------------------------------
0.5.0,"If false, no module index is generated."
0.5.0,latex_domain_indices = True
0.5.0,One entry per manual page. List of tuples
0.5.0,"(source start file, name, description, authors, manual section)."
0.5.0,"If true, show URL addresses after external links."
0.5.0,man_show_urls = False
0.5.0,-- Options for Texinfo output -------------------------------------------
0.5.0,Grouping the document tree into Texinfo files. List of tuples
0.5.0,"(source start file, target name, title, author,"
0.5.0,"dir menu entry, description, category)"
0.5.0,"def generate_example_rst(app, what, name, obj, options, lines):"
0.5.0,"# generate empty examples files, so that we don't get"
0.5.0,# inclusion errors if there are no examples for a class / module
0.5.0,"examples_path = os.path.join(app.srcdir, ""generated"","
0.5.0,"""%s.examples"" % name)"
0.5.0,if not os.path.exists(examples_path):
0.5.0,# touch file
0.5.0,"open(examples_path, 'w').close()"
0.5.0,Config for sphinx_issues
0.5.0,"app.connect('autodoc-process-docstring', generate_example_rst)"
0.5.0,Documents to append as an appendix to all manuals.
0.5.0,texinfo_appendices = []
0.5.0,"If false, no module index is generated."
0.5.0,texinfo_domain_indices = True
0.5.0,"How to display URL addresses: 'footnote', 'no', or 'inline'."
0.5.0,texinfo_show_urls = 'footnote'
0.5.0,"If true, do not generate a @detailmenu in the ""Top"" node's menu."
0.5.0,texinfo_no_detailmenu = False
0.5.0,The following is used by sphinx.ext.linkcode to provide links to github
0.5.0,get the styles from the current theme
0.5.0,create and add the button to all the code blocks that contain >>>
0.5.0,tracebacks (.gt) contain bare text elements that need to be
0.5.0,wrapped in a span to work with .nextUntil() (see later)
0.5.0,define the behavior of the button when it's clicked
0.5.0,hide the code output
0.5.0,show the code output
0.5.0,-*- coding: utf-8 -*-
0.5.0,Format template for issues URI
0.5.0,e.g. 'https://github.com/sloria/marshmallow/issues/{issue}
0.5.0,Format template for PR URI
0.5.0,e.g. 'https://github.com/sloria/marshmallow/pull/{issue}
0.5.0,Format template for commit URI
0.5.0,e.g. 'https://github.com/sloria/marshmallow/commits/{commit}
0.5.0,"Shortcut for Github, e.g. 'sloria/marshmallow'"
0.5.0,Format template for user profile URI
0.5.0,e.g. 'https://github.com/{user}'
0.5.0,Python 2 only
0.5.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.5.0,License: MIT
0.5.0,##############################################################################
0.5.0,"First, we will create an imbalanced data set from a the iris data set."
0.5.0,##############################################################################
0.5.0,Using ``sampling_strategy`` in resampling algorithms
0.5.0,##############################################################################
0.5.0,##############################################################################
0.5.0,``sampling_strategy`` as a ``float``
0.5.0,....................................
0.5.0,
0.5.0,``sampling_strategy`` can be given a ``float``. For **under-sampling
0.5.0,"methods**, it corresponds to the ratio :math:`\\alpha_{us}` defined by"
0.5.0,:math:`N_{rM} = \\alpha_{us} \\times N_{m}` where :math:`N_{rM}` and
0.5.0,:math:`N_{m}` are the number of samples in the majority class after
0.5.0,"resampling and the number of samples in the minority class, respectively."
0.5.0,select only 2 classes since the ratio make sense in this case
0.5.0,##############################################################################
0.5.0,"For **over-sampling methods**, it correspond to the ratio"
0.5.0,:math:`\\alpha_{os}` defined by :math:`N_{rm} = \\alpha_{os} \\times N_{M}`
0.5.0,where :math:`N_{rm}` and :math:`N_{M}` are the number of samples in the
0.5.0,minority class after resampling and the number of samples in the majority
0.5.0,"class, respectively."
0.5.0,##############################################################################
0.5.0,``sampling_strategy`` has a ``str``
0.5.0,...................................
0.5.0,
0.5.0,``sampling_strategy`` can be given as a string which specify the class
0.5.0,"targeted by the resampling. With under- and over-sampling, the number of"
0.5.0,samples will be equalized.
0.5.0,
0.5.0,Note that we are using multiple classes from now on.
0.5.0,##############################################################################
0.5.0,"With **cleaning method**, the number of samples in each class will not be"
0.5.0,equalized even if targeted.
0.5.0,##############################################################################
0.5.0,``sampling_strategy`` as a ``dict``
0.5.0,...................................
0.5.0,
0.5.0,"When ``sampling_strategy`` is a ``dict``, the keys correspond to the targeted"
0.5.0,classes. The values correspond to the desired number of samples for each
0.5.0,targeted class. This is working for both **under- and over-sampling**
0.5.0,algorithms but not for the **cleaning algorithms**. Use a ``list`` instead.
0.5.0,##############################################################################
0.5.0,``sampling_strategy`` as a ``list``
0.5.0,...................................
0.5.0,
0.5.0,"When ``sampling_strategy`` is a ``list``, the list contains the targeted"
0.5.0,classes. It is used only for **cleaning methods** and raise an error
0.5.0,otherwise.
0.5.0,##############################################################################
0.5.0,``sampling_strategy`` as a callable
0.5.0,...................................
0.5.0,
0.5.0,"When callable, function taking ``y`` and returns a ``dict``. The keys"
0.5.0,correspond to the targeted classes. The values correspond to the desired
0.5.0,number of samples for each class.
0.5.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.5.0,License: MIT
0.5.0,#############################################################################
0.5.0,Toy data generation
0.5.0,#############################################################################
0.5.0,#############################################################################
0.5.0,We are generating some non Gaussian data set contaminated with some unform
0.5.0,noise.
0.5.0,#############################################################################
0.5.0,We will generate some cleaned test data without outliers.
0.5.0,#############################################################################
0.5.0,How to use the :class:`imblearn.FunctionSampler`
0.5.0,#############################################################################
0.5.0,#############################################################################
0.5.0,We first define a function which will use
0.5.0,:class:`sklearn.ensemble.IsolationForest` to eliminate some outliers from
0.5.0,our dataset during training. The function passed to the
0.5.0,:class:`imblearn.FunctionSampler` will be called when using the method
0.5.0,``fit_resample``.
0.5.0,#############################################################################
0.5.0,Integrate it within a pipeline
0.5.0,#############################################################################
0.5.0,#############################################################################
0.5.0,"By elimnating outliers before the training, the classifier will be less"
0.5.0,affected during the prediction.
0.5.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.5.0,License: MIT
0.5.0,generate some data points
0.5.0,plot the majority and minority samples
0.5.0,draw the circle in which the new sample will generated
0.5.0,plot the line on which the sample will be generated
0.5.0,create and plot the new sample
0.5.0,make the plot nicer with legend and label
0.5.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.5.0,License: MIT
0.5.0,##############################################################################
0.5.0,The following function will be used to create toy dataset. It using the
0.5.0,``make_classification`` from scikit-learn but fixing some parameters.
0.5.0,##############################################################################
0.5.0,The following function will be used to plot the sample space after resampling
0.5.0,to illustrate the characterisitic of an algorithm.
0.5.0,make nice plotting
0.5.0,##############################################################################
0.5.0,The following function will be used to plot the decision function of a
0.5.0,classifier given some data.
0.5.0,##############################################################################
0.5.0,Illustration of the influence of the balancing ratio
0.5.0,##############################################################################
0.5.0,##############################################################################
0.5.0,We will first illustrate the influence of the balancing ratio on some toy
0.5.0,data using a linear SVM classifier. Greater is the difference between the
0.5.0,"number of samples in each class, poorer are the classfication results."
0.5.0,##############################################################################
0.5.0,Random over-sampling to balance the data set
0.5.0,##############################################################################
0.5.0,##############################################################################
0.5.0,Random over-sampling can be used to repeat some samples and balance the
0.5.0,number of samples between the dataset. It can be seen that with this trivial
0.5.0,approach the boundary decision is already less biaised toward the majority
0.5.0,class.
0.5.0,##############################################################################
0.5.0,More advanced over-sampling using ADASYN and SMOTE
0.5.0,##############################################################################
0.5.0,##############################################################################
0.5.0,"Instead of repeating the same samples when over-sampling, we can use some"
0.5.0,specific heuristic instead. ADASYN and SMOTE can be used in this case.
0.5.0,Make an identity sampler
0.5.0,##############################################################################
0.5.0,The following plot illustrate the difference between ADASYN and SMOTE. ADASYN
0.5.0,will focus on the samples which are difficult to classify with a
0.5.0,nearest-neighbors rule while regular SMOTE will not make any distinction.
0.5.0,"Therefore, the decision function depending of the algorithm."
0.5.0,##############################################################################
0.5.0,"Due to those sampling particularities, it can give rise to some specific"
0.5.0,issues as illustrated below.
0.5.0,##############################################################################
0.5.0,SMOTE proposes several variants by identifying specific samples to consider
0.5.0,during the resampling. The borderline version will detect which point to
0.5.0,select which are in the border between two classes. The SVM version will use
0.5.0,the support vectors found using an SVM algorithm to create new sample while
0.5.0,the KMeans version will make a clustering before to generate samples in each
0.5.0,cluster independently depending each cluster density.
0.5.0,##############################################################################
0.5.0,"When dealing with a mixed of continuous and categorical features, SMOTE-NC"
0.5.0,is the only method which can handle this case.
0.5.0,create a synthetic data set with continuous and categorical features
0.5.0,Authors: Christos Aridas
0.5.0,Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.5.0,License: MIT
0.5.0,Generate the dataset
0.5.0,make nice plotting
0.5.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.5.0,License: MIT
0.5.0,Generate a dataset
0.5.0,Split the data
0.5.0,Train the classifier with balancing
0.5.0,Test the classifier and get the prediction
0.5.0,Show the classification report
0.5.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.5.0,License: MIT
0.5.0,Generate a dataset
0.5.0,Split the data
0.5.0,Train the classifier with balancing
0.5.0,Test the classifier and get the prediction
0.5.0,##############################################################################
0.5.0,The geometric mean corresponds to the square root of the product of the
0.5.0,sensitivity and specificity. Combining the two metrics should account for
0.5.0,the balancing of the dataset.
0.5.0,##############################################################################
0.5.0,The index balanced accuracy can transform any metric to be used in
0.5.0,imbalanced learning problems.
0.5.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.5.0,License: MIT
0.5.0,##############################################################################
0.5.0,The following function will be used to create toy dataset. It using the
0.5.0,``make_classification`` from scikit-learn but fixing some parameters.
0.5.0,##############################################################################
0.5.0,The following function will be used to plot the sample space after resampling
0.5.0,to illustrate the characteristic of an algorithm.
0.5.0,make nice plotting
0.5.0,##############################################################################
0.5.0,The following function will be used to plot the decision function of a
0.5.0,classifier given some data.
0.5.0,##############################################################################
0.5.0,"``SMOTE`` allows to generate samples. However, this method of over-sampling"
0.5.0,"does not have any knowledge regarding the underlying distribution. Therefore,"
0.5.0,"some noisy samples can be generated, e.g. when the different classes cannot"
0.5.0,"be well separated. Hence, it can be beneficial to apply an under-sampling"
0.5.0,algorithm to clean the noisy samples. Two methods are usually used in the
0.5.0,literature: (i) Tomek's link and (ii) edited nearest neighbours cleaning
0.5.0,methods. Imbalanced-learn provides two ready-to-use samplers ``SMOTETomek``
0.5.0,"and ``SMOTEENN``. In general, ``SMOTEENN`` cleans more noisy data than"
0.5.0,``SMOTETomek``.
0.5.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.5.0,License: MIT
0.5.0,##############################################################################
0.5.0,Load an imbalanced dataset
0.5.0,##############################################################################
0.5.0,We will load the UCI SatImage dataset which has an imbalanced ratio of 9.3:1
0.5.0,(number of majority sample for a minority sample). The data are then split
0.5.0,into training and testing.
0.5.0,##############################################################################
0.5.0,Classification using a single decision tree
0.5.0,##############################################################################
0.5.0,We train a decision tree classifier which will be used as a baseline for the
0.5.0,rest of this example.
0.5.0,##############################################################################
0.5.0,The results are reported in terms of balanced accuracy and geometric mean
0.5.0,which are metrics widely used in the literature to validate model trained on
0.5.0,imbalanced set.
0.5.0,##############################################################################
0.5.0,Classification using bagging classifier with and without sampling
0.5.0,##############################################################################
0.5.0,"Instead of using a single tree, we will check if an ensemble of decsion tree"
0.5.0,"can actually alleviate the issue induced by the class imbalancing. First, we"
0.5.0,will use a bagging classifier and its counter part which internally uses a
0.5.0,random under-sampling to balanced each boostrap sample.
0.5.0,##############################################################################
0.5.0,Balancing each bootstrap sample allows to increase significantly the balanced
0.5.0,accuracy and the geometric mean.
0.5.0,##############################################################################
0.5.0,Classification using random forest classifier with and without sampling
0.5.0,##############################################################################
0.5.0,Random forest is another popular ensemble method and it is usually
0.5.0,"outperforming bagging. Here, we used a vanilla random forest and its balanced"
0.5.0,counterpart in which each bootstrap sample is balanced.
0.5.0,"Similarly to the previous experiment, the balanced classifier outperform the"
0.5.0,"classifier which learn from imbalanced bootstrap samples. In addition, random"
0.5.0,forest outsperforms the bagging classifier.
0.5.0,##############################################################################
0.5.0,Boosting classifier
0.5.0,##############################################################################
0.5.0,"In the same manner, easy ensemble classifier is a bag of balanced AdaBoost"
0.5.0,"classifier. However, it will be slower to train than random forest and will"
0.5.0,achieve worse performance.
0.5.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.5.0,License: MIT
0.5.0,##############################################################################
0.5.0,The following function will be used to create toy dataset. It using the
0.5.0,``make_classification`` from scikit-learn but fixing some parameters.
0.5.0,##############################################################################
0.5.0,The following function will be used to plot the sample space after resampling
0.5.0,to illustrate the characteristic of an algorithm.
0.5.0,make nice plotting
0.5.0,##############################################################################
0.5.0,The following function will be used to plot the decision function of a
0.5.0,classifier given some data.
0.5.0,##############################################################################
0.5.0,Prototype generation: under-sampling by generating new samples
0.5.0,##############################################################################
0.5.0,##############################################################################
0.5.0,``ClusterCentroids`` under-samples by replacing the original samples by the
0.5.0,centroids of the cluster found.
0.5.0,##############################################################################
0.5.0,Prototype selection: under-sampling by selecting existing samples
0.5.0,##############################################################################
0.5.0,##############################################################################
0.5.0,The algorithm performing prototype selection can be subdivided into two
0.5.0,groups: (i) the controlled under-sampling methods and (ii) the cleaning
0.5.0,under-sampling methods.
0.5.0,##############################################################################
0.5.0,"With the controlled under-sampling methods, the number of samples to be"
0.5.0,selected can be specified. ``RandomUnderSampler`` is the most naive way of
0.5.0,performing such selection by randomly selecting a given number of samples by
0.5.0,the targetted class.
0.5.0,##############################################################################
0.5.0,``NearMiss`` algorithms implement some heuristic rules in order to select
0.5.0,samples. NearMiss-1 selects samples from the majority class for which the
0.5.0,average distance of the :math:`k`` nearest samples of the minority class is
0.5.0,the smallest. NearMiss-2 selects the samples from the majority class for
0.5.0,which the average distance to the farthest samples of the negative class is
0.5.0,"the smallest. NearMiss-3 is a 2-step algorithm: first, for each minority"
0.5.0,"sample, their ::math:`m` nearest-neighbors will be kept; then, the majority"
0.5.0,samples selected are the on for which the average distance to the :math:`k`
0.5.0,nearest neighbors is the largest.
0.5.0,##############################################################################
0.5.0,``EditedNearestNeighbours`` removes samples of the majority class for which
0.5.0,their class differ from the one of their nearest-neighbors. This sieve can be
0.5.0,repeated which is the principle of the
0.5.0,``RepeatedEditedNearestNeighbours``. ``AllKNN`` is slightly different from
0.5.0,the ``RepeatedEditedNearestNeighbours`` by changing the :math:`k` parameter
0.5.0,"of the internal nearest neighors algorithm, increasing it at each iteration."
0.5.0,##############################################################################
0.5.0,``CondensedNearestNeighbour`` makes use of a 1-NN to iteratively decide if a
0.5.0,sample should be kept in a dataset or not. The issue is that
0.5.0,``CondensedNearestNeighbour`` is sensitive to noise by preserving the noisy
0.5.0,samples. ``OneSidedSelection`` also used the 1-NN and use ``TomekLinks`` to
0.5.0,remove the samples considered noisy. The ``NeighbourhoodCleaningRule`` use a
0.5.0,"``EditedNearestNeighbours`` to remove some sample. Additionally, they use a 3"
0.5.0,nearest-neighbors to remove samples which do not agree with this rule.
0.5.0,##############################################################################
0.5.0,``InstanceHardnessThreshold`` uses the prediction of classifier to exclude
0.5.0,samples. All samples which are classified with a low probability will be
0.5.0,removed.
0.5.0,##############################################################################
0.5.0,This function allows to make nice plotting
0.5.0,##############################################################################
0.5.0,Generate some data with one Tomek link
0.5.0,minority class
0.5.0,majority class
0.5.0,##############################################################################
0.5.0,"In the figure above, the samples highlighted in green form a Tomek link since"
0.5.0,they are of different classes and are nearest neighbours of each other.
0.5.0,highlight the samples of interest
0.5.0,##############################################################################
0.5.0,We can run the ``TomekLinks`` sampling to remove the corresponding
0.5.0,samples. If ``sampling_strategy='auto'`` only the sample from the majority
0.5.0,class will be removed. If ``sampling_strategy='all'`` both samples will be
0.5.0,removed.
0.5.0,highlight the samples of interest
0.5.0,##############################################################################
0.5.0,This function allows to make nice plotting
0.5.0,##############################################################################
0.5.0,We can start by generating some data to later illustrate the principle of
0.5.0,each NearMiss heuritic rules.
0.5.0,minority class
0.5.0,majority class
0.5.0,##############################################################################
0.5.0,NearMiss-1
0.5.0,##############################################################################
0.5.0,##############################################################################
0.5.0,NearMiss-1 selects samples from the majority class for which the average
0.5.0,distance to some nearest neighbours is the smallest. In the following
0.5.0,"example, we use a 3-NN to compute the average distance on 2 specific samples"
0.5.0,"of the majority class. Therefore, in this case the point linked by the"
0.5.0,green-dashed line will be selected since the average distance is smaller.
0.5.0,##############################################################################
0.5.0,NearMiss-2
0.5.0,##############################################################################
0.5.0,##############################################################################
0.5.0,NearMiss-2 selects samples from the majority class for which the average
0.5.0,distance to the farthest neighbors is the smallest. With the same
0.5.0,"configuration as previously presented, the sample linked to the green-dashed"
0.5.0,line will be selected since its distance the 3 farthest neighbors is the
0.5.0,smallest.
0.5.0,##############################################################################
0.5.0,NearMiss-3
0.5.0,##############################################################################
0.5.0,##############################################################################
0.5.0,"NearMiss-3 can be divided into 2 steps. First, a nearest-neighbors is used to"
0.5.0,short-list samples from the majority class (i.e. correspond to the
0.5.0,"highlighted samples in the following plot). Then, the sample with the largest"
0.5.0,average distance to the *k* nearest-neighbors are selected.
0.5.0,select only the majority point of interest
0.5.0,Authors: Christos Aridas
0.5.0,Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.5.0,License: MIT
0.5.0,Generate the dataset
0.5.0,Instanciate a PCA object for the sake of easy visualisation
0.5.0,Create the samplers
0.5.0,Create the classifier
0.5.0,Make the splits
0.5.0,Add one transformers and two samplers in the pipeline object
0.5.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.5.0,License: MIT
0.5.0,##############################################################################
0.5.0,Data loading
0.5.0,##############################################################################
0.5.0,##############################################################################
0.5.0,"First, you should download the Porto Seguro data set from Kaggle. See the"
0.5.0,link in the introduction.
0.5.0,##############################################################################
0.5.0,The data set is imbalanced and it will have an effect on the fitting.
0.5.0,##############################################################################
0.5.0,Define the pre-processing pipeline
0.5.0,##############################################################################
0.5.0,##############################################################################
0.5.0,We want to standard scale the numerical features while we want to one-hot
0.5.0,"encode the categorical features. In this regard, we make use of the"
0.5.0,:class:`sklearn.compose.ColumnTransformer`.
0.5.0,Create an environment variable to avoid using the GPU. This can be changed.
0.5.0,##############################################################################
0.5.0,Create a neural-network
0.5.0,##############################################################################
0.5.0,##############################################################################
0.5.0,We create a decorator to report the computation time
0.5.0,##############################################################################
0.5.0,The first model will be trained using the ``fit`` method and with imbalanced
0.5.0,mini-batches.
0.5.0,##############################################################################
0.5.0,"In the contrary, we will use imbalanced-learn to create a generator of"
0.5.0,mini-batches which will yield balanced mini-batches.
0.5.0,##############################################################################
0.5.0,Classification loop
0.5.0,##############################################################################
0.5.0,##############################################################################
0.5.0,We will perform a 10-fold cross-validation and train the neural-network with
0.5.0,the two different strategies previously presented.
0.5.0,##############################################################################
0.5.0,Plot of the results and computation time
0.5.0,##############################################################################
0.5.0,Authors: Christos Aridas
0.5.0,Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.5.0,License: MIT
0.5.0,Load the dataset
0.5.0,make nice plotting
0.5.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.5.0,License: MIT
0.5.0,Create a folder to fetch the dataset
0.5.0,Create a pipeline
0.5.0,Classify and report the results
0.5.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.5.0,License: MIT
0.5.0,##############################################################################
0.5.0,Setting the data set
0.5.0,##############################################################################
0.5.0,##############################################################################
0.5.0,We use a part of the 20 newsgroups data set by loading 4 topics. Using the
0.5.0,"scikit-learn loader, the data are split into a training and a testing set."
0.5.0,
0.5.0,Note the class \#3 is the minority class and has almost twice less samples
0.5.0,than the majority class.
0.5.0,##############################################################################
0.5.0,The usual scikit-learn pipeline
0.5.0,##############################################################################
0.5.0,##############################################################################
0.5.0,You might usually use scikit-learn pipeline by combining the TF-IDF
0.5.0,vectorizer to feed a multinomial naive bayes classifier. A classification
0.5.0,report summarized the results on the testing set.
0.5.0,
0.5.0,"As expected, the recall of the class \#3 is low mainly due to the class"
0.5.0,imbalanced.
0.5.0,##############################################################################
0.5.0,Balancing the class before classification
0.5.0,##############################################################################
0.5.0,##############################################################################
0.5.0,"To improve the prediction of the class \#3, it could be interesting to apply"
0.5.0,"a balancing before to train the naive bayes classifier. Therefore, we will"
0.5.0,use a ``RandomUnderSampler`` to equalize the number of samples in all the
0.5.0,classes before the training.
0.5.0,
0.5.0,It is also important to note that we are using the ``make_pipeline`` function
0.5.0,implemented in imbalanced-learn to properly handle the samplers.
0.5.0,##############################################################################
0.5.0,"Although the results are almost identical, it can be seen that the resampling"
0.5.0,allowed to correct the poor recall of the class \#3 at the cost of reducing
0.5.0,"the other metrics for the other classes. However, the overall results are"
0.5.0,slightly better.
0.5.0,Authors: Dayvid Oliveira
0.5.0,Christos Aridas
0.5.0,Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.5.0,License: MIT
0.5.0,Generate the dataset
0.5.0,"Two subplots, unpack the axes array immediately"
0.5.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.5.0,License: MIT
0.5.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.5.0,Christos Aridas
0.5.0,License: MIT
0.5.0,define an alias for back-compatibility
0.5.0,FIXME: remove in 0.6
0.5.0,FIXME: remove in 0.6
0.5.0,both ratio and sampling_strategy should not be set
0.5.0,Adapted from scikit-learn
0.5.0,Author: Edouard Duchesnay
0.5.0,Gael Varoquaux
0.5.0,Virgile Fritsch
0.5.0,Alexandre Gramfort
0.5.0,Lars Buitinck
0.5.0,Christos Aridas
0.5.0,Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.5.0,License: BSD
0.5.0,BaseEstimator interface
0.5.0,validate names
0.5.0,validate estimators
0.5.0,We allow last estimator to be None as an identity transformation
0.5.0,Estimator interface
0.5.0,Setup the memory
0.5.0,joblib >= 0.12
0.5.0,we do not clone when caching is disabled to
0.5.0,preserve backward compatibility
0.5.0,joblib < 0.11
0.5.0,we do not clone when caching is disabled to
0.5.0,preserve backward compatibility
0.5.0,Fit or load from cache the current transfomer
0.5.0,Replace the transformer of the step with the fitted
0.5.0,transformer. This is necessary when loading the transformer
0.5.0,from the cache.
0.5.0,"_final_estimator is None or has transform, otherwise attribute error"
0.5.0,raise AttributeError if necessary for hasattr behaviour
0.5.0,"if we have a weight for this transformer, multiply output"
0.5.0,Based on NiLearn package
0.5.0,License: simplified BSD
0.5.0,"PEP0440 compatible formatted version, see:"
0.5.0,https://www.python.org/dev/peps/pep-0440/
0.5.0,
0.5.0,Generic release markers:
0.5.0,X.Y
0.5.0,X.Y.Z # For bugfix releases
0.5.0,
0.5.0,Admissible pre-release markers:
0.5.0,X.YaN # Alpha release
0.5.0,X.YbN # Beta release
0.5.0,X.YrcN # Release Candidate
0.5.0,X.Y # Final release
0.5.0,
0.5.0,Dev branch marker is: 'X.Y.dev' or 'X.Y.devN' where N is an integer.
0.5.0,'X.Y.dev0' is the canonical version of 'X.Y.dev'
0.5.0,
0.5.0,coding: utf-8
0.5.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.5.0,Dariusz Brzezinski
0.5.0,License: MIT
0.5.0,Only negative labels
0.5.0,"Calculate tp_sum, pred_sum, true_sum ###"
0.5.0,labels are now from 0 to len(labels) - 1 -> use bincount
0.5.0,Pathological case
0.5.0,Compute the true negative
0.5.0,Retain only selected labels
0.5.0,"Finally, we have all our sufficient statistics. Divide! #"
0.5.0,"Divide, and on zero-division, set scores to 0 and warn:"
0.5.0,"Oddly, we may get an ""invalid"" rather than a ""divide"" error"
0.5.0,here.
0.5.0,Average the results
0.5.0,labels are now from 0 to len(labels) - 1 -> use bincount
0.5.0,Pathological case
0.5.0,Retain only selected labels
0.5.0,old version of scipy return MaskedConstant instead of 0.0
0.5.0,Create the list of tags
0.5.0,check that the scoring function does not need a score
0.5.0,and only a prediction
0.5.0,Compute the score from the scoring function
0.5.0,Square if desired
0.5.0,Get the signature of the sens/spec function
0.5.0,We need to extract from kwargs only the one needed by the
0.5.0,specificity and specificity
0.5.0,Make the intersection between the parameters
0.5.0,Create a sub dictionary
0.5.0,Check if the metric is the geometric mean
0.5.0,We do not support multilabel so the only average supported
0.5.0,is binary
0.5.0,Create the list of parameters through signature binding
0.5.0,Call the sens/spec function
0.5.0,Compute the dominance
0.5.0,Compute the different metrics
0.5.0,Precision/recall/f1
0.5.0,Specificity
0.5.0,Geometric mean
0.5.0,Index balanced accuracy
0.5.0,compute averages
0.5.0,coding: utf-8
0.5.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.5.0,Christos Aridas
0.5.0,License: MIT
0.5.0,##############################################################################
0.5.0,Utilities for testing
0.5.0,import some data to play with
0.5.0,restrict to a binary classification task
0.5.0,add noisy features to make the problem harder and avoid perfect results
0.5.0,"run classifier, get class probabilities and label predictions"
0.5.0,only interested in probabilities of the positive case
0.5.0,XXX: do we really want a special API for the binary case?
0.5.0,##############################################################################
0.5.0,Tests
0.5.0,detailed measures for each class
0.5.0,individual scoring function that can be used for grid search: in the
0.5.0,binary class case the score is the value of the measure for the positive
0.5.0,class (e.g. label == 1). This is deprecated for average != 'binary'.
0.5.0,Such a case may occur with non-stratified cross-validation
0.5.0,ensure the above were meaningful tests:
0.5.0,Bad pos_label
0.5.0,Bad average option
0.5.0,but average != 'binary'; even if data is binary
0.5.0,compute the geometric mean for the binary problem
0.5.0,print classification report with class names
0.5.0,print classification report with label detection
0.5.0,print classification report with class names
0.5.0,print classification report with label detection
0.5.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.5.0,Christos Aridas
0.5.0,License: MIT
0.5.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.5.0,Christos Aridas
0.5.0,License: MIT
0.5.0,The ratio is computed using a one-vs-rest manner. Using majority
0.5.0,in multi-class would lead to slightly different results at the
0.5.0,cost of introducing a new parameter.
0.5.0,the nearest neighbors need to be fitted only on the current class
0.5.0,to find the class NN to generate new samples
0.5.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.5.0,Christos Aridas
0.5.0,License: MIT
0.5.0,TODO: remove the str tag once the following PR is merged:
0.5.0,https://github.com/scikit-learn/scikit-learn/pull/14043
0.5.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.5.0,Christos Aridas
0.5.0,License: MIT
0.5.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.5.0,Fernando Nogueira
0.5.0,Christos Aridas
0.5.0,Dzianis Dudnik
0.5.0,License: MIT
0.5.0,FIXME: remove in 0.6
0.5.0,Samples are in danger for m/2 <= m' < m
0.5.0,Samples are noise for m = m'
0.5.0,FIXME: rename _sample -> _fit_resample in 0.6
0.5.0,divergence between borderline-1 and borderline-2
0.5.0,Create synthetic samples for borderline points.
0.5.0,only minority
0.5.0,we use a one-vs-rest policy to handle the multiclass in which
0.5.0,new samples will be created considering not only the majority
0.5.0,class but all over classes.
0.5.0,FIXME: rename _sample -> _fit_resample in 0.6
0.5.0,"FIXME: In 0.6, SMOTE should inherit only from BaseSMOTE."
0.5.0,FIXME: in 0.6 call super()
0.5.0,FIXME: in 0.6 call super()
0.5.0,FIXME: remove in 0.6 after deprecation cycle
0.5.0,FIXME: to be removed in 0.6
0.5.0,FIXME: uncomment in version 0.6
0.5.0,self._validate_estimator()
0.5.0,@Substitution(
0.5.0,"sampling_strategy=BaseOverSampler._sampling_strategy_docstring,"
0.5.0,random_state=_random_state_docstring)
0.5.0,compute the median of the standard deviation of the minority class
0.5.0,the input of the OneHotEncoder needs to be dense
0.5.0,we can replace the 1 entries of the categorical features with the
0.5.0,median of the standard deviation. It will ensure that whenever
0.5.0,"distance is computed between 2 samples, the difference will be equal"
0.5.0,to the median of the standard deviation as in the original paper.
0.5.0,reverse the encoding of the categorical features
0.5.0,the matrix is supposed to be in the CSR format after the stacking
0.5.0,"To avoid conversion and since there is only few samples used, we"
0.5.0,convert those samples to dense array.
0.5.0,tie breaking argmax
0.5.0,validate the parameters
0.5.0,negate diagonal elements
0.5.0,FIXME: rename _sample -> _fit_resample in 0.6
0.5.0,target_class_indices = np.flatnonzero(y == class_sample)
0.5.0,"X_class = safe_indexing(X, target_class_indices)"
0.5.0,identify cluster which are answering the requirements
0.5.0,the cluster is already considered balanced
0.5.0,not enough samples to apply SMOTE
0.5.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.5.0,Christos Aridas
0.5.0,License: MIT
0.5.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.5.0,Christos Aridas
0.5.0,Dzianis Dudnik
0.5.0,License: MIT
0.5.0,create 2 random continuous feature
0.5.0,create a categorical feature using some string
0.5.0,create a categorical feature using some integer
0.5.0,return the categories
0.5.0,create 2 random continuous feature
0.5.0,create a categorical feature using some string
0.5.0,create a categorical feature using some integer
0.5.0,return the categories
0.5.0,create 2 random continuous feature
0.5.0,create a categorical feature using some string
0.5.0,create a categorical feature using some integer
0.5.0,return the categories
0.5.0,create 2 random continuous feature
0.5.0,create a categorical feature using some string
0.5.0,create a categorical feature using some integer
0.5.0,return the categories
0.5.0,create 2 random continuous feature
0.5.0,create a categorical feature using some string
0.5.0,create a categorical feature using some integer
0.5.0,part of the common test which apply to SMOTE-NC even if it is not default
0.5.0,constructible
0.5.0,Check that the samplers handle pandas dataframe and pandas series
0.5.0,Cast X and y to not default dtype
0.5.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.5.0,Christos Aridas
0.5.0,License: MIT
0.5.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.5.0,Christos Aridas
0.5.0,License: MIT
0.5.0,check that m_neighbors is properly set. Regression test for:
0.5.0,https://github.com/scikit-learn-contrib/imbalanced-learn/issues/568
0.5.0,FIXME: Remove in 0.6
0.5.0,shuffle the indices since the sampler are packing them by class
0.5.0,helper functions
0.5.0,input and output
0.5.0,build the model and weights
0.5.0,"build the loss, predict, and train operator"
0.5.0,Initialization of all variables in the graph
0.5.0,"For each epoch, run accuracy on train and test"
0.5.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.5.0,License: MIT
0.5.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.5.0,Fernando Nogueira
0.5.0,Christos Aridas
0.5.0,License: MIT
0.5.0,find which class to not consider
0.5.0,there is a Tomek link between two samples if they are both nearest
0.5.0,neighbors of each others.
0.5.0,check for deprecated random_state
0.5.0,Find the nearest neighbour of every point
0.5.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.5.0,Christos Aridas
0.5.0,License: MIT
0.5.0,Randomly get one sample from the majority class
0.5.0,Generate the index to select
0.5.0,Create the set C - One majority samples and all minority
0.5.0,Create the set S - all majority samples
0.5.0,fit knn on C
0.5.0,Check each sample in S if we keep it or drop it
0.5.0,Do not select sample which are already well classified
0.5.0,Classify on S
0.5.0,If the prediction do not agree with the true label
0.5.0,append it in C_x
0.5.0,Keep the index for later
0.5.0,Update C
0.5.0,fit a knn on C
0.5.0,This experimental to speed up the search
0.5.0,Classify all the element in S and avoid to test the
0.5.0,well classified elements
0.5.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.5.0,Dayvid Oliveira
0.5.0,Christos Aridas
0.5.0,License: MIT
0.5.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.5.0,Christos Aridas
0.5.0,License: MIT
0.5.0,Compute the distance considering the farthest neighbour
0.5.0,Sort the list of distance and get the index
0.5.0,Throw a warning to tell the user that we did not have enough samples
0.5.0,to select and that we just select everything
0.5.0,Select the desired number of samples
0.5.0,check for deprecated random_state
0.5.0,idx_tmp is relative to the feature selected in the
0.5.0,previous step and we need to find the indirection
0.5.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.5.0,Christos Aridas
0.5.0,License: MIT
0.5.0,select a sample from the current class
0.5.0,create the set composed of all minority samples and one
0.5.0,sample from the current class.
0.5.0,create the set S with removing the seed from S
0.5.0,since that it will be added anyway
0.5.0,apply Tomek cleaning
0.5.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.5.0,Dayvid Oliveira
0.5.0,Christos Aridas
0.5.0,License: MIT
0.5.0,check for deprecated random_state
0.5.0,check for deprecated random_state
0.5.0,Check the stopping criterion
0.5.0,1. If there is no changes for the vector y
0.5.0,2. If the number of samples in the other class become inferior to
0.5.0,the number of samples in the majority class
0.5.0,3. If one of the class is disappearing
0.5.0,Case 1
0.5.0,Case 2
0.5.0,Case 3
0.5.0,check for deprecated random_state
0.5.0,Check the stopping criterion
0.5.0,1. If the number of samples in the other class become inferior to
0.5.0,the number of samples in the majority class
0.5.0,2. If one of the class is disappearing
0.5.0,Case 1else:
0.5.0,overwrite b_min_bec_maj
0.5.0,Case 2
0.5.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.5.0,Christos Aridas
0.5.0,License: MIT
0.5.0,check for deprecated random_state
0.5.0,clean the neighborhood
0.5.0,compute which classes to consider for cleaning for the A2 group
0.5.0,compute a2 group
0.5.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.5.0,Christos Aridas
0.5.0,License: MIT
0.5.0,TODO: remove the str tag once the following PR is merged:
0.5.0,https://github.com/scikit-learn/scikit-learn/pull/14043
0.5.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.5.0,Christos Aridas
0.5.0,License: MIT
0.5.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.5.0,Christos Aridas
0.5.0,License: MIT
0.5.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.5.0,Christos Aridas
0.5.0,License: MIT
0.5.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.5.0,Christos Aridas
0.5.0,License: MIT
0.5.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.5.0,Christos Aridas
0.5.0,License: MIT
0.5.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.5.0,Christos Aridas
0.5.0,License: MIT
0.5.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.5.0,Christos Aridas
0.5.0,License: MIT
0.5.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.5.0,Christos Aridas
0.5.0,License: MIT
0.5.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.5.0,Christos Aridas
0.5.0,License: MIT
0.5.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.5.0,Christos Aridas
0.5.0,License: MIT
0.5.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.5.0,Fernando Nogueira
0.5.0,Christos Aridas
0.5.0,License: MIT
0.5.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.5.0,Christos Aridas
0.5.0,License: MIT
0.5.0,test that all_estimators doesn't find abstract classes.
0.5.0,don't run twice the sampler tests. Meta-estimator do not have a
0.5.0,fit_resample method.
0.5.0,input validation etc for non-meta estimators
0.5.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.5.0,License: MIT
0.5.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.5.0,Christos Aridas
0.5.0,License: MIT
0.5.0,store timestamp to figure out whether the result of 'fit' has been
0.5.0,cached or not
0.5.0,store timestamp to figure out whether the result of 'fit' has been
0.5.0,cached or not
0.5.0,Pipeline accepts steps as tuple
0.5.0,Test the various init parameters of the pipeline.
0.5.0,Check that we can't instantiate pipelines with objects without fit
0.5.0,method
0.5.0,Smoke test with only an estimator
0.5.0,Check that params are set
0.5.0,Smoke test the repr:
0.5.0,Test with two objects
0.5.0,Check that we can't instantiate with non-transformers on the way
0.5.0,"Note that NoTrans implements fit, but not transform"
0.5.0,Check that params are set
0.5.0,Smoke test the repr:
0.5.0,Check that params are not set when naming them wrong
0.5.0,Test clone
0.5.0,"Check that apart from estimators, the parameters are the same"
0.5.0,Remove estimators that where copied
0.5.0,Test the various methods of the pipeline (anova).
0.5.0,Test with Anova + LogisticRegression
0.5.0,Test that the pipeline can take fit parameters
0.5.0,classifier should return True
0.5.0,and transformer params should not be changed
0.5.0,invalid parameters should raise an error message
0.5.0,Pipeline should pass sample_weight
0.5.0,When sample_weight is None it shouldn't be passed
0.5.0,Test pipeline raises set params error message for nested models.
0.5.0,nested model check
0.5.0,Test the various methods of the pipeline (pca + svm).
0.5.0,Test with PCA + SVC
0.5.0,Test the various methods of the pipeline (preprocessing + svm).
0.5.0,check shapes of various prediction functions
0.5.0,test that the fit_predict method is implemented on a pipeline
0.5.0,test that the fit_predict on pipeline yields same results as applying
0.5.0,transform and clustering steps separately
0.5.0,"As pipeline doesn't clone estimators on construction,"
0.5.0,it must have its own estimators
0.5.0,first compute the transform and clustering step separately
0.5.0,use a pipeline to do the transform and clustering in one step
0.5.0,tests that a pipeline does not have fit_predict method when final
0.5.0,step of pipeline does not have fit_predict defined
0.5.0,tests that Pipeline passes fit_params to intermediate steps
0.5.0,when fit_predict is invoked
0.5.0,Test whether pipeline works with a transformer at the end.
0.5.0,Also test pipeline.transform and pipeline.inverse_transform
0.5.0,test transform and fit_transform:
0.5.0,Test whether pipeline works with a transformer missing fit_transform
0.5.0,test fit_transform:
0.5.0,Directly setting attr
0.5.0,Using set_params
0.5.0,Using set_params to replace single step
0.5.0,With invalid data
0.5.0,Test setting Pipeline steps to None
0.5.0,"for other methods, ensure no AttributeErrors on None:"
0.5.0,mult2 and mult3 are active
0.5.0,Check 'passthrough' step at construction time
0.5.0,Test that an error is raised when memory is not a string or a Memory
0.5.0,instance
0.5.0,Define memory as an integer
0.5.0,Test with Transformer + SVC
0.5.0,Memoize the transformer at the first fit
0.5.0,Get the time stamp of the tranformer in the cached pipeline
0.5.0,Check that cached_pipe and pipe yield identical results
0.5.0,Check that we are reading the cache while fitting
0.5.0,a second time
0.5.0,Check that cached_pipe and pipe yield identical results
0.5.0,Create a new pipeline with cloned estimators
0.5.0,Check that even changing the name step does not affect the cache hit
0.5.0,Check that cached_pipe and pipe yield identical results
0.5.0,Test with Transformer + SVC
0.5.0,Memoize the transformer at the first fit
0.5.0,Get the time stamp of the tranformer in the cached pipeline
0.5.0,Check that cached_pipe and pipe yield identical results
0.5.0,Check that we are reading the cache while fitting
0.5.0,a second time
0.5.0,Check that cached_pipe and pipe yield identical results
0.5.0,Create a new pipeline with cloned estimators
0.5.0,Check that even changing the name step does not affect the cache hit
0.5.0,Check that cached_pipe and pipe yield identical results
0.5.0,Test the various methods of the pipeline (pca + svm).
0.5.0,Test with PCA + SVC
0.5.0,Test the various methods of the pipeline (pca + svm).
0.5.0,Test with PCA + SVC
0.5.0,Test whether pipeline works with a sampler at the end.
0.5.0,Also test pipeline.sampler
0.5.0,test transform and fit_transform:
0.5.0,We round the value near to zero. It seems that PCA has some issue
0.5.0,with that
0.5.0,Test whether pipeline works with a sampler at the end.
0.5.0,Also test pipeline.sampler
0.5.0,Test pipeline using None as preprocessing step and a classifier
0.5.0,"Test pipeline using None, RUS and a classifier"
0.5.0,"Test pipeline using RUS, None and a classifier"
0.5.0,Test pipeline using None step and a sampler
0.5.0,Test pipeline using None and a transformer that implements transform and
0.5.0,inverse_transform
0.5.0,Test the various methods of the pipeline (anova).
0.5.0,Test with RandomUnderSampling + Anova + LogisticRegression
0.5.0,Test the various methods of the pipeline (anova).
0.5.0,Test the various methods of the pipeline (anova).
0.5.0,Test with RandomUnderSampling + Anova + LogisticRegression
0.5.0,tests that Pipeline passes predict_params to the final estimator
0.5.0,when predict is invoked
0.5.0,Test that the score_samples method is implemented on a pipeline.
0.5.0,Test that the score_samples method on pipeline yields same results as
0.5.0,applying transform and score_samples steps separately.
0.5.0,Check the shapes
0.5.0,Check the values
0.5.0,Test that a pipeline does not have score_samples method when the final
0.5.0,step of the pipeline does not have score_samples defined.
0.5.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.5.0,Christos Aridas
0.5.0,License: MIT
0.5.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.5.0,License: MIT
0.5.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.5.0,License: MIT
0.5.0,Adapated from scikit-learn
0.5.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.5.0,License: MIT
0.5.0,FIXME: remove in 0.6
0.5.0,trigger our checks if this is a SamplerMixin
0.5.0,scikit-learn common tests
0.5.0,should raise warning if the target is continuous (we cannot raise error)
0.5.0,FIXME: in 0.6 set the random_state for all
0.5.0,if the target is multilabel then we should raise an error
0.5.0,IHT does not enforce the number of samples but provide a number
0.5.0,of samples the closest to the desired target.
0.5.0,FIXME remove in 0.6 -> ratio will be deprecated
0.5.0,in this test we will force all samplers to not change the class 1
0.5.0,in this test we will force all samplers to not change the class 1
0.5.0,check that sparse matrices can be passed through the sampler leading to
0.5.0,the same results than dense
0.5.0,set KMeans to full since it support sparse and dense
0.5.0,FIXME: in 0.6 set the random_state for all
0.5.0,Check that the samplers handle pandas dataframe and pandas series
0.5.0,FIXME: in 0.6 set the random_state for all
0.5.0,Check that multiclass target lead to the same results than OVA encoding
0.5.0,FIXME: in 0.6 set the random_state for all
0.5.0,Cast X and y to not default dtype
0.5.0,FIXME: in 0.6 set the random_state for all
0.5.0,Adapted from scikit-learn
0.5.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.5.0,License: MIT
0.5.0,meta-estimators need another estimator to be instantiated.
0.5.0,estimators that there is no way to default-construct sensibly
0.5.0,some strange ones
0.5.0,get parent folder
0.5.0,get rid of abstract base classes
0.5.0,get rid of sklearn estimators which have been imported in some classes
0.5.0,possibly get rid of meta estimators
0.5.0,"drop duplicates, sort for reproducibility"
0.5.0,itemgetter is used to ensure the sort does not extend to the 2nd item of
0.5.0,the tuple
0.5.0,Author: Alexander L. Hayes <hayesall@iu.edu>
0.5.0,License: MIT
0.5.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.5.0,License: MIT
0.5.0,check that all keys in sampling_strategy are also in y
0.5.0,check that there is no negative number
0.5.0,FIXME: Turn into an error in 0.6
0.5.0,clean-sampling can be more permissive since those samplers do not
0.5.0,use samples
0.5.0,check that all keys in sampling_strategy are also in y
0.5.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.5.0,Christos Aridas
0.5.0,License: MIT
0.5.0,this function could create an equal number of samples
0.5.0,We pass on purpose a non sorted dictionary and check that the resulting
0.5.0,dictionary is sorted. Refer to issue #428.
0.5.0,Author: Alexander L. Hayes <hayesall@iu.edu>
0.5.0,License: MIT
0.5.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.5.0,License: MIT
0.5.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.5.0,Christos Aridas
0.5.0,License: MIT
0.5.0,check if the filtering is working with a list or a single string
0.5.0,check that all estimators are sampler
0.5.0,check that an error is raised when the type is unknown
0.5.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.5.0,License: MIT
0.5.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.5.0,Christos Aridas
0.5.0,License: MIT
0.5.0,Otherwise create a default SMOTE
0.5.0,Otherwise create a default TomekLinks
0.5.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.5.0,Christos Aridas
0.5.0,License: MIT
0.5.0,Otherwise create a default SMOTE
0.5.0,Otherwise create a default EditedNearestNeighbours
0.5.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.5.0,Christos Aridas
0.5.0,License: MIT
0.5.0,Check if default job count is 1
0.5.0,Check if job count is set
0.5.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.5.0,Christos Aridas
0.5.0,License: MIT
0.5.0,Check if default job count is 1
0.5.0,Check if job count is set
0.5.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.5.0,License: MIT
0.5.0,resample before to fit the tree
0.5.0,Validate or convert input data
0.5.0,Pre-sort indices to avoid that each individual tree of the
0.5.0,ensemble sorts the indices.
0.5.0,Remap output
0.5.0,reshape is necessary to preserve the data contiguity against vs
0.5.0,"[:, np.newaxis] that does not."
0.5.0,Check parameters
0.5.0,"Free allocated memory, if any"
0.5.0,We draw from the random state to get the random state we
0.5.0,would have got if we hadn't used a warm_start.
0.5.0,Parallel loop: we prefer the threading backend as the Cython code
0.5.0,for fitting the trees is internally releasing the Python GIL
0.5.0,making threading more efficient than multiprocessing in
0.5.0,"that case. However, we respect any parallel_backend contexts set"
0.5.0,"at a higher level, since correctness does not rely on using"
0.5.0,threads.
0.5.0,Collect newly grown trees
0.5.0,Create pipeline with the fitted samplers and trees
0.5.0,Decapsulate classes_ attributes
0.5.0,Instances incorrectly classified
0.5.0,Error fraction
0.5.0,Stop if classification is perfect
0.5.0,Construct y coding as described in Zhu et al [2]:
0.5.0,
0.5.0,y_k = 1 if c == k else -1 / (K - 1)
0.5.0,
0.5.0,"where K == n_classes_ and c, k in [0, K) are indices along the second"
0.5.0,axis of the y coding with c being the index corresponding to the true
0.5.0,class label.
0.5.0,Displace zero probabilities so the log is defined.
0.5.0,Also fix negative elements which may occur with
0.5.0,negative sample weights.
0.5.0,Boost weight using multi-class AdaBoost SAMME.R alg
0.5.0,Only boost the weights if it will fit again
0.5.0,Only boost positive weights
0.5.0,Instances incorrectly classified
0.5.0,Error fraction
0.5.0,Stop if classification is perfect
0.5.0,Stop if the error is at least as bad as random guessing
0.5.0,Boost weight using multi-class AdaBoost SAMME alg
0.5.0,Only boost the weights if I will fit again
0.5.0,Only boost positive weights
0.5.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.5.0,Christos Aridas
0.5.0,License: MIT
0.5.0,doctest: +ELLIPSIS
0.5.0,array to know which samples are available to be taken
0.5.0,where the different set will be stored
0.5.0,store the index of the data to under-sample
0.5.0,value which will be picked at each round
0.5.0,extract the data of interest for this round from the
0.5.0,current class
0.5.0,select randomly the desired features
0.5.0,store the set created
0.5.0,fit and predict using cross validation
0.5.0,extract the prediction about the targeted classes only
0.5.0,check the stopping criterion
0.5.0,check that there is enough samples for another round
0.5.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.5.0,License: MIT
0.5.0,Ensemble are a bit specific since they are returning an array of
0.5.0,resampled arrays.
0.5.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.5.0,Christos Aridas
0.5.0,License: MIT
0.5.0,RandomUnderSampler is not supporting sample_weight. We need to pass
0.5.0,None.
0.5.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.5.0,Christos Aridas
0.5.0,License: MIT
0.5.0,RandomUnderSampler is not supporting sample_weight. We need to pass
0.5.0,None.
0.5.0,check that we have an ensemble of samplers and estimators with a
0.5.0,consistent size
0.5.0,each sampler in the ensemble should have different random state
0.5.0,each estimator in the ensemble should have different random state
0.5.0,check the consistency of the feature importances
0.5.0,check the consistency of the prediction outpus
0.5.0,Predictions should be the same when sample_weight are all ones
0.5.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.5.0,Christos Aridas
0.5.0,License: MIT
0.5.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.5.0,Christos Aridas
0.5.0,License: MIT
0.5.0,Check classification for various parameter settings.
0.5.0,Test that bootstrapping samples generate non-perfect base estimators.
0.5.0,"without bootstrap, all trees are perfect on the training set"
0.5.0,disable the resampling by passing an empty dictionary.
0.5.0,"with bootstrap, trees are no longer perfect on the training set"
0.5.0,Test that bootstrapping features may generate duplicate features.
0.5.0,Predict probabilities.
0.5.0,Normal case
0.5.0,"Degenerate case, where some classes are missing"
0.5.0,Check that oob prediction is a good estimation of the generalization
0.5.0,error.
0.5.0,Test with few estimators
0.5.0,Check singleton ensembles.
0.5.0,Test that it gives proper exception on deficient input.
0.5.0,Test n_estimators
0.5.0,Test max_samples
0.5.0,Test max_features
0.5.0,Test support of decision_function
0.5.0,Check that bagging ensembles can be grid-searched.
0.5.0,Transform iris into a binary classification task
0.5.0,Grid search with scoring based on decision_function
0.5.0,Check base_estimator and its default values.
0.5.0,Test if fitting incrementally with warm start gives a forest of the
0.5.0,right size and the same results as a normal fit.
0.5.0,Test if warm start'ed second fit with smaller n_estimators raises error.
0.5.0,Test that nothing happens when fitting without increasing n_estimators
0.5.0,"modify X to nonsense values, this should not change anything"
0.5.0,warm started classifier with 5+5 estimators should be equivalent to
0.5.0,one classifier with 10 estimators
0.5.0,Check using oob_score and warm_start simultaneously fails
0.5.0,"Make sure OOB scores are identical when random_state, estimator, and"
0.5.0,training data are fixed and fitting is done twice
0.5.0,Check that format of estimators_samples_ is correct and that results
0.5.0,generated at fit time can be identically reproduced at a later time
0.5.0,using data saved in object attributes.
0.5.0,remap the y outside of the BalancedBaggingclassifier
0.5.0,"_, y = np.unique(y, return_inverse=True)"
0.5.0,Get relevant attributes
0.5.0,Test for correct formatting
0.5.0,Re-fit single estimator to test for consistent sampling
0.5.0,Make sure validated max_samples and original max_samples are identical
0.5.0,when valid integer max_samples supplied by user
0.5.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.5.0,Christos Aridas
0.5.0,License: MIT
0.5.0,Generate a global dataset to use
0.5.0,Define a sampling_strategy
0.5.0,Define the sampling_strategy parameter
0.5.0,Create the sampling object
0.5.0,Get the different subset
0.5.0,Define the sampling_strategy parameter
0.5.0,Create the sampling object
0.5.0,Get the different subset
0.5.0,Define the sampling_strategy parameter
0.5.0,Create the sampling object
0.5.0,Get the different subset
0.5.0,Check classification for various parameter settings.
0.5.0,test the different prediction function
0.5.0,Check base_estimator and its default values.
0.5.0,Test if fitting incrementally with warm start gives a forest of the
0.5.0,right size and the same results as a normal fit.
0.5.0,Test if warm start'ed second fit with smaller n_estimators raises error.
0.5.0,Test that nothing happens when fitting without increasing n_estimators
0.5.0,"modify X to nonsense values, this should not change anything"
0.5.0,warm started classifier with 5+5 estimators should be equivalent to
0.5.0,one classifier with 10 estimators
0.5.0,Check warning if not enough estimators
0.5.0,Author: Guillaume Lemaitre
0.5.0,License: BSD 3 clause
0.5.0,"The index start at one, then we need to remove one"
0.5.0,to not have issue with the indexing.
0.5.0,go through the list and check if the data are available
0.5.0,Authors: Dayvid Oliveira
0.5.0,Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.5.0,Christos Aridas
0.5.0,License: MIT
0.5.0,restrict ratio to be a dict or a callable
0.5.0,FIXME remove ratio at 0.6
0.5.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.5.0,Christos Aridas
0.5.0,License: MIT
0.5.0,check an error is raised with we don't pass sampling_strategy and ratio
0.5.0,"we are reusing part of utils.check_sampling_strategy, however this is not"
0.5.0,cover in the common tests so we will repeat it here
0.5.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.5.0,Christos Aridas
0.5.0,License: MIT
0.5.0,This is a trick to avoid an error during tests collection with pytest. We
0.5.0,avoid the error when importing the package raise the error at the moment of
0.5.0,creating the instance.
0.5.0,flag for keras sequence duck-typing
0.5.0,FIXME: Remove in 0.6
0.5.0,shuffle the indices since the sampler are packing them by class
0.4.3,This file is here so that when running from the root folder
0.4.3,./sklearn is added to sys.path by pytest.
0.4.3,See https://docs.pytest.org/en/latest/pythonpath.html for more details.
0.4.3,"For example, this allows to build extensions in place and run pytest"
0.4.3,doc/modules/clustering.rst and use sklearn from the local folder
0.4.3,rather than the one from site-packages.
0.4.3,Set numpy array str/repr to legacy behaviour on numpy > 1.13 to make
0.4.3,the doctests pass
0.4.3,! /usr/bin/env python
0.4.3,get __version__ from _version.py
0.4.3,-*- coding: utf-8 -*-
0.4.3,
0.4.3,"imbalanced-learn documentation build configuration file, created by"
0.4.3,sphinx-quickstart on Mon Jan 18 14:44:12 2016.
0.4.3,
0.4.3,This file is execfile()d with the current directory set to its
0.4.3,containing dir.
0.4.3,
0.4.3,Note that not all possible configuration values are present in this
0.4.3,autogenerated file.
0.4.3,
0.4.3,All configuration values have a default; values that are commented out
0.4.3,serve to show the default.
0.4.3,"If extensions (or modules to document with autodoc) are in another directory,"
0.4.3,add these directories to sys.path here. If the directory is relative to the
0.4.3,"documentation root, use os.path.abspath to make it absolute, like shown here."
0.4.3,-- General configuration ------------------------------------------------
0.4.3,"If your documentation needs a minimal Sphinx version, state it here."
0.4.3,needs_sphinx = '1.0'
0.4.3,"Add any Sphinx extension module names here, as strings. They can be"
0.4.3,extensions coming with Sphinx (named 'sphinx.ext.*') or your custom
0.4.3,ones.
0.4.3,this is needed for some reason...
0.4.3,see https://github.com/numpy/numpydoc/issues/69
0.4.3,pngmath / imgmath compatibility layer for different sphinx versions
0.4.3,"Add any paths that contain templates here, relative to this directory."
0.4.3,generate autosummary even if no references
0.4.3,The suffix of source filenames.
0.4.3,The encoding of source files.
0.4.3,source_encoding = 'utf-8-sig'
0.4.3,Generate the plot for the gallery
0.4.3,The master toctree document.
0.4.3,General information about the project.
0.4.3,"The version info for the project you're documenting, acts as replacement for"
0.4.3,"|version| and |release|, also used in various other places throughout the"
0.4.3,built documents.
0.4.3,
0.4.3,The short X.Y version.
0.4.3,"The full version, including alpha/beta/rc tags."
0.4.3,The language for content autogenerated by Sphinx. Refer to documentation
0.4.3,for a list of supported languages.
0.4.3,language = None
0.4.3,"There are two options for replacing |today|: either, you set today to some"
0.4.3,"non-false value, then it is used:"
0.4.3,today = ''
0.4.3,"Else, today_fmt is used as the format for a strftime call."
0.4.3,"today_fmt = '%B %d, %Y'"
0.4.3,"List of patterns, relative to source directory, that match files and"
0.4.3,directories to ignore when looking for source files.
0.4.3,The reST default role (used for this markup: `text`) to use for all
0.4.3,documents.
0.4.3,default_role = None
0.4.3,"If true, '()' will be appended to :func: etc. cross-reference text."
0.4.3,"If true, the current module name will be prepended to all description"
0.4.3,unit titles (such as .. function::).
0.4.3,add_module_names = True
0.4.3,"If true, sectionauthor and moduleauthor directives will be shown in the"
0.4.3,output. They are ignored by default.
0.4.3,show_authors = False
0.4.3,The name of the Pygments (syntax highlighting) style to use.
0.4.3,Custom style
0.4.3,A list of ignored prefixes for module index sorting.
0.4.3,modindex_common_prefix = []
0.4.3,"If true, keep warnings as ""system message"" paragraphs in the built documents."
0.4.3,keep_warnings = False
0.4.3,-- Options for HTML output ----------------------------------------------
0.4.3,The theme to use for HTML and HTML Help pages.  See the documentation for
0.4.3,a list of builtin themes.
0.4.3,Theme options are theme-specific and customize the look and feel of a theme
0.4.3,"further.  For a list of options available for each theme, see the"
0.4.3,documentation.
0.4.3,html_theme_options = {'prev_next_buttons_location': None}
0.4.3,"Add any paths that contain custom themes here, relative to this directory."
0.4.3,"The name for this set of Sphinx documents.  If None, it defaults to"
0.4.3,"""<project> v<release> documentation""."
0.4.3,html_title = None
0.4.3,A shorter title for the navigation bar.  Default is the same as html_title.
0.4.3,html_short_title = None
0.4.3,The name of an image file (relative to this directory) to place at the top
0.4.3,of the sidebar.
0.4.3,html_logo = None
0.4.3,The name of an image file (within the static path) to use as favicon of the
0.4.3,docs.  This file should be a Windows icon file (.ico) being 16x16 or 32x32
0.4.3,pixels large.
0.4.3,html_favicon = None
0.4.3,"Add any paths that contain custom static files (such as style sheets) here,"
0.4.3,"relative to this directory. They are copied after the builtin static files,"
0.4.3,"so a file named ""default.css"" will overwrite the builtin ""default.css""."
0.4.3,Add any extra paths that contain custom files (such as robots.txt or
0.4.3,".htaccess) here, relative to this directory. These files are copied"
0.4.3,directly to the root of the documentation.
0.4.3,html_extra_path = []
0.4.3,"If not '', a 'Last updated on:' timestamp is inserted at every page bottom,"
0.4.3,using the given strftime format.
0.4.3,"html_last_updated_fmt = '%b %d, %Y'"
0.4.3,"If true, SmartyPants will be used to convert quotes and dashes to"
0.4.3,typographically correct entities.
0.4.3,html_use_smartypants = True
0.4.3,"Custom sidebar templates, maps document names to template names."
0.4.3,html_sidebars = {}
0.4.3,"Additional templates that should be rendered to pages, maps page names to"
0.4.3,template names.
0.4.3,html_additional_pages = {}
0.4.3,"If false, no module index is generated."
0.4.3,html_domain_indices = True
0.4.3,"If false, no index is generated."
0.4.3,html_use_index = True
0.4.3,"If true, the index is split into individual pages for each letter."
0.4.3,html_split_index = False
0.4.3,"If true, links to the reST sources are added to the pages."
0.4.3,html_show_sourcelink = True
0.4.3,"If true, ""Created using Sphinx"" is shown in the HTML footer. Default is True."
0.4.3,html_show_sphinx = True
0.4.3,"If true, ""(C) Copyright ..."" is shown in the HTML footer. Default is True."
0.4.3,html_show_copyright = True
0.4.3,"If true, an OpenSearch description file will be output, and all pages will"
0.4.3,contain a <link> tag referring to it.  The value of this option must be the
0.4.3,base URL from which the finished HTML is served.
0.4.3,html_use_opensearch = ''
0.4.3,"This is the file name suffix for HTML files (e.g. "".xhtml"")."
0.4.3,html_file_suffix = None
0.4.3,Output file base name for HTML help builder.
0.4.3,-- Options for LaTeX output ---------------------------------------------
0.4.3,The paper size ('letterpaper' or 'a4paper').
0.4.3,"'papersize': 'letterpaper',"
0.4.3,"The font size ('10pt', '11pt' or '12pt')."
0.4.3,"'pointsize': '10pt',"
0.4.3,Additional stuff for the LaTeX preamble.
0.4.3,"'preamble': '',"
0.4.3,Grouping the document tree into LaTeX files. List of tuples
0.4.3,"(source start file, target name, title,"
0.4.3,"author, documentclass [howto, manual, or own class])."
0.4.3,The name of an image file (relative to this directory) to place at the top of
0.4.3,the title page.
0.4.3,latex_logo = None
0.4.3,"For ""manual"" documents, if this is true, then toplevel headings are parts,"
0.4.3,not chapters.
0.4.3,latex_use_parts = False
0.4.3,"If true, show page references after internal links."
0.4.3,latex_show_pagerefs = False
0.4.3,"If true, show URL addresses after external links."
0.4.3,latex_show_urls = False
0.4.3,Documents to append as an appendix to all manuals.
0.4.3,latex_appendices = []
0.4.3,intersphinx configuration
0.4.3,sphinx-gallery configuration
0.4.3,-- Options for manual page output ---------------------------------------
0.4.3,"If false, no module index is generated."
0.4.3,latex_domain_indices = True
0.4.3,One entry per manual page. List of tuples
0.4.3,"(source start file, name, description, authors, manual section)."
0.4.3,"If true, show URL addresses after external links."
0.4.3,man_show_urls = False
0.4.3,-- Options for Texinfo output -------------------------------------------
0.4.3,Grouping the document tree into Texinfo files. List of tuples
0.4.3,"(source start file, target name, title, author,"
0.4.3,"dir menu entry, description, category)"
0.4.3,"def generate_example_rst(app, what, name, obj, options, lines):"
0.4.3,"# generate empty examples files, so that we don't get"
0.4.3,# inclusion errors if there are no examples for a class / module
0.4.3,"examples_path = os.path.join(app.srcdir, ""generated"","
0.4.3,"""%s.examples"" % name)"
0.4.3,if not os.path.exists(examples_path):
0.4.3,# touch file
0.4.3,"open(examples_path, 'w').close()"
0.4.3,Config for sphinx_issues
0.4.3,"app.connect('autodoc-process-docstring', generate_example_rst)"
0.4.3,Documents to append as an appendix to all manuals.
0.4.3,texinfo_appendices = []
0.4.3,"If false, no module index is generated."
0.4.3,texinfo_domain_indices = True
0.4.3,"How to display URL addresses: 'footnote', 'no', or 'inline'."
0.4.3,texinfo_show_urls = 'footnote'
0.4.3,"If true, do not generate a @detailmenu in the ""Top"" node's menu."
0.4.3,texinfo_no_detailmenu = False
0.4.3,The following is used by sphinx.ext.linkcode to provide links to github
0.4.3,get the styles from the current theme
0.4.3,create and add the button to all the code blocks that contain >>>
0.4.3,tracebacks (.gt) contain bare text elements that need to be
0.4.3,wrapped in a span to work with .nextUntil() (see later)
0.4.3,define the behavior of the button when it's clicked
0.4.3,hide the code output
0.4.3,show the code output
0.4.3,-*- coding: utf-8 -*-
0.4.3,Format template for issues URI
0.4.3,e.g. 'https://github.com/sloria/marshmallow/issues/{issue}
0.4.3,"Shortcut for Github, e.g. 'sloria/marshmallow'"
0.4.3,Format template for user profile URI
0.4.3,e.g. 'https://github.com/{user}'
0.4.3,Python 2 only
0.4.3,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.4.3,License: MIT
0.4.3,##############################################################################
0.4.3,"First, we will create an imbalanced data set from a the iris data set."
0.4.3,##############################################################################
0.4.3,Using ``sampling_strategy`` in resampling algorithms
0.4.3,##############################################################################
0.4.3,##############################################################################
0.4.3,``sampling_strategy`` as a ``float``
0.4.3,....................................
0.4.3,
0.4.3,``sampling_strategy`` can be given a ``float``. For **under-sampling
0.4.3,"methods**, it corresponds to the ratio :math:`\\alpha_{us}` defined by"
0.4.3,:math:`N_{rM} = \\alpha_{us} \\times N_{m}` where :math:`N_{rM}` and
0.4.3,:math:`N_{m}` are the number of samples in the majority class after
0.4.3,"resampling and the number of samples in the minority class, respectively."
0.4.3,select only 2 classes since the ratio make sense in this case
0.4.3,##############################################################################
0.4.3,"For **over-sampling methods**, it correspond to the ratio"
0.4.3,:math:`\\alpha_{os}` defined by :math:`N_{rm} = \\alpha_{os} \\times N_{m}`
0.4.3,where :math:`N_{rm}` and :math:`N_{M}` are the number of samples in the
0.4.3,minority class after resampling and the number of samples in the majority
0.4.3,"class, respectively."
0.4.3,##############################################################################
0.4.3,``sampling_strategy`` has a ``str``
0.4.3,...................................
0.4.3,
0.4.3,``sampling_strategy`` can be given as a string which specify the class
0.4.3,"targeted by the resampling. With under- and over-sampling, the number of"
0.4.3,samples will be equalized.
0.4.3,
0.4.3,Note that we are using multiple classes from now on.
0.4.3,##############################################################################
0.4.3,"With **cleaning method**, the number of samples in each class will not be"
0.4.3,equalized even if targeted.
0.4.3,##############################################################################
0.4.3,``sampling_strategy`` as a ``dict``
0.4.3,...................................
0.4.3,
0.4.3,"When ``sampling_strategy`` is a ``dict``, the keys correspond to the targeted"
0.4.3,classes. The values correspond to the desired number of samples for each
0.4.3,targeted class. This is working for both **under- and over-sampling**
0.4.3,algorithms but not for the **cleaning algorithms**. Use a ``list`` instead.
0.4.3,##############################################################################
0.4.3,``sampling_strategy`` as a ``list``
0.4.3,...................................
0.4.3,
0.4.3,"When ``sampling_strategy`` is a ``list``, the list contains the targeted"
0.4.3,classes. It is used only for **cleaning methods** and raise an error
0.4.3,otherwise.
0.4.3,##############################################################################
0.4.3,``sampling_strategy`` as a callable
0.4.3,...................................
0.4.3,
0.4.3,"When callable, function taking ``y`` and returns a ``dict``. The keys"
0.4.3,correspond to the targeted classes. The values correspond to the desired
0.4.3,number of samples for each class.
0.4.3,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.4.3,License: MIT
0.4.3,#############################################################################
0.4.3,Toy data generation
0.4.3,#############################################################################
0.4.3,#############################################################################
0.4.3,We are generating some non Gaussian data set contaminated with some unform
0.4.3,noise.
0.4.3,#############################################################################
0.4.3,We will generate some cleaned test data without outliers.
0.4.3,#############################################################################
0.4.3,How to use the :class:`imblearn.FunctionSampler`
0.4.3,#############################################################################
0.4.3,#############################################################################
0.4.3,We first define a function which will use
0.4.3,:class:`sklearn.ensemble.IsolationForest` to eliminate some outliers from
0.4.3,our dataset during training. The function passed to the
0.4.3,:class:`imblearn.FunctionSampler` will be called when using the method
0.4.3,``fit_resample``.
0.4.3,#############################################################################
0.4.3,Integrate it within a pipeline
0.4.3,#############################################################################
0.4.3,#############################################################################
0.4.3,"By elimnating outliers before the training, the classifier will be less"
0.4.3,affected during the prediction.
0.4.3,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.4.3,License: MIT
0.4.3,generate some data points
0.4.3,plot the majority and minority samples
0.4.3,draw the circle in which the new sample will generated
0.4.3,plot the line on which the sample will be generated
0.4.3,create and plot the new sample
0.4.3,make the plot nicer with legend and label
0.4.3,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.4.3,License: MIT
0.4.3,##############################################################################
0.4.3,The following function will be used to create toy dataset. It using the
0.4.3,``make_classification`` from scikit-learn but fixing some parameters.
0.4.3,##############################################################################
0.4.3,The following function will be used to plot the sample space after resampling
0.4.3,to illustrate the characterisitic of an algorithm.
0.4.3,make nice plotting
0.4.3,##############################################################################
0.4.3,The following function will be used to plot the decision function of a
0.4.3,classifier given some data.
0.4.3,##############################################################################
0.4.3,Illustration of the influence of the balancing ratio
0.4.3,##############################################################################
0.4.3,##############################################################################
0.4.3,We will first illustrate the influence of the balancing ratio on some toy
0.4.3,data using a linear SVM classifier. Greater is the difference between the
0.4.3,"number of samples in each class, poorer are the classfication results."
0.4.3,##############################################################################
0.4.3,Random over-sampling to balance the data set
0.4.3,##############################################################################
0.4.3,##############################################################################
0.4.3,Random over-sampling can be used to repeat some samples and balance the
0.4.3,number of samples between the dataset. It can be seen that with this trivial
0.4.3,approach the boundary decision is already less biaised toward the majority
0.4.3,class.
0.4.3,##############################################################################
0.4.3,More advanced over-sampling using ADASYN and SMOTE
0.4.3,##############################################################################
0.4.3,##############################################################################
0.4.3,"Instead of repeating the same samples when over-sampling, we can use some"
0.4.3,specific heuristic instead. ADASYN and SMOTE can be used in this case.
0.4.3,Make an identity sampler
0.4.3,##############################################################################
0.4.3,The following plot illustrate the difference between ADASYN and SMOTE. ADASYN
0.4.3,will focus on the samples which are difficult to classify with a
0.4.3,nearest-neighbors rule while regular SMOTE will not make any distinction.
0.4.3,"Therefore, the decision function depending of the algorithm."
0.4.3,##############################################################################
0.4.3,"Due to those sampling particularities, it can give rise to some specific"
0.4.3,issues as illustrated below.
0.4.3,##############################################################################
0.4.3,SMOTE proposes several variants by identifying specific samples to consider
0.4.3,during the resampling. The borderline version will detect which point to
0.4.3,select which are in the border between two classes. The SVM version will use
0.4.3,the support vectors found using an SVM algorithm to create new samples.
0.4.3,##############################################################################
0.4.3,"When dealing with a mixed of continuous and categorical features, SMOTE-NC"
0.4.3,is the only method which can handle this case.
0.4.3,create a synthetic data set with continuous and categorical features
0.4.3,Authors: Christos Aridas
0.4.3,Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.4.3,License: MIT
0.4.3,Generate the dataset
0.4.3,make nice plotting
0.4.3,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.4.3,License: MIT
0.4.3,Generate a dataset
0.4.3,Split the data
0.4.3,Train the classifier with balancing
0.4.3,Test the classifier and get the prediction
0.4.3,Show the classification report
0.4.3,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.4.3,License: MIT
0.4.3,Generate a dataset
0.4.3,Split the data
0.4.3,Train the classifier with balancing
0.4.3,Test the classifier and get the prediction
0.4.3,##############################################################################
0.4.3,The geometric mean corresponds to the square root of the product of the
0.4.3,sensitivity and specificity. Combining the two metrics should account for
0.4.3,the balancing of the dataset.
0.4.3,##############################################################################
0.4.3,The index balanced accuracy can transform any metric to be used in
0.4.3,imbalanced learning problems.
0.4.3,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.4.3,License: MIT
0.4.3,##############################################################################
0.4.3,The following function will be used to create toy dataset. It using the
0.4.3,``make_classification`` from scikit-learn but fixing some parameters.
0.4.3,##############################################################################
0.4.3,The following function will be used to plot the sample space after resampling
0.4.3,to illustrate the characteristic of an algorithm.
0.4.3,make nice plotting
0.4.3,##############################################################################
0.4.3,The following function will be used to plot the decision function of a
0.4.3,classifier given some data.
0.4.3,##############################################################################
0.4.3,"``SMOTE`` allows to generate samples. However, this method of over-sampling"
0.4.3,"does not have any knowledge regarding the underlying distribution. Therefore,"
0.4.3,"some noisy samples can be generated, e.g. when the different classes cannot"
0.4.3,"be well separated. Hence, it can be beneficial to apply an under-sampling"
0.4.3,algorithm to clean the noisy samples. Two methods are usually used in the
0.4.3,literature: (i) Tomek's link and (ii) edited nearest neighbours cleaning
0.4.3,methods. Imbalanced-learn provides two ready-to-use samplers ``SMOTETomek``
0.4.3,"and ``SMOTEENN``. In general, ``SMOTEENN`` cleans more noisy data than"
0.4.3,``SMOTETomek``.
0.4.3,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.4.3,License: MIT
0.4.3,##############################################################################
0.4.3,Load an imbalanced dataset
0.4.3,##############################################################################
0.4.3,We will load the UCI SatImage dataset which has an imbalanced ratio of 9.3:1
0.4.3,(number of majority sample for a minority sample). The data are then split
0.4.3,into training and testing.
0.4.3,##############################################################################
0.4.3,Classification using a single decision tree
0.4.3,##############################################################################
0.4.3,We train a decision tree classifier which will be used as a baseline for the
0.4.3,rest of this example.
0.4.3,##############################################################################
0.4.3,The results are reported in terms of balanced accuracy and geometric mean
0.4.3,which are metrics widely used in the literature to validate model trained on
0.4.3,imbalanced set.
0.4.3,##############################################################################
0.4.3,Classification using bagging classifier with and without sampling
0.4.3,##############################################################################
0.4.3,"Instead of using a single tree, we will check if an ensemble of decsion tree"
0.4.3,"can actually alleviate the issue induced by the class imbalancing. First, we"
0.4.3,will use a bagging classifier and its counter part which internally uses a
0.4.3,random under-sampling to balanced each boostrap sample.
0.4.3,##############################################################################
0.4.3,Balancing each bootstrap sample allows to increase significantly the balanced
0.4.3,accuracy and the geometric mean.
0.4.3,##############################################################################
0.4.3,Classification using random forest classifier with and without sampling
0.4.3,##############################################################################
0.4.3,Random forest is another popular ensemble method and it is usually
0.4.3,"outperforming bagging. Here, we used a vanilla random forest and its balanced"
0.4.3,counterpart in which each bootstrap sample is balanced.
0.4.3,"Similarly to the previous experiment, the balanced classifier outperform the"
0.4.3,"classifier which learn from imbalanced bootstrap samples. In addition, random"
0.4.3,forest outsperforms the bagging classifier.
0.4.3,##############################################################################
0.4.3,Boosting classifier
0.4.3,##############################################################################
0.4.3,"In the same manner, easy ensemble classifier is a bag of balanced AdaBoost"
0.4.3,"classifier. However, it will be slower to train than random forest and will"
0.4.3,achieve worse performance.
0.4.3,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.4.3,License: MIT
0.4.3,##############################################################################
0.4.3,The following function will be used to create toy dataset. It using the
0.4.3,``make_classification`` from scikit-learn but fixing some parameters.
0.4.3,##############################################################################
0.4.3,The following function will be used to plot the sample space after resampling
0.4.3,to illustrate the characteristic of an algorithm.
0.4.3,make nice plotting
0.4.3,##############################################################################
0.4.3,The following function will be used to plot the decision function of a
0.4.3,classifier given some data.
0.4.3,##############################################################################
0.4.3,Prototype generation: under-sampling by generating new samples
0.4.3,##############################################################################
0.4.3,##############################################################################
0.4.3,``ClusterCentroids`` under-samples by replacing the original samples by the
0.4.3,centroids of the cluster found.
0.4.3,##############################################################################
0.4.3,Prototype selection: under-sampling by selecting existing samples
0.4.3,##############################################################################
0.4.3,##############################################################################
0.4.3,The algorithm performing prototype selection can be subdivided into two
0.4.3,groups: (i) the controlled under-sampling methods and (ii) the cleaning
0.4.3,under-sampling methods.
0.4.3,##############################################################################
0.4.3,"With the controlled under-sampling methods, the number of samples to be"
0.4.3,selected can be specified. ``RandomUnderSampler`` is the most naive way of
0.4.3,performing such selection by randomly selecting a given number of samples by
0.4.3,the targetted class.
0.4.3,##############################################################################
0.4.3,``NearMiss`` algorithms implement some heuristic rules in order to select
0.4.3,samples. NearMiss-1 selects samples from the majority class for which the
0.4.3,average distance of the :math:`k`` nearest samples of the minority class is
0.4.3,the smallest. NearMiss-2 selects the samples from the majority class for
0.4.3,which the average distance to the farthest samples of the negative class is
0.4.3,"the smallest. NearMiss-3 is a 2-step algorithm: first, for each minority"
0.4.3,"sample, their ::math:`m` nearest-neighbors will be kept; then, the majority"
0.4.3,samples selected are the on for which the average distance to the :math:`k`
0.4.3,nearest neighbors is the largest.
0.4.3,##############################################################################
0.4.3,``EditedNearestNeighbours`` removes samples of the majority class for which
0.4.3,their class differ from the one of their nearest-neighbors. This sieve can be
0.4.3,repeated which is the principle of the
0.4.3,``RepeatedEditedNearestNeighbours``. ``AllKNN`` is slightly different from
0.4.3,the ``RepeatedEditedNearestNeighbours`` by changing the :math:`k` parameter
0.4.3,"of the internal nearest neighors algorithm, increasing it at each iteration."
0.4.3,##############################################################################
0.4.3,``CondensedNearestNeighbour`` makes use of a 1-NN to iteratively decide if a
0.4.3,sample should be kept in a dataset or not. The issue is that
0.4.3,``CondensedNearestNeighbour`` is sensitive to noise by preserving the noisy
0.4.3,samples. ``OneSidedSelection`` also used the 1-NN and use ``TomekLinks`` to
0.4.3,remove the samples considered noisy. The ``NeighbourhoodCleaningRule`` use a
0.4.3,"``EditedNearestNeighbours`` to remove some sample. Additionally, they use a 3"
0.4.3,nearest-neighbors to remove samples which do not agree with this rule.
0.4.3,##############################################################################
0.4.3,``InstanceHardnessThreshold`` uses the prediction of classifier to exclude
0.4.3,samples. All samples which are classified with a low probability will be
0.4.3,removed.
0.4.3,##############################################################################
0.4.3,This function allows to make nice plotting
0.4.3,##############################################################################
0.4.3,Generate some data with one Tomek link
0.4.3,minority class
0.4.3,majority class
0.4.3,##############################################################################
0.4.3,"In the figure above, the samples highlighted in green form a Tomek link since"
0.4.3,they are of different classes and are nearest neighbours of each other.
0.4.3,highlight the samples of interest
0.4.3,##############################################################################
0.4.3,We can run the ``TomekLinks`` sampling to remove the corresponding
0.4.3,samples. If ``sampling_strategy='auto'`` only the sample from the majority
0.4.3,class will be removed. If ``sampling_strategy='all'`` both samples will be
0.4.3,removed.
0.4.3,highlight the samples of interest
0.4.3,##############################################################################
0.4.3,This function allows to make nice plotting
0.4.3,##############################################################################
0.4.3,We can start by generating some data to later illustrate the principle of
0.4.3,each NearMiss heuritic rules.
0.4.3,minority class
0.4.3,majority class
0.4.3,##############################################################################
0.4.3,NearMiss-1
0.4.3,##############################################################################
0.4.3,##############################################################################
0.4.3,NearMiss-1 selects samples from the majority class for which the average
0.4.3,distance to some nearest neighbours is the smallest. In the following
0.4.3,"example, we use a 3-NN to compute the average distance on 2 specific samples"
0.4.3,"of the majority class. Therefore, in this case the point linked by the"
0.4.3,green-dashed line will be selected since the average distance is smaller.
0.4.3,##############################################################################
0.4.3,NearMiss-2
0.4.3,##############################################################################
0.4.3,##############################################################################
0.4.3,NearMiss-2 selects samples from the majority class for which the average
0.4.3,distance to the farthest neighbors is the smallest. With the same
0.4.3,"configuration as previously presented, the sample linked to the green-dashed"
0.4.3,line will be selected since its distance the 3 farthest neighbors is the
0.4.3,smallest.
0.4.3,##############################################################################
0.4.3,NearMiss-3
0.4.3,##############################################################################
0.4.3,##############################################################################
0.4.3,"NearMiss-3 can be divided into 2 steps. First, a nearest-neighbors is used to"
0.4.3,short-list samples from the majority class (i.e. correspond to the
0.4.3,"highlighted samples in the following plot). Then, the sample with the largest"
0.4.3,average distance to the *k* nearest-neighbors are selected.
0.4.3,select only the majority point of interest
0.4.3,Authors: Christos Aridas
0.4.3,Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.4.3,License: MIT
0.4.3,Generate the dataset
0.4.3,Instanciate a PCA object for the sake of easy visualisation
0.4.3,Create the samplers
0.4.3,Create the classifier
0.4.3,Make the splits
0.4.3,Add one transformers and two samplers in the pipeline object
0.4.3,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.4.3,License: MIT
0.4.3,##############################################################################
0.4.3,Data loading
0.4.3,##############################################################################
0.4.3,##############################################################################
0.4.3,"First, you should download the Porto Seguro data set from Kaggle. See the"
0.4.3,link in the introduction.
0.4.3,##############################################################################
0.4.3,The data set is imbalanced and it will have an effect on the fitting.
0.4.3,##############################################################################
0.4.3,Define the pre-processing pipeline
0.4.3,##############################################################################
0.4.3,##############################################################################
0.4.3,We want to standard scale the numerical features while we want to one-hot
0.4.3,"encode the categorical features. In this regard, we make use of the"
0.4.3,:class:`sklearn.compose.ColumnTransformer`.
0.4.3,Create an environment variable to avoid using the GPU. This can be changed.
0.4.3,##############################################################################
0.4.3,Create a neural-network
0.4.3,##############################################################################
0.4.3,##############################################################################
0.4.3,We create a decorator to report the computation time
0.4.3,##############################################################################
0.4.3,The first model will be trained using the ``fit`` method and with imbalanced
0.4.3,mini-batches.
0.4.3,##############################################################################
0.4.3,"In the contrary, we will use imbalanced-learn to create a generator of"
0.4.3,mini-batches which will yield balanced mini-batches.
0.4.3,##############################################################################
0.4.3,Classification loop
0.4.3,##############################################################################
0.4.3,##############################################################################
0.4.3,We will perform a 10-fold cross-validation and train the neural-network with
0.4.3,the two different strategies previously presented.
0.4.3,##############################################################################
0.4.3,Plot of the results and computation time
0.4.3,##############################################################################
0.4.3,Authors: Christos Aridas
0.4.3,Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.4.3,License: MIT
0.4.3,Load the dataset
0.4.3,make nice plotting
0.4.3,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.4.3,License: MIT
0.4.3,Create a folder to fetch the dataset
0.4.3,Create a pipeline
0.4.3,Classify and report the results
0.4.3,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.4.3,License: MIT
0.4.3,##############################################################################
0.4.3,Setting the data set
0.4.3,##############################################################################
0.4.3,##############################################################################
0.4.3,We use a part of the 20 newsgroups data set by loading 4 topics. Using the
0.4.3,"scikit-learn loader, the data are split into a training and a testing set."
0.4.3,
0.4.3,Note the class \#3 is the minority class and has almost twice less samples
0.4.3,than the majority class.
0.4.3,##############################################################################
0.4.3,The usual scikit-learn pipeline
0.4.3,##############################################################################
0.4.3,##############################################################################
0.4.3,You might usually use scikit-learn pipeline by combining the TF-IDF
0.4.3,vectorizer to feed a multinomial naive bayes classifier. A classification
0.4.3,report summarized the results on the testing set.
0.4.3,
0.4.3,"As expected, the recall of the class \#3 is low mainly due to the class"
0.4.3,imbalanced.
0.4.3,##############################################################################
0.4.3,Balancing the class before classification
0.4.3,##############################################################################
0.4.3,##############################################################################
0.4.3,"To improve the prediction of the class \#3, it could be interesting to apply"
0.4.3,"a balancing before to train the naive bayes classifier. Therefore, we will"
0.4.3,use a ``RandomUnderSampler`` to equalize the number of samples in all the
0.4.3,classes before the training.
0.4.3,
0.4.3,It is also important to note that we are using the ``make_pipeline`` function
0.4.3,implemented in imbalanced-learn to properly handle the samplers.
0.4.3,##############################################################################
0.4.3,"Although the results are almost identical, it can be seen that the resampling"
0.4.3,allowed to correct the poor recall of the class \#3 at the cost of reducing
0.4.3,"the other metrics for the other classes. However, the overall results are"
0.4.3,slightly better.
0.4.3,Authors: Dayvid Oliveira
0.4.3,Christos Aridas
0.4.3,Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.4.3,License: MIT
0.4.3,Generate the dataset
0.4.3,"Two subplots, unpack the axes array immediately"
0.4.3,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.4.3,License: MIT
0.4.3,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.4.3,Christos Aridas
0.4.3,License: MIT
0.4.3,define an alias for back-compatibility
0.4.3,FIXME: remove in 0.6
0.4.3,FIXME: remove in 0.6
0.4.3,both ratio and sampling_strategy should not be set
0.4.3,Adapted from scikit-learn
0.4.3,Author: Edouard Duchesnay
0.4.3,Gael Varoquaux
0.4.3,Virgile Fritsch
0.4.3,Alexandre Gramfort
0.4.3,Lars Buitinck
0.4.3,Christos Aridas
0.4.3,Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.4.3,License: BSD
0.4.3,BaseEstimator interface
0.4.3,validate names
0.4.3,validate estimators
0.4.3,We allow last estimator to be None as an identity transformation
0.4.3,Estimator interface
0.4.3,Setup the memory
0.4.3,joblib >= 0.12
0.4.3,we do not clone when caching is disabled to
0.4.3,preserve backward compatibility
0.4.3,joblib < 0.11
0.4.3,we do not clone when caching is disabled to
0.4.3,preserve backward compatibility
0.4.3,Fit or load from cache the current transfomer
0.4.3,Replace the transformer of the step with the fitted
0.4.3,transformer. This is necessary when loading the transformer
0.4.3,from the cache.
0.4.3,"_final_estimator is None or has transform, otherwise attribute error"
0.4.3,raise AttributeError if necessary for hasattr behaviour
0.4.3,"if we have a weight for this transformer, multiply output"
0.4.3,Based on NiLearn package
0.4.3,License: simplified BSD
0.4.3,"PEP0440 compatible formatted version, see:"
0.4.3,https://www.python.org/dev/peps/pep-0440/
0.4.3,
0.4.3,Generic release markers:
0.4.3,X.Y
0.4.3,X.Y.Z # For bugfix releases
0.4.3,
0.4.3,Admissible pre-release markers:
0.4.3,X.YaN # Alpha release
0.4.3,X.YbN # Beta release
0.4.3,X.YrcN # Release Candidate
0.4.3,X.Y # Final release
0.4.3,
0.4.3,Dev branch marker is: 'X.Y.dev' or 'X.Y.devN' where N is an integer.
0.4.3,'X.Y.dev0' is the canonical version of 'X.Y.dev'
0.4.3,
0.4.3,coding: utf-8
0.4.3,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.4.3,Dariusz Brzezinski
0.4.3,License: MIT
0.4.3,Only negative labels
0.4.3,"Calculate tp_sum, pred_sum, true_sum ###"
0.4.3,labels are now from 0 to len(labels) - 1 -> use bincount
0.4.3,Pathological case
0.4.3,Compute the true negative
0.4.3,Retain only selected labels
0.4.3,"Finally, we have all our sufficient statistics. Divide! #"
0.4.3,"Divide, and on zero-division, set scores to 0 and warn:"
0.4.3,"Oddly, we may get an ""invalid"" rather than a ""divide"" error"
0.4.3,here.
0.4.3,Average the results
0.4.3,labels are now from 0 to len(labels) - 1 -> use bincount
0.4.3,Pathological case
0.4.3,Retain only selected labels
0.4.3,old version of scipy return MaskedConstant instead of 0.0
0.4.3,Create the list of tags
0.4.3,check that the scoring function does not need a score
0.4.3,and only a prediction
0.4.3,Compute the score from the scoring function
0.4.3,Square if desired
0.4.3,Get the signature of the sens/spec function
0.4.3,We need to extract from kwargs only the one needed by the
0.4.3,specificity and specificity
0.4.3,Make the intersection between the parameters
0.4.3,Create a sub dictionary
0.4.3,Check if the metric is the geometric mean
0.4.3,We do not support multilabel so the only average supported
0.4.3,is binary
0.4.3,Create the list of parameters through signature binding
0.4.3,Call the sens/spec function
0.4.3,Compute the dominance
0.4.3,Compute the different metrics
0.4.3,Precision/recall/f1
0.4.3,Specificity
0.4.3,Geometric mean
0.4.3,Index balanced accuracy
0.4.3,compute averages
0.4.3,coding: utf-8
0.4.3,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.4.3,Christos Aridas
0.4.3,License: MIT
0.4.3,##############################################################################
0.4.3,Utilities for testing
0.4.3,import some data to play with
0.4.3,restrict to a binary classification task
0.4.3,add noisy features to make the problem harder and avoid perfect results
0.4.3,"run classifier, get class probabilities and label predictions"
0.4.3,only interested in probabilities of the positive case
0.4.3,XXX: do we really want a special API for the binary case?
0.4.3,##############################################################################
0.4.3,Tests
0.4.3,detailed measures for each class
0.4.3,individual scoring function that can be used for grid search: in the
0.4.3,binary class case the score is the value of the measure for the positive
0.4.3,class (e.g. label == 1). This is deprecated for average != 'binary'.
0.4.3,Such a case may occur with non-stratified cross-validation
0.4.3,ensure the above were meaningful tests:
0.4.3,Bad pos_label
0.4.3,Bad average option
0.4.3,but average != 'binary'; even if data is binary
0.4.3,compute the geometric mean for the binary problem
0.4.3,print classification report with class names
0.4.3,print classification report with label detection
0.4.3,print classification report with class names
0.4.3,print classification report with label detection
0.4.3,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.4.3,Christos Aridas
0.4.3,License: MIT
0.4.3,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.4.3,Christos Aridas
0.4.3,License: MIT
0.4.3,The ratio is computed using a one-vs-rest manner. Using majority
0.4.3,in multi-class would lead to slightly different results at the
0.4.3,cost of introducing a new parameter.
0.4.3,the nearest neighbors need to be fitted only on the current class
0.4.3,to find the class NN to generate new samples
0.4.3,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.4.3,Christos Aridas
0.4.3,License: MIT
0.4.3,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.4.3,Christos Aridas
0.4.3,License: MIT
0.4.3,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.4.3,Fernando Nogueira
0.4.3,Christos Aridas
0.4.3,Dzianis Dudnik
0.4.3,License: MIT
0.4.3,FIXME: remove in 0.6
0.4.3,Samples are in danger for m/2 <= m' < m
0.4.3,Samples are noise for m = m'
0.4.3,FIXME: rename _sample -> _fit_resample in 0.6
0.4.3,divergence between borderline-1 and borderline-2
0.4.3,Create synthetic samples for borderline points.
0.4.3,only minority
0.4.3,we use a one-vs-rest policy to handle the multiclass in which
0.4.3,new samples will be created considering not only the majority
0.4.3,class but all over classes.
0.4.3,FIXME: rename _sample -> _fit_resample in 0.6
0.4.3,"FIXME: In 0.6, SMOTE should inherit only from BaseSMOTE."
0.4.3,FIXME: in 0.6 call super()
0.4.3,FIXME: in 0.6 call super()
0.4.3,FIXME: remove in 0.6 after deprecation cycle
0.4.3,FIXME: to be removed in 0.6
0.4.3,FIXME: uncomment in version 0.6
0.4.3,self._validate_estimator()
0.4.3,compute the median of the standard deviation of the minority class
0.4.3,the input of the OneHotEncoder needs to be dense
0.4.3,we can replace the 1 entries of the categorical features with the
0.4.3,median of the standard deviation. It will ensure that whenever
0.4.3,"distance is computed between 2 samples, the difference will be equal"
0.4.3,to the median of the standard deviation as in the original paper.
0.4.3,reverse the encoding of the categorical features
0.4.3,the matrix is supposed to be in the CSR format after the stacking
0.4.3,"To avoid conversion and since there is only few samples used, we"
0.4.3,convert those samples to dense array.
0.4.3,tie breaking argmax
0.4.3,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.4.3,Christos Aridas
0.4.3,License: MIT
0.4.3,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.4.3,Christos Aridas
0.4.3,Dzianis Dudnik
0.4.3,License: MIT
0.4.3,create 2 random continuous feature
0.4.3,create a categorical feature using some string
0.4.3,create a categorical feature using some integer
0.4.3,return the categories
0.4.3,create 2 random continuous feature
0.4.3,create a categorical feature using some string
0.4.3,create a categorical feature using some integer
0.4.3,return the categories
0.4.3,create 2 random continuous feature
0.4.3,create a categorical feature using some string
0.4.3,create a categorical feature using some integer
0.4.3,return the categories
0.4.3,create 2 random continuous feature
0.4.3,create a categorical feature using some string
0.4.3,create a categorical feature using some integer
0.4.3,return the categories
0.4.3,create 2 random continuous feature
0.4.3,create a categorical feature using some string
0.4.3,create a categorical feature using some integer
0.4.3,part of the common test which apply to SMOTE-NC even if it is not default
0.4.3,constructible
0.4.3,Check that the samplers handle pandas dataframe and pandas series
0.4.3,Cast X and y to not default dtype
0.4.3,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.4.3,Christos Aridas
0.4.3,License: MIT
0.4.3,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.4.3,Christos Aridas
0.4.3,License: MIT
0.4.3,FIXME: Remove in 0.6
0.4.3,shuffle the indices since the sampler are packing them by class
0.4.3,helper functions
0.4.3,input and output
0.4.3,build the model and weights
0.4.3,"build the loss, predict, and train operator"
0.4.3,Initialization of all variables in the graph
0.4.3,"For each epoch, run accuracy on train and test"
0.4.3,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.4.3,License: MIT
0.4.3,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.4.3,Fernando Nogueira
0.4.3,Christos Aridas
0.4.3,License: MIT
0.4.3,find which class to not consider
0.4.3,there is a Tomek link between two samples if they are both nearest
0.4.3,neighbors of each others.
0.4.3,check for deprecated random_state
0.4.3,Find the nearest neighbour of every point
0.4.3,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.4.3,Christos Aridas
0.4.3,License: MIT
0.4.3,Randomly get one sample from the majority class
0.4.3,Generate the index to select
0.4.3,Create the set C - One majority samples and all minority
0.4.3,Create the set S - all majority samples
0.4.3,fit knn on C
0.4.3,Check each sample in S if we keep it or drop it
0.4.3,Do not select sample which are already well classified
0.4.3,Classify on S
0.4.3,If the prediction do not agree with the true label
0.4.3,append it in C_x
0.4.3,Keep the index for later
0.4.3,Update C
0.4.3,fit a knn on C
0.4.3,This experimental to speed up the search
0.4.3,Classify all the element in S and avoid to test the
0.4.3,well classified elements
0.4.3,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.4.3,Dayvid Oliveira
0.4.3,Christos Aridas
0.4.3,License: MIT
0.4.3,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.4.3,Christos Aridas
0.4.3,License: MIT
0.4.3,Compute the distance considering the farthest neighbour
0.4.3,Sort the list of distance and get the index
0.4.3,Throw a warning to tell the user that we did not have enough samples
0.4.3,to select and that we just select everything
0.4.3,Select the desired number of samples
0.4.3,check for deprecated random_state
0.4.3,idx_tmp is relative to the feature selected in the
0.4.3,previous step and we need to find the indirection
0.4.3,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.4.3,Christos Aridas
0.4.3,License: MIT
0.4.3,select a sample from the current class
0.4.3,create the set composed of all minority samples and one
0.4.3,sample from the current class.
0.4.3,create the set S with removing the seed from S
0.4.3,since that it will be added anyway
0.4.3,apply Tomek cleaning
0.4.3,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.4.3,Dayvid Oliveira
0.4.3,Christos Aridas
0.4.3,License: MIT
0.4.3,check for deprecated random_state
0.4.3,check for deprecated random_state
0.4.3,Check the stopping criterion
0.4.3,1. If there is no changes for the vector y
0.4.3,2. If the number of samples in the other class become inferior to
0.4.3,the number of samples in the majority class
0.4.3,3. If one of the class is disappearing
0.4.3,Case 1
0.4.3,Case 2
0.4.3,Case 3
0.4.3,check for deprecated random_state
0.4.3,Check the stopping criterion
0.4.3,1. If the number of samples in the other class become inferior to
0.4.3,the number of samples in the majority class
0.4.3,2. If one of the class is disappearing
0.4.3,Case 1else:
0.4.3,overwrite b_min_bec_maj
0.4.3,Case 2
0.4.3,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.4.3,Christos Aridas
0.4.3,License: MIT
0.4.3,check for deprecated random_state
0.4.3,clean the neighborhood
0.4.3,compute which classes to consider for cleaning for the A2 group
0.4.3,compute a2 group
0.4.3,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.4.3,Christos Aridas
0.4.3,License: MIT
0.4.3,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.4.3,Christos Aridas
0.4.3,License: MIT
0.4.3,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.4.3,Christos Aridas
0.4.3,License: MIT
0.4.3,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.4.3,Christos Aridas
0.4.3,License: MIT
0.4.3,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.4.3,Christos Aridas
0.4.3,License: MIT
0.4.3,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.4.3,Christos Aridas
0.4.3,License: MIT
0.4.3,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.4.3,Christos Aridas
0.4.3,License: MIT
0.4.3,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.4.3,Christos Aridas
0.4.3,License: MIT
0.4.3,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.4.3,Christos Aridas
0.4.3,License: MIT
0.4.3,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.4.3,Christos Aridas
0.4.3,License: MIT
0.4.3,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.4.3,Christos Aridas
0.4.3,License: MIT
0.4.3,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.4.3,Fernando Nogueira
0.4.3,Christos Aridas
0.4.3,License: MIT
0.4.3,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.4.3,Christos Aridas
0.4.3,License: MIT
0.4.3,test that all_estimators doesn't find abstract classes.
0.4.3,don't run twice the sampler tests. Meta-estimator do not have a
0.4.3,fit_resample method.
0.4.3,input validation etc for non-meta estimators
0.4.3,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.4.3,License: MIT
0.4.3,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.4.3,Christos Aridas
0.4.3,License: MIT
0.4.3,store timestamp to figure out whether the result of 'fit' has been
0.4.3,cached or not
0.4.3,store timestamp to figure out whether the result of 'fit' has been
0.4.3,cached or not
0.4.3,Test the various init parameters of the pipeline.
0.4.3,Check that we can't instantiate pipelines with objects without fit
0.4.3,method
0.4.3,Smoke test with only an estimator
0.4.3,Check that params are set
0.4.3,Smoke test the repr:
0.4.3,Test with two objects
0.4.3,Check that we can't instantiate with non-transformers on the way
0.4.3,"Note that NoTrans implements fit, but not transform"
0.4.3,Check that params are set
0.4.3,Smoke test the repr:
0.4.3,Check that params are not set when naming them wrong
0.4.3,Test clone
0.4.3,"Check that apart from estimators, the parameters are the same"
0.4.3,Remove estimators that where copied
0.4.3,Test the various methods of the pipeline (anova).
0.4.3,Test with Anova + LogisticRegression
0.4.3,Test that the pipeline can take fit parameters
0.4.3,classifier should return True
0.4.3,and transformer params should not be changed
0.4.3,invalid parameters should raise an error message
0.4.3,Pipeline should pass sample_weight
0.4.3,When sample_weight is None it shouldn't be passed
0.4.3,Test pipeline raises set params error message for nested models.
0.4.3,nested model check
0.4.3,Test the various methods of the pipeline (pca + svm).
0.4.3,Test with PCA + SVC
0.4.3,Test the various methods of the pipeline (preprocessing + svm).
0.4.3,check shapes of various prediction functions
0.4.3,test that the fit_predict method is implemented on a pipeline
0.4.3,test that the fit_predict on pipeline yields same results as applying
0.4.3,transform and clustering steps separately
0.4.3,"As pipeline doesn't clone estimators on construction,"
0.4.3,it must have its own estimators
0.4.3,first compute the transform and clustering step separately
0.4.3,use a pipeline to do the transform and clustering in one step
0.4.3,tests that a pipeline does not have fit_predict method when final
0.4.3,step of pipeline does not have fit_predict defined
0.4.3,tests that Pipeline passes fit_params to intermediate steps
0.4.3,when fit_predict is invoked
0.4.3,Test whether pipeline works with a transformer at the end.
0.4.3,Also test pipeline.transform and pipeline.inverse_transform
0.4.3,test transform and fit_transform:
0.4.3,Test whether pipeline works with a transformer missing fit_transform
0.4.3,test fit_transform:
0.4.3,Directly setting attr
0.4.3,Using set_params
0.4.3,Using set_params to replace single step
0.4.3,With invalid data
0.4.3,Test setting Pipeline steps to None
0.4.3,"for other methods, ensure no AttributeErrors on None:"
0.4.3,mult2 and mult3 are active
0.4.3,Check None step at construction time
0.4.3,Test that an error is raised when memory is not a string or a Memory
0.4.3,instance
0.4.3,Define memory as an integer
0.4.3,Test with Transformer + SVC
0.4.3,Memoize the transformer at the first fit
0.4.3,Get the time stamp of the tranformer in the cached pipeline
0.4.3,Check that cached_pipe and pipe yield identical results
0.4.3,Check that we are reading the cache while fitting
0.4.3,a second time
0.4.3,Check that cached_pipe and pipe yield identical results
0.4.3,Create a new pipeline with cloned estimators
0.4.3,Check that even changing the name step does not affect the cache hit
0.4.3,Check that cached_pipe and pipe yield identical results
0.4.3,Test with Transformer + SVC
0.4.3,Memoize the transformer at the first fit
0.4.3,Get the time stamp of the tranformer in the cached pipeline
0.4.3,Check that cached_pipe and pipe yield identical results
0.4.3,Check that we are reading the cache while fitting
0.4.3,a second time
0.4.3,Check that cached_pipe and pipe yield identical results
0.4.3,Create a new pipeline with cloned estimators
0.4.3,Check that even changing the name step does not affect the cache hit
0.4.3,Check that cached_pipe and pipe yield identical results
0.4.3,Test the various methods of the pipeline (pca + svm).
0.4.3,Test with PCA + SVC
0.4.3,Test the various methods of the pipeline (pca + svm).
0.4.3,Test with PCA + SVC
0.4.3,Test whether pipeline works with a sampler at the end.
0.4.3,Also test pipeline.sampler
0.4.3,test transform and fit_transform:
0.4.3,We round the value near to zero. It seems that PCA has some issue
0.4.3,with that
0.4.3,Test whether pipeline works with a sampler at the end.
0.4.3,Also test pipeline.sampler
0.4.3,Test pipeline using None as preprocessing step and a classifier
0.4.3,"Test pipeline using None, RUS and a classifier"
0.4.3,"Test pipeline using RUS, None and a classifier"
0.4.3,Test pipeline using None step and a sampler
0.4.3,Test pipeline using None and a transformer that implements transform and
0.4.3,inverse_transform
0.4.3,Test the various methods of the pipeline (anova).
0.4.3,Test with RandomUnderSampling + Anova + LogisticRegression
0.4.3,Test the various methods of the pipeline (anova).
0.4.3,Test the various methods of the pipeline (anova).
0.4.3,Test with RandomUnderSampling + Anova + LogisticRegression
0.4.3,tests that Pipeline passes predict_params to the final estimator
0.4.3,when predict is invoked
0.4.3,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.4.3,Christos Aridas
0.4.3,License: MIT
0.4.3,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.4.3,License: MIT
0.4.3,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.4.3,License: MIT
0.4.3,Adapated from scikit-learn
0.4.3,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.4.3,License: MIT
0.4.3,FIXME: remove in 0.6
0.4.3,check that estimators treat dtype object as numeric if possible
0.4.3,trigger our checks if this is a SamplerMixin
0.4.3,monkey patch check_dtype_object for the sampler allowing strings
0.4.3,scikit-learn common tests
0.4.3,FIXME: in 0.6 set the random_state for all
0.4.3,IHT does not enforce the number of samples but provide a number
0.4.3,of samples the closest to the desired target.
0.4.3,FIXME remove in 0.6 -> ratio will be deprecated
0.4.3,in this test we will force all samplers to not change the class 1
0.4.3,in this test we will force all samplers to not change the class 1
0.4.3,check that sparse matrices can be passed through the sampler leading to
0.4.3,the same results than dense
0.4.3,set KMeans to full since it support sparse and dense
0.4.3,FIXME: in 0.6 set the random_state for all
0.4.3,Check that the samplers handle pandas dataframe and pandas series
0.4.3,FIXME: in 0.6 set the random_state for all
0.4.3,Check that multiclass target lead to the same results than OVA encoding
0.4.3,FIXME: in 0.6 set the random_state for all
0.4.3,Cast X and y to not default dtype
0.4.3,FIXME: in 0.6 set the random_state for all
0.4.3,Adapted from scikit-learn
0.4.3,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.4.3,License: MIT
0.4.3,meta-estimators need another estimator to be instantiated.
0.4.3,estimators that there is no way to default-construct sensibly
0.4.3,some strange ones
0.4.3,get parent folder
0.4.3,get rid of abstract base classes
0.4.3,get rid of sklearn estimators which have been imported in some classes
0.4.3,possibly get rid of meta estimators
0.4.3,"drop duplicates, sort for reproducibility"
0.4.3,itemgetter is used to ensure the sort does not extend to the 2nd item of
0.4.3,the tuple
0.4.3,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.4.3,License: MIT
0.4.3,FIXME: perfectly we should raise an error but the sklearn API does
0.4.3,not allow for it
0.4.3,check that all keys in sampling_strategy are also in y
0.4.3,check that there is no negative number
0.4.3,FIXME: Turn into an error in 0.6
0.4.3,clean-sampling can be more permissive since those samplers do not
0.4.3,use samples
0.4.3,check that all keys in sampling_strategy are also in y
0.4.3,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.4.3,Christos Aridas
0.4.3,License: MIT
0.4.3,this function could create an equal number of samples
0.4.3,We pass on purpose a non sorted dictionary and check that the resulting
0.4.3,dictionary is sorted. Refer to issue #428.
0.4.3,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.4.3,License: MIT
0.4.3,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.4.3,Christos Aridas
0.4.3,License: MIT
0.4.3,check if the filtering is working with a list or a single string
0.4.3,check that all estimators are sampler
0.4.3,check that an error is raised when the type is unknown
0.4.3,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.4.3,License: MIT
0.4.3,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.4.3,Christos Aridas
0.4.3,License: MIT
0.4.3,Otherwise create a default SMOTE
0.4.3,Otherwise create a default TomekLinks
0.4.3,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.4.3,Christos Aridas
0.4.3,License: MIT
0.4.3,Otherwise create a default SMOTE
0.4.3,Otherwise create a default EditedNearestNeighbours
0.4.3,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.4.3,Christos Aridas
0.4.3,License: MIT
0.4.3,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.4.3,Christos Aridas
0.4.3,License: MIT
0.4.3,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.4.3,License: MIT
0.4.3,resample before to fit the tree
0.4.3,Validate or convert input data
0.4.3,Pre-sort indices to avoid that each individual tree of the
0.4.3,ensemble sorts the indices.
0.4.3,Remap output
0.4.3,reshape is necessary to preserve the data contiguity against vs
0.4.3,"[:, np.newaxis] that does not."
0.4.3,Check parameters
0.4.3,"Free allocated memory, if any"
0.4.3,We draw from the random state to get the random state we
0.4.3,would have got if we hadn't used a warm_start.
0.4.3,Parallel loop: we prefer the threading backend as the Cython code
0.4.3,for fitting the trees is internally releasing the Python GIL
0.4.3,making threading more efficient than multiprocessing in
0.4.3,"that case. However, we respect any parallel_backend contexts set"
0.4.3,"at a higher level, since correctness does not rely on using"
0.4.3,threads.
0.4.3,Collect newly grown trees
0.4.3,Create pipeline with the fitted samplers and trees
0.4.3,Decapsulate classes_ attributes
0.4.3,Instances incorrectly classified
0.4.3,Error fraction
0.4.3,Stop if classification is perfect
0.4.3,Construct y coding as described in Zhu et al [2]:
0.4.3,
0.4.3,y_k = 1 if c == k else -1 / (K - 1)
0.4.3,
0.4.3,"where K == n_classes_ and c, k in [0, K) are indices along the second"
0.4.3,axis of the y coding with c being the index corresponding to the true
0.4.3,class label.
0.4.3,Displace zero probabilities so the log is defined.
0.4.3,Also fix negative elements which may occur with
0.4.3,negative sample weights.
0.4.3,Boost weight using multi-class AdaBoost SAMME.R alg
0.4.3,Only boost the weights if it will fit again
0.4.3,Only boost positive weights
0.4.3,Instances incorrectly classified
0.4.3,Error fraction
0.4.3,Stop if classification is perfect
0.4.3,Stop if the error is at least as bad as random guessing
0.4.3,Boost weight using multi-class AdaBoost SAMME alg
0.4.3,Only boost the weights if I will fit again
0.4.3,Only boost positive weights
0.4.3,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.4.3,Christos Aridas
0.4.3,License: MIT
0.4.3,doctest: +ELLIPSIS
0.4.3,array to know which samples are available to be taken
0.4.3,where the different set will be stored
0.4.3,store the index of the data to under-sample
0.4.3,value which will be picked at each round
0.4.3,extract the data of interest for this round from the
0.4.3,current class
0.4.3,select randomly the desired features
0.4.3,store the set created
0.4.3,fit and predict using cross validation
0.4.3,extract the prediction about the targeted classes only
0.4.3,check the stopping criterion
0.4.3,check that there is enough samples for another round
0.4.3,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.4.3,License: MIT
0.4.3,Ensemble are a bit specific since they are returning an array of
0.4.3,resampled arrays.
0.4.3,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.4.3,Christos Aridas
0.4.3,License: MIT
0.4.3,RandomUnderSampler is not supporting sample_weight. We need to pass
0.4.3,None.
0.4.3,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.4.3,Christos Aridas
0.4.3,License: MIT
0.4.3,RandomUnderSampler is not supporting sample_weight. We need to pass
0.4.3,None.
0.4.3,check that we have an ensemble of samplers and estimators with a
0.4.3,consistent size
0.4.3,each sampler in the ensemble should have different random state
0.4.3,each estimator in the ensemble should have different random state
0.4.3,check the consistency of the feature importances
0.4.3,check the consistency of the prediction outpus
0.4.3,Predictions should be the same when sample_weight are all ones
0.4.3,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.4.3,Christos Aridas
0.4.3,License: MIT
0.4.3,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.4.3,Christos Aridas
0.4.3,License: MIT
0.4.3,Check classification for various parameter settings.
0.4.3,Test that bootstrapping samples generate non-perfect base estimators.
0.4.3,"without bootstrap, all trees are perfect on the training set"
0.4.3,disable the resampling by passing an empty dictionary.
0.4.3,"with bootstrap, trees are no longer perfect on the training set"
0.4.3,Test that bootstrapping features may generate duplicate features.
0.4.3,Predict probabilities.
0.4.3,Normal case
0.4.3,"Degenerate case, where some classes are missing"
0.4.3,Check that oob prediction is a good estimation of the generalization
0.4.3,error.
0.4.3,Test with few estimators
0.4.3,Check singleton ensembles.
0.4.3,Test that it gives proper exception on deficient input.
0.4.3,Test n_estimators
0.4.3,Test max_samples
0.4.3,Test max_features
0.4.3,Test support of decision_function
0.4.3,Check that bagging ensembles can be grid-searched.
0.4.3,Transform iris into a binary classification task
0.4.3,Grid search with scoring based on decision_function
0.4.3,Check base_estimator and its default values.
0.4.3,Test if fitting incrementally with warm start gives a forest of the
0.4.3,right size and the same results as a normal fit.
0.4.3,Test if warm start'ed second fit with smaller n_estimators raises error.
0.4.3,Test that nothing happens when fitting without increasing n_estimators
0.4.3,"modify X to nonsense values, this should not change anything"
0.4.3,warm started classifier with 5+5 estimators should be equivalent to
0.4.3,one classifier with 10 estimators
0.4.3,Check using oob_score and warm_start simultaneously fails
0.4.3,"Make sure OOB scores are identical when random_state, estimator, and"
0.4.3,training data are fixed and fitting is done twice
0.4.3,Check that format of estimators_samples_ is correct and that results
0.4.3,generated at fit time can be identically reproduced at a later time
0.4.3,using data saved in object attributes.
0.4.3,remap the y outside of the BalancedBaggingclassifier
0.4.3,"_, y = np.unique(y, return_inverse=True)"
0.4.3,Get relevant attributes
0.4.3,Test for correct formatting
0.4.3,Re-fit single estimator to test for consistent sampling
0.4.3,Make sure validated max_samples and original max_samples are identical
0.4.3,when valid integer max_samples supplied by user
0.4.3,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.4.3,Christos Aridas
0.4.3,License: MIT
0.4.3,Generate a global dataset to use
0.4.3,Define a sampling_strategy
0.4.3,Define the sampling_strategy parameter
0.4.3,Create the sampling object
0.4.3,Get the different subset
0.4.3,Define the sampling_strategy parameter
0.4.3,Create the sampling object
0.4.3,Get the different subset
0.4.3,Define the sampling_strategy parameter
0.4.3,Create the sampling object
0.4.3,Get the different subset
0.4.3,Check classification for various parameter settings.
0.4.3,test the different prediction function
0.4.3,Check base_estimator and its default values.
0.4.3,Test if fitting incrementally with warm start gives a forest of the
0.4.3,right size and the same results as a normal fit.
0.4.3,Test if warm start'ed second fit with smaller n_estimators raises error.
0.4.3,Test that nothing happens when fitting without increasing n_estimators
0.4.3,"modify X to nonsense values, this should not change anything"
0.4.3,warm started classifier with 5+5 estimators should be equivalent to
0.4.3,one classifier with 10 estimators
0.4.3,Check warning if not enough estimators
0.4.3,Author: Guillaume Lemaitre
0.4.3,License: BSD 3 clause
0.4.3,"The index start at one, then we need to remove one"
0.4.3,to not have issue with the indexing.
0.4.3,go through the list and check if the data are available
0.4.3,Authors: Dayvid Oliveira
0.4.3,Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.4.3,Christos Aridas
0.4.3,License: MIT
0.4.3,restrict ratio to be a dict or a callable
0.4.3,FIXME remove ratio at 0.6
0.4.3,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.4.3,Christos Aridas
0.4.3,License: MIT
0.4.3,check an error is raised with we don't pass sampling_strategy and ratio
0.4.3,"we are reusing part of utils.check_sampling_strategy, however this is not"
0.4.3,cover in the common tests so we will repeat it here
0.4.3,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.4.3,Christos Aridas
0.4.3,License: MIT
0.4.3,This is a trick to avoid an error during tests collection with pytest. We
0.4.3,avoid the error when importing the package raise the error at the moment of
0.4.3,creating the instance.
0.4.3,FIXME: Remove in 0.6
0.4.3,shuffle the indices since the sampler are packing them by class
0.4.2,This file is here so that when running from the root folder
0.4.2,./sklearn is added to sys.path by pytest.
0.4.2,See https://docs.pytest.org/en/latest/pythonpath.html for more details.
0.4.2,"For example, this allows to build extensions in place and run pytest"
0.4.2,doc/modules/clustering.rst and use sklearn from the local folder
0.4.2,rather than the one from site-packages.
0.4.2,Set numpy array str/repr to legacy behaviour on numpy > 1.13 to make
0.4.2,the doctests pass
0.4.2,! /usr/bin/env python
0.4.2,get __version__ from _version.py
0.4.2,-*- coding: utf-8 -*-
0.4.2,
0.4.2,"imbalanced-learn documentation build configuration file, created by"
0.4.2,sphinx-quickstart on Mon Jan 18 14:44:12 2016.
0.4.2,
0.4.2,This file is execfile()d with the current directory set to its
0.4.2,containing dir.
0.4.2,
0.4.2,Note that not all possible configuration values are present in this
0.4.2,autogenerated file.
0.4.2,
0.4.2,All configuration values have a default; values that are commented out
0.4.2,serve to show the default.
0.4.2,"If extensions (or modules to document with autodoc) are in another directory,"
0.4.2,add these directories to sys.path here. If the directory is relative to the
0.4.2,"documentation root, use os.path.abspath to make it absolute, like shown here."
0.4.2,-- General configuration ------------------------------------------------
0.4.2,"If your documentation needs a minimal Sphinx version, state it here."
0.4.2,needs_sphinx = '1.0'
0.4.2,"Add any Sphinx extension module names here, as strings. They can be"
0.4.2,extensions coming with Sphinx (named 'sphinx.ext.*') or your custom
0.4.2,ones.
0.4.2,this is needed for some reason...
0.4.2,see https://github.com/numpy/numpydoc/issues/69
0.4.2,pngmath / imgmath compatibility layer for different sphinx versions
0.4.2,"Add any paths that contain templates here, relative to this directory."
0.4.2,generate autosummary even if no references
0.4.2,The suffix of source filenames.
0.4.2,The encoding of source files.
0.4.2,source_encoding = 'utf-8-sig'
0.4.2,Generate the plot for the gallery
0.4.2,The master toctree document.
0.4.2,General information about the project.
0.4.2,"The version info for the project you're documenting, acts as replacement for"
0.4.2,"|version| and |release|, also used in various other places throughout the"
0.4.2,built documents.
0.4.2,
0.4.2,The short X.Y version.
0.4.2,"The full version, including alpha/beta/rc tags."
0.4.2,The language for content autogenerated by Sphinx. Refer to documentation
0.4.2,for a list of supported languages.
0.4.2,language = None
0.4.2,"There are two options for replacing |today|: either, you set today to some"
0.4.2,"non-false value, then it is used:"
0.4.2,today = ''
0.4.2,"Else, today_fmt is used as the format for a strftime call."
0.4.2,"today_fmt = '%B %d, %Y'"
0.4.2,"List of patterns, relative to source directory, that match files and"
0.4.2,directories to ignore when looking for source files.
0.4.2,The reST default role (used for this markup: `text`) to use for all
0.4.2,documents.
0.4.2,default_role = None
0.4.2,"If true, '()' will be appended to :func: etc. cross-reference text."
0.4.2,"If true, the current module name will be prepended to all description"
0.4.2,unit titles (such as .. function::).
0.4.2,add_module_names = True
0.4.2,"If true, sectionauthor and moduleauthor directives will be shown in the"
0.4.2,output. They are ignored by default.
0.4.2,show_authors = False
0.4.2,The name of the Pygments (syntax highlighting) style to use.
0.4.2,Custom style
0.4.2,A list of ignored prefixes for module index sorting.
0.4.2,modindex_common_prefix = []
0.4.2,"If true, keep warnings as ""system message"" paragraphs in the built documents."
0.4.2,keep_warnings = False
0.4.2,-- Options for HTML output ----------------------------------------------
0.4.2,The theme to use for HTML and HTML Help pages.  See the documentation for
0.4.2,a list of builtin themes.
0.4.2,Theme options are theme-specific and customize the look and feel of a theme
0.4.2,"further.  For a list of options available for each theme, see the"
0.4.2,documentation.
0.4.2,html_theme_options = {'prev_next_buttons_location': None}
0.4.2,"Add any paths that contain custom themes here, relative to this directory."
0.4.2,"The name for this set of Sphinx documents.  If None, it defaults to"
0.4.2,"""<project> v<release> documentation""."
0.4.2,html_title = None
0.4.2,A shorter title for the navigation bar.  Default is the same as html_title.
0.4.2,html_short_title = None
0.4.2,The name of an image file (relative to this directory) to place at the top
0.4.2,of the sidebar.
0.4.2,html_logo = None
0.4.2,The name of an image file (within the static path) to use as favicon of the
0.4.2,docs.  This file should be a Windows icon file (.ico) being 16x16 or 32x32
0.4.2,pixels large.
0.4.2,html_favicon = None
0.4.2,"Add any paths that contain custom static files (such as style sheets) here,"
0.4.2,"relative to this directory. They are copied after the builtin static files,"
0.4.2,"so a file named ""default.css"" will overwrite the builtin ""default.css""."
0.4.2,Add any extra paths that contain custom files (such as robots.txt or
0.4.2,".htaccess) here, relative to this directory. These files are copied"
0.4.2,directly to the root of the documentation.
0.4.2,html_extra_path = []
0.4.2,"If not '', a 'Last updated on:' timestamp is inserted at every page bottom,"
0.4.2,using the given strftime format.
0.4.2,"html_last_updated_fmt = '%b %d, %Y'"
0.4.2,"If true, SmartyPants will be used to convert quotes and dashes to"
0.4.2,typographically correct entities.
0.4.2,html_use_smartypants = True
0.4.2,"Custom sidebar templates, maps document names to template names."
0.4.2,html_sidebars = {}
0.4.2,"Additional templates that should be rendered to pages, maps page names to"
0.4.2,template names.
0.4.2,html_additional_pages = {}
0.4.2,"If false, no module index is generated."
0.4.2,html_domain_indices = True
0.4.2,"If false, no index is generated."
0.4.2,html_use_index = True
0.4.2,"If true, the index is split into individual pages for each letter."
0.4.2,html_split_index = False
0.4.2,"If true, links to the reST sources are added to the pages."
0.4.2,html_show_sourcelink = True
0.4.2,"If true, ""Created using Sphinx"" is shown in the HTML footer. Default is True."
0.4.2,html_show_sphinx = True
0.4.2,"If true, ""(C) Copyright ..."" is shown in the HTML footer. Default is True."
0.4.2,html_show_copyright = True
0.4.2,"If true, an OpenSearch description file will be output, and all pages will"
0.4.2,contain a <link> tag referring to it.  The value of this option must be the
0.4.2,base URL from which the finished HTML is served.
0.4.2,html_use_opensearch = ''
0.4.2,"This is the file name suffix for HTML files (e.g. "".xhtml"")."
0.4.2,html_file_suffix = None
0.4.2,Output file base name for HTML help builder.
0.4.2,-- Options for LaTeX output ---------------------------------------------
0.4.2,The paper size ('letterpaper' or 'a4paper').
0.4.2,"'papersize': 'letterpaper',"
0.4.2,"The font size ('10pt', '11pt' or '12pt')."
0.4.2,"'pointsize': '10pt',"
0.4.2,Additional stuff for the LaTeX preamble.
0.4.2,"'preamble': '',"
0.4.2,Grouping the document tree into LaTeX files. List of tuples
0.4.2,"(source start file, target name, title,"
0.4.2,"author, documentclass [howto, manual, or own class])."
0.4.2,The name of an image file (relative to this directory) to place at the top of
0.4.2,the title page.
0.4.2,latex_logo = None
0.4.2,"For ""manual"" documents, if this is true, then toplevel headings are parts,"
0.4.2,not chapters.
0.4.2,latex_use_parts = False
0.4.2,"If true, show page references after internal links."
0.4.2,latex_show_pagerefs = False
0.4.2,"If true, show URL addresses after external links."
0.4.2,latex_show_urls = False
0.4.2,Documents to append as an appendix to all manuals.
0.4.2,latex_appendices = []
0.4.2,intersphinx configuration
0.4.2,sphinx-gallery configuration
0.4.2,-- Options for manual page output ---------------------------------------
0.4.2,"If false, no module index is generated."
0.4.2,latex_domain_indices = True
0.4.2,One entry per manual page. List of tuples
0.4.2,"(source start file, name, description, authors, manual section)."
0.4.2,"If true, show URL addresses after external links."
0.4.2,man_show_urls = False
0.4.2,-- Options for Texinfo output -------------------------------------------
0.4.2,Grouping the document tree into Texinfo files. List of tuples
0.4.2,"(source start file, target name, title, author,"
0.4.2,"dir menu entry, description, category)"
0.4.2,"def generate_example_rst(app, what, name, obj, options, lines):"
0.4.2,"# generate empty examples files, so that we don't get"
0.4.2,# inclusion errors if there are no examples for a class / module
0.4.2,"examples_path = os.path.join(app.srcdir, ""generated"","
0.4.2,"""%s.examples"" % name)"
0.4.2,if not os.path.exists(examples_path):
0.4.2,# touch file
0.4.2,"open(examples_path, 'w').close()"
0.4.2,Config for sphinx_issues
0.4.2,"app.connect('autodoc-process-docstring', generate_example_rst)"
0.4.2,Documents to append as an appendix to all manuals.
0.4.2,texinfo_appendices = []
0.4.2,"If false, no module index is generated."
0.4.2,texinfo_domain_indices = True
0.4.2,"How to display URL addresses: 'footnote', 'no', or 'inline'."
0.4.2,texinfo_show_urls = 'footnote'
0.4.2,"If true, do not generate a @detailmenu in the ""Top"" node's menu."
0.4.2,texinfo_no_detailmenu = False
0.4.2,The following is used by sphinx.ext.linkcode to provide links to github
0.4.2,get the styles from the current theme
0.4.2,create and add the button to all the code blocks that contain >>>
0.4.2,tracebacks (.gt) contain bare text elements that need to be
0.4.2,wrapped in a span to work with .nextUntil() (see later)
0.4.2,define the behavior of the button when it's clicked
0.4.2,hide the code output
0.4.2,show the code output
0.4.2,-*- coding: utf-8 -*-
0.4.2,Format template for issues URI
0.4.2,e.g. 'https://github.com/sloria/marshmallow/issues/{issue}
0.4.2,"Shortcut for Github, e.g. 'sloria/marshmallow'"
0.4.2,Format template for user profile URI
0.4.2,e.g. 'https://github.com/{user}'
0.4.2,Python 2 only
0.4.2,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.4.2,License: MIT
0.4.2,##############################################################################
0.4.2,"First, we will create an imbalanced data set from a the iris data set."
0.4.2,##############################################################################
0.4.2,Using ``sampling_strategy`` in resampling algorithms
0.4.2,##############################################################################
0.4.2,##############################################################################
0.4.2,``sampling_strategy`` as a ``float``
0.4.2,....................................
0.4.2,
0.4.2,``sampling_strategy`` can be given a ``float``. For **under-sampling
0.4.2,"methods**, it corresponds to the ratio :math:`\\alpha_{us}` defined by"
0.4.2,:math:`N_{rM} = \\alpha_{us} \\times N_{m}` where :math:`N_{rM}` and
0.4.2,:math:`N_{m}` are the number of samples in the majority class after
0.4.2,"resampling and the number of samples in the minority class, respectively."
0.4.2,select only 2 classes since the ratio make sense in this case
0.4.2,##############################################################################
0.4.2,"For **over-sampling methods**, it correspond to the ratio"
0.4.2,:math:`\\alpha_{os}` defined by :math:`N_{rm} = \\alpha_{os} \\times N_{m}`
0.4.2,where :math:`N_{rm}` and :math:`N_{M}` are the number of samples in the
0.4.2,minority class after resampling and the number of samples in the majority
0.4.2,"class, respectively."
0.4.2,##############################################################################
0.4.2,``sampling_strategy`` has a ``str``
0.4.2,...................................
0.4.2,
0.4.2,``sampling_strategy`` can be given as a string which specify the class
0.4.2,"targeted by the resampling. With under- and over-sampling, the number of"
0.4.2,samples will be equalized.
0.4.2,
0.4.2,Note that we are using multiple classes from now on.
0.4.2,##############################################################################
0.4.2,"With **cleaning method**, the number of samples in each class will not be"
0.4.2,equalized even if targeted.
0.4.2,##############################################################################
0.4.2,``sampling_strategy`` as a ``dict``
0.4.2,...................................
0.4.2,
0.4.2,"When ``sampling_strategy`` is a ``dict``, the keys correspond to the targeted"
0.4.2,classes. The values correspond to the desired number of samples for each
0.4.2,targeted class. This is working for both **under- and over-sampling**
0.4.2,algorithms but not for the **cleaning algorithms**. Use a ``list`` instead.
0.4.2,##############################################################################
0.4.2,``sampling_strategy`` as a ``list``
0.4.2,...................................
0.4.2,
0.4.2,"When ``sampling_strategy`` is a ``list``, the list contains the targeted"
0.4.2,classes. It is used only for **cleaning methods** and raise an error
0.4.2,otherwise.
0.4.2,##############################################################################
0.4.2,``sampling_strategy`` as a callable
0.4.2,...................................
0.4.2,
0.4.2,"When callable, function taking ``y`` and returns a ``dict``. The keys"
0.4.2,correspond to the targeted classes. The values correspond to the desired
0.4.2,number of samples for each class.
0.4.2,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.4.2,License: MIT
0.4.2,#############################################################################
0.4.2,Toy data generation
0.4.2,#############################################################################
0.4.2,#############################################################################
0.4.2,We are generating some non Gaussian data set contaminated with some unform
0.4.2,noise.
0.4.2,#############################################################################
0.4.2,We will generate some cleaned test data without outliers.
0.4.2,#############################################################################
0.4.2,How to use the :class:`imblearn.FunctionSampler`
0.4.2,#############################################################################
0.4.2,#############################################################################
0.4.2,We first define a function which will use
0.4.2,:class:`sklearn.ensemble.IsolationForest` to eliminate some outliers from
0.4.2,our dataset during training. The function passed to the
0.4.2,:class:`imblearn.FunctionSampler` will be called when using the method
0.4.2,``fit_resample``.
0.4.2,#############################################################################
0.4.2,Integrate it within a pipeline
0.4.2,#############################################################################
0.4.2,#############################################################################
0.4.2,"By elimnating outliers before the training, the classifier will be less"
0.4.2,affected during the prediction.
0.4.2,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.4.2,License: MIT
0.4.2,generate some data points
0.4.2,plot the majority and minority samples
0.4.2,draw the circle in which the new sample will generated
0.4.2,plot the line on which the sample will be generated
0.4.2,create and plot the new sample
0.4.2,make the plot nicer with legend and label
0.4.2,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.4.2,License: MIT
0.4.2,##############################################################################
0.4.2,The following function will be used to create toy dataset. It using the
0.4.2,``make_classification`` from scikit-learn but fixing some parameters.
0.4.2,##############################################################################
0.4.2,The following function will be used to plot the sample space after resampling
0.4.2,to illustrate the characterisitic of an algorithm.
0.4.2,make nice plotting
0.4.2,##############################################################################
0.4.2,The following function will be used to plot the decision function of a
0.4.2,classifier given some data.
0.4.2,##############################################################################
0.4.2,Illustration of the influence of the balancing ratio
0.4.2,##############################################################################
0.4.2,##############################################################################
0.4.2,We will first illustrate the influence of the balancing ratio on some toy
0.4.2,data using a linear SVM classifier. Greater is the difference between the
0.4.2,"number of samples in each class, poorer are the classfication results."
0.4.2,##############################################################################
0.4.2,Random over-sampling to balance the data set
0.4.2,##############################################################################
0.4.2,##############################################################################
0.4.2,Random over-sampling can be used to repeat some samples and balance the
0.4.2,number of samples between the dataset. It can be seen that with this trivial
0.4.2,approach the boundary decision is already less biaised toward the majority
0.4.2,class.
0.4.2,##############################################################################
0.4.2,More advanced over-sampling using ADASYN and SMOTE
0.4.2,##############################################################################
0.4.2,##############################################################################
0.4.2,"Instead of repeating the same samples when over-sampling, we can use some"
0.4.2,specific heuristic instead. ADASYN and SMOTE can be used in this case.
0.4.2,Make an identity sampler
0.4.2,##############################################################################
0.4.2,The following plot illustrate the difference between ADASYN and SMOTE. ADASYN
0.4.2,will focus on the samples which are difficult to classify with a
0.4.2,nearest-neighbors rule while regular SMOTE will not make any distinction.
0.4.2,"Therefore, the decision function depending of the algorithm."
0.4.2,##############################################################################
0.4.2,"Due to those sampling particularities, it can give rise to some specific"
0.4.2,issues as illustrated below.
0.4.2,##############################################################################
0.4.2,SMOTE proposes several variants by identifying specific samples to consider
0.4.2,during the resampling. The borderline version will detect which point to
0.4.2,select which are in the border between two classes. The SVM version will use
0.4.2,the support vectors found using an SVM algorithm to create new samples.
0.4.2,##############################################################################
0.4.2,"When dealing with a mixed of continuous and categorical features, SMOTE-NC"
0.4.2,is the only method which can handle this case.
0.4.2,create a synthetic data set with continuous and categorical features
0.4.2,Authors: Christos Aridas
0.4.2,Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.4.2,License: MIT
0.4.2,Generate the dataset
0.4.2,make nice plotting
0.4.2,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.4.2,License: MIT
0.4.2,Generate a dataset
0.4.2,Split the data
0.4.2,Train the classifier with balancing
0.4.2,Test the classifier and get the prediction
0.4.2,Show the classification report
0.4.2,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.4.2,License: MIT
0.4.2,Generate a dataset
0.4.2,Split the data
0.4.2,Train the classifier with balancing
0.4.2,Test the classifier and get the prediction
0.4.2,##############################################################################
0.4.2,The geometric mean corresponds to the square root of the product of the
0.4.2,sensitivity and specificity. Combining the two metrics should account for
0.4.2,the balancing of the dataset.
0.4.2,##############################################################################
0.4.2,The index balanced accuracy can transform any metric to be used in
0.4.2,imbalanced learning problems.
0.4.2,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.4.2,License: MIT
0.4.2,##############################################################################
0.4.2,The following function will be used to create toy dataset. It using the
0.4.2,``make_classification`` from scikit-learn but fixing some parameters.
0.4.2,##############################################################################
0.4.2,The following function will be used to plot the sample space after resampling
0.4.2,to illustrate the characteristic of an algorithm.
0.4.2,make nice plotting
0.4.2,##############################################################################
0.4.2,The following function will be used to plot the decision function of a
0.4.2,classifier given some data.
0.4.2,##############################################################################
0.4.2,"``SMOTE`` allows to generate samples. However, this method of over-sampling"
0.4.2,"does not have any knowledge regarding the underlying distribution. Therefore,"
0.4.2,"some noisy samples can be generated, e.g. when the different classes cannot"
0.4.2,"be well separated. Hence, it can be beneficial to apply an under-sampling"
0.4.2,algorithm to clean the noisy samples. Two methods are usually used in the
0.4.2,literature: (i) Tomek's link and (ii) edited nearest neighbours cleaning
0.4.2,methods. Imbalanced-learn provides two ready-to-use samplers ``SMOTETomek``
0.4.2,"and ``SMOTEENN``. In general, ``SMOTEENN`` cleans more noisy data than"
0.4.2,``SMOTETomek``.
0.4.2,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.4.2,License: MIT
0.4.2,##############################################################################
0.4.2,Load an imbalanced dataset
0.4.2,##############################################################################
0.4.2,We will load the UCI SatImage dataset which has an imbalanced ratio of 9.3:1
0.4.2,(number of majority sample for a minority sample). The data are then split
0.4.2,into training and testing.
0.4.2,##############################################################################
0.4.2,Classification using a single decision tree
0.4.2,##############################################################################
0.4.2,We train a decision tree classifier which will be used as a baseline for the
0.4.2,rest of this example.
0.4.2,##############################################################################
0.4.2,The results are reported in terms of balanced accuracy and geometric mean
0.4.2,which are metrics widely used in the literature to validate model trained on
0.4.2,imbalanced set.
0.4.2,##############################################################################
0.4.2,Classification using bagging classifier with and without sampling
0.4.2,##############################################################################
0.4.2,"Instead of using a single tree, we will check if an ensemble of decsion tree"
0.4.2,"can actually alleviate the issue induced by the class imbalancing. First, we"
0.4.2,will use a bagging classifier and its counter part which internally uses a
0.4.2,random under-sampling to balanced each boostrap sample.
0.4.2,##############################################################################
0.4.2,Balancing each bootstrap sample allows to increase significantly the balanced
0.4.2,accuracy and the geometric mean.
0.4.2,##############################################################################
0.4.2,Classification using random forest classifier with and without sampling
0.4.2,##############################################################################
0.4.2,Random forest is another popular ensemble method and it is usually
0.4.2,"outperforming bagging. Here, we used a vanilla random forest and its balanced"
0.4.2,counterpart in which each bootstrap sample is balanced.
0.4.2,"Similarly to the previous experiment, the balanced classifier outperform the"
0.4.2,"classifier which learn from imbalanced bootstrap samples. In addition, random"
0.4.2,forest outsperforms the bagging classifier.
0.4.2,##############################################################################
0.4.2,Boosting classifier
0.4.2,##############################################################################
0.4.2,"In the same manner, easy ensemble classifier is a bag of balanced AdaBoost"
0.4.2,"classifier. However, it will be slower to train than random forest and will"
0.4.2,achieve worse performance.
0.4.2,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.4.2,License: MIT
0.4.2,##############################################################################
0.4.2,The following function will be used to create toy dataset. It using the
0.4.2,``make_classification`` from scikit-learn but fixing some parameters.
0.4.2,##############################################################################
0.4.2,The following function will be used to plot the sample space after resampling
0.4.2,to illustrate the characteristic of an algorithm.
0.4.2,make nice plotting
0.4.2,##############################################################################
0.4.2,The following function will be used to plot the decision function of a
0.4.2,classifier given some data.
0.4.2,##############################################################################
0.4.2,Prototype generation: under-sampling by generating new samples
0.4.2,##############################################################################
0.4.2,##############################################################################
0.4.2,``ClusterCentroids`` under-samples by replacing the original samples by the
0.4.2,centroids of the cluster found.
0.4.2,##############################################################################
0.4.2,Prototype selection: under-sampling by selecting existing samples
0.4.2,##############################################################################
0.4.2,##############################################################################
0.4.2,The algorithm performing prototype selection can be subdivided into two
0.4.2,groups: (i) the controlled under-sampling methods and (ii) the cleaning
0.4.2,under-sampling methods.
0.4.2,##############################################################################
0.4.2,"With the controlled under-sampling methods, the number of samples to be"
0.4.2,selected can be specified. ``RandomUnderSampler`` is the most naive way of
0.4.2,performing such selection by randomly selecting a given number of samples by
0.4.2,the targetted class.
0.4.2,##############################################################################
0.4.2,``NearMiss`` algorithms implement some heuristic rules in order to select
0.4.2,samples. NearMiss-1 selects samples from the majority class for which the
0.4.2,average distance of the :math:`k`` nearest samples of the minority class is
0.4.2,the smallest. NearMiss-2 selects the samples from the majority class for
0.4.2,which the average distance to the farthest samples of the negative class is
0.4.2,"the smallest. NearMiss-3 is a 2-step algorithm: first, for each minority"
0.4.2,"sample, their ::math:`m` nearest-neighbors will be kept; then, the majority"
0.4.2,samples selected are the on for which the average distance to the :math:`k`
0.4.2,nearest neighbors is the largest.
0.4.2,##############################################################################
0.4.2,``EditedNearestNeighbours`` removes samples of the majority class for which
0.4.2,their class differ from the one of their nearest-neighbors. This sieve can be
0.4.2,repeated which is the principle of the
0.4.2,``RepeatedEditedNearestNeighbours``. ``AllKNN`` is slightly different from
0.4.2,the ``RepeatedEditedNearestNeighbours`` by changing the :math:`k` parameter
0.4.2,"of the internal nearest neighors algorithm, increasing it at each iteration."
0.4.2,##############################################################################
0.4.2,``CondensedNearestNeighbour`` makes use of a 1-NN to iteratively decide if a
0.4.2,sample should be kept in a dataset or not. The issue is that
0.4.2,``CondensedNearestNeighbour`` is sensitive to noise by preserving the noisy
0.4.2,samples. ``OneSidedSelection`` also used the 1-NN and use ``TomekLinks`` to
0.4.2,remove the samples considered noisy. The ``NeighbourhoodCleaningRule`` use a
0.4.2,"``EditedNearestNeighbours`` to remove some sample. Additionally, they use a 3"
0.4.2,nearest-neighbors to remove samples which do not agree with this rule.
0.4.2,##############################################################################
0.4.2,``InstanceHardnessThreshold`` uses the prediction of classifier to exclude
0.4.2,samples. All samples which are classified with a low probability will be
0.4.2,removed.
0.4.2,##############################################################################
0.4.2,This function allows to make nice plotting
0.4.2,##############################################################################
0.4.2,Generate some data with one Tomek link
0.4.2,minority class
0.4.2,majority class
0.4.2,##############################################################################
0.4.2,"In the figure above, the samples highlighted in green form a Tomek link since"
0.4.2,they are of different classes and are nearest neighbours of each other.
0.4.2,highlight the samples of interest
0.4.2,##############################################################################
0.4.2,We can run the ``TomekLinks`` sampling to remove the corresponding
0.4.2,samples. If ``sampling_strategy='auto'`` only the sample from the majority
0.4.2,class will be removed. If ``sampling_strategy='all'`` both samples will be
0.4.2,removed.
0.4.2,highlight the samples of interest
0.4.2,##############################################################################
0.4.2,This function allows to make nice plotting
0.4.2,##############################################################################
0.4.2,We can start by generating some data to later illustrate the principle of
0.4.2,each NearMiss heuritic rules.
0.4.2,minority class
0.4.2,majority class
0.4.2,##############################################################################
0.4.2,NearMiss-1
0.4.2,##############################################################################
0.4.2,##############################################################################
0.4.2,NearMiss-1 selects samples from the majority class for which the average
0.4.2,distance to some nearest neighbours is the smallest. In the following
0.4.2,"example, we use a 3-NN to compute the average distance on 2 specific samples"
0.4.2,"of the majority class. Therefore, in this case the point linked by the"
0.4.2,green-dashed line will be selected since the average distance is smaller.
0.4.2,##############################################################################
0.4.2,NearMiss-2
0.4.2,##############################################################################
0.4.2,##############################################################################
0.4.2,NearMiss-2 selects samples from the majority class for which the average
0.4.2,distance to the farthest neighbors is the smallest. With the same
0.4.2,"configuration as previously presented, the sample linked to the green-dashed"
0.4.2,line will be selected since its distance the 3 farthest neighbors is the
0.4.2,smallest.
0.4.2,##############################################################################
0.4.2,NearMiss-3
0.4.2,##############################################################################
0.4.2,##############################################################################
0.4.2,"NearMiss-3 can be divided into 2 steps. First, a nearest-neighbors is used to"
0.4.2,short-list samples from the majority class (i.e. correspond to the
0.4.2,"highlighted samples in the following plot). Then, the sample with the largest"
0.4.2,average distance to the *k* nearest-neighbors are selected.
0.4.2,select only the majority point of interest
0.4.2,Authors: Christos Aridas
0.4.2,Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.4.2,License: MIT
0.4.2,Generate the dataset
0.4.2,Instanciate a PCA object for the sake of easy visualisation
0.4.2,Create the samplers
0.4.2,Create the classifier
0.4.2,Make the splits
0.4.2,Add one transformers and two samplers in the pipeline object
0.4.2,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.4.2,License: MIT
0.4.2,##############################################################################
0.4.2,Data loading
0.4.2,##############################################################################
0.4.2,##############################################################################
0.4.2,"First, you should download the Porto Seguro data set from Kaggle. See the"
0.4.2,link in the introduction.
0.4.2,##############################################################################
0.4.2,The data set is imbalanced and it will have an effect on the fitting.
0.4.2,##############################################################################
0.4.2,Define the pre-processing pipeline
0.4.2,##############################################################################
0.4.2,##############################################################################
0.4.2,We want to standard scale the numerical features while we want to one-hot
0.4.2,"encode the categorical features. In this regard, we make use of the"
0.4.2,:class:`sklearn.compose.ColumnTransformer`.
0.4.2,Create an environment variable to avoid using the GPU. This can be changed.
0.4.2,##############################################################################
0.4.2,Create a neural-network
0.4.2,##############################################################################
0.4.2,##############################################################################
0.4.2,We create a decorator to report the computation time
0.4.2,##############################################################################
0.4.2,The first model will be trained using the ``fit`` method and with imbalanced
0.4.2,mini-batches.
0.4.2,##############################################################################
0.4.2,"In the contrary, we will use imbalanced-learn to create a generator of"
0.4.2,mini-batches which will yield balanced mini-batches.
0.4.2,##############################################################################
0.4.2,Classification loop
0.4.2,##############################################################################
0.4.2,##############################################################################
0.4.2,We will perform a 10-fold cross-validation and train the neural-network with
0.4.2,the two different strategies previously presented.
0.4.2,##############################################################################
0.4.2,Plot of the results and computation time
0.4.2,##############################################################################
0.4.2,Authors: Christos Aridas
0.4.2,Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.4.2,License: MIT
0.4.2,Load the dataset
0.4.2,make nice plotting
0.4.2,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.4.2,License: MIT
0.4.2,Create a folder to fetch the dataset
0.4.2,Create a pipeline
0.4.2,Classify and report the results
0.4.2,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.4.2,License: MIT
0.4.2,##############################################################################
0.4.2,Setting the data set
0.4.2,##############################################################################
0.4.2,##############################################################################
0.4.2,We use a part of the 20 newsgroups data set by loading 4 topics. Using the
0.4.2,"scikit-learn loader, the data are split into a training and a testing set."
0.4.2,
0.4.2,Note the class \#3 is the minority class and has almost twice less samples
0.4.2,than the majority class.
0.4.2,##############################################################################
0.4.2,The usual scikit-learn pipeline
0.4.2,##############################################################################
0.4.2,##############################################################################
0.4.2,You might usually use scikit-learn pipeline by combining the TF-IDF
0.4.2,vectorizer to feed a multinomial naive bayes classifier. A classification
0.4.2,report summarized the results on the testing set.
0.4.2,
0.4.2,"As expected, the recall of the class \#3 is low mainly due to the class"
0.4.2,imbalanced.
0.4.2,##############################################################################
0.4.2,Balancing the class before classification
0.4.2,##############################################################################
0.4.2,##############################################################################
0.4.2,"To improve the prediction of the class \#3, it could be interesting to apply"
0.4.2,"a balancing before to train the naive bayes classifier. Therefore, we will"
0.4.2,use a ``RandomUnderSampler`` to equalize the number of samples in all the
0.4.2,classes before the training.
0.4.2,
0.4.2,It is also important to note that we are using the ``make_pipeline`` function
0.4.2,implemented in imbalanced-learn to properly handle the samplers.
0.4.2,##############################################################################
0.4.2,"Although the results are almost identical, it can be seen that the resampling"
0.4.2,allowed to correct the poor recall of the class \#3 at the cost of reducing
0.4.2,"the other metrics for the other classes. However, the overall results are"
0.4.2,slightly better.
0.4.2,Authors: Dayvid Oliveira
0.4.2,Christos Aridas
0.4.2,Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.4.2,License: MIT
0.4.2,Generate the dataset
0.4.2,"Two subplots, unpack the axes array immediately"
0.4.2,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.4.2,License: MIT
0.4.2,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.4.2,Christos Aridas
0.4.2,License: MIT
0.4.2,define an alias for back-compatibility
0.4.2,FIXME: remove in 0.6
0.4.2,FIXME: remove in 0.6
0.4.2,both ratio and sampling_strategy should not be set
0.4.2,Adapted from scikit-learn
0.4.2,Author: Edouard Duchesnay
0.4.2,Gael Varoquaux
0.4.2,Virgile Fritsch
0.4.2,Alexandre Gramfort
0.4.2,Lars Buitinck
0.4.2,Christos Aridas
0.4.2,Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.4.2,License: BSD
0.4.2,BaseEstimator interface
0.4.2,validate names
0.4.2,validate estimators
0.4.2,We allow last estimator to be None as an identity transformation
0.4.2,Estimator interface
0.4.2,Setup the memory
0.4.2,joblib >= 0.12
0.4.2,we do not clone when caching is disabled to
0.4.2,preserve backward compatibility
0.4.2,joblib < 0.11
0.4.2,we do not clone when caching is disabled to
0.4.2,preserve backward compatibility
0.4.2,Fit or load from cache the current transfomer
0.4.2,Replace the transformer of the step with the fitted
0.4.2,transformer. This is necessary when loading the transformer
0.4.2,from the cache.
0.4.2,"_final_estimator is None or has transform, otherwise attribute error"
0.4.2,raise AttributeError if necessary for hasattr behaviour
0.4.2,"if we have a weight for this transformer, multiply output"
0.4.2,Based on NiLearn package
0.4.2,License: simplified BSD
0.4.2,"PEP0440 compatible formatted version, see:"
0.4.2,https://www.python.org/dev/peps/pep-0440/
0.4.2,
0.4.2,Generic release markers:
0.4.2,X.Y
0.4.2,X.Y.Z # For bugfix releases
0.4.2,
0.4.2,Admissible pre-release markers:
0.4.2,X.YaN # Alpha release
0.4.2,X.YbN # Beta release
0.4.2,X.YrcN # Release Candidate
0.4.2,X.Y # Final release
0.4.2,
0.4.2,Dev branch marker is: 'X.Y.dev' or 'X.Y.devN' where N is an integer.
0.4.2,'X.Y.dev0' is the canonical version of 'X.Y.dev'
0.4.2,
0.4.2,coding: utf-8
0.4.2,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.4.2,Dariusz Brzezinski
0.4.2,License: MIT
0.4.2,Only negative labels
0.4.2,"Calculate tp_sum, pred_sum, true_sum ###"
0.4.2,labels are now from 0 to len(labels) - 1 -> use bincount
0.4.2,Pathological case
0.4.2,Compute the true negative
0.4.2,Retain only selected labels
0.4.2,"Finally, we have all our sufficient statistics. Divide! #"
0.4.2,"Divide, and on zero-division, set scores to 0 and warn:"
0.4.2,"Oddly, we may get an ""invalid"" rather than a ""divide"" error"
0.4.2,here.
0.4.2,Average the results
0.4.2,labels are now from 0 to len(labels) - 1 -> use bincount
0.4.2,Pathological case
0.4.2,Retain only selected labels
0.4.2,old version of scipy return MaskedConstant instead of 0.0
0.4.2,Create the list of tags
0.4.2,check that the scoring function does not need a score
0.4.2,and only a prediction
0.4.2,Compute the score from the scoring function
0.4.2,Square if desired
0.4.2,Get the signature of the sens/spec function
0.4.2,We need to extract from kwargs only the one needed by the
0.4.2,specificity and specificity
0.4.2,Make the intersection between the parameters
0.4.2,Create a sub dictionary
0.4.2,Check if the metric is the geometric mean
0.4.2,We do not support multilabel so the only average supported
0.4.2,is binary
0.4.2,Create the list of parameters through signature binding
0.4.2,Call the sens/spec function
0.4.2,Compute the dominance
0.4.2,Compute the different metrics
0.4.2,Precision/recall/f1
0.4.2,Specificity
0.4.2,Geometric mean
0.4.2,Index balanced accuracy
0.4.2,compute averages
0.4.2,coding: utf-8
0.4.2,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.4.2,Christos Aridas
0.4.2,License: MIT
0.4.2,##############################################################################
0.4.2,Utilities for testing
0.4.2,import some data to play with
0.4.2,restrict to a binary classification task
0.4.2,add noisy features to make the problem harder and avoid perfect results
0.4.2,"run classifier, get class probabilities and label predictions"
0.4.2,only interested in probabilities of the positive case
0.4.2,XXX: do we really want a special API for the binary case?
0.4.2,##############################################################################
0.4.2,Tests
0.4.2,detailed measures for each class
0.4.2,individual scoring function that can be used for grid search: in the
0.4.2,binary class case the score is the value of the measure for the positive
0.4.2,class (e.g. label == 1). This is deprecated for average != 'binary'.
0.4.2,Such a case may occur with non-stratified cross-validation
0.4.2,ensure the above were meaningful tests:
0.4.2,Bad pos_label
0.4.2,Bad average option
0.4.2,but average != 'binary'; even if data is binary
0.4.2,compute the geometric mean for the binary problem
0.4.2,print classification report with class names
0.4.2,print classification report with label detection
0.4.2,print classification report with class names
0.4.2,print classification report with label detection
0.4.2,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.4.2,Christos Aridas
0.4.2,License: MIT
0.4.2,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.4.2,Christos Aridas
0.4.2,License: MIT
0.4.2,The ratio is computed using a one-vs-rest manner. Using majority
0.4.2,in multi-class would lead to slightly different results at the
0.4.2,cost of introducing a new parameter.
0.4.2,the nearest neighbors need to be fitted only on the current class
0.4.2,to find the class NN to generate new samples
0.4.2,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.4.2,Christos Aridas
0.4.2,License: MIT
0.4.2,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.4.2,Christos Aridas
0.4.2,License: MIT
0.4.2,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.4.2,Fernando Nogueira
0.4.2,Christos Aridas
0.4.2,Dzianis Dudnik
0.4.2,License: MIT
0.4.2,FIXME: remove in 0.6
0.4.2,Samples are in danger for m/2 <= m' < m
0.4.2,Samples are noise for m = m'
0.4.2,FIXME: rename _sample -> _fit_resample in 0.6
0.4.2,divergence between borderline-1 and borderline-2
0.4.2,Create synthetic samples for borderline points.
0.4.2,only minority
0.4.2,we use a one-vs-rest policy to handle the multiclass in which
0.4.2,new samples will be created considering not only the majority
0.4.2,class but all over classes.
0.4.2,FIXME: rename _sample -> _fit_resample in 0.6
0.4.2,"FIXME: In 0.6, SMOTE should inherit only from BaseSMOTE."
0.4.2,FIXME: in 0.6 call super()
0.4.2,FIXME: in 0.6 call super()
0.4.2,FIXME: remove in 0.6 after deprecation cycle
0.4.2,FIXME: to be removed in 0.6
0.4.2,FIXME: uncomment in version 0.6
0.4.2,self._validate_estimator()
0.4.2,compute the median of the standard deviation of the minority class
0.4.2,the input of the OneHotEncoder needs to be dense
0.4.2,we can replace the 1 entries of the categorical features with the
0.4.2,median of the standard deviation. It will ensure that whenever
0.4.2,"distance is computed between 2 samples, the difference will be equal"
0.4.2,to the median of the standard deviation as in the original paper.
0.4.2,reverse the encoding of the categorical features
0.4.2,the matrix is supposed to be in the CSR format after the stacking
0.4.2,"To avoid conversion and since there is only few samples used, we"
0.4.2,convert those samples to dense array.
0.4.2,tie breaking argmax
0.4.2,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.4.2,Christos Aridas
0.4.2,License: MIT
0.4.2,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.4.2,Christos Aridas
0.4.2,Dzianis Dudnik
0.4.2,License: MIT
0.4.2,create 2 random continuous feature
0.4.2,create a categorical feature using some string
0.4.2,create a categorical feature using some integer
0.4.2,return the categories
0.4.2,create 2 random continuous feature
0.4.2,create a categorical feature using some string
0.4.2,create a categorical feature using some integer
0.4.2,return the categories
0.4.2,create 2 random continuous feature
0.4.2,create a categorical feature using some string
0.4.2,create a categorical feature using some integer
0.4.2,return the categories
0.4.2,create 2 random continuous feature
0.4.2,create a categorical feature using some string
0.4.2,create a categorical feature using some integer
0.4.2,return the categories
0.4.2,create 2 random continuous feature
0.4.2,create a categorical feature using some string
0.4.2,create a categorical feature using some integer
0.4.2,part of the common test which apply to SMOTE-NC even if it is not default
0.4.2,constructible
0.4.2,Check that the samplers handle pandas dataframe and pandas series
0.4.2,Cast X and y to not default dtype
0.4.2,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.4.2,Christos Aridas
0.4.2,License: MIT
0.4.2,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.4.2,Christos Aridas
0.4.2,License: MIT
0.4.2,FIXME: Remove in 0.6
0.4.2,shuffle the indices since the sampler are packing them by class
0.4.2,helper functions
0.4.2,input and output
0.4.2,build the model and weights
0.4.2,"build the loss, predict, and train operator"
0.4.2,Initialization of all variables in the graph
0.4.2,"For each epoch, run accuracy on train and test"
0.4.2,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.4.2,License: MIT
0.4.2,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.4.2,Fernando Nogueira
0.4.2,Christos Aridas
0.4.2,License: MIT
0.4.2,find which class to not consider
0.4.2,there is a Tomek link between two samples if they are both nearest
0.4.2,neighbors of each others.
0.4.2,check for deprecated random_state
0.4.2,Find the nearest neighbour of every point
0.4.2,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.4.2,Christos Aridas
0.4.2,License: MIT
0.4.2,Randomly get one sample from the majority class
0.4.2,Generate the index to select
0.4.2,Create the set C - One majority samples and all minority
0.4.2,Create the set S - all majority samples
0.4.2,fit knn on C
0.4.2,Check each sample in S if we keep it or drop it
0.4.2,Do not select sample which are already well classified
0.4.2,Classify on S
0.4.2,If the prediction do not agree with the true label
0.4.2,append it in C_x
0.4.2,Keep the index for later
0.4.2,Update C
0.4.2,fit a knn on C
0.4.2,This experimental to speed up the search
0.4.2,Classify all the element in S and avoid to test the
0.4.2,well classified elements
0.4.2,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.4.2,Dayvid Oliveira
0.4.2,Christos Aridas
0.4.2,License: MIT
0.4.2,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.4.2,Christos Aridas
0.4.2,License: MIT
0.4.2,Compute the distance considering the farthest neighbour
0.4.2,Sort the list of distance and get the index
0.4.2,Throw a warning to tell the user that we did not have enough samples
0.4.2,to select and that we just select everything
0.4.2,Select the desired number of samples
0.4.2,check for deprecated random_state
0.4.2,idx_tmp is relative to the feature selected in the
0.4.2,previous step and we need to find the indirection
0.4.2,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.4.2,Christos Aridas
0.4.2,License: MIT
0.4.2,select a sample from the current class
0.4.2,create the set composed of all minority samples and one
0.4.2,sample from the current class.
0.4.2,create the set S with removing the seed from S
0.4.2,since that it will be added anyway
0.4.2,apply Tomek cleaning
0.4.2,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.4.2,Dayvid Oliveira
0.4.2,Christos Aridas
0.4.2,License: MIT
0.4.2,check for deprecated random_state
0.4.2,check for deprecated random_state
0.4.2,Check the stopping criterion
0.4.2,1. If there is no changes for the vector y
0.4.2,2. If the number of samples in the other class become inferior to
0.4.2,the number of samples in the majority class
0.4.2,3. If one of the class is disappearing
0.4.2,Case 1
0.4.2,Case 2
0.4.2,Case 3
0.4.2,check for deprecated random_state
0.4.2,Check the stopping criterion
0.4.2,1. If the number of samples in the other class become inferior to
0.4.2,the number of samples in the majority class
0.4.2,2. If one of the class is disappearing
0.4.2,Case 1else:
0.4.2,overwrite b_min_bec_maj
0.4.2,Case 2
0.4.2,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.4.2,Christos Aridas
0.4.2,License: MIT
0.4.2,check for deprecated random_state
0.4.2,clean the neighborhood
0.4.2,compute which classes to consider for cleaning for the A2 group
0.4.2,compute a2 group
0.4.2,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.4.2,Christos Aridas
0.4.2,License: MIT
0.4.2,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.4.2,Christos Aridas
0.4.2,License: MIT
0.4.2,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.4.2,Christos Aridas
0.4.2,License: MIT
0.4.2,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.4.2,Christos Aridas
0.4.2,License: MIT
0.4.2,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.4.2,Christos Aridas
0.4.2,License: MIT
0.4.2,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.4.2,Christos Aridas
0.4.2,License: MIT
0.4.2,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.4.2,Christos Aridas
0.4.2,License: MIT
0.4.2,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.4.2,Christos Aridas
0.4.2,License: MIT
0.4.2,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.4.2,Christos Aridas
0.4.2,License: MIT
0.4.2,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.4.2,Christos Aridas
0.4.2,License: MIT
0.4.2,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.4.2,Christos Aridas
0.4.2,License: MIT
0.4.2,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.4.2,Fernando Nogueira
0.4.2,Christos Aridas
0.4.2,License: MIT
0.4.2,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.4.2,Christos Aridas
0.4.2,License: MIT
0.4.2,test that all_estimators doesn't find abstract classes.
0.4.2,don't run twice the sampler tests. Meta-estimator do not have a
0.4.2,fit_resample method.
0.4.2,input validation etc for non-meta estimators
0.4.2,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.4.2,License: MIT
0.4.2,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.4.2,Christos Aridas
0.4.2,License: MIT
0.4.2,store timestamp to figure out whether the result of 'fit' has been
0.4.2,cached or not
0.4.2,store timestamp to figure out whether the result of 'fit' has been
0.4.2,cached or not
0.4.2,Test the various init parameters of the pipeline.
0.4.2,Check that we can't instantiate pipelines with objects without fit
0.4.2,method
0.4.2,Smoke test with only an estimator
0.4.2,Check that params are set
0.4.2,Smoke test the repr:
0.4.2,Test with two objects
0.4.2,Check that we can't instantiate with non-transformers on the way
0.4.2,"Note that NoTrans implements fit, but not transform"
0.4.2,Check that params are set
0.4.2,Smoke test the repr:
0.4.2,Check that params are not set when naming them wrong
0.4.2,Test clone
0.4.2,"Check that apart from estimators, the parameters are the same"
0.4.2,Remove estimators that where copied
0.4.2,Test the various methods of the pipeline (anova).
0.4.2,Test with Anova + LogisticRegression
0.4.2,Test that the pipeline can take fit parameters
0.4.2,classifier should return True
0.4.2,and transformer params should not be changed
0.4.2,invalid parameters should raise an error message
0.4.2,Pipeline should pass sample_weight
0.4.2,When sample_weight is None it shouldn't be passed
0.4.2,Test pipeline raises set params error message for nested models.
0.4.2,nested model check
0.4.2,Test the various methods of the pipeline (pca + svm).
0.4.2,Test with PCA + SVC
0.4.2,Test the various methods of the pipeline (preprocessing + svm).
0.4.2,check shapes of various prediction functions
0.4.2,test that the fit_predict method is implemented on a pipeline
0.4.2,test that the fit_predict on pipeline yields same results as applying
0.4.2,transform and clustering steps separately
0.4.2,"As pipeline doesn't clone estimators on construction,"
0.4.2,it must have its own estimators
0.4.2,first compute the transform and clustering step separately
0.4.2,use a pipeline to do the transform and clustering in one step
0.4.2,tests that a pipeline does not have fit_predict method when final
0.4.2,step of pipeline does not have fit_predict defined
0.4.2,tests that Pipeline passes fit_params to intermediate steps
0.4.2,when fit_predict is invoked
0.4.2,Test whether pipeline works with a transformer at the end.
0.4.2,Also test pipeline.transform and pipeline.inverse_transform
0.4.2,test transform and fit_transform:
0.4.2,Test whether pipeline works with a transformer missing fit_transform
0.4.2,test fit_transform:
0.4.2,Directly setting attr
0.4.2,Using set_params
0.4.2,Using set_params to replace single step
0.4.2,With invalid data
0.4.2,Test setting Pipeline steps to None
0.4.2,"for other methods, ensure no AttributeErrors on None:"
0.4.2,mult2 and mult3 are active
0.4.2,Check None step at construction time
0.4.2,Test that an error is raised when memory is not a string or a Memory
0.4.2,instance
0.4.2,Define memory as an integer
0.4.2,Test with Transformer + SVC
0.4.2,Memoize the transformer at the first fit
0.4.2,Get the time stamp of the tranformer in the cached pipeline
0.4.2,Check that cached_pipe and pipe yield identical results
0.4.2,Check that we are reading the cache while fitting
0.4.2,a second time
0.4.2,Check that cached_pipe and pipe yield identical results
0.4.2,Create a new pipeline with cloned estimators
0.4.2,Check that even changing the name step does not affect the cache hit
0.4.2,Check that cached_pipe and pipe yield identical results
0.4.2,Test with Transformer + SVC
0.4.2,Memoize the transformer at the first fit
0.4.2,Get the time stamp of the tranformer in the cached pipeline
0.4.2,Check that cached_pipe and pipe yield identical results
0.4.2,Check that we are reading the cache while fitting
0.4.2,a second time
0.4.2,Check that cached_pipe and pipe yield identical results
0.4.2,Create a new pipeline with cloned estimators
0.4.2,Check that even changing the name step does not affect the cache hit
0.4.2,Check that cached_pipe and pipe yield identical results
0.4.2,Test the various methods of the pipeline (pca + svm).
0.4.2,Test with PCA + SVC
0.4.2,Test the various methods of the pipeline (pca + svm).
0.4.2,Test with PCA + SVC
0.4.2,Test whether pipeline works with a sampler at the end.
0.4.2,Also test pipeline.sampler
0.4.2,test transform and fit_transform:
0.4.2,We round the value near to zero. It seems that PCA has some issue
0.4.2,with that
0.4.2,Test whether pipeline works with a sampler at the end.
0.4.2,Also test pipeline.sampler
0.4.2,Test pipeline using None as preprocessing step and a classifier
0.4.2,"Test pipeline using None, RUS and a classifier"
0.4.2,"Test pipeline using RUS, None and a classifier"
0.4.2,Test pipeline using None step and a sampler
0.4.2,Test pipeline using None and a transformer that implements transform and
0.4.2,inverse_transform
0.4.2,Test the various methods of the pipeline (anova).
0.4.2,Test with RandomUnderSampling + Anova + LogisticRegression
0.4.2,Test the various methods of the pipeline (anova).
0.4.2,Test the various methods of the pipeline (anova).
0.4.2,Test with RandomUnderSampling + Anova + LogisticRegression
0.4.2,tests that Pipeline passes predict_params to the final estimator
0.4.2,when predict is invoked
0.4.2,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.4.2,Christos Aridas
0.4.2,License: MIT
0.4.2,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.4.2,License: MIT
0.4.2,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.4.2,License: MIT
0.4.2,Adapated from scikit-learn
0.4.2,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.4.2,License: MIT
0.4.2,FIXME: remove in 0.6
0.4.2,check that estimators treat dtype object as numeric if possible
0.4.2,trigger our checks if this is a SamplerMixin
0.4.2,monkey patch check_dtype_object for the sampler allowing strings
0.4.2,scikit-learn common tests
0.4.2,FIXME: in 0.6 set the random_state for all
0.4.2,IHT does not enforce the number of samples but provide a number
0.4.2,of samples the closest to the desired target.
0.4.2,FIXME remove in 0.6 -> ratio will be deprecated
0.4.2,in this test we will force all samplers to not change the class 1
0.4.2,in this test we will force all samplers to not change the class 1
0.4.2,check that sparse matrices can be passed through the sampler leading to
0.4.2,the same results than dense
0.4.2,set KMeans to full since it support sparse and dense
0.4.2,FIXME: in 0.6 set the random_state for all
0.4.2,Check that the samplers handle pandas dataframe and pandas series
0.4.2,FIXME: in 0.6 set the random_state for all
0.4.2,Check that multiclass target lead to the same results than OVA encoding
0.4.2,FIXME: in 0.6 set the random_state for all
0.4.2,Cast X and y to not default dtype
0.4.2,FIXME: in 0.6 set the random_state for all
0.4.2,Adapted from scikit-learn
0.4.2,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.4.2,License: MIT
0.4.2,meta-estimators need another estimator to be instantiated.
0.4.2,estimators that there is no way to default-construct sensibly
0.4.2,some strange ones
0.4.2,get parent folder
0.4.2,get rid of abstract base classes
0.4.2,get rid of sklearn estimators which have been imported in some classes
0.4.2,possibly get rid of meta estimators
0.4.2,"drop duplicates, sort for reproducibility"
0.4.2,itemgetter is used to ensure the sort does not extend to the 2nd item of
0.4.2,the tuple
0.4.2,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.4.2,License: MIT
0.4.2,FIXME: perfectly we should raise an error but the sklearn API does
0.4.2,not allow for it
0.4.2,check that all keys in sampling_strategy are also in y
0.4.2,check that there is no negative number
0.4.2,FIXME: Turn into an error in 0.6
0.4.2,clean-sampling can be more permissive since those samplers do not
0.4.2,use samples
0.4.2,check that all keys in sampling_strategy are also in y
0.4.2,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.4.2,Christos Aridas
0.4.2,License: MIT
0.4.2,this function could create an equal number of samples
0.4.2,We pass on purpose a non sorted dictionary and check that the resulting
0.4.2,dictionary is sorted. Refer to issue #428.
0.4.2,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.4.2,License: MIT
0.4.2,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.4.2,Christos Aridas
0.4.2,License: MIT
0.4.2,check if the filtering is working with a list or a single string
0.4.2,check that all estimators are sampler
0.4.2,check that an error is raised when the type is unknown
0.4.2,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.4.2,License: MIT
0.4.2,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.4.2,Christos Aridas
0.4.2,License: MIT
0.4.2,Otherwise create a default SMOTE
0.4.2,Otherwise create a default TomekLinks
0.4.2,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.4.2,Christos Aridas
0.4.2,License: MIT
0.4.2,Otherwise create a default SMOTE
0.4.2,Otherwise create a default EditedNearestNeighbours
0.4.2,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.4.2,Christos Aridas
0.4.2,License: MIT
0.4.2,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.4.2,Christos Aridas
0.4.2,License: MIT
0.4.2,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.4.2,License: MIT
0.4.2,resample before to fit the tree
0.4.2,Validate or convert input data
0.4.2,Pre-sort indices to avoid that each individual tree of the
0.4.2,ensemble sorts the indices.
0.4.2,Remap output
0.4.2,reshape is necessary to preserve the data contiguity against vs
0.4.2,"[:, np.newaxis] that does not."
0.4.2,Check parameters
0.4.2,"Free allocated memory, if any"
0.4.2,We draw from the random state to get the random state we
0.4.2,would have got if we hadn't used a warm_start.
0.4.2,Parallel loop: we prefer the threading backend as the Cython code
0.4.2,for fitting the trees is internally releasing the Python GIL
0.4.2,making threading more efficient than multiprocessing in
0.4.2,"that case. However, we respect any parallel_backend contexts set"
0.4.2,"at a higher level, since correctness does not rely on using"
0.4.2,threads.
0.4.2,Collect newly grown trees
0.4.2,Create pipeline with the fitted samplers and trees
0.4.2,Decapsulate classes_ attributes
0.4.2,Instances incorrectly classified
0.4.2,Error fraction
0.4.2,Stop if classification is perfect
0.4.2,Construct y coding as described in Zhu et al [2]:
0.4.2,
0.4.2,y_k = 1 if c == k else -1 / (K - 1)
0.4.2,
0.4.2,"where K == n_classes_ and c, k in [0, K) are indices along the second"
0.4.2,axis of the y coding with c being the index corresponding to the true
0.4.2,class label.
0.4.2,Displace zero probabilities so the log is defined.
0.4.2,Also fix negative elements which may occur with
0.4.2,negative sample weights.
0.4.2,Boost weight using multi-class AdaBoost SAMME.R alg
0.4.2,Only boost the weights if it will fit again
0.4.2,Only boost positive weights
0.4.2,Instances incorrectly classified
0.4.2,Error fraction
0.4.2,Stop if classification is perfect
0.4.2,Stop if the error is at least as bad as random guessing
0.4.2,Boost weight using multi-class AdaBoost SAMME alg
0.4.2,Only boost the weights if I will fit again
0.4.2,Only boost positive weights
0.4.2,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.4.2,Christos Aridas
0.4.2,License: MIT
0.4.2,doctest: +ELLIPSIS
0.4.2,array to know which samples are available to be taken
0.4.2,where the different set will be stored
0.4.2,store the index of the data to under-sample
0.4.2,value which will be picked at each round
0.4.2,extract the data of interest for this round from the
0.4.2,current class
0.4.2,select randomly the desired features
0.4.2,store the set created
0.4.2,fit and predict using cross validation
0.4.2,extract the prediction about the targeted classes only
0.4.2,check the stopping criterion
0.4.2,check that there is enough samples for another round
0.4.2,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.4.2,License: MIT
0.4.2,Ensemble are a bit specific since they are returning an array of
0.4.2,resampled arrays.
0.4.2,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.4.2,Christos Aridas
0.4.2,License: MIT
0.4.2,RandomUnderSampler is not supporting sample_weight. We need to pass
0.4.2,None.
0.4.2,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.4.2,Christos Aridas
0.4.2,License: MIT
0.4.2,RandomUnderSampler is not supporting sample_weight. We need to pass
0.4.2,None.
0.4.2,check that we have an ensemble of samplers and estimators with a
0.4.2,consistent size
0.4.2,each sampler in the ensemble should have different random state
0.4.2,each estimator in the ensemble should have different random state
0.4.2,check the consistency of the feature importances
0.4.2,check the consistency of the prediction outpus
0.4.2,Predictions should be the same when sample_weight are all ones
0.4.2,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.4.2,Christos Aridas
0.4.2,License: MIT
0.4.2,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.4.2,Christos Aridas
0.4.2,License: MIT
0.4.2,Check classification for various parameter settings.
0.4.2,Test that bootstrapping samples generate non-perfect base estimators.
0.4.2,"without bootstrap, all trees are perfect on the training set"
0.4.2,disable the resampling by passing an empty dictionary.
0.4.2,"with bootstrap, trees are no longer perfect on the training set"
0.4.2,Test that bootstrapping features may generate duplicate features.
0.4.2,Predict probabilities.
0.4.2,Normal case
0.4.2,"Degenerate case, where some classes are missing"
0.4.2,Check that oob prediction is a good estimation of the generalization
0.4.2,error.
0.4.2,Test with few estimators
0.4.2,Check singleton ensembles.
0.4.2,Test that it gives proper exception on deficient input.
0.4.2,Test n_estimators
0.4.2,Test max_samples
0.4.2,Test max_features
0.4.2,Test support of decision_function
0.4.2,Check that bagging ensembles can be grid-searched.
0.4.2,Transform iris into a binary classification task
0.4.2,Grid search with scoring based on decision_function
0.4.2,Check base_estimator and its default values.
0.4.2,Test if fitting incrementally with warm start gives a forest of the
0.4.2,right size and the same results as a normal fit.
0.4.2,Test if warm start'ed second fit with smaller n_estimators raises error.
0.4.2,Test that nothing happens when fitting without increasing n_estimators
0.4.2,"modify X to nonsense values, this should not change anything"
0.4.2,warm started classifier with 5+5 estimators should be equivalent to
0.4.2,one classifier with 10 estimators
0.4.2,Check using oob_score and warm_start simultaneously fails
0.4.2,"Make sure OOB scores are identical when random_state, estimator, and"
0.4.2,training data are fixed and fitting is done twice
0.4.2,Check that format of estimators_samples_ is correct and that results
0.4.2,generated at fit time can be identically reproduced at a later time
0.4.2,using data saved in object attributes.
0.4.2,remap the y outside of the BalancedBaggingclassifier
0.4.2,"_, y = np.unique(y, return_inverse=True)"
0.4.2,Get relevant attributes
0.4.2,Test for correct formatting
0.4.2,Re-fit single estimator to test for consistent sampling
0.4.2,Make sure validated max_samples and original max_samples are identical
0.4.2,when valid integer max_samples supplied by user
0.4.2,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.4.2,Christos Aridas
0.4.2,License: MIT
0.4.2,Generate a global dataset to use
0.4.2,Define a sampling_strategy
0.4.2,Define the sampling_strategy parameter
0.4.2,Create the sampling object
0.4.2,Get the different subset
0.4.2,Define the sampling_strategy parameter
0.4.2,Create the sampling object
0.4.2,Get the different subset
0.4.2,Define the sampling_strategy parameter
0.4.2,Create the sampling object
0.4.2,Get the different subset
0.4.2,Check classification for various parameter settings.
0.4.2,test the different prediction function
0.4.2,Check base_estimator and its default values.
0.4.2,Test if fitting incrementally with warm start gives a forest of the
0.4.2,right size and the same results as a normal fit.
0.4.2,Test if warm start'ed second fit with smaller n_estimators raises error.
0.4.2,Test that nothing happens when fitting without increasing n_estimators
0.4.2,"modify X to nonsense values, this should not change anything"
0.4.2,warm started classifier with 5+5 estimators should be equivalent to
0.4.2,one classifier with 10 estimators
0.4.2,Check warning if not enough estimators
0.4.2,Author: Guillaume Lemaitre
0.4.2,License: BSD 3 clause
0.4.2,"The index start at one, then we need to remove one"
0.4.2,to not have issue with the indexing.
0.4.2,go through the list and check if the data are available
0.4.2,Authors: Dayvid Oliveira
0.4.2,Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.4.2,Christos Aridas
0.4.2,License: MIT
0.4.2,restrict ratio to be a dict or a callable
0.4.2,FIXME remove ratio at 0.6
0.4.2,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.4.2,Christos Aridas
0.4.2,License: MIT
0.4.2,check an error is raised with we don't pass sampling_strategy and ratio
0.4.2,"we are reusing part of utils.check_sampling_strategy, however this is not"
0.4.2,cover in the common tests so we will repeat it here
0.4.2,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.4.2,Christos Aridas
0.4.2,License: MIT
0.4.2,This is a trick to avoid an error during tests collection with pytest. We
0.4.2,avoid the error when importing the package raise the error at the moment of
0.4.2,creating the instance.
0.4.2,FIXME: Remove in 0.6
0.4.2,shuffle the indices since the sampler are packing them by class
0.4.1,This file is here so that when running from the root folder
0.4.1,./sklearn is added to sys.path by pytest.
0.4.1,See https://docs.pytest.org/en/latest/pythonpath.html for more details.
0.4.1,"For example, this allows to build extensions in place and run pytest"
0.4.1,doc/modules/clustering.rst and use sklearn from the local folder
0.4.1,rather than the one from site-packages.
0.4.1,Set numpy array str/repr to legacy behaviour on numpy > 1.13 to make
0.4.1,the doctests pass
0.4.1,! /usr/bin/env python
0.4.1,get __version__ from _version.py
0.4.1,-*- coding: utf-8 -*-
0.4.1,
0.4.1,"imbalanced-learn documentation build configuration file, created by"
0.4.1,sphinx-quickstart on Mon Jan 18 14:44:12 2016.
0.4.1,
0.4.1,This file is execfile()d with the current directory set to its
0.4.1,containing dir.
0.4.1,
0.4.1,Note that not all possible configuration values are present in this
0.4.1,autogenerated file.
0.4.1,
0.4.1,All configuration values have a default; values that are commented out
0.4.1,serve to show the default.
0.4.1,"If extensions (or modules to document with autodoc) are in another directory,"
0.4.1,add these directories to sys.path here. If the directory is relative to the
0.4.1,"documentation root, use os.path.abspath to make it absolute, like shown here."
0.4.1,-- General configuration ------------------------------------------------
0.4.1,"If your documentation needs a minimal Sphinx version, state it here."
0.4.1,needs_sphinx = '1.0'
0.4.1,"Add any Sphinx extension module names here, as strings. They can be"
0.4.1,extensions coming with Sphinx (named 'sphinx.ext.*') or your custom
0.4.1,ones.
0.4.1,this is needed for some reason...
0.4.1,see https://github.com/numpy/numpydoc/issues/69
0.4.1,pngmath / imgmath compatibility layer for different sphinx versions
0.4.1,"Add any paths that contain templates here, relative to this directory."
0.4.1,generate autosummary even if no references
0.4.1,The suffix of source filenames.
0.4.1,The encoding of source files.
0.4.1,source_encoding = 'utf-8-sig'
0.4.1,Generate the plot for the gallery
0.4.1,The master toctree document.
0.4.1,General information about the project.
0.4.1,"The version info for the project you're documenting, acts as replacement for"
0.4.1,"|version| and |release|, also used in various other places throughout the"
0.4.1,built documents.
0.4.1,
0.4.1,The short X.Y version.
0.4.1,"The full version, including alpha/beta/rc tags."
0.4.1,The language for content autogenerated by Sphinx. Refer to documentation
0.4.1,for a list of supported languages.
0.4.1,language = None
0.4.1,"There are two options for replacing |today|: either, you set today to some"
0.4.1,"non-false value, then it is used:"
0.4.1,today = ''
0.4.1,"Else, today_fmt is used as the format for a strftime call."
0.4.1,"today_fmt = '%B %d, %Y'"
0.4.1,"List of patterns, relative to source directory, that match files and"
0.4.1,directories to ignore when looking for source files.
0.4.1,The reST default role (used for this markup: `text`) to use for all
0.4.1,documents.
0.4.1,default_role = None
0.4.1,"If true, '()' will be appended to :func: etc. cross-reference text."
0.4.1,"If true, the current module name will be prepended to all description"
0.4.1,unit titles (such as .. function::).
0.4.1,add_module_names = True
0.4.1,"If true, sectionauthor and moduleauthor directives will be shown in the"
0.4.1,output. They are ignored by default.
0.4.1,show_authors = False
0.4.1,The name of the Pygments (syntax highlighting) style to use.
0.4.1,Custom style
0.4.1,A list of ignored prefixes for module index sorting.
0.4.1,modindex_common_prefix = []
0.4.1,"If true, keep warnings as ""system message"" paragraphs in the built documents."
0.4.1,keep_warnings = False
0.4.1,-- Options for HTML output ----------------------------------------------
0.4.1,The theme to use for HTML and HTML Help pages.  See the documentation for
0.4.1,a list of builtin themes.
0.4.1,Theme options are theme-specific and customize the look and feel of a theme
0.4.1,"further.  For a list of options available for each theme, see the"
0.4.1,documentation.
0.4.1,html_theme_options = {'prev_next_buttons_location': None}
0.4.1,"Add any paths that contain custom themes here, relative to this directory."
0.4.1,"The name for this set of Sphinx documents.  If None, it defaults to"
0.4.1,"""<project> v<release> documentation""."
0.4.1,html_title = None
0.4.1,A shorter title for the navigation bar.  Default is the same as html_title.
0.4.1,html_short_title = None
0.4.1,The name of an image file (relative to this directory) to place at the top
0.4.1,of the sidebar.
0.4.1,html_logo = None
0.4.1,The name of an image file (within the static path) to use as favicon of the
0.4.1,docs.  This file should be a Windows icon file (.ico) being 16x16 or 32x32
0.4.1,pixels large.
0.4.1,html_favicon = None
0.4.1,"Add any paths that contain custom static files (such as style sheets) here,"
0.4.1,"relative to this directory. They are copied after the builtin static files,"
0.4.1,"so a file named ""default.css"" will overwrite the builtin ""default.css""."
0.4.1,Add any extra paths that contain custom files (such as robots.txt or
0.4.1,".htaccess) here, relative to this directory. These files are copied"
0.4.1,directly to the root of the documentation.
0.4.1,html_extra_path = []
0.4.1,"If not '', a 'Last updated on:' timestamp is inserted at every page bottom,"
0.4.1,using the given strftime format.
0.4.1,"html_last_updated_fmt = '%b %d, %Y'"
0.4.1,"If true, SmartyPants will be used to convert quotes and dashes to"
0.4.1,typographically correct entities.
0.4.1,html_use_smartypants = True
0.4.1,"Custom sidebar templates, maps document names to template names."
0.4.1,html_sidebars = {}
0.4.1,"Additional templates that should be rendered to pages, maps page names to"
0.4.1,template names.
0.4.1,html_additional_pages = {}
0.4.1,"If false, no module index is generated."
0.4.1,html_domain_indices = True
0.4.1,"If false, no index is generated."
0.4.1,html_use_index = True
0.4.1,"If true, the index is split into individual pages for each letter."
0.4.1,html_split_index = False
0.4.1,"If true, links to the reST sources are added to the pages."
0.4.1,html_show_sourcelink = True
0.4.1,"If true, ""Created using Sphinx"" is shown in the HTML footer. Default is True."
0.4.1,html_show_sphinx = True
0.4.1,"If true, ""(C) Copyright ..."" is shown in the HTML footer. Default is True."
0.4.1,html_show_copyright = True
0.4.1,"If true, an OpenSearch description file will be output, and all pages will"
0.4.1,contain a <link> tag referring to it.  The value of this option must be the
0.4.1,base URL from which the finished HTML is served.
0.4.1,html_use_opensearch = ''
0.4.1,"This is the file name suffix for HTML files (e.g. "".xhtml"")."
0.4.1,html_file_suffix = None
0.4.1,Output file base name for HTML help builder.
0.4.1,-- Options for LaTeX output ---------------------------------------------
0.4.1,The paper size ('letterpaper' or 'a4paper').
0.4.1,"'papersize': 'letterpaper',"
0.4.1,"The font size ('10pt', '11pt' or '12pt')."
0.4.1,"'pointsize': '10pt',"
0.4.1,Additional stuff for the LaTeX preamble.
0.4.1,"'preamble': '',"
0.4.1,Grouping the document tree into LaTeX files. List of tuples
0.4.1,"(source start file, target name, title,"
0.4.1,"author, documentclass [howto, manual, or own class])."
0.4.1,The name of an image file (relative to this directory) to place at the top of
0.4.1,the title page.
0.4.1,latex_logo = None
0.4.1,"For ""manual"" documents, if this is true, then toplevel headings are parts,"
0.4.1,not chapters.
0.4.1,latex_use_parts = False
0.4.1,"If true, show page references after internal links."
0.4.1,latex_show_pagerefs = False
0.4.1,"If true, show URL addresses after external links."
0.4.1,latex_show_urls = False
0.4.1,Documents to append as an appendix to all manuals.
0.4.1,latex_appendices = []
0.4.1,intersphinx configuration
0.4.1,sphinx-gallery configuration
0.4.1,-- Options for manual page output ---------------------------------------
0.4.1,"If false, no module index is generated."
0.4.1,latex_domain_indices = True
0.4.1,One entry per manual page. List of tuples
0.4.1,"(source start file, name, description, authors, manual section)."
0.4.1,"If true, show URL addresses after external links."
0.4.1,man_show_urls = False
0.4.1,-- Options for Texinfo output -------------------------------------------
0.4.1,Grouping the document tree into Texinfo files. List of tuples
0.4.1,"(source start file, target name, title, author,"
0.4.1,"dir menu entry, description, category)"
0.4.1,"def generate_example_rst(app, what, name, obj, options, lines):"
0.4.1,"# generate empty examples files, so that we don't get"
0.4.1,# inclusion errors if there are no examples for a class / module
0.4.1,"examples_path = os.path.join(app.srcdir, ""generated"","
0.4.1,"""%s.examples"" % name)"
0.4.1,if not os.path.exists(examples_path):
0.4.1,# touch file
0.4.1,"open(examples_path, 'w').close()"
0.4.1,Config for sphinx_issues
0.4.1,"app.connect('autodoc-process-docstring', generate_example_rst)"
0.4.1,Documents to append as an appendix to all manuals.
0.4.1,texinfo_appendices = []
0.4.1,"If false, no module index is generated."
0.4.1,texinfo_domain_indices = True
0.4.1,"How to display URL addresses: 'footnote', 'no', or 'inline'."
0.4.1,texinfo_show_urls = 'footnote'
0.4.1,"If true, do not generate a @detailmenu in the ""Top"" node's menu."
0.4.1,texinfo_no_detailmenu = False
0.4.1,The following is used by sphinx.ext.linkcode to provide links to github
0.4.1,get the styles from the current theme
0.4.1,create and add the button to all the code blocks that contain >>>
0.4.1,tracebacks (.gt) contain bare text elements that need to be
0.4.1,wrapped in a span to work with .nextUntil() (see later)
0.4.1,define the behavior of the button when it's clicked
0.4.1,hide the code output
0.4.1,show the code output
0.4.1,-*- coding: utf-8 -*-
0.4.1,Format template for issues URI
0.4.1,e.g. 'https://github.com/sloria/marshmallow/issues/{issue}
0.4.1,"Shortcut for Github, e.g. 'sloria/marshmallow'"
0.4.1,Format template for user profile URI
0.4.1,e.g. 'https://github.com/{user}'
0.4.1,Python 2 only
0.4.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.4.1,License: MIT
0.4.1,##############################################################################
0.4.1,"First, we will create an imbalanced data set from a the iris data set."
0.4.1,##############################################################################
0.4.1,Using ``sampling_strategy`` in resampling algorithms
0.4.1,##############################################################################
0.4.1,##############################################################################
0.4.1,``sampling_strategy`` as a ``float``
0.4.1,....................................
0.4.1,
0.4.1,``sampling_strategy`` can be given a ``float``. For **under-sampling
0.4.1,"methods**, it corresponds to the ratio :math:`\\alpha_{us}` defined by"
0.4.1,:math:`N_{rM} = \\alpha_{us} \\times N_{m}` where :math:`N_{rM}` and
0.4.1,:math:`N_{m}` are the number of samples in the majority class after
0.4.1,"resampling and the number of samples in the minority class, respectively."
0.4.1,select only 2 classes since the ratio make sense in this case
0.4.1,##############################################################################
0.4.1,"For **over-sampling methods**, it correspond to the ratio"
0.4.1,:math:`\\alpha_{os}` defined by :math:`N_{rm} = \\alpha_{os} \\times N_{m}`
0.4.1,where :math:`N_{rm}` and :math:`N_{M}` are the number of samples in the
0.4.1,minority class after resampling and the number of samples in the majority
0.4.1,"class, respectively."
0.4.1,##############################################################################
0.4.1,``sampling_strategy`` has a ``str``
0.4.1,...................................
0.4.1,
0.4.1,``sampling_strategy`` can be given as a string which specify the class
0.4.1,"targeted by the resampling. With under- and over-sampling, the number of"
0.4.1,samples will be equalized.
0.4.1,
0.4.1,Note that we are using multiple classes from now on.
0.4.1,##############################################################################
0.4.1,"With **cleaning method**, the number of samples in each class will not be"
0.4.1,equalized even if targeted.
0.4.1,##############################################################################
0.4.1,``sampling_strategy`` as a ``dict``
0.4.1,...................................
0.4.1,
0.4.1,"When ``sampling_strategy`` is a ``dict``, the keys correspond to the targeted"
0.4.1,classes. The values correspond to the desired number of samples for each
0.4.1,targeted class. This is working for both **under- and over-sampling**
0.4.1,algorithms but not for the **cleaning algorithms**. Use a ``list`` instead.
0.4.1,##############################################################################
0.4.1,``sampling_strategy`` as a ``list``
0.4.1,...................................
0.4.1,
0.4.1,"When ``sampling_strategy`` is a ``list``, the list contains the targeted"
0.4.1,classes. It is used only for **cleaning methods** and raise an error
0.4.1,otherwise.
0.4.1,##############################################################################
0.4.1,``sampling_strategy`` as a callable
0.4.1,...................................
0.4.1,
0.4.1,"When callable, function taking ``y`` and returns a ``dict``. The keys"
0.4.1,correspond to the targeted classes. The values correspond to the desired
0.4.1,number of samples for each class.
0.4.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.4.1,License: MIT
0.4.1,#############################################################################
0.4.1,Toy data generation
0.4.1,#############################################################################
0.4.1,#############################################################################
0.4.1,We are generating some non Gaussian data set contaminated with some unform
0.4.1,noise.
0.4.1,#############################################################################
0.4.1,We will generate some cleaned test data without outliers.
0.4.1,#############################################################################
0.4.1,How to use the :class:`imblearn.FunctionSampler`
0.4.1,#############################################################################
0.4.1,#############################################################################
0.4.1,We first define a function which will use
0.4.1,:class:`sklearn.ensemble.IsolationForest` to eliminate some outliers from
0.4.1,our dataset during training. The function passed to the
0.4.1,:class:`imblearn.FunctionSampler` will be called when using the method
0.4.1,``fit_resample``.
0.4.1,#############################################################################
0.4.1,Integrate it within a pipeline
0.4.1,#############################################################################
0.4.1,#############################################################################
0.4.1,"By elimnating outliers before the training, the classifier will be less"
0.4.1,affected during the prediction.
0.4.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.4.1,License: MIT
0.4.1,generate some data points
0.4.1,plot the majority and minority samples
0.4.1,draw the circle in which the new sample will generated
0.4.1,plot the line on which the sample will be generated
0.4.1,create and plot the new sample
0.4.1,make the plot nicer with legend and label
0.4.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.4.1,License: MIT
0.4.1,##############################################################################
0.4.1,The following function will be used to create toy dataset. It using the
0.4.1,``make_classification`` from scikit-learn but fixing some parameters.
0.4.1,##############################################################################
0.4.1,The following function will be used to plot the sample space after resampling
0.4.1,to illustrate the characterisitic of an algorithm.
0.4.1,make nice plotting
0.4.1,##############################################################################
0.4.1,The following function will be used to plot the decision function of a
0.4.1,classifier given some data.
0.4.1,##############################################################################
0.4.1,Illustration of the influence of the balancing ratio
0.4.1,##############################################################################
0.4.1,##############################################################################
0.4.1,We will first illustrate the influence of the balancing ratio on some toy
0.4.1,data using a linear SVM classifier. Greater is the difference between the
0.4.1,"number of samples in each class, poorer are the classfication results."
0.4.1,##############################################################################
0.4.1,Random over-sampling to balance the data set
0.4.1,##############################################################################
0.4.1,##############################################################################
0.4.1,Random over-sampling can be used to repeat some samples and balance the
0.4.1,number of samples between the dataset. It can be seen that with this trivial
0.4.1,approach the boundary decision is already less biaised toward the majority
0.4.1,class.
0.4.1,##############################################################################
0.4.1,More advanced over-sampling using ADASYN and SMOTE
0.4.1,##############################################################################
0.4.1,##############################################################################
0.4.1,"Instead of repeating the same samples when over-sampling, we can use some"
0.4.1,specific heuristic instead. ADASYN and SMOTE can be used in this case.
0.4.1,Make an identity sampler
0.4.1,##############################################################################
0.4.1,The following plot illustrate the difference between ADASYN and SMOTE. ADASYN
0.4.1,will focus on the samples which are difficult to classify with a
0.4.1,nearest-neighbors rule while regular SMOTE will not make any distinction.
0.4.1,"Therefore, the decision function depending of the algorithm."
0.4.1,##############################################################################
0.4.1,"Due to those sampling particularities, it can give rise to some specific"
0.4.1,issues as illustrated below.
0.4.1,##############################################################################
0.4.1,SMOTE proposes several variants by identifying specific samples to consider
0.4.1,during the resampling. The borderline version will detect which point to
0.4.1,select which are in the border between two classes. The SVM version will use
0.4.1,the support vectors found using an SVM algorithm to create new samples.
0.4.1,##############################################################################
0.4.1,"When dealing with a mixed of continuous and categorical features, SMOTE-NC"
0.4.1,is the only method which can handle this case.
0.4.1,create a synthetic data set with continuous and categorical features
0.4.1,Authors: Christos Aridas
0.4.1,Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.4.1,License: MIT
0.4.1,Generate the dataset
0.4.1,make nice plotting
0.4.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.4.1,License: MIT
0.4.1,Generate a dataset
0.4.1,Split the data
0.4.1,Train the classifier with balancing
0.4.1,Test the classifier and get the prediction
0.4.1,Show the classification report
0.4.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.4.1,License: MIT
0.4.1,Generate a dataset
0.4.1,Split the data
0.4.1,Train the classifier with balancing
0.4.1,Test the classifier and get the prediction
0.4.1,##############################################################################
0.4.1,The geometric mean corresponds to the square root of the product of the
0.4.1,sensitivity and specificity. Combining the two metrics should account for
0.4.1,the balancing of the dataset.
0.4.1,##############################################################################
0.4.1,The index balanced accuracy can transform any metric to be used in
0.4.1,imbalanced learning problems.
0.4.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.4.1,License: MIT
0.4.1,##############################################################################
0.4.1,The following function will be used to create toy dataset. It using the
0.4.1,``make_classification`` from scikit-learn but fixing some parameters.
0.4.1,##############################################################################
0.4.1,The following function will be used to plot the sample space after resampling
0.4.1,to illustrate the characteristic of an algorithm.
0.4.1,make nice plotting
0.4.1,##############################################################################
0.4.1,The following function will be used to plot the decision function of a
0.4.1,classifier given some data.
0.4.1,##############################################################################
0.4.1,"``SMOTE`` allows to generate samples. However, this method of over-sampling"
0.4.1,"does not have any knowledge regarding the underlying distribution. Therefore,"
0.4.1,"some noisy samples can be generated, e.g. when the different classes cannot"
0.4.1,"be well separated. Hence, it can be beneficial to apply an under-sampling"
0.4.1,algorithm to clean the noisy samples. Two methods are usually used in the
0.4.1,literature: (i) Tomek's link and (ii) edited nearest neighbours cleaning
0.4.1,methods. Imbalanced-learn provides two ready-to-use samplers ``SMOTETomek``
0.4.1,"and ``SMOTEENN``. In general, ``SMOTEENN`` cleans more noisy data than"
0.4.1,``SMOTETomek``.
0.4.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.4.1,License: MIT
0.4.1,##############################################################################
0.4.1,Load an imbalanced dataset
0.4.1,##############################################################################
0.4.1,We will load the UCI SatImage dataset which has an imbalanced ratio of 9.3:1
0.4.1,(number of majority sample for a minority sample). The data are then split
0.4.1,into training and testing.
0.4.1,##############################################################################
0.4.1,Classification using a single decision tree
0.4.1,##############################################################################
0.4.1,We train a decision tree classifier which will be used as a baseline for the
0.4.1,rest of this example.
0.4.1,##############################################################################
0.4.1,The results are reported in terms of balanced accuracy and geometric mean
0.4.1,which are metrics widely used in the literature to validate model trained on
0.4.1,imbalanced set.
0.4.1,##############################################################################
0.4.1,Classification using bagging classifier with and without sampling
0.4.1,##############################################################################
0.4.1,"Instead of using a single tree, we will check if an ensemble of decsion tree"
0.4.1,"can actually alleviate the issue induced by the class imbalancing. First, we"
0.4.1,will use a bagging classifier and its counter part which internally uses a
0.4.1,random under-sampling to balanced each boostrap sample.
0.4.1,##############################################################################
0.4.1,Balancing each bootstrap sample allows to increase significantly the balanced
0.4.1,accuracy and the geometric mean.
0.4.1,##############################################################################
0.4.1,Classification using random forest classifier with and without sampling
0.4.1,##############################################################################
0.4.1,Random forest is another popular ensemble method and it is usually
0.4.1,"outperforming bagging. Here, we used a vanilla random forest and its balanced"
0.4.1,counterpart in which each bootstrap sample is balanced.
0.4.1,"Similarly to the previous experiment, the balanced classifier outperform the"
0.4.1,"classifier which learn from imbalanced bootstrap samples. In addition, random"
0.4.1,forest outsperforms the bagging classifier.
0.4.1,##############################################################################
0.4.1,Boosting classifier
0.4.1,##############################################################################
0.4.1,"In the same manner, easy ensemble classifier is a bag of balanced AdaBoost"
0.4.1,"classifier. However, it will be slower to train than random forest and will"
0.4.1,achieve worse performance.
0.4.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.4.1,License: MIT
0.4.1,##############################################################################
0.4.1,The following function will be used to create toy dataset. It using the
0.4.1,``make_classification`` from scikit-learn but fixing some parameters.
0.4.1,##############################################################################
0.4.1,The following function will be used to plot the sample space after resampling
0.4.1,to illustrate the characteristic of an algorithm.
0.4.1,make nice plotting
0.4.1,##############################################################################
0.4.1,The following function will be used to plot the decision function of a
0.4.1,classifier given some data.
0.4.1,##############################################################################
0.4.1,Prototype generation: under-sampling by generating new samples
0.4.1,##############################################################################
0.4.1,##############################################################################
0.4.1,``ClusterCentroids`` under-samples by replacing the original samples by the
0.4.1,centroids of the cluster found.
0.4.1,##############################################################################
0.4.1,Prototype selection: under-sampling by selecting existing samples
0.4.1,##############################################################################
0.4.1,##############################################################################
0.4.1,The algorithm performing prototype selection can be subdivided into two
0.4.1,groups: (i) the controlled under-sampling methods and (ii) the cleaning
0.4.1,under-sampling methods.
0.4.1,##############################################################################
0.4.1,"With the controlled under-sampling methods, the number of samples to be"
0.4.1,selected can be specified. ``RandomUnderSampler`` is the most naive way of
0.4.1,performing such selection by randomly selecting a given number of samples by
0.4.1,the targetted class.
0.4.1,##############################################################################
0.4.1,``NearMiss`` algorithms implement some heuristic rules in order to select
0.4.1,samples. NearMiss-1 selects samples from the majority class for which the
0.4.1,average distance of the :math:`k`` nearest samples of the minority class is
0.4.1,the smallest. NearMiss-2 selects the samples from the majority class for
0.4.1,which the average distance to the farthest samples of the negative class is
0.4.1,"the smallest. NearMiss-3 is a 2-step algorithm: first, for each minority"
0.4.1,"sample, their ::math:`m` nearest-neighbors will be kept; then, the majority"
0.4.1,samples selected are the on for which the average distance to the :math:`k`
0.4.1,nearest neighbors is the largest.
0.4.1,##############################################################################
0.4.1,``EditedNearestNeighbours`` removes samples of the majority class for which
0.4.1,their class differ from the one of their nearest-neighbors. This sieve can be
0.4.1,repeated which is the principle of the
0.4.1,``RepeatedEditedNearestNeighbours``. ``AllKNN`` is slightly different from
0.4.1,the ``RepeatedEditedNearestNeighbours`` by changing the :math:`k` parameter
0.4.1,"of the internal nearest neighors algorithm, increasing it at each iteration."
0.4.1,##############################################################################
0.4.1,``CondensedNearestNeighbour`` makes use of a 1-NN to iteratively decide if a
0.4.1,sample should be kept in a dataset or not. The issue is that
0.4.1,``CondensedNearestNeighbour`` is sensitive to noise by preserving the noisy
0.4.1,samples. ``OneSidedSelection`` also used the 1-NN and use ``TomekLinks`` to
0.4.1,remove the samples considered noisy. The ``NeighbourhoodCleaningRule`` use a
0.4.1,"``EditedNearestNeighbours`` to remove some sample. Additionally, they use a 3"
0.4.1,nearest-neighbors to remove samples which do not agree with this rule.
0.4.1,##############################################################################
0.4.1,``InstanceHardnessThreshold`` uses the prediction of classifier to exclude
0.4.1,samples. All samples which are classified with a low probability will be
0.4.1,removed.
0.4.1,##############################################################################
0.4.1,This function allows to make nice plotting
0.4.1,##############################################################################
0.4.1,Generate some data with one Tomek link
0.4.1,minority class
0.4.1,majority class
0.4.1,##############################################################################
0.4.1,"In the figure above, the samples highlighted in green form a Tomek link since"
0.4.1,they are of different classes and are nearest neighbours of each other.
0.4.1,highlight the samples of interest
0.4.1,##############################################################################
0.4.1,We can run the ``TomekLinks`` sampling to remove the corresponding
0.4.1,samples. If ``sampling_strategy='auto'`` only the sample from the majority
0.4.1,class will be removed. If ``sampling_strategy='all'`` both samples will be
0.4.1,removed.
0.4.1,highlight the samples of interest
0.4.1,##############################################################################
0.4.1,This function allows to make nice plotting
0.4.1,##############################################################################
0.4.1,We can start by generating some data to later illustrate the principle of
0.4.1,each NearMiss heuritic rules.
0.4.1,minority class
0.4.1,majority class
0.4.1,##############################################################################
0.4.1,NearMiss-1
0.4.1,##############################################################################
0.4.1,##############################################################################
0.4.1,NearMiss-1 selects samples from the majority class for which the average
0.4.1,distance to some nearest neighbours is the smallest. In the following
0.4.1,"example, we use a 3-NN to compute the average distance on 2 specific samples"
0.4.1,"of the majority class. Therefore, in this case the point linked by the"
0.4.1,green-dashed line will be selected since the average distance is smaller.
0.4.1,##############################################################################
0.4.1,NearMiss-2
0.4.1,##############################################################################
0.4.1,##############################################################################
0.4.1,NearMiss-2 selects samples from the majority class for which the average
0.4.1,distance to the farthest neighbors is the smallest. With the same
0.4.1,"configuration as previously presented, the sample linked to the green-dashed"
0.4.1,line will be selected since its distance the 3 farthest neighbors is the
0.4.1,smallest.
0.4.1,##############################################################################
0.4.1,NearMiss-3
0.4.1,##############################################################################
0.4.1,##############################################################################
0.4.1,"NearMiss-3 can be divided into 2 steps. First, a nearest-neighbors is used to"
0.4.1,short-list samples from the majority class (i.e. correspond to the
0.4.1,"highlighted samples in the following plot). Then, the sample with the largest"
0.4.1,average distance to the *k* nearest-neighbors are selected.
0.4.1,select only the majority point of interest
0.4.1,Authors: Christos Aridas
0.4.1,Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.4.1,License: MIT
0.4.1,Generate the dataset
0.4.1,Instanciate a PCA object for the sake of easy visualisation
0.4.1,Create the samplers
0.4.1,Create the classifier
0.4.1,Make the splits
0.4.1,Add one transformers and two samplers in the pipeline object
0.4.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.4.1,License: MIT
0.4.1,##############################################################################
0.4.1,Data loading
0.4.1,##############################################################################
0.4.1,##############################################################################
0.4.1,"First, you should download the Porto Seguro data set from Kaggle. See the"
0.4.1,link in the introduction.
0.4.1,##############################################################################
0.4.1,The data set is imbalanced and it will have an effect on the fitting.
0.4.1,##############################################################################
0.4.1,Define the pre-processing pipeline
0.4.1,##############################################################################
0.4.1,##############################################################################
0.4.1,We want to standard scale the numerical features while we want to one-hot
0.4.1,"encode the categorical features. In this regard, we make use of the"
0.4.1,:class:`sklearn.compose.ColumnTransformer`.
0.4.1,Create an environment variable to avoid using the GPU. This can be changed.
0.4.1,##############################################################################
0.4.1,Create a neural-network
0.4.1,##############################################################################
0.4.1,##############################################################################
0.4.1,We create a decorator to report the computation time
0.4.1,##############################################################################
0.4.1,The first model will be trained using the ``fit`` method and with imbalanced
0.4.1,mini-batches.
0.4.1,##############################################################################
0.4.1,"In the contrary, we will use imbalanced-learn to create a generator of"
0.4.1,mini-batches which will yield balanced mini-batches.
0.4.1,##############################################################################
0.4.1,Classification loop
0.4.1,##############################################################################
0.4.1,##############################################################################
0.4.1,We will perform a 10-fold cross-validation and train the neural-network with
0.4.1,the two different strategies previously presented.
0.4.1,##############################################################################
0.4.1,Plot of the results and computation time
0.4.1,##############################################################################
0.4.1,Authors: Christos Aridas
0.4.1,Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.4.1,License: MIT
0.4.1,Load the dataset
0.4.1,make nice plotting
0.4.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.4.1,License: MIT
0.4.1,Create a folder to fetch the dataset
0.4.1,Create a pipeline
0.4.1,Classify and report the results
0.4.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.4.1,License: MIT
0.4.1,##############################################################################
0.4.1,Setting the data set
0.4.1,##############################################################################
0.4.1,##############################################################################
0.4.1,We use a part of the 20 newsgroups data set by loading 4 topics. Using the
0.4.1,"scikit-learn loader, the data are split into a training and a testing set."
0.4.1,
0.4.1,Note the class \#3 is the minority class and has almost twice less samples
0.4.1,than the majority class.
0.4.1,##############################################################################
0.4.1,The usual scikit-learn pipeline
0.4.1,##############################################################################
0.4.1,##############################################################################
0.4.1,You might usually use scikit-learn pipeline by combining the TF-IDF
0.4.1,vectorizer to feed a multinomial naive bayes classifier. A classification
0.4.1,report summarized the results on the testing set.
0.4.1,
0.4.1,"As expected, the recall of the class \#3 is low mainly due to the class"
0.4.1,imbalanced.
0.4.1,##############################################################################
0.4.1,Balancing the class before classification
0.4.1,##############################################################################
0.4.1,##############################################################################
0.4.1,"To improve the prediction of the class \#3, it could be interesting to apply"
0.4.1,"a balancing before to train the naive bayes classifier. Therefore, we will"
0.4.1,use a ``RandomUnderSampler`` to equalize the number of samples in all the
0.4.1,classes before the training.
0.4.1,
0.4.1,It is also important to note that we are using the ``make_pipeline`` function
0.4.1,implemented in imbalanced-learn to properly handle the samplers.
0.4.1,##############################################################################
0.4.1,"Although the results are almost identical, it can be seen that the resampling"
0.4.1,allowed to correct the poor recall of the class \#3 at the cost of reducing
0.4.1,"the other metrics for the other classes. However, the overall results are"
0.4.1,slightly better.
0.4.1,Authors: Dayvid Oliveira
0.4.1,Christos Aridas
0.4.1,Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.4.1,License: MIT
0.4.1,Generate the dataset
0.4.1,"Two subplots, unpack the axes array immediately"
0.4.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.4.1,License: MIT
0.4.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.4.1,Christos Aridas
0.4.1,License: MIT
0.4.1,define an alias for back-compatibility
0.4.1,FIXME: remove in 0.6
0.4.1,FIXME: remove in 0.6
0.4.1,both ratio and sampling_strategy should not be set
0.4.1,Adapted from scikit-learn
0.4.1,Author: Edouard Duchesnay
0.4.1,Gael Varoquaux
0.4.1,Virgile Fritsch
0.4.1,Alexandre Gramfort
0.4.1,Lars Buitinck
0.4.1,Christos Aridas
0.4.1,Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.4.1,License: BSD
0.4.1,BaseEstimator interface
0.4.1,validate names
0.4.1,validate estimators
0.4.1,We allow last estimator to be None as an identity transformation
0.4.1,Estimator interface
0.4.1,Setup the memory
0.4.1,joblib >= 0.12
0.4.1,we do not clone when caching is disabled to
0.4.1,preserve backward compatibility
0.4.1,joblib < 0.11
0.4.1,we do not clone when caching is disabled to
0.4.1,preserve backward compatibility
0.4.1,Fit or load from cache the current transfomer
0.4.1,Replace the transformer of the step with the fitted
0.4.1,transformer. This is necessary when loading the transformer
0.4.1,from the cache.
0.4.1,"_final_estimator is None or has transform, otherwise attribute error"
0.4.1,raise AttributeError if necessary for hasattr behaviour
0.4.1,"if we have a weight for this transformer, multiply output"
0.4.1,Based on NiLearn package
0.4.1,License: simplified BSD
0.4.1,"PEP0440 compatible formatted version, see:"
0.4.1,https://www.python.org/dev/peps/pep-0440/
0.4.1,
0.4.1,Generic release markers:
0.4.1,X.Y
0.4.1,X.Y.Z # For bugfix releases
0.4.1,
0.4.1,Admissible pre-release markers:
0.4.1,X.YaN # Alpha release
0.4.1,X.YbN # Beta release
0.4.1,X.YrcN # Release Candidate
0.4.1,X.Y # Final release
0.4.1,
0.4.1,Dev branch marker is: 'X.Y.dev' or 'X.Y.devN' where N is an integer.
0.4.1,'X.Y.dev0' is the canonical version of 'X.Y.dev'
0.4.1,
0.4.1,coding: utf-8
0.4.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.4.1,Dariusz Brzezinski
0.4.1,License: MIT
0.4.1,Only negative labels
0.4.1,"Calculate tp_sum, pred_sum, true_sum ###"
0.4.1,labels are now from 0 to len(labels) - 1 -> use bincount
0.4.1,Pathological case
0.4.1,Compute the true negative
0.4.1,Retain only selected labels
0.4.1,"Finally, we have all our sufficient statistics. Divide! #"
0.4.1,"Divide, and on zero-division, set scores to 0 and warn:"
0.4.1,"Oddly, we may get an ""invalid"" rather than a ""divide"" error"
0.4.1,here.
0.4.1,Average the results
0.4.1,labels are now from 0 to len(labels) - 1 -> use bincount
0.4.1,Pathological case
0.4.1,Retain only selected labels
0.4.1,old version of scipy return MaskedConstant instead of 0.0
0.4.1,Create the list of tags
0.4.1,check that the scoring function does not need a score
0.4.1,and only a prediction
0.4.1,Compute the score from the scoring function
0.4.1,Square if desired
0.4.1,Get the signature of the sens/spec function
0.4.1,We need to extract from kwargs only the one needed by the
0.4.1,specificity and specificity
0.4.1,Make the intersection between the parameters
0.4.1,Create a sub dictionary
0.4.1,Check if the metric is the geometric mean
0.4.1,We do not support multilabel so the only average supported
0.4.1,is binary
0.4.1,Create the list of parameters through signature binding
0.4.1,Call the sens/spec function
0.4.1,Compute the dominance
0.4.1,Compute the different metrics
0.4.1,Precision/recall/f1
0.4.1,Specificity
0.4.1,Geometric mean
0.4.1,Index balanced accuracy
0.4.1,compute averages
0.4.1,coding: utf-8
0.4.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.4.1,Christos Aridas
0.4.1,License: MIT
0.4.1,##############################################################################
0.4.1,Utilities for testing
0.4.1,import some data to play with
0.4.1,restrict to a binary classification task
0.4.1,add noisy features to make the problem harder and avoid perfect results
0.4.1,"run classifier, get class probabilities and label predictions"
0.4.1,only interested in probabilities of the positive case
0.4.1,XXX: do we really want a special API for the binary case?
0.4.1,##############################################################################
0.4.1,Tests
0.4.1,detailed measures for each class
0.4.1,individual scoring function that can be used for grid search: in the
0.4.1,binary class case the score is the value of the measure for the positive
0.4.1,class (e.g. label == 1). This is deprecated for average != 'binary'.
0.4.1,Such a case may occur with non-stratified cross-validation
0.4.1,ensure the above were meaningful tests:
0.4.1,Bad pos_label
0.4.1,Bad average option
0.4.1,but average != 'binary'; even if data is binary
0.4.1,compute the geometric mean for the binary problem
0.4.1,print classification report with class names
0.4.1,print classification report with label detection
0.4.1,print classification report with class names
0.4.1,print classification report with label detection
0.4.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.4.1,Christos Aridas
0.4.1,License: MIT
0.4.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.4.1,Christos Aridas
0.4.1,License: MIT
0.4.1,The ratio is computed using a one-vs-rest manner. Using majority
0.4.1,in multi-class would lead to slightly different results at the
0.4.1,cost of introducing a new parameter.
0.4.1,the nearest neighbors need to be fitted only on the current class
0.4.1,to find the class NN to generate new samples
0.4.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.4.1,Christos Aridas
0.4.1,License: MIT
0.4.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.4.1,Christos Aridas
0.4.1,License: MIT
0.4.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.4.1,Fernando Nogueira
0.4.1,Christos Aridas
0.4.1,Dzianis Dudnik
0.4.1,License: MIT
0.4.1,FIXME: remove in 0.6
0.4.1,Samples are in danger for m/2 <= m' < m
0.4.1,Samples are noise for m = m'
0.4.1,FIXME: rename _sample -> _fit_resample in 0.6
0.4.1,divergence between borderline-1 and borderline-2
0.4.1,Create synthetic samples for borderline points.
0.4.1,only minority
0.4.1,we use a one-vs-rest policy to handle the multiclass in which
0.4.1,new samples will be created considering not only the majority
0.4.1,class but all over classes.
0.4.1,FIXME: rename _sample -> _fit_resample in 0.6
0.4.1,"FIXME: In 0.6, SMOTE should inherit only from BaseSMOTE."
0.4.1,FIXME: in 0.6 call super()
0.4.1,FIXME: in 0.6 call super()
0.4.1,FIXME: remove in 0.6 after deprecation cycle
0.4.1,FIXME: to be removed in 0.6
0.4.1,FIXME: uncomment in version 0.6
0.4.1,self._validate_estimator()
0.4.1,compute the median of the standard deviation of the minority class
0.4.1,the input of the OneHotEncoder needs to be dense
0.4.1,we can replace the 1 entries of the categorical features with the
0.4.1,median of the standard deviation. It will ensure that whenever
0.4.1,"distance is computed between 2 samples, the difference will be equal"
0.4.1,to the median of the standard deviation as in the original paper.
0.4.1,reverse the encoding of the categorical features
0.4.1,the matrix is supposed to be in the CSR format after the stacking
0.4.1,"To avoid conversion and since there is only few samples used, we"
0.4.1,convert those samples to dense array.
0.4.1,tie breaking argmax
0.4.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.4.1,Christos Aridas
0.4.1,License: MIT
0.4.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.4.1,Christos Aridas
0.4.1,Dzianis Dudnik
0.4.1,License: MIT
0.4.1,create 2 random continuous feature
0.4.1,create a categorical feature using some string
0.4.1,create a categorical feature using some integer
0.4.1,return the categories
0.4.1,create 2 random continuous feature
0.4.1,create a categorical feature using some string
0.4.1,create a categorical feature using some integer
0.4.1,return the categories
0.4.1,create 2 random continuous feature
0.4.1,create a categorical feature using some string
0.4.1,create a categorical feature using some integer
0.4.1,return the categories
0.4.1,create 2 random continuous feature
0.4.1,create a categorical feature using some string
0.4.1,create a categorical feature using some integer
0.4.1,return the categories
0.4.1,create 2 random continuous feature
0.4.1,create a categorical feature using some string
0.4.1,create a categorical feature using some integer
0.4.1,part of the common test which apply to SMOTE-NC even if it is not default
0.4.1,constructible
0.4.1,Check that the samplers handle pandas dataframe and pandas series
0.4.1,Cast X and y to not default dtype
0.4.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.4.1,Christos Aridas
0.4.1,License: MIT
0.4.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.4.1,Christos Aridas
0.4.1,License: MIT
0.4.1,FIXME: Remove in 0.6
0.4.1,shuffle the indices since the sampler are packing them by class
0.4.1,helper functions
0.4.1,input and output
0.4.1,build the model and weights
0.4.1,"build the loss, predict, and train operator"
0.4.1,Initialization of all variables in the graph
0.4.1,"For each epoch, run accuracy on train and test"
0.4.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.4.1,License: MIT
0.4.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.4.1,Fernando Nogueira
0.4.1,Christos Aridas
0.4.1,License: MIT
0.4.1,find which class to not consider
0.4.1,there is a Tomek link between two samples if they are both nearest
0.4.1,neighbors of each others.
0.4.1,check for deprecated random_state
0.4.1,Find the nearest neighbour of every point
0.4.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.4.1,Christos Aridas
0.4.1,License: MIT
0.4.1,Randomly get one sample from the majority class
0.4.1,Generate the index to select
0.4.1,Create the set C - One majority samples and all minority
0.4.1,Create the set S - all majority samples
0.4.1,fit knn on C
0.4.1,Check each sample in S if we keep it or drop it
0.4.1,Do not select sample which are already well classified
0.4.1,Classify on S
0.4.1,If the prediction do not agree with the true label
0.4.1,append it in C_x
0.4.1,Keep the index for later
0.4.1,Update C
0.4.1,fit a knn on C
0.4.1,This experimental to speed up the search
0.4.1,Classify all the element in S and avoid to test the
0.4.1,well classified elements
0.4.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.4.1,Dayvid Oliveira
0.4.1,Christos Aridas
0.4.1,License: MIT
0.4.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.4.1,Christos Aridas
0.4.1,License: MIT
0.4.1,Compute the distance considering the farthest neighbour
0.4.1,Sort the list of distance and get the index
0.4.1,Throw a warning to tell the user that we did not have enough samples
0.4.1,to select and that we just select everything
0.4.1,Select the desired number of samples
0.4.1,check for deprecated random_state
0.4.1,idx_tmp is relative to the feature selected in the
0.4.1,previous step and we need to find the indirection
0.4.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.4.1,Christos Aridas
0.4.1,License: MIT
0.4.1,select a sample from the current class
0.4.1,create the set composed of all minority samples and one
0.4.1,sample from the current class.
0.4.1,create the set S with removing the seed from S
0.4.1,since that it will be added anyway
0.4.1,apply Tomek cleaning
0.4.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.4.1,Dayvid Oliveira
0.4.1,Christos Aridas
0.4.1,License: MIT
0.4.1,check for deprecated random_state
0.4.1,check for deprecated random_state
0.4.1,Check the stopping criterion
0.4.1,1. If there is no changes for the vector y
0.4.1,2. If the number of samples in the other class become inferior to
0.4.1,the number of samples in the majority class
0.4.1,3. If one of the class is disappearing
0.4.1,Case 1
0.4.1,Case 2
0.4.1,Case 3
0.4.1,check for deprecated random_state
0.4.1,Check the stopping criterion
0.4.1,1. If the number of samples in the other class become inferior to
0.4.1,the number of samples in the majority class
0.4.1,2. If one of the class is disappearing
0.4.1,Case 1else:
0.4.1,overwrite b_min_bec_maj
0.4.1,Case 2
0.4.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.4.1,Christos Aridas
0.4.1,License: MIT
0.4.1,check for deprecated random_state
0.4.1,clean the neighborhood
0.4.1,compute which classes to consider for cleaning for the A2 group
0.4.1,compute a2 group
0.4.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.4.1,Christos Aridas
0.4.1,License: MIT
0.4.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.4.1,Christos Aridas
0.4.1,License: MIT
0.4.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.4.1,Christos Aridas
0.4.1,License: MIT
0.4.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.4.1,Christos Aridas
0.4.1,License: MIT
0.4.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.4.1,Christos Aridas
0.4.1,License: MIT
0.4.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.4.1,Christos Aridas
0.4.1,License: MIT
0.4.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.4.1,Christos Aridas
0.4.1,License: MIT
0.4.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.4.1,Christos Aridas
0.4.1,License: MIT
0.4.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.4.1,Christos Aridas
0.4.1,License: MIT
0.4.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.4.1,Christos Aridas
0.4.1,License: MIT
0.4.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.4.1,Christos Aridas
0.4.1,License: MIT
0.4.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.4.1,Fernando Nogueira
0.4.1,Christos Aridas
0.4.1,License: MIT
0.4.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.4.1,Christos Aridas
0.4.1,License: MIT
0.4.1,test that all_estimators doesn't find abstract classes.
0.4.1,don't run twice the sampler tests. Meta-estimator do not have a
0.4.1,fit_resample method.
0.4.1,input validation etc for non-meta estimators
0.4.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.4.1,License: MIT
0.4.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.4.1,Christos Aridas
0.4.1,License: MIT
0.4.1,store timestamp to figure out whether the result of 'fit' has been
0.4.1,cached or not
0.4.1,store timestamp to figure out whether the result of 'fit' has been
0.4.1,cached or not
0.4.1,Test the various init parameters of the pipeline.
0.4.1,Check that we can't instantiate pipelines with objects without fit
0.4.1,method
0.4.1,Smoke test with only an estimator
0.4.1,Check that params are set
0.4.1,Smoke test the repr:
0.4.1,Test with two objects
0.4.1,Check that we can't instantiate with non-transformers on the way
0.4.1,"Note that NoTrans implements fit, but not transform"
0.4.1,Check that params are set
0.4.1,Smoke test the repr:
0.4.1,Check that params are not set when naming them wrong
0.4.1,Test clone
0.4.1,"Check that apart from estimators, the parameters are the same"
0.4.1,Remove estimators that where copied
0.4.1,Test the various methods of the pipeline (anova).
0.4.1,Test with Anova + LogisticRegression
0.4.1,Test that the pipeline can take fit parameters
0.4.1,classifier should return True
0.4.1,and transformer params should not be changed
0.4.1,invalid parameters should raise an error message
0.4.1,Pipeline should pass sample_weight
0.4.1,When sample_weight is None it shouldn't be passed
0.4.1,Test pipeline raises set params error message for nested models.
0.4.1,nested model check
0.4.1,Test the various methods of the pipeline (pca + svm).
0.4.1,Test with PCA + SVC
0.4.1,Test the various methods of the pipeline (preprocessing + svm).
0.4.1,check shapes of various prediction functions
0.4.1,test that the fit_predict method is implemented on a pipeline
0.4.1,test that the fit_predict on pipeline yields same results as applying
0.4.1,transform and clustering steps separately
0.4.1,"As pipeline doesn't clone estimators on construction,"
0.4.1,it must have its own estimators
0.4.1,first compute the transform and clustering step separately
0.4.1,use a pipeline to do the transform and clustering in one step
0.4.1,tests that a pipeline does not have fit_predict method when final
0.4.1,step of pipeline does not have fit_predict defined
0.4.1,tests that Pipeline passes fit_params to intermediate steps
0.4.1,when fit_predict is invoked
0.4.1,Test whether pipeline works with a transformer at the end.
0.4.1,Also test pipeline.transform and pipeline.inverse_transform
0.4.1,test transform and fit_transform:
0.4.1,Test whether pipeline works with a transformer missing fit_transform
0.4.1,test fit_transform:
0.4.1,Directly setting attr
0.4.1,Using set_params
0.4.1,Using set_params to replace single step
0.4.1,With invalid data
0.4.1,Test setting Pipeline steps to None
0.4.1,"for other methods, ensure no AttributeErrors on None:"
0.4.1,mult2 and mult3 are active
0.4.1,Check None step at construction time
0.4.1,Test that an error is raised when memory is not a string or a Memory
0.4.1,instance
0.4.1,Define memory as an integer
0.4.1,Test with Transformer + SVC
0.4.1,Memoize the transformer at the first fit
0.4.1,Get the time stamp of the tranformer in the cached pipeline
0.4.1,Check that cached_pipe and pipe yield identical results
0.4.1,Check that we are reading the cache while fitting
0.4.1,a second time
0.4.1,Check that cached_pipe and pipe yield identical results
0.4.1,Create a new pipeline with cloned estimators
0.4.1,Check that even changing the name step does not affect the cache hit
0.4.1,Check that cached_pipe and pipe yield identical results
0.4.1,Test with Transformer + SVC
0.4.1,Memoize the transformer at the first fit
0.4.1,Get the time stamp of the tranformer in the cached pipeline
0.4.1,Check that cached_pipe and pipe yield identical results
0.4.1,Check that we are reading the cache while fitting
0.4.1,a second time
0.4.1,Check that cached_pipe and pipe yield identical results
0.4.1,Create a new pipeline with cloned estimators
0.4.1,Check that even changing the name step does not affect the cache hit
0.4.1,Check that cached_pipe and pipe yield identical results
0.4.1,Test the various methods of the pipeline (pca + svm).
0.4.1,Test with PCA + SVC
0.4.1,Test the various methods of the pipeline (pca + svm).
0.4.1,Test with PCA + SVC
0.4.1,Test whether pipeline works with a sampler at the end.
0.4.1,Also test pipeline.sampler
0.4.1,test transform and fit_transform:
0.4.1,We round the value near to zero. It seems that PCA has some issue
0.4.1,with that
0.4.1,Test whether pipeline works with a sampler at the end.
0.4.1,Also test pipeline.sampler
0.4.1,Test pipeline using None as preprocessing step and a classifier
0.4.1,"Test pipeline using None, RUS and a classifier"
0.4.1,"Test pipeline using RUS, None and a classifier"
0.4.1,Test pipeline using None step and a sampler
0.4.1,Test pipeline using None and a transformer that implements transform and
0.4.1,inverse_transform
0.4.1,Test the various methods of the pipeline (anova).
0.4.1,Test with RandomUnderSampling + Anova + LogisticRegression
0.4.1,Test the various methods of the pipeline (anova).
0.4.1,Test the various methods of the pipeline (anova).
0.4.1,Test with RandomUnderSampling + Anova + LogisticRegression
0.4.1,tests that Pipeline passes predict_params to the final estimator
0.4.1,when predict is invoked
0.4.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.4.1,Christos Aridas
0.4.1,License: MIT
0.4.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.4.1,License: MIT
0.4.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.4.1,License: MIT
0.4.1,Adapated from scikit-learn
0.4.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.4.1,License: MIT
0.4.1,FIXME: remove in 0.6
0.4.1,check that estimators treat dtype object as numeric if possible
0.4.1,trigger our checks if this is a SamplerMixin
0.4.1,monkey patch check_dtype_object for the sampler allowing strings
0.4.1,scikit-learn common tests
0.4.1,FIXME: in 0.6 set the random_state for all
0.4.1,IHT does not enforce the number of samples but provide a number
0.4.1,of samples the closest to the desired target.
0.4.1,FIXME remove in 0.6 -> ratio will be deprecated
0.4.1,in this test we will force all samplers to not change the class 1
0.4.1,in this test we will force all samplers to not change the class 1
0.4.1,check that sparse matrices can be passed through the sampler leading to
0.4.1,the same results than dense
0.4.1,set KMeans to full since it support sparse and dense
0.4.1,FIXME: in 0.6 set the random_state for all
0.4.1,Check that the samplers handle pandas dataframe and pandas series
0.4.1,FIXME: in 0.6 set the random_state for all
0.4.1,Check that multiclass target lead to the same results than OVA encoding
0.4.1,FIXME: in 0.6 set the random_state for all
0.4.1,Cast X and y to not default dtype
0.4.1,FIXME: in 0.6 set the random_state for all
0.4.1,Adapted from scikit-learn
0.4.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.4.1,License: MIT
0.4.1,meta-estimators need another estimator to be instantiated.
0.4.1,estimators that there is no way to default-construct sensibly
0.4.1,some strange ones
0.4.1,get parent folder
0.4.1,get rid of abstract base classes
0.4.1,get rid of sklearn estimators which have been imported in some classes
0.4.1,possibly get rid of meta estimators
0.4.1,"drop duplicates, sort for reproducibility"
0.4.1,itemgetter is used to ensure the sort does not extend to the 2nd item of
0.4.1,the tuple
0.4.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.4.1,License: MIT
0.4.1,FIXME: perfectly we should raise an error but the sklearn API does
0.4.1,not allow for it
0.4.1,check that all keys in sampling_strategy are also in y
0.4.1,check that there is no negative number
0.4.1,FIXME: Turn into an error in 0.6
0.4.1,clean-sampling can be more permissive since those samplers do not
0.4.1,use samples
0.4.1,check that all keys in sampling_strategy are also in y
0.4.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.4.1,Christos Aridas
0.4.1,License: MIT
0.4.1,this function could create an equal number of samples
0.4.1,We pass on purpose a non sorted dictionary and check that the resulting
0.4.1,dictionary is sorted. Refer to issue #428.
0.4.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.4.1,License: MIT
0.4.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.4.1,Christos Aridas
0.4.1,License: MIT
0.4.1,check if the filtering is working with a list or a single string
0.4.1,check that all estimators are sampler
0.4.1,check that an error is raised when the type is unknown
0.4.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.4.1,License: MIT
0.4.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.4.1,Christos Aridas
0.4.1,License: MIT
0.4.1,Otherwise create a default SMOTE
0.4.1,Otherwise create a default TomekLinks
0.4.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.4.1,Christos Aridas
0.4.1,License: MIT
0.4.1,Otherwise create a default SMOTE
0.4.1,Otherwise create a default EditedNearestNeighbours
0.4.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.4.1,Christos Aridas
0.4.1,License: MIT
0.4.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.4.1,Christos Aridas
0.4.1,License: MIT
0.4.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.4.1,License: MIT
0.4.1,resample before to fit the tree
0.4.1,Validate or convert input data
0.4.1,Pre-sort indices to avoid that each individual tree of the
0.4.1,ensemble sorts the indices.
0.4.1,Remap output
0.4.1,reshape is necessary to preserve the data contiguity against vs
0.4.1,"[:, np.newaxis] that does not."
0.4.1,Check parameters
0.4.1,"Free allocated memory, if any"
0.4.1,We draw from the random state to get the random state we
0.4.1,would have got if we hadn't used a warm_start.
0.4.1,Parallel loop: we prefer the threading backend as the Cython code
0.4.1,for fitting the trees is internally releasing the Python GIL
0.4.1,making threading more efficient than multiprocessing in
0.4.1,"that case. However, we respect any parallel_backend contexts set"
0.4.1,"at a higher level, since correctness does not rely on using"
0.4.1,threads.
0.4.1,Collect newly grown trees
0.4.1,Create pipeline with the fitted samplers and trees
0.4.1,Decapsulate classes_ attributes
0.4.1,Instances incorrectly classified
0.4.1,Error fraction
0.4.1,Stop if classification is perfect
0.4.1,Construct y coding as described in Zhu et al [2]:
0.4.1,
0.4.1,y_k = 1 if c == k else -1 / (K - 1)
0.4.1,
0.4.1,"where K == n_classes_ and c, k in [0, K) are indices along the second"
0.4.1,axis of the y coding with c being the index corresponding to the true
0.4.1,class label.
0.4.1,Displace zero probabilities so the log is defined.
0.4.1,Also fix negative elements which may occur with
0.4.1,negative sample weights.
0.4.1,Boost weight using multi-class AdaBoost SAMME.R alg
0.4.1,Only boost the weights if it will fit again
0.4.1,Only boost positive weights
0.4.1,Instances incorrectly classified
0.4.1,Error fraction
0.4.1,Stop if classification is perfect
0.4.1,Stop if the error is at least as bad as random guessing
0.4.1,Boost weight using multi-class AdaBoost SAMME alg
0.4.1,Only boost the weights if I will fit again
0.4.1,Only boost positive weights
0.4.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.4.1,Christos Aridas
0.4.1,License: MIT
0.4.1,doctest: +ELLIPSIS
0.4.1,array to know which samples are available to be taken
0.4.1,where the different set will be stored
0.4.1,store the index of the data to under-sample
0.4.1,value which will be picked at each round
0.4.1,extract the data of interest for this round from the
0.4.1,current class
0.4.1,select randomly the desired features
0.4.1,store the set created
0.4.1,fit and predict using cross validation
0.4.1,extract the prediction about the targeted classes only
0.4.1,check the stopping criterion
0.4.1,check that there is enough samples for another round
0.4.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.4.1,License: MIT
0.4.1,Ensemble are a bit specific since they are returning an array of
0.4.1,resampled arrays.
0.4.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.4.1,Christos Aridas
0.4.1,License: MIT
0.4.1,RandomUnderSampler is not supporting sample_weight. We need to pass
0.4.1,None.
0.4.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.4.1,Christos Aridas
0.4.1,License: MIT
0.4.1,RandomUnderSampler is not supporting sample_weight. We need to pass
0.4.1,None.
0.4.1,check that we have an ensemble of samplers and estimators with a
0.4.1,consistent size
0.4.1,each sampler in the ensemble should have different random state
0.4.1,each estimator in the ensemble should have different random state
0.4.1,check the consistency of the feature importances
0.4.1,check the consistency of the prediction outpus
0.4.1,Predictions should be the same when sample_weight are all ones
0.4.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.4.1,Christos Aridas
0.4.1,License: MIT
0.4.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.4.1,Christos Aridas
0.4.1,License: MIT
0.4.1,Check classification for various parameter settings.
0.4.1,Test that bootstrapping samples generate non-perfect base estimators.
0.4.1,"without bootstrap, all trees are perfect on the training set"
0.4.1,disable the resampling by passing an empty dictionary.
0.4.1,"with bootstrap, trees are no longer perfect on the training set"
0.4.1,Test that bootstrapping features may generate duplicate features.
0.4.1,Predict probabilities.
0.4.1,Normal case
0.4.1,"Degenerate case, where some classes are missing"
0.4.1,Check that oob prediction is a good estimation of the generalization
0.4.1,error.
0.4.1,Test with few estimators
0.4.1,Check singleton ensembles.
0.4.1,Test that it gives proper exception on deficient input.
0.4.1,Test n_estimators
0.4.1,Test max_samples
0.4.1,Test max_features
0.4.1,Test support of decision_function
0.4.1,Check that bagging ensembles can be grid-searched.
0.4.1,Transform iris into a binary classification task
0.4.1,Grid search with scoring based on decision_function
0.4.1,Check base_estimator and its default values.
0.4.1,Test if fitting incrementally with warm start gives a forest of the
0.4.1,right size and the same results as a normal fit.
0.4.1,Test if warm start'ed second fit with smaller n_estimators raises error.
0.4.1,Test that nothing happens when fitting without increasing n_estimators
0.4.1,"modify X to nonsense values, this should not change anything"
0.4.1,warm started classifier with 5+5 estimators should be equivalent to
0.4.1,one classifier with 10 estimators
0.4.1,Check using oob_score and warm_start simultaneously fails
0.4.1,"Make sure OOB scores are identical when random_state, estimator, and"
0.4.1,training data are fixed and fitting is done twice
0.4.1,Check that format of estimators_samples_ is correct and that results
0.4.1,generated at fit time can be identically reproduced at a later time
0.4.1,using data saved in object attributes.
0.4.1,remap the y outside of the BalancedBaggingclassifier
0.4.1,"_, y = np.unique(y, return_inverse=True)"
0.4.1,Get relevant attributes
0.4.1,Test for correct formatting
0.4.1,Re-fit single estimator to test for consistent sampling
0.4.1,Make sure validated max_samples and original max_samples are identical
0.4.1,when valid integer max_samples supplied by user
0.4.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.4.1,Christos Aridas
0.4.1,License: MIT
0.4.1,Generate a global dataset to use
0.4.1,Define a sampling_strategy
0.4.1,Define the sampling_strategy parameter
0.4.1,Create the sampling object
0.4.1,Get the different subset
0.4.1,Define the sampling_strategy parameter
0.4.1,Create the sampling object
0.4.1,Get the different subset
0.4.1,Define the sampling_strategy parameter
0.4.1,Create the sampling object
0.4.1,Get the different subset
0.4.1,Check classification for various parameter settings.
0.4.1,test the different prediction function
0.4.1,Check base_estimator and its default values.
0.4.1,Test if fitting incrementally with warm start gives a forest of the
0.4.1,right size and the same results as a normal fit.
0.4.1,Test if warm start'ed second fit with smaller n_estimators raises error.
0.4.1,Test that nothing happens when fitting without increasing n_estimators
0.4.1,"modify X to nonsense values, this should not change anything"
0.4.1,warm started classifier with 5+5 estimators should be equivalent to
0.4.1,one classifier with 10 estimators
0.4.1,Check warning if not enough estimators
0.4.1,Author: Guillaume Lemaitre
0.4.1,License: BSD 3 clause
0.4.1,"The index start at one, then we need to remove one"
0.4.1,to not have issue with the indexing.
0.4.1,go through the list and check if the data are available
0.4.1,Authors: Dayvid Oliveira
0.4.1,Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.4.1,Christos Aridas
0.4.1,License: MIT
0.4.1,restrict ratio to be a dict or a callable
0.4.1,FIXME remove ratio at 0.6
0.4.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.4.1,Christos Aridas
0.4.1,License: MIT
0.4.1,check an error is raised with we don't pass sampling_strategy and ratio
0.4.1,"we are reusing part of utils.check_sampling_strategy, however this is not"
0.4.1,cover in the common tests so we will repeat it here
0.4.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.4.1,Christos Aridas
0.4.1,License: MIT
0.4.1,This is a trick to avoid an error during tests collection with pytest. We
0.4.1,avoid the error when importing the package raise the error at the moment of
0.4.1,creating the instance.
0.4.1,FIXME: Remove in 0.6
0.4.1,shuffle the indices since the sampler are packing them by class
0.4.0,This file is here so that when running from the root folder
0.4.0,./sklearn is added to sys.path by pytest.
0.4.0,See https://docs.pytest.org/en/latest/pythonpath.html for more details.
0.4.0,"For example, this allows to build extensions in place and run pytest"
0.4.0,doc/modules/clustering.rst and use sklearn from the local folder
0.4.0,rather than the one from site-packages.
0.4.0,Set numpy array str/repr to legacy behaviour on numpy > 1.13 to make
0.4.0,the doctests pass
0.4.0,! /usr/bin/env python
0.4.0,get __version__ from _version.py
0.4.0,-*- coding: utf-8 -*-
0.4.0,
0.4.0,"imbalanced-learn documentation build configuration file, created by"
0.4.0,sphinx-quickstart on Mon Jan 18 14:44:12 2016.
0.4.0,
0.4.0,This file is execfile()d with the current directory set to its
0.4.0,containing dir.
0.4.0,
0.4.0,Note that not all possible configuration values are present in this
0.4.0,autogenerated file.
0.4.0,
0.4.0,All configuration values have a default; values that are commented out
0.4.0,serve to show the default.
0.4.0,"If extensions (or modules to document with autodoc) are in another directory,"
0.4.0,add these directories to sys.path here. If the directory is relative to the
0.4.0,"documentation root, use os.path.abspath to make it absolute, like shown here."
0.4.0,-- General configuration ------------------------------------------------
0.4.0,"If your documentation needs a minimal Sphinx version, state it here."
0.4.0,needs_sphinx = '1.0'
0.4.0,"Add any Sphinx extension module names here, as strings. They can be"
0.4.0,extensions coming with Sphinx (named 'sphinx.ext.*') or your custom
0.4.0,ones.
0.4.0,this is needed for some reason...
0.4.0,see https://github.com/numpy/numpydoc/issues/69
0.4.0,pngmath / imgmath compatibility layer for different sphinx versions
0.4.0,"Add any paths that contain templates here, relative to this directory."
0.4.0,generate autosummary even if no references
0.4.0,The suffix of source filenames.
0.4.0,The encoding of source files.
0.4.0,source_encoding = 'utf-8-sig'
0.4.0,Generate the plot for the gallery
0.4.0,The master toctree document.
0.4.0,General information about the project.
0.4.0,"The version info for the project you're documenting, acts as replacement for"
0.4.0,"|version| and |release|, also used in various other places throughout the"
0.4.0,built documents.
0.4.0,
0.4.0,The short X.Y version.
0.4.0,"The full version, including alpha/beta/rc tags."
0.4.0,The language for content autogenerated by Sphinx. Refer to documentation
0.4.0,for a list of supported languages.
0.4.0,language = None
0.4.0,"There are two options for replacing |today|: either, you set today to some"
0.4.0,"non-false value, then it is used:"
0.4.0,today = ''
0.4.0,"Else, today_fmt is used as the format for a strftime call."
0.4.0,"today_fmt = '%B %d, %Y'"
0.4.0,"List of patterns, relative to source directory, that match files and"
0.4.0,directories to ignore when looking for source files.
0.4.0,The reST default role (used for this markup: `text`) to use for all
0.4.0,documents.
0.4.0,default_role = None
0.4.0,"If true, '()' will be appended to :func: etc. cross-reference text."
0.4.0,"If true, the current module name will be prepended to all description"
0.4.0,unit titles (such as .. function::).
0.4.0,add_module_names = True
0.4.0,"If true, sectionauthor and moduleauthor directives will be shown in the"
0.4.0,output. They are ignored by default.
0.4.0,show_authors = False
0.4.0,The name of the Pygments (syntax highlighting) style to use.
0.4.0,Custom style
0.4.0,A list of ignored prefixes for module index sorting.
0.4.0,modindex_common_prefix = []
0.4.0,"If true, keep warnings as ""system message"" paragraphs in the built documents."
0.4.0,keep_warnings = False
0.4.0,-- Options for HTML output ----------------------------------------------
0.4.0,The theme to use for HTML and HTML Help pages.  See the documentation for
0.4.0,a list of builtin themes.
0.4.0,Theme options are theme-specific and customize the look and feel of a theme
0.4.0,"further.  For a list of options available for each theme, see the"
0.4.0,documentation.
0.4.0,html_theme_options = {'prev_next_buttons_location': None}
0.4.0,"Add any paths that contain custom themes here, relative to this directory."
0.4.0,"The name for this set of Sphinx documents.  If None, it defaults to"
0.4.0,"""<project> v<release> documentation""."
0.4.0,html_title = None
0.4.0,A shorter title for the navigation bar.  Default is the same as html_title.
0.4.0,html_short_title = None
0.4.0,The name of an image file (relative to this directory) to place at the top
0.4.0,of the sidebar.
0.4.0,html_logo = None
0.4.0,The name of an image file (within the static path) to use as favicon of the
0.4.0,docs.  This file should be a Windows icon file (.ico) being 16x16 or 32x32
0.4.0,pixels large.
0.4.0,html_favicon = None
0.4.0,"Add any paths that contain custom static files (such as style sheets) here,"
0.4.0,"relative to this directory. They are copied after the builtin static files,"
0.4.0,"so a file named ""default.css"" will overwrite the builtin ""default.css""."
0.4.0,Add any extra paths that contain custom files (such as robots.txt or
0.4.0,".htaccess) here, relative to this directory. These files are copied"
0.4.0,directly to the root of the documentation.
0.4.0,html_extra_path = []
0.4.0,"If not '', a 'Last updated on:' timestamp is inserted at every page bottom,"
0.4.0,using the given strftime format.
0.4.0,"html_last_updated_fmt = '%b %d, %Y'"
0.4.0,"If true, SmartyPants will be used to convert quotes and dashes to"
0.4.0,typographically correct entities.
0.4.0,html_use_smartypants = True
0.4.0,"Custom sidebar templates, maps document names to template names."
0.4.0,html_sidebars = {}
0.4.0,"Additional templates that should be rendered to pages, maps page names to"
0.4.0,template names.
0.4.0,html_additional_pages = {}
0.4.0,"If false, no module index is generated."
0.4.0,html_domain_indices = True
0.4.0,"If false, no index is generated."
0.4.0,html_use_index = True
0.4.0,"If true, the index is split into individual pages for each letter."
0.4.0,html_split_index = False
0.4.0,"If true, links to the reST sources are added to the pages."
0.4.0,html_show_sourcelink = True
0.4.0,"If true, ""Created using Sphinx"" is shown in the HTML footer. Default is True."
0.4.0,html_show_sphinx = True
0.4.0,"If true, ""(C) Copyright ..."" is shown in the HTML footer. Default is True."
0.4.0,html_show_copyright = True
0.4.0,"If true, an OpenSearch description file will be output, and all pages will"
0.4.0,contain a <link> tag referring to it.  The value of this option must be the
0.4.0,base URL from which the finished HTML is served.
0.4.0,html_use_opensearch = ''
0.4.0,"This is the file name suffix for HTML files (e.g. "".xhtml"")."
0.4.0,html_file_suffix = None
0.4.0,Output file base name for HTML help builder.
0.4.0,-- Options for LaTeX output ---------------------------------------------
0.4.0,The paper size ('letterpaper' or 'a4paper').
0.4.0,"'papersize': 'letterpaper',"
0.4.0,"The font size ('10pt', '11pt' or '12pt')."
0.4.0,"'pointsize': '10pt',"
0.4.0,Additional stuff for the LaTeX preamble.
0.4.0,"'preamble': '',"
0.4.0,Grouping the document tree into LaTeX files. List of tuples
0.4.0,"(source start file, target name, title,"
0.4.0,"author, documentclass [howto, manual, or own class])."
0.4.0,The name of an image file (relative to this directory) to place at the top of
0.4.0,the title page.
0.4.0,latex_logo = None
0.4.0,"For ""manual"" documents, if this is true, then toplevel headings are parts,"
0.4.0,not chapters.
0.4.0,latex_use_parts = False
0.4.0,"If true, show page references after internal links."
0.4.0,latex_show_pagerefs = False
0.4.0,"If true, show URL addresses after external links."
0.4.0,latex_show_urls = False
0.4.0,Documents to append as an appendix to all manuals.
0.4.0,latex_appendices = []
0.4.0,intersphinx configuration
0.4.0,sphinx-gallery configuration
0.4.0,-- Options for manual page output ---------------------------------------
0.4.0,"If false, no module index is generated."
0.4.0,latex_domain_indices = True
0.4.0,One entry per manual page. List of tuples
0.4.0,"(source start file, name, description, authors, manual section)."
0.4.0,"If true, show URL addresses after external links."
0.4.0,man_show_urls = False
0.4.0,-- Options for Texinfo output -------------------------------------------
0.4.0,Grouping the document tree into Texinfo files. List of tuples
0.4.0,"(source start file, target name, title, author,"
0.4.0,"dir menu entry, description, category)"
0.4.0,"def generate_example_rst(app, what, name, obj, options, lines):"
0.4.0,"# generate empty examples files, so that we don't get"
0.4.0,# inclusion errors if there are no examples for a class / module
0.4.0,"examples_path = os.path.join(app.srcdir, ""generated"","
0.4.0,"""%s.examples"" % name)"
0.4.0,if not os.path.exists(examples_path):
0.4.0,# touch file
0.4.0,"open(examples_path, 'w').close()"
0.4.0,Config for sphinx_issues
0.4.0,"app.connect('autodoc-process-docstring', generate_example_rst)"
0.4.0,Documents to append as an appendix to all manuals.
0.4.0,texinfo_appendices = []
0.4.0,"If false, no module index is generated."
0.4.0,texinfo_domain_indices = True
0.4.0,"How to display URL addresses: 'footnote', 'no', or 'inline'."
0.4.0,texinfo_show_urls = 'footnote'
0.4.0,"If true, do not generate a @detailmenu in the ""Top"" node's menu."
0.4.0,texinfo_no_detailmenu = False
0.4.0,The following is used by sphinx.ext.linkcode to provide links to github
0.4.0,get the styles from the current theme
0.4.0,create and add the button to all the code blocks that contain >>>
0.4.0,tracebacks (.gt) contain bare text elements that need to be
0.4.0,wrapped in a span to work with .nextUntil() (see later)
0.4.0,define the behavior of the button when it's clicked
0.4.0,hide the code output
0.4.0,show the code output
0.4.0,-*- coding: utf-8 -*-
0.4.0,Format template for issues URI
0.4.0,e.g. 'https://github.com/sloria/marshmallow/issues/{issue}
0.4.0,"Shortcut for Github, e.g. 'sloria/marshmallow'"
0.4.0,Format template for user profile URI
0.4.0,e.g. 'https://github.com/{user}'
0.4.0,Python 2 only
0.4.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.4.0,License: MIT
0.4.0,##############################################################################
0.4.0,"First, we will create an imbalanced data set from a the iris data set."
0.4.0,##############################################################################
0.4.0,Using ``sampling_strategy`` in resampling algorithms
0.4.0,##############################################################################
0.4.0,##############################################################################
0.4.0,``sampling_strategy`` as a ``float``
0.4.0,....................................
0.4.0,
0.4.0,``sampling_strategy`` can be given a ``float``. For **under-sampling
0.4.0,"methods**, it corresponds to the ratio :math:`\\alpha_{us}` defined by"
0.4.0,:math:`N_{rM} = \\alpha_{us} \\times N_{m}` where :math:`N_{rM}` and
0.4.0,:math:`N_{m}` are the number of samples in the majority class after
0.4.0,"resampling and the number of samples in the minority class, respectively."
0.4.0,select only 2 classes since the ratio make sense in this case
0.4.0,##############################################################################
0.4.0,"For **over-sampling methods**, it correspond to the ratio"
0.4.0,:math:`\\alpha_{os}` defined by :math:`N_{rm} = \\alpha_{os} \\times N_{m}`
0.4.0,where :math:`N_{rm}` and :math:`N_{M}` are the number of samples in the
0.4.0,minority class after resampling and the number of samples in the majority
0.4.0,"class, respectively."
0.4.0,##############################################################################
0.4.0,``sampling_strategy`` has a ``str``
0.4.0,...................................
0.4.0,
0.4.0,``sampling_strategy`` can be given as a string which specify the class
0.4.0,"targeted by the resampling. With under- and over-sampling, the number of"
0.4.0,samples will be equalized.
0.4.0,
0.4.0,Note that we are using multiple classes from now on.
0.4.0,##############################################################################
0.4.0,"With **cleaning method**, the number of samples in each class will not be"
0.4.0,equalized even if targeted.
0.4.0,##############################################################################
0.4.0,``sampling_strategy`` as a ``dict``
0.4.0,...................................
0.4.0,
0.4.0,"When ``sampling_strategy`` is a ``dict``, the keys correspond to the targeted"
0.4.0,classes. The values correspond to the desired number of samples for each
0.4.0,targeted class. This is working for both **under- and over-sampling**
0.4.0,algorithms but not for the **cleaning algorithms**. Use a ``list`` instead.
0.4.0,##############################################################################
0.4.0,``sampling_strategy`` as a ``list``
0.4.0,...................................
0.4.0,
0.4.0,"When ``sampling_strategy`` is a ``list``, the list contains the targeted"
0.4.0,classes. It is used only for **cleaning methods** and raise an error
0.4.0,otherwise.
0.4.0,##############################################################################
0.4.0,``sampling_strategy`` as a callable
0.4.0,...................................
0.4.0,
0.4.0,"When callable, function taking ``y`` and returns a ``dict``. The keys"
0.4.0,correspond to the targeted classes. The values correspond to the desired
0.4.0,number of samples for each class.
0.4.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.4.0,License: MIT
0.4.0,#############################################################################
0.4.0,Toy data generation
0.4.0,#############################################################################
0.4.0,#############################################################################
0.4.0,We are generating some non Gaussian data set contaminated with some unform
0.4.0,noise.
0.4.0,#############################################################################
0.4.0,We will generate some cleaned test data without outliers.
0.4.0,#############################################################################
0.4.0,How to use the :class:`imblearn.FunctionSampler`
0.4.0,#############################################################################
0.4.0,#############################################################################
0.4.0,We first define a function which will use
0.4.0,:class:`sklearn.ensemble.IsolationForest` to eliminate some outliers from
0.4.0,our dataset during training. The function passed to the
0.4.0,:class:`imblearn.FunctionSampler` will be called when using the method
0.4.0,``fit_resample``.
0.4.0,#############################################################################
0.4.0,Integrate it within a pipeline
0.4.0,#############################################################################
0.4.0,#############################################################################
0.4.0,"By elimnating outliers before the training, the classifier will be less"
0.4.0,affected during the prediction.
0.4.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.4.0,License: MIT
0.4.0,generate some data points
0.4.0,plot the majority and minority samples
0.4.0,draw the circle in which the new sample will generated
0.4.0,plot the line on which the sample will be generated
0.4.0,create and plot the new sample
0.4.0,make the plot nicer with legend and label
0.4.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.4.0,License: MIT
0.4.0,##############################################################################
0.4.0,The following function will be used to create toy dataset. It using the
0.4.0,``make_classification`` from scikit-learn but fixing some parameters.
0.4.0,##############################################################################
0.4.0,The following function will be used to plot the sample space after resampling
0.4.0,to illustrate the characterisitic of an algorithm.
0.4.0,make nice plotting
0.4.0,##############################################################################
0.4.0,The following function will be used to plot the decision function of a
0.4.0,classifier given some data.
0.4.0,##############################################################################
0.4.0,Illustration of the influence of the balancing ratio
0.4.0,##############################################################################
0.4.0,##############################################################################
0.4.0,We will first illustrate the influence of the balancing ratio on some toy
0.4.0,data using a linear SVM classifier. Greater is the difference between the
0.4.0,"number of samples in each class, poorer are the classfication results."
0.4.0,##############################################################################
0.4.0,Random over-sampling to balance the data set
0.4.0,##############################################################################
0.4.0,##############################################################################
0.4.0,Random over-sampling can be used to repeat some samples and balance the
0.4.0,number of samples between the dataset. It can be seen that with this trivial
0.4.0,approach the boundary decision is already less biaised toward the majority
0.4.0,class.
0.4.0,##############################################################################
0.4.0,More advanced over-sampling using ADASYN and SMOTE
0.4.0,##############################################################################
0.4.0,##############################################################################
0.4.0,"Instead of repeating the same samples when over-sampling, we can use some"
0.4.0,specific heuristic instead. ADASYN and SMOTE can be used in this case.
0.4.0,Make an identity sampler
0.4.0,##############################################################################
0.4.0,The following plot illustrate the difference between ADASYN and SMOTE. ADASYN
0.4.0,will focus on the samples which are difficult to classify with a
0.4.0,nearest-neighbors rule while regular SMOTE will not make any distinction.
0.4.0,"Therefore, the decision function depending of the algorithm."
0.4.0,##############################################################################
0.4.0,"Due to those sampling particularities, it can give rise to some specific"
0.4.0,issues as illustrated below.
0.4.0,##############################################################################
0.4.0,SMOTE proposes several variants by identifying specific samples to consider
0.4.0,during the resampling. The borderline version will detect which point to
0.4.0,select which are in the border between two classes. The SVM version will use
0.4.0,the support vectors found using an SVM algorithm to create new samples.
0.4.0,##############################################################################
0.4.0,"When dealing with a mixed of continuous and categorical features, SMOTE-NC"
0.4.0,is the only method which can handle this case.
0.4.0,create a synthetic data set with continuous and categorical features
0.4.0,Authors: Christos Aridas
0.4.0,Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.4.0,License: MIT
0.4.0,Generate the dataset
0.4.0,make nice plotting
0.4.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.4.0,License: MIT
0.4.0,Generate a dataset
0.4.0,Split the data
0.4.0,Train the classifier with balancing
0.4.0,Test the classifier and get the prediction
0.4.0,Show the classification report
0.4.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.4.0,License: MIT
0.4.0,Generate a dataset
0.4.0,Split the data
0.4.0,Train the classifier with balancing
0.4.0,Test the classifier and get the prediction
0.4.0,##############################################################################
0.4.0,The geometric mean corresponds to the square root of the product of the
0.4.0,sensitivity and specificity. Combining the two metrics should account for
0.4.0,the balancing of the dataset.
0.4.0,##############################################################################
0.4.0,The index balanced accuracy can transform any metric to be used in
0.4.0,imbalanced learning problems.
0.4.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.4.0,License: MIT
0.4.0,##############################################################################
0.4.0,The following function will be used to create toy dataset. It using the
0.4.0,``make_classification`` from scikit-learn but fixing some parameters.
0.4.0,##############################################################################
0.4.0,The following function will be used to plot the sample space after resampling
0.4.0,to illustrate the characteristic of an algorithm.
0.4.0,make nice plotting
0.4.0,##############################################################################
0.4.0,The following function will be used to plot the decision function of a
0.4.0,classifier given some data.
0.4.0,##############################################################################
0.4.0,"``SMOTE`` allows to generate samples. However, this method of over-sampling"
0.4.0,"does not have any knowledge regarding the underlying distribution. Therefore,"
0.4.0,"some noisy samples can be generated, e.g. when the different classes cannot"
0.4.0,"be well separated. Hence, it can be beneficial to apply an under-sampling"
0.4.0,algorithm to clean the noisy samples. Two methods are usually used in the
0.4.0,literature: (i) Tomek's link and (ii) edited nearest neighbours cleaning
0.4.0,methods. Imbalanced-learn provides two ready-to-use samplers ``SMOTETomek``
0.4.0,"and ``SMOTEENN``. In general, ``SMOTEENN`` cleans more noisy data than"
0.4.0,``SMOTETomek``.
0.4.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.4.0,License: MIT
0.4.0,##############################################################################
0.4.0,Load an imbalanced dataset
0.4.0,##############################################################################
0.4.0,We will load the UCI SatImage dataset which has an imbalanced ratio of 9.3:1
0.4.0,(number of majority sample for a minority sample). The data are then split
0.4.0,into training and testing.
0.4.0,##############################################################################
0.4.0,Classification using a single decision tree
0.4.0,##############################################################################
0.4.0,We train a decision tree classifier which will be used as a baseline for the
0.4.0,rest of this example.
0.4.0,##############################################################################
0.4.0,The results are reported in terms of balanced accuracy and geometric mean
0.4.0,which are metrics widely used in the literature to validate model trained on
0.4.0,imbalanced set.
0.4.0,##############################################################################
0.4.0,Classification using bagging classifier with and without sampling
0.4.0,##############################################################################
0.4.0,"Instead of using a single tree, we will check if an ensemble of decsion tree"
0.4.0,"can actually alleviate the issue induced by the class imbalancing. First, we"
0.4.0,will use a bagging classifier and its counter part which internally uses a
0.4.0,random under-sampling to balanced each boostrap sample.
0.4.0,##############################################################################
0.4.0,Balancing each bootstrap sample allows to increase significantly the balanced
0.4.0,accuracy and the geometric mean.
0.4.0,##############################################################################
0.4.0,Classification using random forest classifier with and without sampling
0.4.0,##############################################################################
0.4.0,Random forest is another popular ensemble method and it is usually
0.4.0,"outperforming bagging. Here, we used a vanilla random forest and its balanced"
0.4.0,counterpart in which each bootstrap sample is balanced.
0.4.0,"Similarly to the previous experiment, the balanced classifier outperform the"
0.4.0,"classifier which learn from imbalanced bootstrap samples. In addition, random"
0.4.0,forest outsperforms the bagging classifier.
0.4.0,##############################################################################
0.4.0,Boosting classifier
0.4.0,##############################################################################
0.4.0,"In the same manner, easy ensemble classifier is a bag of balanced AdaBoost"
0.4.0,"classifier. However, it will be slower to train than random forest and will"
0.4.0,achieve worse performance.
0.4.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.4.0,License: MIT
0.4.0,##############################################################################
0.4.0,The following function will be used to create toy dataset. It using the
0.4.0,``make_classification`` from scikit-learn but fixing some parameters.
0.4.0,##############################################################################
0.4.0,The following function will be used to plot the sample space after resampling
0.4.0,to illustrate the characteristic of an algorithm.
0.4.0,make nice plotting
0.4.0,##############################################################################
0.4.0,The following function will be used to plot the decision function of a
0.4.0,classifier given some data.
0.4.0,##############################################################################
0.4.0,Prototype generation: under-sampling by generating new samples
0.4.0,##############################################################################
0.4.0,##############################################################################
0.4.0,``ClusterCentroids`` under-samples by replacing the original samples by the
0.4.0,centroids of the cluster found.
0.4.0,##############################################################################
0.4.0,Prototype selection: under-sampling by selecting existing samples
0.4.0,##############################################################################
0.4.0,##############################################################################
0.4.0,The algorithm performing prototype selection can be subdivided into two
0.4.0,groups: (i) the controlled under-sampling methods and (ii) the cleaning
0.4.0,under-sampling methods.
0.4.0,##############################################################################
0.4.0,"With the controlled under-sampling methods, the number of samples to be"
0.4.0,selected can be specified. ``RandomUnderSampler`` is the most naive way of
0.4.0,performing such selection by randomly selecting a given number of samples by
0.4.0,the targetted class.
0.4.0,##############################################################################
0.4.0,``NearMiss`` algorithms implement some heuristic rules in order to select
0.4.0,samples. NearMiss-1 selects samples from the majority class for which the
0.4.0,average distance of the :math:`k`` nearest samples of the minority class is
0.4.0,the smallest. NearMiss-2 selects the samples from the majority class for
0.4.0,which the average distance to the farthest samples of the negative class is
0.4.0,"the smallest. NearMiss-3 is a 2-step algorithm: first, for each minority"
0.4.0,"sample, their ::math:`m` nearest-neighbors will be kept; then, the majority"
0.4.0,samples selected are the on for which the average distance to the :math:`k`
0.4.0,nearest neighbors is the largest.
0.4.0,##############################################################################
0.4.0,``EditedNearestNeighbours`` removes samples of the majority class for which
0.4.0,their class differ from the one of their nearest-neighbors. This sieve can be
0.4.0,repeated which is the principle of the
0.4.0,``RepeatedEditedNearestNeighbours``. ``AllKNN`` is slightly different from
0.4.0,the ``RepeatedEditedNearestNeighbours`` by changing the :math:`k` parameter
0.4.0,"of the internal nearest neighors algorithm, increasing it at each iteration."
0.4.0,##############################################################################
0.4.0,``CondensedNearestNeighbour`` makes use of a 1-NN to iteratively decide if a
0.4.0,sample should be kept in a dataset or not. The issue is that
0.4.0,``CondensedNearestNeighbour`` is sensitive to noise by preserving the noisy
0.4.0,samples. ``OneSidedSelection`` also used the 1-NN and use ``TomekLinks`` to
0.4.0,remove the samples considered noisy. The ``NeighbourhoodCleaningRule`` use a
0.4.0,"``EditedNearestNeighbours`` to remove some sample. Additionally, they use a 3"
0.4.0,nearest-neighbors to remove samples which do not agree with this rule.
0.4.0,##############################################################################
0.4.0,``InstanceHardnessThreshold`` uses the prediction of classifier to exclude
0.4.0,samples. All samples which are classified with a low probability will be
0.4.0,removed.
0.4.0,##############################################################################
0.4.0,This function allows to make nice plotting
0.4.0,##############################################################################
0.4.0,Generate some data with one Tomek link
0.4.0,minority class
0.4.0,majority class
0.4.0,##############################################################################
0.4.0,"In the figure above, the samples highlighted in green form a Tomek link since"
0.4.0,they are of different classes and are nearest neighbours of each other.
0.4.0,highlight the samples of interest
0.4.0,##############################################################################
0.4.0,We can run the ``TomekLinks`` sampling to remove the corresponding
0.4.0,samples. If ``sampling_strategy='auto'`` only the sample from the majority
0.4.0,class will be removed. If ``sampling_strategy='all'`` both samples will be
0.4.0,removed.
0.4.0,highlight the samples of interest
0.4.0,##############################################################################
0.4.0,This function allows to make nice plotting
0.4.0,##############################################################################
0.4.0,We can start by generating some data to later illustrate the principle of
0.4.0,each NearMiss heuritic rules.
0.4.0,minority class
0.4.0,majority class
0.4.0,##############################################################################
0.4.0,NearMiss-1
0.4.0,##############################################################################
0.4.0,##############################################################################
0.4.0,NearMiss-1 selects samples from the majority class for which the average
0.4.0,distance to some nearest neighbours is the smallest. In the following
0.4.0,"example, we use a 3-NN to compute the average distance on 2 specific samples"
0.4.0,"of the majority class. Therefore, in this case the point linked by the"
0.4.0,green-dashed line will be selected since the average distance is smaller.
0.4.0,##############################################################################
0.4.0,NearMiss-2
0.4.0,##############################################################################
0.4.0,##############################################################################
0.4.0,NearMiss-2 selects samples from the majority class for which the average
0.4.0,distance to the farthest neighbors is the smallest. With the same
0.4.0,"configuration as previously presented, the sample linked to the green-dashed"
0.4.0,line will be selected since its distance the 3 farthest neighbors is the
0.4.0,smallest.
0.4.0,##############################################################################
0.4.0,NearMiss-3
0.4.0,##############################################################################
0.4.0,##############################################################################
0.4.0,"NearMiss-3 can be divided into 2 steps. First, a nearest-neighbors is used to"
0.4.0,short-list samples from the majority class (i.e. correspond to the
0.4.0,"highlighted samples in the following plot). Then, the sample with the largest"
0.4.0,average distance to the *k* nearest-neighbors are selected.
0.4.0,select only the majority point of interest
0.4.0,Authors: Christos Aridas
0.4.0,Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.4.0,License: MIT
0.4.0,Generate the dataset
0.4.0,Instanciate a PCA object for the sake of easy visualisation
0.4.0,Create the samplers
0.4.0,Create the classifier
0.4.0,Make the splits
0.4.0,Add one transformers and two samplers in the pipeline object
0.4.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.4.0,License: MIT
0.4.0,##############################################################################
0.4.0,Data loading
0.4.0,##############################################################################
0.4.0,##############################################################################
0.4.0,"First, you should download the Porto Seguro data set from Kaggle. See the"
0.4.0,link in the introduction.
0.4.0,##############################################################################
0.4.0,The data set is imbalanced and it will have an effect on the fitting.
0.4.0,##############################################################################
0.4.0,Define the pre-processing pipeline
0.4.0,##############################################################################
0.4.0,##############################################################################
0.4.0,We want to standard scale the numerical features while we want to one-hot
0.4.0,"encode the categorical features. In this regard, we make use of the"
0.4.0,:class:`sklearn.compose.ColumnTransformer`.
0.4.0,Create an environment variable to avoid using the GPU. This can be changed.
0.4.0,##############################################################################
0.4.0,Create a neural-network
0.4.0,##############################################################################
0.4.0,##############################################################################
0.4.0,We create a decorator to report the computation time
0.4.0,##############################################################################
0.4.0,The first model will be trained using the ``fit`` method and with imbalanced
0.4.0,mini-batches.
0.4.0,##############################################################################
0.4.0,"In the contrary, we will use imbalanced-learn to create a generator of"
0.4.0,mini-batches which will yield balanced mini-batches.
0.4.0,##############################################################################
0.4.0,Classification loop
0.4.0,##############################################################################
0.4.0,##############################################################################
0.4.0,We will perform a 10-fold cross-validation and train the neural-network with
0.4.0,the two different strategies previously presented.
0.4.0,##############################################################################
0.4.0,Plot of the results and computation time
0.4.0,##############################################################################
0.4.0,Authors: Christos Aridas
0.4.0,Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.4.0,License: MIT
0.4.0,Load the dataset
0.4.0,make nice plotting
0.4.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.4.0,License: MIT
0.4.0,Create a folder to fetch the dataset
0.4.0,Create a pipeline
0.4.0,Classify and report the results
0.4.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.4.0,License: MIT
0.4.0,##############################################################################
0.4.0,Setting the data set
0.4.0,##############################################################################
0.4.0,##############################################################################
0.4.0,We use a part of the 20 newsgroups data set by loading 4 topics. Using the
0.4.0,"scikit-learn loader, the data are split into a training and a testing set."
0.4.0,
0.4.0,Note the class \#3 is the minority class and has almost twice less samples
0.4.0,than the majority class.
0.4.0,##############################################################################
0.4.0,The usual scikit-learn pipeline
0.4.0,##############################################################################
0.4.0,##############################################################################
0.4.0,You might usually use scikit-learn pipeline by combining the TF-IDF
0.4.0,vectorizer to feed a multinomial naive bayes classifier. A classification
0.4.0,report summarized the results on the testing set.
0.4.0,
0.4.0,"As expected, the recall of the class \#3 is low mainly due to the class"
0.4.0,imbalanced.
0.4.0,##############################################################################
0.4.0,Balancing the class before classification
0.4.0,##############################################################################
0.4.0,##############################################################################
0.4.0,"To improve the prediction of the class \#3, it could be interesting to apply"
0.4.0,"a balancing before to train the naive bayes classifier. Therefore, we will"
0.4.0,use a ``RandomUnderSampler`` to equalize the number of samples in all the
0.4.0,classes before the training.
0.4.0,
0.4.0,It is also important to note that we are using the ``make_pipeline`` function
0.4.0,implemented in imbalanced-learn to properly handle the samplers.
0.4.0,##############################################################################
0.4.0,"Although the results are almost identical, it can be seen that the resampling"
0.4.0,allowed to correct the poor recall of the class \#3 at the cost of reducing
0.4.0,"the other metrics for the other classes. However, the overall results are"
0.4.0,slightly better.
0.4.0,Authors: Dayvid Oliveira
0.4.0,Christos Aridas
0.4.0,Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.4.0,License: MIT
0.4.0,Generate the dataset
0.4.0,"Two subplots, unpack the axes array immediately"
0.4.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.4.0,License: MIT
0.4.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.4.0,Christos Aridas
0.4.0,License: MIT
0.4.0,define an alias for back-compatibility
0.4.0,FIXME: remove in 0.6
0.4.0,FIXME: remove in 0.6
0.4.0,both ratio and sampling_strategy should not be set
0.4.0,Adapted from scikit-learn
0.4.0,Author: Edouard Duchesnay
0.4.0,Gael Varoquaux
0.4.0,Virgile Fritsch
0.4.0,Alexandre Gramfort
0.4.0,Lars Buitinck
0.4.0,Christos Aridas
0.4.0,Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.4.0,License: BSD
0.4.0,BaseEstimator interface
0.4.0,validate names
0.4.0,validate estimators
0.4.0,We allow last estimator to be None as an identity transformation
0.4.0,Estimator interface
0.4.0,Setup the memory
0.4.0,joblib >= 0.12
0.4.0,we do not clone when caching is disabled to
0.4.0,preserve backward compatibility
0.4.0,joblib < 0.11
0.4.0,we do not clone when caching is disabled to
0.4.0,preserve backward compatibility
0.4.0,Fit or load from cache the current transfomer
0.4.0,Replace the transformer of the step with the fitted
0.4.0,transformer. This is necessary when loading the transformer
0.4.0,from the cache.
0.4.0,"_final_estimator is None or has transform, otherwise attribute error"
0.4.0,raise AttributeError if necessary for hasattr behaviour
0.4.0,"if we have a weight for this transformer, multiply output"
0.4.0,Based on NiLearn package
0.4.0,License: simplified BSD
0.4.0,"PEP0440 compatible formatted version, see:"
0.4.0,https://www.python.org/dev/peps/pep-0440/
0.4.0,
0.4.0,Generic release markers:
0.4.0,X.Y
0.4.0,X.Y.Z # For bugfix releases
0.4.0,
0.4.0,Admissible pre-release markers:
0.4.0,X.YaN # Alpha release
0.4.0,X.YbN # Beta release
0.4.0,X.YrcN # Release Candidate
0.4.0,X.Y # Final release
0.4.0,
0.4.0,Dev branch marker is: 'X.Y.dev' or 'X.Y.devN' where N is an integer.
0.4.0,'X.Y.dev0' is the canonical version of 'X.Y.dev'
0.4.0,
0.4.0,coding: utf-8
0.4.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.4.0,Dariusz Brzezinski
0.4.0,License: MIT
0.4.0,Only negative labels
0.4.0,"Calculate tp_sum, pred_sum, true_sum ###"
0.4.0,labels are now from 0 to len(labels) - 1 -> use bincount
0.4.0,Pathological case
0.4.0,Compute the true negative
0.4.0,Retain only selected labels
0.4.0,"Finally, we have all our sufficient statistics. Divide! #"
0.4.0,"Divide, and on zero-division, set scores to 0 and warn:"
0.4.0,"Oddly, we may get an ""invalid"" rather than a ""divide"" error"
0.4.0,here.
0.4.0,Average the results
0.4.0,labels are now from 0 to len(labels) - 1 -> use bincount
0.4.0,Pathological case
0.4.0,Retain only selected labels
0.4.0,old version of scipy return MaskedConstant instead of 0.0
0.4.0,Create the list of tags
0.4.0,check that the scoring function does not need a score
0.4.0,and only a prediction
0.4.0,Compute the score from the scoring function
0.4.0,Square if desired
0.4.0,Get the signature of the sens/spec function
0.4.0,We need to extract from kwargs only the one needed by the
0.4.0,specificity and specificity
0.4.0,Make the intersection between the parameters
0.4.0,Create a sub dictionary
0.4.0,Check if the metric is the geometric mean
0.4.0,We do not support multilabel so the only average supported
0.4.0,is binary
0.4.0,Create the list of parameters through signature binding
0.4.0,Call the sens/spec function
0.4.0,Compute the dominance
0.4.0,Compute the different metrics
0.4.0,Precision/recall/f1
0.4.0,Specificity
0.4.0,Geometric mean
0.4.0,Index balanced accuracy
0.4.0,compute averages
0.4.0,coding: utf-8
0.4.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.4.0,Christos Aridas
0.4.0,License: MIT
0.4.0,##############################################################################
0.4.0,Utilities for testing
0.4.0,import some data to play with
0.4.0,restrict to a binary classification task
0.4.0,add noisy features to make the problem harder and avoid perfect results
0.4.0,"run classifier, get class probabilities and label predictions"
0.4.0,only interested in probabilities of the positive case
0.4.0,XXX: do we really want a special API for the binary case?
0.4.0,##############################################################################
0.4.0,Tests
0.4.0,detailed measures for each class
0.4.0,individual scoring function that can be used for grid search: in the
0.4.0,binary class case the score is the value of the measure for the positive
0.4.0,class (e.g. label == 1). This is deprecated for average != 'binary'.
0.4.0,Such a case may occur with non-stratified cross-validation
0.4.0,ensure the above were meaningful tests:
0.4.0,Bad pos_label
0.4.0,Bad average option
0.4.0,but average != 'binary'; even if data is binary
0.4.0,compute the geometric mean for the binary problem
0.4.0,print classification report with class names
0.4.0,print classification report with label detection
0.4.0,print classification report with class names
0.4.0,print classification report with label detection
0.4.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.4.0,Christos Aridas
0.4.0,License: MIT
0.4.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.4.0,Christos Aridas
0.4.0,License: MIT
0.4.0,The ratio is computed using a one-vs-rest manner. Using majority
0.4.0,in multi-class would lead to slightly different results at the
0.4.0,cost of introducing a new parameter.
0.4.0,the nearest neighbors need to be fitted only on the current class
0.4.0,to find the class NN to generate new samples
0.4.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.4.0,Christos Aridas
0.4.0,License: MIT
0.4.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.4.0,Christos Aridas
0.4.0,License: MIT
0.4.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.4.0,Fernando Nogueira
0.4.0,Christos Aridas
0.4.0,Dzianis Dudnik
0.4.0,License: MIT
0.4.0,FIXME: remove in 0.6
0.4.0,Samples are in danger for m/2 <= m' < m
0.4.0,Samples are noise for m = m'
0.4.0,FIXME: rename _sample -> _fit_resample in 0.6
0.4.0,divergence between borderline-1 and borderline-2
0.4.0,Create synthetic samples for borderline points.
0.4.0,only minority
0.4.0,we use a one-vs-rest policy to handle the multiclass in which
0.4.0,new samples will be created considering not only the majority
0.4.0,class but all over classes.
0.4.0,FIXME: rename _sample -> _fit_resample in 0.6
0.4.0,"FIXME: In 0.6, SMOTE should inherit only from BaseSMOTE."
0.4.0,FIXME: in 0.6 call super()
0.4.0,FIXME: in 0.6 call super()
0.4.0,FIXME: remove in 0.6 after deprecation cycle
0.4.0,FIXME: to be removed in 0.6
0.4.0,FIXME: uncomment in version 0.6
0.4.0,self._validate_estimator()
0.4.0,compute the median of the standard deviation of the minority class
0.4.0,the input of the OneHotEncoder needs to be dense
0.4.0,we can replace the 1 entries of the categorical features with the
0.4.0,median of the standard deviation. It will ensure that whenever
0.4.0,"distance is computed between 2 samples, the difference will be equal"
0.4.0,to the median of the standard deviation as in the original paper.
0.4.0,reverse the encoding of the categorical features
0.4.0,the matrix is supposed to be in the CSR format after the stacking
0.4.0,"To avoid conversion and since there is only few samples used, we"
0.4.0,convert those samples to dense array.
0.4.0,tie breaking argmax
0.4.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.4.0,Christos Aridas
0.4.0,License: MIT
0.4.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.4.0,Christos Aridas
0.4.0,Dzianis Dudnik
0.4.0,License: MIT
0.4.0,create 2 random continuous feature
0.4.0,create a categorical feature using some string
0.4.0,create a categorical feature using some integer
0.4.0,return the categories
0.4.0,create 2 random continuous feature
0.4.0,create a categorical feature using some string
0.4.0,create a categorical feature using some integer
0.4.0,return the categories
0.4.0,create 2 random continuous feature
0.4.0,create a categorical feature using some string
0.4.0,create a categorical feature using some integer
0.4.0,return the categories
0.4.0,create 2 random continuous feature
0.4.0,create a categorical feature using some string
0.4.0,create a categorical feature using some integer
0.4.0,return the categories
0.4.0,create 2 random continuous feature
0.4.0,create a categorical feature using some string
0.4.0,create a categorical feature using some integer
0.4.0,part of the common test which apply to SMOTE-NC even if it is not default
0.4.0,constructible
0.4.0,Check that the samplers handle pandas dataframe and pandas series
0.4.0,Cast X and y to not default dtype
0.4.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.4.0,Christos Aridas
0.4.0,License: MIT
0.4.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.4.0,Christos Aridas
0.4.0,License: MIT
0.4.0,FIXME: Remove in 0.6
0.4.0,shuffle the indices since the sampler are packing them by class
0.4.0,helper functions
0.4.0,input and output
0.4.0,build the model and weights
0.4.0,"build the loss, predict, and train operator"
0.4.0,Initialization of all variables in the graph
0.4.0,"For each epoch, run accuracy on train and test"
0.4.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.4.0,License: MIT
0.4.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.4.0,Fernando Nogueira
0.4.0,Christos Aridas
0.4.0,License: MIT
0.4.0,find which class to not consider
0.4.0,there is a Tomek link between two samples if they are both nearest
0.4.0,neighbors of each others.
0.4.0,check for deprecated random_state
0.4.0,Find the nearest neighbour of every point
0.4.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.4.0,Christos Aridas
0.4.0,License: MIT
0.4.0,Randomly get one sample from the majority class
0.4.0,Generate the index to select
0.4.0,Create the set C - One majority samples and all minority
0.4.0,Create the set S - all majority samples
0.4.0,fit knn on C
0.4.0,Check each sample in S if we keep it or drop it
0.4.0,Do not select sample which are already well classified
0.4.0,Classify on S
0.4.0,If the prediction do not agree with the true label
0.4.0,append it in C_x
0.4.0,Keep the index for later
0.4.0,Update C
0.4.0,fit a knn on C
0.4.0,This experimental to speed up the search
0.4.0,Classify all the element in S and avoid to test the
0.4.0,well classified elements
0.4.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.4.0,Dayvid Oliveira
0.4.0,Christos Aridas
0.4.0,License: MIT
0.4.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.4.0,Christos Aridas
0.4.0,License: MIT
0.4.0,Compute the distance considering the farthest neighbour
0.4.0,Sort the list of distance and get the index
0.4.0,Throw a warning to tell the user that we did not have enough samples
0.4.0,to select and that we just select everything
0.4.0,Select the desired number of samples
0.4.0,check for deprecated random_state
0.4.0,idx_tmp is relative to the feature selected in the
0.4.0,previous step and we need to find the indirection
0.4.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.4.0,Christos Aridas
0.4.0,License: MIT
0.4.0,select a sample from the current class
0.4.0,create the set composed of all minority samples and one
0.4.0,sample from the current class.
0.4.0,create the set S with removing the seed from S
0.4.0,since that it will be added anyway
0.4.0,apply Tomek cleaning
0.4.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.4.0,Dayvid Oliveira
0.4.0,Christos Aridas
0.4.0,License: MIT
0.4.0,check for deprecated random_state
0.4.0,check for deprecated random_state
0.4.0,Check the stopping criterion
0.4.0,1. If there is no changes for the vector y
0.4.0,2. If the number of samples in the other class become inferior to
0.4.0,the number of samples in the majority class
0.4.0,3. If one of the class is disappearing
0.4.0,Case 1
0.4.0,Case 2
0.4.0,Case 3
0.4.0,check for deprecated random_state
0.4.0,Check the stopping criterion
0.4.0,1. If the number of samples in the other class become inferior to
0.4.0,the number of samples in the majority class
0.4.0,2. If one of the class is disappearing
0.4.0,Case 1else:
0.4.0,overwrite b_min_bec_maj
0.4.0,Case 2
0.4.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.4.0,Christos Aridas
0.4.0,License: MIT
0.4.0,check for deprecated random_state
0.4.0,clean the neighborhood
0.4.0,compute which classes to consider for cleaning for the A2 group
0.4.0,compute a2 group
0.4.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.4.0,Christos Aridas
0.4.0,License: MIT
0.4.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.4.0,Christos Aridas
0.4.0,License: MIT
0.4.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.4.0,Christos Aridas
0.4.0,License: MIT
0.4.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.4.0,Christos Aridas
0.4.0,License: MIT
0.4.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.4.0,Christos Aridas
0.4.0,License: MIT
0.4.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.4.0,Christos Aridas
0.4.0,License: MIT
0.4.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.4.0,Christos Aridas
0.4.0,License: MIT
0.4.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.4.0,Christos Aridas
0.4.0,License: MIT
0.4.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.4.0,Christos Aridas
0.4.0,License: MIT
0.4.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.4.0,Christos Aridas
0.4.0,License: MIT
0.4.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.4.0,Christos Aridas
0.4.0,License: MIT
0.4.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.4.0,Fernando Nogueira
0.4.0,Christos Aridas
0.4.0,License: MIT
0.4.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.4.0,Christos Aridas
0.4.0,License: MIT
0.4.0,test that all_estimators doesn't find abstract classes.
0.4.0,don't run twice the sampler tests. Meta-estimator do not have a
0.4.0,fit_resample method.
0.4.0,input validation etc for non-meta estimators
0.4.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.4.0,License: MIT
0.4.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.4.0,Christos Aridas
0.4.0,License: MIT
0.4.0,store timestamp to figure out whether the result of 'fit' has been
0.4.0,cached or not
0.4.0,store timestamp to figure out whether the result of 'fit' has been
0.4.0,cached or not
0.4.0,Test the various init parameters of the pipeline.
0.4.0,Check that we can't instantiate pipelines with objects without fit
0.4.0,method
0.4.0,Smoke test with only an estimator
0.4.0,Check that params are set
0.4.0,Smoke test the repr:
0.4.0,Test with two objects
0.4.0,Check that we can't instantiate with non-transformers on the way
0.4.0,"Note that NoTrans implements fit, but not transform"
0.4.0,Check that params are set
0.4.0,Smoke test the repr:
0.4.0,Check that params are not set when naming them wrong
0.4.0,Test clone
0.4.0,"Check that apart from estimators, the parameters are the same"
0.4.0,Remove estimators that where copied
0.4.0,Test the various methods of the pipeline (anova).
0.4.0,Test with Anova + LogisticRegression
0.4.0,Test that the pipeline can take fit parameters
0.4.0,classifier should return True
0.4.0,and transformer params should not be changed
0.4.0,invalid parameters should raise an error message
0.4.0,Pipeline should pass sample_weight
0.4.0,When sample_weight is None it shouldn't be passed
0.4.0,Test pipeline raises set params error message for nested models.
0.4.0,nested model check
0.4.0,Test the various methods of the pipeline (pca + svm).
0.4.0,Test with PCA + SVC
0.4.0,Test the various methods of the pipeline (preprocessing + svm).
0.4.0,check shapes of various prediction functions
0.4.0,test that the fit_predict method is implemented on a pipeline
0.4.0,test that the fit_predict on pipeline yields same results as applying
0.4.0,transform and clustering steps separately
0.4.0,"As pipeline doesn't clone estimators on construction,"
0.4.0,it must have its own estimators
0.4.0,first compute the transform and clustering step separately
0.4.0,use a pipeline to do the transform and clustering in one step
0.4.0,tests that a pipeline does not have fit_predict method when final
0.4.0,step of pipeline does not have fit_predict defined
0.4.0,tests that Pipeline passes fit_params to intermediate steps
0.4.0,when fit_predict is invoked
0.4.0,Test whether pipeline works with a transformer at the end.
0.4.0,Also test pipeline.transform and pipeline.inverse_transform
0.4.0,test transform and fit_transform:
0.4.0,Test whether pipeline works with a transformer missing fit_transform
0.4.0,test fit_transform:
0.4.0,Directly setting attr
0.4.0,Using set_params
0.4.0,Using set_params to replace single step
0.4.0,With invalid data
0.4.0,Test setting Pipeline steps to None
0.4.0,"for other methods, ensure no AttributeErrors on None:"
0.4.0,mult2 and mult3 are active
0.4.0,Check None step at construction time
0.4.0,Test that an error is raised when memory is not a string or a Memory
0.4.0,instance
0.4.0,Define memory as an integer
0.4.0,Test with Transformer + SVC
0.4.0,Memoize the transformer at the first fit
0.4.0,Get the time stamp of the tranformer in the cached pipeline
0.4.0,Check that cached_pipe and pipe yield identical results
0.4.0,Check that we are reading the cache while fitting
0.4.0,a second time
0.4.0,Check that cached_pipe and pipe yield identical results
0.4.0,Create a new pipeline with cloned estimators
0.4.0,Check that even changing the name step does not affect the cache hit
0.4.0,Check that cached_pipe and pipe yield identical results
0.4.0,Test with Transformer + SVC
0.4.0,Memoize the transformer at the first fit
0.4.0,Get the time stamp of the tranformer in the cached pipeline
0.4.0,Check that cached_pipe and pipe yield identical results
0.4.0,Check that we are reading the cache while fitting
0.4.0,a second time
0.4.0,Check that cached_pipe and pipe yield identical results
0.4.0,Create a new pipeline with cloned estimators
0.4.0,Check that even changing the name step does not affect the cache hit
0.4.0,Check that cached_pipe and pipe yield identical results
0.4.0,Test the various methods of the pipeline (pca + svm).
0.4.0,Test with PCA + SVC
0.4.0,Test the various methods of the pipeline (pca + svm).
0.4.0,Test with PCA + SVC
0.4.0,Test whether pipeline works with a sampler at the end.
0.4.0,Also test pipeline.sampler
0.4.0,test transform and fit_transform:
0.4.0,We round the value near to zero. It seems that PCA has some issue
0.4.0,with that
0.4.0,Test whether pipeline works with a sampler at the end.
0.4.0,Also test pipeline.sampler
0.4.0,Test pipeline using None as preprocessing step and a classifier
0.4.0,"Test pipeline using None, RUS and a classifier"
0.4.0,"Test pipeline using RUS, None and a classifier"
0.4.0,Test pipeline using None step and a sampler
0.4.0,Test pipeline using None and a transformer that implements transform and
0.4.0,inverse_transform
0.4.0,Test the various methods of the pipeline (anova).
0.4.0,Test with RandomUnderSampling + Anova + LogisticRegression
0.4.0,Test the various methods of the pipeline (anova).
0.4.0,Test the various methods of the pipeline (anova).
0.4.0,Test with RandomUnderSampling + Anova + LogisticRegression
0.4.0,tests that Pipeline passes predict_params to the final estimator
0.4.0,when predict is invoked
0.4.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.4.0,Christos Aridas
0.4.0,License: MIT
0.4.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.4.0,License: MIT
0.4.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.4.0,License: MIT
0.4.0,Adapated from scikit-learn
0.4.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.4.0,License: MIT
0.4.0,FIXME: remove in 0.6
0.4.0,check that estimators treat dtype object as numeric if possible
0.4.0,trigger our checks if this is a SamplerMixin
0.4.0,monkey patch check_dtype_object for the sampler allowing strings
0.4.0,scikit-learn common tests
0.4.0,FIXME: in 0.6 set the random_state for all
0.4.0,IHT does not enforce the number of samples but provide a number
0.4.0,of samples the closest to the desired target.
0.4.0,FIXME remove in 0.6 -> ratio will be deprecated
0.4.0,in this test we will force all samplers to not change the class 1
0.4.0,in this test we will force all samplers to not change the class 1
0.4.0,check that sparse matrices can be passed through the sampler leading to
0.4.0,the same results than dense
0.4.0,set KMeans to full since it support sparse and dense
0.4.0,FIXME: in 0.6 set the random_state for all
0.4.0,Check that the samplers handle pandas dataframe and pandas series
0.4.0,FIXME: in 0.6 set the random_state for all
0.4.0,Check that multiclass target lead to the same results than OVA encoding
0.4.0,FIXME: in 0.6 set the random_state for all
0.4.0,Cast X and y to not default dtype
0.4.0,FIXME: in 0.6 set the random_state for all
0.4.0,Adapted from scikit-learn
0.4.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.4.0,License: MIT
0.4.0,meta-estimators need another estimator to be instantiated.
0.4.0,estimators that there is no way to default-construct sensibly
0.4.0,some strange ones
0.4.0,get parent folder
0.4.0,get rid of abstract base classes
0.4.0,get rid of sklearn estimators which have been imported in some classes
0.4.0,possibly get rid of meta estimators
0.4.0,"drop duplicates, sort for reproducibility"
0.4.0,itemgetter is used to ensure the sort does not extend to the 2nd item of
0.4.0,the tuple
0.4.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.4.0,License: MIT
0.4.0,FIXME: perfectly we should raise an error but the sklearn API does
0.4.0,not allow for it
0.4.0,check that all keys in sampling_strategy are also in y
0.4.0,check that there is no negative number
0.4.0,FIXME: Turn into an error in 0.6
0.4.0,clean-sampling can be more permissive since those samplers do not
0.4.0,use samples
0.4.0,check that all keys in sampling_strategy are also in y
0.4.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.4.0,Christos Aridas
0.4.0,License: MIT
0.4.0,this function could create an equal number of samples
0.4.0,We pass on purpose a non sorted dictionary and check that the resulting
0.4.0,dictionary is sorted. Refer to issue #428.
0.4.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.4.0,License: MIT
0.4.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.4.0,Christos Aridas
0.4.0,License: MIT
0.4.0,check if the filtering is working with a list or a single string
0.4.0,check that all estimators are sampler
0.4.0,check that an error is raised when the type is unknown
0.4.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.4.0,License: MIT
0.4.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.4.0,Christos Aridas
0.4.0,License: MIT
0.4.0,Otherwise create a default SMOTE
0.4.0,Otherwise create a default TomekLinks
0.4.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.4.0,Christos Aridas
0.4.0,License: MIT
0.4.0,Otherwise create a default SMOTE
0.4.0,Otherwise create a default EditedNearestNeighbours
0.4.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.4.0,Christos Aridas
0.4.0,License: MIT
0.4.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.4.0,Christos Aridas
0.4.0,License: MIT
0.4.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.4.0,License: MIT
0.4.0,resample before to fit the tree
0.4.0,Validate or convert input data
0.4.0,Pre-sort indices to avoid that each individual tree of the
0.4.0,ensemble sorts the indices.
0.4.0,Remap output
0.4.0,reshape is necessary to preserve the data contiguity against vs
0.4.0,"[:, np.newaxis] that does not."
0.4.0,Check parameters
0.4.0,"Free allocated memory, if any"
0.4.0,We draw from the random state to get the random state we
0.4.0,would have got if we hadn't used a warm_start.
0.4.0,Parallel loop: we prefer the threading backend as the Cython code
0.4.0,for fitting the trees is internally releasing the Python GIL
0.4.0,making threading more efficient than multiprocessing in
0.4.0,"that case. However, we respect any parallel_backend contexts set"
0.4.0,"at a higher level, since correctness does not rely on using"
0.4.0,threads.
0.4.0,Collect newly grown trees
0.4.0,Create pipeline with the fitted samplers and trees
0.4.0,Decapsulate classes_ attributes
0.4.0,Instances incorrectly classified
0.4.0,Error fraction
0.4.0,Stop if classification is perfect
0.4.0,Construct y coding as described in Zhu et al [2]:
0.4.0,
0.4.0,y_k = 1 if c == k else -1 / (K - 1)
0.4.0,
0.4.0,"where K == n_classes_ and c, k in [0, K) are indices along the second"
0.4.0,axis of the y coding with c being the index corresponding to the true
0.4.0,class label.
0.4.0,Displace zero probabilities so the log is defined.
0.4.0,Also fix negative elements which may occur with
0.4.0,negative sample weights.
0.4.0,Boost weight using multi-class AdaBoost SAMME.R alg
0.4.0,Only boost the weights if it will fit again
0.4.0,Only boost positive weights
0.4.0,Instances incorrectly classified
0.4.0,Error fraction
0.4.0,Stop if classification is perfect
0.4.0,Stop if the error is at least as bad as random guessing
0.4.0,Boost weight using multi-class AdaBoost SAMME alg
0.4.0,Only boost the weights if I will fit again
0.4.0,Only boost positive weights
0.4.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.4.0,Christos Aridas
0.4.0,License: MIT
0.4.0,doctest: +ELLIPSIS
0.4.0,array to know which samples are available to be taken
0.4.0,where the different set will be stored
0.4.0,store the index of the data to under-sample
0.4.0,value which will be picked at each round
0.4.0,extract the data of interest for this round from the
0.4.0,current class
0.4.0,select randomly the desired features
0.4.0,store the set created
0.4.0,fit and predict using cross validation
0.4.0,extract the prediction about the targeted classes only
0.4.0,check the stopping criterion
0.4.0,check that there is enough samples for another round
0.4.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.4.0,License: MIT
0.4.0,Ensemble are a bit specific since they are returning an array of
0.4.0,resampled arrays.
0.4.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.4.0,Christos Aridas
0.4.0,License: MIT
0.4.0,RandomUnderSampler is not supporting sample_weight. We need to pass
0.4.0,None.
0.4.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.4.0,Christos Aridas
0.4.0,License: MIT
0.4.0,RandomUnderSampler is not supporting sample_weight. We need to pass
0.4.0,None.
0.4.0,check that we have an ensemble of samplers and estimators with a
0.4.0,consistent size
0.4.0,each sampler in the ensemble should have different random state
0.4.0,each estimator in the ensemble should have different random state
0.4.0,check the consistency of the feature importances
0.4.0,check the consistency of the prediction outpus
0.4.0,Predictions should be the same when sample_weight are all ones
0.4.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.4.0,Christos Aridas
0.4.0,License: MIT
0.4.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.4.0,Christos Aridas
0.4.0,License: MIT
0.4.0,Check classification for various parameter settings.
0.4.0,Test that bootstrapping samples generate non-perfect base estimators.
0.4.0,"without bootstrap, all trees are perfect on the training set"
0.4.0,disable the resampling by passing an empty dictionary.
0.4.0,"with bootstrap, trees are no longer perfect on the training set"
0.4.0,Test that bootstrapping features may generate duplicate features.
0.4.0,Predict probabilities.
0.4.0,Normal case
0.4.0,"Degenerate case, where some classes are missing"
0.4.0,Check that oob prediction is a good estimation of the generalization
0.4.0,error.
0.4.0,Test with few estimators
0.4.0,Check singleton ensembles.
0.4.0,Test that it gives proper exception on deficient input.
0.4.0,Test n_estimators
0.4.0,Test max_samples
0.4.0,Test max_features
0.4.0,Test support of decision_function
0.4.0,Check that bagging ensembles can be grid-searched.
0.4.0,Transform iris into a binary classification task
0.4.0,Grid search with scoring based on decision_function
0.4.0,Check base_estimator and its default values.
0.4.0,Test if fitting incrementally with warm start gives a forest of the
0.4.0,right size and the same results as a normal fit.
0.4.0,Test if warm start'ed second fit with smaller n_estimators raises error.
0.4.0,Test that nothing happens when fitting without increasing n_estimators
0.4.0,"modify X to nonsense values, this should not change anything"
0.4.0,warm started classifier with 5+5 estimators should be equivalent to
0.4.0,one classifier with 10 estimators
0.4.0,Check using oob_score and warm_start simultaneously fails
0.4.0,"Make sure OOB scores are identical when random_state, estimator, and"
0.4.0,training data are fixed and fitting is done twice
0.4.0,Check that format of estimators_samples_ is correct and that results
0.4.0,generated at fit time can be identically reproduced at a later time
0.4.0,using data saved in object attributes.
0.4.0,remap the y outside of the BalancedBaggingclassifier
0.4.0,"_, y = np.unique(y, return_inverse=True)"
0.4.0,Get relevant attributes
0.4.0,Test for correct formatting
0.4.0,Re-fit single estimator to test for consistent sampling
0.4.0,Make sure validated max_samples and original max_samples are identical
0.4.0,when valid integer max_samples supplied by user
0.4.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.4.0,Christos Aridas
0.4.0,License: MIT
0.4.0,Generate a global dataset to use
0.4.0,Define a sampling_strategy
0.4.0,Define the sampling_strategy parameter
0.4.0,Create the sampling object
0.4.0,Get the different subset
0.4.0,Define the sampling_strategy parameter
0.4.0,Create the sampling object
0.4.0,Get the different subset
0.4.0,Define the sampling_strategy parameter
0.4.0,Create the sampling object
0.4.0,Get the different subset
0.4.0,Check classification for various parameter settings.
0.4.0,test the different prediction function
0.4.0,Check base_estimator and its default values.
0.4.0,Test if fitting incrementally with warm start gives a forest of the
0.4.0,right size and the same results as a normal fit.
0.4.0,Test if warm start'ed second fit with smaller n_estimators raises error.
0.4.0,Test that nothing happens when fitting without increasing n_estimators
0.4.0,"modify X to nonsense values, this should not change anything"
0.4.0,warm started classifier with 5+5 estimators should be equivalent to
0.4.0,one classifier with 10 estimators
0.4.0,Check warning if not enough estimators
0.4.0,Author: Guillaume Lemaitre
0.4.0,License: BSD 3 clause
0.4.0,"The index start at one, then we need to remove one"
0.4.0,to not have issue with the indexing.
0.4.0,go through the list and check if the data are available
0.4.0,Authors: Dayvid Oliveira
0.4.0,Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.4.0,Christos Aridas
0.4.0,License: MIT
0.4.0,restrict ratio to be a dict or a callable
0.4.0,FIXME remove ratio at 0.6
0.4.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.4.0,Christos Aridas
0.4.0,License: MIT
0.4.0,check an error is raised with we don't pass sampling_strategy and ratio
0.4.0,"we are reusing part of utils.check_sampling_strategy, however this is not"
0.4.0,cover in the common tests so we will repeat it here
0.4.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.4.0,Christos Aridas
0.4.0,License: MIT
0.4.0,This is a trick to avoid an error during tests collection with pytest. We
0.4.0,avoid the error when importing the package raise the error at the moment of
0.4.0,creating the instance.
0.4.0,FIXME: Remove in 0.6
0.4.0,shuffle the indices since the sampler are packing them by class
0.3.4,! /usr/bin/env python
0.3.4,"load all vars into globals, otherwise"
0.3.4,the later function call using global vars doesn't work.
0.3.4,"Allow command-lines such as ""python setup.py build install"""
0.3.4,Make sources available using relative paths from this file's directory.
0.3.4,-*- coding: utf-8 -*-
0.3.4,
0.3.4,"imbalanced-learn documentation build configuration file, created by"
0.3.4,sphinx-quickstart on Mon Jan 18 14:44:12 2016.
0.3.4,
0.3.4,This file is execfile()d with the current directory set to its
0.3.4,containing dir.
0.3.4,
0.3.4,Note that not all possible configuration values are present in this
0.3.4,autogenerated file.
0.3.4,
0.3.4,All configuration values have a default; values that are commented out
0.3.4,serve to show the default.
0.3.4,"If extensions (or modules to document with autodoc) are in another directory,"
0.3.4,add these directories to sys.path here. If the directory is relative to the
0.3.4,"documentation root, use os.path.abspath to make it absolute, like shown here."
0.3.4,"sys.path.insert(0, os.path.abspath('.'))"
0.3.4,-- General configuration ---------------------------------------------------
0.3.4,Try to override the matplotlib configuration as early as possible
0.3.4,-- General configuration ------------------------------------------------
0.3.4,If extensions (or modules to document with autodoc) are in another
0.3.4,"directory, add these directories to sys.path here. If the directory"
0.3.4,"is relative to the documentation root, use os.path.abspath to make it"
0.3.4,"absolute, like shown here."
0.3.4,"If your documentation needs a minimal Sphinx version, state it here."
0.3.4,needs_sphinx = '1.0'
0.3.4,"Add any Sphinx extension module names here, as strings. They can be"
0.3.4,extensions coming with Sphinx (named 'sphinx.ext.*') or your custom
0.3.4,ones.
0.3.4,path to your examples scripts
0.3.4,path where to save gallery generated examples
0.3.4,to make references clickable
0.3.4,"Add any paths that contain templates here, relative to this directory."
0.3.4,generate autosummary even if no references
0.3.4,The suffix of source filenames.
0.3.4,The encoding of source files.
0.3.4,source_encoding = 'utf-8-sig'
0.3.4,Generate the plots for the gallery
0.3.4,The master toctree document.
0.3.4,General information about the project.
0.3.4,"The version info for the project you're documenting, acts as replacement for"
0.3.4,"|version| and |release|, also used in various other places throughout the"
0.3.4,built documents.
0.3.4,
0.3.4,The short X.Y version.
0.3.4,"The full version, including alpha/beta/rc tags."
0.3.4,The language for content autogenerated by Sphinx. Refer to documentation
0.3.4,for a list of supported languages.
0.3.4,language = None
0.3.4,"There are two options for replacing |today|: either, you set today to some"
0.3.4,"non-false value, then it is used:"
0.3.4,today = ''
0.3.4,"Else, today_fmt is used as the format for a strftime call."
0.3.4,"today_fmt = '%B %d, %Y'"
0.3.4,"List of patterns, relative to source directory, that match files and"
0.3.4,directories to ignore when looking for source files.
0.3.4,The reST default role (used for this markup: `text`) to use for all
0.3.4,documents.
0.3.4,default_role = None
0.3.4,"If true, '()' will be appended to :func: etc. cross-reference text."
0.3.4,"If true, the current module name will be prepended to all description"
0.3.4,unit titles (such as .. function::).
0.3.4,add_module_names = True
0.3.4,"If true, sectionauthor and moduleauthor directives will be shown in the"
0.3.4,output. They are ignored by default.
0.3.4,show_authors = False
0.3.4,The name of the Pygments (syntax highlighting) style to use.
0.3.4,Custom style
0.3.4,A list of ignored prefixes for module index sorting.
0.3.4,modindex_common_prefix = []
0.3.4,"If true, keep warnings as ""system message"" paragraphs in the built documents."
0.3.4,keep_warnings = False
0.3.4,-- Options for HTML output ----------------------------------------------
0.3.4,The theme to use for HTML and HTML Help pages.  See the documentation for
0.3.4,a list of builtin themes.
0.3.4,Theme options are theme-specific and customize the look and feel of a theme
0.3.4,"further.  For a list of options available for each theme, see the"
0.3.4,documentation.
0.3.4,html_theme_options = {'prev_next_buttons_location': None}
0.3.4,"Add any paths that contain custom themes here, relative to this directory."
0.3.4,"The name for this set of Sphinx documents.  If None, it defaults to"
0.3.4,"""<project> v<release> documentation""."
0.3.4,html_title = None
0.3.4,A shorter title for the navigation bar.  Default is the same as html_title.
0.3.4,html_short_title = None
0.3.4,The name of an image file (relative to this directory) to place at the top
0.3.4,of the sidebar.
0.3.4,html_logo = None
0.3.4,The name of an image file (within the static path) to use as favicon of the
0.3.4,docs.  This file should be a Windows icon file (.ico) being 16x16 or 32x32
0.3.4,pixels large.
0.3.4,html_favicon = None
0.3.4,"Add any paths that contain custom static files (such as style sheets) here,"
0.3.4,"relative to this directory. They are copied after the builtin static files,"
0.3.4,"so a file named ""default.css"" will overwrite the builtin ""default.css""."
0.3.4,Add any extra paths that contain custom files (such as robots.txt or
0.3.4,".htaccess) here, relative to this directory. These files are copied"
0.3.4,directly to the root of the documentation.
0.3.4,html_extra_path = []
0.3.4,"If not '', a 'Last updated on:' timestamp is inserted at every page bottom,"
0.3.4,using the given strftime format.
0.3.4,"html_last_updated_fmt = '%b %d, %Y'"
0.3.4,"If true, SmartyPants will be used to convert quotes and dashes to"
0.3.4,typographically correct entities.
0.3.4,html_use_smartypants = True
0.3.4,"Custom sidebar templates, maps document names to template names."
0.3.4,html_sidebars = {}
0.3.4,"Additional templates that should be rendered to pages, maps page names to"
0.3.4,template names.
0.3.4,html_additional_pages = {}
0.3.4,"If false, no module index is generated."
0.3.4,html_domain_indices = True
0.3.4,"If false, no index is generated."
0.3.4,html_use_index = True
0.3.4,"If true, the index is split into individual pages for each letter."
0.3.4,html_split_index = False
0.3.4,"If true, links to the reST sources are added to the pages."
0.3.4,html_show_sourcelink = True
0.3.4,"If true, ""Created using Sphinx"" is shown in the HTML footer. Default is True."
0.3.4,html_show_sphinx = True
0.3.4,"If true, ""(C) Copyright ..."" is shown in the HTML footer. Default is True."
0.3.4,html_show_copyright = True
0.3.4,"If true, an OpenSearch description file will be output, and all pages will"
0.3.4,contain a <link> tag referring to it.  The value of this option must be the
0.3.4,base URL from which the finished HTML is served.
0.3.4,html_use_opensearch = ''
0.3.4,"This is the file name suffix for HTML files (e.g. "".xhtml"")."
0.3.4,html_file_suffix = None
0.3.4,Output file base name for HTML help builder.
0.3.4,-- Options for LaTeX output ---------------------------------------------
0.3.4,The paper size ('letterpaper' or 'a4paper').
0.3.4,"'papersize': 'letterpaper',"
0.3.4,"The font size ('10pt', '11pt' or '12pt')."
0.3.4,"'pointsize': '10pt',"
0.3.4,Additional stuff for the LaTeX preamble.
0.3.4,"'preamble': '',"
0.3.4,Grouping the document tree into LaTeX files. List of tuples
0.3.4,"(source start file, target name, title,"
0.3.4,"author, documentclass [howto, manual, or own class])."
0.3.4,The name of an image file (relative to this directory) to place at the top of
0.3.4,the title page.
0.3.4,latex_logo = None
0.3.4,"For ""manual"" documents, if this is true, then toplevel headings are parts,"
0.3.4,not chapters.
0.3.4,latex_use_parts = False
0.3.4,"If true, show page references after internal links."
0.3.4,latex_show_pagerefs = False
0.3.4,"If true, show URL addresses after external links."
0.3.4,latex_show_urls = False
0.3.4,Documents to append as an appendix to all manuals.
0.3.4,latex_appendices = []
0.3.4,"If false, no module index is generated."
0.3.4,latex_domain_indices = True
0.3.4,-- Options for manual page output ---------------------------------------
0.3.4,One entry per manual page. List of tuples
0.3.4,"(source start file, name, description, authors, manual section)."
0.3.4,"If true, show URL addresses after external links."
0.3.4,man_show_urls = False
0.3.4,-- Options for Texinfo output -------------------------------------------
0.3.4,Grouping the document tree into Texinfo files. List of tuples
0.3.4,"(source start file, target name, title, author,"
0.3.4,"dir menu entry, description, category)"
0.3.4,"generate empty examples files, so that we don't get"
0.3.4,inclusion errors if there are no examples for a class / module
0.3.4,touch file
0.3.4,Config for sphinx_issues
0.3.4,Documents to append as an appendix to all manuals.
0.3.4,texinfo_appendices = []
0.3.4,"If false, no module index is generated."
0.3.4,texinfo_domain_indices = True
0.3.4,"How to display URL addresses: 'footnote', 'no', or 'inline'."
0.3.4,texinfo_show_urls = 'footnote'
0.3.4,"If true, do not generate a @detailmenu in the ""Top"" node's menu."
0.3.4,texinfo_no_detailmenu = False
0.3.4,Example configuration for intersphinx: refer to the Python standard library.
0.3.4,The following is used by sphinx.ext.linkcode to provide links to github
0.3.4,get the styles from the current theme
0.3.4,create and add the button to all the code blocks that contain >>>
0.3.4,tracebacks (.gt) contain bare text elements that need to be
0.3.4,wrapped in a span to work with .nextUntil() (see later)
0.3.4,define the behavior of the button when it's clicked
0.3.4,hide the code output
0.3.4,show the code output
0.3.4,-*- coding: utf-8 -*-
0.3.4,Format template for issues URI
0.3.4,e.g. 'https://github.com/sloria/marshmallow/issues/{issue}
0.3.4,"Shortcut for Github, e.g. 'sloria/marshmallow'"
0.3.4,Format template for user profile URI
0.3.4,e.g. 'https://github.com/{user}'
0.3.4,Python 2 only
0.3.4,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.4,License: MIT
0.3.4,##############################################################################
0.3.4,Creation of an imbalanced data set from a balanced data set
0.3.4,##############################################################################
0.3.4,##############################################################################
0.3.4,We will show how to use the parameter ``ratio`` when dealing with the
0.3.4,"``make_imbalance`` function. For this function, this parameter accepts both"
0.3.4,"dictionary and callable. When using a dictionary, each key will correspond to"
0.3.4,the class of interest and the corresponding value will be the number of
0.3.4,samples desired in this class.
0.3.4,##############################################################################
0.3.4,You might required more flexibility and require your own heuristic to
0.3.4,determine the number of samples by class and you can define your own callable
0.3.4,as follow. In this case we will define a function which will use a float
0.3.4,multiplier to define the number of samples per class.
0.3.4,##############################################################################
0.3.4,Using ``ratio`` in resampling algorithm
0.3.4,##############################################################################
0.3.4,##############################################################################
0.3.4,"In all sampling algorithms, ``ratio`` can be used as illustrated earlier. In"
0.3.4,"addition, some predefined functions are available and can be executed using a"
0.3.4,``str`` with the following choices: (i) ``'minority'``: resample the minority
0.3.4,"class; (ii) ``'majority'``: resample the majority class, (iii) ``'not"
0.3.4,"minority'``: resample all classes apart of the minority class, (iv)"
0.3.4,"``'all'``: resample all classes, and (v) ``'auto'``: correspond to 'all' with"
0.3.4,for over-sampling methods and 'not minority' for under-sampling methods. The
0.3.4,classes targeted will be over-sampled or under-sampled to achieve an equal
0.3.4,number of sample with the majority or minority class.
0.3.4,##############################################################################
0.3.4,"However, you can use the dictionary or the callable options as previously"
0.3.4,mentioned.
0.3.4,Authors: Fernando Nogueira
0.3.4,Christos Aridas
0.3.4,Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.4,License: MIT
0.3.4,Generate the dataset
0.3.4,Instanciate a PCA object for the sake of easy visualisation
0.3.4,Fit and transform x to visualise inside a 2D feature space
0.3.4,Apply regular SMOTE
0.3.4,"Two subplots, unpack the axes array immediately"
0.3.4,Remove axis for second plot
0.3.4,Authors: Christos Aridas
0.3.4,Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.4,License: MIT
0.3.4,Generate the dataset
0.3.4,Instanciate a PCA object for the sake of easy visualisation
0.3.4,Fit and transform x to visualise inside a 2D feature space
0.3.4,Apply the random over-sampling
0.3.4,"Two subplots, unpack the axes array immediately"
0.3.4,make nice plotting
0.3.4,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.4,License: MIT
0.3.4,generate some data points
0.3.4,plot the majority and minority samples
0.3.4,draw the circle in which the new sample will generated
0.3.4,plot the line on which the sample will be generated
0.3.4,create and plot the new sample
0.3.4,make the plot nicer with legend and label
0.3.4,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.4,License: MIT
0.3.4,##############################################################################
0.3.4,The following function will be used to create toy dataset. It using the
0.3.4,``make_classification`` from scikit-learn but fixing some parameters.
0.3.4,##############################################################################
0.3.4,The following function will be used to plot the sample space after resampling
0.3.4,to illustrate the characterisitic of an algorithm.
0.3.4,make nice plotting
0.3.4,##############################################################################
0.3.4,The following function will be used to plot the decision function of a
0.3.4,classifier given some data.
0.3.4,##############################################################################
0.3.4,Illustration of the influence of the balancing ratio
0.3.4,##############################################################################
0.3.4,##############################################################################
0.3.4,We will first illustrate the influence of the balancing ratio on some toy
0.3.4,data using a linear SVM classifier. Greater is the difference between the
0.3.4,"number of samples in each class, poorer are the classfication results."
0.3.4,##############################################################################
0.3.4,Random over-sampling to balance the data set
0.3.4,##############################################################################
0.3.4,##############################################################################
0.3.4,Random over-sampling can be used to repeat some samples and balance the
0.3.4,number of samples between the dataset. It can be seen that with this trivial
0.3.4,approach the boundary decision is already less biaised toward the majority
0.3.4,class.
0.3.4,##############################################################################
0.3.4,More advanced over-sampling using ADASYN and SMOTE
0.3.4,##############################################################################
0.3.4,##############################################################################
0.3.4,"Instead of repeating the same samples when over-sampling, we can use some"
0.3.4,specific heuristic instead. ADASYN and SMOTE can be used in this case.
0.3.4,Make an identity sampler
0.3.4,##############################################################################
0.3.4,The following plot illustrate the difference between ADASYN and SMOTE. ADASYN
0.3.4,will focus on the samples which are difficult to classify with a
0.3.4,nearest-neighbors rule while regular SMOTE will not make any distinction.
0.3.4,"Therefore, the decision function depending of the algorithm."
0.3.4,##############################################################################
0.3.4,"Due to those sampling particularities, it can give rise to some specific"
0.3.4,issues as illustrated below.
0.3.4,##############################################################################
0.3.4,SMOTE proposes several variants by identifying specific samples to consider
0.3.4,during the resampling. The borderline version will detect which point to
0.3.4,select which are in the border between two classes. The SVM version will use
0.3.4,the support vectors found using an SVM algorithm to create new samples.
0.3.4,Authors: Christos Aridas
0.3.4,Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.4,License: MIT
0.3.4,Generate the dataset
0.3.4,Instanciate a PCA object for the sake of easy visualisation
0.3.4,Fit and transform x to visualise inside a 2D feature space
0.3.4,Apply the random over-sampling
0.3.4,"Two subplots, unpack the axes array immediately"
0.3.4,make nice plotting
0.3.4,Authors: Christos Aridas
0.3.4,Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.4,License: MIT
0.3.4,Generate the dataset
0.3.4,make nice plotting
0.3.4,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.4,License: MIT
0.3.4,Generate a dataset
0.3.4,Split the data
0.3.4,Train the classifier with balancing
0.3.4,Test the classifier and get the prediction
0.3.4,Show the classification report
0.3.4,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.4,License: MIT
0.3.4,Generate a dataset
0.3.4,Split the data
0.3.4,Train the classifier with balancing
0.3.4,Test the classifier and get the prediction
0.3.4,##############################################################################
0.3.4,The geometric mean corresponds to the square root of the product of the
0.3.4,sensitivity and specificity. Combining the two metrics should account for
0.3.4,the balancing of the dataset.
0.3.4,##############################################################################
0.3.4,The index balanced accuracy can transform any metric to be used in
0.3.4,imbalanced learning problems.
0.3.4,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.4,License: MIT
0.3.4,##############################################################################
0.3.4,The following function will be used to create toy dataset. It using the
0.3.4,``make_classification`` from scikit-learn but fixing some parameters.
0.3.4,##############################################################################
0.3.4,The following function will be used to plot the sample space after resampling
0.3.4,to illustrate the characteristic of an algorithm.
0.3.4,make nice plotting
0.3.4,##############################################################################
0.3.4,The following function will be used to plot the decision function of a
0.3.4,classifier given some data.
0.3.4,##############################################################################
0.3.4,"``SMOTE`` allows to generate samples. However, this method of over-sampling"
0.3.4,"does not have any knowledge regarding the underlying distribution. Therefore,"
0.3.4,"some noisy samples can be generated, e.g. when the different classes cannot"
0.3.4,"be well separated. Hence, it can be beneficial to apply an under-sampling"
0.3.4,algorithm to clean the noisy samples. Two methods are usually used in the
0.3.4,literature: (i) Tomek's link and (ii) edited nearest neighbours cleaning
0.3.4,methods. Imbalanced-learn provides two ready-to-use samplers ``SMOTETomek``
0.3.4,"and ``SMOTEENN``. In general, ``SMOTEENN`` cleans more noisy data than"
0.3.4,``SMOTETomek``.
0.3.4,Authors: Christos Aridas
0.3.4,Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.4,License: MIT
0.3.4,Generate the dataset
0.3.4,Instanciate a PCA object for the sake of easy visualisation
0.3.4,Fit and transform x to visualise inside a 2D feature space
0.3.4,Apply SMOTE + ENN
0.3.4,"Two subplots, unpack the axes array immediately"
0.3.4,make nice plotting
0.3.4,Authors: Christos Aridas
0.3.4,Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.4,License: MIT
0.3.4,Generate the dataset
0.3.4,Instanciate a PCA object for the sake of easy visualisation
0.3.4,Fit and transform x to visualise inside a 2D feature space
0.3.4,Apply SMOTE + Tomek links
0.3.4,"Two subplots, unpack the axes array immediately"
0.3.4,make nice plotting
0.3.4,Authors: Christos Aridas
0.3.4,Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.4,License: MIT
0.3.4,Generate the dataset
0.3.4,Instanciate a PCA object for the sake of easy visualisation
0.3.4,Fit and transform x to visualise inside a 2D feature space
0.3.4,Apply Balance Cascade method
0.3.4,"Two subplots, unpack the axes array immediately"
0.3.4,make nice plotting
0.3.4,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.4,License: MIT
0.3.4,##############################################################################
0.3.4,Turning the balanced bagging classifier into a balanced random forest
0.3.4,##############################################################################
0.3.4,It is possible to turn the ``BalancedBaggingClassifier`` into a balanced
0.3.4,random forest by using a ``DecisionTreeClassifier`` with
0.3.4,``max_features='auto'``. We illustrate such changes below.
0.3.4,Authors: Christos Aridas
0.3.4,Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.4,License: MIT
0.3.4,Generate the dataset
0.3.4,Instanciate a PCA object for the sake of easy visualisation
0.3.4,Fit and transform x to visualise inside a 2D feature space
0.3.4,Apply Easy Ensemble
0.3.4,"Two subplots, unpack the axes array immediately"
0.3.4,make nice plotting
0.3.4,Authors: Andreas Mueller
0.3.4,Christos Aridas
0.3.4,Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.4,License: MIT
0.3.4,remove Tomek links
0.3.4,make nice plotting
0.3.4,Authors: Dayvid Oliveira
0.3.4,Christos Aridas
0.3.4,Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.4,License: MIT
0.3.4,Generate the dataset
0.3.4,Instanciate a PCA object for the sake of easy visualisation
0.3.4,Fit and transform x to visualise inside a 2D feature space
0.3.4,"Three subplots, unpack the axes array immediately"
0.3.4,Apply the ENN
0.3.4,Apply the RENN
0.3.4,Apply the AllKNN
0.3.4,Authors: Christos Aridas
0.3.4,Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.4,License: MIT
0.3.4,Generate the dataset
0.3.4,Instanciate a PCA object for the sake of easy visualisation
0.3.4,Fit and transform x to visualise inside a 2D feature space
0.3.4,Apply Nearmiss
0.3.4,"Two subplots, unpack the axes array immediately"
0.3.4,plot the missing samples
0.3.4,Authors: Christos Aridas
0.3.4,Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.4,License: MIT
0.3.4,Generate the dataset
0.3.4,Instanciate a PCA object for the sake of easy visualisation
0.3.4,Fit and transform x to visualise inside a 2D feature space
0.3.4,Apply One-Sided Selection
0.3.4,make nice plotting
0.3.4,Authors: Christos Aridas
0.3.4,Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.4,License: MIT
0.3.4,Generate the dataset
0.3.4,Instanciate a PCA object for the sake of easy visualisation
0.3.4,Fit and transform x to visualise inside a 2D feature space
0.3.4,Apply neighbourhood cleaning rule
0.3.4,make nice plotting
0.3.4,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.4,License: MIT
0.3.4,##############################################################################
0.3.4,The following function will be used to create toy dataset. It using the
0.3.4,``make_classification`` from scikit-learn but fixing some parameters.
0.3.4,##############################################################################
0.3.4,The following function will be used to plot the sample space after resampling
0.3.4,to illustrate the characteristic of an algorithm.
0.3.4,make nice plotting
0.3.4,##############################################################################
0.3.4,The following function will be used to plot the decision function of a
0.3.4,classifier given some data.
0.3.4,##############################################################################
0.3.4,Prototype generation: under-sampling by generating new samples
0.3.4,##############################################################################
0.3.4,##############################################################################
0.3.4,``ClusterCentroids`` under-samples by replacing the original samples by the
0.3.4,centroids of the cluster found.
0.3.4,##############################################################################
0.3.4,Prototype selection: under-sampling by selecting existing samples
0.3.4,##############################################################################
0.3.4,##############################################################################
0.3.4,The algorithm performing prototype selection can be subdivided into two
0.3.4,groups: (i) the controlled under-sampling methods and (ii) the cleaning
0.3.4,under-sampling methods.
0.3.4,##############################################################################
0.3.4,"With the controlled under-sampling methods, the number of samples to be"
0.3.4,selected can be specified. ``RandomUnderSampler`` is the most naive way of
0.3.4,performing such selection by randomly selecting a given number of samples by
0.3.4,the targetted class.
0.3.4,##############################################################################
0.3.4,``NearMiss`` algorithms implement some heuristic rules in order to select
0.3.4,samples. NearMiss-1 selects samples from the majority class for which the
0.3.4,average distance of the :math:`k`` nearest samples of the minority class is
0.3.4,the smallest. NearMiss-2 selects the samples from the majority class for
0.3.4,which the average distance to the farthest samples of the negative class is
0.3.4,"the smallest. NearMiss-3 is a 2-step algorithm: first, for each minority"
0.3.4,"sample, their ::math:`m` nearest-neighbors will be kept; then, the majority"
0.3.4,samples selected are the on for which the average distance to the :math:`k`
0.3.4,nearest neighbors is the largest.
0.3.4,##############################################################################
0.3.4,``EditedNearestNeighbours`` removes samples of the majority class for which
0.3.4,their class differ from the one of their nearest-neighbors. This sieve can be
0.3.4,repeated which is the principle of the
0.3.4,``RepeatedEditedNearestNeighbours``. ``AllKNN`` is slightly different from
0.3.4,the ``RepeatedEditedNearestNeighbours`` by changing the :math:`k` parameter
0.3.4,"of the internal nearest neighors algorithm, increasing it at each iteration."
0.3.4,##############################################################################
0.3.4,``CondensedNearestNeighbour`` makes use of a 1-NN to iteratively decide if a
0.3.4,sample should be kept in a dataset or not. The issue is that
0.3.4,``CondensedNearestNeighbour`` is sensitive to noise by preserving the noisy
0.3.4,samples. ``OneSidedSelection`` also used the 1-NN and use ``TomekLinks`` to
0.3.4,remove the samples considered noisy. The ``NeighbourhoodCleaningRule`` use a
0.3.4,"``EditedNearestNeighbours`` to remove some sample. Additionally, they use a 3"
0.3.4,nearest-neighbors to remove samples which do not agree with this rule.
0.3.4,##############################################################################
0.3.4,``InstanceHardnessThreshold`` uses the prediction of classifier to exclude
0.3.4,samples. All samples which are classified with a low probability will be
0.3.4,removed.
0.3.4,Authors: Christos Aridas
0.3.4,Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.4,License: MIT
0.3.4,Generate the dataset
0.3.4,Instanciate a PCA object for the sake of easy visualisation
0.3.4,Fit and transform x to visualise inside a 2D feature space
0.3.4,Apply Condensed Nearest Neighbours
0.3.4,make nice plotting
0.3.4,Authors: Fernando Nogueira
0.3.4,Christos Aridas
0.3.4,Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.4,License: MIT
0.3.4,Generate the dataset
0.3.4,Instanciate a PCA object for the sake of easy visualisation
0.3.4,Fit and transform x to visualise inside a 2D feature space
0.3.4,Apply Cluster Centroids
0.3.4,Use hard voting instead of soft voting
0.3.4,"Two subplots, unpack the axes array immediately"
0.3.4,make nice plotting
0.3.4,##############################################################################
0.3.4,This function allows to make nice plotting
0.3.4,##############################################################################
0.3.4,Generate some data with one Tomek link
0.3.4,minority class
0.3.4,majority class
0.3.4,##############################################################################
0.3.4,"In the figure above, the samples highlighted in green form a Tomek link since"
0.3.4,they are of different classes and are nearest neighbours of each other.
0.3.4,highlight the samples of interest
0.3.4,##############################################################################
0.3.4,We can run the ``TomekLinks`` sampling to remove the corresponding
0.3.4,samples. If ``ratio='auto'`` only the sample from the majority class will be
0.3.4,removed. If ``ratio='all'`` both samples will be removed.
0.3.4,highlight the samples of interest
0.3.4,Authors: Christos Aridas
0.3.4,Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.4,License: MIT
0.3.4,Generate the dataset
0.3.4,Instanciate a PCA object for the sake of easy visualisation
0.3.4,Fit and transform x to visualise inside a 2D feature space
0.3.4,Apply the random under-sampling
0.3.4,make nice plotting
0.3.4,Authors: Dayvid Oliveira
0.3.4,Christos Aridas
0.3.4,Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.4,License: MIT
0.3.4,Generate the dataset
0.3.4,"Two subplots, unpack the axes array immediately"
0.3.4,plot samples which have been removed
0.3.4,##############################################################################
0.3.4,This function allows to make nice plotting
0.3.4,##############################################################################
0.3.4,We can start by generating some data to later illustrate the principle of
0.3.4,each NearMiss heuritic rules.
0.3.4,minority class
0.3.4,majority class
0.3.4,##############################################################################
0.3.4,NearMiss-1
0.3.4,##############################################################################
0.3.4,##############################################################################
0.3.4,NearMiss-1 selects samples from the majority class for which the average
0.3.4,distance to some nearest neighbours is the smallest. In the following
0.3.4,"example, we use a 3-NN to compute the average distance on 2 specific samples"
0.3.4,"of the majority class. Therefore, in this case the point linked by the"
0.3.4,green-dashed line will be selected since the average distance is smaller.
0.3.4,##############################################################################
0.3.4,NearMiss-2
0.3.4,##############################################################################
0.3.4,##############################################################################
0.3.4,NearMiss-2 selects samples from the majority class for which the average
0.3.4,distance to the farthest neighbors is the smallest. With the same
0.3.4,"configuration as previously presented, the sample linked to the green-dashed"
0.3.4,line will be selected since its distance the 3 farthest neighbors is the
0.3.4,smallest.
0.3.4,##############################################################################
0.3.4,NearMiss-3
0.3.4,##############################################################################
0.3.4,##############################################################################
0.3.4,"NearMiss-3 can be divided into 2 steps. First, a nearest-neighbors is used to"
0.3.4,short-list samples from the majority class (i.e. correspond to the
0.3.4,"highlighted samples in the following plot). Then, the sample with the largest"
0.3.4,average distance to the *k* nearest-neighbors are selected.
0.3.4,select only the majority point of interest
0.3.4,Authors: Christos Aridas
0.3.4,Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.4,License: MIT
0.3.4,Generate the dataset
0.3.4,Instanciate a PCA object for the sake of easy visualisation
0.3.4,Create the samplers
0.3.4,Create the classifier
0.3.4,Make the splits
0.3.4,Add one transformers and two samplers in the pipeline object
0.3.4,Authors: Christos Aridas
0.3.4,Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.4,License: MIT
0.3.4,Load the dataset
0.3.4,make nice plotting
0.3.4,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.4,License: MIT
0.3.4,Create a folder to fetch the dataset
0.3.4,Create a pipeline
0.3.4,Classify and report the results
0.3.4,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.4,License: MIT
0.3.4,##############################################################################
0.3.4,Setting the data set
0.3.4,##############################################################################
0.3.4,##############################################################################
0.3.4,We use a part of the 20 newsgroups data set by loading 4 topics. Using the
0.3.4,"scikit-learn loader, the data are split into a training and a testing set."
0.3.4,
0.3.4,Note the class \#3 is the minority class and has almost twice less samples
0.3.4,than the majority class.
0.3.4,##############################################################################
0.3.4,The usual scikit-learn pipeline
0.3.4,##############################################################################
0.3.4,##############################################################################
0.3.4,You might usually use scikit-learn pipeline by combining the TF-IDF
0.3.4,vectorizer to feed a multinomial naive bayes classifier. A classification
0.3.4,report summarized the results on the testing set.
0.3.4,
0.3.4,"As expected, the recall of the class \#3 is low mainly due to the class"
0.3.4,imbalanced.
0.3.4,##############################################################################
0.3.4,Balancing the class before classification
0.3.4,##############################################################################
0.3.4,##############################################################################
0.3.4,"To improve the prediction of the class \#3, it could be interesting to apply"
0.3.4,"a balancing before to train the naive bayes classifier. Therefore, we will"
0.3.4,use a ``RandomUnderSampler`` to equalize the number of samples in all the
0.3.4,classes before the training.
0.3.4,
0.3.4,It is also important to note that we are using the ``make_pipeline`` function
0.3.4,implemented in imbalanced-learn to properly handle the samplers.
0.3.4,##############################################################################
0.3.4,"Although the results are almost identical, it can be seen that the resampling"
0.3.4,allowed to correct the poor recall of the class \#3 at the cost of reducing
0.3.4,"the other metrics for the other classes. However, the overall results are"
0.3.4,slightly better.
0.3.4,Authors: Dayvid Oliveira
0.3.4,Christos Aridas
0.3.4,Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.4,License: MIT
0.3.4,Generate the dataset
0.3.4,"Two subplots, unpack the axes array immediately"
0.3.4,Based on NiLearn package
0.3.4,License: simplified BSD
0.3.4,"PEP0440 compatible formatted version, see:"
0.3.4,https://www.python.org/dev/peps/pep-0440/
0.3.4,
0.3.4,Generic release markers:
0.3.4,X.Y
0.3.4,X.Y.Z # For bugfix releases
0.3.4,
0.3.4,Admissible pre-release markers:
0.3.4,X.YaN # Alpha release
0.3.4,X.YbN # Beta release
0.3.4,X.YrcN # Release Candidate
0.3.4,X.Y # Final release
0.3.4,
0.3.4,Dev branch marker is: 'X.Y.dev' or 'X.Y.devN' where N is an integer.
0.3.4,'X.Y.dev0' is the canonical version of 'X.Y.dev'
0.3.4,
0.3.4,"This is a tuple to preserve order, so that dependencies are checked"
0.3.4,in some meaningful order (more => less 'core').  We avoid using
0.3.4,collections.OrderedDict to preserve Python 2.6 compatibility.
0.3.4,Avoid choking on modules with no __version__ attribute
0.3.4,Skip check only when installing and it's a module that
0.3.4,will be auto-installed.
0.3.4,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.4,License: MIT
0.3.4,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.4,Christos Aridas
0.3.4,License: MIT
0.3.4,Check the consistency of X and y
0.3.4,self.sampling_type is already checked in check_ratio
0.3.4,Adapted from scikit-learn
0.3.4,Author: Edouard Duchesnay
0.3.4,Gael Varoquaux
0.3.4,Virgile Fritsch
0.3.4,Alexandre Gramfort
0.3.4,Lars Buitinck
0.3.4,Christos Aridas
0.3.4,Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.4,License: BSD
0.3.4,BaseEstimator interface
0.3.4,shallow copy of steps
0.3.4,validate names
0.3.4,validate estimators
0.3.4,We allow last estimator to be None as an identity transformation
0.3.4,Estimator interface
0.3.4,Setup the memory
0.3.4,we do not clone when caching is disabled to preserve
0.3.4,backward compatibility
0.3.4,Fit or load from cache the current transfomer
0.3.4,Replace the transformer of the step with the fitted
0.3.4,transformer. This is necessary when loading the transformer
0.3.4,from the cache.
0.3.4,XXX: Calling sample in pipeline it means that the
0.3.4,last estimator is a sampler. Samplers don't carry
0.3.4,"the sampled data. So, call 'fit_sample' in all intermediate"
0.3.4,steps to get the sampled data for the last estimator.
0.3.4,"_final_estimator is None or has transform, otherwise attribute error"
0.3.4,raise AttributeError if necessary for hasattr behaviour
0.3.4,"if we have a weight for this transformer, multiply output"
0.3.4,Boolean controlling whether the joblib caches should be
0.3.4,"flushed if the version of certain modules changes (eg nibabel, as it"
0.3.4,does not respect the backward compatibility in some of its internal
0.3.4,structures
0.3.4,This  is used in nilearn._utils.cache_mixin
0.3.4,list all submodules available in imblearn and version
0.3.4,coding: utf-8
0.3.4,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.4,Dariusz Brzezinski
0.3.4,License: MIT
0.3.4,Only negative labels
0.3.4,"Calculate tp_sum, pred_sum, true_sum ###"
0.3.4,labels are now from 0 to len(labels) - 1 -> use bincount
0.3.4,Pathological case
0.3.4,Compute the true negative
0.3.4,Retain only selected labels
0.3.4,"Finally, we have all our sufficient statistics. Divide! #"
0.3.4,"Divide, and on zero-division, set scores to 0 and warn:"
0.3.4,"Oddly, we may get an ""invalid"" rather than a ""divide"" error"
0.3.4,here.
0.3.4,Average the results
0.3.4,labels are now from 0 to len(labels) - 1 -> use bincount
0.3.4,Pathological case
0.3.4,Retain only selected labels
0.3.4,old version of scipy return MaskedConstant instead of 0.0
0.3.4,Create the list of tags
0.3.4,check that the scoring function does not need a score
0.3.4,and only a prediction
0.3.4,Compute the score from the scoring function
0.3.4,Square if desired
0.3.4,Get the signature of the sens/spec function
0.3.4,We need to extract from kwargs only the one needed by the
0.3.4,specificity and specificity
0.3.4,Make the intersection between the parameters
0.3.4,Create a sub dictionary
0.3.4,Check if the metric is the geometric mean
0.3.4,We do not support multilabel so the only average supported
0.3.4,is binary
0.3.4,Create the list of parameters through signature binding
0.3.4,Call the sens/spec function
0.3.4,Compute the dominance
0.3.4,Compute the different metrics
0.3.4,Precision/recall/f1
0.3.4,Specificity
0.3.4,Geometric mean
0.3.4,Index balanced accuracy
0.3.4,compute averages
0.3.4,coding: utf-8
0.3.4,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.4,Christos Aridas
0.3.4,License: MIT
0.3.4,##############################################################################
0.3.4,Utilities for testing
0.3.4,import some data to play with
0.3.4,restrict to a binary classification task
0.3.4,add noisy features to make the problem harder and avoid perfect results
0.3.4,"run classifier, get class probabilities and label predictions"
0.3.4,only interested in probabilities of the positive case
0.3.4,XXX: do we really want a special API for the binary case?
0.3.4,##############################################################################
0.3.4,Tests
0.3.4,detailed measures for each class
0.3.4,individual scoring function that can be used for grid search: in the
0.3.4,binary class case the score is the value of the measure for the positive
0.3.4,class (e.g. label == 1). This is deprecated for average != 'binary'.
0.3.4,Such a case may occur with non-stratified cross-validation
0.3.4,No average: zeros in array
0.3.4,Macro average is changed
0.3.4,Check for micro
0.3.4,Check for weighted
0.3.4,ensure the above were meaningful tests:
0.3.4,Bad pos_label
0.3.4,Bad average option
0.3.4,but average != 'binary'; even if data is binary
0.3.4,compute the geometric mean for the binary problem
0.3.4,Compute the geometric mean for each of the classes
0.3.4,average tests
0.3.4,print classification report with class names
0.3.4,print classification report with label detection
0.3.4,print classification report with class names
0.3.4,print classification report with label detection
0.3.4,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.4,Christos Aridas
0.3.4,License: MIT
0.3.4,Get the version
0.3.4,sensitivity scorer
0.3.4,specificity scorer
0.3.4,geometric_mean scorer
0.3.4,make a iba metric before a scorer
0.3.4,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.4,Christos Aridas
0.3.4,License: MIT
0.3.4,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.4,Christos Aridas
0.3.4,License: MIT
0.3.4,FIXME: Deprecated in 0.2. To be removed in 0.4.
0.3.4,The ratio is computed using a one-vs-rest manner. Using majority
0.3.4,in multi-class would lead to slightly different results at the
0.3.4,cost of introducing a new parameter.
0.3.4,the nearest neighbors need to be fitted only on the current class
0.3.4,to find the class NN to generate new samples
0.3.4,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.4,Christos Aridas
0.3.4,License: MIT
0.3.4,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.4,Fernando Nogueira
0.3.4,Christos Aridas
0.3.4,License: MIT
0.3.4,Samples are in danger for m/2 <= m' < m
0.3.4,Samples are noise for m = m'
0.3.4,"FIXME Deprecated in 0.2, to be removed in 0.4"
0.3.4,divergence between borderline-1 and borderline-2
0.3.4,Create synthetic samples for borderline points.
0.3.4,only minority
0.3.4,we use a one-vs-rest policy to handle the multiclass in which
0.3.4,new samples will be created considering not only the majority
0.3.4,class but all over classes.
0.3.4,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.4,Christos Aridas
0.3.4,License: MIT
0.3.4,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.4,Christos Aridas
0.3.4,License: MIT
0.3.4,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.4,Christos Aridas
0.3.4,License: MIT
0.3.4,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.4,License: MIT
0.3.4,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.4,Fernando Nogueira
0.3.4,Christos Aridas
0.3.4,License: MIT
0.3.4,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.4,Christos Aridas
0.3.4,License: MIT
0.3.4,FIXME: Deprecated in 0.2. To be removed in 0.4.
0.3.4,select a sample from the current class
0.3.4,create the set composed of all minority samples and one
0.3.4,sample from the current class.
0.3.4,create the set S with removing the seed from S
0.3.4,since that it will be added anyway
0.3.4,apply Tomek cleaning
0.3.4,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.4,Christos Aridas
0.3.4,License: MIT
0.3.4,Compute the distance considering the farthest neighbour
0.3.4,Sort the list of distance and get the index
0.3.4,Throw a warning to tell the user that we did not have enough samples
0.3.4,to select and that we just select everything
0.3.4,Select the desired number of samples
0.3.4,FIXME: Deprecated in 0.2. To be removed in 0.4.
0.3.4,idx_tmp is relative to the feature selected in the
0.3.4,previous step and we need to find the indirection
0.3.4,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.4,Christos Aridas
0.3.4,License: MIT
0.3.4,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.4,Fernando Nogueira
0.3.4,Christos Aridas
0.3.4,License: MIT
0.3.4,find which class to not consider
0.3.4,there is a Tomek link between two samples if they are both nearest
0.3.4,neighbors of each others.
0.3.4,Find the nearest neighbour of every point
0.3.4,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.4,Christos Aridas
0.3.4,License: MIT
0.3.4,FIXME: Deprecated in 0.2. To be removed in 0.4
0.3.4,Randomly get one sample from the majority class
0.3.4,Generate the index to select
0.3.4,Create the set C - One majority samples and all minority
0.3.4,Create the set S - all majority samples
0.3.4,fit knn on C
0.3.4,Check each sample in S if we keep it or drop it
0.3.4,Do not select sample which are already well classified
0.3.4,Classify on S
0.3.4,If the prediction do not agree with the true label
0.3.4,append it in C_x
0.3.4,Keep the index for later
0.3.4,Update C
0.3.4,fit a knn on C
0.3.4,This experimental to speed up the search
0.3.4,Classify all the element in S and avoid to test the
0.3.4,well classified elements
0.3.4,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.4,Christos Aridas
0.3.4,License: MIT
0.3.4,FIXME: Deprecated from 0.2. To be removed in 0.4.
0.3.4,clean the neighborhood
0.3.4,compute which classes to consider for cleaning for the A2 group
0.3.4,compute a2 group
0.3.4,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.4,Dayvid Oliveira
0.3.4,Christos Aridas
0.3.4,License: MIT
0.3.4,FIXME: Deprecated in 0.2. To be removed in 0.4
0.3.4,Check the stopping criterion
0.3.4,1. If there is no changes for the vector y
0.3.4,2. If the number of samples in the other class become inferior to
0.3.4,the number of samples in the majority class
0.3.4,3. If one of the class is disappearing
0.3.4,Case 1
0.3.4,Case 2
0.3.4,Case 3
0.3.4,Check the stopping criterion
0.3.4,1. If the number of samples in the other class become inferior to
0.3.4,the number of samples in the majority class
0.3.4,2. If one of the class is disappearing
0.3.4,Case 1
0.3.4,overwrite b_min_bec_maj
0.3.4,Case 2
0.3.4,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.4,Dayvid Oliveira
0.3.4,Christos Aridas
0.3.4,License: MIT
0.3.4,To be removed in 0.4
0.3.4,Select the appropriate classifier
0.3.4,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.4,Christos Aridas
0.3.4,License: MIT
0.3.4,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.4,Christos Aridas
0.3.4,License: MIT
0.3.4,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.4,Christos Aridas
0.3.4,License: MIT
0.3.4,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.4,Christos Aridas
0.3.4,License: MIT
0.3.4,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.4,Christos Aridas
0.3.4,License: MIT
0.3.4,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.4,Christos Aridas
0.3.4,License: MIT
0.3.4,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.4,Christos Aridas
0.3.4,License: MIT
0.3.4,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.4,Christos Aridas
0.3.4,License: MIT
0.3.4,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.4,Christos Aridas
0.3.4,License: MIT
0.3.4,FIXME remove at the end of the deprecation 0.4
0.3.4,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.4,Christos Aridas
0.3.4,License: MIT
0.3.4,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.4,Christos Aridas
0.3.4,License: MIT
0.3.4,test that all_estimators doesn't find abstract classes.
0.3.4,some can just not be sensibly default constructed
0.3.4,input validation etc for non-meta estimators
0.3.4,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.4,Christos Aridas
0.3.4,License: MIT
0.3.4,store timestamp to figure out whether the result of 'fit' has been
0.3.4,cached or not
0.3.4,store timestamp to figure out whether the result of 'fit' has been
0.3.4,cached or not
0.3.4,Test the various init parameters of the pipeline.
0.3.4,Check that we can't instantiate pipelines with objects without fit
0.3.4,method
0.3.4,Smoke test with only an estimator
0.3.4,Check that params are set
0.3.4,Smoke test the repr:
0.3.4,Test with two objects
0.3.4,Check that we can't instantiate with non-transformers on the way
0.3.4,"Note that NoTrans implements fit, but not transform"
0.3.4,Check that params are set
0.3.4,Smoke test the repr:
0.3.4,Check that params are not set when naming them wrong
0.3.4,Test clone
0.3.4,"Check that apart from estimators, the parameters are the same"
0.3.4,Remove estimators that where copied
0.3.4,Test the various methods of the pipeline (anova).
0.3.4,Test with Anova + LogisticRegression
0.3.4,Test that the pipeline can take fit parameters
0.3.4,classifier should return True
0.3.4,and transformer params should not be changed
0.3.4,invalid parameters should raise an error message
0.3.4,Pipeline should pass sample_weight
0.3.4,When sample_weight is None it shouldn't be passed
0.3.4,Test pipeline raises set params error message for nested models.
0.3.4,nested model check
0.3.4,Test the various methods of the pipeline (pca + svm).
0.3.4,Test with PCA + SVC
0.3.4,Test the various methods of the pipeline (preprocessing + svm).
0.3.4,check shapes of various prediction functions
0.3.4,test that the fit_predict method is implemented on a pipeline
0.3.4,test that the fit_predict on pipeline yields same results as applying
0.3.4,transform and clustering steps separately
0.3.4,"As pipeline doesn't clone estimators on construction,"
0.3.4,it must have its own estimators
0.3.4,first compute the transform and clustering step separately
0.3.4,use a pipeline to do the transform and clustering in one step
0.3.4,tests that a pipeline does not have fit_predict method when final
0.3.4,step of pipeline does not have fit_predict defined
0.3.4,tests that Pipeline passes fit_params to intermediate steps
0.3.4,when fit_predict is invoked
0.3.4,Test whether pipeline works with a transformer at the end.
0.3.4,Also test pipeline.transform and pipeline.inverse_transform
0.3.4,test transform and fit_transform:
0.3.4,Test whether pipeline works with a transformer missing fit_transform
0.3.4,test fit_transform:
0.3.4,Directly setting attr
0.3.4,Using set_params
0.3.4,Using set_params to replace single step
0.3.4,With invalid data
0.3.4,Test setting Pipeline steps to None
0.3.4,"for other methods, ensure no AttributeErrors on None:"
0.3.4,mult2 and mult3 are active
0.3.4,Check None step at construction time
0.3.4,Test that an error is raised when memory is not a string or a Memory
0.3.4,instance
0.3.4,Define memory as an integer
0.3.4,Test with Transformer + SVC
0.3.4,Memoize the transformer at the first fit
0.3.4,Get the time stamp of the tranformer in the cached pipeline
0.3.4,Check that cached_pipe and pipe yield identical results
0.3.4,Check that we are reading the cache while fitting
0.3.4,a second time
0.3.4,Check that cached_pipe and pipe yield identical results
0.3.4,Create a new pipeline with cloned estimators
0.3.4,Check that even changing the name step does not affect the cache hit
0.3.4,Check that cached_pipe and pipe yield identical results
0.3.4,Test with Transformer + SVC
0.3.4,Memoize the transformer at the first fit
0.3.4,Get the time stamp of the tranformer in the cached pipeline
0.3.4,Check that cached_pipe and pipe yield identical results
0.3.4,Check that we are reading the cache while fitting
0.3.4,a second time
0.3.4,Check that cached_pipe and pipe yield identical results
0.3.4,Create a new pipeline with cloned estimators
0.3.4,Check that even changing the name step does not affect the cache hit
0.3.4,Check that cached_pipe and pipe yield identical results
0.3.4,Test the various methods of the pipeline (pca + svm).
0.3.4,Test with PCA + SVC
0.3.4,Test the various methods of the pipeline (pca + svm).
0.3.4,Test with PCA + SVC
0.3.4,Test whether pipeline works with a sampler at the end.
0.3.4,Also test pipeline.sampler
0.3.4,test transform and fit_transform:
0.3.4,We round the value near to zero. It seems that PCA has some issue
0.3.4,with that
0.3.4,Test whether pipeline works with a sampler at the end.
0.3.4,Also test pipeline.sampler
0.3.4,Test pipeline using None as preprocessing step and a classifier
0.3.4,"Test pipeline using None, RUS and a classifier"
0.3.4,"Test pipeline using RUS, None and a classifier"
0.3.4,Test pipeline using None step and a sampler
0.3.4,Test pipeline using None and a transformer that implements transform and
0.3.4,inverse_transform
0.3.4,Test the various methods of the pipeline (anova).
0.3.4,Test with RandomUnderSampling + Anova + LogisticRegression
0.3.4,Test the various methods of the pipeline (anova).
0.3.4,Test the various methods of the pipeline (anova).
0.3.4,Test with RandomUnderSampling + Anova + LogisticRegression
0.3.4,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.4,Christos Aridas
0.3.4,License: MIT
0.3.4,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.4,License: MIT
0.3.4,Adapated from scikit-learn
0.3.4,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.4,License: MIT
0.3.4,make the checks from scikit-learn
0.3.4,trigger our checks if this is a SamplerMixin
0.3.4,FIXME already present in scikit-learn 0.19
0.3.4,test scikit-learn compatibility
0.3.4,Estimators in mono_output_task_error raise ValueError if y is of 1-D
0.3.4,Convert into a 2-D y for those estimators.
0.3.4,check that fit method only changes or sets private attributes
0.3.4,to not check deprecated classes
0.3.4,check that fit doesn't add any public attribute
0.3.4,check that fit doesn't change any public attribute
0.3.4,in this test we will force all samplers to not change the class 1
0.3.4,check that sparse matrices can be passed through the sampler leading to
0.3.4,the same results than dense
0.3.4,set KMeans to full since it support sparse and dense
0.3.4,Check that the samplers handle pandas dataframe and pandas series
0.3.4,Adapted from scikit-learn
0.3.4,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.4,License: MIT
0.3.4,meta-estimators need another estimator to be instantiated.
0.3.4,estimators that there is no way to default-construct sensibly
0.3.4,some strange ones
0.3.4,get parent folder
0.3.4,get rid of abstract base classes
0.3.4,get rid of sklearn estimators which have been imported in some classes
0.3.4,possibly get rid of meta estimators
0.3.4,"drop duplicates, sort for reproducibility"
0.3.4,itemgetter is used to ensure the sort does not extend to the 2nd item of
0.3.4,the tuple
0.3.4,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.4,License: MIT
0.3.4,FIXME: perfectly we should raise an error but the sklearn API does
0.3.4,not allow for it
0.3.4,check that all keys in ratio are also in y
0.3.4,check that there is no negative number
0.3.4,clean-sampling can be more permissive since those samplers do not
0.3.4,use samples
0.3.4,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.4,Christos Aridas
0.3.4,License: MIT
0.3.4,this function could create an equal number of samples
0.3.4,"tests that the estimator actually fails on ""bad"" estimators."
0.3.4,"not a complete test of all checks, which are very extensive."
0.3.4,check that we have a set_params and can clone
0.3.4,check that we have a fit method
0.3.4,check that fit does input validation
0.3.4,check that predict does input validation (doesn't accept dicts in input)
0.3.4,check that estimator state does not change
0.3.4,at transform/predict/predict_proba time
0.3.4,check that `fit` only changes attributes that
0.3.4,are private (start with an _ or end with a _).
0.3.4,check that `fit` doesn't add any public attribute
0.3.4,check for sparse matrix input handling
0.3.4,"the check for sparse input handling prints to the stdout,"
0.3.4,"instead of raising an error, so as not to remove the original traceback."
0.3.4,that means we need to jump through some hoops to catch it.
0.3.4,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.4,License: MIT
0.3.4,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.4,Christos Aridas
0.3.4,License: MIT
0.3.4,check if the filtering is working with a list or a single string
0.3.4,check that all estimators are sampler
0.3.4,check that an error is raised when the type is unknown
0.3.4,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.4,Christos Aridas
0.3.4,License: MIT
0.3.4,Check any parameters for SMOTE was provided
0.3.4,Anounce deprecation
0.3.4,We need to list each parameter and decide if we affect a default
0.3.4,value or not
0.3.4,"If an object was given, affect"
0.3.4,Otherwise create a default SMOTE
0.3.4,Check any parameters for ENN was provided
0.3.4,Anounce deprecation
0.3.4,We need to list each parameter and decide if we affect a default
0.3.4,value or not
0.3.4,"If an object was given, affect"
0.3.4,Otherwise create a default EditedNearestNeighbours
0.3.4,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.4,Christos Aridas
0.3.4,License: MIT
0.3.4,Check any parameters for SMOTE was provided
0.3.4,Anounce deprecation
0.3.4,We need to list each parameter and decide if we affect a default
0.3.4,value or not
0.3.4,"If an object was given, affect"
0.3.4,Otherwise create a default SMOTE
0.3.4,Check any parameters for ENN was provided
0.3.4,Anounce deprecation
0.3.4,"If an object was given, affect"
0.3.4,Otherwise create a default TomekLinks
0.3.4,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.4,Christos Aridas
0.3.4,License: MIT
0.3.4,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.4,Christos Aridas
0.3.4,License: MIT
0.3.4,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.4,Christos Aridas
0.3.4,License: MIT
0.3.4,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.4,Christos Aridas
0.3.4,License: MIT
0.3.4,RandomUnderSampler is not supporting sample_weight. We need to pass
0.3.4,None.
0.3.4,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.4,Christos Aridas
0.3.4,License: MIT
0.3.4,doctest: +ELLIPSIS
0.3.4,To be removed in 0.4
0.3.4,Define the classifier to use
0.3.4,array to know which samples are available to be taken
0.3.4,where the different set will be stored
0.3.4,store the index of the data to under-sample
0.3.4,value which will be picked at each round
0.3.4,extract the data of interest for this round from the
0.3.4,current class
0.3.4,select randomly the desired features
0.3.4,store the set created
0.3.4,fit and predict using cross validation
0.3.4,extract the prediction about the targeted classes only
0.3.4,check the stopping criterion
0.3.4,check that there is enough samples for another round
0.3.4,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.4,License: MIT
0.3.4,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.4,Christos Aridas
0.3.4,License: MIT
0.3.4,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.4,Christos Aridas
0.3.4,License: MIT
0.3.4,Check classification for various parameter settings.
0.3.4,Test that bootstrapping samples generate non-perfect base estimators.
0.3.4,"without bootstrap, all trees are perfect on the training set"
0.3.4,disable the resampling by passing an empty dictionary.
0.3.4,"with bootstrap, trees are no longer perfect on the training set"
0.3.4,Test that bootstrapping features may generate duplicate features.
0.3.4,Predict probabilities.
0.3.4,Normal case
0.3.4,"Degenerate case, where some classes are missing"
0.3.4,Check that oob prediction is a good estimation of the generalization
0.3.4,error.
0.3.4,Test with few estimators
0.3.4,Check singleton ensembles.
0.3.4,Test that it gives proper exception on deficient input.
0.3.4,Test n_estimators
0.3.4,Test max_samples
0.3.4,Test max_features
0.3.4,Test support of decision_function
0.3.4,Check that bagging ensembles can be grid-searched.
0.3.4,Transform iris into a binary classification task
0.3.4,Grid search with scoring based on decision_function
0.3.4,Check base_estimator and its default values.
0.3.4,Test if fitting incrementally with warm start gives a forest of the
0.3.4,right size and the same results as a normal fit.
0.3.4,Test if warm start'ed second fit with smaller n_estimators raises error.
0.3.4,Test that nothing happens when fitting without increasing n_estimators
0.3.4,"modify X to nonsense values, this should not change anything"
0.3.4,warm started classifier with 5+5 estimators should be equivalent to
0.3.4,one classifier with 10 estimators
0.3.4,Check using oob_score and warm_start simultaneously fails
0.3.4,"Make sure OOB scores are identical when random_state, estimator, and"
0.3.4,training data are fixed and fitting is done twice
0.3.4,FIXME: uncomment when #9723 is merged in scikit-learn
0.3.4,def test_estimators_samples():
0.3.4,# Check that format of estimators_samples_ is correct and that results
0.3.4,# generated at fit time can be identically reproduced at a later time
0.3.4,# using data saved in object attributes.
0.3.4,"X, y = make_hastie_10_2(n_samples=200, random_state=1)"
0.3.4,# remap the y outside of the BalancedBaggingclassifier
0.3.4,"# _, y = np.unique(y, return_inverse=True)"
0.3.4,"bagging = BalancedBaggingClassifier(LogisticRegression(),"
0.3.4,"max_samples=0.5,"
0.3.4,"max_features=0.5, random_state=1,"
0.3.4,bootstrap=False)
0.3.4,"bagging.fit(X, y)"
0.3.4,# Get relevant attributes
0.3.4,estimators_samples = bagging.estimators_samples_
0.3.4,estimators_features = bagging.estimators_features_
0.3.4,estimators = bagging.estimators_
0.3.4,# Test for correct formatting
0.3.4,assert len(estimators_samples) == len(estimators)
0.3.4,assert len(estimators_samples[0]) == len(X)
0.3.4,assert estimators_samples[0].dtype.kind == 'b'
0.3.4,# Re-fit single estimator to test for consistent sampling
0.3.4,estimator_index = 0
0.3.4,estimator_samples = estimators_samples[estimator_index]
0.3.4,estimator_features = estimators_features[estimator_index]
0.3.4,estimator = estimators[estimator_index]
0.3.4,"X_train = (X[estimator_samples])[:, estimator_features]"
0.3.4,y_train = y[estimator_samples]
0.3.4,orig_coefs = estimator.steps[-1][1].coef_
0.3.4,"estimator.fit(X_train, y_train)"
0.3.4,new_coefs = estimator.steps[-1][1].coef_
0.3.4,"assert_array_almost_equal(orig_coefs, new_coefs)"
0.3.4,Make sure validated max_samples and original max_samples are identical
0.3.4,when valid integer max_samples supplied by user
0.3.4,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.4,Christos Aridas
0.3.4,License: MIT
0.3.4,Generate a global dataset to use
0.3.4,Define a ratio
0.3.4,Define the ratio parameter
0.3.4,Create the sampling object
0.3.4,Get the different subset
0.3.4,Define the ratio parameter
0.3.4,Create the sampling object
0.3.4,Get the different subset
0.3.4,Define the ratio parameter
0.3.4,Create the sampling object
0.3.4,Get the different subset
0.3.4,Author: Guillaume Lemaitre
0.3.4,License: BSD 3 clause
0.3.4,"The index start at one, then we need to remove one"
0.3.4,to not have issue with the indexing.
0.3.4,go through the list and check if the data are available
0.3.4,Authors: Dayvid Oliveira
0.3.4,Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.4,Christos Aridas
0.3.4,License: MIT
0.3.4,restrict ratio to be a dict or a callable
0.3.4,FIXME: deprecated in 0.2 to be removed in 0.4
0.3.4,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.4,Christos Aridas
0.3.4,License: MIT
0.3.4,"we are reusing part of utils.check_ratio, however this is not cover in"
0.3.4,the common tests so we will repeat it here
0.3.4,FIXME: to be removed in 0.4 due to deprecation
0.3.4,resample without using min_c_
0.3.4,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.4,Christos Aridas
0.3.4,License: MIT
0.3.3,! /usr/bin/env python
0.3.3,"load all vars into globals, otherwise"
0.3.3,the later function call using global vars doesn't work.
0.3.3,"Allow command-lines such as ""python setup.py build install"""
0.3.3,Make sources available using relative paths from this file's directory.
0.3.3,-*- coding: utf-8 -*-
0.3.3,
0.3.3,"imbalanced-learn documentation build configuration file, created by"
0.3.3,sphinx-quickstart on Mon Jan 18 14:44:12 2016.
0.3.3,
0.3.3,This file is execfile()d with the current directory set to its
0.3.3,containing dir.
0.3.3,
0.3.3,Note that not all possible configuration values are present in this
0.3.3,autogenerated file.
0.3.3,
0.3.3,All configuration values have a default; values that are commented out
0.3.3,serve to show the default.
0.3.3,"If extensions (or modules to document with autodoc) are in another directory,"
0.3.3,add these directories to sys.path here. If the directory is relative to the
0.3.3,"documentation root, use os.path.abspath to make it absolute, like shown here."
0.3.3,"sys.path.insert(0, os.path.abspath('.'))"
0.3.3,-- General configuration ---------------------------------------------------
0.3.3,Try to override the matplotlib configuration as early as possible
0.3.3,-- General configuration ------------------------------------------------
0.3.3,If extensions (or modules to document with autodoc) are in another
0.3.3,"directory, add these directories to sys.path here. If the directory"
0.3.3,"is relative to the documentation root, use os.path.abspath to make it"
0.3.3,"absolute, like shown here."
0.3.3,"If your documentation needs a minimal Sphinx version, state it here."
0.3.3,needs_sphinx = '1.0'
0.3.3,"Add any Sphinx extension module names here, as strings. They can be"
0.3.3,extensions coming with Sphinx (named 'sphinx.ext.*') or your custom
0.3.3,ones.
0.3.3,path to your examples scripts
0.3.3,path where to save gallery generated examples
0.3.3,to make references clickable
0.3.3,"Add any paths that contain templates here, relative to this directory."
0.3.3,generate autosummary even if no references
0.3.3,The suffix of source filenames.
0.3.3,The encoding of source files.
0.3.3,source_encoding = 'utf-8-sig'
0.3.3,Generate the plots for the gallery
0.3.3,The master toctree document.
0.3.3,General information about the project.
0.3.3,"The version info for the project you're documenting, acts as replacement for"
0.3.3,"|version| and |release|, also used in various other places throughout the"
0.3.3,built documents.
0.3.3,
0.3.3,The short X.Y version.
0.3.3,"The full version, including alpha/beta/rc tags."
0.3.3,The language for content autogenerated by Sphinx. Refer to documentation
0.3.3,for a list of supported languages.
0.3.3,language = None
0.3.3,"There are two options for replacing |today|: either, you set today to some"
0.3.3,"non-false value, then it is used:"
0.3.3,today = ''
0.3.3,"Else, today_fmt is used as the format for a strftime call."
0.3.3,"today_fmt = '%B %d, %Y'"
0.3.3,"List of patterns, relative to source directory, that match files and"
0.3.3,directories to ignore when looking for source files.
0.3.3,The reST default role (used for this markup: `text`) to use for all
0.3.3,documents.
0.3.3,default_role = None
0.3.3,"If true, '()' will be appended to :func: etc. cross-reference text."
0.3.3,"If true, the current module name will be prepended to all description"
0.3.3,unit titles (such as .. function::).
0.3.3,add_module_names = True
0.3.3,"If true, sectionauthor and moduleauthor directives will be shown in the"
0.3.3,output. They are ignored by default.
0.3.3,show_authors = False
0.3.3,The name of the Pygments (syntax highlighting) style to use.
0.3.3,Custom style
0.3.3,A list of ignored prefixes for module index sorting.
0.3.3,modindex_common_prefix = []
0.3.3,"If true, keep warnings as ""system message"" paragraphs in the built documents."
0.3.3,keep_warnings = False
0.3.3,-- Options for HTML output ----------------------------------------------
0.3.3,The theme to use for HTML and HTML Help pages.  See the documentation for
0.3.3,a list of builtin themes.
0.3.3,Theme options are theme-specific and customize the look and feel of a theme
0.3.3,"further.  For a list of options available for each theme, see the"
0.3.3,documentation.
0.3.3,html_theme_options = {'prev_next_buttons_location': None}
0.3.3,"Add any paths that contain custom themes here, relative to this directory."
0.3.3,"The name for this set of Sphinx documents.  If None, it defaults to"
0.3.3,"""<project> v<release> documentation""."
0.3.3,html_title = None
0.3.3,A shorter title for the navigation bar.  Default is the same as html_title.
0.3.3,html_short_title = None
0.3.3,The name of an image file (relative to this directory) to place at the top
0.3.3,of the sidebar.
0.3.3,html_logo = None
0.3.3,The name of an image file (within the static path) to use as favicon of the
0.3.3,docs.  This file should be a Windows icon file (.ico) being 16x16 or 32x32
0.3.3,pixels large.
0.3.3,html_favicon = None
0.3.3,"Add any paths that contain custom static files (such as style sheets) here,"
0.3.3,"relative to this directory. They are copied after the builtin static files,"
0.3.3,"so a file named ""default.css"" will overwrite the builtin ""default.css""."
0.3.3,Add any extra paths that contain custom files (such as robots.txt or
0.3.3,".htaccess) here, relative to this directory. These files are copied"
0.3.3,directly to the root of the documentation.
0.3.3,html_extra_path = []
0.3.3,"If not '', a 'Last updated on:' timestamp is inserted at every page bottom,"
0.3.3,using the given strftime format.
0.3.3,"html_last_updated_fmt = '%b %d, %Y'"
0.3.3,"If true, SmartyPants will be used to convert quotes and dashes to"
0.3.3,typographically correct entities.
0.3.3,html_use_smartypants = True
0.3.3,"Custom sidebar templates, maps document names to template names."
0.3.3,html_sidebars = {}
0.3.3,"Additional templates that should be rendered to pages, maps page names to"
0.3.3,template names.
0.3.3,html_additional_pages = {}
0.3.3,"If false, no module index is generated."
0.3.3,html_domain_indices = True
0.3.3,"If false, no index is generated."
0.3.3,html_use_index = True
0.3.3,"If true, the index is split into individual pages for each letter."
0.3.3,html_split_index = False
0.3.3,"If true, links to the reST sources are added to the pages."
0.3.3,html_show_sourcelink = True
0.3.3,"If true, ""Created using Sphinx"" is shown in the HTML footer. Default is True."
0.3.3,html_show_sphinx = True
0.3.3,"If true, ""(C) Copyright ..."" is shown in the HTML footer. Default is True."
0.3.3,html_show_copyright = True
0.3.3,"If true, an OpenSearch description file will be output, and all pages will"
0.3.3,contain a <link> tag referring to it.  The value of this option must be the
0.3.3,base URL from which the finished HTML is served.
0.3.3,html_use_opensearch = ''
0.3.3,"This is the file name suffix for HTML files (e.g. "".xhtml"")."
0.3.3,html_file_suffix = None
0.3.3,Output file base name for HTML help builder.
0.3.3,-- Options for LaTeX output ---------------------------------------------
0.3.3,The paper size ('letterpaper' or 'a4paper').
0.3.3,"'papersize': 'letterpaper',"
0.3.3,"The font size ('10pt', '11pt' or '12pt')."
0.3.3,"'pointsize': '10pt',"
0.3.3,Additional stuff for the LaTeX preamble.
0.3.3,"'preamble': '',"
0.3.3,Grouping the document tree into LaTeX files. List of tuples
0.3.3,"(source start file, target name, title,"
0.3.3,"author, documentclass [howto, manual, or own class])."
0.3.3,The name of an image file (relative to this directory) to place at the top of
0.3.3,the title page.
0.3.3,latex_logo = None
0.3.3,"For ""manual"" documents, if this is true, then toplevel headings are parts,"
0.3.3,not chapters.
0.3.3,latex_use_parts = False
0.3.3,"If true, show page references after internal links."
0.3.3,latex_show_pagerefs = False
0.3.3,"If true, show URL addresses after external links."
0.3.3,latex_show_urls = False
0.3.3,Documents to append as an appendix to all manuals.
0.3.3,latex_appendices = []
0.3.3,"If false, no module index is generated."
0.3.3,latex_domain_indices = True
0.3.3,-- Options for manual page output ---------------------------------------
0.3.3,One entry per manual page. List of tuples
0.3.3,"(source start file, name, description, authors, manual section)."
0.3.3,"If true, show URL addresses after external links."
0.3.3,man_show_urls = False
0.3.3,-- Options for Texinfo output -------------------------------------------
0.3.3,Grouping the document tree into Texinfo files. List of tuples
0.3.3,"(source start file, target name, title, author,"
0.3.3,"dir menu entry, description, category)"
0.3.3,"generate empty examples files, so that we don't get"
0.3.3,inclusion errors if there are no examples for a class / module
0.3.3,touch file
0.3.3,Config for sphinx_issues
0.3.3,Documents to append as an appendix to all manuals.
0.3.3,texinfo_appendices = []
0.3.3,"If false, no module index is generated."
0.3.3,texinfo_domain_indices = True
0.3.3,"How to display URL addresses: 'footnote', 'no', or 'inline'."
0.3.3,texinfo_show_urls = 'footnote'
0.3.3,"If true, do not generate a @detailmenu in the ""Top"" node's menu."
0.3.3,texinfo_no_detailmenu = False
0.3.3,Example configuration for intersphinx: refer to the Python standard library.
0.3.3,The following is used by sphinx.ext.linkcode to provide links to github
0.3.3,get the styles from the current theme
0.3.3,create and add the button to all the code blocks that contain >>>
0.3.3,tracebacks (.gt) contain bare text elements that need to be
0.3.3,wrapped in a span to work with .nextUntil() (see later)
0.3.3,define the behavior of the button when it's clicked
0.3.3,hide the code output
0.3.3,show the code output
0.3.3,-*- coding: utf-8 -*-
0.3.3,Format template for issues URI
0.3.3,e.g. 'https://github.com/sloria/marshmallow/issues/{issue}
0.3.3,"Shortcut for Github, e.g. 'sloria/marshmallow'"
0.3.3,Format template for user profile URI
0.3.3,e.g. 'https://github.com/{user}'
0.3.3,Python 2 only
0.3.3,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.3,License: MIT
0.3.3,##############################################################################
0.3.3,Creation of an imbalanced data set from a balanced data set
0.3.3,##############################################################################
0.3.3,##############################################################################
0.3.3,We will show how to use the parameter ``ratio`` when dealing with the
0.3.3,"``make_imbalance`` function. For this function, this parameter accepts both"
0.3.3,"dictionary and callable. When using a dictionary, each key will correspond to"
0.3.3,the class of interest and the corresponding value will be the number of
0.3.3,samples desired in this class.
0.3.3,##############################################################################
0.3.3,You might required more flexibility and require your own heuristic to
0.3.3,determine the number of samples by class and you can define your own callable
0.3.3,as follow. In this case we will define a function which will use a float
0.3.3,multiplier to define the number of samples per class.
0.3.3,##############################################################################
0.3.3,Using ``ratio`` in resampling algorithm
0.3.3,##############################################################################
0.3.3,##############################################################################
0.3.3,"In all sampling algorithms, ``ratio`` can be used as illustrated earlier. In"
0.3.3,"addition, some predefined functions are available and can be executed using a"
0.3.3,``str`` with the following choices: (i) ``'minority'``: resample the minority
0.3.3,"class; (ii) ``'majority'``: resample the majority class, (iii) ``'not"
0.3.3,"minority'``: resample all classes apart of the minority class, (iv)"
0.3.3,"``'all'``: resample all classes, and (v) ``'auto'``: correspond to 'all' with"
0.3.3,for over-sampling methods and 'not minority' for under-sampling methods. The
0.3.3,classes targeted will be over-sampled or under-sampled to achieve an equal
0.3.3,number of sample with the majority or minority class.
0.3.3,##############################################################################
0.3.3,"However, you can use the dictionary or the callable options as previously"
0.3.3,mentioned.
0.3.3,Authors: Fernando Nogueira
0.3.3,Christos Aridas
0.3.3,Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.3,License: MIT
0.3.3,Generate the dataset
0.3.3,Instanciate a PCA object for the sake of easy visualisation
0.3.3,Fit and transform x to visualise inside a 2D feature space
0.3.3,Apply regular SMOTE
0.3.3,"Two subplots, unpack the axes array immediately"
0.3.3,Remove axis for second plot
0.3.3,Authors: Christos Aridas
0.3.3,Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.3,License: MIT
0.3.3,Generate the dataset
0.3.3,Instanciate a PCA object for the sake of easy visualisation
0.3.3,Fit and transform x to visualise inside a 2D feature space
0.3.3,Apply the random over-sampling
0.3.3,"Two subplots, unpack the axes array immediately"
0.3.3,make nice plotting
0.3.3,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.3,License: MIT
0.3.3,generate some data points
0.3.3,plot the majority and minority samples
0.3.3,draw the circle in which the new sample will generated
0.3.3,plot the line on which the sample will be generated
0.3.3,create and plot the new sample
0.3.3,make the plot nicer with legend and label
0.3.3,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.3,License: MIT
0.3.3,##############################################################################
0.3.3,The following function will be used to create toy dataset. It using the
0.3.3,``make_classification`` from scikit-learn but fixing some parameters.
0.3.3,##############################################################################
0.3.3,The following function will be used to plot the sample space after resampling
0.3.3,to illustrate the characterisitic of an algorithm.
0.3.3,make nice plotting
0.3.3,##############################################################################
0.3.3,The following function will be used to plot the decision function of a
0.3.3,classifier given some data.
0.3.3,##############################################################################
0.3.3,Illustration of the influence of the balancing ratio
0.3.3,##############################################################################
0.3.3,##############################################################################
0.3.3,We will first illustrate the influence of the balancing ratio on some toy
0.3.3,data using a linear SVM classifier. Greater is the difference between the
0.3.3,"number of samples in each class, poorer are the classfication results."
0.3.3,##############################################################################
0.3.3,Random over-sampling to balance the data set
0.3.3,##############################################################################
0.3.3,##############################################################################
0.3.3,Random over-sampling can be used to repeat some samples and balance the
0.3.3,number of samples between the dataset. It can be seen that with this trivial
0.3.3,approach the boundary decision is already less biaised toward the majority
0.3.3,class.
0.3.3,##############################################################################
0.3.3,More advanced over-sampling using ADASYN and SMOTE
0.3.3,##############################################################################
0.3.3,##############################################################################
0.3.3,"Instead of repeating the same samples when over-sampling, we can use some"
0.3.3,specific heuristic instead. ADASYN and SMOTE can be used in this case.
0.3.3,Make an identity sampler
0.3.3,##############################################################################
0.3.3,The following plot illustrate the difference between ADASYN and SMOTE. ADASYN
0.3.3,will focus on the samples which are difficult to classify with a
0.3.3,nearest-neighbors rule while regular SMOTE will not make any distinction.
0.3.3,"Therefore, the decision function depending of the algorithm."
0.3.3,##############################################################################
0.3.3,"Due to those sampling particularities, it can give rise to some specific"
0.3.3,issues as illustrated below.
0.3.3,##############################################################################
0.3.3,SMOTE proposes several variants by identifying specific samples to consider
0.3.3,during the resampling. The borderline version will detect which point to
0.3.3,select which are in the border between two classes. The SVM version will use
0.3.3,the support vectors found using an SVM algorithm to create new samples.
0.3.3,Authors: Christos Aridas
0.3.3,Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.3,License: MIT
0.3.3,Generate the dataset
0.3.3,Instanciate a PCA object for the sake of easy visualisation
0.3.3,Fit and transform x to visualise inside a 2D feature space
0.3.3,Apply the random over-sampling
0.3.3,"Two subplots, unpack the axes array immediately"
0.3.3,make nice plotting
0.3.3,Authors: Christos Aridas
0.3.3,Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.3,License: MIT
0.3.3,Generate the dataset
0.3.3,make nice plotting
0.3.3,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.3,License: MIT
0.3.3,Generate a dataset
0.3.3,Split the data
0.3.3,Train the classifier with balancing
0.3.3,Test the classifier and get the prediction
0.3.3,Show the classification report
0.3.3,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.3,License: MIT
0.3.3,Generate a dataset
0.3.3,Split the data
0.3.3,Train the classifier with balancing
0.3.3,Test the classifier and get the prediction
0.3.3,##############################################################################
0.3.3,The geometric mean corresponds to the square root of the product of the
0.3.3,sensitivity and specificity. Combining the two metrics should account for
0.3.3,the balancing of the dataset.
0.3.3,##############################################################################
0.3.3,The index balanced accuracy can transform any metric to be used in
0.3.3,imbalanced learning problems.
0.3.3,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.3,License: MIT
0.3.3,##############################################################################
0.3.3,The following function will be used to create toy dataset. It using the
0.3.3,``make_classification`` from scikit-learn but fixing some parameters.
0.3.3,##############################################################################
0.3.3,The following function will be used to plot the sample space after resampling
0.3.3,to illustrate the characteristic of an algorithm.
0.3.3,make nice plotting
0.3.3,##############################################################################
0.3.3,The following function will be used to plot the decision function of a
0.3.3,classifier given some data.
0.3.3,##############################################################################
0.3.3,"``SMOTE`` allows to generate samples. However, this method of over-sampling"
0.3.3,"does not have any knowledge regarding the underlying distribution. Therefore,"
0.3.3,"some noisy samples can be generated, e.g. when the different classes cannot"
0.3.3,"be well separated. Hence, it can be beneficial to apply an under-sampling"
0.3.3,algorithm to clean the noisy samples. Two methods are usually used in the
0.3.3,literature: (i) Tomek's link and (ii) edited nearest neighbours cleaning
0.3.3,methods. Imbalanced-learn provides two ready-to-use samplers ``SMOTETomek``
0.3.3,"and ``SMOTEENN``. In general, ``SMOTEENN`` cleans more noisy data than"
0.3.3,``SMOTETomek``.
0.3.3,Authors: Christos Aridas
0.3.3,Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.3,License: MIT
0.3.3,Generate the dataset
0.3.3,Instanciate a PCA object for the sake of easy visualisation
0.3.3,Fit and transform x to visualise inside a 2D feature space
0.3.3,Apply SMOTE + ENN
0.3.3,"Two subplots, unpack the axes array immediately"
0.3.3,make nice plotting
0.3.3,Authors: Christos Aridas
0.3.3,Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.3,License: MIT
0.3.3,Generate the dataset
0.3.3,Instanciate a PCA object for the sake of easy visualisation
0.3.3,Fit and transform x to visualise inside a 2D feature space
0.3.3,Apply SMOTE + Tomek links
0.3.3,"Two subplots, unpack the axes array immediately"
0.3.3,make nice plotting
0.3.3,Authors: Christos Aridas
0.3.3,Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.3,License: MIT
0.3.3,Generate the dataset
0.3.3,Instanciate a PCA object for the sake of easy visualisation
0.3.3,Fit and transform x to visualise inside a 2D feature space
0.3.3,Apply Balance Cascade method
0.3.3,"Two subplots, unpack the axes array immediately"
0.3.3,make nice plotting
0.3.3,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.3,License: MIT
0.3.3,##############################################################################
0.3.3,Turning the balanced bagging classifier into a balanced random forest
0.3.3,##############################################################################
0.3.3,It is possible to turn the ``BalancedBaggingClassifier`` into a balanced
0.3.3,random forest by using a ``DecisionTreeClassifier`` with
0.3.3,``max_features='auto'``. We illustrate such changes below.
0.3.3,Authors: Christos Aridas
0.3.3,Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.3,License: MIT
0.3.3,Generate the dataset
0.3.3,Instanciate a PCA object for the sake of easy visualisation
0.3.3,Fit and transform x to visualise inside a 2D feature space
0.3.3,Apply Easy Ensemble
0.3.3,"Two subplots, unpack the axes array immediately"
0.3.3,make nice plotting
0.3.3,Authors: Andreas Mueller
0.3.3,Christos Aridas
0.3.3,Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.3,License: MIT
0.3.3,remove Tomek links
0.3.3,make nice plotting
0.3.3,Authors: Dayvid Oliveira
0.3.3,Christos Aridas
0.3.3,Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.3,License: MIT
0.3.3,Generate the dataset
0.3.3,Instanciate a PCA object for the sake of easy visualisation
0.3.3,Fit and transform x to visualise inside a 2D feature space
0.3.3,"Three subplots, unpack the axes array immediately"
0.3.3,Apply the ENN
0.3.3,Apply the RENN
0.3.3,Apply the AllKNN
0.3.3,Authors: Christos Aridas
0.3.3,Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.3,License: MIT
0.3.3,Generate the dataset
0.3.3,Instanciate a PCA object for the sake of easy visualisation
0.3.3,Fit and transform x to visualise inside a 2D feature space
0.3.3,Apply Nearmiss
0.3.3,"Two subplots, unpack the axes array immediately"
0.3.3,plot the missing samples
0.3.3,Authors: Christos Aridas
0.3.3,Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.3,License: MIT
0.3.3,Generate the dataset
0.3.3,Instanciate a PCA object for the sake of easy visualisation
0.3.3,Fit and transform x to visualise inside a 2D feature space
0.3.3,Apply One-Sided Selection
0.3.3,make nice plotting
0.3.3,Authors: Christos Aridas
0.3.3,Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.3,License: MIT
0.3.3,Generate the dataset
0.3.3,Instanciate a PCA object for the sake of easy visualisation
0.3.3,Fit and transform x to visualise inside a 2D feature space
0.3.3,Apply neighbourhood cleaning rule
0.3.3,make nice plotting
0.3.3,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.3,License: MIT
0.3.3,##############################################################################
0.3.3,The following function will be used to create toy dataset. It using the
0.3.3,``make_classification`` from scikit-learn but fixing some parameters.
0.3.3,##############################################################################
0.3.3,The following function will be used to plot the sample space after resampling
0.3.3,to illustrate the characteristic of an algorithm.
0.3.3,make nice plotting
0.3.3,##############################################################################
0.3.3,The following function will be used to plot the decision function of a
0.3.3,classifier given some data.
0.3.3,##############################################################################
0.3.3,Prototype generation: under-sampling by generating new samples
0.3.3,##############################################################################
0.3.3,##############################################################################
0.3.3,``ClusterCentroids`` under-samples by replacing the original samples by the
0.3.3,centroids of the cluster found.
0.3.3,##############################################################################
0.3.3,Prototype selection: under-sampling by selecting existing samples
0.3.3,##############################################################################
0.3.3,##############################################################################
0.3.3,The algorithm performing prototype selection can be subdivided into two
0.3.3,groups: (i) the controlled under-sampling methods and (ii) the cleaning
0.3.3,under-sampling methods.
0.3.3,##############################################################################
0.3.3,"With the controlled under-sampling methods, the number of samples to be"
0.3.3,selected can be specified. ``RandomUnderSampler`` is the most naive way of
0.3.3,performing such selection by randomly selecting a given number of samples by
0.3.3,the targetted class.
0.3.3,##############################################################################
0.3.3,``NearMiss`` algorithms implement some heuristic rules in order to select
0.3.3,samples. NearMiss-1 selects samples from the majority class for which the
0.3.3,average distance of the :math:`k`` nearest samples of the minority class is
0.3.3,the smallest. NearMiss-2 selects the samples from the majority class for
0.3.3,which the average distance to the farthest samples of the negative class is
0.3.3,"the smallest. NearMiss-3 is a 2-step algorithm: first, for each minority"
0.3.3,"sample, their ::math:`m` nearest-neighbors will be kept; then, the majority"
0.3.3,samples selected are the on for which the average distance to the :math:`k`
0.3.3,nearest neighbors is the largest.
0.3.3,##############################################################################
0.3.3,``EditedNearestNeighbours`` removes samples of the majority class for which
0.3.3,their class differ from the one of their nearest-neighbors. This sieve can be
0.3.3,repeated which is the principle of the
0.3.3,``RepeatedEditedNearestNeighbours``. ``AllKNN`` is slightly different from
0.3.3,the ``RepeatedEditedNearestNeighbours`` by changing the :math:`k` parameter
0.3.3,"of the internal nearest neighors algorithm, increasing it at each iteration."
0.3.3,##############################################################################
0.3.3,``CondensedNearestNeighbour`` makes use of a 1-NN to iteratively decide if a
0.3.3,sample should be kept in a dataset or not. The issue is that
0.3.3,``CondensedNearestNeighbour`` is sensitive to noise by preserving the noisy
0.3.3,samples. ``OneSidedSelection`` also used the 1-NN and use ``TomekLinks`` to
0.3.3,remove the samples considered noisy. The ``NeighbourhoodCleaningRule`` use a
0.3.3,"``EditedNearestNeighbours`` to remove some sample. Additionally, they use a 3"
0.3.3,nearest-neighbors to remove samples which do not agree with this rule.
0.3.3,##############################################################################
0.3.3,``InstanceHardnessThreshold`` uses the prediction of classifier to exclude
0.3.3,samples. All samples which are classified with a low probability will be
0.3.3,removed.
0.3.3,Authors: Christos Aridas
0.3.3,Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.3,License: MIT
0.3.3,Generate the dataset
0.3.3,Instanciate a PCA object for the sake of easy visualisation
0.3.3,Fit and transform x to visualise inside a 2D feature space
0.3.3,Apply Condensed Nearest Neighbours
0.3.3,make nice plotting
0.3.3,Authors: Fernando Nogueira
0.3.3,Christos Aridas
0.3.3,Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.3,License: MIT
0.3.3,Generate the dataset
0.3.3,Instanciate a PCA object for the sake of easy visualisation
0.3.3,Fit and transform x to visualise inside a 2D feature space
0.3.3,Apply Cluster Centroids
0.3.3,Use hard voting instead of soft voting
0.3.3,"Two subplots, unpack the axes array immediately"
0.3.3,make nice plotting
0.3.3,##############################################################################
0.3.3,This function allows to make nice plotting
0.3.3,##############################################################################
0.3.3,Generate some data with one Tomek link
0.3.3,minority class
0.3.3,majority class
0.3.3,##############################################################################
0.3.3,"In the figure above, the samples highlighted in green form a Tomek link since"
0.3.3,they are of different classes and are nearest neighbours of each other.
0.3.3,highlight the samples of interest
0.3.3,##############################################################################
0.3.3,We can run the ``TomekLinks`` sampling to remove the corresponding
0.3.3,samples. If ``ratio='auto'`` only the sample from the majority class will be
0.3.3,removed. If ``ratio='all'`` both samples will be removed.
0.3.3,highlight the samples of interest
0.3.3,Authors: Christos Aridas
0.3.3,Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.3,License: MIT
0.3.3,Generate the dataset
0.3.3,Instanciate a PCA object for the sake of easy visualisation
0.3.3,Fit and transform x to visualise inside a 2D feature space
0.3.3,Apply the random under-sampling
0.3.3,make nice plotting
0.3.3,Authors: Dayvid Oliveira
0.3.3,Christos Aridas
0.3.3,Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.3,License: MIT
0.3.3,Generate the dataset
0.3.3,"Two subplots, unpack the axes array immediately"
0.3.3,plot samples which have been removed
0.3.3,##############################################################################
0.3.3,This function allows to make nice plotting
0.3.3,##############################################################################
0.3.3,We can start by generating some data to later illustrate the principle of
0.3.3,each NearMiss heuritic rules.
0.3.3,minority class
0.3.3,majority class
0.3.3,##############################################################################
0.3.3,NearMiss-1
0.3.3,##############################################################################
0.3.3,##############################################################################
0.3.3,NearMiss-1 selects samples from the majority class for which the average
0.3.3,distance to some nearest neighbours is the smallest. In the following
0.3.3,"example, we use a 3-NN to compute the average distance on 2 specific samples"
0.3.3,"of the majority class. Therefore, in this case the point linked by the"
0.3.3,green-dashed line will be selected since the average distance is smaller.
0.3.3,##############################################################################
0.3.3,NearMiss-2
0.3.3,##############################################################################
0.3.3,##############################################################################
0.3.3,NearMiss-2 selects samples from the majority class for which the average
0.3.3,distance to the farthest neighbors is the smallest. With the same
0.3.3,"configuration as previously presented, the sample linked to the green-dashed"
0.3.3,line will be selected since its distance the 3 farthest neighbors is the
0.3.3,smallest.
0.3.3,##############################################################################
0.3.3,NearMiss-3
0.3.3,##############################################################################
0.3.3,##############################################################################
0.3.3,"NearMiss-3 can be divided into 2 steps. First, a nearest-neighbors is used to"
0.3.3,short-list samples from the majority class (i.e. correspond to the
0.3.3,"highlighted samples in the following plot). Then, the sample with the largest"
0.3.3,average distance to the *k* nearest-neighbors are selected.
0.3.3,select only the majority point of interest
0.3.3,Authors: Christos Aridas
0.3.3,Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.3,License: MIT
0.3.3,Generate the dataset
0.3.3,Instanciate a PCA object for the sake of easy visualisation
0.3.3,Create the samplers
0.3.3,Create the classifier
0.3.3,Make the splits
0.3.3,Add one transformers and two samplers in the pipeline object
0.3.3,Authors: Christos Aridas
0.3.3,Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.3,License: MIT
0.3.3,Load the dataset
0.3.3,make nice plotting
0.3.3,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.3,License: MIT
0.3.3,Create a folder to fetch the dataset
0.3.3,Create a pipeline
0.3.3,Classify and report the results
0.3.3,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.3,License: MIT
0.3.3,##############################################################################
0.3.3,Setting the data set
0.3.3,##############################################################################
0.3.3,##############################################################################
0.3.3,We use a part of the 20 newsgroups data set by loading 4 topics. Using the
0.3.3,"scikit-learn loader, the data are split into a training and a testing set."
0.3.3,
0.3.3,Note the class \#3 is the minority class and has almost twice less samples
0.3.3,than the majority class.
0.3.3,##############################################################################
0.3.3,The usual scikit-learn pipeline
0.3.3,##############################################################################
0.3.3,##############################################################################
0.3.3,You might usually use scikit-learn pipeline by combining the TF-IDF
0.3.3,vectorizer to feed a multinomial naive bayes classifier. A classification
0.3.3,report summarized the results on the testing set.
0.3.3,
0.3.3,"As expected, the recall of the class \#3 is low mainly due to the class"
0.3.3,imbalanced.
0.3.3,##############################################################################
0.3.3,Balancing the class before classification
0.3.3,##############################################################################
0.3.3,##############################################################################
0.3.3,"To improve the prediction of the class \#3, it could be interesting to apply"
0.3.3,"a balancing before to train the naive bayes classifier. Therefore, we will"
0.3.3,use a ``RandomUnderSampler`` to equalize the number of samples in all the
0.3.3,classes before the training.
0.3.3,
0.3.3,It is also important to note that we are using the ``make_pipeline`` function
0.3.3,implemented in imbalanced-learn to properly handle the samplers.
0.3.3,##############################################################################
0.3.3,"Although the results are almost identical, it can be seen that the resampling"
0.3.3,allowed to correct the poor recall of the class \#3 at the cost of reducing
0.3.3,"the other metrics for the other classes. However, the overall results are"
0.3.3,slightly better.
0.3.3,Authors: Dayvid Oliveira
0.3.3,Christos Aridas
0.3.3,Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.3,License: MIT
0.3.3,Generate the dataset
0.3.3,"Two subplots, unpack the axes array immediately"
0.3.3,Based on NiLearn package
0.3.3,License: simplified BSD
0.3.3,"PEP0440 compatible formatted version, see:"
0.3.3,https://www.python.org/dev/peps/pep-0440/
0.3.3,
0.3.3,Generic release markers:
0.3.3,X.Y
0.3.3,X.Y.Z # For bugfix releases
0.3.3,
0.3.3,Admissible pre-release markers:
0.3.3,X.YaN # Alpha release
0.3.3,X.YbN # Beta release
0.3.3,X.YrcN # Release Candidate
0.3.3,X.Y # Final release
0.3.3,
0.3.3,Dev branch marker is: 'X.Y.dev' or 'X.Y.devN' where N is an integer.
0.3.3,'X.Y.dev0' is the canonical version of 'X.Y.dev'
0.3.3,
0.3.3,"This is a tuple to preserve order, so that dependencies are checked"
0.3.3,in some meaningful order (more => less 'core').  We avoid using
0.3.3,collections.OrderedDict to preserve Python 2.6 compatibility.
0.3.3,Avoid choking on modules with no __version__ attribute
0.3.3,Skip check only when installing and it's a module that
0.3.3,will be auto-installed.
0.3.3,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.3,License: MIT
0.3.3,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.3,Christos Aridas
0.3.3,License: MIT
0.3.3,Check the consistency of X and y
0.3.3,self.sampling_type is already checked in check_ratio
0.3.3,Adapted from scikit-learn
0.3.3,Author: Edouard Duchesnay
0.3.3,Gael Varoquaux
0.3.3,Virgile Fritsch
0.3.3,Alexandre Gramfort
0.3.3,Lars Buitinck
0.3.3,Christos Aridas
0.3.3,Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.3,License: BSD
0.3.3,BaseEstimator interface
0.3.3,shallow copy of steps
0.3.3,validate names
0.3.3,validate estimators
0.3.3,We allow last estimator to be None as an identity transformation
0.3.3,Estimator interface
0.3.3,Setup the memory
0.3.3,we do not clone when caching is disabled to preserve
0.3.3,backward compatibility
0.3.3,Fit or load from cache the current transfomer
0.3.3,Replace the transformer of the step with the fitted
0.3.3,transformer. This is necessary when loading the transformer
0.3.3,from the cache.
0.3.3,XXX: Calling sample in pipeline it means that the
0.3.3,last estimator is a sampler. Samplers don't carry
0.3.3,"the sampled data. So, call 'fit_sample' in all intermediate"
0.3.3,steps to get the sampled data for the last estimator.
0.3.3,"_final_estimator is None or has transform, otherwise attribute error"
0.3.3,raise AttributeError if necessary for hasattr behaviour
0.3.3,"if we have a weight for this transformer, multiply output"
0.3.3,Boolean controlling whether the joblib caches should be
0.3.3,"flushed if the version of certain modules changes (eg nibabel, as it"
0.3.3,does not respect the backward compatibility in some of its internal
0.3.3,structures
0.3.3,This  is used in nilearn._utils.cache_mixin
0.3.3,list all submodules available in imblearn and version
0.3.3,coding: utf-8
0.3.3,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.3,Dariusz Brzezinski
0.3.3,License: MIT
0.3.3,Only negative labels
0.3.3,"Calculate tp_sum, pred_sum, true_sum ###"
0.3.3,labels are now from 0 to len(labels) - 1 -> use bincount
0.3.3,Pathological case
0.3.3,Compute the true negative
0.3.3,Retain only selected labels
0.3.3,"Finally, we have all our sufficient statistics. Divide! #"
0.3.3,"Divide, and on zero-division, set scores to 0 and warn:"
0.3.3,"Oddly, we may get an ""invalid"" rather than a ""divide"" error"
0.3.3,here.
0.3.3,Average the results
0.3.3,labels are now from 0 to len(labels) - 1 -> use bincount
0.3.3,Pathological case
0.3.3,Retain only selected labels
0.3.3,old version of scipy return MaskedConstant instead of 0.0
0.3.3,Create the list of tags
0.3.3,check that the scoring function does not need a score
0.3.3,and only a prediction
0.3.3,Compute the score from the scoring function
0.3.3,Square if desired
0.3.3,Get the signature of the sens/spec function
0.3.3,We need to extract from kwargs only the one needed by the
0.3.3,specificity and specificity
0.3.3,Make the intersection between the parameters
0.3.3,Create a sub dictionary
0.3.3,Check if the metric is the geometric mean
0.3.3,We do not support multilabel so the only average supported
0.3.3,is binary
0.3.3,Create the list of parameters through signature binding
0.3.3,Call the sens/spec function
0.3.3,Compute the dominance
0.3.3,Compute the different metrics
0.3.3,Precision/recall/f1
0.3.3,Specificity
0.3.3,Geometric mean
0.3.3,Index balanced accuracy
0.3.3,compute averages
0.3.3,coding: utf-8
0.3.3,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.3,Christos Aridas
0.3.3,License: MIT
0.3.3,##############################################################################
0.3.3,Utilities for testing
0.3.3,import some data to play with
0.3.3,restrict to a binary classification task
0.3.3,add noisy features to make the problem harder and avoid perfect results
0.3.3,"run classifier, get class probabilities and label predictions"
0.3.3,only interested in probabilities of the positive case
0.3.3,XXX: do we really want a special API for the binary case?
0.3.3,##############################################################################
0.3.3,Tests
0.3.3,detailed measures for each class
0.3.3,individual scoring function that can be used for grid search: in the
0.3.3,binary class case the score is the value of the measure for the positive
0.3.3,class (e.g. label == 1). This is deprecated for average != 'binary'.
0.3.3,Such a case may occur with non-stratified cross-validation
0.3.3,No average: zeros in array
0.3.3,Macro average is changed
0.3.3,Check for micro
0.3.3,Check for weighted
0.3.3,ensure the above were meaningful tests:
0.3.3,Bad pos_label
0.3.3,Bad average option
0.3.3,but average != 'binary'; even if data is binary
0.3.3,compute the geometric mean for the binary problem
0.3.3,Compute the geometric mean for each of the classes
0.3.3,average tests
0.3.3,print classification report with class names
0.3.3,print classification report with label detection
0.3.3,print classification report with class names
0.3.3,print classification report with label detection
0.3.3,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.3,Christos Aridas
0.3.3,License: MIT
0.3.3,Get the version
0.3.3,sensitivity scorer
0.3.3,specificity scorer
0.3.3,geometric_mean scorer
0.3.3,make a iba metric before a scorer
0.3.3,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.3,Christos Aridas
0.3.3,License: MIT
0.3.3,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.3,Christos Aridas
0.3.3,License: MIT
0.3.3,FIXME: Deprecated in 0.2. To be removed in 0.4.
0.3.3,The ratio is computed using a one-vs-rest manner. Using majority
0.3.3,in multi-class would lead to slightly different results at the
0.3.3,cost of introducing a new parameter.
0.3.3,the nearest neighbors need to be fitted only on the current class
0.3.3,to find the class NN to generate new samples
0.3.3,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.3,Christos Aridas
0.3.3,License: MIT
0.3.3,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.3,Fernando Nogueira
0.3.3,Christos Aridas
0.3.3,License: MIT
0.3.3,Samples are in danger for m/2 <= m' < m
0.3.3,Samples are noise for m = m'
0.3.3,"FIXME Deprecated in 0.2, to be removed in 0.4"
0.3.3,divergence between borderline-1 and borderline-2
0.3.3,Create synthetic samples for borderline points.
0.3.3,only minority
0.3.3,we use a one-vs-rest policy to handle the multiclass in which
0.3.3,new samples will be created considering not only the majority
0.3.3,class but all over classes.
0.3.3,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.3,Christos Aridas
0.3.3,License: MIT
0.3.3,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.3,Christos Aridas
0.3.3,License: MIT
0.3.3,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.3,Christos Aridas
0.3.3,License: MIT
0.3.3,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.3,License: MIT
0.3.3,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.3,Fernando Nogueira
0.3.3,Christos Aridas
0.3.3,License: MIT
0.3.3,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.3,Christos Aridas
0.3.3,License: MIT
0.3.3,FIXME: Deprecated in 0.2. To be removed in 0.4.
0.3.3,select a sample from the current class
0.3.3,create the set composed of all minority samples and one
0.3.3,sample from the current class.
0.3.3,create the set S with removing the seed from S
0.3.3,since that it will be added anyway
0.3.3,apply Tomek cleaning
0.3.3,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.3,Christos Aridas
0.3.3,License: MIT
0.3.3,Compute the distance considering the farthest neighbour
0.3.3,Sort the list of distance and get the index
0.3.3,Throw a warning to tell the user that we did not have enough samples
0.3.3,to select and that we just select everything
0.3.3,Select the desired number of samples
0.3.3,FIXME: Deprecated in 0.2. To be removed in 0.4.
0.3.3,idx_tmp is relative to the feature selected in the
0.3.3,previous step and we need to find the indirection
0.3.3,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.3,Christos Aridas
0.3.3,License: MIT
0.3.3,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.3,Fernando Nogueira
0.3.3,Christos Aridas
0.3.3,License: MIT
0.3.3,find which class to not consider
0.3.3,there is a Tomek link between two samples if they are both nearest
0.3.3,neighbors of each others.
0.3.3,Find the nearest neighbour of every point
0.3.3,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.3,Christos Aridas
0.3.3,License: MIT
0.3.3,FIXME: Deprecated in 0.2. To be removed in 0.4
0.3.3,Randomly get one sample from the majority class
0.3.3,Generate the index to select
0.3.3,Create the set C - One majority samples and all minority
0.3.3,Create the set S - all majority samples
0.3.3,fit knn on C
0.3.3,Check each sample in S if we keep it or drop it
0.3.3,Do not select sample which are already well classified
0.3.3,Classify on S
0.3.3,If the prediction do not agree with the true label
0.3.3,append it in C_x
0.3.3,Keep the index for later
0.3.3,Update C
0.3.3,fit a knn on C
0.3.3,This experimental to speed up the search
0.3.3,Classify all the element in S and avoid to test the
0.3.3,well classified elements
0.3.3,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.3,Christos Aridas
0.3.3,License: MIT
0.3.3,FIXME: Deprecated from 0.2. To be removed in 0.4.
0.3.3,clean the neighborhood
0.3.3,compute which classes to consider for cleaning for the A2 group
0.3.3,compute a2 group
0.3.3,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.3,Dayvid Oliveira
0.3.3,Christos Aridas
0.3.3,License: MIT
0.3.3,FIXME: Deprecated in 0.2. To be removed in 0.4
0.3.3,Check the stopping criterion
0.3.3,1. If there is no changes for the vector y
0.3.3,2. If the number of samples in the other class become inferior to
0.3.3,the number of samples in the majority class
0.3.3,3. If one of the class is disappearing
0.3.3,Case 1
0.3.3,Case 2
0.3.3,Case 3
0.3.3,Check the stopping criterion
0.3.3,1. If the number of samples in the other class become inferior to
0.3.3,the number of samples in the majority class
0.3.3,2. If one of the class is disappearing
0.3.3,Case 1
0.3.3,overwrite b_min_bec_maj
0.3.3,Case 2
0.3.3,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.3,Dayvid Oliveira
0.3.3,Christos Aridas
0.3.3,License: MIT
0.3.3,To be removed in 0.4
0.3.3,Select the appropriate classifier
0.3.3,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.3,Christos Aridas
0.3.3,License: MIT
0.3.3,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.3,Christos Aridas
0.3.3,License: MIT
0.3.3,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.3,Christos Aridas
0.3.3,License: MIT
0.3.3,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.3,Christos Aridas
0.3.3,License: MIT
0.3.3,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.3,Christos Aridas
0.3.3,License: MIT
0.3.3,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.3,Christos Aridas
0.3.3,License: MIT
0.3.3,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.3,Christos Aridas
0.3.3,License: MIT
0.3.3,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.3,Christos Aridas
0.3.3,License: MIT
0.3.3,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.3,Christos Aridas
0.3.3,License: MIT
0.3.3,FIXME remove at the end of the deprecation 0.4
0.3.3,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.3,Christos Aridas
0.3.3,License: MIT
0.3.3,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.3,Christos Aridas
0.3.3,License: MIT
0.3.3,test that all_estimators doesn't find abstract classes.
0.3.3,some can just not be sensibly default constructed
0.3.3,input validation etc for non-meta estimators
0.3.3,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.3,Christos Aridas
0.3.3,License: MIT
0.3.3,store timestamp to figure out whether the result of 'fit' has been
0.3.3,cached or not
0.3.3,store timestamp to figure out whether the result of 'fit' has been
0.3.3,cached or not
0.3.3,Test the various init parameters of the pipeline.
0.3.3,Check that we can't instantiate pipelines with objects without fit
0.3.3,method
0.3.3,Smoke test with only an estimator
0.3.3,Check that params are set
0.3.3,Smoke test the repr:
0.3.3,Test with two objects
0.3.3,Check that we can't instantiate with non-transformers on the way
0.3.3,"Note that NoTrans implements fit, but not transform"
0.3.3,Check that params are set
0.3.3,Smoke test the repr:
0.3.3,Check that params are not set when naming them wrong
0.3.3,Test clone
0.3.3,"Check that apart from estimators, the parameters are the same"
0.3.3,Remove estimators that where copied
0.3.3,Test the various methods of the pipeline (anova).
0.3.3,Test with Anova + LogisticRegression
0.3.3,Test that the pipeline can take fit parameters
0.3.3,classifier should return True
0.3.3,and transformer params should not be changed
0.3.3,invalid parameters should raise an error message
0.3.3,Pipeline should pass sample_weight
0.3.3,When sample_weight is None it shouldn't be passed
0.3.3,Test pipeline raises set params error message for nested models.
0.3.3,nested model check
0.3.3,Test the various methods of the pipeline (pca + svm).
0.3.3,Test with PCA + SVC
0.3.3,Test the various methods of the pipeline (preprocessing + svm).
0.3.3,check shapes of various prediction functions
0.3.3,test that the fit_predict method is implemented on a pipeline
0.3.3,test that the fit_predict on pipeline yields same results as applying
0.3.3,transform and clustering steps separately
0.3.3,"As pipeline doesn't clone estimators on construction,"
0.3.3,it must have its own estimators
0.3.3,first compute the transform and clustering step separately
0.3.3,use a pipeline to do the transform and clustering in one step
0.3.3,tests that a pipeline does not have fit_predict method when final
0.3.3,step of pipeline does not have fit_predict defined
0.3.3,tests that Pipeline passes fit_params to intermediate steps
0.3.3,when fit_predict is invoked
0.3.3,Test whether pipeline works with a transformer at the end.
0.3.3,Also test pipeline.transform and pipeline.inverse_transform
0.3.3,test transform and fit_transform:
0.3.3,Test whether pipeline works with a transformer missing fit_transform
0.3.3,test fit_transform:
0.3.3,Directly setting attr
0.3.3,Using set_params
0.3.3,Using set_params to replace single step
0.3.3,With invalid data
0.3.3,Test setting Pipeline steps to None
0.3.3,"for other methods, ensure no AttributeErrors on None:"
0.3.3,mult2 and mult3 are active
0.3.3,Check None step at construction time
0.3.3,Test that an error is raised when memory is not a string or a Memory
0.3.3,instance
0.3.3,Define memory as an integer
0.3.3,Test with Transformer + SVC
0.3.3,Memoize the transformer at the first fit
0.3.3,Get the time stamp of the tranformer in the cached pipeline
0.3.3,Check that cached_pipe and pipe yield identical results
0.3.3,Check that we are reading the cache while fitting
0.3.3,a second time
0.3.3,Check that cached_pipe and pipe yield identical results
0.3.3,Create a new pipeline with cloned estimators
0.3.3,Check that even changing the name step does not affect the cache hit
0.3.3,Check that cached_pipe and pipe yield identical results
0.3.3,Test with Transformer + SVC
0.3.3,Memoize the transformer at the first fit
0.3.3,Get the time stamp of the tranformer in the cached pipeline
0.3.3,Check that cached_pipe and pipe yield identical results
0.3.3,Check that we are reading the cache while fitting
0.3.3,a second time
0.3.3,Check that cached_pipe and pipe yield identical results
0.3.3,Create a new pipeline with cloned estimators
0.3.3,Check that even changing the name step does not affect the cache hit
0.3.3,Check that cached_pipe and pipe yield identical results
0.3.3,Test the various methods of the pipeline (pca + svm).
0.3.3,Test with PCA + SVC
0.3.3,Test the various methods of the pipeline (pca + svm).
0.3.3,Test with PCA + SVC
0.3.3,Test whether pipeline works with a sampler at the end.
0.3.3,Also test pipeline.sampler
0.3.3,test transform and fit_transform:
0.3.3,We round the value near to zero. It seems that PCA has some issue
0.3.3,with that
0.3.3,Test whether pipeline works with a sampler at the end.
0.3.3,Also test pipeline.sampler
0.3.3,Test pipeline using None as preprocessing step and a classifier
0.3.3,"Test pipeline using None, RUS and a classifier"
0.3.3,"Test pipeline using RUS, None and a classifier"
0.3.3,Test pipeline using None step and a sampler
0.3.3,Test pipeline using None and a transformer that implements transform and
0.3.3,inverse_transform
0.3.3,Test the various methods of the pipeline (anova).
0.3.3,Test with RandomUnderSampling + Anova + LogisticRegression
0.3.3,Test the various methods of the pipeline (anova).
0.3.3,Test the various methods of the pipeline (anova).
0.3.3,Test with RandomUnderSampling + Anova + LogisticRegression
0.3.3,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.3,Christos Aridas
0.3.3,License: MIT
0.3.3,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.3,License: MIT
0.3.3,Adapated from scikit-learn
0.3.3,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.3,License: MIT
0.3.3,make the checks from scikit-learn
0.3.3,trigger our checks if this is a SamplerMixin
0.3.3,FIXME already present in scikit-learn 0.19
0.3.3,test scikit-learn compatibility
0.3.3,Estimators in mono_output_task_error raise ValueError if y is of 1-D
0.3.3,Convert into a 2-D y for those estimators.
0.3.3,check that fit method only changes or sets private attributes
0.3.3,to not check deprecated classes
0.3.3,check that fit doesn't add any public attribute
0.3.3,check that fit doesn't change any public attribute
0.3.3,in this test we will force all samplers to not change the class 1
0.3.3,check that sparse matrices can be passed through the sampler leading to
0.3.3,the same results than dense
0.3.3,set KMeans to full since it support sparse and dense
0.3.3,Check that the samplers handle pandas dataframe and pandas series
0.3.3,Adapted from scikit-learn
0.3.3,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.3,License: MIT
0.3.3,meta-estimators need another estimator to be instantiated.
0.3.3,estimators that there is no way to default-construct sensibly
0.3.3,some strange ones
0.3.3,get parent folder
0.3.3,get rid of abstract base classes
0.3.3,get rid of sklearn estimators which have been imported in some classes
0.3.3,possibly get rid of meta estimators
0.3.3,"drop duplicates, sort for reproducibility"
0.3.3,itemgetter is used to ensure the sort does not extend to the 2nd item of
0.3.3,the tuple
0.3.3,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.3,License: MIT
0.3.3,FIXME: perfectly we should raise an error but the sklearn API does
0.3.3,not allow for it
0.3.3,check that all keys in ratio are also in y
0.3.3,check that there is no negative number
0.3.3,clean-sampling can be more permissive since those samplers do not
0.3.3,use samples
0.3.3,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.3,Christos Aridas
0.3.3,License: MIT
0.3.3,this function could create an equal number of samples
0.3.3,"tests that the estimator actually fails on ""bad"" estimators."
0.3.3,"not a complete test of all checks, which are very extensive."
0.3.3,check that we have a set_params and can clone
0.3.3,check that we have a fit method
0.3.3,check that fit does input validation
0.3.3,check that predict does input validation (doesn't accept dicts in input)
0.3.3,check that estimator state does not change
0.3.3,at transform/predict/predict_proba time
0.3.3,check that `fit` only changes attributes that
0.3.3,are private (start with an _ or end with a _).
0.3.3,check that `fit` doesn't add any public attribute
0.3.3,check for sparse matrix input handling
0.3.3,"the check for sparse input handling prints to the stdout,"
0.3.3,"instead of raising an error, so as not to remove the original traceback."
0.3.3,that means we need to jump through some hoops to catch it.
0.3.3,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.3,License: MIT
0.3.3,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.3,Christos Aridas
0.3.3,License: MIT
0.3.3,check if the filtering is working with a list or a single string
0.3.3,check that all estimators are sampler
0.3.3,check that an error is raised when the type is unknown
0.3.3,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.3,Christos Aridas
0.3.3,License: MIT
0.3.3,Check any parameters for SMOTE was provided
0.3.3,Anounce deprecation
0.3.3,We need to list each parameter and decide if we affect a default
0.3.3,value or not
0.3.3,"If an object was given, affect"
0.3.3,Otherwise create a default SMOTE
0.3.3,Check any parameters for ENN was provided
0.3.3,Anounce deprecation
0.3.3,We need to list each parameter and decide if we affect a default
0.3.3,value or not
0.3.3,"If an object was given, affect"
0.3.3,Otherwise create a default EditedNearestNeighbours
0.3.3,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.3,Christos Aridas
0.3.3,License: MIT
0.3.3,Check any parameters for SMOTE was provided
0.3.3,Anounce deprecation
0.3.3,We need to list each parameter and decide if we affect a default
0.3.3,value or not
0.3.3,"If an object was given, affect"
0.3.3,Otherwise create a default SMOTE
0.3.3,Check any parameters for ENN was provided
0.3.3,Anounce deprecation
0.3.3,"If an object was given, affect"
0.3.3,Otherwise create a default TomekLinks
0.3.3,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.3,Christos Aridas
0.3.3,License: MIT
0.3.3,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.3,Christos Aridas
0.3.3,License: MIT
0.3.3,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.3,Christos Aridas
0.3.3,License: MIT
0.3.3,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.3,Christos Aridas
0.3.3,License: MIT
0.3.3,RandomUnderSampler is not supporting sample_weight. We need to pass
0.3.3,None.
0.3.3,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.3,Christos Aridas
0.3.3,License: MIT
0.3.3,doctest: +ELLIPSIS
0.3.3,To be removed in 0.4
0.3.3,Define the classifier to use
0.3.3,array to know which samples are available to be taken
0.3.3,where the different set will be stored
0.3.3,store the index of the data to under-sample
0.3.3,value which will be picked at each round
0.3.3,extract the data of interest for this round from the
0.3.3,current class
0.3.3,select randomly the desired features
0.3.3,store the set created
0.3.3,fit and predict using cross validation
0.3.3,extract the prediction about the targeted classes only
0.3.3,check the stopping criterion
0.3.3,check that there is enough samples for another round
0.3.3,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.3,License: MIT
0.3.3,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.3,Christos Aridas
0.3.3,License: MIT
0.3.3,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.3,Christos Aridas
0.3.3,License: MIT
0.3.3,Check classification for various parameter settings.
0.3.3,Test that bootstrapping samples generate non-perfect base estimators.
0.3.3,"without bootstrap, all trees are perfect on the training set"
0.3.3,disable the resampling by passing an empty dictionary.
0.3.3,"with bootstrap, trees are no longer perfect on the training set"
0.3.3,Test that bootstrapping features may generate duplicate features.
0.3.3,Predict probabilities.
0.3.3,Normal case
0.3.3,"Degenerate case, where some classes are missing"
0.3.3,Check that oob prediction is a good estimation of the generalization
0.3.3,error.
0.3.3,Test with few estimators
0.3.3,Check singleton ensembles.
0.3.3,Test that it gives proper exception on deficient input.
0.3.3,Test n_estimators
0.3.3,Test max_samples
0.3.3,Test max_features
0.3.3,Test support of decision_function
0.3.3,Check that bagging ensembles can be grid-searched.
0.3.3,Transform iris into a binary classification task
0.3.3,Grid search with scoring based on decision_function
0.3.3,Check base_estimator and its default values.
0.3.3,Test if fitting incrementally with warm start gives a forest of the
0.3.3,right size and the same results as a normal fit.
0.3.3,Test if warm start'ed second fit with smaller n_estimators raises error.
0.3.3,Test that nothing happens when fitting without increasing n_estimators
0.3.3,"modify X to nonsense values, this should not change anything"
0.3.3,warm started classifier with 5+5 estimators should be equivalent to
0.3.3,one classifier with 10 estimators
0.3.3,Check using oob_score and warm_start simultaneously fails
0.3.3,"Make sure OOB scores are identical when random_state, estimator, and"
0.3.3,training data are fixed and fitting is done twice
0.3.3,FIXME: uncomment when #9723 is merged in scikit-learn
0.3.3,def test_estimators_samples():
0.3.3,# Check that format of estimators_samples_ is correct and that results
0.3.3,# generated at fit time can be identically reproduced at a later time
0.3.3,# using data saved in object attributes.
0.3.3,"X, y = make_hastie_10_2(n_samples=200, random_state=1)"
0.3.3,# remap the y outside of the BalancedBaggingclassifier
0.3.3,"# _, y = np.unique(y, return_inverse=True)"
0.3.3,"bagging = BalancedBaggingClassifier(LogisticRegression(),"
0.3.3,"max_samples=0.5,"
0.3.3,"max_features=0.5, random_state=1,"
0.3.3,bootstrap=False)
0.3.3,"bagging.fit(X, y)"
0.3.3,# Get relevant attributes
0.3.3,estimators_samples = bagging.estimators_samples_
0.3.3,estimators_features = bagging.estimators_features_
0.3.3,estimators = bagging.estimators_
0.3.3,# Test for correct formatting
0.3.3,assert len(estimators_samples) == len(estimators)
0.3.3,assert len(estimators_samples[0]) == len(X)
0.3.3,assert estimators_samples[0].dtype.kind == 'b'
0.3.3,# Re-fit single estimator to test for consistent sampling
0.3.3,estimator_index = 0
0.3.3,estimator_samples = estimators_samples[estimator_index]
0.3.3,estimator_features = estimators_features[estimator_index]
0.3.3,estimator = estimators[estimator_index]
0.3.3,"X_train = (X[estimator_samples])[:, estimator_features]"
0.3.3,y_train = y[estimator_samples]
0.3.3,orig_coefs = estimator.steps[-1][1].coef_
0.3.3,"estimator.fit(X_train, y_train)"
0.3.3,new_coefs = estimator.steps[-1][1].coef_
0.3.3,"assert_array_almost_equal(orig_coefs, new_coefs)"
0.3.3,Make sure validated max_samples and original max_samples are identical
0.3.3,when valid integer max_samples supplied by user
0.3.3,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.3,Christos Aridas
0.3.3,License: MIT
0.3.3,Generate a global dataset to use
0.3.3,Define a ratio
0.3.3,Define the ratio parameter
0.3.3,Create the sampling object
0.3.3,Get the different subset
0.3.3,Define the ratio parameter
0.3.3,Create the sampling object
0.3.3,Get the different subset
0.3.3,Define the ratio parameter
0.3.3,Create the sampling object
0.3.3,Get the different subset
0.3.3,Author: Guillaume Lemaitre
0.3.3,License: BSD 3 clause
0.3.3,"The index start at one, then we need to remove one"
0.3.3,to not have issue with the indexing.
0.3.3,go through the list and check if the data are available
0.3.3,Authors: Dayvid Oliveira
0.3.3,Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.3,Christos Aridas
0.3.3,License: MIT
0.3.3,restrict ratio to be a dict or a callable
0.3.3,FIXME: deprecated in 0.2 to be removed in 0.4
0.3.3,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.3,Christos Aridas
0.3.3,License: MIT
0.3.3,"we are reusing part of utils.check_ratio, however this is not cover in"
0.3.3,the common tests so we will repeat it here
0.3.3,FIXME: to be removed in 0.4 due to deprecation
0.3.3,resample without using min_c_
0.3.3,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.3,Christos Aridas
0.3.3,License: MIT
0.3.2,! /usr/bin/env python
0.3.2,"load all vars into globals, otherwise"
0.3.2,the later function call using global vars doesn't work.
0.3.2,"Allow command-lines such as ""python setup.py build install"""
0.3.2,Make sources available using relative paths from this file's directory.
0.3.2,-*- coding: utf-8 -*-
0.3.2,
0.3.2,"imbalanced-learn documentation build configuration file, created by"
0.3.2,sphinx-quickstart on Mon Jan 18 14:44:12 2016.
0.3.2,
0.3.2,This file is execfile()d with the current directory set to its
0.3.2,containing dir.
0.3.2,
0.3.2,Note that not all possible configuration values are present in this
0.3.2,autogenerated file.
0.3.2,
0.3.2,All configuration values have a default; values that are commented out
0.3.2,serve to show the default.
0.3.2,"If extensions (or modules to document with autodoc) are in another directory,"
0.3.2,add these directories to sys.path here. If the directory is relative to the
0.3.2,"documentation root, use os.path.abspath to make it absolute, like shown here."
0.3.2,"sys.path.insert(0, os.path.abspath('.'))"
0.3.2,-- General configuration ---------------------------------------------------
0.3.2,Try to override the matplotlib configuration as early as possible
0.3.2,-- General configuration ------------------------------------------------
0.3.2,If extensions (or modules to document with autodoc) are in another
0.3.2,"directory, add these directories to sys.path here. If the directory"
0.3.2,"is relative to the documentation root, use os.path.abspath to make it"
0.3.2,"absolute, like shown here."
0.3.2,"If your documentation needs a minimal Sphinx version, state it here."
0.3.2,needs_sphinx = '1.0'
0.3.2,"Add any Sphinx extension module names here, as strings. They can be"
0.3.2,extensions coming with Sphinx (named 'sphinx.ext.*') or your custom
0.3.2,ones.
0.3.2,path to your examples scripts
0.3.2,path where to save gallery generated examples
0.3.2,to make references clickable
0.3.2,"Add any paths that contain templates here, relative to this directory."
0.3.2,generate autosummary even if no references
0.3.2,The suffix of source filenames.
0.3.2,The encoding of source files.
0.3.2,source_encoding = 'utf-8-sig'
0.3.2,Generate the plots for the gallery
0.3.2,The master toctree document.
0.3.2,General information about the project.
0.3.2,"The version info for the project you're documenting, acts as replacement for"
0.3.2,"|version| and |release|, also used in various other places throughout the"
0.3.2,built documents.
0.3.2,
0.3.2,The short X.Y version.
0.3.2,"The full version, including alpha/beta/rc tags."
0.3.2,The language for content autogenerated by Sphinx. Refer to documentation
0.3.2,for a list of supported languages.
0.3.2,language = None
0.3.2,"There are two options for replacing |today|: either, you set today to some"
0.3.2,"non-false value, then it is used:"
0.3.2,today = ''
0.3.2,"Else, today_fmt is used as the format for a strftime call."
0.3.2,"today_fmt = '%B %d, %Y'"
0.3.2,"List of patterns, relative to source directory, that match files and"
0.3.2,directories to ignore when looking for source files.
0.3.2,The reST default role (used for this markup: `text`) to use for all
0.3.2,documents.
0.3.2,default_role = None
0.3.2,"If true, '()' will be appended to :func: etc. cross-reference text."
0.3.2,"If true, the current module name will be prepended to all description"
0.3.2,unit titles (such as .. function::).
0.3.2,add_module_names = True
0.3.2,"If true, sectionauthor and moduleauthor directives will be shown in the"
0.3.2,output. They are ignored by default.
0.3.2,show_authors = False
0.3.2,The name of the Pygments (syntax highlighting) style to use.
0.3.2,Custom style
0.3.2,A list of ignored prefixes for module index sorting.
0.3.2,modindex_common_prefix = []
0.3.2,"If true, keep warnings as ""system message"" paragraphs in the built documents."
0.3.2,keep_warnings = False
0.3.2,-- Options for HTML output ----------------------------------------------
0.3.2,The theme to use for HTML and HTML Help pages.  See the documentation for
0.3.2,a list of builtin themes.
0.3.2,Theme options are theme-specific and customize the look and feel of a theme
0.3.2,"further.  For a list of options available for each theme, see the"
0.3.2,documentation.
0.3.2,html_theme_options = {'prev_next_buttons_location': None}
0.3.2,"Add any paths that contain custom themes here, relative to this directory."
0.3.2,"The name for this set of Sphinx documents.  If None, it defaults to"
0.3.2,"""<project> v<release> documentation""."
0.3.2,html_title = None
0.3.2,A shorter title for the navigation bar.  Default is the same as html_title.
0.3.2,html_short_title = None
0.3.2,The name of an image file (relative to this directory) to place at the top
0.3.2,of the sidebar.
0.3.2,html_logo = None
0.3.2,The name of an image file (within the static path) to use as favicon of the
0.3.2,docs.  This file should be a Windows icon file (.ico) being 16x16 or 32x32
0.3.2,pixels large.
0.3.2,html_favicon = None
0.3.2,"Add any paths that contain custom static files (such as style sheets) here,"
0.3.2,"relative to this directory. They are copied after the builtin static files,"
0.3.2,"so a file named ""default.css"" will overwrite the builtin ""default.css""."
0.3.2,Add any extra paths that contain custom files (such as robots.txt or
0.3.2,".htaccess) here, relative to this directory. These files are copied"
0.3.2,directly to the root of the documentation.
0.3.2,html_extra_path = []
0.3.2,"If not '', a 'Last updated on:' timestamp is inserted at every page bottom,"
0.3.2,using the given strftime format.
0.3.2,"html_last_updated_fmt = '%b %d, %Y'"
0.3.2,"If true, SmartyPants will be used to convert quotes and dashes to"
0.3.2,typographically correct entities.
0.3.2,html_use_smartypants = True
0.3.2,"Custom sidebar templates, maps document names to template names."
0.3.2,html_sidebars = {}
0.3.2,"Additional templates that should be rendered to pages, maps page names to"
0.3.2,template names.
0.3.2,html_additional_pages = {}
0.3.2,"If false, no module index is generated."
0.3.2,html_domain_indices = True
0.3.2,"If false, no index is generated."
0.3.2,html_use_index = True
0.3.2,"If true, the index is split into individual pages for each letter."
0.3.2,html_split_index = False
0.3.2,"If true, links to the reST sources are added to the pages."
0.3.2,html_show_sourcelink = True
0.3.2,"If true, ""Created using Sphinx"" is shown in the HTML footer. Default is True."
0.3.2,html_show_sphinx = True
0.3.2,"If true, ""(C) Copyright ..."" is shown in the HTML footer. Default is True."
0.3.2,html_show_copyright = True
0.3.2,"If true, an OpenSearch description file will be output, and all pages will"
0.3.2,contain a <link> tag referring to it.  The value of this option must be the
0.3.2,base URL from which the finished HTML is served.
0.3.2,html_use_opensearch = ''
0.3.2,"This is the file name suffix for HTML files (e.g. "".xhtml"")."
0.3.2,html_file_suffix = None
0.3.2,Output file base name for HTML help builder.
0.3.2,-- Options for LaTeX output ---------------------------------------------
0.3.2,The paper size ('letterpaper' or 'a4paper').
0.3.2,"'papersize': 'letterpaper',"
0.3.2,"The font size ('10pt', '11pt' or '12pt')."
0.3.2,"'pointsize': '10pt',"
0.3.2,Additional stuff for the LaTeX preamble.
0.3.2,"'preamble': '',"
0.3.2,Grouping the document tree into LaTeX files. List of tuples
0.3.2,"(source start file, target name, title,"
0.3.2,"author, documentclass [howto, manual, or own class])."
0.3.2,The name of an image file (relative to this directory) to place at the top of
0.3.2,the title page.
0.3.2,latex_logo = None
0.3.2,"For ""manual"" documents, if this is true, then toplevel headings are parts,"
0.3.2,not chapters.
0.3.2,latex_use_parts = False
0.3.2,"If true, show page references after internal links."
0.3.2,latex_show_pagerefs = False
0.3.2,"If true, show URL addresses after external links."
0.3.2,latex_show_urls = False
0.3.2,Documents to append as an appendix to all manuals.
0.3.2,latex_appendices = []
0.3.2,"If false, no module index is generated."
0.3.2,latex_domain_indices = True
0.3.2,-- Options for manual page output ---------------------------------------
0.3.2,One entry per manual page. List of tuples
0.3.2,"(source start file, name, description, authors, manual section)."
0.3.2,"If true, show URL addresses after external links."
0.3.2,man_show_urls = False
0.3.2,-- Options for Texinfo output -------------------------------------------
0.3.2,Grouping the document tree into Texinfo files. List of tuples
0.3.2,"(source start file, target name, title, author,"
0.3.2,"dir menu entry, description, category)"
0.3.2,"generate empty examples files, so that we don't get"
0.3.2,inclusion errors if there are no examples for a class / module
0.3.2,touch file
0.3.2,Config for sphinx_issues
0.3.2,Documents to append as an appendix to all manuals.
0.3.2,texinfo_appendices = []
0.3.2,"If false, no module index is generated."
0.3.2,texinfo_domain_indices = True
0.3.2,"How to display URL addresses: 'footnote', 'no', or 'inline'."
0.3.2,texinfo_show_urls = 'footnote'
0.3.2,"If true, do not generate a @detailmenu in the ""Top"" node's menu."
0.3.2,texinfo_no_detailmenu = False
0.3.2,Example configuration for intersphinx: refer to the Python standard library.
0.3.2,The following is used by sphinx.ext.linkcode to provide links to github
0.3.2,get the styles from the current theme
0.3.2,create and add the button to all the code blocks that contain >>>
0.3.2,tracebacks (.gt) contain bare text elements that need to be
0.3.2,wrapped in a span to work with .nextUntil() (see later)
0.3.2,define the behavior of the button when it's clicked
0.3.2,hide the code output
0.3.2,show the code output
0.3.2,-*- coding: utf-8 -*-
0.3.2,Format template for issues URI
0.3.2,e.g. 'https://github.com/sloria/marshmallow/issues/{issue}
0.3.2,"Shortcut for Github, e.g. 'sloria/marshmallow'"
0.3.2,Format template for user profile URI
0.3.2,e.g. 'https://github.com/{user}'
0.3.2,Python 2 only
0.3.2,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.2,License: MIT
0.3.2,##############################################################################
0.3.2,Creation of an imbalanced data set from a balanced data set
0.3.2,##############################################################################
0.3.2,##############################################################################
0.3.2,We will show how to use the parameter ``ratio`` when dealing with the
0.3.2,"``make_imbalance`` function. For this function, this parameter accepts both"
0.3.2,"dictionary and callable. When using a dictionary, each key will correspond to"
0.3.2,the class of interest and the corresponding value will be the number of
0.3.2,samples desired in this class.
0.3.2,##############################################################################
0.3.2,You might required more flexibility and require your own heuristic to
0.3.2,determine the number of samples by class and you can define your own callable
0.3.2,as follow. In this case we will define a function which will use a float
0.3.2,multiplier to define the number of samples per class.
0.3.2,##############################################################################
0.3.2,Using ``ratio`` in resampling algorithm
0.3.2,##############################################################################
0.3.2,##############################################################################
0.3.2,"In all sampling algorithms, ``ratio`` can be used as illustrated earlier. In"
0.3.2,"addition, some predefined functions are available and can be executed using a"
0.3.2,``str`` with the following choices: (i) ``'minority'``: resample the minority
0.3.2,"class; (ii) ``'majority'``: resample the majority class, (iii) ``'not"
0.3.2,"minority'``: resample all classes apart of the minority class, (iv)"
0.3.2,"``'all'``: resample all classes, and (v) ``'auto'``: correspond to 'all' with"
0.3.2,for over-sampling methods and 'not minority' for under-sampling methods. The
0.3.2,classes targeted will be over-sampled or under-sampled to achieve an equal
0.3.2,number of sample with the majority or minority class.
0.3.2,##############################################################################
0.3.2,"However, you can use the dictionary or the callable options as previously"
0.3.2,mentioned.
0.3.2,Authors: Fernando Nogueira
0.3.2,Christos Aridas
0.3.2,Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.2,License: MIT
0.3.2,Generate the dataset
0.3.2,Instanciate a PCA object for the sake of easy visualisation
0.3.2,Fit and transform x to visualise inside a 2D feature space
0.3.2,Apply regular SMOTE
0.3.2,"Two subplots, unpack the axes array immediately"
0.3.2,Remove axis for second plot
0.3.2,Authors: Christos Aridas
0.3.2,Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.2,License: MIT
0.3.2,Generate the dataset
0.3.2,Instanciate a PCA object for the sake of easy visualisation
0.3.2,Fit and transform x to visualise inside a 2D feature space
0.3.2,Apply the random over-sampling
0.3.2,"Two subplots, unpack the axes array immediately"
0.3.2,make nice plotting
0.3.2,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.2,License: MIT
0.3.2,generate some data points
0.3.2,plot the majority and minority samples
0.3.2,draw the circle in which the new sample will generated
0.3.2,plot the line on which the sample will be generated
0.3.2,create and plot the new sample
0.3.2,make the plot nicer with legend and label
0.3.2,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.2,License: MIT
0.3.2,##############################################################################
0.3.2,The following function will be used to create toy dataset. It using the
0.3.2,``make_classification`` from scikit-learn but fixing some parameters.
0.3.2,##############################################################################
0.3.2,The following function will be used to plot the sample space after resampling
0.3.2,to illustrate the characterisitic of an algorithm.
0.3.2,make nice plotting
0.3.2,##############################################################################
0.3.2,The following function will be used to plot the decision function of a
0.3.2,classifier given some data.
0.3.2,##############################################################################
0.3.2,Illustration of the influence of the balancing ratio
0.3.2,##############################################################################
0.3.2,##############################################################################
0.3.2,We will first illustrate the influence of the balancing ratio on some toy
0.3.2,data using a linear SVM classifier. Greater is the difference between the
0.3.2,"number of samples in each class, poorer are the classfication results."
0.3.2,##############################################################################
0.3.2,Random over-sampling to balance the data set
0.3.2,##############################################################################
0.3.2,##############################################################################
0.3.2,Random over-sampling can be used to repeat some samples and balance the
0.3.2,number of samples between the dataset. It can be seen that with this trivial
0.3.2,approach the boundary decision is already less biaised toward the majority
0.3.2,class.
0.3.2,##############################################################################
0.3.2,More advanced over-sampling using ADASYN and SMOTE
0.3.2,##############################################################################
0.3.2,##############################################################################
0.3.2,"Instead of repeating the same samples when over-sampling, we can use some"
0.3.2,specific heuristic instead. ADASYN and SMOTE can be used in this case.
0.3.2,Make an identity sampler
0.3.2,##############################################################################
0.3.2,The following plot illustrate the difference between ADASYN and SMOTE. ADASYN
0.3.2,will focus on the samples which are difficult to classify with a
0.3.2,nearest-neighbors rule while regular SMOTE will not make any distinction.
0.3.2,"Therefore, the decision function depending of the algorithm."
0.3.2,##############################################################################
0.3.2,"Due to those sampling particularities, it can give rise to some specific"
0.3.2,issues as illustrated below.
0.3.2,##############################################################################
0.3.2,SMOTE proposes several variants by identifying specific samples to consider
0.3.2,during the resampling. The borderline version will detect which point to
0.3.2,select which are in the border between two classes. The SVM version will use
0.3.2,the support vectors found using an SVM algorithm to create new samples.
0.3.2,Authors: Christos Aridas
0.3.2,Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.2,License: MIT
0.3.2,Generate the dataset
0.3.2,Instanciate a PCA object for the sake of easy visualisation
0.3.2,Fit and transform x to visualise inside a 2D feature space
0.3.2,Apply the random over-sampling
0.3.2,"Two subplots, unpack the axes array immediately"
0.3.2,make nice plotting
0.3.2,Authors: Christos Aridas
0.3.2,Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.2,License: MIT
0.3.2,Generate the dataset
0.3.2,make nice plotting
0.3.2,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.2,License: MIT
0.3.2,Generate a dataset
0.3.2,Split the data
0.3.2,Train the classifier with balancing
0.3.2,Test the classifier and get the prediction
0.3.2,Show the classification report
0.3.2,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.2,License: MIT
0.3.2,Generate a dataset
0.3.2,Split the data
0.3.2,Train the classifier with balancing
0.3.2,Test the classifier and get the prediction
0.3.2,##############################################################################
0.3.2,The geometric mean corresponds to the square root of the product of the
0.3.2,sensitivity and specificity. Combining the two metrics should account for
0.3.2,the balancing of the dataset.
0.3.2,##############################################################################
0.3.2,The index balanced accuracy can transform any metric to be used in
0.3.2,imbalanced learning problems.
0.3.2,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.2,License: MIT
0.3.2,##############################################################################
0.3.2,The following function will be used to create toy dataset. It using the
0.3.2,``make_classification`` from scikit-learn but fixing some parameters.
0.3.2,##############################################################################
0.3.2,The following function will be used to plot the sample space after resampling
0.3.2,to illustrate the characteristic of an algorithm.
0.3.2,make nice plotting
0.3.2,##############################################################################
0.3.2,The following function will be used to plot the decision function of a
0.3.2,classifier given some data.
0.3.2,##############################################################################
0.3.2,"``SMOTE`` allows to generate samples. However, this method of over-sampling"
0.3.2,"does not have any knowledge regarding the underlying distribution. Therefore,"
0.3.2,"some noisy samples can be generated, e.g. when the different classes cannot"
0.3.2,"be well separated. Hence, it can be beneficial to apply an under-sampling"
0.3.2,algorithm to clean the noisy samples. Two methods are usually used in the
0.3.2,literature: (i) Tomek's link and (ii) edited nearest neighbours cleaning
0.3.2,methods. Imbalanced-learn provides two ready-to-use samplers ``SMOTETomek``
0.3.2,"and ``SMOTEENN``. In general, ``SMOTEENN`` cleans more noisy data than"
0.3.2,``SMOTETomek``.
0.3.2,Authors: Christos Aridas
0.3.2,Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.2,License: MIT
0.3.2,Generate the dataset
0.3.2,Instanciate a PCA object for the sake of easy visualisation
0.3.2,Fit and transform x to visualise inside a 2D feature space
0.3.2,Apply SMOTE + ENN
0.3.2,"Two subplots, unpack the axes array immediately"
0.3.2,make nice plotting
0.3.2,Authors: Christos Aridas
0.3.2,Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.2,License: MIT
0.3.2,Generate the dataset
0.3.2,Instanciate a PCA object for the sake of easy visualisation
0.3.2,Fit and transform x to visualise inside a 2D feature space
0.3.2,Apply SMOTE + Tomek links
0.3.2,"Two subplots, unpack the axes array immediately"
0.3.2,make nice plotting
0.3.2,Authors: Christos Aridas
0.3.2,Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.2,License: MIT
0.3.2,Generate the dataset
0.3.2,Instanciate a PCA object for the sake of easy visualisation
0.3.2,Fit and transform x to visualise inside a 2D feature space
0.3.2,Apply Balance Cascade method
0.3.2,"Two subplots, unpack the axes array immediately"
0.3.2,make nice plotting
0.3.2,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.2,License: MIT
0.3.2,##############################################################################
0.3.2,Turning the balanced bagging classifier into a balanced random forest
0.3.2,##############################################################################
0.3.2,It is possible to turn the ``BalancedBaggingClassifier`` into a balanced
0.3.2,random forest by using a ``DecisionTreeClassifier`` with
0.3.2,``max_features='auto'``. We illustrate such changes below.
0.3.2,Authors: Christos Aridas
0.3.2,Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.2,License: MIT
0.3.2,Generate the dataset
0.3.2,Instanciate a PCA object for the sake of easy visualisation
0.3.2,Fit and transform x to visualise inside a 2D feature space
0.3.2,Apply Easy Ensemble
0.3.2,"Two subplots, unpack the axes array immediately"
0.3.2,make nice plotting
0.3.2,Authors: Andreas Mueller
0.3.2,Christos Aridas
0.3.2,Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.2,License: MIT
0.3.2,remove Tomek links
0.3.2,make nice plotting
0.3.2,Authors: Dayvid Oliveira
0.3.2,Christos Aridas
0.3.2,Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.2,License: MIT
0.3.2,Generate the dataset
0.3.2,Instanciate a PCA object for the sake of easy visualisation
0.3.2,Fit and transform x to visualise inside a 2D feature space
0.3.2,"Three subplots, unpack the axes array immediately"
0.3.2,Apply the ENN
0.3.2,Apply the RENN
0.3.2,Apply the AllKNN
0.3.2,Authors: Christos Aridas
0.3.2,Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.2,License: MIT
0.3.2,Generate the dataset
0.3.2,Instanciate a PCA object for the sake of easy visualisation
0.3.2,Fit and transform x to visualise inside a 2D feature space
0.3.2,Apply Nearmiss
0.3.2,"Two subplots, unpack the axes array immediately"
0.3.2,plot the missing samples
0.3.2,Authors: Christos Aridas
0.3.2,Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.2,License: MIT
0.3.2,Generate the dataset
0.3.2,Instanciate a PCA object for the sake of easy visualisation
0.3.2,Fit and transform x to visualise inside a 2D feature space
0.3.2,Apply One-Sided Selection
0.3.2,make nice plotting
0.3.2,Authors: Christos Aridas
0.3.2,Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.2,License: MIT
0.3.2,Generate the dataset
0.3.2,Instanciate a PCA object for the sake of easy visualisation
0.3.2,Fit and transform x to visualise inside a 2D feature space
0.3.2,Apply neighbourhood cleaning rule
0.3.2,make nice plotting
0.3.2,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.2,License: MIT
0.3.2,##############################################################################
0.3.2,The following function will be used to create toy dataset. It using the
0.3.2,``make_classification`` from scikit-learn but fixing some parameters.
0.3.2,##############################################################################
0.3.2,The following function will be used to plot the sample space after resampling
0.3.2,to illustrate the characteristic of an algorithm.
0.3.2,make nice plotting
0.3.2,##############################################################################
0.3.2,The following function will be used to plot the decision function of a
0.3.2,classifier given some data.
0.3.2,##############################################################################
0.3.2,Prototype generation: under-sampling by generating new samples
0.3.2,##############################################################################
0.3.2,##############################################################################
0.3.2,``ClusterCentroids`` under-samples by replacing the original samples by the
0.3.2,centroids of the cluster found.
0.3.2,##############################################################################
0.3.2,Prototype selection: under-sampling by selecting existing samples
0.3.2,##############################################################################
0.3.2,##############################################################################
0.3.2,The algorithm performing prototype selection can be subdivided into two
0.3.2,groups: (i) the controlled under-sampling methods and (ii) the cleaning
0.3.2,under-sampling methods.
0.3.2,##############################################################################
0.3.2,"With the controlled under-sampling methods, the number of samples to be"
0.3.2,selected can be specified. ``RandomUnderSampler`` is the most naive way of
0.3.2,performing such selection by randomly selecting a given number of samples by
0.3.2,the targetted class.
0.3.2,##############################################################################
0.3.2,``NearMiss`` algorithms implement some heuristic rules in order to select
0.3.2,samples. NearMiss-1 selects samples from the majority class for which the
0.3.2,average distance of the :math:`k`` nearest samples of the minority class is
0.3.2,the smallest. NearMiss-2 selects the samples from the majority class for
0.3.2,which the average distance to the farthest samples of the negative class is
0.3.2,"the smallest. NearMiss-3 is a 2-step algorithm: first, for each minority"
0.3.2,"sample, their ::math:`m` nearest-neighbors will be kept; then, the majority"
0.3.2,samples selected are the on for which the average distance to the :math:`k`
0.3.2,nearest neighbors is the largest.
0.3.2,##############################################################################
0.3.2,``EditedNearestNeighbours`` removes samples of the majority class for which
0.3.2,their class differ from the one of their nearest-neighbors. This sieve can be
0.3.2,repeated which is the principle of the
0.3.2,``RepeatedEditedNearestNeighbours``. ``AllKNN`` is slightly different from
0.3.2,the ``RepeatedEditedNearestNeighbours`` by changing the :math:`k` parameter
0.3.2,"of the internal nearest neighors algorithm, increasing it at each iteration."
0.3.2,##############################################################################
0.3.2,``CondensedNearestNeighbour`` makes use of a 1-NN to iteratively decide if a
0.3.2,sample should be kept in a dataset or not. The issue is that
0.3.2,``CondensedNearestNeighbour`` is sensitive to noise by preserving the noisy
0.3.2,samples. ``OneSidedSelection`` also used the 1-NN and use ``TomekLinks`` to
0.3.2,remove the samples considered noisy. The ``NeighbourhoodCleaningRule`` use a
0.3.2,"``EditedNearestNeighbours`` to remove some sample. Additionally, they use a 3"
0.3.2,nearest-neighbors to remove samples which do not agree with this rule.
0.3.2,##############################################################################
0.3.2,``InstanceHardnessThreshold`` uses the prediction of classifier to exclude
0.3.2,samples. All samples which are classified with a low probability will be
0.3.2,removed.
0.3.2,Authors: Christos Aridas
0.3.2,Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.2,License: MIT
0.3.2,Generate the dataset
0.3.2,Instanciate a PCA object for the sake of easy visualisation
0.3.2,Fit and transform x to visualise inside a 2D feature space
0.3.2,Apply Condensed Nearest Neighbours
0.3.2,make nice plotting
0.3.2,Authors: Fernando Nogueira
0.3.2,Christos Aridas
0.3.2,Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.2,License: MIT
0.3.2,Generate the dataset
0.3.2,Instanciate a PCA object for the sake of easy visualisation
0.3.2,Fit and transform x to visualise inside a 2D feature space
0.3.2,Apply Cluster Centroids
0.3.2,Use hard voting instead of soft voting
0.3.2,"Two subplots, unpack the axes array immediately"
0.3.2,make nice plotting
0.3.2,##############################################################################
0.3.2,This function allows to make nice plotting
0.3.2,##############################################################################
0.3.2,Generate some data with one Tomek link
0.3.2,minority class
0.3.2,majority class
0.3.2,##############################################################################
0.3.2,"In the figure above, the samples highlighted in green form a Tomek link since"
0.3.2,they are of different classes and are nearest neighbours of each other.
0.3.2,highlight the samples of interest
0.3.2,##############################################################################
0.3.2,We can run the ``TomekLinks`` sampling to remove the corresponding
0.3.2,samples. If ``ratio='auto'`` only the sample from the majority class will be
0.3.2,removed. If ``ratio='all'`` both samples will be removed.
0.3.2,highlight the samples of interest
0.3.2,Authors: Christos Aridas
0.3.2,Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.2,License: MIT
0.3.2,Generate the dataset
0.3.2,Instanciate a PCA object for the sake of easy visualisation
0.3.2,Fit and transform x to visualise inside a 2D feature space
0.3.2,Apply the random under-sampling
0.3.2,make nice plotting
0.3.2,Authors: Dayvid Oliveira
0.3.2,Christos Aridas
0.3.2,Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.2,License: MIT
0.3.2,Generate the dataset
0.3.2,"Two subplots, unpack the axes array immediately"
0.3.2,plot samples which have been removed
0.3.2,##############################################################################
0.3.2,This function allows to make nice plotting
0.3.2,##############################################################################
0.3.2,We can start by generating some data to later illustrate the principle of
0.3.2,each NearMiss heuritic rules.
0.3.2,minority class
0.3.2,majority class
0.3.2,##############################################################################
0.3.2,NearMiss-1
0.3.2,##############################################################################
0.3.2,##############################################################################
0.3.2,NearMiss-1 selects samples from the majority class for which the average
0.3.2,distance to some nearest neighbours is the smallest. In the following
0.3.2,"example, we use a 3-NN to compute the average distance on 2 specific samples"
0.3.2,"of the majority class. Therefore, in this case the point linked by the"
0.3.2,green-dashed line will be selected since the average distance is smaller.
0.3.2,##############################################################################
0.3.2,NearMiss-2
0.3.2,##############################################################################
0.3.2,##############################################################################
0.3.2,NearMiss-2 selects samples from the majority class for which the average
0.3.2,distance to the farthest neighbors is the smallest. With the same
0.3.2,"configuration as previously presented, the sample linked to the green-dashed"
0.3.2,line will be selected since its distance the 3 farthest neighbors is the
0.3.2,smallest.
0.3.2,##############################################################################
0.3.2,NearMiss-3
0.3.2,##############################################################################
0.3.2,##############################################################################
0.3.2,"NearMiss-3 can be divided into 2 steps. First, a nearest-neighbors is used to"
0.3.2,short-list samples from the majority class (i.e. correspond to the
0.3.2,"highlighted samples in the following plot). Then, the sample with the largest"
0.3.2,average distance to the *k* nearest-neighbors are selected.
0.3.2,select only the majority point of interest
0.3.2,Authors: Christos Aridas
0.3.2,Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.2,License: MIT
0.3.2,Generate the dataset
0.3.2,Instanciate a PCA object for the sake of easy visualisation
0.3.2,Create the samplers
0.3.2,Create the classifier
0.3.2,Make the splits
0.3.2,Add one transformers and two samplers in the pipeline object
0.3.2,Authors: Christos Aridas
0.3.2,Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.2,License: MIT
0.3.2,Load the dataset
0.3.2,make nice plotting
0.3.2,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.2,License: MIT
0.3.2,Create a folder to fetch the dataset
0.3.2,Create a pipeline
0.3.2,Classify and report the results
0.3.2,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.2,License: MIT
0.3.2,##############################################################################
0.3.2,Setting the data set
0.3.2,##############################################################################
0.3.2,##############################################################################
0.3.2,We use a part of the 20 newsgroups data set by loading 4 topics. Using the
0.3.2,"scikit-learn loader, the data are split into a training and a testing set."
0.3.2,
0.3.2,Note the class \#3 is the minority class and has almost twice less samples
0.3.2,than the majority class.
0.3.2,##############################################################################
0.3.2,The usual scikit-learn pipeline
0.3.2,##############################################################################
0.3.2,##############################################################################
0.3.2,You might usually use scikit-learn pipeline by combining the TF-IDF
0.3.2,vectorizer to feed a multinomial naive bayes classifier. A classification
0.3.2,report summarized the results on the testing set.
0.3.2,
0.3.2,"As expected, the recall of the class \#3 is low mainly due to the class"
0.3.2,imbalanced.
0.3.2,##############################################################################
0.3.2,Balancing the class before classification
0.3.2,##############################################################################
0.3.2,##############################################################################
0.3.2,"To improve the prediction of the class \#3, it could be interesting to apply"
0.3.2,"a balancing before to train the naive bayes classifier. Therefore, we will"
0.3.2,use a ``RandomUnderSampler`` to equalize the number of samples in all the
0.3.2,classes before the training.
0.3.2,
0.3.2,It is also important to note that we are using the ``make_pipeline`` function
0.3.2,implemented in imbalanced-learn to properly handle the samplers.
0.3.2,##############################################################################
0.3.2,"Although the results are almost identical, it can be seen that the resampling"
0.3.2,allowed to correct the poor recall of the class \#3 at the cost of reducing
0.3.2,"the other metrics for the other classes. However, the overall results are"
0.3.2,slightly better.
0.3.2,Authors: Dayvid Oliveira
0.3.2,Christos Aridas
0.3.2,Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.2,License: MIT
0.3.2,Generate the dataset
0.3.2,"Two subplots, unpack the axes array immediately"
0.3.2,Based on NiLearn package
0.3.2,License: simplified BSD
0.3.2,"PEP0440 compatible formatted version, see:"
0.3.2,https://www.python.org/dev/peps/pep-0440/
0.3.2,
0.3.2,Generic release markers:
0.3.2,X.Y
0.3.2,X.Y.Z # For bugfix releases
0.3.2,
0.3.2,Admissible pre-release markers:
0.3.2,X.YaN # Alpha release
0.3.2,X.YbN # Beta release
0.3.2,X.YrcN # Release Candidate
0.3.2,X.Y # Final release
0.3.2,
0.3.2,Dev branch marker is: 'X.Y.dev' or 'X.Y.devN' where N is an integer.
0.3.2,'X.Y.dev0' is the canonical version of 'X.Y.dev'
0.3.2,
0.3.2,"This is a tuple to preserve order, so that dependencies are checked"
0.3.2,in some meaningful order (more => less 'core').  We avoid using
0.3.2,collections.OrderedDict to preserve Python 2.6 compatibility.
0.3.2,Avoid choking on modules with no __version__ attribute
0.3.2,Skip check only when installing and it's a module that
0.3.2,will be auto-installed.
0.3.2,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.2,License: MIT
0.3.2,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.2,Christos Aridas
0.3.2,License: MIT
0.3.2,Check the consistency of X and y
0.3.2,self.sampling_type is already checked in check_ratio
0.3.2,Adapted from scikit-learn
0.3.2,Author: Edouard Duchesnay
0.3.2,Gael Varoquaux
0.3.2,Virgile Fritsch
0.3.2,Alexandre Gramfort
0.3.2,Lars Buitinck
0.3.2,Christos Aridas
0.3.2,Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.2,License: BSD
0.3.2,BaseEstimator interface
0.3.2,shallow copy of steps
0.3.2,validate names
0.3.2,validate estimators
0.3.2,We allow last estimator to be None as an identity transformation
0.3.2,Estimator interface
0.3.2,Setup the memory
0.3.2,we do not clone when caching is disabled to preserve
0.3.2,backward compatibility
0.3.2,Fit or load from cache the current transfomer
0.3.2,Replace the transformer of the step with the fitted
0.3.2,transformer. This is necessary when loading the transformer
0.3.2,from the cache.
0.3.2,XXX: Calling sample in pipeline it means that the
0.3.2,last estimator is a sampler. Samplers don't carry
0.3.2,"the sampled data. So, call 'fit_sample' in all intermediate"
0.3.2,steps to get the sampled data for the last estimator.
0.3.2,"_final_estimator is None or has transform, otherwise attribute error"
0.3.2,raise AttributeError if necessary for hasattr behaviour
0.3.2,"if we have a weight for this transformer, multiply output"
0.3.2,Boolean controlling whether the joblib caches should be
0.3.2,"flushed if the version of certain modules changes (eg nibabel, as it"
0.3.2,does not respect the backward compatibility in some of its internal
0.3.2,structures
0.3.2,This  is used in nilearn._utils.cache_mixin
0.3.2,list all submodules available in imblearn and version
0.3.2,coding: utf-8
0.3.2,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.2,Dariusz Brzezinski
0.3.2,License: MIT
0.3.2,Only negative labels
0.3.2,"Calculate tp_sum, pred_sum, true_sum ###"
0.3.2,labels are now from 0 to len(labels) - 1 -> use bincount
0.3.2,Pathological case
0.3.2,Compute the true negative
0.3.2,Retain only selected labels
0.3.2,"Finally, we have all our sufficient statistics. Divide! #"
0.3.2,"Divide, and on zero-division, set scores to 0 and warn:"
0.3.2,"Oddly, we may get an ""invalid"" rather than a ""divide"" error"
0.3.2,here.
0.3.2,Average the results
0.3.2,labels are now from 0 to len(labels) - 1 -> use bincount
0.3.2,Pathological case
0.3.2,Retain only selected labels
0.3.2,old version of scipy return MaskedConstant instead of 0.0
0.3.2,Create the list of tags
0.3.2,check that the scoring function does not need a score
0.3.2,and only a prediction
0.3.2,Compute the score from the scoring function
0.3.2,Square if desired
0.3.2,Get the signature of the sens/spec function
0.3.2,We need to extract from kwargs only the one needed by the
0.3.2,specificity and specificity
0.3.2,Make the intersection between the parameters
0.3.2,Create a sub dictionary
0.3.2,Check if the metric is the geometric mean
0.3.2,We do not support multilabel so the only average supported
0.3.2,is binary
0.3.2,Create the list of parameters through signature binding
0.3.2,Call the sens/spec function
0.3.2,Compute the dominance
0.3.2,Compute the different metrics
0.3.2,Precision/recall/f1
0.3.2,Specificity
0.3.2,Geometric mean
0.3.2,Index balanced accuracy
0.3.2,compute averages
0.3.2,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.2,Christos Aridas
0.3.2,License: MIT
0.3.2,##############################################################################
0.3.2,Utilities for testing
0.3.2,import some data to play with
0.3.2,restrict to a binary classification task
0.3.2,add noisy features to make the problem harder and avoid perfect results
0.3.2,"run classifier, get class probabilities and label predictions"
0.3.2,only interested in probabilities of the positive case
0.3.2,XXX: do we really want a special API for the binary case?
0.3.2,##############################################################################
0.3.2,Tests
0.3.2,detailed measures for each class
0.3.2,individual scoring function that can be used for grid search: in the
0.3.2,binary class case the score is the value of the measure for the positive
0.3.2,class (e.g. label == 1). This is deprecated for average != 'binary'.
0.3.2,Such a case may occur with non-stratified cross-validation
0.3.2,No average: zeros in array
0.3.2,Macro average is changed
0.3.2,Check for micro
0.3.2,Check for weighted
0.3.2,ensure the above were meaningful tests:
0.3.2,Bad pos_label
0.3.2,Bad average option
0.3.2,but average != 'binary'; even if data is binary
0.3.2,compute the geometric mean for the binary problem
0.3.2,Compute the geometric mean for each of the classes
0.3.2,average tests
0.3.2,print classification report with class names
0.3.2,print classification report with label detection
0.3.2,print classification report with class names
0.3.2,print classification report with label detection
0.3.2,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.2,Christos Aridas
0.3.2,License: MIT
0.3.2,Get the version
0.3.2,sensitivity scorer
0.3.2,specificity scorer
0.3.2,geometric_mean scorer
0.3.2,make a iba metric before a scorer
0.3.2,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.2,Christos Aridas
0.3.2,License: MIT
0.3.2,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.2,Christos Aridas
0.3.2,License: MIT
0.3.2,FIXME: Deprecated in 0.2. To be removed in 0.4.
0.3.2,The ratio is computed using a one-vs-rest manner. Using majority
0.3.2,in multi-class would lead to slightly different results at the
0.3.2,cost of introducing a new parameter.
0.3.2,the nearest neighbors need to be fitted only on the current class
0.3.2,to find the class NN to generate new samples
0.3.2,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.2,Christos Aridas
0.3.2,License: MIT
0.3.2,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.2,Fernando Nogueira
0.3.2,Christos Aridas
0.3.2,License: MIT
0.3.2,Samples are in danger for m/2 <= m' < m
0.3.2,Samples are noise for m = m'
0.3.2,"FIXME Deprecated in 0.2, to be removed in 0.4"
0.3.2,divergence between borderline-1 and borderline-2
0.3.2,Create synthetic samples for borderline points.
0.3.2,only minority
0.3.2,we use a one-vs-rest policy to handle the multiclass in which
0.3.2,new samples will be created considering not only the majority
0.3.2,class but all over classes.
0.3.2,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.2,Christos Aridas
0.3.2,License: MIT
0.3.2,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.2,Christos Aridas
0.3.2,License: MIT
0.3.2,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.2,Christos Aridas
0.3.2,License: MIT
0.3.2,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.2,License: MIT
0.3.2,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.2,Fernando Nogueira
0.3.2,Christos Aridas
0.3.2,License: MIT
0.3.2,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.2,Christos Aridas
0.3.2,License: MIT
0.3.2,FIXME: Deprecated in 0.2. To be removed in 0.4.
0.3.2,select a sample from the current class
0.3.2,create the set composed of all minority samples and one
0.3.2,sample from the current class.
0.3.2,create the set S with removing the seed from S
0.3.2,since that it will be added anyway
0.3.2,apply Tomek cleaning
0.3.2,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.2,Christos Aridas
0.3.2,License: MIT
0.3.2,Compute the distance considering the farthest neighbour
0.3.2,Sort the list of distance and get the index
0.3.2,Throw a warning to tell the user that we did not have enough samples
0.3.2,to select and that we just select everything
0.3.2,Select the desired number of samples
0.3.2,FIXME: Deprecated in 0.2. To be removed in 0.4.
0.3.2,idx_tmp is relative to the feature selected in the
0.3.2,previous step and we need to find the indirection
0.3.2,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.2,Christos Aridas
0.3.2,License: MIT
0.3.2,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.2,Fernando Nogueira
0.3.2,Christos Aridas
0.3.2,License: MIT
0.3.2,find which class to not consider
0.3.2,there is a Tomek link between two samples if they are both nearest
0.3.2,neighbors of each others.
0.3.2,Find the nearest neighbour of every point
0.3.2,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.2,Christos Aridas
0.3.2,License: MIT
0.3.2,FIXME: Deprecated in 0.2. To be removed in 0.4
0.3.2,Randomly get one sample from the majority class
0.3.2,Generate the index to select
0.3.2,Create the set C - One majority samples and all minority
0.3.2,Create the set S - all majority samples
0.3.2,fit knn on C
0.3.2,Check each sample in S if we keep it or drop it
0.3.2,Do not select sample which are already well classified
0.3.2,Classify on S
0.3.2,If the prediction do not agree with the true label
0.3.2,append it in C_x
0.3.2,Keep the index for later
0.3.2,Update C
0.3.2,fit a knn on C
0.3.2,This experimental to speed up the search
0.3.2,Classify all the element in S and avoid to test the
0.3.2,well classified elements
0.3.2,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.2,Christos Aridas
0.3.2,License: MIT
0.3.2,FIXME: Deprecated from 0.2. To be removed in 0.4.
0.3.2,clean the neighborhood
0.3.2,compute which classes to consider for cleaning for the A2 group
0.3.2,compute a2 group
0.3.2,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.2,Dayvid Oliveira
0.3.2,Christos Aridas
0.3.2,License: MIT
0.3.2,FIXME: Deprecated in 0.2. To be removed in 0.4
0.3.2,Check the stopping criterion
0.3.2,1. If there is no changes for the vector y
0.3.2,2. If the number of samples in the other class become inferior to
0.3.2,the number of samples in the majority class
0.3.2,3. If one of the class is disappearing
0.3.2,Case 1
0.3.2,Case 2
0.3.2,Case 3
0.3.2,Check the stopping criterion
0.3.2,1. If the number of samples in the other class become inferior to
0.3.2,the number of samples in the majority class
0.3.2,2. If one of the class is disappearing
0.3.2,Case 1
0.3.2,overwrite b_min_bec_maj
0.3.2,Case 2
0.3.2,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.2,Dayvid Oliveira
0.3.2,Christos Aridas
0.3.2,License: MIT
0.3.2,To be removed in 0.4
0.3.2,Select the appropriate classifier
0.3.2,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.2,Christos Aridas
0.3.2,License: MIT
0.3.2,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.2,Christos Aridas
0.3.2,License: MIT
0.3.2,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.2,Christos Aridas
0.3.2,License: MIT
0.3.2,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.2,Christos Aridas
0.3.2,License: MIT
0.3.2,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.2,Christos Aridas
0.3.2,License: MIT
0.3.2,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.2,Christos Aridas
0.3.2,License: MIT
0.3.2,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.2,Christos Aridas
0.3.2,License: MIT
0.3.2,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.2,Christos Aridas
0.3.2,License: MIT
0.3.2,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.2,Christos Aridas
0.3.2,License: MIT
0.3.2,FIXME remove at the end of the deprecation 0.4
0.3.2,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.2,Christos Aridas
0.3.2,License: MIT
0.3.2,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.2,Christos Aridas
0.3.2,License: MIT
0.3.2,test that all_estimators doesn't find abstract classes.
0.3.2,some can just not be sensibly default constructed
0.3.2,input validation etc for non-meta estimators
0.3.2,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.2,Christos Aridas
0.3.2,License: MIT
0.3.2,store timestamp to figure out whether the result of 'fit' has been
0.3.2,cached or not
0.3.2,store timestamp to figure out whether the result of 'fit' has been
0.3.2,cached or not
0.3.2,Test the various init parameters of the pipeline.
0.3.2,Check that we can't instantiate pipelines with objects without fit
0.3.2,method
0.3.2,Smoke test with only an estimator
0.3.2,Check that params are set
0.3.2,Smoke test the repr:
0.3.2,Test with two objects
0.3.2,Check that we can't instantiate with non-transformers on the way
0.3.2,"Note that NoTrans implements fit, but not transform"
0.3.2,Check that params are set
0.3.2,Smoke test the repr:
0.3.2,Check that params are not set when naming them wrong
0.3.2,Test clone
0.3.2,"Check that apart from estimators, the parameters are the same"
0.3.2,Remove estimators that where copied
0.3.2,Test the various methods of the pipeline (anova).
0.3.2,Test with Anova + LogisticRegression
0.3.2,Test that the pipeline can take fit parameters
0.3.2,classifier should return True
0.3.2,and transformer params should not be changed
0.3.2,invalid parameters should raise an error message
0.3.2,Pipeline should pass sample_weight
0.3.2,When sample_weight is None it shouldn't be passed
0.3.2,Test pipeline raises set params error message for nested models.
0.3.2,nested model check
0.3.2,Test the various methods of the pipeline (pca + svm).
0.3.2,Test with PCA + SVC
0.3.2,Test the various methods of the pipeline (preprocessing + svm).
0.3.2,check shapes of various prediction functions
0.3.2,test that the fit_predict method is implemented on a pipeline
0.3.2,test that the fit_predict on pipeline yields same results as applying
0.3.2,transform and clustering steps separately
0.3.2,"As pipeline doesn't clone estimators on construction,"
0.3.2,it must have its own estimators
0.3.2,first compute the transform and clustering step separately
0.3.2,use a pipeline to do the transform and clustering in one step
0.3.2,tests that a pipeline does not have fit_predict method when final
0.3.2,step of pipeline does not have fit_predict defined
0.3.2,tests that Pipeline passes fit_params to intermediate steps
0.3.2,when fit_predict is invoked
0.3.2,Test whether pipeline works with a transformer at the end.
0.3.2,Also test pipeline.transform and pipeline.inverse_transform
0.3.2,test transform and fit_transform:
0.3.2,Test whether pipeline works with a transformer missing fit_transform
0.3.2,test fit_transform:
0.3.2,Directly setting attr
0.3.2,Using set_params
0.3.2,Using set_params to replace single step
0.3.2,With invalid data
0.3.2,Test setting Pipeline steps to None
0.3.2,"for other methods, ensure no AttributeErrors on None:"
0.3.2,mult2 and mult3 are active
0.3.2,Check None step at construction time
0.3.2,Test that an error is raised when memory is not a string or a Memory
0.3.2,instance
0.3.2,Define memory as an integer
0.3.2,Test with Transformer + SVC
0.3.2,Memoize the transformer at the first fit
0.3.2,Get the time stamp of the tranformer in the cached pipeline
0.3.2,Check that cached_pipe and pipe yield identical results
0.3.2,Check that we are reading the cache while fitting
0.3.2,a second time
0.3.2,Check that cached_pipe and pipe yield identical results
0.3.2,Create a new pipeline with cloned estimators
0.3.2,Check that even changing the name step does not affect the cache hit
0.3.2,Check that cached_pipe and pipe yield identical results
0.3.2,Test with Transformer + SVC
0.3.2,Memoize the transformer at the first fit
0.3.2,Get the time stamp of the tranformer in the cached pipeline
0.3.2,Check that cached_pipe and pipe yield identical results
0.3.2,Check that we are reading the cache while fitting
0.3.2,a second time
0.3.2,Check that cached_pipe and pipe yield identical results
0.3.2,Create a new pipeline with cloned estimators
0.3.2,Check that even changing the name step does not affect the cache hit
0.3.2,Check that cached_pipe and pipe yield identical results
0.3.2,Test the various methods of the pipeline (pca + svm).
0.3.2,Test with PCA + SVC
0.3.2,Test the various methods of the pipeline (pca + svm).
0.3.2,Test with PCA + SVC
0.3.2,Test whether pipeline works with a sampler at the end.
0.3.2,Also test pipeline.sampler
0.3.2,test transform and fit_transform:
0.3.2,We round the value near to zero. It seems that PCA has some issue
0.3.2,with that
0.3.2,Test whether pipeline works with a sampler at the end.
0.3.2,Also test pipeline.sampler
0.3.2,Test pipeline using None as preprocessing step and a classifier
0.3.2,"Test pipeline using None, RUS and a classifier"
0.3.2,"Test pipeline using RUS, None and a classifier"
0.3.2,Test pipeline using None step and a sampler
0.3.2,Test pipeline using None and a transformer that implements transform and
0.3.2,inverse_transform
0.3.2,Test the various methods of the pipeline (anova).
0.3.2,Test with RandomUnderSampling + Anova + LogisticRegression
0.3.2,Test the various methods of the pipeline (anova).
0.3.2,Test the various methods of the pipeline (anova).
0.3.2,Test with RandomUnderSampling + Anova + LogisticRegression
0.3.2,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.2,Christos Aridas
0.3.2,License: MIT
0.3.2,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.2,License: MIT
0.3.2,Adapated from scikit-learn
0.3.2,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.2,License: MIT
0.3.2,make the checks from scikit-learn
0.3.2,trigger our checks if this is a SamplerMixin
0.3.2,FIXME already present in scikit-learn 0.19
0.3.2,test scikit-learn compatibility
0.3.2,Estimators in mono_output_task_error raise ValueError if y is of 1-D
0.3.2,Convert into a 2-D y for those estimators.
0.3.2,check that fit method only changes or sets private attributes
0.3.2,to not check deprecated classes
0.3.2,check that fit doesn't add any public attribute
0.3.2,check that fit doesn't change any public attribute
0.3.2,in this test we will force all samplers to not change the class 1
0.3.2,check that sparse matrices can be passed through the sampler leading to
0.3.2,the same results than dense
0.3.2,set KMeans to full since it support sparse and dense
0.3.2,Check that the samplers handle pandas dataframe and pandas series
0.3.2,Adapted from scikit-learn
0.3.2,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.2,License: MIT
0.3.2,meta-estimators need another estimator to be instantiated.
0.3.2,estimators that there is no way to default-construct sensibly
0.3.2,some strange ones
0.3.2,get parent folder
0.3.2,get rid of abstract base classes
0.3.2,get rid of sklearn estimators which have been imported in some classes
0.3.2,possibly get rid of meta estimators
0.3.2,"drop duplicates, sort for reproducibility"
0.3.2,itemgetter is used to ensure the sort does not extend to the 2nd item of
0.3.2,the tuple
0.3.2,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.2,License: MIT
0.3.2,FIXME: perfectly we should raise an error but the sklearn API does
0.3.2,not allow for it
0.3.2,check that all keys in ratio are also in y
0.3.2,check that there is no negative number
0.3.2,clean-sampling can be more permissive since those samplers do not
0.3.2,use samples
0.3.2,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.2,Christos Aridas
0.3.2,License: MIT
0.3.2,this function could create an equal number of samples
0.3.2,"tests that the estimator actually fails on ""bad"" estimators."
0.3.2,"not a complete test of all checks, which are very extensive."
0.3.2,check that we have a set_params and can clone
0.3.2,check that we have a fit method
0.3.2,check that fit does input validation
0.3.2,check that predict does input validation (doesn't accept dicts in input)
0.3.2,check that estimator state does not change
0.3.2,at transform/predict/predict_proba time
0.3.2,check that `fit` only changes attributes that
0.3.2,are private (start with an _ or end with a _).
0.3.2,check that `fit` doesn't add any public attribute
0.3.2,check for sparse matrix input handling
0.3.2,"the check for sparse input handling prints to the stdout,"
0.3.2,"instead of raising an error, so as not to remove the original traceback."
0.3.2,that means we need to jump through some hoops to catch it.
0.3.2,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.2,License: MIT
0.3.2,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.2,Christos Aridas
0.3.2,License: MIT
0.3.2,check if the filtering is working with a list or a single string
0.3.2,check that all estimators are sampler
0.3.2,check that an error is raised when the type is unknown
0.3.2,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.2,Christos Aridas
0.3.2,License: MIT
0.3.2,Check any parameters for SMOTE was provided
0.3.2,Anounce deprecation
0.3.2,We need to list each parameter and decide if we affect a default
0.3.2,value or not
0.3.2,"If an object was given, affect"
0.3.2,Otherwise create a default SMOTE
0.3.2,Check any parameters for ENN was provided
0.3.2,Anounce deprecation
0.3.2,We need to list each parameter and decide if we affect a default
0.3.2,value or not
0.3.2,"If an object was given, affect"
0.3.2,Otherwise create a default EditedNearestNeighbours
0.3.2,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.2,Christos Aridas
0.3.2,License: MIT
0.3.2,Check any parameters for SMOTE was provided
0.3.2,Anounce deprecation
0.3.2,We need to list each parameter and decide if we affect a default
0.3.2,value or not
0.3.2,"If an object was given, affect"
0.3.2,Otherwise create a default SMOTE
0.3.2,Check any parameters for ENN was provided
0.3.2,Anounce deprecation
0.3.2,"If an object was given, affect"
0.3.2,Otherwise create a default TomekLinks
0.3.2,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.2,Christos Aridas
0.3.2,License: MIT
0.3.2,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.2,Christos Aridas
0.3.2,License: MIT
0.3.2,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.2,Christos Aridas
0.3.2,License: MIT
0.3.2,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.2,Christos Aridas
0.3.2,License: MIT
0.3.2,RandomUnderSampler is not supporting sample_weight. We need to pass
0.3.2,None.
0.3.2,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.2,Christos Aridas
0.3.2,License: MIT
0.3.2,doctest: +ELLIPSIS
0.3.2,To be removed in 0.4
0.3.2,Define the classifier to use
0.3.2,array to know which samples are available to be taken
0.3.2,where the different set will be stored
0.3.2,store the index of the data to under-sample
0.3.2,value which will be picked at each round
0.3.2,extract the data of interest for this round from the
0.3.2,current class
0.3.2,select randomly the desired features
0.3.2,store the set created
0.3.2,fit and predict using cross validation
0.3.2,extract the prediction about the targeted classes only
0.3.2,check the stopping criterion
0.3.2,check that there is enough samples for another round
0.3.2,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.2,License: MIT
0.3.2,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.2,Christos Aridas
0.3.2,License: MIT
0.3.2,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.2,Christos Aridas
0.3.2,License: MIT
0.3.2,Check classification for various parameter settings.
0.3.2,Test that bootstrapping samples generate non-perfect base estimators.
0.3.2,"without bootstrap, all trees are perfect on the training set"
0.3.2,disable the resampling by passing an empty dictionary.
0.3.2,"with bootstrap, trees are no longer perfect on the training set"
0.3.2,Test that bootstrapping features may generate duplicate features.
0.3.2,Predict probabilities.
0.3.2,Normal case
0.3.2,"Degenerate case, where some classes are missing"
0.3.2,Check that oob prediction is a good estimation of the generalization
0.3.2,error.
0.3.2,Test with few estimators
0.3.2,Check singleton ensembles.
0.3.2,Test that it gives proper exception on deficient input.
0.3.2,Test n_estimators
0.3.2,Test max_samples
0.3.2,Test max_features
0.3.2,Test support of decision_function
0.3.2,Check that bagging ensembles can be grid-searched.
0.3.2,Transform iris into a binary classification task
0.3.2,Grid search with scoring based on decision_function
0.3.2,Check base_estimator and its default values.
0.3.2,Test if fitting incrementally with warm start gives a forest of the
0.3.2,right size and the same results as a normal fit.
0.3.2,Test if warm start'ed second fit with smaller n_estimators raises error.
0.3.2,Test that nothing happens when fitting without increasing n_estimators
0.3.2,"modify X to nonsense values, this should not change anything"
0.3.2,warm started classifier with 5+5 estimators should be equivalent to
0.3.2,one classifier with 10 estimators
0.3.2,Check using oob_score and warm_start simultaneously fails
0.3.2,"Make sure OOB scores are identical when random_state, estimator, and"
0.3.2,training data are fixed and fitting is done twice
0.3.2,FIXME: uncomment when #9723 is merged in scikit-learn
0.3.2,def test_estimators_samples():
0.3.2,# Check that format of estimators_samples_ is correct and that results
0.3.2,# generated at fit time can be identically reproduced at a later time
0.3.2,# using data saved in object attributes.
0.3.2,"X, y = make_hastie_10_2(n_samples=200, random_state=1)"
0.3.2,# remap the y outside of the BalancedBaggingclassifier
0.3.2,"# _, y = np.unique(y, return_inverse=True)"
0.3.2,"bagging = BalancedBaggingClassifier(LogisticRegression(),"
0.3.2,"max_samples=0.5,"
0.3.2,"max_features=0.5, random_state=1,"
0.3.2,bootstrap=False)
0.3.2,"bagging.fit(X, y)"
0.3.2,# Get relevant attributes
0.3.2,estimators_samples = bagging.estimators_samples_
0.3.2,estimators_features = bagging.estimators_features_
0.3.2,estimators = bagging.estimators_
0.3.2,# Test for correct formatting
0.3.2,assert len(estimators_samples) == len(estimators)
0.3.2,assert len(estimators_samples[0]) == len(X)
0.3.2,assert estimators_samples[0].dtype.kind == 'b'
0.3.2,# Re-fit single estimator to test for consistent sampling
0.3.2,estimator_index = 0
0.3.2,estimator_samples = estimators_samples[estimator_index]
0.3.2,estimator_features = estimators_features[estimator_index]
0.3.2,estimator = estimators[estimator_index]
0.3.2,"X_train = (X[estimator_samples])[:, estimator_features]"
0.3.2,y_train = y[estimator_samples]
0.3.2,orig_coefs = estimator.steps[-1][1].coef_
0.3.2,"estimator.fit(X_train, y_train)"
0.3.2,new_coefs = estimator.steps[-1][1].coef_
0.3.2,"assert_array_almost_equal(orig_coefs, new_coefs)"
0.3.2,Make sure validated max_samples and original max_samples are identical
0.3.2,when valid integer max_samples supplied by user
0.3.2,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.2,Christos Aridas
0.3.2,License: MIT
0.3.2,Generate a global dataset to use
0.3.2,Define a ratio
0.3.2,Define the ratio parameter
0.3.2,Create the sampling object
0.3.2,Get the different subset
0.3.2,Define the ratio parameter
0.3.2,Create the sampling object
0.3.2,Get the different subset
0.3.2,Define the ratio parameter
0.3.2,Create the sampling object
0.3.2,Get the different subset
0.3.2,Author: Guillaume Lemaitre
0.3.2,License: BSD 3 clause
0.3.2,"The index start at one, then we need to remove one"
0.3.2,to not have issue with the indexing.
0.3.2,go through the list and check if the data are available
0.3.2,Authors: Dayvid Oliveira
0.3.2,Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.2,Christos Aridas
0.3.2,License: MIT
0.3.2,restrict ratio to be a dict or a callable
0.3.2,FIXME: deprecated in 0.2 to be removed in 0.4
0.3.2,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.2,Christos Aridas
0.3.2,License: MIT
0.3.2,"we are reusing part of utils.check_ratio, however this is not cover in"
0.3.2,the common tests so we will repeat it here
0.3.2,FIXME: to be removed in 0.4 due to deprecation
0.3.2,resample without using min_c_
0.3.2,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.2,Christos Aridas
0.3.2,License: MIT
0.3.1,! /usr/bin/env python
0.3.1,"load all vars into globals, otherwise"
0.3.1,the later function call using global vars doesn't work.
0.3.1,"Allow command-lines such as ""python setup.py build install"""
0.3.1,Make sources available using relative paths from this file's directory.
0.3.1,-*- coding: utf-8 -*-
0.3.1,
0.3.1,"imbalanced-learn documentation build configuration file, created by"
0.3.1,sphinx-quickstart on Mon Jan 18 14:44:12 2016.
0.3.1,
0.3.1,This file is execfile()d with the current directory set to its
0.3.1,containing dir.
0.3.1,
0.3.1,Note that not all possible configuration values are present in this
0.3.1,autogenerated file.
0.3.1,
0.3.1,All configuration values have a default; values that are commented out
0.3.1,serve to show the default.
0.3.1,"If extensions (or modules to document with autodoc) are in another directory,"
0.3.1,add these directories to sys.path here. If the directory is relative to the
0.3.1,"documentation root, use os.path.abspath to make it absolute, like shown here."
0.3.1,"sys.path.insert(0, os.path.abspath('.'))"
0.3.1,-- General configuration ---------------------------------------------------
0.3.1,Try to override the matplotlib configuration as early as possible
0.3.1,-- General configuration ------------------------------------------------
0.3.1,If extensions (or modules to document with autodoc) are in another
0.3.1,"directory, add these directories to sys.path here. If the directory"
0.3.1,"is relative to the documentation root, use os.path.abspath to make it"
0.3.1,"absolute, like shown here."
0.3.1,"If your documentation needs a minimal Sphinx version, state it here."
0.3.1,needs_sphinx = '1.0'
0.3.1,"Add any Sphinx extension module names here, as strings. They can be"
0.3.1,extensions coming with Sphinx (named 'sphinx.ext.*') or your custom
0.3.1,ones.
0.3.1,path to your examples scripts
0.3.1,path where to save gallery generated examples
0.3.1,to make references clickable
0.3.1,"Add any paths that contain templates here, relative to this directory."
0.3.1,generate autosummary even if no references
0.3.1,The suffix of source filenames.
0.3.1,The encoding of source files.
0.3.1,source_encoding = 'utf-8-sig'
0.3.1,Generate the plots for the gallery
0.3.1,The master toctree document.
0.3.1,General information about the project.
0.3.1,"The version info for the project you're documenting, acts as replacement for"
0.3.1,"|version| and |release|, also used in various other places throughout the"
0.3.1,built documents.
0.3.1,
0.3.1,The short X.Y version.
0.3.1,"The full version, including alpha/beta/rc tags."
0.3.1,The language for content autogenerated by Sphinx. Refer to documentation
0.3.1,for a list of supported languages.
0.3.1,language = None
0.3.1,"There are two options for replacing |today|: either, you set today to some"
0.3.1,"non-false value, then it is used:"
0.3.1,today = ''
0.3.1,"Else, today_fmt is used as the format for a strftime call."
0.3.1,"today_fmt = '%B %d, %Y'"
0.3.1,"List of patterns, relative to source directory, that match files and"
0.3.1,directories to ignore when looking for source files.
0.3.1,The reST default role (used for this markup: `text`) to use for all
0.3.1,documents.
0.3.1,default_role = None
0.3.1,"If true, '()' will be appended to :func: etc. cross-reference text."
0.3.1,"If true, the current module name will be prepended to all description"
0.3.1,unit titles (such as .. function::).
0.3.1,add_module_names = True
0.3.1,"If true, sectionauthor and moduleauthor directives will be shown in the"
0.3.1,output. They are ignored by default.
0.3.1,show_authors = False
0.3.1,The name of the Pygments (syntax highlighting) style to use.
0.3.1,Custom style
0.3.1,A list of ignored prefixes for module index sorting.
0.3.1,modindex_common_prefix = []
0.3.1,"If true, keep warnings as ""system message"" paragraphs in the built documents."
0.3.1,keep_warnings = False
0.3.1,-- Options for HTML output ----------------------------------------------
0.3.1,The theme to use for HTML and HTML Help pages.  See the documentation for
0.3.1,a list of builtin themes.
0.3.1,Theme options are theme-specific and customize the look and feel of a theme
0.3.1,"further.  For a list of options available for each theme, see the"
0.3.1,documentation.
0.3.1,html_theme_options = {'prev_next_buttons_location': None}
0.3.1,"Add any paths that contain custom themes here, relative to this directory."
0.3.1,"The name for this set of Sphinx documents.  If None, it defaults to"
0.3.1,"""<project> v<release> documentation""."
0.3.1,html_title = None
0.3.1,A shorter title for the navigation bar.  Default is the same as html_title.
0.3.1,html_short_title = None
0.3.1,The name of an image file (relative to this directory) to place at the top
0.3.1,of the sidebar.
0.3.1,html_logo = None
0.3.1,The name of an image file (within the static path) to use as favicon of the
0.3.1,docs.  This file should be a Windows icon file (.ico) being 16x16 or 32x32
0.3.1,pixels large.
0.3.1,html_favicon = None
0.3.1,"Add any paths that contain custom static files (such as style sheets) here,"
0.3.1,"relative to this directory. They are copied after the builtin static files,"
0.3.1,"so a file named ""default.css"" will overwrite the builtin ""default.css""."
0.3.1,Add any extra paths that contain custom files (such as robots.txt or
0.3.1,".htaccess) here, relative to this directory. These files are copied"
0.3.1,directly to the root of the documentation.
0.3.1,html_extra_path = []
0.3.1,"If not '', a 'Last updated on:' timestamp is inserted at every page bottom,"
0.3.1,using the given strftime format.
0.3.1,"html_last_updated_fmt = '%b %d, %Y'"
0.3.1,"If true, SmartyPants will be used to convert quotes and dashes to"
0.3.1,typographically correct entities.
0.3.1,html_use_smartypants = True
0.3.1,"Custom sidebar templates, maps document names to template names."
0.3.1,html_sidebars = {}
0.3.1,"Additional templates that should be rendered to pages, maps page names to"
0.3.1,template names.
0.3.1,html_additional_pages = {}
0.3.1,"If false, no module index is generated."
0.3.1,html_domain_indices = True
0.3.1,"If false, no index is generated."
0.3.1,html_use_index = True
0.3.1,"If true, the index is split into individual pages for each letter."
0.3.1,html_split_index = False
0.3.1,"If true, links to the reST sources are added to the pages."
0.3.1,html_show_sourcelink = True
0.3.1,"If true, ""Created using Sphinx"" is shown in the HTML footer. Default is True."
0.3.1,html_show_sphinx = True
0.3.1,"If true, ""(C) Copyright ..."" is shown in the HTML footer. Default is True."
0.3.1,html_show_copyright = True
0.3.1,"If true, an OpenSearch description file will be output, and all pages will"
0.3.1,contain a <link> tag referring to it.  The value of this option must be the
0.3.1,base URL from which the finished HTML is served.
0.3.1,html_use_opensearch = ''
0.3.1,"This is the file name suffix for HTML files (e.g. "".xhtml"")."
0.3.1,html_file_suffix = None
0.3.1,Output file base name for HTML help builder.
0.3.1,-- Options for LaTeX output ---------------------------------------------
0.3.1,The paper size ('letterpaper' or 'a4paper').
0.3.1,"'papersize': 'letterpaper',"
0.3.1,"The font size ('10pt', '11pt' or '12pt')."
0.3.1,"'pointsize': '10pt',"
0.3.1,Additional stuff for the LaTeX preamble.
0.3.1,"'preamble': '',"
0.3.1,Grouping the document tree into LaTeX files. List of tuples
0.3.1,"(source start file, target name, title,"
0.3.1,"author, documentclass [howto, manual, or own class])."
0.3.1,The name of an image file (relative to this directory) to place at the top of
0.3.1,the title page.
0.3.1,latex_logo = None
0.3.1,"For ""manual"" documents, if this is true, then toplevel headings are parts,"
0.3.1,not chapters.
0.3.1,latex_use_parts = False
0.3.1,"If true, show page references after internal links."
0.3.1,latex_show_pagerefs = False
0.3.1,"If true, show URL addresses after external links."
0.3.1,latex_show_urls = False
0.3.1,Documents to append as an appendix to all manuals.
0.3.1,latex_appendices = []
0.3.1,"If false, no module index is generated."
0.3.1,latex_domain_indices = True
0.3.1,-- Options for manual page output ---------------------------------------
0.3.1,One entry per manual page. List of tuples
0.3.1,"(source start file, name, description, authors, manual section)."
0.3.1,"If true, show URL addresses after external links."
0.3.1,man_show_urls = False
0.3.1,-- Options for Texinfo output -------------------------------------------
0.3.1,Grouping the document tree into Texinfo files. List of tuples
0.3.1,"(source start file, target name, title, author,"
0.3.1,"dir menu entry, description, category)"
0.3.1,"generate empty examples files, so that we don't get"
0.3.1,inclusion errors if there are no examples for a class / module
0.3.1,touch file
0.3.1,Config for sphinx_issues
0.3.1,Documents to append as an appendix to all manuals.
0.3.1,texinfo_appendices = []
0.3.1,"If false, no module index is generated."
0.3.1,texinfo_domain_indices = True
0.3.1,"How to display URL addresses: 'footnote', 'no', or 'inline'."
0.3.1,texinfo_show_urls = 'footnote'
0.3.1,"If true, do not generate a @detailmenu in the ""Top"" node's menu."
0.3.1,texinfo_no_detailmenu = False
0.3.1,Example configuration for intersphinx: refer to the Python standard library.
0.3.1,The following is used by sphinx.ext.linkcode to provide links to github
0.3.1,get the styles from the current theme
0.3.1,create and add the button to all the code blocks that contain >>>
0.3.1,tracebacks (.gt) contain bare text elements that need to be
0.3.1,wrapped in a span to work with .nextUntil() (see later)
0.3.1,define the behavior of the button when it's clicked
0.3.1,hide the code output
0.3.1,show the code output
0.3.1,-*- coding: utf-8 -*-
0.3.1,Format template for issues URI
0.3.1,e.g. 'https://github.com/sloria/marshmallow/issues/{issue}
0.3.1,"Shortcut for Github, e.g. 'sloria/marshmallow'"
0.3.1,Format template for user profile URI
0.3.1,e.g. 'https://github.com/{user}'
0.3.1,Python 2 only
0.3.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.1,License: MIT
0.3.1,##############################################################################
0.3.1,Creation of an imbalanced data set from a balanced data set
0.3.1,##############################################################################
0.3.1,##############################################################################
0.3.1,We will show how to use the parameter ``ratio`` when dealing with the
0.3.1,"``make_imbalance`` function. For this function, this parameter accepts both"
0.3.1,"dictionary and callable. When using a dictionary, each key will correspond to"
0.3.1,the class of interest and the corresponding value will be the number of
0.3.1,samples desired in this class.
0.3.1,##############################################################################
0.3.1,You might required more flexibility and require your own heuristic to
0.3.1,determine the number of samples by class and you can define your own callable
0.3.1,as follow. In this case we will define a function which will use a float
0.3.1,multiplier to define the number of samples per class.
0.3.1,##############################################################################
0.3.1,Using ``ratio`` in resampling algorithm
0.3.1,##############################################################################
0.3.1,##############################################################################
0.3.1,"In all sampling algorithms, ``ratio`` can be used as illustrated earlier. In"
0.3.1,"addition, some predefined functions are available and can be executed using a"
0.3.1,``str`` with the following choices: (i) ``'minority'``: resample the minority
0.3.1,"class; (ii) ``'majority'``: resample the majority class, (iii) ``'not"
0.3.1,"minority'``: resample all classes apart of the minority class, (iv)"
0.3.1,"``'all'``: resample all classes, and (v) ``'auto'``: correspond to 'all' with"
0.3.1,for over-sampling methods and 'not minority' for under-sampling methods. The
0.3.1,classes targeted will be over-sampled or under-sampled to achieve an equal
0.3.1,number of sample with the majority or minority class.
0.3.1,##############################################################################
0.3.1,"However, you can use the dictionary or the callable options as previously"
0.3.1,mentioned.
0.3.1,Authors: Fernando Nogueira
0.3.1,Christos Aridas
0.3.1,Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.1,License: MIT
0.3.1,Generate the dataset
0.3.1,Instanciate a PCA object for the sake of easy visualisation
0.3.1,Fit and transform x to visualise inside a 2D feature space
0.3.1,Apply regular SMOTE
0.3.1,"Two subplots, unpack the axes array immediately"
0.3.1,Remove axis for second plot
0.3.1,Authors: Christos Aridas
0.3.1,Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.1,License: MIT
0.3.1,Generate the dataset
0.3.1,Instanciate a PCA object for the sake of easy visualisation
0.3.1,Fit and transform x to visualise inside a 2D feature space
0.3.1,Apply the random over-sampling
0.3.1,"Two subplots, unpack the axes array immediately"
0.3.1,make nice plotting
0.3.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.1,License: MIT
0.3.1,generate some data points
0.3.1,plot the majority and minority samples
0.3.1,draw the circle in which the new sample will generated
0.3.1,plot the line on which the sample will be generated
0.3.1,create and plot the new sample
0.3.1,make the plot nicer with legend and label
0.3.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.1,License: MIT
0.3.1,##############################################################################
0.3.1,The following function will be used to create toy dataset. It using the
0.3.1,``make_classification`` from scikit-learn but fixing some parameters.
0.3.1,##############################################################################
0.3.1,The following function will be used to plot the sample space after resampling
0.3.1,to illustrate the characterisitic of an algorithm.
0.3.1,make nice plotting
0.3.1,##############################################################################
0.3.1,The following function will be used to plot the decision function of a
0.3.1,classifier given some data.
0.3.1,##############################################################################
0.3.1,Illustration of the influence of the balancing ratio
0.3.1,##############################################################################
0.3.1,##############################################################################
0.3.1,We will first illustrate the influence of the balancing ratio on some toy
0.3.1,data using a linear SVM classifier. Greater is the difference between the
0.3.1,"number of samples in each class, poorer are the classfication results."
0.3.1,##############################################################################
0.3.1,Random over-sampling to balance the data set
0.3.1,##############################################################################
0.3.1,##############################################################################
0.3.1,Random over-sampling can be used to repeat some samples and balance the
0.3.1,number of samples between the dataset. It can be seen that with this trivial
0.3.1,approach the boundary decision is already less biaised toward the majority
0.3.1,class.
0.3.1,##############################################################################
0.3.1,More advanced over-sampling using ADASYN and SMOTE
0.3.1,##############################################################################
0.3.1,##############################################################################
0.3.1,"Instead of repeating the same samples when over-sampling, we can use some"
0.3.1,specific heuristic instead. ADASYN and SMOTE can be used in this case.
0.3.1,Make an identity sampler
0.3.1,##############################################################################
0.3.1,The following plot illustrate the difference between ADASYN and SMOTE. ADASYN
0.3.1,will focus on the samples which are difficult to classify with a
0.3.1,nearest-neighbors rule while regular SMOTE will not make any distinction.
0.3.1,"Therefore, the decision function depending of the algorithm."
0.3.1,##############################################################################
0.3.1,"Due to those sampling particularities, it can give rise to some specific"
0.3.1,issues as illustrated below.
0.3.1,##############################################################################
0.3.1,SMOTE proposes several variants by identifying specific samples to consider
0.3.1,during the resampling. The borderline version will detect which point to
0.3.1,select which are in the border between two classes. The SVM version will use
0.3.1,the support vectors found using an SVM algorithm to create new samples.
0.3.1,Authors: Christos Aridas
0.3.1,Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.1,License: MIT
0.3.1,Generate the dataset
0.3.1,Instanciate a PCA object for the sake of easy visualisation
0.3.1,Fit and transform x to visualise inside a 2D feature space
0.3.1,Apply the random over-sampling
0.3.1,"Two subplots, unpack the axes array immediately"
0.3.1,make nice plotting
0.3.1,Authors: Christos Aridas
0.3.1,Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.1,License: MIT
0.3.1,Generate the dataset
0.3.1,make nice plotting
0.3.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.1,License: MIT
0.3.1,Generate a dataset
0.3.1,Split the data
0.3.1,Train the classifier with balancing
0.3.1,Test the classifier and get the prediction
0.3.1,Show the classification report
0.3.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.1,License: MIT
0.3.1,Generate a dataset
0.3.1,Split the data
0.3.1,Train the classifier with balancing
0.3.1,Test the classifier and get the prediction
0.3.1,##############################################################################
0.3.1,The geometric mean corresponds to the square root of the product of the
0.3.1,sensitivity and specificity. Combining the two metrics should account for
0.3.1,the balancing of the dataset.
0.3.1,##############################################################################
0.3.1,The index balanced accuracy can transform any metric to be used in
0.3.1,imbalanced learning problems.
0.3.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.1,License: MIT
0.3.1,##############################################################################
0.3.1,The following function will be used to create toy dataset. It using the
0.3.1,``make_classification`` from scikit-learn but fixing some parameters.
0.3.1,##############################################################################
0.3.1,The following function will be used to plot the sample space after resampling
0.3.1,to illustrate the characteristic of an algorithm.
0.3.1,make nice plotting
0.3.1,##############################################################################
0.3.1,The following function will be used to plot the decision function of a
0.3.1,classifier given some data.
0.3.1,##############################################################################
0.3.1,"``SMOTE`` allows to generate samples. However, this method of over-sampling"
0.3.1,"does not have any knowledge regarding the underlying distribution. Therefore,"
0.3.1,"some noisy samples can be generated, e.g. when the different classes cannot"
0.3.1,"be well separated. Hence, it can be beneficial to apply an under-sampling"
0.3.1,algorithm to clean the noisy samples. Two methods are usually used in the
0.3.1,literature: (i) Tomek's link and (ii) edited nearest neighbours cleaning
0.3.1,methods. Imbalanced-learn provides two ready-to-use samplers ``SMOTETomek``
0.3.1,"and ``SMOTEENN``. In general, ``SMOTEENN`` cleans more noisy data than"
0.3.1,``SMOTETomek``.
0.3.1,Authors: Christos Aridas
0.3.1,Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.1,License: MIT
0.3.1,Generate the dataset
0.3.1,Instanciate a PCA object for the sake of easy visualisation
0.3.1,Fit and transform x to visualise inside a 2D feature space
0.3.1,Apply SMOTE + ENN
0.3.1,"Two subplots, unpack the axes array immediately"
0.3.1,make nice plotting
0.3.1,Authors: Christos Aridas
0.3.1,Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.1,License: MIT
0.3.1,Generate the dataset
0.3.1,Instanciate a PCA object for the sake of easy visualisation
0.3.1,Fit and transform x to visualise inside a 2D feature space
0.3.1,Apply SMOTE + Tomek links
0.3.1,"Two subplots, unpack the axes array immediately"
0.3.1,make nice plotting
0.3.1,Authors: Christos Aridas
0.3.1,Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.1,License: MIT
0.3.1,Generate the dataset
0.3.1,Instanciate a PCA object for the sake of easy visualisation
0.3.1,Fit and transform x to visualise inside a 2D feature space
0.3.1,Apply Balance Cascade method
0.3.1,"Two subplots, unpack the axes array immediately"
0.3.1,make nice plotting
0.3.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.1,License: MIT
0.3.1,Authors: Christos Aridas
0.3.1,Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.1,License: MIT
0.3.1,Generate the dataset
0.3.1,Instanciate a PCA object for the sake of easy visualisation
0.3.1,Fit and transform x to visualise inside a 2D feature space
0.3.1,Apply Easy Ensemble
0.3.1,"Two subplots, unpack the axes array immediately"
0.3.1,make nice plotting
0.3.1,Authors: Andreas Mueller
0.3.1,Christos Aridas
0.3.1,Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.1,License: MIT
0.3.1,remove Tomek links
0.3.1,make nice plotting
0.3.1,Authors: Dayvid Oliveira
0.3.1,Christos Aridas
0.3.1,Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.1,License: MIT
0.3.1,Generate the dataset
0.3.1,Instanciate a PCA object for the sake of easy visualisation
0.3.1,Fit and transform x to visualise inside a 2D feature space
0.3.1,"Three subplots, unpack the axes array immediately"
0.3.1,Apply the ENN
0.3.1,Apply the RENN
0.3.1,Apply the AllKNN
0.3.1,Authors: Christos Aridas
0.3.1,Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.1,License: MIT
0.3.1,Generate the dataset
0.3.1,Instanciate a PCA object for the sake of easy visualisation
0.3.1,Fit and transform x to visualise inside a 2D feature space
0.3.1,Apply Nearmiss
0.3.1,"Two subplots, unpack the axes array immediately"
0.3.1,plot the missing samples
0.3.1,Authors: Christos Aridas
0.3.1,Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.1,License: MIT
0.3.1,Generate the dataset
0.3.1,Instanciate a PCA object for the sake of easy visualisation
0.3.1,Fit and transform x to visualise inside a 2D feature space
0.3.1,Apply One-Sided Selection
0.3.1,make nice plotting
0.3.1,Authors: Christos Aridas
0.3.1,Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.1,License: MIT
0.3.1,Generate the dataset
0.3.1,Instanciate a PCA object for the sake of easy visualisation
0.3.1,Fit and transform x to visualise inside a 2D feature space
0.3.1,Apply neighbourhood cleaning rule
0.3.1,make nice plotting
0.3.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.1,License: MIT
0.3.1,##############################################################################
0.3.1,The following function will be used to create toy dataset. It using the
0.3.1,``make_classification`` from scikit-learn but fixing some parameters.
0.3.1,##############################################################################
0.3.1,The following function will be used to plot the sample space after resampling
0.3.1,to illustrate the characteristic of an algorithm.
0.3.1,make nice plotting
0.3.1,##############################################################################
0.3.1,The following function will be used to plot the decision function of a
0.3.1,classifier given some data.
0.3.1,##############################################################################
0.3.1,Prototype generation: under-sampling by generating new samples
0.3.1,##############################################################################
0.3.1,##############################################################################
0.3.1,``ClusterCentroids`` under-samples by replacing the original samples by the
0.3.1,centroids of the cluster found.
0.3.1,##############################################################################
0.3.1,Prototype selection: under-sampling by selecting existing samples
0.3.1,##############################################################################
0.3.1,##############################################################################
0.3.1,The algorithm performing prototype selection can be subdivided into two
0.3.1,groups: (i) the controlled under-sampling methods and (ii) the cleaning
0.3.1,under-sampling methods.
0.3.1,##############################################################################
0.3.1,"With the controlled under-sampling methods, the number of samples to be"
0.3.1,selected can be specified. ``RandomUnderSampler`` is the most naive way of
0.3.1,performing such selection by randomly selecting a given number of samples by
0.3.1,the targetted class.
0.3.1,##############################################################################
0.3.1,``NearMiss`` algorithms implement some heuristic rules in order to select
0.3.1,samples. NearMiss-1 selects samples from the majority class for which the
0.3.1,average distance of the :math:`k`` nearest samples of the minority class is
0.3.1,the smallest. NearMiss-2 selects the samples from the majority class for
0.3.1,which the average distance to the farthest samples of the negative class is
0.3.1,"the smallest. NearMiss-3 is a 2-step algorithm: first, for each minority"
0.3.1,"sample, their ::math:`m` nearest-neighbors will be kept; then, the majority"
0.3.1,samples selected are the on for which the average distance to the :math:`k`
0.3.1,nearest neighbors is the largest.
0.3.1,##############################################################################
0.3.1,``EditedNearestNeighbours`` removes samples of the majority class for which
0.3.1,their class differ from the one of their nearest-neighbors. This sieve can be
0.3.1,repeated which is the principle of the
0.3.1,``RepeatedEditedNearestNeighbours``. ``AllKNN`` is slightly different from
0.3.1,the ``RepeatedEditedNearestNeighbours`` by changing the :math:`k` parameter
0.3.1,"of the internal nearest neighors algorithm, increasing it at each iteration."
0.3.1,##############################################################################
0.3.1,``CondensedNearestNeighbour`` makes use of a 1-NN to iteratively decide if a
0.3.1,sample should be kept in a dataset or not. The issue is that
0.3.1,``CondensedNearestNeighbour`` is sensitive to noise by preserving the noisy
0.3.1,samples. ``OneSidedSelection`` also used the 1-NN and use ``TomekLinks`` to
0.3.1,remove the samples considered noisy. The ``NeighbourhoodCleaningRule`` use a
0.3.1,"``EditedNearestNeighbours`` to remove some sample. Additionally, they use a 3"
0.3.1,nearest-neighbors to remove samples which do not agree with this rule.
0.3.1,##############################################################################
0.3.1,``InstanceHardnessThreshold`` uses the prediction of classifier to exclude
0.3.1,samples. All samples which are classified with a low probability will be
0.3.1,removed.
0.3.1,Authors: Christos Aridas
0.3.1,Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.1,License: MIT
0.3.1,Generate the dataset
0.3.1,Instanciate a PCA object for the sake of easy visualisation
0.3.1,Fit and transform x to visualise inside a 2D feature space
0.3.1,Apply Condensed Nearest Neighbours
0.3.1,make nice plotting
0.3.1,Authors: Fernando Nogueira
0.3.1,Christos Aridas
0.3.1,Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.1,License: MIT
0.3.1,Generate the dataset
0.3.1,Instanciate a PCA object for the sake of easy visualisation
0.3.1,Fit and transform x to visualise inside a 2D feature space
0.3.1,Apply Cluster Centroids
0.3.1,Use hard voting instead of soft voting
0.3.1,"Two subplots, unpack the axes array immediately"
0.3.1,make nice plotting
0.3.1,##############################################################################
0.3.1,This function allows to make nice plotting
0.3.1,##############################################################################
0.3.1,Generate some data with one Tomek link
0.3.1,minority class
0.3.1,majority class
0.3.1,##############################################################################
0.3.1,"In the figure above, the samples highlighted in green form a Tomek link since"
0.3.1,they are of different classes and are nearest neighbours of each other.
0.3.1,highlight the samples of interest
0.3.1,##############################################################################
0.3.1,We can run the ``TomekLinks`` sampling to remove the corresponding
0.3.1,samples. If ``ratio='auto'`` only the sample from the majority class will be
0.3.1,removed. If ``ratio='all'`` both samples will be removed.
0.3.1,highlight the samples of interest
0.3.1,Authors: Christos Aridas
0.3.1,Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.1,License: MIT
0.3.1,Generate the dataset
0.3.1,Instanciate a PCA object for the sake of easy visualisation
0.3.1,Fit and transform x to visualise inside a 2D feature space
0.3.1,Apply the random under-sampling
0.3.1,make nice plotting
0.3.1,Authors: Dayvid Oliveira
0.3.1,Christos Aridas
0.3.1,Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.1,License: MIT
0.3.1,Generate the dataset
0.3.1,"Two subplots, unpack the axes array immediately"
0.3.1,plot samples which have been removed
0.3.1,##############################################################################
0.3.1,This function allows to make nice plotting
0.3.1,##############################################################################
0.3.1,We can start by generating some data to later illustrate the principle of
0.3.1,each NearMiss heuritic rules.
0.3.1,minority class
0.3.1,majority class
0.3.1,##############################################################################
0.3.1,NearMiss-1
0.3.1,##############################################################################
0.3.1,##############################################################################
0.3.1,NearMiss-1 selects samples from the majority class for which the average
0.3.1,distance to some nearest neighbours is the smallest. In the following
0.3.1,"example, we use a 3-NN to compute the average distance on 2 specific samples"
0.3.1,"of the majority class. Therefore, in this case the point linked by the"
0.3.1,green-dashed line will be selected since the average distance is smaller.
0.3.1,##############################################################################
0.3.1,NearMiss-2
0.3.1,##############################################################################
0.3.1,##############################################################################
0.3.1,NearMiss-2 selects samples from the majority class for which the average
0.3.1,distance to the farthest neighbors is the smallest. With the same
0.3.1,"configuration as previously presented, the sample linked to the green-dashed"
0.3.1,line will be selected since its distance the 3 farthest neighbors is the
0.3.1,smallest.
0.3.1,##############################################################################
0.3.1,NearMiss-3
0.3.1,##############################################################################
0.3.1,##############################################################################
0.3.1,"NearMiss-3 can be divided into 2 steps. First, a nearest-neighbors is used to"
0.3.1,short-list samples from the majority class (i.e. correspond to the
0.3.1,"highlighted samples in the following plot). Then, the sample with the largest"
0.3.1,average distance to the *k* nearest-neighbors are selected.
0.3.1,select only the majority point of interest
0.3.1,Authors: Christos Aridas
0.3.1,Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.1,License: MIT
0.3.1,Generate the dataset
0.3.1,Instanciate a PCA object for the sake of easy visualisation
0.3.1,Create the samplers
0.3.1,Create the classifier
0.3.1,Make the splits
0.3.1,Add one transformers and two samplers in the pipeline object
0.3.1,Authors: Christos Aridas
0.3.1,Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.1,License: MIT
0.3.1,Load the dataset
0.3.1,make nice plotting
0.3.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.1,License: MIT
0.3.1,Create a folder to fetch the dataset
0.3.1,Create a pipeline
0.3.1,Classify and report the results
0.3.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.1,License: MIT
0.3.1,##############################################################################
0.3.1,Setting the data set
0.3.1,##############################################################################
0.3.1,##############################################################################
0.3.1,We use a part of the 20 newsgroups data set by loading 4 topics. Using the
0.3.1,"scikit-learn loader, the data are split into a training and a testing set."
0.3.1,
0.3.1,Note the class \#3 is the minority class and has almost twice less samples
0.3.1,than the majority class.
0.3.1,##############################################################################
0.3.1,The usual scikit-learn pipeline
0.3.1,##############################################################################
0.3.1,##############################################################################
0.3.1,You might usually use scikit-learn pipeline by combining the TF-IDF
0.3.1,vectorizer to feed a multinomial naive bayes classifier. A classification
0.3.1,report summarized the results on the testing set.
0.3.1,
0.3.1,"As expected, the recall of the class \#3 is low mainly due to the class"
0.3.1,imbalanced.
0.3.1,##############################################################################
0.3.1,Balancing the class before classification
0.3.1,##############################################################################
0.3.1,##############################################################################
0.3.1,"To improve the prediction of the class \#3, it could be interesting to apply"
0.3.1,"a balancing before to train the naive bayes classifier. Therefore, we will"
0.3.1,use a ``RandomUnderSampler`` to equalize the number of samples in all the
0.3.1,classes before the training.
0.3.1,
0.3.1,It is also important to note that we are using the ``make_pipeline`` function
0.3.1,implemented in imbalanced-learn to properly handle the samplers.
0.3.1,##############################################################################
0.3.1,"Although the results are almost identical, it can be seen that the resampling"
0.3.1,allowed to correct the poor recall of the class \#3 at the cost of reducing
0.3.1,"the other metrics for the other classes. However, the overall results are"
0.3.1,slightly better.
0.3.1,Authors: Dayvid Oliveira
0.3.1,Christos Aridas
0.3.1,Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.1,License: MIT
0.3.1,Generate the dataset
0.3.1,"Two subplots, unpack the axes array immediately"
0.3.1,Based on NiLearn package
0.3.1,License: simplified BSD
0.3.1,"PEP0440 compatible formatted version, see:"
0.3.1,https://www.python.org/dev/peps/pep-0440/
0.3.1,
0.3.1,Generic release markers:
0.3.1,X.Y
0.3.1,X.Y.Z # For bugfix releases
0.3.1,
0.3.1,Admissible pre-release markers:
0.3.1,X.YaN # Alpha release
0.3.1,X.YbN # Beta release
0.3.1,X.YrcN # Release Candidate
0.3.1,X.Y # Final release
0.3.1,
0.3.1,Dev branch marker is: 'X.Y.dev' or 'X.Y.devN' where N is an integer.
0.3.1,'X.Y.dev0' is the canonical version of 'X.Y.dev'
0.3.1,
0.3.1,"This is a tuple to preserve order, so that dependencies are checked"
0.3.1,in some meaningful order (more => less 'core').  We avoid using
0.3.1,collections.OrderedDict to preserve Python 2.6 compatibility.
0.3.1,Avoid choking on modules with no __version__ attribute
0.3.1,Skip check only when installing and it's a module that
0.3.1,will be auto-installed.
0.3.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.1,License: MIT
0.3.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.1,Christos Aridas
0.3.1,License: MIT
0.3.1,Check the consistency of X and y
0.3.1,self.sampling_type is already checked in check_ratio
0.3.1,Adapted from scikit-learn
0.3.1,Author: Edouard Duchesnay
0.3.1,Gael Varoquaux
0.3.1,Virgile Fritsch
0.3.1,Alexandre Gramfort
0.3.1,Lars Buitinck
0.3.1,Christos Aridas
0.3.1,Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.1,License: BSD
0.3.1,BaseEstimator interface
0.3.1,shallow copy of steps
0.3.1,validate names
0.3.1,validate estimators
0.3.1,We allow last estimator to be None as an identity transformation
0.3.1,Estimator interface
0.3.1,Setup the memory
0.3.1,we do not clone when caching is disabled to preserve
0.3.1,backward compatibility
0.3.1,Fit or load from cache the current transfomer
0.3.1,Replace the transformer of the step with the fitted
0.3.1,transformer. This is necessary when loading the transformer
0.3.1,from the cache.
0.3.1,XXX: Calling sample in pipeline it means that the
0.3.1,last estimator is a sampler. Samplers don't carry
0.3.1,"the sampled data. So, call 'fit_sample' in all intermediate"
0.3.1,steps to get the sampled data for the last estimator.
0.3.1,"_final_estimator is None or has transform, otherwise attribute error"
0.3.1,raise AttributeError if necessary for hasattr behaviour
0.3.1,"if we have a weight for this transformer, multiply output"
0.3.1,Boolean controlling whether the joblib caches should be
0.3.1,"flushed if the version of certain modules changes (eg nibabel, as it"
0.3.1,does not respect the backward compatibility in some of its internal
0.3.1,structures
0.3.1,This  is used in nilearn._utils.cache_mixin
0.3.1,list all submodules available in imblearn and version
0.3.1,coding: utf-8
0.3.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.1,Dariusz Brzezinski
0.3.1,License: MIT
0.3.1,Only negative labels
0.3.1,"Calculate tp_sum, pred_sum, true_sum ###"
0.3.1,labels are now from 0 to len(labels) - 1 -> use bincount
0.3.1,Pathological case
0.3.1,Compute the true negative
0.3.1,Retain only selected labels
0.3.1,"Finally, we have all our sufficient statistics. Divide! #"
0.3.1,"Divide, and on zero-division, set scores to 0 and warn:"
0.3.1,"Oddly, we may get an ""invalid"" rather than a ""divide"" error"
0.3.1,here.
0.3.1,Average the results
0.3.1,labels are now from 0 to len(labels) - 1 -> use bincount
0.3.1,Pathological case
0.3.1,Retain only selected labels
0.3.1,old version of scipy return MaskedConstant instead of 0.0
0.3.1,Create the list of tags
0.3.1,check that the scoring function does not need a score
0.3.1,and only a prediction
0.3.1,Compute the score from the scoring function
0.3.1,Square if desired
0.3.1,Get the signature of the sens/spec function
0.3.1,We need to extract from kwargs only the one needed by the
0.3.1,specificity and specificity
0.3.1,Make the intersection between the parameters
0.3.1,Create a sub dictionary
0.3.1,Check if the metric is the geometric mean
0.3.1,We do not support multilabel so the only average supported
0.3.1,is binary
0.3.1,Create the list of parameters through signature binding
0.3.1,Call the sens/spec function
0.3.1,Compute the dominance
0.3.1,Compute the different metrics
0.3.1,Precision/recall/f1
0.3.1,Specificity
0.3.1,Geometric mean
0.3.1,Index balanced accuracy
0.3.1,compute averages
0.3.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.1,Christos Aridas
0.3.1,License: MIT
0.3.1,##############################################################################
0.3.1,Utilities for testing
0.3.1,import some data to play with
0.3.1,restrict to a binary classification task
0.3.1,add noisy features to make the problem harder and avoid perfect results
0.3.1,"run classifier, get class probabilities and label predictions"
0.3.1,only interested in probabilities of the positive case
0.3.1,XXX: do we really want a special API for the binary case?
0.3.1,##############################################################################
0.3.1,Tests
0.3.1,detailed measures for each class
0.3.1,individual scoring function that can be used for grid search: in the
0.3.1,binary class case the score is the value of the measure for the positive
0.3.1,class (e.g. label == 1). This is deprecated for average != 'binary'.
0.3.1,Such a case may occur with non-stratified cross-validation
0.3.1,No average: zeros in array
0.3.1,Macro average is changed
0.3.1,Check for micro
0.3.1,Check for weighted
0.3.1,ensure the above were meaningful tests:
0.3.1,Bad pos_label
0.3.1,Bad average option
0.3.1,but average != 'binary'; even if data is binary
0.3.1,compute the geometric mean for the binary problem
0.3.1,Compute the geometric mean for each of the classes
0.3.1,average tests
0.3.1,print classification report with class names
0.3.1,print classification report with label detection
0.3.1,print classification report with class names
0.3.1,print classification report with label detection
0.3.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.1,Christos Aridas
0.3.1,License: MIT
0.3.1,Get the version
0.3.1,sensitivity scorer
0.3.1,specificity scorer
0.3.1,geometric_mean scorer
0.3.1,make a iba metric before a scorer
0.3.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.1,Christos Aridas
0.3.1,License: MIT
0.3.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.1,Christos Aridas
0.3.1,License: MIT
0.3.1,FIXME: Deprecated in 0.2. To be removed in 0.4.
0.3.1,The ratio is computed using a one-vs-rest manner. Using majority
0.3.1,in multi-class would lead to slightly different results at the
0.3.1,cost of introducing a new parameter.
0.3.1,the nearest neighbors need to be fitted only on the current class
0.3.1,to find the class NN to generate new samples
0.3.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.1,Christos Aridas
0.3.1,License: MIT
0.3.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.1,Fernando Nogueira
0.3.1,Christos Aridas
0.3.1,License: MIT
0.3.1,Samples are in danger for m/2 <= m' < m
0.3.1,Samples are noise for m = m'
0.3.1,"FIXME Deprecated in 0.2, to be removed in 0.4"
0.3.1,divergence between borderline-1 and borderline-2
0.3.1,Create synthetic samples for borderline points.
0.3.1,only minority
0.3.1,we use a one-vs-rest policy to handle the multiclass in which
0.3.1,new samples will be created considering not only the majority
0.3.1,class but all over classes.
0.3.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.1,Christos Aridas
0.3.1,License: MIT
0.3.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.1,Christos Aridas
0.3.1,License: MIT
0.3.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.1,Christos Aridas
0.3.1,License: MIT
0.3.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.1,License: MIT
0.3.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.1,Fernando Nogueira
0.3.1,Christos Aridas
0.3.1,License: MIT
0.3.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.1,Christos Aridas
0.3.1,License: MIT
0.3.1,FIXME: Deprecated in 0.2. To be removed in 0.4.
0.3.1,select a sample from the current class
0.3.1,create the set composed of all minority samples and one
0.3.1,sample from the current class.
0.3.1,create the set S with removing the seed from S
0.3.1,since that it will be added anyway
0.3.1,apply Tomek cleaning
0.3.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.1,Christos Aridas
0.3.1,License: MIT
0.3.1,Compute the distance considering the farthest neighbour
0.3.1,Sort the list of distance and get the index
0.3.1,Throw a warning to tell the user that we did not have enough samples
0.3.1,to select and that we just select everything
0.3.1,Select the desired number of samples
0.3.1,FIXME: Deprecated in 0.2. To be removed in 0.4.
0.3.1,idx_tmp is relative to the feature selected in the
0.3.1,previous step and we need to find the indirection
0.3.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.1,Christos Aridas
0.3.1,License: MIT
0.3.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.1,Fernando Nogueira
0.3.1,Christos Aridas
0.3.1,License: MIT
0.3.1,find which class to not consider
0.3.1,there is a Tomek link between two samples if they are both nearest
0.3.1,neighbors of each others.
0.3.1,Find the nearest neighbour of every point
0.3.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.1,Christos Aridas
0.3.1,License: MIT
0.3.1,FIXME: Deprecated in 0.2. To be removed in 0.4
0.3.1,Randomly get one sample from the majority class
0.3.1,Generate the index to select
0.3.1,Create the set C - One majority samples and all minority
0.3.1,Create the set S - all majority samples
0.3.1,fit knn on C
0.3.1,Check each sample in S if we keep it or drop it
0.3.1,Do not select sample which are already well classified
0.3.1,Classify on S
0.3.1,If the prediction do not agree with the true label
0.3.1,append it in C_x
0.3.1,Keep the index for later
0.3.1,Update C
0.3.1,fit a knn on C
0.3.1,This experimental to speed up the search
0.3.1,Classify all the element in S and avoid to test the
0.3.1,well classified elements
0.3.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.1,Christos Aridas
0.3.1,License: MIT
0.3.1,FIXME: Deprecated from 0.2. To be removed in 0.4.
0.3.1,clean the neighborhood
0.3.1,compute which classes to consider for cleaning for the A2 group
0.3.1,compute a2 group
0.3.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.1,Dayvid Oliveira
0.3.1,Christos Aridas
0.3.1,License: MIT
0.3.1,FIXME: Deprecated in 0.2. To be removed in 0.4
0.3.1,Check the stopping criterion
0.3.1,1. If there is no changes for the vector y
0.3.1,2. If the number of samples in the other class become inferior to
0.3.1,the number of samples in the majority class
0.3.1,3. If one of the class is disappearing
0.3.1,Case 1
0.3.1,Case 2
0.3.1,Case 3
0.3.1,Check the stopping criterion
0.3.1,1. If the number of samples in the other class become inferior to
0.3.1,the number of samples in the majority class
0.3.1,2. If one of the class is disappearing
0.3.1,Case 1
0.3.1,overwrite b_min_bec_maj
0.3.1,Case 2
0.3.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.1,Dayvid Oliveira
0.3.1,Christos Aridas
0.3.1,License: MIT
0.3.1,To be removed in 0.4
0.3.1,Select the appropriate classifier
0.3.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.1,Christos Aridas
0.3.1,License: MIT
0.3.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.1,Christos Aridas
0.3.1,License: MIT
0.3.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.1,Christos Aridas
0.3.1,License: MIT
0.3.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.1,Christos Aridas
0.3.1,License: MIT
0.3.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.1,Christos Aridas
0.3.1,License: MIT
0.3.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.1,Christos Aridas
0.3.1,License: MIT
0.3.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.1,Christos Aridas
0.3.1,License: MIT
0.3.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.1,Christos Aridas
0.3.1,License: MIT
0.3.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.1,Christos Aridas
0.3.1,License: MIT
0.3.1,FIXME remove at the end of the deprecation 0.4
0.3.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.1,Christos Aridas
0.3.1,License: MIT
0.3.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.1,Christos Aridas
0.3.1,License: MIT
0.3.1,test that all_estimators doesn't find abstract classes.
0.3.1,some can just not be sensibly default constructed
0.3.1,input validation etc for non-meta estimators
0.3.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.1,Christos Aridas
0.3.1,License: MIT
0.3.1,store timestamp to figure out whether the result of 'fit' has been
0.3.1,cached or not
0.3.1,store timestamp to figure out whether the result of 'fit' has been
0.3.1,cached or not
0.3.1,Test the various init parameters of the pipeline.
0.3.1,Check that we can't instantiate pipelines with objects without fit
0.3.1,method
0.3.1,Smoke test with only an estimator
0.3.1,Check that params are set
0.3.1,Smoke test the repr:
0.3.1,Test with two objects
0.3.1,Check that we can't instantiate with non-transformers on the way
0.3.1,"Note that NoTrans implements fit, but not transform"
0.3.1,Check that params are set
0.3.1,Smoke test the repr:
0.3.1,Check that params are not set when naming them wrong
0.3.1,Test clone
0.3.1,"Check that apart from estimators, the parameters are the same"
0.3.1,Remove estimators that where copied
0.3.1,Test the various methods of the pipeline (anova).
0.3.1,Test with Anova + LogisticRegression
0.3.1,Test that the pipeline can take fit parameters
0.3.1,classifier should return True
0.3.1,and transformer params should not be changed
0.3.1,invalid parameters should raise an error message
0.3.1,Pipeline should pass sample_weight
0.3.1,When sample_weight is None it shouldn't be passed
0.3.1,Test pipeline raises set params error message for nested models.
0.3.1,nested model check
0.3.1,Test the various methods of the pipeline (pca + svm).
0.3.1,Test with PCA + SVC
0.3.1,Test the various methods of the pipeline (preprocessing + svm).
0.3.1,check shapes of various prediction functions
0.3.1,test that the fit_predict method is implemented on a pipeline
0.3.1,test that the fit_predict on pipeline yields same results as applying
0.3.1,transform and clustering steps separately
0.3.1,"As pipeline doesn't clone estimators on construction,"
0.3.1,it must have its own estimators
0.3.1,first compute the transform and clustering step separately
0.3.1,use a pipeline to do the transform and clustering in one step
0.3.1,tests that a pipeline does not have fit_predict method when final
0.3.1,step of pipeline does not have fit_predict defined
0.3.1,tests that Pipeline passes fit_params to intermediate steps
0.3.1,when fit_predict is invoked
0.3.1,Test whether pipeline works with a transformer at the end.
0.3.1,Also test pipeline.transform and pipeline.inverse_transform
0.3.1,test transform and fit_transform:
0.3.1,Test whether pipeline works with a transformer missing fit_transform
0.3.1,test fit_transform:
0.3.1,Directly setting attr
0.3.1,Using set_params
0.3.1,Using set_params to replace single step
0.3.1,With invalid data
0.3.1,Test setting Pipeline steps to None
0.3.1,"for other methods, ensure no AttributeErrors on None:"
0.3.1,mult2 and mult3 are active
0.3.1,Check None step at construction time
0.3.1,Test that an error is raised when memory is not a string or a Memory
0.3.1,instance
0.3.1,Define memory as an integer
0.3.1,Test with Transformer + SVC
0.3.1,Memoize the transformer at the first fit
0.3.1,Get the time stamp of the tranformer in the cached pipeline
0.3.1,Check that cached_pipe and pipe yield identical results
0.3.1,Check that we are reading the cache while fitting
0.3.1,a second time
0.3.1,Check that cached_pipe and pipe yield identical results
0.3.1,Create a new pipeline with cloned estimators
0.3.1,Check that even changing the name step does not affect the cache hit
0.3.1,Check that cached_pipe and pipe yield identical results
0.3.1,Test with Transformer + SVC
0.3.1,Memoize the transformer at the first fit
0.3.1,Get the time stamp of the tranformer in the cached pipeline
0.3.1,Check that cached_pipe and pipe yield identical results
0.3.1,Check that we are reading the cache while fitting
0.3.1,a second time
0.3.1,Check that cached_pipe and pipe yield identical results
0.3.1,Create a new pipeline with cloned estimators
0.3.1,Check that even changing the name step does not affect the cache hit
0.3.1,Check that cached_pipe and pipe yield identical results
0.3.1,Test the various methods of the pipeline (pca + svm).
0.3.1,Test with PCA + SVC
0.3.1,Test the various methods of the pipeline (pca + svm).
0.3.1,Test with PCA + SVC
0.3.1,Test whether pipeline works with a sampler at the end.
0.3.1,Also test pipeline.sampler
0.3.1,test transform and fit_transform:
0.3.1,We round the value near to zero. It seems that PCA has some issue
0.3.1,with that
0.3.1,Test whether pipeline works with a sampler at the end.
0.3.1,Also test pipeline.sampler
0.3.1,Test pipeline using None as preprocessing step and a classifier
0.3.1,"Test pipeline using None, RUS and a classifier"
0.3.1,"Test pipeline using RUS, None and a classifier"
0.3.1,Test pipeline using None step and a sampler
0.3.1,Test pipeline using None and a transformer that implements transform and
0.3.1,inverse_transform
0.3.1,Test the various methods of the pipeline (anova).
0.3.1,Test with RandomUnderSampling + Anova + LogisticRegression
0.3.1,Test the various methods of the pipeline (anova).
0.3.1,Test the various methods of the pipeline (anova).
0.3.1,Test with RandomUnderSampling + Anova + LogisticRegression
0.3.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.1,Christos Aridas
0.3.1,License: MIT
0.3.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.1,License: MIT
0.3.1,Adapated from scikit-learn
0.3.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.1,License: MIT
0.3.1,make the checks from scikit-learn
0.3.1,trigger our checks if this is a SamplerMixin
0.3.1,FIXME already present in scikit-learn 0.19
0.3.1,test scikit-learn compatibility
0.3.1,Estimators in mono_output_task_error raise ValueError if y is of 1-D
0.3.1,Convert into a 2-D y for those estimators.
0.3.1,check that fit method only changes or sets private attributes
0.3.1,to not check deprecated classes
0.3.1,check that fit doesn't add any public attribute
0.3.1,check that fit doesn't change any public attribute
0.3.1,in this test we will force all samplers to not change the class 1
0.3.1,check that sparse matrices can be passed through the sampler leading to
0.3.1,the same results than dense
0.3.1,set KMeans to full since it support sparse and dense
0.3.1,Check that the samplers handle pandas dataframe and pandas series
0.3.1,Adapted from scikit-learn
0.3.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.1,License: MIT
0.3.1,meta-estimators need another estimator to be instantiated.
0.3.1,estimators that there is no way to default-construct sensibly
0.3.1,some strange ones
0.3.1,get parent folder
0.3.1,get rid of abstract base classes
0.3.1,get rid of sklearn estimators which have been imported in some classes
0.3.1,possibly get rid of meta estimators
0.3.1,"drop duplicates, sort for reproducibility"
0.3.1,itemgetter is used to ensure the sort does not extend to the 2nd item of
0.3.1,the tuple
0.3.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.1,License: MIT
0.3.1,FIXME: perfectly we should raise an error but the sklearn API does
0.3.1,not allow for it
0.3.1,check that all keys in ratio are also in y
0.3.1,check that there is no negative number
0.3.1,clean-sampling can be more permissive since those samplers do not
0.3.1,use samples
0.3.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.1,Christos Aridas
0.3.1,License: MIT
0.3.1,this function could create an equal number of samples
0.3.1,"tests that the estimator actually fails on ""bad"" estimators."
0.3.1,"not a complete test of all checks, which are very extensive."
0.3.1,check that we have a set_params and can clone
0.3.1,check that we have a fit method
0.3.1,check that fit does input validation
0.3.1,check that predict does input validation (doesn't accept dicts in input)
0.3.1,check that estimator state does not change
0.3.1,at transform/predict/predict_proba time
0.3.1,check that `fit` only changes attributes that
0.3.1,are private (start with an _ or end with a _).
0.3.1,check that `fit` doesn't add any public attribute
0.3.1,check for sparse matrix input handling
0.3.1,"the check for sparse input handling prints to the stdout,"
0.3.1,"instead of raising an error, so as not to remove the original traceback."
0.3.1,that means we need to jump through some hoops to catch it.
0.3.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.1,License: MIT
0.3.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.1,Christos Aridas
0.3.1,License: MIT
0.3.1,check if the filtering is working with a list or a single string
0.3.1,check that all estimators are sampler
0.3.1,check that an error is raised when the type is unknown
0.3.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.1,Christos Aridas
0.3.1,License: MIT
0.3.1,Check any parameters for SMOTE was provided
0.3.1,Anounce deprecation
0.3.1,We need to list each parameter and decide if we affect a default
0.3.1,value or not
0.3.1,"If an object was given, affect"
0.3.1,Otherwise create a default SMOTE
0.3.1,Check any parameters for ENN was provided
0.3.1,Anounce deprecation
0.3.1,We need to list each parameter and decide if we affect a default
0.3.1,value or not
0.3.1,"If an object was given, affect"
0.3.1,Otherwise create a default EditedNearestNeighbours
0.3.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.1,Christos Aridas
0.3.1,License: MIT
0.3.1,Check any parameters for SMOTE was provided
0.3.1,Anounce deprecation
0.3.1,We need to list each parameter and decide if we affect a default
0.3.1,value or not
0.3.1,"If an object was given, affect"
0.3.1,Otherwise create a default SMOTE
0.3.1,Check any parameters for ENN was provided
0.3.1,Anounce deprecation
0.3.1,"If an object was given, affect"
0.3.1,Otherwise create a default TomekLinks
0.3.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.1,Christos Aridas
0.3.1,License: MIT
0.3.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.1,Christos Aridas
0.3.1,License: MIT
0.3.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.1,Christos Aridas
0.3.1,License: MIT
0.3.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.1,Christos Aridas
0.3.1,License: MIT
0.3.1,RandomUnderSampler is not supporting sample_weight. We need to pass
0.3.1,None.
0.3.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.1,Christos Aridas
0.3.1,License: MIT
0.3.1,doctest: +ELLIPSIS
0.3.1,To be removed in 0.4
0.3.1,Define the classifier to use
0.3.1,array to know which samples are available to be taken
0.3.1,where the different set will be stored
0.3.1,store the index of the data to under-sample
0.3.1,value which will be picked at each round
0.3.1,extract the data of interest for this round from the
0.3.1,current class
0.3.1,select randomly the desired features
0.3.1,store the set created
0.3.1,fit and predict using cross validation
0.3.1,extract the prediction about the targeted classes only
0.3.1,check the stopping criterion
0.3.1,check that there is enough samples for another round
0.3.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.1,License: MIT
0.3.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.1,Christos Aridas
0.3.1,License: MIT
0.3.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.1,Christos Aridas
0.3.1,License: MIT
0.3.1,Check classification for various parameter settings.
0.3.1,Test that bootstrapping samples generate non-perfect base estimators.
0.3.1,"without bootstrap, all trees are perfect on the training set"
0.3.1,disable the resampling by passing an empty dictionary.
0.3.1,"with bootstrap, trees are no longer perfect on the training set"
0.3.1,Test that bootstrapping features may generate duplicate features.
0.3.1,Predict probabilities.
0.3.1,Normal case
0.3.1,"Degenerate case, where some classes are missing"
0.3.1,Check that oob prediction is a good estimation of the generalization
0.3.1,error.
0.3.1,Test with few estimators
0.3.1,Check singleton ensembles.
0.3.1,Test that it gives proper exception on deficient input.
0.3.1,Test n_estimators
0.3.1,Test max_samples
0.3.1,Test max_features
0.3.1,Test support of decision_function
0.3.1,Check that bagging ensembles can be grid-searched.
0.3.1,Transform iris into a binary classification task
0.3.1,Grid search with scoring based on decision_function
0.3.1,Check base_estimator and its default values.
0.3.1,Test if fitting incrementally with warm start gives a forest of the
0.3.1,right size and the same results as a normal fit.
0.3.1,Test if warm start'ed second fit with smaller n_estimators raises error.
0.3.1,Test that nothing happens when fitting without increasing n_estimators
0.3.1,"modify X to nonsense values, this should not change anything"
0.3.1,warm started classifier with 5+5 estimators should be equivalent to
0.3.1,one classifier with 10 estimators
0.3.1,Check using oob_score and warm_start simultaneously fails
0.3.1,"Make sure OOB scores are identical when random_state, estimator, and"
0.3.1,training data are fixed and fitting is done twice
0.3.1,Check that format of estimators_samples_ is correct and that results
0.3.1,generated at fit time can be identically reproduced at a later time
0.3.1,using data saved in object attributes.
0.3.1,remap the y outside of the BalancedBaggingclassifier
0.3.1,"_, y = np.unique(y, return_inverse=True)"
0.3.1,Get relevant attributes
0.3.1,Test for correct formatting
0.3.1,Re-fit single estimator to test for consistent sampling
0.3.1,Make sure validated max_samples and original max_samples are identical
0.3.1,when valid integer max_samples supplied by user
0.3.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.1,Christos Aridas
0.3.1,License: MIT
0.3.1,Generate a global dataset to use
0.3.1,Define a ratio
0.3.1,Define the ratio parameter
0.3.1,Create the sampling object
0.3.1,Get the different subset
0.3.1,Define the ratio parameter
0.3.1,Create the sampling object
0.3.1,Get the different subset
0.3.1,Define the ratio parameter
0.3.1,Create the sampling object
0.3.1,Get the different subset
0.3.1,Author: Guillaume Lemaitre
0.3.1,License: BSD 3 clause
0.3.1,"The index start at one, then we need to remove one"
0.3.1,to not have issue with the indexing.
0.3.1,go through the list and check if the data are available
0.3.1,Authors: Dayvid Oliveira
0.3.1,Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.1,Christos Aridas
0.3.1,License: MIT
0.3.1,restrict ratio to be a dict or a callable
0.3.1,FIXME: deprecated in 0.2 to be removed in 0.4
0.3.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.1,Christos Aridas
0.3.1,License: MIT
0.3.1,"we are reusing part of utils.check_ratio, however this is not cover in"
0.3.1,the common tests so we will repeat it here
0.3.1,FIXME: to be removed in 0.4 due to deprecation
0.3.1,resample without using min_c_
0.3.1,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.1,Christos Aridas
0.3.1,License: MIT
0.3.0,! /usr/bin/env python
0.3.0,"load all vars into globals, otherwise"
0.3.0,the later function call using global vars doesn't work.
0.3.0,"Allow command-lines such as ""python setup.py build install"""
0.3.0,Make sources available using relative paths from this file's directory.
0.3.0,-*- coding: utf-8 -*-
0.3.0,
0.3.0,"imbalanced-learn documentation build configuration file, created by"
0.3.0,sphinx-quickstart on Mon Jan 18 14:44:12 2016.
0.3.0,
0.3.0,This file is execfile()d with the current directory set to its
0.3.0,containing dir.
0.3.0,
0.3.0,Note that not all possible configuration values are present in this
0.3.0,autogenerated file.
0.3.0,
0.3.0,All configuration values have a default; values that are commented out
0.3.0,serve to show the default.
0.3.0,"If extensions (or modules to document with autodoc) are in another directory,"
0.3.0,add these directories to sys.path here. If the directory is relative to the
0.3.0,"documentation root, use os.path.abspath to make it absolute, like shown here."
0.3.0,"sys.path.insert(0, os.path.abspath('.'))"
0.3.0,-- General configuration ---------------------------------------------------
0.3.0,Try to override the matplotlib configuration as early as possible
0.3.0,-- General configuration ------------------------------------------------
0.3.0,If extensions (or modules to document with autodoc) are in another
0.3.0,"directory, add these directories to sys.path here. If the directory"
0.3.0,"is relative to the documentation root, use os.path.abspath to make it"
0.3.0,"absolute, like shown here."
0.3.0,"If your documentation needs a minimal Sphinx version, state it here."
0.3.0,needs_sphinx = '1.0'
0.3.0,"Add any Sphinx extension module names here, as strings. They can be"
0.3.0,extensions coming with Sphinx (named 'sphinx.ext.*') or your custom
0.3.0,ones.
0.3.0,path to your examples scripts
0.3.0,path where to save gallery generated examples
0.3.0,to make references clickable
0.3.0,"Add any paths that contain templates here, relative to this directory."
0.3.0,generate autosummary even if no references
0.3.0,The suffix of source filenames.
0.3.0,The encoding of source files.
0.3.0,source_encoding = 'utf-8-sig'
0.3.0,Generate the plots for the gallery
0.3.0,The master toctree document.
0.3.0,General information about the project.
0.3.0,"The version info for the project you're documenting, acts as replacement for"
0.3.0,"|version| and |release|, also used in various other places throughout the"
0.3.0,built documents.
0.3.0,
0.3.0,The short X.Y version.
0.3.0,"The full version, including alpha/beta/rc tags."
0.3.0,The language for content autogenerated by Sphinx. Refer to documentation
0.3.0,for a list of supported languages.
0.3.0,language = None
0.3.0,"There are two options for replacing |today|: either, you set today to some"
0.3.0,"non-false value, then it is used:"
0.3.0,today = ''
0.3.0,"Else, today_fmt is used as the format for a strftime call."
0.3.0,"today_fmt = '%B %d, %Y'"
0.3.0,"List of patterns, relative to source directory, that match files and"
0.3.0,directories to ignore when looking for source files.
0.3.0,The reST default role (used for this markup: `text`) to use for all
0.3.0,documents.
0.3.0,default_role = None
0.3.0,"If true, '()' will be appended to :func: etc. cross-reference text."
0.3.0,"If true, the current module name will be prepended to all description"
0.3.0,unit titles (such as .. function::).
0.3.0,add_module_names = True
0.3.0,"If true, sectionauthor and moduleauthor directives will be shown in the"
0.3.0,output. They are ignored by default.
0.3.0,show_authors = False
0.3.0,The name of the Pygments (syntax highlighting) style to use.
0.3.0,Custom style
0.3.0,A list of ignored prefixes for module index sorting.
0.3.0,modindex_common_prefix = []
0.3.0,"If true, keep warnings as ""system message"" paragraphs in the built documents."
0.3.0,keep_warnings = False
0.3.0,-- Options for HTML output ----------------------------------------------
0.3.0,The theme to use for HTML and HTML Help pages.  See the documentation for
0.3.0,a list of builtin themes.
0.3.0,Theme options are theme-specific and customize the look and feel of a theme
0.3.0,"further.  For a list of options available for each theme, see the"
0.3.0,documentation.
0.3.0,html_theme_options = {'prev_next_buttons_location': None}
0.3.0,"Add any paths that contain custom themes here, relative to this directory."
0.3.0,"The name for this set of Sphinx documents.  If None, it defaults to"
0.3.0,"""<project> v<release> documentation""."
0.3.0,html_title = None
0.3.0,A shorter title for the navigation bar.  Default is the same as html_title.
0.3.0,html_short_title = None
0.3.0,The name of an image file (relative to this directory) to place at the top
0.3.0,of the sidebar.
0.3.0,html_logo = None
0.3.0,The name of an image file (within the static path) to use as favicon of the
0.3.0,docs.  This file should be a Windows icon file (.ico) being 16x16 or 32x32
0.3.0,pixels large.
0.3.0,html_favicon = None
0.3.0,"Add any paths that contain custom static files (such as style sheets) here,"
0.3.0,"relative to this directory. They are copied after the builtin static files,"
0.3.0,"so a file named ""default.css"" will overwrite the builtin ""default.css""."
0.3.0,Add any extra paths that contain custom files (such as robots.txt or
0.3.0,".htaccess) here, relative to this directory. These files are copied"
0.3.0,directly to the root of the documentation.
0.3.0,html_extra_path = []
0.3.0,"If not '', a 'Last updated on:' timestamp is inserted at every page bottom,"
0.3.0,using the given strftime format.
0.3.0,"html_last_updated_fmt = '%b %d, %Y'"
0.3.0,"If true, SmartyPants will be used to convert quotes and dashes to"
0.3.0,typographically correct entities.
0.3.0,html_use_smartypants = True
0.3.0,"Custom sidebar templates, maps document names to template names."
0.3.0,html_sidebars = {}
0.3.0,"Additional templates that should be rendered to pages, maps page names to"
0.3.0,template names.
0.3.0,html_additional_pages = {}
0.3.0,"If false, no module index is generated."
0.3.0,html_domain_indices = True
0.3.0,"If false, no index is generated."
0.3.0,html_use_index = True
0.3.0,"If true, the index is split into individual pages for each letter."
0.3.0,html_split_index = False
0.3.0,"If true, links to the reST sources are added to the pages."
0.3.0,html_show_sourcelink = True
0.3.0,"If true, ""Created using Sphinx"" is shown in the HTML footer. Default is True."
0.3.0,html_show_sphinx = True
0.3.0,"If true, ""(C) Copyright ..."" is shown in the HTML footer. Default is True."
0.3.0,html_show_copyright = True
0.3.0,"If true, an OpenSearch description file will be output, and all pages will"
0.3.0,contain a <link> tag referring to it.  The value of this option must be the
0.3.0,base URL from which the finished HTML is served.
0.3.0,html_use_opensearch = ''
0.3.0,"This is the file name suffix for HTML files (e.g. "".xhtml"")."
0.3.0,html_file_suffix = None
0.3.0,Output file base name for HTML help builder.
0.3.0,-- Options for LaTeX output ---------------------------------------------
0.3.0,The paper size ('letterpaper' or 'a4paper').
0.3.0,"'papersize': 'letterpaper',"
0.3.0,"The font size ('10pt', '11pt' or '12pt')."
0.3.0,"'pointsize': '10pt',"
0.3.0,Additional stuff for the LaTeX preamble.
0.3.0,"'preamble': '',"
0.3.0,Grouping the document tree into LaTeX files. List of tuples
0.3.0,"(source start file, target name, title,"
0.3.0,"author, documentclass [howto, manual, or own class])."
0.3.0,The name of an image file (relative to this directory) to place at the top of
0.3.0,the title page.
0.3.0,latex_logo = None
0.3.0,"For ""manual"" documents, if this is true, then toplevel headings are parts,"
0.3.0,not chapters.
0.3.0,latex_use_parts = False
0.3.0,"If true, show page references after internal links."
0.3.0,latex_show_pagerefs = False
0.3.0,"If true, show URL addresses after external links."
0.3.0,latex_show_urls = False
0.3.0,Documents to append as an appendix to all manuals.
0.3.0,latex_appendices = []
0.3.0,"If false, no module index is generated."
0.3.0,latex_domain_indices = True
0.3.0,-- Options for manual page output ---------------------------------------
0.3.0,One entry per manual page. List of tuples
0.3.0,"(source start file, name, description, authors, manual section)."
0.3.0,"If true, show URL addresses after external links."
0.3.0,man_show_urls = False
0.3.0,-- Options for Texinfo output -------------------------------------------
0.3.0,Grouping the document tree into Texinfo files. List of tuples
0.3.0,"(source start file, target name, title, author,"
0.3.0,"dir menu entry, description, category)"
0.3.0,"generate empty examples files, so that we don't get"
0.3.0,inclusion errors if there are no examples for a class / module
0.3.0,touch file
0.3.0,Config for sphinx_issues
0.3.0,Documents to append as an appendix to all manuals.
0.3.0,texinfo_appendices = []
0.3.0,"If false, no module index is generated."
0.3.0,texinfo_domain_indices = True
0.3.0,"How to display URL addresses: 'footnote', 'no', or 'inline'."
0.3.0,texinfo_show_urls = 'footnote'
0.3.0,"If true, do not generate a @detailmenu in the ""Top"" node's menu."
0.3.0,texinfo_no_detailmenu = False
0.3.0,Example configuration for intersphinx: refer to the Python standard library.
0.3.0,The following is used by sphinx.ext.linkcode to provide links to github
0.3.0,get the styles from the current theme
0.3.0,create and add the button to all the code blocks that contain >>>
0.3.0,tracebacks (.gt) contain bare text elements that need to be
0.3.0,wrapped in a span to work with .nextUntil() (see later)
0.3.0,define the behavior of the button when it's clicked
0.3.0,hide the code output
0.3.0,show the code output
0.3.0,-*- coding: utf-8 -*-
0.3.0,Format template for issues URI
0.3.0,e.g. 'https://github.com/sloria/marshmallow/issues/{issue}
0.3.0,"Shortcut for Github, e.g. 'sloria/marshmallow'"
0.3.0,Format template for user profile URI
0.3.0,e.g. 'https://github.com/{user}'
0.3.0,Python 2 only
0.3.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.0,License: MIT
0.3.0,##############################################################################
0.3.0,Creation of an imbalanced data set from a balanced data set
0.3.0,##############################################################################
0.3.0,##############################################################################
0.3.0,We will show how to use the parameter ``ratio`` when dealing with the
0.3.0,"``make_imbalance`` function. For this function, this parameter accepts both"
0.3.0,"dictionary and callable. When using a dictionary, each key will correspond to"
0.3.0,the class of interest and the corresponding value will be the number of
0.3.0,samples desired in this class.
0.3.0,##############################################################################
0.3.0,You might required more flexibility and require your own heuristic to
0.3.0,determine the number of samples by class and you can define your own callable
0.3.0,as follow. In this case we will define a function which will use a float
0.3.0,multiplier to define the number of samples per class.
0.3.0,##############################################################################
0.3.0,Using ``ratio`` in resampling algorithm
0.3.0,##############################################################################
0.3.0,##############################################################################
0.3.0,"In all sampling algorithms, ``ratio`` can be used as illustrated earlier. In"
0.3.0,"addition, some predefined functions are available and can be executed using a"
0.3.0,``str`` with the following choices: (i) ``'minority'``: resample the minority
0.3.0,"class; (ii) ``'majority'``: resample the majority class, (iii) ``'not"
0.3.0,"minority'``: resample all classes apart of the minority class, (iv)"
0.3.0,"``'all'``: resample all classes, and (v) ``'auto'``: correspond to 'all' with"
0.3.0,for over-sampling methods and 'not minority' for under-sampling methods. The
0.3.0,classes targeted will be over-sampled or under-sampled to achieve an equal
0.3.0,number of sample with the majority or minority class.
0.3.0,##############################################################################
0.3.0,"However, you can use the dictionary or the callable options as previously"
0.3.0,mentioned.
0.3.0,Authors: Fernando Nogueira
0.3.0,Christos Aridas
0.3.0,Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.0,License: MIT
0.3.0,Generate the dataset
0.3.0,Instanciate a PCA object for the sake of easy visualisation
0.3.0,Fit and transform x to visualise inside a 2D feature space
0.3.0,Apply regular SMOTE
0.3.0,"Two subplots, unpack the axes array immediately"
0.3.0,Remove axis for second plot
0.3.0,Authors: Christos Aridas
0.3.0,Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.0,License: MIT
0.3.0,Generate the dataset
0.3.0,Instanciate a PCA object for the sake of easy visualisation
0.3.0,Fit and transform x to visualise inside a 2D feature space
0.3.0,Apply the random over-sampling
0.3.0,"Two subplots, unpack the axes array immediately"
0.3.0,make nice plotting
0.3.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.0,License: MIT
0.3.0,generate some data points
0.3.0,plot the majority and minority samples
0.3.0,draw the circle in which the new sample will generated
0.3.0,plot the line on which the sample will be generated
0.3.0,create and plot the new sample
0.3.0,make the plot nicer with legend and label
0.3.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.0,License: MIT
0.3.0,##############################################################################
0.3.0,The following function will be used to create toy dataset. It using the
0.3.0,``make_classification`` from scikit-learn but fixing some parameters.
0.3.0,##############################################################################
0.3.0,The following function will be used to plot the sample space after resampling
0.3.0,to illustrate the characterisitic of an algorithm.
0.3.0,make nice plotting
0.3.0,##############################################################################
0.3.0,The following function will be used to plot the decision function of a
0.3.0,classifier given some data.
0.3.0,##############################################################################
0.3.0,Illustration of the influence of the balancing ratio
0.3.0,##############################################################################
0.3.0,##############################################################################
0.3.0,We will first illustrate the influence of the balancing ratio on some toy
0.3.0,data using a linear SVM classifier. Greater is the difference between the
0.3.0,"number of samples in each class, poorer are the classfication results."
0.3.0,##############################################################################
0.3.0,Random over-sampling to balance the data set
0.3.0,##############################################################################
0.3.0,##############################################################################
0.3.0,Random over-sampling can be used to repeat some samples and balance the
0.3.0,number of samples between the dataset. It can be seen that with this trivial
0.3.0,approach the boundary decision is already less biaised toward the majority
0.3.0,class.
0.3.0,##############################################################################
0.3.0,More advanced over-sampling using ADASYN and SMOTE
0.3.0,##############################################################################
0.3.0,##############################################################################
0.3.0,"Instead of repeating the same samples when over-sampling, we can use some"
0.3.0,specific heuristic instead. ADASYN and SMOTE can be used in this case.
0.3.0,Make an identity sampler
0.3.0,##############################################################################
0.3.0,The following plot illustrate the difference between ADASYN and SMOTE. ADASYN
0.3.0,will focus on the samples which are difficult to classify with a
0.3.0,nearest-neighbors rule while regular SMOTE will not make any distinction.
0.3.0,"Therefore, the decision function depending of the algorithm."
0.3.0,##############################################################################
0.3.0,"Due to those sampling particularities, it can give rise to some specific"
0.3.0,issues as illustrated below.
0.3.0,##############################################################################
0.3.0,SMOTE proposes several variants by identifying specific samples to consider
0.3.0,during the resampling. The borderline version will detect which point to
0.3.0,select which are in the border between two classes. The SVM version will use
0.3.0,the support vectors found using an SVM algorithm to create new samples.
0.3.0,Authors: Christos Aridas
0.3.0,Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.0,License: MIT
0.3.0,Generate the dataset
0.3.0,Instanciate a PCA object for the sake of easy visualisation
0.3.0,Fit and transform x to visualise inside a 2D feature space
0.3.0,Apply the random over-sampling
0.3.0,"Two subplots, unpack the axes array immediately"
0.3.0,make nice plotting
0.3.0,Authors: Christos Aridas
0.3.0,Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.0,License: MIT
0.3.0,Generate the dataset
0.3.0,make nice plotting
0.3.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.0,License: MIT
0.3.0,Generate a dataset
0.3.0,Split the data
0.3.0,Train the classifier with balancing
0.3.0,Test the classifier and get the prediction
0.3.0,Show the classification report
0.3.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.0,License: MIT
0.3.0,Generate a dataset
0.3.0,Split the data
0.3.0,Train the classifier with balancing
0.3.0,Test the classifier and get the prediction
0.3.0,##############################################################################
0.3.0,The geometric mean corresponds to the square root of the product of the
0.3.0,sensitivity and specificity. Combining the two metrics should account for
0.3.0,the balancing of the dataset.
0.3.0,##############################################################################
0.3.0,The index balanced accuracy can transform any metric to be used in
0.3.0,imbalanced learning problems.
0.3.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.0,License: MIT
0.3.0,##############################################################################
0.3.0,The following function will be used to create toy dataset. It using the
0.3.0,``make_classification`` from scikit-learn but fixing some parameters.
0.3.0,##############################################################################
0.3.0,The following function will be used to plot the sample space after resampling
0.3.0,to illustrate the characteristic of an algorithm.
0.3.0,make nice plotting
0.3.0,##############################################################################
0.3.0,The following function will be used to plot the decision function of a
0.3.0,classifier given some data.
0.3.0,##############################################################################
0.3.0,"``SMOTE`` allows to generate samples. However, this method of over-sampling"
0.3.0,"does not have any knowledge regarding the underlying distribution. Therefore,"
0.3.0,"some noisy samples can be generated, e.g. when the different classes cannot"
0.3.0,"be well separated. Hence, it can be beneficial to apply an under-sampling"
0.3.0,algorithm to clean the noisy samples. Two methods are usually used in the
0.3.0,literature: (i) Tomek's link and (ii) edited nearest neighbours cleaning
0.3.0,methods. Imbalanced-learn provides two ready-to-use samplers ``SMOTETomek``
0.3.0,"and ``SMOTEENN``. In general, ``SMOTEENN`` cleans more noisy data than"
0.3.0,``SMOTETomek``.
0.3.0,Authors: Christos Aridas
0.3.0,Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.0,License: MIT
0.3.0,Generate the dataset
0.3.0,Instanciate a PCA object for the sake of easy visualisation
0.3.0,Fit and transform x to visualise inside a 2D feature space
0.3.0,Apply SMOTE + ENN
0.3.0,"Two subplots, unpack the axes array immediately"
0.3.0,make nice plotting
0.3.0,Authors: Christos Aridas
0.3.0,Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.0,License: MIT
0.3.0,Generate the dataset
0.3.0,Instanciate a PCA object for the sake of easy visualisation
0.3.0,Fit and transform x to visualise inside a 2D feature space
0.3.0,Apply SMOTE + Tomek links
0.3.0,"Two subplots, unpack the axes array immediately"
0.3.0,make nice plotting
0.3.0,Authors: Christos Aridas
0.3.0,Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.0,License: MIT
0.3.0,Generate the dataset
0.3.0,Instanciate a PCA object for the sake of easy visualisation
0.3.0,Fit and transform x to visualise inside a 2D feature space
0.3.0,Apply Balance Cascade method
0.3.0,"Two subplots, unpack the axes array immediately"
0.3.0,make nice plotting
0.3.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.0,License: MIT
0.3.0,Authors: Christos Aridas
0.3.0,Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.0,License: MIT
0.3.0,Generate the dataset
0.3.0,Instanciate a PCA object for the sake of easy visualisation
0.3.0,Fit and transform x to visualise inside a 2D feature space
0.3.0,Apply Easy Ensemble
0.3.0,"Two subplots, unpack the axes array immediately"
0.3.0,make nice plotting
0.3.0,Authors: Andreas Mueller
0.3.0,Christos Aridas
0.3.0,Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.0,License: MIT
0.3.0,remove Tomek links
0.3.0,make nice plotting
0.3.0,Authors: Dayvid Oliveira
0.3.0,Christos Aridas
0.3.0,Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.0,License: MIT
0.3.0,Generate the dataset
0.3.0,Instanciate a PCA object for the sake of easy visualisation
0.3.0,Fit and transform x to visualise inside a 2D feature space
0.3.0,"Three subplots, unpack the axes array immediately"
0.3.0,Apply the ENN
0.3.0,Apply the RENN
0.3.0,Apply the AllKNN
0.3.0,Authors: Christos Aridas
0.3.0,Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.0,License: MIT
0.3.0,Generate the dataset
0.3.0,Instanciate a PCA object for the sake of easy visualisation
0.3.0,Fit and transform x to visualise inside a 2D feature space
0.3.0,Apply Nearmiss
0.3.0,"Two subplots, unpack the axes array immediately"
0.3.0,plot the missing samples
0.3.0,Authors: Christos Aridas
0.3.0,Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.0,License: MIT
0.3.0,Generate the dataset
0.3.0,Instanciate a PCA object for the sake of easy visualisation
0.3.0,Fit and transform x to visualise inside a 2D feature space
0.3.0,Apply One-Sided Selection
0.3.0,make nice plotting
0.3.0,Authors: Christos Aridas
0.3.0,Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.0,License: MIT
0.3.0,Generate the dataset
0.3.0,Instanciate a PCA object for the sake of easy visualisation
0.3.0,Fit and transform x to visualise inside a 2D feature space
0.3.0,Apply neighbourhood cleaning rule
0.3.0,make nice plotting
0.3.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.0,License: MIT
0.3.0,##############################################################################
0.3.0,The following function will be used to create toy dataset. It using the
0.3.0,``make_classification`` from scikit-learn but fixing some parameters.
0.3.0,##############################################################################
0.3.0,The following function will be used to plot the sample space after resampling
0.3.0,to illustrate the characteristic of an algorithm.
0.3.0,make nice plotting
0.3.0,##############################################################################
0.3.0,The following function will be used to plot the decision function of a
0.3.0,classifier given some data.
0.3.0,##############################################################################
0.3.0,Prototype generation: under-sampling by generating new samples
0.3.0,##############################################################################
0.3.0,##############################################################################
0.3.0,``ClusterCentroids`` under-samples by replacing the original samples by the
0.3.0,centroids of the cluster found.
0.3.0,##############################################################################
0.3.0,Prototype selection: under-sampling by selecting existing samples
0.3.0,##############################################################################
0.3.0,##############################################################################
0.3.0,The algorithm performing prototype selection can be subdivided into two
0.3.0,groups: (i) the controlled under-sampling methods and (ii) the cleaning
0.3.0,under-sampling methods.
0.3.0,##############################################################################
0.3.0,"With the controlled under-sampling methods, the number of samples to be"
0.3.0,selected can be specified. ``RandomUnderSampler`` is the most naive way of
0.3.0,performing such selection by randomly selecting a given number of samples by
0.3.0,the targetted class.
0.3.0,##############################################################################
0.3.0,``NearMiss`` algorithms implement some heuristic rules in order to select
0.3.0,samples. NearMiss-1 selects samples from the majority class for which the
0.3.0,average distance of the :math:`k`` nearest samples of the minority class is
0.3.0,the smallest. NearMiss-2 selects the samples from the majority class for
0.3.0,which the average distance to the farthest samples of the negative class is
0.3.0,"the smallest. NearMiss-3 is a 2-step algorithm: first, for each minority"
0.3.0,"sample, their ::math:`m` nearest-neighbors will be kept; then, the majority"
0.3.0,samples selected are the on for which the average distance to the :math:`k`
0.3.0,nearest neighbors is the largest.
0.3.0,##############################################################################
0.3.0,``EditedNearestNeighbours`` removes samples of the majority class for which
0.3.0,their class differ from the one of their nearest-neighbors. This sieve can be
0.3.0,repeated which is the principle of the
0.3.0,``RepeatedEditedNearestNeighbours``. ``AllKNN`` is slightly different from
0.3.0,the ``RepeatedEditedNearestNeighbours`` by changing the :math:`k` parameter
0.3.0,"of the internal nearest neighors algorithm, increasing it at each iteration."
0.3.0,##############################################################################
0.3.0,``CondensedNearestNeighbour`` makes use of a 1-NN to iteratively decide if a
0.3.0,sample should be kept in a dataset or not. The issue is that
0.3.0,``CondensedNearestNeighbour`` is sensitive to noise by preserving the noisy
0.3.0,samples. ``OneSidedSelection`` also used the 1-NN and use ``TomekLinks`` to
0.3.0,remove the samples considered noisy. The ``NeighbourhoodCleaningRule`` use a
0.3.0,"``EditedNearestNeighbours`` to remove some sample. Additionally, they use a 3"
0.3.0,nearest-neighbors to remove samples which do not agree with this rule.
0.3.0,##############################################################################
0.3.0,``InstanceHardnessThreshold`` uses the prediction of classifier to exclude
0.3.0,samples. All samples which are classified with a low probability will be
0.3.0,removed.
0.3.0,Authors: Christos Aridas
0.3.0,Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.0,License: MIT
0.3.0,Generate the dataset
0.3.0,Instanciate a PCA object for the sake of easy visualisation
0.3.0,Fit and transform x to visualise inside a 2D feature space
0.3.0,Apply Condensed Nearest Neighbours
0.3.0,make nice plotting
0.3.0,Authors: Fernando Nogueira
0.3.0,Christos Aridas
0.3.0,Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.0,License: MIT
0.3.0,Generate the dataset
0.3.0,Instanciate a PCA object for the sake of easy visualisation
0.3.0,Fit and transform x to visualise inside a 2D feature space
0.3.0,Apply Cluster Centroids
0.3.0,Use hard voting instead of soft voting
0.3.0,"Two subplots, unpack the axes array immediately"
0.3.0,make nice plotting
0.3.0,##############################################################################
0.3.0,This function allows to make nice plotting
0.3.0,##############################################################################
0.3.0,Generate some data with one Tomek link
0.3.0,minority class
0.3.0,majority class
0.3.0,##############################################################################
0.3.0,"In the figure above, the samples highlighted in green form a Tomek link since"
0.3.0,they are of different classes and are nearest neighbours of each other.
0.3.0,highlight the samples of interest
0.3.0,##############################################################################
0.3.0,We can run the ``TomekLinks`` sampling to remove the corresponding
0.3.0,samples. If ``ratio='auto'`` only the sample from the majority class will be
0.3.0,removed. If ``ratio='all'`` both samples will be removed.
0.3.0,highlight the samples of interest
0.3.0,Authors: Christos Aridas
0.3.0,Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.0,License: MIT
0.3.0,Generate the dataset
0.3.0,Instanciate a PCA object for the sake of easy visualisation
0.3.0,Fit and transform x to visualise inside a 2D feature space
0.3.0,Apply the random under-sampling
0.3.0,make nice plotting
0.3.0,Authors: Dayvid Oliveira
0.3.0,Christos Aridas
0.3.0,Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.0,License: MIT
0.3.0,Generate the dataset
0.3.0,"Two subplots, unpack the axes array immediately"
0.3.0,plot samples which have been removed
0.3.0,##############################################################################
0.3.0,This function allows to make nice plotting
0.3.0,##############################################################################
0.3.0,We can start by generating some data to later illustrate the principle of
0.3.0,each NearMiss heuritic rules.
0.3.0,minority class
0.3.0,majority class
0.3.0,##############################################################################
0.3.0,NearMiss-1
0.3.0,##############################################################################
0.3.0,##############################################################################
0.3.0,NearMiss-1 selects samples from the majority class for which the average
0.3.0,distance to some nearest neighbours is the smallest. In the following
0.3.0,"example, we use a 3-NN to compute the average distance on 2 specific samples"
0.3.0,"of the majority class. Therefore, in this case the point linked by the"
0.3.0,green-dashed line will be selected since the average distance is smaller.
0.3.0,##############################################################################
0.3.0,NearMiss-2
0.3.0,##############################################################################
0.3.0,##############################################################################
0.3.0,NearMiss-2 selects samples from the majority class for which the average
0.3.0,distance to the farthest neighbors is the smallest. With the same
0.3.0,"configuration as previously presented, the sample linked to the green-dashed"
0.3.0,line will be selected since its distance the 3 farthest neighbors is the
0.3.0,smallest.
0.3.0,##############################################################################
0.3.0,NearMiss-3
0.3.0,##############################################################################
0.3.0,##############################################################################
0.3.0,"NearMiss-3 can be divided into 2 steps. First, a nearest-neighbors is used to"
0.3.0,short-list samples from the majority class (i.e. correspond to the
0.3.0,"highlighted samples in the following plot). Then, the sample with the largest"
0.3.0,average distance to the *k* nearest-neighbors are selected.
0.3.0,select only the majority point of interest
0.3.0,Authors: Christos Aridas
0.3.0,Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.0,License: MIT
0.3.0,Generate the dataset
0.3.0,Instanciate a PCA object for the sake of easy visualisation
0.3.0,Create the samplers
0.3.0,Create the classifier
0.3.0,Make the splits
0.3.0,Add one transformers and two samplers in the pipeline object
0.3.0,Authors: Christos Aridas
0.3.0,Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.0,License: MIT
0.3.0,Load the dataset
0.3.0,make nice plotting
0.3.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.0,License: MIT
0.3.0,Create a folder to fetch the dataset
0.3.0,Create a pipeline
0.3.0,Classify and report the results
0.3.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.0,License: MIT
0.3.0,##############################################################################
0.3.0,Setting the data set
0.3.0,##############################################################################
0.3.0,##############################################################################
0.3.0,We use a part of the 20 newsgroups data set by loading 4 topics. Using the
0.3.0,"scikit-learn loader, the data are split into a training and a testing set."
0.3.0,
0.3.0,Note the class \#3 is the minority class and has almost twice less samples
0.3.0,than the majority class.
0.3.0,##############################################################################
0.3.0,The usual scikit-learn pipeline
0.3.0,##############################################################################
0.3.0,##############################################################################
0.3.0,You might usually use scikit-learn pipeline by combining the TF-IDF
0.3.0,vectorizer to feed a multinomial naive bayes classifier. A classification
0.3.0,report summarized the results on the testing set.
0.3.0,
0.3.0,"As expected, the recall of the class \#3 is low mainly due to the class"
0.3.0,imbalanced.
0.3.0,##############################################################################
0.3.0,Balancing the class before classification
0.3.0,##############################################################################
0.3.0,##############################################################################
0.3.0,"To improve the prediction of the class \#3, it could be interesting to apply"
0.3.0,"a balancing before to train the naive bayes classifier. Therefore, we will"
0.3.0,use a ``RandomUnderSampler`` to equalize the number of samples in all the
0.3.0,classes before the training.
0.3.0,
0.3.0,It is also important to note that we are using the ``make_pipeline`` function
0.3.0,implemented in imbalanced-learn to properly handle the samplers.
0.3.0,##############################################################################
0.3.0,"Although the results are almost identical, it can be seen that the resampling"
0.3.0,allowed to correct the poor recall of the class \#3 at the cost of reducing
0.3.0,"the other metrics for the other classes. However, the overall results are"
0.3.0,slightly better.
0.3.0,Authors: Dayvid Oliveira
0.3.0,Christos Aridas
0.3.0,Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.0,License: MIT
0.3.0,Generate the dataset
0.3.0,"Two subplots, unpack the axes array immediately"
0.3.0,Based on NiLearn package
0.3.0,License: simplified BSD
0.3.0,"PEP0440 compatible formatted version, see:"
0.3.0,https://www.python.org/dev/peps/pep-0440/
0.3.0,
0.3.0,Generic release markers:
0.3.0,X.Y
0.3.0,X.Y.Z # For bugfix releases
0.3.0,
0.3.0,Admissible pre-release markers:
0.3.0,X.YaN # Alpha release
0.3.0,X.YbN # Beta release
0.3.0,X.YrcN # Release Candidate
0.3.0,X.Y # Final release
0.3.0,
0.3.0,Dev branch marker is: 'X.Y.dev' or 'X.Y.devN' where N is an integer.
0.3.0,'X.Y.dev0' is the canonical version of 'X.Y.dev'
0.3.0,
0.3.0,"This is a tuple to preserve order, so that dependencies are checked"
0.3.0,in some meaningful order (more => less 'core').  We avoid using
0.3.0,collections.OrderedDict to preserve Python 2.6 compatibility.
0.3.0,Avoid choking on modules with no __version__ attribute
0.3.0,Skip check only when installing and it's a module that
0.3.0,will be auto-installed.
0.3.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.0,License: MIT
0.3.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.0,Christos Aridas
0.3.0,License: MIT
0.3.0,Check the consistency of X and y
0.3.0,self.sampling_type is already checked in check_ratio
0.3.0,Adapted from scikit-learn
0.3.0,Author: Edouard Duchesnay
0.3.0,Gael Varoquaux
0.3.0,Virgile Fritsch
0.3.0,Alexandre Gramfort
0.3.0,Lars Buitinck
0.3.0,Christos Aridas
0.3.0,Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.0,License: BSD
0.3.0,BaseEstimator interface
0.3.0,shallow copy of steps
0.3.0,validate names
0.3.0,validate estimators
0.3.0,We allow last estimator to be None as an identity transformation
0.3.0,Estimator interface
0.3.0,Setup the memory
0.3.0,we do not clone when caching is disabled to preserve
0.3.0,backward compatibility
0.3.0,Fit or load from cache the current transfomer
0.3.0,Replace the transformer of the step with the fitted
0.3.0,transformer. This is necessary when loading the transformer
0.3.0,from the cache.
0.3.0,XXX: Calling sample in pipeline it means that the
0.3.0,last estimator is a sampler. Samplers don't carry
0.3.0,"the sampled data. So, call 'fit_sample' in all intermediate"
0.3.0,steps to get the sampled data for the last estimator.
0.3.0,"_final_estimator is None or has transform, otherwise attribute error"
0.3.0,raise AttributeError if necessary for hasattr behaviour
0.3.0,"if we have a weight for this transformer, multiply output"
0.3.0,Boolean controlling whether the joblib caches should be
0.3.0,"flushed if the version of certain modules changes (eg nibabel, as it"
0.3.0,does not respect the backward compatibility in some of its internal
0.3.0,structures
0.3.0,This  is used in nilearn._utils.cache_mixin
0.3.0,list all submodules available in imblearn and version
0.3.0,coding: utf-8
0.3.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.0,Dariusz Brzezinski
0.3.0,License: MIT
0.3.0,Only negative labels
0.3.0,"Calculate tp_sum, pred_sum, true_sum ###"
0.3.0,labels are now from 0 to len(labels) - 1 -> use bincount
0.3.0,Pathological case
0.3.0,Compute the true negative
0.3.0,Retain only selected labels
0.3.0,"Finally, we have all our sufficient statistics. Divide! #"
0.3.0,"Divide, and on zero-division, set scores to 0 and warn:"
0.3.0,"Oddly, we may get an ""invalid"" rather than a ""divide"" error"
0.3.0,here.
0.3.0,Average the results
0.3.0,labels are now from 0 to len(labels) - 1 -> use bincount
0.3.0,Pathological case
0.3.0,Retain only selected labels
0.3.0,old version of scipy return MaskedConstant instead of 0.0
0.3.0,Create the list of tags
0.3.0,check that the scoring function does not need a score
0.3.0,and only a prediction
0.3.0,Compute the score from the scoring function
0.3.0,Square if desired
0.3.0,Get the signature of the sens/spec function
0.3.0,We need to extract from kwargs only the one needed by the
0.3.0,specificity and specificity
0.3.0,Make the intersection between the parameters
0.3.0,Create a sub dictionary
0.3.0,Check if the metric is the geometric mean
0.3.0,We do not support multilabel so the only average supported
0.3.0,is binary
0.3.0,Create the list of parameters through signature binding
0.3.0,Call the sens/spec function
0.3.0,Compute the dominance
0.3.0,Compute the different metrics
0.3.0,Precision/recall/f1
0.3.0,Specificity
0.3.0,Geometric mean
0.3.0,Index balanced accuracy
0.3.0,compute averages
0.3.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.0,Christos Aridas
0.3.0,License: MIT
0.3.0,##############################################################################
0.3.0,Utilities for testing
0.3.0,import some data to play with
0.3.0,restrict to a binary classification task
0.3.0,add noisy features to make the problem harder and avoid perfect results
0.3.0,"run classifier, get class probabilities and label predictions"
0.3.0,only interested in probabilities of the positive case
0.3.0,XXX: do we really want a special API for the binary case?
0.3.0,##############################################################################
0.3.0,Tests
0.3.0,detailed measures for each class
0.3.0,individual scoring function that can be used for grid search: in the
0.3.0,binary class case the score is the value of the measure for the positive
0.3.0,class (e.g. label == 1). This is deprecated for average != 'binary'.
0.3.0,Such a case may occur with non-stratified cross-validation
0.3.0,No average: zeros in array
0.3.0,Macro average is changed
0.3.0,Check for micro
0.3.0,Check for weighted
0.3.0,ensure the above were meaningful tests:
0.3.0,Bad pos_label
0.3.0,Bad average option
0.3.0,but average != 'binary'; even if data is binary
0.3.0,compute the geometric mean for the binary problem
0.3.0,Compute the geometric mean for each of the classes
0.3.0,average tests
0.3.0,print classification report with class names
0.3.0,print classification report with label detection
0.3.0,print classification report with class names
0.3.0,print classification report with label detection
0.3.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.0,Christos Aridas
0.3.0,License: MIT
0.3.0,Get the version
0.3.0,sensitivity scorer
0.3.0,specificity scorer
0.3.0,geometric_mean scorer
0.3.0,make a iba metric before a scorer
0.3.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.0,Christos Aridas
0.3.0,License: MIT
0.3.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.0,Christos Aridas
0.3.0,License: MIT
0.3.0,FIXME: Deprecated in 0.2. To be removed in 0.4.
0.3.0,The ratio is computed using a one-vs-rest manner. Using majority
0.3.0,in multi-class would lead to slightly different results at the
0.3.0,cost of introducing a new parameter.
0.3.0,the nearest neighbors need to be fitted only on the current class
0.3.0,to find the class NN to generate new samples
0.3.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.0,Christos Aridas
0.3.0,License: MIT
0.3.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.0,Fernando Nogueira
0.3.0,Christos Aridas
0.3.0,License: MIT
0.3.0,Samples are in danger for m/2 <= m' < m
0.3.0,Samples are noise for m = m'
0.3.0,"FIXME Deprecated in 0.2, to be removed in 0.4"
0.3.0,divergence between borderline-1 and borderline-2
0.3.0,Create synthetic samples for borderline points.
0.3.0,only minority
0.3.0,we use a one-vs-rest policy to handle the multiclass in which
0.3.0,new samples will be created considering not only the majority
0.3.0,class but all over classes.
0.3.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.0,Christos Aridas
0.3.0,License: MIT
0.3.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.0,Christos Aridas
0.3.0,License: MIT
0.3.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.0,Christos Aridas
0.3.0,License: MIT
0.3.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.0,License: MIT
0.3.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.0,Fernando Nogueira
0.3.0,Christos Aridas
0.3.0,License: MIT
0.3.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.0,Christos Aridas
0.3.0,License: MIT
0.3.0,FIXME: Deprecated in 0.2. To be removed in 0.4.
0.3.0,select a sample from the current class
0.3.0,create the set composed of all minority samples and one
0.3.0,sample from the current class.
0.3.0,create the set S with removing the seed from S
0.3.0,since that it will be added anyway
0.3.0,apply Tomek cleaning
0.3.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.0,Christos Aridas
0.3.0,License: MIT
0.3.0,Compute the distance considering the farthest neighbour
0.3.0,Sort the list of distance and get the index
0.3.0,Throw a warning to tell the user that we did not have enough samples
0.3.0,to select and that we just select everything
0.3.0,Select the desired number of samples
0.3.0,FIXME: Deprecated in 0.2. To be removed in 0.4.
0.3.0,idx_tmp is relative to the feature selected in the
0.3.0,previous step and we need to find the indirection
0.3.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.0,Christos Aridas
0.3.0,License: MIT
0.3.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.0,Fernando Nogueira
0.3.0,Christos Aridas
0.3.0,License: MIT
0.3.0,find which class to not consider
0.3.0,there is a Tomek link between two samples if they are both nearest
0.3.0,neighbors of each others.
0.3.0,Find the nearest neighbour of every point
0.3.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.0,Christos Aridas
0.3.0,License: MIT
0.3.0,FIXME: Deprecated in 0.2. To be removed in 0.4
0.3.0,Randomly get one sample from the majority class
0.3.0,Generate the index to select
0.3.0,Create the set C - One majority samples and all minority
0.3.0,Create the set S - all majority samples
0.3.0,fit knn on C
0.3.0,Check each sample in S if we keep it or drop it
0.3.0,Do not select sample which are already well classified
0.3.0,Classify on S
0.3.0,If the prediction do not agree with the true label
0.3.0,append it in C_x
0.3.0,Keep the index for later
0.3.0,Update C
0.3.0,fit a knn on C
0.3.0,This experimental to speed up the search
0.3.0,Classify all the element in S and avoid to test the
0.3.0,well classified elements
0.3.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.0,Christos Aridas
0.3.0,License: MIT
0.3.0,FIXME: Deprecated from 0.2. To be removed in 0.4.
0.3.0,clean the neighborhood
0.3.0,compute which classes to consider for cleaning for the A2 group
0.3.0,compute a2 group
0.3.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.0,Dayvid Oliveira
0.3.0,Christos Aridas
0.3.0,License: MIT
0.3.0,FIXME: Deprecated in 0.2. To be removed in 0.4
0.3.0,Check the stopping criterion
0.3.0,1. If there is no changes for the vector y
0.3.0,2. If the number of samples in the other class become inferior to
0.3.0,the number of samples in the majority class
0.3.0,3. If one of the class is disappearing
0.3.0,Case 1
0.3.0,Case 2
0.3.0,Case 3
0.3.0,Check the stopping criterion
0.3.0,1. If the number of samples in the other class become inferior to
0.3.0,the number of samples in the majority class
0.3.0,2. If one of the class is disappearing
0.3.0,Case 1
0.3.0,overwrite b_min_bec_maj
0.3.0,Case 2
0.3.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.0,Dayvid Oliveira
0.3.0,Christos Aridas
0.3.0,License: MIT
0.3.0,To be removed in 0.4
0.3.0,Select the appropriate classifier
0.3.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.0,Christos Aridas
0.3.0,License: MIT
0.3.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.0,Christos Aridas
0.3.0,License: MIT
0.3.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.0,Christos Aridas
0.3.0,License: MIT
0.3.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.0,Christos Aridas
0.3.0,License: MIT
0.3.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.0,Christos Aridas
0.3.0,License: MIT
0.3.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.0,Christos Aridas
0.3.0,License: MIT
0.3.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.0,Christos Aridas
0.3.0,License: MIT
0.3.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.0,Christos Aridas
0.3.0,License: MIT
0.3.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.0,Christos Aridas
0.3.0,License: MIT
0.3.0,FIXME remove at the end of the deprecation 0.4
0.3.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.0,Christos Aridas
0.3.0,License: MIT
0.3.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.0,Christos Aridas
0.3.0,License: MIT
0.3.0,test that all_estimators doesn't find abstract classes.
0.3.0,some can just not be sensibly default constructed
0.3.0,input validation etc for non-meta estimators
0.3.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.0,Christos Aridas
0.3.0,License: MIT
0.3.0,store timestamp to figure out whether the result of 'fit' has been
0.3.0,cached or not
0.3.0,store timestamp to figure out whether the result of 'fit' has been
0.3.0,cached or not
0.3.0,Test the various init parameters of the pipeline.
0.3.0,Check that we can't instantiate pipelines with objects without fit
0.3.0,method
0.3.0,Smoke test with only an estimator
0.3.0,Check that params are set
0.3.0,Smoke test the repr:
0.3.0,Test with two objects
0.3.0,Check that we can't instantiate with non-transformers on the way
0.3.0,"Note that NoTrans implements fit, but not transform"
0.3.0,Check that params are set
0.3.0,Smoke test the repr:
0.3.0,Check that params are not set when naming them wrong
0.3.0,Test clone
0.3.0,"Check that apart from estimators, the parameters are the same"
0.3.0,Remove estimators that where copied
0.3.0,Test the various methods of the pipeline (anova).
0.3.0,Test with Anova + LogisticRegression
0.3.0,Test that the pipeline can take fit parameters
0.3.0,classifier should return True
0.3.0,and transformer params should not be changed
0.3.0,invalid parameters should raise an error message
0.3.0,Pipeline should pass sample_weight
0.3.0,When sample_weight is None it shouldn't be passed
0.3.0,Test pipeline raises set params error message for nested models.
0.3.0,nested model check
0.3.0,Test the various methods of the pipeline (pca + svm).
0.3.0,Test with PCA + SVC
0.3.0,Test the various methods of the pipeline (preprocessing + svm).
0.3.0,check shapes of various prediction functions
0.3.0,test that the fit_predict method is implemented on a pipeline
0.3.0,test that the fit_predict on pipeline yields same results as applying
0.3.0,transform and clustering steps separately
0.3.0,"As pipeline doesn't clone estimators on construction,"
0.3.0,it must have its own estimators
0.3.0,first compute the transform and clustering step separately
0.3.0,use a pipeline to do the transform and clustering in one step
0.3.0,tests that a pipeline does not have fit_predict method when final
0.3.0,step of pipeline does not have fit_predict defined
0.3.0,tests that Pipeline passes fit_params to intermediate steps
0.3.0,when fit_predict is invoked
0.3.0,Test whether pipeline works with a transformer at the end.
0.3.0,Also test pipeline.transform and pipeline.inverse_transform
0.3.0,test transform and fit_transform:
0.3.0,Test whether pipeline works with a transformer missing fit_transform
0.3.0,test fit_transform:
0.3.0,Directly setting attr
0.3.0,Using set_params
0.3.0,Using set_params to replace single step
0.3.0,With invalid data
0.3.0,Test setting Pipeline steps to None
0.3.0,"for other methods, ensure no AttributeErrors on None:"
0.3.0,mult2 and mult3 are active
0.3.0,Check None step at construction time
0.3.0,Test that an error is raised when memory is not a string or a Memory
0.3.0,instance
0.3.0,Define memory as an integer
0.3.0,Test with Transformer + SVC
0.3.0,Memoize the transformer at the first fit
0.3.0,Get the time stamp of the tranformer in the cached pipeline
0.3.0,Check that cached_pipe and pipe yield identical results
0.3.0,Check that we are reading the cache while fitting
0.3.0,a second time
0.3.0,Check that cached_pipe and pipe yield identical results
0.3.0,Create a new pipeline with cloned estimators
0.3.0,Check that even changing the name step does not affect the cache hit
0.3.0,Check that cached_pipe and pipe yield identical results
0.3.0,Test with Transformer + SVC
0.3.0,Memoize the transformer at the first fit
0.3.0,Get the time stamp of the tranformer in the cached pipeline
0.3.0,Check that cached_pipe and pipe yield identical results
0.3.0,Check that we are reading the cache while fitting
0.3.0,a second time
0.3.0,Check that cached_pipe and pipe yield identical results
0.3.0,Create a new pipeline with cloned estimators
0.3.0,Check that even changing the name step does not affect the cache hit
0.3.0,Check that cached_pipe and pipe yield identical results
0.3.0,Test the various methods of the pipeline (pca + svm).
0.3.0,Test with PCA + SVC
0.3.0,Test the various methods of the pipeline (pca + svm).
0.3.0,Test with PCA + SVC
0.3.0,Test whether pipeline works with a sampler at the end.
0.3.0,Also test pipeline.sampler
0.3.0,test transform and fit_transform:
0.3.0,We round the value near to zero. It seems that PCA has some issue
0.3.0,with that
0.3.0,Test whether pipeline works with a sampler at the end.
0.3.0,Also test pipeline.sampler
0.3.0,Test pipeline using None as preprocessing step and a classifier
0.3.0,"Test pipeline using None, RUS and a classifier"
0.3.0,"Test pipeline using RUS, None and a classifier"
0.3.0,Test pipeline using None step and a sampler
0.3.0,Test pipeline using None and a transformer that implements transform and
0.3.0,inverse_transform
0.3.0,Test the various methods of the pipeline (anova).
0.3.0,Test with RandomUnderSampling + Anova + LogisticRegression
0.3.0,Test the various methods of the pipeline (anova).
0.3.0,Test the various methods of the pipeline (anova).
0.3.0,Test with RandomUnderSampling + Anova + LogisticRegression
0.3.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.0,Christos Aridas
0.3.0,License: MIT
0.3.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.0,License: MIT
0.3.0,Adapated from scikit-learn
0.3.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.0,License: MIT
0.3.0,make the checks from scikit-learn
0.3.0,trigger our checks if this is a SamplerMixin
0.3.0,FIXME already present in scikit-learn 0.19
0.3.0,test scikit-learn compatibility
0.3.0,Estimators in mono_output_task_error raise ValueError if y is of 1-D
0.3.0,Convert into a 2-D y for those estimators.
0.3.0,check that fit method only changes or sets private attributes
0.3.0,to not check deprecated classes
0.3.0,check that fit doesn't add any public attribute
0.3.0,check that fit doesn't change any public attribute
0.3.0,in this test we will force all samplers to not change the class 1
0.3.0,check that sparse matrices can be passed through the sampler leading to
0.3.0,the same results than dense
0.3.0,set KMeans to full since it support sparse and dense
0.3.0,Check that the samplers handle pandas dataframe and pandas series
0.3.0,Adapted from scikit-learn
0.3.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.0,License: MIT
0.3.0,meta-estimators need another estimator to be instantiated.
0.3.0,estimators that there is no way to default-construct sensibly
0.3.0,some strange ones
0.3.0,get parent folder
0.3.0,get rid of abstract base classes
0.3.0,get rid of sklearn estimators which have been imported in some classes
0.3.0,possibly get rid of meta estimators
0.3.0,"drop duplicates, sort for reproducibility"
0.3.0,itemgetter is used to ensure the sort does not extend to the 2nd item of
0.3.0,the tuple
0.3.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.0,License: MIT
0.3.0,FIXME: perfectly we should raise an error but the sklearn API does
0.3.0,not allow for it
0.3.0,check that all keys in ratio are also in y
0.3.0,check that there is no negative number
0.3.0,clean-sampling can be more permissive since those samplers do not
0.3.0,use samples
0.3.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.0,Christos Aridas
0.3.0,License: MIT
0.3.0,this function could create an equal number of samples
0.3.0,"tests that the estimator actually fails on ""bad"" estimators."
0.3.0,"not a complete test of all checks, which are very extensive."
0.3.0,check that we have a set_params and can clone
0.3.0,check that we have a fit method
0.3.0,check that fit does input validation
0.3.0,check that predict does input validation (doesn't accept dicts in input)
0.3.0,check that estimator state does not change
0.3.0,at transform/predict/predict_proba time
0.3.0,check that `fit` only changes attributes that
0.3.0,are private (start with an _ or end with a _).
0.3.0,check that `fit` doesn't add any public attribute
0.3.0,check for sparse matrix input handling
0.3.0,"the check for sparse input handling prints to the stdout,"
0.3.0,"instead of raising an error, so as not to remove the original traceback."
0.3.0,that means we need to jump through some hoops to catch it.
0.3.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.0,License: MIT
0.3.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.0,Christos Aridas
0.3.0,License: MIT
0.3.0,check if the filtering is working with a list or a single string
0.3.0,check that all estimators are sampler
0.3.0,check that an error is raised when the type is unknown
0.3.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.0,Christos Aridas
0.3.0,License: MIT
0.3.0,Check any parameters for SMOTE was provided
0.3.0,Anounce deprecation
0.3.0,We need to list each parameter and decide if we affect a default
0.3.0,value or not
0.3.0,"If an object was given, affect"
0.3.0,Otherwise create a default SMOTE
0.3.0,Check any parameters for ENN was provided
0.3.0,Anounce deprecation
0.3.0,We need to list each parameter and decide if we affect a default
0.3.0,value or not
0.3.0,"If an object was given, affect"
0.3.0,Otherwise create a default EditedNearestNeighbours
0.3.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.0,Christos Aridas
0.3.0,License: MIT
0.3.0,Check any parameters for SMOTE was provided
0.3.0,Anounce deprecation
0.3.0,We need to list each parameter and decide if we affect a default
0.3.0,value or not
0.3.0,"If an object was given, affect"
0.3.0,Otherwise create a default SMOTE
0.3.0,Check any parameters for ENN was provided
0.3.0,Anounce deprecation
0.3.0,"If an object was given, affect"
0.3.0,Otherwise create a default TomekLinks
0.3.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.0,Christos Aridas
0.3.0,License: MIT
0.3.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.0,Christos Aridas
0.3.0,License: MIT
0.3.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.0,Christos Aridas
0.3.0,License: MIT
0.3.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.0,Christos Aridas
0.3.0,License: MIT
0.3.0,RandomUnderSampler is not supporting sample_weight. We need to pass
0.3.0,None.
0.3.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.0,Christos Aridas
0.3.0,License: MIT
0.3.0,doctest: +ELLIPSIS
0.3.0,To be removed in 0.4
0.3.0,Define the classifier to use
0.3.0,array to know which samples are available to be taken
0.3.0,where the different set will be stored
0.3.0,store the index of the data to under-sample
0.3.0,value which will be picked at each round
0.3.0,extract the data of interest for this round from the
0.3.0,current class
0.3.0,select randomly the desired features
0.3.0,store the set created
0.3.0,fit and predict using cross validation
0.3.0,extract the prediction about the targeted classes only
0.3.0,check the stopping criterion
0.3.0,check that there is enough samples for another round
0.3.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.0,License: MIT
0.3.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.0,Christos Aridas
0.3.0,License: MIT
0.3.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.0,Christos Aridas
0.3.0,License: MIT
0.3.0,Check classification for various parameter settings.
0.3.0,Test that bootstrapping samples generate non-perfect base estimators.
0.3.0,"without bootstrap, all trees are perfect on the training set"
0.3.0,disable the resampling by passing an empty dictionary.
0.3.0,"with bootstrap, trees are no longer perfect on the training set"
0.3.0,Test that bootstrapping features may generate duplicate features.
0.3.0,Predict probabilities.
0.3.0,Normal case
0.3.0,"Degenerate case, where some classes are missing"
0.3.0,Check that oob prediction is a good estimation of the generalization
0.3.0,error.
0.3.0,Test with few estimators
0.3.0,Check singleton ensembles.
0.3.0,Test that it gives proper exception on deficient input.
0.3.0,Test n_estimators
0.3.0,Test max_samples
0.3.0,Test max_features
0.3.0,Test support of decision_function
0.3.0,Check that bagging ensembles can be grid-searched.
0.3.0,Transform iris into a binary classification task
0.3.0,Grid search with scoring based on decision_function
0.3.0,Check base_estimator and its default values.
0.3.0,Test if fitting incrementally with warm start gives a forest of the
0.3.0,right size and the same results as a normal fit.
0.3.0,Test if warm start'ed second fit with smaller n_estimators raises error.
0.3.0,Test that nothing happens when fitting without increasing n_estimators
0.3.0,"modify X to nonsense values, this should not change anything"
0.3.0,warm started classifier with 5+5 estimators should be equivalent to
0.3.0,one classifier with 10 estimators
0.3.0,Check using oob_score and warm_start simultaneously fails
0.3.0,"Make sure OOB scores are identical when random_state, estimator, and"
0.3.0,training data are fixed and fitting is done twice
0.3.0,Check that format of estimators_samples_ is correct and that results
0.3.0,generated at fit time can be identically reproduced at a later time
0.3.0,using data saved in object attributes.
0.3.0,remap the y outside of the BalancedBaggingclassifier
0.3.0,"_, y = np.unique(y, return_inverse=True)"
0.3.0,Get relevant attributes
0.3.0,Test for correct formatting
0.3.0,Re-fit single estimator to test for consistent sampling
0.3.0,Make sure validated max_samples and original max_samples are identical
0.3.0,when valid integer max_samples supplied by user
0.3.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.0,Christos Aridas
0.3.0,License: MIT
0.3.0,Generate a global dataset to use
0.3.0,Define a ratio
0.3.0,Define the ratio parameter
0.3.0,Create the sampling object
0.3.0,Get the different subset
0.3.0,Define the ratio parameter
0.3.0,Create the sampling object
0.3.0,Get the different subset
0.3.0,Define the ratio parameter
0.3.0,Create the sampling object
0.3.0,Get the different subset
0.3.0,Author: Guillaume Lemaitre
0.3.0,License: BSD 3 clause
0.3.0,"The index start at one, then we need to remove one"
0.3.0,to not have issue with the indexing.
0.3.0,go through the list and check if the data are available
0.3.0,Authors: Dayvid Oliveira
0.3.0,Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.0,Christos Aridas
0.3.0,License: MIT
0.3.0,restrict ratio to be a dict or a callable
0.3.0,FIXME: deprecated in 0.2 to be removed in 0.4
0.3.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.0,Christos Aridas
0.3.0,License: MIT
0.3.0,"we are reusing part of utils.check_ratio, however this is not cover in"
0.3.0,the common tests so we will repeat it here
0.3.0,FIXME: to be removed in 0.4 due to deprecation
0.3.0,resample without using min_c_
0.3.0,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
0.3.0,Christos Aridas
0.3.0,License: MIT
0.2.1,! /usr/bin/env python
0.2.1,"load all vars into globals, otherwise"
0.2.1,the later function call using global vars doesn't work.
0.2.1,"Allow command-lines such as ""python setup.py build install"""
0.2.1,Make sources available using relative paths from this file's directory.
0.2.1,-*- coding: utf-8 -*-
0.2.1,
0.2.1,"imbalanced-learn documentation build configuration file, created by"
0.2.1,sphinx-quickstart on Mon Jan 18 14:44:12 2016.
0.2.1,
0.2.1,This file is execfile()d with the current directory set to its
0.2.1,containing dir.
0.2.1,
0.2.1,Note that not all possible configuration values are present in this
0.2.1,autogenerated file.
0.2.1,
0.2.1,All configuration values have a default; values that are commented out
0.2.1,serve to show the default.
0.2.1,"If extensions (or modules to document with autodoc) are in another directory,"
0.2.1,add these directories to sys.path here. If the directory is relative to the
0.2.1,"documentation root, use os.path.abspath to make it absolute, like shown here."
0.2.1,"sys.path.insert(0, os.path.abspath('.'))"
0.2.1,-- General configuration ---------------------------------------------------
0.2.1,Try to override the matplotlib configuration as early as possible
0.2.1,-- General configuration ------------------------------------------------
0.2.1,"If your documentation needs a minimal Sphinx version, state it here."
0.2.1,needs_sphinx = '1.0'
0.2.1,"Add any Sphinx extension module names here, as strings. They can be"
0.2.1,extensions coming with Sphinx (named 'sphinx.ext.*') or your custom
0.2.1,ones.
0.2.1,path to your examples scripts
0.2.1,path where to save gallery generated examples
0.2.1,"Add any paths that contain templates here, relative to this directory."
0.2.1,generate autosummary even if no references
0.2.1,The suffix of source filenames.
0.2.1,The encoding of source files.
0.2.1,source_encoding = 'utf-8-sig'
0.2.1,Generate the plots for the gallery
0.2.1,The master toctree document.
0.2.1,General information about the project.
0.2.1,"The version info for the project you're documenting, acts as replacement for"
0.2.1,"|version| and |release|, also used in various other places throughout the"
0.2.1,built documents.
0.2.1,
0.2.1,The short X.Y version.
0.2.1,"The full version, including alpha/beta/rc tags."
0.2.1,The language for content autogenerated by Sphinx. Refer to documentation
0.2.1,for a list of supported languages.
0.2.1,language = None
0.2.1,"There are two options for replacing |today|: either, you set today to some"
0.2.1,"non-false value, then it is used:"
0.2.1,today = ''
0.2.1,"Else, today_fmt is used as the format for a strftime call."
0.2.1,"today_fmt = '%B %d, %Y'"
0.2.1,"List of patterns, relative to source directory, that match files and"
0.2.1,directories to ignore when looking for source files.
0.2.1,The reST default role (used for this markup: `text`) to use for all
0.2.1,documents.
0.2.1,default_role = None
0.2.1,"If true, '()' will be appended to :func: etc. cross-reference text."
0.2.1,"If true, the current module name will be prepended to all description"
0.2.1,unit titles (such as .. function::).
0.2.1,add_module_names = True
0.2.1,"If true, sectionauthor and moduleauthor directives will be shown in the"
0.2.1,output. They are ignored by default.
0.2.1,show_authors = False
0.2.1,The name of the Pygments (syntax highlighting) style to use.
0.2.1,A list of ignored prefixes for module index sorting.
0.2.1,modindex_common_prefix = []
0.2.1,"If true, keep warnings as ""system message"" paragraphs in the built documents."
0.2.1,keep_warnings = False
0.2.1,-- Options for HTML output ----------------------------------------------
0.2.1,The theme to use for HTML and HTML Help pages.  See the documentation for
0.2.1,a list of builtin themes.
0.2.1,Theme options are theme-specific and customize the look and feel of a theme
0.2.1,"further.  For a list of options available for each theme, see the"
0.2.1,documentation.
0.2.1,html_theme_options = {}
0.2.1,"Add any paths that contain custom themes here, relative to this directory."
0.2.1,"The name for this set of Sphinx documents.  If None, it defaults to"
0.2.1,"""<project> v<release> documentation""."
0.2.1,html_title = None
0.2.1,A shorter title for the navigation bar.  Default is the same as html_title.
0.2.1,html_short_title = None
0.2.1,The name of an image file (relative to this directory) to place at the top
0.2.1,of the sidebar.
0.2.1,html_logo = None
0.2.1,The name of an image file (within the static path) to use as favicon of the
0.2.1,docs.  This file should be a Windows icon file (.ico) being 16x16 or 32x32
0.2.1,pixels large.
0.2.1,html_favicon = None
0.2.1,"Add any paths that contain custom static files (such as style sheets) here,"
0.2.1,"relative to this directory. They are copied after the builtin static files,"
0.2.1,"so a file named ""default.css"" will overwrite the builtin ""default.css""."
0.2.1,Add any extra paths that contain custom files (such as robots.txt or
0.2.1,".htaccess) here, relative to this directory. These files are copied"
0.2.1,directly to the root of the documentation.
0.2.1,html_extra_path = []
0.2.1,"If not '', a 'Last updated on:' timestamp is inserted at every page bottom,"
0.2.1,using the given strftime format.
0.2.1,"html_last_updated_fmt = '%b %d, %Y'"
0.2.1,"If true, SmartyPants will be used to convert quotes and dashes to"
0.2.1,typographically correct entities.
0.2.1,html_use_smartypants = True
0.2.1,"Custom sidebar templates, maps document names to template names."
0.2.1,html_sidebars = {}
0.2.1,"Additional templates that should be rendered to pages, maps page names to"
0.2.1,template names.
0.2.1,html_additional_pages = {}
0.2.1,"If false, no module index is generated."
0.2.1,html_domain_indices = True
0.2.1,"If false, no index is generated."
0.2.1,html_use_index = True
0.2.1,"If true, the index is split into individual pages for each letter."
0.2.1,html_split_index = False
0.2.1,"If true, links to the reST sources are added to the pages."
0.2.1,html_show_sourcelink = True
0.2.1,"If true, ""Created using Sphinx"" is shown in the HTML footer. Default is True."
0.2.1,html_show_sphinx = True
0.2.1,"If true, ""(C) Copyright ..."" is shown in the HTML footer. Default is True."
0.2.1,html_show_copyright = True
0.2.1,"If true, an OpenSearch description file will be output, and all pages will"
0.2.1,contain a <link> tag referring to it.  The value of this option must be the
0.2.1,base URL from which the finished HTML is served.
0.2.1,html_use_opensearch = ''
0.2.1,"This is the file name suffix for HTML files (e.g. "".xhtml"")."
0.2.1,html_file_suffix = None
0.2.1,Output file base name for HTML help builder.
0.2.1,-- Options for LaTeX output ---------------------------------------------
0.2.1,The paper size ('letterpaper' or 'a4paper').
0.2.1,"'papersize': 'letterpaper',"
0.2.1,"The font size ('10pt', '11pt' or '12pt')."
0.2.1,"'pointsize': '10pt',"
0.2.1,Additional stuff for the LaTeX preamble.
0.2.1,"'preamble': '',"
0.2.1,Grouping the document tree into LaTeX files. List of tuples
0.2.1,"(source start file, target name, title,"
0.2.1,"author, documentclass [howto, manual, or own class])."
0.2.1,The name of an image file (relative to this directory) to place at the top of
0.2.1,the title page.
0.2.1,latex_logo = None
0.2.1,"For ""manual"" documents, if this is true, then toplevel headings are parts,"
0.2.1,not chapters.
0.2.1,latex_use_parts = False
0.2.1,"If true, show page references after internal links."
0.2.1,latex_show_pagerefs = False
0.2.1,"If true, show URL addresses after external links."
0.2.1,latex_show_urls = False
0.2.1,Documents to append as an appendix to all manuals.
0.2.1,latex_appendices = []
0.2.1,"If false, no module index is generated."
0.2.1,latex_domain_indices = True
0.2.1,-- Options for manual page output ---------------------------------------
0.2.1,One entry per manual page. List of tuples
0.2.1,"(source start file, name, description, authors, manual section)."
0.2.1,"If true, show URL addresses after external links."
0.2.1,man_show_urls = False
0.2.1,-- Options for Texinfo output -------------------------------------------
0.2.1,Grouping the document tree into Texinfo files. List of tuples
0.2.1,"(source start file, target name, title, author,"
0.2.1,"dir menu entry, description, category)"
0.2.1,"generate empty examples files, so that we don't get"
0.2.1,inclusion errors if there are no examples for a class / module
0.2.1,touch file
0.2.1,Documents to append as an appendix to all manuals.
0.2.1,texinfo_appendices = []
0.2.1,"If false, no module index is generated."
0.2.1,texinfo_domain_indices = True
0.2.1,"How to display URL addresses: 'footnote', 'no', or 'inline'."
0.2.1,texinfo_show_urls = 'footnote'
0.2.1,"If true, do not generate a @detailmenu in the ""Top"" node's menu."
0.2.1,texinfo_no_detailmenu = False
0.2.1,Example configuration for intersphinx: refer to the Python standard library.
0.2.1,Define some color for the plotting
0.2.1,Generate the dataset
0.2.1,Instanciate a PCA object for the sake of easy visualisation
0.2.1,Fit and transform x to visualise inside a 2D feature space
0.2.1,Apply SMOTE SVM
0.2.1,"Two subplots, unpack the axes array immediately"
0.2.1,Define some color for the plotting
0.2.1,Generate the dataset
0.2.1,Instanciate a PCA object for the sake of easy visualisation
0.2.1,Fit and transform x to visualise inside a 2D feature space
0.2.1,Apply Borderline SMOTE 2
0.2.1,"Two subplots, unpack the axes array immediately"
0.2.1,Define some color for the plotting
0.2.1,Generate the dataset
0.2.1,Instanciate a PCA object for the sake of easy visualisation
0.2.1,Fit and transform x to visualise inside a 2D feature space
0.2.1,Apply regular SMOTE
0.2.1,"Two subplots, unpack the axes array immediately"
0.2.1,Define some color for the plotting
0.2.1,Generate the dataset
0.2.1,Instanciate a PCA object for the sake of easy visualisation
0.2.1,Fit and transform x to visualise inside a 2D feature space
0.2.1,Apply the random over-sampling
0.2.1,"Two subplots, unpack the axes array immediately"
0.2.1,Define some color for the plotting
0.2.1,Generate the dataset
0.2.1,Instanciate a PCA object for the sake of easy visualisation
0.2.1,Fit and transform x to visualise inside a 2D feature space
0.2.1,Apply Borderline SMOTE 1
0.2.1,"Two subplots, unpack the axes array immediately"
0.2.1,Define some color for the plotting
0.2.1,Generate the dataset
0.2.1,Instanciate a PCA object for the sake of easy visualisation
0.2.1,Fit and transform x to visualise inside a 2D feature space
0.2.1,Apply the random over-sampling
0.2.1,"Two subplots, unpack the axes array immediately"
0.2.1,Generate the dataset
0.2.1,Generate a dataset
0.2.1,Split the data
0.2.1,Train the classifier with balancing
0.2.1,Test the classifier and get the prediction
0.2.1,Show the classification report
0.2.1,Generate a dataset
0.2.1,Split the data
0.2.1,Train the classifier with balancing
0.2.1,Test the classifier and get the prediction
0.2.1,##############################################################################
0.2.1,The geometric mean corresponds to the square root of the product of the
0.2.1,sensitivity and specificity. Combining the two metrics should account for
0.2.1,the balancing of the dataset.
0.2.1,##############################################################################
0.2.1,The index balanced accuracy can transform any metric to be used in
0.2.1,imbalanced learning problems.
0.2.1,Define some color for the plotting
0.2.1,Generate the dataset
0.2.1,Instanciate a PCA object for the sake of easy visualisation
0.2.1,Fit and transform x to visualise inside a 2D feature space
0.2.1,Apply SMOTE + ENN
0.2.1,"Two subplots, unpack the axes array immediately"
0.2.1,Define some color for the plotting
0.2.1,Generate the dataset
0.2.1,Instanciate a PCA object for the sake of easy visualisation
0.2.1,Fit and transform x to visualise inside a 2D feature space
0.2.1,Apply SMOTE + Tomek links
0.2.1,"Two subplots, unpack the axes array immediately"
0.2.1,Define some color for the plotting
0.2.1,Generate the dataset
0.2.1,Instanciate a PCA object for the sake of easy visualisation
0.2.1,Fit and transform x to visualise inside a 2D feature space
0.2.1,Apply Balance Cascade method
0.2.1,"Two subplots, unpack the axes array immediately"
0.2.1,Define some color for the plotting
0.2.1,Generate the dataset
0.2.1,Instanciate a PCA object for the sake of easy visualisation
0.2.1,Fit and transform x to visualise inside a 2D feature space
0.2.1,Apply Easy Ensemble
0.2.1,"Two subplots, unpack the axes array immediately"
0.2.1,Define some color for the plotting
0.2.1,Generate the dataset
0.2.1,Instanciate a PCA object for the sake of easy visualisation
0.2.1,Fit and transform x to visualise inside a 2D feature space
0.2.1,"Three subplots, unpack the axes array immediately"
0.2.1,Apply the ENN
0.2.1,Apply the RENN
0.2.1,Apply the AllKNN
0.2.1,Define some color for the plotting
0.2.1,Generate the dataset
0.2.1,Instanciate a PCA object for the sake of easy visualisation
0.2.1,Fit and transform x to visualise inside a 2D feature space
0.2.1,"Three subplots, unpack the axes array immediately"
0.2.1,Apply the ENN
0.2.1,Apply the RENN
0.2.1,Define some color for the plotting
0.2.1,Generate the dataset
0.2.1,Instanciate a PCA object for the sake of easy visualisation
0.2.1,Fit and transform x to visualise inside a 2D feature space
0.2.1,Apply Nearmiss 3
0.2.1,"Two subplots, unpack the axes array immediately"
0.2.1,Define some color for the plotting
0.2.1,Generate the dataset
0.2.1,Instanciate a PCA object for the sake of easy visualisation
0.2.1,Fit and transform x to visualise inside a 2D feature space
0.2.1,Apply Edited Nearest Neighbours
0.2.1,"Two subplots, unpack the axes array immediately"
0.2.1,Define some color for the plotting
0.2.1,Generate the dataset
0.2.1,Instanciate a PCA object for the sake of easy visualisation
0.2.1,Fit and transform x to visualise inside a 2D feature space
0.2.1,Apply Tomek Links cleaning
0.2.1,"Two subplots, unpack the axes array immediately"
0.2.1,Define some color for the plotting
0.2.1,Generate the dataset
0.2.1,Instanciate a PCA object for the sake of easy visualisation
0.2.1,Fit and transform x to visualise inside a 2D feature space
0.2.1,Apply One-Sided Selection
0.2.1,"Two subplots, unpack the axes array immediately"
0.2.1,Define some color for the plotting
0.2.1,Generate the dataset
0.2.1,Instanciate a PCA object for the sake of easy visualisation
0.2.1,Fit and transform x to visualise inside a 2D feature space
0.2.1,Apply Nearmiss 2
0.2.1,"Two subplots, unpack the axes array immediately"
0.2.1,Define some color for the plotting
0.2.1,Generate the dataset
0.2.1,Instanciate a PCA object for the sake of easy visualisation
0.2.1,Fit and transform x to visualise inside a 2D feature space
0.2.1,Apply neighbourhood cleaning rule
0.2.1,"Two subplots, unpack the axes array immediately"
0.2.1,Define some color for the plotting
0.2.1,Generate the dataset
0.2.1,Instanciate a PCA object for the sake of easy visualisation
0.2.1,Fit and transform x to visualise inside a 2D feature space
0.2.1,Apply Condensed Nearest Neighbours
0.2.1,"Two subplots, unpack the axes array immediately"
0.2.1,Define some color for the plotting
0.2.1,Generate the dataset
0.2.1,Instanciate a PCA object for the sake of easy visualisation
0.2.1,Fit and transform x to visualise inside a 2D feature space
0.2.1,Apply Cluster Centroids
0.2.1,"Two subplots, unpack the axes array immediately"
0.2.1,Define some color for the plotting
0.2.1,Generate the dataset
0.2.1,Instanciate a PCA object for the sake of easy visualisation
0.2.1,Fit and transform x to visualise inside a 2D feature space
0.2.1,Apply the random under-sampling
0.2.1,"Two subplots, unpack the axes array immediately"
0.2.1,Define some color for the plotting
0.2.1,Generate the dataset
0.2.1,"Two subplots, unpack the axes array immediately"
0.2.1,Define some color for the plotting
0.2.1,Generate the dataset
0.2.1,Instanciate a PCA object for the sake of easy visualisation
0.2.1,Fit and transform x to visualise inside a 2D feature space
0.2.1,Apply Nearmiss 1
0.2.1,"Two subplots, unpack the axes array immediately"
0.2.1,Generate the dataset
0.2.1,Instanciate a PCA object for the sake of easy visualisation
0.2.1,Create the samplers
0.2.1,Create teh classifier
0.2.1,Make the splits
0.2.1,Add one transformers and two samplers in the pipeline object
0.2.1,Load the dataset
0.2.1,Create a folder to fetch the dataset
0.2.1,Make the dataset imbalanced
0.2.1,Select only half of the first class
0.2.1,Create a pipeline
0.2.1,Classify and report the results
0.2.1,Define some color for the plotting
0.2.1,Generate the dataset
0.2.1,"Two subplots, unpack the axes array immediately"
0.2.1,Based on NiLearn package
0.2.1,License: simplified BSD
0.2.1,"PEP0440 compatible formatted version, see:"
0.2.1,https://www.python.org/dev/peps/pep-0440/
0.2.1,
0.2.1,Generic release markers:
0.2.1,X.Y
0.2.1,X.Y.Z # For bugfix releases
0.2.1,
0.2.1,Admissible pre-release markers:
0.2.1,X.YaN # Alpha release
0.2.1,X.YbN # Beta release
0.2.1,X.YrcN # Release Candidate
0.2.1,X.Y # Final release
0.2.1,
0.2.1,Dev branch marker is: 'X.Y.dev' or 'X.Y.devN' where N is an integer.
0.2.1,'X.Y.dev0' is the canonical version of 'X.Y.dev'
0.2.1,
0.2.1,"This is a tuple to preserve order, so that dependencies are checked"
0.2.1,in some meaningful order (more => less 'core').  We avoid using
0.2.1,collections.OrderedDict to preserve Python 2.6 compatibility.
0.2.1,Avoid choking on modules with no __version__ attribute
0.2.1,Skip check only when installing and it's a module that
0.2.1,will be auto-installed.
0.2.1,Check the consistency of X and y
0.2.1,Raise an error if there is only one class
0.2.1,if uniques.size == 1:
0.2.1,"raise RuntimeError(""Only one class detected, aborting..."")"
0.2.1,Raise a warning for the moment to be compatible with BaseEstimator
0.2.1,Store the size of X to check at sampling time if we have the
0.2.1,same data
0.2.1,Create a dictionary containing the class statistics
0.2.1,Find the minority and majority classes
0.2.1,Check if the ratio provided at initialisation make sense
0.2.1,Check the consistency of X and y
0.2.1,Check that the data have been fitted
0.2.1,Check if the size of the data is identical than at fitting
0.2.1,The ratio correspond to the number of samples in the minority class
0.2.1,"over the number of samples in the majority class. Thus, the ratio"
0.2.1,cannot be greater than 1.0
0.2.1,Announce deprecation if necessary
0.2.1,Check that the target type is binary
0.2.1,Check that the target type is either binary or multiclass
0.2.1,Adapted from scikit-learn
0.2.1,Author: Edouard Duchesnay
0.2.1,Gael Varoquaux
0.2.1,Virgile Fritsch
0.2.1,Alexandre Gramfort
0.2.1,Lars Buitinck
0.2.1,chkoar
0.2.1,License: BSD
0.2.1,BaseEstimator interface
0.2.1,shallow copy of steps
0.2.1,Estimator interface
0.2.1,XXX: Calling sample in pipeline it means that the
0.2.1,last estimator is a sampler. Samplers don't carry
0.2.1,"the sampled data. So, call 'fit_sample' in all intermediate"
0.2.1,steps to get the sampled data for the last estimator.
0.2.1,Boolean controlling whether the joblib caches should be
0.2.1,"flushed if the version of certain modules changes (eg nibabel, as it"
0.2.1,does not respect the backward compatibility in some of its internal
0.2.1,structures
0.2.1,This  is used in nilearn._utils.cache_mixin
0.2.1,list all submodules available in imblearn and version
0.2.1,coding: utf-8
0.2.1,Only negative labels
0.2.1,"Calculate tp_sum, pred_sum, true_sum ###"
0.2.1,labels are now from 0 to len(labels) - 1 -> use bincount
0.2.1,Pathological case
0.2.1,Compute the true negative
0.2.1,Retain only selected labels
0.2.1,"Finally, we have all our sufficient statistics. Divide! #"
0.2.1,"Divide, and on zero-division, set scores to 0 and warn:"
0.2.1,"Oddly, we may get an ""invalid"" rather than a ""divide"" error"
0.2.1,here.
0.2.1,Average the results
0.2.1,Compute the score from the scoring function
0.2.1,Square if desired
0.2.1,Create the list of tags
0.2.1,Get the signature of the sens/spec function
0.2.1,Filter the inputs required by the sens/spec function
0.2.1,Call the sens/spec function
0.2.1,Compute the dominance
0.2.1,Compute the different metrics
0.2.1,Precision/recall/f1
0.2.1,Specificity
0.2.1,Geometric mean
0.2.1,Index balanced accuracy
0.2.1,compute averages
0.2.1,##############################################################################
0.2.1,Utilities for testing
0.2.1,import some data to play with
0.2.1,restrict to a binary classification task
0.2.1,add noisy features to make the problem harder and avoid perfect results
0.2.1,"run classifier, get class probabilities and label predictions"
0.2.1,only interested in probabilities of the positive case
0.2.1,XXX: do we really want a special API for the binary case?
0.2.1,##############################################################################
0.2.1,Tests
0.2.1,detailed measures for each class
0.2.1,individual scoring function that can be used for grid search: in the
0.2.1,binary class case the score is the value of the measure for the positive
0.2.1,class (e.g. label == 1). This is deprecated for average != 'binary'.
0.2.1,Such a case may occur with non-stratified cross-validation
0.2.1,No average: zeros in array
0.2.1,Macro average is changed
0.2.1,Check for micro
0.2.1,Check for weighted
0.2.1,ensure the above were meaningful tests:
0.2.1,Bad pos_label
0.2.1,Bad average option
0.2.1,"but average != 'binary'; even if data is binary"""""""
0.2.1,compute the geometric mean for the binary problem
0.2.1,Compute the geometric mean for each of the classes
0.2.1,average tests
0.2.1,print classification report with class names
0.2.1,print classification report with label detection
0.2.1,print classification report with class names
0.2.1,print classification report with label detection
0.2.1,Get the version
0.2.1,sensitivity scorer
0.2.1,specificity scorer
0.2.1,geometric_mean scorer
0.2.1,make a iba metric before a scorer
0.2.1,Keep the samples from the majority class
0.2.1,Loop over the other classes over picking at random
0.2.1,"If this is the majority class, skip it"
0.2.1,Define the number of sample to create
0.2.1,Pick some elements at random
0.2.1,Concatenate to the majority class
0.2.1,Keep the samples from the majority class
0.2.1,Define the number of sample to create
0.2.1,We handle only two classes problem for the moment.
0.2.1,Start by separating minority class features and target values.
0.2.1,Print if verbose is true
0.2.1,"Look for k-th nearest neighbours, excluding, of course, the"
0.2.1,point itself.
0.2.1,Get the distance to the NN
0.2.1,Compute the ratio of majority samples next to minority samples
0.2.1,Check that we found at least some neighbours belonging to the
0.2.1,majority class
0.2.1,Normalize the ratio
0.2.1,Compute the number of sample to be generated
0.2.1,For each minority samples
0.2.1,Pick-up the neighbors wanted
0.2.1,Create a new sample
0.2.1,Find the NN for each samples
0.2.1,Exclude the sample itself
0.2.1,Count how many NN belong to the minority class
0.2.1,Find the class corresponding to the label in x
0.2.1,Compute the number of majority samples in the NN
0.2.1,Samples are in danger for m/2 <= m' < m
0.2.1,Samples are noise for m = m'
0.2.1,Check the consistency of X
0.2.1,Check the random state
0.2.1,A matrix to store the synthetic samples
0.2.1,# Set seeds
0.2.1,"seeds = random_state.randint(low=0,"
0.2.1,"high=100 * len(nn_num.flatten()),"
0.2.1,size=n_samples)
0.2.1,Randomly pick samples to construct neighbours from
0.2.1,Loop over the NN matrix and create new samples
0.2.1,"NN lines relate to original sample, columns to its"
0.2.1,nearest neighbours
0.2.1,"Take a step of random size (0,1) in the direction of the"
0.2.1,n nearest neighbours
0.2.1,if self.random_state is None:
0.2.1,np.random.seed(seeds[i])
0.2.1,else:
0.2.1,np.random.seed(self.random_state)
0.2.1,Construct synthetic sample
0.2.1,The returned target vector is simply a repetition of the
0.2.1,minority label
0.2.1,--- NN object
0.2.1,Import the NN object from scikit-learn library. Since in the smote
0.2.1,"variations we must first find samples that are in danger, we"
0.2.1,initialize the NN object differently depending on the method chosen
0.2.1,"Regular smote does not look for samples in danger, instead it"
0.2.1,creates synthetic samples directly from the k-th nearest
0.2.1,neighbours with not filtering
0.2.1,"Borderline1, 2 and SVM variations of smote must first look for"
0.2.1,samples that could be considered noise and samples that live
0.2.1,"near the boundary between the classes. Therefore, before"
0.2.1,"creating synthetic samples from the k-th nns, it first look"
0.2.1,for m nearest neighbors to decide whether or not a sample is
0.2.1,noise or near the boundary.
0.2.1,--- SVM smote
0.2.1,"Unlike the borderline variations, the SVM variation uses the support"
0.2.1,vectors to decide which samples are in danger (near the boundary).
0.2.1,Additionally it also introduces extrapolation for samples that are
0.2.1,considered safe (far from boundary) and interpolation for samples
0.2.1,in danger (near the boundary). The level of extrapolation is
0.2.1,controled by the out_step.
0.2.1,Store SVM object with any parameters
0.2.1,Define the number of sample to create
0.2.1,We handle only two classes problem for the moment.
0.2.1,Start by separating minority class features and target values.
0.2.1,If regular SMOTE is to be performed
0.2.1,"Look for k-th nearest neighbours, excluding, of course, the"
0.2.1,point itself.
0.2.1,Matrix with k-th nearest neighbours indexes for each minority
0.2.1,element.
0.2.1,--- Generating synthetic samples
0.2.1,Use static method make_samples to generate minority samples
0.2.1,Concatenate the newly generated samples to the original data set
0.2.1,Find the NNs for all samples in the data set.
0.2.1,Boolean array with True for minority samples in danger
0.2.1,"If all minority samples are safe, return the original data set."
0.2.1,"All are safe, nothing to be done here."
0.2.1,"If we got here is because some samples are in danger, we need to"
0.2.1,find the NNs among the minority class to create the new synthetic
0.2.1,samples.
0.2.1,
0.2.1,We start by changing the number of NNs to consider from m + 1
0.2.1,to k + 1
0.2.1,nns...#
0.2.1,B1 and B2 types diverge here!!!
0.2.1,Create synthetic samples for borderline points.
0.2.1,Concatenate the newly generated samples to the original
0.2.1,dataset
0.2.1,Split the number of synthetic samples between only minority
0.2.1,"(type 1), or minority and majority (with reduced step size)"
0.2.1,(type 2).
0.2.1,The fraction is sampled from a beta distribution centered
0.2.1,around 0.5 with variance ~0.01
0.2.1,Only minority
0.2.1,Only majority with smaller step size
0.2.1,Concatenate the newly generated samples to the original
0.2.1,data set
0.2.1,The SVM smote model fits a support vector machine
0.2.1,classifier to the data and uses the support vector to
0.2.1,"provide a notion of boundary. Unlike regular smote, where"
0.2.1,such notion relies on proportion of nearest neighbours
0.2.1,belonging to each class.
0.2.1,Fit SVM to the full data#
0.2.1,Find the support vectors and their corresponding indexes
0.2.1,"First, find the nn of all the samples to identify samples"
0.2.1,in danger and noisy ones
0.2.1,"As usual, fit a nearest neighbour model to the data"
0.2.1,"Now, get rid of noisy support vectors"
0.2.1,Remove noisy support vectors
0.2.1,Proceed to find support vectors NNs among the minority class
0.2.1,Split the number of synthetic samples between interpolation and
0.2.1,extrapolation
0.2.1,The fraction are sampled from a beta distribution with mean
0.2.1,0.5 and variance 0.01#
0.2.1,Interpolate samples in danger
0.2.1,Extrapolate safe samples
0.2.1,Concatenate the newly generated samples to the original data set
0.2.1,not any support vectors in danger
0.2.1,All the support vector in danger
0.2.1,Generate a global dataset to use
0.2.1,Data generated for the toy example
0.2.1,Define a negative ratio
0.2.1,Define a ratio greater than 1
0.2.1,Define ratio as an unknown string
0.2.1,Define ratio as a list which is not supported
0.2.1,Define a ratio
0.2.1,Create the object
0.2.1,Resample the data
0.2.1,Create a wrong y
0.2.1,Create the object
0.2.1,Fit the data
0.2.1,Create the object
0.2.1,Fit the data
0.2.1,Check if the data information have been computed
0.2.1,Create the object
0.2.1,Resample the data
0.2.1,Resample the data
0.2.1,Create the object
0.2.1,continuous case
0.2.1,Make y to be multiclass
0.2.1,Resample the data
0.2.1,Check the size of y
0.2.1,Generate a global dataset to use
0.2.1,Define a negative ratio
0.2.1,Define a ratio greater than 1
0.2.1,Define ratio as an unknown string
0.2.1,Define ratio as a list which is not supported
0.2.1,Define a ratio
0.2.1,Create the object
0.2.1,Resample the data
0.2.1,Create a wrong y
0.2.1,Create the object
0.2.1,Fit the data
0.2.1,Create the object
0.2.1,Fit the data
0.2.1,Check if the data information have been computed
0.2.1,Create the object
0.2.1,Resample the data
0.2.1,Resample the data
0.2.1,Create the object
0.2.1,continuous case
0.2.1,multiclass case
0.2.1,Resample the data
0.2.1,Resample the data
0.2.1,Generate a global dataset to use
0.2.1,Define a negative ratio
0.2.1,Define a ratio greater than 1
0.2.1,Define ratio as an unknown string
0.2.1,Define ratio as a list which is not supported
0.2.1,Create the object
0.2.1,Resample the data
0.2.1,Create a wrong y
0.2.1,Create the object
0.2.1,Fit the data
0.2.1,Check if the data information have been computed
0.2.1,Create the object
0.2.1,Create the object
0.2.1,Fit the data
0.2.1,Create the object
0.2.1,Fit the data
0.2.1,Create the object
0.2.1,Fit the data
0.2.1,Create the object
0.2.1,Fit the data
0.2.1,Create the object
0.2.1,Fit the data
0.2.1,Create the object
0.2.1,continuous case
0.2.1,multiclass case
0.2.1,Create the object
0.2.1,Create the object
0.2.1,Create the object
0.2.1,Create the object
0.2.1,Create the object
0.2.1,Start with the minority class
0.2.1,All the minority class samples will be preserved
0.2.1,If we need to offer support for the indices
0.2.1,Loop over the other classes under picking at random
0.2.1,"If the minority class is up, skip it"
0.2.1,Randomly get one sample from the majority class
0.2.1,Generate the index to select
0.2.1,Create the set C
0.2.1,Create the set S
0.2.1,Remove the seed from S since that it will be added anyway
0.2.1,Fit C into the knn
0.2.1,Classify on S
0.2.1,Find the misclassified S_y
0.2.1,If we need to offer support for the indices selected
0.2.1,We concatenate the misclassified samples with the seed and the
0.2.1,minority samples
0.2.1,Find the nearest neighbour of every point
0.2.1,Send the information to is_tomek function to get boolean vector back
0.2.1,Check if the indices of the samples selected should be returned too
0.2.1,Return the indices of interest
0.2.1,Return data set without majority Tomek links.
0.2.1,Compute the distance considering the farthest neighbour
0.2.1,Sort the list of distance and get the index
0.2.1,Throw a warning to tell the user that we did not have enough samples
0.2.1,to select and that we just select everything
0.2.1,Select the desired number of samples
0.2.1,Announce deprecation if needed
0.2.1,Assign the parameter of the element of this class
0.2.1,Check that the version asked is implemented
0.2.1,Start with the minority class
0.2.1,All the minority class samples will be preserved
0.2.1,Compute the number of cluster needed
0.2.1,If we need to offer support for the indices
0.2.1,Fit the minority class since that we want to know the distance
0.2.1,to these point
0.2.1,Loop over the other classes under picking at random
0.2.1,"If the minority class is up, skip it"
0.2.1,Get the samples corresponding to the current class
0.2.1,Find the NN
0.2.1,Select the right samples
0.2.1,Find the NN
0.2.1,Select the right samples
0.2.1,We need a new NN object to fit the current class
0.2.1,Find the set of NN to the minority class
0.2.1,Create the subset containing the samples found during the NN
0.2.1,search. Linearize the indexes and remove the double values
0.2.1,Create the subset
0.2.1,Compute the NN considering the current class
0.2.1,If we need to offer support for the indices selected
0.2.1,Check if the indices of the samples selected should be returned too
0.2.1,Return the indices of interest
0.2.1,Compute the number of clusters needed
0.2.1,All the minority class samples will be preserved
0.2.1,If we need to offer support for the indices
0.2.1,Loop over the other classes under-picking at random
0.2.1,"If the minority class is up, skip it"
0.2.1,Pick some elements at random
0.2.1,If we need to offer support for the indices selected
0.2.1,Concatenate to the minority class
0.2.1,Check if the indices of the samples selected should be returned as
0.2.1,well
0.2.1,Return the indices of interest
0.2.1,"Initialize the boolean result as false, and also a counter"
0.2.1,Loop through each sample and looks whether it belongs to the minority
0.2.1,"class. If it does, we don't consider it since we want to keep all"
0.2.1,"minority samples. If, however, it belongs to the majority sample we"
0.2.1,look at its first neighbour. If its closest neighbour also has the
0.2.1,"current sample as its closest neighbour, the two form a Tomek link."
0.2.1,"If they form a tomek link, put a True marker on this"
0.2.1,"sample, and increase counter by one."
0.2.1,Find the nearest neighbour of every point
0.2.1,Send the information to is_tomek function to get boolean vector back
0.2.1,Check if the indices of the samples selected should be returned too
0.2.1,Return the indices of interest
0.2.1,Return data set without majority Tomek links.
0.2.1,Start with the minority class
0.2.1,All the minority class samples will be preserved
0.2.1,If we need to offer support for the indices
0.2.1,Loop over the other classes under picking at random
0.2.1,"If the minority class is up, skip it"
0.2.1,Randomly get one sample from the majority class
0.2.1,Generate the index to select
0.2.1,Create the set C - One majority samples and all minority
0.2.1,Create the set S - all majority samples
0.2.1,Fit C into the knn
0.2.1,Check each sample in S if we keep it or drop it
0.2.1,Do not select sample which are already well classified
0.2.1,Classify on S
0.2.1,If the prediction do not agree with the true label
0.2.1,append it in C_x
0.2.1,Keep the index for later
0.2.1,Update C
0.2.1,Fit C into the knn
0.2.1,This experimental to speed up the search
0.2.1,Classify all the element in S and avoid to test the
0.2.1,well classified elements
0.2.1,Find the misclassified S_y
0.2.1,"The indexes found are relative to the current class, we need to"
0.2.1,find the absolute value
0.2.1,Build the array with the absolute position
0.2.1,If we need to offer support for the indices selected
0.2.1,Check if the indices of the samples selected should be returned too
0.2.1,Return the indices of interest
0.2.1,Compute the number of cluster needed
0.2.1,Set the number of sample for the estimator
0.2.1,Start with the minority class
0.2.1,All the minority class samples will be preserved
0.2.1,Loop over the other classes under picking at random
0.2.1,"If the minority class is up, skip it."
0.2.1,Find the centroids via k-means
0.2.1,Concatenate to the minority class
0.2.1,Start with the minority class
0.2.1,All the minority class samples will be preserved
0.2.1,If we need to offer support for the indices
0.2.1,Fit the whole dataset
0.2.1,Loop over the other classes under picking at random
0.2.1,Get the sample of the current class
0.2.1,Get the samples associated
0.2.1,Find the NN for the current class
0.2.1,Get the label of the corresponding to the index
0.2.1,Check which one are the same label than the current class
0.2.1,Make an AND operation through the three neighbours
0.2.1,If the minority class remove the majority samples
0.2.1,Get the index to exclude
0.2.1,Get the index to exclude
0.2.1,Create a vector with the sample to select
0.2.1,Exclude as well the minority sample since that they will be
0.2.1,concatenated later
0.2.1,Get the samples from the majority classes
0.2.1,If we need to offer support for the indices selected
0.2.1,Check if the indices of the samples selected should be returned too
0.2.1,Return the indices of interest
0.2.1,Start with the minority class
0.2.1,All the minority class samples will be preserved
0.2.1,If we need to offer support for the indices
0.2.1,Fit the data
0.2.1,Loop over the other classes under picking at random
0.2.1,"If the minority class is up, skip it"
0.2.1,Get the sample of the current class
0.2.1,Find the NN for the current class
0.2.1,Get the label of the corresponding to the index
0.2.1,Check which one are the same label than the current class
0.2.1,Make the majority vote
0.2.1,Get the samples which agree all together
0.2.1,If we need to offer support for the indices selected
0.2.1,Check if the indices of the samples selected should be returned too
0.2.1,Return the indices of interest
0.2.1,Check the stopping criterion
0.2.1,1. If there is no changes for the vector y
0.2.1,2. If the number of samples in the other class become inferior to
0.2.1,the number of samples in the majority class
0.2.1,3. If one of the class is disappearing
0.2.1,Case 1
0.2.1,Case 2
0.2.1,Get the number of samples in the non-minority classes
0.2.1,Check the minority stop to be the minority
0.2.1,Case 3
0.2.1,"If this is a normal convergence, get the last data"
0.2.1,Log the variables to explain the stop of the algorithm
0.2.1,Update the data for the next iteration
0.2.1,Check if the indices of the samples selected should be returned too
0.2.1,Return the indices of interest
0.2.1,updating ENN size_ngh
0.2.1,Check the stopping criterion
0.2.1,1. If the number of samples in the other class become inferior to
0.2.1,the number of samples in the majority class
0.2.1,2. If one of the class is disappearing
0.2.1,Case 1
0.2.1,Get the number of samples in the non-minority classes
0.2.1,Check the minority stop to be the minority
0.2.1,Case 2
0.2.1,Log the variables to explain the stop of the algorithm
0.2.1,Update the data for the next iteration
0.2.1,Check if the indices of the samples selected should be returned too
0.2.1,Return the indices of interest
0.2.1,To be removed in 0.4
0.2.1,Select the appropriate classifier
0.2.1,Create the different folds
0.2.1,Compute the number of cluster needed
0.2.1,Find the percentile corresponding to the top num_samples
0.2.1,Sample the data
0.2.1,If we need to offer support for the indices
0.2.1,Generate a global dataset to use
0.2.1,Define a negative ratio
0.2.1,Define a ratio greater than 1
0.2.1,Define ratio as an unknown string
0.2.1,Define ratio as a list which is not supported
0.2.1,Define a ratio
0.2.1,Define the parameter for the under-sampling
0.2.1,Create the object
0.2.1,Resample the data
0.2.1,Create a wrong y
0.2.1,Create the object
0.2.1,Fit the data
0.2.1,Define the parameter for the under-sampling
0.2.1,Create the object
0.2.1,Fit the data
0.2.1,Check if the data information have been computed
0.2.1,Define the parameter for the under-sampling
0.2.1,Create the object
0.2.1,Define the parameter for the under-sampling
0.2.1,Create the object
0.2.1,Fit and sample
0.2.1,Define the parameter for the under-sampling
0.2.1,Create the object
0.2.1,Fit and sample
0.2.1,Define the parameter for the under-sampling
0.2.1,Create the object
0.2.1,Fit and sample
0.2.1,Create the object
0.2.1,continuous case
0.2.1,Define the parameter for the under-sampling
0.2.1,Create the object
0.2.1,Fit and sample
0.2.1,Define the parameter for the under-sampling
0.2.1,Create the object
0.2.1,Fit and sample
0.2.1,Create the object
0.2.1,Fit and sample
0.2.1,Generate a global dataset to use
0.2.1,Define a ratio
0.2.1,Create the object
0.2.1,Resample the data
0.2.1,Create a wrong y
0.2.1,Create the object
0.2.1,Fit the data
0.2.1,Check if the data information have been computed
0.2.1,Create the object
0.2.1,Resample the data
0.2.1,Resample the data
0.2.1,Create the object
0.2.1,continuous case
0.2.1,multiclass case
0.2.1,Generate a global dataset to use
0.2.1,Define a ratio
0.2.1,Create the object
0.2.1,Resample the data
0.2.1,Create a wrong y
0.2.1,Create the object
0.2.1,Fit the data
0.2.1,Check if the data information have been computed
0.2.1,Create the object
0.2.1,Resample the data
0.2.1,Resample the data
0.2.1,Create the object
0.2.1,continuous case
0.2.1,Resample the data
0.2.1,Resample the data
0.2.1,Generate a global dataset to use
0.2.1,Define a ratio
0.2.1,Create the object
0.2.1,Resample the data
0.2.1,Create a wrong y
0.2.1,Create the object
0.2.1,Fit the data
0.2.1,Check if the data information have been computed
0.2.1,Create the object
0.2.1,Resample the data
0.2.1,Resample the data
0.2.1,Resample the data
0.2.1,Create the object
0.2.1,continuous case
0.2.1,Resample the data
0.2.1,Resample the data
0.2.1,Generate a global dataset to use
0.2.1,Define a ratio
0.2.1,Create the object
0.2.1,Resample the data
0.2.1,Create a wrong y
0.2.1,Create the object
0.2.1,Fit the data
0.2.1,Check if the data information have been computed
0.2.1,Create the object
0.2.1,Resample the data
0.2.1,Resample the data
0.2.1,Resample the data
0.2.1,Create the object
0.2.1,continuous case
0.2.1,Resample the data
0.2.1,Generate a global dataset to use
0.2.1,Data generated for the toy example
0.2.1,Define a negative ratio
0.2.1,Define a ratio greater than 1
0.2.1,Define ratio as an unknown string
0.2.1,Define ratio as a list which is not supported
0.2.1,Define a ratio
0.2.1,Create the object
0.2.1,Resample the data
0.2.1,Create a wrong y
0.2.1,Create the object
0.2.1,Fit the data
0.2.1,Create the object
0.2.1,Fit the data
0.2.1,Check if the data information have been computed
0.2.1,Create the object
0.2.1,Resample the data
0.2.1,Resample the data
0.2.1,Resample the data
0.2.1,Create the object
0.2.1,continuous case
0.2.1,Make y to be multiclass
0.2.1,Resample the data
0.2.1,Check the size of y
0.2.1,Generate a global dataset to use
0.2.1,Define a negative ratio
0.2.1,Define a ratio greater than 1
0.2.1,Define ratio as an unknown string
0.2.1,Define ratio as a list which is not supported
0.2.1,Resample the data
0.2.1,Define a ratio
0.2.1,Create the object
0.2.1,Resample the data
0.2.1,Create a wrong y
0.2.1,Create the object
0.2.1,Fit the data
0.2.1,Create the object
0.2.1,Fit the data
0.2.1,Check if the data information have been computed
0.2.1,Create the object
0.2.1,Resample the data
0.2.1,Resample the data
0.2.1,Resample the data
0.2.1,Resample the data
0.2.1,Resample the data
0.2.1,Resample the data
0.2.1,Resample the data
0.2.1,Resample the data
0.2.1,Resample the data
0.2.1,Create the object
0.2.1,continuous case
0.2.1,multiclass case
0.2.1,Resample the data
0.2.1,Resample the data
0.2.1,Generate a global dataset to use
0.2.1,Define a ratio
0.2.1,Create the object
0.2.1,Create the object
0.2.1,Resample the data
0.2.1,Create a wrong y
0.2.1,Create the object
0.2.1,Fit the data
0.2.1,Check if the data information have been computed
0.2.1,Create the object
0.2.1,Resample the data
0.2.1,Resample the data
0.2.1,Resample the data
0.2.1,Create the object
0.2.1,continuous case
0.2.1,Resample the data
0.2.1,Resample the data
0.2.1,Generate a global dataset to use
0.2.1,Define a negative ratio
0.2.1,Define a ratio greater than 1
0.2.1,Define ratio as an unknown string
0.2.1,Define ratio as a list which is not supported
0.2.1,Define a ratio
0.2.1,Define the parameter for the under-sampling
0.2.1,Create the object
0.2.1,Resample the data
0.2.1,Create a wrong y
0.2.1,Create the object
0.2.1,Fit the data
0.2.1,Define the parameter for the under-sampling
0.2.1,Create the object
0.2.1,Fit the data
0.2.1,Check if the data information have been computed
0.2.1,Define the parameter for the under-sampling
0.2.1,Create the object
0.2.1,Define the parameter for the under-sampling
0.2.1,Create the object
0.2.1,Fit and sample
0.2.1,Define the parameter for the under-sampling
0.2.1,Create the object
0.2.1,Fit and sample
0.2.1,Define the parameter for the under-sampling
0.2.1,Create the object
0.2.1,Fit and sample
0.2.1,Create the object
0.2.1,continuous case
0.2.1,Define the parameter for the under-sampling
0.2.1,Create the object
0.2.1,Fit and sample
0.2.1,Define the parameter for the under-sampling
0.2.1,Create the object
0.2.1,Fit and sample
0.2.1,Generate a global dataset to use
0.2.1,Define a ratio
0.2.1,Create the object
0.2.1,Resample the data
0.2.1,Create a wrong y
0.2.1,Create the object
0.2.1,Fit the data
0.2.1,Check if the data information have been computed
0.2.1,Create the object
0.2.1,Resample the data
0.2.1,Resample the data
0.2.1,Create the object
0.2.1,continuous case
0.2.1,Resample the data
0.2.1,Resample the data
0.2.1,Generate a global dataset to use
0.2.1,Data generated for the toy example
0.2.1,Define a negative ratio
0.2.1,Define a ratio greater than 1
0.2.1,Define ratio as an unknown string
0.2.1,Define ratio as a list which is not supported
0.2.1,Define a ratio
0.2.1,Define the parameter for the under-sampling
0.2.1,Create the object
0.2.1,Resample the data
0.2.1,Create a wrong y
0.2.1,Create the object
0.2.1,Fit the data
0.2.1,Define the parameter for the under-sampling
0.2.1,Create the object
0.2.1,Fit the data
0.2.1,Check if the data information have been computed
0.2.1,Define the parameter for the under-sampling
0.2.1,Create the object
0.2.1,Define the parameter for the under-sampling
0.2.1,Create the object
0.2.1,Define the parameter for the under-sampling
0.2.1,Create the object
0.2.1,Fit and sample
0.2.1,Define the parameter for the under-sampling
0.2.1,Create the object
0.2.1,Fit and sample
0.2.1,Create the object
0.2.1,continuous case
0.2.1,Make y to be multiclass
0.2.1,Resample the data
0.2.1,Check the size of y
0.2.1,Define the parameter for the under-sampling
0.2.1,Create the object
0.2.1,Fit and sample
0.2.1,Define the parameter for the under-sampling
0.2.1,Create the object
0.2.1,Fit and sample
0.2.1,Generate a global dataset to use
0.2.1,Define a negative ratio
0.2.1,Define a ratio greater than 1
0.2.1,Define ratio as an unknown string
0.2.1,Define ratio as a list which is not supported
0.2.1,Define a ratio
0.2.1,Define the parameter for the under-sampling
0.2.1,Create the object
0.2.1,Resample the data
0.2.1,Create a wrong y
0.2.1,Create the object
0.2.1,Fit the data
0.2.1,Define the parameter for the under-sampling
0.2.1,Create the object
0.2.1,Fit the data
0.2.1,Check if the data information have been computed
0.2.1,Define the parameter for the under-sampling
0.2.1,Create the object
0.2.1,Define the parameter for the under-sampling
0.2.1,Create the object
0.2.1,Fit and sample
0.2.1,Define the parameter for the under-sampling
0.2.1,Create the object
0.2.1,Fit and sample
0.2.1,Define the parameter for the under-sampling
0.2.1,Create the object
0.2.1,Fit and sample
0.2.1,Create the object
0.2.1,continuous case
0.2.1,Define the parameter for the under-sampling
0.2.1,Create the object
0.2.1,Fit and sample
0.2.1,Define the parameter for the under-sampling
0.2.1,Create the object
0.2.1,Fit and sample
0.2.1,Generate a global dataset to use
0.2.1,Define a ratio
0.2.1,Create the object
0.2.1,Resample the data
0.2.1,Create a wrong y
0.2.1,Create the object
0.2.1,Fit the data
0.2.1,Check if the data information have been computed
0.2.1,Create the object
0.2.1,Resample the data
0.2.1,Resample the data
0.2.1,Create the object
0.2.1,continuous case
0.2.1,multiclass case
0.2.1,Resample the data
0.2.1,Resample the data
0.2.1,Resample the data
0.2.1,Test the various init parameters of the pipeline.
0.2.1,Check that we can't instantiate pipelines with objects without fit
0.2.1,method
0.2.1,Smoke test with only an estimator
0.2.1,Check that params are set
0.2.1,Smoke test the repr:
0.2.1,Test with two objects
0.2.1,Check that we can't use the same stage name twice
0.2.1,Check that params are set
0.2.1,Smoke test the repr:
0.2.1,Check that params are not set when naming them wrong
0.2.1,Test clone
0.2.1,"Check that apart from estimators, the parameters are the same"
0.2.1,Remove estimators that where copied
0.2.1,Test the various methods of the pipeline (anova).
0.2.1,Test with Anova + LogisticRegression
0.2.1,Test that the pipeline can take fit parameters
0.2.1,classifier should return True
0.2.1,and transformer params should not be changed
0.2.1,Test pipeline raises set params error message for nested models.
0.2.1,expected error message
0.2.1,nested model check
0.2.1,Test the various methods of the pipeline (pca + svm).
0.2.1,Test with PCA + SVC
0.2.1,Test the various methods of the pipeline (preprocessing + svm).
0.2.1,check shapes of various prediction functions
0.2.1,test that the fit_predict method is implemented on a pipeline
0.2.1,test that the fit_predict on pipeline yields same results as applying
0.2.1,transform and clustering steps separately
0.2.1,first compute the transform and clustering step separately
0.2.1,use a pipeline to do the transform and clustering in one step
0.2.1,tests that a pipeline does not have fit_predict method when final
0.2.1,step of pipeline does not have fit_predict defined
0.2.1,Test whether pipeline works with a transformer at the end.
0.2.1,Also test pipeline.transform and pipeline.inverse_transform
0.2.1,test transform and fit_transform:
0.2.1,Test whether pipeline works with a transformer missing fit_transform
0.2.1,test fit_transform:
0.2.1,Test the various methods of the pipeline (pca + svm).
0.2.1,Test with PCA + SVC
0.2.1,Test the various methods of the pipeline (pca + svm).
0.2.1,Test with PCA + SVC
0.2.1,Test whether pipeline works with a sampler at the end.
0.2.1,Also test pipeline.sampler
0.2.1,test transform and fit_transform:
0.2.1,Test whether pipeline works with a sampler at the end.
0.2.1,Also test pipeline.sampler
0.2.1,Test the various methods of the pipeline (anova).
0.2.1,Test with RandomUnderSampling + Anova + LogisticRegression
0.2.1,Test the various methods of the pipeline (anova).
0.2.1,"assert_raises(TypeError, lambda x: [][0])"
0.2.1,Test the various methods of the pipeline (anova).
0.2.1,Test with RandomUnderSampling + Anova + LogisticRegression
0.2.1,Check any parameters for SMOTE was provided
0.2.1,Anounce deprecation
0.2.1,We need to list each parameter and decide if we affect a default
0.2.1,value or not
0.2.1,"If an object was given, affect"
0.2.1,Otherwise create a default SMOTE
0.2.1,Check any parameters for ENN was provided
0.2.1,Anounce deprecation
0.2.1,We need to list each parameter and decide if we affect a default
0.2.1,value or not
0.2.1,"If an object was given, affect"
0.2.1,Otherwise create a default EditedNearestNeighbours
0.2.1,Fit using SMOTE
0.2.1,Transform using SMOTE
0.2.1,Fit and transform using ENN
0.2.1,Check any parameters for SMOTE was provided
0.2.1,Anounce deprecation
0.2.1,We need to list each parameter and decide if we affect a default
0.2.1,value or not
0.2.1,"If an object was given, affect"
0.2.1,Otherwise create a default SMOTE
0.2.1,Check any parameters for ENN was provided
0.2.1,Anounce deprecation
0.2.1,"If an object was given, affect"
0.2.1,Otherwise create a default EditedNearestNeighbours
0.2.1,Fit using SMOTE
0.2.1,Transform using SMOTE
0.2.1,Fit and transform using ENN
0.2.1,Generate a global dataset to use
0.2.1,Define a negative ratio
0.2.1,Define a ratio greater than 1
0.2.1,Define ratio as an unknown string
0.2.1,Define ratio as a list which is not supported
0.2.1,Create the object
0.2.1,Resample the data
0.2.1,Create a wrong y
0.2.1,Create the object
0.2.1,Fit the data
0.2.1,Check if the data information have been computed
0.2.1,Create the object
0.2.1,Create the object
0.2.1,Fit the data
0.2.1,Create the object
0.2.1,Fit the data
0.2.1,Create the object
0.2.1,continuous case
0.2.1,multiclass case
0.2.1,Create a SMOTE and Tomek object
0.2.1,Create a SMOTE and Tomek object
0.2.1,Generate a global dataset to use
0.2.1,Define a negative ratio
0.2.1,Define a ratio greater than 1
0.2.1,Define ratio as an unknown string
0.2.1,Define ratio as a list which is not supported
0.2.1,Create the object
0.2.1,Resample the data
0.2.1,Create a wrong y
0.2.1,Create the object
0.2.1,Fit the data
0.2.1,Check if the data information have been computed
0.2.1,Create the object
0.2.1,Create the object
0.2.1,Fit the data
0.2.1,Create the object
0.2.1,Fit the data
0.2.1,Create the object
0.2.1,continuous case
0.2.1,multiclass case
0.2.1,Create a SMOTE and Tomek object
0.2.1,Create a SMOTE and Tomek object
0.2.1,Check the random state
0.2.1,To be removed in 0.4
0.2.1,Define the classifier to use
0.2.1,Start with the minority class
0.2.1,Keep the indices of the minority class somewhere if we need to
0.2.1,return them later
0.2.1,Condition to initiliase before the search
0.2.1,Get the initial number of samples to select in the majority class
0.2.1,Create the array characterising the array containing the majority
0.2.1,class
0.2.1,Loop to create the different subsets
0.2.1,Generate an appropriate number of index to extract
0.2.1,from the majority class depending of the false classification
0.2.1,rate of the previous iteration
0.2.1,Mark these indexes as not being considered for next sampling
0.2.1,"For now, we will train and classify on the same data"
0.2.1,"Let see if we should find another solution. Anyway,"
0.2.1,random stuff are still random stuff
0.2.1,Push these data into a new subset
0.2.1,Get the indices of interest
0.2.1,"Draw samples, using sample weights, and then fit"
0.2.1,"Draw samples, using a mask, and then fit"
0.2.1,Predict using only the majority class
0.2.1,Basically let's find which sample have to be retained for the
0.2.1,next round
0.2.1,Find the misclassified index to keep them for the next round
0.2.1,Count how many random element will be selected
0.2.1,"We found a new subset, increase the counter"
0.2.1,Check if we have to make an early stopping
0.2.1,Select the remaining data
0.2.1,Select the final batch
0.2.1,Push these data into a new subset
0.2.1,"We found a new subset, increase the counter"
0.2.1,Specific case with n_max_subset = 1
0.2.1,Also check that we will have enough sample to extract at the
0.2.1,next round
0.2.1,Select the remaining data
0.2.1,Select the final batch
0.2.1,Push these data into a new subset
0.2.1,"We found a new subset, increase the counter"
0.2.1,Generate a global dataset to use
0.2.1,Define a negative ratio
0.2.1,Define a ratio greater than 1
0.2.1,Define ratio as an unknown string
0.2.1,Define ratio as a list which is not supported
0.2.1,Define a ratio
0.2.1,Define the parameter for the under-sampling
0.2.1,Create the object
0.2.1,Resample the data
0.2.1,Create a wrong y
0.2.1,Create the object
0.2.1,Fit the data
0.2.1,Define the parameter for the under-sampling
0.2.1,Create the object
0.2.1,Fit the data
0.2.1,Check if the data information have been computed
0.2.1,Define the parameter for the under-sampling
0.2.1,Create the object
0.2.1,Define the ratio parameter
0.2.1,Create the sampling object
0.2.1,Get the different subset
0.2.1,Check each array
0.2.1,Define the ratio parameter
0.2.1,Create the sampling object
0.2.1,Get the different subset
0.2.1,Check each array
0.2.1,Define the ratio parameter
0.2.1,Create the sampling object
0.2.1,Get the different subset
0.2.1,Check each array
0.2.1,Define the ratio parameter
0.2.1,Create the sampling object
0.2.1,Get the different subset
0.2.1,Check each array
0.2.1,Define the ratio parameter
0.2.1,Create the sampling object
0.2.1,Get the different subset
0.2.1,Check each array
0.2.1,Define the ratio parameter
0.2.1,Create the sampling object
0.2.1,Get the different subset
0.2.1,Check each array
0.2.1,Define the ratio parameter
0.2.1,Create the sampling object
0.2.1,Get the different subset
0.2.1,Check each array
0.2.1,Define the ratio parameter
0.2.1,Define the ratio parameter
0.2.1,Create the sampling object
0.2.1,Get the different subset
0.2.1,Check each array
0.2.1,Define the ratio parameter
0.2.1,Create the sampling object
0.2.1,Get the different subset
0.2.1,Check each array
0.2.1,Create the object
0.2.1,continuous case
0.2.1,multiclass case
0.2.1,Define the ratio parameter
0.2.1,Create the sampling object
0.2.1,Get the different subset
0.2.1,Check each array
0.2.1,Define the ratio parameter
0.2.1,Create the sampling object
0.2.1,Get the different subset
0.2.1,Define the ratio parameter
0.2.1,Create the sampling object
0.2.1,Get the different subset
0.2.1,Check each array
0.2.1,Generate a global dataset to use
0.2.1,Define a negative ratio
0.2.1,Define a ratio greater than 1
0.2.1,Define ratio as an unknown string
0.2.1,Define ratio as a list which is not supported
0.2.1,Define a ratio
0.2.1,Define the parameter for the under-sampling
0.2.1,Create the object
0.2.1,Resample the data
0.2.1,Create a wrong y
0.2.1,Create the object
0.2.1,Fit the data
0.2.1,Define the parameter for the under-sampling
0.2.1,Create the object
0.2.1,Fit the data
0.2.1,Check if the data information have been computed
0.2.1,Define the parameter for the under-sampling
0.2.1,Create the object
0.2.1,Define the ratio parameter
0.2.1,Create the sampling object
0.2.1,Get the different subset
0.2.1,Define the ratio parameter
0.2.1,Create the sampling object
0.2.1,Get the different subset
0.2.1,Define the ratio parameter
0.2.1,Create the sampling object
0.2.1,Get the different subset
0.2.1,Create the object
0.2.1,continuous case
0.2.1,Generate a global dataset to use
0.2.1,Define a zero ratio
0.2.1,Define a negative ratio
0.2.1,Define a ratio greater than 1
0.2.1,Define ratio as a list which is not supported
0.2.1,Make y to be multiclass
0.2.1,Resample the data
0.2.0,! /usr/bin/env python
0.2.0,"load all vars into globals, otherwise"
0.2.0,the later function call using global vars doesn't work.
0.2.0,"Allow command-lines such as ""python setup.py build install"""
0.2.0,Make sources available using relative paths from this file's directory.
0.2.0,-*- coding: utf-8 -*-
0.2.0,
0.2.0,"imbalanced-learn documentation build configuration file, created by"
0.2.0,sphinx-quickstart on Mon Jan 18 14:44:12 2016.
0.2.0,
0.2.0,This file is execfile()d with the current directory set to its
0.2.0,containing dir.
0.2.0,
0.2.0,Note that not all possible configuration values are present in this
0.2.0,autogenerated file.
0.2.0,
0.2.0,All configuration values have a default; values that are commented out
0.2.0,serve to show the default.
0.2.0,"If extensions (or modules to document with autodoc) are in another directory,"
0.2.0,add these directories to sys.path here. If the directory is relative to the
0.2.0,"documentation root, use os.path.abspath to make it absolute, like shown here."
0.2.0,"sys.path.insert(0, os.path.abspath('.'))"
0.2.0,-- General configuration ---------------------------------------------------
0.2.0,Try to override the matplotlib configuration as early as possible
0.2.0,-- General configuration ------------------------------------------------
0.2.0,"If your documentation needs a minimal Sphinx version, state it here."
0.2.0,needs_sphinx = '1.0'
0.2.0,"Add any Sphinx extension module names here, as strings. They can be"
0.2.0,extensions coming with Sphinx (named 'sphinx.ext.*') or your custom
0.2.0,ones.
0.2.0,path to your examples scripts
0.2.0,path where to save gallery generated examples
0.2.0,"Add any paths that contain templates here, relative to this directory."
0.2.0,generate autosummary even if no references
0.2.0,The suffix of source filenames.
0.2.0,The encoding of source files.
0.2.0,source_encoding = 'utf-8-sig'
0.2.0,Generate the plots for the gallery
0.2.0,The master toctree document.
0.2.0,General information about the project.
0.2.0,"The version info for the project you're documenting, acts as replacement for"
0.2.0,"|version| and |release|, also used in various other places throughout the"
0.2.0,built documents.
0.2.0,
0.2.0,The short X.Y version.
0.2.0,"The full version, including alpha/beta/rc tags."
0.2.0,The language for content autogenerated by Sphinx. Refer to documentation
0.2.0,for a list of supported languages.
0.2.0,language = None
0.2.0,"There are two options for replacing |today|: either, you set today to some"
0.2.0,"non-false value, then it is used:"
0.2.0,today = ''
0.2.0,"Else, today_fmt is used as the format for a strftime call."
0.2.0,"today_fmt = '%B %d, %Y'"
0.2.0,"List of patterns, relative to source directory, that match files and"
0.2.0,directories to ignore when looking for source files.
0.2.0,The reST default role (used for this markup: `text`) to use for all
0.2.0,documents.
0.2.0,default_role = None
0.2.0,"If true, '()' will be appended to :func: etc. cross-reference text."
0.2.0,"If true, the current module name will be prepended to all description"
0.2.0,unit titles (such as .. function::).
0.2.0,add_module_names = True
0.2.0,"If true, sectionauthor and moduleauthor directives will be shown in the"
0.2.0,output. They are ignored by default.
0.2.0,show_authors = False
0.2.0,The name of the Pygments (syntax highlighting) style to use.
0.2.0,A list of ignored prefixes for module index sorting.
0.2.0,modindex_common_prefix = []
0.2.0,"If true, keep warnings as ""system message"" paragraphs in the built documents."
0.2.0,keep_warnings = False
0.2.0,-- Options for HTML output ----------------------------------------------
0.2.0,The theme to use for HTML and HTML Help pages.  See the documentation for
0.2.0,a list of builtin themes.
0.2.0,Theme options are theme-specific and customize the look and feel of a theme
0.2.0,"further.  For a list of options available for each theme, see the"
0.2.0,documentation.
0.2.0,html_theme_options = {}
0.2.0,"Add any paths that contain custom themes here, relative to this directory."
0.2.0,"The name for this set of Sphinx documents.  If None, it defaults to"
0.2.0,"""<project> v<release> documentation""."
0.2.0,html_title = None
0.2.0,A shorter title for the navigation bar.  Default is the same as html_title.
0.2.0,html_short_title = None
0.2.0,The name of an image file (relative to this directory) to place at the top
0.2.0,of the sidebar.
0.2.0,html_logo = None
0.2.0,The name of an image file (within the static path) to use as favicon of the
0.2.0,docs.  This file should be a Windows icon file (.ico) being 16x16 or 32x32
0.2.0,pixels large.
0.2.0,html_favicon = None
0.2.0,"Add any paths that contain custom static files (such as style sheets) here,"
0.2.0,"relative to this directory. They are copied after the builtin static files,"
0.2.0,"so a file named ""default.css"" will overwrite the builtin ""default.css""."
0.2.0,Add any extra paths that contain custom files (such as robots.txt or
0.2.0,".htaccess) here, relative to this directory. These files are copied"
0.2.0,directly to the root of the documentation.
0.2.0,html_extra_path = []
0.2.0,"If not '', a 'Last updated on:' timestamp is inserted at every page bottom,"
0.2.0,using the given strftime format.
0.2.0,"html_last_updated_fmt = '%b %d, %Y'"
0.2.0,"If true, SmartyPants will be used to convert quotes and dashes to"
0.2.0,typographically correct entities.
0.2.0,html_use_smartypants = True
0.2.0,"Custom sidebar templates, maps document names to template names."
0.2.0,html_sidebars = {}
0.2.0,"Additional templates that should be rendered to pages, maps page names to"
0.2.0,template names.
0.2.0,html_additional_pages = {}
0.2.0,"If false, no module index is generated."
0.2.0,html_domain_indices = True
0.2.0,"If false, no index is generated."
0.2.0,html_use_index = True
0.2.0,"If true, the index is split into individual pages for each letter."
0.2.0,html_split_index = False
0.2.0,"If true, links to the reST sources are added to the pages."
0.2.0,html_show_sourcelink = True
0.2.0,"If true, ""Created using Sphinx"" is shown in the HTML footer. Default is True."
0.2.0,html_show_sphinx = True
0.2.0,"If true, ""(C) Copyright ..."" is shown in the HTML footer. Default is True."
0.2.0,html_show_copyright = True
0.2.0,"If true, an OpenSearch description file will be output, and all pages will"
0.2.0,contain a <link> tag referring to it.  The value of this option must be the
0.2.0,base URL from which the finished HTML is served.
0.2.0,html_use_opensearch = ''
0.2.0,"This is the file name suffix for HTML files (e.g. "".xhtml"")."
0.2.0,html_file_suffix = None
0.2.0,Output file base name for HTML help builder.
0.2.0,-- Options for LaTeX output ---------------------------------------------
0.2.0,The paper size ('letterpaper' or 'a4paper').
0.2.0,"'papersize': 'letterpaper',"
0.2.0,"The font size ('10pt', '11pt' or '12pt')."
0.2.0,"'pointsize': '10pt',"
0.2.0,Additional stuff for the LaTeX preamble.
0.2.0,"'preamble': '',"
0.2.0,Grouping the document tree into LaTeX files. List of tuples
0.2.0,"(source start file, target name, title,"
0.2.0,"author, documentclass [howto, manual, or own class])."
0.2.0,The name of an image file (relative to this directory) to place at the top of
0.2.0,the title page.
0.2.0,latex_logo = None
0.2.0,"For ""manual"" documents, if this is true, then toplevel headings are parts,"
0.2.0,not chapters.
0.2.0,latex_use_parts = False
0.2.0,"If true, show page references after internal links."
0.2.0,latex_show_pagerefs = False
0.2.0,"If true, show URL addresses after external links."
0.2.0,latex_show_urls = False
0.2.0,Documents to append as an appendix to all manuals.
0.2.0,latex_appendices = []
0.2.0,"If false, no module index is generated."
0.2.0,latex_domain_indices = True
0.2.0,-- Options for manual page output ---------------------------------------
0.2.0,One entry per manual page. List of tuples
0.2.0,"(source start file, name, description, authors, manual section)."
0.2.0,"If true, show URL addresses after external links."
0.2.0,man_show_urls = False
0.2.0,-- Options for Texinfo output -------------------------------------------
0.2.0,Grouping the document tree into Texinfo files. List of tuples
0.2.0,"(source start file, target name, title, author,"
0.2.0,"dir menu entry, description, category)"
0.2.0,"generate empty examples files, so that we don't get"
0.2.0,inclusion errors if there are no examples for a class / module
0.2.0,touch file
0.2.0,Documents to append as an appendix to all manuals.
0.2.0,texinfo_appendices = []
0.2.0,"If false, no module index is generated."
0.2.0,texinfo_domain_indices = True
0.2.0,"How to display URL addresses: 'footnote', 'no', or 'inline'."
0.2.0,texinfo_show_urls = 'footnote'
0.2.0,"If true, do not generate a @detailmenu in the ""Top"" node's menu."
0.2.0,texinfo_no_detailmenu = False
0.2.0,Example configuration for intersphinx: refer to the Python standard library.
0.2.0,Define some color for the plotting
0.2.0,Generate the dataset
0.2.0,Instanciate a PCA object for the sake of easy visualisation
0.2.0,Fit and transform x to visualise inside a 2D feature space
0.2.0,Apply SMOTE SVM
0.2.0,"Two subplots, unpack the axes array immediately"
0.2.0,Define some color for the plotting
0.2.0,Generate the dataset
0.2.0,Instanciate a PCA object for the sake of easy visualisation
0.2.0,Fit and transform x to visualise inside a 2D feature space
0.2.0,Apply Borderline SMOTE 2
0.2.0,"Two subplots, unpack the axes array immediately"
0.2.0,Define some color for the plotting
0.2.0,Generate the dataset
0.2.0,Instanciate a PCA object for the sake of easy visualisation
0.2.0,Fit and transform x to visualise inside a 2D feature space
0.2.0,Apply regular SMOTE
0.2.0,"Two subplots, unpack the axes array immediately"
0.2.0,Define some color for the plotting
0.2.0,Generate the dataset
0.2.0,Instanciate a PCA object for the sake of easy visualisation
0.2.0,Fit and transform x to visualise inside a 2D feature space
0.2.0,Apply the random over-sampling
0.2.0,"Two subplots, unpack the axes array immediately"
0.2.0,Define some color for the plotting
0.2.0,Generate the dataset
0.2.0,Instanciate a PCA object for the sake of easy visualisation
0.2.0,Fit and transform x to visualise inside a 2D feature space
0.2.0,Apply Borderline SMOTE 1
0.2.0,"Two subplots, unpack the axes array immediately"
0.2.0,Define some color for the plotting
0.2.0,Generate the dataset
0.2.0,Instanciate a PCA object for the sake of easy visualisation
0.2.0,Fit and transform x to visualise inside a 2D feature space
0.2.0,Apply the random over-sampling
0.2.0,"Two subplots, unpack the axes array immediately"
0.2.0,Generate the dataset
0.2.0,Generate a dataset
0.2.0,Split the data
0.2.0,Train the classifier with balancing
0.2.0,Test the classifier and get the prediction
0.2.0,Show the classification report
0.2.0,Generate a dataset
0.2.0,Split the data
0.2.0,Train the classifier with balancing
0.2.0,Test the classifier and get the prediction
0.2.0,##############################################################################
0.2.0,The geometric mean corresponds to the square root of the product of the
0.2.0,sensitivity and specificity. Combining the two metrics should account for
0.2.0,the balancing of the dataset.
0.2.0,##############################################################################
0.2.0,The index balanced accuracy can transform any metric to be used in
0.2.0,imbalanced learning problems.
0.2.0,Define some color for the plotting
0.2.0,Generate the dataset
0.2.0,Instanciate a PCA object for the sake of easy visualisation
0.2.0,Fit and transform x to visualise inside a 2D feature space
0.2.0,Apply SMOTE + ENN
0.2.0,"Two subplots, unpack the axes array immediately"
0.2.0,Define some color for the plotting
0.2.0,Generate the dataset
0.2.0,Instanciate a PCA object for the sake of easy visualisation
0.2.0,Fit and transform x to visualise inside a 2D feature space
0.2.0,Apply SMOTE + Tomek links
0.2.0,"Two subplots, unpack the axes array immediately"
0.2.0,Define some color for the plotting
0.2.0,Generate the dataset
0.2.0,Instanciate a PCA object for the sake of easy visualisation
0.2.0,Fit and transform x to visualise inside a 2D feature space
0.2.0,Apply Balance Cascade method
0.2.0,"Two subplots, unpack the axes array immediately"
0.2.0,Define some color for the plotting
0.2.0,Generate the dataset
0.2.0,Instanciate a PCA object for the sake of easy visualisation
0.2.0,Fit and transform x to visualise inside a 2D feature space
0.2.0,Apply Easy Ensemble
0.2.0,"Two subplots, unpack the axes array immediately"
0.2.0,Define some color for the plotting
0.2.0,Generate the dataset
0.2.0,Instanciate a PCA object for the sake of easy visualisation
0.2.0,Fit and transform x to visualise inside a 2D feature space
0.2.0,"Three subplots, unpack the axes array immediately"
0.2.0,Apply the ENN
0.2.0,Apply the RENN
0.2.0,Apply the AllKNN
0.2.0,Define some color for the plotting
0.2.0,Generate the dataset
0.2.0,Instanciate a PCA object for the sake of easy visualisation
0.2.0,Fit and transform x to visualise inside a 2D feature space
0.2.0,"Three subplots, unpack the axes array immediately"
0.2.0,Apply the ENN
0.2.0,Apply the RENN
0.2.0,Define some color for the plotting
0.2.0,Generate the dataset
0.2.0,Instanciate a PCA object for the sake of easy visualisation
0.2.0,Fit and transform x to visualise inside a 2D feature space
0.2.0,Apply Nearmiss 3
0.2.0,"Two subplots, unpack the axes array immediately"
0.2.0,Define some color for the plotting
0.2.0,Generate the dataset
0.2.0,Instanciate a PCA object for the sake of easy visualisation
0.2.0,Fit and transform x to visualise inside a 2D feature space
0.2.0,Apply Edited Nearest Neighbours
0.2.0,"Two subplots, unpack the axes array immediately"
0.2.0,Define some color for the plotting
0.2.0,Generate the dataset
0.2.0,Instanciate a PCA object for the sake of easy visualisation
0.2.0,Fit and transform x to visualise inside a 2D feature space
0.2.0,Apply Tomek Links cleaning
0.2.0,"Two subplots, unpack the axes array immediately"
0.2.0,Define some color for the plotting
0.2.0,Generate the dataset
0.2.0,Instanciate a PCA object for the sake of easy visualisation
0.2.0,Fit and transform x to visualise inside a 2D feature space
0.2.0,Apply One-Sided Selection
0.2.0,"Two subplots, unpack the axes array immediately"
0.2.0,Define some color for the plotting
0.2.0,Generate the dataset
0.2.0,Instanciate a PCA object for the sake of easy visualisation
0.2.0,Fit and transform x to visualise inside a 2D feature space
0.2.0,Apply Nearmiss 2
0.2.0,"Two subplots, unpack the axes array immediately"
0.2.0,Define some color for the plotting
0.2.0,Generate the dataset
0.2.0,Instanciate a PCA object for the sake of easy visualisation
0.2.0,Fit and transform x to visualise inside a 2D feature space
0.2.0,Apply neighbourhood cleaning rule
0.2.0,"Two subplots, unpack the axes array immediately"
0.2.0,Define some color for the plotting
0.2.0,Generate the dataset
0.2.0,Instanciate a PCA object for the sake of easy visualisation
0.2.0,Fit and transform x to visualise inside a 2D feature space
0.2.0,Apply Condensed Nearest Neighbours
0.2.0,"Two subplots, unpack the axes array immediately"
0.2.0,Define some color for the plotting
0.2.0,Generate the dataset
0.2.0,Instanciate a PCA object for the sake of easy visualisation
0.2.0,Fit and transform x to visualise inside a 2D feature space
0.2.0,Apply Cluster Centroids
0.2.0,"Two subplots, unpack the axes array immediately"
0.2.0,Define some color for the plotting
0.2.0,Generate the dataset
0.2.0,Instanciate a PCA object for the sake of easy visualisation
0.2.0,Fit and transform x to visualise inside a 2D feature space
0.2.0,Apply the random under-sampling
0.2.0,"Two subplots, unpack the axes array immediately"
0.2.0,Define some color for the plotting
0.2.0,Generate the dataset
0.2.0,"Two subplots, unpack the axes array immediately"
0.2.0,Define some color for the plotting
0.2.0,Generate the dataset
0.2.0,Instanciate a PCA object for the sake of easy visualisation
0.2.0,Fit and transform x to visualise inside a 2D feature space
0.2.0,Apply Nearmiss 1
0.2.0,"Two subplots, unpack the axes array immediately"
0.2.0,Generate the dataset
0.2.0,Instanciate a PCA object for the sake of easy visualisation
0.2.0,Create the samplers
0.2.0,Create teh classifier
0.2.0,Make the splits
0.2.0,Add one transformers and two samplers in the pipeline object
0.2.0,Load the dataset
0.2.0,Create a folder to fetch the dataset
0.2.0,Make the dataset imbalanced
0.2.0,Select only half of the first class
0.2.0,Create a pipeline
0.2.0,Classify and report the results
0.2.0,Define some color for the plotting
0.2.0,Generate the dataset
0.2.0,"Two subplots, unpack the axes array immediately"
0.2.0,Based on NiLearn package
0.2.0,License: simplified BSD
0.2.0,"PEP0440 compatible formatted version, see:"
0.2.0,https://www.python.org/dev/peps/pep-0440/
0.2.0,
0.2.0,Generic release markers:
0.2.0,X.Y
0.2.0,X.Y.Z # For bugfix releases
0.2.0,
0.2.0,Admissible pre-release markers:
0.2.0,X.YaN # Alpha release
0.2.0,X.YbN # Beta release
0.2.0,X.YrcN # Release Candidate
0.2.0,X.Y # Final release
0.2.0,
0.2.0,Dev branch marker is: 'X.Y.dev' or 'X.Y.devN' where N is an integer.
0.2.0,'X.Y.dev0' is the canonical version of 'X.Y.dev'
0.2.0,
0.2.0,"This is a tuple to preserve order, so that dependencies are checked"
0.2.0,in some meaningful order (more => less 'core').  We avoid using
0.2.0,collections.OrderedDict to preserve Python 2.6 compatibility.
0.2.0,Avoid choking on modules with no __version__ attribute
0.2.0,Skip check only when installing and it's a module that
0.2.0,will be auto-installed.
0.2.0,Check the consistency of X and y
0.2.0,Raise an error if there is only one class
0.2.0,if uniques.size == 1:
0.2.0,"raise RuntimeError(""Only one class detected, aborting..."")"
0.2.0,Raise a warning for the moment to be compatible with BaseEstimator
0.2.0,Store the size of X to check at sampling time if we have the
0.2.0,same data
0.2.0,Create a dictionary containing the class statistics
0.2.0,Find the minority and majority classes
0.2.0,Check if the ratio provided at initialisation make sense
0.2.0,Check the consistency of X and y
0.2.0,Check that the data have been fitted
0.2.0,Check if the size of the data is identical than at fitting
0.2.0,The ratio correspond to the number of samples in the minority class
0.2.0,"over the number of samples in the majority class. Thus, the ratio"
0.2.0,cannot be greater than 1.0
0.2.0,Announce deprecation if necessary
0.2.0,Check that the target type is binary
0.2.0,Check that the target type is either binary or multiclass
0.2.0,Adapted from scikit-learn
0.2.0,Author: Edouard Duchesnay
0.2.0,Gael Varoquaux
0.2.0,Virgile Fritsch
0.2.0,Alexandre Gramfort
0.2.0,Lars Buitinck
0.2.0,chkoar
0.2.0,License: BSD
0.2.0,BaseEstimator interface
0.2.0,shallow copy of steps
0.2.0,Estimator interface
0.2.0,XXX: Calling sample in pipeline it means that the
0.2.0,last estimator is a sampler. Samplers don't carry
0.2.0,"the sampled data. So, call 'fit_sample' in all intermediate"
0.2.0,steps to get the sampled data for the last estimator.
0.2.0,Boolean controlling whether the joblib caches should be
0.2.0,"flushed if the version of certain modules changes (eg nibabel, as it"
0.2.0,does not respect the backward compatibility in some of its internal
0.2.0,structures
0.2.0,This  is used in nilearn._utils.cache_mixin
0.2.0,list all submodules available in imblearn and version
0.2.0,coding: utf-8
0.2.0,Only negative labels
0.2.0,"Calculate tp_sum, pred_sum, true_sum ###"
0.2.0,labels are now from 0 to len(labels) - 1 -> use bincount
0.2.0,Pathological case
0.2.0,Compute the true negative
0.2.0,Retain only selected labels
0.2.0,"Finally, we have all our sufficient statistics. Divide! #"
0.2.0,"Divide, and on zero-division, set scores to 0 and warn:"
0.2.0,"Oddly, we may get an ""invalid"" rather than a ""divide"" error"
0.2.0,here.
0.2.0,Average the results
0.2.0,Compute the score from the scoring function
0.2.0,Square if desired
0.2.0,Create the list of tags
0.2.0,Get the signature of the sens/spec function
0.2.0,Filter the inputs required by the sens/spec function
0.2.0,Call the sens/spec function
0.2.0,Compute the dominance
0.2.0,Compute the different metrics
0.2.0,Precision/recall/f1
0.2.0,Specificity
0.2.0,Geometric mean
0.2.0,Index balanced accuracy
0.2.0,compute averages
0.2.0,##############################################################################
0.2.0,Utilities for testing
0.2.0,import some data to play with
0.2.0,restrict to a binary classification task
0.2.0,add noisy features to make the problem harder and avoid perfect results
0.2.0,"run classifier, get class probabilities and label predictions"
0.2.0,only interested in probabilities of the positive case
0.2.0,XXX: do we really want a special API for the binary case?
0.2.0,##############################################################################
0.2.0,Tests
0.2.0,detailed measures for each class
0.2.0,individual scoring function that can be used for grid search: in the
0.2.0,binary class case the score is the value of the measure for the positive
0.2.0,class (e.g. label == 1). This is deprecated for average != 'binary'.
0.2.0,Such a case may occur with non-stratified cross-validation
0.2.0,No average: zeros in array
0.2.0,Macro average is changed
0.2.0,Check for micro
0.2.0,Check for weighted
0.2.0,ensure the above were meaningful tests:
0.2.0,Bad pos_label
0.2.0,Bad average option
0.2.0,"but average != 'binary'; even if data is binary"""""""
0.2.0,compute the geometric mean for the binary problem
0.2.0,Compute the geometric mean for each of the classes
0.2.0,average tests
0.2.0,print classification report with class names
0.2.0,print classification report with label detection
0.2.0,print classification report with class names
0.2.0,print classification report with label detection
0.2.0,Get the version
0.2.0,sensitivity scorer
0.2.0,specificity scorer
0.2.0,geometric_mean scorer
0.2.0,make a iba metric before a scorer
0.2.0,Keep the samples from the majority class
0.2.0,Loop over the other classes over picking at random
0.2.0,"If this is the majority class, skip it"
0.2.0,Define the number of sample to create
0.2.0,Pick some elements at random
0.2.0,Concatenate to the majority class
0.2.0,Keep the samples from the majority class
0.2.0,Define the number of sample to create
0.2.0,We handle only two classes problem for the moment.
0.2.0,Start by separating minority class features and target values.
0.2.0,Print if verbose is true
0.2.0,"Look for k-th nearest neighbours, excluding, of course, the"
0.2.0,point itself.
0.2.0,Get the distance to the NN
0.2.0,Compute the ratio of majority samples next to minority samples
0.2.0,Check that we found at least some neighbours belonging to the
0.2.0,majority class
0.2.0,Normalize the ratio
0.2.0,Compute the number of sample to be generated
0.2.0,For each minority samples
0.2.0,Pick-up the neighbors wanted
0.2.0,Create a new sample
0.2.0,Find the NN for each samples
0.2.0,Exclude the sample itself
0.2.0,Count how many NN belong to the minority class
0.2.0,Find the class corresponding to the label in x
0.2.0,Compute the number of majority samples in the NN
0.2.0,Samples are in danger for m/2 <= m' < m
0.2.0,Samples are noise for m = m'
0.2.0,Check the consistency of X
0.2.0,Check the random state
0.2.0,A matrix to store the synthetic samples
0.2.0,# Set seeds
0.2.0,"seeds = random_state.randint(low=0,"
0.2.0,"high=100 * len(nn_num.flatten()),"
0.2.0,size=n_samples)
0.2.0,Randomly pick samples to construct neighbours from
0.2.0,Loop over the NN matrix and create new samples
0.2.0,"NN lines relate to original sample, columns to its"
0.2.0,nearest neighbours
0.2.0,"Take a step of random size (0,1) in the direction of the"
0.2.0,n nearest neighbours
0.2.0,if self.random_state is None:
0.2.0,np.random.seed(seeds[i])
0.2.0,else:
0.2.0,np.random.seed(self.random_state)
0.2.0,Construct synthetic sample
0.2.0,The returned target vector is simply a repetition of the
0.2.0,minority label
0.2.0,--- NN object
0.2.0,Import the NN object from scikit-learn library. Since in the smote
0.2.0,"variations we must first find samples that are in danger, we"
0.2.0,initialize the NN object differently depending on the method chosen
0.2.0,"Regular smote does not look for samples in danger, instead it"
0.2.0,creates synthetic samples directly from the k-th nearest
0.2.0,neighbours with not filtering
0.2.0,"Borderline1, 2 and SVM variations of smote must first look for"
0.2.0,samples that could be considered noise and samples that live
0.2.0,"near the boundary between the classes. Therefore, before"
0.2.0,"creating synthetic samples from the k-th nns, it first look"
0.2.0,for m nearest neighbors to decide whether or not a sample is
0.2.0,noise or near the boundary.
0.2.0,--- SVM smote
0.2.0,"Unlike the borderline variations, the SVM variation uses the support"
0.2.0,vectors to decide which samples are in danger (near the boundary).
0.2.0,Additionally it also introduces extrapolation for samples that are
0.2.0,considered safe (far from boundary) and interpolation for samples
0.2.0,in danger (near the boundary). The level of extrapolation is
0.2.0,controled by the out_step.
0.2.0,Store SVM object with any parameters
0.2.0,Define the number of sample to create
0.2.0,We handle only two classes problem for the moment.
0.2.0,Start by separating minority class features and target values.
0.2.0,If regular SMOTE is to be performed
0.2.0,"Look for k-th nearest neighbours, excluding, of course, the"
0.2.0,point itself.
0.2.0,Matrix with k-th nearest neighbours indexes for each minority
0.2.0,element.
0.2.0,--- Generating synthetic samples
0.2.0,Use static method make_samples to generate minority samples
0.2.0,Concatenate the newly generated samples to the original data set
0.2.0,Find the NNs for all samples in the data set.
0.2.0,Boolean array with True for minority samples in danger
0.2.0,"If all minority samples are safe, return the original data set."
0.2.0,"All are safe, nothing to be done here."
0.2.0,"If we got here is because some samples are in danger, we need to"
0.2.0,find the NNs among the minority class to create the new synthetic
0.2.0,samples.
0.2.0,
0.2.0,We start by changing the number of NNs to consider from m + 1
0.2.0,to k + 1
0.2.0,nns...#
0.2.0,B1 and B2 types diverge here!!!
0.2.0,Create synthetic samples for borderline points.
0.2.0,Concatenate the newly generated samples to the original
0.2.0,dataset
0.2.0,Split the number of synthetic samples between only minority
0.2.0,"(type 1), or minority and majority (with reduced step size)"
0.2.0,(type 2).
0.2.0,The fraction is sampled from a beta distribution centered
0.2.0,around 0.5 with variance ~0.01
0.2.0,Only minority
0.2.0,Only majority with smaller step size
0.2.0,Concatenate the newly generated samples to the original
0.2.0,data set
0.2.0,The SVM smote model fits a support vector machine
0.2.0,classifier to the data and uses the support vector to
0.2.0,"provide a notion of boundary. Unlike regular smote, where"
0.2.0,such notion relies on proportion of nearest neighbours
0.2.0,belonging to each class.
0.2.0,Fit SVM to the full data#
0.2.0,Find the support vectors and their corresponding indexes
0.2.0,"First, find the nn of all the samples to identify samples"
0.2.0,in danger and noisy ones
0.2.0,"As usual, fit a nearest neighbour model to the data"
0.2.0,"Now, get rid of noisy support vectors"
0.2.0,Remove noisy support vectors
0.2.0,Proceed to find support vectors NNs among the minority class
0.2.0,Split the number of synthetic samples between interpolation and
0.2.0,extrapolation
0.2.0,The fraction are sampled from a beta distribution with mean
0.2.0,0.5 and variance 0.01#
0.2.0,Interpolate samples in danger
0.2.0,Extrapolate safe samples
0.2.0,Concatenate the newly generated samples to the original data set
0.2.0,not any support vectors in danger
0.2.0,All the support vector in danger
0.2.0,Generate a global dataset to use
0.2.0,Data generated for the toy example
0.2.0,Define a negative ratio
0.2.0,Define a ratio greater than 1
0.2.0,Define ratio as an unknown string
0.2.0,Define ratio as a list which is not supported
0.2.0,Define a ratio
0.2.0,Create the object
0.2.0,Resample the data
0.2.0,Create a wrong y
0.2.0,Create the object
0.2.0,Fit the data
0.2.0,Create the object
0.2.0,Fit the data
0.2.0,Check if the data information have been computed
0.2.0,Create the object
0.2.0,Resample the data
0.2.0,Resample the data
0.2.0,Create the object
0.2.0,continuous case
0.2.0,Make y to be multiclass
0.2.0,Resample the data
0.2.0,Check the size of y
0.2.0,Generate a global dataset to use
0.2.0,Define a negative ratio
0.2.0,Define a ratio greater than 1
0.2.0,Define ratio as an unknown string
0.2.0,Define ratio as a list which is not supported
0.2.0,Define a ratio
0.2.0,Create the object
0.2.0,Resample the data
0.2.0,Create a wrong y
0.2.0,Create the object
0.2.0,Fit the data
0.2.0,Create the object
0.2.0,Fit the data
0.2.0,Check if the data information have been computed
0.2.0,Create the object
0.2.0,Resample the data
0.2.0,Resample the data
0.2.0,Create the object
0.2.0,continuous case
0.2.0,multiclass case
0.2.0,Resample the data
0.2.0,Resample the data
0.2.0,Generate a global dataset to use
0.2.0,Define a negative ratio
0.2.0,Define a ratio greater than 1
0.2.0,Define ratio as an unknown string
0.2.0,Define ratio as a list which is not supported
0.2.0,Create the object
0.2.0,Resample the data
0.2.0,Create a wrong y
0.2.0,Create the object
0.2.0,Fit the data
0.2.0,Check if the data information have been computed
0.2.0,Create the object
0.2.0,Create the object
0.2.0,Fit the data
0.2.0,Create the object
0.2.0,Fit the data
0.2.0,Create the object
0.2.0,Fit the data
0.2.0,Create the object
0.2.0,Fit the data
0.2.0,Create the object
0.2.0,Fit the data
0.2.0,Create the object
0.2.0,continuous case
0.2.0,multiclass case
0.2.0,Create the object
0.2.0,Create the object
0.2.0,Create the object
0.2.0,Create the object
0.2.0,Create the object
0.2.0,Start with the minority class
0.2.0,All the minority class samples will be preserved
0.2.0,If we need to offer support for the indices
0.2.0,Loop over the other classes under picking at random
0.2.0,"If the minority class is up, skip it"
0.2.0,Randomly get one sample from the majority class
0.2.0,Generate the index to select
0.2.0,Create the set C
0.2.0,Create the set S
0.2.0,Remove the seed from S since that it will be added anyway
0.2.0,Fit C into the knn
0.2.0,Classify on S
0.2.0,Find the misclassified S_y
0.2.0,If we need to offer support for the indices selected
0.2.0,We concatenate the misclassified samples with the seed and the
0.2.0,minority samples
0.2.0,Find the nearest neighbour of every point
0.2.0,Send the information to is_tomek function to get boolean vector back
0.2.0,Check if the indices of the samples selected should be returned too
0.2.0,Return the indices of interest
0.2.0,Return data set without majority Tomek links.
0.2.0,Compute the distance considering the farthest neighbour
0.2.0,Sort the list of distance and get the index
0.2.0,Throw a warning to tell the user that we did not have enough samples
0.2.0,to select and that we just select everything
0.2.0,Select the desired number of samples
0.2.0,Announce deprecation if needed
0.2.0,Assign the parameter of the element of this class
0.2.0,Check that the version asked is implemented
0.2.0,Start with the minority class
0.2.0,All the minority class samples will be preserved
0.2.0,Compute the number of cluster needed
0.2.0,If we need to offer support for the indices
0.2.0,Fit the minority class since that we want to know the distance
0.2.0,to these point
0.2.0,Loop over the other classes under picking at random
0.2.0,"If the minority class is up, skip it"
0.2.0,Get the samples corresponding to the current class
0.2.0,Find the NN
0.2.0,Select the right samples
0.2.0,Find the NN
0.2.0,Select the right samples
0.2.0,We need a new NN object to fit the current class
0.2.0,Find the set of NN to the minority class
0.2.0,Create the subset containing the samples found during the NN
0.2.0,search. Linearize the indexes and remove the double values
0.2.0,Create the subset
0.2.0,Compute the NN considering the current class
0.2.0,If we need to offer support for the indices selected
0.2.0,Check if the indices of the samples selected should be returned too
0.2.0,Return the indices of interest
0.2.0,Compute the number of clusters needed
0.2.0,All the minority class samples will be preserved
0.2.0,If we need to offer support for the indices
0.2.0,Loop over the other classes under-picking at random
0.2.0,"If the minority class is up, skip it"
0.2.0,Pick some elements at random
0.2.0,If we need to offer support for the indices selected
0.2.0,Concatenate to the minority class
0.2.0,Check if the indices of the samples selected should be returned as
0.2.0,well
0.2.0,Return the indices of interest
0.2.0,"Initialize the boolean result as false, and also a counter"
0.2.0,Loop through each sample and looks whether it belongs to the minority
0.2.0,"class. If it does, we don't consider it since we want to keep all"
0.2.0,"minority samples. If, however, it belongs to the majority sample we"
0.2.0,look at its first neighbour. If its closest neighbour also has the
0.2.0,"current sample as its closest neighbour, the two form a Tomek link."
0.2.0,"If they form a tomek link, put a True marker on this"
0.2.0,"sample, and increase counter by one."
0.2.0,Find the nearest neighbour of every point
0.2.0,Send the information to is_tomek function to get boolean vector back
0.2.0,Check if the indices of the samples selected should be returned too
0.2.0,Return the indices of interest
0.2.0,Return data set without majority Tomek links.
0.2.0,Start with the minority class
0.2.0,All the minority class samples will be preserved
0.2.0,If we need to offer support for the indices
0.2.0,Loop over the other classes under picking at random
0.2.0,"If the minority class is up, skip it"
0.2.0,Randomly get one sample from the majority class
0.2.0,Generate the index to select
0.2.0,Create the set C - One majority samples and all minority
0.2.0,Create the set S - all majority samples
0.2.0,Fit C into the knn
0.2.0,Check each sample in S if we keep it or drop it
0.2.0,Do not select sample which are already well classified
0.2.0,Classify on S
0.2.0,If the prediction do not agree with the true label
0.2.0,append it in C_x
0.2.0,Keep the index for later
0.2.0,Update C
0.2.0,Fit C into the knn
0.2.0,This experimental to speed up the search
0.2.0,Classify all the element in S and avoid to test the
0.2.0,well classified elements
0.2.0,Find the misclassified S_y
0.2.0,"The indexes found are relative to the current class, we need to"
0.2.0,find the absolute value
0.2.0,Build the array with the absolute position
0.2.0,If we need to offer support for the indices selected
0.2.0,Check if the indices of the samples selected should be returned too
0.2.0,Return the indices of interest
0.2.0,Compute the number of cluster needed
0.2.0,Set the number of sample for the estimator
0.2.0,Start with the minority class
0.2.0,All the minority class samples will be preserved
0.2.0,Loop over the other classes under picking at random
0.2.0,"If the minority class is up, skip it."
0.2.0,Find the centroids via k-means
0.2.0,Concatenate to the minority class
0.2.0,Start with the minority class
0.2.0,All the minority class samples will be preserved
0.2.0,If we need to offer support for the indices
0.2.0,Fit the whole dataset
0.2.0,Loop over the other classes under picking at random
0.2.0,Get the sample of the current class
0.2.0,Get the samples associated
0.2.0,Find the NN for the current class
0.2.0,Get the label of the corresponding to the index
0.2.0,Check which one are the same label than the current class
0.2.0,Make an AND operation through the three neighbours
0.2.0,If the minority class remove the majority samples
0.2.0,Get the index to exclude
0.2.0,Get the index to exclude
0.2.0,Create a vector with the sample to select
0.2.0,Exclude as well the minority sample since that they will be
0.2.0,concatenated later
0.2.0,Get the samples from the majority classes
0.2.0,If we need to offer support for the indices selected
0.2.0,Check if the indices of the samples selected should be returned too
0.2.0,Return the indices of interest
0.2.0,Start with the minority class
0.2.0,All the minority class samples will be preserved
0.2.0,If we need to offer support for the indices
0.2.0,Fit the data
0.2.0,Loop over the other classes under picking at random
0.2.0,"If the minority class is up, skip it"
0.2.0,Get the sample of the current class
0.2.0,Find the NN for the current class
0.2.0,Get the label of the corresponding to the index
0.2.0,Check which one are the same label than the current class
0.2.0,Make the majority vote
0.2.0,Get the samples which agree all together
0.2.0,If we need to offer support for the indices selected
0.2.0,Check if the indices of the samples selected should be returned too
0.2.0,Return the indices of interest
0.2.0,Check the stopping criterion
0.2.0,1. If there is no changes for the vector y
0.2.0,2. If the number of samples in the other class become inferior to
0.2.0,the number of samples in the majority class
0.2.0,3. If one of the class is disappearing
0.2.0,Case 1
0.2.0,Case 2
0.2.0,Get the number of samples in the non-minority classes
0.2.0,Check the minority stop to be the minority
0.2.0,Case 3
0.2.0,"If this is a normal convergence, get the last data"
0.2.0,Log the variables to explain the stop of the algorithm
0.2.0,Update the data for the next iteration
0.2.0,Check if the indices of the samples selected should be returned too
0.2.0,Return the indices of interest
0.2.0,updating ENN size_ngh
0.2.0,Check the stopping criterion
0.2.0,1. If the number of samples in the other class become inferior to
0.2.0,the number of samples in the majority class
0.2.0,2. If one of the class is disappearing
0.2.0,Case 1
0.2.0,Get the number of samples in the non-minority classes
0.2.0,Check the minority stop to be the minority
0.2.0,Case 2
0.2.0,Log the variables to explain the stop of the algorithm
0.2.0,Update the data for the next iteration
0.2.0,Check if the indices of the samples selected should be returned too
0.2.0,Return the indices of interest
0.2.0,To be removed in 0.4
0.2.0,Select the appropriate classifier
0.2.0,Create the different folds
0.2.0,Compute the number of cluster needed
0.2.0,Find the percentile corresponding to the top num_samples
0.2.0,Sample the data
0.2.0,If we need to offer support for the indices
0.2.0,Generate a global dataset to use
0.2.0,Define a negative ratio
0.2.0,Define a ratio greater than 1
0.2.0,Define ratio as an unknown string
0.2.0,Define ratio as a list which is not supported
0.2.0,Define a ratio
0.2.0,Define the parameter for the under-sampling
0.2.0,Create the object
0.2.0,Resample the data
0.2.0,Create a wrong y
0.2.0,Create the object
0.2.0,Fit the data
0.2.0,Define the parameter for the under-sampling
0.2.0,Create the object
0.2.0,Fit the data
0.2.0,Check if the data information have been computed
0.2.0,Define the parameter for the under-sampling
0.2.0,Create the object
0.2.0,Define the parameter for the under-sampling
0.2.0,Create the object
0.2.0,Fit and sample
0.2.0,Define the parameter for the under-sampling
0.2.0,Create the object
0.2.0,Fit and sample
0.2.0,Define the parameter for the under-sampling
0.2.0,Create the object
0.2.0,Fit and sample
0.2.0,Create the object
0.2.0,continuous case
0.2.0,Define the parameter for the under-sampling
0.2.0,Create the object
0.2.0,Fit and sample
0.2.0,Define the parameter for the under-sampling
0.2.0,Create the object
0.2.0,Fit and sample
0.2.0,Create the object
0.2.0,Fit and sample
0.2.0,Generate a global dataset to use
0.2.0,Define a ratio
0.2.0,Create the object
0.2.0,Resample the data
0.2.0,Create a wrong y
0.2.0,Create the object
0.2.0,Fit the data
0.2.0,Check if the data information have been computed
0.2.0,Create the object
0.2.0,Resample the data
0.2.0,Resample the data
0.2.0,Create the object
0.2.0,continuous case
0.2.0,multiclass case
0.2.0,Generate a global dataset to use
0.2.0,Define a ratio
0.2.0,Create the object
0.2.0,Resample the data
0.2.0,Create a wrong y
0.2.0,Create the object
0.2.0,Fit the data
0.2.0,Check if the data information have been computed
0.2.0,Create the object
0.2.0,Resample the data
0.2.0,Resample the data
0.2.0,Create the object
0.2.0,continuous case
0.2.0,Resample the data
0.2.0,Resample the data
0.2.0,Generate a global dataset to use
0.2.0,Define a ratio
0.2.0,Create the object
0.2.0,Resample the data
0.2.0,Create a wrong y
0.2.0,Create the object
0.2.0,Fit the data
0.2.0,Check if the data information have been computed
0.2.0,Create the object
0.2.0,Resample the data
0.2.0,Resample the data
0.2.0,Resample the data
0.2.0,Create the object
0.2.0,continuous case
0.2.0,Resample the data
0.2.0,Resample the data
0.2.0,Generate a global dataset to use
0.2.0,Define a ratio
0.2.0,Create the object
0.2.0,Resample the data
0.2.0,Create a wrong y
0.2.0,Create the object
0.2.0,Fit the data
0.2.0,Check if the data information have been computed
0.2.0,Create the object
0.2.0,Resample the data
0.2.0,Resample the data
0.2.0,Resample the data
0.2.0,Create the object
0.2.0,continuous case
0.2.0,Resample the data
0.2.0,Generate a global dataset to use
0.2.0,Data generated for the toy example
0.2.0,Define a negative ratio
0.2.0,Define a ratio greater than 1
0.2.0,Define ratio as an unknown string
0.2.0,Define ratio as a list which is not supported
0.2.0,Define a ratio
0.2.0,Create the object
0.2.0,Resample the data
0.2.0,Create a wrong y
0.2.0,Create the object
0.2.0,Fit the data
0.2.0,Create the object
0.2.0,Fit the data
0.2.0,Check if the data information have been computed
0.2.0,Create the object
0.2.0,Resample the data
0.2.0,Resample the data
0.2.0,Resample the data
0.2.0,Create the object
0.2.0,continuous case
0.2.0,Make y to be multiclass
0.2.0,Resample the data
0.2.0,Check the size of y
0.2.0,Generate a global dataset to use
0.2.0,Define a negative ratio
0.2.0,Define a ratio greater than 1
0.2.0,Define ratio as an unknown string
0.2.0,Define ratio as a list which is not supported
0.2.0,Resample the data
0.2.0,Define a ratio
0.2.0,Create the object
0.2.0,Resample the data
0.2.0,Create a wrong y
0.2.0,Create the object
0.2.0,Fit the data
0.2.0,Create the object
0.2.0,Fit the data
0.2.0,Check if the data information have been computed
0.2.0,Create the object
0.2.0,Resample the data
0.2.0,Resample the data
0.2.0,Resample the data
0.2.0,Resample the data
0.2.0,Resample the data
0.2.0,Resample the data
0.2.0,Resample the data
0.2.0,Resample the data
0.2.0,Resample the data
0.2.0,Create the object
0.2.0,continuous case
0.2.0,multiclass case
0.2.0,Resample the data
0.2.0,Resample the data
0.2.0,Generate a global dataset to use
0.2.0,Define a ratio
0.2.0,Create the object
0.2.0,Create the object
0.2.0,Resample the data
0.2.0,Create a wrong y
0.2.0,Create the object
0.2.0,Fit the data
0.2.0,Check if the data information have been computed
0.2.0,Create the object
0.2.0,Resample the data
0.2.0,Resample the data
0.2.0,Resample the data
0.2.0,Create the object
0.2.0,continuous case
0.2.0,Resample the data
0.2.0,Resample the data
0.2.0,Generate a global dataset to use
0.2.0,Define a negative ratio
0.2.0,Define a ratio greater than 1
0.2.0,Define ratio as an unknown string
0.2.0,Define ratio as a list which is not supported
0.2.0,Define a ratio
0.2.0,Define the parameter for the under-sampling
0.2.0,Create the object
0.2.0,Resample the data
0.2.0,Create a wrong y
0.2.0,Create the object
0.2.0,Fit the data
0.2.0,Define the parameter for the under-sampling
0.2.0,Create the object
0.2.0,Fit the data
0.2.0,Check if the data information have been computed
0.2.0,Define the parameter for the under-sampling
0.2.0,Create the object
0.2.0,Define the parameter for the under-sampling
0.2.0,Create the object
0.2.0,Fit and sample
0.2.0,Define the parameter for the under-sampling
0.2.0,Create the object
0.2.0,Fit and sample
0.2.0,Define the parameter for the under-sampling
0.2.0,Create the object
0.2.0,Fit and sample
0.2.0,Create the object
0.2.0,continuous case
0.2.0,Define the parameter for the under-sampling
0.2.0,Create the object
0.2.0,Fit and sample
0.2.0,Define the parameter for the under-sampling
0.2.0,Create the object
0.2.0,Fit and sample
0.2.0,Generate a global dataset to use
0.2.0,Define a ratio
0.2.0,Create the object
0.2.0,Resample the data
0.2.0,Create a wrong y
0.2.0,Create the object
0.2.0,Fit the data
0.2.0,Check if the data information have been computed
0.2.0,Create the object
0.2.0,Resample the data
0.2.0,Resample the data
0.2.0,Create the object
0.2.0,continuous case
0.2.0,Resample the data
0.2.0,Resample the data
0.2.0,Generate a global dataset to use
0.2.0,Data generated for the toy example
0.2.0,Define a negative ratio
0.2.0,Define a ratio greater than 1
0.2.0,Define ratio as an unknown string
0.2.0,Define ratio as a list which is not supported
0.2.0,Define a ratio
0.2.0,Define the parameter for the under-sampling
0.2.0,Create the object
0.2.0,Resample the data
0.2.0,Create a wrong y
0.2.0,Create the object
0.2.0,Fit the data
0.2.0,Define the parameter for the under-sampling
0.2.0,Create the object
0.2.0,Fit the data
0.2.0,Check if the data information have been computed
0.2.0,Define the parameter for the under-sampling
0.2.0,Create the object
0.2.0,Define the parameter for the under-sampling
0.2.0,Create the object
0.2.0,Define the parameter for the under-sampling
0.2.0,Create the object
0.2.0,Fit and sample
0.2.0,Define the parameter for the under-sampling
0.2.0,Create the object
0.2.0,Fit and sample
0.2.0,Create the object
0.2.0,continuous case
0.2.0,Make y to be multiclass
0.2.0,Resample the data
0.2.0,Check the size of y
0.2.0,Define the parameter for the under-sampling
0.2.0,Create the object
0.2.0,Fit and sample
0.2.0,Define the parameter for the under-sampling
0.2.0,Create the object
0.2.0,Fit and sample
0.2.0,Generate a global dataset to use
0.2.0,Define a negative ratio
0.2.0,Define a ratio greater than 1
0.2.0,Define ratio as an unknown string
0.2.0,Define ratio as a list which is not supported
0.2.0,Define a ratio
0.2.0,Define the parameter for the under-sampling
0.2.0,Create the object
0.2.0,Resample the data
0.2.0,Create a wrong y
0.2.0,Create the object
0.2.0,Fit the data
0.2.0,Define the parameter for the under-sampling
0.2.0,Create the object
0.2.0,Fit the data
0.2.0,Check if the data information have been computed
0.2.0,Define the parameter for the under-sampling
0.2.0,Create the object
0.2.0,Define the parameter for the under-sampling
0.2.0,Create the object
0.2.0,Fit and sample
0.2.0,Define the parameter for the under-sampling
0.2.0,Create the object
0.2.0,Fit and sample
0.2.0,Define the parameter for the under-sampling
0.2.0,Create the object
0.2.0,Fit and sample
0.2.0,Create the object
0.2.0,continuous case
0.2.0,Define the parameter for the under-sampling
0.2.0,Create the object
0.2.0,Fit and sample
0.2.0,Define the parameter for the under-sampling
0.2.0,Create the object
0.2.0,Fit and sample
0.2.0,Generate a global dataset to use
0.2.0,Define a ratio
0.2.0,Create the object
0.2.0,Resample the data
0.2.0,Create a wrong y
0.2.0,Create the object
0.2.0,Fit the data
0.2.0,Check if the data information have been computed
0.2.0,Create the object
0.2.0,Resample the data
0.2.0,Resample the data
0.2.0,Create the object
0.2.0,continuous case
0.2.0,multiclass case
0.2.0,Resample the data
0.2.0,Resample the data
0.2.0,Resample the data
0.2.0,Test the various init parameters of the pipeline.
0.2.0,Check that we can't instantiate pipelines with objects without fit
0.2.0,method
0.2.0,Smoke test with only an estimator
0.2.0,Check that params are set
0.2.0,Smoke test the repr:
0.2.0,Test with two objects
0.2.0,Check that we can't use the same stage name twice
0.2.0,Check that params are set
0.2.0,Smoke test the repr:
0.2.0,Check that params are not set when naming them wrong
0.2.0,Test clone
0.2.0,"Check that apart from estimators, the parameters are the same"
0.2.0,Remove estimators that where copied
0.2.0,Test the various methods of the pipeline (anova).
0.2.0,Test with Anova + LogisticRegression
0.2.0,Test that the pipeline can take fit parameters
0.2.0,classifier should return True
0.2.0,and transformer params should not be changed
0.2.0,Test pipeline raises set params error message for nested models.
0.2.0,expected error message
0.2.0,nested model check
0.2.0,Test the various methods of the pipeline (pca + svm).
0.2.0,Test with PCA + SVC
0.2.0,Test the various methods of the pipeline (preprocessing + svm).
0.2.0,check shapes of various prediction functions
0.2.0,test that the fit_predict method is implemented on a pipeline
0.2.0,test that the fit_predict on pipeline yields same results as applying
0.2.0,transform and clustering steps separately
0.2.0,first compute the transform and clustering step separately
0.2.0,use a pipeline to do the transform and clustering in one step
0.2.0,tests that a pipeline does not have fit_predict method when final
0.2.0,step of pipeline does not have fit_predict defined
0.2.0,Test whether pipeline works with a transformer at the end.
0.2.0,Also test pipeline.transform and pipeline.inverse_transform
0.2.0,test transform and fit_transform:
0.2.0,Test whether pipeline works with a transformer missing fit_transform
0.2.0,test fit_transform:
0.2.0,Test the various methods of the pipeline (pca + svm).
0.2.0,Test with PCA + SVC
0.2.0,Test the various methods of the pipeline (pca + svm).
0.2.0,Test with PCA + SVC
0.2.0,Test whether pipeline works with a sampler at the end.
0.2.0,Also test pipeline.sampler
0.2.0,test transform and fit_transform:
0.2.0,Test whether pipeline works with a sampler at the end.
0.2.0,Also test pipeline.sampler
0.2.0,Test the various methods of the pipeline (anova).
0.2.0,Test with RandomUnderSampling + Anova + LogisticRegression
0.2.0,Test the various methods of the pipeline (anova).
0.2.0,"assert_raises(TypeError, lambda x: [][0])"
0.2.0,Test the various methods of the pipeline (anova).
0.2.0,Test with RandomUnderSampling + Anova + LogisticRegression
0.2.0,Check any parameters for SMOTE was provided
0.2.0,Anounce deprecation
0.2.0,We need to list each parameter and decide if we affect a default
0.2.0,value or not
0.2.0,"If an object was given, affect"
0.2.0,Otherwise create a default SMOTE
0.2.0,Check any parameters for ENN was provided
0.2.0,Anounce deprecation
0.2.0,We need to list each parameter and decide if we affect a default
0.2.0,value or not
0.2.0,"If an object was given, affect"
0.2.0,Otherwise create a default EditedNearestNeighbours
0.2.0,Fit using SMOTE
0.2.0,Transform using SMOTE
0.2.0,Fit and transform using ENN
0.2.0,Check any parameters for SMOTE was provided
0.2.0,Anounce deprecation
0.2.0,We need to list each parameter and decide if we affect a default
0.2.0,value or not
0.2.0,"If an object was given, affect"
0.2.0,Otherwise create a default SMOTE
0.2.0,Check any parameters for ENN was provided
0.2.0,Anounce deprecation
0.2.0,"If an object was given, affect"
0.2.0,Otherwise create a default EditedNearestNeighbours
0.2.0,Fit using SMOTE
0.2.0,Transform using SMOTE
0.2.0,Fit and transform using ENN
0.2.0,Generate a global dataset to use
0.2.0,Define a negative ratio
0.2.0,Define a ratio greater than 1
0.2.0,Define ratio as an unknown string
0.2.0,Define ratio as a list which is not supported
0.2.0,Create the object
0.2.0,Resample the data
0.2.0,Create a wrong y
0.2.0,Create the object
0.2.0,Fit the data
0.2.0,Check if the data information have been computed
0.2.0,Create the object
0.2.0,Create the object
0.2.0,Fit the data
0.2.0,Create the object
0.2.0,Fit the data
0.2.0,Create the object
0.2.0,continuous case
0.2.0,multiclass case
0.2.0,Create a SMOTE and Tomek object
0.2.0,Create a SMOTE and Tomek object
0.2.0,Generate a global dataset to use
0.2.0,Define a negative ratio
0.2.0,Define a ratio greater than 1
0.2.0,Define ratio as an unknown string
0.2.0,Define ratio as a list which is not supported
0.2.0,Create the object
0.2.0,Resample the data
0.2.0,Create a wrong y
0.2.0,Create the object
0.2.0,Fit the data
0.2.0,Check if the data information have been computed
0.2.0,Create the object
0.2.0,Create the object
0.2.0,Fit the data
0.2.0,Create the object
0.2.0,Fit the data
0.2.0,Create the object
0.2.0,continuous case
0.2.0,multiclass case
0.2.0,Create a SMOTE and Tomek object
0.2.0,Create a SMOTE and Tomek object
0.2.0,Check the random state
0.2.0,To be removed in 0.4
0.2.0,Define the classifier to use
0.2.0,Start with the minority class
0.2.0,Keep the indices of the minority class somewhere if we need to
0.2.0,return them later
0.2.0,Condition to initiliase before the search
0.2.0,Get the initial number of samples to select in the majority class
0.2.0,Create the array characterising the array containing the majority
0.2.0,class
0.2.0,Loop to create the different subsets
0.2.0,Generate an appropriate number of index to extract
0.2.0,from the majority class depending of the false classification
0.2.0,rate of the previous iteration
0.2.0,Mark these indexes as not being considered for next sampling
0.2.0,"For now, we will train and classify on the same data"
0.2.0,"Let see if we should find another solution. Anyway,"
0.2.0,random stuff are still random stuff
0.2.0,Push these data into a new subset
0.2.0,Get the indices of interest
0.2.0,"Draw samples, using sample weights, and then fit"
0.2.0,"Draw samples, using a mask, and then fit"
0.2.0,Predict using only the majority class
0.2.0,Basically let's find which sample have to be retained for the
0.2.0,next round
0.2.0,Find the misclassified index to keep them for the next round
0.2.0,Count how many random element will be selected
0.2.0,"We found a new subset, increase the counter"
0.2.0,Check if we have to make an early stopping
0.2.0,Select the remaining data
0.2.0,Select the final batch
0.2.0,Push these data into a new subset
0.2.0,"We found a new subset, increase the counter"
0.2.0,Specific case with n_max_subset = 1
0.2.0,Also check that we will have enough sample to extract at the
0.2.0,next round
0.2.0,Select the remaining data
0.2.0,Select the final batch
0.2.0,Push these data into a new subset
0.2.0,"We found a new subset, increase the counter"
0.2.0,Generate a global dataset to use
0.2.0,Define a negative ratio
0.2.0,Define a ratio greater than 1
0.2.0,Define ratio as an unknown string
0.2.0,Define ratio as a list which is not supported
0.2.0,Define a ratio
0.2.0,Define the parameter for the under-sampling
0.2.0,Create the object
0.2.0,Resample the data
0.2.0,Create a wrong y
0.2.0,Create the object
0.2.0,Fit the data
0.2.0,Define the parameter for the under-sampling
0.2.0,Create the object
0.2.0,Fit the data
0.2.0,Check if the data information have been computed
0.2.0,Define the parameter for the under-sampling
0.2.0,Create the object
0.2.0,Define the ratio parameter
0.2.0,Create the sampling object
0.2.0,Get the different subset
0.2.0,Check each array
0.2.0,Define the ratio parameter
0.2.0,Create the sampling object
0.2.0,Get the different subset
0.2.0,Check each array
0.2.0,Define the ratio parameter
0.2.0,Create the sampling object
0.2.0,Get the different subset
0.2.0,Check each array
0.2.0,Define the ratio parameter
0.2.0,Create the sampling object
0.2.0,Get the different subset
0.2.0,Check each array
0.2.0,Define the ratio parameter
0.2.0,Create the sampling object
0.2.0,Get the different subset
0.2.0,Check each array
0.2.0,Define the ratio parameter
0.2.0,Create the sampling object
0.2.0,Get the different subset
0.2.0,Check each array
0.2.0,Define the ratio parameter
0.2.0,Create the sampling object
0.2.0,Get the different subset
0.2.0,Check each array
0.2.0,Define the ratio parameter
0.2.0,Define the ratio parameter
0.2.0,Create the sampling object
0.2.0,Get the different subset
0.2.0,Check each array
0.2.0,Define the ratio parameter
0.2.0,Create the sampling object
0.2.0,Get the different subset
0.2.0,Check each array
0.2.0,Create the object
0.2.0,continuous case
0.2.0,multiclass case
0.2.0,Define the ratio parameter
0.2.0,Create the sampling object
0.2.0,Get the different subset
0.2.0,Check each array
0.2.0,Define the ratio parameter
0.2.0,Create the sampling object
0.2.0,Get the different subset
0.2.0,Define the ratio parameter
0.2.0,Create the sampling object
0.2.0,Get the different subset
0.2.0,Check each array
0.2.0,Generate a global dataset to use
0.2.0,Define a negative ratio
0.2.0,Define a ratio greater than 1
0.2.0,Define ratio as an unknown string
0.2.0,Define ratio as a list which is not supported
0.2.0,Define a ratio
0.2.0,Define the parameter for the under-sampling
0.2.0,Create the object
0.2.0,Resample the data
0.2.0,Create a wrong y
0.2.0,Create the object
0.2.0,Fit the data
0.2.0,Define the parameter for the under-sampling
0.2.0,Create the object
0.2.0,Fit the data
0.2.0,Check if the data information have been computed
0.2.0,Define the parameter for the under-sampling
0.2.0,Create the object
0.2.0,Define the ratio parameter
0.2.0,Create the sampling object
0.2.0,Get the different subset
0.2.0,Define the ratio parameter
0.2.0,Create the sampling object
0.2.0,Get the different subset
0.2.0,Define the ratio parameter
0.2.0,Create the sampling object
0.2.0,Get the different subset
0.2.0,Create the object
0.2.0,continuous case
0.2.0,Generate a global dataset to use
0.2.0,Define a zero ratio
0.2.0,Define a negative ratio
0.2.0,Define a ratio greater than 1
0.2.0,Define ratio as a list which is not supported
0.2.0,Make y to be multiclass
0.2.0,Resample the data
0.1.9,! /usr/bin/env python
0.1.9,"load all vars into globals, otherwise"
0.1.9,the later function call using global vars doesn't work.
0.1.9,"Allow command-lines such as ""python setup.py build install"""
0.1.9,Make sources available using relative paths from this file's directory.
0.1.9,-*- coding: utf-8 -*-
0.1.9,
0.1.9,"imbalanced-learn documentation build configuration file, created by"
0.1.9,sphinx-quickstart on Mon Jan 18 14:44:12 2016.
0.1.9,
0.1.9,This file is execfile()d with the current directory set to its
0.1.9,containing dir.
0.1.9,
0.1.9,Note that not all possible configuration values are present in this
0.1.9,autogenerated file.
0.1.9,
0.1.9,All configuration values have a default; values that are commented out
0.1.9,serve to show the default.
0.1.9,"If extensions (or modules to document with autodoc) are in another directory,"
0.1.9,add these directories to sys.path here. If the directory is relative to the
0.1.9,"documentation root, use os.path.abspath to make it absolute, like shown here."
0.1.9,"sys.path.insert(0, os.path.abspath('.'))"
0.1.9,-- General configuration ---------------------------------------------------
0.1.9,Try to override the matplotlib configuration as early as possible
0.1.9,-- General configuration ------------------------------------------------
0.1.9,"If your documentation needs a minimal Sphinx version, state it here."
0.1.9,needs_sphinx = '1.0'
0.1.9,"Add any Sphinx extension module names here, as strings. They can be"
0.1.9,extensions coming with Sphinx (named 'sphinx.ext.*') or your custom
0.1.9,ones.
0.1.9,path to your examples scripts
0.1.9,path where to save gallery generated examples
0.1.9,"Add any paths that contain templates here, relative to this directory."
0.1.9,generate autosummary even if no references
0.1.9,The suffix of source filenames.
0.1.9,The encoding of source files.
0.1.9,source_encoding = 'utf-8-sig'
0.1.9,Generate the plots for the gallery
0.1.9,The master toctree document.
0.1.9,General information about the project.
0.1.9,"The version info for the project you're documenting, acts as replacement for"
0.1.9,"|version| and |release|, also used in various other places throughout the"
0.1.9,built documents.
0.1.9,
0.1.9,The short X.Y version.
0.1.9,"The full version, including alpha/beta/rc tags."
0.1.9,The language for content autogenerated by Sphinx. Refer to documentation
0.1.9,for a list of supported languages.
0.1.9,language = None
0.1.9,"There are two options for replacing |today|: either, you set today to some"
0.1.9,"non-false value, then it is used:"
0.1.9,today = ''
0.1.9,"Else, today_fmt is used as the format for a strftime call."
0.1.9,"today_fmt = '%B %d, %Y'"
0.1.9,"List of patterns, relative to source directory, that match files and"
0.1.9,directories to ignore when looking for source files.
0.1.9,The reST default role (used for this markup: `text`) to use for all
0.1.9,documents.
0.1.9,default_role = None
0.1.9,"If true, '()' will be appended to :func: etc. cross-reference text."
0.1.9,"If true, the current module name will be prepended to all description"
0.1.9,unit titles (such as .. function::).
0.1.9,add_module_names = True
0.1.9,"If true, sectionauthor and moduleauthor directives will be shown in the"
0.1.9,output. They are ignored by default.
0.1.9,show_authors = False
0.1.9,The name of the Pygments (syntax highlighting) style to use.
0.1.9,A list of ignored prefixes for module index sorting.
0.1.9,modindex_common_prefix = []
0.1.9,"If true, keep warnings as ""system message"" paragraphs in the built documents."
0.1.9,keep_warnings = False
0.1.9,-- Options for HTML output ----------------------------------------------
0.1.9,The theme to use for HTML and HTML Help pages.  See the documentation for
0.1.9,a list of builtin themes.
0.1.9,Theme options are theme-specific and customize the look and feel of a theme
0.1.9,"further.  For a list of options available for each theme, see the"
0.1.9,documentation.
0.1.9,html_theme_options = {}
0.1.9,"Add any paths that contain custom themes here, relative to this directory."
0.1.9,"The name for this set of Sphinx documents.  If None, it defaults to"
0.1.9,"""<project> v<release> documentation""."
0.1.9,html_title = None
0.1.9,A shorter title for the navigation bar.  Default is the same as html_title.
0.1.9,html_short_title = None
0.1.9,The name of an image file (relative to this directory) to place at the top
0.1.9,of the sidebar.
0.1.9,html_logo = None
0.1.9,The name of an image file (within the static path) to use as favicon of the
0.1.9,docs.  This file should be a Windows icon file (.ico) being 16x16 or 32x32
0.1.9,pixels large.
0.1.9,html_favicon = None
0.1.9,"Add any paths that contain custom static files (such as style sheets) here,"
0.1.9,"relative to this directory. They are copied after the builtin static files,"
0.1.9,"so a file named ""default.css"" will overwrite the builtin ""default.css""."
0.1.9,Add any extra paths that contain custom files (such as robots.txt or
0.1.9,".htaccess) here, relative to this directory. These files are copied"
0.1.9,directly to the root of the documentation.
0.1.9,html_extra_path = []
0.1.9,"If not '', a 'Last updated on:' timestamp is inserted at every page bottom,"
0.1.9,using the given strftime format.
0.1.9,"html_last_updated_fmt = '%b %d, %Y'"
0.1.9,"If true, SmartyPants will be used to convert quotes and dashes to"
0.1.9,typographically correct entities.
0.1.9,html_use_smartypants = True
0.1.9,"Custom sidebar templates, maps document names to template names."
0.1.9,html_sidebars = {}
0.1.9,"Additional templates that should be rendered to pages, maps page names to"
0.1.9,template names.
0.1.9,html_additional_pages = {}
0.1.9,"If false, no module index is generated."
0.1.9,html_domain_indices = True
0.1.9,"If false, no index is generated."
0.1.9,html_use_index = True
0.1.9,"If true, the index is split into individual pages for each letter."
0.1.9,html_split_index = False
0.1.9,"If true, links to the reST sources are added to the pages."
0.1.9,html_show_sourcelink = True
0.1.9,"If true, ""Created using Sphinx"" is shown in the HTML footer. Default is True."
0.1.9,html_show_sphinx = True
0.1.9,"If true, ""(C) Copyright ..."" is shown in the HTML footer. Default is True."
0.1.9,html_show_copyright = True
0.1.9,"If true, an OpenSearch description file will be output, and all pages will"
0.1.9,contain a <link> tag referring to it.  The value of this option must be the
0.1.9,base URL from which the finished HTML is served.
0.1.9,html_use_opensearch = ''
0.1.9,"This is the file name suffix for HTML files (e.g. "".xhtml"")."
0.1.9,html_file_suffix = None
0.1.9,Output file base name for HTML help builder.
0.1.9,-- Options for LaTeX output ---------------------------------------------
0.1.9,The paper size ('letterpaper' or 'a4paper').
0.1.9,"'papersize': 'letterpaper',"
0.1.9,"The font size ('10pt', '11pt' or '12pt')."
0.1.9,"'pointsize': '10pt',"
0.1.9,Additional stuff for the LaTeX preamble.
0.1.9,"'preamble': '',"
0.1.9,Grouping the document tree into LaTeX files. List of tuples
0.1.9,"(source start file, target name, title,"
0.1.9,"author, documentclass [howto, manual, or own class])."
0.1.9,The name of an image file (relative to this directory) to place at the top of
0.1.9,the title page.
0.1.9,latex_logo = None
0.1.9,"For ""manual"" documents, if this is true, then toplevel headings are parts,"
0.1.9,not chapters.
0.1.9,latex_use_parts = False
0.1.9,"If true, show page references after internal links."
0.1.9,latex_show_pagerefs = False
0.1.9,"If true, show URL addresses after external links."
0.1.9,latex_show_urls = False
0.1.9,Documents to append as an appendix to all manuals.
0.1.9,latex_appendices = []
0.1.9,"If false, no module index is generated."
0.1.9,latex_domain_indices = True
0.1.9,-- Options for manual page output ---------------------------------------
0.1.9,One entry per manual page. List of tuples
0.1.9,"(source start file, name, description, authors, manual section)."
0.1.9,"If true, show URL addresses after external links."
0.1.9,man_show_urls = False
0.1.9,-- Options for Texinfo output -------------------------------------------
0.1.9,Grouping the document tree into Texinfo files. List of tuples
0.1.9,"(source start file, target name, title, author,"
0.1.9,"dir menu entry, description, category)"
0.1.9,"generate empty examples files, so that we don't get"
0.1.9,inclusion errors if there are no examples for a class / module
0.1.9,touch file
0.1.9,Documents to append as an appendix to all manuals.
0.1.9,texinfo_appendices = []
0.1.9,"If false, no module index is generated."
0.1.9,texinfo_domain_indices = True
0.1.9,"How to display URL addresses: 'footnote', 'no', or 'inline'."
0.1.9,texinfo_show_urls = 'footnote'
0.1.9,"If true, do not generate a @detailmenu in the ""Top"" node's menu."
0.1.9,texinfo_no_detailmenu = False
0.1.9,Example configuration for intersphinx: refer to the Python standard library.
0.1.9,Define some color for the plotting
0.1.9,Generate the dataset
0.1.9,Instanciate a PCA object for the sake of easy visualisation
0.1.9,Fit and transform x to visualise inside a 2D feature space
0.1.9,Apply SMOTE SVM
0.1.9,"Two subplots, unpack the axes array immediately"
0.1.9,Define some color for the plotting
0.1.9,Generate the dataset
0.1.9,Instanciate a PCA object for the sake of easy visualisation
0.1.9,Fit and transform x to visualise inside a 2D feature space
0.1.9,Apply Borderline SMOTE 2
0.1.9,"Two subplots, unpack the axes array immediately"
0.1.9,Define some color for the plotting
0.1.9,Generate the dataset
0.1.9,Instanciate a PCA object for the sake of easy visualisation
0.1.9,Fit and transform x to visualise inside a 2D feature space
0.1.9,Apply regular SMOTE
0.1.9,"Two subplots, unpack the axes array immediately"
0.1.9,Define some color for the plotting
0.1.9,Generate the dataset
0.1.9,Instanciate a PCA object for the sake of easy visualisation
0.1.9,Fit and transform x to visualise inside a 2D feature space
0.1.9,Apply the random over-sampling
0.1.9,"Two subplots, unpack the axes array immediately"
0.1.9,Define some color for the plotting
0.1.9,Generate the dataset
0.1.9,Instanciate a PCA object for the sake of easy visualisation
0.1.9,Fit and transform x to visualise inside a 2D feature space
0.1.9,Apply Borderline SMOTE 1
0.1.9,"Two subplots, unpack the axes array immediately"
0.1.9,Define some color for the plotting
0.1.9,Generate the dataset
0.1.9,Instanciate a PCA object for the sake of easy visualisation
0.1.9,Fit and transform x to visualise inside a 2D feature space
0.1.9,Apply the random over-sampling
0.1.9,"Two subplots, unpack the axes array immediately"
0.1.9,Define some color for the plotting
0.1.9,Generate the dataset
0.1.9,Instanciate a PCA object for the sake of easy visualisation
0.1.9,Fit and transform x to visualise inside a 2D feature space
0.1.9,Apply SMOTE + ENN
0.1.9,"Two subplots, unpack the axes array immediately"
0.1.9,Define some color for the plotting
0.1.9,Generate the dataset
0.1.9,Instanciate a PCA object for the sake of easy visualisation
0.1.9,Fit and transform x to visualise inside a 2D feature space
0.1.9,Apply SMOTE + Tomek links
0.1.9,"Two subplots, unpack the axes array immediately"
0.1.9,Define some color for the plotting
0.1.9,Generate the dataset
0.1.9,Instanciate a PCA object for the sake of easy visualisation
0.1.9,Fit and transform x to visualise inside a 2D feature space
0.1.9,Apply Balance Cascade method
0.1.9,"Two subplots, unpack the axes array immediately"
0.1.9,Define some color for the plotting
0.1.9,Generate the dataset
0.1.9,Instanciate a PCA object for the sake of easy visualisation
0.1.9,Fit and transform x to visualise inside a 2D feature space
0.1.9,Apply Easy Ensemble
0.1.9,"Two subplots, unpack the axes array immediately"
0.1.9,Define some color for the plotting
0.1.9,Generate the dataset
0.1.9,Instanciate a PCA object for the sake of easy visualisation
0.1.9,Fit and transform x to visualise inside a 2D feature space
0.1.9,"Three subplots, unpack the axes array immediately"
0.1.9,Apply the ENN
0.1.9,Apply the RENN
0.1.9,Define some color for the plotting
0.1.9,Generate the dataset
0.1.9,Instanciate a PCA object for the sake of easy visualisation
0.1.9,Fit and transform x to visualise inside a 2D feature space
0.1.9,Apply Nearmiss 3
0.1.9,"Two subplots, unpack the axes array immediately"
0.1.9,Define some color for the plotting
0.1.9,Generate the dataset
0.1.9,Instanciate a PCA object for the sake of easy visualisation
0.1.9,Fit and transform x to visualise inside a 2D feature space
0.1.9,Apply Edited Nearest Neighbours
0.1.9,"Two subplots, unpack the axes array immediately"
0.1.9,Define some color for the plotting
0.1.9,Generate the dataset
0.1.9,Instanciate a PCA object for the sake of easy visualisation
0.1.9,Fit and transform x to visualise inside a 2D feature space
0.1.9,Apply Tomek Links cleaning
0.1.9,"Two subplots, unpack the axes array immediately"
0.1.9,Define some color for the plotting
0.1.9,Generate the dataset
0.1.9,Instanciate a PCA object for the sake of easy visualisation
0.1.9,Fit and transform x to visualise inside a 2D feature space
0.1.9,Apply One-Sided Selection
0.1.9,"Two subplots, unpack the axes array immediately"
0.1.9,Define some color for the plotting
0.1.9,Generate the dataset
0.1.9,Instanciate a PCA object for the sake of easy visualisation
0.1.9,Fit and transform x to visualise inside a 2D feature space
0.1.9,Apply Nearmiss 2
0.1.9,"Two subplots, unpack the axes array immediately"
0.1.9,Define some color for the plotting
0.1.9,Generate the dataset
0.1.9,Instanciate a PCA object for the sake of easy visualisation
0.1.9,Fit and transform x to visualise inside a 2D feature space
0.1.9,Apply neighbourhood cleaning rule
0.1.9,"Two subplots, unpack the axes array immediately"
0.1.9,Define some color for the plotting
0.1.9,Generate the dataset
0.1.9,Instanciate a PCA object for the sake of easy visualisation
0.1.9,Fit and transform x to visualise inside a 2D feature space
0.1.9,Apply Condensed Nearest Neighbours
0.1.9,"Two subplots, unpack the axes array immediately"
0.1.9,Define some color for the plotting
0.1.9,Generate the dataset
0.1.9,Instanciate a PCA object for the sake of easy visualisation
0.1.9,Fit and transform x to visualise inside a 2D feature space
0.1.9,Apply Cluster Centroids
0.1.9,"Two subplots, unpack the axes array immediately"
0.1.9,Define some color for the plotting
0.1.9,Generate the dataset
0.1.9,Instanciate a PCA object for the sake of easy visualisation
0.1.9,Fit and transform x to visualise inside a 2D feature space
0.1.9,Apply the random under-sampling
0.1.9,"Two subplots, unpack the axes array immediately"
0.1.9,Define some color for the plotting
0.1.9,Generate the dataset
0.1.9,"Two subplots, unpack the axes array immediately"
0.1.9,Define some color for the plotting
0.1.9,Generate the dataset
0.1.9,Instanciate a PCA object for the sake of easy visualisation
0.1.9,Fit and transform x to visualise inside a 2D feature space
0.1.9,Apply Nearmiss 1
0.1.9,"Two subplots, unpack the axes array immediately"
0.1.9,Generate the dataset
0.1.9,Instanciate a PCA object for the sake of easy visualisation
0.1.9,Create the samplers
0.1.9,Create teh classifier
0.1.9,Make the splits
0.1.9,Add one transformers and two samplers in the pipeline object
0.1.9,Based on NiLearn package
0.1.9,License: simplified BSD
0.1.9,"PEP0440 compatible formatted version, see:"
0.1.9,https://www.python.org/dev/peps/pep-0440/
0.1.9,
0.1.9,Generic release markers:
0.1.9,X.Y
0.1.9,X.Y.Z # For bugfix releases
0.1.9,
0.1.9,Admissible pre-release markers:
0.1.9,X.YaN # Alpha release
0.1.9,X.YbN # Beta release
0.1.9,X.YrcN # Release Candidate
0.1.9,X.Y # Final release
0.1.9,
0.1.9,Dev branch marker is: 'X.Y.dev' or 'X.Y.devN' where N is an integer.
0.1.9,'X.Y.dev0' is the canonical version of 'X.Y.dev'
0.1.9,
0.1.9,"This is a tuple to preserve order, so that dependencies are checked"
0.1.9,in some meaningful order (more => less 'core').  We avoid using
0.1.9,collections.OrderedDict to preserve Python 2.6 compatibility.
0.1.9,Avoid choking on modules with no __version__ attribute
0.1.9,Skip check only when installing and it's a module that
0.1.9,will be auto-installed.
0.1.9,Check the consistency of X and y
0.1.9,Get all the unique elements in the target array
0.1.9,# Raise an error if there is only one class
0.1.9,if uniques.size == 1:
0.1.9,"raise RuntimeError(""Only one class detected, aborting..."")"
0.1.9,Raise a warning for the moment to be compatible with BaseEstimator
0.1.9,Store the size of X to check at sampling time if we have the
0.1.9,same data
0.1.9,Create a dictionary containing the class statistics
0.1.9,Find the minority and majority classes
0.1.9,Check if the ratio provided at initialisation make sense
0.1.9,Check the consistency of X and y
0.1.9,Check that the data have been fitted
0.1.9,Check if the size of the data is identical than at fitting
0.1.9,The ratio correspond to the number of samples in the minority class
0.1.9,"over the number of samples in the majority class. Thus, the ratio"
0.1.9,cannot be greater than 1.0
0.1.9,Adapted from scikit-learn
0.1.9,Author: Edouard Duchesnay
0.1.9,Gael Varoquaux
0.1.9,Virgile Fritsch
0.1.9,Alexandre Gramfort
0.1.9,Lars Buitinck
0.1.9,chkoar
0.1.9,License: BSD
0.1.9,BaseEstimator interface
0.1.9,shallow copy of steps
0.1.9,Estimator interface
0.1.9,Boolean controlling whether the joblib caches should be
0.1.9,"flushed if the version of certain modules changes (eg nibabel, as it"
0.1.9,does not respect the backward compatibility in some of its internal
0.1.9,structures
0.1.9,This  is used in nilearn._utils.cache_mixin
0.1.9,list all submodules available in imblearn and version
0.1.9,Keep the samples from the majority class
0.1.9,Loop over the other classes over picking at random
0.1.9,"If this is the majority class, skip it"
0.1.9,Define the number of sample to create
0.1.9,Pick some elements at random
0.1.9,Concatenate to the majority class
0.1.9,Keep the samples from the majority class
0.1.9,Define the number of sample to create
0.1.9,We handle only two classes problem for the moment.
0.1.9,Start by separating minority class features and target values.
0.1.9,Print if verbose is true
0.1.9,"Look for k-th nearest neighbours, excluding, of course, the"
0.1.9,point itself.
0.1.9,Get the distance to the NN
0.1.9,Compute the ratio of majority samples next to minority samples
0.1.9,Check that we found at least some neighbours belonging to the
0.1.9,majority class
0.1.9,Normalize the ratio
0.1.9,Compute the number of sample to be generated
0.1.9,For each minority samples
0.1.9,Pick-up the neighbors wanted
0.1.9,Create a new sample
0.1.9,Find the NN for each samples
0.1.9,Exclude the sample itself
0.1.9,Count how many NN belong to the minority class
0.1.9,Find the class corresponding to the label in x
0.1.9,Compute the number of majority samples in the NN
0.1.9,Samples are in danger for m/2 <= m' < m
0.1.9,Samples are noise for m = m'
0.1.9,Check the consistency of X
0.1.9,Check the random state
0.1.9,A matrix to store the synthetic samples
0.1.9,# Set seeds
0.1.9,"seeds = random_state.randint(low=0,"
0.1.9,"high=100 * len(nn_num.flatten()),"
0.1.9,size=n_samples)
0.1.9,Randomly pick samples to construct neighbours from
0.1.9,Loop over the NN matrix and create new samples
0.1.9,"NN lines relate to original sample, columns to its"
0.1.9,nearest neighbours
0.1.9,"Take a step of random size (0,1) in the direction of the"
0.1.9,n nearest neighbours
0.1.9,if self.random_state is None:
0.1.9,np.random.seed(seeds[i])
0.1.9,else:
0.1.9,np.random.seed(self.random_state)
0.1.9,Construct synthetic sample
0.1.9,The returned target vector is simply a repetition of the
0.1.9,minority label
0.1.9,Define the number of sample to create
0.1.9,We handle only two classes problem for the moment.
0.1.9,Start by separating minority class features and target values.
0.1.9,If regular SMOTE is to be performed
0.1.9,"Look for k-th nearest neighbours, excluding, of course, the"
0.1.9,point itself.
0.1.9,Matrix with k-th nearest neighbours indexes for each minority
0.1.9,element.
0.1.9,--- Generating synthetic samples
0.1.9,Use static method make_samples to generate minority samples
0.1.9,Concatenate the newly generated samples to the original data set
0.1.9,Find the NNs for all samples in the data set.
0.1.9,Boolean array with True for minority samples in danger
0.1.9,"If all minority samples are safe, return the original data set."
0.1.9,"All are safe, nothing to be done here."
0.1.9,"If we got here is because some samples are in danger, we need to"
0.1.9,find the NNs among the minority class to create the new synthetic
0.1.9,samples.
0.1.9,
0.1.9,We start by changing the number of NNs to consider from m + 1
0.1.9,to k + 1
0.1.9,nns...#
0.1.9,B1 and B2 types diverge here!!!
0.1.9,Create synthetic samples for borderline points.
0.1.9,Concatenate the newly generated samples to the original
0.1.9,dataset
0.1.9,Reset the k-neighbours to m+1 neighbours
0.1.9,Split the number of synthetic samples between only minority
0.1.9,"(type 1), or minority and majority (with reduced step size)"
0.1.9,(type 2).
0.1.9,The fraction is sampled from a beta distribution centered
0.1.9,around 0.5 with variance ~0.01
0.1.9,Only minority
0.1.9,Only majority with smaller step size
0.1.9,Concatenate the newly generated samples to the original
0.1.9,data set
0.1.9,Reset the k-neighbours to m+1 neighbours
0.1.9,The SVM smote model fits a support vector machine
0.1.9,classifier to the data and uses the support vector to
0.1.9,"provide a notion of boundary. Unlike regular smote, where"
0.1.9,such notion relies on proportion of nearest neighbours
0.1.9,belonging to each class.
0.1.9,Fit SVM to the full data#
0.1.9,Find the support vectors and their corresponding indexes
0.1.9,"First, find the nn of all the samples to identify samples"
0.1.9,in danger and noisy ones
0.1.9,"As usual, fit a nearest neighbour model to the data"
0.1.9,"Now, get rid of noisy support vectors"
0.1.9,Remove noisy support vectors
0.1.9,Proceed to find support vectors NNs among the minority class
0.1.9,Split the number of synthetic samples between interpolation and
0.1.9,extrapolation
0.1.9,The fraction are sampled from a beta distribution with mean
0.1.9,0.5 and variance 0.01#
0.1.9,Interpolate samples in danger
0.1.9,Extrapolate safe samples
0.1.9,Concatenate the newly generated samples to the original data set
0.1.9,not any support vectors in danger
0.1.9,All the support vector in danger
0.1.9,Reset the k-neighbours to m+1 neighbours
0.1.9,--- NN object
0.1.9,Import the NN object from scikit-learn library. Since in the smote
0.1.9,"variations we must first find samples that are in danger, we"
0.1.9,initialize the NN object differently depending on the method chosen
0.1.9,"Regular smote does not look for samples in danger, instead it"
0.1.9,creates synthetic samples directly from the k-th nearest
0.1.9,neighbours with not filtering
0.1.9,"Borderline1, 2 and SVM variations of smote must first look for"
0.1.9,samples that could be considered noise and samples that live
0.1.9,"near the boundary between the classes. Therefore, before"
0.1.9,"creating synthetic samples from the k-th nns, it first look"
0.1.9,for m nearest neighbors to decide whether or not a sample is
0.1.9,noise or near the boundary.
0.1.9,--- SVM smote
0.1.9,"Unlike the borderline variations, the SVM variation uses the support"
0.1.9,vectors to decide which samples are in danger (near the boundary).
0.1.9,Additionally it also introduces extrapolation for samples that are
0.1.9,considered safe (far from boundary) and interpolation for samples
0.1.9,in danger (near the boundary). The level of extrapolation is
0.1.9,controled by the out_step.
0.1.9,Store SVM object with any parameters
0.1.9,Generate a global dataset to use
0.1.9,Define a negative ratio
0.1.9,Define a ratio greater than 1
0.1.9,Define ratio as an unknown string
0.1.9,Define ratio as a list which is not supported
0.1.9,Define a ratio
0.1.9,Create the object
0.1.9,Resample the data
0.1.9,Create a wrong y
0.1.9,Create the object
0.1.9,Fit the data
0.1.9,Create the object
0.1.9,Fit the data
0.1.9,Check if the data information have been computed
0.1.9,Create the object
0.1.9,Resample the data
0.1.9,Resample the data
0.1.9,Create the object
0.1.9,Generate a global dataset to use
0.1.9,Define a negative ratio
0.1.9,Define a ratio greater than 1
0.1.9,Define ratio as an unknown string
0.1.9,Define ratio as a list which is not supported
0.1.9,Define a ratio
0.1.9,Create the object
0.1.9,Resample the data
0.1.9,Create a wrong y
0.1.9,Create the object
0.1.9,Fit the data
0.1.9,Create the object
0.1.9,Fit the data
0.1.9,Check if the data information have been computed
0.1.9,Create the object
0.1.9,Resample the data
0.1.9,Resample the data
0.1.9,Create the object
0.1.9,Generate a global dataset to use
0.1.9,Define a negative ratio
0.1.9,Define a ratio greater than 1
0.1.9,Define ratio as an unknown string
0.1.9,Define ratio as a list which is not supported
0.1.9,Create the object
0.1.9,Resample the data
0.1.9,Create a wrong y
0.1.9,Create the object
0.1.9,Fit the data
0.1.9,Check if the data information have been computed
0.1.9,Create the object
0.1.9,Create the object
0.1.9,Fit the data
0.1.9,Create the object
0.1.9,Fit the data
0.1.9,Create the object
0.1.9,Fit the data
0.1.9,Create the object
0.1.9,Fit the data
0.1.9,Create the object
0.1.9,Fit the data
0.1.9,Create the object
0.1.9,Start with the minority class
0.1.9,All the minority class samples will be preserved
0.1.9,If we need to offer support for the indices
0.1.9,Loop over the other classes under picking at random
0.1.9,"If the minority class is up, skip it"
0.1.9,Randomly get one sample from the majority class
0.1.9,Generate the index to select
0.1.9,Create the set C
0.1.9,Create the set S
0.1.9,Remove the seed from S since that it will be added anyway
0.1.9,Create a k-NN classifier
0.1.9,Fit C into the knn
0.1.9,Classify on S
0.1.9,Find the misclassified S_y
0.1.9,If we need to offer support for the indices selected
0.1.9,We concatenate the misclassified samples with the seed and the
0.1.9,minority samples
0.1.9,Find the nearest neighbour of every point
0.1.9,Send the information to is_tomek function to get boolean vector back
0.1.9,Check if the indices of the samples selected should be returned too
0.1.9,Return the indices of interest
0.1.9,Return data set without majority Tomek links.
0.1.9,Compute the distance considering the farthest neighbour
0.1.9,Sort the list of distance and get the index
0.1.9,Throw a warning to tell the user that we did not have enough samples
0.1.9,to select and that we just select everything
0.1.9,Select the desired number of samples
0.1.9,Assign the parameter of the element of this class
0.1.9,Check that the version asked is implemented
0.1.9,Start with the minority class
0.1.9,All the minority class samples will be preserved
0.1.9,Compute the number of cluster needed
0.1.9,If we need to offer support for the indices
0.1.9,"For each element of the current class, find the set of NN"
0.1.9,of the minority class
0.1.9,Call the constructor of the NN
0.1.9,Fit the minority class since that we want to know the distance
0.1.9,to these point
0.1.9,Loop over the other classes under picking at random
0.1.9,"If the minority class is up, skip it"
0.1.9,Get the samples corresponding to the current class
0.1.9,Find the NN
0.1.9,Select the right samples
0.1.9,Find the NN
0.1.9,Select the right samples
0.1.9,We need a new NN object to fit the current class
0.1.9,Find the set of NN to the minority class
0.1.9,Create the subset containing the samples found during the NN
0.1.9,search. Linearize the indexes and remove the double values
0.1.9,Create the subset
0.1.9,Compute the NN considering the current class
0.1.9,If we need to offer support for the indices selected
0.1.9,Check if the indices of the samples selected should be returned too
0.1.9,Return the indices of interest
0.1.9,Compute the number of clusters needed
0.1.9,All the minority class samples will be preserved
0.1.9,If we need to offer support for the indices
0.1.9,Loop over the other classes under-picking at random
0.1.9,"If the minority class is up, skip it"
0.1.9,Pick some elements at random
0.1.9,If we need to offer support for the indices selected
0.1.9,Concatenate to the minority class
0.1.9,Check if the indices of the samples selected should be returned as
0.1.9,well
0.1.9,Return the indices of interest
0.1.9,"Initialize the boolean result as false, and also a counter"
0.1.9,Loop through each sample and looks whether it belongs to the minority
0.1.9,"class. If it does, we don't consider it since we want to keep all"
0.1.9,"minority samples. If, however, it belongs to the majority sample we"
0.1.9,look at its first neighbour. If its closest neighbour also has the
0.1.9,"current sample as its closest neighbour, the two form a Tomek link."
0.1.9,"If they form a tomek link, put a True marker on this"
0.1.9,"sample, and increase counter by one."
0.1.9,Find the nearest neighbour of every point
0.1.9,Send the information to is_tomek function to get boolean vector back
0.1.9,Check if the indices of the samples selected should be returned too
0.1.9,Return the indices of interest
0.1.9,Return data set without majority Tomek links.
0.1.9,Start with the minority class
0.1.9,All the minority class samples will be preserved
0.1.9,If we need to offer support for the indices
0.1.9,Loop over the other classes under picking at random
0.1.9,"If the minority class is up, skip it"
0.1.9,Randomly get one sample from the majority class
0.1.9,Generate the index to select
0.1.9,Create the set C - One majority samples and all minority
0.1.9,Create the set S - all majority samples
0.1.9,Create a k-NN classifier
0.1.9,Fit C into the knn
0.1.9,Check each sample in S if we keep it or drop it
0.1.9,Do not select sample which are already well classified
0.1.9,Classify on S
0.1.9,If the prediction do not agree with the true label
0.1.9,append it in C_x
0.1.9,Keep the index for later
0.1.9,Update C
0.1.9,Fit C into the knn
0.1.9,This experimental to speed up the search
0.1.9,Classify all the element in S and avoid to test the
0.1.9,well classified elements
0.1.9,Find the misclassified S_y
0.1.9,"The indexes found are relative to the current class, we need to"
0.1.9,find the absolute value
0.1.9,Build the array with the absolute position
0.1.9,If we need to offer support for the indices selected
0.1.9,Check if the indices of the samples selected should be returned too
0.1.9,Return the indices of interest
0.1.9,Compute the number of cluster needed
0.1.9,Create the clustering object
0.1.9,Start with the minority class
0.1.9,All the minority class samples will be preserved
0.1.9,Loop over the other classes under picking at random
0.1.9,"If the minority class is up, skip it."
0.1.9,Find the centroids via k-means
0.1.9,Concatenate to the minority class
0.1.9,Start with the minority class
0.1.9,All the minority class samples will be preserved
0.1.9,If we need to offer support for the indices
0.1.9,Create a k-NN to fit the whole data
0.1.9,Fit the whole dataset
0.1.9,Loop over the other classes under picking at random
0.1.9,Get the sample of the current class
0.1.9,Get the samples associated
0.1.9,Find the NN for the current class
0.1.9,Get the label of the corresponding to the index
0.1.9,Check which one are the same label than the current class
0.1.9,Make an AND operation through the three neighbours
0.1.9,If the minority class remove the majority samples
0.1.9,Get the index to exclude
0.1.9,Get the index to exclude
0.1.9,Create a vector with the sample to select
0.1.9,Exclude as well the minority sample since that they will be
0.1.9,concatenated later
0.1.9,Get the samples from the majority classes
0.1.9,If we need to offer support for the indices selected
0.1.9,Check if the indices of the samples selected should be returned too
0.1.9,Return the indices of interest
0.1.9,Start with the minority class
0.1.9,All the minority class samples will be preserved
0.1.9,If we need to offer support for the indices
0.1.9,Create a k-NN to fit the whole data
0.1.9,Fit the data
0.1.9,Loop over the other classes under picking at random
0.1.9,"If the minority class is up, skip it"
0.1.9,Get the sample of the current class
0.1.9,Find the NN for the current class
0.1.9,Get the label of the corresponding to the index
0.1.9,Check which one are the same label than the current class
0.1.9,Make the majority vote
0.1.9,Get the samples which agree all together
0.1.9,If we need to offer support for the indices selected
0.1.9,Check if the indices of the samples selected should be returned too
0.1.9,Return the indices of interest
0.1.9,Check the stopping criterion
0.1.9,1. If there is no changes for the vector y
0.1.9,2. If the number of samples in the other class become inferior to
0.1.9,the number of samples in the majority class
0.1.9,3. If one of the class is disappearing
0.1.9,Case 1
0.1.9,Case 2
0.1.9,Get the number of samples in the non-minority classes
0.1.9,Check the minority stop to be the minority
0.1.9,Case 3
0.1.9,"If this is a normal convergence, get the last data"
0.1.9,Log the variables to explain the stop of the algorithm
0.1.9,Update the data for the next iteration
0.1.9,Check if the indices of the samples selected should be returned too
0.1.9,Return the indices of interest
0.1.9,Select the appropriate classifier
0.1.9,Create the different folds
0.1.9,Compute the number of cluster needed
0.1.9,Find the percentile corresponding to the top num_samples
0.1.9,Sample the data
0.1.9,If we need to offer support for the indices
0.1.9,Generate a global dataset to use
0.1.9,Define a negative ratio
0.1.9,Define a ratio greater than 1
0.1.9,Define ratio as an unknown string
0.1.9,Define ratio as a list which is not supported
0.1.9,Define a ratio
0.1.9,Define the parameter for the under-sampling
0.1.9,Create the object
0.1.9,Resample the data
0.1.9,Create a wrong y
0.1.9,Create the object
0.1.9,Fit the data
0.1.9,Define the parameter for the under-sampling
0.1.9,Create the object
0.1.9,Fit the data
0.1.9,Check if the data information have been computed
0.1.9,Define the parameter for the under-sampling
0.1.9,Create the object
0.1.9,Define the parameter for the under-sampling
0.1.9,Create the object
0.1.9,Fit and sample
0.1.9,Define the parameter for the under-sampling
0.1.9,Create the object
0.1.9,Fit and sample
0.1.9,Define the parameter for the under-sampling
0.1.9,Create the object
0.1.9,Fit and sample
0.1.9,Create the object
0.1.9,Generate a global dataset to use
0.1.9,Define a ratio
0.1.9,Create the object
0.1.9,Resample the data
0.1.9,Create a wrong y
0.1.9,Create the object
0.1.9,Fit the data
0.1.9,Check if the data information have been computed
0.1.9,Create the object
0.1.9,Resample the data
0.1.9,Resample the data
0.1.9,Create the object
0.1.9,Generate a global dataset to use
0.1.9,Define a ratio
0.1.9,Create the object
0.1.9,Resample the data
0.1.9,Create a wrong y
0.1.9,Create the object
0.1.9,Fit the data
0.1.9,Check if the data information have been computed
0.1.9,Create the object
0.1.9,Resample the data
0.1.9,Resample the data
0.1.9,Create the object
0.1.9,Generate a global dataset to use
0.1.9,Define a ratio
0.1.9,Create the object
0.1.9,Resample the data
0.1.9,Create a wrong y
0.1.9,Create the object
0.1.9,Fit the data
0.1.9,Check if the data information have been computed
0.1.9,Create the object
0.1.9,Resample the data
0.1.9,Resample the data
0.1.9,Resample the data
0.1.9,Create the object
0.1.9,Generate a global dataset to use
0.1.9,Define a negative ratio
0.1.9,Define a ratio greater than 1
0.1.9,Define ratio as an unknown string
0.1.9,Define ratio as a list which is not supported
0.1.9,Define a ratio
0.1.9,Create the object
0.1.9,Resample the data
0.1.9,Create a wrong y
0.1.9,Create the object
0.1.9,Fit the data
0.1.9,Create the object
0.1.9,Fit the data
0.1.9,Check if the data information have been computed
0.1.9,Create the object
0.1.9,Resample the data
0.1.9,Resample the data
0.1.9,Resample the data
0.1.9,Create the object
0.1.9,Generate a global dataset to use
0.1.9,Define a negative ratio
0.1.9,Define a ratio greater than 1
0.1.9,Define ratio as an unknown string
0.1.9,Define ratio as a list which is not supported
0.1.9,Resample the data
0.1.9,Define a ratio
0.1.9,Create the object
0.1.9,Resample the data
0.1.9,Create a wrong y
0.1.9,Create the object
0.1.9,Fit the data
0.1.9,Create the object
0.1.9,Fit the data
0.1.9,Check if the data information have been computed
0.1.9,Create the object
0.1.9,Resample the data
0.1.9,Resample the data
0.1.9,Resample the data
0.1.9,Resample the data
0.1.9,Resample the data
0.1.9,Resample the data
0.1.9,Resample the data
0.1.9,Resample the data
0.1.9,Resample the data
0.1.9,Create the object
0.1.9,Generate a global dataset to use
0.1.9,Define a ratio
0.1.9,Create the object
0.1.9,Create the object
0.1.9,Resample the data
0.1.9,Create a wrong y
0.1.9,Create the object
0.1.9,Fit the data
0.1.9,Check if the data information have been computed
0.1.9,Create the object
0.1.9,Resample the data
0.1.9,Resample the data
0.1.9,Resample the data
0.1.9,Create the object
0.1.9,Generate a global dataset to use
0.1.9,Define a negative ratio
0.1.9,Define a ratio greater than 1
0.1.9,Define ratio as an unknown string
0.1.9,Define ratio as a list which is not supported
0.1.9,Define a ratio
0.1.9,Define the parameter for the under-sampling
0.1.9,Create the object
0.1.9,Resample the data
0.1.9,Create a wrong y
0.1.9,Create the object
0.1.9,Fit the data
0.1.9,Define the parameter for the under-sampling
0.1.9,Create the object
0.1.9,Fit the data
0.1.9,Check if the data information have been computed
0.1.9,Define the parameter for the under-sampling
0.1.9,Create the object
0.1.9,Define the parameter for the under-sampling
0.1.9,Create the object
0.1.9,Fit and sample
0.1.9,Define the parameter for the under-sampling
0.1.9,Create the object
0.1.9,Fit and sample
0.1.9,Define the parameter for the under-sampling
0.1.9,Create the object
0.1.9,Fit and sample
0.1.9,Create the object
0.1.9,Generate a global dataset to use
0.1.9,Define a ratio
0.1.9,Create the object
0.1.9,Resample the data
0.1.9,Create a wrong y
0.1.9,Create the object
0.1.9,Fit the data
0.1.9,Check if the data information have been computed
0.1.9,Create the object
0.1.9,Resample the data
0.1.9,Resample the data
0.1.9,Create the object
0.1.9,Generate a global dataset to use
0.1.9,Define a negative ratio
0.1.9,Define a ratio greater than 1
0.1.9,Define ratio as an unknown string
0.1.9,Define ratio as a list which is not supported
0.1.9,Define a ratio
0.1.9,Define the parameter for the under-sampling
0.1.9,Create the object
0.1.9,Resample the data
0.1.9,Create a wrong y
0.1.9,Create the object
0.1.9,Fit the data
0.1.9,Define the parameter for the under-sampling
0.1.9,Create the object
0.1.9,Fit the data
0.1.9,Check if the data information have been computed
0.1.9,Define the parameter for the under-sampling
0.1.9,Create the object
0.1.9,Define the parameter for the under-sampling
0.1.9,Create the object
0.1.9,Define the parameter for the under-sampling
0.1.9,Create the object
0.1.9,Fit and sample
0.1.9,Define the parameter for the under-sampling
0.1.9,Create the object
0.1.9,Fit and sample
0.1.9,Generate a global dataset to use
0.1.9,Define a negative ratio
0.1.9,Define a ratio greater than 1
0.1.9,Define ratio as an unknown string
0.1.9,Define ratio as a list which is not supported
0.1.9,Define a ratio
0.1.9,Define the parameter for the under-sampling
0.1.9,Create the object
0.1.9,Resample the data
0.1.9,Create a wrong y
0.1.9,Create the object
0.1.9,Fit the data
0.1.9,Define the parameter for the under-sampling
0.1.9,Create the object
0.1.9,Fit the data
0.1.9,Check if the data information have been computed
0.1.9,Define the parameter for the under-sampling
0.1.9,Create the object
0.1.9,Define the parameter for the under-sampling
0.1.9,Create the object
0.1.9,Fit and sample
0.1.9,Define the parameter for the under-sampling
0.1.9,Create the object
0.1.9,Fit and sample
0.1.9,Define the parameter for the under-sampling
0.1.9,Create the object
0.1.9,Fit and sample
0.1.9,Create the object
0.1.9,Generate a global dataset to use
0.1.9,Define a ratio
0.1.9,Create the object
0.1.9,Resample the data
0.1.9,Create a wrong y
0.1.9,Create the object
0.1.9,Fit the data
0.1.9,Check if the data information have been computed
0.1.9,Create the object
0.1.9,Resample the data
0.1.9,Resample the data
0.1.9,Create the object
0.1.9,Test the various init parameters of the pipeline.
0.1.9,Check that we can't instantiate pipelines with objects without fit
0.1.9,method
0.1.9,Smoke test with only an estimator
0.1.9,Check that params are set
0.1.9,Smoke test the repr:
0.1.9,Test with two objects
0.1.9,Check that we can't use the same stage name twice
0.1.9,Check that params are set
0.1.9,Smoke test the repr:
0.1.9,Check that params are not set when naming them wrong
0.1.9,Test clone
0.1.9,"Check that apart from estimators, the parameters are the same"
0.1.9,Remove estimators that where copied
0.1.9,Test the various methods of the pipeline (anova).
0.1.9,Test with Anova + LogisticRegression
0.1.9,Test that the pipeline can take fit parameters
0.1.9,classifier should return True
0.1.9,and transformer params should not be changed
0.1.9,Test pipeline raises set params error message for nested models.
0.1.9,expected error message
0.1.9,nested model check
0.1.9,Test the various methods of the pipeline (pca + svm).
0.1.9,Test with PCA + SVC
0.1.9,Test the various methods of the pipeline (preprocessing + svm).
0.1.9,check shapes of various prediction functions
0.1.9,test that the fit_predict method is implemented on a pipeline
0.1.9,test that the fit_predict on pipeline yields same results as applying
0.1.9,transform and clustering steps separately
0.1.9,first compute the transform and clustering step separately
0.1.9,use a pipeline to do the transform and clustering in one step
0.1.9,tests that a pipeline does not have fit_predict method when final
0.1.9,step of pipeline does not have fit_predict defined
0.1.9,Test whether pipeline works with a transformer at the end.
0.1.9,Also test pipeline.transform and pipeline.inverse_transform
0.1.9,test transform and fit_transform:
0.1.9,Test whether pipeline works with a transformer missing fit_transform
0.1.9,test fit_transform:
0.1.9,Test the various methods of the pipeline (pca + svm).
0.1.9,Test with PCA + SVC
0.1.9,Test the various methods of the pipeline (pca + svm).
0.1.9,Test with PCA + SVC
0.1.9,Test whether pipeline works with a sampler at the end.
0.1.9,Also test pipeline.sampler
0.1.9,test transform and fit_transform:
0.1.9,Test whether pipeline works with a sampler at the end.
0.1.9,Also test pipeline.sampler
0.1.9,Test the various methods of the pipeline (anova).
0.1.9,Test with RandomUnderSampling + Anova + LogisticRegression
0.1.9,Fit using SMOTE
0.1.9,Transform using SMOTE
0.1.9,Fit and transform using ENN
0.1.9,Fit using SMOTE
0.1.9,Transform using SMOTE
0.1.9,Fit and transform using ENN
0.1.9,Generate a global dataset to use
0.1.9,Define a negative ratio
0.1.9,Define a ratio greater than 1
0.1.9,Define ratio as an unknown string
0.1.9,Define ratio as a list which is not supported
0.1.9,Create the object
0.1.9,Resample the data
0.1.9,Create a wrong y
0.1.9,Create the object
0.1.9,Fit the data
0.1.9,Check if the data information have been computed
0.1.9,Create the object
0.1.9,Create the object
0.1.9,Fit the data
0.1.9,Create the object
0.1.9,Fit the data
0.1.9,Create the object
0.1.9,Generate a global dataset to use
0.1.9,Define a negative ratio
0.1.9,Define a ratio greater than 1
0.1.9,Define ratio as an unknown string
0.1.9,Define ratio as a list which is not supported
0.1.9,Create the object
0.1.9,Resample the data
0.1.9,Create a wrong y
0.1.9,Create the object
0.1.9,Fit the data
0.1.9,Check if the data information have been computed
0.1.9,Create the object
0.1.9,Create the object
0.1.9,Fit the data
0.1.9,Create the object
0.1.9,Fit the data
0.1.9,Create the object
0.1.9,Check the random state
0.1.9,Define the classifier to use
0.1.9,Start with the minority class
0.1.9,Keep the indices of the minority class somewhere if we need to
0.1.9,return them later
0.1.9,Condition to initiliase before the search
0.1.9,Get the initial number of samples to select in the majority class
0.1.9,Create the array characterising the array containing the majority
0.1.9,class
0.1.9,Loop to create the different subsets
0.1.9,Generate an appropriate number of index to extract
0.1.9,from the majority class depending of the false classification
0.1.9,rate of the previous iteration
0.1.9,Mark these indexes as not being considered for next sampling
0.1.9,"For now, we will train and classify on the same data"
0.1.9,"Let see if we should find another solution. Anyway,"
0.1.9,random stuff are still random stuff
0.1.9,Push these data into a new subset
0.1.9,Apply a bootstrap on x_data
0.1.9,Train the classifier using the current data
0.1.9,Train the classifier using the current data
0.1.9,Predict using only the majority class
0.1.9,Basically let's find which sample have to be retained for the
0.1.9,next round
0.1.9,Find the misclassified index to keep them for the next round
0.1.9,Count how many random element will be selected
0.1.9,"We found a new subset, increase the counter"
0.1.9,Check if we have to make an early stopping
0.1.9,Select the remaining data
0.1.9,Select the final batch
0.1.9,Push these data into a new subset
0.1.9,"We found a new subset, increase the counter"
0.1.9,Specific case with n_max_subset = 1
0.1.9,Also check that we will have enough sample to extract at the
0.1.9,next round
0.1.9,Select the remaining data
0.1.9,Select the final batch
0.1.9,Push these data into a new subset
0.1.9,"We found a new subset, increase the counter"
0.1.9,Generate a global dataset to use
0.1.9,Define a negative ratio
0.1.9,Define a ratio greater than 1
0.1.9,Define ratio as an unknown string
0.1.9,Define ratio as a list which is not supported
0.1.9,Define a ratio
0.1.9,Define the parameter for the under-sampling
0.1.9,Create the object
0.1.9,Resample the data
0.1.9,Create a wrong y
0.1.9,Create the object
0.1.9,Fit the data
0.1.9,Define the parameter for the under-sampling
0.1.9,Create the object
0.1.9,Fit the data
0.1.9,Check if the data information have been computed
0.1.9,Define the parameter for the under-sampling
0.1.9,Create the object
0.1.9,Define the ratio parameter
0.1.9,Create the sampling object
0.1.9,Get the different subset
0.1.9,Check each array
0.1.9,Define the ratio parameter
0.1.9,Create the sampling object
0.1.9,Get the different subset
0.1.9,Check each array
0.1.9,Define the ratio parameter
0.1.9,Create the sampling object
0.1.9,Get the different subset
0.1.9,Check each array
0.1.9,Define the ratio parameter
0.1.9,Create the sampling object
0.1.9,Get the different subset
0.1.9,Check each array
0.1.9,Define the ratio parameter
0.1.9,Create the sampling object
0.1.9,Get the different subset
0.1.9,Check each array
0.1.9,Define the ratio parameter
0.1.9,Create the sampling object
0.1.9,Get the different subset
0.1.9,Check each array
0.1.9,Define the ratio parameter
0.1.9,Create the sampling object
0.1.9,Get the different subset
0.1.9,Check each array
0.1.9,Define the ratio parameter
0.1.9,Define the ratio parameter
0.1.9,Create the sampling object
0.1.9,Get the different subset
0.1.9,Check each array
0.1.9,Create the object
0.1.9,Generate a global dataset to use
0.1.9,Define a negative ratio
0.1.9,Define a ratio greater than 1
0.1.9,Define ratio as an unknown string
0.1.9,Define ratio as a list which is not supported
0.1.9,Define a ratio
0.1.9,Define the parameter for the under-sampling
0.1.9,Create the object
0.1.9,Resample the data
0.1.9,Create a wrong y
0.1.9,Create the object
0.1.9,Fit the data
0.1.9,Define the parameter for the under-sampling
0.1.9,Create the object
0.1.9,Fit the data
0.1.9,Check if the data information have been computed
0.1.9,Define the parameter for the under-sampling
0.1.9,Create the object
0.1.9,Define the ratio parameter
0.1.9,Create the sampling object
0.1.9,Get the different subset
0.1.9,Define the ratio parameter
0.1.9,Create the sampling object
0.1.9,Get the different subset
0.1.9,Define the ratio parameter
0.1.9,Create the sampling object
0.1.9,Get the different subset
0.1.9,Create the object
0.1.8,! /usr/bin/env python
0.1.8,"load all vars into globals, otherwise"
0.1.8,the later function call using global vars doesn't work.
0.1.8,"Allow command-lines such as ""python setup.py build install"""
0.1.8,Make sources available using relative paths from this file's directory.
0.1.8,-*- coding: utf-8 -*-
0.1.8,
0.1.8,"imbalanced-learn documentation build configuration file, created by"
0.1.8,sphinx-quickstart on Mon Jan 18 14:44:12 2016.
0.1.8,
0.1.8,This file is execfile()d with the current directory set to its
0.1.8,containing dir.
0.1.8,
0.1.8,Note that not all possible configuration values are present in this
0.1.8,autogenerated file.
0.1.8,
0.1.8,All configuration values have a default; values that are commented out
0.1.8,serve to show the default.
0.1.8,"If extensions (or modules to document with autodoc) are in another directory,"
0.1.8,add these directories to sys.path here. If the directory is relative to the
0.1.8,"documentation root, use os.path.abspath to make it absolute, like shown here."
0.1.8,"sys.path.insert(0, os.path.abspath('.'))"
0.1.8,-- General configuration ---------------------------------------------------
0.1.8,Try to override the matplotlib configuration as early as possible
0.1.8,-- General configuration ------------------------------------------------
0.1.8,"If your documentation needs a minimal Sphinx version, state it here."
0.1.8,needs_sphinx = '1.0'
0.1.8,"Add any Sphinx extension module names here, as strings. They can be"
0.1.8,extensions coming with Sphinx (named 'sphinx.ext.*') or your custom
0.1.8,ones.
0.1.8,path to your examples scripts
0.1.8,path where to save gallery generated examples
0.1.8,"Add any paths that contain templates here, relative to this directory."
0.1.8,generate autosummary even if no references
0.1.8,The suffix of source filenames.
0.1.8,The encoding of source files.
0.1.8,source_encoding = 'utf-8-sig'
0.1.8,Generate the plots for the gallery
0.1.8,The master toctree document.
0.1.8,General information about the project.
0.1.8,"The version info for the project you're documenting, acts as replacement for"
0.1.8,"|version| and |release|, also used in various other places throughout the"
0.1.8,built documents.
0.1.8,
0.1.8,The short X.Y version.
0.1.8,"The full version, including alpha/beta/rc tags."
0.1.8,The language for content autogenerated by Sphinx. Refer to documentation
0.1.8,for a list of supported languages.
0.1.8,language = None
0.1.8,"There are two options for replacing |today|: either, you set today to some"
0.1.8,"non-false value, then it is used:"
0.1.8,today = ''
0.1.8,"Else, today_fmt is used as the format for a strftime call."
0.1.8,"today_fmt = '%B %d, %Y'"
0.1.8,"List of patterns, relative to source directory, that match files and"
0.1.8,directories to ignore when looking for source files.
0.1.8,The reST default role (used for this markup: `text`) to use for all
0.1.8,documents.
0.1.8,default_role = None
0.1.8,"If true, '()' will be appended to :func: etc. cross-reference text."
0.1.8,"If true, the current module name will be prepended to all description"
0.1.8,unit titles (such as .. function::).
0.1.8,add_module_names = True
0.1.8,"If true, sectionauthor and moduleauthor directives will be shown in the"
0.1.8,output. They are ignored by default.
0.1.8,show_authors = False
0.1.8,The name of the Pygments (syntax highlighting) style to use.
0.1.8,A list of ignored prefixes for module index sorting.
0.1.8,modindex_common_prefix = []
0.1.8,"If true, keep warnings as ""system message"" paragraphs in the built documents."
0.1.8,keep_warnings = False
0.1.8,-- Options for HTML output ----------------------------------------------
0.1.8,The theme to use for HTML and HTML Help pages.  See the documentation for
0.1.8,a list of builtin themes.
0.1.8,Theme options are theme-specific and customize the look and feel of a theme
0.1.8,"further.  For a list of options available for each theme, see the"
0.1.8,documentation.
0.1.8,html_theme_options = {}
0.1.8,"Add any paths that contain custom themes here, relative to this directory."
0.1.8,"The name for this set of Sphinx documents.  If None, it defaults to"
0.1.8,"""<project> v<release> documentation""."
0.1.8,html_title = None
0.1.8,A shorter title for the navigation bar.  Default is the same as html_title.
0.1.8,html_short_title = None
0.1.8,The name of an image file (relative to this directory) to place at the top
0.1.8,of the sidebar.
0.1.8,html_logo = None
0.1.8,The name of an image file (within the static path) to use as favicon of the
0.1.8,docs.  This file should be a Windows icon file (.ico) being 16x16 or 32x32
0.1.8,pixels large.
0.1.8,html_favicon = None
0.1.8,"Add any paths that contain custom static files (such as style sheets) here,"
0.1.8,"relative to this directory. They are copied after the builtin static files,"
0.1.8,"so a file named ""default.css"" will overwrite the builtin ""default.css""."
0.1.8,Add any extra paths that contain custom files (such as robots.txt or
0.1.8,".htaccess) here, relative to this directory. These files are copied"
0.1.8,directly to the root of the documentation.
0.1.8,html_extra_path = []
0.1.8,"If not '', a 'Last updated on:' timestamp is inserted at every page bottom,"
0.1.8,using the given strftime format.
0.1.8,"html_last_updated_fmt = '%b %d, %Y'"
0.1.8,"If true, SmartyPants will be used to convert quotes and dashes to"
0.1.8,typographically correct entities.
0.1.8,html_use_smartypants = True
0.1.8,"Custom sidebar templates, maps document names to template names."
0.1.8,html_sidebars = {}
0.1.8,"Additional templates that should be rendered to pages, maps page names to"
0.1.8,template names.
0.1.8,html_additional_pages = {}
0.1.8,"If false, no module index is generated."
0.1.8,html_domain_indices = True
0.1.8,"If false, no index is generated."
0.1.8,html_use_index = True
0.1.8,"If true, the index is split into individual pages for each letter."
0.1.8,html_split_index = False
0.1.8,"If true, links to the reST sources are added to the pages."
0.1.8,html_show_sourcelink = True
0.1.8,"If true, ""Created using Sphinx"" is shown in the HTML footer. Default is True."
0.1.8,html_show_sphinx = True
0.1.8,"If true, ""(C) Copyright ..."" is shown in the HTML footer. Default is True."
0.1.8,html_show_copyright = True
0.1.8,"If true, an OpenSearch description file will be output, and all pages will"
0.1.8,contain a <link> tag referring to it.  The value of this option must be the
0.1.8,base URL from which the finished HTML is served.
0.1.8,html_use_opensearch = ''
0.1.8,"This is the file name suffix for HTML files (e.g. "".xhtml"")."
0.1.8,html_file_suffix = None
0.1.8,Output file base name for HTML help builder.
0.1.8,-- Options for LaTeX output ---------------------------------------------
0.1.8,The paper size ('letterpaper' or 'a4paper').
0.1.8,"'papersize': 'letterpaper',"
0.1.8,"The font size ('10pt', '11pt' or '12pt')."
0.1.8,"'pointsize': '10pt',"
0.1.8,Additional stuff for the LaTeX preamble.
0.1.8,"'preamble': '',"
0.1.8,Grouping the document tree into LaTeX files. List of tuples
0.1.8,"(source start file, target name, title,"
0.1.8,"author, documentclass [howto, manual, or own class])."
0.1.8,The name of an image file (relative to this directory) to place at the top of
0.1.8,the title page.
0.1.8,latex_logo = None
0.1.8,"For ""manual"" documents, if this is true, then toplevel headings are parts,"
0.1.8,not chapters.
0.1.8,latex_use_parts = False
0.1.8,"If true, show page references after internal links."
0.1.8,latex_show_pagerefs = False
0.1.8,"If true, show URL addresses after external links."
0.1.8,latex_show_urls = False
0.1.8,Documents to append as an appendix to all manuals.
0.1.8,latex_appendices = []
0.1.8,"If false, no module index is generated."
0.1.8,latex_domain_indices = True
0.1.8,-- Options for manual page output ---------------------------------------
0.1.8,One entry per manual page. List of tuples
0.1.8,"(source start file, name, description, authors, manual section)."
0.1.8,"If true, show URL addresses after external links."
0.1.8,man_show_urls = False
0.1.8,-- Options for Texinfo output -------------------------------------------
0.1.8,Grouping the document tree into Texinfo files. List of tuples
0.1.8,"(source start file, target name, title, author,"
0.1.8,"dir menu entry, description, category)"
0.1.8,"generate empty examples files, so that we don't get"
0.1.8,inclusion errors if there are no examples for a class / module
0.1.8,touch file
0.1.8,Documents to append as an appendix to all manuals.
0.1.8,texinfo_appendices = []
0.1.8,"If false, no module index is generated."
0.1.8,texinfo_domain_indices = True
0.1.8,"How to display URL addresses: 'footnote', 'no', or 'inline'."
0.1.8,texinfo_show_urls = 'footnote'
0.1.8,"If true, do not generate a @detailmenu in the ""Top"" node's menu."
0.1.8,texinfo_no_detailmenu = False
0.1.8,Example configuration for intersphinx: refer to the Python standard library.
0.1.8,Define some color for the plotting
0.1.8,Generate the dataset
0.1.8,Instanciate a PCA object for the sake of easy visualisation
0.1.8,Fit and transform x to visualise inside a 2D feature space
0.1.8,Apply SMOTE SVM
0.1.8,"Two subplots, unpack the axes array immediately"
0.1.8,Define some color for the plotting
0.1.8,Generate the dataset
0.1.8,Instanciate a PCA object for the sake of easy visualisation
0.1.8,Fit and transform x to visualise inside a 2D feature space
0.1.8,Apply Borderline SMOTE 2
0.1.8,"Two subplots, unpack the axes array immediately"
0.1.8,Define some color for the plotting
0.1.8,Generate the dataset
0.1.8,Instanciate a PCA object for the sake of easy visualisation
0.1.8,Fit and transform x to visualise inside a 2D feature space
0.1.8,Apply regular SMOTE
0.1.8,"Two subplots, unpack the axes array immediately"
0.1.8,Define some color for the plotting
0.1.8,Generate the dataset
0.1.8,Instanciate a PCA object for the sake of easy visualisation
0.1.8,Fit and transform x to visualise inside a 2D feature space
0.1.8,Apply the random over-sampling
0.1.8,"Two subplots, unpack the axes array immediately"
0.1.8,Define some color for the plotting
0.1.8,Generate the dataset
0.1.8,Instanciate a PCA object for the sake of easy visualisation
0.1.8,Fit and transform x to visualise inside a 2D feature space
0.1.8,Apply Borderline SMOTE 1
0.1.8,"Two subplots, unpack the axes array immediately"
0.1.8,Define some color for the plotting
0.1.8,Generate the dataset
0.1.8,Instanciate a PCA object for the sake of easy visualisation
0.1.8,Fit and transform x to visualise inside a 2D feature space
0.1.8,Apply the random over-sampling
0.1.8,"Two subplots, unpack the axes array immediately"
0.1.8,Define some color for the plotting
0.1.8,Generate the dataset
0.1.8,Instanciate a PCA object for the sake of easy visualisation
0.1.8,Fit and transform x to visualise inside a 2D feature space
0.1.8,Apply SMOTE + ENN
0.1.8,"Two subplots, unpack the axes array immediately"
0.1.8,Define some color for the plotting
0.1.8,Generate the dataset
0.1.8,Instanciate a PCA object for the sake of easy visualisation
0.1.8,Fit and transform x to visualise inside a 2D feature space
0.1.8,Apply SMOTE + Tomek links
0.1.8,"Two subplots, unpack the axes array immediately"
0.1.8,Define some color for the plotting
0.1.8,Generate the dataset
0.1.8,Instanciate a PCA object for the sake of easy visualisation
0.1.8,Fit and transform x to visualise inside a 2D feature space
0.1.8,Apply Balance Cascade method
0.1.8,"Two subplots, unpack the axes array immediately"
0.1.8,Define some color for the plotting
0.1.8,Generate the dataset
0.1.8,Instanciate a PCA object for the sake of easy visualisation
0.1.8,Fit and transform x to visualise inside a 2D feature space
0.1.8,Apply Easy Ensemble
0.1.8,"Two subplots, unpack the axes array immediately"
0.1.8,Define some color for the plotting
0.1.8,Generate the dataset
0.1.8,Instanciate a PCA object for the sake of easy visualisation
0.1.8,Fit and transform x to visualise inside a 2D feature space
0.1.8,"Three subplots, unpack the axes array immediately"
0.1.8,Apply the ENN
0.1.8,Apply the RENN
0.1.8,Define some color for the plotting
0.1.8,Generate the dataset
0.1.8,Instanciate a PCA object for the sake of easy visualisation
0.1.8,Fit and transform x to visualise inside a 2D feature space
0.1.8,Apply Nearmiss 3
0.1.8,"Two subplots, unpack the axes array immediately"
0.1.8,Define some color for the plotting
0.1.8,Generate the dataset
0.1.8,Instanciate a PCA object for the sake of easy visualisation
0.1.8,Fit and transform x to visualise inside a 2D feature space
0.1.8,Apply Edited Nearest Neighbours
0.1.8,"Two subplots, unpack the axes array immediately"
0.1.8,Define some color for the plotting
0.1.8,Generate the dataset
0.1.8,Instanciate a PCA object for the sake of easy visualisation
0.1.8,Fit and transform x to visualise inside a 2D feature space
0.1.8,Apply Tomek Links cleaning
0.1.8,"Two subplots, unpack the axes array immediately"
0.1.8,Define some color for the plotting
0.1.8,Generate the dataset
0.1.8,Instanciate a PCA object for the sake of easy visualisation
0.1.8,Fit and transform x to visualise inside a 2D feature space
0.1.8,Apply One-Sided Selection
0.1.8,"Two subplots, unpack the axes array immediately"
0.1.8,Define some color for the plotting
0.1.8,Generate the dataset
0.1.8,Instanciate a PCA object for the sake of easy visualisation
0.1.8,Fit and transform x to visualise inside a 2D feature space
0.1.8,Apply Nearmiss 2
0.1.8,"Two subplots, unpack the axes array immediately"
0.1.8,Define some color for the plotting
0.1.8,Generate the dataset
0.1.8,Instanciate a PCA object for the sake of easy visualisation
0.1.8,Fit and transform x to visualise inside a 2D feature space
0.1.8,Apply neighbourhood cleaning rule
0.1.8,"Two subplots, unpack the axes array immediately"
0.1.8,Define some color for the plotting
0.1.8,Generate the dataset
0.1.8,Instanciate a PCA object for the sake of easy visualisation
0.1.8,Fit and transform x to visualise inside a 2D feature space
0.1.8,Apply Condensed Nearest Neighbours
0.1.8,"Two subplots, unpack the axes array immediately"
0.1.8,Define some color for the plotting
0.1.8,Generate the dataset
0.1.8,Instanciate a PCA object for the sake of easy visualisation
0.1.8,Fit and transform x to visualise inside a 2D feature space
0.1.8,Apply Cluster Centroids
0.1.8,"Two subplots, unpack the axes array immediately"
0.1.8,Define some color for the plotting
0.1.8,Generate the dataset
0.1.8,Instanciate a PCA object for the sake of easy visualisation
0.1.8,Fit and transform x to visualise inside a 2D feature space
0.1.8,Apply the random under-sampling
0.1.8,"Two subplots, unpack the axes array immediately"
0.1.8,Define some color for the plotting
0.1.8,Generate the dataset
0.1.8,"Two subplots, unpack the axes array immediately"
0.1.8,Define some color for the plotting
0.1.8,Generate the dataset
0.1.8,Instanciate a PCA object for the sake of easy visualisation
0.1.8,Fit and transform x to visualise inside a 2D feature space
0.1.8,Apply Nearmiss 1
0.1.8,"Two subplots, unpack the axes array immediately"
0.1.8,Generate the dataset
0.1.8,Instanciate a PCA object for the sake of easy visualisation
0.1.8,Create the samplers
0.1.8,Create teh classifier
0.1.8,Make the splits
0.1.8,Add one transformers and two samplers in the pipeline object
0.1.8,Based on NiLearn package
0.1.8,License: simplified BSD
0.1.8,"PEP0440 compatible formatted version, see:"
0.1.8,https://www.python.org/dev/peps/pep-0440/
0.1.8,
0.1.8,Generic release markers:
0.1.8,X.Y
0.1.8,X.Y.Z # For bugfix releases
0.1.8,
0.1.8,Admissible pre-release markers:
0.1.8,X.YaN # Alpha release
0.1.8,X.YbN # Beta release
0.1.8,X.YrcN # Release Candidate
0.1.8,X.Y # Final release
0.1.8,
0.1.8,Dev branch marker is: 'X.Y.dev' or 'X.Y.devN' where N is an integer.
0.1.8,'X.Y.dev0' is the canonical version of 'X.Y.dev'
0.1.8,
0.1.8,"This is a tuple to preserve order, so that dependencies are checked"
0.1.8,in some meaningful order (more => less 'core').  We avoid using
0.1.8,collections.OrderedDict to preserve Python 2.6 compatibility.
0.1.8,Avoid choking on modules with no __version__ attribute
0.1.8,Skip check only when installing and it's a module that
0.1.8,will be auto-installed.
0.1.8,Check the consistency of X and y
0.1.8,Get all the unique elements in the target array
0.1.8,# Raise an error if there is only one class
0.1.8,if uniques.size == 1:
0.1.8,"raise RuntimeError(""Only one class detected, aborting..."")"
0.1.8,Raise a warning for the moment to be compatible with BaseEstimator
0.1.8,Store the size of X to check at sampling time if we have the
0.1.8,same data
0.1.8,Create a dictionary containing the class statistics
0.1.8,Find the minority and majority classes
0.1.8,Check if the ratio provided at initialisation make sense
0.1.8,Check the consistency of X and y
0.1.8,Check that the data have been fitted
0.1.8,Check if the size of the data is identical than at fitting
0.1.8,The ratio correspond to the number of samples in the minority class
0.1.8,"over the number of samples in the majority class. Thus, the ratio"
0.1.8,cannot be greater than 1.0
0.1.8,Adapted from scikit-learn
0.1.8,Author: Edouard Duchesnay
0.1.8,Gael Varoquaux
0.1.8,Virgile Fritsch
0.1.8,Alexandre Gramfort
0.1.8,Lars Buitinck
0.1.8,chkoar
0.1.8,License: BSD
0.1.8,BaseEstimator interface
0.1.8,shallow copy of steps
0.1.8,Estimator interface
0.1.8,Boolean controlling whether the joblib caches should be
0.1.8,"flushed if the version of certain modules changes (eg nibabel, as it"
0.1.8,does not respect the backward compatibility in some of its internal
0.1.8,structures
0.1.8,This  is used in nilearn._utils.cache_mixin
0.1.8,list all submodules available in imblearn and version
0.1.8,Keep the samples from the majority class
0.1.8,Loop over the other classes over picking at random
0.1.8,"If this is the majority class, skip it"
0.1.8,Define the number of sample to create
0.1.8,Pick some elements at random
0.1.8,Concatenate to the majority class
0.1.8,Keep the samples from the majority class
0.1.8,Define the number of sample to create
0.1.8,We handle only two classes problem for the moment.
0.1.8,Start by separating minority class features and target values.
0.1.8,Print if verbose is true
0.1.8,"Look for k-th nearest neighbours, excluding, of course, the"
0.1.8,point itself.
0.1.8,Get the distance to the NN
0.1.8,Compute the ratio of majority samples next to minority samples
0.1.8,Check that we found at least some neighbours belonging to the
0.1.8,majority class
0.1.8,Normalize the ratio
0.1.8,Compute the number of sample to be generated
0.1.8,For each minority samples
0.1.8,Pick-up the neighbors wanted
0.1.8,Create a new sample
0.1.8,Find the NN for each samples
0.1.8,Exclude the sample itself
0.1.8,Count how many NN belong to the minority class
0.1.8,Find the class corresponding to the label in x
0.1.8,Compute the number of majority samples in the NN
0.1.8,Samples are in danger for m/2 <= m' < m
0.1.8,Samples are noise for m = m'
0.1.8,Check the consistency of X
0.1.8,Check the random state
0.1.8,A matrix to store the synthetic samples
0.1.8,# Set seeds
0.1.8,"seeds = random_state.randint(low=0,"
0.1.8,"high=100 * len(nn_num.flatten()),"
0.1.8,size=n_samples)
0.1.8,Randomly pick samples to construct neighbours from
0.1.8,Loop over the NN matrix and create new samples
0.1.8,"NN lines relate to original sample, columns to its"
0.1.8,nearest neighbours
0.1.8,"Take a step of random size (0,1) in the direction of the"
0.1.8,n nearest neighbours
0.1.8,if self.random_state is None:
0.1.8,np.random.seed(seeds[i])
0.1.8,else:
0.1.8,np.random.seed(self.random_state)
0.1.8,Construct synthetic sample
0.1.8,The returned target vector is simply a repetition of the
0.1.8,minority label
0.1.8,Define the number of sample to create
0.1.8,We handle only two classes problem for the moment.
0.1.8,Start by separating minority class features and target values.
0.1.8,If regular SMOTE is to be performed
0.1.8,"Look for k-th nearest neighbours, excluding, of course, the"
0.1.8,point itself.
0.1.8,Matrix with k-th nearest neighbours indexes for each minority
0.1.8,element.
0.1.8,--- Generating synthetic samples
0.1.8,Use static method make_samples to generate minority samples
0.1.8,Concatenate the newly generated samples to the original data set
0.1.8,Find the NNs for all samples in the data set.
0.1.8,Boolean array with True for minority samples in danger
0.1.8,"If all minority samples are safe, return the original data set."
0.1.8,"All are safe, nothing to be done here."
0.1.8,"If we got here is because some samples are in danger, we need to"
0.1.8,find the NNs among the minority class to create the new synthetic
0.1.8,samples.
0.1.8,
0.1.8,We start by changing the number of NNs to consider from m + 1
0.1.8,to k + 1
0.1.8,nns...#
0.1.8,B1 and B2 types diverge here!!!
0.1.8,Create synthetic samples for borderline points.
0.1.8,Concatenate the newly generated samples to the original
0.1.8,dataset
0.1.8,Reset the k-neighbours to m+1 neighbours
0.1.8,Split the number of synthetic samples between only minority
0.1.8,"(type 1), or minority and majority (with reduced step size)"
0.1.8,(type 2).
0.1.8,The fraction is sampled from a beta distribution centered
0.1.8,around 0.5 with variance ~0.01
0.1.8,Only minority
0.1.8,Only majority with smaller step size
0.1.8,Concatenate the newly generated samples to the original
0.1.8,data set
0.1.8,Reset the k-neighbours to m+1 neighbours
0.1.8,The SVM smote model fits a support vector machine
0.1.8,classifier to the data and uses the support vector to
0.1.8,"provide a notion of boundary. Unlike regular smote, where"
0.1.8,such notion relies on proportion of nearest neighbours
0.1.8,belonging to each class.
0.1.8,Fit SVM to the full data#
0.1.8,Find the support vectors and their corresponding indexes
0.1.8,"First, find the nn of all the samples to identify samples"
0.1.8,in danger and noisy ones
0.1.8,"As usual, fit a nearest neighbour model to the data"
0.1.8,"Now, get rid of noisy support vectors"
0.1.8,Remove noisy support vectors
0.1.8,Proceed to find support vectors NNs among the minority class
0.1.8,Split the number of synthetic samples between interpolation and
0.1.8,extrapolation
0.1.8,The fraction are sampled from a beta distribution with mean
0.1.8,0.5 and variance 0.01#
0.1.8,Interpolate samples in danger
0.1.8,Extrapolate safe samples
0.1.8,Concatenate the newly generated samples to the original data set
0.1.8,not any support vectors in danger
0.1.8,All the support vector in danger
0.1.8,Reset the k-neighbours to m+1 neighbours
0.1.8,--- NN object
0.1.8,Import the NN object from scikit-learn library. Since in the smote
0.1.8,"variations we must first find samples that are in danger, we"
0.1.8,initialize the NN object differently depending on the method chosen
0.1.8,"Regular smote does not look for samples in danger, instead it"
0.1.8,creates synthetic samples directly from the k-th nearest
0.1.8,neighbours with not filtering
0.1.8,"Borderline1, 2 and SVM variations of smote must first look for"
0.1.8,samples that could be considered noise and samples that live
0.1.8,"near the boundary between the classes. Therefore, before"
0.1.8,"creating synthetic samples from the k-th nns, it first look"
0.1.8,for m nearest neighbors to decide whether or not a sample is
0.1.8,noise or near the boundary.
0.1.8,--- SVM smote
0.1.8,"Unlike the borderline variations, the SVM variation uses the support"
0.1.8,vectors to decide which samples are in danger (near the boundary).
0.1.8,Additionally it also introduces extrapolation for samples that are
0.1.8,considered safe (far from boundary) and interpolation for samples
0.1.8,in danger (near the boundary). The level of extrapolation is
0.1.8,controled by the out_step.
0.1.8,Store SVM object with any parameters
0.1.8,Generate a global dataset to use
0.1.8,Define a negative ratio
0.1.8,Define a ratio greater than 1
0.1.8,Define ratio as an unknown string
0.1.8,Define ratio as a list which is not supported
0.1.8,Define a ratio
0.1.8,Create the object
0.1.8,Resample the data
0.1.8,Create a wrong y
0.1.8,Create the object
0.1.8,Fit the data
0.1.8,Create the object
0.1.8,Fit the data
0.1.8,Check if the data information have been computed
0.1.8,Create the object
0.1.8,Resample the data
0.1.8,Resample the data
0.1.8,Create the object
0.1.8,Generate a global dataset to use
0.1.8,Define a negative ratio
0.1.8,Define a ratio greater than 1
0.1.8,Define ratio as an unknown string
0.1.8,Define ratio as a list which is not supported
0.1.8,Define a ratio
0.1.8,Create the object
0.1.8,Resample the data
0.1.8,Create a wrong y
0.1.8,Create the object
0.1.8,Fit the data
0.1.8,Create the object
0.1.8,Fit the data
0.1.8,Check if the data information have been computed
0.1.8,Create the object
0.1.8,Resample the data
0.1.8,Resample the data
0.1.8,Create the object
0.1.8,Generate a global dataset to use
0.1.8,Define a negative ratio
0.1.8,Define a ratio greater than 1
0.1.8,Define ratio as an unknown string
0.1.8,Define ratio as a list which is not supported
0.1.8,Create the object
0.1.8,Resample the data
0.1.8,Create a wrong y
0.1.8,Create the object
0.1.8,Fit the data
0.1.8,Check if the data information have been computed
0.1.8,Create the object
0.1.8,Create the object
0.1.8,Fit the data
0.1.8,Create the object
0.1.8,Fit the data
0.1.8,Create the object
0.1.8,Fit the data
0.1.8,Create the object
0.1.8,Fit the data
0.1.8,Create the object
0.1.8,Fit the data
0.1.8,Create the object
0.1.8,Start with the minority class
0.1.8,All the minority class samples will be preserved
0.1.8,If we need to offer support for the indices
0.1.8,Loop over the other classes under picking at random
0.1.8,"If the minority class is up, skip it"
0.1.8,Randomly get one sample from the majority class
0.1.8,Generate the index to select
0.1.8,Create the set C
0.1.8,Create the set S
0.1.8,Remove the seed from S since that it will be added anyway
0.1.8,Create a k-NN classifier
0.1.8,Fit C into the knn
0.1.8,Classify on S
0.1.8,Find the misclassified S_y
0.1.8,If we need to offer support for the indices selected
0.1.8,We concatenate the misclassified samples with the seed and the
0.1.8,minority samples
0.1.8,Find the nearest neighbour of every point
0.1.8,Send the information to is_tomek function to get boolean vector back
0.1.8,Check if the indices of the samples selected should be returned too
0.1.8,Return the indices of interest
0.1.8,Return data set without majority Tomek links.
0.1.8,Compute the distance considering the farthest neighbour
0.1.8,Sort the list of distance and get the index
0.1.8,Throw a warning to tell the user that we did not have enough samples
0.1.8,to select and that we just select everything
0.1.8,Select the desired number of samples
0.1.8,Assign the parameter of the element of this class
0.1.8,Check that the version asked is implemented
0.1.8,Start with the minority class
0.1.8,All the minority class samples will be preserved
0.1.8,Compute the number of cluster needed
0.1.8,If we need to offer support for the indices
0.1.8,"For each element of the current class, find the set of NN"
0.1.8,of the minority class
0.1.8,Call the constructor of the NN
0.1.8,Fit the minority class since that we want to know the distance
0.1.8,to these point
0.1.8,Loop over the other classes under picking at random
0.1.8,"If the minority class is up, skip it"
0.1.8,Get the samples corresponding to the current class
0.1.8,Find the NN
0.1.8,Select the right samples
0.1.8,Find the NN
0.1.8,Select the right samples
0.1.8,We need a new NN object to fit the current class
0.1.8,Find the set of NN to the minority class
0.1.8,Create the subset containing the samples found during the NN
0.1.8,search. Linearize the indexes and remove the double values
0.1.8,Create the subset
0.1.8,Compute the NN considering the current class
0.1.8,If we need to offer support for the indices selected
0.1.8,Check if the indices of the samples selected should be returned too
0.1.8,Return the indices of interest
0.1.8,Compute the number of clusters needed
0.1.8,All the minority class samples will be preserved
0.1.8,If we need to offer support for the indices
0.1.8,Loop over the other classes under-picking at random
0.1.8,"If the minority class is up, skip it"
0.1.8,Pick some elements at random
0.1.8,If we need to offer support for the indices selected
0.1.8,Concatenate to the minority class
0.1.8,Check if the indices of the samples selected should be returned as
0.1.8,well
0.1.8,Return the indices of interest
0.1.8,"Initialize the boolean result as false, and also a counter"
0.1.8,Loop through each sample and looks whether it belongs to the minority
0.1.8,"class. If it does, we don't consider it since we want to keep all"
0.1.8,"minority samples. If, however, it belongs to the majority sample we"
0.1.8,look at its first neighbour. If its closest neighbour also has the
0.1.8,"current sample as its closest neighbour, the two form a Tomek link."
0.1.8,"If they form a tomek link, put a True marker on this"
0.1.8,"sample, and increase counter by one."
0.1.8,Find the nearest neighbour of every point
0.1.8,Send the information to is_tomek function to get boolean vector back
0.1.8,Check if the indices of the samples selected should be returned too
0.1.8,Return the indices of interest
0.1.8,Return data set without majority Tomek links.
0.1.8,Start with the minority class
0.1.8,All the minority class samples will be preserved
0.1.8,If we need to offer support for the indices
0.1.8,Loop over the other classes under picking at random
0.1.8,"If the minority class is up, skip it"
0.1.8,Randomly get one sample from the majority class
0.1.8,Generate the index to select
0.1.8,Create the set C - One majority samples and all minority
0.1.8,Create the set S - all majority samples
0.1.8,Create a k-NN classifier
0.1.8,Fit C into the knn
0.1.8,Check each sample in S if we keep it or drop it
0.1.8,Do not select sample which are already well classified
0.1.8,Classify on S
0.1.8,If the prediction do not agree with the true label
0.1.8,append it in C_x
0.1.8,Keep the index for later
0.1.8,Update C
0.1.8,Fit C into the knn
0.1.8,This experimental to speed up the search
0.1.8,Classify all the element in S and avoid to test the
0.1.8,well classified elements
0.1.8,Find the misclassified S_y
0.1.8,"The indexes found are relative to the current class, we need to"
0.1.8,find the absolute value
0.1.8,Build the array with the absolute position
0.1.8,If we need to offer support for the indices selected
0.1.8,Check if the indices of the samples selected should be returned too
0.1.8,Return the indices of interest
0.1.8,Compute the number of cluster needed
0.1.8,Create the clustering object
0.1.8,Start with the minority class
0.1.8,All the minority class samples will be preserved
0.1.8,Loop over the other classes under picking at random
0.1.8,"If the minority class is up, skip it."
0.1.8,Find the centroids via k-means
0.1.8,Concatenate to the minority class
0.1.8,Start with the minority class
0.1.8,All the minority class samples will be preserved
0.1.8,If we need to offer support for the indices
0.1.8,Create a k-NN to fit the whole data
0.1.8,Fit the whole dataset
0.1.8,Loop over the other classes under picking at random
0.1.8,Get the sample of the current class
0.1.8,Get the samples associated
0.1.8,Find the NN for the current class
0.1.8,Get the label of the corresponding to the index
0.1.8,Check which one are the same label than the current class
0.1.8,Make an AND operation through the three neighbours
0.1.8,If the minority class remove the majority samples
0.1.8,Get the index to exclude
0.1.8,Get the index to exclude
0.1.8,Create a vector with the sample to select
0.1.8,Exclude as well the minority sample since that they will be
0.1.8,concatenated later
0.1.8,Get the samples from the majority classes
0.1.8,If we need to offer support for the indices selected
0.1.8,Check if the indices of the samples selected should be returned too
0.1.8,Return the indices of interest
0.1.8,Start with the minority class
0.1.8,All the minority class samples will be preserved
0.1.8,If we need to offer support for the indices
0.1.8,Create a k-NN to fit the whole data
0.1.8,Fit the data
0.1.8,Loop over the other classes under picking at random
0.1.8,"If the minority class is up, skip it"
0.1.8,Get the sample of the current class
0.1.8,Find the NN for the current class
0.1.8,Get the label of the corresponding to the index
0.1.8,Check which one are the same label than the current class
0.1.8,Make the majority vote
0.1.8,Get the samples which agree all together
0.1.8,If we need to offer support for the indices selected
0.1.8,Check if the indices of the samples selected should be returned too
0.1.8,Return the indices of interest
0.1.8,Check the stopping criterion
0.1.8,1. If there is no changes for the vector y
0.1.8,2. If the number of samples in the other class become inferior to
0.1.8,the number of samples in the majority class
0.1.8,3. If one of the class is disappearing
0.1.8,Case 1
0.1.8,Case 2
0.1.8,Get the number of samples in the non-minority classes
0.1.8,Check the minority stop to be the minority
0.1.8,Case 3
0.1.8,"If this is a normal convergence, get the last data"
0.1.8,Log the variables to explain the stop of the algorithm
0.1.8,Update the data for the next iteration
0.1.8,Check if the indices of the samples selected should be returned too
0.1.8,Return the indices of interest
0.1.8,Select the appropriate classifier
0.1.8,Create the different folds
0.1.8,Compute the number of cluster needed
0.1.8,Find the percentile corresponding to the top num_samples
0.1.8,Sample the data
0.1.8,If we need to offer support for the indices
0.1.8,Generate a global dataset to use
0.1.8,Define a negative ratio
0.1.8,Define a ratio greater than 1
0.1.8,Define ratio as an unknown string
0.1.8,Define ratio as a list which is not supported
0.1.8,Define a ratio
0.1.8,Define the parameter for the under-sampling
0.1.8,Create the object
0.1.8,Resample the data
0.1.8,Create a wrong y
0.1.8,Create the object
0.1.8,Fit the data
0.1.8,Define the parameter for the under-sampling
0.1.8,Create the object
0.1.8,Fit the data
0.1.8,Check if the data information have been computed
0.1.8,Define the parameter for the under-sampling
0.1.8,Create the object
0.1.8,Define the parameter for the under-sampling
0.1.8,Create the object
0.1.8,Fit and sample
0.1.8,Define the parameter for the under-sampling
0.1.8,Create the object
0.1.8,Fit and sample
0.1.8,Define the parameter for the under-sampling
0.1.8,Create the object
0.1.8,Fit and sample
0.1.8,Create the object
0.1.8,Generate a global dataset to use
0.1.8,Define a ratio
0.1.8,Create the object
0.1.8,Resample the data
0.1.8,Create a wrong y
0.1.8,Create the object
0.1.8,Fit the data
0.1.8,Check if the data information have been computed
0.1.8,Create the object
0.1.8,Resample the data
0.1.8,Resample the data
0.1.8,Create the object
0.1.8,Generate a global dataset to use
0.1.8,Define a ratio
0.1.8,Create the object
0.1.8,Resample the data
0.1.8,Create a wrong y
0.1.8,Create the object
0.1.8,Fit the data
0.1.8,Check if the data information have been computed
0.1.8,Create the object
0.1.8,Resample the data
0.1.8,Resample the data
0.1.8,Create the object
0.1.8,Generate a global dataset to use
0.1.8,Define a ratio
0.1.8,Create the object
0.1.8,Resample the data
0.1.8,Create a wrong y
0.1.8,Create the object
0.1.8,Fit the data
0.1.8,Check if the data information have been computed
0.1.8,Create the object
0.1.8,Resample the data
0.1.8,Resample the data
0.1.8,Resample the data
0.1.8,Create the object
0.1.8,Generate a global dataset to use
0.1.8,Define a negative ratio
0.1.8,Define a ratio greater than 1
0.1.8,Define ratio as an unknown string
0.1.8,Define ratio as a list which is not supported
0.1.8,Define a ratio
0.1.8,Create the object
0.1.8,Resample the data
0.1.8,Create a wrong y
0.1.8,Create the object
0.1.8,Fit the data
0.1.8,Create the object
0.1.8,Fit the data
0.1.8,Check if the data information have been computed
0.1.8,Create the object
0.1.8,Resample the data
0.1.8,Resample the data
0.1.8,Resample the data
0.1.8,Create the object
0.1.8,Generate a global dataset to use
0.1.8,Define a negative ratio
0.1.8,Define a ratio greater than 1
0.1.8,Define ratio as an unknown string
0.1.8,Define ratio as a list which is not supported
0.1.8,Resample the data
0.1.8,Define a ratio
0.1.8,Create the object
0.1.8,Resample the data
0.1.8,Create a wrong y
0.1.8,Create the object
0.1.8,Fit the data
0.1.8,Create the object
0.1.8,Fit the data
0.1.8,Check if the data information have been computed
0.1.8,Create the object
0.1.8,Resample the data
0.1.8,Resample the data
0.1.8,Resample the data
0.1.8,Resample the data
0.1.8,Resample the data
0.1.8,Resample the data
0.1.8,Resample the data
0.1.8,Resample the data
0.1.8,Resample the data
0.1.8,Create the object
0.1.8,Generate a global dataset to use
0.1.8,Define a ratio
0.1.8,Create the object
0.1.8,Create the object
0.1.8,Resample the data
0.1.8,Create a wrong y
0.1.8,Create the object
0.1.8,Fit the data
0.1.8,Check if the data information have been computed
0.1.8,Create the object
0.1.8,Resample the data
0.1.8,Resample the data
0.1.8,Resample the data
0.1.8,Create the object
0.1.8,Generate a global dataset to use
0.1.8,Define a negative ratio
0.1.8,Define a ratio greater than 1
0.1.8,Define ratio as an unknown string
0.1.8,Define ratio as a list which is not supported
0.1.8,Define a ratio
0.1.8,Define the parameter for the under-sampling
0.1.8,Create the object
0.1.8,Resample the data
0.1.8,Create a wrong y
0.1.8,Create the object
0.1.8,Fit the data
0.1.8,Define the parameter for the under-sampling
0.1.8,Create the object
0.1.8,Fit the data
0.1.8,Check if the data information have been computed
0.1.8,Define the parameter for the under-sampling
0.1.8,Create the object
0.1.8,Define the parameter for the under-sampling
0.1.8,Create the object
0.1.8,Fit and sample
0.1.8,Define the parameter for the under-sampling
0.1.8,Create the object
0.1.8,Fit and sample
0.1.8,Define the parameter for the under-sampling
0.1.8,Create the object
0.1.8,Fit and sample
0.1.8,Create the object
0.1.8,Generate a global dataset to use
0.1.8,Define a ratio
0.1.8,Create the object
0.1.8,Resample the data
0.1.8,Create a wrong y
0.1.8,Create the object
0.1.8,Fit the data
0.1.8,Check if the data information have been computed
0.1.8,Create the object
0.1.8,Resample the data
0.1.8,Resample the data
0.1.8,Create the object
0.1.8,Generate a global dataset to use
0.1.8,Define a negative ratio
0.1.8,Define a ratio greater than 1
0.1.8,Define ratio as an unknown string
0.1.8,Define ratio as a list which is not supported
0.1.8,Define a ratio
0.1.8,Define the parameter for the under-sampling
0.1.8,Create the object
0.1.8,Resample the data
0.1.8,Create a wrong y
0.1.8,Create the object
0.1.8,Fit the data
0.1.8,Define the parameter for the under-sampling
0.1.8,Create the object
0.1.8,Fit the data
0.1.8,Check if the data information have been computed
0.1.8,Define the parameter for the under-sampling
0.1.8,Create the object
0.1.8,Define the parameter for the under-sampling
0.1.8,Create the object
0.1.8,Define the parameter for the under-sampling
0.1.8,Create the object
0.1.8,Fit and sample
0.1.8,Define the parameter for the under-sampling
0.1.8,Create the object
0.1.8,Fit and sample
0.1.8,Generate a global dataset to use
0.1.8,Define a negative ratio
0.1.8,Define a ratio greater than 1
0.1.8,Define ratio as an unknown string
0.1.8,Define ratio as a list which is not supported
0.1.8,Define a ratio
0.1.8,Define the parameter for the under-sampling
0.1.8,Create the object
0.1.8,Resample the data
0.1.8,Create a wrong y
0.1.8,Create the object
0.1.8,Fit the data
0.1.8,Define the parameter for the under-sampling
0.1.8,Create the object
0.1.8,Fit the data
0.1.8,Check if the data information have been computed
0.1.8,Define the parameter for the under-sampling
0.1.8,Create the object
0.1.8,Define the parameter for the under-sampling
0.1.8,Create the object
0.1.8,Fit and sample
0.1.8,Define the parameter for the under-sampling
0.1.8,Create the object
0.1.8,Fit and sample
0.1.8,Define the parameter for the under-sampling
0.1.8,Create the object
0.1.8,Fit and sample
0.1.8,Create the object
0.1.8,Generate a global dataset to use
0.1.8,Define a ratio
0.1.8,Create the object
0.1.8,Resample the data
0.1.8,Create a wrong y
0.1.8,Create the object
0.1.8,Fit the data
0.1.8,Check if the data information have been computed
0.1.8,Create the object
0.1.8,Resample the data
0.1.8,Resample the data
0.1.8,Create the object
0.1.8,Test the various init parameters of the pipeline.
0.1.8,Check that we can't instantiate pipelines with objects without fit
0.1.8,method
0.1.8,Smoke test with only an estimator
0.1.8,Check that params are set
0.1.8,Smoke test the repr:
0.1.8,Test with two objects
0.1.8,Check that we can't use the same stage name twice
0.1.8,Check that params are set
0.1.8,Smoke test the repr:
0.1.8,Check that params are not set when naming them wrong
0.1.8,Test clone
0.1.8,"Check that apart from estimators, the parameters are the same"
0.1.8,Remove estimators that where copied
0.1.8,Test the various methods of the pipeline (anova).
0.1.8,Test with Anova + LogisticRegression
0.1.8,Test that the pipeline can take fit parameters
0.1.8,classifier should return True
0.1.8,and transformer params should not be changed
0.1.8,Test pipeline raises set params error message for nested models.
0.1.8,expected error message
0.1.8,nested model check
0.1.8,Test the various methods of the pipeline (pca + svm).
0.1.8,Test with PCA + SVC
0.1.8,Test the various methods of the pipeline (preprocessing + svm).
0.1.8,check shapes of various prediction functions
0.1.8,test that the fit_predict method is implemented on a pipeline
0.1.8,test that the fit_predict on pipeline yields same results as applying
0.1.8,transform and clustering steps separately
0.1.8,first compute the transform and clustering step separately
0.1.8,use a pipeline to do the transform and clustering in one step
0.1.8,tests that a pipeline does not have fit_predict method when final
0.1.8,step of pipeline does not have fit_predict defined
0.1.8,Test whether pipeline works with a transformer at the end.
0.1.8,Also test pipeline.transform and pipeline.inverse_transform
0.1.8,test transform and fit_transform:
0.1.8,Test whether pipeline works with a transformer missing fit_transform
0.1.8,test fit_transform:
0.1.8,Test the various methods of the pipeline (pca + svm).
0.1.8,Test with PCA + SVC
0.1.8,Test the various methods of the pipeline (pca + svm).
0.1.8,Test with PCA + SVC
0.1.8,Test whether pipeline works with a sampler at the end.
0.1.8,Also test pipeline.sampler
0.1.8,test transform and fit_transform:
0.1.8,Test whether pipeline works with a sampler at the end.
0.1.8,Also test pipeline.sampler
0.1.8,Test the various methods of the pipeline (anova).
0.1.8,Test with RandomUnderSampling + Anova + LogisticRegression
0.1.8,Fit using SMOTE
0.1.8,Transform using SMOTE
0.1.8,Fit and transform using ENN
0.1.8,Fit using SMOTE
0.1.8,Transform using SMOTE
0.1.8,Fit and transform using ENN
0.1.8,Generate a global dataset to use
0.1.8,Define a negative ratio
0.1.8,Define a ratio greater than 1
0.1.8,Define ratio as an unknown string
0.1.8,Define ratio as a list which is not supported
0.1.8,Create the object
0.1.8,Resample the data
0.1.8,Create a wrong y
0.1.8,Create the object
0.1.8,Fit the data
0.1.8,Check if the data information have been computed
0.1.8,Create the object
0.1.8,Create the object
0.1.8,Fit the data
0.1.8,Create the object
0.1.8,Fit the data
0.1.8,Create the object
0.1.8,Generate a global dataset to use
0.1.8,Define a negative ratio
0.1.8,Define a ratio greater than 1
0.1.8,Define ratio as an unknown string
0.1.8,Define ratio as a list which is not supported
0.1.8,Create the object
0.1.8,Resample the data
0.1.8,Create a wrong y
0.1.8,Create the object
0.1.8,Fit the data
0.1.8,Check if the data information have been computed
0.1.8,Create the object
0.1.8,Create the object
0.1.8,Fit the data
0.1.8,Create the object
0.1.8,Fit the data
0.1.8,Create the object
0.1.8,Check the random state
0.1.8,Define the classifier to use
0.1.8,Start with the minority class
0.1.8,Keep the indices of the minority class somewhere if we need to
0.1.8,return them later
0.1.8,Condition to initiliase before the search
0.1.8,Get the initial number of samples to select in the majority class
0.1.8,Create the array characterising the array containing the majority
0.1.8,class
0.1.8,Loop to create the different subsets
0.1.8,Generate an appropriate number of index to extract
0.1.8,from the majority class depending of the false classification
0.1.8,rate of the previous iteration
0.1.8,Mark these indexes as not being considered for next sampling
0.1.8,"For now, we will train and classify on the same data"
0.1.8,"Let see if we should find another solution. Anyway,"
0.1.8,random stuff are still random stuff
0.1.8,Push these data into a new subset
0.1.8,Apply a bootstrap on x_data
0.1.8,Train the classifier using the current data
0.1.8,Train the classifier using the current data
0.1.8,Predict using only the majority class
0.1.8,Basically let's find which sample have to be retained for the
0.1.8,next round
0.1.8,Find the misclassified index to keep them for the next round
0.1.8,Count how many random element will be selected
0.1.8,"We found a new subset, increase the counter"
0.1.8,Check if we have to make an early stopping
0.1.8,Select the remaining data
0.1.8,Select the final batch
0.1.8,Push these data into a new subset
0.1.8,"We found a new subset, increase the counter"
0.1.8,Specific case with n_max_subset = 1
0.1.8,Also check that we will have enough sample to extract at the
0.1.8,next round
0.1.8,Select the remaining data
0.1.8,Select the final batch
0.1.8,Push these data into a new subset
0.1.8,"We found a new subset, increase the counter"
0.1.8,Generate a global dataset to use
0.1.8,Define a negative ratio
0.1.8,Define a ratio greater than 1
0.1.8,Define ratio as an unknown string
0.1.8,Define ratio as a list which is not supported
0.1.8,Define a ratio
0.1.8,Define the parameter for the under-sampling
0.1.8,Create the object
0.1.8,Resample the data
0.1.8,Create a wrong y
0.1.8,Create the object
0.1.8,Fit the data
0.1.8,Define the parameter for the under-sampling
0.1.8,Create the object
0.1.8,Fit the data
0.1.8,Check if the data information have been computed
0.1.8,Define the parameter for the under-sampling
0.1.8,Create the object
0.1.8,Define the ratio parameter
0.1.8,Create the sampling object
0.1.8,Get the different subset
0.1.8,Check each array
0.1.8,Define the ratio parameter
0.1.8,Create the sampling object
0.1.8,Get the different subset
0.1.8,Check each array
0.1.8,Define the ratio parameter
0.1.8,Create the sampling object
0.1.8,Get the different subset
0.1.8,Check each array
0.1.8,Define the ratio parameter
0.1.8,Create the sampling object
0.1.8,Get the different subset
0.1.8,Check each array
0.1.8,Define the ratio parameter
0.1.8,Create the sampling object
0.1.8,Get the different subset
0.1.8,Check each array
0.1.8,Define the ratio parameter
0.1.8,Create the sampling object
0.1.8,Get the different subset
0.1.8,Check each array
0.1.8,Define the ratio parameter
0.1.8,Create the sampling object
0.1.8,Get the different subset
0.1.8,Check each array
0.1.8,Define the ratio parameter
0.1.8,Define the ratio parameter
0.1.8,Create the sampling object
0.1.8,Get the different subset
0.1.8,Check each array
0.1.8,Create the object
0.1.8,Generate a global dataset to use
0.1.8,Define a negative ratio
0.1.8,Define a ratio greater than 1
0.1.8,Define ratio as an unknown string
0.1.8,Define ratio as a list which is not supported
0.1.8,Define a ratio
0.1.8,Define the parameter for the under-sampling
0.1.8,Create the object
0.1.8,Resample the data
0.1.8,Create a wrong y
0.1.8,Create the object
0.1.8,Fit the data
0.1.8,Define the parameter for the under-sampling
0.1.8,Create the object
0.1.8,Fit the data
0.1.8,Check if the data information have been computed
0.1.8,Define the parameter for the under-sampling
0.1.8,Create the object
0.1.8,Define the ratio parameter
0.1.8,Create the sampling object
0.1.8,Get the different subset
0.1.8,Define the ratio parameter
0.1.8,Create the sampling object
0.1.8,Get the different subset
0.1.8,Define the ratio parameter
0.1.8,Create the sampling object
0.1.8,Get the different subset
0.1.8,Create the object
0.2.0.dev0,! /usr/bin/env python
0.2.0.dev0,"load all vars into globals, otherwise"
0.2.0.dev0,the later function call using global vars doesn't work.
0.2.0.dev0,"Allow command-lines such as ""python setup.py build install"""
0.2.0.dev0,Make sources available using relative paths from this file's directory.
0.2.0.dev0,-*- coding: utf-8 -*-
0.2.0.dev0,
0.2.0.dev0,"imbalanced-learn documentation build configuration file, created by"
0.2.0.dev0,sphinx-quickstart on Mon Jan 18 14:44:12 2016.
0.2.0.dev0,
0.2.0.dev0,This file is execfile()d with the current directory set to its
0.2.0.dev0,containing dir.
0.2.0.dev0,
0.2.0.dev0,Note that not all possible configuration values are present in this
0.2.0.dev0,autogenerated file.
0.2.0.dev0,
0.2.0.dev0,All configuration values have a default; values that are commented out
0.2.0.dev0,serve to show the default.
0.2.0.dev0,"If extensions (or modules to document with autodoc) are in another directory,"
0.2.0.dev0,add these directories to sys.path here. If the directory is relative to the
0.2.0.dev0,"documentation root, use os.path.abspath to make it absolute, like shown here."
0.2.0.dev0,"sys.path.insert(0, os.path.abspath('.'))"
0.2.0.dev0,-- General configuration ---------------------------------------------------
0.2.0.dev0,Try to override the matplotlib configuration as early as possible
0.2.0.dev0,-- General configuration ------------------------------------------------
0.2.0.dev0,"If your documentation needs a minimal Sphinx version, state it here."
0.2.0.dev0,needs_sphinx = '1.0'
0.2.0.dev0,"Add any Sphinx extension module names here, as strings. They can be"
0.2.0.dev0,extensions coming with Sphinx (named 'sphinx.ext.*') or your custom
0.2.0.dev0,ones.
0.2.0.dev0,path to your examples scripts
0.2.0.dev0,path where to save gallery generated examples
0.2.0.dev0,"Add any paths that contain templates here, relative to this directory."
0.2.0.dev0,generate autosummary even if no references
0.2.0.dev0,The suffix of source filenames.
0.2.0.dev0,The encoding of source files.
0.2.0.dev0,source_encoding = 'utf-8-sig'
0.2.0.dev0,Generate the plots for the gallery
0.2.0.dev0,The master toctree document.
0.2.0.dev0,General information about the project.
0.2.0.dev0,"The version info for the project you're documenting, acts as replacement for"
0.2.0.dev0,"|version| and |release|, also used in various other places throughout the"
0.2.0.dev0,built documents.
0.2.0.dev0,
0.2.0.dev0,The short X.Y version.
0.2.0.dev0,"The full version, including alpha/beta/rc tags."
0.2.0.dev0,The language for content autogenerated by Sphinx. Refer to documentation
0.2.0.dev0,for a list of supported languages.
0.2.0.dev0,language = None
0.2.0.dev0,"There are two options for replacing |today|: either, you set today to some"
0.2.0.dev0,"non-false value, then it is used:"
0.2.0.dev0,today = ''
0.2.0.dev0,"Else, today_fmt is used as the format for a strftime call."
0.2.0.dev0,"today_fmt = '%B %d, %Y'"
0.2.0.dev0,"List of patterns, relative to source directory, that match files and"
0.2.0.dev0,directories to ignore when looking for source files.
0.2.0.dev0,The reST default role (used for this markup: `text`) to use for all
0.2.0.dev0,documents.
0.2.0.dev0,default_role = None
0.2.0.dev0,"If true, '()' will be appended to :func: etc. cross-reference text."
0.2.0.dev0,"If true, the current module name will be prepended to all description"
0.2.0.dev0,unit titles (such as .. function::).
0.2.0.dev0,add_module_names = True
0.2.0.dev0,"If true, sectionauthor and moduleauthor directives will be shown in the"
0.2.0.dev0,output. They are ignored by default.
0.2.0.dev0,show_authors = False
0.2.0.dev0,The name of the Pygments (syntax highlighting) style to use.
0.2.0.dev0,A list of ignored prefixes for module index sorting.
0.2.0.dev0,modindex_common_prefix = []
0.2.0.dev0,"If true, keep warnings as ""system message"" paragraphs in the built documents."
0.2.0.dev0,keep_warnings = False
0.2.0.dev0,-- Options for HTML output ----------------------------------------------
0.2.0.dev0,The theme to use for HTML and HTML Help pages.  See the documentation for
0.2.0.dev0,a list of builtin themes.
0.2.0.dev0,Theme options are theme-specific and customize the look and feel of a theme
0.2.0.dev0,"further.  For a list of options available for each theme, see the"
0.2.0.dev0,documentation.
0.2.0.dev0,html_theme_options = {}
0.2.0.dev0,"Add any paths that contain custom themes here, relative to this directory."
0.2.0.dev0,"The name for this set of Sphinx documents.  If None, it defaults to"
0.2.0.dev0,"""<project> v<release> documentation""."
0.2.0.dev0,html_title = None
0.2.0.dev0,A shorter title for the navigation bar.  Default is the same as html_title.
0.2.0.dev0,html_short_title = None
0.2.0.dev0,The name of an image file (relative to this directory) to place at the top
0.2.0.dev0,of the sidebar.
0.2.0.dev0,html_logo = None
0.2.0.dev0,The name of an image file (within the static path) to use as favicon of the
0.2.0.dev0,docs.  This file should be a Windows icon file (.ico) being 16x16 or 32x32
0.2.0.dev0,pixels large.
0.2.0.dev0,html_favicon = None
0.2.0.dev0,"Add any paths that contain custom static files (such as style sheets) here,"
0.2.0.dev0,"relative to this directory. They are copied after the builtin static files,"
0.2.0.dev0,"so a file named ""default.css"" will overwrite the builtin ""default.css""."
0.2.0.dev0,Add any extra paths that contain custom files (such as robots.txt or
0.2.0.dev0,".htaccess) here, relative to this directory. These files are copied"
0.2.0.dev0,directly to the root of the documentation.
0.2.0.dev0,html_extra_path = []
0.2.0.dev0,"If not '', a 'Last updated on:' timestamp is inserted at every page bottom,"
0.2.0.dev0,using the given strftime format.
0.2.0.dev0,"html_last_updated_fmt = '%b %d, %Y'"
0.2.0.dev0,"If true, SmartyPants will be used to convert quotes and dashes to"
0.2.0.dev0,typographically correct entities.
0.2.0.dev0,html_use_smartypants = True
0.2.0.dev0,"Custom sidebar templates, maps document names to template names."
0.2.0.dev0,html_sidebars = {}
0.2.0.dev0,"Additional templates that should be rendered to pages, maps page names to"
0.2.0.dev0,template names.
0.2.0.dev0,html_additional_pages = {}
0.2.0.dev0,"If false, no module index is generated."
0.2.0.dev0,html_domain_indices = True
0.2.0.dev0,"If false, no index is generated."
0.2.0.dev0,html_use_index = True
0.2.0.dev0,"If true, the index is split into individual pages for each letter."
0.2.0.dev0,html_split_index = False
0.2.0.dev0,"If true, links to the reST sources are added to the pages."
0.2.0.dev0,html_show_sourcelink = True
0.2.0.dev0,"If true, ""Created using Sphinx"" is shown in the HTML footer. Default is True."
0.2.0.dev0,html_show_sphinx = True
0.2.0.dev0,"If true, ""(C) Copyright ..."" is shown in the HTML footer. Default is True."
0.2.0.dev0,html_show_copyright = True
0.2.0.dev0,"If true, an OpenSearch description file will be output, and all pages will"
0.2.0.dev0,contain a <link> tag referring to it.  The value of this option must be the
0.2.0.dev0,base URL from which the finished HTML is served.
0.2.0.dev0,html_use_opensearch = ''
0.2.0.dev0,"This is the file name suffix for HTML files (e.g. "".xhtml"")."
0.2.0.dev0,html_file_suffix = None
0.2.0.dev0,Output file base name for HTML help builder.
0.2.0.dev0,-- Options for LaTeX output ---------------------------------------------
0.2.0.dev0,The paper size ('letterpaper' or 'a4paper').
0.2.0.dev0,"'papersize': 'letterpaper',"
0.2.0.dev0,"The font size ('10pt', '11pt' or '12pt')."
0.2.0.dev0,"'pointsize': '10pt',"
0.2.0.dev0,Additional stuff for the LaTeX preamble.
0.2.0.dev0,"'preamble': '',"
0.2.0.dev0,Grouping the document tree into LaTeX files. List of tuples
0.2.0.dev0,"(source start file, target name, title,"
0.2.0.dev0,"author, documentclass [howto, manual, or own class])."
0.2.0.dev0,The name of an image file (relative to this directory) to place at the top of
0.2.0.dev0,the title page.
0.2.0.dev0,latex_logo = None
0.2.0.dev0,"For ""manual"" documents, if this is true, then toplevel headings are parts,"
0.2.0.dev0,not chapters.
0.2.0.dev0,latex_use_parts = False
0.2.0.dev0,"If true, show page references after internal links."
0.2.0.dev0,latex_show_pagerefs = False
0.2.0.dev0,"If true, show URL addresses after external links."
0.2.0.dev0,latex_show_urls = False
0.2.0.dev0,Documents to append as an appendix to all manuals.
0.2.0.dev0,latex_appendices = []
0.2.0.dev0,"If false, no module index is generated."
0.2.0.dev0,latex_domain_indices = True
0.2.0.dev0,-- Options for manual page output ---------------------------------------
0.2.0.dev0,One entry per manual page. List of tuples
0.2.0.dev0,"(source start file, name, description, authors, manual section)."
0.2.0.dev0,"If true, show URL addresses after external links."
0.2.0.dev0,man_show_urls = False
0.2.0.dev0,-- Options for Texinfo output -------------------------------------------
0.2.0.dev0,Grouping the document tree into Texinfo files. List of tuples
0.2.0.dev0,"(source start file, target name, title, author,"
0.2.0.dev0,"dir menu entry, description, category)"
0.2.0.dev0,"generate empty examples files, so that we don't get"
0.2.0.dev0,inclusion errors if there are no examples for a class / module
0.2.0.dev0,touch file
0.2.0.dev0,Documents to append as an appendix to all manuals.
0.2.0.dev0,texinfo_appendices = []
0.2.0.dev0,"If false, no module index is generated."
0.2.0.dev0,texinfo_domain_indices = True
0.2.0.dev0,"How to display URL addresses: 'footnote', 'no', or 'inline'."
0.2.0.dev0,texinfo_show_urls = 'footnote'
0.2.0.dev0,"If true, do not generate a @detailmenu in the ""Top"" node's menu."
0.2.0.dev0,texinfo_no_detailmenu = False
0.2.0.dev0,Example configuration for intersphinx: refer to the Python standard library.
0.2.0.dev0,Define some color for the plotting
0.2.0.dev0,Generate the dataset
0.2.0.dev0,Instanciate a PCA object for the sake of easy visualisation
0.2.0.dev0,Fit and transform x to visualise inside a 2D feature space
0.2.0.dev0,Apply SMOTE SVM
0.2.0.dev0,"Two subplots, unpack the axes array immediately"
0.2.0.dev0,Define some color for the plotting
0.2.0.dev0,Generate the dataset
0.2.0.dev0,Instanciate a PCA object for the sake of easy visualisation
0.2.0.dev0,Fit and transform x to visualise inside a 2D feature space
0.2.0.dev0,Apply Borderline SMOTE 2
0.2.0.dev0,"Two subplots, unpack the axes array immediately"
0.2.0.dev0,Define some color for the plotting
0.2.0.dev0,Generate the dataset
0.2.0.dev0,Instanciate a PCA object for the sake of easy visualisation
0.2.0.dev0,Fit and transform x to visualise inside a 2D feature space
0.2.0.dev0,Apply regular SMOTE
0.2.0.dev0,"Two subplots, unpack the axes array immediately"
0.2.0.dev0,Define some color for the plotting
0.2.0.dev0,Generate the dataset
0.2.0.dev0,Instanciate a PCA object for the sake of easy visualisation
0.2.0.dev0,Fit and transform x to visualise inside a 2D feature space
0.2.0.dev0,Apply the random over-sampling
0.2.0.dev0,"Two subplots, unpack the axes array immediately"
0.2.0.dev0,Define some color for the plotting
0.2.0.dev0,Generate the dataset
0.2.0.dev0,Instanciate a PCA object for the sake of easy visualisation
0.2.0.dev0,Fit and transform x to visualise inside a 2D feature space
0.2.0.dev0,Apply Borderline SMOTE 1
0.2.0.dev0,"Two subplots, unpack the axes array immediately"
0.2.0.dev0,Define some color for the plotting
0.2.0.dev0,Generate the dataset
0.2.0.dev0,Instanciate a PCA object for the sake of easy visualisation
0.2.0.dev0,Fit and transform x to visualise inside a 2D feature space
0.2.0.dev0,Apply the random over-sampling
0.2.0.dev0,"Two subplots, unpack the axes array immediately"
0.2.0.dev0,Define some color for the plotting
0.2.0.dev0,Generate the dataset
0.2.0.dev0,Instanciate a PCA object for the sake of easy visualisation
0.2.0.dev0,Fit and transform x to visualise inside a 2D feature space
0.2.0.dev0,Apply SMOTE + ENN
0.2.0.dev0,"Two subplots, unpack the axes array immediately"
0.2.0.dev0,Define some color for the plotting
0.2.0.dev0,Generate the dataset
0.2.0.dev0,Instanciate a PCA object for the sake of easy visualisation
0.2.0.dev0,Fit and transform x to visualise inside a 2D feature space
0.2.0.dev0,Apply SMOTE + Tomek links
0.2.0.dev0,"Two subplots, unpack the axes array immediately"
0.2.0.dev0,Define some color for the plotting
0.2.0.dev0,Generate the dataset
0.2.0.dev0,Instanciate a PCA object for the sake of easy visualisation
0.2.0.dev0,Fit and transform x to visualise inside a 2D feature space
0.2.0.dev0,Apply Balance Cascade method
0.2.0.dev0,"Two subplots, unpack the axes array immediately"
0.2.0.dev0,Define some color for the plotting
0.2.0.dev0,Generate the dataset
0.2.0.dev0,Instanciate a PCA object for the sake of easy visualisation
0.2.0.dev0,Fit and transform x to visualise inside a 2D feature space
0.2.0.dev0,Apply Easy Ensemble
0.2.0.dev0,"Two subplots, unpack the axes array immediately"
0.2.0.dev0,Define some color for the plotting
0.2.0.dev0,Generate the dataset
0.2.0.dev0,Instanciate a PCA object for the sake of easy visualisation
0.2.0.dev0,Fit and transform x to visualise inside a 2D feature space
0.2.0.dev0,"Three subplots, unpack the axes array immediately"
0.2.0.dev0,Apply the ENN
0.2.0.dev0,Apply the RENN
0.2.0.dev0,Apply the AllKNN
0.2.0.dev0,Define some color for the plotting
0.2.0.dev0,Generate the dataset
0.2.0.dev0,Instanciate a PCA object for the sake of easy visualisation
0.2.0.dev0,Fit and transform x to visualise inside a 2D feature space
0.2.0.dev0,"Three subplots, unpack the axes array immediately"
0.2.0.dev0,Apply the ENN
0.2.0.dev0,Apply the RENN
0.2.0.dev0,Define some color for the plotting
0.2.0.dev0,Generate the dataset
0.2.0.dev0,Instanciate a PCA object for the sake of easy visualisation
0.2.0.dev0,Fit and transform x to visualise inside a 2D feature space
0.2.0.dev0,Apply Nearmiss 3
0.2.0.dev0,"Two subplots, unpack the axes array immediately"
0.2.0.dev0,Define some color for the plotting
0.2.0.dev0,Generate the dataset
0.2.0.dev0,Instanciate a PCA object for the sake of easy visualisation
0.2.0.dev0,Fit and transform x to visualise inside a 2D feature space
0.2.0.dev0,Apply Edited Nearest Neighbours
0.2.0.dev0,"Two subplots, unpack the axes array immediately"
0.2.0.dev0,Define some color for the plotting
0.2.0.dev0,Generate the dataset
0.2.0.dev0,Instanciate a PCA object for the sake of easy visualisation
0.2.0.dev0,Fit and transform x to visualise inside a 2D feature space
0.2.0.dev0,Apply Tomek Links cleaning
0.2.0.dev0,"Two subplots, unpack the axes array immediately"
0.2.0.dev0,Define some color for the plotting
0.2.0.dev0,Generate the dataset
0.2.0.dev0,Instanciate a PCA object for the sake of easy visualisation
0.2.0.dev0,Fit and transform x to visualise inside a 2D feature space
0.2.0.dev0,Apply One-Sided Selection
0.2.0.dev0,"Two subplots, unpack the axes array immediately"
0.2.0.dev0,Define some color for the plotting
0.2.0.dev0,Generate the dataset
0.2.0.dev0,Instanciate a PCA object for the sake of easy visualisation
0.2.0.dev0,Fit and transform x to visualise inside a 2D feature space
0.2.0.dev0,Apply Nearmiss 2
0.2.0.dev0,"Two subplots, unpack the axes array immediately"
0.2.0.dev0,Define some color for the plotting
0.2.0.dev0,Generate the dataset
0.2.0.dev0,Instanciate a PCA object for the sake of easy visualisation
0.2.0.dev0,Fit and transform x to visualise inside a 2D feature space
0.2.0.dev0,Apply neighbourhood cleaning rule
0.2.0.dev0,"Two subplots, unpack the axes array immediately"
0.2.0.dev0,Define some color for the plotting
0.2.0.dev0,Generate the dataset
0.2.0.dev0,Instanciate a PCA object for the sake of easy visualisation
0.2.0.dev0,Fit and transform x to visualise inside a 2D feature space
0.2.0.dev0,Apply Condensed Nearest Neighbours
0.2.0.dev0,"Two subplots, unpack the axes array immediately"
0.2.0.dev0,Define some color for the plotting
0.2.0.dev0,Generate the dataset
0.2.0.dev0,Instanciate a PCA object for the sake of easy visualisation
0.2.0.dev0,Fit and transform x to visualise inside a 2D feature space
0.2.0.dev0,Apply Cluster Centroids
0.2.0.dev0,"Two subplots, unpack the axes array immediately"
0.2.0.dev0,Define some color for the plotting
0.2.0.dev0,Generate the dataset
0.2.0.dev0,Instanciate a PCA object for the sake of easy visualisation
0.2.0.dev0,Fit and transform x to visualise inside a 2D feature space
0.2.0.dev0,Apply the random under-sampling
0.2.0.dev0,"Two subplots, unpack the axes array immediately"
0.2.0.dev0,Define some color for the plotting
0.2.0.dev0,Generate the dataset
0.2.0.dev0,"Two subplots, unpack the axes array immediately"
0.2.0.dev0,Define some color for the plotting
0.2.0.dev0,Generate the dataset
0.2.0.dev0,Instanciate a PCA object for the sake of easy visualisation
0.2.0.dev0,Fit and transform x to visualise inside a 2D feature space
0.2.0.dev0,Apply Nearmiss 1
0.2.0.dev0,"Two subplots, unpack the axes array immediately"
0.2.0.dev0,Generate the dataset
0.2.0.dev0,Instanciate a PCA object for the sake of easy visualisation
0.2.0.dev0,Create the samplers
0.2.0.dev0,Create teh classifier
0.2.0.dev0,Make the splits
0.2.0.dev0,Add one transformers and two samplers in the pipeline object
0.2.0.dev0,Define some color for the plotting
0.2.0.dev0,Generate the dataset
0.2.0.dev0,"Two subplots, unpack the axes array immediately"
0.2.0.dev0,Based on NiLearn package
0.2.0.dev0,License: simplified BSD
0.2.0.dev0,"PEP0440 compatible formatted version, see:"
0.2.0.dev0,https://www.python.org/dev/peps/pep-0440/
0.2.0.dev0,
0.2.0.dev0,Generic release markers:
0.2.0.dev0,X.Y
0.2.0.dev0,X.Y.Z # For bugfix releases
0.2.0.dev0,
0.2.0.dev0,Admissible pre-release markers:
0.2.0.dev0,X.YaN # Alpha release
0.2.0.dev0,X.YbN # Beta release
0.2.0.dev0,X.YrcN # Release Candidate
0.2.0.dev0,X.Y # Final release
0.2.0.dev0,
0.2.0.dev0,Dev branch marker is: 'X.Y.dev' or 'X.Y.devN' where N is an integer.
0.2.0.dev0,'X.Y.dev0' is the canonical version of 'X.Y.dev'
0.2.0.dev0,
0.2.0.dev0,"This is a tuple to preserve order, so that dependencies are checked"
0.2.0.dev0,in some meaningful order (more => less 'core').  We avoid using
0.2.0.dev0,collections.OrderedDict to preserve Python 2.6 compatibility.
0.2.0.dev0,Avoid choking on modules with no __version__ attribute
0.2.0.dev0,Skip check only when installing and it's a module that
0.2.0.dev0,will be auto-installed.
0.2.0.dev0,Check the consistency of X and y
0.2.0.dev0,Get all the unique elements in the target array
0.2.0.dev0,# Raise an error if there is only one class
0.2.0.dev0,if uniques.size == 1:
0.2.0.dev0,"raise RuntimeError(""Only one class detected, aborting..."")"
0.2.0.dev0,Raise a warning for the moment to be compatible with BaseEstimator
0.2.0.dev0,Store the size of X to check at sampling time if we have the
0.2.0.dev0,same data
0.2.0.dev0,Create a dictionary containing the class statistics
0.2.0.dev0,Find the minority and majority classes
0.2.0.dev0,Check if the ratio provided at initialisation make sense
0.2.0.dev0,Check the consistency of X and y
0.2.0.dev0,Check that the data have been fitted
0.2.0.dev0,Check if the size of the data is identical than at fitting
0.2.0.dev0,The ratio correspond to the number of samples in the minority class
0.2.0.dev0,"over the number of samples in the majority class. Thus, the ratio"
0.2.0.dev0,cannot be greater than 1.0
0.2.0.dev0,Check that the target type is binary
0.2.0.dev0,Check that the target type is either binary or multiclass
0.2.0.dev0,Adapted from scikit-learn
0.2.0.dev0,Author: Edouard Duchesnay
0.2.0.dev0,Gael Varoquaux
0.2.0.dev0,Virgile Fritsch
0.2.0.dev0,Alexandre Gramfort
0.2.0.dev0,Lars Buitinck
0.2.0.dev0,chkoar
0.2.0.dev0,License: BSD
0.2.0.dev0,BaseEstimator interface
0.2.0.dev0,shallow copy of steps
0.2.0.dev0,Estimator interface
0.2.0.dev0,Boolean controlling whether the joblib caches should be
0.2.0.dev0,"flushed if the version of certain modules changes (eg nibabel, as it"
0.2.0.dev0,does not respect the backward compatibility in some of its internal
0.2.0.dev0,structures
0.2.0.dev0,This  is used in nilearn._utils.cache_mixin
0.2.0.dev0,list all submodules available in imblearn and version
0.2.0.dev0,Keep the samples from the majority class
0.2.0.dev0,Loop over the other classes over picking at random
0.2.0.dev0,"If this is the majority class, skip it"
0.2.0.dev0,Define the number of sample to create
0.2.0.dev0,Pick some elements at random
0.2.0.dev0,Concatenate to the majority class
0.2.0.dev0,Keep the samples from the majority class
0.2.0.dev0,Define the number of sample to create
0.2.0.dev0,We handle only two classes problem for the moment.
0.2.0.dev0,Start by separating minority class features and target values.
0.2.0.dev0,Print if verbose is true
0.2.0.dev0,"Look for k-th nearest neighbours, excluding, of course, the"
0.2.0.dev0,point itself.
0.2.0.dev0,Get the distance to the NN
0.2.0.dev0,Compute the ratio of majority samples next to minority samples
0.2.0.dev0,Check that we found at least some neighbours belonging to the
0.2.0.dev0,majority class
0.2.0.dev0,Normalize the ratio
0.2.0.dev0,Compute the number of sample to be generated
0.2.0.dev0,For each minority samples
0.2.0.dev0,Pick-up the neighbors wanted
0.2.0.dev0,Create a new sample
0.2.0.dev0,Find the NN for each samples
0.2.0.dev0,Exclude the sample itself
0.2.0.dev0,Count how many NN belong to the minority class
0.2.0.dev0,Find the class corresponding to the label in x
0.2.0.dev0,Compute the number of majority samples in the NN
0.2.0.dev0,Samples are in danger for m/2 <= m' < m
0.2.0.dev0,Samples are noise for m = m'
0.2.0.dev0,Check the consistency of X
0.2.0.dev0,Check the random state
0.2.0.dev0,A matrix to store the synthetic samples
0.2.0.dev0,# Set seeds
0.2.0.dev0,"seeds = random_state.randint(low=0,"
0.2.0.dev0,"high=100 * len(nn_num.flatten()),"
0.2.0.dev0,size=n_samples)
0.2.0.dev0,Randomly pick samples to construct neighbours from
0.2.0.dev0,Loop over the NN matrix and create new samples
0.2.0.dev0,"NN lines relate to original sample, columns to its"
0.2.0.dev0,nearest neighbours
0.2.0.dev0,"Take a step of random size (0,1) in the direction of the"
0.2.0.dev0,n nearest neighbours
0.2.0.dev0,if self.random_state is None:
0.2.0.dev0,np.random.seed(seeds[i])
0.2.0.dev0,else:
0.2.0.dev0,np.random.seed(self.random_state)
0.2.0.dev0,Construct synthetic sample
0.2.0.dev0,The returned target vector is simply a repetition of the
0.2.0.dev0,minority label
0.2.0.dev0,Define the number of sample to create
0.2.0.dev0,We handle only two classes problem for the moment.
0.2.0.dev0,Start by separating minority class features and target values.
0.2.0.dev0,If regular SMOTE is to be performed
0.2.0.dev0,"Look for k-th nearest neighbours, excluding, of course, the"
0.2.0.dev0,point itself.
0.2.0.dev0,Matrix with k-th nearest neighbours indexes for each minority
0.2.0.dev0,element.
0.2.0.dev0,--- Generating synthetic samples
0.2.0.dev0,Use static method make_samples to generate minority samples
0.2.0.dev0,Concatenate the newly generated samples to the original data set
0.2.0.dev0,Find the NNs for all samples in the data set.
0.2.0.dev0,Boolean array with True for minority samples in danger
0.2.0.dev0,"If all minority samples are safe, return the original data set."
0.2.0.dev0,"All are safe, nothing to be done here."
0.2.0.dev0,"If we got here is because some samples are in danger, we need to"
0.2.0.dev0,find the NNs among the minority class to create the new synthetic
0.2.0.dev0,samples.
0.2.0.dev0,
0.2.0.dev0,We start by changing the number of NNs to consider from m + 1
0.2.0.dev0,to k + 1
0.2.0.dev0,nns...#
0.2.0.dev0,B1 and B2 types diverge here!!!
0.2.0.dev0,Create synthetic samples for borderline points.
0.2.0.dev0,Concatenate the newly generated samples to the original
0.2.0.dev0,dataset
0.2.0.dev0,Reset the k-neighbours to m+1 neighbours
0.2.0.dev0,Split the number of synthetic samples between only minority
0.2.0.dev0,"(type 1), or minority and majority (with reduced step size)"
0.2.0.dev0,(type 2).
0.2.0.dev0,The fraction is sampled from a beta distribution centered
0.2.0.dev0,around 0.5 with variance ~0.01
0.2.0.dev0,Only minority
0.2.0.dev0,Only majority with smaller step size
0.2.0.dev0,Concatenate the newly generated samples to the original
0.2.0.dev0,data set
0.2.0.dev0,Reset the k-neighbours to m+1 neighbours
0.2.0.dev0,The SVM smote model fits a support vector machine
0.2.0.dev0,classifier to the data and uses the support vector to
0.2.0.dev0,"provide a notion of boundary. Unlike regular smote, where"
0.2.0.dev0,such notion relies on proportion of nearest neighbours
0.2.0.dev0,belonging to each class.
0.2.0.dev0,Fit SVM to the full data#
0.2.0.dev0,Find the support vectors and their corresponding indexes
0.2.0.dev0,"First, find the nn of all the samples to identify samples"
0.2.0.dev0,in danger and noisy ones
0.2.0.dev0,"As usual, fit a nearest neighbour model to the data"
0.2.0.dev0,"Now, get rid of noisy support vectors"
0.2.0.dev0,Remove noisy support vectors
0.2.0.dev0,Proceed to find support vectors NNs among the minority class
0.2.0.dev0,Split the number of synthetic samples between interpolation and
0.2.0.dev0,extrapolation
0.2.0.dev0,The fraction are sampled from a beta distribution with mean
0.2.0.dev0,0.5 and variance 0.01#
0.2.0.dev0,Interpolate samples in danger
0.2.0.dev0,Extrapolate safe samples
0.2.0.dev0,Concatenate the newly generated samples to the original data set
0.2.0.dev0,not any support vectors in danger
0.2.0.dev0,All the support vector in danger
0.2.0.dev0,Reset the k-neighbours to m+1 neighbours
0.2.0.dev0,--- NN object
0.2.0.dev0,Import the NN object from scikit-learn library. Since in the smote
0.2.0.dev0,"variations we must first find samples that are in danger, we"
0.2.0.dev0,initialize the NN object differently depending on the method chosen
0.2.0.dev0,"Regular smote does not look for samples in danger, instead it"
0.2.0.dev0,creates synthetic samples directly from the k-th nearest
0.2.0.dev0,neighbours with not filtering
0.2.0.dev0,"Borderline1, 2 and SVM variations of smote must first look for"
0.2.0.dev0,samples that could be considered noise and samples that live
0.2.0.dev0,"near the boundary between the classes. Therefore, before"
0.2.0.dev0,"creating synthetic samples from the k-th nns, it first look"
0.2.0.dev0,for m nearest neighbors to decide whether or not a sample is
0.2.0.dev0,noise or near the boundary.
0.2.0.dev0,--- SVM smote
0.2.0.dev0,"Unlike the borderline variations, the SVM variation uses the support"
0.2.0.dev0,vectors to decide which samples are in danger (near the boundary).
0.2.0.dev0,Additionally it also introduces extrapolation for samples that are
0.2.0.dev0,considered safe (far from boundary) and interpolation for samples
0.2.0.dev0,in danger (near the boundary). The level of extrapolation is
0.2.0.dev0,controled by the out_step.
0.2.0.dev0,Store SVM object with any parameters
0.2.0.dev0,Generate a global dataset to use
0.2.0.dev0,Data generated for the toy example
0.2.0.dev0,Define a negative ratio
0.2.0.dev0,Define a ratio greater than 1
0.2.0.dev0,Define ratio as an unknown string
0.2.0.dev0,Define ratio as a list which is not supported
0.2.0.dev0,Define a ratio
0.2.0.dev0,Create the object
0.2.0.dev0,Resample the data
0.2.0.dev0,Create a wrong y
0.2.0.dev0,Create the object
0.2.0.dev0,Fit the data
0.2.0.dev0,Create the object
0.2.0.dev0,Fit the data
0.2.0.dev0,Check if the data information have been computed
0.2.0.dev0,Create the object
0.2.0.dev0,Resample the data
0.2.0.dev0,Resample the data
0.2.0.dev0,Create the object
0.2.0.dev0,continuous case
0.2.0.dev0,Make y to be multiclass
0.2.0.dev0,Resample the data
0.2.0.dev0,Check the size of y
0.2.0.dev0,Generate a global dataset to use
0.2.0.dev0,Define a negative ratio
0.2.0.dev0,Define a ratio greater than 1
0.2.0.dev0,Define ratio as an unknown string
0.2.0.dev0,Define ratio as a list which is not supported
0.2.0.dev0,Define a ratio
0.2.0.dev0,Create the object
0.2.0.dev0,Resample the data
0.2.0.dev0,Create a wrong y
0.2.0.dev0,Create the object
0.2.0.dev0,Fit the data
0.2.0.dev0,Create the object
0.2.0.dev0,Fit the data
0.2.0.dev0,Check if the data information have been computed
0.2.0.dev0,Create the object
0.2.0.dev0,Resample the data
0.2.0.dev0,Resample the data
0.2.0.dev0,Create the object
0.2.0.dev0,continuous case
0.2.0.dev0,multiclass case
0.2.0.dev0,Generate a global dataset to use
0.2.0.dev0,Define a negative ratio
0.2.0.dev0,Define a ratio greater than 1
0.2.0.dev0,Define ratio as an unknown string
0.2.0.dev0,Define ratio as a list which is not supported
0.2.0.dev0,Create the object
0.2.0.dev0,Resample the data
0.2.0.dev0,Create a wrong y
0.2.0.dev0,Create the object
0.2.0.dev0,Fit the data
0.2.0.dev0,Check if the data information have been computed
0.2.0.dev0,Create the object
0.2.0.dev0,Create the object
0.2.0.dev0,Fit the data
0.2.0.dev0,Create the object
0.2.0.dev0,Fit the data
0.2.0.dev0,Create the object
0.2.0.dev0,Fit the data
0.2.0.dev0,Create the object
0.2.0.dev0,Fit the data
0.2.0.dev0,Create the object
0.2.0.dev0,Fit the data
0.2.0.dev0,Create the object
0.2.0.dev0,continuous case
0.2.0.dev0,multiclass case
0.2.0.dev0,Start with the minority class
0.2.0.dev0,All the minority class samples will be preserved
0.2.0.dev0,If we need to offer support for the indices
0.2.0.dev0,Loop over the other classes under picking at random
0.2.0.dev0,"If the minority class is up, skip it"
0.2.0.dev0,Randomly get one sample from the majority class
0.2.0.dev0,Generate the index to select
0.2.0.dev0,Create the set C
0.2.0.dev0,Create the set S
0.2.0.dev0,Remove the seed from S since that it will be added anyway
0.2.0.dev0,Create a k-NN classifier
0.2.0.dev0,Fit C into the knn
0.2.0.dev0,Classify on S
0.2.0.dev0,Find the misclassified S_y
0.2.0.dev0,If we need to offer support for the indices selected
0.2.0.dev0,We concatenate the misclassified samples with the seed and the
0.2.0.dev0,minority samples
0.2.0.dev0,Find the nearest neighbour of every point
0.2.0.dev0,Send the information to is_tomek function to get boolean vector back
0.2.0.dev0,Check if the indices of the samples selected should be returned too
0.2.0.dev0,Return the indices of interest
0.2.0.dev0,Return data set without majority Tomek links.
0.2.0.dev0,Compute the distance considering the farthest neighbour
0.2.0.dev0,Sort the list of distance and get the index
0.2.0.dev0,Throw a warning to tell the user that we did not have enough samples
0.2.0.dev0,to select and that we just select everything
0.2.0.dev0,Select the desired number of samples
0.2.0.dev0,Assign the parameter of the element of this class
0.2.0.dev0,Check that the version asked is implemented
0.2.0.dev0,Start with the minority class
0.2.0.dev0,All the minority class samples will be preserved
0.2.0.dev0,Compute the number of cluster needed
0.2.0.dev0,If we need to offer support for the indices
0.2.0.dev0,"For each element of the current class, find the set of NN"
0.2.0.dev0,of the minority class
0.2.0.dev0,Call the constructor of the NN
0.2.0.dev0,Fit the minority class since that we want to know the distance
0.2.0.dev0,to these point
0.2.0.dev0,Loop over the other classes under picking at random
0.2.0.dev0,"If the minority class is up, skip it"
0.2.0.dev0,Get the samples corresponding to the current class
0.2.0.dev0,Find the NN
0.2.0.dev0,Select the right samples
0.2.0.dev0,Find the NN
0.2.0.dev0,Select the right samples
0.2.0.dev0,We need a new NN object to fit the current class
0.2.0.dev0,Find the set of NN to the minority class
0.2.0.dev0,Create the subset containing the samples found during the NN
0.2.0.dev0,search. Linearize the indexes and remove the double values
0.2.0.dev0,Create the subset
0.2.0.dev0,Compute the NN considering the current class
0.2.0.dev0,If we need to offer support for the indices selected
0.2.0.dev0,Check if the indices of the samples selected should be returned too
0.2.0.dev0,Return the indices of interest
0.2.0.dev0,Compute the number of clusters needed
0.2.0.dev0,All the minority class samples will be preserved
0.2.0.dev0,If we need to offer support for the indices
0.2.0.dev0,Loop over the other classes under-picking at random
0.2.0.dev0,"If the minority class is up, skip it"
0.2.0.dev0,Pick some elements at random
0.2.0.dev0,If we need to offer support for the indices selected
0.2.0.dev0,Concatenate to the minority class
0.2.0.dev0,Check if the indices of the samples selected should be returned as
0.2.0.dev0,well
0.2.0.dev0,Return the indices of interest
0.2.0.dev0,"Initialize the boolean result as false, and also a counter"
0.2.0.dev0,Loop through each sample and looks whether it belongs to the minority
0.2.0.dev0,"class. If it does, we don't consider it since we want to keep all"
0.2.0.dev0,"minority samples. If, however, it belongs to the majority sample we"
0.2.0.dev0,look at its first neighbour. If its closest neighbour also has the
0.2.0.dev0,"current sample as its closest neighbour, the two form a Tomek link."
0.2.0.dev0,"If they form a tomek link, put a True marker on this"
0.2.0.dev0,"sample, and increase counter by one."
0.2.0.dev0,Find the nearest neighbour of every point
0.2.0.dev0,Send the information to is_tomek function to get boolean vector back
0.2.0.dev0,Check if the indices of the samples selected should be returned too
0.2.0.dev0,Return the indices of interest
0.2.0.dev0,Return data set without majority Tomek links.
0.2.0.dev0,Start with the minority class
0.2.0.dev0,All the minority class samples will be preserved
0.2.0.dev0,If we need to offer support for the indices
0.2.0.dev0,Loop over the other classes under picking at random
0.2.0.dev0,"If the minority class is up, skip it"
0.2.0.dev0,Randomly get one sample from the majority class
0.2.0.dev0,Generate the index to select
0.2.0.dev0,Create the set C - One majority samples and all minority
0.2.0.dev0,Create the set S - all majority samples
0.2.0.dev0,Create a k-NN classifier
0.2.0.dev0,Fit C into the knn
0.2.0.dev0,Check each sample in S if we keep it or drop it
0.2.0.dev0,Do not select sample which are already well classified
0.2.0.dev0,Classify on S
0.2.0.dev0,If the prediction do not agree with the true label
0.2.0.dev0,append it in C_x
0.2.0.dev0,Keep the index for later
0.2.0.dev0,Update C
0.2.0.dev0,Fit C into the knn
0.2.0.dev0,This experimental to speed up the search
0.2.0.dev0,Classify all the element in S and avoid to test the
0.2.0.dev0,well classified elements
0.2.0.dev0,Find the misclassified S_y
0.2.0.dev0,"The indexes found are relative to the current class, we need to"
0.2.0.dev0,find the absolute value
0.2.0.dev0,Build the array with the absolute position
0.2.0.dev0,If we need to offer support for the indices selected
0.2.0.dev0,Check if the indices of the samples selected should be returned too
0.2.0.dev0,Return the indices of interest
0.2.0.dev0,Compute the number of cluster needed
0.2.0.dev0,Create the clustering object
0.2.0.dev0,Start with the minority class
0.2.0.dev0,All the minority class samples will be preserved
0.2.0.dev0,Loop over the other classes under picking at random
0.2.0.dev0,"If the minority class is up, skip it."
0.2.0.dev0,Find the centroids via k-means
0.2.0.dev0,Concatenate to the minority class
0.2.0.dev0,Start with the minority class
0.2.0.dev0,All the minority class samples will be preserved
0.2.0.dev0,If we need to offer support for the indices
0.2.0.dev0,Create a k-NN to fit the whole data
0.2.0.dev0,Fit the whole dataset
0.2.0.dev0,Loop over the other classes under picking at random
0.2.0.dev0,Get the sample of the current class
0.2.0.dev0,Get the samples associated
0.2.0.dev0,Find the NN for the current class
0.2.0.dev0,Get the label of the corresponding to the index
0.2.0.dev0,Check which one are the same label than the current class
0.2.0.dev0,Make an AND operation through the three neighbours
0.2.0.dev0,If the minority class remove the majority samples
0.2.0.dev0,Get the index to exclude
0.2.0.dev0,Get the index to exclude
0.2.0.dev0,Create a vector with the sample to select
0.2.0.dev0,Exclude as well the minority sample since that they will be
0.2.0.dev0,concatenated later
0.2.0.dev0,Get the samples from the majority classes
0.2.0.dev0,If we need to offer support for the indices selected
0.2.0.dev0,Check if the indices of the samples selected should be returned too
0.2.0.dev0,Return the indices of interest
0.2.0.dev0,Start with the minority class
0.2.0.dev0,All the minority class samples will be preserved
0.2.0.dev0,If we need to offer support for the indices
0.2.0.dev0,Create a k-NN to fit the whole data
0.2.0.dev0,Fit the data
0.2.0.dev0,Loop over the other classes under picking at random
0.2.0.dev0,"If the minority class is up, skip it"
0.2.0.dev0,Get the sample of the current class
0.2.0.dev0,Find the NN for the current class
0.2.0.dev0,Get the label of the corresponding to the index
0.2.0.dev0,Check which one are the same label than the current class
0.2.0.dev0,Make the majority vote
0.2.0.dev0,Get the samples which agree all together
0.2.0.dev0,If we need to offer support for the indices selected
0.2.0.dev0,Check if the indices of the samples selected should be returned too
0.2.0.dev0,Return the indices of interest
0.2.0.dev0,Check the stopping criterion
0.2.0.dev0,1. If there is no changes for the vector y
0.2.0.dev0,2. If the number of samples in the other class become inferior to
0.2.0.dev0,the number of samples in the majority class
0.2.0.dev0,3. If one of the class is disappearing
0.2.0.dev0,Case 1
0.2.0.dev0,Case 2
0.2.0.dev0,Get the number of samples in the non-minority classes
0.2.0.dev0,Check the minority stop to be the minority
0.2.0.dev0,Case 3
0.2.0.dev0,"If this is a normal convergence, get the last data"
0.2.0.dev0,Log the variables to explain the stop of the algorithm
0.2.0.dev0,Update the data for the next iteration
0.2.0.dev0,Check if the indices of the samples selected should be returned too
0.2.0.dev0,Return the indices of interest
0.2.0.dev0,updating ENN size_ngh
0.2.0.dev0,Check the stopping criterion
0.2.0.dev0,1. If the number of samples in the other class become inferior to
0.2.0.dev0,the number of samples in the majority class
0.2.0.dev0,2. If one of the class is disappearing
0.2.0.dev0,Case 1
0.2.0.dev0,Get the number of samples in the non-minority classes
0.2.0.dev0,Check the minority stop to be the minority
0.2.0.dev0,Case 2
0.2.0.dev0,Log the variables to explain the stop of the algorithm
0.2.0.dev0,Update the data for the next iteration
0.2.0.dev0,Check if the indices of the samples selected should be returned too
0.2.0.dev0,Return the indices of interest
0.2.0.dev0,Select the appropriate classifier
0.2.0.dev0,Create the different folds
0.2.0.dev0,Compute the number of cluster needed
0.2.0.dev0,Find the percentile corresponding to the top num_samples
0.2.0.dev0,Sample the data
0.2.0.dev0,If we need to offer support for the indices
0.2.0.dev0,Generate a global dataset to use
0.2.0.dev0,Define a negative ratio
0.2.0.dev0,Define a ratio greater than 1
0.2.0.dev0,Define ratio as an unknown string
0.2.0.dev0,Define ratio as a list which is not supported
0.2.0.dev0,Define a ratio
0.2.0.dev0,Define the parameter for the under-sampling
0.2.0.dev0,Create the object
0.2.0.dev0,Resample the data
0.2.0.dev0,Create a wrong y
0.2.0.dev0,Create the object
0.2.0.dev0,Fit the data
0.2.0.dev0,Define the parameter for the under-sampling
0.2.0.dev0,Create the object
0.2.0.dev0,Fit the data
0.2.0.dev0,Check if the data information have been computed
0.2.0.dev0,Define the parameter for the under-sampling
0.2.0.dev0,Create the object
0.2.0.dev0,Define the parameter for the under-sampling
0.2.0.dev0,Create the object
0.2.0.dev0,Fit and sample
0.2.0.dev0,Define the parameter for the under-sampling
0.2.0.dev0,Create the object
0.2.0.dev0,Fit and sample
0.2.0.dev0,Define the parameter for the under-sampling
0.2.0.dev0,Create the object
0.2.0.dev0,Fit and sample
0.2.0.dev0,Create the object
0.2.0.dev0,continuous case
0.2.0.dev0,Generate a global dataset to use
0.2.0.dev0,Define a ratio
0.2.0.dev0,Create the object
0.2.0.dev0,Resample the data
0.2.0.dev0,Create a wrong y
0.2.0.dev0,Create the object
0.2.0.dev0,Fit the data
0.2.0.dev0,Check if the data information have been computed
0.2.0.dev0,Create the object
0.2.0.dev0,Resample the data
0.2.0.dev0,Resample the data
0.2.0.dev0,Create the object
0.2.0.dev0,continuous case
0.2.0.dev0,multiclass case
0.2.0.dev0,Generate a global dataset to use
0.2.0.dev0,Define a ratio
0.2.0.dev0,Create the object
0.2.0.dev0,Resample the data
0.2.0.dev0,Create a wrong y
0.2.0.dev0,Create the object
0.2.0.dev0,Fit the data
0.2.0.dev0,Check if the data information have been computed
0.2.0.dev0,Create the object
0.2.0.dev0,Resample the data
0.2.0.dev0,Resample the data
0.2.0.dev0,Create the object
0.2.0.dev0,continuous case
0.2.0.dev0,Generate a global dataset to use
0.2.0.dev0,Define a ratio
0.2.0.dev0,Create the object
0.2.0.dev0,Resample the data
0.2.0.dev0,Create a wrong y
0.2.0.dev0,Create the object
0.2.0.dev0,Fit the data
0.2.0.dev0,Check if the data information have been computed
0.2.0.dev0,Create the object
0.2.0.dev0,Resample the data
0.2.0.dev0,Resample the data
0.2.0.dev0,Resample the data
0.2.0.dev0,Create the object
0.2.0.dev0,continuous case
0.2.0.dev0,Generate a global dataset to use
0.2.0.dev0,Define a ratio
0.2.0.dev0,Create the object
0.2.0.dev0,Resample the data
0.2.0.dev0,Create a wrong y
0.2.0.dev0,Create the object
0.2.0.dev0,Fit the data
0.2.0.dev0,Check if the data information have been computed
0.2.0.dev0,Create the object
0.2.0.dev0,Resample the data
0.2.0.dev0,Resample the data
0.2.0.dev0,Resample the data
0.2.0.dev0,Create the object
0.2.0.dev0,continuous case
0.2.0.dev0,Generate a global dataset to use
0.2.0.dev0,Data generated for the toy example
0.2.0.dev0,Define a negative ratio
0.2.0.dev0,Define a ratio greater than 1
0.2.0.dev0,Define ratio as an unknown string
0.2.0.dev0,Define ratio as a list which is not supported
0.2.0.dev0,Define a ratio
0.2.0.dev0,Create the object
0.2.0.dev0,Resample the data
0.2.0.dev0,Create a wrong y
0.2.0.dev0,Create the object
0.2.0.dev0,Fit the data
0.2.0.dev0,Create the object
0.2.0.dev0,Fit the data
0.2.0.dev0,Check if the data information have been computed
0.2.0.dev0,Create the object
0.2.0.dev0,Resample the data
0.2.0.dev0,Resample the data
0.2.0.dev0,Resample the data
0.2.0.dev0,Create the object
0.2.0.dev0,continuous case
0.2.0.dev0,Make y to be multiclass
0.2.0.dev0,Resample the data
0.2.0.dev0,Check the size of y
0.2.0.dev0,Generate a global dataset to use
0.2.0.dev0,Define a negative ratio
0.2.0.dev0,Define a ratio greater than 1
0.2.0.dev0,Define ratio as an unknown string
0.2.0.dev0,Define ratio as a list which is not supported
0.2.0.dev0,Resample the data
0.2.0.dev0,Define a ratio
0.2.0.dev0,Create the object
0.2.0.dev0,Resample the data
0.2.0.dev0,Create a wrong y
0.2.0.dev0,Create the object
0.2.0.dev0,Fit the data
0.2.0.dev0,Create the object
0.2.0.dev0,Fit the data
0.2.0.dev0,Check if the data information have been computed
0.2.0.dev0,Create the object
0.2.0.dev0,Resample the data
0.2.0.dev0,Resample the data
0.2.0.dev0,Resample the data
0.2.0.dev0,Resample the data
0.2.0.dev0,Resample the data
0.2.0.dev0,Resample the data
0.2.0.dev0,Resample the data
0.2.0.dev0,Resample the data
0.2.0.dev0,Resample the data
0.2.0.dev0,Create the object
0.2.0.dev0,continuous case
0.2.0.dev0,multiclass case
0.2.0.dev0,Generate a global dataset to use
0.2.0.dev0,Define a ratio
0.2.0.dev0,Create the object
0.2.0.dev0,Create the object
0.2.0.dev0,Resample the data
0.2.0.dev0,Create a wrong y
0.2.0.dev0,Create the object
0.2.0.dev0,Fit the data
0.2.0.dev0,Check if the data information have been computed
0.2.0.dev0,Create the object
0.2.0.dev0,Resample the data
0.2.0.dev0,Resample the data
0.2.0.dev0,Resample the data
0.2.0.dev0,Create the object
0.2.0.dev0,continuous case
0.2.0.dev0,Generate a global dataset to use
0.2.0.dev0,Define a negative ratio
0.2.0.dev0,Define a ratio greater than 1
0.2.0.dev0,Define ratio as an unknown string
0.2.0.dev0,Define ratio as a list which is not supported
0.2.0.dev0,Define a ratio
0.2.0.dev0,Define the parameter for the under-sampling
0.2.0.dev0,Create the object
0.2.0.dev0,Resample the data
0.2.0.dev0,Create a wrong y
0.2.0.dev0,Create the object
0.2.0.dev0,Fit the data
0.2.0.dev0,Define the parameter for the under-sampling
0.2.0.dev0,Create the object
0.2.0.dev0,Fit the data
0.2.0.dev0,Check if the data information have been computed
0.2.0.dev0,Define the parameter for the under-sampling
0.2.0.dev0,Create the object
0.2.0.dev0,Define the parameter for the under-sampling
0.2.0.dev0,Create the object
0.2.0.dev0,Fit and sample
0.2.0.dev0,Define the parameter for the under-sampling
0.2.0.dev0,Create the object
0.2.0.dev0,Fit and sample
0.2.0.dev0,Define the parameter for the under-sampling
0.2.0.dev0,Create the object
0.2.0.dev0,Fit and sample
0.2.0.dev0,Create the object
0.2.0.dev0,continuous case
0.2.0.dev0,Generate a global dataset to use
0.2.0.dev0,Define a ratio
0.2.0.dev0,Create the object
0.2.0.dev0,Resample the data
0.2.0.dev0,Create a wrong y
0.2.0.dev0,Create the object
0.2.0.dev0,Fit the data
0.2.0.dev0,Check if the data information have been computed
0.2.0.dev0,Create the object
0.2.0.dev0,Resample the data
0.2.0.dev0,Resample the data
0.2.0.dev0,Create the object
0.2.0.dev0,continuous case
0.2.0.dev0,Generate a global dataset to use
0.2.0.dev0,Data generated for the toy example
0.2.0.dev0,Define a negative ratio
0.2.0.dev0,Define a ratio greater than 1
0.2.0.dev0,Define ratio as an unknown string
0.2.0.dev0,Define ratio as a list which is not supported
0.2.0.dev0,Define a ratio
0.2.0.dev0,Define the parameter for the under-sampling
0.2.0.dev0,Create the object
0.2.0.dev0,Resample the data
0.2.0.dev0,Create a wrong y
0.2.0.dev0,Create the object
0.2.0.dev0,Fit the data
0.2.0.dev0,Define the parameter for the under-sampling
0.2.0.dev0,Create the object
0.2.0.dev0,Fit the data
0.2.0.dev0,Check if the data information have been computed
0.2.0.dev0,Define the parameter for the under-sampling
0.2.0.dev0,Create the object
0.2.0.dev0,Define the parameter for the under-sampling
0.2.0.dev0,Create the object
0.2.0.dev0,Define the parameter for the under-sampling
0.2.0.dev0,Create the object
0.2.0.dev0,Fit and sample
0.2.0.dev0,Define the parameter for the under-sampling
0.2.0.dev0,Create the object
0.2.0.dev0,Fit and sample
0.2.0.dev0,Create the object
0.2.0.dev0,continuous case
0.2.0.dev0,Make y to be multiclass
0.2.0.dev0,Resample the data
0.2.0.dev0,Check the size of y
0.2.0.dev0,Generate a global dataset to use
0.2.0.dev0,Define a negative ratio
0.2.0.dev0,Define a ratio greater than 1
0.2.0.dev0,Define ratio as an unknown string
0.2.0.dev0,Define ratio as a list which is not supported
0.2.0.dev0,Define a ratio
0.2.0.dev0,Define the parameter for the under-sampling
0.2.0.dev0,Create the object
0.2.0.dev0,Resample the data
0.2.0.dev0,Create a wrong y
0.2.0.dev0,Create the object
0.2.0.dev0,Fit the data
0.2.0.dev0,Define the parameter for the under-sampling
0.2.0.dev0,Create the object
0.2.0.dev0,Fit the data
0.2.0.dev0,Check if the data information have been computed
0.2.0.dev0,Define the parameter for the under-sampling
0.2.0.dev0,Create the object
0.2.0.dev0,Define the parameter for the under-sampling
0.2.0.dev0,Create the object
0.2.0.dev0,Fit and sample
0.2.0.dev0,Define the parameter for the under-sampling
0.2.0.dev0,Create the object
0.2.0.dev0,Fit and sample
0.2.0.dev0,Define the parameter for the under-sampling
0.2.0.dev0,Create the object
0.2.0.dev0,Fit and sample
0.2.0.dev0,Create the object
0.2.0.dev0,continuous case
0.2.0.dev0,Generate a global dataset to use
0.2.0.dev0,Define a ratio
0.2.0.dev0,Create the object
0.2.0.dev0,Resample the data
0.2.0.dev0,Create a wrong y
0.2.0.dev0,Create the object
0.2.0.dev0,Fit the data
0.2.0.dev0,Check if the data information have been computed
0.2.0.dev0,Create the object
0.2.0.dev0,Resample the data
0.2.0.dev0,Resample the data
0.2.0.dev0,Create the object
0.2.0.dev0,continuous case
0.2.0.dev0,multiclass case
0.2.0.dev0,Test the various init parameters of the pipeline.
0.2.0.dev0,Check that we can't instantiate pipelines with objects without fit
0.2.0.dev0,method
0.2.0.dev0,Smoke test with only an estimator
0.2.0.dev0,Check that params are set
0.2.0.dev0,Smoke test the repr:
0.2.0.dev0,Test with two objects
0.2.0.dev0,Check that we can't use the same stage name twice
0.2.0.dev0,Check that params are set
0.2.0.dev0,Smoke test the repr:
0.2.0.dev0,Check that params are not set when naming them wrong
0.2.0.dev0,Test clone
0.2.0.dev0,"Check that apart from estimators, the parameters are the same"
0.2.0.dev0,Remove estimators that where copied
0.2.0.dev0,Test the various methods of the pipeline (anova).
0.2.0.dev0,Test with Anova + LogisticRegression
0.2.0.dev0,Test that the pipeline can take fit parameters
0.2.0.dev0,classifier should return True
0.2.0.dev0,and transformer params should not be changed
0.2.0.dev0,Test pipeline raises set params error message for nested models.
0.2.0.dev0,expected error message
0.2.0.dev0,nested model check
0.2.0.dev0,Test the various methods of the pipeline (pca + svm).
0.2.0.dev0,Test with PCA + SVC
0.2.0.dev0,Test the various methods of the pipeline (preprocessing + svm).
0.2.0.dev0,check shapes of various prediction functions
0.2.0.dev0,test that the fit_predict method is implemented on a pipeline
0.2.0.dev0,test that the fit_predict on pipeline yields same results as applying
0.2.0.dev0,transform and clustering steps separately
0.2.0.dev0,first compute the transform and clustering step separately
0.2.0.dev0,use a pipeline to do the transform and clustering in one step
0.2.0.dev0,tests that a pipeline does not have fit_predict method when final
0.2.0.dev0,step of pipeline does not have fit_predict defined
0.2.0.dev0,Test whether pipeline works with a transformer at the end.
0.2.0.dev0,Also test pipeline.transform and pipeline.inverse_transform
0.2.0.dev0,test transform and fit_transform:
0.2.0.dev0,Test whether pipeline works with a transformer missing fit_transform
0.2.0.dev0,test fit_transform:
0.2.0.dev0,Test the various methods of the pipeline (pca + svm).
0.2.0.dev0,Test with PCA + SVC
0.2.0.dev0,Test the various methods of the pipeline (pca + svm).
0.2.0.dev0,Test with PCA + SVC
0.2.0.dev0,Test whether pipeline works with a sampler at the end.
0.2.0.dev0,Also test pipeline.sampler
0.2.0.dev0,test transform and fit_transform:
0.2.0.dev0,Test whether pipeline works with a sampler at the end.
0.2.0.dev0,Also test pipeline.sampler
0.2.0.dev0,Test the various methods of the pipeline (anova).
0.2.0.dev0,Test with RandomUnderSampling + Anova + LogisticRegression
0.2.0.dev0,Fit using SMOTE
0.2.0.dev0,Transform using SMOTE
0.2.0.dev0,Fit and transform using ENN
0.2.0.dev0,Fit using SMOTE
0.2.0.dev0,Transform using SMOTE
0.2.0.dev0,Fit and transform using ENN
0.2.0.dev0,Generate a global dataset to use
0.2.0.dev0,Define a negative ratio
0.2.0.dev0,Define a ratio greater than 1
0.2.0.dev0,Define ratio as an unknown string
0.2.0.dev0,Define ratio as a list which is not supported
0.2.0.dev0,Create the object
0.2.0.dev0,Resample the data
0.2.0.dev0,Create a wrong y
0.2.0.dev0,Create the object
0.2.0.dev0,Fit the data
0.2.0.dev0,Check if the data information have been computed
0.2.0.dev0,Create the object
0.2.0.dev0,Create the object
0.2.0.dev0,Fit the data
0.2.0.dev0,Create the object
0.2.0.dev0,Fit the data
0.2.0.dev0,Create the object
0.2.0.dev0,continuous case
0.2.0.dev0,multiclass case
0.2.0.dev0,Generate a global dataset to use
0.2.0.dev0,Define a negative ratio
0.2.0.dev0,Define a ratio greater than 1
0.2.0.dev0,Define ratio as an unknown string
0.2.0.dev0,Define ratio as a list which is not supported
0.2.0.dev0,Create the object
0.2.0.dev0,Resample the data
0.2.0.dev0,Create a wrong y
0.2.0.dev0,Create the object
0.2.0.dev0,Fit the data
0.2.0.dev0,Check if the data information have been computed
0.2.0.dev0,Create the object
0.2.0.dev0,Create the object
0.2.0.dev0,Fit the data
0.2.0.dev0,Create the object
0.2.0.dev0,Fit the data
0.2.0.dev0,Create the object
0.2.0.dev0,continuous case
0.2.0.dev0,multiclass case
0.2.0.dev0,Check the random state
0.2.0.dev0,Define the classifier to use
0.2.0.dev0,Start with the minority class
0.2.0.dev0,Keep the indices of the minority class somewhere if we need to
0.2.0.dev0,return them later
0.2.0.dev0,Condition to initiliase before the search
0.2.0.dev0,Get the initial number of samples to select in the majority class
0.2.0.dev0,Create the array characterising the array containing the majority
0.2.0.dev0,class
0.2.0.dev0,Loop to create the different subsets
0.2.0.dev0,Generate an appropriate number of index to extract
0.2.0.dev0,from the majority class depending of the false classification
0.2.0.dev0,rate of the previous iteration
0.2.0.dev0,Mark these indexes as not being considered for next sampling
0.2.0.dev0,"For now, we will train and classify on the same data"
0.2.0.dev0,"Let see if we should find another solution. Anyway,"
0.2.0.dev0,random stuff are still random stuff
0.2.0.dev0,Push these data into a new subset
0.2.0.dev0,Apply a bootstrap on x_data
0.2.0.dev0,Train the classifier using the current data
0.2.0.dev0,Train the classifier using the current data
0.2.0.dev0,Predict using only the majority class
0.2.0.dev0,Basically let's find which sample have to be retained for the
0.2.0.dev0,next round
0.2.0.dev0,Find the misclassified index to keep them for the next round
0.2.0.dev0,Count how many random element will be selected
0.2.0.dev0,"We found a new subset, increase the counter"
0.2.0.dev0,Check if we have to make an early stopping
0.2.0.dev0,Select the remaining data
0.2.0.dev0,Select the final batch
0.2.0.dev0,Push these data into a new subset
0.2.0.dev0,"We found a new subset, increase the counter"
0.2.0.dev0,Specific case with n_max_subset = 1
0.2.0.dev0,Also check that we will have enough sample to extract at the
0.2.0.dev0,next round
0.2.0.dev0,Select the remaining data
0.2.0.dev0,Select the final batch
0.2.0.dev0,Push these data into a new subset
0.2.0.dev0,"We found a new subset, increase the counter"
0.2.0.dev0,Generate a global dataset to use
0.2.0.dev0,Define a negative ratio
0.2.0.dev0,Define a ratio greater than 1
0.2.0.dev0,Define ratio as an unknown string
0.2.0.dev0,Define ratio as a list which is not supported
0.2.0.dev0,Define a ratio
0.2.0.dev0,Define the parameter for the under-sampling
0.2.0.dev0,Create the object
0.2.0.dev0,Resample the data
0.2.0.dev0,Create a wrong y
0.2.0.dev0,Create the object
0.2.0.dev0,Fit the data
0.2.0.dev0,Define the parameter for the under-sampling
0.2.0.dev0,Create the object
0.2.0.dev0,Fit the data
0.2.0.dev0,Check if the data information have been computed
0.2.0.dev0,Define the parameter for the under-sampling
0.2.0.dev0,Create the object
0.2.0.dev0,Define the ratio parameter
0.2.0.dev0,Create the sampling object
0.2.0.dev0,Get the different subset
0.2.0.dev0,Check each array
0.2.0.dev0,Define the ratio parameter
0.2.0.dev0,Create the sampling object
0.2.0.dev0,Get the different subset
0.2.0.dev0,Check each array
0.2.0.dev0,Define the ratio parameter
0.2.0.dev0,Create the sampling object
0.2.0.dev0,Get the different subset
0.2.0.dev0,Check each array
0.2.0.dev0,Define the ratio parameter
0.2.0.dev0,Create the sampling object
0.2.0.dev0,Get the different subset
0.2.0.dev0,Check each array
0.2.0.dev0,Define the ratio parameter
0.2.0.dev0,Create the sampling object
0.2.0.dev0,Get the different subset
0.2.0.dev0,Check each array
0.2.0.dev0,Define the ratio parameter
0.2.0.dev0,Create the sampling object
0.2.0.dev0,Get the different subset
0.2.0.dev0,Check each array
0.2.0.dev0,Define the ratio parameter
0.2.0.dev0,Create the sampling object
0.2.0.dev0,Get the different subset
0.2.0.dev0,Check each array
0.2.0.dev0,Define the ratio parameter
0.2.0.dev0,Define the ratio parameter
0.2.0.dev0,Create the sampling object
0.2.0.dev0,Get the different subset
0.2.0.dev0,Check each array
0.2.0.dev0,Define the ratio parameter
0.2.0.dev0,Create the sampling object
0.2.0.dev0,Get the different subset
0.2.0.dev0,Check each array
0.2.0.dev0,Create the object
0.2.0.dev0,continuous case
0.2.0.dev0,multiclass case
0.2.0.dev0,Generate a global dataset to use
0.2.0.dev0,Define a negative ratio
0.2.0.dev0,Define a ratio greater than 1
0.2.0.dev0,Define ratio as an unknown string
0.2.0.dev0,Define ratio as a list which is not supported
0.2.0.dev0,Define a ratio
0.2.0.dev0,Define the parameter for the under-sampling
0.2.0.dev0,Create the object
0.2.0.dev0,Resample the data
0.2.0.dev0,Create a wrong y
0.2.0.dev0,Create the object
0.2.0.dev0,Fit the data
0.2.0.dev0,Define the parameter for the under-sampling
0.2.0.dev0,Create the object
0.2.0.dev0,Fit the data
0.2.0.dev0,Check if the data information have been computed
0.2.0.dev0,Define the parameter for the under-sampling
0.2.0.dev0,Create the object
0.2.0.dev0,Define the ratio parameter
0.2.0.dev0,Create the sampling object
0.2.0.dev0,Get the different subset
0.2.0.dev0,Define the ratio parameter
0.2.0.dev0,Create the sampling object
0.2.0.dev0,Get the different subset
0.2.0.dev0,Define the ratio parameter
0.2.0.dev0,Create the sampling object
0.2.0.dev0,Get the different subset
0.2.0.dev0,Create the object
0.2.0.dev0,continuous case
0.2.0.dev0,Generate a global dataset to use
0.2.0.dev0,Define a zero ratio
0.2.0.dev0,Define a negative ratio
0.2.0.dev0,Define a ratio greater than 1
0.2.0.dev0,Define ratio as a list which is not supported
0.2.0.dev0,Make y to be multiclass
0.2.0.dev0,Resample the data
0.1.7,! /usr/bin/env python
0.1.7,"load all vars into globals, otherwise"
0.1.7,the later function call using global vars doesn't work.
0.1.7,"Allow command-lines such as ""python setup.py build install"""
0.1.7,Make sources available using relative paths from this file's directory.
0.1.7,-*- coding: utf-8 -*-
0.1.7,
0.1.7,"imbalanced-learn documentation build configuration file, created by"
0.1.7,sphinx-quickstart on Mon Jan 18 14:44:12 2016.
0.1.7,
0.1.7,This file is execfile()d with the current directory set to its
0.1.7,containing dir.
0.1.7,
0.1.7,Note that not all possible configuration values are present in this
0.1.7,autogenerated file.
0.1.7,
0.1.7,All configuration values have a default; values that are commented out
0.1.7,serve to show the default.
0.1.7,"If extensions (or modules to document with autodoc) are in another directory,"
0.1.7,add these directories to sys.path here. If the directory is relative to the
0.1.7,"documentation root, use os.path.abspath to make it absolute, like shown here."
0.1.7,"sys.path.insert(0, os.path.abspath('.'))"
0.1.7,-- General configuration ---------------------------------------------------
0.1.7,Try to override the matplotlib configuration as early as possible
0.1.7,-- General configuration ------------------------------------------------
0.1.7,"If your documentation needs a minimal Sphinx version, state it here."
0.1.7,needs_sphinx = '1.0'
0.1.7,"Add any Sphinx extension module names here, as strings. They can be"
0.1.7,extensions coming with Sphinx (named 'sphinx.ext.*') or your custom
0.1.7,ones.
0.1.7,path to your examples scripts
0.1.7,path where to save gallery generated examples
0.1.7,"Add any paths that contain templates here, relative to this directory."
0.1.7,generate autosummary even if no references
0.1.7,The suffix of source filenames.
0.1.7,The encoding of source files.
0.1.7,source_encoding = 'utf-8-sig'
0.1.7,Generate the plots for the gallery
0.1.7,The master toctree document.
0.1.7,General information about the project.
0.1.7,"The version info for the project you're documenting, acts as replacement for"
0.1.7,"|version| and |release|, also used in various other places throughout the"
0.1.7,built documents.
0.1.7,
0.1.7,The short X.Y version.
0.1.7,"The full version, including alpha/beta/rc tags."
0.1.7,The language for content autogenerated by Sphinx. Refer to documentation
0.1.7,for a list of supported languages.
0.1.7,language = None
0.1.7,"There are two options for replacing |today|: either, you set today to some"
0.1.7,"non-false value, then it is used:"
0.1.7,today = ''
0.1.7,"Else, today_fmt is used as the format for a strftime call."
0.1.7,"today_fmt = '%B %d, %Y'"
0.1.7,"List of patterns, relative to source directory, that match files and"
0.1.7,directories to ignore when looking for source files.
0.1.7,The reST default role (used for this markup: `text`) to use for all
0.1.7,documents.
0.1.7,default_role = None
0.1.7,"If true, '()' will be appended to :func: etc. cross-reference text."
0.1.7,"If true, the current module name will be prepended to all description"
0.1.7,unit titles (such as .. function::).
0.1.7,add_module_names = True
0.1.7,"If true, sectionauthor and moduleauthor directives will be shown in the"
0.1.7,output. They are ignored by default.
0.1.7,show_authors = False
0.1.7,The name of the Pygments (syntax highlighting) style to use.
0.1.7,A list of ignored prefixes for module index sorting.
0.1.7,modindex_common_prefix = []
0.1.7,"If true, keep warnings as ""system message"" paragraphs in the built documents."
0.1.7,keep_warnings = False
0.1.7,-- Options for HTML output ----------------------------------------------
0.1.7,The theme to use for HTML and HTML Help pages.  See the documentation for
0.1.7,a list of builtin themes.
0.1.7,Theme options are theme-specific and customize the look and feel of a theme
0.1.7,"further.  For a list of options available for each theme, see the"
0.1.7,documentation.
0.1.7,html_theme_options = {}
0.1.7,"Add any paths that contain custom themes here, relative to this directory."
0.1.7,"The name for this set of Sphinx documents.  If None, it defaults to"
0.1.7,"""<project> v<release> documentation""."
0.1.7,html_title = None
0.1.7,A shorter title for the navigation bar.  Default is the same as html_title.
0.1.7,html_short_title = None
0.1.7,The name of an image file (relative to this directory) to place at the top
0.1.7,of the sidebar.
0.1.7,html_logo = None
0.1.7,The name of an image file (within the static path) to use as favicon of the
0.1.7,docs.  This file should be a Windows icon file (.ico) being 16x16 or 32x32
0.1.7,pixels large.
0.1.7,html_favicon = None
0.1.7,"Add any paths that contain custom static files (such as style sheets) here,"
0.1.7,"relative to this directory. They are copied after the builtin static files,"
0.1.7,"so a file named ""default.css"" will overwrite the builtin ""default.css""."
0.1.7,Add any extra paths that contain custom files (such as robots.txt or
0.1.7,".htaccess) here, relative to this directory. These files are copied"
0.1.7,directly to the root of the documentation.
0.1.7,html_extra_path = []
0.1.7,"If not '', a 'Last updated on:' timestamp is inserted at every page bottom,"
0.1.7,using the given strftime format.
0.1.7,"html_last_updated_fmt = '%b %d, %Y'"
0.1.7,"If true, SmartyPants will be used to convert quotes and dashes to"
0.1.7,typographically correct entities.
0.1.7,html_use_smartypants = True
0.1.7,"Custom sidebar templates, maps document names to template names."
0.1.7,html_sidebars = {}
0.1.7,"Additional templates that should be rendered to pages, maps page names to"
0.1.7,template names.
0.1.7,html_additional_pages = {}
0.1.7,"If false, no module index is generated."
0.1.7,html_domain_indices = True
0.1.7,"If false, no index is generated."
0.1.7,html_use_index = True
0.1.7,"If true, the index is split into individual pages for each letter."
0.1.7,html_split_index = False
0.1.7,"If true, links to the reST sources are added to the pages."
0.1.7,html_show_sourcelink = True
0.1.7,"If true, ""Created using Sphinx"" is shown in the HTML footer. Default is True."
0.1.7,html_show_sphinx = True
0.1.7,"If true, ""(C) Copyright ..."" is shown in the HTML footer. Default is True."
0.1.7,html_show_copyright = True
0.1.7,"If true, an OpenSearch description file will be output, and all pages will"
0.1.7,contain a <link> tag referring to it.  The value of this option must be the
0.1.7,base URL from which the finished HTML is served.
0.1.7,html_use_opensearch = ''
0.1.7,"This is the file name suffix for HTML files (e.g. "".xhtml"")."
0.1.7,html_file_suffix = None
0.1.7,Output file base name for HTML help builder.
0.1.7,-- Options for LaTeX output ---------------------------------------------
0.1.7,The paper size ('letterpaper' or 'a4paper').
0.1.7,"'papersize': 'letterpaper',"
0.1.7,"The font size ('10pt', '11pt' or '12pt')."
0.1.7,"'pointsize': '10pt',"
0.1.7,Additional stuff for the LaTeX preamble.
0.1.7,"'preamble': '',"
0.1.7,Grouping the document tree into LaTeX files. List of tuples
0.1.7,"(source start file, target name, title,"
0.1.7,"author, documentclass [howto, manual, or own class])."
0.1.7,The name of an image file (relative to this directory) to place at the top of
0.1.7,the title page.
0.1.7,latex_logo = None
0.1.7,"For ""manual"" documents, if this is true, then toplevel headings are parts,"
0.1.7,not chapters.
0.1.7,latex_use_parts = False
0.1.7,"If true, show page references after internal links."
0.1.7,latex_show_pagerefs = False
0.1.7,"If true, show URL addresses after external links."
0.1.7,latex_show_urls = False
0.1.7,Documents to append as an appendix to all manuals.
0.1.7,latex_appendices = []
0.1.7,"If false, no module index is generated."
0.1.7,latex_domain_indices = True
0.1.7,-- Options for manual page output ---------------------------------------
0.1.7,One entry per manual page. List of tuples
0.1.7,"(source start file, name, description, authors, manual section)."
0.1.7,"If true, show URL addresses after external links."
0.1.7,man_show_urls = False
0.1.7,-- Options for Texinfo output -------------------------------------------
0.1.7,Grouping the document tree into Texinfo files. List of tuples
0.1.7,"(source start file, target name, title, author,"
0.1.7,"dir menu entry, description, category)"
0.1.7,"generate empty examples files, so that we don't get"
0.1.7,inclusion errors if there are no examples for a class / module
0.1.7,touch file
0.1.7,Documents to append as an appendix to all manuals.
0.1.7,texinfo_appendices = []
0.1.7,"If false, no module index is generated."
0.1.7,texinfo_domain_indices = True
0.1.7,"How to display URL addresses: 'footnote', 'no', or 'inline'."
0.1.7,texinfo_show_urls = 'footnote'
0.1.7,"If true, do not generate a @detailmenu in the ""Top"" node's menu."
0.1.7,texinfo_no_detailmenu = False
0.1.7,Example configuration for intersphinx: refer to the Python standard library.
0.1.7,Define some color for the plotting
0.1.7,Generate the dataset
0.1.7,Instanciate a PCA object for the sake of easy visualisation
0.1.7,Fit and transform x to visualise inside a 2D feature space
0.1.7,Apply SMOTE SVM
0.1.7,"Two subplots, unpack the axes array immediately"
0.1.7,Define some color for the plotting
0.1.7,Generate the dataset
0.1.7,Instanciate a PCA object for the sake of easy visualisation
0.1.7,Fit and transform x to visualise inside a 2D feature space
0.1.7,Apply Borderline SMOTE 2
0.1.7,"Two subplots, unpack the axes array immediately"
0.1.7,Define some color for the plotting
0.1.7,Generate the dataset
0.1.7,Instanciate a PCA object for the sake of easy visualisation
0.1.7,Fit and transform x to visualise inside a 2D feature space
0.1.7,Apply regular SMOTE
0.1.7,"Two subplots, unpack the axes array immediately"
0.1.7,Define some color for the plotting
0.1.7,Generate the dataset
0.1.7,Instanciate a PCA object for the sake of easy visualisation
0.1.7,Fit and transform x to visualise inside a 2D feature space
0.1.7,Apply the random over-sampling
0.1.7,"Two subplots, unpack the axes array immediately"
0.1.7,Define some color for the plotting
0.1.7,Generate the dataset
0.1.7,Instanciate a PCA object for the sake of easy visualisation
0.1.7,Fit and transform x to visualise inside a 2D feature space
0.1.7,Apply Borderline SMOTE 1
0.1.7,"Two subplots, unpack the axes array immediately"
0.1.7,Define some color for the plotting
0.1.7,Generate the dataset
0.1.7,Instanciate a PCA object for the sake of easy visualisation
0.1.7,Fit and transform x to visualise inside a 2D feature space
0.1.7,Apply the random over-sampling
0.1.7,"Two subplots, unpack the axes array immediately"
0.1.7,Define some color for the plotting
0.1.7,Generate the dataset
0.1.7,Instanciate a PCA object for the sake of easy visualisation
0.1.7,Fit and transform x to visualise inside a 2D feature space
0.1.7,Apply SMOTE + ENN
0.1.7,"Two subplots, unpack the axes array immediately"
0.1.7,Define some color for the plotting
0.1.7,Generate the dataset
0.1.7,Instanciate a PCA object for the sake of easy visualisation
0.1.7,Fit and transform x to visualise inside a 2D feature space
0.1.7,Apply SMOTE + Tomek links
0.1.7,"Two subplots, unpack the axes array immediately"
0.1.7,Define some color for the plotting
0.1.7,Generate the dataset
0.1.7,Instanciate a PCA object for the sake of easy visualisation
0.1.7,Fit and transform x to visualise inside a 2D feature space
0.1.7,Apply Balance Cascade method
0.1.7,"Two subplots, unpack the axes array immediately"
0.1.7,Define some color for the plotting
0.1.7,Generate the dataset
0.1.7,Instanciate a PCA object for the sake of easy visualisation
0.1.7,Fit and transform x to visualise inside a 2D feature space
0.1.7,Apply Easy Ensemble
0.1.7,"Two subplots, unpack the axes array immediately"
0.1.7,Define some color for the plotting
0.1.7,Generate the dataset
0.1.7,Instanciate a PCA object for the sake of easy visualisation
0.1.7,Fit and transform x to visualise inside a 2D feature space
0.1.7,"Three subplots, unpack the axes array immediately"
0.1.7,Apply the ENN
0.1.7,Apply the RENN
0.1.7,Define some color for the plotting
0.1.7,Generate the dataset
0.1.7,Instanciate a PCA object for the sake of easy visualisation
0.1.7,Fit and transform x to visualise inside a 2D feature space
0.1.7,Apply Nearmiss 3
0.1.7,"Two subplots, unpack the axes array immediately"
0.1.7,Define some color for the plotting
0.1.7,Generate the dataset
0.1.7,Instanciate a PCA object for the sake of easy visualisation
0.1.7,Fit and transform x to visualise inside a 2D feature space
0.1.7,Apply Edited Nearest Neighbours
0.1.7,"Two subplots, unpack the axes array immediately"
0.1.7,Define some color for the plotting
0.1.7,Generate the dataset
0.1.7,Instanciate a PCA object for the sake of easy visualisation
0.1.7,Fit and transform x to visualise inside a 2D feature space
0.1.7,Apply Tomek Links cleaning
0.1.7,"Two subplots, unpack the axes array immediately"
0.1.7,Define some color for the plotting
0.1.7,Generate the dataset
0.1.7,Instanciate a PCA object for the sake of easy visualisation
0.1.7,Fit and transform x to visualise inside a 2D feature space
0.1.7,Apply One-Sided Selection
0.1.7,"Two subplots, unpack the axes array immediately"
0.1.7,Define some color for the plotting
0.1.7,Generate the dataset
0.1.7,Instanciate a PCA object for the sake of easy visualisation
0.1.7,Fit and transform x to visualise inside a 2D feature space
0.1.7,Apply Nearmiss 2
0.1.7,"Two subplots, unpack the axes array immediately"
0.1.7,Define some color for the plotting
0.1.7,Generate the dataset
0.1.7,Instanciate a PCA object for the sake of easy visualisation
0.1.7,Fit and transform x to visualise inside a 2D feature space
0.1.7,Apply neighbourhood cleaning rule
0.1.7,"Two subplots, unpack the axes array immediately"
0.1.7,Define some color for the plotting
0.1.7,Generate the dataset
0.1.7,Instanciate a PCA object for the sake of easy visualisation
0.1.7,Fit and transform x to visualise inside a 2D feature space
0.1.7,Apply Condensed Nearest Neighbours
0.1.7,"Two subplots, unpack the axes array immediately"
0.1.7,Define some color for the plotting
0.1.7,Generate the dataset
0.1.7,Instanciate a PCA object for the sake of easy visualisation
0.1.7,Fit and transform x to visualise inside a 2D feature space
0.1.7,Apply Cluster Centroids
0.1.7,"Two subplots, unpack the axes array immediately"
0.1.7,Define some color for the plotting
0.1.7,Generate the dataset
0.1.7,Instanciate a PCA object for the sake of easy visualisation
0.1.7,Fit and transform x to visualise inside a 2D feature space
0.1.7,Apply the random under-sampling
0.1.7,"Two subplots, unpack the axes array immediately"
0.1.7,Define some color for the plotting
0.1.7,Generate the dataset
0.1.7,"Two subplots, unpack the axes array immediately"
0.1.7,Define some color for the plotting
0.1.7,Generate the dataset
0.1.7,Instanciate a PCA object for the sake of easy visualisation
0.1.7,Fit and transform x to visualise inside a 2D feature space
0.1.7,Apply Nearmiss 1
0.1.7,"Two subplots, unpack the axes array immediately"
0.1.7,Generate the dataset
0.1.7,Instanciate a PCA object for the sake of easy visualisation
0.1.7,Create the samplers
0.1.7,Create teh classifier
0.1.7,Make the splits
0.1.7,Add one transformers and two samplers in the pipeline object
0.1.7,Based on NiLearn package
0.1.7,License: simplified BSD
0.1.7,"PEP0440 compatible formatted version, see:"
0.1.7,https://www.python.org/dev/peps/pep-0440/
0.1.7,
0.1.7,Generic release markers:
0.1.7,X.Y
0.1.7,X.Y.Z # For bugfix releases
0.1.7,
0.1.7,Admissible pre-release markers:
0.1.7,X.YaN # Alpha release
0.1.7,X.YbN # Beta release
0.1.7,X.YrcN # Release Candidate
0.1.7,X.Y # Final release
0.1.7,
0.1.7,Dev branch marker is: 'X.Y.dev' or 'X.Y.devN' where N is an integer.
0.1.7,'X.Y.dev0' is the canonical version of 'X.Y.dev'
0.1.7,
0.1.7,"This is a tuple to preserve order, so that dependencies are checked"
0.1.7,in some meaningful order (more => less 'core').  We avoid using
0.1.7,collections.OrderedDict to preserve Python 2.6 compatibility.
0.1.7,Avoid choking on modules with no __version__ attribute
0.1.7,Skip check only when installing and it's a module that
0.1.7,will be auto-installed.
0.1.7,Check the consistency of X and y
0.1.7,Get all the unique elements in the target array
0.1.7,# Raise an error if there is only one class
0.1.7,if uniques.size == 1:
0.1.7,"raise RuntimeError(""Only one class detected, aborting..."")"
0.1.7,Raise a warning for the moment to be compatible with BaseEstimator
0.1.7,Store the size of X to check at sampling time if we have the
0.1.7,same data
0.1.7,Create a dictionary containing the class statistics
0.1.7,Find the minority and majority classes
0.1.7,Check if the ratio provided at initialisation make sense
0.1.7,Check the consistency of X and y
0.1.7,Check that the data have been fitted
0.1.7,Check if the size of the data is identical than at fitting
0.1.7,The ratio correspond to the number of samples in the minority class
0.1.7,"over the number of samples in the majority class. Thus, the ratio"
0.1.7,cannot be greater than 1.0
0.1.7,Adapted from scikit-learn
0.1.7,Author: Edouard Duchesnay
0.1.7,Gael Varoquaux
0.1.7,Virgile Fritsch
0.1.7,Alexandre Gramfort
0.1.7,Lars Buitinck
0.1.7,chkoar
0.1.7,License: BSD
0.1.7,BaseEstimator interface
0.1.7,shallow copy of steps
0.1.7,Estimator interface
0.1.7,Boolean controlling whether the joblib caches should be
0.1.7,"flushed if the version of certain modules changes (eg nibabel, as it"
0.1.7,does not respect the backward compatibility in some of its internal
0.1.7,structures
0.1.7,This  is used in nilearn._utils.cache_mixin
0.1.7,list all submodules available in imblearn and version
0.1.7,Keep the samples from the majority class
0.1.7,Loop over the other classes over picking at random
0.1.7,"If this is the majority class, skip it"
0.1.7,Define the number of sample to create
0.1.7,Pick some elements at random
0.1.7,Concatenate to the majority class
0.1.7,Keep the samples from the majority class
0.1.7,Define the number of sample to create
0.1.7,We handle only two classes problem for the moment.
0.1.7,Start by separating minority class features and target values.
0.1.7,Print if verbose is true
0.1.7,"Look for k-th nearest neighbours, excluding, of course, the"
0.1.7,point itself.
0.1.7,Get the distance to the NN
0.1.7,Compute the ratio of majority samples next to minority samples
0.1.7,Check that we found at least some neighbours belonging to the
0.1.7,majority class
0.1.7,Normalize the ratio
0.1.7,Compute the number of sample to be generated
0.1.7,For each minority samples
0.1.7,Pick-up the neighbors wanted
0.1.7,Create a new sample
0.1.7,Find the NN for each samples
0.1.7,Exclude the sample itself
0.1.7,Count how many NN belong to the minority class
0.1.7,Find the class corresponding to the label in x
0.1.7,Compute the number of majority samples in the NN
0.1.7,Samples are in danger for m/2 <= m' < m
0.1.7,Samples are noise for m = m'
0.1.7,Check the consistency of X
0.1.7,Check the random state
0.1.7,A matrix to store the synthetic samples
0.1.7,# Set seeds
0.1.7,"seeds = random_state.randint(low=0,"
0.1.7,"high=100 * len(nn_num.flatten()),"
0.1.7,size=n_samples)
0.1.7,Randomly pick samples to construct neighbours from
0.1.7,Loop over the NN matrix and create new samples
0.1.7,"NN lines relate to original sample, columns to its"
0.1.7,nearest neighbours
0.1.7,"Take a step of random size (0,1) in the direction of the"
0.1.7,n nearest neighbours
0.1.7,if self.random_state is None:
0.1.7,np.random.seed(seeds[i])
0.1.7,else:
0.1.7,np.random.seed(self.random_state)
0.1.7,Construct synthetic sample
0.1.7,The returned target vector is simply a repetition of the
0.1.7,minority label
0.1.7,Define the number of sample to create
0.1.7,We handle only two classes problem for the moment.
0.1.7,Start by separating minority class features and target values.
0.1.7,If regular SMOTE is to be performed
0.1.7,"Look for k-th nearest neighbours, excluding, of course, the"
0.1.7,point itself.
0.1.7,Matrix with k-th nearest neighbours indexes for each minority
0.1.7,element.
0.1.7,--- Generating synthetic samples
0.1.7,Use static method make_samples to generate minority samples
0.1.7,Concatenate the newly generated samples to the original data set
0.1.7,Find the NNs for all samples in the data set.
0.1.7,Boolean array with True for minority samples in danger
0.1.7,"If all minority samples are safe, return the original data set."
0.1.7,"All are safe, nothing to be done here."
0.1.7,"If we got here is because some samples are in danger, we need to"
0.1.7,find the NNs among the minority class to create the new synthetic
0.1.7,samples.
0.1.7,
0.1.7,We start by changing the number of NNs to consider from m + 1
0.1.7,to k + 1
0.1.7,nns...#
0.1.7,B1 and B2 types diverge here!!!
0.1.7,Create synthetic samples for borderline points.
0.1.7,Concatenate the newly generated samples to the original
0.1.7,dataset
0.1.7,Reset the k-neighbours to m+1 neighbours
0.1.7,Split the number of synthetic samples between only minority
0.1.7,"(type 1), or minority and majority (with reduced step size)"
0.1.7,(type 2).
0.1.7,The fraction is sampled from a beta distribution centered
0.1.7,around 0.5 with variance ~0.01
0.1.7,Only minority
0.1.7,Only majority with smaller step size
0.1.7,Concatenate the newly generated samples to the original
0.1.7,data set
0.1.7,Reset the k-neighbours to m+1 neighbours
0.1.7,The SVM smote model fits a support vector machine
0.1.7,classifier to the data and uses the support vector to
0.1.7,"provide a notion of boundary. Unlike regular smote, where"
0.1.7,such notion relies on proportion of nearest neighbours
0.1.7,belonging to each class.
0.1.7,Fit SVM to the full data#
0.1.7,Find the support vectors and their corresponding indexes
0.1.7,"First, find the nn of all the samples to identify samples"
0.1.7,in danger and noisy ones
0.1.7,"As usual, fit a nearest neighbour model to the data"
0.1.7,"Now, get rid of noisy support vectors"
0.1.7,Remove noisy support vectors
0.1.7,Proceed to find support vectors NNs among the minority class
0.1.7,Split the number of synthetic samples between interpolation and
0.1.7,extrapolation
0.1.7,The fraction are sampled from a beta distribution with mean
0.1.7,0.5 and variance 0.01#
0.1.7,Interpolate samples in danger
0.1.7,Extrapolate safe samples
0.1.7,Concatenate the newly generated samples to the original data set
0.1.7,not any support vectors in danger
0.1.7,All the support vector in danger
0.1.7,Reset the k-neighbours to m+1 neighbours
0.1.7,--- NN object
0.1.7,Import the NN object from scikit-learn library. Since in the smote
0.1.7,"variations we must first find samples that are in danger, we"
0.1.7,initialize the NN object differently depending on the method chosen
0.1.7,"Regular smote does not look for samples in danger, instead it"
0.1.7,creates synthetic samples directly from the k-th nearest
0.1.7,neighbours with not filtering
0.1.7,"Borderline1, 2 and SVM variations of smote must first look for"
0.1.7,samples that could be considered noise and samples that live
0.1.7,"near the boundary between the classes. Therefore, before"
0.1.7,"creating synthetic samples from the k-th nns, it first look"
0.1.7,for m nearest neighbors to decide whether or not a sample is
0.1.7,noise or near the boundary.
0.1.7,--- SVM smote
0.1.7,"Unlike the borderline variations, the SVM variation uses the support"
0.1.7,vectors to decide which samples are in danger (near the boundary).
0.1.7,Additionally it also introduces extrapolation for samples that are
0.1.7,considered safe (far from boundary) and interpolation for samples
0.1.7,in danger (near the boundary). The level of extrapolation is
0.1.7,controled by the out_step.
0.1.7,Store SVM object with any parameters
0.1.7,Generate a global dataset to use
0.1.7,Define a negative ratio
0.1.7,Define a ratio greater than 1
0.1.7,Define ratio as an unknown string
0.1.7,Define ratio as a list which is not supported
0.1.7,Define a ratio
0.1.7,Create the object
0.1.7,Resample the data
0.1.7,Create a wrong y
0.1.7,Create the object
0.1.7,Fit the data
0.1.7,Create the object
0.1.7,Fit the data
0.1.7,Check if the data information have been computed
0.1.7,Create the object
0.1.7,Resample the data
0.1.7,Resample the data
0.1.7,Create the object
0.1.7,Generate a global dataset to use
0.1.7,Define a negative ratio
0.1.7,Define a ratio greater than 1
0.1.7,Define ratio as an unknown string
0.1.7,Define ratio as a list which is not supported
0.1.7,Define a ratio
0.1.7,Create the object
0.1.7,Resample the data
0.1.7,Create a wrong y
0.1.7,Create the object
0.1.7,Fit the data
0.1.7,Create the object
0.1.7,Fit the data
0.1.7,Check if the data information have been computed
0.1.7,Create the object
0.1.7,Resample the data
0.1.7,Resample the data
0.1.7,Create the object
0.1.7,Generate a global dataset to use
0.1.7,Define a negative ratio
0.1.7,Define a ratio greater than 1
0.1.7,Define ratio as an unknown string
0.1.7,Define ratio as a list which is not supported
0.1.7,Create the object
0.1.7,Resample the data
0.1.7,Create a wrong y
0.1.7,Create the object
0.1.7,Fit the data
0.1.7,Check if the data information have been computed
0.1.7,Create the object
0.1.7,Create the object
0.1.7,Fit the data
0.1.7,Create the object
0.1.7,Fit the data
0.1.7,Create the object
0.1.7,Fit the data
0.1.7,Create the object
0.1.7,Fit the data
0.1.7,Create the object
0.1.7,Fit the data
0.1.7,Create the object
0.1.7,Start with the minority class
0.1.7,All the minority class samples will be preserved
0.1.7,If we need to offer support for the indices
0.1.7,Loop over the other classes under picking at random
0.1.7,"If the minority class is up, skip it"
0.1.7,Randomly get one sample from the majority class
0.1.7,Generate the index to select
0.1.7,Create the set C
0.1.7,Create the set S
0.1.7,Remove the seed from S since that it will be added anyway
0.1.7,Create a k-NN classifier
0.1.7,Fit C into the knn
0.1.7,Classify on S
0.1.7,Find the misclassified S_y
0.1.7,If we need to offer support for the indices selected
0.1.7,We concatenate the misclassified samples with the seed and the
0.1.7,minority samples
0.1.7,Find the nearest neighbour of every point
0.1.7,Send the information to is_tomek function to get boolean vector back
0.1.7,Check if the indices of the samples selected should be returned too
0.1.7,Return the indices of interest
0.1.7,Return data set without majority Tomek links.
0.1.7,Compute the distance considering the farthest neighbour
0.1.7,Sort the list of distance and get the index
0.1.7,Throw a warning to tell the user that we did not have enough samples
0.1.7,to select and that we just select everything
0.1.7,Select the desired number of samples
0.1.7,Assign the parameter of the element of this class
0.1.7,Check that the version asked is implemented
0.1.7,Start with the minority class
0.1.7,All the minority class samples will be preserved
0.1.7,Compute the number of cluster needed
0.1.7,If we need to offer support for the indices
0.1.7,"For each element of the current class, find the set of NN"
0.1.7,of the minority class
0.1.7,Call the constructor of the NN
0.1.7,Fit the minority class since that we want to know the distance
0.1.7,to these point
0.1.7,Loop over the other classes under picking at random
0.1.7,"If the minority class is up, skip it"
0.1.7,Get the samples corresponding to the current class
0.1.7,Find the NN
0.1.7,Select the right samples
0.1.7,Find the NN
0.1.7,Select the right samples
0.1.7,We need a new NN object to fit the current class
0.1.7,Find the set of NN to the minority class
0.1.7,Create the subset containing the samples found during the NN
0.1.7,search. Linearize the indexes and remove the double values
0.1.7,Create the subset
0.1.7,Compute the NN considering the current class
0.1.7,If we need to offer support for the indices selected
0.1.7,Check if the indices of the samples selected should be returned too
0.1.7,Return the indices of interest
0.1.7,Compute the number of clusters needed
0.1.7,All the minority class samples will be preserved
0.1.7,If we need to offer support for the indices
0.1.7,Loop over the other classes under-picking at random
0.1.7,"If the minority class is up, skip it"
0.1.7,Pick some elements at random
0.1.7,If we need to offer support for the indices selected
0.1.7,Concatenate to the minority class
0.1.7,Check if the indices of the samples selected should be returned as
0.1.7,well
0.1.7,Return the indices of interest
0.1.7,"Initialize the boolean result as false, and also a counter"
0.1.7,Loop through each sample and looks whether it belongs to the minority
0.1.7,"class. If it does, we don't consider it since we want to keep all"
0.1.7,"minority samples. If, however, it belongs to the majority sample we"
0.1.7,look at its first neighbour. If its closest neighbour also has the
0.1.7,"current sample as its closest neighbour, the two form a Tomek link."
0.1.7,"If they form a tomek link, put a True marker on this"
0.1.7,"sample, and increase counter by one."
0.1.7,Find the nearest neighbour of every point
0.1.7,Send the information to is_tomek function to get boolean vector back
0.1.7,Check if the indices of the samples selected should be returned too
0.1.7,Return the indices of interest
0.1.7,Return data set without majority Tomek links.
0.1.7,Start with the minority class
0.1.7,All the minority class samples will be preserved
0.1.7,If we need to offer support for the indices
0.1.7,Loop over the other classes under picking at random
0.1.7,"If the minority class is up, skip it"
0.1.7,Randomly get one sample from the majority class
0.1.7,Generate the index to select
0.1.7,Create the set C - One majority samples and all minority
0.1.7,Create the set S - all majority samples
0.1.7,Create a k-NN classifier
0.1.7,Fit C into the knn
0.1.7,Check each sample in S if we keep it or drop it
0.1.7,Do not select sample which are already well classified
0.1.7,Classify on S
0.1.7,If the prediction do not agree with the true label
0.1.7,append it in C_x
0.1.7,Keep the index for later
0.1.7,Update C
0.1.7,Fit C into the knn
0.1.7,This experimental to speed up the search
0.1.7,Classify all the element in S and avoid to test the
0.1.7,well classified elements
0.1.7,Find the misclassified S_y
0.1.7,"The indexes found are relative to the current class, we need to"
0.1.7,find the absolute value
0.1.7,Build the array with the absolute position
0.1.7,If we need to offer support for the indices selected
0.1.7,Check if the indices of the samples selected should be returned too
0.1.7,Return the indices of interest
0.1.7,Compute the number of cluster needed
0.1.7,Create the clustering object
0.1.7,Start with the minority class
0.1.7,All the minority class samples will be preserved
0.1.7,Loop over the other classes under picking at random
0.1.7,"If the minority class is up, skip it."
0.1.7,Find the centroids via k-means
0.1.7,Concatenate to the minority class
0.1.7,Start with the minority class
0.1.7,All the minority class samples will be preserved
0.1.7,If we need to offer support for the indices
0.1.7,Create a k-NN to fit the whole data
0.1.7,Fit the whole dataset
0.1.7,Loop over the other classes under picking at random
0.1.7,Get the sample of the current class
0.1.7,Get the samples associated
0.1.7,Find the NN for the current class
0.1.7,Get the label of the corresponding to the index
0.1.7,Check which one are the same label than the current class
0.1.7,Make an AND operation through the three neighbours
0.1.7,If the minority class remove the majority samples
0.1.7,Get the index to exclude
0.1.7,Get the index to exclude
0.1.7,Create a vector with the sample to select
0.1.7,Exclude as well the minority sample since that they will be
0.1.7,concatenated later
0.1.7,Get the samples from the majority classes
0.1.7,If we need to offer support for the indices selected
0.1.7,Check if the indices of the samples selected should be returned too
0.1.7,Return the indices of interest
0.1.7,Start with the minority class
0.1.7,All the minority class samples will be preserved
0.1.7,If we need to offer support for the indices
0.1.7,Create a k-NN to fit the whole data
0.1.7,Fit the data
0.1.7,Loop over the other classes under picking at random
0.1.7,"If the minority class is up, skip it"
0.1.7,Get the sample of the current class
0.1.7,Find the NN for the current class
0.1.7,Get the label of the corresponding to the index
0.1.7,Check which one are the same label than the current class
0.1.7,Make the majority vote
0.1.7,Get the samples which agree all together
0.1.7,If we need to offer support for the indices selected
0.1.7,Check if the indices of the samples selected should be returned too
0.1.7,Return the indices of interest
0.1.7,Check the stopping criterion
0.1.7,1. If there is no changes for the vector y
0.1.7,2. If the number of samples in the other class become inferior to
0.1.7,the number of samples in the majority class
0.1.7,3. If one of the class is disappearing
0.1.7,Case 1
0.1.7,Case 2
0.1.7,Get the number of samples in the non-minority classes
0.1.7,Check the minority stop to be the minority
0.1.7,Case 3
0.1.7,"If this is a normal convergence, get the last data"
0.1.7,Log the variables to explain the stop of the algorithm
0.1.7,Update the data for the next iteration
0.1.7,Check if the indices of the samples selected should be returned too
0.1.7,Return the indices of interest
0.1.7,Select the appropriate classifier
0.1.7,Create the different folds
0.1.7,Compute the number of cluster needed
0.1.7,Find the percentile corresponding to the top num_samples
0.1.7,Sample the data
0.1.7,If we need to offer support for the indices
0.1.7,Generate a global dataset to use
0.1.7,Define a negative ratio
0.1.7,Define a ratio greater than 1
0.1.7,Define ratio as an unknown string
0.1.7,Define ratio as a list which is not supported
0.1.7,Define a ratio
0.1.7,Define the parameter for the under-sampling
0.1.7,Create the object
0.1.7,Resample the data
0.1.7,Create a wrong y
0.1.7,Create the object
0.1.7,Fit the data
0.1.7,Define the parameter for the under-sampling
0.1.7,Create the object
0.1.7,Fit the data
0.1.7,Check if the data information have been computed
0.1.7,Define the parameter for the under-sampling
0.1.7,Create the object
0.1.7,Define the parameter for the under-sampling
0.1.7,Create the object
0.1.7,Fit and sample
0.1.7,Define the parameter for the under-sampling
0.1.7,Create the object
0.1.7,Fit and sample
0.1.7,Define the parameter for the under-sampling
0.1.7,Create the object
0.1.7,Fit and sample
0.1.7,Create the object
0.1.7,Generate a global dataset to use
0.1.7,Define a ratio
0.1.7,Create the object
0.1.7,Resample the data
0.1.7,Create a wrong y
0.1.7,Create the object
0.1.7,Fit the data
0.1.7,Check if the data information have been computed
0.1.7,Create the object
0.1.7,Resample the data
0.1.7,Resample the data
0.1.7,Create the object
0.1.7,Generate a global dataset to use
0.1.7,Define a ratio
0.1.7,Create the object
0.1.7,Resample the data
0.1.7,Create a wrong y
0.1.7,Create the object
0.1.7,Fit the data
0.1.7,Check if the data information have been computed
0.1.7,Create the object
0.1.7,Resample the data
0.1.7,Resample the data
0.1.7,Create the object
0.1.7,Generate a global dataset to use
0.1.7,Define a ratio
0.1.7,Create the object
0.1.7,Resample the data
0.1.7,Create a wrong y
0.1.7,Create the object
0.1.7,Fit the data
0.1.7,Check if the data information have been computed
0.1.7,Create the object
0.1.7,Resample the data
0.1.7,Resample the data
0.1.7,Resample the data
0.1.7,Create the object
0.1.7,Generate a global dataset to use
0.1.7,Define a negative ratio
0.1.7,Define a ratio greater than 1
0.1.7,Define ratio as an unknown string
0.1.7,Define ratio as a list which is not supported
0.1.7,Define a ratio
0.1.7,Create the object
0.1.7,Resample the data
0.1.7,Create a wrong y
0.1.7,Create the object
0.1.7,Fit the data
0.1.7,Create the object
0.1.7,Fit the data
0.1.7,Check if the data information have been computed
0.1.7,Create the object
0.1.7,Resample the data
0.1.7,Resample the data
0.1.7,Resample the data
0.1.7,Create the object
0.1.7,Generate a global dataset to use
0.1.7,Define a negative ratio
0.1.7,Define a ratio greater than 1
0.1.7,Define ratio as an unknown string
0.1.7,Define ratio as a list which is not supported
0.1.7,Resample the data
0.1.7,Define a ratio
0.1.7,Create the object
0.1.7,Resample the data
0.1.7,Create a wrong y
0.1.7,Create the object
0.1.7,Fit the data
0.1.7,Create the object
0.1.7,Fit the data
0.1.7,Check if the data information have been computed
0.1.7,Create the object
0.1.7,Resample the data
0.1.7,Resample the data
0.1.7,Resample the data
0.1.7,Resample the data
0.1.7,Resample the data
0.1.7,Resample the data
0.1.7,Resample the data
0.1.7,Resample the data
0.1.7,Resample the data
0.1.7,Create the object
0.1.7,Generate a global dataset to use
0.1.7,Define a ratio
0.1.7,Create the object
0.1.7,Create the object
0.1.7,Resample the data
0.1.7,Create a wrong y
0.1.7,Create the object
0.1.7,Fit the data
0.1.7,Check if the data information have been computed
0.1.7,Create the object
0.1.7,Resample the data
0.1.7,Resample the data
0.1.7,Resample the data
0.1.7,Create the object
0.1.7,Generate a global dataset to use
0.1.7,Define a negative ratio
0.1.7,Define a ratio greater than 1
0.1.7,Define ratio as an unknown string
0.1.7,Define ratio as a list which is not supported
0.1.7,Define a ratio
0.1.7,Define the parameter for the under-sampling
0.1.7,Create the object
0.1.7,Resample the data
0.1.7,Create a wrong y
0.1.7,Create the object
0.1.7,Fit the data
0.1.7,Define the parameter for the under-sampling
0.1.7,Create the object
0.1.7,Fit the data
0.1.7,Check if the data information have been computed
0.1.7,Define the parameter for the under-sampling
0.1.7,Create the object
0.1.7,Define the parameter for the under-sampling
0.1.7,Create the object
0.1.7,Fit and sample
0.1.7,Define the parameter for the under-sampling
0.1.7,Create the object
0.1.7,Fit and sample
0.1.7,Define the parameter for the under-sampling
0.1.7,Create the object
0.1.7,Fit and sample
0.1.7,Create the object
0.1.7,Generate a global dataset to use
0.1.7,Define a ratio
0.1.7,Create the object
0.1.7,Resample the data
0.1.7,Create a wrong y
0.1.7,Create the object
0.1.7,Fit the data
0.1.7,Check if the data information have been computed
0.1.7,Create the object
0.1.7,Resample the data
0.1.7,Resample the data
0.1.7,Create the object
0.1.7,Generate a global dataset to use
0.1.7,Define a negative ratio
0.1.7,Define a ratio greater than 1
0.1.7,Define ratio as an unknown string
0.1.7,Define ratio as a list which is not supported
0.1.7,Define a ratio
0.1.7,Define the parameter for the under-sampling
0.1.7,Create the object
0.1.7,Resample the data
0.1.7,Create a wrong y
0.1.7,Create the object
0.1.7,Fit the data
0.1.7,Define the parameter for the under-sampling
0.1.7,Create the object
0.1.7,Fit the data
0.1.7,Check if the data information have been computed
0.1.7,Define the parameter for the under-sampling
0.1.7,Create the object
0.1.7,Define the parameter for the under-sampling
0.1.7,Create the object
0.1.7,Define the parameter for the under-sampling
0.1.7,Create the object
0.1.7,Fit and sample
0.1.7,Define the parameter for the under-sampling
0.1.7,Create the object
0.1.7,Fit and sample
0.1.7,Generate a global dataset to use
0.1.7,Define a negative ratio
0.1.7,Define a ratio greater than 1
0.1.7,Define ratio as an unknown string
0.1.7,Define ratio as a list which is not supported
0.1.7,Define a ratio
0.1.7,Define the parameter for the under-sampling
0.1.7,Create the object
0.1.7,Resample the data
0.1.7,Create a wrong y
0.1.7,Create the object
0.1.7,Fit the data
0.1.7,Define the parameter for the under-sampling
0.1.7,Create the object
0.1.7,Fit the data
0.1.7,Check if the data information have been computed
0.1.7,Define the parameter for the under-sampling
0.1.7,Create the object
0.1.7,Define the parameter for the under-sampling
0.1.7,Create the object
0.1.7,Fit and sample
0.1.7,Define the parameter for the under-sampling
0.1.7,Create the object
0.1.7,Fit and sample
0.1.7,Define the parameter for the under-sampling
0.1.7,Create the object
0.1.7,Fit and sample
0.1.7,Create the object
0.1.7,Generate a global dataset to use
0.1.7,Define a ratio
0.1.7,Create the object
0.1.7,Resample the data
0.1.7,Create a wrong y
0.1.7,Create the object
0.1.7,Fit the data
0.1.7,Check if the data information have been computed
0.1.7,Create the object
0.1.7,Resample the data
0.1.7,Resample the data
0.1.7,Create the object
0.1.7,Test the various init parameters of the pipeline.
0.1.7,Check that we can't instantiate pipelines with objects without fit
0.1.7,method
0.1.7,Smoke test with only an estimator
0.1.7,Check that params are set
0.1.7,Smoke test the repr:
0.1.7,Test with two objects
0.1.7,Check that we can't use the same stage name twice
0.1.7,Check that params are set
0.1.7,Smoke test the repr:
0.1.7,Check that params are not set when naming them wrong
0.1.7,Test clone
0.1.7,"Check that apart from estimators, the parameters are the same"
0.1.7,Remove estimators that where copied
0.1.7,Test the various methods of the pipeline (anova).
0.1.7,Test with Anova + LogisticRegression
0.1.7,Test that the pipeline can take fit parameters
0.1.7,classifier should return True
0.1.7,and transformer params should not be changed
0.1.7,Test pipeline raises set params error message for nested models.
0.1.7,expected error message
0.1.7,nested model check
0.1.7,Test the various methods of the pipeline (pca + svm).
0.1.7,Test with PCA + SVC
0.1.7,Test the various methods of the pipeline (preprocessing + svm).
0.1.7,check shapes of various prediction functions
0.1.7,test that the fit_predict method is implemented on a pipeline
0.1.7,test that the fit_predict on pipeline yields same results as applying
0.1.7,transform and clustering steps separately
0.1.7,first compute the transform and clustering step separately
0.1.7,use a pipeline to do the transform and clustering in one step
0.1.7,tests that a pipeline does not have fit_predict method when final
0.1.7,step of pipeline does not have fit_predict defined
0.1.7,Test whether pipeline works with a transformer at the end.
0.1.7,Also test pipeline.transform and pipeline.inverse_transform
0.1.7,test transform and fit_transform:
0.1.7,Test whether pipeline works with a transformer missing fit_transform
0.1.7,test fit_transform:
0.1.7,Test the various methods of the pipeline (pca + svm).
0.1.7,Test with PCA + SVC
0.1.7,Test the various methods of the pipeline (pca + svm).
0.1.7,Test with PCA + SVC
0.1.7,Test whether pipeline works with a sampler at the end.
0.1.7,Also test pipeline.sampler
0.1.7,test transform and fit_transform:
0.1.7,Test whether pipeline works with a sampler at the end.
0.1.7,Also test pipeline.sampler
0.1.7,Test the various methods of the pipeline (anova).
0.1.7,Test with RandomUnderSampling + Anova + LogisticRegression
0.1.7,Fit using SMOTE
0.1.7,Transform using SMOTE
0.1.7,Fit and transform using ENN
0.1.7,Fit using SMOTE
0.1.7,Transform using SMOTE
0.1.7,Fit and transform using ENN
0.1.7,Generate a global dataset to use
0.1.7,Define a negative ratio
0.1.7,Define a ratio greater than 1
0.1.7,Define ratio as an unknown string
0.1.7,Define ratio as a list which is not supported
0.1.7,Create the object
0.1.7,Resample the data
0.1.7,Create a wrong y
0.1.7,Create the object
0.1.7,Fit the data
0.1.7,Check if the data information have been computed
0.1.7,Create the object
0.1.7,Create the object
0.1.7,Fit the data
0.1.7,Create the object
0.1.7,Fit the data
0.1.7,Create the object
0.1.7,Generate a global dataset to use
0.1.7,Define a negative ratio
0.1.7,Define a ratio greater than 1
0.1.7,Define ratio as an unknown string
0.1.7,Define ratio as a list which is not supported
0.1.7,Create the object
0.1.7,Resample the data
0.1.7,Create a wrong y
0.1.7,Create the object
0.1.7,Fit the data
0.1.7,Check if the data information have been computed
0.1.7,Create the object
0.1.7,Create the object
0.1.7,Fit the data
0.1.7,Create the object
0.1.7,Fit the data
0.1.7,Create the object
0.1.7,Check the random state
0.1.7,Define the classifier to use
0.1.7,Start with the minority class
0.1.7,Keep the indices of the minority class somewhere if we need to
0.1.7,return them later
0.1.7,Condition to initiliase before the search
0.1.7,Get the initial number of samples to select in the majority class
0.1.7,Create the array characterising the array containing the majority
0.1.7,class
0.1.7,Loop to create the different subsets
0.1.7,Generate an appropriate number of index to extract
0.1.7,from the majority class depending of the false classification
0.1.7,rate of the previous iteration
0.1.7,Mark these indexes as not being considered for next sampling
0.1.7,"For now, we will train and classify on the same data"
0.1.7,"Let see if we should find another solution. Anyway,"
0.1.7,random stuff are still random stuff
0.1.7,Push these data into a new subset
0.1.7,Apply a bootstrap on x_data
0.1.7,Train the classifier using the current data
0.1.7,Train the classifier using the current data
0.1.7,Predict using only the majority class
0.1.7,Basically let's find which sample have to be retained for the
0.1.7,next round
0.1.7,Find the misclassified index to keep them for the next round
0.1.7,Count how many random element will be selected
0.1.7,"We found a new subset, increase the counter"
0.1.7,Check if we have to make an early stopping
0.1.7,Select the remaining data
0.1.7,Select the final batch
0.1.7,Push these data into a new subset
0.1.7,"We found a new subset, increase the counter"
0.1.7,Specific case with n_max_subset = 1
0.1.7,Also check that we will have enough sample to extract at the
0.1.7,next round
0.1.7,Select the remaining data
0.1.7,Select the final batch
0.1.7,Push these data into a new subset
0.1.7,"We found a new subset, increase the counter"
0.1.7,Generate a global dataset to use
0.1.7,Define a negative ratio
0.1.7,Define a ratio greater than 1
0.1.7,Define ratio as an unknown string
0.1.7,Define ratio as a list which is not supported
0.1.7,Define a ratio
0.1.7,Define the parameter for the under-sampling
0.1.7,Create the object
0.1.7,Resample the data
0.1.7,Create a wrong y
0.1.7,Create the object
0.1.7,Fit the data
0.1.7,Define the parameter for the under-sampling
0.1.7,Create the object
0.1.7,Fit the data
0.1.7,Check if the data information have been computed
0.1.7,Define the parameter for the under-sampling
0.1.7,Create the object
0.1.7,Define the ratio parameter
0.1.7,Create the sampling object
0.1.7,Get the different subset
0.1.7,Check each array
0.1.7,Define the ratio parameter
0.1.7,Create the sampling object
0.1.7,Get the different subset
0.1.7,Check each array
0.1.7,Define the ratio parameter
0.1.7,Create the sampling object
0.1.7,Get the different subset
0.1.7,Check each array
0.1.7,Define the ratio parameter
0.1.7,Create the sampling object
0.1.7,Get the different subset
0.1.7,Check each array
0.1.7,Define the ratio parameter
0.1.7,Create the sampling object
0.1.7,Get the different subset
0.1.7,Check each array
0.1.7,Define the ratio parameter
0.1.7,Create the sampling object
0.1.7,Get the different subset
0.1.7,Check each array
0.1.7,Define the ratio parameter
0.1.7,Create the sampling object
0.1.7,Get the different subset
0.1.7,Check each array
0.1.7,Define the ratio parameter
0.1.7,Define the ratio parameter
0.1.7,Create the sampling object
0.1.7,Get the different subset
0.1.7,Check each array
0.1.7,Create the object
0.1.7,Generate a global dataset to use
0.1.7,Define a negative ratio
0.1.7,Define a ratio greater than 1
0.1.7,Define ratio as an unknown string
0.1.7,Define ratio as a list which is not supported
0.1.7,Define a ratio
0.1.7,Define the parameter for the under-sampling
0.1.7,Create the object
0.1.7,Resample the data
0.1.7,Create a wrong y
0.1.7,Create the object
0.1.7,Fit the data
0.1.7,Define the parameter for the under-sampling
0.1.7,Create the object
0.1.7,Fit the data
0.1.7,Check if the data information have been computed
0.1.7,Define the parameter for the under-sampling
0.1.7,Create the object
0.1.7,Define the ratio parameter
0.1.7,Create the sampling object
0.1.7,Get the different subset
0.1.7,Define the ratio parameter
0.1.7,Create the sampling object
0.1.7,Get the different subset
0.1.7,Define the ratio parameter
0.1.7,Create the sampling object
0.1.7,Get the different subset
0.1.7,Create the object
0.1.6,! /usr/bin/env python
0.1.6,"load all vars into globals, otherwise"
0.1.6,the later function call using global vars doesn't work.
0.1.6,"Allow command-lines such as ""python setup.py build install"""
0.1.6,Make sources available using relative paths from this file's directory.
0.1.6,-*- coding: utf-8 -*-
0.1.6,
0.1.6,"imbalanced-learn documentation build configuration file, created by"
0.1.6,sphinx-quickstart on Mon Jan 18 14:44:12 2016.
0.1.6,
0.1.6,This file is execfile()d with the current directory set to its
0.1.6,containing dir.
0.1.6,
0.1.6,Note that not all possible configuration values are present in this
0.1.6,autogenerated file.
0.1.6,
0.1.6,All configuration values have a default; values that are commented out
0.1.6,serve to show the default.
0.1.6,"If extensions (or modules to document with autodoc) are in another directory,"
0.1.6,add these directories to sys.path here. If the directory is relative to the
0.1.6,"documentation root, use os.path.abspath to make it absolute, like shown here."
0.1.6,"sys.path.insert(0, os.path.abspath('.'))"
0.1.6,-- General configuration ---------------------------------------------------
0.1.6,Try to override the matplotlib configuration as early as possible
0.1.6,-- General configuration ------------------------------------------------
0.1.6,"If your documentation needs a minimal Sphinx version, state it here."
0.1.6,needs_sphinx = '1.0'
0.1.6,"Add any Sphinx extension module names here, as strings. They can be"
0.1.6,extensions coming with Sphinx (named 'sphinx.ext.*') or your custom
0.1.6,ones.
0.1.6,path to your examples scripts
0.1.6,path where to save gallery generated examples
0.1.6,"Add any paths that contain templates here, relative to this directory."
0.1.6,generate autosummary even if no references
0.1.6,The suffix of source filenames.
0.1.6,The encoding of source files.
0.1.6,source_encoding = 'utf-8-sig'
0.1.6,Generate the plots for the gallery
0.1.6,The master toctree document.
0.1.6,General information about the project.
0.1.6,"The version info for the project you're documenting, acts as replacement for"
0.1.6,"|version| and |release|, also used in various other places throughout the"
0.1.6,built documents.
0.1.6,
0.1.6,The short X.Y version.
0.1.6,"The full version, including alpha/beta/rc tags."
0.1.6,The language for content autogenerated by Sphinx. Refer to documentation
0.1.6,for a list of supported languages.
0.1.6,language = None
0.1.6,"There are two options for replacing |today|: either, you set today to some"
0.1.6,"non-false value, then it is used:"
0.1.6,today = ''
0.1.6,"Else, today_fmt is used as the format for a strftime call."
0.1.6,"today_fmt = '%B %d, %Y'"
0.1.6,"List of patterns, relative to source directory, that match files and"
0.1.6,directories to ignore when looking for source files.
0.1.6,The reST default role (used for this markup: `text`) to use for all
0.1.6,documents.
0.1.6,default_role = None
0.1.6,"If true, '()' will be appended to :func: etc. cross-reference text."
0.1.6,"If true, the current module name will be prepended to all description"
0.1.6,unit titles (such as .. function::).
0.1.6,add_module_names = True
0.1.6,"If true, sectionauthor and moduleauthor directives will be shown in the"
0.1.6,output. They are ignored by default.
0.1.6,show_authors = False
0.1.6,The name of the Pygments (syntax highlighting) style to use.
0.1.6,A list of ignored prefixes for module index sorting.
0.1.6,modindex_common_prefix = []
0.1.6,"If true, keep warnings as ""system message"" paragraphs in the built documents."
0.1.6,keep_warnings = False
0.1.6,-- Options for HTML output ----------------------------------------------
0.1.6,The theme to use for HTML and HTML Help pages.  See the documentation for
0.1.6,a list of builtin themes.
0.1.6,Theme options are theme-specific and customize the look and feel of a theme
0.1.6,"further.  For a list of options available for each theme, see the"
0.1.6,documentation.
0.1.6,html_theme_options = {}
0.1.6,"Add any paths that contain custom themes here, relative to this directory."
0.1.6,"The name for this set of Sphinx documents.  If None, it defaults to"
0.1.6,"""<project> v<release> documentation""."
0.1.6,html_title = None
0.1.6,A shorter title for the navigation bar.  Default is the same as html_title.
0.1.6,html_short_title = None
0.1.6,The name of an image file (relative to this directory) to place at the top
0.1.6,of the sidebar.
0.1.6,html_logo = None
0.1.6,The name of an image file (within the static path) to use as favicon of the
0.1.6,docs.  This file should be a Windows icon file (.ico) being 16x16 or 32x32
0.1.6,pixels large.
0.1.6,html_favicon = None
0.1.6,"Add any paths that contain custom static files (such as style sheets) here,"
0.1.6,"relative to this directory. They are copied after the builtin static files,"
0.1.6,"so a file named ""default.css"" will overwrite the builtin ""default.css""."
0.1.6,Add any extra paths that contain custom files (such as robots.txt or
0.1.6,".htaccess) here, relative to this directory. These files are copied"
0.1.6,directly to the root of the documentation.
0.1.6,html_extra_path = []
0.1.6,"If not '', a 'Last updated on:' timestamp is inserted at every page bottom,"
0.1.6,using the given strftime format.
0.1.6,"html_last_updated_fmt = '%b %d, %Y'"
0.1.6,"If true, SmartyPants will be used to convert quotes and dashes to"
0.1.6,typographically correct entities.
0.1.6,html_use_smartypants = True
0.1.6,"Custom sidebar templates, maps document names to template names."
0.1.6,html_sidebars = {}
0.1.6,"Additional templates that should be rendered to pages, maps page names to"
0.1.6,template names.
0.1.6,html_additional_pages = {}
0.1.6,"If false, no module index is generated."
0.1.6,html_domain_indices = True
0.1.6,"If false, no index is generated."
0.1.6,html_use_index = True
0.1.6,"If true, the index is split into individual pages for each letter."
0.1.6,html_split_index = False
0.1.6,"If true, links to the reST sources are added to the pages."
0.1.6,html_show_sourcelink = True
0.1.6,"If true, ""Created using Sphinx"" is shown in the HTML footer. Default is True."
0.1.6,html_show_sphinx = True
0.1.6,"If true, ""(C) Copyright ..."" is shown in the HTML footer. Default is True."
0.1.6,html_show_copyright = True
0.1.6,"If true, an OpenSearch description file will be output, and all pages will"
0.1.6,contain a <link> tag referring to it.  The value of this option must be the
0.1.6,base URL from which the finished HTML is served.
0.1.6,html_use_opensearch = ''
0.1.6,"This is the file name suffix for HTML files (e.g. "".xhtml"")."
0.1.6,html_file_suffix = None
0.1.6,Output file base name for HTML help builder.
0.1.6,-- Options for LaTeX output ---------------------------------------------
0.1.6,The paper size ('letterpaper' or 'a4paper').
0.1.6,"'papersize': 'letterpaper',"
0.1.6,"The font size ('10pt', '11pt' or '12pt')."
0.1.6,"'pointsize': '10pt',"
0.1.6,Additional stuff for the LaTeX preamble.
0.1.6,"'preamble': '',"
0.1.6,Grouping the document tree into LaTeX files. List of tuples
0.1.6,"(source start file, target name, title,"
0.1.6,"author, documentclass [howto, manual, or own class])."
0.1.6,The name of an image file (relative to this directory) to place at the top of
0.1.6,the title page.
0.1.6,latex_logo = None
0.1.6,"For ""manual"" documents, if this is true, then toplevel headings are parts,"
0.1.6,not chapters.
0.1.6,latex_use_parts = False
0.1.6,"If true, show page references after internal links."
0.1.6,latex_show_pagerefs = False
0.1.6,"If true, show URL addresses after external links."
0.1.6,latex_show_urls = False
0.1.6,Documents to append as an appendix to all manuals.
0.1.6,latex_appendices = []
0.1.6,"If false, no module index is generated."
0.1.6,latex_domain_indices = True
0.1.6,-- Options for manual page output ---------------------------------------
0.1.6,One entry per manual page. List of tuples
0.1.6,"(source start file, name, description, authors, manual section)."
0.1.6,"If true, show URL addresses after external links."
0.1.6,man_show_urls = False
0.1.6,-- Options for Texinfo output -------------------------------------------
0.1.6,Grouping the document tree into Texinfo files. List of tuples
0.1.6,"(source start file, target name, title, author,"
0.1.6,"dir menu entry, description, category)"
0.1.6,"generate empty examples files, so that we don't get"
0.1.6,inclusion errors if there are no examples for a class / module
0.1.6,touch file
0.1.6,Documents to append as an appendix to all manuals.
0.1.6,texinfo_appendices = []
0.1.6,"If false, no module index is generated."
0.1.6,texinfo_domain_indices = True
0.1.6,"How to display URL addresses: 'footnote', 'no', or 'inline'."
0.1.6,texinfo_show_urls = 'footnote'
0.1.6,"If true, do not generate a @detailmenu in the ""Top"" node's menu."
0.1.6,texinfo_no_detailmenu = False
0.1.6,Example configuration for intersphinx: refer to the Python standard library.
0.1.6,Define some color for the plotting
0.1.6,Generate the dataset
0.1.6,Instanciate a PCA object for the sake of easy visualisation
0.1.6,Fit and transform x to visualise inside a 2D feature space
0.1.6,Apply SMOTE SVM
0.1.6,"Two subplots, unpack the axes array immediately"
0.1.6,Define some color for the plotting
0.1.6,Generate the dataset
0.1.6,Instanciate a PCA object for the sake of easy visualisation
0.1.6,Fit and transform x to visualise inside a 2D feature space
0.1.6,Apply Borderline SMOTE 2
0.1.6,"Two subplots, unpack the axes array immediately"
0.1.6,Define some color for the plotting
0.1.6,Generate the dataset
0.1.6,Instanciate a PCA object for the sake of easy visualisation
0.1.6,Fit and transform x to visualise inside a 2D feature space
0.1.6,Apply regular SMOTE
0.1.6,"Two subplots, unpack the axes array immediately"
0.1.6,Define some color for the plotting
0.1.6,Generate the dataset
0.1.6,Instanciate a PCA object for the sake of easy visualisation
0.1.6,Fit and transform x to visualise inside a 2D feature space
0.1.6,Apply the random over-sampling
0.1.6,"Two subplots, unpack the axes array immediately"
0.1.6,Define some color for the plotting
0.1.6,Generate the dataset
0.1.6,Instanciate a PCA object for the sake of easy visualisation
0.1.6,Fit and transform x to visualise inside a 2D feature space
0.1.6,Apply Borderline SMOTE 1
0.1.6,"Two subplots, unpack the axes array immediately"
0.1.6,Define some color for the plotting
0.1.6,Generate the dataset
0.1.6,Instanciate a PCA object for the sake of easy visualisation
0.1.6,Fit and transform x to visualise inside a 2D feature space
0.1.6,Apply the random over-sampling
0.1.6,"Two subplots, unpack the axes array immediately"
0.1.6,Define some color for the plotting
0.1.6,Generate the dataset
0.1.6,Instanciate a PCA object for the sake of easy visualisation
0.1.6,Fit and transform x to visualise inside a 2D feature space
0.1.6,Apply SMOTE + ENN
0.1.6,"Two subplots, unpack the axes array immediately"
0.1.6,Define some color for the plotting
0.1.6,Generate the dataset
0.1.6,Instanciate a PCA object for the sake of easy visualisation
0.1.6,Fit and transform x to visualise inside a 2D feature space
0.1.6,Apply SMOTE + Tomek links
0.1.6,"Two subplots, unpack the axes array immediately"
0.1.6,Define some color for the plotting
0.1.6,Generate the dataset
0.1.6,Instanciate a PCA object for the sake of easy visualisation
0.1.6,Fit and transform x to visualise inside a 2D feature space
0.1.6,Apply Balance Cascade method
0.1.6,"Two subplots, unpack the axes array immediately"
0.1.6,Define some color for the plotting
0.1.6,Generate the dataset
0.1.6,Instanciate a PCA object for the sake of easy visualisation
0.1.6,Fit and transform x to visualise inside a 2D feature space
0.1.6,Apply Easy Ensemble
0.1.6,"Two subplots, unpack the axes array immediately"
0.1.6,Define some color for the plotting
0.1.6,Generate the dataset
0.1.6,Instanciate a PCA object for the sake of easy visualisation
0.1.6,Fit and transform x to visualise inside a 2D feature space
0.1.6,"Three subplots, unpack the axes array immediately"
0.1.6,Apply the ENN
0.1.6,Apply the RENN
0.1.6,Define some color for the plotting
0.1.6,Generate the dataset
0.1.6,Instanciate a PCA object for the sake of easy visualisation
0.1.6,Fit and transform x to visualise inside a 2D feature space
0.1.6,Apply Nearmiss 3
0.1.6,"Two subplots, unpack the axes array immediately"
0.1.6,Define some color for the plotting
0.1.6,Generate the dataset
0.1.6,Instanciate a PCA object for the sake of easy visualisation
0.1.6,Fit and transform x to visualise inside a 2D feature space
0.1.6,Apply Edited Nearest Neighbours
0.1.6,"Two subplots, unpack the axes array immediately"
0.1.6,Define some color for the plotting
0.1.6,Generate the dataset
0.1.6,Instanciate a PCA object for the sake of easy visualisation
0.1.6,Fit and transform x to visualise inside a 2D feature space
0.1.6,Apply Tomek Links cleaning
0.1.6,"Two subplots, unpack the axes array immediately"
0.1.6,Define some color for the plotting
0.1.6,Generate the dataset
0.1.6,Instanciate a PCA object for the sake of easy visualisation
0.1.6,Fit and transform x to visualise inside a 2D feature space
0.1.6,Apply One-Sided Selection
0.1.6,"Two subplots, unpack the axes array immediately"
0.1.6,Define some color for the plotting
0.1.6,Generate the dataset
0.1.6,Instanciate a PCA object for the sake of easy visualisation
0.1.6,Fit and transform x to visualise inside a 2D feature space
0.1.6,Apply Nearmiss 2
0.1.6,"Two subplots, unpack the axes array immediately"
0.1.6,Define some color for the plotting
0.1.6,Generate the dataset
0.1.6,Instanciate a PCA object for the sake of easy visualisation
0.1.6,Fit and transform x to visualise inside a 2D feature space
0.1.6,Apply neighbourhood cleaning rule
0.1.6,"Two subplots, unpack the axes array immediately"
0.1.6,Define some color for the plotting
0.1.6,Generate the dataset
0.1.6,Instanciate a PCA object for the sake of easy visualisation
0.1.6,Fit and transform x to visualise inside a 2D feature space
0.1.6,Apply Condensed Nearest Neighbours
0.1.6,"Two subplots, unpack the axes array immediately"
0.1.6,Define some color for the plotting
0.1.6,Generate the dataset
0.1.6,Instanciate a PCA object for the sake of easy visualisation
0.1.6,Fit and transform x to visualise inside a 2D feature space
0.1.6,Apply Cluster Centroids
0.1.6,"Two subplots, unpack the axes array immediately"
0.1.6,Define some color for the plotting
0.1.6,Generate the dataset
0.1.6,Instanciate a PCA object for the sake of easy visualisation
0.1.6,Fit and transform x to visualise inside a 2D feature space
0.1.6,Apply the random under-sampling
0.1.6,"Two subplots, unpack the axes array immediately"
0.1.6,Define some color for the plotting
0.1.6,Generate the dataset
0.1.6,"Two subplots, unpack the axes array immediately"
0.1.6,Define some color for the plotting
0.1.6,Generate the dataset
0.1.6,Instanciate a PCA object for the sake of easy visualisation
0.1.6,Fit and transform x to visualise inside a 2D feature space
0.1.6,Apply Nearmiss 1
0.1.6,"Two subplots, unpack the axes array immediately"
0.1.6,Generate the dataset
0.1.6,Instanciate a PCA object for the sake of easy visualisation
0.1.6,Create the samplers
0.1.6,Create teh classifier
0.1.6,Make the splits
0.1.6,Add one transformers and two samplers in the pipeline object
0.1.6,Based on NiLearn package
0.1.6,License: simplified BSD
0.1.6,"PEP0440 compatible formatted version, see:"
0.1.6,https://www.python.org/dev/peps/pep-0440/
0.1.6,
0.1.6,Generic release markers:
0.1.6,X.Y
0.1.6,X.Y.Z # For bugfix releases
0.1.6,
0.1.6,Admissible pre-release markers:
0.1.6,X.YaN # Alpha release
0.1.6,X.YbN # Beta release
0.1.6,X.YrcN # Release Candidate
0.1.6,X.Y # Final release
0.1.6,
0.1.6,Dev branch marker is: 'X.Y.dev' or 'X.Y.devN' where N is an integer.
0.1.6,'X.Y.dev0' is the canonical version of 'X.Y.dev'
0.1.6,
0.1.6,"This is a tuple to preserve order, so that dependencies are checked"
0.1.6,in some meaningful order (more => less 'core').  We avoid using
0.1.6,collections.OrderedDict to preserve Python 2.6 compatibility.
0.1.6,Avoid choking on modules with no __version__ attribute
0.1.6,Skip check only when installing and it's a module that
0.1.6,will be auto-installed.
0.1.6,Check the consistency of X and y
0.1.6,Get all the unique elements in the target array
0.1.6,# Raise an error if there is only one class
0.1.6,if uniques.size == 1:
0.1.6,"raise RuntimeError(""Only one class detected, aborting..."")"
0.1.6,Raise a warning for the moment to be compatible with BaseEstimator
0.1.6,Store the size of X to check at sampling time if we have the
0.1.6,same data
0.1.6,Create a dictionary containing the class statistics
0.1.6,Find the minority and majority classes
0.1.6,Check if the ratio provided at initialisation make sense
0.1.6,Check the consistency of X and y
0.1.6,Check that the data have been fitted
0.1.6,Check if the size of the data is identical than at fitting
0.1.6,The ratio correspond to the number of samples in the minority class
0.1.6,"over the number of samples in the majority class. Thus, the ratio"
0.1.6,cannot be greater than 1.0
0.1.6,Adapted from scikit-learn
0.1.6,Author: Edouard Duchesnay
0.1.6,Gael Varoquaux
0.1.6,Virgile Fritsch
0.1.6,Alexandre Gramfort
0.1.6,Lars Buitinck
0.1.6,chkoar
0.1.6,License: BSD
0.1.6,BaseEstimator interface
0.1.6,shallow copy of steps
0.1.6,Estimator interface
0.1.6,Boolean controlling whether the joblib caches should be
0.1.6,"flushed if the version of certain modules changes (eg nibabel, as it"
0.1.6,does not respect the backward compatibility in some of its internal
0.1.6,structures
0.1.6,This  is used in nilearn._utils.cache_mixin
0.1.6,list all submodules available in imblearn and version
0.1.6,Keep the samples from the majority class
0.1.6,Loop over the other classes over picking at random
0.1.6,"If this is the majority class, skip it"
0.1.6,Define the number of sample to create
0.1.6,Pick some elements at random
0.1.6,Concatenate to the majority class
0.1.6,Keep the samples from the majority class
0.1.6,Define the number of sample to create
0.1.6,We handle only two classes problem for the moment.
0.1.6,Start by separating minority class features and target values.
0.1.6,Print if verbose is true
0.1.6,"Look for k-th nearest neighbours, excluding, of course, the"
0.1.6,point itself.
0.1.6,Get the distance to the NN
0.1.6,Compute the ratio of majority samples next to minority samples
0.1.6,Check that we found at least some neighbours belonging to the
0.1.6,majority class
0.1.6,Normalize the ratio
0.1.6,Compute the number of sample to be generated
0.1.6,For each minority samples
0.1.6,Pick-up the neighbors wanted
0.1.6,Create a new sample
0.1.6,Find the NN for each samples
0.1.6,Exclude the sample itself
0.1.6,Count how many NN belong to the minority class
0.1.6,Find the class corresponding to the label in x
0.1.6,Compute the number of majority samples in the NN
0.1.6,Samples are in danger for m/2 <= m' < m
0.1.6,Samples are noise for m = m'
0.1.6,Check the consistency of X
0.1.6,Check the random state
0.1.6,A matrix to store the synthetic samples
0.1.6,# Set seeds
0.1.6,"seeds = random_state.randint(low=0,"
0.1.6,"high=100 * len(nn_num.flatten()),"
0.1.6,size=n_samples)
0.1.6,Randomly pick samples to construct neighbours from
0.1.6,Loop over the NN matrix and create new samples
0.1.6,"NN lines relate to original sample, columns to its"
0.1.6,nearest neighbours
0.1.6,"Take a step of random size (0,1) in the direction of the"
0.1.6,n nearest neighbours
0.1.6,if self.random_state is None:
0.1.6,np.random.seed(seeds[i])
0.1.6,else:
0.1.6,np.random.seed(self.random_state)
0.1.6,Construct synthetic sample
0.1.6,The returned target vector is simply a repetition of the
0.1.6,minority label
0.1.6,Define the number of sample to create
0.1.6,We handle only two classes problem for the moment.
0.1.6,Start by separating minority class features and target values.
0.1.6,If regular SMOTE is to be performed
0.1.6,"Look for k-th nearest neighbours, excluding, of course, the"
0.1.6,point itself.
0.1.6,Matrix with k-th nearest neighbours indexes for each minority
0.1.6,element.
0.1.6,--- Generating synthetic samples
0.1.6,Use static method make_samples to generate minority samples
0.1.6,Concatenate the newly generated samples to the original data set
0.1.6,Find the NNs for all samples in the data set.
0.1.6,Boolean array with True for minority samples in danger
0.1.6,"If all minority samples are safe, return the original data set."
0.1.6,"All are safe, nothing to be done here."
0.1.6,"If we got here is because some samples are in danger, we need to"
0.1.6,find the NNs among the minority class to create the new synthetic
0.1.6,samples.
0.1.6,
0.1.6,We start by changing the number of NNs to consider from m + 1
0.1.6,to k + 1
0.1.6,nns...#
0.1.6,B1 and B2 types diverge here!!!
0.1.6,Create synthetic samples for borderline points.
0.1.6,Concatenate the newly generated samples to the original
0.1.6,dataset
0.1.6,Reset the k-neighbours to m+1 neighbours
0.1.6,Split the number of synthetic samples between only minority
0.1.6,"(type 1), or minority and majority (with reduced step size)"
0.1.6,(type 2).
0.1.6,The fraction is sampled from a beta distribution centered
0.1.6,around 0.5 with variance ~0.01
0.1.6,Only minority
0.1.6,Only majority with smaller step size
0.1.6,Concatenate the newly generated samples to the original
0.1.6,data set
0.1.6,Reset the k-neighbours to m+1 neighbours
0.1.6,The SVM smote model fits a support vector machine
0.1.6,classifier to the data and uses the support vector to
0.1.6,"provide a notion of boundary. Unlike regular smote, where"
0.1.6,such notion relies on proportion of nearest neighbours
0.1.6,belonging to each class.
0.1.6,Fit SVM to the full data#
0.1.6,Find the support vectors and their corresponding indexes
0.1.6,"First, find the nn of all the samples to identify samples"
0.1.6,in danger and noisy ones
0.1.6,"As usual, fit a nearest neighbour model to the data"
0.1.6,"Now, get rid of noisy support vectors"
0.1.6,Remove noisy support vectors
0.1.6,Proceed to find support vectors NNs among the minority class
0.1.6,Split the number of synthetic samples between interpolation and
0.1.6,extrapolation
0.1.6,The fraction are sampled from a beta distribution with mean
0.1.6,0.5 and variance 0.01#
0.1.6,Interpolate samples in danger
0.1.6,Extrapolate safe samples
0.1.6,Concatenate the newly generated samples to the original data set
0.1.6,not any support vectors in danger
0.1.6,All the support vector in danger
0.1.6,Reset the k-neighbours to m+1 neighbours
0.1.6,--- NN object
0.1.6,Import the NN object from scikit-learn library. Since in the smote
0.1.6,"variations we must first find samples that are in danger, we"
0.1.6,initialize the NN object differently depending on the method chosen
0.1.6,"Regular smote does not look for samples in danger, instead it"
0.1.6,creates synthetic samples directly from the k-th nearest
0.1.6,neighbours with not filtering
0.1.6,"Borderline1, 2 and SVM variations of smote must first look for"
0.1.6,samples that could be considered noise and samples that live
0.1.6,"near the boundary between the classes. Therefore, before"
0.1.6,"creating synthetic samples from the k-th nns, it first look"
0.1.6,for m nearest neighbors to decide whether or not a sample is
0.1.6,noise or near the boundary.
0.1.6,--- SVM smote
0.1.6,"Unlike the borderline variations, the SVM variation uses the support"
0.1.6,vectors to decide which samples are in danger (near the boundary).
0.1.6,Additionally it also introduces extrapolation for samples that are
0.1.6,considered safe (far from boundary) and interpolation for samples
0.1.6,in danger (near the boundary). The level of extrapolation is
0.1.6,controled by the out_step.
0.1.6,Store SVM object with any parameters
0.1.6,Generate a global dataset to use
0.1.6,Define a negative ratio
0.1.6,Define a ratio greater than 1
0.1.6,Define ratio as an unknown string
0.1.6,Define ratio as a list which is not supported
0.1.6,Define a ratio
0.1.6,Create the object
0.1.6,Resample the data
0.1.6,Create a wrong y
0.1.6,Create the object
0.1.6,Fit the data
0.1.6,Create the object
0.1.6,Fit the data
0.1.6,Check if the data information have been computed
0.1.6,Create the object
0.1.6,Resample the data
0.1.6,Resample the data
0.1.6,Create the object
0.1.6,Generate a global dataset to use
0.1.6,Define a negative ratio
0.1.6,Define a ratio greater than 1
0.1.6,Define ratio as an unknown string
0.1.6,Define ratio as a list which is not supported
0.1.6,Define a ratio
0.1.6,Create the object
0.1.6,Resample the data
0.1.6,Create a wrong y
0.1.6,Create the object
0.1.6,Fit the data
0.1.6,Create the object
0.1.6,Fit the data
0.1.6,Check if the data information have been computed
0.1.6,Create the object
0.1.6,Resample the data
0.1.6,Resample the data
0.1.6,Create the object
0.1.6,Generate a global dataset to use
0.1.6,Define a negative ratio
0.1.6,Define a ratio greater than 1
0.1.6,Define ratio as an unknown string
0.1.6,Define ratio as a list which is not supported
0.1.6,Create the object
0.1.6,Resample the data
0.1.6,Create a wrong y
0.1.6,Create the object
0.1.6,Fit the data
0.1.6,Check if the data information have been computed
0.1.6,Create the object
0.1.6,Create the object
0.1.6,Fit the data
0.1.6,Create the object
0.1.6,Fit the data
0.1.6,Create the object
0.1.6,Fit the data
0.1.6,Create the object
0.1.6,Fit the data
0.1.6,Create the object
0.1.6,Fit the data
0.1.6,Create the object
0.1.6,Start with the minority class
0.1.6,All the minority class samples will be preserved
0.1.6,If we need to offer support for the indices
0.1.6,Loop over the other classes under picking at random
0.1.6,"If the minority class is up, skip it"
0.1.6,Randomly get one sample from the majority class
0.1.6,Generate the index to select
0.1.6,Create the set C
0.1.6,Create the set S
0.1.6,Remove the seed from S since that it will be added anyway
0.1.6,Create a k-NN classifier
0.1.6,Fit C into the knn
0.1.6,Classify on S
0.1.6,Find the misclassified S_y
0.1.6,If we need to offer support for the indices selected
0.1.6,We concatenate the misclassified samples with the seed and the
0.1.6,minority samples
0.1.6,Find the nearest neighbour of every point
0.1.6,Send the information to is_tomek function to get boolean vector back
0.1.6,Check if the indices of the samples selected should be returned too
0.1.6,Return the indices of interest
0.1.6,Return data set without majority Tomek links.
0.1.6,Compute the distance considering the farthest neighbour
0.1.6,Sort the list of distance and get the index
0.1.6,Select the desired number of samples
0.1.6,Assign the parameter of the element of this class
0.1.6,Check that the version asked is implemented
0.1.6,Start with the minority class
0.1.6,All the minority class samples will be preserved
0.1.6,Compute the number of cluster needed
0.1.6,If we need to offer support for the indices
0.1.6,"For each element of the current class, find the set of NN"
0.1.6,of the minority class
0.1.6,Call the constructor of the NN
0.1.6,Fit the minority class since that we want to know the distance
0.1.6,to these point
0.1.6,Loop over the other classes under picking at random
0.1.6,"If the minority class is up, skip it"
0.1.6,Get the samples corresponding to the current class
0.1.6,Find the NN
0.1.6,Select the right samples
0.1.6,Find the NN
0.1.6,Select the right samples
0.1.6,We need a new NN object to fit the current class
0.1.6,Find the set of NN to the minority class
0.1.6,Create the subset containing the samples found during the NN
0.1.6,search. Linearize the indexes and remove the double values
0.1.6,Create the subset
0.1.6,Compute the NN considering the current class
0.1.6,If we need to offer support for the indices selected
0.1.6,Check if the indices of the samples selected should be returned too
0.1.6,Return the indices of interest
0.1.6,Compute the number of clusters needed
0.1.6,All the minority class samples will be preserved
0.1.6,If we need to offer support for the indices
0.1.6,Loop over the other classes under-picking at random
0.1.6,"If the minority class is up, skip it"
0.1.6,Pick some elements at random
0.1.6,If we need to offer support for the indices selected
0.1.6,Concatenate to the minority class
0.1.6,Check if the indices of the samples selected should be returned as
0.1.6,well
0.1.6,Return the indices of interest
0.1.6,"Initialize the boolean result as false, and also a counter"
0.1.6,Loop through each sample and looks whether it belongs to the minority
0.1.6,"class. If it does, we don't consider it since we want to keep all"
0.1.6,"minority samples. If, however, it belongs to the majority sample we"
0.1.6,look at its first neighbour. If its closest neighbour also has the
0.1.6,"current sample as its closest neighbour, the two form a Tomek link."
0.1.6,"If they form a tomek link, put a True marker on this"
0.1.6,"sample, and increase counter by one."
0.1.6,Find the nearest neighbour of every point
0.1.6,Send the information to is_tomek function to get boolean vector back
0.1.6,Check if the indices of the samples selected should be returned too
0.1.6,Return the indices of interest
0.1.6,Return data set without majority Tomek links.
0.1.6,Start with the minority class
0.1.6,All the minority class samples will be preserved
0.1.6,If we need to offer support for the indices
0.1.6,Loop over the other classes under picking at random
0.1.6,"If the minority class is up, skip it"
0.1.6,Randomly get one sample from the majority class
0.1.6,Generate the index to select
0.1.6,Create the set C - One majority samples and all minority
0.1.6,Create the set S - all majority samples
0.1.6,Create a k-NN classifier
0.1.6,Fit C into the knn
0.1.6,Check each sample in S if we keep it or drop it
0.1.6,Do not select sample which are already well classified
0.1.6,Classify on S
0.1.6,If the prediction do not agree with the true label
0.1.6,append it in C_x
0.1.6,Keep the index for later
0.1.6,Update C
0.1.6,Fit C into the knn
0.1.6,This experimental to speed up the search
0.1.6,Classify all the element in S and avoid to test the
0.1.6,well classified elements
0.1.6,Find the misclassified S_y
0.1.6,If we need to offer support for the indices selected
0.1.6,Check if the indices of the samples selected should be returned too
0.1.6,Return the indices of interest
0.1.6,Compute the number of cluster needed
0.1.6,Create the clustering object
0.1.6,Start with the minority class
0.1.6,All the minority class samples will be preserved
0.1.6,Loop over the other classes under picking at random
0.1.6,"If the minority class is up, skip it."
0.1.6,Find the centroids via k-means
0.1.6,Concatenate to the minority class
0.1.6,Start with the minority class
0.1.6,All the minority class samples will be preserved
0.1.6,If we need to offer support for the indices
0.1.6,Create a k-NN to fit the whole data
0.1.6,Fit the whole dataset
0.1.6,Loop over the other classes under picking at random
0.1.6,Get the sample of the current class
0.1.6,Get the samples associated
0.1.6,Find the NN for the current class
0.1.6,Get the label of the corresponding to the index
0.1.6,Check which one are the same label than the current class
0.1.6,Make an AND operation through the three neighbours
0.1.6,If the minority class remove the majority samples
0.1.6,Get the index to exclude
0.1.6,Get the index to exclude
0.1.6,Create a vector with the sample to select
0.1.6,Exclude as well the minority sample since that they will be
0.1.6,concatenated later
0.1.6,Get the samples from the majority classes
0.1.6,If we need to offer support for the indices selected
0.1.6,Check if the indices of the samples selected should be returned too
0.1.6,Return the indices of interest
0.1.6,Start with the minority class
0.1.6,All the minority class samples will be preserved
0.1.6,If we need to offer support for the indices
0.1.6,Create a k-NN to fit the whole data
0.1.6,Fit the data
0.1.6,Loop over the other classes under picking at random
0.1.6,"If the minority class is up, skip it"
0.1.6,Get the sample of the current class
0.1.6,Find the NN for the current class
0.1.6,Get the label of the corresponding to the index
0.1.6,Check which one are the same label than the current class
0.1.6,Make the majority vote
0.1.6,Get the samples which agree all together
0.1.6,If we need to offer support for the indices selected
0.1.6,Check if the indices of the samples selected should be returned too
0.1.6,Return the indices of interest
0.1.6,Check if the indices of the samples selected should be returned too
0.1.6,Return the indices of interest
0.1.6,Select the appropriate classifier
0.1.6,Create the different folds
0.1.6,Compute the number of cluster needed
0.1.6,Find the percentile corresponding to the top num_samples
0.1.6,Sample the data
0.1.6,If we need to offer support for the indices
0.1.6,Generate a global dataset to use
0.1.6,Define a negative ratio
0.1.6,Define a ratio greater than 1
0.1.6,Define ratio as an unknown string
0.1.6,Define ratio as a list which is not supported
0.1.6,Define a ratio
0.1.6,Define the parameter for the under-sampling
0.1.6,Create the object
0.1.6,Resample the data
0.1.6,Create a wrong y
0.1.6,Create the object
0.1.6,Fit the data
0.1.6,Define the parameter for the under-sampling
0.1.6,Create the object
0.1.6,Fit the data
0.1.6,Check if the data information have been computed
0.1.6,Define the parameter for the under-sampling
0.1.6,Create the object
0.1.6,Define the parameter for the under-sampling
0.1.6,Create the object
0.1.6,Fit and sample
0.1.6,Define the parameter for the under-sampling
0.1.6,Create the object
0.1.6,Fit and sample
0.1.6,Define the parameter for the under-sampling
0.1.6,Create the object
0.1.6,Fit and sample
0.1.6,Create the object
0.1.6,Generate a global dataset to use
0.1.6,Define a ratio
0.1.6,Create the object
0.1.6,Resample the data
0.1.6,Create a wrong y
0.1.6,Create the object
0.1.6,Fit the data
0.1.6,Check if the data information have been computed
0.1.6,Create the object
0.1.6,Resample the data
0.1.6,Resample the data
0.1.6,Create the object
0.1.6,Generate a global dataset to use
0.1.6,Define a ratio
0.1.6,Create the object
0.1.6,Resample the data
0.1.6,Create a wrong y
0.1.6,Create the object
0.1.6,Fit the data
0.1.6,Check if the data information have been computed
0.1.6,Create the object
0.1.6,Resample the data
0.1.6,Resample the data
0.1.6,Create the object
0.1.6,Generate a global dataset to use
0.1.6,Define a ratio
0.1.6,Create the object
0.1.6,Resample the data
0.1.6,Create a wrong y
0.1.6,Create the object
0.1.6,Fit the data
0.1.6,Check if the data information have been computed
0.1.6,Create the object
0.1.6,Resample the data
0.1.6,Resample the data
0.1.6,Resample the data
0.1.6,Create the object
0.1.6,Generate a global dataset to use
0.1.6,Define a negative ratio
0.1.6,Define a ratio greater than 1
0.1.6,Define ratio as an unknown string
0.1.6,Define ratio as a list which is not supported
0.1.6,Define a ratio
0.1.6,Create the object
0.1.6,Resample the data
0.1.6,Create a wrong y
0.1.6,Create the object
0.1.6,Fit the data
0.1.6,Create the object
0.1.6,Fit the data
0.1.6,Check if the data information have been computed
0.1.6,Create the object
0.1.6,Resample the data
0.1.6,Resample the data
0.1.6,Resample the data
0.1.6,Create the object
0.1.6,Generate a global dataset to use
0.1.6,Define a negative ratio
0.1.6,Define a ratio greater than 1
0.1.6,Define ratio as an unknown string
0.1.6,Define ratio as a list which is not supported
0.1.6,Resample the data
0.1.6,Define a ratio
0.1.6,Create the object
0.1.6,Resample the data
0.1.6,Create a wrong y
0.1.6,Create the object
0.1.6,Fit the data
0.1.6,Create the object
0.1.6,Fit the data
0.1.6,Check if the data information have been computed
0.1.6,Create the object
0.1.6,Resample the data
0.1.6,Resample the data
0.1.6,Resample the data
0.1.6,Resample the data
0.1.6,Resample the data
0.1.6,Resample the data
0.1.6,Resample the data
0.1.6,Resample the data
0.1.6,Resample the data
0.1.6,Create the object
0.1.6,Generate a global dataset to use
0.1.6,Define a ratio
0.1.6,Create the object
0.1.6,Create the object
0.1.6,Resample the data
0.1.6,Create a wrong y
0.1.6,Create the object
0.1.6,Fit the data
0.1.6,Check if the data information have been computed
0.1.6,Create the object
0.1.6,Resample the data
0.1.6,Resample the data
0.1.6,Resample the data
0.1.6,Create the object
0.1.6,Generate a global dataset to use
0.1.6,Define a negative ratio
0.1.6,Define a ratio greater than 1
0.1.6,Define ratio as an unknown string
0.1.6,Define ratio as a list which is not supported
0.1.6,Define a ratio
0.1.6,Define the parameter for the under-sampling
0.1.6,Create the object
0.1.6,Resample the data
0.1.6,Create a wrong y
0.1.6,Create the object
0.1.6,Fit the data
0.1.6,Define the parameter for the under-sampling
0.1.6,Create the object
0.1.6,Fit the data
0.1.6,Check if the data information have been computed
0.1.6,Define the parameter for the under-sampling
0.1.6,Create the object
0.1.6,Define the parameter for the under-sampling
0.1.6,Create the object
0.1.6,Fit and sample
0.1.6,Define the parameter for the under-sampling
0.1.6,Create the object
0.1.6,Fit and sample
0.1.6,Define the parameter for the under-sampling
0.1.6,Create the object
0.1.6,Fit and sample
0.1.6,Create the object
0.1.6,Generate a global dataset to use
0.1.6,Define a ratio
0.1.6,Create the object
0.1.6,Resample the data
0.1.6,Create a wrong y
0.1.6,Create the object
0.1.6,Fit the data
0.1.6,Check if the data information have been computed
0.1.6,Create the object
0.1.6,Resample the data
0.1.6,Resample the data
0.1.6,Create the object
0.1.6,Generate a global dataset to use
0.1.6,Define a negative ratio
0.1.6,Define a ratio greater than 1
0.1.6,Define ratio as an unknown string
0.1.6,Define ratio as a list which is not supported
0.1.6,Define a ratio
0.1.6,Define the parameter for the under-sampling
0.1.6,Create the object
0.1.6,Resample the data
0.1.6,Create a wrong y
0.1.6,Create the object
0.1.6,Fit the data
0.1.6,Define the parameter for the under-sampling
0.1.6,Create the object
0.1.6,Fit the data
0.1.6,Check if the data information have been computed
0.1.6,Define the parameter for the under-sampling
0.1.6,Create the object
0.1.6,Define the parameter for the under-sampling
0.1.6,Create the object
0.1.6,Define the parameter for the under-sampling
0.1.6,Create the object
0.1.6,Fit and sample
0.1.6,Define the parameter for the under-sampling
0.1.6,Create the object
0.1.6,Fit and sample
0.1.6,Generate a global dataset to use
0.1.6,Define a negative ratio
0.1.6,Define a ratio greater than 1
0.1.6,Define ratio as an unknown string
0.1.6,Define ratio as a list which is not supported
0.1.6,Define a ratio
0.1.6,Define the parameter for the under-sampling
0.1.6,Create the object
0.1.6,Resample the data
0.1.6,Create a wrong y
0.1.6,Create the object
0.1.6,Fit the data
0.1.6,Define the parameter for the under-sampling
0.1.6,Create the object
0.1.6,Fit the data
0.1.6,Check if the data information have been computed
0.1.6,Define the parameter for the under-sampling
0.1.6,Create the object
0.1.6,Define the parameter for the under-sampling
0.1.6,Create the object
0.1.6,Fit and sample
0.1.6,Define the parameter for the under-sampling
0.1.6,Create the object
0.1.6,Fit and sample
0.1.6,Define the parameter for the under-sampling
0.1.6,Create the object
0.1.6,Fit and sample
0.1.6,Create the object
0.1.6,Generate a global dataset to use
0.1.6,Define a ratio
0.1.6,Create the object
0.1.6,Resample the data
0.1.6,Create a wrong y
0.1.6,Create the object
0.1.6,Fit the data
0.1.6,Check if the data information have been computed
0.1.6,Create the object
0.1.6,Resample the data
0.1.6,Resample the data
0.1.6,Create the object
0.1.6,Test the various init parameters of the pipeline.
0.1.6,Check that we can't instantiate pipelines with objects without fit
0.1.6,method
0.1.6,Smoke test with only an estimator
0.1.6,Check that params are set
0.1.6,Smoke test the repr:
0.1.6,Test with two objects
0.1.6,Check that we can't use the same stage name twice
0.1.6,Check that params are set
0.1.6,Smoke test the repr:
0.1.6,Check that params are not set when naming them wrong
0.1.6,Test clone
0.1.6,"Check that apart from estimators, the parameters are the same"
0.1.6,Remove estimators that where copied
0.1.6,Test the various methods of the pipeline (anova).
0.1.6,Test with Anova + LogisticRegression
0.1.6,Test that the pipeline can take fit parameters
0.1.6,classifier should return True
0.1.6,and transformer params should not be changed
0.1.6,Test pipeline raises set params error message for nested models.
0.1.6,expected error message
0.1.6,nested model check
0.1.6,Test the various methods of the pipeline (pca + svm).
0.1.6,Test with PCA + SVC
0.1.6,Test the various methods of the pipeline (preprocessing + svm).
0.1.6,check shapes of various prediction functions
0.1.6,test that the fit_predict method is implemented on a pipeline
0.1.6,test that the fit_predict on pipeline yields same results as applying
0.1.6,transform and clustering steps separately
0.1.6,first compute the transform and clustering step separately
0.1.6,use a pipeline to do the transform and clustering in one step
0.1.6,tests that a pipeline does not have fit_predict method when final
0.1.6,step of pipeline does not have fit_predict defined
0.1.6,Test whether pipeline works with a transformer at the end.
0.1.6,Also test pipeline.transform and pipeline.inverse_transform
0.1.6,test transform and fit_transform:
0.1.6,Test whether pipeline works with a transformer missing fit_transform
0.1.6,test fit_transform:
0.1.6,Test the various methods of the pipeline (pca + svm).
0.1.6,Test with PCA + SVC
0.1.6,Test the various methods of the pipeline (pca + svm).
0.1.6,Test with PCA + SVC
0.1.6,Test whether pipeline works with a sampler at the end.
0.1.6,Also test pipeline.sampler
0.1.6,test transform and fit_transform:
0.1.6,Test whether pipeline works with a sampler at the end.
0.1.6,Also test pipeline.sampler
0.1.6,Test the various methods of the pipeline (anova).
0.1.6,Test with RandomUnderSampling + Anova + LogisticRegression
0.1.6,Fit using SMOTE
0.1.6,Transform using SMOTE
0.1.6,Fit and transform using ENN
0.1.6,Fit using SMOTE
0.1.6,Transform using SMOTE
0.1.6,Fit and transform using ENN
0.1.6,Generate a global dataset to use
0.1.6,Define a negative ratio
0.1.6,Define a ratio greater than 1
0.1.6,Define ratio as an unknown string
0.1.6,Define ratio as a list which is not supported
0.1.6,Create the object
0.1.6,Resample the data
0.1.6,Create a wrong y
0.1.6,Create the object
0.1.6,Fit the data
0.1.6,Check if the data information have been computed
0.1.6,Create the object
0.1.6,Create the object
0.1.6,Fit the data
0.1.6,Create the object
0.1.6,Fit the data
0.1.6,Create the object
0.1.6,Generate a global dataset to use
0.1.6,Define a negative ratio
0.1.6,Define a ratio greater than 1
0.1.6,Define ratio as an unknown string
0.1.6,Define ratio as a list which is not supported
0.1.6,Create the object
0.1.6,Resample the data
0.1.6,Create a wrong y
0.1.6,Create the object
0.1.6,Fit the data
0.1.6,Check if the data information have been computed
0.1.6,Create the object
0.1.6,Create the object
0.1.6,Fit the data
0.1.6,Create the object
0.1.6,Fit the data
0.1.6,Create the object
0.1.6,Check the random state
0.1.6,Define the classifier to use
0.1.6,Start with the minority class
0.1.6,Keep the indices of the minority class somewhere if we need to
0.1.6,return them later
0.1.6,Condition to initiliase before the search
0.1.6,Get the initial number of samples to select in the majority class
0.1.6,Create the array characterising the array containing the majority
0.1.6,class
0.1.6,Loop to create the different subsets
0.1.6,Generate an appropriate number of index to extract
0.1.6,from the majority class depending of the false classification
0.1.6,rate of the previous iteration
0.1.6,Mark these indexes as not being considered for next sampling
0.1.6,"For now, we will train and classify on the same data"
0.1.6,"Let see if we should find another solution. Anyway,"
0.1.6,random stuff are still random stuff
0.1.6,Push these data into a new subset
0.1.6,Apply a bootstrap on x_data
0.1.6,Train the classifier using the current data
0.1.6,Train the classifier using the current data
0.1.6,Predict using only the majority class
0.1.6,Basically let's find which sample have to be retained for the
0.1.6,next round
0.1.6,Find the misclassified index to keep them for the next round
0.1.6,Count how many random element will be selected
0.1.6,"We found a new subset, increase the counter"
0.1.6,Check if we have to make an early stopping
0.1.6,Select the remaining data
0.1.6,Select the final batch
0.1.6,Push these data into a new subset
0.1.6,"We found a new subset, increase the counter"
0.1.6,Also check that we will have enough sample to extract at the
0.1.6,next round
0.1.6,Select the remaining data
0.1.6,Select the final batch
0.1.6,Push these data into a new subset
0.1.6,"We found a new subset, increase the counter"
0.1.6,Generate a global dataset to use
0.1.6,Define a negative ratio
0.1.6,Define a ratio greater than 1
0.1.6,Define ratio as an unknown string
0.1.6,Define ratio as a list which is not supported
0.1.6,Define a ratio
0.1.6,Define the parameter for the under-sampling
0.1.6,Create the object
0.1.6,Resample the data
0.1.6,Create a wrong y
0.1.6,Create the object
0.1.6,Fit the data
0.1.6,Define the parameter for the under-sampling
0.1.6,Create the object
0.1.6,Fit the data
0.1.6,Check if the data information have been computed
0.1.6,Define the parameter for the under-sampling
0.1.6,Create the object
0.1.6,Define the ratio parameter
0.1.6,Create the sampling object
0.1.6,Get the different subset
0.1.6,Check each array
0.1.6,Define the ratio parameter
0.1.6,Create the sampling object
0.1.6,Get the different subset
0.1.6,Check each array
0.1.6,Define the ratio parameter
0.1.6,Create the sampling object
0.1.6,Get the different subset
0.1.6,Check each array
0.1.6,Define the ratio parameter
0.1.6,Create the sampling object
0.1.6,Get the different subset
0.1.6,Check each array
0.1.6,Define the ratio parameter
0.1.6,Create the sampling object
0.1.6,Get the different subset
0.1.6,Check each array
0.1.6,Define the ratio parameter
0.1.6,Create the sampling object
0.1.6,Get the different subset
0.1.6,Check each array
0.1.6,Define the ratio parameter
0.1.6,Create the sampling object
0.1.6,Get the different subset
0.1.6,Check each array
0.1.6,Define the ratio parameter
0.1.6,Define the ratio parameter
0.1.6,Create the sampling object
0.1.6,Get the different subset
0.1.6,Check each array
0.1.6,Create the object
0.1.6,Generate a global dataset to use
0.1.6,Define a negative ratio
0.1.6,Define a ratio greater than 1
0.1.6,Define ratio as an unknown string
0.1.6,Define ratio as a list which is not supported
0.1.6,Define a ratio
0.1.6,Define the parameter for the under-sampling
0.1.6,Create the object
0.1.6,Resample the data
0.1.6,Create a wrong y
0.1.6,Create the object
0.1.6,Fit the data
0.1.6,Define the parameter for the under-sampling
0.1.6,Create the object
0.1.6,Fit the data
0.1.6,Check if the data information have been computed
0.1.6,Define the parameter for the under-sampling
0.1.6,Create the object
0.1.6,Define the ratio parameter
0.1.6,Create the sampling object
0.1.6,Get the different subset
0.1.6,Define the ratio parameter
0.1.6,Create the sampling object
0.1.6,Get the different subset
0.1.6,Define the ratio parameter
0.1.6,Create the sampling object
0.1.6,Get the different subset
0.1.6,Create the object
0.1.5,! /usr/bin/env python
0.1.5,"load all vars into globals, otherwise"
0.1.5,the later function call using global vars doesn't work.
0.1.5,"Allow command-lines such as ""python setup.py build install"""
0.1.5,Make sources available using relative paths from this file's directory.
0.1.5,-*- coding: utf-8 -*-
0.1.5,
0.1.5,"imbalanced-learn documentation build configuration file, created by"
0.1.5,sphinx-quickstart on Mon Jan 18 14:44:12 2016.
0.1.5,
0.1.5,This file is execfile()d with the current directory set to its
0.1.5,containing dir.
0.1.5,
0.1.5,Note that not all possible configuration values are present in this
0.1.5,autogenerated file.
0.1.5,
0.1.5,All configuration values have a default; values that are commented out
0.1.5,serve to show the default.
0.1.5,"If extensions (or modules to document with autodoc) are in another directory,"
0.1.5,add these directories to sys.path here. If the directory is relative to the
0.1.5,"documentation root, use os.path.abspath to make it absolute, like shown here."
0.1.5,"sys.path.insert(0, os.path.abspath('.'))"
0.1.5,-- General configuration ---------------------------------------------------
0.1.5,Try to override the matplotlib configuration as early as possible
0.1.5,-- General configuration ------------------------------------------------
0.1.5,"If your documentation needs a minimal Sphinx version, state it here."
0.1.5,needs_sphinx = '1.0'
0.1.5,"Add any Sphinx extension module names here, as strings. They can be"
0.1.5,extensions coming with Sphinx (named 'sphinx.ext.*') or your custom
0.1.5,ones.
0.1.5,path to your examples scripts
0.1.5,path where to save gallery generated examples
0.1.5,"Add any paths that contain templates here, relative to this directory."
0.1.5,generate autosummary even if no references
0.1.5,The suffix of source filenames.
0.1.5,The encoding of source files.
0.1.5,source_encoding = 'utf-8-sig'
0.1.5,Generate the plots for the gallery
0.1.5,The master toctree document.
0.1.5,General information about the project.
0.1.5,"The version info for the project you're documenting, acts as replacement for"
0.1.5,"|version| and |release|, also used in various other places throughout the"
0.1.5,built documents.
0.1.5,
0.1.5,The short X.Y version.
0.1.5,"The full version, including alpha/beta/rc tags."
0.1.5,The language for content autogenerated by Sphinx. Refer to documentation
0.1.5,for a list of supported languages.
0.1.5,language = None
0.1.5,"There are two options for replacing |today|: either, you set today to some"
0.1.5,"non-false value, then it is used:"
0.1.5,today = ''
0.1.5,"Else, today_fmt is used as the format for a strftime call."
0.1.5,"today_fmt = '%B %d, %Y'"
0.1.5,"List of patterns, relative to source directory, that match files and"
0.1.5,directories to ignore when looking for source files.
0.1.5,The reST default role (used for this markup: `text`) to use for all
0.1.5,documents.
0.1.5,default_role = None
0.1.5,"If true, '()' will be appended to :func: etc. cross-reference text."
0.1.5,"If true, the current module name will be prepended to all description"
0.1.5,unit titles (such as .. function::).
0.1.5,add_module_names = True
0.1.5,"If true, sectionauthor and moduleauthor directives will be shown in the"
0.1.5,output. They are ignored by default.
0.1.5,show_authors = False
0.1.5,The name of the Pygments (syntax highlighting) style to use.
0.1.5,A list of ignored prefixes for module index sorting.
0.1.5,modindex_common_prefix = []
0.1.5,"If true, keep warnings as ""system message"" paragraphs in the built documents."
0.1.5,keep_warnings = False
0.1.5,-- Options for HTML output ----------------------------------------------
0.1.5,The theme to use for HTML and HTML Help pages.  See the documentation for
0.1.5,a list of builtin themes.
0.1.5,Theme options are theme-specific and customize the look and feel of a theme
0.1.5,"further.  For a list of options available for each theme, see the"
0.1.5,documentation.
0.1.5,html_theme_options = {}
0.1.5,"Add any paths that contain custom themes here, relative to this directory."
0.1.5,"The name for this set of Sphinx documents.  If None, it defaults to"
0.1.5,"""<project> v<release> documentation""."
0.1.5,html_title = None
0.1.5,A shorter title for the navigation bar.  Default is the same as html_title.
0.1.5,html_short_title = None
0.1.5,The name of an image file (relative to this directory) to place at the top
0.1.5,of the sidebar.
0.1.5,html_logo = None
0.1.5,The name of an image file (within the static path) to use as favicon of the
0.1.5,docs.  This file should be a Windows icon file (.ico) being 16x16 or 32x32
0.1.5,pixels large.
0.1.5,html_favicon = None
0.1.5,"Add any paths that contain custom static files (such as style sheets) here,"
0.1.5,"relative to this directory. They are copied after the builtin static files,"
0.1.5,"so a file named ""default.css"" will overwrite the builtin ""default.css""."
0.1.5,Add any extra paths that contain custom files (such as robots.txt or
0.1.5,".htaccess) here, relative to this directory. These files are copied"
0.1.5,directly to the root of the documentation.
0.1.5,html_extra_path = []
0.1.5,"If not '', a 'Last updated on:' timestamp is inserted at every page bottom,"
0.1.5,using the given strftime format.
0.1.5,"html_last_updated_fmt = '%b %d, %Y'"
0.1.5,"If true, SmartyPants will be used to convert quotes and dashes to"
0.1.5,typographically correct entities.
0.1.5,html_use_smartypants = True
0.1.5,"Custom sidebar templates, maps document names to template names."
0.1.5,html_sidebars = {}
0.1.5,"Additional templates that should be rendered to pages, maps page names to"
0.1.5,template names.
0.1.5,html_additional_pages = {}
0.1.5,"If false, no module index is generated."
0.1.5,html_domain_indices = True
0.1.5,"If false, no index is generated."
0.1.5,html_use_index = True
0.1.5,"If true, the index is split into individual pages for each letter."
0.1.5,html_split_index = False
0.1.5,"If true, links to the reST sources are added to the pages."
0.1.5,html_show_sourcelink = True
0.1.5,"If true, ""Created using Sphinx"" is shown in the HTML footer. Default is True."
0.1.5,html_show_sphinx = True
0.1.5,"If true, ""(C) Copyright ..."" is shown in the HTML footer. Default is True."
0.1.5,html_show_copyright = True
0.1.5,"If true, an OpenSearch description file will be output, and all pages will"
0.1.5,contain a <link> tag referring to it.  The value of this option must be the
0.1.5,base URL from which the finished HTML is served.
0.1.5,html_use_opensearch = ''
0.1.5,"This is the file name suffix for HTML files (e.g. "".xhtml"")."
0.1.5,html_file_suffix = None
0.1.5,Output file base name for HTML help builder.
0.1.5,-- Options for LaTeX output ---------------------------------------------
0.1.5,The paper size ('letterpaper' or 'a4paper').
0.1.5,"'papersize': 'letterpaper',"
0.1.5,"The font size ('10pt', '11pt' or '12pt')."
0.1.5,"'pointsize': '10pt',"
0.1.5,Additional stuff for the LaTeX preamble.
0.1.5,"'preamble': '',"
0.1.5,Grouping the document tree into LaTeX files. List of tuples
0.1.5,"(source start file, target name, title,"
0.1.5,"author, documentclass [howto, manual, or own class])."
0.1.5,The name of an image file (relative to this directory) to place at the top of
0.1.5,the title page.
0.1.5,latex_logo = None
0.1.5,"For ""manual"" documents, if this is true, then toplevel headings are parts,"
0.1.5,not chapters.
0.1.5,latex_use_parts = False
0.1.5,"If true, show page references after internal links."
0.1.5,latex_show_pagerefs = False
0.1.5,"If true, show URL addresses after external links."
0.1.5,latex_show_urls = False
0.1.5,Documents to append as an appendix to all manuals.
0.1.5,latex_appendices = []
0.1.5,"If false, no module index is generated."
0.1.5,latex_domain_indices = True
0.1.5,-- Options for manual page output ---------------------------------------
0.1.5,One entry per manual page. List of tuples
0.1.5,"(source start file, name, description, authors, manual section)."
0.1.5,"If true, show URL addresses after external links."
0.1.5,man_show_urls = False
0.1.5,-- Options for Texinfo output -------------------------------------------
0.1.5,Grouping the document tree into Texinfo files. List of tuples
0.1.5,"(source start file, target name, title, author,"
0.1.5,"dir menu entry, description, category)"
0.1.5,"generate empty examples files, so that we don't get"
0.1.5,inclusion errors if there are no examples for a class / module
0.1.5,touch file
0.1.5,Documents to append as an appendix to all manuals.
0.1.5,texinfo_appendices = []
0.1.5,"If false, no module index is generated."
0.1.5,texinfo_domain_indices = True
0.1.5,"How to display URL addresses: 'footnote', 'no', or 'inline'."
0.1.5,texinfo_show_urls = 'footnote'
0.1.5,"If true, do not generate a @detailmenu in the ""Top"" node's menu."
0.1.5,texinfo_no_detailmenu = False
0.1.5,Example configuration for intersphinx: refer to the Python standard library.
0.1.5,Define some color for the plotting
0.1.5,Generate the dataset
0.1.5,Instanciate a PCA object for the sake of easy visualisation
0.1.5,Fit and transform x to visualise inside a 2D feature space
0.1.5,Apply SMOTE SVM
0.1.5,"Two subplots, unpack the axes array immediately"
0.1.5,Define some color for the plotting
0.1.5,Generate the dataset
0.1.5,Instanciate a PCA object for the sake of easy visualisation
0.1.5,Fit and transform x to visualise inside a 2D feature space
0.1.5,Apply Borderline SMOTE 2
0.1.5,"Two subplots, unpack the axes array immediately"
0.1.5,Define some color for the plotting
0.1.5,Generate the dataset
0.1.5,Instanciate a PCA object for the sake of easy visualisation
0.1.5,Fit and transform x to visualise inside a 2D feature space
0.1.5,Apply regular SMOTE
0.1.5,"Two subplots, unpack the axes array immediately"
0.1.5,Define some color for the plotting
0.1.5,Generate the dataset
0.1.5,Instanciate a PCA object for the sake of easy visualisation
0.1.5,Fit and transform x to visualise inside a 2D feature space
0.1.5,Apply the random over-sampling
0.1.5,"Two subplots, unpack the axes array immediately"
0.1.5,Define some color for the plotting
0.1.5,Generate the dataset
0.1.5,Instanciate a PCA object for the sake of easy visualisation
0.1.5,Fit and transform x to visualise inside a 2D feature space
0.1.5,Apply Borderline SMOTE 1
0.1.5,"Two subplots, unpack the axes array immediately"
0.1.5,Define some color for the plotting
0.1.5,Generate the dataset
0.1.5,Instanciate a PCA object for the sake of easy visualisation
0.1.5,Fit and transform x to visualise inside a 2D feature space
0.1.5,Apply the random over-sampling
0.1.5,"Two subplots, unpack the axes array immediately"
0.1.5,Define some color for the plotting
0.1.5,Generate the dataset
0.1.5,Instanciate a PCA object for the sake of easy visualisation
0.1.5,Fit and transform x to visualise inside a 2D feature space
0.1.5,Apply SMOTE + ENN
0.1.5,"Two subplots, unpack the axes array immediately"
0.1.5,Define some color for the plotting
0.1.5,Generate the dataset
0.1.5,Instanciate a PCA object for the sake of easy visualisation
0.1.5,Fit and transform x to visualise inside a 2D feature space
0.1.5,Apply SMOTE + Tomek links
0.1.5,"Two subplots, unpack the axes array immediately"
0.1.5,Define some color for the plotting
0.1.5,Generate the dataset
0.1.5,Instanciate a PCA object for the sake of easy visualisation
0.1.5,Fit and transform x to visualise inside a 2D feature space
0.1.5,Apply Balance Cascade method
0.1.5,"Two subplots, unpack the axes array immediately"
0.1.5,Define some color for the plotting
0.1.5,Generate the dataset
0.1.5,Instanciate a PCA object for the sake of easy visualisation
0.1.5,Fit and transform x to visualise inside a 2D feature space
0.1.5,Apply Easy Ensemble
0.1.5,"Two subplots, unpack the axes array immediately"
0.1.5,Define some color for the plotting
0.1.5,Generate the dataset
0.1.5,Instanciate a PCA object for the sake of easy visualisation
0.1.5,Fit and transform x to visualise inside a 2D feature space
0.1.5,"Three subplots, unpack the axes array immediately"
0.1.5,Apply the ENN
0.1.5,Apply the RENN
0.1.5,Define some color for the plotting
0.1.5,Generate the dataset
0.1.5,Instanciate a PCA object for the sake of easy visualisation
0.1.5,Fit and transform x to visualise inside a 2D feature space
0.1.5,Apply Nearmiss 3
0.1.5,"Two subplots, unpack the axes array immediately"
0.1.5,Define some color for the plotting
0.1.5,Generate the dataset
0.1.5,Instanciate a PCA object for the sake of easy visualisation
0.1.5,Fit and transform x to visualise inside a 2D feature space
0.1.5,Apply Edited Nearest Neighbours
0.1.5,"Two subplots, unpack the axes array immediately"
0.1.5,Define some color for the plotting
0.1.5,Generate the dataset
0.1.5,Instanciate a PCA object for the sake of easy visualisation
0.1.5,Fit and transform x to visualise inside a 2D feature space
0.1.5,Apply Tomek Links cleaning
0.1.5,"Two subplots, unpack the axes array immediately"
0.1.5,Define some color for the plotting
0.1.5,Generate the dataset
0.1.5,Instanciate a PCA object for the sake of easy visualisation
0.1.5,Fit and transform x to visualise inside a 2D feature space
0.1.5,Apply One-Sided Selection
0.1.5,"Two subplots, unpack the axes array immediately"
0.1.5,Define some color for the plotting
0.1.5,Generate the dataset
0.1.5,Instanciate a PCA object for the sake of easy visualisation
0.1.5,Fit and transform x to visualise inside a 2D feature space
0.1.5,Apply Nearmiss 2
0.1.5,"Two subplots, unpack the axes array immediately"
0.1.5,Define some color for the plotting
0.1.5,Generate the dataset
0.1.5,Instanciate a PCA object for the sake of easy visualisation
0.1.5,Fit and transform x to visualise inside a 2D feature space
0.1.5,Apply neighbourhood cleaning rule
0.1.5,"Two subplots, unpack the axes array immediately"
0.1.5,Define some color for the plotting
0.1.5,Generate the dataset
0.1.5,Instanciate a PCA object for the sake of easy visualisation
0.1.5,Fit and transform x to visualise inside a 2D feature space
0.1.5,Apply Condensed Nearest Neighbours
0.1.5,"Two subplots, unpack the axes array immediately"
0.1.5,Define some color for the plotting
0.1.5,Generate the dataset
0.1.5,Instanciate a PCA object for the sake of easy visualisation
0.1.5,Fit and transform x to visualise inside a 2D feature space
0.1.5,Apply Cluster Centroids
0.1.5,"Two subplots, unpack the axes array immediately"
0.1.5,Define some color for the plotting
0.1.5,Generate the dataset
0.1.5,Instanciate a PCA object for the sake of easy visualisation
0.1.5,Fit and transform x to visualise inside a 2D feature space
0.1.5,Apply the random under-sampling
0.1.5,"Two subplots, unpack the axes array immediately"
0.1.5,Define some color for the plotting
0.1.5,Generate the dataset
0.1.5,"Two subplots, unpack the axes array immediately"
0.1.5,Define some color for the plotting
0.1.5,Generate the dataset
0.1.5,Instanciate a PCA object for the sake of easy visualisation
0.1.5,Fit and transform x to visualise inside a 2D feature space
0.1.5,Apply Nearmiss 1
0.1.5,"Two subplots, unpack the axes array immediately"
0.1.5,Generate the dataset
0.1.5,Instanciate a PCA object for the sake of easy visualisation
0.1.5,Create the samplers
0.1.5,Create teh classifier
0.1.5,Make the splits
0.1.5,Add one transformers and two samplers in the pipeline object
0.1.5,Based on NiLearn package
0.1.5,License: simplified BSD
0.1.5,"PEP0440 compatible formatted version, see:"
0.1.5,https://www.python.org/dev/peps/pep-0440/
0.1.5,
0.1.5,Generic release markers:
0.1.5,X.Y
0.1.5,X.Y.Z # For bugfix releases
0.1.5,
0.1.5,Admissible pre-release markers:
0.1.5,X.YaN # Alpha release
0.1.5,X.YbN # Beta release
0.1.5,X.YrcN # Release Candidate
0.1.5,X.Y # Final release
0.1.5,
0.1.5,Dev branch marker is: 'X.Y.dev' or 'X.Y.devN' where N is an integer.
0.1.5,'X.Y.dev0' is the canonical version of 'X.Y.dev'
0.1.5,
0.1.5,"This is a tuple to preserve order, so that dependencies are checked"
0.1.5,in some meaningful order (more => less 'core').  We avoid using
0.1.5,collections.OrderedDict to preserve Python 2.6 compatibility.
0.1.5,Avoid choking on modules with no __version__ attribute
0.1.5,Skip check only when installing and it's a module that
0.1.5,will be auto-installed.
0.1.5,Check the consistency of X and y
0.1.5,Get all the unique elements in the target array
0.1.5,# Raise an error if there is only one class
0.1.5,if uniques.size == 1:
0.1.5,"raise RuntimeError(""Only one class detected, aborting..."")"
0.1.5,Raise a warning for the moment to be compatible with BaseEstimator
0.1.5,Store the size of X to check at sampling time if we have the
0.1.5,same data
0.1.5,Create a dictionary containing the class statistics
0.1.5,Find the minority and majority classes
0.1.5,Check if the ratio provided at initialisation make sense
0.1.5,Check the consistency of X and y
0.1.5,Check that the data have been fitted
0.1.5,Check if the size of the data is identical than at fitting
0.1.5,The ratio correspond to the number of samples in the minority class
0.1.5,"over the number of samples in the majority class. Thus, the ratio"
0.1.5,cannot be greater than 1.0
0.1.5,Adapted from scikit-learn
0.1.5,Author: Edouard Duchesnay
0.1.5,Gael Varoquaux
0.1.5,Virgile Fritsch
0.1.5,Alexandre Gramfort
0.1.5,Lars Buitinck
0.1.5,chkoar
0.1.5,License: BSD
0.1.5,BaseEstimator interface
0.1.5,shallow copy of steps
0.1.5,Estimator interface
0.1.5,Boolean controlling whether the joblib caches should be
0.1.5,"flushed if the version of certain modules changes (eg nibabel, as it"
0.1.5,does not respect the backward compatibility in some of its internal
0.1.5,structures
0.1.5,This  is used in nilearn._utils.cache_mixin
0.1.5,list all submodules available in imblearn and version
0.1.5,Keep the samples from the majority class
0.1.5,Loop over the other classes over picking at random
0.1.5,"If this is the majority class, skip it"
0.1.5,Define the number of sample to create
0.1.5,Pick some elements at random
0.1.5,Concatenate to the majority class
0.1.5,Keep the samples from the majority class
0.1.5,Define the number of sample to create
0.1.5,We handle only two classes problem for the moment.
0.1.5,Start by separating minority class features and target values.
0.1.5,Print if verbose is true
0.1.5,"Look for k-th nearest neighbours, excluding, of course, the"
0.1.5,point itself.
0.1.5,Get the distance to the NN
0.1.5,Compute the ratio of majority samples next to minority samples
0.1.5,Check that we found at least some neighbours belonging to the
0.1.5,majority class
0.1.5,Normalize the ratio
0.1.5,Compute the number of sample to be generated
0.1.5,For each minority samples
0.1.5,Pick-up the neighbors wanted
0.1.5,Create a new sample
0.1.5,Find the NN for each samples
0.1.5,Exclude the sample itself
0.1.5,Count how many NN belong to the minority class
0.1.5,Find the class corresponding to the label in x
0.1.5,Compute the number of majority samples in the NN
0.1.5,Samples are in danger for m/2 <= m' < m
0.1.5,Samples are noise for m = m'
0.1.5,Check the consistency of X
0.1.5,Check the random state
0.1.5,A matrix to store the synthetic samples
0.1.5,# Set seeds
0.1.5,"seeds = random_state.randint(low=0,"
0.1.5,"high=100 * len(nn_num.flatten()),"
0.1.5,size=n_samples)
0.1.5,Randomly pick samples to construct neighbours from
0.1.5,Loop over the NN matrix and create new samples
0.1.5,"NN lines relate to original sample, columns to its"
0.1.5,nearest neighbours
0.1.5,"Take a step of random size (0,1) in the direction of the"
0.1.5,n nearest neighbours
0.1.5,if self.random_state is None:
0.1.5,np.random.seed(seeds[i])
0.1.5,else:
0.1.5,np.random.seed(self.random_state)
0.1.5,Construct synthetic sample
0.1.5,The returned target vector is simply a repetition of the
0.1.5,minority label
0.1.5,Define the number of sample to create
0.1.5,We handle only two classes problem for the moment.
0.1.5,Start by separating minority class features and target values.
0.1.5,If regular SMOTE is to be performed
0.1.5,"Look for k-th nearest neighbours, excluding, of course, the"
0.1.5,point itself.
0.1.5,Matrix with k-th nearest neighbours indexes for each minority
0.1.5,element.
0.1.5,--- Generating synthetic samples
0.1.5,Use static method make_samples to generate minority samples
0.1.5,Concatenate the newly generated samples to the original data set
0.1.5,Find the NNs for all samples in the data set.
0.1.5,Boolean array with True for minority samples in danger
0.1.5,"If all minority samples are safe, return the original data set."
0.1.5,"All are safe, nothing to be done here."
0.1.5,"If we got here is because some samples are in danger, we need to"
0.1.5,find the NNs among the minority class to create the new synthetic
0.1.5,samples.
0.1.5,
0.1.5,We start by changing the number of NNs to consider from m + 1
0.1.5,to k + 1
0.1.5,nns...#
0.1.5,B1 and B2 types diverge here!!!
0.1.5,Create synthetic samples for borderline points.
0.1.5,Concatenate the newly generated samples to the original
0.1.5,dataset
0.1.5,Reset the k-neighbours to m+1 neighbours
0.1.5,Split the number of synthetic samples between only minority
0.1.5,"(type 1), or minority and majority (with reduced step size)"
0.1.5,(type 2).
0.1.5,The fraction is sampled from a beta distribution centered
0.1.5,around 0.5 with variance ~0.01
0.1.5,Only minority
0.1.5,Only majority with smaller step size
0.1.5,Concatenate the newly generated samples to the original
0.1.5,data set
0.1.5,Reset the k-neighbours to m+1 neighbours
0.1.5,The SVM smote model fits a support vector machine
0.1.5,classifier to the data and uses the support vector to
0.1.5,"provide a notion of boundary. Unlike regular smote, where"
0.1.5,such notion relies on proportion of nearest neighbours
0.1.5,belonging to each class.
0.1.5,Fit SVM to the full data#
0.1.5,Find the support vectors and their corresponding indexes
0.1.5,"First, find the nn of all the samples to identify samples"
0.1.5,in danger and noisy ones
0.1.5,"As usual, fit a nearest neighbour model to the data"
0.1.5,"Now, get rid of noisy support vectors"
0.1.5,Remove noisy support vectors
0.1.5,Proceed to find support vectors NNs among the minority class
0.1.5,Split the number of synthetic samples between interpolation and
0.1.5,extrapolation
0.1.5,The fraction are sampled from a beta distribution with mean
0.1.5,0.5 and variance 0.01#
0.1.5,Interpolate samples in danger
0.1.5,Extrapolate safe samples
0.1.5,Concatenate the newly generated samples to the original data set
0.1.5,not any support vectors in danger
0.1.5,All the support vector in danger
0.1.5,Reset the k-neighbours to m+1 neighbours
0.1.5,--- NN object
0.1.5,Import the NN object from scikit-learn library. Since in the smote
0.1.5,"variations we must first find samples that are in danger, we"
0.1.5,initialize the NN object differently depending on the method chosen
0.1.5,"Regular smote does not look for samples in danger, instead it"
0.1.5,creates synthetic samples directly from the k-th nearest
0.1.5,neighbours with not filtering
0.1.5,"Borderline1, 2 and SVM variations of smote must first look for"
0.1.5,samples that could be considered noise and samples that live
0.1.5,"near the boundary between the classes. Therefore, before"
0.1.5,"creating synthetic samples from the k-th nns, it first look"
0.1.5,for m nearest neighbors to decide whether or not a sample is
0.1.5,noise or near the boundary.
0.1.5,--- SVM smote
0.1.5,"Unlike the borderline variations, the SVM variation uses the support"
0.1.5,vectors to decide which samples are in danger (near the boundary).
0.1.5,Additionally it also introduces extrapolation for samples that are
0.1.5,considered safe (far from boundary) and interpolation for samples
0.1.5,in danger (near the boundary). The level of extrapolation is
0.1.5,controled by the out_step.
0.1.5,Store SVM object with any parameters
0.1.5,Generate a global dataset to use
0.1.5,Define a negative ratio
0.1.5,Define a ratio greater than 1
0.1.5,Define ratio as an unknown string
0.1.5,Define ratio as a list which is not supported
0.1.5,Define a ratio
0.1.5,Create the object
0.1.5,Resample the data
0.1.5,Create a wrong y
0.1.5,Create the object
0.1.5,Fit the data
0.1.5,Create the object
0.1.5,Fit the data
0.1.5,Check if the data information have been computed
0.1.5,Create the object
0.1.5,Resample the data
0.1.5,Resample the data
0.1.5,Create the object
0.1.5,Generate a global dataset to use
0.1.5,Define a negative ratio
0.1.5,Define a ratio greater than 1
0.1.5,Define ratio as an unknown string
0.1.5,Define ratio as a list which is not supported
0.1.5,Define a ratio
0.1.5,Create the object
0.1.5,Resample the data
0.1.5,Create a wrong y
0.1.5,Create the object
0.1.5,Fit the data
0.1.5,Create the object
0.1.5,Fit the data
0.1.5,Check if the data information have been computed
0.1.5,Create the object
0.1.5,Resample the data
0.1.5,Resample the data
0.1.5,Create the object
0.1.5,Generate a global dataset to use
0.1.5,Define a negative ratio
0.1.5,Define a ratio greater than 1
0.1.5,Define ratio as an unknown string
0.1.5,Define ratio as a list which is not supported
0.1.5,Create the object
0.1.5,Resample the data
0.1.5,Create a wrong y
0.1.5,Create the object
0.1.5,Fit the data
0.1.5,Check if the data information have been computed
0.1.5,Create the object
0.1.5,Create the object
0.1.5,Fit the data
0.1.5,Create the object
0.1.5,Fit the data
0.1.5,Create the object
0.1.5,Fit the data
0.1.5,Create the object
0.1.5,Fit the data
0.1.5,Create the object
0.1.5,Fit the data
0.1.5,Create the object
0.1.5,Start with the minority class
0.1.5,All the minority class samples will be preserved
0.1.5,If we need to offer support for the indices
0.1.5,Loop over the other classes under picking at random
0.1.5,"If the minority class is up, skip it"
0.1.5,Randomly get one sample from the majority class
0.1.5,Generate the index to select
0.1.5,Create the set C
0.1.5,Create the set S
0.1.5,Remove the seed from S since that it will be added anyway
0.1.5,Create a k-NN classifier
0.1.5,Fit C into the knn
0.1.5,Classify on S
0.1.5,Find the misclassified S_y
0.1.5,If we need to offer support for the indices selected
0.1.5,We concatenate the misclassified samples with the seed and the
0.1.5,minority samples
0.1.5,Find the nearest neighbour of every point
0.1.5,Send the information to is_tomek function to get boolean vector back
0.1.5,Check if the indices of the samples selected should be returned too
0.1.5,Return the indices of interest
0.1.5,Return data set without majority Tomek links.
0.1.5,Compute the distance considering the farthest neighbour
0.1.5,Sort the list of distance and get the index
0.1.5,Select the desired number of samples
0.1.5,Assign the parameter of the element of this class
0.1.5,Check that the version asked is implemented
0.1.5,Start with the minority class
0.1.5,All the minority class samples will be preserved
0.1.5,Compute the number of cluster needed
0.1.5,If we need to offer support for the indices
0.1.5,"For each element of the current class, find the set of NN"
0.1.5,of the minority class
0.1.5,Call the constructor of the NN
0.1.5,Fit the minority class since that we want to know the distance
0.1.5,to these point
0.1.5,Loop over the other classes under picking at random
0.1.5,"If the minority class is up, skip it"
0.1.5,Get the samples corresponding to the current class
0.1.5,Find the NN
0.1.5,Select the right samples
0.1.5,Find the NN
0.1.5,Select the right samples
0.1.5,We need a new NN object to fit the current class
0.1.5,Find the set of NN to the minority class
0.1.5,Create the subset containing the samples found during the NN
0.1.5,search. Linearize the indexes and remove the double values
0.1.5,Create the subset
0.1.5,Compute the NN considering the current class
0.1.5,If we need to offer support for the indices selected
0.1.5,Check if the indices of the samples selected should be returned too
0.1.5,Return the indices of interest
0.1.5,Compute the number of clusters needed
0.1.5,All the minority class samples will be preserved
0.1.5,If we need to offer support for the indices
0.1.5,Loop over the other classes under-picking at random
0.1.5,"If the minority class is up, skip it"
0.1.5,Pick some elements at random
0.1.5,If we need to offer support for the indices selected
0.1.5,Concatenate to the minority class
0.1.5,Check if the indices of the samples selected should be returned as
0.1.5,well
0.1.5,Return the indices of interest
0.1.5,"Initialize the boolean result as false, and also a counter"
0.1.5,Loop through each sample and looks whether it belongs to the minority
0.1.5,"class. If it does, we don't consider it since we want to keep all"
0.1.5,"minority samples. If, however, it belongs to the majority sample we"
0.1.5,look at its first neighbour. If its closest neighbour also has the
0.1.5,"current sample as its closest neighbour, the two form a Tomek link."
0.1.5,"If they form a tomek link, put a True marker on this"
0.1.5,"sample, and increase counter by one."
0.1.5,Find the nearest neighbour of every point
0.1.5,Send the information to is_tomek function to get boolean vector back
0.1.5,Check if the indices of the samples selected should be returned too
0.1.5,Return the indices of interest
0.1.5,Return data set without majority Tomek links.
0.1.5,Start with the minority class
0.1.5,All the minority class samples will be preserved
0.1.5,If we need to offer support for the indices
0.1.5,Loop over the other classes under picking at random
0.1.5,"If the minority class is up, skip it"
0.1.5,Randomly get one sample from the majority class
0.1.5,Generate the index to select
0.1.5,Create the set C - One majority samples and all minority
0.1.5,Create the set S - all majority samples
0.1.5,Create a k-NN classifier
0.1.5,Fit C into the knn
0.1.5,Check each sample in S if we keep it or drop it
0.1.5,Do not select sample which are already well classified
0.1.5,Classify on S
0.1.5,If the prediction do not agree with the true label
0.1.5,append it in C_x
0.1.5,Keep the index for later
0.1.5,Update C
0.1.5,Fit C into the knn
0.1.5,This experimental to speed up the search
0.1.5,Classify all the element in S and avoid to test the
0.1.5,well classified elements
0.1.5,Find the misclassified S_y
0.1.5,If we need to offer support for the indices selected
0.1.5,Check if the indices of the samples selected should be returned too
0.1.5,Return the indices of interest
0.1.5,Compute the number of cluster needed
0.1.5,Create the clustering object
0.1.5,Start with the minority class
0.1.5,All the minority class samples will be preserved
0.1.5,Loop over the other classes under picking at random
0.1.5,"If the minority class is up, skip it."
0.1.5,Find the centroids via k-means
0.1.5,Concatenate to the minority class
0.1.5,Start with the minority class
0.1.5,All the minority class samples will be preserved
0.1.5,If we need to offer support for the indices
0.1.5,Create a k-NN to fit the whole data
0.1.5,Fit the whole dataset
0.1.5,Loop over the other classes under picking at random
0.1.5,Get the sample of the current class
0.1.5,Get the samples associated
0.1.5,Find the NN for the current class
0.1.5,Get the label of the corresponding to the index
0.1.5,Check which one are the same label than the current class
0.1.5,Make an AND operation through the three neighbours
0.1.5,If the minority class remove the majority samples
0.1.5,Get the index to exclude
0.1.5,Get the index to exclude
0.1.5,Create a vector with the sample to select
0.1.5,Exclude as well the minority sample since that they will be
0.1.5,concatenated later
0.1.5,Get the samples from the majority classes
0.1.5,If we need to offer support for the indices selected
0.1.5,Check if the indices of the samples selected should be returned too
0.1.5,Return the indices of interest
0.1.5,Start with the minority class
0.1.5,All the minority class samples will be preserved
0.1.5,If we need to offer support for the indices
0.1.5,Create a k-NN to fit the whole data
0.1.5,Fit the data
0.1.5,Loop over the other classes under picking at random
0.1.5,"If the minority class is up, skip it"
0.1.5,Get the sample of the current class
0.1.5,Find the NN for the current class
0.1.5,Get the label of the corresponding to the index
0.1.5,Check which one are the same label than the current class
0.1.5,Make the majority vote
0.1.5,Get the samples which agree all together
0.1.5,If we need to offer support for the indices selected
0.1.5,Check if the indices of the samples selected should be returned too
0.1.5,Return the indices of interest
0.1.5,Check if the indices of the samples selected should be returned too
0.1.5,Return the indices of interest
0.1.5,Select the appropriate classifier
0.1.5,Create the different folds
0.1.5,Compute the number of cluster needed
0.1.5,Find the percentile corresponding to the top num_samples
0.1.5,Sample the data
0.1.5,If we need to offer support for the indices
0.1.5,Generate a global dataset to use
0.1.5,Define a negative ratio
0.1.5,Define a ratio greater than 1
0.1.5,Define ratio as an unknown string
0.1.5,Define ratio as a list which is not supported
0.1.5,Define a ratio
0.1.5,Define the parameter for the under-sampling
0.1.5,Create the object
0.1.5,Resample the data
0.1.5,Create a wrong y
0.1.5,Create the object
0.1.5,Fit the data
0.1.5,Define the parameter for the under-sampling
0.1.5,Create the object
0.1.5,Fit the data
0.1.5,Check if the data information have been computed
0.1.5,Define the parameter for the under-sampling
0.1.5,Create the object
0.1.5,Define the parameter for the under-sampling
0.1.5,Create the object
0.1.5,Fit and sample
0.1.5,Define the parameter for the under-sampling
0.1.5,Create the object
0.1.5,Fit and sample
0.1.5,Define the parameter for the under-sampling
0.1.5,Create the object
0.1.5,Fit and sample
0.1.5,Create the object
0.1.5,Generate a global dataset to use
0.1.5,Define a ratio
0.1.5,Create the object
0.1.5,Resample the data
0.1.5,Create a wrong y
0.1.5,Create the object
0.1.5,Fit the data
0.1.5,Check if the data information have been computed
0.1.5,Create the object
0.1.5,Resample the data
0.1.5,Resample the data
0.1.5,Create the object
0.1.5,Generate a global dataset to use
0.1.5,Define a ratio
0.1.5,Create the object
0.1.5,Resample the data
0.1.5,Create a wrong y
0.1.5,Create the object
0.1.5,Fit the data
0.1.5,Check if the data information have been computed
0.1.5,Create the object
0.1.5,Resample the data
0.1.5,Resample the data
0.1.5,Create the object
0.1.5,Generate a global dataset to use
0.1.5,Define a ratio
0.1.5,Create the object
0.1.5,Resample the data
0.1.5,Create a wrong y
0.1.5,Create the object
0.1.5,Fit the data
0.1.5,Check if the data information have been computed
0.1.5,Create the object
0.1.5,Resample the data
0.1.5,Resample the data
0.1.5,Resample the data
0.1.5,Create the object
0.1.5,Generate a global dataset to use
0.1.5,Define a negative ratio
0.1.5,Define a ratio greater than 1
0.1.5,Define ratio as an unknown string
0.1.5,Define ratio as a list which is not supported
0.1.5,Define a ratio
0.1.5,Create the object
0.1.5,Resample the data
0.1.5,Create a wrong y
0.1.5,Create the object
0.1.5,Fit the data
0.1.5,Create the object
0.1.5,Fit the data
0.1.5,Check if the data information have been computed
0.1.5,Create the object
0.1.5,Resample the data
0.1.5,Resample the data
0.1.5,Resample the data
0.1.5,Create the object
0.1.5,Generate a global dataset to use
0.1.5,Define a negative ratio
0.1.5,Define a ratio greater than 1
0.1.5,Define ratio as an unknown string
0.1.5,Define ratio as a list which is not supported
0.1.5,Resample the data
0.1.5,Define a ratio
0.1.5,Create the object
0.1.5,Resample the data
0.1.5,Create a wrong y
0.1.5,Create the object
0.1.5,Fit the data
0.1.5,Create the object
0.1.5,Fit the data
0.1.5,Check if the data information have been computed
0.1.5,Create the object
0.1.5,Resample the data
0.1.5,Resample the data
0.1.5,Resample the data
0.1.5,Resample the data
0.1.5,Resample the data
0.1.5,Resample the data
0.1.5,Resample the data
0.1.5,Resample the data
0.1.5,Resample the data
0.1.5,Create the object
0.1.5,Generate a global dataset to use
0.1.5,Define a ratio
0.1.5,Create the object
0.1.5,Create the object
0.1.5,Resample the data
0.1.5,Create a wrong y
0.1.5,Create the object
0.1.5,Fit the data
0.1.5,Check if the data information have been computed
0.1.5,Create the object
0.1.5,Resample the data
0.1.5,Resample the data
0.1.5,Resample the data
0.1.5,Create the object
0.1.5,Generate a global dataset to use
0.1.5,Define a negative ratio
0.1.5,Define a ratio greater than 1
0.1.5,Define ratio as an unknown string
0.1.5,Define ratio as a list which is not supported
0.1.5,Define a ratio
0.1.5,Define the parameter for the under-sampling
0.1.5,Create the object
0.1.5,Resample the data
0.1.5,Create a wrong y
0.1.5,Create the object
0.1.5,Fit the data
0.1.5,Define the parameter for the under-sampling
0.1.5,Create the object
0.1.5,Fit the data
0.1.5,Check if the data information have been computed
0.1.5,Define the parameter for the under-sampling
0.1.5,Create the object
0.1.5,Define the parameter for the under-sampling
0.1.5,Create the object
0.1.5,Fit and sample
0.1.5,Define the parameter for the under-sampling
0.1.5,Create the object
0.1.5,Fit and sample
0.1.5,Define the parameter for the under-sampling
0.1.5,Create the object
0.1.5,Fit and sample
0.1.5,Create the object
0.1.5,Generate a global dataset to use
0.1.5,Define a ratio
0.1.5,Create the object
0.1.5,Resample the data
0.1.5,Create a wrong y
0.1.5,Create the object
0.1.5,Fit the data
0.1.5,Check if the data information have been computed
0.1.5,Create the object
0.1.5,Resample the data
0.1.5,Resample the data
0.1.5,Create the object
0.1.5,Generate a global dataset to use
0.1.5,Define a negative ratio
0.1.5,Define a ratio greater than 1
0.1.5,Define ratio as an unknown string
0.1.5,Define ratio as a list which is not supported
0.1.5,Define a ratio
0.1.5,Define the parameter for the under-sampling
0.1.5,Create the object
0.1.5,Resample the data
0.1.5,Create a wrong y
0.1.5,Create the object
0.1.5,Fit the data
0.1.5,Define the parameter for the under-sampling
0.1.5,Create the object
0.1.5,Fit the data
0.1.5,Check if the data information have been computed
0.1.5,Define the parameter for the under-sampling
0.1.5,Create the object
0.1.5,Define the parameter for the under-sampling
0.1.5,Create the object
0.1.5,Define the parameter for the under-sampling
0.1.5,Create the object
0.1.5,Fit and sample
0.1.5,Define the parameter for the under-sampling
0.1.5,Create the object
0.1.5,Fit and sample
0.1.5,Generate a global dataset to use
0.1.5,Define a negative ratio
0.1.5,Define a ratio greater than 1
0.1.5,Define ratio as an unknown string
0.1.5,Define ratio as a list which is not supported
0.1.5,Define a ratio
0.1.5,Define the parameter for the under-sampling
0.1.5,Create the object
0.1.5,Resample the data
0.1.5,Create a wrong y
0.1.5,Create the object
0.1.5,Fit the data
0.1.5,Define the parameter for the under-sampling
0.1.5,Create the object
0.1.5,Fit the data
0.1.5,Check if the data information have been computed
0.1.5,Define the parameter for the under-sampling
0.1.5,Create the object
0.1.5,Define the parameter for the under-sampling
0.1.5,Create the object
0.1.5,Fit and sample
0.1.5,Define the parameter for the under-sampling
0.1.5,Create the object
0.1.5,Fit and sample
0.1.5,Define the parameter for the under-sampling
0.1.5,Create the object
0.1.5,Fit and sample
0.1.5,Create the object
0.1.5,Generate a global dataset to use
0.1.5,Define a ratio
0.1.5,Create the object
0.1.5,Resample the data
0.1.5,Create a wrong y
0.1.5,Create the object
0.1.5,Fit the data
0.1.5,Check if the data information have been computed
0.1.5,Create the object
0.1.5,Resample the data
0.1.5,Resample the data
0.1.5,Create the object
0.1.5,Test the various init parameters of the pipeline.
0.1.5,Check that we can't instantiate pipelines with objects without fit
0.1.5,method
0.1.5,Smoke test with only an estimator
0.1.5,Check that params are set
0.1.5,Smoke test the repr:
0.1.5,Test with two objects
0.1.5,Check that we can't use the same stage name twice
0.1.5,Check that params are set
0.1.5,Smoke test the repr:
0.1.5,Check that params are not set when naming them wrong
0.1.5,Test clone
0.1.5,"Check that apart from estimators, the parameters are the same"
0.1.5,Remove estimators that where copied
0.1.5,Test the various methods of the pipeline (anova).
0.1.5,Test with Anova + LogisticRegression
0.1.5,Test that the pipeline can take fit parameters
0.1.5,classifier should return True
0.1.5,and transformer params should not be changed
0.1.5,Test pipeline raises set params error message for nested models.
0.1.5,expected error message
0.1.5,nested model check
0.1.5,Test the various methods of the pipeline (pca + svm).
0.1.5,Test with PCA + SVC
0.1.5,Test the various methods of the pipeline (preprocessing + svm).
0.1.5,check shapes of various prediction functions
0.1.5,test that the fit_predict method is implemented on a pipeline
0.1.5,test that the fit_predict on pipeline yields same results as applying
0.1.5,transform and clustering steps separately
0.1.5,first compute the transform and clustering step separately
0.1.5,use a pipeline to do the transform and clustering in one step
0.1.5,tests that a pipeline does not have fit_predict method when final
0.1.5,step of pipeline does not have fit_predict defined
0.1.5,Test whether pipeline works with a transformer at the end.
0.1.5,Also test pipeline.transform and pipeline.inverse_transform
0.1.5,test transform and fit_transform:
0.1.5,Test whether pipeline works with a transformer missing fit_transform
0.1.5,test fit_transform:
0.1.5,Test the various methods of the pipeline (pca + svm).
0.1.5,Test with PCA + SVC
0.1.5,Test the various methods of the pipeline (pca + svm).
0.1.5,Test with PCA + SVC
0.1.5,Test whether pipeline works with a sampler at the end.
0.1.5,Also test pipeline.sampler
0.1.5,test transform and fit_transform:
0.1.5,Test whether pipeline works with a sampler at the end.
0.1.5,Also test pipeline.sampler
0.1.5,Test the various methods of the pipeline (anova).
0.1.5,Test with RandomUnderSampling + Anova + LogisticRegression
0.1.5,Fit using SMOTE
0.1.5,Transform using SMOTE
0.1.5,Fit and transform using ENN
0.1.5,Fit using SMOTE
0.1.5,Transform using SMOTE
0.1.5,Fit and transform using ENN
0.1.5,Generate a global dataset to use
0.1.5,Define a negative ratio
0.1.5,Define a ratio greater than 1
0.1.5,Define ratio as an unknown string
0.1.5,Define ratio as a list which is not supported
0.1.5,Create the object
0.1.5,Resample the data
0.1.5,Create a wrong y
0.1.5,Create the object
0.1.5,Fit the data
0.1.5,Check if the data information have been computed
0.1.5,Create the object
0.1.5,Create the object
0.1.5,Fit the data
0.1.5,Create the object
0.1.5,Fit the data
0.1.5,Create the object
0.1.5,Generate a global dataset to use
0.1.5,Define a negative ratio
0.1.5,Define a ratio greater than 1
0.1.5,Define ratio as an unknown string
0.1.5,Define ratio as a list which is not supported
0.1.5,Create the object
0.1.5,Resample the data
0.1.5,Create a wrong y
0.1.5,Create the object
0.1.5,Fit the data
0.1.5,Check if the data information have been computed
0.1.5,Create the object
0.1.5,Create the object
0.1.5,Fit the data
0.1.5,Create the object
0.1.5,Fit the data
0.1.5,Create the object
0.1.5,Check the random state
0.1.5,Define the classifier to use
0.1.5,Start with the minority class
0.1.5,Keep the indices of the minority class somewhere if we need to
0.1.5,return them later
0.1.5,Condition to initiliase before the search
0.1.5,Get the initial number of samples to select in the majority class
0.1.5,Create the array characterising the array containing the majority
0.1.5,class
0.1.5,Loop to create the different subsets
0.1.5,Generate an appropriate number of index to extract
0.1.5,from the majority class depending of the false classification
0.1.5,rate of the previous iteration
0.1.5,Mark these indexes as not being considered for next sampling
0.1.5,"For now, we will train and classify on the same data"
0.1.5,"Let see if we should find another solution. Anyway,"
0.1.5,random stuff are still random stuff
0.1.5,Push these data into a new subset
0.1.5,Apply a bootstrap on x_data
0.1.5,Train the classifier using the current data
0.1.5,Train the classifier using the current data
0.1.5,Predict using only the majority class
0.1.5,Basically let's find which sample have to be retained for the
0.1.5,next round
0.1.5,Find the misclassified index to keep them for the next round
0.1.5,Count how many random element will be selected
0.1.5,"We found a new subset, increase the counter"
0.1.5,Check if we have to make an early stopping
0.1.5,Select the remaining data
0.1.5,Select the final batch
0.1.5,Push these data into a new subset
0.1.5,"We found a new subset, increase the counter"
0.1.5,Also check that we will have enough sample to extract at the
0.1.5,next round
0.1.5,Select the remaining data
0.1.5,Select the final batch
0.1.5,Push these data into a new subset
0.1.5,"We found a new subset, increase the counter"
0.1.5,Generate a global dataset to use
0.1.5,Define a negative ratio
0.1.5,Define a ratio greater than 1
0.1.5,Define ratio as an unknown string
0.1.5,Define ratio as a list which is not supported
0.1.5,Define a ratio
0.1.5,Define the parameter for the under-sampling
0.1.5,Create the object
0.1.5,Resample the data
0.1.5,Create a wrong y
0.1.5,Create the object
0.1.5,Fit the data
0.1.5,Define the parameter for the under-sampling
0.1.5,Create the object
0.1.5,Fit the data
0.1.5,Check if the data information have been computed
0.1.5,Define the parameter for the under-sampling
0.1.5,Create the object
0.1.5,Define the ratio parameter
0.1.5,Create the sampling object
0.1.5,Get the different subset
0.1.5,Check each array
0.1.5,Define the ratio parameter
0.1.5,Create the sampling object
0.1.5,Get the different subset
0.1.5,Check each array
0.1.5,Define the ratio parameter
0.1.5,Create the sampling object
0.1.5,Get the different subset
0.1.5,Check each array
0.1.5,Define the ratio parameter
0.1.5,Create the sampling object
0.1.5,Get the different subset
0.1.5,Check each array
0.1.5,Define the ratio parameter
0.1.5,Create the sampling object
0.1.5,Get the different subset
0.1.5,Check each array
0.1.5,Define the ratio parameter
0.1.5,Create the sampling object
0.1.5,Get the different subset
0.1.5,Check each array
0.1.5,Define the ratio parameter
0.1.5,Create the sampling object
0.1.5,Get the different subset
0.1.5,Check each array
0.1.5,Define the ratio parameter
0.1.5,Define the ratio parameter
0.1.5,Create the sampling object
0.1.5,Get the different subset
0.1.5,Check each array
0.1.5,Create the object
0.1.5,Generate a global dataset to use
0.1.5,Define a negative ratio
0.1.5,Define a ratio greater than 1
0.1.5,Define ratio as an unknown string
0.1.5,Define ratio as a list which is not supported
0.1.5,Define a ratio
0.1.5,Define the parameter for the under-sampling
0.1.5,Create the object
0.1.5,Resample the data
0.1.5,Create a wrong y
0.1.5,Create the object
0.1.5,Fit the data
0.1.5,Define the parameter for the under-sampling
0.1.5,Create the object
0.1.5,Fit the data
0.1.5,Check if the data information have been computed
0.1.5,Define the parameter for the under-sampling
0.1.5,Create the object
0.1.5,Define the ratio parameter
0.1.5,Create the sampling object
0.1.5,Get the different subset
0.1.5,Define the ratio parameter
0.1.5,Create the sampling object
0.1.5,Get the different subset
0.1.5,Define the ratio parameter
0.1.5,Create the sampling object
0.1.5,Get the different subset
0.1.5,Create the object
0.1.4,! /usr/bin/env python
0.1.4,"load all vars into globals, otherwise"
0.1.4,the later function call using global vars doesn't work.
0.1.4,"Allow command-lines such as ""python setup.py build install"""
0.1.4,Make sources available using relative paths from this file's directory.
0.1.4,-*- coding: utf-8 -*-
0.1.4,
0.1.4,"imbalanced-learn documentation build configuration file, created by"
0.1.4,sphinx-quickstart on Mon Jan 18 14:44:12 2016.
0.1.4,
0.1.4,This file is execfile()d with the current directory set to its
0.1.4,containing dir.
0.1.4,
0.1.4,Note that not all possible configuration values are present in this
0.1.4,autogenerated file.
0.1.4,
0.1.4,All configuration values have a default; values that are commented out
0.1.4,serve to show the default.
0.1.4,"If extensions (or modules to document with autodoc) are in another directory,"
0.1.4,add these directories to sys.path here. If the directory is relative to the
0.1.4,"documentation root, use os.path.abspath to make it absolute, like shown here."
0.1.4,"sys.path.insert(0, os.path.abspath('.'))"
0.1.4,-- General configuration ---------------------------------------------------
0.1.4,Try to override the matplotlib configuration as early as possible
0.1.4,-- General configuration ------------------------------------------------
0.1.4,"If your documentation needs a minimal Sphinx version, state it here."
0.1.4,needs_sphinx = '1.0'
0.1.4,"Add any Sphinx extension module names here, as strings. They can be"
0.1.4,extensions coming with Sphinx (named 'sphinx.ext.*') or your custom
0.1.4,ones.
0.1.4,path to your examples scripts
0.1.4,path where to save gallery generated examples
0.1.4,"Add any paths that contain templates here, relative to this directory."
0.1.4,generate autosummary even if no references
0.1.4,The suffix of source filenames.
0.1.4,The encoding of source files.
0.1.4,source_encoding = 'utf-8-sig'
0.1.4,Generate the plots for the gallery
0.1.4,The master toctree document.
0.1.4,General information about the project.
0.1.4,"The version info for the project you're documenting, acts as replacement for"
0.1.4,"|version| and |release|, also used in various other places throughout the"
0.1.4,built documents.
0.1.4,
0.1.4,The short X.Y version.
0.1.4,"The full version, including alpha/beta/rc tags."
0.1.4,The language for content autogenerated by Sphinx. Refer to documentation
0.1.4,for a list of supported languages.
0.1.4,language = None
0.1.4,"There are two options for replacing |today|: either, you set today to some"
0.1.4,"non-false value, then it is used:"
0.1.4,today = ''
0.1.4,"Else, today_fmt is used as the format for a strftime call."
0.1.4,"today_fmt = '%B %d, %Y'"
0.1.4,"List of patterns, relative to source directory, that match files and"
0.1.4,directories to ignore when looking for source files.
0.1.4,The reST default role (used for this markup: `text`) to use for all
0.1.4,documents.
0.1.4,default_role = None
0.1.4,"If true, '()' will be appended to :func: etc. cross-reference text."
0.1.4,"If true, the current module name will be prepended to all description"
0.1.4,unit titles (such as .. function::).
0.1.4,add_module_names = True
0.1.4,"If true, sectionauthor and moduleauthor directives will be shown in the"
0.1.4,output. They are ignored by default.
0.1.4,show_authors = False
0.1.4,The name of the Pygments (syntax highlighting) style to use.
0.1.4,A list of ignored prefixes for module index sorting.
0.1.4,modindex_common_prefix = []
0.1.4,"If true, keep warnings as ""system message"" paragraphs in the built documents."
0.1.4,keep_warnings = False
0.1.4,-- Options for HTML output ----------------------------------------------
0.1.4,The theme to use for HTML and HTML Help pages.  See the documentation for
0.1.4,a list of builtin themes.
0.1.4,Theme options are theme-specific and customize the look and feel of a theme
0.1.4,"further.  For a list of options available for each theme, see the"
0.1.4,documentation.
0.1.4,html_theme_options = {}
0.1.4,"Add any paths that contain custom themes here, relative to this directory."
0.1.4,"The name for this set of Sphinx documents.  If None, it defaults to"
0.1.4,"""<project> v<release> documentation""."
0.1.4,html_title = None
0.1.4,A shorter title for the navigation bar.  Default is the same as html_title.
0.1.4,html_short_title = None
0.1.4,The name of an image file (relative to this directory) to place at the top
0.1.4,of the sidebar.
0.1.4,html_logo = None
0.1.4,The name of an image file (within the static path) to use as favicon of the
0.1.4,docs.  This file should be a Windows icon file (.ico) being 16x16 or 32x32
0.1.4,pixels large.
0.1.4,html_favicon = None
0.1.4,"Add any paths that contain custom static files (such as style sheets) here,"
0.1.4,"relative to this directory. They are copied after the builtin static files,"
0.1.4,"so a file named ""default.css"" will overwrite the builtin ""default.css""."
0.1.4,Add any extra paths that contain custom files (such as robots.txt or
0.1.4,".htaccess) here, relative to this directory. These files are copied"
0.1.4,directly to the root of the documentation.
0.1.4,html_extra_path = []
0.1.4,"If not '', a 'Last updated on:' timestamp is inserted at every page bottom,"
0.1.4,using the given strftime format.
0.1.4,"html_last_updated_fmt = '%b %d, %Y'"
0.1.4,"If true, SmartyPants will be used to convert quotes and dashes to"
0.1.4,typographically correct entities.
0.1.4,html_use_smartypants = True
0.1.4,"Custom sidebar templates, maps document names to template names."
0.1.4,html_sidebars = {}
0.1.4,"Additional templates that should be rendered to pages, maps page names to"
0.1.4,template names.
0.1.4,html_additional_pages = {}
0.1.4,"If false, no module index is generated."
0.1.4,html_domain_indices = True
0.1.4,"If false, no index is generated."
0.1.4,html_use_index = True
0.1.4,"If true, the index is split into individual pages for each letter."
0.1.4,html_split_index = False
0.1.4,"If true, links to the reST sources are added to the pages."
0.1.4,html_show_sourcelink = True
0.1.4,"If true, ""Created using Sphinx"" is shown in the HTML footer. Default is True."
0.1.4,html_show_sphinx = True
0.1.4,"If true, ""(C) Copyright ..."" is shown in the HTML footer. Default is True."
0.1.4,html_show_copyright = True
0.1.4,"If true, an OpenSearch description file will be output, and all pages will"
0.1.4,contain a <link> tag referring to it.  The value of this option must be the
0.1.4,base URL from which the finished HTML is served.
0.1.4,html_use_opensearch = ''
0.1.4,"This is the file name suffix for HTML files (e.g. "".xhtml"")."
0.1.4,html_file_suffix = None
0.1.4,Output file base name for HTML help builder.
0.1.4,-- Options for LaTeX output ---------------------------------------------
0.1.4,The paper size ('letterpaper' or 'a4paper').
0.1.4,"'papersize': 'letterpaper',"
0.1.4,"The font size ('10pt', '11pt' or '12pt')."
0.1.4,"'pointsize': '10pt',"
0.1.4,Additional stuff for the LaTeX preamble.
0.1.4,"'preamble': '',"
0.1.4,Grouping the document tree into LaTeX files. List of tuples
0.1.4,"(source start file, target name, title,"
0.1.4,"author, documentclass [howto, manual, or own class])."
0.1.4,The name of an image file (relative to this directory) to place at the top of
0.1.4,the title page.
0.1.4,latex_logo = None
0.1.4,"For ""manual"" documents, if this is true, then toplevel headings are parts,"
0.1.4,not chapters.
0.1.4,latex_use_parts = False
0.1.4,"If true, show page references after internal links."
0.1.4,latex_show_pagerefs = False
0.1.4,"If true, show URL addresses after external links."
0.1.4,latex_show_urls = False
0.1.4,Documents to append as an appendix to all manuals.
0.1.4,latex_appendices = []
0.1.4,"If false, no module index is generated."
0.1.4,latex_domain_indices = True
0.1.4,-- Options for manual page output ---------------------------------------
0.1.4,One entry per manual page. List of tuples
0.1.4,"(source start file, name, description, authors, manual section)."
0.1.4,"If true, show URL addresses after external links."
0.1.4,man_show_urls = False
0.1.4,-- Options for Texinfo output -------------------------------------------
0.1.4,Grouping the document tree into Texinfo files. List of tuples
0.1.4,"(source start file, target name, title, author,"
0.1.4,"dir menu entry, description, category)"
0.1.4,"generate empty examples files, so that we don't get"
0.1.4,inclusion errors if there are no examples for a class / module
0.1.4,touch file
0.1.4,Documents to append as an appendix to all manuals.
0.1.4,texinfo_appendices = []
0.1.4,"If false, no module index is generated."
0.1.4,texinfo_domain_indices = True
0.1.4,"How to display URL addresses: 'footnote', 'no', or 'inline'."
0.1.4,texinfo_show_urls = 'footnote'
0.1.4,"If true, do not generate a @detailmenu in the ""Top"" node's menu."
0.1.4,texinfo_no_detailmenu = False
0.1.4,Example configuration for intersphinx: refer to the Python standard library.
0.1.4,Define some color for the plotting
0.1.4,Generate the dataset
0.1.4,Instanciate a PCA object for the sake of easy visualisation
0.1.4,Fit and transform x to visualise inside a 2D feature space
0.1.4,Apply SMOTE SVM
0.1.4,"Two subplots, unpack the axes array immediately"
0.1.4,Define some color for the plotting
0.1.4,Generate the dataset
0.1.4,Instanciate a PCA object for the sake of easy visualisation
0.1.4,Fit and transform x to visualise inside a 2D feature space
0.1.4,Apply Borderline SMOTE 2
0.1.4,"Two subplots, unpack the axes array immediately"
0.1.4,Define some color for the plotting
0.1.4,Generate the dataset
0.1.4,Instanciate a PCA object for the sake of easy visualisation
0.1.4,Fit and transform x to visualise inside a 2D feature space
0.1.4,Apply regular SMOTE
0.1.4,"Two subplots, unpack the axes array immediately"
0.1.4,Define some color for the plotting
0.1.4,Generate the dataset
0.1.4,Instanciate a PCA object for the sake of easy visualisation
0.1.4,Fit and transform x to visualise inside a 2D feature space
0.1.4,Apply the random over-sampling
0.1.4,"Two subplots, unpack the axes array immediately"
0.1.4,Define some color for the plotting
0.1.4,Generate the dataset
0.1.4,Instanciate a PCA object for the sake of easy visualisation
0.1.4,Fit and transform x to visualise inside a 2D feature space
0.1.4,Apply Borderline SMOTE 1
0.1.4,"Two subplots, unpack the axes array immediately"
0.1.4,Define some color for the plotting
0.1.4,Generate the dataset
0.1.4,Instanciate a PCA object for the sake of easy visualisation
0.1.4,Fit and transform x to visualise inside a 2D feature space
0.1.4,Apply the random over-sampling
0.1.4,"Two subplots, unpack the axes array immediately"
0.1.4,Define some color for the plotting
0.1.4,Generate the dataset
0.1.4,Instanciate a PCA object for the sake of easy visualisation
0.1.4,Fit and transform x to visualise inside a 2D feature space
0.1.4,Apply SMOTE + ENN
0.1.4,"Two subplots, unpack the axes array immediately"
0.1.4,Define some color for the plotting
0.1.4,Generate the dataset
0.1.4,Instanciate a PCA object for the sake of easy visualisation
0.1.4,Fit and transform x to visualise inside a 2D feature space
0.1.4,Apply SMOTE + Tomek links
0.1.4,"Two subplots, unpack the axes array immediately"
0.1.4,Define some color for the plotting
0.1.4,Generate the dataset
0.1.4,Instanciate a PCA object for the sake of easy visualisation
0.1.4,Fit and transform x to visualise inside a 2D feature space
0.1.4,Apply Balance Cascade method
0.1.4,"Two subplots, unpack the axes array immediately"
0.1.4,Define some color for the plotting
0.1.4,Generate the dataset
0.1.4,Instanciate a PCA object for the sake of easy visualisation
0.1.4,Fit and transform x to visualise inside a 2D feature space
0.1.4,Apply Easy Ensemble
0.1.4,"Two subplots, unpack the axes array immediately"
0.1.4,Define some color for the plotting
0.1.4,Generate the dataset
0.1.4,Instanciate a PCA object for the sake of easy visualisation
0.1.4,Fit and transform x to visualise inside a 2D feature space
0.1.4,"Three subplots, unpack the axes array immediately"
0.1.4,Apply the ENN
0.1.4,Apply the RENN
0.1.4,Define some color for the plotting
0.1.4,Generate the dataset
0.1.4,Instanciate a PCA object for the sake of easy visualisation
0.1.4,Fit and transform x to visualise inside a 2D feature space
0.1.4,Apply Nearmiss 3
0.1.4,"Two subplots, unpack the axes array immediately"
0.1.4,Define some color for the plotting
0.1.4,Generate the dataset
0.1.4,Instanciate a PCA object for the sake of easy visualisation
0.1.4,Fit and transform x to visualise inside a 2D feature space
0.1.4,Apply Edited Nearest Neighbours
0.1.4,"Two subplots, unpack the axes array immediately"
0.1.4,Define some color for the plotting
0.1.4,Generate the dataset
0.1.4,Instanciate a PCA object for the sake of easy visualisation
0.1.4,Fit and transform x to visualise inside a 2D feature space
0.1.4,Apply Tomek Links cleaning
0.1.4,"Two subplots, unpack the axes array immediately"
0.1.4,Define some color for the plotting
0.1.4,Generate the dataset
0.1.4,Instanciate a PCA object for the sake of easy visualisation
0.1.4,Fit and transform x to visualise inside a 2D feature space
0.1.4,Apply One-Sided Selection
0.1.4,"Two subplots, unpack the axes array immediately"
0.1.4,Define some color for the plotting
0.1.4,Generate the dataset
0.1.4,Instanciate a PCA object for the sake of easy visualisation
0.1.4,Fit and transform x to visualise inside a 2D feature space
0.1.4,Apply Nearmiss 2
0.1.4,"Two subplots, unpack the axes array immediately"
0.1.4,Define some color for the plotting
0.1.4,Generate the dataset
0.1.4,Instanciate a PCA object for the sake of easy visualisation
0.1.4,Fit and transform x to visualise inside a 2D feature space
0.1.4,Apply neighbourhood cleaning rule
0.1.4,"Two subplots, unpack the axes array immediately"
0.1.4,Define some color for the plotting
0.1.4,Generate the dataset
0.1.4,Instanciate a PCA object for the sake of easy visualisation
0.1.4,Fit and transform x to visualise inside a 2D feature space
0.1.4,Apply Condensed Nearest Neighbours
0.1.4,"Two subplots, unpack the axes array immediately"
0.1.4,Define some color for the plotting
0.1.4,Generate the dataset
0.1.4,Instanciate a PCA object for the sake of easy visualisation
0.1.4,Fit and transform x to visualise inside a 2D feature space
0.1.4,Apply Cluster Centroids
0.1.4,"Two subplots, unpack the axes array immediately"
0.1.4,Define some color for the plotting
0.1.4,Generate the dataset
0.1.4,Instanciate a PCA object for the sake of easy visualisation
0.1.4,Fit and transform x to visualise inside a 2D feature space
0.1.4,Apply the random under-sampling
0.1.4,"Two subplots, unpack the axes array immediately"
0.1.4,Define some color for the plotting
0.1.4,Generate the dataset
0.1.4,"Two subplots, unpack the axes array immediately"
0.1.4,Define some color for the plotting
0.1.4,Generate the dataset
0.1.4,Instanciate a PCA object for the sake of easy visualisation
0.1.4,Fit and transform x to visualise inside a 2D feature space
0.1.4,Apply Nearmiss 1
0.1.4,"Two subplots, unpack the axes array immediately"
0.1.4,Generate the dataset
0.1.4,Instanciate a PCA object for the sake of easy visualisation
0.1.4,Create the samplers
0.1.4,Create teh classifier
0.1.4,Make the splits
0.1.4,Add one transformers and two samplers in the pipeline object
0.1.4,Based on NiLearn package
0.1.4,License: simplified BSD
0.1.4,"PEP0440 compatible formatted version, see:"
0.1.4,https://www.python.org/dev/peps/pep-0440/
0.1.4,
0.1.4,Generic release markers:
0.1.4,X.Y
0.1.4,X.Y.Z # For bugfix releases
0.1.4,
0.1.4,Admissible pre-release markers:
0.1.4,X.YaN # Alpha release
0.1.4,X.YbN # Beta release
0.1.4,X.YrcN # Release Candidate
0.1.4,X.Y # Final release
0.1.4,
0.1.4,Dev branch marker is: 'X.Y.dev' or 'X.Y.devN' where N is an integer.
0.1.4,'X.Y.dev0' is the canonical version of 'X.Y.dev'
0.1.4,
0.1.4,"This is a tuple to preserve order, so that dependencies are checked"
0.1.4,in some meaningful order (more => less 'core').  We avoid using
0.1.4,collections.OrderedDict to preserve Python 2.6 compatibility.
0.1.4,Avoid choking on modules with no __version__ attribute
0.1.4,Skip check only when installing and it's a module that
0.1.4,will be auto-installed.
0.1.4,Check the consistency of X and y
0.1.4,Get all the unique elements in the target array
0.1.4,# Raise an error if there is only one class
0.1.4,if uniques.size == 1:
0.1.4,"raise RuntimeError(""Only one class detected, aborting..."")"
0.1.4,Raise a warning for the moment to be compatible with BaseEstimator
0.1.4,Store the size of X to check at sampling time if we have the
0.1.4,same data
0.1.4,Create a dictionary containing the class statistics
0.1.4,Find the minority and majority classes
0.1.4,Check if the ratio provided at initialisation make sense
0.1.4,Check the consistency of X and y
0.1.4,Check that the data have been fitted
0.1.4,Check if the size of the data is identical than at fitting
0.1.4,The ratio correspond to the number of samples in the minority class
0.1.4,"over the number of samples in the majority class. Thus, the ratio"
0.1.4,cannot be greater than 1.0
0.1.4,Adapted from scikit-learn
0.1.4,Author: Edouard Duchesnay
0.1.4,Gael Varoquaux
0.1.4,Virgile Fritsch
0.1.4,Alexandre Gramfort
0.1.4,Lars Buitinck
0.1.4,chkoar
0.1.4,License: BSD
0.1.4,BaseEstimator interface
0.1.4,shallow copy of steps
0.1.4,Estimator interface
0.1.4,Boolean controlling whether the joblib caches should be
0.1.4,"flushed if the version of certain modules changes (eg nibabel, as it"
0.1.4,does not respect the backward compatibility in some of its internal
0.1.4,structures
0.1.4,This  is used in nilearn._utils.cache_mixin
0.1.4,list all submodules available in imblearn and version
0.1.4,Keep the samples from the majority class
0.1.4,Loop over the other classes over picking at random
0.1.4,"If this is the majority class, skip it"
0.1.4,Define the number of sample to create
0.1.4,Pick some elements at random
0.1.4,Concatenate to the majority class
0.1.4,Keep the samples from the majority class
0.1.4,Define the number of sample to create
0.1.4,We handle only two classes problem for the moment.
0.1.4,Start by separating minority class features and target values.
0.1.4,Print if verbose is true
0.1.4,"Look for k-th nearest neighbours, excluding, of course, the"
0.1.4,point itself.
0.1.4,Get the distance to the NN
0.1.4,Compute the ratio of majority samples next to minority samples
0.1.4,Check that we found at least some neighbours belonging to the
0.1.4,majority class
0.1.4,Normalize the ratio
0.1.4,Compute the number of sample to be generated
0.1.4,For each minority samples
0.1.4,Pick-up the neighbors wanted
0.1.4,Create a new sample
0.1.4,Find the NN for each samples
0.1.4,Exclude the sample itself
0.1.4,Count how many NN belong to the minority class
0.1.4,Find the class corresponding to the label in x
0.1.4,Compute the number of majority samples in the NN
0.1.4,Samples are in danger for m/2 <= m' < m
0.1.4,Samples are noise for m = m'
0.1.4,Check the consistency of X
0.1.4,Check the random state
0.1.4,A matrix to store the synthetic samples
0.1.4,# Set seeds
0.1.4,"seeds = random_state.randint(low=0,"
0.1.4,"high=100 * len(nn_num.flatten()),"
0.1.4,size=n_samples)
0.1.4,Randomly pick samples to construct neighbours from
0.1.4,Loop over the NN matrix and create new samples
0.1.4,"NN lines relate to original sample, columns to its"
0.1.4,nearest neighbours
0.1.4,"Take a step of random size (0,1) in the direction of the"
0.1.4,n nearest neighbours
0.1.4,if self.random_state is None:
0.1.4,np.random.seed(seeds[i])
0.1.4,else:
0.1.4,np.random.seed(self.random_state)
0.1.4,Construct synthetic sample
0.1.4,The returned target vector is simply a repetition of the
0.1.4,minority label
0.1.4,Define the number of sample to create
0.1.4,We handle only two classes problem for the moment.
0.1.4,Start by separating minority class features and target values.
0.1.4,If regular SMOTE is to be performed
0.1.4,"Look for k-th nearest neighbours, excluding, of course, the"
0.1.4,point itself.
0.1.4,Matrix with k-th nearest neighbours indexes for each minority
0.1.4,element.
0.1.4,--- Generating synthetic samples
0.1.4,Use static method make_samples to generate minority samples
0.1.4,Concatenate the newly generated samples to the original data set
0.1.4,Find the NNs for all samples in the data set.
0.1.4,Boolean array with True for minority samples in danger
0.1.4,"If all minority samples are safe, return the original data set."
0.1.4,"All are safe, nothing to be done here."
0.1.4,"If we got here is because some samples are in danger, we need to"
0.1.4,find the NNs among the minority class to create the new synthetic
0.1.4,samples.
0.1.4,
0.1.4,We start by changing the number of NNs to consider from m + 1
0.1.4,to k + 1
0.1.4,nns...#
0.1.4,B1 and B2 types diverge here!!!
0.1.4,Create synthetic samples for borderline points.
0.1.4,Concatenate the newly generated samples to the original
0.1.4,dataset
0.1.4,Reset the k-neighbours to m+1 neighbours
0.1.4,Split the number of synthetic samples between only minority
0.1.4,"(type 1), or minority and majority (with reduced step size)"
0.1.4,(type 2).
0.1.4,The fraction is sampled from a beta distribution centered
0.1.4,around 0.5 with variance ~0.01
0.1.4,Only minority
0.1.4,Only majority with smaller step size
0.1.4,Concatenate the newly generated samples to the original
0.1.4,data set
0.1.4,Reset the k-neighbours to m+1 neighbours
0.1.4,The SVM smote model fits a support vector machine
0.1.4,classifier to the data and uses the support vector to
0.1.4,"provide a notion of boundary. Unlike regular smote, where"
0.1.4,such notion relies on proportion of nearest neighbours
0.1.4,belonging to each class.
0.1.4,Fit SVM to the full data#
0.1.4,Find the support vectors and their corresponding indexes
0.1.4,"First, find the nn of all the samples to identify samples"
0.1.4,in danger and noisy ones
0.1.4,"As usual, fit a nearest neighbour model to the data"
0.1.4,"Now, get rid of noisy support vectors"
0.1.4,Remove noisy support vectors
0.1.4,Proceed to find support vectors NNs among the minority class
0.1.4,Split the number of synthetic samples between interpolation and
0.1.4,extrapolation
0.1.4,The fraction are sampled from a beta distribution with mean
0.1.4,0.5 and variance 0.01#
0.1.4,Interpolate samples in danger
0.1.4,Extrapolate safe samples
0.1.4,Concatenate the newly generated samples to the original data set
0.1.4,not any support vectors in danger
0.1.4,All the support vector in danger
0.1.4,Reset the k-neighbours to m+1 neighbours
0.1.4,--- NN object
0.1.4,Import the NN object from scikit-learn library. Since in the smote
0.1.4,"variations we must first find samples that are in danger, we"
0.1.4,initialize the NN object differently depending on the method chosen
0.1.4,"Regular smote does not look for samples in danger, instead it"
0.1.4,creates synthetic samples directly from the k-th nearest
0.1.4,neighbours with not filtering
0.1.4,"Borderline1, 2 and SVM variations of smote must first look for"
0.1.4,samples that could be considered noise and samples that live
0.1.4,"near the boundary between the classes. Therefore, before"
0.1.4,"creating synthetic samples from the k-th nns, it first look"
0.1.4,for m nearest neighbors to decide whether or not a sample is
0.1.4,noise or near the boundary.
0.1.4,--- SVM smote
0.1.4,"Unlike the borderline variations, the SVM variation uses the support"
0.1.4,vectors to decide which samples are in danger (near the boundary).
0.1.4,Additionally it also introduces extrapolation for samples that are
0.1.4,considered safe (far from boundary) and interpolation for samples
0.1.4,in danger (near the boundary). The level of extrapolation is
0.1.4,controled by the out_step.
0.1.4,Store SVM object with any parameters
0.1.4,Generate a global dataset to use
0.1.4,Define a negative ratio
0.1.4,Define a ratio greater than 1
0.1.4,Define ratio as an unknown string
0.1.4,Define ratio as a list which is not supported
0.1.4,Define a ratio
0.1.4,Create the object
0.1.4,Resample the data
0.1.4,Create a wrong y
0.1.4,Create the object
0.1.4,Fit the data
0.1.4,Create the object
0.1.4,Fit the data
0.1.4,Check if the data information have been computed
0.1.4,Create the object
0.1.4,Resample the data
0.1.4,Resample the data
0.1.4,Create the object
0.1.4,Generate a global dataset to use
0.1.4,Define a negative ratio
0.1.4,Define a ratio greater than 1
0.1.4,Define ratio as an unknown string
0.1.4,Define ratio as a list which is not supported
0.1.4,Define a ratio
0.1.4,Create the object
0.1.4,Resample the data
0.1.4,Create a wrong y
0.1.4,Create the object
0.1.4,Fit the data
0.1.4,Create the object
0.1.4,Fit the data
0.1.4,Check if the data information have been computed
0.1.4,Create the object
0.1.4,Resample the data
0.1.4,Resample the data
0.1.4,Create the object
0.1.4,Generate a global dataset to use
0.1.4,Define a negative ratio
0.1.4,Define a ratio greater than 1
0.1.4,Define ratio as an unknown string
0.1.4,Define ratio as a list which is not supported
0.1.4,Create the object
0.1.4,Resample the data
0.1.4,Create a wrong y
0.1.4,Create the object
0.1.4,Fit the data
0.1.4,Check if the data information have been computed
0.1.4,Create the object
0.1.4,Create the object
0.1.4,Fit the data
0.1.4,Create the object
0.1.4,Fit the data
0.1.4,Create the object
0.1.4,Fit the data
0.1.4,Create the object
0.1.4,Fit the data
0.1.4,Create the object
0.1.4,Fit the data
0.1.4,Create the object
0.1.4,Start with the minority class
0.1.4,All the minority class samples will be preserved
0.1.4,If we need to offer support for the indices
0.1.4,Loop over the other classes under picking at random
0.1.4,"If the minority class is up, skip it"
0.1.4,Randomly get one sample from the majority class
0.1.4,Generate the index to select
0.1.4,Create the set C
0.1.4,Create the set S
0.1.4,Remove the seed from S since that it will be added anyway
0.1.4,Create a k-NN classifier
0.1.4,Fit C into the knn
0.1.4,Classify on S
0.1.4,Find the misclassified S_y
0.1.4,If we need to offer support for the indices selected
0.1.4,We concatenate the misclassified samples with the seed and the
0.1.4,minority samples
0.1.4,Find the nearest neighbour of every point
0.1.4,Send the information to is_tomek function to get boolean vector back
0.1.4,Check if the indices of the samples selected should be returned too
0.1.4,Return the indices of interest
0.1.4,Return data set without majority Tomek links.
0.1.4,Compute the distance considering the farthest neighbour
0.1.4,Sort the list of distance and get the index
0.1.4,Select the desired number of samples
0.1.4,Assign the parameter of the element of this class
0.1.4,Check that the version asked is implemented
0.1.4,Start with the minority class
0.1.4,All the minority class samples will be preserved
0.1.4,Compute the number of cluster needed
0.1.4,If we need to offer support for the indices
0.1.4,"For each element of the current class, find the set of NN"
0.1.4,of the minority class
0.1.4,Call the constructor of the NN
0.1.4,Fit the minority class since that we want to know the distance
0.1.4,to these point
0.1.4,Loop over the other classes under picking at random
0.1.4,"If the minority class is up, skip it"
0.1.4,Get the samples corresponding to the current class
0.1.4,Find the NN
0.1.4,Select the right samples
0.1.4,Find the NN
0.1.4,Select the right samples
0.1.4,We need a new NN object to fit the current class
0.1.4,Find the set of NN to the minority class
0.1.4,Create the subset containing the samples found during the NN
0.1.4,search. Linearize the indexes and remove the double values
0.1.4,Create the subset
0.1.4,Compute the NN considering the current class
0.1.4,If we need to offer support for the indices selected
0.1.4,Check if the indices of the samples selected should be returned too
0.1.4,Return the indices of interest
0.1.4,Compute the number of clusters needed
0.1.4,All the minority class samples will be preserved
0.1.4,If we need to offer support for the indices
0.1.4,Loop over the other classes under-picking at random
0.1.4,"If the minority class is up, skip it"
0.1.4,Pick some elements at random
0.1.4,If we need to offer support for the indices selected
0.1.4,Concatenate to the minority class
0.1.4,Check if the indices of the samples selected should be returned as
0.1.4,well
0.1.4,Return the indices of interest
0.1.4,"Initialize the boolean result as false, and also a counter"
0.1.4,Loop through each sample and looks whether it belongs to the minority
0.1.4,"class. If it does, we don't consider it since we want to keep all"
0.1.4,"minority samples. If, however, it belongs to the majority sample we"
0.1.4,look at its first neighbour. If its closest neighbour also has the
0.1.4,"current sample as its closest neighbour, the two form a Tomek link."
0.1.4,"If they form a tomek link, put a True marker on this"
0.1.4,"sample, and increase counter by one."
0.1.4,Find the nearest neighbour of every point
0.1.4,Send the information to is_tomek function to get boolean vector back
0.1.4,Check if the indices of the samples selected should be returned too
0.1.4,Return the indices of interest
0.1.4,Return data set without majority Tomek links.
0.1.4,Start with the minority class
0.1.4,All the minority class samples will be preserved
0.1.4,If we need to offer support for the indices
0.1.4,Loop over the other classes under picking at random
0.1.4,"If the minority class is up, skip it"
0.1.4,Randomly get one sample from the majority class
0.1.4,Generate the index to select
0.1.4,Create the set C - One majority samples and all minority
0.1.4,Create the set S - all majority samples
0.1.4,Create a k-NN classifier
0.1.4,Fit C into the knn
0.1.4,Check each sample in S if we keep it or drop it
0.1.4,Do not select sample which are already well classified
0.1.4,Classify on S
0.1.4,If the prediction do not agree with the true label
0.1.4,append it in C_x
0.1.4,Keep the index for later
0.1.4,Update C
0.1.4,Fit C into the knn
0.1.4,This experimental to speed up the search
0.1.4,Classify all the element in S and avoid to test the
0.1.4,well classified elements
0.1.4,Find the misclassified S_y
0.1.4,If we need to offer support for the indices selected
0.1.4,Check if the indices of the samples selected should be returned too
0.1.4,Return the indices of interest
0.1.4,Compute the number of cluster needed
0.1.4,Create the clustering object
0.1.4,Start with the minority class
0.1.4,All the minority class samples will be preserved
0.1.4,Loop over the other classes under picking at random
0.1.4,"If the minority class is up, skip it."
0.1.4,Find the centroids via k-means
0.1.4,Concatenate to the minority class
0.1.4,Start with the minority class
0.1.4,All the minority class samples will be preserved
0.1.4,If we need to offer support for the indices
0.1.4,Create a k-NN to fit the whole data
0.1.4,Fit the whole dataset
0.1.4,Loop over the other classes under picking at random
0.1.4,Get the sample of the current class
0.1.4,Get the samples associated
0.1.4,Find the NN for the current class
0.1.4,Get the label of the corresponding to the index
0.1.4,Check which one are the same label than the current class
0.1.4,Make an AND operation through the three neighbours
0.1.4,If the minority class remove the majority samples
0.1.4,Get the index to exclude
0.1.4,Get the index to exclude
0.1.4,Create a vector with the sample to select
0.1.4,Exclude as well the minority sample since that they will be
0.1.4,concatenated later
0.1.4,Get the samples from the majority classes
0.1.4,If we need to offer support for the indices selected
0.1.4,Check if the indices of the samples selected should be returned too
0.1.4,Return the indices of interest
0.1.4,Start with the minority class
0.1.4,All the minority class samples will be preserved
0.1.4,If we need to offer support for the indices
0.1.4,Create a k-NN to fit the whole data
0.1.4,Fit the data
0.1.4,Loop over the other classes under picking at random
0.1.4,"If the minority class is up, skip it"
0.1.4,Get the sample of the current class
0.1.4,Find the NN for the current class
0.1.4,Get the label of the corresponding to the index
0.1.4,Check which one are the same label than the current class
0.1.4,Make the majority vote
0.1.4,Get the samples which agree all together
0.1.4,If we need to offer support for the indices selected
0.1.4,Check if the indices of the samples selected should be returned too
0.1.4,Return the indices of interest
0.1.4,Check if the indices of the samples selected should be returned too
0.1.4,Return the indices of interest
0.1.4,Select the appropriate classifier
0.1.4,Create the different folds
0.1.4,Compute the number of cluster needed
0.1.4,Find the percentile corresponding to the top num_samples
0.1.4,Sample the data
0.1.4,If we need to offer support for the indices
0.1.4,Generate a global dataset to use
0.1.4,Define a negative ratio
0.1.4,Define a ratio greater than 1
0.1.4,Define ratio as an unknown string
0.1.4,Define ratio as a list which is not supported
0.1.4,Define a ratio
0.1.4,Define the parameter for the under-sampling
0.1.4,Create the object
0.1.4,Resample the data
0.1.4,Create a wrong y
0.1.4,Create the object
0.1.4,Fit the data
0.1.4,Define the parameter for the under-sampling
0.1.4,Create the object
0.1.4,Fit the data
0.1.4,Check if the data information have been computed
0.1.4,Define the parameter for the under-sampling
0.1.4,Create the object
0.1.4,Define the parameter for the under-sampling
0.1.4,Create the object
0.1.4,Fit and sample
0.1.4,Define the parameter for the under-sampling
0.1.4,Create the object
0.1.4,Fit and sample
0.1.4,Define the parameter for the under-sampling
0.1.4,Create the object
0.1.4,Fit and sample
0.1.4,Create the object
0.1.4,Generate a global dataset to use
0.1.4,Define a ratio
0.1.4,Create the object
0.1.4,Resample the data
0.1.4,Create a wrong y
0.1.4,Create the object
0.1.4,Fit the data
0.1.4,Check if the data information have been computed
0.1.4,Create the object
0.1.4,Resample the data
0.1.4,Resample the data
0.1.4,Create the object
0.1.4,Generate a global dataset to use
0.1.4,Define a ratio
0.1.4,Create the object
0.1.4,Resample the data
0.1.4,Create a wrong y
0.1.4,Create the object
0.1.4,Fit the data
0.1.4,Check if the data information have been computed
0.1.4,Create the object
0.1.4,Resample the data
0.1.4,Resample the data
0.1.4,Create the object
0.1.4,Generate a global dataset to use
0.1.4,Define a ratio
0.1.4,Create the object
0.1.4,Resample the data
0.1.4,Create a wrong y
0.1.4,Create the object
0.1.4,Fit the data
0.1.4,Check if the data information have been computed
0.1.4,Create the object
0.1.4,Resample the data
0.1.4,Resample the data
0.1.4,Resample the data
0.1.4,Create the object
0.1.4,Generate a global dataset to use
0.1.4,Define a negative ratio
0.1.4,Define a ratio greater than 1
0.1.4,Define ratio as an unknown string
0.1.4,Define ratio as a list which is not supported
0.1.4,Define a ratio
0.1.4,Create the object
0.1.4,Resample the data
0.1.4,Create a wrong y
0.1.4,Create the object
0.1.4,Fit the data
0.1.4,Create the object
0.1.4,Fit the data
0.1.4,Check if the data information have been computed
0.1.4,Create the object
0.1.4,Resample the data
0.1.4,Resample the data
0.1.4,Resample the data
0.1.4,Create the object
0.1.4,Generate a global dataset to use
0.1.4,Define a negative ratio
0.1.4,Define a ratio greater than 1
0.1.4,Define ratio as an unknown string
0.1.4,Define ratio as a list which is not supported
0.1.4,Resample the data
0.1.4,Define a ratio
0.1.4,Create the object
0.1.4,Resample the data
0.1.4,Create a wrong y
0.1.4,Create the object
0.1.4,Fit the data
0.1.4,Create the object
0.1.4,Fit the data
0.1.4,Check if the data information have been computed
0.1.4,Create the object
0.1.4,Resample the data
0.1.4,Resample the data
0.1.4,Resample the data
0.1.4,Resample the data
0.1.4,Resample the data
0.1.4,Resample the data
0.1.4,Resample the data
0.1.4,Resample the data
0.1.4,Resample the data
0.1.4,Create the object
0.1.4,Generate a global dataset to use
0.1.4,Define a ratio
0.1.4,Create the object
0.1.4,Create the object
0.1.4,Resample the data
0.1.4,Create a wrong y
0.1.4,Create the object
0.1.4,Fit the data
0.1.4,Check if the data information have been computed
0.1.4,Create the object
0.1.4,Resample the data
0.1.4,Resample the data
0.1.4,Resample the data
0.1.4,Create the object
0.1.4,Generate a global dataset to use
0.1.4,Define a negative ratio
0.1.4,Define a ratio greater than 1
0.1.4,Define ratio as an unknown string
0.1.4,Define ratio as a list which is not supported
0.1.4,Define a ratio
0.1.4,Define the parameter for the under-sampling
0.1.4,Create the object
0.1.4,Resample the data
0.1.4,Create a wrong y
0.1.4,Create the object
0.1.4,Fit the data
0.1.4,Define the parameter for the under-sampling
0.1.4,Create the object
0.1.4,Fit the data
0.1.4,Check if the data information have been computed
0.1.4,Define the parameter for the under-sampling
0.1.4,Create the object
0.1.4,Define the parameter for the under-sampling
0.1.4,Create the object
0.1.4,Fit and sample
0.1.4,Define the parameter for the under-sampling
0.1.4,Create the object
0.1.4,Fit and sample
0.1.4,Define the parameter for the under-sampling
0.1.4,Create the object
0.1.4,Fit and sample
0.1.4,Create the object
0.1.4,Generate a global dataset to use
0.1.4,Define a ratio
0.1.4,Create the object
0.1.4,Resample the data
0.1.4,Create a wrong y
0.1.4,Create the object
0.1.4,Fit the data
0.1.4,Check if the data information have been computed
0.1.4,Create the object
0.1.4,Resample the data
0.1.4,Resample the data
0.1.4,Create the object
0.1.4,Generate a global dataset to use
0.1.4,Define a negative ratio
0.1.4,Define a ratio greater than 1
0.1.4,Define ratio as an unknown string
0.1.4,Define ratio as a list which is not supported
0.1.4,Define a ratio
0.1.4,Define the parameter for the under-sampling
0.1.4,Create the object
0.1.4,Resample the data
0.1.4,Create a wrong y
0.1.4,Create the object
0.1.4,Fit the data
0.1.4,Define the parameter for the under-sampling
0.1.4,Create the object
0.1.4,Fit the data
0.1.4,Check if the data information have been computed
0.1.4,Define the parameter for the under-sampling
0.1.4,Create the object
0.1.4,Define the parameter for the under-sampling
0.1.4,Create the object
0.1.4,Define the parameter for the under-sampling
0.1.4,Create the object
0.1.4,Fit and sample
0.1.4,Define the parameter for the under-sampling
0.1.4,Create the object
0.1.4,Fit and sample
0.1.4,Generate a global dataset to use
0.1.4,Define a negative ratio
0.1.4,Define a ratio greater than 1
0.1.4,Define ratio as an unknown string
0.1.4,Define ratio as a list which is not supported
0.1.4,Define a ratio
0.1.4,Define the parameter for the under-sampling
0.1.4,Create the object
0.1.4,Resample the data
0.1.4,Create a wrong y
0.1.4,Create the object
0.1.4,Fit the data
0.1.4,Define the parameter for the under-sampling
0.1.4,Create the object
0.1.4,Fit the data
0.1.4,Check if the data information have been computed
0.1.4,Define the parameter for the under-sampling
0.1.4,Create the object
0.1.4,Define the parameter for the under-sampling
0.1.4,Create the object
0.1.4,Fit and sample
0.1.4,Define the parameter for the under-sampling
0.1.4,Create the object
0.1.4,Fit and sample
0.1.4,Define the parameter for the under-sampling
0.1.4,Create the object
0.1.4,Fit and sample
0.1.4,Create the object
0.1.4,Generate a global dataset to use
0.1.4,Define a ratio
0.1.4,Create the object
0.1.4,Resample the data
0.1.4,Create a wrong y
0.1.4,Create the object
0.1.4,Fit the data
0.1.4,Check if the data information have been computed
0.1.4,Create the object
0.1.4,Resample the data
0.1.4,Resample the data
0.1.4,Create the object
0.1.4,Test the various init parameters of the pipeline.
0.1.4,Check that we can't instantiate pipelines with objects without fit
0.1.4,method
0.1.4,Smoke test with only an estimator
0.1.4,Check that params are set
0.1.4,Smoke test the repr:
0.1.4,Test with two objects
0.1.4,Check that we can't use the same stage name twice
0.1.4,Check that params are set
0.1.4,Smoke test the repr:
0.1.4,Check that params are not set when naming them wrong
0.1.4,Test clone
0.1.4,"Check that apart from estimators, the parameters are the same"
0.1.4,Remove estimators that where copied
0.1.4,Test the various methods of the pipeline (anova).
0.1.4,Test with Anova + LogisticRegression
0.1.4,Test that the pipeline can take fit parameters
0.1.4,classifier should return True
0.1.4,and transformer params should not be changed
0.1.4,Test pipeline raises set params error message for nested models.
0.1.4,expected error message
0.1.4,nested model check
0.1.4,Test the various methods of the pipeline (pca + svm).
0.1.4,Test with PCA + SVC
0.1.4,Test the various methods of the pipeline (preprocessing + svm).
0.1.4,check shapes of various prediction functions
0.1.4,test that the fit_predict method is implemented on a pipeline
0.1.4,test that the fit_predict on pipeline yields same results as applying
0.1.4,transform and clustering steps separately
0.1.4,first compute the transform and clustering step separately
0.1.4,use a pipeline to do the transform and clustering in one step
0.1.4,tests that a pipeline does not have fit_predict method when final
0.1.4,step of pipeline does not have fit_predict defined
0.1.4,Test whether pipeline works with a transformer at the end.
0.1.4,Also test pipeline.transform and pipeline.inverse_transform
0.1.4,test transform and fit_transform:
0.1.4,Test whether pipeline works with a transformer missing fit_transform
0.1.4,test fit_transform:
0.1.4,Test the various methods of the pipeline (pca + svm).
0.1.4,Test with PCA + SVC
0.1.4,Test the various methods of the pipeline (pca + svm).
0.1.4,Test with PCA + SVC
0.1.4,Test whether pipeline works with a sampler at the end.
0.1.4,Also test pipeline.sampler
0.1.4,test transform and fit_transform:
0.1.4,Test whether pipeline works with a sampler at the end.
0.1.4,Also test pipeline.sampler
0.1.4,Test the various methods of the pipeline (anova).
0.1.4,Test with RandomUnderSampling + Anova + LogisticRegression
0.1.4,Fit using SMOTE
0.1.4,Transform using SMOTE
0.1.4,Fit and transform using ENN
0.1.4,Fit using SMOTE
0.1.4,Transform using SMOTE
0.1.4,Fit and transform using ENN
0.1.4,Generate a global dataset to use
0.1.4,Define a negative ratio
0.1.4,Define a ratio greater than 1
0.1.4,Define ratio as an unknown string
0.1.4,Define ratio as a list which is not supported
0.1.4,Create the object
0.1.4,Resample the data
0.1.4,Create a wrong y
0.1.4,Create the object
0.1.4,Fit the data
0.1.4,Check if the data information have been computed
0.1.4,Create the object
0.1.4,Create the object
0.1.4,Fit the data
0.1.4,Create the object
0.1.4,Fit the data
0.1.4,Create the object
0.1.4,Generate a global dataset to use
0.1.4,Define a negative ratio
0.1.4,Define a ratio greater than 1
0.1.4,Define ratio as an unknown string
0.1.4,Define ratio as a list which is not supported
0.1.4,Create the object
0.1.4,Resample the data
0.1.4,Create a wrong y
0.1.4,Create the object
0.1.4,Fit the data
0.1.4,Check if the data information have been computed
0.1.4,Create the object
0.1.4,Create the object
0.1.4,Fit the data
0.1.4,Create the object
0.1.4,Fit the data
0.1.4,Create the object
0.1.4,Check the random state
0.1.4,Define the classifier to use
0.1.4,Start with the minority class
0.1.4,Keep the indices of the minority class somewhere if we need to
0.1.4,return them later
0.1.4,Condition to initiliase before the search
0.1.4,Get the initial number of samples to select in the majority class
0.1.4,Create the array characterising the array containing the majority
0.1.4,class
0.1.4,Loop to create the different subsets
0.1.4,Generate an appropriate number of index to extract
0.1.4,from the majority class depending of the false classification
0.1.4,rate of the previous iteration
0.1.4,Mark these indexes as not being considered for next sampling
0.1.4,"For now, we will train and classify on the same data"
0.1.4,"Let see if we should find another solution. Anyway,"
0.1.4,random stuff are still random stuff
0.1.4,Push these data into a new subset
0.1.4,Apply a bootstrap on x_data
0.1.4,Train the classifier using the current data
0.1.4,Train the classifier using the current data
0.1.4,Predict using only the majority class
0.1.4,Basically let's find which sample have to be retained for the
0.1.4,next round
0.1.4,Find the misclassified index to keep them for the next round
0.1.4,Count how many random element will be selected
0.1.4,"We found a new subset, increase the counter"
0.1.4,Check if we have to make an early stopping
0.1.4,Select the remaining data
0.1.4,Select the final batch
0.1.4,Push these data into a new subset
0.1.4,"We found a new subset, increase the counter"
0.1.4,Also check that we will have enough sample to extract at the
0.1.4,next round
0.1.4,Select the remaining data
0.1.4,Select the final batch
0.1.4,Push these data into a new subset
0.1.4,"We found a new subset, increase the counter"
0.1.4,Generate a global dataset to use
0.1.4,Define a negative ratio
0.1.4,Define a ratio greater than 1
0.1.4,Define ratio as an unknown string
0.1.4,Define ratio as a list which is not supported
0.1.4,Define a ratio
0.1.4,Define the parameter for the under-sampling
0.1.4,Create the object
0.1.4,Resample the data
0.1.4,Create a wrong y
0.1.4,Create the object
0.1.4,Fit the data
0.1.4,Define the parameter for the under-sampling
0.1.4,Create the object
0.1.4,Fit the data
0.1.4,Check if the data information have been computed
0.1.4,Define the parameter for the under-sampling
0.1.4,Create the object
0.1.4,Define the ratio parameter
0.1.4,Create the sampling object
0.1.4,Get the different subset
0.1.4,Check each array
0.1.4,Define the ratio parameter
0.1.4,Create the sampling object
0.1.4,Get the different subset
0.1.4,Check each array
0.1.4,Define the ratio parameter
0.1.4,Create the sampling object
0.1.4,Get the different subset
0.1.4,Check each array
0.1.4,Define the ratio parameter
0.1.4,Create the sampling object
0.1.4,Get the different subset
0.1.4,Check each array
0.1.4,Define the ratio parameter
0.1.4,Create the sampling object
0.1.4,Get the different subset
0.1.4,Check each array
0.1.4,Define the ratio parameter
0.1.4,Create the sampling object
0.1.4,Get the different subset
0.1.4,Check each array
0.1.4,Define the ratio parameter
0.1.4,Create the sampling object
0.1.4,Get the different subset
0.1.4,Check each array
0.1.4,Define the ratio parameter
0.1.4,Define the ratio parameter
0.1.4,Create the sampling object
0.1.4,Get the different subset
0.1.4,Check each array
0.1.4,Create the object
0.1.4,Generate a global dataset to use
0.1.4,Define a negative ratio
0.1.4,Define a ratio greater than 1
0.1.4,Define ratio as an unknown string
0.1.4,Define ratio as a list which is not supported
0.1.4,Define a ratio
0.1.4,Define the parameter for the under-sampling
0.1.4,Create the object
0.1.4,Resample the data
0.1.4,Create a wrong y
0.1.4,Create the object
0.1.4,Fit the data
0.1.4,Define the parameter for the under-sampling
0.1.4,Create the object
0.1.4,Fit the data
0.1.4,Check if the data information have been computed
0.1.4,Define the parameter for the under-sampling
0.1.4,Create the object
0.1.4,Define the ratio parameter
0.1.4,Create the sampling object
0.1.4,Get the different subset
0.1.4,Define the ratio parameter
0.1.4,Create the sampling object
0.1.4,Get the different subset
0.1.4,Define the ratio parameter
0.1.4,Create the sampling object
0.1.4,Get the different subset
0.1.4,Create the object
0.1.3,! /usr/bin/env python
0.1.3,"load all vars into globals, otherwise"
0.1.3,the later function call using global vars doesn't work.
0.1.3,"Allow command-lines such as ""python setup.py build install"""
0.1.3,Make sources available using relative paths from this file's directory.
0.1.3,-*- coding: utf-8 -*-
0.1.3,
0.1.3,"imbalanced-learn documentation build configuration file, created by"
0.1.3,sphinx-quickstart on Mon Jan 18 14:44:12 2016.
0.1.3,
0.1.3,This file is execfile()d with the current directory set to its
0.1.3,containing dir.
0.1.3,
0.1.3,Note that not all possible configuration values are present in this
0.1.3,autogenerated file.
0.1.3,
0.1.3,All configuration values have a default; values that are commented out
0.1.3,serve to show the default.
0.1.3,"If extensions (or modules to document with autodoc) are in another directory,"
0.1.3,add these directories to sys.path here. If the directory is relative to the
0.1.3,"documentation root, use os.path.abspath to make it absolute, like shown here."
0.1.3,"sys.path.insert(0, os.path.abspath('.'))"
0.1.3,-- General configuration ---------------------------------------------------
0.1.3,Try to override the matplotlib configuration as early as possible
0.1.3,-- General configuration ------------------------------------------------
0.1.3,"If your documentation needs a minimal Sphinx version, state it here."
0.1.3,needs_sphinx = '1.0'
0.1.3,"Add any Sphinx extension module names here, as strings. They can be"
0.1.3,extensions coming with Sphinx (named 'sphinx.ext.*') or your custom
0.1.3,ones.
0.1.3,path to your examples scripts
0.1.3,path where to save gallery generated examples
0.1.3,"Add any paths that contain templates here, relative to this directory."
0.1.3,generate autosummary even if no references
0.1.3,The suffix of source filenames.
0.1.3,The encoding of source files.
0.1.3,source_encoding = 'utf-8-sig'
0.1.3,Generate the plots for the gallery
0.1.3,The master toctree document.
0.1.3,General information about the project.
0.1.3,"The version info for the project you're documenting, acts as replacement for"
0.1.3,"|version| and |release|, also used in various other places throughout the"
0.1.3,built documents.
0.1.3,
0.1.3,The short X.Y version.
0.1.3,"The full version, including alpha/beta/rc tags."
0.1.3,The language for content autogenerated by Sphinx. Refer to documentation
0.1.3,for a list of supported languages.
0.1.3,language = None
0.1.3,"There are two options for replacing |today|: either, you set today to some"
0.1.3,"non-false value, then it is used:"
0.1.3,today = ''
0.1.3,"Else, today_fmt is used as the format for a strftime call."
0.1.3,"today_fmt = '%B %d, %Y'"
0.1.3,"List of patterns, relative to source directory, that match files and"
0.1.3,directories to ignore when looking for source files.
0.1.3,The reST default role (used for this markup: `text`) to use for all
0.1.3,documents.
0.1.3,default_role = None
0.1.3,"If true, '()' will be appended to :func: etc. cross-reference text."
0.1.3,"If true, the current module name will be prepended to all description"
0.1.3,unit titles (such as .. function::).
0.1.3,add_module_names = True
0.1.3,"If true, sectionauthor and moduleauthor directives will be shown in the"
0.1.3,output. They are ignored by default.
0.1.3,show_authors = False
0.1.3,The name of the Pygments (syntax highlighting) style to use.
0.1.3,A list of ignored prefixes for module index sorting.
0.1.3,modindex_common_prefix = []
0.1.3,"If true, keep warnings as ""system message"" paragraphs in the built documents."
0.1.3,keep_warnings = False
0.1.3,-- Options for HTML output ----------------------------------------------
0.1.3,The theme to use for HTML and HTML Help pages.  See the documentation for
0.1.3,a list of builtin themes.
0.1.3,Theme options are theme-specific and customize the look and feel of a theme
0.1.3,"further.  For a list of options available for each theme, see the"
0.1.3,documentation.
0.1.3,html_theme_options = {}
0.1.3,"Add any paths that contain custom themes here, relative to this directory."
0.1.3,"The name for this set of Sphinx documents.  If None, it defaults to"
0.1.3,"""<project> v<release> documentation""."
0.1.3,html_title = None
0.1.3,A shorter title for the navigation bar.  Default is the same as html_title.
0.1.3,html_short_title = None
0.1.3,The name of an image file (relative to this directory) to place at the top
0.1.3,of the sidebar.
0.1.3,html_logo = None
0.1.3,The name of an image file (within the static path) to use as favicon of the
0.1.3,docs.  This file should be a Windows icon file (.ico) being 16x16 or 32x32
0.1.3,pixels large.
0.1.3,html_favicon = None
0.1.3,"Add any paths that contain custom static files (such as style sheets) here,"
0.1.3,"relative to this directory. They are copied after the builtin static files,"
0.1.3,"so a file named ""default.css"" will overwrite the builtin ""default.css""."
0.1.3,Add any extra paths that contain custom files (such as robots.txt or
0.1.3,".htaccess) here, relative to this directory. These files are copied"
0.1.3,directly to the root of the documentation.
0.1.3,html_extra_path = []
0.1.3,"If not '', a 'Last updated on:' timestamp is inserted at every page bottom,"
0.1.3,using the given strftime format.
0.1.3,"html_last_updated_fmt = '%b %d, %Y'"
0.1.3,"If true, SmartyPants will be used to convert quotes and dashes to"
0.1.3,typographically correct entities.
0.1.3,html_use_smartypants = True
0.1.3,"Custom sidebar templates, maps document names to template names."
0.1.3,html_sidebars = {}
0.1.3,"Additional templates that should be rendered to pages, maps page names to"
0.1.3,template names.
0.1.3,html_additional_pages = {}
0.1.3,"If false, no module index is generated."
0.1.3,html_domain_indices = True
0.1.3,"If false, no index is generated."
0.1.3,html_use_index = True
0.1.3,"If true, the index is split into individual pages for each letter."
0.1.3,html_split_index = False
0.1.3,"If true, links to the reST sources are added to the pages."
0.1.3,html_show_sourcelink = True
0.1.3,"If true, ""Created using Sphinx"" is shown in the HTML footer. Default is True."
0.1.3,html_show_sphinx = True
0.1.3,"If true, ""(C) Copyright ..."" is shown in the HTML footer. Default is True."
0.1.3,html_show_copyright = True
0.1.3,"If true, an OpenSearch description file will be output, and all pages will"
0.1.3,contain a <link> tag referring to it.  The value of this option must be the
0.1.3,base URL from which the finished HTML is served.
0.1.3,html_use_opensearch = ''
0.1.3,"This is the file name suffix for HTML files (e.g. "".xhtml"")."
0.1.3,html_file_suffix = None
0.1.3,Output file base name for HTML help builder.
0.1.3,-- Options for LaTeX output ---------------------------------------------
0.1.3,The paper size ('letterpaper' or 'a4paper').
0.1.3,"'papersize': 'letterpaper',"
0.1.3,"The font size ('10pt', '11pt' or '12pt')."
0.1.3,"'pointsize': '10pt',"
0.1.3,Additional stuff for the LaTeX preamble.
0.1.3,"'preamble': '',"
0.1.3,Grouping the document tree into LaTeX files. List of tuples
0.1.3,"(source start file, target name, title,"
0.1.3,"author, documentclass [howto, manual, or own class])."
0.1.3,The name of an image file (relative to this directory) to place at the top of
0.1.3,the title page.
0.1.3,latex_logo = None
0.1.3,"For ""manual"" documents, if this is true, then toplevel headings are parts,"
0.1.3,not chapters.
0.1.3,latex_use_parts = False
0.1.3,"If true, show page references after internal links."
0.1.3,latex_show_pagerefs = False
0.1.3,"If true, show URL addresses after external links."
0.1.3,latex_show_urls = False
0.1.3,Documents to append as an appendix to all manuals.
0.1.3,latex_appendices = []
0.1.3,"If false, no module index is generated."
0.1.3,latex_domain_indices = True
0.1.3,-- Options for manual page output ---------------------------------------
0.1.3,One entry per manual page. List of tuples
0.1.3,"(source start file, name, description, authors, manual section)."
0.1.3,"If true, show URL addresses after external links."
0.1.3,man_show_urls = False
0.1.3,-- Options for Texinfo output -------------------------------------------
0.1.3,Grouping the document tree into Texinfo files. List of tuples
0.1.3,"(source start file, target name, title, author,"
0.1.3,"dir menu entry, description, category)"
0.1.3,"generate empty examples files, so that we don't get"
0.1.3,inclusion errors if there are no examples for a class / module
0.1.3,touch file
0.1.3,Documents to append as an appendix to all manuals.
0.1.3,texinfo_appendices = []
0.1.3,"If false, no module index is generated."
0.1.3,texinfo_domain_indices = True
0.1.3,"How to display URL addresses: 'footnote', 'no', or 'inline'."
0.1.3,texinfo_show_urls = 'footnote'
0.1.3,"If true, do not generate a @detailmenu in the ""Top"" node's menu."
0.1.3,texinfo_no_detailmenu = False
0.1.3,Example configuration for intersphinx: refer to the Python standard library.
0.1.3,Define some color for the plotting
0.1.3,Generate the dataset
0.1.3,Instanciate a PCA object for the sake of easy visualisation
0.1.3,Fit and transform x to visualise inside a 2D feature space
0.1.3,Apply SMOTE SVM
0.1.3,"Two subplots, unpack the axes array immediately"
0.1.3,Define some color for the plotting
0.1.3,Generate the dataset
0.1.3,Instanciate a PCA object for the sake of easy visualisation
0.1.3,Fit and transform x to visualise inside a 2D feature space
0.1.3,Apply Borderline SMOTE 2
0.1.3,"Two subplots, unpack the axes array immediately"
0.1.3,Define some color for the plotting
0.1.3,Generate the dataset
0.1.3,Instanciate a PCA object for the sake of easy visualisation
0.1.3,Fit and transform x to visualise inside a 2D feature space
0.1.3,Apply regular SMOTE
0.1.3,"Two subplots, unpack the axes array immediately"
0.1.3,Define some color for the plotting
0.1.3,Generate the dataset
0.1.3,Instanciate a PCA object for the sake of easy visualisation
0.1.3,Fit and transform x to visualise inside a 2D feature space
0.1.3,Apply the random over-sampling
0.1.3,"Two subplots, unpack the axes array immediately"
0.1.3,Define some color for the plotting
0.1.3,Generate the dataset
0.1.3,Instanciate a PCA object for the sake of easy visualisation
0.1.3,Fit and transform x to visualise inside a 2D feature space
0.1.3,Apply Borderline SMOTE 1
0.1.3,"Two subplots, unpack the axes array immediately"
0.1.3,Define some color for the plotting
0.1.3,Generate the dataset
0.1.3,Instanciate a PCA object for the sake of easy visualisation
0.1.3,Fit and transform x to visualise inside a 2D feature space
0.1.3,Apply the random over-sampling
0.1.3,"Two subplots, unpack the axes array immediately"
0.1.3,Define some color for the plotting
0.1.3,Generate the dataset
0.1.3,Instanciate a PCA object for the sake of easy visualisation
0.1.3,Fit and transform x to visualise inside a 2D feature space
0.1.3,Apply SMOTE + ENN
0.1.3,"Two subplots, unpack the axes array immediately"
0.1.3,Define some color for the plotting
0.1.3,Generate the dataset
0.1.3,Instanciate a PCA object for the sake of easy visualisation
0.1.3,Fit and transform x to visualise inside a 2D feature space
0.1.3,Apply SMOTE + Tomek links
0.1.3,"Two subplots, unpack the axes array immediately"
0.1.3,Define some color for the plotting
0.1.3,Generate the dataset
0.1.3,Instanciate a PCA object for the sake of easy visualisation
0.1.3,Fit and transform x to visualise inside a 2D feature space
0.1.3,Apply Balance Cascade method
0.1.3,"Two subplots, unpack the axes array immediately"
0.1.3,Define some color for the plotting
0.1.3,Generate the dataset
0.1.3,Instanciate a PCA object for the sake of easy visualisation
0.1.3,Fit and transform x to visualise inside a 2D feature space
0.1.3,Apply Easy Ensemble
0.1.3,"Two subplots, unpack the axes array immediately"
0.1.3,Define some color for the plotting
0.1.3,Generate the dataset
0.1.3,Instanciate a PCA object for the sake of easy visualisation
0.1.3,Fit and transform x to visualise inside a 2D feature space
0.1.3,"Three subplots, unpack the axes array immediately"
0.1.3,Apply the ENN
0.1.3,Apply the RENN
0.1.3,Define some color for the plotting
0.1.3,Generate the dataset
0.1.3,Instanciate a PCA object for the sake of easy visualisation
0.1.3,Fit and transform x to visualise inside a 2D feature space
0.1.3,Apply Nearmiss 3
0.1.3,"Two subplots, unpack the axes array immediately"
0.1.3,Define some color for the plotting
0.1.3,Generate the dataset
0.1.3,Instanciate a PCA object for the sake of easy visualisation
0.1.3,Fit and transform x to visualise inside a 2D feature space
0.1.3,Apply Edited Nearest Neighbours
0.1.3,"Two subplots, unpack the axes array immediately"
0.1.3,Define some color for the plotting
0.1.3,Generate the dataset
0.1.3,Instanciate a PCA object for the sake of easy visualisation
0.1.3,Fit and transform x to visualise inside a 2D feature space
0.1.3,Apply Tomek Links cleaning
0.1.3,"Two subplots, unpack the axes array immediately"
0.1.3,Define some color for the plotting
0.1.3,Generate the dataset
0.1.3,Instanciate a PCA object for the sake of easy visualisation
0.1.3,Fit and transform x to visualise inside a 2D feature space
0.1.3,Apply One-Sided Selection
0.1.3,"Two subplots, unpack the axes array immediately"
0.1.3,Define some color for the plotting
0.1.3,Generate the dataset
0.1.3,Instanciate a PCA object for the sake of easy visualisation
0.1.3,Fit and transform x to visualise inside a 2D feature space
0.1.3,Apply Nearmiss 2
0.1.3,"Two subplots, unpack the axes array immediately"
0.1.3,Define some color for the plotting
0.1.3,Generate the dataset
0.1.3,Instanciate a PCA object for the sake of easy visualisation
0.1.3,Fit and transform x to visualise inside a 2D feature space
0.1.3,Apply neighbourhood cleaning rule
0.1.3,"Two subplots, unpack the axes array immediately"
0.1.3,Define some color for the plotting
0.1.3,Generate the dataset
0.1.3,Instanciate a PCA object for the sake of easy visualisation
0.1.3,Fit and transform x to visualise inside a 2D feature space
0.1.3,Apply Condensed Nearest Neighbours
0.1.3,"Two subplots, unpack the axes array immediately"
0.1.3,Define some color for the plotting
0.1.3,Generate the dataset
0.1.3,Instanciate a PCA object for the sake of easy visualisation
0.1.3,Fit and transform x to visualise inside a 2D feature space
0.1.3,Apply Cluster Centroids
0.1.3,"Two subplots, unpack the axes array immediately"
0.1.3,Define some color for the plotting
0.1.3,Generate the dataset
0.1.3,Instanciate a PCA object for the sake of easy visualisation
0.1.3,Fit and transform x to visualise inside a 2D feature space
0.1.3,Apply the random under-sampling
0.1.3,"Two subplots, unpack the axes array immediately"
0.1.3,Define some color for the plotting
0.1.3,Generate the dataset
0.1.3,"Two subplots, unpack the axes array immediately"
0.1.3,Define some color for the plotting
0.1.3,Generate the dataset
0.1.3,Instanciate a PCA object for the sake of easy visualisation
0.1.3,Fit and transform x to visualise inside a 2D feature space
0.1.3,Apply Nearmiss 1
0.1.3,"Two subplots, unpack the axes array immediately"
0.1.3,Generate the dataset
0.1.3,Instanciate a PCA object for the sake of easy visualisation
0.1.3,Create the samplers
0.1.3,Create teh classifier
0.1.3,Make the splits
0.1.3,Add one transformers and two samplers in the pipeline object
0.1.3,Based on NiLearn package
0.1.3,License: simplified BSD
0.1.3,"PEP0440 compatible formatted version, see:"
0.1.3,https://www.python.org/dev/peps/pep-0440/
0.1.3,
0.1.3,Generic release markers:
0.1.3,X.Y
0.1.3,X.Y.Z # For bugfix releases
0.1.3,
0.1.3,Admissible pre-release markers:
0.1.3,X.YaN # Alpha release
0.1.3,X.YbN # Beta release
0.1.3,X.YrcN # Release Candidate
0.1.3,X.Y # Final release
0.1.3,
0.1.3,Dev branch marker is: 'X.Y.dev' or 'X.Y.devN' where N is an integer.
0.1.3,'X.Y.dev0' is the canonical version of 'X.Y.dev'
0.1.3,
0.1.3,"This is a tuple to preserve order, so that dependencies are checked"
0.1.3,in some meaningful order (more => less 'core').  We avoid using
0.1.3,collections.OrderedDict to preserve Python 2.6 compatibility.
0.1.3,Avoid choking on modules with no __version__ attribute
0.1.3,Skip check only when installing and it's a module that
0.1.3,will be auto-installed.
0.1.3,Check the consistency of X and y
0.1.3,Get all the unique elements in the target array
0.1.3,# Raise an error if there is only one class
0.1.3,if uniques.size == 1:
0.1.3,"raise RuntimeError(""Only one class detected, aborting..."")"
0.1.3,Raise a warning for the moment to be compatible with BaseEstimator
0.1.3,Store the size of X to check at sampling time if we have the
0.1.3,same data
0.1.3,Create a dictionary containing the class statistics
0.1.3,Find the minority and majority classes
0.1.3,Check if the ratio provided at initialisation make sense
0.1.3,Check the consistency of X and y
0.1.3,Check that the data have been fitted
0.1.3,Check if the size of the data is identical than at fitting
0.1.3,The ratio correspond to the number of samples in the minority class
0.1.3,"over the number of samples in the majority class. Thus, the ratio"
0.1.3,cannot be greater than 1.0
0.1.3,Adapted from scikit-learn
0.1.3,Author: Edouard Duchesnay
0.1.3,Gael Varoquaux
0.1.3,Virgile Fritsch
0.1.3,Alexandre Gramfort
0.1.3,Lars Buitinck
0.1.3,chkoar
0.1.3,License: BSD
0.1.3,BaseEstimator interface
0.1.3,shallow copy of steps
0.1.3,Estimator interface
0.1.3,Boolean controlling whether the joblib caches should be
0.1.3,"flushed if the version of certain modules changes (eg nibabel, as it"
0.1.3,does not respect the backward compatibility in some of its internal
0.1.3,structures
0.1.3,This  is used in nilearn._utils.cache_mixin
0.1.3,list all submodules available in imblearn and version
0.1.3,Keep the samples from the majority class
0.1.3,Loop over the other classes over picking at random
0.1.3,"If this is the majority class, skip it"
0.1.3,Define the number of sample to create
0.1.3,Pick some elements at random
0.1.3,Concatenate to the majority class
0.1.3,Keep the samples from the majority class
0.1.3,Define the number of sample to create
0.1.3,We handle only two classes problem for the moment.
0.1.3,Start by separating minority class features and target values.
0.1.3,Print if verbose is true
0.1.3,"Look for k-th nearest neighbours, excluding, of course, the"
0.1.3,point itself.
0.1.3,Get the distance to the NN
0.1.3,Compute the ratio of majority samples next to minority samples
0.1.3,Check that we found at least some neighbours belonging to the
0.1.3,majority class
0.1.3,Normalize the ratio
0.1.3,Compute the number of sample to be generated
0.1.3,For each minority samples
0.1.3,Pick-up the neighbors wanted
0.1.3,Create a new sample
0.1.3,Find the NN for each samples
0.1.3,Exclude the sample itself
0.1.3,Count how many NN belong to the minority class
0.1.3,Find the class corresponding to the label in x
0.1.3,Compute the number of majority samples in the NN
0.1.3,Samples are in danger for m/2 <= m' < m
0.1.3,Samples are noise for m = m'
0.1.3,Check the consistency of X
0.1.3,Check the random state
0.1.3,A matrix to store the synthetic samples
0.1.3,# Set seeds
0.1.3,"seeds = random_state.randint(low=0,"
0.1.3,"high=100 * len(nn_num.flatten()),"
0.1.3,size=n_samples)
0.1.3,Randomly pick samples to construct neighbours from
0.1.3,Loop over the NN matrix and create new samples
0.1.3,"NN lines relate to original sample, columns to its"
0.1.3,nearest neighbours
0.1.3,"Take a step of random size (0,1) in the direction of the"
0.1.3,n nearest neighbours
0.1.3,if self.random_state is None:
0.1.3,np.random.seed(seeds[i])
0.1.3,else:
0.1.3,np.random.seed(self.random_state)
0.1.3,Construct synthetic sample
0.1.3,The returned target vector is simply a repetition of the
0.1.3,minority label
0.1.3,Define the number of sample to create
0.1.3,We handle only two classes problem for the moment.
0.1.3,Start by separating minority class features and target values.
0.1.3,If regular SMOTE is to be performed
0.1.3,"Look for k-th nearest neighbours, excluding, of course, the"
0.1.3,point itself.
0.1.3,Matrix with k-th nearest neighbours indexes for each minority
0.1.3,element.
0.1.3,--- Generating synthetic samples
0.1.3,Use static method make_samples to generate minority samples
0.1.3,Concatenate the newly generated samples to the original data set
0.1.3,Find the NNs for all samples in the data set.
0.1.3,Boolean array with True for minority samples in danger
0.1.3,"If all minority samples are safe, return the original data set."
0.1.3,"All are safe, nothing to be done here."
0.1.3,"If we got here is because some samples are in danger, we need to"
0.1.3,find the NNs among the minority class to create the new synthetic
0.1.3,samples.
0.1.3,
0.1.3,We start by changing the number of NNs to consider from m + 1
0.1.3,to k + 1
0.1.3,nns...#
0.1.3,B1 and B2 types diverge here!!!
0.1.3,Create synthetic samples for borderline points.
0.1.3,Concatenate the newly generated samples to the original
0.1.3,dataset
0.1.3,Reset the k-neighbours to m+1 neighbours
0.1.3,Split the number of synthetic samples between only minority
0.1.3,"(type 1), or minority and majority (with reduced step size)"
0.1.3,(type 2).
0.1.3,The fraction is sampled from a beta distribution centered
0.1.3,around 0.5 with variance ~0.01
0.1.3,Only minority
0.1.3,Only majority with smaller step size
0.1.3,Concatenate the newly generated samples to the original
0.1.3,data set
0.1.3,Reset the k-neighbours to m+1 neighbours
0.1.3,The SVM smote model fits a support vector machine
0.1.3,classifier to the data and uses the support vector to
0.1.3,"provide a notion of boundary. Unlike regular smote, where"
0.1.3,such notion relies on proportion of nearest neighbours
0.1.3,belonging to each class.
0.1.3,Fit SVM to the full data#
0.1.3,Find the support vectors and their corresponding indexes
0.1.3,"First, find the nn of all the samples to identify samples"
0.1.3,in danger and noisy ones
0.1.3,"As usual, fit a nearest neighbour model to the data"
0.1.3,"Now, get rid of noisy support vectors"
0.1.3,Remove noisy support vectors
0.1.3,Proceed to find support vectors NNs among the minority class
0.1.3,Split the number of synthetic samples between interpolation and
0.1.3,extrapolation
0.1.3,The fraction are sampled from a beta distribution with mean
0.1.3,0.5 and variance 0.01#
0.1.3,Interpolate samples in danger
0.1.3,Extrapolate safe samples
0.1.3,Concatenate the newly generated samples to the original data set
0.1.3,not any support vectors in danger
0.1.3,All the support vector in danger
0.1.3,Reset the k-neighbours to m+1 neighbours
0.1.3,--- NN object
0.1.3,Import the NN object from scikit-learn library. Since in the smote
0.1.3,"variations we must first find samples that are in danger, we"
0.1.3,initialize the NN object differently depending on the method chosen
0.1.3,"Regular smote does not look for samples in danger, instead it"
0.1.3,creates synthetic samples directly from the k-th nearest
0.1.3,neighbours with not filtering
0.1.3,"Borderline1, 2 and SVM variations of smote must first look for"
0.1.3,samples that could be considered noise and samples that live
0.1.3,"near the boundary between the classes. Therefore, before"
0.1.3,"creating synthetic samples from the k-th nns, it first look"
0.1.3,for m nearest neighbors to decide whether or not a sample is
0.1.3,noise or near the boundary.
0.1.3,--- SVM smote
0.1.3,"Unlike the borderline variations, the SVM variation uses the support"
0.1.3,vectors to decide which samples are in danger (near the boundary).
0.1.3,Additionally it also introduces extrapolation for samples that are
0.1.3,considered safe (far from boundary) and interpolation for samples
0.1.3,in danger (near the boundary). The level of extrapolation is
0.1.3,controled by the out_step.
0.1.3,Store SVM object with any parameters
0.1.3,Generate a global dataset to use
0.1.3,Define a negative ratio
0.1.3,Define a ratio greater than 1
0.1.3,Define ratio as an unknown string
0.1.3,Define ratio as a list which is not supported
0.1.3,Define a ratio
0.1.3,Create the object
0.1.3,Resample the data
0.1.3,Create a wrong y
0.1.3,Create the object
0.1.3,Fit the data
0.1.3,Create the object
0.1.3,Fit the data
0.1.3,Check if the data information have been computed
0.1.3,Create the object
0.1.3,Resample the data
0.1.3,Resample the data
0.1.3,Create the object
0.1.3,Generate a global dataset to use
0.1.3,Define a negative ratio
0.1.3,Define a ratio greater than 1
0.1.3,Define ratio as an unknown string
0.1.3,Define ratio as a list which is not supported
0.1.3,Define a ratio
0.1.3,Create the object
0.1.3,Resample the data
0.1.3,Create a wrong y
0.1.3,Create the object
0.1.3,Fit the data
0.1.3,Create the object
0.1.3,Fit the data
0.1.3,Check if the data information have been computed
0.1.3,Create the object
0.1.3,Resample the data
0.1.3,Resample the data
0.1.3,Create the object
0.1.3,Generate a global dataset to use
0.1.3,Define a negative ratio
0.1.3,Define a ratio greater than 1
0.1.3,Define ratio as an unknown string
0.1.3,Define ratio as a list which is not supported
0.1.3,Create the object
0.1.3,Resample the data
0.1.3,Create a wrong y
0.1.3,Create the object
0.1.3,Fit the data
0.1.3,Check if the data information have been computed
0.1.3,Create the object
0.1.3,Create the object
0.1.3,Fit the data
0.1.3,Create the object
0.1.3,Fit the data
0.1.3,Create the object
0.1.3,Fit the data
0.1.3,Create the object
0.1.3,Fit the data
0.1.3,Create the object
0.1.3,Fit the data
0.1.3,Create the object
0.1.3,Start with the minority class
0.1.3,All the minority class samples will be preserved
0.1.3,If we need to offer support for the indices
0.1.3,Loop over the other classes under picking at random
0.1.3,"If the minority class is up, skip it"
0.1.3,Randomly get one sample from the majority class
0.1.3,Generate the index to select
0.1.3,Create the set C
0.1.3,Create the set S
0.1.3,Remove the seed from S since that it will be added anyway
0.1.3,Create a k-NN classifier
0.1.3,Fit C into the knn
0.1.3,Classify on S
0.1.3,Find the misclassified S_y
0.1.3,If we need to offer support for the indices selected
0.1.3,We concatenate the misclassified samples with the seed and the
0.1.3,minority samples
0.1.3,Find the nearest neighbour of every point
0.1.3,Send the information to is_tomek function to get boolean vector back
0.1.3,Check if the indices of the samples selected should be returned too
0.1.3,Return the indices of interest
0.1.3,Return data set without majority Tomek links.
0.1.3,Compute the distance considering the farthest neighbour
0.1.3,Sort the list of distance and get the index
0.1.3,Select the desired number of samples
0.1.3,Assign the parameter of the element of this class
0.1.3,Check that the version asked is implemented
0.1.3,Start with the minority class
0.1.3,All the minority class samples will be preserved
0.1.3,Compute the number of cluster needed
0.1.3,If we need to offer support for the indices
0.1.3,"For each element of the current class, find the set of NN"
0.1.3,of the minority class
0.1.3,Call the constructor of the NN
0.1.3,Fit the minority class since that we want to know the distance
0.1.3,to these point
0.1.3,Loop over the other classes under picking at random
0.1.3,"If the minority class is up, skip it"
0.1.3,Get the samples corresponding to the current class
0.1.3,Find the NN
0.1.3,Select the right samples
0.1.3,Find the NN
0.1.3,Select the right samples
0.1.3,We need a new NN object to fit the current class
0.1.3,Find the set of NN to the minority class
0.1.3,Create the subset containing the samples found during the NN
0.1.3,search. Linearize the indexes and remove the double values
0.1.3,Create the subset
0.1.3,Compute the NN considering the current class
0.1.3,If we need to offer support for the indices selected
0.1.3,Check if the indices of the samples selected should be returned too
0.1.3,Return the indices of interest
0.1.3,Compute the number of clusters needed
0.1.3,All the minority class samples will be preserved
0.1.3,If we need to offer support for the indices
0.1.3,Loop over the other classes under-picking at random
0.1.3,"If the minority class is up, skip it"
0.1.3,Pick some elements at random
0.1.3,If we need to offer support for the indices selected
0.1.3,Concatenate to the minority class
0.1.3,Check if the indices of the samples selected should be returned as
0.1.3,well
0.1.3,Return the indices of interest
0.1.3,"Initialize the boolean result as false, and also a counter"
0.1.3,Loop through each sample and looks whether it belongs to the minority
0.1.3,"class. If it does, we don't consider it since we want to keep all"
0.1.3,"minority samples. If, however, it belongs to the majority sample we"
0.1.3,look at its first neighbour. If its closest neighbour also has the
0.1.3,"current sample as its closest neighbour, the two form a Tomek link."
0.1.3,"If they form a tomek link, put a True marker on this"
0.1.3,"sample, and increase counter by one."
0.1.3,Find the nearest neighbour of every point
0.1.3,Send the information to is_tomek function to get boolean vector back
0.1.3,Check if the indices of the samples selected should be returned too
0.1.3,Return the indices of interest
0.1.3,Return data set without majority Tomek links.
0.1.3,Start with the minority class
0.1.3,All the minority class samples will be preserved
0.1.3,If we need to offer support for the indices
0.1.3,Loop over the other classes under picking at random
0.1.3,"If the minority class is up, skip it"
0.1.3,Randomly get one sample from the majority class
0.1.3,Generate the index to select
0.1.3,Create the set C - One majority samples and all minority
0.1.3,Create the set S - all majority samples
0.1.3,Create a k-NN classifier
0.1.3,Fit C into the knn
0.1.3,Check each sample in S if we keep it or drop it
0.1.3,Do not select sample which are already well classified
0.1.3,Classify on S
0.1.3,If the prediction do not agree with the true label
0.1.3,append it in C_x
0.1.3,Keep the index for later
0.1.3,Update C
0.1.3,Fit C into the knn
0.1.3,This experimental to speed up the search
0.1.3,Classify all the element in S and avoid to test the
0.1.3,well classified elements
0.1.3,Find the misclassified S_y
0.1.3,If we need to offer support for the indices selected
0.1.3,Check if the indices of the samples selected should be returned too
0.1.3,Return the indices of interest
0.1.3,Compute the number of cluster needed
0.1.3,Create the clustering object
0.1.3,Start with the minority class
0.1.3,All the minority class samples will be preserved
0.1.3,Loop over the other classes under picking at random
0.1.3,"If the minority class is up, skip it."
0.1.3,Find the centroids via k-means
0.1.3,Concatenate to the minority class
0.1.3,Start with the minority class
0.1.3,All the minority class samples will be preserved
0.1.3,If we need to offer support for the indices
0.1.3,Create a k-NN to fit the whole data
0.1.3,Fit the whole dataset
0.1.3,Loop over the other classes under picking at random
0.1.3,Get the sample of the current class
0.1.3,Get the samples associated
0.1.3,Find the NN for the current class
0.1.3,Get the label of the corresponding to the index
0.1.3,Check which one are the same label than the current class
0.1.3,Make an AND operation through the three neighbours
0.1.3,If the minority class remove the majority samples
0.1.3,Get the index to exclude
0.1.3,Get the index to exclude
0.1.3,Create a vector with the sample to select
0.1.3,Exclude as well the minority sample since that they will be
0.1.3,concatenated later
0.1.3,Get the samples from the majority classes
0.1.3,If we need to offer support for the indices selected
0.1.3,Check if the indices of the samples selected should be returned too
0.1.3,Return the indices of interest
0.1.3,Start with the minority class
0.1.3,All the minority class samples will be preserved
0.1.3,If we need to offer support for the indices
0.1.3,Create a k-NN to fit the whole data
0.1.3,Fit the data
0.1.3,Loop over the other classes under picking at random
0.1.3,"If the minority class is up, skip it"
0.1.3,Get the sample of the current class
0.1.3,Find the NN for the current class
0.1.3,Get the label of the corresponding to the index
0.1.3,Check which one are the same label than the current class
0.1.3,Make the majority vote
0.1.3,Get the samples which agree all together
0.1.3,If we need to offer support for the indices selected
0.1.3,Check if the indices of the samples selected should be returned too
0.1.3,Return the indices of interest
0.1.3,Check if the indices of the samples selected should be returned too
0.1.3,Return the indices of interest
0.1.3,Select the appropriate classifier
0.1.3,Create the different folds
0.1.3,Compute the number of cluster needed
0.1.3,Find the percentile corresponding to the top num_samples
0.1.3,Sample the data
0.1.3,If we need to offer support for the indices
0.1.3,Generate a global dataset to use
0.1.3,Define a negative ratio
0.1.3,Define a ratio greater than 1
0.1.3,Define ratio as an unknown string
0.1.3,Define ratio as a list which is not supported
0.1.3,Define a ratio
0.1.3,Define the parameter for the under-sampling
0.1.3,Create the object
0.1.3,Resample the data
0.1.3,Create a wrong y
0.1.3,Create the object
0.1.3,Fit the data
0.1.3,Define the parameter for the under-sampling
0.1.3,Create the object
0.1.3,Fit the data
0.1.3,Check if the data information have been computed
0.1.3,Define the parameter for the under-sampling
0.1.3,Create the object
0.1.3,Define the parameter for the under-sampling
0.1.3,Create the object
0.1.3,Fit and sample
0.1.3,Define the parameter for the under-sampling
0.1.3,Create the object
0.1.3,Fit and sample
0.1.3,Define the parameter for the under-sampling
0.1.3,Create the object
0.1.3,Fit and sample
0.1.3,Create the object
0.1.3,Generate a global dataset to use
0.1.3,Define a ratio
0.1.3,Create the object
0.1.3,Resample the data
0.1.3,Create a wrong y
0.1.3,Create the object
0.1.3,Fit the data
0.1.3,Check if the data information have been computed
0.1.3,Create the object
0.1.3,Resample the data
0.1.3,Resample the data
0.1.3,Create the object
0.1.3,Generate a global dataset to use
0.1.3,Define a ratio
0.1.3,Create the object
0.1.3,Resample the data
0.1.3,Create a wrong y
0.1.3,Create the object
0.1.3,Fit the data
0.1.3,Check if the data information have been computed
0.1.3,Create the object
0.1.3,Resample the data
0.1.3,Resample the data
0.1.3,Create the object
0.1.3,Generate a global dataset to use
0.1.3,Define a ratio
0.1.3,Create the object
0.1.3,Resample the data
0.1.3,Create a wrong y
0.1.3,Create the object
0.1.3,Fit the data
0.1.3,Check if the data information have been computed
0.1.3,Create the object
0.1.3,Resample the data
0.1.3,Resample the data
0.1.3,Resample the data
0.1.3,Create the object
0.1.3,Generate a global dataset to use
0.1.3,Define a negative ratio
0.1.3,Define a ratio greater than 1
0.1.3,Define ratio as an unknown string
0.1.3,Define ratio as a list which is not supported
0.1.3,Define a ratio
0.1.3,Create the object
0.1.3,Resample the data
0.1.3,Create a wrong y
0.1.3,Create the object
0.1.3,Fit the data
0.1.3,Create the object
0.1.3,Fit the data
0.1.3,Check if the data information have been computed
0.1.3,Create the object
0.1.3,Resample the data
0.1.3,Resample the data
0.1.3,Resample the data
0.1.3,Create the object
0.1.3,Generate a global dataset to use
0.1.3,Define a negative ratio
0.1.3,Define a ratio greater than 1
0.1.3,Define ratio as an unknown string
0.1.3,Define ratio as a list which is not supported
0.1.3,Resample the data
0.1.3,Define a ratio
0.1.3,Create the object
0.1.3,Resample the data
0.1.3,Create a wrong y
0.1.3,Create the object
0.1.3,Fit the data
0.1.3,Create the object
0.1.3,Fit the data
0.1.3,Check if the data information have been computed
0.1.3,Create the object
0.1.3,Resample the data
0.1.3,Resample the data
0.1.3,Resample the data
0.1.3,Resample the data
0.1.3,Resample the data
0.1.3,Resample the data
0.1.3,Resample the data
0.1.3,Resample the data
0.1.3,Resample the data
0.1.3,Create the object
0.1.3,Generate a global dataset to use
0.1.3,Define a ratio
0.1.3,Create the object
0.1.3,Create the object
0.1.3,Resample the data
0.1.3,Create a wrong y
0.1.3,Create the object
0.1.3,Fit the data
0.1.3,Check if the data information have been computed
0.1.3,Create the object
0.1.3,Resample the data
0.1.3,Resample the data
0.1.3,Resample the data
0.1.3,Create the object
0.1.3,Generate a global dataset to use
0.1.3,Define a negative ratio
0.1.3,Define a ratio greater than 1
0.1.3,Define ratio as an unknown string
0.1.3,Define ratio as a list which is not supported
0.1.3,Define a ratio
0.1.3,Define the parameter for the under-sampling
0.1.3,Create the object
0.1.3,Resample the data
0.1.3,Create a wrong y
0.1.3,Create the object
0.1.3,Fit the data
0.1.3,Define the parameter for the under-sampling
0.1.3,Create the object
0.1.3,Fit the data
0.1.3,Check if the data information have been computed
0.1.3,Define the parameter for the under-sampling
0.1.3,Create the object
0.1.3,Define the parameter for the under-sampling
0.1.3,Create the object
0.1.3,Fit and sample
0.1.3,Define the parameter for the under-sampling
0.1.3,Create the object
0.1.3,Fit and sample
0.1.3,Define the parameter for the under-sampling
0.1.3,Create the object
0.1.3,Fit and sample
0.1.3,Create the object
0.1.3,Generate a global dataset to use
0.1.3,Define a ratio
0.1.3,Create the object
0.1.3,Resample the data
0.1.3,Create a wrong y
0.1.3,Create the object
0.1.3,Fit the data
0.1.3,Check if the data information have been computed
0.1.3,Create the object
0.1.3,Resample the data
0.1.3,Resample the data
0.1.3,Create the object
0.1.3,Generate a global dataset to use
0.1.3,Define a negative ratio
0.1.3,Define a ratio greater than 1
0.1.3,Define ratio as an unknown string
0.1.3,Define ratio as a list which is not supported
0.1.3,Define a ratio
0.1.3,Define the parameter for the under-sampling
0.1.3,Create the object
0.1.3,Resample the data
0.1.3,Create a wrong y
0.1.3,Create the object
0.1.3,Fit the data
0.1.3,Define the parameter for the under-sampling
0.1.3,Create the object
0.1.3,Fit the data
0.1.3,Check if the data information have been computed
0.1.3,Define the parameter for the under-sampling
0.1.3,Create the object
0.1.3,Define the parameter for the under-sampling
0.1.3,Create the object
0.1.3,Define the parameter for the under-sampling
0.1.3,Create the object
0.1.3,Fit and sample
0.1.3,Define the parameter for the under-sampling
0.1.3,Create the object
0.1.3,Fit and sample
0.1.3,Generate a global dataset to use
0.1.3,Define a negative ratio
0.1.3,Define a ratio greater than 1
0.1.3,Define ratio as an unknown string
0.1.3,Define ratio as a list which is not supported
0.1.3,Define a ratio
0.1.3,Define the parameter for the under-sampling
0.1.3,Create the object
0.1.3,Resample the data
0.1.3,Create a wrong y
0.1.3,Create the object
0.1.3,Fit the data
0.1.3,Define the parameter for the under-sampling
0.1.3,Create the object
0.1.3,Fit the data
0.1.3,Check if the data information have been computed
0.1.3,Define the parameter for the under-sampling
0.1.3,Create the object
0.1.3,Define the parameter for the under-sampling
0.1.3,Create the object
0.1.3,Fit and sample
0.1.3,Define the parameter for the under-sampling
0.1.3,Create the object
0.1.3,Fit and sample
0.1.3,Define the parameter for the under-sampling
0.1.3,Create the object
0.1.3,Fit and sample
0.1.3,Create the object
0.1.3,Generate a global dataset to use
0.1.3,Define a ratio
0.1.3,Create the object
0.1.3,Resample the data
0.1.3,Create a wrong y
0.1.3,Create the object
0.1.3,Fit the data
0.1.3,Check if the data information have been computed
0.1.3,Create the object
0.1.3,Resample the data
0.1.3,Resample the data
0.1.3,Create the object
0.1.3,Test the various init parameters of the pipeline.
0.1.3,Check that we can't instantiate pipelines with objects without fit
0.1.3,method
0.1.3,Smoke test with only an estimator
0.1.3,Check that params are set
0.1.3,Smoke test the repr:
0.1.3,Test with two objects
0.1.3,Check that we can't use the same stage name twice
0.1.3,Check that params are set
0.1.3,Smoke test the repr:
0.1.3,Check that params are not set when naming them wrong
0.1.3,Test clone
0.1.3,"Check that apart from estimators, the parameters are the same"
0.1.3,Remove estimators that where copied
0.1.3,Test the various methods of the pipeline (anova).
0.1.3,Test with Anova + LogisticRegression
0.1.3,Test that the pipeline can take fit parameters
0.1.3,classifier should return True
0.1.3,and transformer params should not be changed
0.1.3,Test pipeline raises set params error message for nested models.
0.1.3,expected error message
0.1.3,nested model check
0.1.3,Test the various methods of the pipeline (pca + svm).
0.1.3,Test with PCA + SVC
0.1.3,Test the various methods of the pipeline (preprocessing + svm).
0.1.3,check shapes of various prediction functions
0.1.3,test that the fit_predict method is implemented on a pipeline
0.1.3,test that the fit_predict on pipeline yields same results as applying
0.1.3,transform and clustering steps separately
0.1.3,first compute the transform and clustering step separately
0.1.3,use a pipeline to do the transform and clustering in one step
0.1.3,tests that a pipeline does not have fit_predict method when final
0.1.3,step of pipeline does not have fit_predict defined
0.1.3,Test whether pipeline works with a transformer at the end.
0.1.3,Also test pipeline.transform and pipeline.inverse_transform
0.1.3,test transform and fit_transform:
0.1.3,Test whether pipeline works with a transformer missing fit_transform
0.1.3,test fit_transform:
0.1.3,Test the various methods of the pipeline (pca + svm).
0.1.3,Test with PCA + SVC
0.1.3,Test the various methods of the pipeline (pca + svm).
0.1.3,Test with PCA + SVC
0.1.3,Test whether pipeline works with a sampler at the end.
0.1.3,Also test pipeline.sampler
0.1.3,test transform and fit_transform:
0.1.3,Test whether pipeline works with a sampler at the end.
0.1.3,Also test pipeline.sampler
0.1.3,Test the various methods of the pipeline (anova).
0.1.3,Test with RandomUnderSampling + Anova + LogisticRegression
0.1.3,Fit using SMOTE
0.1.3,Transform using SMOTE
0.1.3,Fit and transform using ENN
0.1.3,Fit using SMOTE
0.1.3,Transform using SMOTE
0.1.3,Fit and transform using ENN
0.1.3,Generate a global dataset to use
0.1.3,Define a negative ratio
0.1.3,Define a ratio greater than 1
0.1.3,Define ratio as an unknown string
0.1.3,Define ratio as a list which is not supported
0.1.3,Create the object
0.1.3,Resample the data
0.1.3,Create a wrong y
0.1.3,Create the object
0.1.3,Fit the data
0.1.3,Check if the data information have been computed
0.1.3,Create the object
0.1.3,Create the object
0.1.3,Fit the data
0.1.3,Create the object
0.1.3,Fit the data
0.1.3,Create the object
0.1.3,Generate a global dataset to use
0.1.3,Define a negative ratio
0.1.3,Define a ratio greater than 1
0.1.3,Define ratio as an unknown string
0.1.3,Define ratio as a list which is not supported
0.1.3,Create the object
0.1.3,Resample the data
0.1.3,Create a wrong y
0.1.3,Create the object
0.1.3,Fit the data
0.1.3,Check if the data information have been computed
0.1.3,Create the object
0.1.3,Create the object
0.1.3,Fit the data
0.1.3,Create the object
0.1.3,Fit the data
0.1.3,Create the object
0.1.3,Create the object for random under-sampling
0.1.3,Define the classifier to use
0.1.3,Start with the minority class
0.1.3,Keep the indices of the minority class somewhere if we need to
0.1.3,return them later
0.1.3,Condition to initiliase before the search
0.1.3,Get the initial number of samples to select in the majority class
0.1.3,Create the array characterising the array containing the majority
0.1.3,class
0.1.3,Loop to create the different subsets
0.1.3,Generate an appropriate number of index to extract
0.1.3,from the majority class depending of the false classification
0.1.3,rate of the previous iteration
0.1.3,Mark these indexes as not being considered for next sampling
0.1.3,"For now, we will train and classify on the same data"
0.1.3,"Let see if we should find another solution. Anyway,"
0.1.3,random stuff are still random stuff
0.1.3,Push these data into a new subset
0.1.3,Apply a bootstrap on x_data
0.1.3,Train the classifier using the current data
0.1.3,Train the classifier using the current data
0.1.3,Predict using only the majority class
0.1.3,Basically let's find which sample have to be retained for the
0.1.3,next round
0.1.3,Find the misclassified index to keep them for the next round
0.1.3,Count how many random element will be selected
0.1.3,"We found a new subset, increase the counter"
0.1.3,Check if we have to make an early stopping
0.1.3,Select the remaining data
0.1.3,Select the final batch
0.1.3,Push these data into a new subset
0.1.3,"We found a new subset, increase the counter"
0.1.3,Also check that we will have enough sample to extract at the
0.1.3,next round
0.1.3,Select the remaining data
0.1.3,Select the final batch
0.1.3,Push these data into a new subset
0.1.3,"We found a new subset, increase the counter"
0.1.3,Generate a global dataset to use
0.1.3,Define a negative ratio
0.1.3,Define a ratio greater than 1
0.1.3,Define ratio as an unknown string
0.1.3,Define ratio as a list which is not supported
0.1.3,Define a ratio
0.1.3,Define the parameter for the under-sampling
0.1.3,Create the object
0.1.3,Resample the data
0.1.3,Create a wrong y
0.1.3,Create the object
0.1.3,Fit the data
0.1.3,Define the parameter for the under-sampling
0.1.3,Create the object
0.1.3,Fit the data
0.1.3,Check if the data information have been computed
0.1.3,Define the parameter for the under-sampling
0.1.3,Create the object
0.1.3,Define the ratio parameter
0.1.3,Create the sampling object
0.1.3,Get the different subset
0.1.3,Check each array
0.1.3,Define the ratio parameter
0.1.3,Create the sampling object
0.1.3,Get the different subset
0.1.3,Check each array
0.1.3,Define the ratio parameter
0.1.3,Create the sampling object
0.1.3,Get the different subset
0.1.3,Check each array
0.1.3,Define the ratio parameter
0.1.3,Create the sampling object
0.1.3,Get the different subset
0.1.3,Check each array
0.1.3,Define the ratio parameter
0.1.3,Create the sampling object
0.1.3,Get the different subset
0.1.3,Check each array
0.1.3,Define the ratio parameter
0.1.3,Create the sampling object
0.1.3,Get the different subset
0.1.3,Check each array
0.1.3,Define the ratio parameter
0.1.3,Create the sampling object
0.1.3,Get the different subset
0.1.3,Check each array
0.1.3,Define the ratio parameter
0.1.3,Define the ratio parameter
0.1.3,Create the sampling object
0.1.3,Get the different subset
0.1.3,Check each array
0.1.3,Create the object
0.1.3,Generate a global dataset to use
0.1.3,Define a negative ratio
0.1.3,Define a ratio greater than 1
0.1.3,Define ratio as an unknown string
0.1.3,Define ratio as a list which is not supported
0.1.3,Define a ratio
0.1.3,Define the parameter for the under-sampling
0.1.3,Create the object
0.1.3,Resample the data
0.1.3,Create a wrong y
0.1.3,Create the object
0.1.3,Fit the data
0.1.3,Define the parameter for the under-sampling
0.1.3,Create the object
0.1.3,Fit the data
0.1.3,Check if the data information have been computed
0.1.3,Define the parameter for the under-sampling
0.1.3,Create the object
0.1.3,Define the ratio parameter
0.1.3,Create the sampling object
0.1.3,Get the different subset
0.1.3,Define the ratio parameter
0.1.3,Create the sampling object
0.1.3,Get the different subset
0.1.3,Create the object
0.1.2,! /usr/bin/env python
0.1.2,"load all vars into globals, otherwise"
0.1.2,the later function call using global vars doesn't work.
0.1.2,"Allow command-lines such as ""python setup.py build install"""
0.1.2,Make sources available using relative paths from this file's directory.
0.1.2,-*- coding: utf-8 -*-
0.1.2,
0.1.2,"imbalanced-learn documentation build configuration file, created by"
0.1.2,sphinx-quickstart on Mon Jan 18 14:44:12 2016.
0.1.2,
0.1.2,This file is execfile()d with the current directory set to its
0.1.2,containing dir.
0.1.2,
0.1.2,Note that not all possible configuration values are present in this
0.1.2,autogenerated file.
0.1.2,
0.1.2,All configuration values have a default; values that are commented out
0.1.2,serve to show the default.
0.1.2,"If extensions (or modules to document with autodoc) are in another directory,"
0.1.2,add these directories to sys.path here. If the directory is relative to the
0.1.2,"documentation root, use os.path.abspath to make it absolute, like shown here."
0.1.2,"sys.path.insert(0, os.path.abspath('.'))"
0.1.2,-- General configuration ---------------------------------------------------
0.1.2,Try to override the matplotlib configuration as early as possible
0.1.2,-- General configuration ------------------------------------------------
0.1.2,"If your documentation needs a minimal Sphinx version, state it here."
0.1.2,needs_sphinx = '1.0'
0.1.2,"Add any Sphinx extension module names here, as strings. They can be"
0.1.2,extensions coming with Sphinx (named 'sphinx.ext.*') or your custom
0.1.2,ones.
0.1.2,path to your examples scripts
0.1.2,path where to save gallery generated examples
0.1.2,"Add any paths that contain templates here, relative to this directory."
0.1.2,generate autosummary even if no references
0.1.2,The suffix of source filenames.
0.1.2,The encoding of source files.
0.1.2,source_encoding = 'utf-8-sig'
0.1.2,Generate the plots for the gallery
0.1.2,The master toctree document.
0.1.2,General information about the project.
0.1.2,"The version info for the project you're documenting, acts as replacement for"
0.1.2,"|version| and |release|, also used in various other places throughout the"
0.1.2,built documents.
0.1.2,
0.1.2,The short X.Y version.
0.1.2,"The full version, including alpha/beta/rc tags."
0.1.2,The language for content autogenerated by Sphinx. Refer to documentation
0.1.2,for a list of supported languages.
0.1.2,language = None
0.1.2,"There are two options for replacing |today|: either, you set today to some"
0.1.2,"non-false value, then it is used:"
0.1.2,today = ''
0.1.2,"Else, today_fmt is used as the format for a strftime call."
0.1.2,"today_fmt = '%B %d, %Y'"
0.1.2,"List of patterns, relative to source directory, that match files and"
0.1.2,directories to ignore when looking for source files.
0.1.2,The reST default role (used for this markup: `text`) to use for all
0.1.2,documents.
0.1.2,default_role = None
0.1.2,"If true, '()' will be appended to :func: etc. cross-reference text."
0.1.2,"If true, the current module name will be prepended to all description"
0.1.2,unit titles (such as .. function::).
0.1.2,add_module_names = True
0.1.2,"If true, sectionauthor and moduleauthor directives will be shown in the"
0.1.2,output. They are ignored by default.
0.1.2,show_authors = False
0.1.2,The name of the Pygments (syntax highlighting) style to use.
0.1.2,A list of ignored prefixes for module index sorting.
0.1.2,modindex_common_prefix = []
0.1.2,"If true, keep warnings as ""system message"" paragraphs in the built documents."
0.1.2,keep_warnings = False
0.1.2,-- Options for HTML output ----------------------------------------------
0.1.2,The theme to use for HTML and HTML Help pages.  See the documentation for
0.1.2,a list of builtin themes.
0.1.2,Theme options are theme-specific and customize the look and feel of a theme
0.1.2,"further.  For a list of options available for each theme, see the"
0.1.2,documentation.
0.1.2,html_theme_options = {}
0.1.2,"Add any paths that contain custom themes here, relative to this directory."
0.1.2,"The name for this set of Sphinx documents.  If None, it defaults to"
0.1.2,"""<project> v<release> documentation""."
0.1.2,html_title = None
0.1.2,A shorter title for the navigation bar.  Default is the same as html_title.
0.1.2,html_short_title = None
0.1.2,The name of an image file (relative to this directory) to place at the top
0.1.2,of the sidebar.
0.1.2,html_logo = None
0.1.2,The name of an image file (within the static path) to use as favicon of the
0.1.2,docs.  This file should be a Windows icon file (.ico) being 16x16 or 32x32
0.1.2,pixels large.
0.1.2,html_favicon = None
0.1.2,"Add any paths that contain custom static files (such as style sheets) here,"
0.1.2,"relative to this directory. They are copied after the builtin static files,"
0.1.2,"so a file named ""default.css"" will overwrite the builtin ""default.css""."
0.1.2,Add any extra paths that contain custom files (such as robots.txt or
0.1.2,".htaccess) here, relative to this directory. These files are copied"
0.1.2,directly to the root of the documentation.
0.1.2,html_extra_path = []
0.1.2,"If not '', a 'Last updated on:' timestamp is inserted at every page bottom,"
0.1.2,using the given strftime format.
0.1.2,"html_last_updated_fmt = '%b %d, %Y'"
0.1.2,"If true, SmartyPants will be used to convert quotes and dashes to"
0.1.2,typographically correct entities.
0.1.2,html_use_smartypants = True
0.1.2,"Custom sidebar templates, maps document names to template names."
0.1.2,html_sidebars = {}
0.1.2,"Additional templates that should be rendered to pages, maps page names to"
0.1.2,template names.
0.1.2,html_additional_pages = {}
0.1.2,"If false, no module index is generated."
0.1.2,html_domain_indices = True
0.1.2,"If false, no index is generated."
0.1.2,html_use_index = True
0.1.2,"If true, the index is split into individual pages for each letter."
0.1.2,html_split_index = False
0.1.2,"If true, links to the reST sources are added to the pages."
0.1.2,html_show_sourcelink = True
0.1.2,"If true, ""Created using Sphinx"" is shown in the HTML footer. Default is True."
0.1.2,html_show_sphinx = True
0.1.2,"If true, ""(C) Copyright ..."" is shown in the HTML footer. Default is True."
0.1.2,html_show_copyright = True
0.1.2,"If true, an OpenSearch description file will be output, and all pages will"
0.1.2,contain a <link> tag referring to it.  The value of this option must be the
0.1.2,base URL from which the finished HTML is served.
0.1.2,html_use_opensearch = ''
0.1.2,"This is the file name suffix for HTML files (e.g. "".xhtml"")."
0.1.2,html_file_suffix = None
0.1.2,Output file base name for HTML help builder.
0.1.2,-- Options for LaTeX output ---------------------------------------------
0.1.2,The paper size ('letterpaper' or 'a4paper').
0.1.2,"'papersize': 'letterpaper',"
0.1.2,"The font size ('10pt', '11pt' or '12pt')."
0.1.2,"'pointsize': '10pt',"
0.1.2,Additional stuff for the LaTeX preamble.
0.1.2,"'preamble': '',"
0.1.2,Grouping the document tree into LaTeX files. List of tuples
0.1.2,"(source start file, target name, title,"
0.1.2,"author, documentclass [howto, manual, or own class])."
0.1.2,The name of an image file (relative to this directory) to place at the top of
0.1.2,the title page.
0.1.2,latex_logo = None
0.1.2,"For ""manual"" documents, if this is true, then toplevel headings are parts,"
0.1.2,not chapters.
0.1.2,latex_use_parts = False
0.1.2,"If true, show page references after internal links."
0.1.2,latex_show_pagerefs = False
0.1.2,"If true, show URL addresses after external links."
0.1.2,latex_show_urls = False
0.1.2,Documents to append as an appendix to all manuals.
0.1.2,latex_appendices = []
0.1.2,"If false, no module index is generated."
0.1.2,latex_domain_indices = True
0.1.2,-- Options for manual page output ---------------------------------------
0.1.2,One entry per manual page. List of tuples
0.1.2,"(source start file, name, description, authors, manual section)."
0.1.2,"If true, show URL addresses after external links."
0.1.2,man_show_urls = False
0.1.2,-- Options for Texinfo output -------------------------------------------
0.1.2,Grouping the document tree into Texinfo files. List of tuples
0.1.2,"(source start file, target name, title, author,"
0.1.2,"dir menu entry, description, category)"
0.1.2,"generate empty examples files, so that we don't get"
0.1.2,inclusion errors if there are no examples for a class / module
0.1.2,touch file
0.1.2,Documents to append as an appendix to all manuals.
0.1.2,texinfo_appendices = []
0.1.2,"If false, no module index is generated."
0.1.2,texinfo_domain_indices = True
0.1.2,"How to display URL addresses: 'footnote', 'no', or 'inline'."
0.1.2,texinfo_show_urls = 'footnote'
0.1.2,"If true, do not generate a @detailmenu in the ""Top"" node's menu."
0.1.2,texinfo_no_detailmenu = False
0.1.2,Example configuration for intersphinx: refer to the Python standard library.
0.1.2,Define some color for the plotting
0.1.2,Generate the dataset
0.1.2,Instanciate a PCA object for the sake of easy visualisation
0.1.2,Fit and transform x to visualise inside a 2D feature space
0.1.2,Apply SMOTE SVM
0.1.2,"Two subplots, unpack the axes array immediately"
0.1.2,Define some color for the plotting
0.1.2,Generate the dataset
0.1.2,Instanciate a PCA object for the sake of easy visualisation
0.1.2,Fit and transform x to visualise inside a 2D feature space
0.1.2,Apply Borderline SMOTE 2
0.1.2,"Two subplots, unpack the axes array immediately"
0.1.2,Define some color for the plotting
0.1.2,Generate the dataset
0.1.2,Instanciate a PCA object for the sake of easy visualisation
0.1.2,Fit and transform x to visualise inside a 2D feature space
0.1.2,Apply regular SMOTE
0.1.2,"Two subplots, unpack the axes array immediately"
0.1.2,Define some color for the plotting
0.1.2,Generate the dataset
0.1.2,Instanciate a PCA object for the sake of easy visualisation
0.1.2,Fit and transform x to visualise inside a 2D feature space
0.1.2,Apply the random over-sampling
0.1.2,"Two subplots, unpack the axes array immediately"
0.1.2,Define some color for the plotting
0.1.2,Generate the dataset
0.1.2,Instanciate a PCA object for the sake of easy visualisation
0.1.2,Fit and transform x to visualise inside a 2D feature space
0.1.2,Apply Borderline SMOTE 1
0.1.2,"Two subplots, unpack the axes array immediately"
0.1.2,Define some color for the plotting
0.1.2,Generate the dataset
0.1.2,Instanciate a PCA object for the sake of easy visualisation
0.1.2,Fit and transform x to visualise inside a 2D feature space
0.1.2,Apply the random over-sampling
0.1.2,"Two subplots, unpack the axes array immediately"
0.1.2,Define some color for the plotting
0.1.2,Generate the dataset
0.1.2,Instanciate a PCA object for the sake of easy visualisation
0.1.2,Fit and transform x to visualise inside a 2D feature space
0.1.2,Apply SMOTE + ENN
0.1.2,"Two subplots, unpack the axes array immediately"
0.1.2,Define some color for the plotting
0.1.2,Generate the dataset
0.1.2,Instanciate a PCA object for the sake of easy visualisation
0.1.2,Fit and transform x to visualise inside a 2D feature space
0.1.2,Apply SMOTE + Tomek links
0.1.2,"Two subplots, unpack the axes array immediately"
0.1.2,Define some color for the plotting
0.1.2,Generate the dataset
0.1.2,Instanciate a PCA object for the sake of easy visualisation
0.1.2,Fit and transform x to visualise inside a 2D feature space
0.1.2,Apply Balance Cascade method
0.1.2,"Two subplots, unpack the axes array immediately"
0.1.2,Define some color for the plotting
0.1.2,Generate the dataset
0.1.2,Instanciate a PCA object for the sake of easy visualisation
0.1.2,Fit and transform x to visualise inside a 2D feature space
0.1.2,Apply Easy Ensemble
0.1.2,"Two subplots, unpack the axes array immediately"
0.1.2,Define some color for the plotting
0.1.2,Generate the dataset
0.1.2,Instanciate a PCA object for the sake of easy visualisation
0.1.2,Fit and transform x to visualise inside a 2D feature space
0.1.2,"Three subplots, unpack the axes array immediately"
0.1.2,Apply the ENN
0.1.2,Apply the RENN
0.1.2,Define some color for the plotting
0.1.2,Generate the dataset
0.1.2,Instanciate a PCA object for the sake of easy visualisation
0.1.2,Fit and transform x to visualise inside a 2D feature space
0.1.2,Apply Nearmiss 3
0.1.2,"Two subplots, unpack the axes array immediately"
0.1.2,Define some color for the plotting
0.1.2,Generate the dataset
0.1.2,Instanciate a PCA object for the sake of easy visualisation
0.1.2,Fit and transform x to visualise inside a 2D feature space
0.1.2,Apply Edited Nearest Neighbours
0.1.2,"Two subplots, unpack the axes array immediately"
0.1.2,Define some color for the plotting
0.1.2,Generate the dataset
0.1.2,Instanciate a PCA object for the sake of easy visualisation
0.1.2,Fit and transform x to visualise inside a 2D feature space
0.1.2,Apply Tomek Links cleaning
0.1.2,"Two subplots, unpack the axes array immediately"
0.1.2,Define some color for the plotting
0.1.2,Generate the dataset
0.1.2,Instanciate a PCA object for the sake of easy visualisation
0.1.2,Fit and transform x to visualise inside a 2D feature space
0.1.2,Apply One-Sided Selection
0.1.2,"Two subplots, unpack the axes array immediately"
0.1.2,Define some color for the plotting
0.1.2,Generate the dataset
0.1.2,Instanciate a PCA object for the sake of easy visualisation
0.1.2,Fit and transform x to visualise inside a 2D feature space
0.1.2,Apply Nearmiss 2
0.1.2,"Two subplots, unpack the axes array immediately"
0.1.2,Define some color for the plotting
0.1.2,Generate the dataset
0.1.2,Instanciate a PCA object for the sake of easy visualisation
0.1.2,Fit and transform x to visualise inside a 2D feature space
0.1.2,Apply neighbourhood cleaning rule
0.1.2,"Two subplots, unpack the axes array immediately"
0.1.2,Define some color for the plotting
0.1.2,Generate the dataset
0.1.2,Instanciate a PCA object for the sake of easy visualisation
0.1.2,Fit and transform x to visualise inside a 2D feature space
0.1.2,Apply Condensed Nearest Neighbours
0.1.2,"Two subplots, unpack the axes array immediately"
0.1.2,Define some color for the plotting
0.1.2,Generate the dataset
0.1.2,Instanciate a PCA object for the sake of easy visualisation
0.1.2,Fit and transform x to visualise inside a 2D feature space
0.1.2,Apply Cluster Centroids
0.1.2,"Two subplots, unpack the axes array immediately"
0.1.2,Define some color for the plotting
0.1.2,Generate the dataset
0.1.2,Instanciate a PCA object for the sake of easy visualisation
0.1.2,Fit and transform x to visualise inside a 2D feature space
0.1.2,Apply the random under-sampling
0.1.2,"Two subplots, unpack the axes array immediately"
0.1.2,Define some color for the plotting
0.1.2,Generate the dataset
0.1.2,"Two subplots, unpack the axes array immediately"
0.1.2,Define some color for the plotting
0.1.2,Generate the dataset
0.1.2,Instanciate a PCA object for the sake of easy visualisation
0.1.2,Fit and transform x to visualise inside a 2D feature space
0.1.2,Apply Nearmiss 1
0.1.2,"Two subplots, unpack the axes array immediately"
0.1.2,Generate the dataset
0.1.2,Instanciate a PCA object for the sake of easy visualisation
0.1.2,Create the samplers
0.1.2,Create teh classifier
0.1.2,Make the splits
0.1.2,Add one transformers and two samplers in the pipeline object
0.1.2,Based on NiLearn package
0.1.2,License: simplified BSD
0.1.2,"PEP0440 compatible formatted version, see:"
0.1.2,https://www.python.org/dev/peps/pep-0440/
0.1.2,
0.1.2,Generic release markers:
0.1.2,X.Y
0.1.2,X.Y.Z # For bugfix releases
0.1.2,
0.1.2,Admissible pre-release markers:
0.1.2,X.YaN # Alpha release
0.1.2,X.YbN # Beta release
0.1.2,X.YrcN # Release Candidate
0.1.2,X.Y # Final release
0.1.2,
0.1.2,Dev branch marker is: 'X.Y.dev' or 'X.Y.devN' where N is an integer.
0.1.2,'X.Y.dev0' is the canonical version of 'X.Y.dev'
0.1.2,
0.1.2,"This is a tuple to preserve order, so that dependencies are checked"
0.1.2,in some meaningful order (more => less 'core').  We avoid using
0.1.2,collections.OrderedDict to preserve Python 2.6 compatibility.
0.1.2,Avoid choking on modules with no __version__ attribute
0.1.2,Skip check only when installing and it's a module that
0.1.2,will be auto-installed.
0.1.2,Check the consistency of X and y
0.1.2,Get all the unique elements in the target array
0.1.2,# Raise an error if there is only one class
0.1.2,if uniques.size == 1:
0.1.2,"raise RuntimeError(""Only one class detected, aborting..."")"
0.1.2,Raise a warning for the moment to be compatible with BaseEstimator
0.1.2,Store the size of X to check at sampling time if we have the
0.1.2,same data
0.1.2,Create a dictionary containing the class statistics
0.1.2,Find the minority and majority classes
0.1.2,Check if the ratio provided at initialisation make sense
0.1.2,Check the consistency of X and y
0.1.2,Check that the data have been fitted
0.1.2,Check if the size of the data is identical than at fitting
0.1.2,The ratio correspond to the number of samples in the minority class
0.1.2,"over the number of samples in the majority class. Thus, the ratio"
0.1.2,cannot be greater than 1.0
0.1.2,Adapted from scikit-learn
0.1.2,Author: Edouard Duchesnay
0.1.2,Gael Varoquaux
0.1.2,Virgile Fritsch
0.1.2,Alexandre Gramfort
0.1.2,Lars Buitinck
0.1.2,chkoar
0.1.2,License: BSD
0.1.2,BaseEstimator interface
0.1.2,shallow copy of steps
0.1.2,Estimator interface
0.1.2,Boolean controlling whether the joblib caches should be
0.1.2,"flushed if the version of certain modules changes (eg nibabel, as it"
0.1.2,does not respect the backward compatibility in some of its internal
0.1.2,structures
0.1.2,This  is used in nilearn._utils.cache_mixin
0.1.2,list all submodules available in imblearn and version
0.1.2,Keep the samples from the majority class
0.1.2,Loop over the other classes over picking at random
0.1.2,"If this is the majority class, skip it"
0.1.2,Define the number of sample to create
0.1.2,Pick some elements at random
0.1.2,Concatenate to the majority class
0.1.2,Keep the samples from the majority class
0.1.2,Define the number of sample to create
0.1.2,We handle only two classes problem for the moment.
0.1.2,Start by separating minority class features and target values.
0.1.2,Print if verbose is true
0.1.2,"Look for k-th nearest neighbours, excluding, of course, the"
0.1.2,point itself.
0.1.2,Get the distance to the NN
0.1.2,Compute the ratio of majority samples next to minority samples
0.1.2,Normalize the ratio
0.1.2,Compute the number of sample to be generated
0.1.2,For each minority samples
0.1.2,Pick-up the neighbors wanted
0.1.2,Create a new sample
0.1.2,Find the NN for each samples
0.1.2,Exclude the sample itself
0.1.2,Count how many NN belong to the minority class
0.1.2,Find the class corresponding to the label in x
0.1.2,Compute the number of majority samples in the NN
0.1.2,Samples are in danger for m/2 <= m' < m
0.1.2,Samples are noise for m = m'
0.1.2,Check the consistency of X
0.1.2,Check the random state
0.1.2,A matrix to store the synthetic samples
0.1.2,# Set seeds
0.1.2,"seeds = random_state.randint(low=0,"
0.1.2,"high=100 * len(nn_num.flatten()),"
0.1.2,size=n_samples)
0.1.2,Randomly pick samples to construct neighbours from
0.1.2,Loop over the NN matrix and create new samples
0.1.2,"NN lines relate to original sample, columns to its"
0.1.2,nearest neighbours
0.1.2,"Take a step of random size (0,1) in the direction of the"
0.1.2,n nearest neighbours
0.1.2,if self.random_state is None:
0.1.2,np.random.seed(seeds[i])
0.1.2,else:
0.1.2,np.random.seed(self.random_state)
0.1.2,Construct synthetic sample
0.1.2,The returned target vector is simply a repetition of the
0.1.2,minority label
0.1.2,Define the number of sample to create
0.1.2,We handle only two classes problem for the moment.
0.1.2,Start by separating minority class features and target values.
0.1.2,If regular SMOTE is to be performed
0.1.2,"Look for k-th nearest neighbours, excluding, of course, the"
0.1.2,point itself.
0.1.2,Matrix with k-th nearest neighbours indexes for each minority
0.1.2,element.
0.1.2,--- Generating synthetic samples
0.1.2,Use static method make_samples to generate minority samples
0.1.2,Concatenate the newly generated samples to the original data set
0.1.2,Find the NNs for all samples in the data set.
0.1.2,Boolean array with True for minority samples in danger
0.1.2,"If all minority samples are safe, return the original data set."
0.1.2,"All are safe, nothing to be done here."
0.1.2,"If we got here is because some samples are in danger, we need to"
0.1.2,find the NNs among the minority class to create the new synthetic
0.1.2,samples.
0.1.2,
0.1.2,We start by changing the number of NNs to consider from m + 1
0.1.2,to k + 1
0.1.2,nns...#
0.1.2,B1 and B2 types diverge here!!!
0.1.2,Create synthetic samples for borderline points.
0.1.2,Concatenate the newly generated samples to the original
0.1.2,dataset
0.1.2,Reset the k-neighbours to m+1 neighbours
0.1.2,Split the number of synthetic samples between only minority
0.1.2,"(type 1), or minority and majority (with reduced step size)"
0.1.2,(type 2).
0.1.2,The fraction is sampled from a beta distribution centered
0.1.2,around 0.5 with variance ~0.01
0.1.2,Only minority
0.1.2,Only majority with smaller step size
0.1.2,Concatenate the newly generated samples to the original
0.1.2,data set
0.1.2,Reset the k-neighbours to m+1 neighbours
0.1.2,The SVM smote model fits a support vector machine
0.1.2,classifier to the data and uses the support vector to
0.1.2,"provide a notion of boundary. Unlike regular smote, where"
0.1.2,such notion relies on proportion of nearest neighbours
0.1.2,belonging to each class.
0.1.2,Fit SVM to the full data#
0.1.2,Find the support vectors and their corresponding indexes
0.1.2,"First, find the nn of all the samples to identify samples"
0.1.2,in danger and noisy ones
0.1.2,"As usual, fit a nearest neighbour model to the data"
0.1.2,"Now, get rid of noisy support vectors"
0.1.2,Remove noisy support vectors
0.1.2,Proceed to find support vectors NNs among the minority class
0.1.2,Split the number of synthetic samples between interpolation and
0.1.2,extrapolation
0.1.2,The fraction are sampled from a beta distribution with mean
0.1.2,0.5 and variance 0.01#
0.1.2,Interpolate samples in danger
0.1.2,Extrapolate safe samples
0.1.2,Concatenate the newly generated samples to the original data set
0.1.2,not any support vectors in danger
0.1.2,All the support vector in danger
0.1.2,Reset the k-neighbours to m+1 neighbours
0.1.2,--- NN object
0.1.2,Import the NN object from scikit-learn library. Since in the smote
0.1.2,"variations we must first find samples that are in danger, we"
0.1.2,initialize the NN object differently depending on the method chosen
0.1.2,"Regular smote does not look for samples in danger, instead it"
0.1.2,creates synthetic samples directly from the k-th nearest
0.1.2,neighbours with not filtering
0.1.2,"Borderline1, 2 and SVM variations of smote must first look for"
0.1.2,samples that could be considered noise and samples that live
0.1.2,"near the boundary between the classes. Therefore, before"
0.1.2,"creating synthetic samples from the k-th nns, it first look"
0.1.2,for m nearest neighbors to decide whether or not a sample is
0.1.2,noise or near the boundary.
0.1.2,--- SVM smote
0.1.2,"Unlike the borderline variations, the SVM variation uses the support"
0.1.2,vectors to decide which samples are in danger (near the boundary).
0.1.2,Additionally it also introduces extrapolation for samples that are
0.1.2,considered safe (far from boundary) and interpolation for samples
0.1.2,in danger (near the boundary). The level of extrapolation is
0.1.2,controled by the out_step.
0.1.2,Store SVM object with any parameters
0.1.2,Generate a global dataset to use
0.1.2,Define a negative ratio
0.1.2,Define a ratio greater than 1
0.1.2,Define ratio as an unknown string
0.1.2,Define ratio as a list which is not supported
0.1.2,Define a ratio
0.1.2,Create the object
0.1.2,Resample the data
0.1.2,Create a wrong y
0.1.2,Create the object
0.1.2,Fit the data
0.1.2,Create the object
0.1.2,Fit the data
0.1.2,Check if the data information have been computed
0.1.2,Create the object
0.1.2,Resample the data
0.1.2,Resample the data
0.1.2,Create the object
0.1.2,Generate a global dataset to use
0.1.2,Define a negative ratio
0.1.2,Define a ratio greater than 1
0.1.2,Define ratio as an unknown string
0.1.2,Define ratio as a list which is not supported
0.1.2,Define a ratio
0.1.2,Create the object
0.1.2,Resample the data
0.1.2,Create a wrong y
0.1.2,Create the object
0.1.2,Fit the data
0.1.2,Create the object
0.1.2,Fit the data
0.1.2,Check if the data information have been computed
0.1.2,Create the object
0.1.2,Resample the data
0.1.2,Resample the data
0.1.2,Create the object
0.1.2,Generate a global dataset to use
0.1.2,Define a negative ratio
0.1.2,Define a ratio greater than 1
0.1.2,Define ratio as an unknown string
0.1.2,Define ratio as a list which is not supported
0.1.2,Create the object
0.1.2,Resample the data
0.1.2,Create a wrong y
0.1.2,Create the object
0.1.2,Fit the data
0.1.2,Check if the data information have been computed
0.1.2,Create the object
0.1.2,Create the object
0.1.2,Fit the data
0.1.2,Create the object
0.1.2,Fit the data
0.1.2,Create the object
0.1.2,Fit the data
0.1.2,Create the object
0.1.2,Fit the data
0.1.2,Create the object
0.1.2,Fit the data
0.1.2,Create the object
0.1.2,Start with the minority class
0.1.2,All the minority class samples will be preserved
0.1.2,If we need to offer support for the indices
0.1.2,Loop over the other classes under picking at random
0.1.2,"If the minority class is up, skip it"
0.1.2,Randomly get one sample from the majority class
0.1.2,Generate the index to select
0.1.2,Create the set C
0.1.2,Create the set S
0.1.2,Remove the seed from S since that it will be added anyway
0.1.2,Create a k-NN classifier
0.1.2,Fit C into the knn
0.1.2,Classify on S
0.1.2,Find the misclassified S_y
0.1.2,If we need to offer support for the indices selected
0.1.2,We concatenate the misclassified samples with the seed and the
0.1.2,minority samples
0.1.2,Find the nearest neighbour of every point
0.1.2,Send the information to is_tomek function to get boolean vector back
0.1.2,Check if the indices of the samples selected should be returned too
0.1.2,Return the indices of interest
0.1.2,Return data set without majority Tomek links.
0.1.2,Compute the distance considering the farthest neighbour
0.1.2,Sort the list of distance and get the index
0.1.2,Select the desired number of samples
0.1.2,Assign the parameter of the element of this class
0.1.2,Check that the version asked is implemented
0.1.2,Start with the minority class
0.1.2,All the minority class samples will be preserved
0.1.2,Compute the number of cluster needed
0.1.2,If we need to offer support for the indices
0.1.2,"For each element of the current class, find the set of NN"
0.1.2,of the minority class
0.1.2,Call the constructor of the NN
0.1.2,Fit the minority class since that we want to know the distance
0.1.2,to these point
0.1.2,Loop over the other classes under picking at random
0.1.2,"If the minority class is up, skip it"
0.1.2,Get the samples corresponding to the current class
0.1.2,Find the NN
0.1.2,Select the right samples
0.1.2,Find the NN
0.1.2,Select the right samples
0.1.2,We need a new NN object to fit the current class
0.1.2,Find the set of NN to the minority class
0.1.2,Create the subset containing the samples found during the NN
0.1.2,search. Linearize the indexes and remove the double values
0.1.2,Create the subset
0.1.2,Compute the NN considering the current class
0.1.2,If we need to offer support for the indices selected
0.1.2,Check if the indices of the samples selected should be returned too
0.1.2,Return the indices of interest
0.1.2,Compute the number of clusters needed
0.1.2,All the minority class samples will be preserved
0.1.2,If we need to offer support for the indices
0.1.2,Loop over the other classes under-picking at random
0.1.2,"If the minority class is up, skip it"
0.1.2,Pick some elements at random
0.1.2,If we need to offer support for the indices selected
0.1.2,Concatenate to the minority class
0.1.2,Check if the indices of the samples selected should be returned as
0.1.2,well
0.1.2,Return the indices of interest
0.1.2,"Initialize the boolean result as false, and also a counter"
0.1.2,Loop through each sample and looks whether it belongs to the minority
0.1.2,"class. If it does, we don't consider it since we want to keep all"
0.1.2,"minority samples. If, however, it belongs to the majority sample we"
0.1.2,look at its first neighbour. If its closest neighbour also has the
0.1.2,"current sample as its closest neighbour, the two form a Tomek link."
0.1.2,"If they form a tomek link, put a True marker on this"
0.1.2,"sample, and increase counter by one."
0.1.2,Find the nearest neighbour of every point
0.1.2,Send the information to is_tomek function to get boolean vector back
0.1.2,Check if the indices of the samples selected should be returned too
0.1.2,Return the indices of interest
0.1.2,Return data set without majority Tomek links.
0.1.2,Start with the minority class
0.1.2,All the minority class samples will be preserved
0.1.2,If we need to offer support for the indices
0.1.2,Loop over the other classes under picking at random
0.1.2,"If the minority class is up, skip it"
0.1.2,Randomly get one sample from the majority class
0.1.2,Generate the index to select
0.1.2,Create the set C - One majority samples and all minority
0.1.2,Create the set S - all majority samples
0.1.2,Create a k-NN classifier
0.1.2,Fit C into the knn
0.1.2,Check each sample in S if we keep it or drop it
0.1.2,Do not select sample which are already well classified
0.1.2,Classify on S
0.1.2,If the prediction do not agree with the true label
0.1.2,append it in C_x
0.1.2,Keep the index for later
0.1.2,Update C
0.1.2,Fit C into the knn
0.1.2,This experimental to speed up the search
0.1.2,Classify all the element in S and avoid to test the
0.1.2,well classified elements
0.1.2,Find the misclassified S_y
0.1.2,If we need to offer support for the indices selected
0.1.2,Check if the indices of the samples selected should be returned too
0.1.2,Return the indices of interest
0.1.2,Compute the number of cluster needed
0.1.2,Create the clustering object
0.1.2,Start with the minority class
0.1.2,All the minority class samples will be preserved
0.1.2,Loop over the other classes under picking at random
0.1.2,"If the minority class is up, skip it."
0.1.2,Find the centroids via k-means
0.1.2,Concatenate to the minority class
0.1.2,Start with the minority class
0.1.2,All the minority class samples will be preserved
0.1.2,If we need to offer support for the indices
0.1.2,Create a k-NN to fit the whole data
0.1.2,Fit the whole dataset
0.1.2,Loop over the other classes under picking at random
0.1.2,Get the sample of the current class
0.1.2,Get the samples associated
0.1.2,Find the NN for the current class
0.1.2,Get the label of the corresponding to the index
0.1.2,Check which one are the same label than the current class
0.1.2,Make an AND operation through the three neighbours
0.1.2,If the minority class remove the majority samples
0.1.2,Get the index to exclude
0.1.2,Get the index to exclude
0.1.2,Create a vector with the sample to select
0.1.2,Exclude as well the minority sample since that they will be
0.1.2,concatenated later
0.1.2,Get the samples from the majority classes
0.1.2,If we need to offer support for the indices selected
0.1.2,Check if the indices of the samples selected should be returned too
0.1.2,Return the indices of interest
0.1.2,Start with the minority class
0.1.2,All the minority class samples will be preserved
0.1.2,If we need to offer support for the indices
0.1.2,Create a k-NN to fit the whole data
0.1.2,Fit the data
0.1.2,Loop over the other classes under picking at random
0.1.2,"If the minority class is up, skip it"
0.1.2,Get the sample of the current class
0.1.2,Find the NN for the current class
0.1.2,Get the label of the corresponding to the index
0.1.2,Check which one are the same label than the current class
0.1.2,Make the majority vote
0.1.2,Get the samples which agree all together
0.1.2,If we need to offer support for the indices selected
0.1.2,Check if the indices of the samples selected should be returned too
0.1.2,Return the indices of interest
0.1.2,Check if the indices of the samples selected should be returned too
0.1.2,Return the indices of interest
0.1.2,Select the appropriate classifier
0.1.2,Create the different folds
0.1.2,Compute the number of cluster needed
0.1.2,Find the percentile corresponding to the top num_samples
0.1.2,Sample the data
0.1.2,If we need to offer support for the indices
0.1.2,Generate a global dataset to use
0.1.2,Define a negative ratio
0.1.2,Define a ratio greater than 1
0.1.2,Define ratio as an unknown string
0.1.2,Define ratio as a list which is not supported
0.1.2,Define a ratio
0.1.2,Define the parameter for the under-sampling
0.1.2,Create the object
0.1.2,Resample the data
0.1.2,Create a wrong y
0.1.2,Create the object
0.1.2,Fit the data
0.1.2,Define the parameter for the under-sampling
0.1.2,Create the object
0.1.2,Fit the data
0.1.2,Check if the data information have been computed
0.1.2,Define the parameter for the under-sampling
0.1.2,Create the object
0.1.2,Define the parameter for the under-sampling
0.1.2,Create the object
0.1.2,Fit and sample
0.1.2,Define the parameter for the under-sampling
0.1.2,Create the object
0.1.2,Fit and sample
0.1.2,Define the parameter for the under-sampling
0.1.2,Create the object
0.1.2,Fit and sample
0.1.2,Create the object
0.1.2,Generate a global dataset to use
0.1.2,Define a ratio
0.1.2,Create the object
0.1.2,Resample the data
0.1.2,Create a wrong y
0.1.2,Create the object
0.1.2,Fit the data
0.1.2,Check if the data information have been computed
0.1.2,Create the object
0.1.2,Resample the data
0.1.2,Resample the data
0.1.2,Create the object
0.1.2,Generate a global dataset to use
0.1.2,Define a ratio
0.1.2,Create the object
0.1.2,Resample the data
0.1.2,Create a wrong y
0.1.2,Create the object
0.1.2,Fit the data
0.1.2,Check if the data information have been computed
0.1.2,Create the object
0.1.2,Resample the data
0.1.2,Resample the data
0.1.2,Create the object
0.1.2,Generate a global dataset to use
0.1.2,Define a ratio
0.1.2,Create the object
0.1.2,Resample the data
0.1.2,Create a wrong y
0.1.2,Create the object
0.1.2,Fit the data
0.1.2,Check if the data information have been computed
0.1.2,Create the object
0.1.2,Resample the data
0.1.2,Resample the data
0.1.2,Resample the data
0.1.2,Create the object
0.1.2,Generate a global dataset to use
0.1.2,Define a negative ratio
0.1.2,Define a ratio greater than 1
0.1.2,Define ratio as an unknown string
0.1.2,Define ratio as a list which is not supported
0.1.2,Define a ratio
0.1.2,Create the object
0.1.2,Resample the data
0.1.2,Create a wrong y
0.1.2,Create the object
0.1.2,Fit the data
0.1.2,Create the object
0.1.2,Fit the data
0.1.2,Check if the data information have been computed
0.1.2,Create the object
0.1.2,Resample the data
0.1.2,Resample the data
0.1.2,Resample the data
0.1.2,Create the object
0.1.2,Generate a global dataset to use
0.1.2,Define a negative ratio
0.1.2,Define a ratio greater than 1
0.1.2,Define ratio as an unknown string
0.1.2,Define ratio as a list which is not supported
0.1.2,Resample the data
0.1.2,Define a ratio
0.1.2,Create the object
0.1.2,Resample the data
0.1.2,Create a wrong y
0.1.2,Create the object
0.1.2,Fit the data
0.1.2,Create the object
0.1.2,Fit the data
0.1.2,Check if the data information have been computed
0.1.2,Create the object
0.1.2,Resample the data
0.1.2,Resample the data
0.1.2,Resample the data
0.1.2,Resample the data
0.1.2,Resample the data
0.1.2,Resample the data
0.1.2,Resample the data
0.1.2,Resample the data
0.1.2,Resample the data
0.1.2,Create the object
0.1.2,Generate a global dataset to use
0.1.2,Define a ratio
0.1.2,Create the object
0.1.2,Create the object
0.1.2,Resample the data
0.1.2,Create a wrong y
0.1.2,Create the object
0.1.2,Fit the data
0.1.2,Check if the data information have been computed
0.1.2,Create the object
0.1.2,Resample the data
0.1.2,Resample the data
0.1.2,Resample the data
0.1.2,Create the object
0.1.2,Generate a global dataset to use
0.1.2,Define a negative ratio
0.1.2,Define a ratio greater than 1
0.1.2,Define ratio as an unknown string
0.1.2,Define ratio as a list which is not supported
0.1.2,Define a ratio
0.1.2,Define the parameter for the under-sampling
0.1.2,Create the object
0.1.2,Resample the data
0.1.2,Create a wrong y
0.1.2,Create the object
0.1.2,Fit the data
0.1.2,Define the parameter for the under-sampling
0.1.2,Create the object
0.1.2,Fit the data
0.1.2,Check if the data information have been computed
0.1.2,Define the parameter for the under-sampling
0.1.2,Create the object
0.1.2,Define the parameter for the under-sampling
0.1.2,Create the object
0.1.2,Fit and sample
0.1.2,Define the parameter for the under-sampling
0.1.2,Create the object
0.1.2,Fit and sample
0.1.2,Define the parameter for the under-sampling
0.1.2,Create the object
0.1.2,Fit and sample
0.1.2,Create the object
0.1.2,Generate a global dataset to use
0.1.2,Define a ratio
0.1.2,Create the object
0.1.2,Resample the data
0.1.2,Create a wrong y
0.1.2,Create the object
0.1.2,Fit the data
0.1.2,Check if the data information have been computed
0.1.2,Create the object
0.1.2,Resample the data
0.1.2,Resample the data
0.1.2,Create the object
0.1.2,Generate a global dataset to use
0.1.2,Define a negative ratio
0.1.2,Define a ratio greater than 1
0.1.2,Define ratio as an unknown string
0.1.2,Define ratio as a list which is not supported
0.1.2,Define a ratio
0.1.2,Define the parameter for the under-sampling
0.1.2,Create the object
0.1.2,Resample the data
0.1.2,Create a wrong y
0.1.2,Create the object
0.1.2,Fit the data
0.1.2,Define the parameter for the under-sampling
0.1.2,Create the object
0.1.2,Fit the data
0.1.2,Check if the data information have been computed
0.1.2,Define the parameter for the under-sampling
0.1.2,Create the object
0.1.2,Define the parameter for the under-sampling
0.1.2,Create the object
0.1.2,Define the parameter for the under-sampling
0.1.2,Create the object
0.1.2,Fit and sample
0.1.2,Define the parameter for the under-sampling
0.1.2,Create the object
0.1.2,Fit and sample
0.1.2,Generate a global dataset to use
0.1.2,Define a negative ratio
0.1.2,Define a ratio greater than 1
0.1.2,Define ratio as an unknown string
0.1.2,Define ratio as a list which is not supported
0.1.2,Define a ratio
0.1.2,Define the parameter for the under-sampling
0.1.2,Create the object
0.1.2,Resample the data
0.1.2,Create a wrong y
0.1.2,Create the object
0.1.2,Fit the data
0.1.2,Define the parameter for the under-sampling
0.1.2,Create the object
0.1.2,Fit the data
0.1.2,Check if the data information have been computed
0.1.2,Define the parameter for the under-sampling
0.1.2,Create the object
0.1.2,Define the parameter for the under-sampling
0.1.2,Create the object
0.1.2,Fit and sample
0.1.2,Define the parameter for the under-sampling
0.1.2,Create the object
0.1.2,Fit and sample
0.1.2,Define the parameter for the under-sampling
0.1.2,Create the object
0.1.2,Fit and sample
0.1.2,Create the object
0.1.2,Generate a global dataset to use
0.1.2,Define a ratio
0.1.2,Create the object
0.1.2,Resample the data
0.1.2,Create a wrong y
0.1.2,Create the object
0.1.2,Fit the data
0.1.2,Check if the data information have been computed
0.1.2,Create the object
0.1.2,Resample the data
0.1.2,Resample the data
0.1.2,Create the object
0.1.2,Test the various init parameters of the pipeline.
0.1.2,Check that we can't instantiate pipelines with objects without fit
0.1.2,method
0.1.2,Smoke test with only an estimator
0.1.2,Check that params are set
0.1.2,Smoke test the repr:
0.1.2,Test with two objects
0.1.2,Check that we can't use the same stage name twice
0.1.2,Check that params are set
0.1.2,Smoke test the repr:
0.1.2,Check that params are not set when naming them wrong
0.1.2,Test clone
0.1.2,"Check that apart from estimators, the parameters are the same"
0.1.2,Remove estimators that where copied
0.1.2,Test the various methods of the pipeline (anova).
0.1.2,Test with Anova + LogisticRegression
0.1.2,Test that the pipeline can take fit parameters
0.1.2,classifier should return True
0.1.2,and transformer params should not be changed
0.1.2,Test pipeline raises set params error message for nested models.
0.1.2,expected error message
0.1.2,nested model check
0.1.2,Test the various methods of the pipeline (pca + svm).
0.1.2,Test with PCA + SVC
0.1.2,Test the various methods of the pipeline (preprocessing + svm).
0.1.2,check shapes of various prediction functions
0.1.2,test that the fit_predict method is implemented on a pipeline
0.1.2,test that the fit_predict on pipeline yields same results as applying
0.1.2,transform and clustering steps separately
0.1.2,first compute the transform and clustering step separately
0.1.2,use a pipeline to do the transform and clustering in one step
0.1.2,tests that a pipeline does not have fit_predict method when final
0.1.2,step of pipeline does not have fit_predict defined
0.1.2,Test whether pipeline works with a transformer at the end.
0.1.2,Also test pipeline.transform and pipeline.inverse_transform
0.1.2,test transform and fit_transform:
0.1.2,Test whether pipeline works with a transformer missing fit_transform
0.1.2,test fit_transform:
0.1.2,Test the various methods of the pipeline (pca + svm).
0.1.2,Test with PCA + SVC
0.1.2,Test the various methods of the pipeline (pca + svm).
0.1.2,Test with PCA + SVC
0.1.2,Test whether pipeline works with a sampler at the end.
0.1.2,Also test pipeline.sampler
0.1.2,test transform and fit_transform:
0.1.2,Test whether pipeline works with a sampler at the end.
0.1.2,Also test pipeline.sampler
0.1.2,Test the various methods of the pipeline (anova).
0.1.2,Test with RandomUnderSampling + Anova + LogisticRegression
0.1.2,Fit using SMOTE
0.1.2,Transform using SMOTE
0.1.2,Fit and transform using ENN
0.1.2,Fit using SMOTE
0.1.2,Transform using SMOTE
0.1.2,Fit and transform using ENN
0.1.2,Generate a global dataset to use
0.1.2,Define a negative ratio
0.1.2,Define a ratio greater than 1
0.1.2,Define ratio as an unknown string
0.1.2,Define ratio as a list which is not supported
0.1.2,Create the object
0.1.2,Resample the data
0.1.2,Create a wrong y
0.1.2,Create the object
0.1.2,Fit the data
0.1.2,Check if the data information have been computed
0.1.2,Create the object
0.1.2,Create the object
0.1.2,Fit the data
0.1.2,Create the object
0.1.2,Fit the data
0.1.2,Create the object
0.1.2,Generate a global dataset to use
0.1.2,Define a negative ratio
0.1.2,Define a ratio greater than 1
0.1.2,Define ratio as an unknown string
0.1.2,Define ratio as a list which is not supported
0.1.2,Create the object
0.1.2,Resample the data
0.1.2,Create a wrong y
0.1.2,Create the object
0.1.2,Fit the data
0.1.2,Check if the data information have been computed
0.1.2,Create the object
0.1.2,Create the object
0.1.2,Fit the data
0.1.2,Create the object
0.1.2,Fit the data
0.1.2,Create the object
0.1.2,Create the object for random under-sampling
0.1.2,Define the classifier to use
0.1.2,Start with the minority class
0.1.2,Keep the indices of the minority class somewhere if we need to
0.1.2,return them later
0.1.2,Condition to initiliase before the search
0.1.2,Get the initial number of samples to select in the majority class
0.1.2,Create the array characterising the array containing the majority
0.1.2,class
0.1.2,Loop to create the different subsets
0.1.2,Generate an appropriate number of index to extract
0.1.2,from the majority class depending of the false classification
0.1.2,rate of the previous iteration
0.1.2,Mark these indexes as not being considered for next sampling
0.1.2,"For now, we will train and classify on the same data"
0.1.2,"Let see if we should find another solution. Anyway,"
0.1.2,random stuff are still random stuff
0.1.2,Push these data into a new subset
0.1.2,Apply a bootstrap on x_data
0.1.2,Train the classifier using the current data
0.1.2,Train the classifier using the current data
0.1.2,Predict using only the majority class
0.1.2,Basically let's find which sample have to be retained for the
0.1.2,next round
0.1.2,Find the misclassified index to keep them for the next round
0.1.2,Count how many random element will be selected
0.1.2,"We found a new subset, increase the counter"
0.1.2,Check if we have to make an early stopping
0.1.2,Select the remaining data
0.1.2,Select the final batch
0.1.2,Push these data into a new subset
0.1.2,"We found a new subset, increase the counter"
0.1.2,Also check that we will have enough sample to extract at the
0.1.2,next round
0.1.2,Select the remaining data
0.1.2,Select the final batch
0.1.2,Push these data into a new subset
0.1.2,"We found a new subset, increase the counter"
0.1.2,Generate a global dataset to use
0.1.2,Define a negative ratio
0.1.2,Define a ratio greater than 1
0.1.2,Define ratio as an unknown string
0.1.2,Define ratio as a list which is not supported
0.1.2,Define a ratio
0.1.2,Define the parameter for the under-sampling
0.1.2,Create the object
0.1.2,Resample the data
0.1.2,Create a wrong y
0.1.2,Create the object
0.1.2,Fit the data
0.1.2,Define the parameter for the under-sampling
0.1.2,Create the object
0.1.2,Fit the data
0.1.2,Check if the data information have been computed
0.1.2,Define the parameter for the under-sampling
0.1.2,Create the object
0.1.2,Define the ratio parameter
0.1.2,Create the sampling object
0.1.2,Get the different subset
0.1.2,Check each array
0.1.2,Define the ratio parameter
0.1.2,Create the sampling object
0.1.2,Get the different subset
0.1.2,Check each array
0.1.2,Define the ratio parameter
0.1.2,Create the sampling object
0.1.2,Get the different subset
0.1.2,Check each array
0.1.2,Define the ratio parameter
0.1.2,Create the sampling object
0.1.2,Get the different subset
0.1.2,Check each array
0.1.2,Define the ratio parameter
0.1.2,Create the sampling object
0.1.2,Get the different subset
0.1.2,Check each array
0.1.2,Define the ratio parameter
0.1.2,Create the sampling object
0.1.2,Get the different subset
0.1.2,Check each array
0.1.2,Define the ratio parameter
0.1.2,Create the sampling object
0.1.2,Get the different subset
0.1.2,Check each array
0.1.2,Define the ratio parameter
0.1.2,Define the ratio parameter
0.1.2,Create the sampling object
0.1.2,Get the different subset
0.1.2,Check each array
0.1.2,Create the object
0.1.2,Generate a global dataset to use
0.1.2,Define a negative ratio
0.1.2,Define a ratio greater than 1
0.1.2,Define ratio as an unknown string
0.1.2,Define ratio as a list which is not supported
0.1.2,Define a ratio
0.1.2,Define the parameter for the under-sampling
0.1.2,Create the object
0.1.2,Resample the data
0.1.2,Create a wrong y
0.1.2,Create the object
0.1.2,Fit the data
0.1.2,Define the parameter for the under-sampling
0.1.2,Create the object
0.1.2,Fit the data
0.1.2,Check if the data information have been computed
0.1.2,Define the parameter for the under-sampling
0.1.2,Create the object
0.1.2,Define the ratio parameter
0.1.2,Create the sampling object
0.1.2,Get the different subset
0.1.2,Define the ratio parameter
0.1.2,Create the sampling object
0.1.2,Get the different subset
0.1.2,Create the object
0.1.1,! /usr/bin/env python
0.1.1,"load all vars into globals, otherwise"
0.1.1,the later function call using global vars doesn't work.
0.1.1,"Allow command-lines such as ""python setup.py build install"""
0.1.1,Make sources available using relative paths from this file's directory.
0.1.1,-*- coding: utf-8 -*-
0.1.1,
0.1.1,"imbalanced-learn documentation build configuration file, created by"
0.1.1,sphinx-quickstart on Mon Jan 18 14:44:12 2016.
0.1.1,
0.1.1,This file is execfile()d with the current directory set to its
0.1.1,containing dir.
0.1.1,
0.1.1,Note that not all possible configuration values are present in this
0.1.1,autogenerated file.
0.1.1,
0.1.1,All configuration values have a default; values that are commented out
0.1.1,serve to show the default.
0.1.1,"If extensions (or modules to document with autodoc) are in another directory,"
0.1.1,add these directories to sys.path here. If the directory is relative to the
0.1.1,"documentation root, use os.path.abspath to make it absolute, like shown here."
0.1.1,"sys.path.insert(0, os.path.abspath('.'))"
0.1.1,-- General configuration ---------------------------------------------------
0.1.1,Try to override the matplotlib configuration as early as possible
0.1.1,-- General configuration ------------------------------------------------
0.1.1,"If your documentation needs a minimal Sphinx version, state it here."
0.1.1,needs_sphinx = '1.0'
0.1.1,"Add any Sphinx extension module names here, as strings. They can be"
0.1.1,extensions coming with Sphinx (named 'sphinx.ext.*') or your custom
0.1.1,ones.
0.1.1,path to your examples scripts
0.1.1,path where to save gallery generated examples
0.1.1,"Add any paths that contain templates here, relative to this directory."
0.1.1,generate autosummary even if no references
0.1.1,The suffix of source filenames.
0.1.1,The encoding of source files.
0.1.1,source_encoding = 'utf-8-sig'
0.1.1,Generate the plots for the gallery
0.1.1,The master toctree document.
0.1.1,General information about the project.
0.1.1,"The version info for the project you're documenting, acts as replacement for"
0.1.1,"|version| and |release|, also used in various other places throughout the"
0.1.1,built documents.
0.1.1,
0.1.1,The short X.Y version.
0.1.1,"The full version, including alpha/beta/rc tags."
0.1.1,The language for content autogenerated by Sphinx. Refer to documentation
0.1.1,for a list of supported languages.
0.1.1,language = None
0.1.1,"There are two options for replacing |today|: either, you set today to some"
0.1.1,"non-false value, then it is used:"
0.1.1,today = ''
0.1.1,"Else, today_fmt is used as the format for a strftime call."
0.1.1,"today_fmt = '%B %d, %Y'"
0.1.1,"List of patterns, relative to source directory, that match files and"
0.1.1,directories to ignore when looking for source files.
0.1.1,The reST default role (used for this markup: `text`) to use for all
0.1.1,documents.
0.1.1,default_role = None
0.1.1,"If true, '()' will be appended to :func: etc. cross-reference text."
0.1.1,"If true, the current module name will be prepended to all description"
0.1.1,unit titles (such as .. function::).
0.1.1,add_module_names = True
0.1.1,"If true, sectionauthor and moduleauthor directives will be shown in the"
0.1.1,output. They are ignored by default.
0.1.1,show_authors = False
0.1.1,The name of the Pygments (syntax highlighting) style to use.
0.1.1,A list of ignored prefixes for module index sorting.
0.1.1,modindex_common_prefix = []
0.1.1,"If true, keep warnings as ""system message"" paragraphs in the built documents."
0.1.1,keep_warnings = False
0.1.1,-- Options for HTML output ----------------------------------------------
0.1.1,The theme to use for HTML and HTML Help pages.  See the documentation for
0.1.1,a list of builtin themes.
0.1.1,Theme options are theme-specific and customize the look and feel of a theme
0.1.1,"further.  For a list of options available for each theme, see the"
0.1.1,documentation.
0.1.1,html_theme_options = {}
0.1.1,"Add any paths that contain custom themes here, relative to this directory."
0.1.1,"The name for this set of Sphinx documents.  If None, it defaults to"
0.1.1,"""<project> v<release> documentation""."
0.1.1,html_title = None
0.1.1,A shorter title for the navigation bar.  Default is the same as html_title.
0.1.1,html_short_title = None
0.1.1,The name of an image file (relative to this directory) to place at the top
0.1.1,of the sidebar.
0.1.1,html_logo = None
0.1.1,The name of an image file (within the static path) to use as favicon of the
0.1.1,docs.  This file should be a Windows icon file (.ico) being 16x16 or 32x32
0.1.1,pixels large.
0.1.1,html_favicon = None
0.1.1,"Add any paths that contain custom static files (such as style sheets) here,"
0.1.1,"relative to this directory. They are copied after the builtin static files,"
0.1.1,"so a file named ""default.css"" will overwrite the builtin ""default.css""."
0.1.1,Add any extra paths that contain custom files (such as robots.txt or
0.1.1,".htaccess) here, relative to this directory. These files are copied"
0.1.1,directly to the root of the documentation.
0.1.1,html_extra_path = []
0.1.1,"If not '', a 'Last updated on:' timestamp is inserted at every page bottom,"
0.1.1,using the given strftime format.
0.1.1,"html_last_updated_fmt = '%b %d, %Y'"
0.1.1,"If true, SmartyPants will be used to convert quotes and dashes to"
0.1.1,typographically correct entities.
0.1.1,html_use_smartypants = True
0.1.1,"Custom sidebar templates, maps document names to template names."
0.1.1,html_sidebars = {}
0.1.1,"Additional templates that should be rendered to pages, maps page names to"
0.1.1,template names.
0.1.1,html_additional_pages = {}
0.1.1,"If false, no module index is generated."
0.1.1,html_domain_indices = True
0.1.1,"If false, no index is generated."
0.1.1,html_use_index = True
0.1.1,"If true, the index is split into individual pages for each letter."
0.1.1,html_split_index = False
0.1.1,"If true, links to the reST sources are added to the pages."
0.1.1,html_show_sourcelink = True
0.1.1,"If true, ""Created using Sphinx"" is shown in the HTML footer. Default is True."
0.1.1,html_show_sphinx = True
0.1.1,"If true, ""(C) Copyright ..."" is shown in the HTML footer. Default is True."
0.1.1,html_show_copyright = True
0.1.1,"If true, an OpenSearch description file will be output, and all pages will"
0.1.1,contain a <link> tag referring to it.  The value of this option must be the
0.1.1,base URL from which the finished HTML is served.
0.1.1,html_use_opensearch = ''
0.1.1,"This is the file name suffix for HTML files (e.g. "".xhtml"")."
0.1.1,html_file_suffix = None
0.1.1,Output file base name for HTML help builder.
0.1.1,-- Options for LaTeX output ---------------------------------------------
0.1.1,The paper size ('letterpaper' or 'a4paper').
0.1.1,"'papersize': 'letterpaper',"
0.1.1,"The font size ('10pt', '11pt' or '12pt')."
0.1.1,"'pointsize': '10pt',"
0.1.1,Additional stuff for the LaTeX preamble.
0.1.1,"'preamble': '',"
0.1.1,Grouping the document tree into LaTeX files. List of tuples
0.1.1,"(source start file, target name, title,"
0.1.1,"author, documentclass [howto, manual, or own class])."
0.1.1,The name of an image file (relative to this directory) to place at the top of
0.1.1,the title page.
0.1.1,latex_logo = None
0.1.1,"For ""manual"" documents, if this is true, then toplevel headings are parts,"
0.1.1,not chapters.
0.1.1,latex_use_parts = False
0.1.1,"If true, show page references after internal links."
0.1.1,latex_show_pagerefs = False
0.1.1,"If true, show URL addresses after external links."
0.1.1,latex_show_urls = False
0.1.1,Documents to append as an appendix to all manuals.
0.1.1,latex_appendices = []
0.1.1,"If false, no module index is generated."
0.1.1,latex_domain_indices = True
0.1.1,-- Options for manual page output ---------------------------------------
0.1.1,One entry per manual page. List of tuples
0.1.1,"(source start file, name, description, authors, manual section)."
0.1.1,"If true, show URL addresses after external links."
0.1.1,man_show_urls = False
0.1.1,-- Options for Texinfo output -------------------------------------------
0.1.1,Grouping the document tree into Texinfo files. List of tuples
0.1.1,"(source start file, target name, title, author,"
0.1.1,"dir menu entry, description, category)"
0.1.1,"generate empty examples files, so that we don't get"
0.1.1,inclusion errors if there are no examples for a class / module
0.1.1,touch file
0.1.1,Documents to append as an appendix to all manuals.
0.1.1,texinfo_appendices = []
0.1.1,"If false, no module index is generated."
0.1.1,texinfo_domain_indices = True
0.1.1,"How to display URL addresses: 'footnote', 'no', or 'inline'."
0.1.1,texinfo_show_urls = 'footnote'
0.1.1,"If true, do not generate a @detailmenu in the ""Top"" node's menu."
0.1.1,texinfo_no_detailmenu = False
0.1.1,Example configuration for intersphinx: refer to the Python standard library.
0.1.1,Define some color for the plotting
0.1.1,Generate the dataset
0.1.1,Instanciate a PCA object for the sake of easy visualisation
0.1.1,Fit and transform x to visualise inside a 2D feature space
0.1.1,Apply SMOTE SVM
0.1.1,"Two subplots, unpack the axes array immediately"
0.1.1,Define some color for the plotting
0.1.1,Generate the dataset
0.1.1,Instanciate a PCA object for the sake of easy visualisation
0.1.1,Fit and transform x to visualise inside a 2D feature space
0.1.1,Apply Borderline SMOTE 2
0.1.1,"Two subplots, unpack the axes array immediately"
0.1.1,Define some color for the plotting
0.1.1,Generate the dataset
0.1.1,Instanciate a PCA object for the sake of easy visualisation
0.1.1,Fit and transform x to visualise inside a 2D feature space
0.1.1,Apply regular SMOTE
0.1.1,"Two subplots, unpack the axes array immediately"
0.1.1,Define some color for the plotting
0.1.1,Generate the dataset
0.1.1,Instanciate a PCA object for the sake of easy visualisation
0.1.1,Fit and transform x to visualise inside a 2D feature space
0.1.1,Apply the random over-sampling
0.1.1,"Two subplots, unpack the axes array immediately"
0.1.1,Define some color for the plotting
0.1.1,Generate the dataset
0.1.1,Instanciate a PCA object for the sake of easy visualisation
0.1.1,Fit and transform x to visualise inside a 2D feature space
0.1.1,Apply Borderline SMOTE 1
0.1.1,"Two subplots, unpack the axes array immediately"
0.1.1,Define some color for the plotting
0.1.1,Generate the dataset
0.1.1,Instanciate a PCA object for the sake of easy visualisation
0.1.1,Fit and transform x to visualise inside a 2D feature space
0.1.1,Apply the random over-sampling
0.1.1,"Two subplots, unpack the axes array immediately"
0.1.1,Define some color for the plotting
0.1.1,Generate the dataset
0.1.1,Instanciate a PCA object for the sake of easy visualisation
0.1.1,Fit and transform x to visualise inside a 2D feature space
0.1.1,Apply SMOTE + ENN
0.1.1,"Two subplots, unpack the axes array immediately"
0.1.1,Define some color for the plotting
0.1.1,Generate the dataset
0.1.1,Instanciate a PCA object for the sake of easy visualisation
0.1.1,Fit and transform x to visualise inside a 2D feature space
0.1.1,Apply SMOTE + Tomek links
0.1.1,"Two subplots, unpack the axes array immediately"
0.1.1,Define some color for the plotting
0.1.1,Generate the dataset
0.1.1,Instanciate a PCA object for the sake of easy visualisation
0.1.1,Fit and transform x to visualise inside a 2D feature space
0.1.1,Apply Balance Cascade method
0.1.1,"Two subplots, unpack the axes array immediately"
0.1.1,Define some color for the plotting
0.1.1,Generate the dataset
0.1.1,Instanciate a PCA object for the sake of easy visualisation
0.1.1,Fit and transform x to visualise inside a 2D feature space
0.1.1,Apply Easy Ensemble
0.1.1,"Two subplots, unpack the axes array immediately"
0.1.1,Define some color for the plotting
0.1.1,Generate the dataset
0.1.1,Instanciate a PCA object for the sake of easy visualisation
0.1.1,Fit and transform x to visualise inside a 2D feature space
0.1.1,"Three subplots, unpack the axes array immediately"
0.1.1,Apply the ENN
0.1.1,Apply the RENN
0.1.1,Define some color for the plotting
0.1.1,Generate the dataset
0.1.1,Instanciate a PCA object for the sake of easy visualisation
0.1.1,Fit and transform x to visualise inside a 2D feature space
0.1.1,Apply Nearmiss 3
0.1.1,"Two subplots, unpack the axes array immediately"
0.1.1,Define some color for the plotting
0.1.1,Generate the dataset
0.1.1,Instanciate a PCA object for the sake of easy visualisation
0.1.1,Fit and transform x to visualise inside a 2D feature space
0.1.1,Apply Edited Nearest Neighbours
0.1.1,"Two subplots, unpack the axes array immediately"
0.1.1,Define some color for the plotting
0.1.1,Generate the dataset
0.1.1,Instanciate a PCA object for the sake of easy visualisation
0.1.1,Fit and transform x to visualise inside a 2D feature space
0.1.1,Apply Tomek Links cleaning
0.1.1,"Two subplots, unpack the axes array immediately"
0.1.1,Define some color for the plotting
0.1.1,Generate the dataset
0.1.1,Instanciate a PCA object for the sake of easy visualisation
0.1.1,Fit and transform x to visualise inside a 2D feature space
0.1.1,Apply One-Sided Selection
0.1.1,"Two subplots, unpack the axes array immediately"
0.1.1,Define some color for the plotting
0.1.1,Generate the dataset
0.1.1,Instanciate a PCA object for the sake of easy visualisation
0.1.1,Fit and transform x to visualise inside a 2D feature space
0.1.1,Apply Nearmiss 2
0.1.1,"Two subplots, unpack the axes array immediately"
0.1.1,Define some color for the plotting
0.1.1,Generate the dataset
0.1.1,Instanciate a PCA object for the sake of easy visualisation
0.1.1,Fit and transform x to visualise inside a 2D feature space
0.1.1,Apply neighbourhood cleaning rule
0.1.1,"Two subplots, unpack the axes array immediately"
0.1.1,Define some color for the plotting
0.1.1,Generate the dataset
0.1.1,Instanciate a PCA object for the sake of easy visualisation
0.1.1,Fit and transform x to visualise inside a 2D feature space
0.1.1,Apply Condensed Nearest Neighbours
0.1.1,"Two subplots, unpack the axes array immediately"
0.1.1,Define some color for the plotting
0.1.1,Generate the dataset
0.1.1,Instanciate a PCA object for the sake of easy visualisation
0.1.1,Fit and transform x to visualise inside a 2D feature space
0.1.1,Apply Cluster Centroids
0.1.1,"Two subplots, unpack the axes array immediately"
0.1.1,Define some color for the plotting
0.1.1,Generate the dataset
0.1.1,Instanciate a PCA object for the sake of easy visualisation
0.1.1,Fit and transform x to visualise inside a 2D feature space
0.1.1,Apply the random under-sampling
0.1.1,"Two subplots, unpack the axes array immediately"
0.1.1,Define some color for the plotting
0.1.1,Generate the dataset
0.1.1,"Two subplots, unpack the axes array immediately"
0.1.1,Define some color for the plotting
0.1.1,Generate the dataset
0.1.1,Instanciate a PCA object for the sake of easy visualisation
0.1.1,Fit and transform x to visualise inside a 2D feature space
0.1.1,Apply Nearmiss 1
0.1.1,"Two subplots, unpack the axes array immediately"
0.1.1,Generate the dataset
0.1.1,Instanciate a PCA object for the sake of easy visualisation
0.1.1,Create the samplers
0.1.1,Create teh classifier
0.1.1,Make the splits
0.1.1,Add one transformers and two samplers in the pipeline object
0.1.1,Based on NiLearn package
0.1.1,License: simplified BSD
0.1.1,"PEP0440 compatible formatted version, see:"
0.1.1,https://www.python.org/dev/peps/pep-0440/
0.1.1,
0.1.1,Generic release markers:
0.1.1,X.Y
0.1.1,X.Y.Z # For bugfix releases
0.1.1,
0.1.1,Admissible pre-release markers:
0.1.1,X.YaN # Alpha release
0.1.1,X.YbN # Beta release
0.1.1,X.YrcN # Release Candidate
0.1.1,X.Y # Final release
0.1.1,
0.1.1,Dev branch marker is: 'X.Y.dev' or 'X.Y.devN' where N is an integer.
0.1.1,'X.Y.dev0' is the canonical version of 'X.Y.dev'
0.1.1,
0.1.1,"This is a tuple to preserve order, so that dependencies are checked"
0.1.1,in some meaningful order (more => less 'core').  We avoid using
0.1.1,collections.OrderedDict to preserve Python 2.6 compatibility.
0.1.1,Avoid choking on modules with no __version__ attribute
0.1.1,Skip check only when installing and it's a module that
0.1.1,will be auto-installed.
0.1.1,Check the consistency of X and y
0.1.1,Get all the unique elements in the target array
0.1.1,# Raise an error if there is only one class
0.1.1,if uniques.size == 1:
0.1.1,"raise RuntimeError(""Only one class detected, aborting..."")"
0.1.1,Raise a warning for the moment to be compatible with BaseEstimator
0.1.1,Store the size of X to check at sampling time if we have the
0.1.1,same data
0.1.1,Create a dictionary containing the class statistics
0.1.1,Find the minority and majority classes
0.1.1,Check if the ratio provided at initialisation make sense
0.1.1,Check the consistency of X and y
0.1.1,Check that the data have been fitted
0.1.1,Check if the size of the data is identical than at fitting
0.1.1,The ratio correspond to the number of samples in the minority class
0.1.1,"over the number of samples in the majority class. Thus, the ratio"
0.1.1,cannot be greater than 1.0
0.1.1,Adapted from scikit-learn
0.1.1,Author: Edouard Duchesnay
0.1.1,Gael Varoquaux
0.1.1,Virgile Fritsch
0.1.1,Alexandre Gramfort
0.1.1,Lars Buitinck
0.1.1,chkoar
0.1.1,License: BSD
0.1.1,BaseEstimator interface
0.1.1,shallow copy of steps
0.1.1,Estimator interface
0.1.1,Boolean controlling whether the joblib caches should be
0.1.1,"flushed if the version of certain modules changes (eg nibabel, as it"
0.1.1,does not respect the backward compatibility in some of its internal
0.1.1,structures
0.1.1,This  is used in nilearn._utils.cache_mixin
0.1.1,list all submodules available in imblearn and version
0.1.1,Keep the samples from the majority class
0.1.1,Loop over the other classes over picking at random
0.1.1,"If this is the majority class, skip it"
0.1.1,Define the number of sample to create
0.1.1,Pick some elements at random
0.1.1,Concatenate to the majority class
0.1.1,Keep the samples from the majority class
0.1.1,Define the number of sample to create
0.1.1,We handle only two classes problem for the moment.
0.1.1,Start by separating minority class features and target values.
0.1.1,Print if verbose is true
0.1.1,"Look for k-th nearest neighbours, excluding, of course, the"
0.1.1,point itself.
0.1.1,Get the distance to the NN
0.1.1,Compute the ratio of majority samples next to minority samples
0.1.1,Normalize the ratio
0.1.1,Compute the number of sample to be generated
0.1.1,For each minority samples
0.1.1,Pick-up the neighbors wanted
0.1.1,Create a new sample
0.1.1,Find the NN for each samples
0.1.1,Exclude the sample itself
0.1.1,Count how many NN belong to the minority class
0.1.1,Find the class corresponding to the label in x
0.1.1,Compute the number of majority samples in the NN
0.1.1,Samples are in danger for m/2 <= m' < m
0.1.1,Samples are noise for m = m'
0.1.1,Check the consistency of X
0.1.1,Check the random state
0.1.1,A matrix to store the synthetic samples
0.1.1,# Set seeds
0.1.1,"seeds = random_state.randint(low=0,"
0.1.1,"high=100 * len(nn_num.flatten()),"
0.1.1,size=n_samples)
0.1.1,Randomly pick samples to construct neighbours from
0.1.1,Loop over the NN matrix and create new samples
0.1.1,"NN lines relate to original sample, columns to its"
0.1.1,nearest neighbours
0.1.1,"Take a step of random size (0,1) in the direction of the"
0.1.1,n nearest neighbours
0.1.1,if self.random_state is None:
0.1.1,np.random.seed(seeds[i])
0.1.1,else:
0.1.1,np.random.seed(self.random_state)
0.1.1,Construct synthetic sample
0.1.1,The returned target vector is simply a repetition of the
0.1.1,minority label
0.1.1,Define the number of sample to create
0.1.1,We handle only two classes problem for the moment.
0.1.1,Start by separating minority class features and target values.
0.1.1,If regular SMOTE is to be performed
0.1.1,"Look for k-th nearest neighbours, excluding, of course, the"
0.1.1,point itself.
0.1.1,Matrix with k-th nearest neighbours indexes for each minority
0.1.1,element.
0.1.1,--- Generating synthetic samples
0.1.1,Use static method make_samples to generate minority samples
0.1.1,Concatenate the newly generated samples to the original data set
0.1.1,Find the NNs for all samples in the data set.
0.1.1,Boolean array with True for minority samples in danger
0.1.1,"If all minority samples are safe, return the original data set."
0.1.1,"All are safe, nothing to be done here."
0.1.1,"If we got here is because some samples are in danger, we need to"
0.1.1,find the NNs among the minority class to create the new synthetic
0.1.1,samples.
0.1.1,
0.1.1,We start by changing the number of NNs to consider from m + 1
0.1.1,to k + 1
0.1.1,nns...#
0.1.1,B1 and B2 types diverge here!!!
0.1.1,Create synthetic samples for borderline points.
0.1.1,Concatenate the newly generated samples to the original
0.1.1,dataset
0.1.1,Reset the k-neighbours to m+1 neighbours
0.1.1,Split the number of synthetic samples between only minority
0.1.1,"(type 1), or minority and majority (with reduced step size)"
0.1.1,(type 2).
0.1.1,The fraction is sampled from a beta distribution centered
0.1.1,around 0.5 with variance ~0.01
0.1.1,Only minority
0.1.1,Only majority with smaller step size
0.1.1,Concatenate the newly generated samples to the original
0.1.1,data set
0.1.1,Reset the k-neighbours to m+1 neighbours
0.1.1,The SVM smote model fits a support vector machine
0.1.1,classifier to the data and uses the support vector to
0.1.1,"provide a notion of boundary. Unlike regular smote, where"
0.1.1,such notion relies on proportion of nearest neighbours
0.1.1,belonging to each class.
0.1.1,Fit SVM to the full data#
0.1.1,Find the support vectors and their corresponding indexes
0.1.1,"First, find the nn of all the samples to identify samples"
0.1.1,in danger and noisy ones
0.1.1,"As usual, fit a nearest neighbour model to the data"
0.1.1,"Now, get rid of noisy support vectors"
0.1.1,Remove noisy support vectors
0.1.1,Proceed to find support vectors NNs among the minority class
0.1.1,Split the number of synthetic samples between interpolation and
0.1.1,extrapolation
0.1.1,The fraction are sampled from a beta distribution with mean
0.1.1,0.5 and variance 0.01#
0.1.1,Interpolate samples in danger
0.1.1,Extrapolate safe samples
0.1.1,Concatenate the newly generated samples to the original data set
0.1.1,not any support vectors in danger
0.1.1,All the support vector in danger
0.1.1,Reset the k-neighbours to m+1 neighbours
0.1.1,--- NN object
0.1.1,Import the NN object from scikit-learn library. Since in the smote
0.1.1,"variations we must first find samples that are in danger, we"
0.1.1,initialize the NN object differently depending on the method chosen
0.1.1,"Regular smote does not look for samples in danger, instead it"
0.1.1,creates synthetic samples directly from the k-th nearest
0.1.1,neighbours with not filtering
0.1.1,"Borderline1, 2 and SVM variations of smote must first look for"
0.1.1,samples that could be considered noise and samples that live
0.1.1,"near the boundary between the classes. Therefore, before"
0.1.1,"creating synthetic samples from the k-th nns, it first look"
0.1.1,for m nearest neighbors to decide whether or not a sample is
0.1.1,noise or near the boundary.
0.1.1,--- SVM smote
0.1.1,"Unlike the borderline variations, the SVM variation uses the support"
0.1.1,vectors to decide which samples are in danger (near the boundary).
0.1.1,Additionally it also introduces extrapolation for samples that are
0.1.1,considered safe (far from boundary) and interpolation for samples
0.1.1,in danger (near the boundary). The level of extrapolation is
0.1.1,controled by the out_step.
0.1.1,Store SVM object with any parameters
0.1.1,Generate a global dataset to use
0.1.1,Define a negative ratio
0.1.1,Define a ratio greater than 1
0.1.1,Define ratio as an unknown string
0.1.1,Define ratio as a list which is not supported
0.1.1,Define a ratio
0.1.1,Create the object
0.1.1,Resample the data
0.1.1,Create a wrong y
0.1.1,Create the object
0.1.1,Fit the data
0.1.1,Create the object
0.1.1,Fit the data
0.1.1,Check if the data information have been computed
0.1.1,Create the object
0.1.1,Resample the data
0.1.1,Resample the data
0.1.1,Create the object
0.1.1,Generate a global dataset to use
0.1.1,Define a negative ratio
0.1.1,Define a ratio greater than 1
0.1.1,Define ratio as an unknown string
0.1.1,Define ratio as a list which is not supported
0.1.1,Define a ratio
0.1.1,Create the object
0.1.1,Resample the data
0.1.1,Create a wrong y
0.1.1,Create the object
0.1.1,Fit the data
0.1.1,Create the object
0.1.1,Fit the data
0.1.1,Check if the data information have been computed
0.1.1,Create the object
0.1.1,Resample the data
0.1.1,Resample the data
0.1.1,Create the object
0.1.1,Generate a global dataset to use
0.1.1,Define a negative ratio
0.1.1,Define a ratio greater than 1
0.1.1,Define ratio as an unknown string
0.1.1,Define ratio as a list which is not supported
0.1.1,Create the object
0.1.1,Resample the data
0.1.1,Create a wrong y
0.1.1,Create the object
0.1.1,Fit the data
0.1.1,Check if the data information have been computed
0.1.1,Create the object
0.1.1,Create the object
0.1.1,Fit the data
0.1.1,Create the object
0.1.1,Fit the data
0.1.1,Create the object
0.1.1,Fit the data
0.1.1,Create the object
0.1.1,Fit the data
0.1.1,Create the object
0.1.1,Fit the data
0.1.1,Create the object
0.1.1,Start with the minority class
0.1.1,All the minority class samples will be preserved
0.1.1,If we need to offer support for the indices
0.1.1,Loop over the other classes under picking at random
0.1.1,"If the minority class is up, skip it"
0.1.1,Randomly get one sample from the majority class
0.1.1,Generate the index to select
0.1.1,Create the set C
0.1.1,Create the set S
0.1.1,Remove the seed from S since that it will be added anyway
0.1.1,Create a k-NN classifier
0.1.1,Fit C into the knn
0.1.1,Classify on S
0.1.1,Find the misclassified S_y
0.1.1,If we need to offer support for the indices selected
0.1.1,We concatenate the misclassified samples with the seed and the
0.1.1,minority samples
0.1.1,Find the nearest neighbour of every point
0.1.1,Send the information to is_tomek function to get boolean vector back
0.1.1,Check if the indices of the samples selected should be returned too
0.1.1,Return the indices of interest
0.1.1,Return data set without majority Tomek links.
0.1.1,Compute the distance considering the farthest neighbour
0.1.1,Sort the list of distance and get the index
0.1.1,Select the desired number of samples
0.1.1,Assign the parameter of the element of this class
0.1.1,Check that the version asked is implemented
0.1.1,Start with the minority class
0.1.1,All the minority class samples will be preserved
0.1.1,Compute the number of cluster needed
0.1.1,If we need to offer support for the indices
0.1.1,"For each element of the current class, find the set of NN"
0.1.1,of the minority class
0.1.1,Call the constructor of the NN
0.1.1,Fit the minority class since that we want to know the distance
0.1.1,to these point
0.1.1,Loop over the other classes under picking at random
0.1.1,"If the minority class is up, skip it"
0.1.1,Get the samples corresponding to the current class
0.1.1,Find the NN
0.1.1,Select the right samples
0.1.1,Find the NN
0.1.1,Select the right samples
0.1.1,We need a new NN object to fit the current class
0.1.1,Find the set of NN to the minority class
0.1.1,Create the subset containing the samples found during the NN
0.1.1,search. Linearize the indexes and remove the double values
0.1.1,Create the subset
0.1.1,Compute the NN considering the current class
0.1.1,If we need to offer support for the indices selected
0.1.1,Check if the indices of the samples selected should be returned too
0.1.1,Return the indices of interest
0.1.1,Compute the number of clusters needed
0.1.1,All the minority class samples will be preserved
0.1.1,If we need to offer support for the indices
0.1.1,Loop over the other classes under-picking at random
0.1.1,"If the minority class is up, skip it"
0.1.1,Pick some elements at random
0.1.1,If we need to offer support for the indices selected
0.1.1,Concatenate to the minority class
0.1.1,Check if the indices of the samples selected should be returned as
0.1.1,well
0.1.1,Return the indices of interest
0.1.1,"Initialize the boolean result as false, and also a counter"
0.1.1,Loop through each sample and looks whether it belongs to the minority
0.1.1,"class. If it does, we don't consider it since we want to keep all"
0.1.1,"minority samples. If, however, it belongs to the majority sample we"
0.1.1,look at its first neighbour. If its closest neighbour also has the
0.1.1,"current sample as its closest neighbour, the two form a Tomek link."
0.1.1,"If they form a tomek link, put a True marker on this"
0.1.1,"sample, and increase counter by one."
0.1.1,Find the nearest neighbour of every point
0.1.1,Send the information to is_tomek function to get boolean vector back
0.1.1,Check if the indices of the samples selected should be returned too
0.1.1,Return the indices of interest
0.1.1,Return data set without majority Tomek links.
0.1.1,Start with the minority class
0.1.1,All the minority class samples will be preserved
0.1.1,If we need to offer support for the indices
0.1.1,Loop over the other classes under picking at random
0.1.1,"If the minority class is up, skip it"
0.1.1,Randomly get one sample from the majority class
0.1.1,Generate the index to select
0.1.1,Create the set C - One majority samples and all minority
0.1.1,Create the set S - all majority samples
0.1.1,Create a k-NN classifier
0.1.1,Fit C into the knn
0.1.1,Check each sample in S if we keep it or drop it
0.1.1,Do not select sample which are already well classified
0.1.1,Classify on S
0.1.1,If the prediction do not agree with the true label
0.1.1,append it in C_x
0.1.1,Keep the index for later
0.1.1,Update C
0.1.1,Fit C into the knn
0.1.1,This experimental to speed up the search
0.1.1,Classify all the element in S and avoid to test the
0.1.1,well classified elements
0.1.1,Find the misclassified S_y
0.1.1,If we need to offer support for the indices selected
0.1.1,Check if the indices of the samples selected should be returned too
0.1.1,Return the indices of interest
0.1.1,Compute the number of cluster needed
0.1.1,Create the clustering object
0.1.1,Start with the minority class
0.1.1,All the minority class samples will be preserved
0.1.1,Loop over the other classes under picking at random
0.1.1,"If the minority class is up, skip it."
0.1.1,Find the centroids via k-means
0.1.1,Concatenate to the minority class
0.1.1,Start with the minority class
0.1.1,All the minority class samples will be preserved
0.1.1,If we need to offer support for the indices
0.1.1,Create a k-NN to fit the whole data
0.1.1,Fit the whole dataset
0.1.1,Loop over the other classes under picking at random
0.1.1,Get the sample of the current class
0.1.1,Get the samples associated
0.1.1,Find the NN for the current class
0.1.1,Get the label of the corresponding to the index
0.1.1,Check which one are the same label than the current class
0.1.1,Make an AND operation through the three neighbours
0.1.1,If the minority class remove the majority samples
0.1.1,Get the index to exclude
0.1.1,Get the index to exclude
0.1.1,Create a vector with the sample to select
0.1.1,Exclude as well the minority sample since that they will be
0.1.1,concatenated later
0.1.1,Get the samples from the majority classes
0.1.1,If we need to offer support for the indices selected
0.1.1,Check if the indices of the samples selected should be returned too
0.1.1,Return the indices of interest
0.1.1,Start with the minority class
0.1.1,All the minority class samples will be preserved
0.1.1,If we need to offer support for the indices
0.1.1,Create a k-NN to fit the whole data
0.1.1,Fit the data
0.1.1,Loop over the other classes under picking at random
0.1.1,"If the minority class is up, skip it"
0.1.1,Get the sample of the current class
0.1.1,Find the NN for the current class
0.1.1,Get the label of the corresponding to the index
0.1.1,Check which one are the same label than the current class
0.1.1,Make the majority vote
0.1.1,Get the samples which agree all together
0.1.1,If we need to offer support for the indices selected
0.1.1,Check if the indices of the samples selected should be returned too
0.1.1,Return the indices of interest
0.1.1,Check if the indices of the samples selected should be returned too
0.1.1,Return the indices of interest
0.1.1,Select the appropriate classifier
0.1.1,Create the different folds
0.1.1,Compute the number of cluster needed
0.1.1,Find the percentile corresponding to the top num_samples
0.1.1,Sample the data
0.1.1,If we need to offer support for the indices
0.1.1,Generate a global dataset to use
0.1.1,Define a negative ratio
0.1.1,Define a ratio greater than 1
0.1.1,Define ratio as an unknown string
0.1.1,Define ratio as a list which is not supported
0.1.1,Define a ratio
0.1.1,Define the parameter for the under-sampling
0.1.1,Create the object
0.1.1,Resample the data
0.1.1,Create a wrong y
0.1.1,Create the object
0.1.1,Fit the data
0.1.1,Define the parameter for the under-sampling
0.1.1,Create the object
0.1.1,Fit the data
0.1.1,Check if the data information have been computed
0.1.1,Define the parameter for the under-sampling
0.1.1,Create the object
0.1.1,Define the parameter for the under-sampling
0.1.1,Create the object
0.1.1,Fit and sample
0.1.1,Define the parameter for the under-sampling
0.1.1,Create the object
0.1.1,Fit and sample
0.1.1,Define the parameter for the under-sampling
0.1.1,Create the object
0.1.1,Fit and sample
0.1.1,Create the object
0.1.1,Generate a global dataset to use
0.1.1,Define a ratio
0.1.1,Create the object
0.1.1,Resample the data
0.1.1,Create a wrong y
0.1.1,Create the object
0.1.1,Fit the data
0.1.1,Check if the data information have been computed
0.1.1,Create the object
0.1.1,Resample the data
0.1.1,Resample the data
0.1.1,Create the object
0.1.1,Generate a global dataset to use
0.1.1,Define a ratio
0.1.1,Create the object
0.1.1,Resample the data
0.1.1,Create a wrong y
0.1.1,Create the object
0.1.1,Fit the data
0.1.1,Check if the data information have been computed
0.1.1,Create the object
0.1.1,Resample the data
0.1.1,Resample the data
0.1.1,Create the object
0.1.1,Generate a global dataset to use
0.1.1,Define a ratio
0.1.1,Create the object
0.1.1,Resample the data
0.1.1,Create a wrong y
0.1.1,Create the object
0.1.1,Fit the data
0.1.1,Check if the data information have been computed
0.1.1,Create the object
0.1.1,Resample the data
0.1.1,Resample the data
0.1.1,Resample the data
0.1.1,Create the object
0.1.1,Generate a global dataset to use
0.1.1,Define a negative ratio
0.1.1,Define a ratio greater than 1
0.1.1,Define ratio as an unknown string
0.1.1,Define ratio as a list which is not supported
0.1.1,Define a ratio
0.1.1,Create the object
0.1.1,Resample the data
0.1.1,Create a wrong y
0.1.1,Create the object
0.1.1,Fit the data
0.1.1,Create the object
0.1.1,Fit the data
0.1.1,Check if the data information have been computed
0.1.1,Create the object
0.1.1,Resample the data
0.1.1,Resample the data
0.1.1,Resample the data
0.1.1,Create the object
0.1.1,Generate a global dataset to use
0.1.1,Define a negative ratio
0.1.1,Define a ratio greater than 1
0.1.1,Define ratio as an unknown string
0.1.1,Define ratio as a list which is not supported
0.1.1,Resample the data
0.1.1,Define a ratio
0.1.1,Create the object
0.1.1,Resample the data
0.1.1,Create a wrong y
0.1.1,Create the object
0.1.1,Fit the data
0.1.1,Create the object
0.1.1,Fit the data
0.1.1,Check if the data information have been computed
0.1.1,Create the object
0.1.1,Resample the data
0.1.1,Resample the data
0.1.1,Resample the data
0.1.1,Resample the data
0.1.1,Resample the data
0.1.1,Resample the data
0.1.1,Resample the data
0.1.1,Resample the data
0.1.1,Resample the data
0.1.1,Create the object
0.1.1,Generate a global dataset to use
0.1.1,Define a ratio
0.1.1,Create the object
0.1.1,Create the object
0.1.1,Resample the data
0.1.1,Create a wrong y
0.1.1,Create the object
0.1.1,Fit the data
0.1.1,Check if the data information have been computed
0.1.1,Create the object
0.1.1,Resample the data
0.1.1,Resample the data
0.1.1,Resample the data
0.1.1,Create the object
0.1.1,Generate a global dataset to use
0.1.1,Define a negative ratio
0.1.1,Define a ratio greater than 1
0.1.1,Define ratio as an unknown string
0.1.1,Define ratio as a list which is not supported
0.1.1,Define a ratio
0.1.1,Define the parameter for the under-sampling
0.1.1,Create the object
0.1.1,Resample the data
0.1.1,Create a wrong y
0.1.1,Create the object
0.1.1,Fit the data
0.1.1,Define the parameter for the under-sampling
0.1.1,Create the object
0.1.1,Fit the data
0.1.1,Check if the data information have been computed
0.1.1,Define the parameter for the under-sampling
0.1.1,Create the object
0.1.1,Define the parameter for the under-sampling
0.1.1,Create the object
0.1.1,Fit and sample
0.1.1,Define the parameter for the under-sampling
0.1.1,Create the object
0.1.1,Fit and sample
0.1.1,Define the parameter for the under-sampling
0.1.1,Create the object
0.1.1,Fit and sample
0.1.1,Create the object
0.1.1,Generate a global dataset to use
0.1.1,Define a ratio
0.1.1,Create the object
0.1.1,Resample the data
0.1.1,Create a wrong y
0.1.1,Create the object
0.1.1,Fit the data
0.1.1,Check if the data information have been computed
0.1.1,Create the object
0.1.1,Resample the data
0.1.1,Resample the data
0.1.1,Create the object
0.1.1,Generate a global dataset to use
0.1.1,Define a negative ratio
0.1.1,Define a ratio greater than 1
0.1.1,Define ratio as an unknown string
0.1.1,Define ratio as a list which is not supported
0.1.1,Define a ratio
0.1.1,Define the parameter for the under-sampling
0.1.1,Create the object
0.1.1,Resample the data
0.1.1,Create a wrong y
0.1.1,Create the object
0.1.1,Fit the data
0.1.1,Define the parameter for the under-sampling
0.1.1,Create the object
0.1.1,Fit the data
0.1.1,Check if the data information have been computed
0.1.1,Define the parameter for the under-sampling
0.1.1,Create the object
0.1.1,Define the parameter for the under-sampling
0.1.1,Create the object
0.1.1,Define the parameter for the under-sampling
0.1.1,Create the object
0.1.1,Fit and sample
0.1.1,Define the parameter for the under-sampling
0.1.1,Create the object
0.1.1,Fit and sample
0.1.1,Generate a global dataset to use
0.1.1,Define a negative ratio
0.1.1,Define a ratio greater than 1
0.1.1,Define ratio as an unknown string
0.1.1,Define ratio as a list which is not supported
0.1.1,Define a ratio
0.1.1,Define the parameter for the under-sampling
0.1.1,Create the object
0.1.1,Resample the data
0.1.1,Create a wrong y
0.1.1,Create the object
0.1.1,Fit the data
0.1.1,Define the parameter for the under-sampling
0.1.1,Create the object
0.1.1,Fit the data
0.1.1,Check if the data information have been computed
0.1.1,Define the parameter for the under-sampling
0.1.1,Create the object
0.1.1,Define the parameter for the under-sampling
0.1.1,Create the object
0.1.1,Fit and sample
0.1.1,Define the parameter for the under-sampling
0.1.1,Create the object
0.1.1,Fit and sample
0.1.1,Define the parameter for the under-sampling
0.1.1,Create the object
0.1.1,Fit and sample
0.1.1,Create the object
0.1.1,Generate a global dataset to use
0.1.1,Define a ratio
0.1.1,Create the object
0.1.1,Resample the data
0.1.1,Create a wrong y
0.1.1,Create the object
0.1.1,Fit the data
0.1.1,Check if the data information have been computed
0.1.1,Create the object
0.1.1,Resample the data
0.1.1,Resample the data
0.1.1,Create the object
0.1.1,Test the various init parameters of the pipeline.
0.1.1,Check that we can't instantiate pipelines with objects without fit
0.1.1,method
0.1.1,Smoke test with only an estimator
0.1.1,Check that params are set
0.1.1,Smoke test the repr:
0.1.1,Test with two objects
0.1.1,Check that we can't use the same stage name twice
0.1.1,Check that params are set
0.1.1,Smoke test the repr:
0.1.1,Check that params are not set when naming them wrong
0.1.1,Test clone
0.1.1,"Check that apart from estimators, the parameters are the same"
0.1.1,Remove estimators that where copied
0.1.1,Test the various methods of the pipeline (anova).
0.1.1,Test with Anova + LogisticRegression
0.1.1,Test that the pipeline can take fit parameters
0.1.1,classifier should return True
0.1.1,and transformer params should not be changed
0.1.1,Test pipeline raises set params error message for nested models.
0.1.1,expected error message
0.1.1,nested model check
0.1.1,Test the various methods of the pipeline (pca + svm).
0.1.1,Test with PCA + SVC
0.1.1,Test the various methods of the pipeline (preprocessing + svm).
0.1.1,check shapes of various prediction functions
0.1.1,test that the fit_predict method is implemented on a pipeline
0.1.1,test that the fit_predict on pipeline yields same results as applying
0.1.1,transform and clustering steps separately
0.1.1,first compute the transform and clustering step separately
0.1.1,use a pipeline to do the transform and clustering in one step
0.1.1,tests that a pipeline does not have fit_predict method when final
0.1.1,step of pipeline does not have fit_predict defined
0.1.1,Test whether pipeline works with a transformer at the end.
0.1.1,Also test pipeline.transform and pipeline.inverse_transform
0.1.1,test transform and fit_transform:
0.1.1,Test whether pipeline works with a transformer missing fit_transform
0.1.1,test fit_transform:
0.1.1,Test the various methods of the pipeline (pca + svm).
0.1.1,Test with PCA + SVC
0.1.1,Test the various methods of the pipeline (pca + svm).
0.1.1,Test with PCA + SVC
0.1.1,Test whether pipeline works with a sampler at the end.
0.1.1,Also test pipeline.sampler
0.1.1,test transform and fit_transform:
0.1.1,Test whether pipeline works with a sampler at the end.
0.1.1,Also test pipeline.sampler
0.1.1,Test the various methods of the pipeline (anova).
0.1.1,Test with RandomUnderSampling + Anova + LogisticRegression
0.1.1,Fit using SMOTE
0.1.1,Transform using SMOTE
0.1.1,Fit and transform using ENN
0.1.1,Fit using SMOTE
0.1.1,Transform using SMOTE
0.1.1,Fit and transform using ENN
0.1.1,Generate a global dataset to use
0.1.1,Define a negative ratio
0.1.1,Define a ratio greater than 1
0.1.1,Define ratio as an unknown string
0.1.1,Define ratio as a list which is not supported
0.1.1,Create the object
0.1.1,Resample the data
0.1.1,Create a wrong y
0.1.1,Create the object
0.1.1,Fit the data
0.1.1,Check if the data information have been computed
0.1.1,Create the object
0.1.1,Create the object
0.1.1,Fit the data
0.1.1,Create the object
0.1.1,Fit the data
0.1.1,Create the object
0.1.1,Generate a global dataset to use
0.1.1,Define a negative ratio
0.1.1,Define a ratio greater than 1
0.1.1,Define ratio as an unknown string
0.1.1,Define ratio as a list which is not supported
0.1.1,Create the object
0.1.1,Resample the data
0.1.1,Create a wrong y
0.1.1,Create the object
0.1.1,Fit the data
0.1.1,Check if the data information have been computed
0.1.1,Create the object
0.1.1,Create the object
0.1.1,Fit the data
0.1.1,Create the object
0.1.1,Fit the data
0.1.1,Create the object
0.1.1,Create the object for random under-sampling
0.1.1,Define the classifier to use
0.1.1,Start with the minority class
0.1.1,Keep the indices of the minority class somewhere if we need to
0.1.1,return them later
0.1.1,Condition to initiliase before the search
0.1.1,Get the initial number of samples to select in the majority class
0.1.1,Create the array characterising the array containing the majority
0.1.1,class
0.1.1,Loop to create the different subsets
0.1.1,Generate an appropriate number of index to extract
0.1.1,from the majority class depending of the false classification
0.1.1,rate of the previous iteration
0.1.1,Mark these indexes as not being considered for next sampling
0.1.1,"For now, we will train and classify on the same data"
0.1.1,"Let see if we should find another solution. Anyway,"
0.1.1,random stuff are still random stuff
0.1.1,Push these data into a new subset
0.1.1,Apply a bootstrap on x_data
0.1.1,Train the classifier using the current data
0.1.1,Train the classifier using the current data
0.1.1,Predict using only the majority class
0.1.1,Basically let's find which sample have to be retained for the
0.1.1,next round
0.1.1,Find the misclassified index to keep them for the next round
0.1.1,Count how many random element will be selected
0.1.1,"We found a new subset, increase the counter"
0.1.1,Check if we have to make an early stopping
0.1.1,Select the remaining data
0.1.1,Select the final batch
0.1.1,Push these data into a new subset
0.1.1,"We found a new subset, increase the counter"
0.1.1,Also check that we will have enough sample to extract at the
0.1.1,next round
0.1.1,Select the remaining data
0.1.1,Select the final batch
0.1.1,Push these data into a new subset
0.1.1,"We found a new subset, increase the counter"
0.1.1,Generate a global dataset to use
0.1.1,Define a negative ratio
0.1.1,Define a ratio greater than 1
0.1.1,Define ratio as an unknown string
0.1.1,Define ratio as a list which is not supported
0.1.1,Define a ratio
0.1.1,Define the parameter for the under-sampling
0.1.1,Create the object
0.1.1,Resample the data
0.1.1,Create a wrong y
0.1.1,Create the object
0.1.1,Fit the data
0.1.1,Define the parameter for the under-sampling
0.1.1,Create the object
0.1.1,Fit the data
0.1.1,Check if the data information have been computed
0.1.1,Define the parameter for the under-sampling
0.1.1,Create the object
0.1.1,Define the ratio parameter
0.1.1,Create the sampling object
0.1.1,Get the different subset
0.1.1,Check each array
0.1.1,Define the ratio parameter
0.1.1,Create the sampling object
0.1.1,Get the different subset
0.1.1,Check each array
0.1.1,Define the ratio parameter
0.1.1,Create the sampling object
0.1.1,Get the different subset
0.1.1,Check each array
0.1.1,Define the ratio parameter
0.1.1,Create the sampling object
0.1.1,Get the different subset
0.1.1,Check each array
0.1.1,Define the ratio parameter
0.1.1,Create the sampling object
0.1.1,Get the different subset
0.1.1,Check each array
0.1.1,Define the ratio parameter
0.1.1,Create the sampling object
0.1.1,Get the different subset
0.1.1,Check each array
0.1.1,Define the ratio parameter
0.1.1,Create the sampling object
0.1.1,Get the different subset
0.1.1,Check each array
0.1.1,Define the ratio parameter
0.1.1,Define the ratio parameter
0.1.1,Create the sampling object
0.1.1,Get the different subset
0.1.1,Check each array
0.1.1,Create the object
0.1.1,Generate a global dataset to use
0.1.1,Define a negative ratio
0.1.1,Define a ratio greater than 1
0.1.1,Define ratio as an unknown string
0.1.1,Define ratio as a list which is not supported
0.1.1,Define a ratio
0.1.1,Define the parameter for the under-sampling
0.1.1,Create the object
0.1.1,Resample the data
0.1.1,Create a wrong y
0.1.1,Create the object
0.1.1,Fit the data
0.1.1,Define the parameter for the under-sampling
0.1.1,Create the object
0.1.1,Fit the data
0.1.1,Check if the data information have been computed
0.1.1,Define the parameter for the under-sampling
0.1.1,Create the object
0.1.1,Define the ratio parameter
0.1.1,Create the sampling object
0.1.1,Get the different subset
0.1.1,Define the ratio parameter
0.1.1,Create the sampling object
0.1.1,Get the different subset
0.1.1,Create the object
