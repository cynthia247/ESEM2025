{
  "v0.12": [
    "-*- coding: utf-8 -*-",
    "",
    "Configuration file for the Sphinx documentation builder.",
    "",
    "This file does only contain a selection of the most common options. For a",
    "full list see the documentation:",
    "http://www.sphinx-doc.org/en/stable/config",
    "-- Path setup --------------------------------------------------------------",
    "If extensions (or modules to document with autodoc) are in another directory,",
    "add these directories to sys.path here. If the directory is relative to the",
    "documentation root, use os.path.abspath to make it absolute, like shown here.",
    "",
    "-- Project information -----------------------------------------------------",
    "Version Information (for version-switcher)",
    "-- General configuration ---------------------------------------------------",
    "If your documentation needs a minimal Sphinx version, state it here.",
    "",
    "needs_sphinx = '1.0'",
    "Add any Sphinx extension module names here, as strings. They can be",
    "extensions coming with Sphinx (named 'sphinx.ext.*') or your custom",
    "ones.",
    "sphinx-panels shouldn't add bootstrap css since the pydata-sphinx-theme",
    "already loads it",
    "Add any paths that contain templates here, relative to this directory.",
    "The suffix(es) of source filenames.",
    "You can specify multiple suffix as a list of string:",
    "",
    "source_suffix = ['.rst', '.md']",
    "The master toctree document.",
    "The language for content autogenerated by Sphinx. Refer to documentation",
    "for a list of supported languages.",
    "",
    "This is also used if you do content translation via gettext catalogs.",
    "Usually you set \"language\" from the command line for these cases.",
    "List of patterns, relative to source directory, that match files and",
    "directories to ignore when looking for source files.",
    "This pattern also affects html_static_path and html_extra_path .",
    "The name of the Pygments (syntax highlighting) style to use.",
    "-- Options for HTML output -------------------------------------------------",
    "The theme to use for HTML and HTML Help pages.  See the documentation for",
    "a list of builtin themes.",
    "",
    "Theme options are theme-specific and customize the look and feel of a theme",
    "further.  For a list of options available for each theme, see the",
    "documentation.",
    "",
    "Add any paths that contain custom static files (such as style sheets) here,",
    "relative to this directory. They are copied after the builtin static files,",
    "so a file named \"default.css\" will overwrite the builtin \"default.css\".",
    "Custom sidebar templates, must be a dictionary that maps document names",
    "to template names.",
    "",
    "The default sidebars (for documents that don't match any pattern) are",
    "defined by theme itself.  Builtin themes are using these templates by",
    "default: ``['localtoc.html', 'relations.html', 'sourcelink.html',",
    "'searchbox.html']``.",
    "",
    "html_sidebars = {}",
    "-- Options for HTMLHelp output ---------------------------------------------",
    "Output file base name for HTML help builder.",
    "-- Options for LaTeX output ------------------------------------------------",
    "The paper size ('letterpaper' or 'a4paper').",
    "",
    "'papersize': 'letterpaper',",
    "The font size ('10pt', '11pt' or '12pt').",
    "",
    "'pointsize': '10pt',",
    "Additional stuff for the LaTeX preamble.",
    "",
    "'preamble': '',",
    "Latex figure (float) alignment",
    "",
    "'figure_align': 'htbp',",
    "Grouping the document tree into LaTeX files. List of tuples",
    "(source start file, target name, title,",
    "author, documentclass [howto, manual, or own class]).",
    "-- Options for manual page output ------------------------------------------",
    "One entry per manual page. List of tuples",
    "(source start file, name, description, authors, manual section).",
    "-- Options for Texinfo output ----------------------------------------------",
    "Grouping the document tree into Texinfo files. List of tuples",
    "(source start file, target name, title, author,",
    "dir menu entry, description, category)",
    "-- Options for Epub output -------------------------------------------------",
    "Bibliographic Dublin Core info.",
    "The unique identifier of the text. This can be a ISBN number",
    "or the project homepage.",
    "",
    "epub_identifier = ''",
    "A unique identification for the text.",
    "",
    "epub_uid = ''",
    "A list of files that should not be packed into the epub file.",
    "-- Extension configuration -------------------------------------------------",
    "-- Options for todo extension ----------------------------------------------",
    "If true, `todo` and `todoList` produce output, else they produce nothing.",
    "init docstrings should also be included in class",
    "Only uncomment for faster testing/building docs without compiling notebooks",
    "nbsphinx_execute = \"never\"",
    "Patch all of the published versions",
    "check old RST version (<= v0.8)",
    "Remove old version links",
    "Append updated version links",
    "requires stdin input for identify in weighting sampler",
    "will be removed",
    "applied notebook, not necessary to test each time",
    "needs xgboost too",
    "Slow Notebooks",
    "TODO: should probably move more notebooks here to ignore, because",
    "most get tested by the documentation generation.",
    "Adding the dowhy root folder to the python path so that jupyter notebooks",
    "can import dowhy",
    "\"--ExecutePreprocessor.timeout=600\",",
    "self.df = pd.read_csv(os.path.join(DATA_PATH,'dgp_1/acic_1_1_data.csv'))",
    "self.ate = np.mean(self.df['y1'] - self.df['y0'])",
    "treated = self.df[self.df['z']==1]",
    "self.att = np.mean(treated['y1'] - treated['y0'])",
    "def test_average_treatment_effect(self):",
    "est_ate = 1",
    "bias = est_ate - self.ate",
    "print(bias)",
    "self.assertAlmostEqual(self.ate, est_ate)",
    "def test_average_treatment_effect_on_treated(self):",
    "est_att = 1",
    "self.att=1",
    "bias = est_att - self.att",
    "print(bias)",
    "self.assertAlmostEqual(self.att, est_att)",
    "removing two common causes",
    "removing two common causes",
    "removing two common causes",
    "removing two common causes",
    "removing two common causes",
    "Remove graph variable with name \"W0\" from observed data.",
    "Ensure that a log record exists that provides a more detailed view",
    "of observed and unobserved graph variables (counts and variable names.)",
    "Default == operator tests if same object. If same object, don't need to check type.",
    "num_frontdoor_variables=1,",
    "creating nx graph instance",
    "to be used later for a test. Does not include the replace operation",
    "check if all partial R^2 values are between 0 and 1",
    "We calculate adjusted estimates for two sets of partial R^2 values.",
    "Test if hypothetical confounding by unobserved confounder u1 leads to an adjusted effect that is farther from the original estimate as compared to u2",
    "Creating a model with no unobserved confounders",
    "check if all partial R^2 values are between 0 and 1",
    "for a dataset with no confounders, the robustness value should be higher than a given threshold (0.95 in our case)",
    "Non Parametric estimator",
    "We calculate adjusted estimates for two sets of partial R^2 values.",
    "adjusted lower ate bound for confounder u1 where r2tu_w = 0.7 and r2yu_tw = 0.9",
    "adjusted lower ate bound for confounder u2 where r2tu_w = 0.2 and r2yu_tw = 0.3",
    "adjusted upper ate bound for confounder u1 where r2tu_w = 0.7 and r2yu_tw = 0.9",
    "adjusted upper ate bound for confounder u2 where r2tu_w = 0.2 and r2yu_tw = 0.3",
    "adjusted lower confidence bound for confounder u1 where r2tu_w = 0.7 and r2yu_tw = 0.9",
    "adjusted lower confidence bound for confounder u2 where r2tu_w = 0.2 and r2yu_tw = 0.3",
    "adjusted upper confidence bound for confounder u1 where r2tu_w = 0.7 and r2yu_tw = 0.9",
    "adjusted upper confidence bound for confounder u2 where r2tu_w = 0.2 and r2yu_tw = 0.3",
    "Test if hypothetical confounding by unobserved confounder u1 leads to an adjusted effect that is farther from the original estimate as compared to u2",
    "we patched figure plotting call to avoid drawing plots during tests",
    "We calculate adjusted estimates for two sets of partial R^2 values.",
    "adjusted lower ate bound for confounder u1 where r2tu_w = 0.7 and r2yu_tw = 0.9",
    "adjusted lower ate bound for confounder u2 where r2tu_w = 0.2 and r2yu_tw = 0.3",
    "adjusted upper ate bound for confounder u1 where r2tu_w = 0.7 and r2yu_tw = 0.9",
    "adjusted upper ate bound for confounder u2 where r2tu_w = 0.2 and r2yu_tw = 0.3",
    "adjusted lower confidence bound for confounder u1 where r2tu_w = 0.7 and r2yu_tw = 0.9",
    "adjusted lower confidence bound for confounder u2 where r2tu_w = 0.2 and r2yu_tw = 0.3",
    "adjusted upper confidence bound for confounder u1 where r2tu_w = 0.7 and r2yu_tw = 0.9",
    "adjusted upper confidence bound for confounder u2 where r2tu_w = 0.2 and r2yu_tw = 0.3",
    "Test if hypothetical confounding by unobserved confounder u1 leads to an adjusted effect that is farther from the original estimate as compared to u2",
    "we patched figure plotting call to avoid drawing plots during tests",
    "comparing test examples from R E-Value package",
    "check implementation of Observed Covariate E-value against R package",
    "The outcome is a linear function of the confounder",
    "The slope is 1,2 and the intercept is 3",
    "As we run with only one common cause and one instrument variable we run with (?, 2)",
    "TODO: Check directly for correct behavior, rather than checking the rules",
    "themselves, which can be non-deterministic (all the following are equivalent)",
    "Supports user-provided dataset object",
    "To test if there are any exceptions",
    "To test if the estimate is identical if refutation parameters are zero",
    "This value is hardcoded to be zero as we are runnning this on a linear dataset.",
    "Ordinarily, we should expect this value to be zero.",
    "This value is hardcoded to be zero as we are runnning this on a linear dataset.",
    "Ordinarily, we should expect this value to be zero.",
    "Construct the graph (the graph is constant for all iterations)",
    "Generate the GML graph",
    "Define the true effect",
    "Define experiment params",
    "Record the results",
    "Run the experiment",
    "Generate the data",
    "Encode as a pandas df",
    "Instantiate the CausalModel",
    "Get the estimand",
    "Get estimate (DML)",
    "Get estimate (Linear Regression)",
    "Instantiate the SCM",
    "Generate observational data",
    "Encode as a pandas df",
    "Create the graph describing the causal structure",
    "With graph",
    "model.view_model()",
    "Only P(Y|T) should be present for test to succeed.",
    "Since undirected graph, identify effect must throw an error.",
    "Compare with ground truth",
    "Compare with ground truth",
    "Compare with ground truth",
    "Compare with ground truth",
    "Causal model initialization",
    "Obtain backdoor sets",
    "Check if backdoor sets are valid i.e. if they block all paths between the treatment and the outcome",
    "Causal model initialization",
    "Obtain backdoor sets",
    "Causal model initialization",
    "Obtain backdoor sets",
    "Check if backdoor sets are valid i.e. if they block all paths between the treatment and the outcome",
    "Causal model initialization",
    "Obtain backdoor sets",
    "Check if backdoor sets are valid i.e. if they block all paths between the treatment and the outcome",
    "Causal model initialization",
    "Obtain backdoor sets",
    "Check if backdoor sets are valid i.e. if they block all paths between the treatment and the outcome",
    "Causal model initialization",
    "Obtain backdoor sets",
    "Check if backdoor sets are valid i.e. if they block all paths between the treatment and the outcome",
    "Example is selected from Pearl J. \"Causality\" 2nd Edition, from chapter 3.3.1 on backoor criterion.",
    "The following simpsons paradox examples are taken from Pearl, J {2013}. \"Understanding Simpson\u2019s Paradox\" - http://ftp.cs.ucla.edu/pub/stat_ser/r414.pdf",
    "The following are examples given in the \"Book of Why\" by Judea Pearl, chapter \"The Do-operator and the Back-Door Criterion\"",
    "Test added for issue #1250",
    "Building the causal model",
    "For all examples from these papers we use X for the treatment variable",
    "instead of A.",
    "Figure 6 from Smucler, Sapienza and Rotnitzky (2021), Biometrika",
    "Figure 5 from Smucler, Sapienza and Rotnitzky (2021), Biometrika",
    "Figure 4 from Smucler, Sapienza and Rotnitzky (2021), Biometrika",
    "Figure 3 from Smucler, Sapienza and Rotnitzky (2021), Biometrika",
    "Figure 2 from Smucler and Rotnitzky (2022), Journal of Causal Inference",
    "L replaces X as the conditional variable",
    "Figure 2 from Smucler and Rotnitzky (2022), Journal of Causal Inference",
    "L replaces X as the conditional variable. Uses different costs",
    "Figure 3 from Smucler and Rotnitzky (2022), Journal of Causal Inference",
    "A graph where optimal, optimal minimal and optimal min cost are different",
    "The graph from Shrier and Platt (2008)",
    "A graph for which the algorithm was producing wrong result due to a bug reported by Sara Taheri",
    "Another graph for which the algorithm was producing wrong result due to a bug reported by Sara Taheri",
    "cov_mat = np.diag(np.ones(num_features))",
    "collider: X->Z<-Y",
    "chain: X->Z->Y",
    "fork: X<-Z->Y",
    "general DAG: X<-Z->Y, X->Y",
    "fork: X<-Z->Y",
    "Only Gaussian component changed",
    "Just checking formats, i.e. no need for correlation.",
    "Just checking formats, i.e. no need for correlation.",
    "The MSE should be 1 due to the variance of the noise. The RMSE is accordingly 1 / var(Y).",
    "Contributions should add up to Var(X2)",
    "H(P(Y)) -- Can be precomputed",
    "-(H(P(Y | do(x_S)) - H(P(Y))) = H(P(Y)) - H(P(Y | do(x_S))",
    "H(P(Y | do(x_S)) = H(E[P(Y | x_S, X'_\\S)])",
    "E[P(Y | x_S, X'_\\S)]",
    "H(E[P(Y | x_S, X'_\\S)])",
    "Using H(P(Y)) based on the origina data, i.e. ignoring baseline_predictions.",
    "E[H(P(Y)) - H(P(Y | do(X_U))] = H(P(Y)) - E[H(P(Y | X))]",
    "Mean from the categorical part is: (-5 + 5+ 10) / 3 = 10/3",
    "Having at least one sample from the second class should not raise an error.",
    "Just some random data, since we are only interested in the omitted data.",
    "This caused an error before with pandas > 2.0",
    "C2 = 3 * A2 + 2 * B2",
    "By default, the strength is measure with respect to the variance.",
    "Here, we misspecified the mechanism on purpose by setting scale to 1 instead of 2.",
    "If we provide the observational data here, we can mitigate the misspecification of the causal mechanism.",
    "Missing connection between X0 and X1.",
    "For X0 and X1, we set the ground truth noise to further emphasize the misspecification. The inferred noise of X1",
    "would otherwise have a dependency with Z due to the missing connection with X0.",
    "Modelling connection between X0 and X1 explicitly.",
    "Here, we misspecify the mechanism on purpose by setting scale to 1 instead of 2.",
    "If we provide the observational data here, we can mitigate the misspecification of the causal mechanism.",
    "Here, changing the mechanism.",
    "Here, changing the mechanism.",
    "Here, changing the mechanism.",
    "Here, changing the mechanism.",
    "Defining an anomaly scorer that handles multidimensional inputs.",
    "Seeing that the expectation of the noise in all nodes is 0, we introduce anomalies by setting some of them to 3.",
    "In the first sample, only the first variable is anomalous. Therefore, it should have the highest contribution",
    "and it should be \"significantly\" higher than the contribution of the other ones (here, we just arbitrarily say",
    "it should be 3x higher. Due to the confounding factor Z, the reconstructed noise variables are pairwise dependent,",
    "which is a violation of our causal sufficiency assumption. However, a confounder is included here to demonstrate",
    "some robustness. Note that due to this and stochastic behaviour of the density estimator, it is",
    "not possible to analytically compute expected results. Therefore, we rather look at the relations here.",
    "Same idea for the second sample, but here, it is the second variable that is anomalous.",
    "In the fourth sample, there are 2 anomalous variables. Therefore, the contribution of these 2 variables should be",
    "\"significantly\" higher than the contribution of the other variables. The contribution of both anomalous variables",
    "should be equal (approximately).",
    "Defining an anomaly scorer that handles multidimensional inputs.",
    "Seeing that the expectation of the noise in all nodes is 0, we introduce anomalies by setting some of them to 3.",
    "reduce the score.",
    "The contributions should add up to g(x) - E[g(X)]",
    "The contributions should add up to g(x) - E[g(X)]",
    "The contributions should add up to g(x) - E[g(X)]",
    "Three examples:",
    "1. X1 is the root cause (+ 10 to the noise)",
    "2. X0 is the root cause (+ 10 to the noise)",
    "3. X0 and X3 are both root causes (+ 10 to both noises)",
    "The sum of the scores should add up to the anomaly score of the target (here, X3).",
    "Three examples:",
    "1. X1 is the root cause (+ 10 to the noise)",
    "2. X0 is the root cause (+ 10 to the noise)",
    "3. X0 and X3 are both root causes (+ 10 to both noises)",
    "The sum of the scores should add up to the anomaly score of the target (here, X3).",
    "Defining ground truth model to avoid SCM learning issues and, hence, to focus on the anomaly attribution",
    "algorithm.",
    "1. X0 is the root cause (+ 10 to the noise)",
    "2. X0 and X1 are the root causes (+ 10 to both noise)",
    "3. X2 and X3 are both root causes (+ 10 to both noises)",
    "The sum of the scores should add up to the anomaly score of the target (here, X3).",
    "collider: X->Z<-Y",
    "collider: X->Z<-Y",
    "chain: X->Y->Z",
    "chain: X->Y->Z",
    "Empty graph",
    "Full DAG",
    "DAG with single node",
    "DAG with single edge",
    "DAG with single edge",
    "chain: X->Z->Y",
    "Setup data",
    "Test LinearDML",
    "Checking that the CATE estimates are not identical",
    "Test ContinuousTreatmentOrthoForest",
    "Checking that the CATE estimates are not identical",
    "Test LinearDRLearner",
    "Test LinearDML",
    "checking that CATE estimates are not identical",
    "predict on new data",
    "Setup data",
    "Test DeepIV",
    "Test IntentToTreatDRIV",
    "Observed data",
    "assumed graph",
    "Identify effect",
    "Estimate effect",
    "A model where X is also a common cause",
    "A model where X is also a common cause",
    "The case where effect modifier is not a common cause",
    "A model where X is also a common cause",
    "Create the graph describing the causal structure",
    "Generate the data",
    "Data to df",
    "Create a model",
    "Estimate the effect with front-door",
    "Not using testsuite from .base/TestEstimtor, custom code below",
    "More cases where Exception  is expected",
    "Compute confidence intervals, standard error and significance tests",
    "Generate a dataset with some categorical variables (common causes)",
    "This configuration is necessary for the test and should not be varied.",
    "For the purposes of the test, these are the categorical columns.",
    "Since their values are integer, convert them to string type to ensure",
    "Categorical handling.",
    "Get the values of row 0 for the specified columns",
    "Find rows where values differ from row 0 in terms of all values in the specified columns",
    "Create a copy of the data and swap the rows.",
    "Test 1: Permuting data order does not affect Effect estimate.",
    "This test will not likely fail with a RegressionEstimator, because",
    "the effect of common cause variables is additive and does not contribute to",
    "the estimated effect. However, it could fail with other Estimators.",
    "Test 2: Verify that estimated Outcomes from \"do-operator\" are unchanged",
    "While for some Estimators the Effect is unaffected by changes to common-cause",
    "data, and data ordering, predicted Outcomes should be as all variables can",
    "contribute to these Outcomes. However, they are only available in a standard",
    "interface for Estimators which support `do(x)`.",
    "",
    "In this test, we verify that the result of `do(x)` does not change when we",
    "present our two datasets (one has swapped first row). If the result differs,",
    "this is likely due to the encoding of these new data, since the model is",
    "unchanged.",
    "",
    "Unlike the Effect test #1 above, this test is verifiable; we can randomize",
    "the values and combinations of the encoded values and verify that under these",
    "conditions the result of `do(x)` *does* change. This is the type of error we",
    "expect to observe if there's an encoding error - all the encoded variables",
    "would change.",
    "Test that do(x) result is unchanged despite row permutation",
    "Verify that this test *does* detect errors, when common-cause data changed.",
    "Defined a linear dataset with a given set of properties",
    "Create a model that captures the same",
    "Identify the effects within the model",
    "Defined a linear dataset with a given set of properties",
    "Create a model that captures the same",
    "Identify the effects within the model",
    "Defined a linear dataset with a given set of properties",
    "Create a model that captures the same",
    "Identify the effects within the model",
    "Defined a linear dataset with a given set of properties",
    "Create a model that captures the same",
    "Identify the effects within the model",
    "Defined a linear dataset with a given set of properties",
    "Create a model that captures the same",
    "Identify the effects within the model",
    "Use a mix of already-numeric and requires encoding cols:",
    "NB There may be small differences in type but since all values will be used in models as float,",
    "comparison is done as this type.",
    "Check same rows",
    "Check same number of cols",
    "Check values",
    "Calculate the sum of absolute differences between the two DataFrames",
    "- should be zero (excl. floating point error)",
    "Use a mix of already-numeric and requires encoding cols:",
    "Initial encode",
    "Create new data with permuted rows.",
    "Output shape should be unchanged.",
    "Encode this new data.",
    "Check same rows",
    "Check same number of cols",
    "Check permuted values are consistent",
    "Check if calling the method causes some import or runtime errors",
    "TODO: Plotting undirected graphs with networkx causes an error when an older networkx version is used with a newer",
    "newer matplotlib version:",
    "AttributeError: module 'matplotlib.cbook' has no attribute 'is_numlike'",
    "Networkx 2.4+ should fix this issue.",
    "plot_adjacency_matrix(causal_graph, is_directed=False)",
    "Helper method to create a DOT file from string content",
    "Can use these lists to specify the models/estimators/refuters that a particular interpreter supports.  Throw a ValueError if the user provides an incompatible object to intepret.",
    "Unpacking the keyword arguments",
    "todo: add docstring for common parameters here and remove from child refuter classes",
    "Default value for the number of simulations to be conducted",
    "joblib params for parallel processing",
    "Concatenate the confounders, instruments and effect modifiers",
    "Shuffle the confounders",
    "Check if all are select or deselect variables",
    "Check if all the required_variables belong to confounders, instrumental variables or effect",
    "This calculates a two-sided percentile p-value",
    "See footnotes in https://journals.sagepub.com/doi/full/10.1177/2515245920911881",
    "Get the mean for the simulations",
    "Get the standard deviation for the simulations",
    "Get the Z Score [(val - mean)/ std_dev ]",
    "Initializing the p_value",
    "Perform Bootstrap Significance Test with the original estimate and the set of refutations",
    "Perform Normal Tests of Significance with the original estimate and the set of refutations",
    "Perform Bootstrap Significance Test with the original estimate and the set of refutations",
    "Perform Normal Tests of Significance with the original estimate and the set of refutations",
    "re.sub only takes string parameter so the first if is to avoid error",
    "if the input is a text file, convert the contained data into string",
    "load dot file",
    "TODO This functionality needs to be deprecated. It is a convenience function but can introduce confusion",
    "as we are now including the option to initialize CausalGraph with DiGraph or GCM model.",
    "Adding node attributes",
    "adding penwidth to make the edge bold",
    "Adding common causes",
    "Adding instruments",
    "Adding effect modifiers",
    "Assuming the simple form of effect modifier",
    "that directly causes the outcome.",
    "self._graph.add_edge(node_name, outcome, style = \"dotted\", headport=\"s\", tailport=\"n\")",
    "self._graph.add_edge(outcome, node_name, style = \"dotted\", headport=\"n\", tailport=\"s\") # TODO make the ports more general so that they apply not just to top-bottom node configurations",
    "Adding columns in the dataframe as confounders that were not in the graph",
    "Adding unobserved confounders",
    "removal of only direct edges wrt a target is not implemented for incoming edges",
    "also return the number of backdoor paths blocked by observed nodes",
    "Assume that nodes1 is the treatment",
    "ignores new_graph parameter, always uses self._graph",
    "remove paths that have nodes1\\node1 or nodes2\\node2 as intermediate nodes",
    "TODO Refactor to remove this from here and only implement this logic in causalIdentifier. Unnecessary assumption of nodes1 to be causing nodes2.",
    "Cannot simply compute ancestors, since that will also include nodes1 and its parents (e.g. instruments)",
    "Return effect modifiers according to the graph",
    "removing all mediators",
    "Also add any effect modifiers that could not be auto-detected (e.g., they are also common causes)",
    "[TODO: double check these work with multivariate implementation:]",
    "Exclusion",
    "As-if-random setup",
    "As-if-random",
    "convert the outputted generator into a list",
    "Condition 1: node 1 ---> node 2 is intercepted by candidate_nodes",
    "Emit a `UserWarning` if there are any unobserved graph variables and",
    "and log a message highlighting data variables that are not part of the graph.",
    "Create causal graph object",
    "Sometimes, effect modifiers from the graph may not match those provided by the user.",
    "(Because some effect modifiers may also be common causes)",
    "In such cases, the user-provided modifiers are used.",
    "If no effect modifiers are provided,  then the ones from the graph are used.",
    "Import causal discovery class",
    "Initialize causal graph object",
    "TODO add propensity score as default backdoor method, iv as default iv method, add an informational message to show which method has been selected.",
    "TODO add dowhy as a prefix to all dowhy estimators",
    "This is done as all dowhy estimators have two parts and external ones have two or more parts",
    "Define the third-party estimation method to be used",
    "Process the dowhy estimators",
    "If not fit_estimator, attempt to retrieve existing estimator.",
    "Keep original behaviour to create new estimator if none found.",
    "Check if estimator's target estimand is identified",
    "Note that while the name of the variable is the same,",
    "\"self.causal_estimator\", this estimator takes in less",
    "parameters than the same from the",
    "estimate_effect code. It is not advisable to use the",
    "estimator from this function to call estimate_effect",
    "with fit_estimator=False.",
    "Estimator had been computed in a previous call",
    "The default number of simulations for statistical testing",
    "The default number of simulations to obtain confidence intervals",
    "This should be at least 399 for a 5% error rate:",
    "https://www.econstor.eu/bitstream/10419/67820/1/587473266.pdf",
    "The portion of the total size that should be taken each time to find the confidence intervals",
    "1 is the recommended value",
    "https://ocw.mit.edu/courses/mathematics/18-05-introduction-to-probability-and-statistics-spring-2014/readings/MIT18_05S14_Reading24.pdf",
    "https://projecteuclid.org/download/pdf_1/euclid.ss/1032280214",
    "The default Confidence Level",
    "Number of quantiles to discretize continuous columns, for applying groupby",
    "Prefix to add to temporary categorical variables created after discretization",
    "std args to be removed from locals() before being passed to args_dict",
    "Setting the default interpret method",
    "Check if some parameters were set, otherwise set to default values",
    "Estimate conditional estimates by default",
    "TODO Only works for binary treatment",
    "Defaulting to class default values if parameters are not provided",
    "Checking that there is at least one effect modifier",
    "Making sure that effect_modifier_names is a list",
    "Making a copy since we are going to be changing effect modifier names",
    "For every numeric effect modifier, adding a temp categorical column",
    "Grouping by effect modifiers and computing effect separately",
    "Deleting the temporary categorical columns",
    "The array that stores the results of all estimations",
    "Find the sample size the proportion with the population size",
    "Perform the set number of simulations",
    "names of treatment and outcome",
    "Using class default parameters if not specified",
    "Checking if bootstrap_estimates are already computed",
    "Checked if any parameter is changed from the previous std error estimate",
    "Now use the data obtained from the simulations to get the value of the confidence estimates",
    "Get the variations of each bootstrap estimate and sort",
    "Now we take the (1-p)/2 th and the 1-(1-p)/2 th variations, where p is the chosen confidence level",
    "Get the lower and upper bounds by subtracting the variations from the estimate",
    "Use existing params, if new user defined params are not present",
    "Checking if bootstrap_estimates are already computed",
    "Check if any parameter is changed from the previous std error estimate",
    "Use existing params, if new user defined params are not present",
    "Processing the null hypothesis estimates",
    "Doing a two-sided test",
    "Being conservative with the p-value reported",
    "Being conservative with the p-value reported",
    "If the estimate_index is 0, it depends on the number of simulations",
    "Need to test r-squared before supporting",
    "effect_r_squared = self._evaluate_effect_strength(estimate, method=\"r-squared\")",
    "'r-squared': effect_r_squared",
    "elif method == \"r-squared\":",
    "outcome_mean = np.mean(self._outcome)",
    "total_variance = np.sum(np.square(self._outcome - outcome_mean))",
    "Assuming a linear model with one variable: the treatment",
    "Currently only works for continuous y",
    "causal_model = outcome_mean + estimate.value*self._treatment",
    "squared_residual = np.sum(np.square(self._outcome - causal_model))",
    "r_squared = 1 - (squared_residual/total_variance)",
    "return r_squared",
    "Check if estimator's target estimand is identified",
    "Store parameters inside estimate object for refutation methods",
    "TODO: This add_params needs to move to the estimator class",
    "inside estimate_effect and estimate_conditional_effect",
    "TODO: Remove _data, _treatment_name and _outcome_name from this object",
    "we save them here to enable the methods that required these properties saved in the estimator",
    "eventually we should call those methods and just save the results in this object",
    "instead of having this object invoke the estimator methods with the data.",
    "No estimand was identified (identification failed)",
    "s += \"Variance in outcome explained by treatment: {}\\n\".format(self.effect_strength[\"r-squared\"])",
    "can't use nx.node_connected_component, because it doesn't work with DiGraphs.",
    "Hence, a manual loop:",
    "also return the number of backdoor paths blocked by observed nodes",
    "Assume that nodes1 is the treatment",
    "ignores new_graph parameter, always uses self._graph",
    "removal of only direct edges wrt a target is not implemented for incoming edges",
    "remove paths that have nodes1\\node1 or nodes2\\node2 as intermediate nodes",
    "[TODO: double check these work with multivariate implementation:]",
    "Exclusion",
    "As-if-random setup",
    "As-if-random",
    "Condition 1: node 1 ---> node 2 is intercepted by candidate_nodes",
    "Adding common causes",
    "Adding instruments",
    "Adding effect modifiers",
    "Assuming the simple form of effect modifier",
    "that directly causes the outcome.",
    "self._graph.add_edge(node_name, outcome, style = \"dotted\", headport=\"s\", tailport=\"n\")",
    "self._graph.add_edge(outcome, node_name, style = \"dotted\", headport=\"n\", tailport=\"s\") # TODO make the ports more general so that they apply not just to top-bottom node configurations",
    "some preprocessing steps",
    "parsing the correct graph based on input graph format",
    "load dot file",
    "Below loop assumes that the last indices of W are alwawys converted to discrete",
    "one-hot encode discrete W",
    "Now deleting the old continuous value",
    "create a dataset with 10 observations one binary treatment and a continuous outcome affected by one common cause",
    "Two continuous treatments, no common cause, an instrumental variable and two effect modifiers - linearly added appropriately",
    "One Hot Encoding",
    "TODO Ensure that we do not generate weak instruments",
    "TODO - test all our methods with random noise added to covariates (instead of the stochastic treatment assignment)",
    "Converting treatment to binary if required",
    "Generating frontdoor variables if asked for",
    "NOTE: We are assuming a linear relationship *even when t is categorical* and integer coded.",
    "For categorical t, this example dataset has the effect size for category 2 being exactly",
    "double the effect for category 1",
    "This could be changed at this stage by one-hot encoding t and using a custom beta that",
    "sets a different effect for each category {0, 1, 2}",
    "Computing ATE",
    "constructing column names for one-hot encoded discrete features",
    "Specifying the correct dtypes",
    "Now specifying the corresponding graph strings",
    "Now writing the gml graph",
    "creating data frame",
    "Specifying the correct dtypes",
    "Now specifying the corresponding graph strings",
    "Now writing the gml graph",
    "Adding edges between common causes and the frontdoor mediator",
    "Error terms",
    "else:",
    "V = 6 + W0 + tterm + E1",
    "Y = 12 + W0*W0 + W0*W0 + yterm + E2  # E2_new",
    "Generating a random normal distribution of integers",
    "Generating data for nodes which have no incoming edges",
    "\"currset\" variable currently has all the successors of the nodes which had no incoming edges",
    "assuming that all unobserved common causes are numerical and are not affected by one hot encoding",
    "Creating a NN to simulate the nuisance function",
    "strength of unobserved confounding",
    "Computing ATE",
    "Specifying the correct dtypes",
    "Now writing the gml graph",
    "The following code for loading the Lalonde dataset was copied from",
    "https://github.com/wayfair/pylift/blob/5afc9088e96f25672423663f5c9b4bb889b4dfc0/examples/Lalonde/Lalonde_sample.ipynb?short_path=b1d451f#L94-L99).",
    "",
    "Copyright 2018, Wayfair, Inc.",
    "",
    "Redistribution and use in source and binary forms, with or without modification, are permitted provided that",
    "the following conditions are met:",
    "",
    "1. Redistributions of source code must retain the above copyright notice, this list of conditions and the",
    "following disclaimer.",
    "",
    "2. Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the",
    "following disclaimer in the documentation and/or other materials provided with the distribution.",
    "",
    "THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND ANY EXPRESS OR IMPLIED",
    "WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A",
    "PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY",
    "DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,",
    "PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER",
    "CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR",
    "OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH",
    "DAMAGE.",
    "The following code is a slight modification of",
    "https://github.com/wayfair/pylift/blob/5afc9088e96f25672423663f5c9b4bb889b4dfc0/examples/Lalonde/Lalonde_sample.ipynb?short_path=b1d451f#L94-L99).",
    "",
    "Copyright 2018, Wayfair, Inc.",
    "",
    "Redistribution and use in source and binary forms, with or without modification, are permitted provided that",
    "the following conditions are met:",
    "",
    "1. Redistributions of source code must retain the above copyright notice, this list of conditions and the",
    "following disclaimer.",
    "",
    "2. Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the",
    "following disclaimer in the documentation and/or other materials provided with the distribution.",
    "",
    "THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND ANY EXPRESS OR IMPLIED",
    "WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A",
    "PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY",
    "DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,",
    "PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER",
    "CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR",
    "OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH",
    "DAMAGE.",
    "",
    "0.0.0 is standard placeholder for poetry-dynamic-versioning",
    "any changes to this should not be checked in",
    "",
    "Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved",
    "Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved",
    "Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved",
    "adapt number of channels",
    "save memory",
    "Keep same dimensions",
    "Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved",
    "single-attribute Causal",
    "test environment",
    "Subsample 2x for computational convenience",
    "Assign a binary label based on the digit",
    "Flip label with probability 0.25",
    "Assign a color based on the label; flip the color with probability environment",
    "Apply the color to the image by zeroing out the other color channel",
    "single-attribute Independent",
    "test environment",
    "Subsample 2x for computational convenience",
    "Assign a binary label based on the digit",
    "Flip label with probability 0.25",
    "multi-attribute Causal + Independent",
    "test environment",
    "Subsample 2x for computational convenience",
    "rotate the image by angle in parameter",
    "Assign a binary label based on the digit",
    "Flip label with probability 0.25",
    "Assign a color based on the label; flip the color with probability environment",
    "Apply the color to the image by zeroing out the other color channel",
    "Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved",
    "Acause regularization",
    "Aconf regularization",
    "Aind regularization",
    "Asel regularization",
    "Compile loss",
    "Check if the optimizer is currently supported",
    "Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved",
    "The currently supported estimators",
    "The default standard deviation for noise",
    "The default scaling factor to determine the bucket size",
    "The minimum number of points for the estimator to run",
    "The Default Transformation, when no arguments are given, or if the number of data points are insufficient for an estimator",
    "The Default True Causal Effect, this is taken to be ZERO by default",
    "The Default split for the number of data points that fall into the training and validation sets",
    "Assuming that outcome is one-dimensional",
    "We need to change the identified estimand",
    "We thus, make a copy. This is done as we don't want",
    "to change the original DataFrame",
    "We use collections.OrderedDict to maintain the order in which the data is stored",
    "Check if we are using an estimator in the transformation list",
    "The rationale behind ordering of the loops is the fact that we induce randomness everytime we create the",
    "Train and the Validation Datasets. Thus, we run the simulation loop followed by the training and the validation",
    "loops. Thus, we can get different values everytime we get the estimator.",
    "for _ in range( self._num_simulations ):",
    "Warn the user that the specified parameter is not applicable when no estimator is present in the transformation",
    "Adding an unobserved confounder if provided by the user",
    "We set X_train = 0 and outcome_train to be 0",
    "Get the final outcome, after running through all the values in the transformation list",
    "Check if the value of true effect has been already stored",
    "We use None as the key as we have no base category for this refutation",
    "As we currently support only one treatment",
    "If the number of data points is too few, run the default transformation: [(\"zero\",\"\"),(\"noise\", {'std_dev':1} )]",
    "Check if the value of true effect has been already stored",
    "This ensures that we calculate the causal effect only once.",
    "We use key_train as we map data with respect to the base category of the data",
    "As we currently support only one treatment",
    "Add h(t) to f(W) to get the dummy outcome",
    "We convert to ndarray for ease in indexing",
    "The data is of the form",
    "sim1: cat1 cat2 ... catn",
    "sim2: cat1 cat2 ... catn",
    "Note: We would like the causal_estimator to find the true causal estimate that we have specified through this",
    "refuter. Let the value of the true causal effect be h(t). In the following section of code, we wish to find out if h(t) falls in the",
    "distribution of the refuter.",
    "True Causal Effect list",
    "Iterating through the refutation for each category",
    "We use string arguments to account for both 32 and 64 bit varaibles",
    "action for continuous variables",
    "Action for categorical variables",
    "Find the set difference for each row",
    "Choose one out of the remaining",
    "We want to see if the estimate falls in the same distribution as the one generated by the refuter",
    "Ideally that should be the case as running bootstrap should not have a significant effect on the ability",
    "of the treatment to affect the outcome",
    "TODO: Check that the target estimand has backdoor variables?",
    "1. Categorical encoding of relevant variables",
    "The encoded data is only used to calculate a parameter, so the encoder can be discarded.",
    "2. Standardizing the data",
    "Fit a model containing all confounders and compare predictions",
    "using all features compared to all features except a given",
    "confounder.",
    "Estimating the regression coefficient from standardized features to t",
    "By default, return a plot with 10 points",
    "consider 10 values of the effect of the unobserved confounder",
    "Fit a model containing all confounders and compare predictions",
    "using all features compared to all features except a given",
    "confounder.",
    "By default, return a plot with 10 points",
    "consider 10 values of the effect of the unobserved confounder",
    "By default, we add the effect of simulated confounder for treatment.",
    "But subtract it from outcome to create a negative correlation",
    "assuming that the original confounder's effect was positive on both.",
    "This is to remove the effect of the original confounder.",
    "By default, we add the effect of simulated confounder for treatment.",
    "But subtract it from outcome to create a negative correlation",
    "assuming that the original confounder's effect was positive on both.",
    "This is to remove the effect of the original confounder.",
    "Obtaining the list of observed variables",
    "Taking a subset of the dataframe that has only observed variables",
    "Residuals from the outcome model obtained by fitting a linear model",
    "Residuals from the treatment model obtained by fitting a linear model",
    "Initialising product_cor_metric_observed with a really low value as finding maximum",
    "The user has an option to give the the effect_strength_on_y and effect_strength_on_t which can be then used instead of maximum correlation with treatment and outcome in the observed variables as it specifies the desired effect.",
    "Choosing a c_star based on the data.",
    "The correlations stop increasing upon increasing c_star after a certain value, that is it plateaus and we choose the value of c_star to be the value it plateaus.",
    "Choosing c1 and c2 based on the hyperbolic relationship once c_star is chosen by going over various combinations of c1 and c2 values and choosing the combination which",
    "which maintains the minimum distance between the product of correlations of the simulated variable and the product of maximum correlations of one of the observed variables",
    "and additionally checks if the ratio of the weights are such that they maintain the ratio of the maximum possible observed coefficients within some confidence interval",
    "c1_final and c2_final are initialised to the values on the hyperbolic curve such that c1_final = c2_final  and c1_final*c2_final = c_star",
    "initialising min_distance_between_product_cor_metrics to be a value greater than 1",
    "TODO (petergtz): This should introduce a Protocol defining the interface this returns instead of the concrete types",
    "If the estimator used is LinearDML, partially linear sensitivity analysis will be automatically chosen",
    "Get a 2D matrix of values",
    "x,y =  np.meshgrid(self.kappa_t, self.kappa_y) # x,y are both MxN",
    "Store the values into the refute object",
    "Adding a label on the contour line for the original estimate",
    "Label every other level using strings",
    "Default value of the p value taken for the distribution",
    "Number of Trials: Number of cointosses to understand if a sample gets the treatment",
    "Mean of the Normal Distribution",
    "Standard Deviation of the Normal Distribution",
    "Create a new column in the data by the name of placebo",
    "Sanity check the data",
    "only permute is supported for iv methods",
    "For IV methods, the estimating_instrument_names should also be",
    "changed. Create a copy to avoid modifying original object",
    "We need to change the identified estimand",
    "We make a copy as a safety measure, we don't want to change the",
    "original DataFrame",
    "Run refutation in parallel",
    "Note: We hardcode the estimate value to ZERO as we want to check if it falls in the distribution of the refuter",
    "Ideally we should expect that ZERO should fall in the distribution of the effect estimates as we have severed any causal",
    "relationship between the treatment and the outcome.",
    "new estimator",
    "new effect estimate",
    "observed covariate E-value",
    "see Table 2 and p.37 in https://dash.harvard.edu/bitstream/handle/1/36874927/EValue_FinalSubmission.pdf",
    "if CI crosses null, set its E-value to 1",
    "only report E-value for CI limit closer to null",
    "see Table 2 and p.37 in https://dash.harvard.edu/bitstream/handle/1/36874927/EValue_FinalSubmission.pdf",
    "whether the DGP is assumed to be partially linear",
    "features are the observed confounders",
    "Now code for benchmarking using covariates begins",
    "R^2 of outcome with observed common causes and treatment",
    "R^2 of treatment with observed common causes",
    "Partial R^2 of outcome after regressing over unobserved confounder, observed common causes and treatment",
    "Assuming that the difference in R2 is the same for wj and new unobserved confounder",
    "for treatment,  Calpha is not a function of the partial R2. So we need a different assumption.",
    "Assuming that the ratio of variance of alpha^2 is the same for wj and new unobserved confounder",
    "(1-ratio_var_alpha_wj) is the numerator of Calpha2, similar to the partial R2 for treatment",
    "wrt unobserved confounders in partial-linear models",
    "Do the support characterization",
    "Recover the samples that are in the support",
    "Assess overlap using propensity scores with cross-fitting",
    "Check if all supported units are considered to be in the overlap set",
    "NOTE: The original paper implements both DNF and CNF rules, but for simplicity, this code only implements DNF rules",
    "For DNF rules, a sample is covered if *any* rule applies",
    "whether the DGP is assumed to be partially linear",
    "can change this to allow default values that are same as the other parameter",
    "Strength of confounding that omitted variables generate in treatment regression",
    "computing the point estimate for the bounds",
    "common causes after removing the benchmark causes",
    "dataframe with treatment and observed common causes after removing benchmark causes",
    "R^2 of treatment with observed common causes removing benchmark causes",
    "return the variance of alpha_s",
    "R^2 of outcome with observed common causes and treatment after removing benchmark causes",
    "Obtaining theta_s (the obtained estimate)",
    "Creating numpy arrays",
    "Setting up cross-validation parameters",
    "tuple of residuals from first stage estimation [0,1], and the confounders [2]",
    "We need to estimate, sigma^2 = (Y-g_s)^2. We use the following derivation.",
    "Yres = Y - E[Y|W]",
    "E[Y|W] = f(x) + theta_s * E[T|W]",
    "Yres = Y - f(x) - theta_s * E[T|W]",
    "g(s) = theta_s * T + f(x)",
    "g(s) = theta_s * (T - E[T|W]) + f(x) + theta_s * E[T|W]",
    "g(s) = theta_s * Tres +f(x) + theta_s * E[T|W]",
    "Y - g(s) = Y - [theta_s * Tres + f(x) + theta_s * E[T|W] )",
    "Y - g(s) = ( Y - f(x) -  theta_s * E[T|W]) - theta_s * Tres",
    "Y - g(s) = Yres - theta_s * Tres",
    "nu_2 is E[alpha_s^2]",
    "Now computing scores for finding the (1-a) confidence interval",
    "R^2 of treatment with observed common causes",
    "R^2 of outcome with treatment and observed common causes",
    "Partial R^2 of outcome after regressing over unobserved confounder, observed common causes and treatment",
    "Partial R^2 of treatment after regressing over unobserved confounder and observed common causes",
    "adding 1.1 as plotting margin  ensure that the benchmarked part is shown fully in plot",
    "adding 1.1 as plotting margin  ensure that the benchmarked part is shown fully in plot",
    "Adding unadjusted point estimate",
    "Adding bounds to partial R^2 values for given strength of confounders",
    "Adding a new backdoor variable to the identified estimand",
    "Run refutation in parallel",
    "We want to see if the estimate falls in the same distribution as the one generated by the refuter",
    "Ideally that should be the case as choosing a subset should not have a significant effect on the ability",
    "of the treatment to affect the outcome",
    "TODO: Sensitivity Analyzers excluded from list due to different return type",
    "Reject H0",
    "a, b and c are all continuous variables",
    "a, b and c are all discrete variables",
    "c is set of continuous and binary variables and",
    "1. either a and b is continuous and the other is binary",
    "2. both a and b are binary",
    "c is discrete and",
    "either a or b is continuous and the other is discrete",
    "a and b are discrete and c is a mixture of discrete and continuous variables. We discretize c and calculate conditional mutual information",
    "original_treatment_name: : stores original variable names for labelling",
    "common_causes_map : maps the original variable names to variable names in OLS regression",
    "benchmark_common_causes: stores variable names in terms of regression model variables",
    "original_benchmark_covariates: stores original variable names for labelling",
    "estimate: estimate of regression",
    "degree_of_freedom: degree of freedom of error in regression",
    "standard_error: standard error in regression",
    "t_stats: Treatment coefficient t-value - measures how many standard errors the estimate is away from zero.",
    "partial_f2: value to determine if a regression model and a nested version of it have a statistically significant difference between them",
    "r2tu_w: partial R^2  of unobserved confounder \"u\" with treatment \"t\", after conditioning on observed covariates \"w\"",
    "r2yu_tw: partial R^2  of unobserved confounder \"u\" with outcome \"y\", after conditioning on observed covariates \"w\" and treatment \"t\"",
    "r2twj_w: partial R^2 of observed covariate wj with treatment \"t\", after conditioning on observed covariates \"w\" excluding wj",
    "r2ywj_tw:  partial R^2 of observed covariate wj with outcome \"y\", after conditioning on observed covariates \"w\" (excluding wj) and treatment \"t\"",
    "benchmarking_results: dataframe containing information about bounds and bias adjusted terms",
    "stats: dictionary containing information like robustness value, partial R^2, estimate, standard error , degree of freedom, partial f^2, t-statistic",
    "partial R^2 (r2yt_w) is the proportion of variation in outcome uniquely explained by treatment",
    "build a new regression model by considering treatment variables as outcome",
    "r2twj_w is partial R^2 of covariate wj with treatment \"t\", after conditioning on covariates w(excluding wj)",
    "r2ywj_tw is partial R^2 of covariate wj with outcome \"y\", after conditioning on covariates w(excluding wj) and treatment \"t\"",
    "r2tu_w is the partial r^2 from regressing u on t after conditioning on w",
    "Compute bias adjusted terms",
    "Plotting the contour plot",
    "Adding contours",
    "Adding threshold contour line",
    "Adding unadjusted point estimate",
    "Adding bounds to partial R^2 values for given strength of confounders",
    "from https://www.bnmetrics.com/blog/factory-pattern-in-python3-simple-version",
    "The default subset of the data to be used",
    "Run refutation in parallel",
    "We want to see if the estimate falls in the same distribution as the one generated by the refuter",
    "Ideally that should be the case as choosing a subset should not have a significant effect on the ability",
    "of the treatment to affect the outcome",
    "Parameters",
    "Bookkeeping",
    "Initialize estimators",
    "Convert to dataframe if not",
    "Format labels",
    "Sample from reference measure and construct features",
    "Add reference samples",
    "Binarize features (fit to data only)",
    "Fit estimator",
    "Store reference volume",
    "Construct features dataframe",
    "Construct features dataframe",
    "Iterate over columns",
    "logging.info(\"Using provided reference range for {}\".format(c))",
    "number of unique values",
    "Constant column",
    "Binary column",
    "Ordinal column (seed = counter so not correlated)",
    "For get_params / set_params",
    "Overlap / Support (y = +1), non-overlap (y = 0), and uniform background (y = -1) samples",
    "We should always have overlap samples, and either background or non-overlap samples",
    "This will throw an error if, for example, all samples are considered to",
    "be in the overlap region",
    "Initialize with empty and singleton conjunctions, i.e. X plus all-ones feature",
    "Feature indicator and conjunction matrices",
    "Iteration counter",
    "Formulate master LP",
    "Variables",
    "Objective function (no penalty on empty conjunction)",
    "Constraints",
    "This gets activated for DNF",
    "Solve problem",
    "Extract dual variables",
    "Beam search for conjunctions with negative reduced cost",
    "Most negative reduced cost among current variables",
    "Negative reduced costs found",
    "Add to existing conjunctions",
    "Reformulate master LP",
    "Variables",
    "Objective function",
    "Constraints",
    "Solve problem",
    "Extract dual variables",
    "Beam search for conjunctions with negative reduced cost",
    "Most negative reduced cost among current variables",
    "print('UB.min():', UB.min())",
    "Save generated conjunctions and coefficients",
    "Restrict conjunctions to those used by LP",
    "NOTE: This is a greedy approach, so it does not incorporate lambda0 explicitly",
    "Similarly, it will prefer a larger number of smaller rules if lambda1 is set",
    "to a larger value, because the incremental cost will be lower.",
    "Fraction of reference samples that each conjunction covers",
    "Regularization (for each conjunction)",
    "Positive samples newly covered (for each conjunction)",
    "Costs (for each conjunction)",
    "Zero out the rules and only take those which are used",
    "Small tolerance on comparisons",
    "This can be useful to break ties and favor larger values of xi",
    "Compute conjunctions of features",
    "Predict labels",
    "Use helper function",
    "Use helper function",
    "Lower bound specific to each singleton solution",
    "Initialize output",
    "Remove redundant rows by grouping by unique feature combinations and summing residual",
    "Initialize queue with root instance",
    "Separate data according to positive and negative residuals",
    "Iterate over increasing degree while queue is non-empty",
    "Initialize list of children to process",
    "Process instances in queue",
    "inst = instCurr[0]",
    "Evaluate all singleton solutions",
    "Best solutions that also improve on current output (allow for duplicate removal)",
    "Append to current output",
    "Remove duplicates",
    "Update output",
    "Compute lower bounds on higher-degree solutions",
    "Evaluate children using weighted average of their costs and LBs",
    "Best children with potential to improve on current output and current candidates (allow for duplicate removal)",
    "Iterate through best children",
    "New \"zero\" solution",
    "Check if duplicate",
    "Add to candidates for further processing",
    "Create pricing instance",
    "Remove covered rows",
    "Remove redundant features",
    "Track number of candidates added",
    "Update candidates",
    "Instances to process in next iteration",
    "Conjunctions corresponding to solutions",
    "List of categorical columns",
    "Number of quantile thresholds used to binarize ordinal features",
    "whether to append negations",
    "whether to convert thresholds on ordinal features to strings",
    "Quantile probabilities",
    "Initialize",
    "Iterate over columns",
    "number of unique values",
    "Constant or binary column",
    "Mapping to 0, 1",
    "Categorical column",
    "OneHotEncoder object",
    "Fit to observed categories",
    "Ordinal column",
    "Few unique values",
    "Thresholds are sorted unique values excluding maximum",
    "Many unique values",
    "Thresholds are quantiles excluding repetitions",
    "Contains NaN values",
    "Initialize dataframe",
    "Iterate over columns",
    "Constant or binary column",
    "Rename values to 0, 1",
    "Categorical column",
    "Apply OneHotEncoder",
    "Append negations",
    "Concatenate",
    "Ordinal column",
    "Threshold values to produce binary arrays",
    "Append negations",
    "Convert to dataframe with column labels",
    "Ensure that rows corresponding to NaN values are zeroed out",
    "Add NaN indicator column",
    "Concatenate",
    "from https://www.bnmetrics.com/blog/factory-pattern-in-python3-simple-version",
    "set attributions to zero for left out invariant nodes",
    "Get parent and child nodes",
    "Don't remove node if node has more than 1 children nodes as it can introduce",
    "hidden confounders.",
    "Remove the middle node",
    "Connect parent and child nodes",
    "Update the causal mechanism for the child nodes",
    "If the relative change of the score is less than the given threshold, we stop the estimation early.",
    "Compute rank of every single test point in the union of the training and the respective test point.",
    "+ 1 here to count the test sample itself.",
    "The probability to get at most rank k from above is k divided by the total number of samples. Similar for",
    "the case of below. Therefore, to get at most rank k either from above or below is",
    "min(2*k/total_num_samples, 1). We then get a p-value for exchangeability:",
    "Note, the output of score_samples are log values.",
    "Note, the output of score_samples are log values.",
    "When all variables are independent (Example C.4), simplify to a group of 0s and a group of 1s (regardless of order).",
    "Otherwise, we just group the consecutive values (Remark C.1).",
    "Train gamma_K:",
    "Train gamma_k for k = K-1, K-2, ..., 1:",
    "Use the fitted values from previous regression",
    "Train classifiers that will go into alpha_k for k = 1, ..., K:",
    "For the case where you want to calibrate on different data,",
    "No need if classifier is CalibratedClassifierCv",
    "k = 0 doesn't have parents, get the marginal RN derivative.",
    "For k > 0 get the conditional RN derivative dividing the RN derivative for \\bar{X}_j",
    "by the RN derivative for \\bar{X}_{j-1}.",
    "alpha_k should integrate to 1. In small samples this might not be the case, so we standardize:",
    "Regression base estimate:",
    "Debiasing terms up to K-1:",
    "When C = [1, 1, ..., 1] we can just take the sample mean of y_eval[T_eval == 1]",
    "When C = [0, 0, ..., 0] we can just take the sample mean of y_eval[T_eval == 0]",
    "Eliminate nodes that are not ancestry of the outcome:",
    "Sort by causal ancestry:",
    "Currently only support continuous distributions for auto selection.",
    "Estimate distribution parameters from data.",
    "Ignore warnings from fitting process.",
    "Fit distribution to data.",
    "Some distributions might not be compatible with the data.",
    "Separate parts of parameters.",
    "Check the KL divergence between the distribution of the given and fitted distribution.",
    "Identify if this distribution is better.",
    "This error is typically raised when the data is discrete and all points are assigned to less cluster than",
    "specified. It can also happen due to duplicated points. In these cases, the current best solution should",
    "be sufficient.",
    "Usual feature relevance using the mean deviation as set function, i.e. g(x) - E[g(X)]",
    "Normally, we need to estimate the KL divergence jointly. However, to avoid issues with high dimensional data,",
    "we approximate it by taking the average over the marginal KL divergences.",
    "Do not compare with same model class",
    "Do not compare with same model class",
    "In case of post non-linear models, we can obtain the conditional expectation directly based on the prediction",
    "model. To do this, we can just in pass 0 as the noise, since this would evaluate Y = f(X) + 0 in case of an",
    "additive noise model and Y = g(f(X) + 0) in case of a more general model.",
    "Estimate the conditional expectation E[Y | x] by generating multiple samples for Y|x and average them.",
    "Since these are categorical values, we just need to look for the most frequent element after we drew",
    "multiple samples for each input.",
    "In the categorical case, this is equivalent to the Brier score. However, the following formulation allows",
    "categorical data with more than two classes.",
    "A convenience function when computing confidence intervals specifically for non-deterministic causal queries. This",
    "function evaluates the provided causal query multiple times to build a confidence interval based on the returned",
    "results.",
    "Note that this function does not re-fit the causal model(s) and only executes the provided query as it is. In order",
    "to re-refit the graphical causal model on random subsets of the data before executing the query, consider using the",
    "fit_and_compute function.",
    "",
    "**Example usage:**",
    "",
    ">>> gcm.fit(causal_model, data)",
    ">>> strength_medians, strength_intervals = gcm.confidence_intervals(",
    ">>>     gcm.bootstrap_sampling(gcm.arrow_strength, causal_model, target_node='Y'))",
    "",
    "In this example, gcm.confidence_intervals is expecting a callable with non-deterministic outputs for building the",
    "confidence intervals. Since each causal query potentially expects a different set of parameters, we use 'partial'",
    "here to configure the function call. In this case,",
    "gcm.bootstrap_sampling(gcm.arrow_strength, causal_model, target_node='Y') would be equivalent to",
    "lambda : gcm.arrow_strength(causal_model, target_node='Y').",
    "",
    "In order to incorporate uncertainties coming from fitting the causal model(s), we can use",
    "gcm.fit_and_compute instead:",
    ">>>  strength_medians, strength_intervals = gcm.confidence_intervals(",
    ">>>        gcm.fit_and_compute(gcm.arrow_strength,",
    ">>>                                            causal_model,",
    ">>>                                            bootstrap_training_data=data,",
    ">>>                                            target_node='Y'))",
    "This would refit the provided causal_model on a subset of the data first before executing gcm.arrow_strength in each",
    "run.",
    "Simulating interventions by propagating the effects through the graph. For this, we iterate over the nodes based",
    "on their topological order.",
    "After drawing samples of the node based on the data generation process, we apply the corresponding",
    "intervention. The inputs of downstream nodes are therefore based on the outcome of the intervention in this",
    "node.",
    "Abduction: For invertible SCMs, we recover exact noise values from data.",
    "Action + Prediction: Propagate the intervention downstream using recovered noise values.",
    "Check if we need to apply an intervention on the given node.",
    "Apply intervention function to the data of the node.",
    "Check if the intervention function changes the shape of the data.",
    "For estimating the effect, we only need to consider the nodes that have a directed path to the target node, i.e.",
    "all ancestors of the target.",
    "The target node can be a continuous real-valued variable or a categorical variable with at most two classes",
    "(i.e. binary).",
    "Find all node names in the expression string.",
    "Nothing to fit here, since we know the ground truth.",
    "Avoid too many features",
    "Making sure there are at least 30% test samples.",
    "Making sure that there are at least 2 samples from one class (here, simply duplicate the point).",
    "Compare number of correct classifications.",
    "Making sure that X and Y have no overlapping values, which would lead to a distance of 0 with k=1 and, thus, to",
    "a division by zero.",
    "All elements are equal (or at least less than k samples are different)",
    "Balance the classes",
    "For estimating Shapley values for multiple samples (e.g. in feature relevance) and the number of samples",
    "is unknown beforehand.",
    "The method stops if either the change between some consecutive runs is below the given threshold or the",
    "maximum number of runs is reached.",
    "In each run, we create one random permutation of players. For instance, given 4 players, a permutation",
    "could be [3,1,4,2].",
    "Generate k random permutations by sorting the indices of the Halton sequence",
    "Create all subsets belonging to the generated permutation. This is, if we have [3,1,4,2], then the",
    "subsets are [3], [3,1], [3,1,4] [3,1,4,2].",
    "The result for each subset is cached such that if a subset that has already been evaluated appears again,",
    "we can take this result directly.",
    "To improve the runtime, multiple permutations are evaluated in each run.",
    "The current Shapley values are the average of the estimated values, i.e. we need to divide by the number",
    "of generated permutations here.",
    "In case Shapley values are estimated for multiple samples, e.g., in feature relevance. So, we have a",
    "matrix of Shapley values instead of a vector.",
    "Here, the change between consecutive runs is below the minimum threshold, but to reduce the",
    "likelihood that this just happened by chance, we require that this happens at least for",
    "num_consecutive_converged_runs times in a row.",
    "Check if change in percentage is below threshold",
    "Check for values that are exactly zero. If they don't change between two runs, we consider it as converging.",
    "Create all (unique) subsets)",
    "Assigning a 'high' weight, since this resembles \"infinity\".",
    "The weight for a subset with a specific length (see paper mentioned in the docstring for more",
    "information).",
    "TODO: Add method for auto select a bin_width/width based on the data. Make sure that the auto selection method is",
    "theoretically sound, i.e. make entropy results from different data comparable.",
    "Extremely small values can somehow result in negative values.",
    "Creating a smaller subgraph, which only contains upstream nodes that are connected to the target node.",
    "Sampling from the conditional distribution based on the current sample.",
    "Sampling from the conditional based on the current sample, but randomizing the inputs of all variables that",
    "are in the given subset. By this, we can simulate the impact on the conditional distribution when removing",
    "only the incoming edges of the variables in the subset.",
    "Creating a smaller subgraph, which only contains upstream nodes that are connected to the target node.",
    "In case of the full subset (no randomization), we get the same predictions as when we apply the",
    "prediction method to the samples of interest, since all noise samples are replaced with a sample of",
    "interest.",
    "In case of the empty subset (all are jointly randomize), it boils down to taking the average over all",
    "predictions, seeing that the randomization yields the same values for each sample of interest (none of the",
    "samples of interest are used to replace a (jointly) 'randomized' sample).",
    "Exact model",
    "This constant is used as key when storing/accessing models as causal mechanisms in graph node attributes",
    "This constant is used as key when storing the parents of a node during fitting. It's used for validation purposes",
    "afterwards.",
    "Todo: Remove after https://github.com/py-why/dowhy/pull/943.",
    "Smallest possible value. This is used in various algorithm for numerical stability.",
    "Note: The alpha level doesn't matter here.",
    "Make copy to avoid manipulating the original matrix.",
    "The given prediction method has to be evaluated multiple times on a large amount of different inputs. Typically,",
    "the batch evaluation of a prediction model on multiple inputs at the same time is significantly faster",
    "than evaluating it on single simples in a for-loop. To make use of this, we try to evaluate as many samples as",
    "possible in one batch call of the prediction method. However, this also requires a lot of memory for many samples.",
    "To overcome potential memory issues, multiple batch calls are performed, each with at most batch_size many",
    "samples. The number of samples that are evaluated is normally",
    "baseline_noise_samples.shape[0] * feature_samples.shape[0]. Here, we reduce it to",
    "batch_size * feature_samples.shape[0]. If the batch_size would be set 1, then each baseline_noise_samples is",
    "evaluated one by one in a for-loop.",
    "Each batch consist of at most batch_size * feature_samples.shape[0] many samples. If there are multiple",
    "batches, the offset indicates the index of the current baseline_noise_samples that has not been evaluated yet.",
    "If the batch size would be larger than the remaining amount of samples, it is reduced to only include the",
    "remaining baseline_noise_samples.",
    "The inputs consist of batch_size many copies of feature_samples. Here, we set the columns of the features",
    "in baseline_feature_indices to their respective values in baseline_noise_samples.",
    "After creating the (potentially large) input data matrix, we can evaluate the prediction method.",
    "Here, offset + index now indicates the sample index in baseline_noise_samples.",
    "This would average all prediction results obtained for the 'offset + index'-th sample in",
    "baseline_noise_samples. This is, y^(offset + index) = E[Y | do(x^(offset + index)_s)].",
    "This would return all prediction results obtained for the 'offset + index'-th sample in",
    "baseline_noise_samples, i.e. the results are not averaged.",
    "Making copy to ensure that the original object is not modified.",
    "Permute samples jointly. This still represents an interventional distribution.",
    "Permute samples independently.",
    "Looks for the first index where the cumulative sum of the probabilities is larger than the threshold.",
    "Note that if there are multiple indices with the same maximum value (as in this case here), the argmax",
    "function returns the first index.",
    "test local Markov condition, null hypothesis: conditional independence",
    "test edge dependence, null hypothesis: independence",
    "The order of the p-values added to the list is deterministic.",
    "To be able to validate that the graph structure did not change between fitting and causal query, we store the",
    "parents of a node during fit. That way, before sampling, we can verify the parents are still the same. While",
    "this would automatically fail when the number of parents is different, there are other more subtle cases,",
    "where the number is still the same, but it's different parents, and therefore different data. That would yield",
    "wrong results, but would not fail.",
    "Independence tests are symmetric",
    "Find out which tests to do",
    "Parallelize over tests",
    "Gather results",
    "Summarize",
    "Find out which tests to do",
    "Parallelize over tests",
    "Gather results",
    "Summarize",
    "Find out which tests to do",
    "Parallelize over tests",
    "Gather results",
    "Summarize",
    "DAG Evaluation",
    "Suggestions",
    "Append list of violations (node, non_desc) to get local information",
    "Plot histograms",
    "Plot given violations",
    "For LMC we highlight X for which X _|/|_ Y \\in ND_X | Pa_X",
    "For PD we highlight the edge (if Y\\in Anc_X -> X are adjacent)",
    "For causal minimality we highlight the edge Y \\in Pa_X -> X",
    "Create Validation header",
    "Create Validation summary",
    "Close Validation",
    "Create Suggestions header",
    "Iterate over suggestions",
    "Test if we have data for X and Y",
    "Test if we have data for Z",
    "Eq. (1) in https://arxiv.org/pdf/1810.11363.pdf",
    "Subtracting Y here since the cumulative sum includes the current element. The same reason we subtract 1",
    "from the count.",
    "Using the negative value here seeing that the Shapley estimation evaluates v(S u {i}) - v(S) for a subset S. In",
    "case of variance, we have v(S u {i}) <= v(S), which would result in a negative contribution of players to the",
    "target quantity (here, variance).",
    "Using the negative value here seeing that the Shapley estimation evaluates v(S u {i}) - v(S) for a subset S. In",
    "case of variance, we have v(S u {i}) <= v(S), which would result in a negative contribution of players to the",
    "target quantity (here, variance).",
    "Calculate Ri, the product of the residuals",
    "Standard deviation of the residuals",
    "Either X and/or Y is constant.",
    "If Z is empty, we are in the pairwise setting.",
    "Either X and/or Y is constant.",
    "If Z is empty, we are in the pairwise setting.",
    "Estimate test statistic multiple times on different permutations of the data. The p-value is then the",
    "probability (i.e. fraction) of obtaining a test statistic that is greater than statistic on the non-permuted",
    "data.",
    "Take the lower dimensional variable as target.",
    "First stage statistical model",
    "Second stage statistical model",
    "Check if the treatment is one-dimensional",
    "First stage",
    "Second Stage",
    "Combining the two estimates",
    "This same estimate is valid for frontdoor as well as mediation (NIE)",
    "Bulding the feature matrix",
    "features = sm.add_constant(features, has_constant='add') # to add an intercept term",
    "TODO move this to the identification step",
    "Obtain estimate by Wald Estimator",
    "Obtain estimate by 2SLS estimator: Cov(y,z) / Cov(x,z)",
    "More than 1 instrument. Use 2sls.",
    "Checking if Y is binary",
    "Enable the user to pass params for a custom propensity model",
    "Convert the categorical variables into dummy/indicator variables",
    "Basically, this gives a one hot encoding for each category",
    "The first category is taken to be the base line.",
    "Check if the treatment is one-dimensional",
    "Checking if the treatment is binary",
    "The model is always built on the entire data",
    "TODO make treatment_value and control value also as local parameters",
    "All treatments are set to the same constant value",
    "Fixing treatment value to the specified value, if provided",
    "treatment_vals and data_df should have same number of rows",
    "Bulding the feature matrix",
    "Replace treatment values with value supplied; note: Don't change column datatype!",
    "The model is always built on the entire data",
    "The average treatment effect is a combination of different",
    "regression coefficients. Complicated to compute the confidence",
    "interval analytically. For example, if y=a + b1.t + b2.tx, then",
    "the average treatment effect is b1+b2.mean(x).",
    "Refer Gelman, Hill. ARM Book. Chapter 9",
    "http://www.stat.columbia.edu/~gelman/arm/chap9.pdf",
    "TODO: Looking for contributions",
    "For a linear regression model, the causal effect of a variable is equal to the coefficient corresponding to the",
    "variable. Hence, the model by default outputs the confidence interval corresponding to treatment=1 and control=0.",
    "So for custom treatment and control values, we must multiply the confidence interval by the difference of the two.",
    "For a linear regression model, the causal effect of a variable is equal to the coefficient corresponding to the",
    "variable. Hence, the model by default outputs the standard error corresponding to treatment=1 and control=0.",
    "So for custom treatment and control values, we must multiply the standard error by the difference of the two.",
    "setting method-specific parameters",
    "Infer the right strata based on clipping threshold",
    "0.5 because there are two values for the treatment",
    "To be conservative and allow most strata to be included in the",
    "analysis",
    "At least 90% of the strata should be included in analysis",
    "sum weighted outcomes over all strata  (weight by treated population)",
    "TODO - how can we add additional information into the returned estimate?",
    "such as how much clipping was done, or per-strata info for debugging?",
    "sort the dataframe by propensity score",
    "create a column 'strata' for each element that marks what strata it belongs to",
    "for each strata, count how many treated and control units there are",
    "throw away strata that have insufficient treatment or control",
    "TODO -- fix: we are actually conditioning on positive treatment (d=1)",
    "Setting method specific parameters",
    "trim propensity score weights",
    "ips ==> (isTreated(y)/ps(y)) + ((1-isTreated(y))/(1-ps(y)))",
    "nips ==> ips / (sum of ips over all units)",
    "icps ==> ps(y)/(1-ps(y)) / (sum of (ps(y)/(1-ps(y))) over all control units)",
    "itps ==> ps(y)/(1-ps(y)) / (sum of (ps(y)/(1-ps(y))) over all treatment units)",
    "Vanilla IPS estimator",
    "The Hajek estimator (or the self-normalized estimator)",
    "Stabilized weights (from Robins, Hernan, Brumback (2000))",
    "Paper: Marginal Structural Models and Causal Inference in Epidemiology",
    "Calculating the effect",
    "Subtracting the weighted means",
    "TODO - how can we add additional information into the returned estimate?",
    "TODO -- fix: we are actually conditioning on positive treatment (d=1)",
    "Save parameters for later refutter fitting",
    "Enforcing this ordering is necessary to feed through the propensity values from dataset",
    "For metalearners only--issue a warning if w contains variables not in x",
    "Override the effect_modifiers set in CausalEstimator.__init__()",
    "Also only update self._effect_modifiers, and create a copy of self._effect_modifier_names",
    "the latter can be used by other estimator methods later",
    "Instrumental variables names, if present",
    "choosing the instrumental variable to use",
    "Calling the econml estimator's fit method",
    "As of v0.9, econml has some kewyord only arguments",
    "from https://www.bnmetrics.com/blog/factory-pattern-in-python3-simple-version",
    "Changing shape to a list for a singleton value",
    "Note that self._control_value is assumed to be a singleton value",
    "TODO -- fix: we are actually conditioning on positive treatment (d=1)",
    "For each unit, return the estimated effect of the treatment value",
    "that was actually applied to the unit",
    "this assumes a binary treatment regime",
    "TODO remove neighbors that are more than a given radius apart",
    "Estimating ATT on treated by summing over difference between matched neighbors",
    "Estimating ATC",
    "TODO -- fix: we are actually conditioning on positive treatment (d=1)",
    "from https://www.bnmetrics.com/blog/factory-pattern-in-python3-simple-version",
    "Handle externally provided estimator classes",
    "allowed types of distance metric",
    "Dictionary of any user-provided params for the distance metric",
    "that will be passed to sklearn nearestneighbors",
    "Check if the treatment is one-dimensional",
    "Checking if the treatment is binary",
    "Convert the categorical variables into dummy/indicator variables",
    "Basically, this gives a one hot encoding for each category",
    "The first category is taken to be the base line.",
    "this assumes a binary treatment regime",
    "TODO remove neighbors that are more than a given radius apart",
    "estimate ATT on treated by summing over difference between matched neighbors",
    "Return indices in the original dataframe",
    "self.matched_indices_att[treated_df_index[i]] = control.iloc[indices[i]].index.tolist()",
    "Now computing ATC",
    "Return indices in the original dataframe",
    "Add the identification method used in the estimator",
    "Check the backdoor variables being used",
    "Add the observed confounders and one hot encode the categorical variables",
    "Get the data of the unobserved confounders",
    "One hot encode the data if they are categorical",
    "Check the instrumental variables involved",
    "Perform the same actions as the above",
    "Both the outcome and the treatment have to be 1D arrays according to the CausalML API",
    "We want to pass 'v0' rather than ['v0'] to prevent a shape mismatch",
    "For CATEs",
    "TODO we are conditioning on a postive treatment",
    "TODO create an expression corresponding to each estimator used",
    "Determine columns being encoded",
    "If all columns are already numerical, there may be nothing to encode.",
    "In this case, return original data.",
    "Columns to keep in the result - not encoded.",
    "Convert the encoded data to a DataFrame",
    "Concatenate the encoded DataFrame with the original non-categorical columns",
    "Remember encoder",
    "variable_type:",
    "A dictionary containing the variable's names and types. 'c' for continuous, 'o'",
    "for ordered, 'd' for discrete, and 'u' for unordered discrete.",
    "[] notation to retain DataFrame rather than Series.",
    "For one_hot_encode type must be categorical, or it won't encode.",
    "Generating data with equal 0 and 1 (since ranks are uniformly distributed)",
    "Flipping some values",
    "Check if the edge already exists",
    "If the edge exists, append the time_lag to the existing tuple",
    "If the edge does not exist, create a new edge with a tuple containing the time_lag",
    "Read the CSV file into a DataFrame",
    "Initialize an empty directed graph",
    "Add edges with time lag to the graph",
    "Add validation for the time lag column to be a number",
    "Check if the edge already exists",
    "If the edge exists, append the time_lag to the existing tuple",
    "If the edge does not exist, create a new edge with a tuple containing the time_lag",
    "Read the DOT file into a MultiDiGraph",
    "Initialize a new DiGraph",
    "Iterate over edges of the MultiDiGraph",
    "Convert the label to a tuple of time lags",
    "Merge the existing time lags with the new ones",
    "Remove duplicates by converting to a set and back to a tuple",
    "Initialize a directed graph",
    "Add nodes with names",
    "Iterate over all pairs of nodes",
    "Check for directed links",
    "Append the time lag to the existing tuple",
    "Create a new edge with a tuple containing the time lag",
    "Append the time lag to the existing tuple",
    "Create a new edge with a tuple containing the time lag",
    "Wrapping labels if they are too long",
    "This layout tries to mimic the graphviz layout in a simpler form. The depth grows horizontally here instead of",
    "vertically.",
    "Set the figure size based on the number of nodes",
    "Nodes that are vertically connected, but not neighbors should be connected via a curved edge.",
    "All other nodes should be connected with a straight line.",
    "Draw labels node labels",
    "Each node gets a depth assigned, based on the distance to the closest root node.",
    "In case of undirected graphs, we just take any node as root node.",
    "No path to root node, ignore this connection then.",
    "Counts the number of vertical nodes in the same layers.",
    "Creates a matrix indicating whether two nodes are vertical neighbors.",
    "Get all y coordinates per layer",
    "Sort the y-coordinates",
    "Finding p-value using student T test",
    "Only consider edges have absolute edge weight > 0.01",
    "Modify graph such that it only contains bidirected edges",
    "Find c components by finding connected components on the undirected graph",
    "Understanding Neural Network weights",
    "Refer to this link:https://stackoverflow.com/questions/50937628/mlp-classifier-neurons-weights",
    "add weight column",
    "before weights are applied we count number rows in each category",
    "which is equivalent to summing over weight=1",
    "after weights are applied we need to sum over the given weights",
    "First, calculating mean differences by strata",
    "Second, without strata",
    "Third, concatenating them and plotting",
    "Setting estimator attribute for convenience",
    "Outcome is numeric",
    "Treatments are also numeric or binary",
    "Outcome is categorical",
    "Treatments are numeric or binary",
    "TODO: A common way to show all plots",
    "from https://www.bnmetrics.com/blog/factory-pattern-in-python3-simple-version",
    "Get adjacency list",
    "If node pair has been fully explored",
    "Add node1 to backdoor set of node_pair",
    "Check if path is backdoor and does not have nodes1\\node1 or nodes2\\node2 as intermediate nodes",
    "True if arrow incoming, False if arrow outgoing",
    "Mark pair (node1, node2) complete",
    "Modify variable count and indices covered",
    "Average total effect",
    "Natural direct effect",
    "Natural indirect effect",
    "Controlled direct effect",
    "Backdoor method names",
    "First, check if there is a directed path from action to outcome",
    "## 1. BACKDOOR IDENTIFICATION",
    "Pick algorithm to compute backdoor sets according to method chosen",
    "First, checking if there are any valid backdoor adjustment sets",
    "Setting default \"backdoor\" identification adjustment set",
    "## 2. INSTRUMENTAL VARIABLE IDENTIFICATION",
    "Now checking if there is also a valid iv estimand",
    "## 3. FRONTDOOR IDENTIFICATION",
    "Now checking if there is a valid frontdoor variable",
    "Finally returning the estimand object",
    "Pick algorithm to compute backdoor sets according to method chosen",
    "Setting default \"backdoor\" identification adjustment set",
    "Finally returning the estimand object",
    "## 1. FIRST DOING BACKDOOR IDENTIFICATION",
    "First, checking if there are any valid backdoor adjustment sets",
    "Setting default \"backdoor\" identification adjustment set",
    "## 2. SECOND, CHECKING FOR MEDIATORS",
    "Now checking if there are valid mediator variables",
    "Finally returning the estimand object",
    "## 1. FIRST DOING BACKDOOR IDENTIFICATION",
    "First, checking if there are any valid backdoor adjustment sets",
    "Setting default \"backdoor\" identification adjustment set",
    "## 2. SECOND, CHECKING FOR MEDIATORS",
    "Now checking if there are valid mediator variables",
    "Finally returning the estimand object",
    "First, checking if empty set is a valid backdoor set",
    "If the method is `minimal-adjustment`, return the empty set right away.",
    "Second, checking for all other sets of variables. If include_unobserved is false, then only observed variables are eligible.",
    "only remove descendants of Y",
    "also allow any causes of Y that are not caused by T (for lower variance)",
    "remove descendants of T (mediators) and descendants of Y",
    "If var is d-separated from both treatment or outcome, it cannot",
    "be a part of the backdoor set",
    "repeat the above search with BACKDOOR_MIN",
    "If `minimal-adjustment` method is specified, start the search from the set with minimum size. Otherwise, start from the largest.",
    "If the backdoor method is `maximal-adjustment` or `minimal-adjustment`, return the first found adjustment set.",
    "If all variables are observed, and the biggest eligible set",
    "does not satisfy backdoor, then none of its subsets will.",
    "Adding a None estimand if no backdoor set found",
    "Default set contains minimum possible number of instrumental variables, to prevent lowering variance in the treatment variable.",
    "Default set is the one with the least number of adjustment variables (optimizing for efficiency)",
    "Cond 1: All directed paths intercepted by candidate_var",
    "Cond 2: No confounding between treatment and candidate var",
    "Cond 3: treatment blocks all confounding between candidate_var and outcome",
    "For simplicity, assuming a one-variable mediation set",
    "Create estimands dict as per the API for backdoor, but do not return it",
    "Setting default \"backdoor\" identification adjustment set",
    "Create estimands dict as per the API for backdoor, but do not return it",
    "Setting default \"backdoor\" identification adjustment set",
    "TODO: outputs string for now, but ideally should do symbolic",
    "expressions Mon 19 Feb 2018 04:54:17 PM DST",
    "TODO Better support for multivariate treatments",
    "TODO: support multivariate treatments better.",
    "TODO: support multivariate treatments better.",
    "TODO: support multivariate treatments better.",
    "For direct effect",
    "If no costs are passed, use uniform costs",
    "restriction to ancestors",
    "back-door graph",
    "moralization",
    "Estimators list for returning after identification",
    "Line 1",
    "If no action has been taken, the effect on Y is just the marginal of the observational distribution P(v) on Y.",
    "Line 2",
    "If we are interested in the effect on Y, it is sufficient to restrict our attention on the parts of the model ancestral to Y.",
    "Modify list of valid nodes",
    "Line 3 - forces an action on any node where such an action would have no effect on Y \u2013 assuming we already acted on X.",
    "Modify adjacency matrix to obtain that corresponding to do(X)",
    "Line 4 - Decomposes the problem into a set of smaller problems using the key property of C-component factorization of causal models.",
    "If the entire graph is a single C-component already, further problem decomposition is impossible, and we must provide base cases.",
    "Modify adjacency matrix to remove treatment variables",
    "Line 5 - The algorithms fails due to the presence of a hedge - the graph G, and a subgraph S that does not contain any X nodes.",
    "Line 6 - If there are no bidirected arcs from X to the other nodes in the current subproblem under consideration, then we can replace acting on X by conditioning, and thus solve the subproblem.",
    "Line 7 - This is the most complicated case in the algorithm. Explain in the second last paragraph on Pg 41 of the link provided in the docstring above.",
    "Do not show backdoor key unless it is the only backdoor set.",
    "Just show the default backdoor set",
    "Ensure parent_time_lag is in tuple form",
    "Find or create the lagged node for the current node",
    "For each lagged node, create new time-lagged parent nodes and add edges",
    "Add the parent to the queue for further exploration",
    "append the lagged nodes",
    "If labels provided",
    "Return in valid DOT format",
    "Get adjacency matrix",
    "If labels not provided",
    "Obtain valid DOT format",
    "If labels provided",
    "Return in valid DOT format",
    "from https://www.bnmetrics.com/blog/factory-pattern-in-python3-simple-version",
    "from https://www.bnmetrics.com/blog/factory-pattern-in-python3-simple-version"
  ],
  "v0.11.1": [
    "-*- coding: utf-8 -*-",
    "",
    "Configuration file for the Sphinx documentation builder.",
    "",
    "This file does only contain a selection of the most common options. For a",
    "full list see the documentation:",
    "http://www.sphinx-doc.org/en/stable/config",
    "-- Path setup --------------------------------------------------------------",
    "If extensions (or modules to document with autodoc) are in another directory,",
    "add these directories to sys.path here. If the directory is relative to the",
    "documentation root, use os.path.abspath to make it absolute, like shown here.",
    "",
    "-- Project information -----------------------------------------------------",
    "Version Information (for version-switcher)",
    "-- General configuration ---------------------------------------------------",
    "If your documentation needs a minimal Sphinx version, state it here.",
    "",
    "needs_sphinx = '1.0'",
    "Add any Sphinx extension module names here, as strings. They can be",
    "extensions coming with Sphinx (named 'sphinx.ext.*') or your custom",
    "ones.",
    "sphinx-panels shouldn't add bootstrap css since the pydata-sphinx-theme",
    "already loads it",
    "Add any paths that contain templates here, relative to this directory.",
    "The suffix(es) of source filenames.",
    "You can specify multiple suffix as a list of string:",
    "",
    "source_suffix = ['.rst', '.md']",
    "The master toctree document.",
    "The language for content autogenerated by Sphinx. Refer to documentation",
    "for a list of supported languages.",
    "",
    "This is also used if you do content translation via gettext catalogs.",
    "Usually you set \"language\" from the command line for these cases.",
    "List of patterns, relative to source directory, that match files and",
    "directories to ignore when looking for source files.",
    "This pattern also affects html_static_path and html_extra_path .",
    "The name of the Pygments (syntax highlighting) style to use.",
    "-- Options for HTML output -------------------------------------------------",
    "The theme to use for HTML and HTML Help pages.  See the documentation for",
    "a list of builtin themes.",
    "",
    "Theme options are theme-specific and customize the look and feel of a theme",
    "further.  For a list of options available for each theme, see the",
    "documentation.",
    "",
    "Add any paths that contain custom static files (such as style sheets) here,",
    "relative to this directory. They are copied after the builtin static files,",
    "so a file named \"default.css\" will overwrite the builtin \"default.css\".",
    "Custom sidebar templates, must be a dictionary that maps document names",
    "to template names.",
    "",
    "The default sidebars (for documents that don't match any pattern) are",
    "defined by theme itself.  Builtin themes are using these templates by",
    "default: ``['localtoc.html', 'relations.html', 'sourcelink.html',",
    "'searchbox.html']``.",
    "",
    "html_sidebars = {}",
    "-- Options for HTMLHelp output ---------------------------------------------",
    "Output file base name for HTML help builder.",
    "-- Options for LaTeX output ------------------------------------------------",
    "The paper size ('letterpaper' or 'a4paper').",
    "",
    "'papersize': 'letterpaper',",
    "The font size ('10pt', '11pt' or '12pt').",
    "",
    "'pointsize': '10pt',",
    "Additional stuff for the LaTeX preamble.",
    "",
    "'preamble': '',",
    "Latex figure (float) alignment",
    "",
    "'figure_align': 'htbp',",
    "Grouping the document tree into LaTeX files. List of tuples",
    "(source start file, target name, title,",
    "author, documentclass [howto, manual, or own class]).",
    "-- Options for manual page output ------------------------------------------",
    "One entry per manual page. List of tuples",
    "(source start file, name, description, authors, manual section).",
    "-- Options for Texinfo output ----------------------------------------------",
    "Grouping the document tree into Texinfo files. List of tuples",
    "(source start file, target name, title, author,",
    "dir menu entry, description, category)",
    "-- Options for Epub output -------------------------------------------------",
    "Bibliographic Dublin Core info.",
    "The unique identifier of the text. This can be a ISBN number",
    "or the project homepage.",
    "",
    "epub_identifier = ''",
    "A unique identification for the text.",
    "",
    "epub_uid = ''",
    "A list of files that should not be packed into the epub file.",
    "-- Extension configuration -------------------------------------------------",
    "-- Options for todo extension ----------------------------------------------",
    "If true, `todo` and `todoList` produce output, else they produce nothing.",
    "init docstrings should also be included in class",
    "Only uncomment for faster testing/building docs without compiling notebooks",
    "nbsphinx_execute = \"never\"",
    "Patch all of the published versions",
    "check old RST version (<= v0.8)",
    "Remove old version links",
    "Append updated version links",
    "requires stdin input for identify in weighting sampler",
    "will be removed",
    "applied notebook, not necessary to test each time",
    "needs xgboost too",
    "Slow Notebooks",
    "TODO: should probably move more notebooks here to ignore, because",
    "most get tested by the documentation generation.",
    "Adding the dowhy root folder to the python path so that jupyter notebooks",
    "can import dowhy",
    "\"--ExecutePreprocessor.timeout=600\",",
    "self.df = pd.read_csv(os.path.join(DATA_PATH,'dgp_1/acic_1_1_data.csv'))",
    "self.ate = np.mean(self.df['y1'] - self.df['y0'])",
    "treated = self.df[self.df['z']==1]",
    "self.att = np.mean(treated['y1'] - treated['y0'])",
    "def test_average_treatment_effect(self):",
    "est_ate = 1",
    "bias = est_ate - self.ate",
    "print(bias)",
    "self.assertAlmostEqual(self.ate, est_ate)",
    "def test_average_treatment_effect_on_treated(self):",
    "est_att = 1",
    "self.att=1",
    "bias = est_att - self.att",
    "print(bias)",
    "self.assertAlmostEqual(self.att, est_att)",
    "removing two common causes",
    "removing two common causes",
    "removing two common causes",
    "removing two common causes",
    "removing two common causes",
    "Remove graph variable with name \"W0\" from observed data.",
    "Ensure that a log record exists that provides a more detailed view",
    "of observed and unobserved graph variables (counts and variable names.)",
    "Default == operator tests if same object. If same object, don't need to check type.",
    "num_frontdoor_variables=1,",
    "creating nx graph instance",
    "to be used later for a test. Does not include the replace operation",
    "check if all partial R^2 values are between 0 and 1",
    "We calculate adjusted estimates for two sets of partial R^2 values.",
    "Test if hypothetical confounding by unobserved confounder u1 leads to an adjusted effect that is farther from the original estimate as compared to u2",
    "Creating a model with no unobserved confounders",
    "check if all partial R^2 values are between 0 and 1",
    "for a dataset with no confounders, the robustness value should be higher than a given threshold (0.95 in our case)",
    "Non Parametric estimator",
    "We calculate adjusted estimates for two sets of partial R^2 values.",
    "adjusted lower ate bound for confounder u1 where r2tu_w = 0.7 and r2yu_tw = 0.9",
    "adjusted lower ate bound for confounder u2 where r2tu_w = 0.2 and r2yu_tw = 0.3",
    "adjusted upper ate bound for confounder u1 where r2tu_w = 0.7 and r2yu_tw = 0.9",
    "adjusted upper ate bound for confounder u2 where r2tu_w = 0.2 and r2yu_tw = 0.3",
    "adjusted lower confidence bound for confounder u1 where r2tu_w = 0.7 and r2yu_tw = 0.9",
    "adjusted lower confidence bound for confounder u2 where r2tu_w = 0.2 and r2yu_tw = 0.3",
    "adjusted upper confidence bound for confounder u1 where r2tu_w = 0.7 and r2yu_tw = 0.9",
    "adjusted upper confidence bound for confounder u2 where r2tu_w = 0.2 and r2yu_tw = 0.3",
    "Test if hypothetical confounding by unobserved confounder u1 leads to an adjusted effect that is farther from the original estimate as compared to u2",
    "we patched figure plotting call to avoid drawing plots during tests",
    "We calculate adjusted estimates for two sets of partial R^2 values.",
    "adjusted lower ate bound for confounder u1 where r2tu_w = 0.7 and r2yu_tw = 0.9",
    "adjusted lower ate bound for confounder u2 where r2tu_w = 0.2 and r2yu_tw = 0.3",
    "adjusted upper ate bound for confounder u1 where r2tu_w = 0.7 and r2yu_tw = 0.9",
    "adjusted upper ate bound for confounder u2 where r2tu_w = 0.2 and r2yu_tw = 0.3",
    "adjusted lower confidence bound for confounder u1 where r2tu_w = 0.7 and r2yu_tw = 0.9",
    "adjusted lower confidence bound for confounder u2 where r2tu_w = 0.2 and r2yu_tw = 0.3",
    "adjusted upper confidence bound for confounder u1 where r2tu_w = 0.7 and r2yu_tw = 0.9",
    "adjusted upper confidence bound for confounder u2 where r2tu_w = 0.2 and r2yu_tw = 0.3",
    "Test if hypothetical confounding by unobserved confounder u1 leads to an adjusted effect that is farther from the original estimate as compared to u2",
    "we patched figure plotting call to avoid drawing plots during tests",
    "comparing test examples from R E-Value package",
    "check implementation of Observed Covariate E-value against R package",
    "The outcome is a linear function of the confounder",
    "The slope is 1,2 and the intercept is 3",
    "As we run with only one common cause and one instrument variable we run with (?, 2)",
    "TODO: Check directly for correct behavior, rather than checking the rules",
    "themselves, which can be non-deterministic (all the following are equivalent)",
    "Supports user-provided dataset object",
    "To test if there are any exceptions",
    "To test if the estimate is identical if refutation parameters are zero",
    "This value is hardcoded to be zero as we are runnning this on a linear dataset.",
    "Ordinarily, we should expect this value to be zero.",
    "This value is hardcoded to be zero as we are runnning this on a linear dataset.",
    "Ordinarily, we should expect this value to be zero.",
    "Only P(Y|T) should be present for test to succeed.",
    "Since undirected graph, identify effect must throw an error.",
    "Compare with ground truth",
    "Compare with ground truth",
    "Compare with ground truth",
    "Compare with ground truth",
    "Causal model initialization",
    "Obtain backdoor sets",
    "Check if backdoor sets are valid i.e. if they block all paths between the treatment and the outcome",
    "Causal model initialization",
    "Obtain backdoor sets",
    "Causal model initialization",
    "Obtain backdoor sets",
    "Check if backdoor sets are valid i.e. if they block all paths between the treatment and the outcome",
    "Causal model initialization",
    "Obtain backdoor sets",
    "Check if backdoor sets are valid i.e. if they block all paths between the treatment and the outcome",
    "Causal model initialization",
    "Obtain backdoor sets",
    "Check if backdoor sets are valid i.e. if they block all paths between the treatment and the outcome",
    "Causal model initialization",
    "Obtain backdoor sets",
    "Check if backdoor sets are valid i.e. if they block all paths between the treatment and the outcome",
    "Example is selected from Pearl J. \"Causality\" 2nd Edition, from chapter 3.3.1 on backoor criterion.",
    "The following simpsons paradox examples are taken from Pearl, J {2013}. \"Understanding Simpson\u2019s Paradox\" - http://ftp.cs.ucla.edu/pub/stat_ser/r414.pdf",
    "The following are examples given in the \"Book of Why\" by Judea Pearl, chapter \"The Do-operator and the Back-Door Criterion\"",
    "Building the causal model",
    "For all examples from these papers we use X for the treatment variable",
    "instead of A.",
    "Figure 6 from Smucler, Sapienza and Rotnitzky (2021), Biometrika",
    "Figure 5 from Smucler, Sapienza and Rotnitzky (2021), Biometrika",
    "Figure 4 from Smucler, Sapienza and Rotnitzky (2021), Biometrika",
    "Figure 3 from Smucler, Sapienza and Rotnitzky (2021), Biometrika",
    "Figure 2 from Smucler and Rotnitzky (2022), Journal of Causal Inference",
    "L replaces X as the conditional variable",
    "Figure 2 from Smucler and Rotnitzky (2022), Journal of Causal Inference",
    "L replaces X as the conditional variable. Uses different costs",
    "Figure 3 from Smucler and Rotnitzky (2022), Journal of Causal Inference",
    "A graph where optimal, optimal minimal and optimal min cost are different",
    "The graph from Shrier and Platt (2008)",
    "A graph for which the algorithm was producing wrong result due to a bug reported by Sara Taheri",
    "Another graph for which the algorithm was producing wrong result due to a bug reported by Sara Taheri",
    "cov_mat = np.diag(np.ones(num_features))",
    "collider: X->Z<-Y",
    "chain: X->Z->Y",
    "fork: X<-Z->Y",
    "general DAG: X<-Z->Y, X->Y",
    "fork: X<-Z->Y",
    "Only Gaussian component changed",
    "Just checking formats, i.e. no need for correlation.",
    "Just checking formats, i.e. no need for correlation.",
    "The MSE should be 1 due to the variance of the noise. The RMSE is accordingly 1 / var(Y).",
    "Contributions should add up to Var(X2)",
    "H(P(Y)) -- Can be precomputed",
    "-(H(P(Y | do(x_S)) - H(P(Y))) = H(P(Y)) - H(P(Y | do(x_S))",
    "H(P(Y | do(x_S)) = H(E[P(Y | x_S, X'_\\S)])",
    "E[P(Y | x_S, X'_\\S)]",
    "H(E[P(Y | x_S, X'_\\S)])",
    "Using H(P(Y)) based on the origina data, i.e. ignoring baseline_predictions.",
    "E[H(P(Y)) - H(P(Y | do(X_U))] = H(P(Y)) - E[H(P(Y | X))]",
    "Mean from the categorical part is: (-5 + 5+ 10) / 3 = 10/3",
    "Having at least one sample from the second class should not raise an error.",
    "Just some random data, since we are only interested in the omitted data.",
    "This caused an error before with pandas > 2.0",
    "C2 = 3 * A2 + 2 * B2",
    "By default, the strength is measure with respect to the variance.",
    "Here, we misspecified the mechanism on purpose by setting scale to 1 instead of 2.",
    "If we provide the observational data here, we can mitigate the misspecification of the causal mechanism.",
    "Missing connection between X0 and X1.",
    "For X0 and X1, we set the ground truth noise to further emphasize the misspecification. The inferred noise of X1",
    "would otherwise have a dependency with Z due to the missing connection with X0.",
    "Modelling connection between X0 and X1 explicitly.",
    "Here, we misspecify the mechanism on purpose by setting scale to 1 instead of 2.",
    "If we provide the observational data here, we can mitigate the misspecification of the causal mechanism.",
    "Here, changing the mechanism.",
    "Here, changing the mechanism.",
    "Here, changing the mechanism.",
    "Here, changing the mechanism.",
    "Defining an anomaly scorer that handles multidimensional inputs.",
    "Seeing that the expectation of the noise in all nodes is 0, we introduce anomalies by setting some of them to 3.",
    "In the first sample, only the first variable is anomalous. Therefore, it should have the highest contribution",
    "and it should be \"significantly\" higher than the contribution of the other ones (here, we just arbitrarily say",
    "it should be 3x higher. Due to the confounding factor Z, the reconstructed noise variables are pairwise dependent,",
    "which is a violation of our causal sufficiency assumption. However, a confounder is included here to demonstrate",
    "some robustness. Note that due to this and stochastic behaviour of the density estimator, it is",
    "not possible to analytically compute expected results. Therefore, we rather look at the relations here.",
    "Same idea for the second sample, but here, it is the second variable that is anomalous.",
    "In the fourth sample, there are 2 anomalous variables. Therefore, the contribution of these 2 variables should be",
    "\"significantly\" higher than the contribution of the other variables. The contribution of both anomalous variables",
    "should be equal (approximately).",
    "Defining an anomaly scorer that handles multidimensional inputs.",
    "Seeing that the expectation of the noise in all nodes is 0, we introduce anomalies by setting some of them to 3.",
    "reduce the score.",
    "The contributions should add up to g(x) - E[g(X)]",
    "The contributions should add up to g(x) - E[g(X)]",
    "The contributions should add up to g(x) - E[g(X)]",
    "Three examples:",
    "1. X1 is the root cause (+ 10 to the noise)",
    "2. X0 is the root cause (+ 10 to the noise)",
    "3. X0 and X3 are both root causes (+ 10 to both noises)",
    "The sum of the scores should add up to the anomaly score of the target (here, X3).",
    "Three examples:",
    "1. X1 is the root cause (+ 10 to the noise)",
    "2. X0 is the root cause (+ 10 to the noise)",
    "3. X0 and X3 are both root causes (+ 10 to both noises)",
    "The sum of the scores should add up to the anomaly score of the target (here, X3).",
    "Defining ground truth model to avoid SCM learning issues and, hence, to focus on the anomaly attribution",
    "algorithm.",
    "1. X0 is the root cause (+ 10 to the noise)",
    "2. X0 and X1 are the root causes (+ 10 to both noise)",
    "3. X2 and X3 are both root causes (+ 10 to both noises)",
    "The sum of the scores should add up to the anomaly score of the target (here, X3).",
    "collider: X->Z<-Y",
    "collider: X->Z<-Y",
    "chain: X->Y->Z",
    "chain: X->Y->Z",
    "Empty graph",
    "Full DAG",
    "DAG with single node",
    "DAG with single edge",
    "DAG with single edge",
    "chain: X->Z->Y",
    "Setup data",
    "Test LinearDML",
    "Checking that the CATE estimates are not identical",
    "Test ContinuousTreatmentOrthoForest",
    "Checking that the CATE estimates are not identical",
    "Test LinearDRLearner",
    "Test LinearDML",
    "checking that CATE estimates are not identical",
    "predict on new data",
    "Setup data",
    "Test DeepIV",
    "Test IntentToTreatDRIV",
    "Observed data",
    "assumed graph",
    "Identify effect",
    "Estimate effect",
    "A model where X is also a common cause",
    "A model where X is also a common cause",
    "The case where effect modifier is not a common cause",
    "A model where X is also a common cause",
    "Create the graph describing the causal structure",
    "Generate the data",
    "Data to df",
    "Create a model",
    "Estimate the effect with front-door",
    "Not using testsuite from .base/TestEstimtor, custom code below",
    "More cases where Exception  is expected",
    "Compute confidence intervals, standard error and significance tests",
    "Defined a linear dataset with a given set of properties",
    "Create a model that captures the same",
    "Identify the effects within the model",
    "Defined a linear dataset with a given set of properties",
    "Create a model that captures the same",
    "Identify the effects within the model",
    "Defined a linear dataset with a given set of properties",
    "Create a model that captures the same",
    "Identify the effects within the model",
    "Defined a linear dataset with a given set of properties",
    "Create a model that captures the same",
    "Identify the effects within the model",
    "Defined a linear dataset with a given set of properties",
    "Create a model that captures the same",
    "Identify the effects within the model",
    "Use a mix of already-numeric and requires encoding cols:",
    "NB There may be small differences in type but since all values will be used in models as float,",
    "comparison is done as this type.",
    "Check same rows",
    "Check same number of cols",
    "Check values",
    "Calculate the sum of absolute differences between the two DataFrames",
    "- should be zero (excl. floating point error)",
    "Use a mix of already-numeric and requires encoding cols:",
    "Initial encode",
    "Create new data with permuted rows.",
    "Output shape should be unchanged.",
    "Encode this new data.",
    "Check same rows",
    "Check same number of cols",
    "Check permuted values are consistent",
    "Check if calling the method causes some import or runtime errors",
    "TODO: Plotting undirected graphs with networkx causes an error when an older networkx version is used with a newer",
    "newer matplotlib version:",
    "AttributeError: module 'matplotlib.cbook' has no attribute 'is_numlike'",
    "Networkx 2.4+ should fix this issue.",
    "plot_adjacency_matrix(causal_graph, is_directed=False)",
    "Can use these lists to specify the models/estimators/refuters that a particular interpreter supports.  Throw a ValueError if the user provides an incompatible object to intepret.",
    "Unpacking the keyword arguments",
    "todo: add docstring for common parameters here and remove from child refuter classes",
    "Default value for the number of simulations to be conducted",
    "joblib params for parallel processing",
    "Concatenate the confounders, instruments and effect modifiers",
    "Shuffle the confounders",
    "Check if all are select or deselect variables",
    "Check if all the required_variables belong to confounders, instrumental variables or effect",
    "This calculates a two-sided percentile p-value",
    "See footnotes in https://journals.sagepub.com/doi/full/10.1177/2515245920911881",
    "Get the mean for the simulations",
    "Get the standard deviation for the simulations",
    "Get the Z Score [(val - mean)/ std_dev ]",
    "Initializing the p_value",
    "Perform Bootstrap Significance Test with the original estimate and the set of refutations",
    "Perform Normal Tests of Significance with the original estimate and the set of refutations",
    "Perform Bootstrap Significance Test with the original estimate and the set of refutations",
    "Perform Normal Tests of Significance with the original estimate and the set of refutations",
    "re.sub only takes string parameter so the first if is to avoid error",
    "if the input is a text file, convert the contained data into string",
    "load dot file",
    "TODO This functionality needs to be deprecated. It is a convenience function but can introduce confusion",
    "as we are now including the option to initialize CausalGraph with DiGraph or GCM model.",
    "Adding node attributes",
    "adding penwidth to make the edge bold",
    "Adding common causes",
    "Adding instruments",
    "Adding effect modifiers",
    "Assuming the simple form of effect modifier",
    "that directly causes the outcome.",
    "self._graph.add_edge(node_name, outcome, style = \"dotted\", headport=\"s\", tailport=\"n\")",
    "self._graph.add_edge(outcome, node_name, style = \"dotted\", headport=\"n\", tailport=\"s\") # TODO make the ports more general so that they apply not just to top-bottom node configurations",
    "Adding columns in the dataframe as confounders that were not in the graph",
    "Adding unobserved confounders",
    "removal of only direct edges wrt a target is not implemented for incoming edges",
    "also return the number of backdoor paths blocked by observed nodes",
    "Assume that nodes1 is the treatment",
    "ignores new_graph parameter, always uses self._graph",
    "remove paths that have nodes1\\node1 or nodes2\\node2 as intermediate nodes",
    "TODO Refactor to remove this from here and only implement this logic in causalIdentifier. Unnecessary assumption of nodes1 to be causing nodes2.",
    "Cannot simply compute ancestors, since that will also include nodes1 and its parents (e.g. instruments)",
    "Return effect modifiers according to the graph",
    "removing all mediators",
    "Also add any effect modifiers that could not be auto-detected (e.g., they are also common causes)",
    "[TODO: double check these work with multivariate implementation:]",
    "Exclusion",
    "As-if-random setup",
    "As-if-random",
    "convert the outputted generator into a list",
    "dpaths = self.get_all_directed_paths(nodes1, nodes2)",
    "return len(dpaths) > 0",
    "Condition 1: node 1 ---> node 2 is intercepted by candidate_nodes",
    "Emit a `UserWarning` if there are any unobserved graph variables and",
    "and log a message highlighting data variables that are not part of the graph.",
    "Create causal graph object",
    "Sometimes, effect modifiers from the graph may not match those provided by the user.",
    "(Because some effect modifiers may also be common causes)",
    "In such cases, the user-provided modifiers are used.",
    "If no effect modifiers are provided,  then the ones from the graph are used.",
    "Import causal discovery class",
    "Initialize causal graph object",
    "TODO add propensity score as default backdoor method, iv as default iv method, add an informational message to show which method has been selected.",
    "TODO add dowhy as a prefix to all dowhy estimators",
    "This is done as all dowhy estimators have two parts and external ones have two or more parts",
    "Define the third-party estimation method to be used",
    "Process the dowhy estimators",
    "If not fit_estimator, attempt to retrieve existing estimator.",
    "Keep original behaviour to create new estimator if none found.",
    "Check if estimator's target estimand is identified",
    "Note that while the name of the variable is the same,",
    "\"self.causal_estimator\", this estimator takes in less",
    "parameters than the same from the",
    "estimate_effect code. It is not advisable to use the",
    "estimator from this function to call estimate_effect",
    "with fit_estimator=False.",
    "Estimator had been computed in a previous call",
    "The default number of simulations for statistical testing",
    "The default number of simulations to obtain confidence intervals",
    "This should be at least 399 for a 5% error rate:",
    "https://www.econstor.eu/bitstream/10419/67820/1/587473266.pdf",
    "The portion of the total size that should be taken each time to find the confidence intervals",
    "1 is the recommended value",
    "https://ocw.mit.edu/courses/mathematics/18-05-introduction-to-probability-and-statistics-spring-2014/readings/MIT18_05S14_Reading24.pdf",
    "https://projecteuclid.org/download/pdf_1/euclid.ss/1032280214",
    "The default Confidence Level",
    "Number of quantiles to discretize continuous columns, for applying groupby",
    "Prefix to add to temporary categorical variables created after discretization",
    "std args to be removed from locals() before being passed to args_dict",
    "Setting the default interpret method",
    "Check if some parameters were set, otherwise set to default values",
    "Estimate conditional estimates by default",
    "TODO Only works for binary treatment",
    "Defaulting to class default values if parameters are not provided",
    "Checking that there is at least one effect modifier",
    "Making sure that effect_modifier_names is a list",
    "Making a copy since we are going to be changing effect modifier names",
    "For every numeric effect modifier, adding a temp categorical column",
    "Grouping by effect modifiers and computing effect separately",
    "Deleting the temporary categorical columns",
    "The array that stores the results of all estimations",
    "Find the sample size the proportion with the population size",
    "Perform the set number of simulations",
    "names of treatment and outcome",
    "Using class default parameters if not specified",
    "Checking if bootstrap_estimates are already computed",
    "Checked if any parameter is changed from the previous std error estimate",
    "Now use the data obtained from the simulations to get the value of the confidence estimates",
    "Get the variations of each bootstrap estimate and sort",
    "Now we take the (1-p)/2 th and the 1-(1-p)/2 th variations, where p is the chosen confidence level",
    "Get the lower and upper bounds by subtracting the variations from the estimate",
    "Use existing params, if new user defined params are not present",
    "Checking if bootstrap_estimates are already computed",
    "Check if any parameter is changed from the previous std error estimate",
    "Use existing params, if new user defined params are not present",
    "Processing the null hypothesis estimates",
    "Doing a two-sided test",
    "Being conservative with the p-value reported",
    "Being conservative with the p-value reported",
    "If the estimate_index is 0, it depends on the number of simulations",
    "Need to test r-squared before supporting",
    "effect_r_squared = self._evaluate_effect_strength(estimate, method=\"r-squared\")",
    "'r-squared': effect_r_squared",
    "elif method == \"r-squared\":",
    "outcome_mean = np.mean(self._outcome)",
    "total_variance = np.sum(np.square(self._outcome - outcome_mean))",
    "Assuming a linear model with one variable: the treatment",
    "Currently only works for continuous y",
    "causal_model = outcome_mean + estimate.value*self._treatment",
    "squared_residual = np.sum(np.square(self._outcome - causal_model))",
    "r_squared = 1 - (squared_residual/total_variance)",
    "return r_squared",
    "Check if estimator's target estimand is identified",
    "Store parameters inside estimate object for refutation methods",
    "TODO: This add_params needs to move to the estimator class",
    "inside estimate_effect and estimate_conditional_effect",
    "TODO: Remove _data, _treatment_name and _outcome_name from this object",
    "we save them here to enable the methods that required these properties saved in the estimator",
    "eventually we should call those methods and just save the results in this object",
    "instead of having this object invoke the estimator methods with the data.",
    "No estimand was identified (identification failed)",
    "s += \"Variance in outcome explained by treatment: {}\\n\".format(self.effect_strength[\"r-squared\"])",
    "can't use nx.node_connected_component, because it doesn't work with DiGraphs.",
    "Hence, a manual loop:",
    "also return the number of backdoor paths blocked by observed nodes",
    "Assume that nodes1 is the treatment",
    "ignores new_graph parameter, always uses self._graph",
    "removal of only direct edges wrt a target is not implemented for incoming edges",
    "remove paths that have nodes1\\node1 or nodes2\\node2 as intermediate nodes",
    "[TODO: double check these work with multivariate implementation:]",
    "Exclusion",
    "As-if-random setup",
    "As-if-random",
    "Condition 1: node 1 ---> node 2 is intercepted by candidate_nodes",
    "dpaths = self.get_all_directed_paths(nodes1, nodes2)",
    "return len(dpaths) > 0",
    "Adding common causes",
    "Adding instruments",
    "Adding effect modifiers",
    "Assuming the simple form of effect modifier",
    "that directly causes the outcome.",
    "self._graph.add_edge(node_name, outcome, style = \"dotted\", headport=\"s\", tailport=\"n\")",
    "self._graph.add_edge(outcome, node_name, style = \"dotted\", headport=\"n\", tailport=\"s\") # TODO make the ports more general so that they apply not just to top-bottom node configurations",
    "some preprocessing steps",
    "parsing the correct graph based on input graph format",
    "load dot file",
    "Below loop assumes that the last indices of W are alwawys converted to discrete",
    "one-hot encode discrete W",
    "Now deleting the old continuous value",
    "create a dataset with 10 observations one binary treatment and a continuous outcome affected by one common cause",
    "Two continuous treatments, no common cause, an instrumental variable and two effect modifiers - linearly added appropriately",
    "One Hot Encoding",
    "TODO Ensure that we do not generate weak instruments",
    "TODO - test all our methods with random noise added to covariates (instead of the stochastic treatment assignment)",
    "Converting treatment to binary if required",
    "Generating frontdoor variables if asked for",
    "NOTE: We are assuming a linear relationship *even when t is categorical* and integer coded.",
    "For categorical t, this example dataset has the effect size for category 2 being exactly",
    "double the effect for category 1",
    "This could be changed at this stage by one-hot encoding t and using a custom beta that",
    "sets a different effect for each category {0, 1, 2}",
    "Computing ATE",
    "constructing column names for one-hot encoded discrete features",
    "Specifying the correct dtypes",
    "Now specifying the corresponding graph strings",
    "Now writing the gml graph",
    "creating data frame",
    "Specifying the correct dtypes",
    "Now specifying the corresponding graph strings",
    "Now writing the gml graph",
    "Adding edges between common causes and the frontdoor mediator",
    "Error terms",
    "else:",
    "V = 6 + W0 + tterm + E1",
    "Y = 12 + W0*W0 + W0*W0 + yterm + E2  # E2_new",
    "Generating a random normal distribution of integers",
    "Generating data for nodes which have no incoming edges",
    "\"currset\" variable currently has all the successors of the nodes which had no incoming edges",
    "assuming that all unobserved common causes are numerical and are not affected by one hot encoding",
    "Creating a NN to simulate the nuisance function",
    "strength of unobserved confounding",
    "Computing ATE",
    "Specifying the correct dtypes",
    "Now writing the gml graph",
    "The following code for loading the Lalonde dataset was copied from",
    "https://github.com/wayfair/pylift/blob/5afc9088e96f25672423663f5c9b4bb889b4dfc0/examples/Lalonde/Lalonde_sample.ipynb?short_path=b1d451f#L94-L99).",
    "",
    "Copyright 2018, Wayfair, Inc.",
    "",
    "Redistribution and use in source and binary forms, with or without modification, are permitted provided that",
    "the following conditions are met:",
    "",
    "1. Redistributions of source code must retain the above copyright notice, this list of conditions and the",
    "following disclaimer.",
    "",
    "2. Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the",
    "following disclaimer in the documentation and/or other materials provided with the distribution.",
    "",
    "THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND ANY EXPRESS OR IMPLIED",
    "WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A",
    "PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY",
    "DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,",
    "PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER",
    "CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR",
    "OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH",
    "DAMAGE.",
    "The following code is a slight modification of",
    "https://github.com/wayfair/pylift/blob/5afc9088e96f25672423663f5c9b4bb889b4dfc0/examples/Lalonde/Lalonde_sample.ipynb?short_path=b1d451f#L94-L99).",
    "",
    "Copyright 2018, Wayfair, Inc.",
    "",
    "Redistribution and use in source and binary forms, with or without modification, are permitted provided that",
    "the following conditions are met:",
    "",
    "1. Redistributions of source code must retain the above copyright notice, this list of conditions and the",
    "following disclaimer.",
    "",
    "2. Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the",
    "following disclaimer in the documentation and/or other materials provided with the distribution.",
    "",
    "THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND ANY EXPRESS OR IMPLIED",
    "WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A",
    "PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY",
    "DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,",
    "PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER",
    "CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR",
    "OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH",
    "DAMAGE.",
    "",
    "0.0.0 is standard placeholder for poetry-dynamic-versioning",
    "any changes to this should not be checked in",
    "",
    "Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved",
    "Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved",
    "Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved",
    "adapt number of channels",
    "save memory",
    "Keep same dimensions",
    "Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved",
    "single-attribute Causal",
    "test environment",
    "Subsample 2x for computational convenience",
    "Assign a binary label based on the digit",
    "Flip label with probability 0.25",
    "Assign a color based on the label; flip the color with probability environment",
    "Apply the color to the image by zeroing out the other color channel",
    "single-attribute Independent",
    "test environment",
    "Subsample 2x for computational convenience",
    "Assign a binary label based on the digit",
    "Flip label with probability 0.25",
    "multi-attribute Causal + Independent",
    "test environment",
    "Subsample 2x for computational convenience",
    "rotate the image by angle in parameter",
    "Assign a binary label based on the digit",
    "Flip label with probability 0.25",
    "Assign a color based on the label; flip the color with probability environment",
    "Apply the color to the image by zeroing out the other color channel",
    "Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved",
    "Acause regularization",
    "Aconf regularization",
    "Aind regularization",
    "Asel regularization",
    "Compile loss",
    "Check if the optimizer is currently supported",
    "Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved",
    "The currently supported estimators",
    "The default standard deviation for noise",
    "The default scaling factor to determine the bucket size",
    "The minimum number of points for the estimator to run",
    "The Default Transformation, when no arguments are given, or if the number of data points are insufficient for an estimator",
    "The Default True Causal Effect, this is taken to be ZERO by default",
    "The Default split for the number of data points that fall into the training and validation sets",
    "Assuming that outcome is one-dimensional",
    "We need to change the identified estimand",
    "We thus, make a copy. This is done as we don't want",
    "to change the original DataFrame",
    "We use collections.OrderedDict to maintain the order in which the data is stored",
    "Check if we are using an estimator in the transformation list",
    "The rationale behind ordering of the loops is the fact that we induce randomness everytime we create the",
    "Train and the Validation Datasets. Thus, we run the simulation loop followed by the training and the validation",
    "loops. Thus, we can get different values everytime we get the estimator.",
    "for _ in range( self._num_simulations ):",
    "Warn the user that the specified parameter is not applicable when no estimator is present in the transformation",
    "Adding an unobserved confounder if provided by the user",
    "We set X_train = 0 and outcome_train to be 0",
    "Get the final outcome, after running through all the values in the transformation list",
    "Check if the value of true effect has been already stored",
    "We use None as the key as we have no base category for this refutation",
    "As we currently support only one treatment",
    "If the number of data points is too few, run the default transformation: [(\"zero\",\"\"),(\"noise\", {'std_dev':1} )]",
    "Check if the value of true effect has been already stored",
    "This ensures that we calculate the causal effect only once.",
    "We use key_train as we map data with respect to the base category of the data",
    "As we currently support only one treatment",
    "Add h(t) to f(W) to get the dummy outcome",
    "We convert to ndarray for ease in indexing",
    "The data is of the form",
    "sim1: cat1 cat2 ... catn",
    "sim2: cat1 cat2 ... catn",
    "Note: We would like the causal_estimator to find the true causal estimate that we have specified through this",
    "refuter. Let the value of the true causal effect be h(t). In the following section of code, we wish to find out if h(t) falls in the",
    "distribution of the refuter.",
    "True Causal Effect list",
    "Iterating through the refutation for each category",
    "We use string arguments to account for both 32 and 64 bit varaibles",
    "action for continuous variables",
    "Action for categorical variables",
    "Find the set difference for each row",
    "Choose one out of the remaining",
    "We want to see if the estimate falls in the same distribution as the one generated by the refuter",
    "Ideally that should be the case as running bootstrap should not have a significant effect on the ability",
    "of the treatment to affect the outcome",
    "TODO: Check that the target estimand has backdoor variables?",
    "Standardizing the data",
    "Fit a model containing all confounders and compare predictions",
    "using all features compared to all features except a given",
    "confounder.",
    "Estimating the regression coefficient from standardized features to t",
    "By default, return a plot with 10 points",
    "consider 10 values of the effect of the unobserved confounder",
    "Standardizing the data",
    "Fit a model containing all confounders and compare predictions",
    "using all features compared to all features except a given",
    "confounder.",
    "By default, return a plot with 10 points",
    "consider 10 values of the effect of the unobserved confounder",
    "By default, we add the effect of simulated confounder for treatment.",
    "But subtract it from outcome to create a negative correlation",
    "assuming that the original confounder's effect was positive on both.",
    "This is to remove the effect of the original confounder.",
    "By default, we add the effect of simulated confounder for treatment.",
    "But subtract it from outcome to create a negative correlation",
    "assuming that the original confounder's effect was positive on both.",
    "This is to remove the effect of the original confounder.",
    "Obtaining the list of observed variables",
    "Taking a subset of the dataframe that has only observed variables",
    "Residuals from the outcome model obtained by fitting a linear model",
    "Residuals from the treatment model obtained by fitting a linear model",
    "Initialising product_cor_metric_observed with a really low value as finding maximum",
    "The user has an option to give the the effect_strength_on_y and effect_strength_on_t which can be then used instead of maximum correlation with treatment and outcome in the observed variables as it specifies the desired effect.",
    "Choosing a c_star based on the data.",
    "The correlations stop increasing upon increasing c_star after a certain value, that is it plateaus and we choose the value of c_star to be the value it plateaus.",
    "Choosing c1 and c2 based on the hyperbolic relationship once c_star is chosen by going over various combinations of c1 and c2 values and choosing the combination which",
    "which maintains the minimum distance between the product of correlations of the simulated variable and the product of maximum correlations of one of the observed variables",
    "and additionally checks if the ratio of the weights are such that they maintain the ratio of the maximum possible observed coefficients within some confidence interval",
    "c1_final and c2_final are initialised to the values on the hyperbolic curve such that c1_final = c2_final  and c1_final*c2_final = c_star",
    "initialising min_distance_between_product_cor_metrics to be a value greater than 1",
    "TODO (petergtz): This should introduce a Protocol defining the interface this returns instead of the concrete types",
    "If the estimator used is LinearDML, partially linear sensitivity analysis will be automatically chosen",
    "Get a 2D matrix of values",
    "x,y =  np.meshgrid(self.kappa_t, self.kappa_y) # x,y are both MxN",
    "Store the values into the refute object",
    "Adding a label on the contour line for the original estimate",
    "Label every other level using strings",
    "Default value of the p value taken for the distribution",
    "Number of Trials: Number of cointosses to understand if a sample gets the treatment",
    "Mean of the Normal Distribution",
    "Standard Deviation of the Normal Distribution",
    "Create a new column in the data by the name of placebo",
    "Sanity check the data",
    "only permute is supported for iv methods",
    "For IV methods, the estimating_instrument_names should also be",
    "changed. Create a copy to avoid modifying original object",
    "We need to change the identified estimand",
    "We make a copy as a safety measure, we don't want to change the",
    "original DataFrame",
    "Run refutation in parallel",
    "Note: We hardcode the estimate value to ZERO as we want to check if it falls in the distribution of the refuter",
    "Ideally we should expect that ZERO should fall in the distribution of the effect estimates as we have severed any causal",
    "relationship between the treatment and the outcome.",
    "new estimator",
    "new effect estimate",
    "observed covariate E-value",
    "see Table 2 and p.37 in https://dash.harvard.edu/bitstream/handle/1/36874927/EValue_FinalSubmission.pdf",
    "if CI crosses null, set its E-value to 1",
    "only report E-value for CI limit closer to null",
    "see Table 2 and p.37 in https://dash.harvard.edu/bitstream/handle/1/36874927/EValue_FinalSubmission.pdf",
    "whether the DGP is assumed to be partially linear",
    "features are the observed confounders",
    "Now code for benchmarking using covariates begins",
    "R^2 of outcome with observed common causes and treatment",
    "R^2 of treatment with observed common causes",
    "Partial R^2 of outcome after regressing over unobserved confounder, observed common causes and treatment",
    "Assuming that the difference in R2 is the same for wj and new unobserved confounder",
    "for treatment,  Calpha is not a function of the partial R2. So we need a different assumption.",
    "Assuming that the ratio of variance of alpha^2 is the same for wj and new unobserved confounder",
    "(1-ratio_var_alpha_wj) is the numerator of Calpha2, similar to the partial R2 for treatment",
    "wrt unobserved confounders in partial-linear models",
    "Do the support characterization",
    "Recover the samples that are in the support",
    "Assess overlap using propensity scores with cross-fitting",
    "Check if all supported units are considered to be in the overlap set",
    "NOTE: The original paper implements both DNF and CNF rules, but for simplicity, this code only implements DNF rules",
    "For DNF rules, a sample is covered if *any* rule applies",
    "whether the DGP is assumed to be partially linear",
    "can change this to allow default values that are same as the other parameter",
    "Strength of confounding that omitted variables generate in treatment regression",
    "computing the point estimate for the bounds",
    "common causes after removing the benchmark causes",
    "dataframe with treatment and observed common causes after removing benchmark causes",
    "R^2 of treatment with observed common causes removing benchmark causes",
    "return the variance of alpha_s",
    "R^2 of outcome with observed common causes and treatment after removing benchmark causes",
    "Obtaining theta_s (the obtained estimate)",
    "Creating numpy arrays",
    "Setting up cross-validation parameters",
    "tuple of residuals from first stage estimation [0,1], and the confounders [2]",
    "We need to estimate, sigma^2 = (Y-g_s)^2. We use the following derivation.",
    "Yres = Y - E[Y|W]",
    "E[Y|W] = f(x) + theta_s * E[T|W]",
    "Yres = Y - f(x) - theta_s * E[T|W]",
    "g(s) = theta_s * T + f(x)",
    "g(s) = theta_s * (T - E[T|W]) + f(x) + theta_s * E[T|W]",
    "g(s) = theta_s * Tres +f(x) + theta_s * E[T|W]",
    "Y - g(s) = Y - [theta_s * Tres + f(x) + theta_s * E[T|W] )",
    "Y - g(s) = ( Y - f(x) -  theta_s * E[T|W]) - theta_s * Tres",
    "Y - g(s) = Yres - theta_s * Tres",
    "nu_2 is E[alpha_s^2]",
    "Now computing scores for finding the (1-a) confidence interval",
    "R^2 of treatment with observed common causes",
    "R^2 of outcome with treatment and observed common causes",
    "Partial R^2 of outcome after regressing over unobserved confounder, observed common causes and treatment",
    "Partial R^2 of treatment after regressing over unobserved confounder and observed common causes",
    "adding 1.1 as plotting margin  ensure that the benchmarked part is shown fully in plot",
    "adding 1.1 as plotting margin  ensure that the benchmarked part is shown fully in plot",
    "Adding unadjusted point estimate",
    "Adding bounds to partial R^2 values for given strength of confounders",
    "Adding a new backdoor variable to the identified estimand",
    "Run refutation in parallel",
    "We want to see if the estimate falls in the same distribution as the one generated by the refuter",
    "Ideally that should be the case as choosing a subset should not have a significant effect on the ability",
    "of the treatment to affect the outcome",
    "TODO: Sensitivity Analyzers excluded from list due to different return type",
    "Reject H0",
    "a, b and c are all continuous variables",
    "a, b and c are all discrete variables",
    "c is set of continuous and binary variables and",
    "1. either a and b is continuous and the other is binary",
    "2. both a and b are binary",
    "c is discrete and",
    "either a or b is continuous and the other is discrete",
    "a and b are discrete and c is a mixture of discrete and continuous variables. We discretize c and calculate conditional mutual information",
    "original_treatment_name: : stores original variable names for labelling",
    "common_causes_map : maps the original variable names to variable names in OLS regression",
    "benchmark_common_causes: stores variable names in terms of regression model variables",
    "original_benchmark_covariates: stores original variable names for labelling",
    "estimate: estimate of regression",
    "degree_of_freedom: degree of freedom of error in regression",
    "standard_error: standard error in regression",
    "t_stats: Treatment coefficient t-value - measures how many standard errors the estimate is away from zero.",
    "partial_f2: value to determine if a regression model and a nested version of it have a statistically significant difference between them",
    "r2tu_w: partial R^2  of unobserved confounder \"u\" with treatment \"t\", after conditioning on observed covariates \"w\"",
    "r2yu_tw: partial R^2  of unobserved confounder \"u\" with outcome \"y\", after conditioning on observed covariates \"w\" and treatment \"t\"",
    "r2twj_w: partial R^2 of observed covariate wj with treatment \"t\", after conditioning on observed covariates \"w\" excluding wj",
    "r2ywj_tw:  partial R^2 of observed covariate wj with outcome \"y\", after conditioning on observed covariates \"w\" (excluding wj) and treatment \"t\"",
    "benchmarking_results: dataframe containing information about bounds and bias adjusted terms",
    "stats: dictionary containing information like robustness value, partial R^2, estimate, standard error , degree of freedom, partial f^2, t-statistic",
    "partial R^2 (r2yt_w) is the proportion of variation in outcome uniquely explained by treatment",
    "build a new regression model by considering treatment variables as outcome",
    "r2twj_w is partial R^2 of covariate wj with treatment \"t\", after conditioning on covariates w(excluding wj)",
    "r2ywj_tw is partial R^2 of covariate wj with outcome \"y\", after conditioning on covariates w(excluding wj) and treatment \"t\"",
    "r2tu_w is the partial r^2 from regressing u on t after conditioning on w",
    "Compute bias adjusted terms",
    "Plotting the contour plot",
    "Adding contours",
    "Adding threshold contour line",
    "Adding unadjusted point estimate",
    "Adding bounds to partial R^2 values for given strength of confounders",
    "from https://www.bnmetrics.com/blog/factory-pattern-in-python3-simple-version",
    "The default subset of the data to be used",
    "Run refutation in parallel",
    "We want to see if the estimate falls in the same distribution as the one generated by the refuter",
    "Ideally that should be the case as choosing a subset should not have a significant effect on the ability",
    "of the treatment to affect the outcome",
    "Parameters",
    "Bookkeeping",
    "Initialize estimators",
    "Convert to dataframe if not",
    "Format labels",
    "Sample from reference measure and construct features",
    "Add reference samples",
    "Binarize features (fit to data only)",
    "Fit estimator",
    "Store reference volume",
    "Construct features dataframe",
    "Construct features dataframe",
    "Iterate over columns",
    "logging.info(\"Using provided reference range for {}\".format(c))",
    "number of unique values",
    "Constant column",
    "Binary column",
    "Ordinal column (seed = counter so not correlated)",
    "For get_params / set_params",
    "Overlap / Support (y = +1), non-overlap (y = 0), and uniform background (y = -1) samples",
    "We should always have overlap samples, and either background or non-overlap samples",
    "This will throw an error if, for example, all samples are considered to",
    "be in the overlap region",
    "Initialize with empty and singleton conjunctions, i.e. X plus all-ones feature",
    "Feature indicator and conjunction matrices",
    "Iteration counter",
    "Formulate master LP",
    "Variables",
    "Objective function (no penalty on empty conjunction)",
    "Constraints",
    "This gets activated for DNF",
    "Solve problem",
    "Extract dual variables",
    "Beam search for conjunctions with negative reduced cost",
    "Most negative reduced cost among current variables",
    "Negative reduced costs found",
    "Add to existing conjunctions",
    "Reformulate master LP",
    "Variables",
    "Objective function",
    "Constraints",
    "Solve problem",
    "Extract dual variables",
    "Beam search for conjunctions with negative reduced cost",
    "Most negative reduced cost among current variables",
    "print('UB.min():', UB.min())",
    "Save generated conjunctions and coefficients",
    "Restrict conjunctions to those used by LP",
    "NOTE: This is a greedy approach, so it does not incorporate lambda0 explicitly",
    "Similarly, it will prefer a larger number of smaller rules if lambda1 is set",
    "to a larger value, because the incremental cost will be lower.",
    "Fraction of reference samples that each conjunction covers",
    "Regularization (for each conjunction)",
    "Positive samples newly covered (for each conjunction)",
    "Costs (for each conjunction)",
    "Zero out the rules and only take those which are used",
    "Small tolerance on comparisons",
    "This can be useful to break ties and favor larger values of xi",
    "Compute conjunctions of features",
    "Predict labels",
    "Use helper function",
    "Use helper function",
    "Lower bound specific to each singleton solution",
    "Initialize output",
    "Remove redundant rows by grouping by unique feature combinations and summing residual",
    "Initialize queue with root instance",
    "Separate data according to positive and negative residuals",
    "Iterate over increasing degree while queue is non-empty",
    "Initialize list of children to process",
    "Process instances in queue",
    "inst = instCurr[0]",
    "Evaluate all singleton solutions",
    "Best solutions that also improve on current output (allow for duplicate removal)",
    "Append to current output",
    "Remove duplicates",
    "Update output",
    "Compute lower bounds on higher-degree solutions",
    "Evaluate children using weighted average of their costs and LBs",
    "Best children with potential to improve on current output and current candidates (allow for duplicate removal)",
    "Iterate through best children",
    "New \"zero\" solution",
    "Check if duplicate",
    "Add to candidates for further processing",
    "Create pricing instance",
    "Remove covered rows",
    "Remove redundant features",
    "Track number of candidates added",
    "Update candidates",
    "Instances to process in next iteration",
    "Conjunctions corresponding to solutions",
    "List of categorical columns",
    "Number of quantile thresholds used to binarize ordinal features",
    "whether to append negations",
    "whether to convert thresholds on ordinal features to strings",
    "Quantile probabilities",
    "Initialize",
    "Iterate over columns",
    "number of unique values",
    "Constant or binary column",
    "Mapping to 0, 1",
    "Categorical column",
    "OneHotEncoder object",
    "Fit to observed categories",
    "Ordinal column",
    "Few unique values",
    "Thresholds are sorted unique values excluding maximum",
    "Many unique values",
    "Thresholds are quantiles excluding repetitions",
    "Contains NaN values",
    "Initialize dataframe",
    "Iterate over columns",
    "Constant or binary column",
    "Rename values to 0, 1",
    "Categorical column",
    "Apply OneHotEncoder",
    "Append negations",
    "Concatenate",
    "Ordinal column",
    "Threshold values to produce binary arrays",
    "Append negations",
    "Convert to dataframe with column labels",
    "Ensure that rows corresponding to NaN values are zeroed out",
    "Add NaN indicator column",
    "Concatenate",
    "from https://www.bnmetrics.com/blog/factory-pattern-in-python3-simple-version",
    "set attributions to zero for left out invariant nodes",
    "Get parent and child nodes",
    "Don't remove node if node has more than 1 children nodes as it can introduce",
    "hidden confounders.",
    "Remove the middle node",
    "Connect parent and child nodes",
    "Update the causal mechanism for the child nodes",
    "If the relative change of the score is less than the given threshold, we stop the estimation early.",
    "Note, the output of score_samples are log values.",
    "Note, the output of score_samples are log values.",
    "Currently only support continuous distributions for auto selection.",
    "Estimate distribution parameters from data.",
    "Ignore warnings from fitting process.",
    "Fit distribution to data.",
    "Some distributions might not be compatible with the data.",
    "Separate parts of parameters.",
    "Check the KL divergence between the distribution of the given and fitted distribution.",
    "Identify if this distribution is better.",
    "This error is typically raised when the data is discrete and all points are assigned to less cluster than",
    "specified. It can also happen due to duplicated points. In these cases, the current best solution should",
    "be sufficient.",
    "Usual feature relevance using the mean deviation as set function, i.e. g(x) - E[g(X)]",
    "Normally, we need to estimate the KL divergence jointly. However, to avoid issues with high dimensional data,",
    "we approximate it by taking the average over the marginal KL divergences.",
    "Do not compare with same model class",
    "Do not compare with same model class",
    "In case of post non-linear models, we can obtain the conditional expectation directly based on the prediction",
    "model. To do this, we can just in pass 0 as the noise, since this would evaluate Y = f(X) + 0 in case of an",
    "additive noise model and Y = g(f(X) + 0) in case of a more general model.",
    "Estimate the conditional expectation E[Y | x] by generating multiple samples for Y|x and average them.",
    "Since these are categorical values, we just need to look for the most frequent element after we drew",
    "multiple samples for each input.",
    "In the categorical case, this is equivalent to the Brier score. However, the following formulation allows",
    "categorical data with more than two classes.",
    "A convenience function when computing confidence intervals specifically for non-deterministic causal queries. This",
    "function evaluates the provided causal query multiple times to build a confidence interval based on the returned",
    "results.",
    "Note that this function does not re-fit the causal model(s) and only executes the provided query as it is. In order",
    "to re-refit the graphical causal model on random subsets of the data before executing the query, consider using the",
    "fit_and_compute function.",
    "",
    "**Example usage:**",
    "",
    ">>> gcm.fit(causal_model, data)",
    ">>> strength_medians, strength_intervals = gcm.confidence_intervals(",
    ">>>     gcm.bootstrap_sampling(gcm.arrow_strength, causal_model, target_node='Y'))",
    "",
    "In this example, gcm.confidence_intervals is expecting a callable with non-deterministic outputs for building the",
    "confidence intervals. Since each causal query potentially expects a different set of parameters, we use 'partial'",
    "here to configure the function call. In this case,",
    "gcm.bootstrap_sampling(gcm.arrow_strength, causal_model, target_node='Y') would be equivalent to",
    "lambda : gcm.arrow_strength(causal_model, target_node='Y').",
    "",
    "In order to incorporate uncertainties coming from fitting the causal model(s), we can use",
    "gcm.fit_and_compute instead:",
    ">>>  strength_medians, strength_intervals = gcm.confidence_intervals(",
    ">>>        gcm.fit_and_compute(gcm.arrow_strength,",
    ">>>                                            causal_model,",
    ">>>                                            bootstrap_training_data=data,",
    ">>>                                            target_node='Y'))",
    "This would refit the provided causal_model on a subset of the data first before executing gcm.arrow_strength in each",
    "run.",
    "Simulating interventions by propagating the effects through the graph. For this, we iterate over the nodes based",
    "on their topological order.",
    "After drawing samples of the node based on the data generation process, we apply the corresponding",
    "intervention. The inputs of downstream nodes are therefore based on the outcome of the intervention in this",
    "node.",
    "Abduction: For invertible SCMs, we recover exact noise values from data.",
    "Action + Prediction: Propagate the intervention downstream using recovered noise values.",
    "Check if we need to apply an intervention on the given node.",
    "Apply intervention function to the data of the node.",
    "Check if the intervention function changes the shape of the data.",
    "For estimating the effect, we only need to consider the nodes that have a directed path to the target node, i.e.",
    "all ancestors of the target.",
    "The target node can be a continuous real-valued variable or a categorical variable with at most two classes",
    "(i.e. binary).",
    "Find all node names in the expression string.",
    "Nothing to fit here, since we know the ground truth.",
    "Avoid too many features",
    "Making sure there are at least 30% test samples.",
    "Making sure that there are at least 2 samples from one class (here, simply duplicate the point).",
    "Compare number of correct classifications.",
    "Making sure that X and Y have no overlapping values, which would lead to a distance of 0 with k=1 and, thus, to",
    "a division by zero.",
    "All elements are equal (or at least less than k samples are different)",
    "Balance the classes",
    "For estimating Shapley values for multiple samples (e.g. in feature relevance) and the number of samples",
    "is unknown beforehand.",
    "The method stops if either the change between some consecutive runs is below the given threshold or the",
    "maximum number of runs is reached.",
    "In each run, we create one random permutation of players. For instance, given 4 players, a permutation",
    "could be [3,1,4,2].",
    "Generate k random permutations by sorting the indices of the Halton sequence",
    "Create all subsets belonging to the generated permutation. This is, if we have [3,1,4,2], then the",
    "subsets are [3], [3,1], [3,1,4] [3,1,4,2].",
    "The result for each subset is cached such that if a subset that has already been evaluated appears again,",
    "we can take this result directly.",
    "To improve the runtime, multiple permutations are evaluated in each run.",
    "The current Shapley values are the average of the estimated values, i.e. we need to divide by the number",
    "of generated permutations here.",
    "In case Shapley values are estimated for multiple samples, e.g., in feature relevance. So, we have a",
    "matrix of Shapley values instead of a vector.",
    "Here, the change between consecutive runs is below the minimum threshold, but to reduce the",
    "likelihood that this just happened by chance, we require that this happens at least for",
    "num_consecutive_converged_runs times in a row.",
    "Check if change in percentage is below threshold",
    "Check for values that are exactly zero. If they don't change between two runs, we consider it as converging.",
    "Create all (unique) subsets)",
    "Assigning a 'high' weight, since this resembles \"infinity\".",
    "The weight for a subset with a specific length (see paper mentioned in the docstring for more",
    "information).",
    "TODO: Add method for auto select a bin_width/width based on the data. Make sure that the auto selection method is",
    "theoretically sound, i.e. make entropy results from different data comparable.",
    "Extremely small values can somehow result in negative values.",
    "Creating a smaller subgraph, which only contains upstream nodes that are connected to the target node.",
    "Sampling from the conditional distribution based on the current sample.",
    "Sampling from the conditional based on the current sample, but randomizing the inputs of all variables that",
    "are in the given subset. By this, we can simulate the impact on the conditional distribution when removing",
    "only the incoming edges of the variables in the subset.",
    "Creating a smaller subgraph, which only contains upstream nodes that are connected to the target node.",
    "In case of the full subset (no randomization), we get the same predictions as when we apply the",
    "prediction method to the samples of interest, since all noise samples are replaced with a sample of",
    "interest.",
    "In case of the empty subset (all are jointly randomize), it boils down to taking the average over all",
    "predictions, seeing that the randomization yields the same values for each sample of interest (none of the",
    "samples of interest are used to replace a (jointly) 'randomized' sample).",
    "Exact model",
    "This constant is used as key when storing/accessing models as causal mechanisms in graph node attributes",
    "This constant is used as key when storing the parents of a node during fitting. It's used for validation purposes",
    "afterwards.",
    "Todo: Remove after https://github.com/py-why/dowhy/pull/943.",
    "Smallest possible value. This is used in various algorithm for numerical stability.",
    "Make copy to avoid manipulating the original matrix.",
    "The given prediction method has to be evaluated multiple times on a large amount of different inputs. Typically,",
    "the batch evaluation of a prediction model on multiple inputs at the same time is significantly faster",
    "than evaluating it on single simples in a for-loop. To make use of this, we try to evaluate as many samples as",
    "possible in one batch call of the prediction method. However, this also requires a lot of memory for many samples.",
    "To overcome potential memory issues, multiple batch calls are performed, each with at most batch_size many",
    "samples. The number of samples that are evaluated is normally",
    "baseline_noise_samples.shape[0] * feature_samples.shape[0]. Here, we reduce it to",
    "batch_size * feature_samples.shape[0]. If the batch_size would be set 1, then each baseline_noise_samples is",
    "evaluated one by one in a for-loop.",
    "Each batch consist of at most batch_size * feature_samples.shape[0] many samples. If there are multiple",
    "batches, the offset indicates the index of the current baseline_noise_samples that has not been evaluated yet.",
    "If the batch size would be larger than the remaining amount of samples, it is reduced to only include the",
    "remaining baseline_noise_samples.",
    "The inputs consist of batch_size many copies of feature_samples. Here, we set the columns of the features",
    "in baseline_feature_indices to their respective values in baseline_noise_samples.",
    "After creating the (potentially large) input data matrix, we can evaluate the prediction method.",
    "Here, offset + index now indicates the sample index in baseline_noise_samples.",
    "This would average all prediction results obtained for the 'offset + index'-th sample in",
    "baseline_noise_samples. This is, y^(offset + index) = E[Y | do(x^(offset + index)_s)].",
    "This would return all prediction results obtained for the 'offset + index'-th sample in",
    "baseline_noise_samples, i.e. the results are not averaged.",
    "Making copy to ensure that the original object is not modified.",
    "Permute samples jointly. This still represents an interventional distribution.",
    "Permute samples independently.",
    "Looks for the first index where the cumulative sum of the probabilities is larger than the threshold.",
    "Note that if there are multiple indices with the same maximum value (as in this case here), the argmax",
    "function returns the first index.",
    "test local Markov condition, null hypothesis: conditional independence",
    "test edge dependence, null hypothesis: independence",
    "The order of the p-values added to the list is deterministic.",
    "To be able to validate that the graph structure did not change between fitting and causal query, we store the",
    "parents of a node during fit. That way, before sampling, we can verify the parents are still the same. While",
    "this would automatically fail when the number of parents is different, there are other more subtle cases,",
    "where the number is still the same, but it's different parents, and therefore different data. That would yield",
    "wrong results, but would not fail.",
    "Independence tests are symmetric",
    "Find out which tests to do",
    "Parallelize over tests",
    "Gather results",
    "Summarize",
    "Find out which tests to do",
    "Parallelize over tests",
    "Gather results",
    "Summarize",
    "Find out which tests to do",
    "Parallelize over tests",
    "Gather results",
    "Summarize",
    "DAG Evaluation",
    "Suggestions",
    "Append list of violations (node, non_desc) to get local information",
    "Plot histograms",
    "Plot given violations",
    "For LMC we highlight X for which X _|/|_ Y \\in ND_X | Pa_X",
    "For PD we highlight the edge (if Y\\in Anc_X -> X are adjacent)",
    "For causal minimality we highlight the edge Y \\in Pa_X -> X",
    "Create Validation header",
    "Create Validation summary",
    "Close Validation",
    "Create Suggestions header",
    "Iterate over suggestions",
    "Test if we have data for X and Y",
    "Test if we have data for Z",
    "Eq. (1) in https://arxiv.org/pdf/1810.11363.pdf",
    "Subtracting Y here since the cumulative sum includes the current element. The same reason we subtract 1",
    "from the count.",
    "Using the negative value here seeing that the Shapley estimation evaluates v(S u {i}) - v(S) for a subset S. In",
    "case of variance, we have v(S u {i}) <= v(S), which would result in a negative contribution of players to the",
    "target quantity (here, variance).",
    "Using the negative value here seeing that the Shapley estimation evaluates v(S u {i}) - v(S) for a subset S. In",
    "case of variance, we have v(S u {i}) <= v(S), which would result in a negative contribution of players to the",
    "target quantity (here, variance).",
    "Calculate Ri, the product of the residuals",
    "Standard deviation of the residuals",
    "Either X and/or Y is constant.",
    "If Z is empty, we are in the pairwise setting.",
    "Either X and/or Y is constant.",
    "If Z is empty, we are in the pairwise setting.",
    "Estimate test statistic multiple times on different permutations of the data. The p-value is then the",
    "probability (i.e. fraction) of obtaining a test statistic that is greater than statistic on the non-permuted",
    "data.",
    "Take the lower dimensional variable as target.",
    "First stage statistical model",
    "Second stage statistical model",
    "Check if the treatment is one-dimensional",
    "First stage",
    "Second Stage",
    "Combining the two estimates",
    "This same estimate is valid for frontdoor as well as mediation (NIE)",
    "Bulding the feature matrix",
    "features = sm.add_constant(features, has_constant='add') # to add an intercept term",
    "TODO move this to the identification step",
    "Obtain estimate by Wald Estimator",
    "Obtain estimate by 2SLS estimator: Cov(y,z) / Cov(x,z)",
    "More than 1 instrument. Use 2sls.",
    "Checking if Y is binary",
    "Enable the user to pass params for a custom propensity model",
    "Convert the categorical variables into dummy/indicator variables",
    "Basically, this gives a one hot encoding for each category",
    "The first category is taken to be the base line.",
    "Check if the treatment is one-dimensional",
    "Checking if the treatment is binary",
    "Data encoders",
    "encoder_drop_first will not encode the first category value with a bit in 1-hot encoding.",
    "It will be implicit instead, by the absence of any bit representing this value in the relevant columns.",
    "Set to False to include a bit for each value of every categorical variable.",
    "Remember encoder",
    "The model is always built on the entire data",
    "TODO make treatment_value and control value also as local parameters",
    "All treatments are set to the same constant value",
    "Fixing treatment value to the specified value, if provided",
    "treatment_vals and data_df should have same number of rows",
    "Bulding the feature matrix",
    "Replace treatment values with value supplied; note: Don't change column datatype!",
    "The model is always built on the entire data",
    "The average treatment effect is a combination of different",
    "regression coefficients. Complicated to compute the confidence",
    "interval analytically. For example, if y=a + b1.t + b2.tx, then",
    "the average treatment effect is b1+b2.mean(x).",
    "Refer Gelman, Hill. ARM Book. Chapter 9",
    "http://www.stat.columbia.edu/~gelman/arm/chap9.pdf",
    "TODO: Looking for contributions",
    "For a linear regression model, the causal effect of a variable is equal to the coefficient corresponding to the",
    "variable. Hence, the model by default outputs the confidence interval corresponding to treatment=1 and control=0.",
    "So for custom treatment and control values, we must multiply the confidence interval by the difference of the two.",
    "For a linear regression model, the causal effect of a variable is equal to the coefficient corresponding to the",
    "variable. Hence, the model by default outputs the standard error corresponding to treatment=1 and control=0.",
    "So for custom treatment and control values, we must multiply the standard error by the difference of the two.",
    "setting method-specific parameters",
    "Infer the right strata based on clipping threshold",
    "0.5 because there are two values for the treatment",
    "To be conservative and allow most strata to be included in the",
    "analysis",
    "At least 90% of the strata should be included in analysis",
    "sum weighted outcomes over all strata  (weight by treated population)",
    "TODO - how can we add additional information into the returned estimate?",
    "such as how much clipping was done, or per-strata info for debugging?",
    "sort the dataframe by propensity score",
    "create a column 'strata' for each element that marks what strata it belongs to",
    "for each strata, count how many treated and control units there are",
    "throw away strata that have insufficient treatment or control",
    "TODO -- fix: we are actually conditioning on positive treatment (d=1)",
    "Setting method specific parameters",
    "trim propensity score weights",
    "ips ==> (isTreated(y)/ps(y)) + ((1-isTreated(y))/(1-ps(y)))",
    "nips ==> ips / (sum of ips over all units)",
    "icps ==> ps(y)/(1-ps(y)) / (sum of (ps(y)/(1-ps(y))) over all control units)",
    "itps ==> ps(y)/(1-ps(y)) / (sum of (ps(y)/(1-ps(y))) over all treatment units)",
    "Vanilla IPS estimator",
    "The Hajek estimator (or the self-normalized estimator)",
    "Stabilized weights (from Robins, Hernan, Brumback (2000))",
    "Paper: Marginal Structural Models and Causal Inference in Epidemiology",
    "Calculating the effect",
    "Subtracting the weighted means",
    "TODO - how can we add additional information into the returned estimate?",
    "TODO -- fix: we are actually conditioning on positive treatment (d=1)",
    "Save parameters for later refutter fitting",
    "Enforcing this ordering is necessary to feed through the propensity values from dataset",
    "For metalearners only--issue a warning if w contains variables not in x",
    "Override the effect_modifiers set in CausalEstimator.__init__()",
    "Also only update self._effect_modifiers, and create a copy of self._effect_modifier_names",
    "the latter can be used by other estimator methods later",
    "Instrumental variables names, if present",
    "choosing the instrumental variable to use",
    "Calling the econml estimator's fit method",
    "As of v0.9, econml has some kewyord only arguments",
    "from https://www.bnmetrics.com/blog/factory-pattern-in-python3-simple-version",
    "Changing shape to a list for a singleton value",
    "Note that self._control_value is assumed to be a singleton value",
    "TODO -- fix: we are actually conditioning on positive treatment (d=1)",
    "For each unit, return the estimated effect of the treatment value",
    "that was actually applied to the unit",
    "this assumes a binary treatment regime",
    "TODO remove neighbors that are more than a given radius apart",
    "estimate ATT on treated by summing over difference between matched neighbors",
    "Now computing ATC",
    "TODO -- fix: we are actually conditioning on positive treatment (d=1)",
    "from https://www.bnmetrics.com/blog/factory-pattern-in-python3-simple-version",
    "Handle externally provided estimator classes",
    "allowed types of distance metric",
    "Dictionary of any user-provided params for the distance metric",
    "that will be passed to sklearn nearestneighbors",
    "Check if the treatment is one-dimensional",
    "Checking if the treatment is binary",
    "Convert the categorical variables into dummy/indicator variables",
    "Basically, this gives a one hot encoding for each category",
    "The first category is taken to be the base line.",
    "this assumes a binary treatment regime",
    "TODO remove neighbors that are more than a given radius apart",
    "estimate ATT on treated by summing over difference between matched neighbors",
    "Return indices in the original dataframe",
    "self.matched_indices_att[treated_df_index[i]] = control.iloc[indices[i]].index.tolist()",
    "Now computing ATC",
    "Return indices in the original dataframe",
    "Add the identification method used in the estimator",
    "Check the backdoor variables being used",
    "Add the observed confounders and one hot encode the categorical variables",
    "Get the data of the unobserved confounders",
    "One hot encode the data if they are categorical",
    "Check the instrumental variables involved",
    "Perform the same actions as the above",
    "Both the outcome and the treatment have to be 1D arrays according to the CausalML API",
    "We want to pass 'v0' rather than ['v0'] to prevent a shape mismatch",
    "For CATEs",
    "TODO we are conditioning on a postive treatment",
    "TODO create an expression corresponding to each estimator used",
    "Determine columns being encoded",
    "If all columns are already numerical, there may be nothing to encode.",
    "In this case, return original data.",
    "Columns to keep in the result - not encoded.",
    "Convert the encoded data to a DataFrame",
    "Concatenate the encoded DataFrame with the original non-categorical columns",
    "Generating data with equal 0 and 1 (since ranks are uniformly distributed)",
    "Flipping some values",
    "Wrapping labels if they are too long",
    "This layout tries to mimic the graphviz layout in a simpler form. The depth grows horizontally here instead of",
    "vertically.",
    "Set the figure size based on the number of nodes",
    "Nodes that are vertically connected, but not neighbors should be connected via a curved edge.",
    "All other nodes should be connected with a straight line.",
    "Draw labels node labels",
    "Each node gets a depth assigned, based on the distance to the closest root node.",
    "In case of undirected graphs, we just take any node as root node.",
    "No path to root node, ignore this connection then.",
    "Counts the number of vertical nodes in the same layers.",
    "Creates a matrix indicating whether two nodes are vertical neighbors.",
    "Get all y coordinates per layer",
    "Sort the y-coordinates",
    "Finding p-value using student T test",
    "Only consider edges have absolute edge weight > 0.01",
    "Modify graph such that it only contains bidirected edges",
    "Find c components by finding connected components on the undirected graph",
    "Understanding Neural Network weights",
    "Refer to this link:https://stackoverflow.com/questions/50937628/mlp-classifier-neurons-weights",
    "add weight column",
    "before weights are applied we count number rows in each category",
    "which is equivalent to summing over weight=1",
    "after weights are applied we need to sum over the given weights",
    "First, calculating mean differences by strata",
    "Second, without strata",
    "Third, concatenating them and plotting",
    "Setting estimator attribute for convenience",
    "Outcome is numeric",
    "Treatments are also numeric or binary",
    "Outcome is categorical",
    "Treatments are numeric or binary",
    "TODO: A common way to show all plots",
    "from https://www.bnmetrics.com/blog/factory-pattern-in-python3-simple-version",
    "Get adjacency list",
    "If node pair has been fully explored",
    "Add node1 to backdoor set of node_pair",
    "Check if path is backdoor and does not have nodes1\\node1 or nodes2\\node2 as intermediate nodes",
    "True if arrow incoming, False if arrow outgoing",
    "Mark pair (node1, node2) complete",
    "Modify variable count and indices covered",
    "Average total effect",
    "Natural direct effect",
    "Natural indirect effect",
    "Controlled direct effect",
    "Backdoor method names",
    "First, check if there is a directed path from action to outcome",
    "## 1. BACKDOOR IDENTIFICATION",
    "Pick algorithm to compute backdoor sets according to method chosen",
    "First, checking if there are any valid backdoor adjustment sets",
    "Setting default \"backdoor\" identification adjustment set",
    "## 2. INSTRUMENTAL VARIABLE IDENTIFICATION",
    "Now checking if there is also a valid iv estimand",
    "## 3. FRONTDOOR IDENTIFICATION",
    "Now checking if there is a valid frontdoor variable",
    "Finally returning the estimand object",
    "Pick algorithm to compute backdoor sets according to method chosen",
    "Setting default \"backdoor\" identification adjustment set",
    "Finally returning the estimand object",
    "## 1. FIRST DOING BACKDOOR IDENTIFICATION",
    "First, checking if there are any valid backdoor adjustment sets",
    "Setting default \"backdoor\" identification adjustment set",
    "## 2. SECOND, CHECKING FOR MEDIATORS",
    "Now checking if there are valid mediator variables",
    "Finally returning the estimand object",
    "## 1. FIRST DOING BACKDOOR IDENTIFICATION",
    "First, checking if there are any valid backdoor adjustment sets",
    "Setting default \"backdoor\" identification adjustment set",
    "## 2. SECOND, CHECKING FOR MEDIATORS",
    "Now checking if there are valid mediator variables",
    "Finally returning the estimand object",
    "First, checking if empty set is a valid backdoor set",
    "If the method is `minimal-adjustment`, return the empty set right away.",
    "Second, checking for all other sets of variables. If include_unobserved is false, then only observed variables are eligible.",
    "only remove descendants of Y",
    "also allow any causes of Y that are not caused by T (for lower variance)",
    "remove descendants of T (mediators) and descendants of Y",
    "If var is d-separated from both treatment or outcome, it cannot",
    "be a part of the backdoor set",
    "repeat the above search with BACKDOOR_MIN",
    "If `minimal-adjustment` method is specified, start the search from the set with minimum size. Otherwise, start from the largest.",
    "If the backdoor method is `maximal-adjustment` or `minimal-adjustment`, return the first found adjustment set.",
    "If all variables are observed, and the biggest eligible set",
    "does not satisfy backdoor, then none of its subsets will.",
    "Adding a None estimand if no backdoor set found",
    "Default set contains minimum possible number of instrumental variables, to prevent lowering variance in the treatment variable.",
    "Default set is the one with the least number of adjustment variables (optimizing for efficiency)",
    "Cond 1: All directed paths intercepted by candidate_var",
    "Cond 2: No confounding between treatment and candidate var",
    "Cond 3: treatment blocks all confounding between candidate_var and outcome",
    "For simplicity, assuming a one-variable mediation set",
    "Create estimands dict as per the API for backdoor, but do not return it",
    "Setting default \"backdoor\" identification adjustment set",
    "Create estimands dict as per the API for backdoor, but do not return it",
    "Setting default \"backdoor\" identification adjustment set",
    "TODO: outputs string for now, but ideally should do symbolic",
    "expressions Mon 19 Feb 2018 04:54:17 PM DST",
    "TODO Better support for multivariate treatments",
    "TODO: support multivariate treatments better.",
    "TODO: support multivariate treatments better.",
    "TODO: support multivariate treatments better.",
    "For direct effect",
    "If no costs are passed, use uniform costs",
    "restriction to ancestors",
    "back-door graph",
    "moralization",
    "Estimators list for returning after identification",
    "Line 1",
    "If no action has been taken, the effect on Y is just the marginal of the observational distribution P(v) on Y.",
    "Line 2",
    "If we are interested in the effect on Y, it is sufficient to restrict our attention on the parts of the model ancestral to Y.",
    "Modify list of valid nodes",
    "Line 3 - forces an action on any node where such an action would have no effect on Y \u2013 assuming we already acted on X.",
    "Modify adjacency matrix to obtain that corresponding to do(X)",
    "Line 4 - Decomposes the problem into a set of smaller problems using the key property of C-component factorization of causal models.",
    "If the entire graph is a single C-component already, further problem decomposition is impossible, and we must provide base cases.",
    "Modify adjacency matrix to remove treatment variables",
    "Line 5 - The algorithms fails due to the presence of a hedge - the graph G, and a subgraph S that does not contain any X nodes.",
    "Line 6 - If there are no bidirected arcs from X to the other nodes in the current subproblem under consideration, then we can replace acting on X by conditioning, and thus solve the subproblem.",
    "Line 7 - This is the most complicated case in the algorithm. Explain in the second last paragraph on Pg 41 of the link provided in the docstring above.",
    "Do not show backdoor key unless it is the only backdoor set.",
    "Just show the default backdoor set",
    "If labels provided",
    "Return in valid DOT format",
    "Get adjacency matrix",
    "If labels not provided",
    "Obtain valid DOT format",
    "If labels provided",
    "Return in valid DOT format",
    "from https://www.bnmetrics.com/blog/factory-pattern-in-python3-simple-version",
    "from https://www.bnmetrics.com/blog/factory-pattern-in-python3-simple-version"
  ],
  "v0.11": [
    "-*- coding: utf-8 -*-",
    "",
    "Configuration file for the Sphinx documentation builder.",
    "",
    "This file does only contain a selection of the most common options. For a",
    "full list see the documentation:",
    "http://www.sphinx-doc.org/en/stable/config",
    "-- Path setup --------------------------------------------------------------",
    "If extensions (or modules to document with autodoc) are in another directory,",
    "add these directories to sys.path here. If the directory is relative to the",
    "documentation root, use os.path.abspath to make it absolute, like shown here.",
    "",
    "-- Project information -----------------------------------------------------",
    "Version Information (for version-switcher)",
    "-- General configuration ---------------------------------------------------",
    "If your documentation needs a minimal Sphinx version, state it here.",
    "",
    "needs_sphinx = '1.0'",
    "Add any Sphinx extension module names here, as strings. They can be",
    "extensions coming with Sphinx (named 'sphinx.ext.*') or your custom",
    "ones.",
    "sphinx-panels shouldn't add bootstrap css since the pydata-sphinx-theme",
    "already loads it",
    "Add any paths that contain templates here, relative to this directory.",
    "The suffix(es) of source filenames.",
    "You can specify multiple suffix as a list of string:",
    "",
    "source_suffix = ['.rst', '.md']",
    "The master toctree document.",
    "The language for content autogenerated by Sphinx. Refer to documentation",
    "for a list of supported languages.",
    "",
    "This is also used if you do content translation via gettext catalogs.",
    "Usually you set \"language\" from the command line for these cases.",
    "List of patterns, relative to source directory, that match files and",
    "directories to ignore when looking for source files.",
    "This pattern also affects html_static_path and html_extra_path .",
    "The name of the Pygments (syntax highlighting) style to use.",
    "-- Options for HTML output -------------------------------------------------",
    "The theme to use for HTML and HTML Help pages.  See the documentation for",
    "a list of builtin themes.",
    "",
    "Theme options are theme-specific and customize the look and feel of a theme",
    "further.  For a list of options available for each theme, see the",
    "documentation.",
    "",
    "Add any paths that contain custom static files (such as style sheets) here,",
    "relative to this directory. They are copied after the builtin static files,",
    "so a file named \"default.css\" will overwrite the builtin \"default.css\".",
    "Custom sidebar templates, must be a dictionary that maps document names",
    "to template names.",
    "",
    "The default sidebars (for documents that don't match any pattern) are",
    "defined by theme itself.  Builtin themes are using these templates by",
    "default: ``['localtoc.html', 'relations.html', 'sourcelink.html',",
    "'searchbox.html']``.",
    "",
    "html_sidebars = {}",
    "-- Options for HTMLHelp output ---------------------------------------------",
    "Output file base name for HTML help builder.",
    "-- Options for LaTeX output ------------------------------------------------",
    "The paper size ('letterpaper' or 'a4paper').",
    "",
    "'papersize': 'letterpaper',",
    "The font size ('10pt', '11pt' or '12pt').",
    "",
    "'pointsize': '10pt',",
    "Additional stuff for the LaTeX preamble.",
    "",
    "'preamble': '',",
    "Latex figure (float) alignment",
    "",
    "'figure_align': 'htbp',",
    "Grouping the document tree into LaTeX files. List of tuples",
    "(source start file, target name, title,",
    "author, documentclass [howto, manual, or own class]).",
    "-- Options for manual page output ------------------------------------------",
    "One entry per manual page. List of tuples",
    "(source start file, name, description, authors, manual section).",
    "-- Options for Texinfo output ----------------------------------------------",
    "Grouping the document tree into Texinfo files. List of tuples",
    "(source start file, target name, title, author,",
    "dir menu entry, description, category)",
    "-- Options for Epub output -------------------------------------------------",
    "Bibliographic Dublin Core info.",
    "The unique identifier of the text. This can be a ISBN number",
    "or the project homepage.",
    "",
    "epub_identifier = ''",
    "A unique identification for the text.",
    "",
    "epub_uid = ''",
    "A list of files that should not be packed into the epub file.",
    "-- Extension configuration -------------------------------------------------",
    "-- Options for todo extension ----------------------------------------------",
    "If true, `todo` and `todoList` produce output, else they produce nothing.",
    "init docstrings should also be included in class",
    "Only uncomment for faster testing/building docs without compiling notebooks",
    "nbsphinx_execute = \"never\"",
    "Patch all of the published versions",
    "check old RST version (<= v0.8)",
    "Remove old version links",
    "Append updated version links",
    "requires stdin input for identify in weighting sampler",
    "will be removed",
    "applied notebook, not necessary to test each time",
    "needs xgboost too",
    "Slow Notebooks",
    "TODO: should probably move more notebooks here to ignore, because",
    "most get tested by the documentation generation.",
    "Adding the dowhy root folder to the python path so that jupyter notebooks",
    "can import dowhy",
    "\"--ExecutePreprocessor.timeout=600\",",
    "self.df = pd.read_csv(os.path.join(DATA_PATH,'dgp_1/acic_1_1_data.csv'))",
    "self.ate = np.mean(self.df['y1'] - self.df['y0'])",
    "treated = self.df[self.df['z']==1]",
    "self.att = np.mean(treated['y1'] - treated['y0'])",
    "def test_average_treatment_effect(self):",
    "est_ate = 1",
    "bias = est_ate - self.ate",
    "print(bias)",
    "self.assertAlmostEqual(self.ate, est_ate)",
    "def test_average_treatment_effect_on_treated(self):",
    "est_att = 1",
    "self.att=1",
    "bias = est_att - self.att",
    "print(bias)",
    "self.assertAlmostEqual(self.att, est_att)",
    "removing two common causes",
    "removing two common causes",
    "removing two common causes",
    "removing two common causes",
    "removing two common causes",
    "Remove graph variable with name \"W0\" from observed data.",
    "Ensure that a log record exists that provides a more detailed view",
    "of observed and unobserved graph variables (counts and variable names.)",
    "num_frontdoor_variables=1,",
    "creating nx graph instance",
    "to be used later for a test. Does not include the replace operation",
    "check if all partial R^2 values are between 0 and 1",
    "We calculate adjusted estimates for two sets of partial R^2 values.",
    "Test if hypothetical confounding by unobserved confounder u1 leads to an adjusted effect that is farther from the original estimate as compared to u2",
    "Creating a model with no unobserved confounders",
    "check if all partial R^2 values are between 0 and 1",
    "for a dataset with no confounders, the robustness value should be higher than a given threshold (0.95 in our case)",
    "Non Parametric estimator",
    "We calculate adjusted estimates for two sets of partial R^2 values.",
    "adjusted lower ate bound for confounder u1 where r2tu_w = 0.7 and r2yu_tw = 0.9",
    "adjusted lower ate bound for confounder u2 where r2tu_w = 0.2 and r2yu_tw = 0.3",
    "adjusted upper ate bound for confounder u1 where r2tu_w = 0.7 and r2yu_tw = 0.9",
    "adjusted upper ate bound for confounder u2 where r2tu_w = 0.2 and r2yu_tw = 0.3",
    "adjusted lower confidence bound for confounder u1 where r2tu_w = 0.7 and r2yu_tw = 0.9",
    "adjusted lower confidence bound for confounder u2 where r2tu_w = 0.2 and r2yu_tw = 0.3",
    "adjusted upper confidence bound for confounder u1 where r2tu_w = 0.7 and r2yu_tw = 0.9",
    "adjusted upper confidence bound for confounder u2 where r2tu_w = 0.2 and r2yu_tw = 0.3",
    "Test if hypothetical confounding by unobserved confounder u1 leads to an adjusted effect that is farther from the original estimate as compared to u2",
    "we patched figure plotting call to avoid drawing plots during tests",
    "We calculate adjusted estimates for two sets of partial R^2 values.",
    "adjusted lower ate bound for confounder u1 where r2tu_w = 0.7 and r2yu_tw = 0.9",
    "adjusted lower ate bound for confounder u2 where r2tu_w = 0.2 and r2yu_tw = 0.3",
    "adjusted upper ate bound for confounder u1 where r2tu_w = 0.7 and r2yu_tw = 0.9",
    "adjusted upper ate bound for confounder u2 where r2tu_w = 0.2 and r2yu_tw = 0.3",
    "adjusted lower confidence bound for confounder u1 where r2tu_w = 0.7 and r2yu_tw = 0.9",
    "adjusted lower confidence bound for confounder u2 where r2tu_w = 0.2 and r2yu_tw = 0.3",
    "adjusted upper confidence bound for confounder u1 where r2tu_w = 0.7 and r2yu_tw = 0.9",
    "adjusted upper confidence bound for confounder u2 where r2tu_w = 0.2 and r2yu_tw = 0.3",
    "Test if hypothetical confounding by unobserved confounder u1 leads to an adjusted effect that is farther from the original estimate as compared to u2",
    "we patched figure plotting call to avoid drawing plots during tests",
    "comparing test examples from R E-Value package",
    "check implementation of Observed Covariate E-value against R package",
    "The outcome is a linear function of the confounder",
    "The slope is 1,2 and the intercept is 3",
    "As we run with only one common cause and one instrument variable we run with (?, 2)",
    "TODO: Check directly for correct behavior, rather than checking the rules",
    "themselves, which can be non-deterministic (all the following are equivalent)",
    "Supports user-provided dataset object",
    "To test if there are any exceptions",
    "To test if the estimate is identical if refutation parameters are zero",
    "This value is hardcoded to be zero as we are runnning this on a linear dataset.",
    "Ordinarily, we should expect this value to be zero.",
    "This value is hardcoded to be zero as we are runnning this on a linear dataset.",
    "Ordinarily, we should expect this value to be zero.",
    "Only P(Y|T) should be present for test to succeed.",
    "Since undirected graph, identify effect must throw an error.",
    "Compare with ground truth",
    "Compare with ground truth",
    "Compare with ground truth",
    "Compare with ground truth",
    "Causal model initialization",
    "Obtain backdoor sets",
    "Check if backdoor sets are valid i.e. if they block all paths between the treatment and the outcome",
    "Causal model initialization",
    "Obtain backdoor sets",
    "Causal model initialization",
    "Obtain backdoor sets",
    "Check if backdoor sets are valid i.e. if they block all paths between the treatment and the outcome",
    "Causal model initialization",
    "Obtain backdoor sets",
    "Check if backdoor sets are valid i.e. if they block all paths between the treatment and the outcome",
    "Causal model initialization",
    "Obtain backdoor sets",
    "Check if backdoor sets are valid i.e. if they block all paths between the treatment and the outcome",
    "Causal model initialization",
    "Obtain backdoor sets",
    "Check if backdoor sets are valid i.e. if they block all paths between the treatment and the outcome",
    "Example is selected from Pearl J. \"Causality\" 2nd Edition, from chapter 3.3.1 on backoor criterion.",
    "The following simpsons paradox examples are taken from Pearl, J {2013}. \"Understanding Simpson\u2019s Paradox\" - http://ftp.cs.ucla.edu/pub/stat_ser/r414.pdf",
    "The following are examples given in the \"Book of Why\" by Judea Pearl, chapter \"The Do-operator and the Back-Door Criterion\"",
    "Building the causal model",
    "For all examples from these papers we use X for the treatment variable",
    "instead of A.",
    "Figure 6 from Smucler, Sapienza and Rotnitzky (2021), Biometrika",
    "Figure 5 from Smucler, Sapienza and Rotnitzky (2021), Biometrika",
    "Figure 4 from Smucler, Sapienza and Rotnitzky (2021), Biometrika",
    "Figure 3 from Smucler, Sapienza and Rotnitzky (2021), Biometrika",
    "Figure 2 from Smucler and Rotnitzky (2022), Journal of Causal Inference",
    "L replaces X as the conditional variable",
    "Figure 2 from Smucler and Rotnitzky (2022), Journal of Causal Inference",
    "L replaces X as the conditional variable. Uses different costs",
    "Figure 3 from Smucler and Rotnitzky (2022), Journal of Causal Inference",
    "A graph where optimal, optimal minimal and optimal min cost are different",
    "The graph from Shrier and Platt (2008)",
    "A graph for which the algorithm was producing wrong result due to a bug reported by Sara Taheri",
    "Another graph for which the algorithm was producing wrong result due to a bug reported by Sara Taheri",
    "cov_mat = np.diag(np.ones(num_features))",
    "collider: X->Z<-Y",
    "chain: X->Z->Y",
    "fork: X<-Z->Y",
    "general DAG: X<-Z->Y, X->Y",
    "fork: X<-Z->Y",
    "Only Gaussian component changed",
    "Just checking formats, i.e. no need for correlation.",
    "Just checking formats, i.e. no need for correlation.",
    "The MSE should be 1 due to the variance of the noise. The RMSE is accordingly 1 / var(Y).",
    "Contributions should add up to Var(X2)",
    "H(P(Y)) -- Can be precomputed",
    "-(H(P(Y | do(x_S)) - H(P(Y))) = H(P(Y)) - H(P(Y | do(x_S))",
    "H(P(Y | do(x_S)) = H(E[P(Y | x_S, X'_\\S)])",
    "E[P(Y | x_S, X'_\\S)]",
    "H(E[P(Y | x_S, X'_\\S)])",
    "Using H(P(Y)) based on the origina data, i.e. ignoring baseline_predictions.",
    "E[H(P(Y)) - H(P(Y | do(X_U))] = H(P(Y)) - E[H(P(Y | X))]",
    "Mean from the categorical part is: (-5 + 5+ 10) / 3 = 10/3",
    "Having at least one sample from the second class should not raise an error.",
    "Just some random data, since we are only interested in the omitted data.",
    "This caused an error before with pandas > 2.0",
    "C2 = 3 * A2 + 2 * B2",
    "By default, the strength is measure with respect to the variance.",
    "Here, we misspecified the mechanism on purpose by setting scale to 1 instead of 2.",
    "If we provide the observational data here, we can mitigate the misspecification of the causal mechanism.",
    "Missing connection between X0 and X1.",
    "For X0 and X1, we set the ground truth noise to further emphasize the misspecification. The inferred noise of X1",
    "would otherwise have a dependency with Z due to the missing connection with X0.",
    "Modelling connection between X0 and X1 explicitly.",
    "Here, we misspecify the mechanism on purpose by setting scale to 1 instead of 2.",
    "If we provide the observational data here, we can mitigate the misspecification of the causal mechanism.",
    "Here, changing the mechanism.",
    "Here, changing the mechanism.",
    "Here, changing the mechanism.",
    "Here, changing the mechanism.",
    "Defining an anomaly scorer that handles multidimensional inputs.",
    "Seeing that the expectation of the noise in all nodes is 0, we introduce anomalies by setting some of them to 3.",
    "In the first sample, only the first variable is anomalous. Therefore, it should have the highest contribution",
    "and it should be \"significantly\" higher than the contribution of the other ones (here, we just arbitrarily say",
    "it should be 3x higher. Due to the confounding factor Z, the reconstructed noise variables are pairwise dependent,",
    "which is a violation of our causal sufficiency assumption. However, a confounder is included here to demonstrate",
    "some robustness. Note that due to this and stochastic behaviour of the density estimator, it is",
    "not possible to analytically compute expected results. Therefore, we rather look at the relations here.",
    "Same idea for the second sample, but here, it is the second variable that is anomalous.",
    "In the fourth sample, there are 2 anomalous variables. Therefore, the contribution of these 2 variables should be",
    "\"significantly\" higher than the contribution of the other variables. The contribution of both anomalous variables",
    "should be equal (approximately).",
    "Defining an anomaly scorer that handles multidimensional inputs.",
    "Seeing that the expectation of the noise in all nodes is 0, we introduce anomalies by setting some of them to 3.",
    "reduce the score.",
    "The contributions should add up to g(x) - E[g(X)]",
    "The contributions should add up to g(x) - E[g(X)]",
    "The contributions should add up to g(x) - E[g(X)]",
    "Three examples:",
    "1. X1 is the root cause (+ 10 to the noise)",
    "2. X0 is the root cause (+ 10 to the noise)",
    "3. X0 and X3 are both root causes (+ 10 to both noises)",
    "The sum of the scores should add up to the anomaly score of the target (here, X3).",
    "Three examples:",
    "1. X1 is the root cause (+ 10 to the noise)",
    "2. X0 is the root cause (+ 10 to the noise)",
    "3. X0 and X3 are both root causes (+ 10 to both noises)",
    "The sum of the scores should add up to the anomaly score of the target (here, X3).",
    "Defining ground truth model to avoid SCM learning issues and, hence, to focus on the anomaly attribution",
    "algorithm.",
    "1. X0 is the root cause (+ 10 to the noise)",
    "2. X0 and X1 are the root causes (+ 10 to both noise)",
    "3. X2 and X3 are both root causes (+ 10 to both noises)",
    "The sum of the scores should add up to the anomaly score of the target (here, X3).",
    "collider: X->Z<-Y",
    "collider: X->Z<-Y",
    "chain: X->Y->Z",
    "chain: X->Y->Z",
    "Empty graph",
    "Full DAG",
    "DAG with single node",
    "DAG with single edge",
    "DAG with single edge",
    "chain: X->Z->Y",
    "Setup data",
    "Test LinearDML",
    "Checking that the CATE estimates are not identical",
    "Test ContinuousTreatmentOrthoForest",
    "Checking that the CATE estimates are not identical",
    "Test LinearDRLearner",
    "Test LinearDML",
    "checking that CATE estimates are not identical",
    "predict on new data",
    "Setup data",
    "Test DeepIV",
    "Test IntentToTreatDRIV",
    "Observed data",
    "assumed graph",
    "Identify effect",
    "Estimate effect",
    "A model where X is also a common cause",
    "A model where X is also a common cause",
    "The case where effect modifier is not a common cause",
    "A model where X is also a common cause",
    "Create the graph describing the causal structure",
    "Generate the data",
    "Data to df",
    "Create a model",
    "Estimate the effect with front-door",
    "Not using testsuite from .base/TestEstimtor, custom code below",
    "More cases where Exception  is expected",
    "Compute confidence intervals, standard error and significance tests",
    "Defined a linear dataset with a given set of properties",
    "Create a model that captures the same",
    "Identify the effects within the model",
    "Defined a linear dataset with a given set of properties",
    "Create a model that captures the same",
    "Identify the effects within the model",
    "Defined a linear dataset with a given set of properties",
    "Create a model that captures the same",
    "Identify the effects within the model",
    "Defined a linear dataset with a given set of properties",
    "Create a model that captures the same",
    "Identify the effects within the model",
    "Defined a linear dataset with a given set of properties",
    "Create a model that captures the same",
    "Identify the effects within the model",
    "Check if calling the method causes some import or runtime errors",
    "TODO: Plotting undirected graphs with networkx causes an error when an older networkx version is used with a newer",
    "newer matplotlib version:",
    "AttributeError: module 'matplotlib.cbook' has no attribute 'is_numlike'",
    "Networkx 2.4+ should fix this issue.",
    "plot_adjacency_matrix(causal_graph, is_directed=False)",
    "Can use these lists to specify the models/estimators/refuters that a particular interpreter supports.  Throw a ValueError if the user provides an incompatible object to intepret.",
    "Unpacking the keyword arguments",
    "todo: add docstring for common parameters here and remove from child refuter classes",
    "Default value for the number of simulations to be conducted",
    "joblib params for parallel processing",
    "Concatenate the confounders, instruments and effect modifiers",
    "Shuffle the confounders",
    "Check if all are select or deselect variables",
    "Check if all the required_variables belong to confounders, instrumental variables or effect",
    "This calculates a two-sided percentile p-value",
    "See footnotes in https://journals.sagepub.com/doi/full/10.1177/2515245920911881",
    "Get the mean for the simulations",
    "Get the standard deviation for the simulations",
    "Get the Z Score [(val - mean)/ std_dev ]",
    "Initializing the p_value",
    "Perform Bootstrap Significance Test with the original estimate and the set of refutations",
    "Perform Normal Tests of Significance with the original estimate and the set of refutations",
    "Perform Bootstrap Significance Test with the original estimate and the set of refutations",
    "Perform Normal Tests of Significance with the original estimate and the set of refutations",
    "re.sub only takes string parameter so the first if is to avoid error",
    "if the input is a text file, convert the contained data into string",
    "load dot file",
    "TODO This functionality needs to be deprecated. It is a convenience function but can introduce confusion",
    "as we are now including the option to initialize CausalGraph with DiGraph or GCM model.",
    "Adding node attributes",
    "adding penwidth to make the edge bold",
    "Adding common causes",
    "Adding instruments",
    "Adding effect modifiers",
    "Assuming the simple form of effect modifier",
    "that directly causes the outcome.",
    "self._graph.add_edge(node_name, outcome, style = \"dotted\", headport=\"s\", tailport=\"n\")",
    "self._graph.add_edge(outcome, node_name, style = \"dotted\", headport=\"n\", tailport=\"s\") # TODO make the ports more general so that they apply not just to top-bottom node configurations",
    "Adding columns in the dataframe as confounders that were not in the graph",
    "Adding unobserved confounders",
    "removal of only direct edges wrt a target is not implemented for incoming edges",
    "also return the number of backdoor paths blocked by observed nodes",
    "Assume that nodes1 is the treatment",
    "ignores new_graph parameter, always uses self._graph",
    "remove paths that have nodes1\\node1 or nodes2\\node2 as intermediate nodes",
    "TODO Refactor to remove this from here and only implement this logic in causalIdentifier. Unnecessary assumption of nodes1 to be causing nodes2.",
    "Cannot simply compute ancestors, since that will also include nodes1 and its parents (e.g. instruments)",
    "Return effect modifiers according to the graph",
    "removing all mediators",
    "Also add any effect modifiers that could not be auto-detected (e.g., they are also common causes)",
    "[TODO: double check these work with multivariate implementation:]",
    "Exclusion",
    "As-if-random setup",
    "As-if-random",
    "convert the outputted generator into a list",
    "dpaths = self.get_all_directed_paths(nodes1, nodes2)",
    "return len(dpaths) > 0",
    "Condition 1: node 1 ---> node 2 is intercepted by candidate_nodes",
    "Emit a `UserWarning` if there are any unobserved graph variables and",
    "and log a message highlighting data variables that are not part of the graph.",
    "Create causal graph object",
    "Sometimes, effect modifiers from the graph may not match those provided by the user.",
    "(Because some effect modifiers may also be common causes)",
    "In such cases, the user-provided modifiers are used.",
    "If no effect modifiers are provided,  then the ones from the graph are used.",
    "Import causal discovery class",
    "Initialize causal graph object",
    "TODO add propensity score as default backdoor method, iv as default iv method, add an informational message to show which method has been selected.",
    "TODO add dowhy as a prefix to all dowhy estimators",
    "This is done as all dowhy estimators have two parts and external ones have two or more parts",
    "Define the third-party estimation method to be used",
    "Process the dowhy estimators",
    "Check if estimator's target estimand is identified",
    "Note that while the name of the variable is the same,",
    "\"self.causal_estimator\", this estimator takes in less",
    "parameters than the same from the",
    "estimate_effect code. It is not advisable to use the",
    "estimator from this function to call estimate_effect",
    "with fit_estimator=False.",
    "Estimator had been computed in a previous call",
    "The default number of simulations for statistical testing",
    "The default number of simulations to obtain confidence intervals",
    "This should be at least 399 for a 5% error rate:",
    "https://www.econstor.eu/bitstream/10419/67820/1/587473266.pdf",
    "The portion of the total size that should be taken each time to find the confidence intervals",
    "1 is the recommended value",
    "https://ocw.mit.edu/courses/mathematics/18-05-introduction-to-probability-and-statistics-spring-2014/readings/MIT18_05S14_Reading24.pdf",
    "https://projecteuclid.org/download/pdf_1/euclid.ss/1032280214",
    "The default Confidence Level",
    "Number of quantiles to discretize continuous columns, for applying groupby",
    "Prefix to add to temporary categorical variables created after discretization",
    "std args to be removed from locals() before being passed to args_dict",
    "Setting the default interpret method",
    "Check if some parameters were set, otherwise set to default values",
    "Estimate conditional estimates by default",
    "TODO Only works for binary treatment",
    "Defaulting to class default values if parameters are not provided",
    "Checking that there is at least one effect modifier",
    "Making sure that effect_modifier_names is a list",
    "Making a copy since we are going to be changing effect modifier names",
    "For every numeric effect modifier, adding a temp categorical column",
    "Grouping by effect modifiers and computing effect separately",
    "Deleting the temporary categorical columns",
    "The array that stores the results of all estimations",
    "Find the sample size the proportion with the population size",
    "Perform the set number of simulations",
    "names of treatment and outcome",
    "Using class default parameters if not specified",
    "Checking if bootstrap_estimates are already computed",
    "Checked if any parameter is changed from the previous std error estimate",
    "Now use the data obtained from the simulations to get the value of the confidence estimates",
    "Get the variations of each bootstrap estimate and sort",
    "Now we take the (1- p)th and the (p)th variations, where p is the chosen confidence level",
    "Get the lower and upper bounds by subtracting the variations from the estimate",
    "Use existing params, if new user defined params are not present",
    "Checking if bootstrap_estimates are already computed",
    "Check if any parameter is changed from the previous std error estimate",
    "Use existing params, if new user defined params are not present",
    "Processing the null hypothesis estimates",
    "Doing a two-sided test",
    "Being conservative with the p-value reported",
    "Being conservative with the p-value reported",
    "If the estimate_index is 0, it depends on the number of simulations",
    "Need to test r-squared before supporting",
    "effect_r_squared = self._evaluate_effect_strength(estimate, method=\"r-squared\")",
    "'r-squared': effect_r_squared",
    "elif method == \"r-squared\":",
    "outcome_mean = np.mean(self._outcome)",
    "total_variance = np.sum(np.square(self._outcome - outcome_mean))",
    "Assuming a linear model with one variable: the treatment",
    "Currently only works for continuous y",
    "causal_model = outcome_mean + estimate.value*self._treatment",
    "squared_residual = np.sum(np.square(self._outcome - causal_model))",
    "r_squared = 1 - (squared_residual/total_variance)",
    "return r_squared",
    "Check if estimator's target estimand is identified",
    "Store parameters inside estimate object for refutation methods",
    "TODO: This add_params needs to move to the estimator class",
    "inside estimate_effect and estimate_conditional_effect",
    "TODO: Remove _data, _treatment_name and _outcome_name from this object",
    "we save them here to enable the methods that required these properties saved in the estimator",
    "eventually we should call those methods and just save the results in this object",
    "instead of having this object invoke the estimator methods with the data.",
    "No estimand was identified (identification failed)",
    "s += \"Variance in outcome explained by treatment: {}\\n\".format(self.effect_strength[\"r-squared\"])",
    "can't use nx.node_connected_component, because it doesn't work with DiGraphs.",
    "Hence, a manual loop:",
    "also return the number of backdoor paths blocked by observed nodes",
    "Assume that nodes1 is the treatment",
    "ignores new_graph parameter, always uses self._graph",
    "removal of only direct edges wrt a target is not implemented for incoming edges",
    "remove paths that have nodes1\\node1 or nodes2\\node2 as intermediate nodes",
    "[TODO: double check these work with multivariate implementation:]",
    "Exclusion",
    "As-if-random setup",
    "As-if-random",
    "Condition 1: node 1 ---> node 2 is intercepted by candidate_nodes",
    "dpaths = self.get_all_directed_paths(nodes1, nodes2)",
    "return len(dpaths) > 0",
    "Adding common causes",
    "Adding instruments",
    "Adding effect modifiers",
    "Assuming the simple form of effect modifier",
    "that directly causes the outcome.",
    "self._graph.add_edge(node_name, outcome, style = \"dotted\", headport=\"s\", tailport=\"n\")",
    "self._graph.add_edge(outcome, node_name, style = \"dotted\", headport=\"n\", tailport=\"s\") # TODO make the ports more general so that they apply not just to top-bottom node configurations",
    "some preprocessing steps",
    "parsing the correct graph based on input graph format",
    "load dot file",
    "Below loop assumes that the last indices of W are alwawys converted to discrete",
    "one-hot encode discrete W",
    "Now deleting the old continuous value",
    "create a dataset with 10 observations one binary treatment and a continuous outcome affected by one common cause",
    "Two continuous treatments, no common cause, an instrumental variable and two effect modifiers - linearly added appropriately",
    "One Hot Encoding",
    "TODO Ensure that we do not generate weak instruments",
    "TODO - test all our methods with random noise added to covariates (instead of the stochastic treatment assignment)",
    "Converting treatment to binary if required",
    "Generating frontdoor variables if asked for",
    "NOTE: We are assuming a linear relationship *even when t is categorical* and integer coded.",
    "For categorical t, this example dataset has the effect size for category 2 being exactly",
    "double the effect for category 1",
    "This could be changed at this stage by one-hot encoding t and using a custom beta that",
    "sets a different effect for each category {0, 1, 2}",
    "Computing ATE",
    "constructing column names for one-hot encoded discrete features",
    "Specifying the correct dtypes",
    "Now specifying the corresponding graph strings",
    "Now writing the gml graph",
    "creating data frame",
    "Specifying the correct dtypes",
    "Now specifying the corresponding graph strings",
    "Now writing the gml graph",
    "Adding edges between common causes and the frontdoor mediator",
    "Error terms",
    "else:",
    "V = 6 + W0 + tterm + E1",
    "Y = 12 + W0*W0 + W0*W0 + yterm + E2  # E2_new",
    "Generating a random normal distribution of integers",
    "Generating data for nodes which have no incoming edges",
    "\"currset\" variable currently has all the successors of the nodes which had no incoming edges",
    "assuming that all unobserved common causes are numerical and are not affected by one hot encoding",
    "Creating a NN to simulate the nuisance function",
    "strength of unobserved confounding",
    "Computing ATE",
    "Specifying the correct dtypes",
    "Now writing the gml graph",
    "The following code for loading the Lalonde dataset was copied from",
    "https://github.com/wayfair/pylift/blob/5afc9088e96f25672423663f5c9b4bb889b4dfc0/examples/Lalonde/Lalonde_sample.ipynb?short_path=b1d451f#L94-L99).",
    "",
    "Copyright 2018, Wayfair, Inc.",
    "",
    "Redistribution and use in source and binary forms, with or without modification, are permitted provided that",
    "the following conditions are met:",
    "",
    "1. Redistributions of source code must retain the above copyright notice, this list of conditions and the",
    "following disclaimer.",
    "",
    "2. Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the",
    "following disclaimer in the documentation and/or other materials provided with the distribution.",
    "",
    "THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND ANY EXPRESS OR IMPLIED",
    "WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A",
    "PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY",
    "DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,",
    "PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER",
    "CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR",
    "OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH",
    "DAMAGE.",
    "The following code is a slight modification of",
    "https://github.com/wayfair/pylift/blob/5afc9088e96f25672423663f5c9b4bb889b4dfc0/examples/Lalonde/Lalonde_sample.ipynb?short_path=b1d451f#L94-L99).",
    "",
    "Copyright 2018, Wayfair, Inc.",
    "",
    "Redistribution and use in source and binary forms, with or without modification, are permitted provided that",
    "the following conditions are met:",
    "",
    "1. Redistributions of source code must retain the above copyright notice, this list of conditions and the",
    "following disclaimer.",
    "",
    "2. Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the",
    "following disclaimer in the documentation and/or other materials provided with the distribution.",
    "",
    "THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND ANY EXPRESS OR IMPLIED",
    "WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A",
    "PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY",
    "DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,",
    "PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER",
    "CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR",
    "OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH",
    "DAMAGE.",
    "",
    "0.0.0 is standard placeholder for poetry-dynamic-versioning",
    "any changes to this should not be checked in",
    "",
    "Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved",
    "Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved",
    "Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved",
    "adapt number of channels",
    "save memory",
    "Keep same dimensions",
    "Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved",
    "single-attribute Causal",
    "test environment",
    "Subsample 2x for computational convenience",
    "Assign a binary label based on the digit",
    "Flip label with probability 0.25",
    "Assign a color based on the label; flip the color with probability environment",
    "Apply the color to the image by zeroing out the other color channel",
    "single-attribute Independent",
    "test environment",
    "Subsample 2x for computational convenience",
    "Assign a binary label based on the digit",
    "Flip label with probability 0.25",
    "multi-attribute Causal + Independent",
    "test environment",
    "Subsample 2x for computational convenience",
    "rotate the image by angle in parameter",
    "Assign a binary label based on the digit",
    "Flip label with probability 0.25",
    "Assign a color based on the label; flip the color with probability environment",
    "Apply the color to the image by zeroing out the other color channel",
    "Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved",
    "Acause regularization",
    "Aconf regularization",
    "Aind regularization",
    "Asel regularization",
    "Compile loss",
    "Check if the optimizer is currently supported",
    "Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved",
    "The currently supported estimators",
    "The default standard deviation for noise",
    "The default scaling factor to determine the bucket size",
    "The minimum number of points for the estimator to run",
    "The Default Transformation, when no arguments are given, or if the number of data points are insufficient for an estimator",
    "The Default True Causal Effect, this is taken to be ZERO by default",
    "The Default split for the number of data points that fall into the training and validation sets",
    "Assuming that outcome is one-dimensional",
    "We need to change the identified estimand",
    "We thus, make a copy. This is done as we don't want",
    "to change the original DataFrame",
    "We use collections.OrderedDict to maintain the order in which the data is stored",
    "Check if we are using an estimator in the transformation list",
    "The rationale behind ordering of the loops is the fact that we induce randomness everytime we create the",
    "Train and the Validation Datasets. Thus, we run the simulation loop followed by the training and the validation",
    "loops. Thus, we can get different values everytime we get the estimator.",
    "for _ in range( self._num_simulations ):",
    "Warn the user that the specified parameter is not applicable when no estimator is present in the transformation",
    "Adding an unobserved confounder if provided by the user",
    "We set X_train = 0 and outcome_train to be 0",
    "Get the final outcome, after running through all the values in the transformation list",
    "Check if the value of true effect has been already stored",
    "We use None as the key as we have no base category for this refutation",
    "As we currently support only one treatment",
    "If the number of data points is too few, run the default transformation: [(\"zero\",\"\"),(\"noise\", {'std_dev':1} )]",
    "Check if the value of true effect has been already stored",
    "This ensures that we calculate the causal effect only once.",
    "We use key_train as we map data with respect to the base category of the data",
    "As we currently support only one treatment",
    "Add h(t) to f(W) to get the dummy outcome",
    "We convert to ndarray for ease in indexing",
    "The data is of the form",
    "sim1: cat1 cat2 ... catn",
    "sim2: cat1 cat2 ... catn",
    "Note: We would like the causal_estimator to find the true causal estimate that we have specified through this",
    "refuter. Let the value of the true causal effect be h(t). In the following section of code, we wish to find out if h(t) falls in the",
    "distribution of the refuter.",
    "True Causal Effect list",
    "Iterating through the refutation for each category",
    "We use string arguments to account for both 32 and 64 bit varaibles",
    "action for continuous variables",
    "Action for categorical variables",
    "Find the set difference for each row",
    "Choose one out of the remaining",
    "We want to see if the estimate falls in the same distribution as the one generated by the refuter",
    "Ideally that should be the case as running bootstrap should not have a significant effect on the ability",
    "of the treatment to affect the outcome",
    "TODO: Check that the target estimand has backdoor variables?",
    "Standardizing the data",
    "Fit a model containing all confounders and compare predictions",
    "using all features compared to all features except a given",
    "confounder.",
    "Estimating the regression coefficient from standardized features to t",
    "By default, return a plot with 10 points",
    "consider 10 values of the effect of the unobserved confounder",
    "Standardizing the data",
    "Fit a model containing all confounders and compare predictions",
    "using all features compared to all features except a given",
    "confounder.",
    "By default, return a plot with 10 points",
    "consider 10 values of the effect of the unobserved confounder",
    "By default, we add the effect of simulated confounder for treatment.",
    "But subtract it from outcome to create a negative correlation",
    "assuming that the original confounder's effect was positive on both.",
    "This is to remove the effect of the original confounder.",
    "By default, we add the effect of simulated confounder for treatment.",
    "But subtract it from outcome to create a negative correlation",
    "assuming that the original confounder's effect was positive on both.",
    "This is to remove the effect of the original confounder.",
    "Obtaining the list of observed variables",
    "Taking a subset of the dataframe that has only observed variables",
    "Residuals from the outcome model obtained by fitting a linear model",
    "Residuals from the treatment model obtained by fitting a linear model",
    "Initialising product_cor_metric_observed with a really low value as finding maximum",
    "The user has an option to give the the effect_strength_on_y and effect_strength_on_t which can be then used instead of maximum correlation with treatment and outcome in the observed variables as it specifies the desired effect.",
    "Choosing a c_star based on the data.",
    "The correlations stop increasing upon increasing c_star after a certain value, that is it plateaus and we choose the value of c_star to be the value it plateaus.",
    "Choosing c1 and c2 based on the hyperbolic relationship once c_star is chosen by going over various combinations of c1 and c2 values and choosing the combination which",
    "which maintains the minimum distance between the product of correlations of the simulated variable and the product of maximum correlations of one of the observed variables",
    "and additionally checks if the ratio of the weights are such that they maintain the ratio of the maximum possible observed coefficients within some confidence interval",
    "c1_final and c2_final are initialised to the values on the hyperbolic curve such that c1_final = c2_final  and c1_final*c2_final = c_star",
    "initialising min_distance_between_product_cor_metrics to be a value greater than 1",
    "TODO (petergtz): This should introduce a Protocol defining the interface this returns instead of the concrete types",
    "If the estimator used is LinearDML, partially linear sensitivity analysis will be automatically chosen",
    "Get a 2D matrix of values",
    "x,y =  np.meshgrid(self.kappa_t, self.kappa_y) # x,y are both MxN",
    "Store the values into the refute object",
    "Adding a label on the contour line for the original estimate",
    "Label every other level using strings",
    "Default value of the p value taken for the distribution",
    "Number of Trials: Number of cointosses to understand if a sample gets the treatment",
    "Mean of the Normal Distribution",
    "Standard Deviation of the Normal Distribution",
    "Create a new column in the data by the name of placebo",
    "Sanity check the data",
    "only permute is supported for iv methods",
    "For IV methods, the estimating_instrument_names should also be",
    "changed. Create a copy to avoid modifying original object",
    "We need to change the identified estimand",
    "We make a copy as a safety measure, we don't want to change the",
    "original DataFrame",
    "Run refutation in parallel",
    "Note: We hardcode the estimate value to ZERO as we want to check if it falls in the distribution of the refuter",
    "Ideally we should expect that ZERO should fall in the distribution of the effect estimates as we have severed any causal",
    "relationship between the treatment and the outcome.",
    "new estimator",
    "new effect estimate",
    "observed covariate E-value",
    "see Table 2 and p.37 in https://dash.harvard.edu/bitstream/handle/1/36874927/EValue_FinalSubmission.pdf",
    "if CI crosses null, set its E-value to 1",
    "only report E-value for CI limit closer to null",
    "see Table 2 and p.37 in https://dash.harvard.edu/bitstream/handle/1/36874927/EValue_FinalSubmission.pdf",
    "whether the DGP is assumed to be partially linear",
    "features are the observed confounders",
    "Now code for benchmarking using covariates begins",
    "R^2 of outcome with observed common causes and treatment",
    "R^2 of treatment with observed common causes",
    "Partial R^2 of outcome after regressing over unobserved confounder, observed common causes and treatment",
    "Assuming that the difference in R2 is the same for wj and new unobserved confounder",
    "for treatment,  Calpha is not a function of the partial R2. So we need a different assumption.",
    "Assuming that the ratio of variance of alpha^2 is the same for wj and new unobserved confounder",
    "(1-ratio_var_alpha_wj) is the numerator of Calpha2, similar to the partial R2 for treatment",
    "wrt unobserved confounders in partial-linear models",
    "Do the support characterization",
    "Recover the samples that are in the support",
    "Assess overlap using propensity scores with cross-fitting",
    "Check if all supported units are considered to be in the overlap set",
    "NOTE: The original paper implements both DNF and CNF rules, but for simplicity, this code only implements DNF rules",
    "For DNF rules, a sample is covered if *any* rule applies",
    "whether the DGP is assumed to be partially linear",
    "can change this to allow default values that are same as the other parameter",
    "Strength of confounding that omitted variables generate in treatment regression",
    "computing the point estimate for the bounds",
    "common causes after removing the benchmark causes",
    "dataframe with treatment and observed common causes after removing benchmark causes",
    "R^2 of treatment with observed common causes removing benchmark causes",
    "return the variance of alpha_s",
    "R^2 of outcome with observed common causes and treatment after removing benchmark causes",
    "Obtaining theta_s (the obtained estimate)",
    "Creating numpy arrays",
    "Setting up cross-validation parameters",
    "tuple of residuals from first stage estimation [0,1], and the confounders [2]",
    "We need to estimate, sigma^2 = (Y-g_s)^2. We use the following derivation.",
    "Yres = Y - E[Y|W]",
    "E[Y|W] = f(x) + theta_s * E[T|W]",
    "Yres = Y - f(x) - theta_s * E[T|W]",
    "g(s) = theta_s * T + f(x)",
    "g(s) = theta_s * (T - E[T|W]) + f(x) + theta_s * E[T|W]",
    "g(s) = theta_s * Tres +f(x) + theta_s * E[T|W]",
    "Y - g(s) = Y - [theta_s * Tres + f(x) + theta_s * E[T|W] )",
    "Y - g(s) = ( Y - f(x) -  theta_s * E[T|W]) - theta_s * Tres",
    "Y - g(s) = Yres - theta_s * Tres",
    "nu_2 is E[alpha_s^2]",
    "Now computing scores for finding the (1-a) confidence interval",
    "R^2 of treatment with observed common causes",
    "R^2 of outcome with treatment and observed common causes",
    "Partial R^2 of outcome after regressing over unobserved confounder, observed common causes and treatment",
    "Partial R^2 of treatment after regressing over unobserved confounder and observed common causes",
    "adding 1.1 as plotting margin  ensure that the benchmarked part is shown fully in plot",
    "adding 1.1 as plotting margin  ensure that the benchmarked part is shown fully in plot",
    "Adding unadjusted point estimate",
    "Adding bounds to partial R^2 values for given strength of confounders",
    "Adding a new backdoor variable to the identified estimand",
    "Run refutation in parallel",
    "We want to see if the estimate falls in the same distribution as the one generated by the refuter",
    "Ideally that should be the case as choosing a subset should not have a significant effect on the ability",
    "of the treatment to affect the outcome",
    "TODO: Sensitivity Analyzers excluded from list due to different return type",
    "Reject H0",
    "a, b and c are all continuous variables",
    "a, b and c are all discrete variables",
    "c is set of continuous and binary variables and",
    "1. either a and b is continuous and the other is binary",
    "2. both a and b are binary",
    "c is discrete and",
    "either a or b is continuous and the other is discrete",
    "a and b are discrete and c is a mixture of discrete and continuous variables. We discretize c and calculate conditional mutual information",
    "original_treatment_name: : stores original variable names for labelling",
    "common_causes_map : maps the original variable names to variable names in OLS regression",
    "benchmark_common_causes: stores variable names in terms of regression model variables",
    "original_benchmark_covariates: stores original variable names for labelling",
    "estimate: estimate of regression",
    "degree_of_freedom: degree of freedom of error in regression",
    "standard_error: standard error in regression",
    "t_stats: Treatment coefficient t-value - measures how many standard errors the estimate is away from zero.",
    "partial_f2: value to determine if a regression model and a nested version of it have a statistically significant difference between them",
    "r2tu_w: partial R^2  of unobserved confounder \"u\" with treatment \"t\", after conditioning on observed covariates \"w\"",
    "r2yu_tw: partial R^2  of unobserved confounder \"u\" with outcome \"y\", after conditioning on observed covariates \"w\" and treatment \"t\"",
    "r2twj_w: partial R^2 of observed covariate wj with treatment \"t\", after conditioning on observed covariates \"w\" excluding wj",
    "r2ywj_tw:  partial R^2 of observed covariate wj with outcome \"y\", after conditioning on observed covariates \"w\" (excluding wj) and treatment \"t\"",
    "benchmarking_results: dataframe containing information about bounds and bias adjusted terms",
    "stats: dictionary containing information like robustness value, partial R^2, estimate, standard error , degree of freedom, partial f^2, t-statistic",
    "partial R^2 (r2yt_w) is the proportion of variation in outcome uniquely explained by treatment",
    "build a new regression model by considering treatment variables as outcome",
    "r2twj_w is partial R^2 of covariate wj with treatment \"t\", after conditioning on covariates w(excluding wj)",
    "r2ywj_tw is partial R^2 of covariate wj with outcome \"y\", after conditioning on covariates w(excluding wj) and treatment \"t\"",
    "r2tu_w is the partial r^2 from regressing u on t after conditioning on w",
    "Compute bias adjusted terms",
    "Plotting the contour plot",
    "Adding contours",
    "Adding threshold contour line",
    "Adding unadjusted point estimate",
    "Adding bounds to partial R^2 values for given strength of confounders",
    "from https://www.bnmetrics.com/blog/factory-pattern-in-python3-simple-version",
    "The default subset of the data to be used",
    "Run refutation in parallel",
    "We want to see if the estimate falls in the same distribution as the one generated by the refuter",
    "Ideally that should be the case as choosing a subset should not have a significant effect on the ability",
    "of the treatment to affect the outcome",
    "Parameters",
    "Bookkeeping",
    "Initialize estimators",
    "Convert to dataframe if not",
    "Format labels",
    "Sample from reference measure and construct features",
    "Add reference samples",
    "Binarize features (fit to data only)",
    "Fit estimator",
    "Store reference volume",
    "Construct features dataframe",
    "Construct features dataframe",
    "Iterate over columns",
    "logging.info(\"Using provided reference range for {}\".format(c))",
    "number of unique values",
    "Constant column",
    "Binary column",
    "Ordinal column (seed = counter so not correlated)",
    "For get_params / set_params",
    "Overlap / Support (y = +1), non-overlap (y = 0), and uniform background (y = -1) samples",
    "We should always have overlap samples, and either background or non-overlap samples",
    "This will throw an error if, for example, all samples are considered to",
    "be in the overlap region",
    "Initialize with empty and singleton conjunctions, i.e. X plus all-ones feature",
    "Feature indicator and conjunction matrices",
    "Iteration counter",
    "Formulate master LP",
    "Variables",
    "Objective function (no penalty on empty conjunction)",
    "Constraints",
    "This gets activated for DNF",
    "Solve problem",
    "Extract dual variables",
    "Beam search for conjunctions with negative reduced cost",
    "Most negative reduced cost among current variables",
    "Negative reduced costs found",
    "Add to existing conjunctions",
    "Reformulate master LP",
    "Variables",
    "Objective function",
    "Constraints",
    "Solve problem",
    "Extract dual variables",
    "Beam search for conjunctions with negative reduced cost",
    "Most negative reduced cost among current variables",
    "print('UB.min():', UB.min())",
    "Save generated conjunctions and coefficients",
    "Restrict conjunctions to those used by LP",
    "NOTE: This is a greedy approach, so it does not incorporate lambda0 explicitly",
    "Similarly, it will prefer a larger number of smaller rules if lambda1 is set",
    "to a larger value, because the incremental cost will be lower.",
    "Fraction of reference samples that each conjunction covers",
    "Regularization (for each conjunction)",
    "Positive samples newly covered (for each conjunction)",
    "Costs (for each conjunction)",
    "Zero out the rules and only take those which are used",
    "Small tolerance on comparisons",
    "This can be useful to break ties and favor larger values of xi",
    "Compute conjunctions of features",
    "Predict labels",
    "Use helper function",
    "Use helper function",
    "Lower bound specific to each singleton solution",
    "Initialize output",
    "Remove redundant rows by grouping by unique feature combinations and summing residual",
    "Initialize queue with root instance",
    "Separate data according to positive and negative residuals",
    "Iterate over increasing degree while queue is non-empty",
    "Initialize list of children to process",
    "Process instances in queue",
    "inst = instCurr[0]",
    "Evaluate all singleton solutions",
    "Best solutions that also improve on current output (allow for duplicate removal)",
    "Append to current output",
    "Remove duplicates",
    "Update output",
    "Compute lower bounds on higher-degree solutions",
    "Evaluate children using weighted average of their costs and LBs",
    "Best children with potential to improve on current output and current candidates (allow for duplicate removal)",
    "Iterate through best children",
    "New \"zero\" solution",
    "Check if duplicate",
    "Add to candidates for further processing",
    "Create pricing instance",
    "Remove covered rows",
    "Remove redundant features",
    "Track number of candidates added",
    "Update candidates",
    "Instances to process in next iteration",
    "Conjunctions corresponding to solutions",
    "List of categorical columns",
    "Number of quantile thresholds used to binarize ordinal features",
    "whether to append negations",
    "whether to convert thresholds on ordinal features to strings",
    "Quantile probabilities",
    "Initialize",
    "Iterate over columns",
    "number of unique values",
    "Constant or binary column",
    "Mapping to 0, 1",
    "Categorical column",
    "OneHotEncoder object",
    "Fit to observed categories",
    "Ordinal column",
    "Few unique values",
    "Thresholds are sorted unique values excluding maximum",
    "Many unique values",
    "Thresholds are quantiles excluding repetitions",
    "Contains NaN values",
    "Initialize dataframe",
    "Iterate over columns",
    "Constant or binary column",
    "Rename values to 0, 1",
    "Categorical column",
    "Apply OneHotEncoder",
    "Append negations",
    "Concatenate",
    "Ordinal column",
    "Threshold values to produce binary arrays",
    "Append negations",
    "Convert to dataframe with column labels",
    "Ensure that rows corresponding to NaN values are zeroed out",
    "Add NaN indicator column",
    "Concatenate",
    "from https://www.bnmetrics.com/blog/factory-pattern-in-python3-simple-version",
    "set attributions to zero for left out invariant nodes",
    "Get parent and child nodes",
    "Don't remove node if node has more than 1 children nodes as it can introduce",
    "hidden confounders.",
    "Remove the middle node",
    "Connect parent and child nodes",
    "Update the causal mechanism for the child nodes",
    "If the relative change of the score is less than the given threshold, we stop the estimation early.",
    "Note, the output of score_samples are log values.",
    "Note, the output of score_samples are log values.",
    "Currently only support continuous distributions for auto selection.",
    "Estimate distribution parameters from data.",
    "Ignore warnings from fitting process.",
    "Fit distribution to data.",
    "Some distributions might not be compatible with the data.",
    "Separate parts of parameters.",
    "Check the KL divergence between the distribution of the given and fitted distribution.",
    "Identify if this distribution is better.",
    "This error is typically raised when the data is discrete and all points are assigned to less cluster than",
    "specified. It can also happen due to duplicated points. In these cases, the current best solution should",
    "be sufficient.",
    "Usual feature relevance using the mean deviation as set function, i.e. g(x) - E[g(X)]",
    "Normally, we need to estimate the KL divergence jointly. However, to avoid issues with high dimensional data,",
    "we approximate it by taking the average over the marginal KL divergences.",
    "Do not compare with same model class",
    "Do not compare with same model class",
    "In case of post non-linear models, we can obtain the conditional expectation directly based on the prediction",
    "model. To do this, we can just in pass 0 as the noise, since this would evaluate Y = f(X) + 0 in case of an",
    "additive noise model and Y = g(f(X) + 0) in case of a more general model.",
    "Estimate the conditional expectation E[Y | x] by generating multiple samples for Y|x and average them.",
    "Since these are categorical values, we just need to look for the most frequent element after we drew",
    "multiple samples for each input.",
    "In the categorical case, this is equivalent to the Brier score. However, the following formulation allows",
    "categorical data with more than two classes.",
    "A convenience function when computing confidence intervals specifically for non-deterministic causal queries. This",
    "function evaluates the provided causal query multiple times to build a confidence interval based on the returned",
    "results.",
    "Note that this function does not re-fit the causal model(s) and only executes the provided query as it is. In order",
    "to re-refit the graphical causal model on random subsets of the data before executing the query, consider using the",
    "fit_and_compute function.",
    "",
    "**Example usage:**",
    "",
    ">>> gcm.fit(causal_model, data)",
    ">>> strength_medians, strength_intervals = gcm.confidence_intervals(",
    ">>>     gcm.bootstrap_sampling(gcm.arrow_strength, causal_model, target_node='Y'))",
    "",
    "In this example, gcm.confidence_intervals is expecting a callable with non-deterministic outputs for building the",
    "confidence intervals. Since each causal query potentially expects a different set of parameters, we use 'partial'",
    "here to configure the function call. In this case,",
    "gcm.bootstrap_sampling(gcm.arrow_strength, causal_model, target_node='Y') would be equivalent to",
    "lambda : gcm.arrow_strength(causal_model, target_node='Y').",
    "",
    "In order to incorporate uncertainties coming from fitting the causal model(s), we can use",
    "gcm.fit_and_compute instead:",
    ">>>  strength_medians, strength_intervals = gcm.confidence_intervals(",
    ">>>        gcm.fit_and_compute(gcm.arrow_strength,",
    ">>>                                            causal_model,",
    ">>>                                            bootstrap_training_data=data,",
    ">>>                                            target_node='Y'))",
    "This would refit the provided causal_model on a subset of the data first before executing gcm.arrow_strength in each",
    "run.",
    "Simulating interventions by propagating the effects through the graph. For this, we iterate over the nodes based",
    "on their topological order.",
    "After drawing samples of the node based on the data generation process, we apply the corresponding",
    "intervention. The inputs of downstream nodes are therefore based on the outcome of the intervention in this",
    "node.",
    "Abduction: For invertible SCMs, we recover exact noise values from data.",
    "Action + Prediction: Propagate the intervention downstream using recovered noise values.",
    "Check if we need to apply an intervention on the given node.",
    "Apply intervention function to the data of the node.",
    "Check if the intervention function changes the shape of the data.",
    "For estimating the effect, we only need to consider the nodes that have a directed path to the target node, i.e.",
    "all ancestors of the target.",
    "The target node can be a continuous real-valued variable or a categorical variable with at most two classes",
    "(i.e. binary).",
    "Avoid too many features",
    "Making sure there are at least 30% test samples.",
    "Making sure that there are at least 2 samples from one class (here, simply duplicate the point).",
    "Compare number of correct classifications.",
    "Making sure that X and Y have no overlapping values, which would lead to a distance of 0 with k=1 and, thus, to",
    "a division by zero.",
    "All elements are equal (or at least less than k samples are different)",
    "Balance the classes",
    "For estimating Shapley values for multiple samples (e.g. in feature relevance) and the number of samples",
    "is unknown beforehand.",
    "The method stops if either the change between some consecutive runs is below the given threshold or the",
    "maximum number of runs is reached.",
    "In each run, we create one random permutation of players. For instance, given 4 players, a permutation",
    "could be [3,1,4,2].",
    "Generate k random permutations by sorting the indices of the Halton sequence",
    "Create all subsets belonging to the generated permutation. This is, if we have [3,1,4,2], then the",
    "subsets are [3], [3,1], [3,1,4] [3,1,4,2].",
    "The result for each subset is cached such that if a subset that has already been evaluated appears again,",
    "we can take this result directly.",
    "To improve the runtime, multiple permutations are evaluated in each run.",
    "The current Shapley values are the average of the estimated values, i.e. we need to divide by the number",
    "of generated permutations here.",
    "In case Shapley values are estimated for multiple samples, e.g., in feature relevance. So, we have a",
    "matrix of Shapley values instead of a vector.",
    "Here, the change between consecutive runs is below the minimum threshold, but to reduce the",
    "likelihood that this just happened by chance, we require that this happens at least for",
    "num_consecutive_converged_runs times in a row.",
    "Check if change in percentage is below threshold",
    "Check for values that are exactly zero. If they don't change between two runs, we consider it as converging.",
    "Create all (unique) subsets)",
    "Assigning a 'high' weight, since this resembles \"infinity\".",
    "The weight for a subset with a specific length (see paper mentioned in the docstring for more",
    "information).",
    "TODO: Add method for auto select a bin_width/width based on the data. Make sure that the auto selection method is",
    "theoretically sound, i.e. make entropy results from different data comparable.",
    "Extremely small values can somehow result in negative values.",
    "Creating a smaller subgraph, which only contains upstream nodes that are connected to the target node.",
    "Sampling from the conditional distribution based on the current sample.",
    "Sampling from the conditional based on the current sample, but randomizing the inputs of all variables that",
    "are in the given subset. By this, we can simulate the impact on the conditional distribution when removing",
    "only the incoming edges of the variables in the subset.",
    "Creating a smaller subgraph, which only contains upstream nodes that are connected to the target node.",
    "In case of the full subset (no randomization), we get the same predictions as when we apply the",
    "prediction method to the samples of interest, since all noise samples are replaced with a sample of",
    "interest.",
    "In case of the empty subset (all are jointly randomize), it boils down to taking the average over all",
    "predictions, seeing that the randomization yields the same values for each sample of interest (none of the",
    "samples of interest are used to replace a (jointly) 'randomized' sample).",
    "Exact model",
    "This constant is used as key when storing/accessing models as causal mechanisms in graph node attributes",
    "This constant is used as key when storing the parents of a node during fitting. It's used for validation purposes",
    "afterwards.",
    "Todo: Remove after https://github.com/py-why/dowhy/pull/943.",
    "Smallest possible value. This is used in various algorithm for numerical stability.",
    "Make copy to avoid manipulating the original matrix.",
    "The given prediction method has to be evaluated multiple times on a large amount of different inputs. Typically,",
    "the batch evaluation of a prediction model on multiple inputs at the same time is significantly faster",
    "than evaluating it on single simples in a for-loop. To make use of this, we try to evaluate as many samples as",
    "possible in one batch call of the prediction method. However, this also requires a lot of memory for many samples.",
    "To overcome potential memory issues, multiple batch calls are performed, each with at most batch_size many",
    "samples. The number of samples that are evaluated is normally",
    "baseline_noise_samples.shape[0] * feature_samples.shape[0]. Here, we reduce it to",
    "batch_size * feature_samples.shape[0]. If the batch_size would be set 1, then each baseline_noise_samples is",
    "evaluated one by one in a for-loop.",
    "Each batch consist of at most batch_size * feature_samples.shape[0] many samples. If there are multiple",
    "batches, the offset indicates the index of the current baseline_noise_samples that has not been evaluated yet.",
    "If the batch size would be larger than the remaining amount of samples, it is reduced to only include the",
    "remaining baseline_noise_samples.",
    "The inputs consist of batch_size many copies of feature_samples. Here, we set the columns of the features",
    "in baseline_feature_indices to their respective values in baseline_noise_samples.",
    "After creating the (potentially large) input data matrix, we can evaluate the prediction method.",
    "Here, offset + index now indicates the sample index in baseline_noise_samples.",
    "This would average all prediction results obtained for the 'offset + index'-th sample in",
    "baseline_noise_samples. This is, y^(offset + index) = E[Y | do(x^(offset + index)_s)].",
    "This would return all prediction results obtained for the 'offset + index'-th sample in",
    "baseline_noise_samples, i.e. the results are not averaged.",
    "Making copy to ensure that the original object is not modified.",
    "Permute samples jointly. This still represents an interventional distribution.",
    "Permute samples independently.",
    "Looks for the first index where the cumulative sum of the probabilities is larger than the threshold.",
    "Note that if there are multiple indices with the same maximum value (as in this case here), the argmax",
    "function returns the first index.",
    "test local Markov condition, null hypothesis: conditional independence",
    "test edge dependence, null hypothesis: independence",
    "The order of the p-values added to the list is deterministic.",
    "To be able to validate that the graph structure did not change between fitting and causal query, we store the",
    "parents of a node during fit. That way, before sampling, we can verify the parents are still the same. While",
    "this would automatically fail when the number of parents is different, there are other more subtle cases,",
    "where the number is still the same, but it's different parents, and therefore different data. That would yield",
    "wrong results, but would not fail.",
    "Independence tests are symmetric",
    "Find out which tests to do",
    "Parallelize over tests",
    "Gather results",
    "Summarize",
    "Find out which tests to do",
    "Parallelize over tests",
    "Gather results",
    "Summarize",
    "Find out which tests to do",
    "Parallelize over tests",
    "Gather results",
    "Summarize",
    "DAG Evaluation",
    "Suggestions",
    "Append list of violations (node, non_desc) to get local information",
    "Plot histograms",
    "Plot given violations",
    "For LMC we highlight X for which X _|/|_ Y \\in ND_X | Pa_X",
    "For PD we highlight the edge (if Y\\in Anc_X -> X are adjacent)",
    "For causal minimality we highlight the edge Y \\in Pa_X -> X",
    "Create Validation header",
    "Create Validation summary",
    "Close Validation",
    "Create Suggestions header",
    "Iterate over suggestions",
    "Test if we have data for X and Y",
    "Test if we have data for Z",
    "Eq. (1) in https://arxiv.org/pdf/1810.11363.pdf",
    "Subtracting Y here since the cumulative sum includes the current element. The same reason we subtract 1",
    "from the count.",
    "Using the negative value here seeing that the Shapley estimation evaluates v(S u {i}) - v(S) for a subset S. In",
    "case of variance, we have v(S u {i}) <= v(S), which would result in a negative contribution of players to the",
    "target quantity (here, variance).",
    "Using the negative value here seeing that the Shapley estimation evaluates v(S u {i}) - v(S) for a subset S. In",
    "case of variance, we have v(S u {i}) <= v(S), which would result in a negative contribution of players to the",
    "target quantity (here, variance).",
    "Calculate Ri, the product of the residuals",
    "Standard deviation of the residuals",
    "Either X and/or Y is constant.",
    "If Z is empty, we are in the pairwise setting.",
    "Either X and/or Y is constant.",
    "If Z is empty, we are in the pairwise setting.",
    "Estimate test statistic multiple times on different permutations of the data. The p-value is then the",
    "probability (i.e. fraction) of obtaining a test statistic that is greater than statistic on the non-permuted",
    "data.",
    "Take the lower dimensional variable as target.",
    "First stage statistical model",
    "Second stage statistical model",
    "Check if the treatment is one-dimensional",
    "First stage",
    "Second Stage",
    "Combining the two estimates",
    "This same estimate is valid for frontdoor as well as mediation (NIE)",
    "Bulding the feature matrix",
    "features = sm.add_constant(features, has_constant='add') # to add an intercept term",
    "TODO move this to the identification step",
    "Obtain estimate by Wald Estimator",
    "Obtain estimate by 2SLS estimator: Cov(y,z) / Cov(x,z)",
    "More than 1 instrument. Use 2sls.",
    "Checking if Y is binary",
    "Enable the user to pass params for a custom propensity model",
    "Convert the categorical variables into dummy/indicator variables",
    "Basically, this gives a one hot encoding for each category",
    "The first category is taken to be the base line.",
    "Check if the treatment is one-dimensional",
    "Checking if the treatment is binary",
    "The model is always built on the entire data",
    "TODO make treatment_value and control value also as local parameters",
    "All treatments are set to the same constant value",
    "Fixing treatment value to the specified value, if provided",
    "treatment_vals and data_df should have same number of rows",
    "Bulding the feature matrix",
    "The model is always built on the entire data",
    "Replacing treatment values by given x",
    "First, create interventional tensor in original space",
    "Then, use pandas to ensure that the dummies are assigned correctly for a categorical treatment",
    "The average treatment effect is a combination of different",
    "regression coefficients. Complicated to compute the confidence",
    "interval analytically. For example, if y=a + b1.t + b2.tx, then",
    "the average treatment effect is b1+b2.mean(x).",
    "Refer Gelman, Hill. ARM Book. Chapter 9",
    "http://www.stat.columbia.edu/~gelman/arm/chap9.pdf",
    "TODO: Looking for contributions",
    "For a linear regression model, the causal effect of a variable is equal to the coefficient corresponding to the",
    "variable. Hence, the model by default outputs the confidence interval corresponding to treatment=1 and control=0.",
    "So for custom treatment and control values, we must multiply the confidence interval by the difference of the two.",
    "For a linear regression model, the causal effect of a variable is equal to the coefficient corresponding to the",
    "variable. Hence, the model by default outputs the standard error corresponding to treatment=1 and control=0.",
    "So for custom treatment and control values, we must multiply the standard error by the difference of the two.",
    "setting method-specific parameters",
    "Infer the right strata based on clipping threshold",
    "0.5 because there are two values for the treatment",
    "To be conservative and allow most strata to be included in the",
    "analysis",
    "At least 90% of the strata should be included in analysis",
    "sum weighted outcomes over all strata  (weight by treated population)",
    "TODO - how can we add additional information into the returned estimate?",
    "such as how much clipping was done, or per-strata info for debugging?",
    "sort the dataframe by propensity score",
    "create a column 'strata' for each element that marks what strata it belongs to",
    "for each strata, count how many treated and control units there are",
    "throw away strata that have insufficient treatment or control",
    "TODO -- fix: we are actually conditioning on positive treatment (d=1)",
    "Setting method specific parameters",
    "trim propensity score weights",
    "ips ==> (isTreated(y)/ps(y)) + ((1-isTreated(y))/(1-ps(y)))",
    "nips ==> ips / (sum of ips over all units)",
    "icps ==> ps(y)/(1-ps(y)) / (sum of (ps(y)/(1-ps(y))) over all control units)",
    "itps ==> ps(y)/(1-ps(y)) / (sum of (ps(y)/(1-ps(y))) over all treatment units)",
    "Vanilla IPS estimator",
    "The Hajek estimator (or the self-normalized estimator)",
    "Stabilized weights (from Robins, Hernan, Brumback (2000))",
    "Paper: Marginal Structural Models and Causal Inference in Epidemiology",
    "Calculating the effect",
    "Subtracting the weighted means",
    "TODO - how can we add additional information into the returned estimate?",
    "TODO -- fix: we are actually conditioning on positive treatment (d=1)",
    "Save parameters for later refutter fitting",
    "Enforcing this ordering is necessary to feed through the propensity values from dataset",
    "For metalearners only--issue a warning if w contains variables not in x",
    "Override the effect_modifiers set in CausalEstimator.__init__()",
    "Also only update self._effect_modifiers, and create a copy of self._effect_modifier_names",
    "the latter can be used by other estimator methods later",
    "Instrumental variables names, if present",
    "choosing the instrumental variable to use",
    "Calling the econml estimator's fit method",
    "As of v0.9, econml has some kewyord only arguments",
    "from https://www.bnmetrics.com/blog/factory-pattern-in-python3-simple-version",
    "Changing shape to a list for a singleton value",
    "Note that self._control_value is assumed to be a singleton value",
    "TODO -- fix: we are actually conditioning on positive treatment (d=1)",
    "For each unit, return the estimated effect of the treatment value",
    "that was actually applied to the unit",
    "this assumes a binary treatment regime",
    "TODO remove neighbors that are more than a given radius apart",
    "estimate ATT on treated by summing over difference between matched neighbors",
    "Now computing ATC",
    "TODO -- fix: we are actually conditioning on positive treatment (d=1)",
    "from https://www.bnmetrics.com/blog/factory-pattern-in-python3-simple-version",
    "Handle externally provided estimator classes",
    "allowed types of distance metric",
    "Dictionary of any user-provided params for the distance metric",
    "that will be passed to sklearn nearestneighbors",
    "Check if the treatment is one-dimensional",
    "Checking if the treatment is binary",
    "Convert the categorical variables into dummy/indicator variables",
    "Basically, this gives a one hot encoding for each category",
    "The first category is taken to be the base line.",
    "this assumes a binary treatment regime",
    "TODO remove neighbors that are more than a given radius apart",
    "estimate ATT on treated by summing over difference between matched neighbors",
    "Return indices in the original dataframe",
    "self.matched_indices_att[treated_df_index[i]] = control.iloc[indices[i]].index.tolist()",
    "Now computing ATC",
    "Return indices in the original dataframe",
    "Add the identification method used in the estimator",
    "Check the backdoor variables being used",
    "Add the observed confounders and one hot encode the categorical variables",
    "Get the data of the unobserved confounders",
    "One hot encode the data if they are categorical",
    "Check the instrumental variables involved",
    "Perform the same actions as the above",
    "Both the outcome and the treatment have to be 1D arrays according to the CausalML API",
    "We want to pass 'v0' rather than ['v0'] to prevent a shape mismatch",
    "For CATEs",
    "TODO we are conditioning on a postive treatment",
    "TODO create an expression corresponding to each estimator used",
    "Generating data with equal 0 and 1 (since ranks are uniformly distributed)",
    "Flipping some values",
    "Wrapping labels if they are too long",
    "This layout tries to mimic the graphviz layout in a simpler form. The depth grows horizontally here instead of",
    "vertically.",
    "Set the figure size based on the number of nodes",
    "Nodes that are vertically connected, but not neighbors should be connected via a curved edge.",
    "All other nodes should be connected with a straight line.",
    "Draw labels node labels",
    "Each node gets a depth assigned, based on the distance to the closest root node.",
    "In case of undirected graphs, we just take any node as root node.",
    "No path to root node, ignore this connection then.",
    "Counts the number of vertical nodes in the same layers.",
    "Creates a matrix indicating whether two nodes are vertical neighbors.",
    "Get all y coordinates per layer",
    "Sort the y-coordinates",
    "Finding p-value using student T test",
    "Only consider edges have absolute edge weight > 0.01",
    "Modify graph such that it only contains bidirected edges",
    "Find c components by finding connected components on the undirected graph",
    "Understanding Neural Network weights",
    "Refer to this link:https://stackoverflow.com/questions/50937628/mlp-classifier-neurons-weights",
    "add weight column",
    "before weights are applied we count number rows in each category",
    "which is equivalent to summing over weight=1",
    "after weights are applied we need to sum over the given weights",
    "First, calculating mean differences by strata",
    "Second, without strata",
    "Third, concatenating them and plotting",
    "Setting estimator attribute for convenience",
    "Outcome is numeric",
    "Treatments are also numeric or binary",
    "Outcome is categorical",
    "Treatments are numeric or binary",
    "TODO: A common way to show all plots",
    "from https://www.bnmetrics.com/blog/factory-pattern-in-python3-simple-version",
    "Get adjacency list",
    "If node pair has been fully explored",
    "Add node1 to backdoor set of node_pair",
    "Check if path is backdoor and does not have nodes1\\node1 or nodes2\\node2 as intermediate nodes",
    "True if arrow incoming, False if arrow outgoing",
    "Mark pair (node1, node2) complete",
    "Modify variable count and indices covered",
    "Average total effect",
    "Natural direct effect",
    "Natural indirect effect",
    "Controlled direct effect",
    "Backdoor method names",
    "First, check if there is a directed path from action to outcome",
    "## 1. BACKDOOR IDENTIFICATION",
    "Pick algorithm to compute backdoor sets according to method chosen",
    "First, checking if there are any valid backdoor adjustment sets",
    "Setting default \"backdoor\" identification adjustment set",
    "## 2. INSTRUMENTAL VARIABLE IDENTIFICATION",
    "Now checking if there is also a valid iv estimand",
    "## 3. FRONTDOOR IDENTIFICATION",
    "Now checking if there is a valid frontdoor variable",
    "Finally returning the estimand object",
    "Pick algorithm to compute backdoor sets according to method chosen",
    "Setting default \"backdoor\" identification adjustment set",
    "Finally returning the estimand object",
    "## 1. FIRST DOING BACKDOOR IDENTIFICATION",
    "First, checking if there are any valid backdoor adjustment sets",
    "Setting default \"backdoor\" identification adjustment set",
    "## 2. SECOND, CHECKING FOR MEDIATORS",
    "Now checking if there are valid mediator variables",
    "Finally returning the estimand object",
    "## 1. FIRST DOING BACKDOOR IDENTIFICATION",
    "First, checking if there are any valid backdoor adjustment sets",
    "Setting default \"backdoor\" identification adjustment set",
    "## 2. SECOND, CHECKING FOR MEDIATORS",
    "Now checking if there are valid mediator variables",
    "Finally returning the estimand object",
    "First, checking if empty set is a valid backdoor set",
    "If the method is `minimal-adjustment`, return the empty set right away.",
    "Second, checking for all other sets of variables. If include_unobserved is false, then only observed variables are eligible.",
    "only remove descendants of Y",
    "also allow any causes of Y that are not caused by T (for lower variance)",
    "remove descendants of T (mediators) and descendants of Y",
    "If var is d-separated from both treatment or outcome, it cannot",
    "be a part of the backdoor set",
    "repeat the above search with BACKDOOR_MIN",
    "If `minimal-adjustment` method is specified, start the search from the set with minimum size. Otherwise, start from the largest.",
    "If the backdoor method is `maximal-adjustment` or `minimal-adjustment`, return the first found adjustment set.",
    "If all variables are observed, and the biggest eligible set",
    "does not satisfy backdoor, then none of its subsets will.",
    "Adding a None estimand if no backdoor set found",
    "Default set contains minimum possible number of instrumental variables, to prevent lowering variance in the treatment variable.",
    "Default set is the one with the least number of adjustment variables (optimizing for efficiency)",
    "Cond 1: All directed paths intercepted by candidate_var",
    "Cond 2: No confounding between treatment and candidate var",
    "Cond 3: treatment blocks all confounding between candidate_var and outcome",
    "For simplicity, assuming a one-variable mediation set",
    "Create estimands dict as per the API for backdoor, but do not return it",
    "Setting default \"backdoor\" identification adjustment set",
    "Create estimands dict as per the API for backdoor, but do not return it",
    "Setting default \"backdoor\" identification adjustment set",
    "TODO: outputs string for now, but ideally should do symbolic",
    "expressions Mon 19 Feb 2018 04:54:17 PM DST",
    "TODO Better support for multivariate treatments",
    "TODO: support multivariate treatments better.",
    "TODO: support multivariate treatments better.",
    "TODO: support multivariate treatments better.",
    "For direct effect",
    "If no costs are passed, use uniform costs",
    "restriction to ancestors",
    "back-door graph",
    "moralization",
    "Estimators list for returning after identification",
    "Line 1",
    "If no action has been taken, the effect on Y is just the marginal of the observational distribution P(v) on Y.",
    "Line 2",
    "If we are interested in the effect on Y, it is sufficient to restrict our attention on the parts of the model ancestral to Y.",
    "Modify list of valid nodes",
    "Line 3 - forces an action on any node where such an action would have no effect on Y \u2013 assuming we already acted on X.",
    "Modify adjacency matrix to obtain that corresponding to do(X)",
    "Line 4 - Decomposes the problem into a set of smaller problems using the key property of C-component factorization of causal models.",
    "If the entire graph is a single C-component already, further problem decomposition is impossible, and we must provide base cases.",
    "Modify adjacency matrix to remove treatment variables",
    "Line 5 - The algorithms fails due to the presence of a hedge - the graph G, and a subgraph S that does not contain any X nodes.",
    "Line 6 - If there are no bidirected arcs from X to the other nodes in the current subproblem under consideration, then we can replace acting on X by conditioning, and thus solve the subproblem.",
    "Line 7 - This is the most complicated case in the algorithm. Explain in the second last paragraph on Pg 41 of the link provided in the docstring above.",
    "Do not show backdoor key unless it is the only backdoor set.",
    "Just show the default backdoor set",
    "If labels provided",
    "Return in valid DOT format",
    "Get adjacency matrix",
    "If labels not provided",
    "Obtain valid DOT format",
    "If labels provided",
    "Return in valid DOT format",
    "from https://www.bnmetrics.com/blog/factory-pattern-in-python3-simple-version",
    "from https://www.bnmetrics.com/blog/factory-pattern-in-python3-simple-version"
  ],
  "v0.10.1": [
    "-*- coding: utf-8 -*-",
    "",
    "Configuration file for the Sphinx documentation builder.",
    "",
    "This file does only contain a selection of the most common options. For a",
    "full list see the documentation:",
    "http://www.sphinx-doc.org/en/stable/config",
    "-- Path setup --------------------------------------------------------------",
    "If extensions (or modules to document with autodoc) are in another directory,",
    "add these directories to sys.path here. If the directory is relative to the",
    "documentation root, use os.path.abspath to make it absolute, like shown here.",
    "",
    "-- Project information -----------------------------------------------------",
    "Version Information (for version-switcher)",
    "-- General configuration ---------------------------------------------------",
    "If your documentation needs a minimal Sphinx version, state it here.",
    "",
    "needs_sphinx = '1.0'",
    "Add any Sphinx extension module names here, as strings. They can be",
    "extensions coming with Sphinx (named 'sphinx.ext.*') or your custom",
    "ones.",
    "sphinx-panels shouldn't add bootstrap css since the pydata-sphinx-theme",
    "already loads it",
    "Add any paths that contain templates here, relative to this directory.",
    "The suffix(es) of source filenames.",
    "You can specify multiple suffix as a list of string:",
    "",
    "source_suffix = ['.rst', '.md']",
    "The master toctree document.",
    "The language for content autogenerated by Sphinx. Refer to documentation",
    "for a list of supported languages.",
    "",
    "This is also used if you do content translation via gettext catalogs.",
    "Usually you set \"language\" from the command line for these cases.",
    "List of patterns, relative to source directory, that match files and",
    "directories to ignore when looking for source files.",
    "This pattern also affects html_static_path and html_extra_path .",
    "The name of the Pygments (syntax highlighting) style to use.",
    "-- Options for HTML output -------------------------------------------------",
    "The theme to use for HTML and HTML Help pages.  See the documentation for",
    "a list of builtin themes.",
    "",
    "Theme options are theme-specific and customize the look and feel of a theme",
    "further.  For a list of options available for each theme, see the",
    "documentation.",
    "",
    "Add any paths that contain custom static files (such as style sheets) here,",
    "relative to this directory. They are copied after the builtin static files,",
    "so a file named \"default.css\" will overwrite the builtin \"default.css\".",
    "Custom sidebar templates, must be a dictionary that maps document names",
    "to template names.",
    "",
    "The default sidebars (for documents that don't match any pattern) are",
    "defined by theme itself.  Builtin themes are using these templates by",
    "default: ``['localtoc.html', 'relations.html', 'sourcelink.html',",
    "'searchbox.html']``.",
    "",
    "html_sidebars = {}",
    "-- Options for HTMLHelp output ---------------------------------------------",
    "Output file base name for HTML help builder.",
    "-- Options for LaTeX output ------------------------------------------------",
    "The paper size ('letterpaper' or 'a4paper').",
    "",
    "'papersize': 'letterpaper',",
    "The font size ('10pt', '11pt' or '12pt').",
    "",
    "'pointsize': '10pt',",
    "Additional stuff for the LaTeX preamble.",
    "",
    "'preamble': '',",
    "Latex figure (float) alignment",
    "",
    "'figure_align': 'htbp',",
    "Grouping the document tree into LaTeX files. List of tuples",
    "(source start file, target name, title,",
    "author, documentclass [howto, manual, or own class]).",
    "-- Options for manual page output ------------------------------------------",
    "One entry per manual page. List of tuples",
    "(source start file, name, description, authors, manual section).",
    "-- Options for Texinfo output ----------------------------------------------",
    "Grouping the document tree into Texinfo files. List of tuples",
    "(source start file, target name, title, author,",
    "dir menu entry, description, category)",
    "-- Options for Epub output -------------------------------------------------",
    "Bibliographic Dublin Core info.",
    "The unique identifier of the text. This can be a ISBN number",
    "or the project homepage.",
    "",
    "epub_identifier = ''",
    "A unique identification for the text.",
    "",
    "epub_uid = ''",
    "A list of files that should not be packed into the epub file.",
    "-- Extension configuration -------------------------------------------------",
    "-- Options for todo extension ----------------------------------------------",
    "If true, `todo` and `todoList` produce output, else they produce nothing.",
    "init docstrings should also be included in class",
    "Only uncomment for faster testing/building docs without compiling notebooks",
    "nbsphinx_execute = \"never\"",
    "Patch all of the published versions",
    "check old RST version (<= v0.8)",
    "Remove old version links",
    "Append updated version links",
    "requires stdin input for identify in weighting sampler",
    "will be removed",
    "applied notebook, not necessary to test each time",
    "needs xgboost too",
    "Slow Notebooks",
    "TODO: should probably move more notebooks here to ignore, because",
    "most get tested by the documentation generation.",
    "Adding the dowhy root folder to the python path so that jupyter notebooks",
    "can import dowhy",
    "\"--ExecutePreprocessor.timeout=600\",",
    "self.df = pd.read_csv(os.path.join(DATA_PATH,'dgp_1/acic_1_1_data.csv'))",
    "self.ate = np.mean(self.df['y1'] - self.df['y0'])",
    "treated = self.df[self.df['z']==1]",
    "self.att = np.mean(treated['y1'] - treated['y0'])",
    "def test_average_treatment_effect(self):",
    "est_ate = 1",
    "bias = est_ate - self.ate",
    "print(bias)",
    "self.assertAlmostEqual(self.ate, est_ate)",
    "def test_average_treatment_effect_on_treated(self):",
    "est_att = 1",
    "self.att=1",
    "bias = est_att - self.att",
    "print(bias)",
    "self.assertAlmostEqual(self.att, est_att)",
    "removing two common causes",
    "removing two common causes",
    "removing two common causes",
    "removing two common causes",
    "Remove graph variable with name \"W0\" from observed data.",
    "Ensure that a log record exists that provides a more detailed view",
    "of observed and unobserved graph variables (counts and variable names.)",
    "check if all partial R^2 values are between 0 and 1",
    "We calculate adjusted estimates for two sets of partial R^2 values.",
    "Test if hypothetical confounding by unobserved confounder u1 leads to an adjusted effect that is farther from the original estimate as compared to u2",
    "Creating a model with no unobserved confounders",
    "check if all partial R^2 values are between 0 and 1",
    "for a dataset with no confounders, the robustness value should be higher than a given threshold (0.95 in our case)",
    "Non Parametric estimator",
    "We calculate adjusted estimates for two sets of partial R^2 values.",
    "adjusted lower ate bound for confounder u1 where r2tu_w = 0.7 and r2yu_tw = 0.9",
    "adjusted lower ate bound for confounder u2 where r2tu_w = 0.2 and r2yu_tw = 0.3",
    "adjusted upper ate bound for confounder u1 where r2tu_w = 0.7 and r2yu_tw = 0.9",
    "adjusted upper ate bound for confounder u2 where r2tu_w = 0.2 and r2yu_tw = 0.3",
    "adjusted lower confidence bound for confounder u1 where r2tu_w = 0.7 and r2yu_tw = 0.9",
    "adjusted lower confidence bound for confounder u2 where r2tu_w = 0.2 and r2yu_tw = 0.3",
    "adjusted upper confidence bound for confounder u1 where r2tu_w = 0.7 and r2yu_tw = 0.9",
    "adjusted upper confidence bound for confounder u2 where r2tu_w = 0.2 and r2yu_tw = 0.3",
    "Test if hypothetical confounding by unobserved confounder u1 leads to an adjusted effect that is farther from the original estimate as compared to u2",
    "we patched figure plotting call to avoid drawing plots during tests",
    "We calculate adjusted estimates for two sets of partial R^2 values.",
    "adjusted lower ate bound for confounder u1 where r2tu_w = 0.7 and r2yu_tw = 0.9",
    "adjusted lower ate bound for confounder u2 where r2tu_w = 0.2 and r2yu_tw = 0.3",
    "adjusted upper ate bound for confounder u1 where r2tu_w = 0.7 and r2yu_tw = 0.9",
    "adjusted upper ate bound for confounder u2 where r2tu_w = 0.2 and r2yu_tw = 0.3",
    "adjusted lower confidence bound for confounder u1 where r2tu_w = 0.7 and r2yu_tw = 0.9",
    "adjusted lower confidence bound for confounder u2 where r2tu_w = 0.2 and r2yu_tw = 0.3",
    "adjusted upper confidence bound for confounder u1 where r2tu_w = 0.7 and r2yu_tw = 0.9",
    "adjusted upper confidence bound for confounder u2 where r2tu_w = 0.2 and r2yu_tw = 0.3",
    "Test if hypothetical confounding by unobserved confounder u1 leads to an adjusted effect that is farther from the original estimate as compared to u2",
    "we patched figure plotting call to avoid drawing plots during tests",
    "comparing test examples from R E-Value package",
    "check implementation of Observed Covariate E-value against R package",
    "The outcome is a linear function of the confounder",
    "The slope is 1,2 and the intercept is 3",
    "As we run with only one common cause and one instrument variable we run with (?, 2)",
    "TODO: Check directly for correct behavior, rather than checking the rules",
    "themselves, which can be non-deterministic (all the following are equivalent)",
    "Supports user-provided dataset object",
    "To test if there are any exceptions",
    "To test if the estimate is identical if refutation parameters are zero",
    "This value is hardcoded to be zero as we are runnning this on a linear dataset.",
    "Ordinarily, we should expect this value to be zero.",
    "This value is hardcoded to be zero as we are runnning this on a linear dataset.",
    "Ordinarily, we should expect this value to be zero.",
    "Calculate causal effect twice: once for unit (t=1, c=0), once for specific increase (t=100, c=50)",
    "Only P(Y|T) should be present for test to succeed.",
    "Calculate causal effect twice: once for unit (t=1, c=0), once for specific increase (t=100, c=50)",
    "Since undirected graph, identify effect must throw an error.",
    "Calculate causal effect twice: once for unit (t=1, c=0), once for specific increase (t=100, c=50)",
    "Compare with ground truth",
    "Calculate causal effect twice: once for unit (t=1, c=0), once for specific increase (t=100, c=50)",
    "Compare with ground truth",
    "Calculate causal effect twice: once for unit (t=1, c=0), once for specific increase (t=100, c=50)",
    "Compare with ground truth",
    "Calculate causal effect twice: once for unit (t=1, c=0), once for specific increase (t=100, c=50)",
    "Compare with ground truth",
    "Causal model initialization",
    "Obtain backdoor sets",
    "Check if backdoor sets are valid i.e. if they block all paths between the treatment and the outcome",
    "Causal model initialization",
    "Obtain backdoor sets",
    "Causal model initialization",
    "Obtain backdoor sets",
    "Check if backdoor sets are valid i.e. if they block all paths between the treatment and the outcome",
    "Causal model initialization",
    "Obtain backdoor sets",
    "Check if backdoor sets are valid i.e. if they block all paths between the treatment and the outcome",
    "Causal model initialization",
    "Obtain backdoor sets",
    "Check if backdoor sets are valid i.e. if they block all paths between the treatment and the outcome",
    "Causal model initialization",
    "Obtain backdoor sets",
    "Check if backdoor sets are valid i.e. if they block all paths between the treatment and the outcome",
    "Example is selected from Pearl J. \"Causality\" 2nd Edition, from chapter 3.3.1 on backoor criterion.",
    "The following simpsons paradox examples are taken from Pearl, J {2013}. \"Understanding Simpson\u2019s Paradox\" - http://ftp.cs.ucla.edu/pub/stat_ser/r414.pdf",
    "The following are examples given in the \"Book of Why\" by Judea Pearl, chapter \"The Do-operator and the Back-Door Criterion\"",
    "For all examples from these papers we use X for the treatment variable",
    "instead of A.",
    "Figure 6 from Smucler, Sapienza and Rotnitzky (2021), Biometrika",
    "Figure 5 from Smucler, Sapienza and Rotnitzky (2021), Biometrika",
    "Figure 4 from Smucler, Sapienza and Rotnitzky (2021), Biometrika",
    "Figure 3 from Smucler, Sapienza and Rotnitzky (2021), Biometrika",
    "Figure 2 from Smucler and Rotnitzky (2022), Journal of Causal Inference",
    "L replaces X as the conditional variable",
    "Figure 2 from Smucler and Rotnitzky (2022), Journal of Causal Inference",
    "L replaces X as the conditional variable. Uses different costs",
    "Figure 3 from Smucler and Rotnitzky (2022), Journal of Causal Inference",
    "A graph where optimal, optimal minimal and optimal min cost are different",
    "The graph from Shrier and Platt (2008)",
    "A graph for which the algorithm was producing wrong result due to a bug reported by Sara Taheri",
    "Another graph for which the algorithm was producing wrong result due to a bug reported by Sara Taheri",
    "cov_mat = np.diag(np.ones(num_features))",
    "collider: X->Z<-Y",
    "chain: X->Z->Y",
    "fork: X<-Z->Y",
    "general DAG: X<-Z->Y, X->Y",
    "fork: X<-Z->Y",
    "Just checking formats, i.e. no need for correlation.",
    "Just checking formats, i.e. no need for correlation.",
    "Contributions should add up to Var(X2)",
    "H(P(Y)) -- Can be precomputed",
    "-(H(P(Y | do(x_S)) - H(P(Y))) = H(P(Y)) - H(P(Y | do(x_S))",
    "H(P(Y | do(x_S)) = H(E[P(Y | x_S, X'_\\S)])",
    "E[P(Y | x_S, X'_\\S)]",
    "H(E[P(Y | x_S, X'_\\S)])",
    "Using H(P(Y)) based on the origina data, i.e. ignoring baseline_predictions.",
    "E[H(P(Y)) - H(P(Y | do(X_U))] = H(P(Y)) - E[H(P(Y | X))]",
    "Mean from the categorical part is: (-5 + 5+ 10) / 3 = 10/3",
    "Just some random data, since we are only interested in the omitted data.",
    "This caused an error before with pandas > 2.0",
    "C2 = 3 * A2 + 2 * B2",
    "By default, the strength is measure with respect to the variance.",
    "Here, we misspecified the mechanism on purpose by setting scale to 1 instead of 2.",
    "If we provide the observational data here, we can mitigate the misspecification of the causal mechanism.",
    "Missing connection between X0 and X1.",
    "For X0 and X1, we set the ground truth noise to further emphasize the misspecification. The inferred noise of X1",
    "would otherwise have a dependency with Z due to the missing connection with X0.",
    "Modelling connection between X0 and X1 explicitly.",
    "Here, we misspecify the mechanism on purpose by setting scale to 1 instead of 2.",
    "If we provide the observational data here, we can mitigate the misspecification of the causal mechanism.",
    "Here, changing the mechanism.",
    "Here, changing the mechanism.",
    "Here, changing the mechanism.",
    "Here, changing the mechanism.",
    "Defining an anomaly scorer that handles multidimensional inputs.",
    "Seeing that the expectation of the noise in all nodes is 0, we introduce anomalies by setting some of them to 3.",
    "In the first sample, only the first variable is anomalous. Therefore, it should have the highest contribution",
    "and it should be \"significantly\" higher than the contribution of the other ones (here, we just arbitrarily say",
    "it should be 3x higher. Due to the confounding factor Z, the reconstructed noise variables are pairwise dependent,",
    "which is a violation of our causal sufficiency assumption. However, a confounder is included here to demonstrate",
    "some robustness. Note that due to this and stochastic behaviour of the density estimator, it is",
    "not possible to analytically compute expected results. Therefore, we rather look at the relations here.",
    "Same idea for the second sample, but here, it is the second variable that is anomalous.",
    "In the fourth sample, there are 2 anomalous variables. Therefore, the contribution of these 2 variables should be",
    "\"significantly\" higher than the contribution of the other variables. The contribution of both anomalous variables",
    "should be equal (approximately).",
    "Defining an anomaly scorer that handles multidimensional inputs.",
    "Seeing that the expectation of the noise in all nodes is 0, we introduce anomalies by setting some of them to 3.",
    "reduce the score.",
    "The contributions should add up to g(x) - E[g(X)]",
    "The contributions should add up to g(x) - E[g(X)]",
    "The contributions should add up to g(x) - E[g(X)]",
    "Three examples:",
    "1. X1 is the root cause (+ 10 to the noise)",
    "2. X0 is the root cause (+ 10 to the noise)",
    "3. X0 and X3 are both root causes (+ 10 to both noises)",
    "The sum of the scores should add up to the anomaly score of the target (here, X3).",
    "Three examples:",
    "1. X1 is the root cause (+ 10 to the noise)",
    "2. X0 is the root cause (+ 10 to the noise)",
    "3. X0 and X3 are both root causes (+ 10 to both noises)",
    "The sum of the scores should add up to the anomaly score of the target (here, X3).",
    "Defining ground truth model to avoid SCM learning issues and, hence, to focus on the anomaly attribution",
    "algorithm.",
    "1. X0 is the root cause (+ 10 to the noise)",
    "2. X0 and X1 are the root causes (+ 10 to both noise)",
    "3. X2 and X3 are both root causes (+ 10 to both noises)",
    "The sum of the scores should add up to the anomaly score of the target (here, X3).",
    "Copyright 2021 Amazon.com, Inc. or its affiliates. All Rights Reserved.",
    "collider: X->Z<-Y",
    "collider: X->Z<-Y",
    "chain: X->Y->Z",
    "chain: X->Y->Z",
    "Empty graph",
    "Full DAG",
    "DAG with single node",
    "DAG with single edge",
    "DAG with single edge",
    "chain: X->Z->Y",
    "Setup data",
    "Test LinearDML",
    "Test ContinuousTreatmentOrthoForest",
    "Test LinearDRLearner",
    "Setup data",
    "Test DeepIV",
    "Treatment model,",
    "Response model",
    "Test IntentToTreatDRIV",
    "Observed data",
    "assumed graph",
    "Identify effect",
    "Estimate effect",
    "A model where X is also a common cause",
    "A model where X is also a common cause",
    "The case where effect modifier is not a common cause",
    "A model where X is also a common cause",
    "Not using testsuite from .base/TestEstimtor, custom code below",
    "More cases where Exception  is expected",
    "Compute confidence intervals, standard error and significance tests",
    "Defined a linear dataset with a given set of properties",
    "Create a model that captures the same",
    "Identify the effects within the model",
    "Defined a linear dataset with a given set of properties",
    "Create a model that captures the same",
    "Identify the effects within the model",
    "Defined a linear dataset with a given set of properties",
    "Create a model that captures the same",
    "Identify the effects within the model",
    "Defined a linear dataset with a given set of properties",
    "Create a model that captures the same",
    "Identify the effects within the model",
    "Defined a linear dataset with a given set of properties",
    "Create a model that captures the same",
    "Identify the effects within the model",
    "Check if calling the method causes some import or runtime errors",
    "TODO: Plotting undirected graphs with networkx causes an error when an older networkx version is used with a newer",
    "newer matplotlib version:",
    "AttributeError: module 'matplotlib.cbook' has no attribute 'is_numlike'",
    "Networkx 2.4+ should fix this issue.",
    "plot_adjacency_matrix(causal_graph, is_directed=False)",
    "Can use these lists to specify the models/estimators/refuters that a particular interpreter supports.  Throw a ValueError if the user provides an incompatible object to intepret.",
    "Unpacking the keyword arguments",
    "todo: add docstring for common parameters here and remove from child refuter classes",
    "Default value for the number of simulations to be conducted",
    "joblib params for parallel processing",
    "Concatenate the confounders, instruments and effect modifiers",
    "Shuffle the confounders",
    "Check if all are select or deselect variables",
    "Check if all the required_variables belong to confounders, instrumental variables or effect",
    "This calculates a two-sided percentile p-value",
    "See footnotes in https://journals.sagepub.com/doi/full/10.1177/2515245920911881",
    "Get the mean for the simulations",
    "Get the standard deviation for the simulations",
    "Get the Z Score [(val - mean)/ std_dev ]",
    "Initializing the p_value",
    "Perform Bootstrap Significance Test with the original estimate and the set of refutations",
    "Perform Normal Tests of Significance with the original estimate and the set of refutations",
    "Perform Bootstrap Significance Test with the original estimate and the set of refutations",
    "Perform Normal Tests of Significance with the original estimate and the set of refutations",
    "re.sub only takes string parameter so the first if is to avoid error",
    "if the input is a text file, convert the contained data into string",
    "load dot file",
    "Adding node attributes",
    "adding penwidth to make the edge bold",
    "Adding common causes",
    "Adding instruments",
    "Adding effect modifiers",
    "Assuming the simple form of effect modifier",
    "that directly causes the outcome.",
    "self._graph.add_edge(node_name, outcome, style = \"dotted\", headport=\"s\", tailport=\"n\")",
    "self._graph.add_edge(outcome, node_name, style = \"dotted\", headport=\"n\", tailport=\"s\") # TODO make the ports more general so that they apply not just to top-bottom node configurations",
    "Adding columns in the dataframe as confounders that were not in the graph",
    "Adding unobserved confounders",
    "removal of only direct edges wrt a target is not implemented for incoming edges",
    "also return the number of backdoor paths blocked by observed nodes",
    "Assume that nodes1 is the treatment",
    "ignores new_graph parameter, always uses self._graph",
    "remove paths that have nodes1\\node1 or nodes2\\node2 as intermediate nodes",
    "TODO Refactor to remove this from here and only implement this logic in causalIdentifier. Unnecessary assumption of nodes1 to be causing nodes2.",
    "Cannot simply compute ancestors, since that will also include nodes1 and its parents (e.g. instruments)",
    "Return effect modifiers according to the graph",
    "removing all mediators",
    "Also add any effect modifiers that could not be auto-detected (e.g., they are also common causes)",
    "[TODO: double check these work with multivariate implementation:]",
    "Exclusion",
    "As-if-random setup",
    "As-if-random",
    "convert the outputted generator into a list",
    "dpaths = self.get_all_directed_paths(nodes1, nodes2)",
    "return len(dpaths) > 0",
    "Condition 1: node 1 ---> node 2 is intercepted by candidate_nodes",
    "Emit a `UserWarning` if there are any unobserved graph variables and",
    "and log a message highlighting data variables that are not part of the graph.",
    "Create causal graph object",
    "Sometimes, effect modifiers from the graph may not match those provided by the user.",
    "(Because some effect modifiers may also be common causes)",
    "In such cases, the user-provided modifiers are used.",
    "If no effect modifiers are provided,  then the ones from the graph are used.",
    "Import causal discovery class",
    "Initialize causal graph object",
    "TODO add propensity score as default backdoor method, iv as default iv method, add an informational message to show which method has been selected.",
    "TODO add dowhy as a prefix to all dowhy estimators",
    "This is done as all dowhy estimators have two parts and external ones have two or more parts",
    "Define the third-party estimation method to be used",
    "Process the dowhy estimators",
    "Check if estimator's target estimand is identified",
    "Note that while the name of the variable is the same,",
    "\"self.causal_estimator\", this estimator takes in less",
    "parameters than the same from the",
    "estimate_effect code. It is not advisable to use the",
    "estimator from this function to call estimate_effect",
    "with fit_estimator=False.",
    "Estimator had been computed in a previous call",
    "The default number of simulations for statistical testing",
    "The default number of simulations to obtain confidence intervals",
    "This should be at least 399 for a 5% error rate:",
    "https://www.econstor.eu/bitstream/10419/67820/1/587473266.pdf",
    "The portion of the total size that should be taken each time to find the confidence intervals",
    "1 is the recommended value",
    "https://ocw.mit.edu/courses/mathematics/18-05-introduction-to-probability-and-statistics-spring-2014/readings/MIT18_05S14_Reading24.pdf",
    "https://projecteuclid.org/download/pdf_1/euclid.ss/1032280214",
    "The default Confidence Level",
    "Number of quantiles to discretize continuous columns, for applying groupby",
    "Prefix to add to temporary categorical variables created after discretization",
    "std args to be removed from locals() before being passed to args_dict",
    "Setting the default interpret method",
    "Check if some parameters were set, otherwise set to default values",
    "Estimate conditional estimates by default",
    "TODO Only works for binary treatment",
    "Defaulting to class default values if parameters are not provided",
    "Checking that there is at least one effect modifier",
    "Making sure that effect_modifier_names is a list",
    "Making a copy since we are going to be changing effect modifier names",
    "For every numeric effect modifier, adding a temp categorical column",
    "Grouping by effect modifiers and computing effect separately",
    "Deleting the temporary categorical columns",
    "The array that stores the results of all estimations",
    "Find the sample size the proportion with the population size",
    "Perform the set number of simulations",
    "names of treatment and outcome",
    "Using class default parameters if not specified",
    "Checking if bootstrap_estimates are already computed",
    "Checked if any parameter is changed from the previous std error estimate",
    "Now use the data obtained from the simulations to get the value of the confidence estimates",
    "Get the variations of each bootstrap estimate and sort",
    "Now we take the (1- p)th and the (p)th variations, where p is the chosen confidence level",
    "Get the lower and upper bounds by subtracting the variations from the estimate",
    "Use existing params, if new user defined params are not present",
    "Checking if bootstrap_estimates are already computed",
    "Check if any parameter is changed from the previous std error estimate",
    "Use existing params, if new user defined params are not present",
    "Processing the null hypothesis estimates",
    "Doing a two-sided test",
    "Being conservative with the p-value reported",
    "Being conservative with the p-value reported",
    "If the estimate_index is 0, it depends on the number of simulations",
    "Need to test r-squared before supporting",
    "effect_r_squared = self._evaluate_effect_strength(estimate, method=\"r-squared\")",
    "'r-squared': effect_r_squared",
    "elif method == \"r-squared\":",
    "outcome_mean = np.mean(self._outcome)",
    "total_variance = np.sum(np.square(self._outcome - outcome_mean))",
    "Assuming a linear model with one variable: the treatment",
    "Currently only works for continuous y",
    "causal_model = outcome_mean + estimate.value*self._treatment",
    "squared_residual = np.sum(np.square(self._outcome - causal_model))",
    "r_squared = 1 - (squared_residual/total_variance)",
    "return r_squared",
    "Check if estimator's target estimand is identified",
    "Store parameters inside estimate object for refutation methods",
    "TODO: This add_params needs to move to the estimator class",
    "inside estimate_effect and estimate_conditional_effect",
    "TODO: Remove _data, _treatment_name and _outcome_name from this object",
    "we save them here to enable the methods that required these properties saved in the estimator",
    "eventually we should call those methods and just save the results in this object",
    "instead of having this object invoke the estimator methods with the data.",
    "No estimand was identified (identification failed)",
    "s += \"Variance in outcome explained by treatment: {}\\n\".format(self.effect_strength[\"r-squared\"])",
    "can't use nx.node_connected_component, because it doesn't work with DiGraphs.",
    "Hence, a manual loop:",
    "Below loop assumes that the last indices of W are alwawys converted to discrete",
    "one-hot encode discrete W",
    "Now deleting the old continuous value",
    "create a dataset with 10 observations one binary treatment and a continuous outcome affected by one common cause",
    "Two continuous treatments, no common cause, an instrumental variable and two effect modifiers - linearly added appropriately",
    "One Hot Encoding",
    "TODO Ensure that we do not generate weak instruments",
    "TODO - test all our methods with random noise added to covariates (instead of the stochastic treatment assignment)",
    "Converting treatment to binary if required",
    "Generating frontdoor variables if asked for",
    "NOTE: We are assuming a linear relationship *even when t is categorical* and integer coded.",
    "For categorical t, this example dataset has the effect size for category 2 being exactly",
    "double the effect for category 1",
    "This could be changed at this stage by one-hot encoding t and using a custom beta that",
    "sets a different effect for each category {0, 1, 2}",
    "Computing ATE",
    "constructing column names for one-hot encoded discrete features",
    "Specifying the correct dtypes",
    "Now specifying the corresponding graph strings",
    "Now writing the gml graph",
    "creating data frame",
    "Specifying the correct dtypes",
    "Now specifying the corresponding graph strings",
    "Now writing the gml graph",
    "Adding edges between common causes and the frontdoor mediator",
    "Error terms",
    "else:",
    "V = 6 + W0 + tterm + E1",
    "Y = 12 + W0*W0 + W0*W0 + yterm + E2  # E2_new",
    "Generating a random normal distribution of integers",
    "Generating data for nodes which have no incoming edges",
    "\"currset\" variable currently has all the successors of the nodes which had no incoming edges",
    "assuming that all unobserved common causes are numerical and are not affected by one hot encoding",
    "Creating a NN to simulate the nuisance function",
    "strength of unobserved confounding",
    "Computing ATE",
    "Specifying the correct dtypes",
    "Now writing the gml graph",
    "The following code for loading the Lalonde dataset was copied from",
    "https://github.com/wayfair/pylift/blob/5afc9088e96f25672423663f5c9b4bb889b4dfc0/examples/Lalonde/Lalonde_sample.ipynb?short_path=b1d451f#L94-L99).",
    "",
    "Copyright 2018, Wayfair, Inc.",
    "",
    "Redistribution and use in source and binary forms, with or without modification, are permitted provided that",
    "the following conditions are met:",
    "",
    "1. Redistributions of source code must retain the above copyright notice, this list of conditions and the",
    "following disclaimer.",
    "",
    "2. Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the",
    "following disclaimer in the documentation and/or other materials provided with the distribution.",
    "",
    "THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND ANY EXPRESS OR IMPLIED",
    "WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A",
    "PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY",
    "DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,",
    "PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER",
    "CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR",
    "OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH",
    "DAMAGE.",
    "The following code is a slight modification of",
    "https://github.com/wayfair/pylift/blob/5afc9088e96f25672423663f5c9b4bb889b4dfc0/examples/Lalonde/Lalonde_sample.ipynb?short_path=b1d451f#L94-L99).",
    "",
    "Copyright 2018, Wayfair, Inc.",
    "",
    "Redistribution and use in source and binary forms, with or without modification, are permitted provided that",
    "the following conditions are met:",
    "",
    "1. Redistributions of source code must retain the above copyright notice, this list of conditions and the",
    "following disclaimer.",
    "",
    "2. Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the",
    "following disclaimer in the documentation and/or other materials provided with the distribution.",
    "",
    "THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND ANY EXPRESS OR IMPLIED",
    "WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A",
    "PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY",
    "DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,",
    "PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER",
    "CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR",
    "OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH",
    "DAMAGE.",
    "",
    "0.0.0 is standard placeholder for poetry-dynamic-versioning",
    "any changes to this should not be checked in",
    "",
    "Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved",
    "Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved",
    "Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved",
    "adapt number of channels",
    "save memory",
    "Keep same dimensions",
    "Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved",
    "single-attribute Causal",
    "test environment",
    "Subsample 2x for computational convenience",
    "Assign a binary label based on the digit",
    "Flip label with probability 0.25",
    "Assign a color based on the label; flip the color with probability environment",
    "Apply the color to the image by zeroing out the other color channel",
    "single-attribute Independent",
    "test environment",
    "Subsample 2x for computational convenience",
    "Assign a binary label based on the digit",
    "Flip label with probability 0.25",
    "multi-attribute Causal + Independent",
    "test environment",
    "Subsample 2x for computational convenience",
    "rotate the image by angle in parameter",
    "Assign a binary label based on the digit",
    "Flip label with probability 0.25",
    "Assign a color based on the label; flip the color with probability environment",
    "Apply the color to the image by zeroing out the other color channel",
    "Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved",
    "Acause regularization",
    "Aconf regularization",
    "Aind regularization",
    "Asel regularization",
    "Compile loss",
    "Check if the optimizer is currently supported",
    "Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved",
    "The currently supported estimators",
    "The default standard deviation for noise",
    "The default scaling factor to determine the bucket size",
    "The minimum number of points for the estimator to run",
    "The Default Transformation, when no arguments are given, or if the number of data points are insufficient for an estimator",
    "The Default True Causal Effect, this is taken to be ZERO by default",
    "The Default split for the number of data points that fall into the training and validation sets",
    "Assuming that outcome is one-dimensional",
    "We need to change the identified estimand",
    "We thus, make a copy. This is done as we don't want",
    "to change the original DataFrame",
    "We use collections.OrderedDict to maintain the order in which the data is stored",
    "Check if we are using an estimator in the transformation list",
    "The rationale behind ordering of the loops is the fact that we induce randomness everytime we create the",
    "Train and the Validation Datasets. Thus, we run the simulation loop followed by the training and the validation",
    "loops. Thus, we can get different values everytime we get the estimator.",
    "for _ in range( self._num_simulations ):",
    "Warn the user that the specified parameter is not applicable when no estimator is present in the transformation",
    "Adding an unobserved confounder if provided by the user",
    "We set X_train = 0 and outcome_train to be 0",
    "Get the final outcome, after running through all the values in the transformation list",
    "Check if the value of true effect has been already stored",
    "We use None as the key as we have no base category for this refutation",
    "As we currently support only one treatment",
    "If the number of data points is too few, run the default transformation: [(\"zero\",\"\"),(\"noise\", {'std_dev':1} )]",
    "Check if the value of true effect has been already stored",
    "This ensures that we calculate the causal effect only once.",
    "We use key_train as we map data with respect to the base category of the data",
    "As we currently support only one treatment",
    "Add h(t) to f(W) to get the dummy outcome",
    "We convert to ndarray for ease in indexing",
    "The data is of the form",
    "sim1: cat1 cat2 ... catn",
    "sim2: cat1 cat2 ... catn",
    "Note: We would like the causal_estimator to find the true causal estimate that we have specified through this",
    "refuter. Let the value of the true causal effect be h(t). In the following section of code, we wish to find out if h(t) falls in the",
    "distribution of the refuter.",
    "True Causal Effect list",
    "Iterating through the refutation for each category",
    "We use string arguments to account for both 32 and 64 bit varaibles",
    "action for continuous variables",
    "Action for categorical variables",
    "Find the set difference for each row",
    "Choose one out of the remaining",
    "We want to see if the estimate falls in the same distribution as the one generated by the refuter",
    "Ideally that should be the case as running bootstrap should not have a significant effect on the ability",
    "of the treatment to affect the outcome",
    "TODO: Check that the target estimand has backdoor variables?",
    "Standardizing the data",
    "Fit a model containing all confounders and compare predictions",
    "using all features compared to all features except a given",
    "confounder.",
    "Estimating the regression coefficient from standardized features to t",
    "By default, return a plot with 10 points",
    "consider 10 values of the effect of the unobserved confounder",
    "Standardizing the data",
    "Fit a model containing all confounders and compare predictions",
    "using all features compared to all features except a given",
    "confounder.",
    "By default, return a plot with 10 points",
    "consider 10 values of the effect of the unobserved confounder",
    "By default, we add the effect of simulated confounder for treatment.",
    "But subtract it from outcome to create a negative correlation",
    "assuming that the original confounder's effect was positive on both.",
    "This is to remove the effect of the original confounder.",
    "By default, we add the effect of simulated confounder for treatment.",
    "But subtract it from outcome to create a negative correlation",
    "assuming that the original confounder's effect was positive on both.",
    "This is to remove the effect of the original confounder.",
    "Obtaining the list of observed variables",
    "Taking a subset of the dataframe that has only observed variables",
    "Residuals from the outcome model obtained by fitting a linear model",
    "Residuals from the treatment model obtained by fitting a linear model",
    "Initialising product_cor_metric_observed with a really low value as finding maximum",
    "The user has an option to give the the effect_strength_on_y and effect_strength_on_t which can be then used instead of maximum correlation with treatment and outcome in the observed variables as it specifies the desired effect.",
    "Choosing a c_star based on the data.",
    "The correlations stop increasing upon increasing c_star after a certain value, that is it plateaus and we choose the value of c_star to be the value it plateaus.",
    "Choosing c1 and c2 based on the hyperbolic relationship once c_star is chosen by going over various combinations of c1 and c2 values and choosing the combination which",
    "which maintains the minimum distance between the product of correlations of the simulated variable and the product of maximum correlations of one of the observed variables",
    "and additionally checks if the ratio of the weights are such that they maintain the ratio of the maximum possible observed coefficients within some confidence interval",
    "c1_final and c2_final are initialised to the values on the hyperbolic curve such that c1_final = c2_final  and c1_final*c2_final = c_star",
    "initialising min_distance_between_product_cor_metrics to be a value greater than 1",
    "TODO (petergtz): This should introduce a Protocol defining the interface this returns instead of the concrete types",
    "If the estimator used is LinearDML, partially linear sensitivity analysis will be automatically chosen",
    "Get a 2D matrix of values",
    "x,y =  np.meshgrid(self.kappa_t, self.kappa_y) # x,y are both MxN",
    "Store the values into the refute object",
    "Adding a label on the contour line for the original estimate",
    "Label every other level using strings",
    "Default value of the p value taken for the distribution",
    "Number of Trials: Number of cointosses to understand if a sample gets the treatment",
    "Mean of the Normal Distribution",
    "Standard Deviation of the Normal Distribution",
    "Create a new column in the data by the name of placebo",
    "Sanity check the data",
    "only permute is supported for iv methods",
    "For IV methods, the estimating_instrument_names should also be",
    "changed. Create a copy to avoid modifying original object",
    "We need to change the identified estimand",
    "We make a copy as a safety measure, we don't want to change the",
    "original DataFrame",
    "Run refutation in parallel",
    "Note: We hardcode the estimate value to ZERO as we want to check if it falls in the distribution of the refuter",
    "Ideally we should expect that ZERO should fall in the distribution of the effect estimates as we have severed any causal",
    "relationship between the treatment and the outcome.",
    "new estimator",
    "new effect estimate",
    "observed covariate E-value",
    "see Table 2 and p.37 in https://dash.harvard.edu/bitstream/handle/1/36874927/EValue_FinalSubmission.pdf",
    "if CI crosses null, set its E-value to 1",
    "only report E-value for CI limit closer to null",
    "see Table 2 and p.37 in https://dash.harvard.edu/bitstream/handle/1/36874927/EValue_FinalSubmission.pdf",
    "whether the DGP is assumed to be partially linear",
    "features are the observed confounders",
    "Now code for benchmarking using covariates begins",
    "R^2 of outcome with observed common causes and treatment",
    "R^2 of treatment with observed common causes",
    "Partial R^2 of outcome after regressing over unobserved confounder, observed common causes and treatment",
    "Assuming that the difference in R2 is the same for wj and new unobserved confounder",
    "for treatment,  Calpha is not a function of the partial R2. So we need a different assumption.",
    "Assuming that the ratio of variance of alpha^2 is the same for wj and new unobserved confounder",
    "(1-ratio_var_alpha_wj) is the numerator of Calpha2, similar to the partial R2 for treatment",
    "wrt unobserved confounders in partial-linear models",
    "Do the support characterization",
    "Recover the samples that are in the support",
    "Assess overlap using propensity scores with cross-fitting",
    "Check if all supported units are considered to be in the overlap set",
    "NOTE: The original paper implements both DNF and CNF rules, but for simplicity, this code only implements DNF rules",
    "For DNF rules, a sample is covered if *any* rule applies",
    "whether the DGP is assumed to be partially linear",
    "can change this to allow default values that are same as the other parameter",
    "Strength of confounding that omitted variables generate in treatment regression",
    "computing the point estimate for the bounds",
    "common causes after removing the benchmark causes",
    "dataframe with treatment and observed common causes after removing benchmark causes",
    "R^2 of treatment with observed common causes removing benchmark causes",
    "return the variance of alpha_s",
    "R^2 of outcome with observed common causes and treatment after removing benchmark causes",
    "Obtaining theta_s (the obtained estimate)",
    "Creating numpy arrays",
    "Setting up cross-validation parameters",
    "tuple of residuals from first stage estimation [0,1], and the confounders [2]",
    "We need to estimate, sigma^2 = (Y-g_s)^2. We use the following derivation.",
    "Yres = Y - E[Y|W]",
    "E[Y|W] = f(x) + theta_s * E[T|W]",
    "Yres = Y - f(x) - theta_s * E[T|W]",
    "g(s) = theta_s * T + f(x)",
    "g(s) = theta_s * (T - E[T|W]) + f(x) + theta_s * E[T|W]",
    "g(s) = theta_s * Tres +f(x) + theta_s * E[T|W]",
    "Y - g(s) = Y - [theta_s * Tres + f(x) + theta_s * E[T|W] )",
    "Y - g(s) = ( Y - f(x) -  theta_s * E[T|W]) - theta_s * Tres",
    "Y - g(s) = Yres - theta_s * Tres",
    "nu_2 is E[alpha_s^2]",
    "Now computing scores for finding the (1-a) confidence interval",
    "R^2 of treatment with observed common causes",
    "R^2 of outcome with treatment and observed common causes",
    "Partial R^2 of outcome after regressing over unobserved confounder, observed common causes and treatment",
    "Partial R^2 of treatment after regressing over unobserved confounder and observed common causes",
    "adding 1.1 as plotting margin  ensure that the benchmarked part is shown fully in plot",
    "adding 1.1 as plotting margin  ensure that the benchmarked part is shown fully in plot",
    "Adding unadjusted point estimate",
    "Adding bounds to partial R^2 values for given strength of confounders",
    "Adding a new backdoor variable to the identified estimand",
    "Run refutation in parallel",
    "We want to see if the estimate falls in the same distribution as the one generated by the refuter",
    "Ideally that should be the case as choosing a subset should not have a significant effect on the ability",
    "of the treatment to affect the outcome",
    "TODO: Sensitivity Analyzers excluded from list due to different return type",
    "Reject H0",
    "a, b and c are all continuous variables",
    "a, b and c are all discrete variables",
    "c is set of continuous and binary variables and",
    "1. either a and b is continuous and the other is binary",
    "2. both a and b are binary",
    "c is discrete and",
    "either a or b is continuous and the other is discrete",
    "a and b are discrete and c is a mixture of discrete and continuous variables. We discretize c and calculate conditional mutual information",
    "original_treatment_name: : stores original variable names for labelling",
    "common_causes_map : maps the original variable names to variable names in OLS regression",
    "benchmark_common_causes: stores variable names in terms of regression model variables",
    "original_benchmark_covariates: stores original variable names for labelling",
    "estimate: estimate of regression",
    "degree_of_freedom: degree of freedom of error in regression",
    "standard_error: standard error in regression",
    "t_stats: Treatment coefficient t-value - measures how many standard errors the estimate is away from zero.",
    "partial_f2: value to determine if a regression model and a nested version of it have a statistically significant difference between them",
    "r2tu_w: partial R^2  of unobserved confounder \"u\" with treatment \"t\", after conditioning on observed covariates \"w\"",
    "r2yu_tw: partial R^2  of unobserved confounder \"u\" with outcome \"y\", after conditioning on observed covariates \"w\" and treatment \"t\"",
    "r2twj_w: partial R^2 of observed covariate wj with treatment \"t\", after conditioning on observed covariates \"w\" excluding wj",
    "r2ywj_tw:  partial R^2 of observed covariate wj with outcome \"y\", after conditioning on observed covariates \"w\" (excluding wj) and treatment \"t\"",
    "benchmarking_results: dataframe containing information about bounds and bias adjusted terms",
    "stats: dictionary containing information like robustness value, partial R^2, estimate, standard error , degree of freedom, partial f^2, t-statistic",
    "partial R^2 (r2yt_w) is the proportion of variation in outcome uniquely explained by treatment",
    "build a new regression model by considering treatment variables as outcome",
    "r2twj_w is partial R^2 of covariate wj with treatment \"t\", after conditioning on covariates w(excluding wj)",
    "r2ywj_tw is partial R^2 of covariate wj with outcome \"y\", after conditioning on covariates w(excluding wj) and treatment \"t\"",
    "r2tu_w is the partial r^2 from regressing u on t after conditioning on w",
    "Compute bias adjusted terms",
    "Plotting the contour plot",
    "Adding contours",
    "Adding threshold contour line",
    "Adding unadjusted point estimate",
    "Adding bounds to partial R^2 values for given strength of confounders",
    "from https://www.bnmetrics.com/blog/factory-pattern-in-python3-simple-version",
    "The default subset of the data to be used",
    "Run refutation in parallel",
    "We want to see if the estimate falls in the same distribution as the one generated by the refuter",
    "Ideally that should be the case as choosing a subset should not have a significant effect on the ability",
    "of the treatment to affect the outcome",
    "Parameters",
    "Bookkeeping",
    "Initialize estimators",
    "Convert to dataframe if not",
    "Format labels",
    "Sample from reference measure and construct features",
    "Add reference samples",
    "Binarize features (fit to data only)",
    "Fit estimator",
    "Store reference volume",
    "Construct features dataframe",
    "Construct features dataframe",
    "Iterate over columns",
    "logging.info(\"Using provided reference range for {}\".format(c))",
    "number of unique values",
    "Constant column",
    "Binary column",
    "Ordinal column (seed = counter so not correlated)",
    "For get_params / set_params",
    "Overlap / Support (y = +1), non-overlap (y = 0), and uniform background (y = -1) samples",
    "We should always have overlap samples, and either background or non-overlap samples",
    "This will throw an error if, for example, all samples are considered to",
    "be in the overlap region",
    "Initialize with empty and singleton conjunctions, i.e. X plus all-ones feature",
    "Feature indicator and conjunction matrices",
    "Iteration counter",
    "Formulate master LP",
    "Variables",
    "Objective function (no penalty on empty conjunction)",
    "Constraints",
    "This gets activated for DNF",
    "Solve problem",
    "Extract dual variables",
    "Beam search for conjunctions with negative reduced cost",
    "Most negative reduced cost among current variables",
    "Negative reduced costs found",
    "Add to existing conjunctions",
    "Reformulate master LP",
    "Variables",
    "Objective function",
    "Constraints",
    "Solve problem",
    "Extract dual variables",
    "Beam search for conjunctions with negative reduced cost",
    "Most negative reduced cost among current variables",
    "print('UB.min():', UB.min())",
    "Save generated conjunctions and coefficients",
    "Restrict conjunctions to those used by LP",
    "NOTE: This is a greedy approach, so it does not incorporate lambda0 explicitly",
    "Similarly, it will prefer a larger number of smaller rules if lambda1 is set",
    "to a larger value, because the incremental cost will be lower.",
    "Fraction of reference samples that each conjunction covers",
    "Regularization (for each conjunction)",
    "Positive samples newly covered (for each conjunction)",
    "Costs (for each conjunction)",
    "Zero out the rules and only take those which are used",
    "Small tolerance on comparisons",
    "This can be useful to break ties and favor larger values of xi",
    "Compute conjunctions of features",
    "Predict labels",
    "Use helper function",
    "Use helper function",
    "Lower bound specific to each singleton solution",
    "Initialize output",
    "Remove redundant rows by grouping by unique feature combinations and summing residual",
    "Initialize queue with root instance",
    "Separate data according to positive and negative residuals",
    "Iterate over increasing degree while queue is non-empty",
    "Initialize list of children to process",
    "Process instances in queue",
    "inst = instCurr[0]",
    "Evaluate all singleton solutions",
    "Best solutions that also improve on current output (allow for duplicate removal)",
    "Append to current output",
    "Remove duplicates",
    "Update output",
    "Compute lower bounds on higher-degree solutions",
    "Evaluate children using weighted average of their costs and LBs",
    "Best children with potential to improve on current output and current candidates (allow for duplicate removal)",
    "Iterate through best children",
    "New \"zero\" solution",
    "Check if duplicate",
    "Add to candidates for further processing",
    "Create pricing instance",
    "Remove covered rows",
    "Remove redundant features",
    "Track number of candidates added",
    "Update candidates",
    "Instances to process in next iteration",
    "Conjunctions corresponding to solutions",
    "List of categorical columns",
    "Number of quantile thresholds used to binarize ordinal features",
    "whether to append negations",
    "whether to convert thresholds on ordinal features to strings",
    "Quantile probabilities",
    "Initialize",
    "Iterate over columns",
    "number of unique values",
    "Constant or binary column",
    "Mapping to 0, 1",
    "Categorical column",
    "OneHotEncoder object",
    "Fit to observed categories",
    "Ordinal column",
    "Few unique values",
    "Thresholds are sorted unique values excluding maximum",
    "Many unique values",
    "Thresholds are quantiles excluding repetitions",
    "Contains NaN values",
    "Initialize dataframe",
    "Iterate over columns",
    "Constant or binary column",
    "Rename values to 0, 1",
    "Categorical column",
    "Apply OneHotEncoder",
    "Append negations",
    "Concatenate",
    "Ordinal column",
    "Threshold values to produce binary arrays",
    "Append negations",
    "Convert to dataframe with column labels",
    "Ensure that rows corresponding to NaN values are zeroed out",
    "Add NaN indicator column",
    "Concatenate",
    "from https://www.bnmetrics.com/blog/factory-pattern-in-python3-simple-version",
    "set attributions to zero for left out invariant nodes",
    "Get parent and child nodes",
    "Don't remove node if node has more than 1 children nodes as it can introduce",
    "hidden confounders.",
    "Remove the middle node",
    "Connect parent and child nodes",
    "Update the causal mechanism for the child nodes",
    "If the relative change of the score is less than the given threshold, we stop the estimation early.",
    "Note, the output of score_samples are log values.",
    "Note, the output of score_samples are log values.",
    "Currently only support continuous distributions for auto selection.",
    "Estimate distribution parameters from data.",
    "Ignore warnings from fitting process.",
    "Fit distribution to data.",
    "Some distributions might not be compatible with the data.",
    "Separate parts of parameters.",
    "Check the KL divergence between the distribution of the given and fitted distribution.",
    "Identify if this distribution is better.",
    "This error is typically raised when the data is discrete and all points are assigned to less cluster than",
    "specified. It can also happen due to duplicated points. In these cases, the current best solution should",
    "be sufficient.",
    "Usual feature relevance using the mean deviation as set function, i.e. g(x) - E[g(X)]",
    "A convenience function when computing confidence intervals specifically for non-deterministic causal queries. This",
    "function evaluates the provided causal query multiple times to build a confidence interval based on the returned",
    "results.",
    "Note that this function does not re-fit the causal model(s) and only executes the provided query as it is. In order",
    "to re-refit the graphical causal model on random subsets of the data before executing the query, consider using the",
    "fit_and_compute function.",
    "",
    "**Example usage:**",
    "",
    ">>> gcm.fit(causal_model, data)",
    ">>> strength_medians, strength_intervals = gcm.confidence_intervals(",
    ">>>     gcm.bootstrap_sampling(gcm.arrow_strength, causal_model, target_node='Y'))",
    "",
    "In this example, gcm.confidence_intervals is expecting a callable with non-deterministic outputs for building the",
    "confidence intervals. Since each causal query potentially expects a different set of parameters, we use 'partial'",
    "here to configure the function call. In this case,",
    "gcm.bootstrap_sampling(gcm.arrow_strength, causal_model, target_node='Y') would be equivalent to",
    "lambda : gcm.arrow_strength(causal_model, target_node='Y').",
    "",
    "In order to incorporate uncertainties coming from fitting the causal model(s), we can use",
    "gcm.fit_and_compute instead:",
    ">>>  strength_medians, strength_intervals = gcm.confidence_intervals(",
    ">>>        gcm.fit_and_compute(gcm.arrow_strength,",
    ">>>                                            causal_model,",
    ">>>                                            bootstrap_training_data=data,",
    ">>>                                            target_node='Y'))",
    "This would refit the provided causal_model on a subset of the data first before executing gcm.arrow_strength in each",
    "run.",
    "Simulating interventions by propagating the effects through the graph. For this, we iterate over the nodes based",
    "on their topological order.",
    "After drawing samples of the node based on the data generation process, we apply the corresponding",
    "intervention. The inputs of downstream nodes are therefore based on the outcome of the intervention in this",
    "node.",
    "Abduction: For invertible SCMs, we recover exact noise values from data.",
    "Action + Prediction: Propagate the intervention downstream using recovered noise values.",
    "Check if we need to apply an intervention on the given node.",
    "Apply intervention function to the data of the node.",
    "Check if the intervention function changes the shape of the data.",
    "For estimating the effect, we only need to consider the nodes that have a directed path to the target node, i.e.",
    "all ancestors of the target.",
    "The target node can be a continuous real-valued variable or a categorical variable with at most two classes",
    "(i.e. binary).",
    "Avoid too many features",
    "Making sure there are at least 30% test samples.",
    "Making sure that there are at least 2 samples from one class (here, simply duplicate the point).",
    "Compare number of correct classifications.",
    "Making sure that X and Y have no overlapping values, which would lead to a distance of 0 with k=1 and, thus, to",
    "a division by zero.",
    "For estimating Shapley values for multiple samples (e.g. in feature relevance) and the number of samples",
    "is unknown beforehand.",
    "The method stops if either the change between some consecutive runs is below the given threshold or the",
    "maximum number of runs is reached.",
    "In each run, we create one random permutation of players. For instance, given 4 players, a permutation",
    "could be [3,1,4,2].",
    "Generate k random permutations by sorting the indices of the Halton sequence",
    "Create all subsets belonging to the generated permutation. This is, if we have [3,1,4,2], then the",
    "subsets are [3], [3,1], [3,1,4] [3,1,4,2].",
    "The result for each subset is cached such that if a subset that has already been evaluated appears again,",
    "we can take this result directly.",
    "To improve the runtime, multiple permutations are evaluated in each run.",
    "The current Shapley values are the average of the estimated values, i.e. we need to divide by the number",
    "of generated permutations here.",
    "In case Shapley values are estimated for multiple samples, e.g., in feature relevance. So, we have a",
    "matrix of Shapley values instead of a vector.",
    "Here, the change between consecutive runs is below the minimum threshold, but to reduce the",
    "likelihood that this just happened by chance, we require that this happens at least for",
    "num_consecutive_converged_runs times in a row.",
    "Check if change in percentage is below threshold",
    "Check for values that are exactly zero. If they don't change between two runs, we consider it as converging.",
    "Create all (unique) subsets)",
    "Assigning a 'high' weight, since this resembles \"infinity\".",
    "The weight for a subset with a specific length (see paper mentioned in the docstring for more",
    "information).",
    "TODO: Add method for auto select a bin_width/width based on the data. Make sure that the auto selection method is",
    "theoretically sound, i.e. make entropy results from different data comparable.",
    "Extremely small values can somehow result in negative values.",
    "Creating a smaller subgraph, which only contains upstream nodes that are connected to the target node.",
    "Sampling from the conditional distribution based on the current sample.",
    "Sampling from the conditional based on the current sample, but randomizing the inputs of all variables that",
    "are in the given subset. By this, we can simulate the impact on the conditional distribution when removing",
    "only the incoming edges of the variables in the subset.",
    "Creating a smaller subgraph, which only contains upstream nodes that are connected to the target node.",
    "In case of the full subset (no randomization), we get the same predictions as when we apply the",
    "prediction method to the samples of interest, since all noise samples are replaced with a sample of",
    "interest.",
    "In case of the empty subset (all are jointly randomize), it boils down to taking the average over all",
    "predictions, seeing that the randomization yields the same values for each sample of interest (none of the",
    "samples of interest are used to replace a (jointly) 'randomized' sample).",
    "Exact model",
    "This constant is used as key when storing/accessing models as causal mechanisms in graph node attributes",
    "This constant is used as key when storing the parents of a node during fitting. It's used for validation purposes",
    "afterwards.",
    "Smallest possible value. This is used in various algorithm for numerical stability.",
    "Make copy to avoid manipulating the original matrix.",
    "The given prediction method has to be evaluated multiple times on a large amount of different inputs. Typically,",
    "the batch evaluation of a prediction model on multiple inputs at the same time is significantly faster",
    "than evaluating it on single simples in a for-loop. To make use of this, we try to evaluate as many samples as",
    "possible in one batch call of the prediction method. However, this also requires a lot of memory for many samples.",
    "To overcome potential memory issues, multiple batch calls are performed, each with at most batch_size many",
    "samples. The number of samples that are evaluated is normally",
    "baseline_noise_samples.shape[0] * feature_samples.shape[0]. Here, we reduce it to",
    "batch_size * feature_samples.shape[0]. If the batch_size would be set 1, then each baseline_noise_samples is",
    "evaluated one by one in a for-loop.",
    "Each batch consist of at most batch_size * feature_samples.shape[0] many samples. If there are multiple",
    "batches, the offset indicates the index of the current baseline_noise_samples that has not been evaluated yet.",
    "If the batch size would be larger than the remaining amount of samples, it is reduced to only include the",
    "remaining baseline_noise_samples.",
    "The inputs consist of batch_size many copies of feature_samples. Here, we set the columns of the features",
    "in baseline_feature_indices to their respective values in baseline_noise_samples.",
    "After creating the (potentially large) input data matrix, we can evaluate the prediction method.",
    "Here, offset + index now indicates the sample index in baseline_noise_samples.",
    "This would average all prediction results obtained for the 'offset + index'-th sample in",
    "baseline_noise_samples. This is, y^(offset + index) = E[Y | do(x^(offset + index)_s)].",
    "This would return all prediction results obtained for the 'offset + index'-th sample in",
    "baseline_noise_samples, i.e. the results are not averaged.",
    "Making copy to ensure that the original object is not modified.",
    "Permute samples jointly. This still represents an interventional distribution.",
    "Permute samples independently.",
    "Looks for the first index where the cumulative sum of the probabilities is larger than the threshold.",
    "Note that if there are multiple indices with the same maximum value (as in this case here), the argmax",
    "function returns the first index.",
    "test local Markov condition, null hypothesis: conditional independence",
    "test edge dependence, null hypothesis: independence",
    "The order of the p-values added to the list is deterministic.",
    "To be able to validate that the graph structure did not change between fitting and causal query, we store the",
    "parents of a node during fit. That way, before sampling, we can verify the parents are still the same. While",
    "this would automatically fail when the number of parents is different, there are other more subtle cases,",
    "where the number is still the same, but it's different parents, and therefore different data. That would yield",
    "wrong results, but would not fail.",
    "Independence tests are symmetric",
    "Find out which tests to do",
    "Parallelize over tests",
    "Gather results",
    "Summarize",
    "Find out which tests to do",
    "Parallelize over tests",
    "Gather results",
    "Summarize",
    "Find out which tests to do",
    "Parallelize over tests",
    "Gather results",
    "Summarize",
    "DAG Evaluation",
    "Suggestions",
    "Append list of violations (node, non_desc) to get local information",
    "Plot histograms",
    "Plot given violations",
    "For LMC we highlight X for which X _|/|_ Y \\in ND_X | Pa_X",
    "For PD we highlight the edge (if Y\\in Anc_X -> X are adjacent)",
    "For causal minimality we highlight the edge Y \\in Pa_X -> X",
    "Create Validation header",
    "Create Validation summary",
    "Close Validation",
    "Create Suggestions header",
    "Iterate over suggestions",
    "Test if we have data for X and Y",
    "Test if we have data for Z",
    "Eq. (1) in https://arxiv.org/pdf/1810.11363.pdf",
    "Subtracting Y here since the cumulative sum includes the current element. The same reason we subtract 1",
    "from the count.",
    "Using the negative value here seeing that the Shapley estimation evaluates v(S u {i}) - v(S) for a subset S. In",
    "case of variance, we have v(S u {i}) <= v(S), which would result in a negative contribution of players to the",
    "target quantity (here, variance).",
    "Using the negative value here seeing that the Shapley estimation evaluates v(S u {i}) - v(S) for a subset S. In",
    "case of variance, we have v(S u {i}) <= v(S), which would result in a negative contribution of players to the",
    "target quantity (here, variance).",
    "Calculate Ri, the product of the residuals",
    "Standard deviation of the residuals",
    "Either X and/or Y is constant.",
    "If Z is empty, we are in the pairwise setting.",
    "Either X and/or Y is constant.",
    "If Z is empty, we are in the pairwise setting.",
    "Estimate test statistic multiple times on different permutations of the data. The p-value is then the",
    "probability (i.e. fraction) of obtaining a test statistic that is greater than statistic on the non-permuted",
    "data.",
    "Take the lower dimensional variable as target.",
    "First stage statistical model",
    "Second stage statistical model",
    "Check if the treatment is one-dimensional",
    "First stage",
    "Second Stage",
    "Combining the two estimates",
    "This same estimate is valid for frontdoor as well as mediation (NIE)",
    "Bulding the feature matrix",
    "features = sm.add_constant(features, has_constant='add') # to add an intercept term",
    "TODO move this to the identification step",
    "Obtain estimate by Wald Estimator",
    "Obtain estimate by 2SLS estimator: Cov(y,z) / Cov(x,z)",
    "More than 1 instrument. Use 2sls.",
    "Checking if Y is binary",
    "Enable the user to pass params for a custom propensity model",
    "Convert the categorical variables into dummy/indicator variables",
    "Basically, this gives a one hot encoding for each category",
    "The first category is taken to be the base line.",
    "Check if the treatment is one-dimensional",
    "Checking if the treatment is binary",
    "The model is always built on the entire data",
    "TODO make treatment_value and control value also as local parameters",
    "All treatments are set to the same constant value",
    "Fixing treatment value to the specified value, if provided",
    "treatment_vals and data_df should have same number of rows",
    "Bulding the feature matrix",
    "The model is always built on the entire data",
    "Replacing treatment values by given x",
    "First, create interventional tensor in original space",
    "Then, use pandas to ensure that the dummies are assigned correctly for a categorical treatment",
    "The average treatment effect is a combination of different",
    "regression coefficients. Complicated to compute the confidence",
    "interval analytically. For example, if y=a + b1.t + b2.tx, then",
    "the average treatment effect is b1+b2.mean(x).",
    "Refer Gelman, Hill. ARM Book. Chapter 9",
    "http://www.stat.columbia.edu/~gelman/arm/chap9.pdf",
    "TODO: Looking for contributions",
    "For a linear regression model, the causal effect of a variable is equal to the coefficient corresponding to the",
    "variable. Hence, the model by default outputs the confidence interval corresponding to treatment=1 and control=0.",
    "So for custom treatment and control values, we must multiply the confidence interval by the difference of the two.",
    "For a linear regression model, the causal effect of a variable is equal to the coefficient corresponding to the",
    "variable. Hence, the model by default outputs the standard error corresponding to treatment=1 and control=0.",
    "So for custom treatment and control values, we must multiply the standard error by the difference of the two.",
    "setting method-specific parameters",
    "Infer the right strata based on clipping threshold",
    "0.5 because there are two values for the treatment",
    "To be conservative and allow most strata to be included in the",
    "analysis",
    "At least 90% of the strata should be included in analysis",
    "sum weighted outcomes over all strata  (weight by treated population)",
    "TODO - how can we add additional information into the returned estimate?",
    "such as how much clipping was done, or per-strata info for debugging?",
    "sort the dataframe by propensity score",
    "create a column 'strata' for each element that marks what strata it belongs to",
    "for each strata, count how many treated and control units there are",
    "throw away strata that have insufficient treatment or control",
    "TODO -- fix: we are actually conditioning on positive treatment (d=1)",
    "Setting method specific parameters",
    "trim propensity score weights",
    "ips ==> (isTreated(y)/ps(y)) + ((1-isTreated(y))/(1-ps(y)))",
    "nips ==> ips / (sum of ips over all units)",
    "icps ==> ps(y)/(1-ps(y)) / (sum of (ps(y)/(1-ps(y))) over all control units)",
    "itps ==> ps(y)/(1-ps(y)) / (sum of (ps(y)/(1-ps(y))) over all treatment units)",
    "Vanilla IPS estimator",
    "The Hajek estimator (or the self-normalized estimator)",
    "Stabilized weights (from Robins, Hernan, Brumback (2000))",
    "Paper: Marginal Structural Models and Causal Inference in Epidemiology",
    "Calculating the effect",
    "Subtracting the weighted means",
    "TODO - how can we add additional information into the returned estimate?",
    "TODO -- fix: we are actually conditioning on positive treatment (d=1)",
    "Save parameters for later refutter fitting",
    "Enforcing this ordering is necessary to feed through the propensity values from dataset",
    "For metalearners only--issue a warning if w contains variables not in x",
    "Override the effect_modifiers set in CausalEstimator.__init__()",
    "Also only update self._effect_modifiers, and create a copy of self._effect_modifier_names",
    "the latter can be used by other estimator methods later",
    "Instrumental variables names, if present",
    "choosing the instrumental variable to use",
    "Calling the econml estimator's fit method",
    "As of v0.9, econml has some kewyord only arguments",
    "from https://www.bnmetrics.com/blog/factory-pattern-in-python3-simple-version",
    "Changing shape to a list for a singleton value",
    "Note that self._control_value is assumed to be a singleton value",
    "TODO -- fix: we are actually conditioning on positive treatment (d=1)",
    "For each unit, return the estimated effect of the treatment value",
    "that was actually applied to the unit",
    "this assumes a binary treatment regime",
    "TODO remove neighbors that are more than a given radius apart",
    "estimate ATT on treated by summing over difference between matched neighbors",
    "Now computing ATC",
    "TODO -- fix: we are actually conditioning on positive treatment (d=1)",
    "from https://www.bnmetrics.com/blog/factory-pattern-in-python3-simple-version",
    "Handle externally provided estimator classes",
    "allowed types of distance metric",
    "Dictionary of any user-provided params for the distance metric",
    "that will be passed to sklearn nearestneighbors",
    "Check if the treatment is one-dimensional",
    "Checking if the treatment is binary",
    "Convert the categorical variables into dummy/indicator variables",
    "Basically, this gives a one hot encoding for each category",
    "The first category is taken to be the base line.",
    "this assumes a binary treatment regime",
    "TODO remove neighbors that are more than a given radius apart",
    "estimate ATT on treated by summing over difference between matched neighbors",
    "Return indices in the original dataframe",
    "self.matched_indices_att[treated_df_index[i]] = control.iloc[indices[i]].index.tolist()",
    "Now computing ATC",
    "Return indices in the original dataframe",
    "Add the identification method used in the estimator",
    "Check the backdoor variables being used",
    "Add the observed confounders and one hot encode the categorical variables",
    "Get the data of the unobserved confounders",
    "One hot encode the data if they are categorical",
    "Check the instrumental variables involved",
    "Perform the same actions as the above",
    "Both the outcome and the treatment have to be 1D arrays according to the CausalML API",
    "We want to pass 'v0' rather than ['v0'] to prevent a shape mismatch",
    "For CATEs",
    "TODO we are conditioning on a postive treatment",
    "TODO create an expression corresponding to each estimator used",
    "Generating data with equal 0 and 1 (since ranks are uniformly distributed)",
    "Flipping some values",
    "Wrapping labels if they are too long",
    "This layout tries to mimic the graphviz layout in a simpler form. The depth grows horizontally here instead of",
    "vertically.",
    "Set the figure size based on the number of nodes",
    "Nodes that are vertically connected, but not neighbors should be connected via a curved edge.",
    "All other nodes should be connected with a straight line.",
    "Draw labels node labels",
    "Each node gets a depth assigned, based on the distance to the closest root node.",
    "In case of undirected graphs, we just take any node as root node.",
    "No path to root node, ignore this connection then.",
    "Counts the number of vertical nodes in the same layers.",
    "Creates a matrix indicating whether two nodes are vertical neighbors.",
    "Get all y coordinates per layer",
    "Sort the y-coordinates",
    "Finding p-value using student T test",
    "Only consider edges have absolute edge weight > 0.01",
    "Modify graph such that it only contains bidirected edges",
    "Find c components by finding connected components on the undirected graph",
    "Understanding Neural Network weights",
    "Refer to this link:https://stackoverflow.com/questions/50937628/mlp-classifier-neurons-weights",
    "add weight column",
    "before weights are applied we count number rows in each category",
    "which is equivalent to summing over weight=1",
    "after weights are applied we need to sum over the given weights",
    "First, calculating mean differences by strata",
    "Second, without strata",
    "Third, concatenating them and plotting",
    "Setting estimator attribute for convenience",
    "Outcome is numeric",
    "Treatments are also numeric or binary",
    "Outcome is categorical",
    "Treatments are numeric or binary",
    "TODO: A common way to show all plots",
    "from https://www.bnmetrics.com/blog/factory-pattern-in-python3-simple-version",
    "Get adjacency list",
    "If node pair has been fully explored",
    "Add node1 to backdoor set of node_pair",
    "Check if path is backdoor and does not have nodes1\\node1 or nodes2\\node2 as intermediate nodes",
    "True if arrow incoming, False if arrow outgoing",
    "Mark pair (node1, node2) complete",
    "Modify variable count and indices covered",
    "Average total effect",
    "Natural direct effect",
    "Natural indirect effect",
    "Controlled direct effect",
    "Backdoor method names",
    "First, check if there is a directed path from action to outcome",
    "## 1. BACKDOOR IDENTIFICATION",
    "Pick algorithm to compute backdoor sets according to method chosen",
    "First, checking if there are any valid backdoor adjustment sets",
    "Setting default \"backdoor\" identification adjustment set",
    "## 2. INSTRUMENTAL VARIABLE IDENTIFICATION",
    "Now checking if there is also a valid iv estimand",
    "## 3. FRONTDOOR IDENTIFICATION",
    "Now checking if there is a valid frontdoor variable",
    "Finally returning the estimand object",
    "Pick algorithm to compute backdoor sets according to method chosen",
    "Setting default \"backdoor\" identification adjustment set",
    "Finally returning the estimand object",
    "## 1. FIRST DOING BACKDOOR IDENTIFICATION",
    "First, checking if there are any valid backdoor adjustment sets",
    "Setting default \"backdoor\" identification adjustment set",
    "## 2. SECOND, CHECKING FOR MEDIATORS",
    "Now checking if there are valid mediator variables",
    "Finally returning the estimand object",
    "## 1. FIRST DOING BACKDOOR IDENTIFICATION",
    "First, checking if there are any valid backdoor adjustment sets",
    "Setting default \"backdoor\" identification adjustment set",
    "## 2. SECOND, CHECKING FOR MEDIATORS",
    "Now checking if there are valid mediator variables",
    "Finally returning the estimand object",
    "First, checking if empty set is a valid backdoor set",
    "If the method is `minimal-adjustment`, return the empty set right away.",
    "Second, checking for all other sets of variables. If include_unobserved is false, then only observed variables are eligible.",
    "only remove descendants of Y",
    "also allow any causes of Y that are not caused by T (for lower variance)",
    "remove descendants of T (mediators) and descendants of Y",
    "If var is d-separated from both treatment or outcome, it cannot",
    "be a part of the backdoor set",
    "repeat the above search with BACKDOOR_MIN",
    "If `minimal-adjustment` method is specified, start the search from the set with minimum size. Otherwise, start from the largest.",
    "If the backdoor method is `maximal-adjustment` or `minimal-adjustment`, return the first found adjustment set.",
    "If all variables are observed, and the biggest eligible set",
    "does not satisfy backdoor, then none of its subsets will.",
    "Adding a None estimand if no backdoor set found",
    "Default set contains minimum possible number of instrumental variables, to prevent lowering variance in the treatment variable.",
    "Default set is the one with the least number of adjustment variables (optimizing for efficiency)",
    "For simplicity, assuming a one-variable frontdoor set",
    "Cond 1: All directed paths intercepted by candidate_var",
    "Cond 2: No confounding between treatment and candidate var",
    "Cond 3: treatment blocks all confounding between candidate_var and outcome",
    "For simplicity, assuming a one-variable mediation set",
    "Create estimands dict as per the API for backdoor, but do not return it",
    "Setting default \"backdoor\" identification adjustment set",
    "Create estimands dict as per the API for backdoor, but do not return it",
    "Setting default \"backdoor\" identification adjustment set",
    "TODO: outputs string for now, but ideally should do symbolic",
    "expressions Mon 19 Feb 2018 04:54:17 PM DST",
    "TODO Better support for multivariate treatments",
    "TODO: support multivariate treatments better.",
    "TODO: support multivariate treatments better.",
    "TODO: support multivariate treatments better.",
    "For direct effect",
    "If no costs are passed, use uniform costs",
    "restriction to ancestors",
    "back-door graph",
    "moralization",
    "Estimators list for returning after identification",
    "Line 1",
    "If no action has been taken, the effect on Y is just the marginal of the observational distribution P(v) on Y.",
    "Line 2",
    "If we are interested in the effect on Y, it is sufficient to restrict our attention on the parts of the model ancestral to Y.",
    "Modify list of valid nodes",
    "Line 3 - forces an action on any node where such an action would have no effect on Y \u2013 assuming we already acted on X.",
    "Modify adjacency matrix to obtain that corresponding to do(X)",
    "Line 4 - Decomposes the problem into a set of smaller problems using the key property of C-component factorization of causal models.",
    "If the entire graph is a single C-component already, further problem decomposition is impossible, and we must provide base cases.",
    "Modify adjacency matrix to remove treatment variables",
    "Line 5 - The algorithms fails due to the presence of a hedge - the graph G, and a subgraph S that does not contain any X nodes.",
    "Line 6 - If there are no bidirected arcs from X to the other nodes in the current subproblem under consideration, then we can replace acting on X by conditioning, and thus solve the subproblem.",
    "Line 7 - This is the most complicated case in the algorithm. Explain in the second last paragraph on Pg 41 of the link provided in the docstring above.",
    "Do not show backdoor key unless it is the only backdoor set.",
    "Just show the default backdoor set",
    "self._identified_estimand = self._causal_model.identify_effect()",
    "self._identified_estimand,",
    "self._causal_model._treatment,",
    "self._causal_model._outcome,",
    "If labels provided",
    "Return in valid DOT format",
    "Get adjacency matrix",
    "If labels not provided",
    "Obtain valid DOT format",
    "If labels provided",
    "Return in valid DOT format",
    "from https://www.bnmetrics.com/blog/factory-pattern-in-python3-simple-version",
    "from https://www.bnmetrics.com/blog/factory-pattern-in-python3-simple-version"
  ],
  "v0.10": [
    "-*- coding: utf-8 -*-",
    "",
    "Configuration file for the Sphinx documentation builder.",
    "",
    "This file does only contain a selection of the most common options. For a",
    "full list see the documentation:",
    "http://www.sphinx-doc.org/en/stable/config",
    "-- Path setup --------------------------------------------------------------",
    "If extensions (or modules to document with autodoc) are in another directory,",
    "add these directories to sys.path here. If the directory is relative to the",
    "documentation root, use os.path.abspath to make it absolute, like shown here.",
    "",
    "-- Project information -----------------------------------------------------",
    "Version Information (for version-switcher)",
    "-- General configuration ---------------------------------------------------",
    "If your documentation needs a minimal Sphinx version, state it here.",
    "",
    "needs_sphinx = '1.0'",
    "Add any Sphinx extension module names here, as strings. They can be",
    "extensions coming with Sphinx (named 'sphinx.ext.*') or your custom",
    "ones.",
    "sphinx-panels shouldn't add bootstrap css since the pydata-sphinx-theme",
    "already loads it",
    "Add any paths that contain templates here, relative to this directory.",
    "The suffix(es) of source filenames.",
    "You can specify multiple suffix as a list of string:",
    "",
    "source_suffix = ['.rst', '.md']",
    "The master toctree document.",
    "The language for content autogenerated by Sphinx. Refer to documentation",
    "for a list of supported languages.",
    "",
    "This is also used if you do content translation via gettext catalogs.",
    "Usually you set \"language\" from the command line for these cases.",
    "List of patterns, relative to source directory, that match files and",
    "directories to ignore when looking for source files.",
    "This pattern also affects html_static_path and html_extra_path .",
    "The name of the Pygments (syntax highlighting) style to use.",
    "-- Options for HTML output -------------------------------------------------",
    "The theme to use for HTML and HTML Help pages.  See the documentation for",
    "a list of builtin themes.",
    "",
    "Theme options are theme-specific and customize the look and feel of a theme",
    "further.  For a list of options available for each theme, see the",
    "documentation.",
    "",
    "Add any paths that contain custom static files (such as style sheets) here,",
    "relative to this directory. They are copied after the builtin static files,",
    "so a file named \"default.css\" will overwrite the builtin \"default.css\".",
    "Custom sidebar templates, must be a dictionary that maps document names",
    "to template names.",
    "",
    "The default sidebars (for documents that don't match any pattern) are",
    "defined by theme itself.  Builtin themes are using these templates by",
    "default: ``['localtoc.html', 'relations.html', 'sourcelink.html',",
    "'searchbox.html']``.",
    "",
    "html_sidebars = {}",
    "-- Options for HTMLHelp output ---------------------------------------------",
    "Output file base name for HTML help builder.",
    "-- Options for LaTeX output ------------------------------------------------",
    "The paper size ('letterpaper' or 'a4paper').",
    "",
    "'papersize': 'letterpaper',",
    "The font size ('10pt', '11pt' or '12pt').",
    "",
    "'pointsize': '10pt',",
    "Additional stuff for the LaTeX preamble.",
    "",
    "'preamble': '',",
    "Latex figure (float) alignment",
    "",
    "'figure_align': 'htbp',",
    "Grouping the document tree into LaTeX files. List of tuples",
    "(source start file, target name, title,",
    "author, documentclass [howto, manual, or own class]).",
    "-- Options for manual page output ------------------------------------------",
    "One entry per manual page. List of tuples",
    "(source start file, name, description, authors, manual section).",
    "-- Options for Texinfo output ----------------------------------------------",
    "Grouping the document tree into Texinfo files. List of tuples",
    "(source start file, target name, title, author,",
    "dir menu entry, description, category)",
    "-- Options for Epub output -------------------------------------------------",
    "Bibliographic Dublin Core info.",
    "The unique identifier of the text. This can be a ISBN number",
    "or the project homepage.",
    "",
    "epub_identifier = ''",
    "A unique identification for the text.",
    "",
    "epub_uid = ''",
    "A list of files that should not be packed into the epub file.",
    "-- Extension configuration -------------------------------------------------",
    "-- Options for todo extension ----------------------------------------------",
    "If true, `todo` and `todoList` produce output, else they produce nothing.",
    "init docstrings should also be included in class",
    "Only uncomment for faster testing/building docs without compiling notebooks",
    "nbsphinx_execute = \"never\"",
    "Patch all of the published versions",
    "check old RST version (<= v0.8)",
    "Remove old version links",
    "Append updated version links",
    "requires stdin input for identify in weighting sampler",
    "will be removed",
    "applied notebook, not necessary to test each time",
    "needs xgboost too",
    "Slow Notebooks",
    "TODO: should probably move more notebooks here to ignore, because",
    "most get tested by the documentation generation.",
    "Adding the dowhy root folder to the python path so that jupyter notebooks",
    "can import dowhy",
    "\"--ExecutePreprocessor.timeout=600\",",
    "self.df = pd.read_csv(os.path.join(DATA_PATH,'dgp_1/acic_1_1_data.csv'))",
    "self.ate = np.mean(self.df['y1'] - self.df['y0'])",
    "treated = self.df[self.df['z']==1]",
    "self.att = np.mean(treated['y1'] - treated['y0'])",
    "def test_average_treatment_effect(self):",
    "est_ate = 1",
    "bias = est_ate - self.ate",
    "print(bias)",
    "self.assertAlmostEqual(self.ate, est_ate)",
    "def test_average_treatment_effect_on_treated(self):",
    "est_att = 1",
    "self.att=1",
    "bias = est_att - self.att",
    "print(bias)",
    "self.assertAlmostEqual(self.att, est_att)",
    "removing two common causes",
    "removing two common causes",
    "removing two common causes",
    "removing two common causes",
    "Remove graph variable with name \"W0\" from observed data.",
    "Ensure that a log record exists that provides a more detailed view",
    "of observed and unobserved graph variables (counts and variable names.)",
    "check if all partial R^2 values are between 0 and 1",
    "We calculate adjusted estimates for two sets of partial R^2 values.",
    "Test if hypothetical confounding by unobserved confounder u1 leads to an adjusted effect that is farther from the original estimate as compared to u2",
    "Creating a model with no unobserved confounders",
    "check if all partial R^2 values are between 0 and 1",
    "for a dataset with no confounders, the robustness value should be higher than a given threshold (0.95 in our case)",
    "Non Parametric estimator",
    "We calculate adjusted estimates for two sets of partial R^2 values.",
    "adjusted lower ate bound for confounder u1 where r2tu_w = 0.7 and r2yu_tw = 0.9",
    "adjusted lower ate bound for confounder u2 where r2tu_w = 0.2 and r2yu_tw = 0.3",
    "adjusted upper ate bound for confounder u1 where r2tu_w = 0.7 and r2yu_tw = 0.9",
    "adjusted upper ate bound for confounder u2 where r2tu_w = 0.2 and r2yu_tw = 0.3",
    "adjusted lower confidence bound for confounder u1 where r2tu_w = 0.7 and r2yu_tw = 0.9",
    "adjusted lower confidence bound for confounder u2 where r2tu_w = 0.2 and r2yu_tw = 0.3",
    "adjusted upper confidence bound for confounder u1 where r2tu_w = 0.7 and r2yu_tw = 0.9",
    "adjusted upper confidence bound for confounder u2 where r2tu_w = 0.2 and r2yu_tw = 0.3",
    "Test if hypothetical confounding by unobserved confounder u1 leads to an adjusted effect that is farther from the original estimate as compared to u2",
    "we patched figure plotting call to avoid drawing plots during tests",
    "We calculate adjusted estimates for two sets of partial R^2 values.",
    "adjusted lower ate bound for confounder u1 where r2tu_w = 0.7 and r2yu_tw = 0.9",
    "adjusted lower ate bound for confounder u2 where r2tu_w = 0.2 and r2yu_tw = 0.3",
    "adjusted upper ate bound for confounder u1 where r2tu_w = 0.7 and r2yu_tw = 0.9",
    "adjusted upper ate bound for confounder u2 where r2tu_w = 0.2 and r2yu_tw = 0.3",
    "adjusted lower confidence bound for confounder u1 where r2tu_w = 0.7 and r2yu_tw = 0.9",
    "adjusted lower confidence bound for confounder u2 where r2tu_w = 0.2 and r2yu_tw = 0.3",
    "adjusted upper confidence bound for confounder u1 where r2tu_w = 0.7 and r2yu_tw = 0.9",
    "adjusted upper confidence bound for confounder u2 where r2tu_w = 0.2 and r2yu_tw = 0.3",
    "Test if hypothetical confounding by unobserved confounder u1 leads to an adjusted effect that is farther from the original estimate as compared to u2",
    "we patched figure plotting call to avoid drawing plots during tests",
    "comparing test examples from R E-Value package",
    "check implementation of Observed Covariate E-value against R package",
    "The outcome is a linear function of the confounder",
    "The slope is 1,2 and the intercept is 3",
    "As we run with only one common cause and one instrument variable we run with (?, 2)",
    "TODO: Check directly for correct behavior, rather than checking the rules",
    "themselves, which can be non-deterministic (all the following are equivalent)",
    "Supports user-provided dataset object",
    "To test if there are any exceptions",
    "To test if the estimate is identical if refutation parameters are zero",
    "This value is hardcoded to be zero as we are runnning this on a linear dataset.",
    "Ordinarily, we should expect this value to be zero.",
    "This value is hardcoded to be zero as we are runnning this on a linear dataset.",
    "Ordinarily, we should expect this value to be zero.",
    "Calculate causal effect twice: once for unit (t=1, c=0), once for specific increase (t=100, c=50)",
    "Only P(Y|T) should be present for test to succeed.",
    "Calculate causal effect twice: once for unit (t=1, c=0), once for specific increase (t=100, c=50)",
    "Since undirected graph, identify effect must throw an error.",
    "Calculate causal effect twice: once for unit (t=1, c=0), once for specific increase (t=100, c=50)",
    "Compare with ground truth",
    "Calculate causal effect twice: once for unit (t=1, c=0), once for specific increase (t=100, c=50)",
    "Compare with ground truth",
    "Calculate causal effect twice: once for unit (t=1, c=0), once for specific increase (t=100, c=50)",
    "Compare with ground truth",
    "Calculate causal effect twice: once for unit (t=1, c=0), once for specific increase (t=100, c=50)",
    "Compare with ground truth",
    "Causal model initialization",
    "Obtain backdoor sets",
    "Check if backdoor sets are valid i.e. if they block all paths between the treatment and the outcome",
    "Causal model initialization",
    "Obtain backdoor sets",
    "Causal model initialization",
    "Obtain backdoor sets",
    "Check if backdoor sets are valid i.e. if they block all paths between the treatment and the outcome",
    "Causal model initialization",
    "Obtain backdoor sets",
    "Check if backdoor sets are valid i.e. if they block all paths between the treatment and the outcome",
    "Causal model initialization",
    "Obtain backdoor sets",
    "Check if backdoor sets are valid i.e. if they block all paths between the treatment and the outcome",
    "Causal model initialization",
    "Obtain backdoor sets",
    "Check if backdoor sets are valid i.e. if they block all paths between the treatment and the outcome",
    "Example is selected from Pearl J. \"Causality\" 2nd Edition, from chapter 3.3.1 on backoor criterion.",
    "The following simpsons paradox examples are taken from Pearl, J {2013}. \"Understanding Simpson\u2019s Paradox\" - http://ftp.cs.ucla.edu/pub/stat_ser/r414.pdf",
    "The following are examples given in the \"Book of Why\" by Judea Pearl, chapter \"The Do-operator and the Back-Door Criterion\"",
    "For all examples from these papers we use X for the treatment variable",
    "instead of A.",
    "Figure 6 from Smucler, Sapienza and Rotnitzky (2021), Biometrika",
    "Figure 5 from Smucler, Sapienza and Rotnitzky (2021), Biometrika",
    "Figure 4 from Smucler, Sapienza and Rotnitzky (2021), Biometrika",
    "Figure 3 from Smucler, Sapienza and Rotnitzky (2021), Biometrika",
    "Figure 2 from Smucler and Rotnitzky (2022), Journal of Causal Inference",
    "L replaces X as the conditional variable",
    "Figure 2 from Smucler and Rotnitzky (2022), Journal of Causal Inference",
    "L replaces X as the conditional variable. Uses different costs",
    "Figure 3 from Smucler and Rotnitzky (2022), Journal of Causal Inference",
    "A graph where optimal, optimal minimal and optimal min cost are different",
    "The graph from Shrier and Platt (2008)",
    "A graph for which the algorithm was producing wrong result due to a bug reported by Sara Taheri",
    "Another graph for which the algorithm was producing wrong result due to a bug reported by Sara Taheri",
    "cov_mat = np.diag(np.ones(num_features))",
    "collider: X->Z<-Y",
    "chain: X->Z->Y",
    "fork: X<-Z->Y",
    "general DAG: X<-Z->Y, X->Y",
    "fork: X<-Z->Y",
    "Just checking formats, i.e. no need for correlation.",
    "Just checking formats, i.e. no need for correlation.",
    "Contributions should add up to Var(X2)",
    "H(P(Y)) -- Can be precomputed",
    "-(H(P(Y | do(x_S)) - H(P(Y))) = H(P(Y)) - H(P(Y | do(x_S))",
    "H(P(Y | do(x_S)) = H(E[P(Y | x_S, X'_\\S)])",
    "E[P(Y | x_S, X'_\\S)]",
    "H(E[P(Y | x_S, X'_\\S)])",
    "Using H(P(Y)) based on the origina data, i.e. ignoring baseline_predictions.",
    "E[H(P(Y)) - H(P(Y | do(X_U))] = H(P(Y)) - E[H(P(Y | X))]",
    "Mean from the categorical part is: (-5 + 5+ 10) / 3 = 10/3",
    "Just some random data, since we are only interested in the omitted data.",
    "C2 = 3 * A2 + 2 * B2",
    "By default, the strength is measure with respect to the variance.",
    "Here, we misspecified the mechanism on purpose by setting scale to 1 instead of 2.",
    "If we provide the observational data here, we can mitigate the misspecification of the causal mechanism.",
    "Missing connection between X0 and X1.",
    "For X0 and X1, we set the ground truth noise to further emphasize the misspecification. The inferred noise of X1",
    "would otherwise have a dependency with Z due to the missing connection with X0.",
    "Modelling connection between X0 and X1 explicitly.",
    "Here, we misspecify the mechanism on purpose by setting scale to 1 instead of 2.",
    "If we provide the observational data here, we can mitigate the misspecification of the causal mechanism.",
    "Here, changing the mechanism.",
    "Here, changing the mechanism.",
    "Here, changing the mechanism.",
    "Here, changing the mechanism.",
    "Defining an anomaly scorer that handles multidimensional inputs.",
    "Seeing that the expectation of the noise in all nodes is 0, we introduce anomalies by setting some of them to 3.",
    "In the first sample, only the first variable is anomalous. Therefore, it should have the highest contribution",
    "and it should be \"significantly\" higher than the contribution of the other ones (here, we just arbitrarily say",
    "it should be 3x higher. Due to the confounding factor Z, the reconstructed noise variables are pairwise dependent,",
    "which is a violation of our causal sufficiency assumption. However, a confounder is included here to demonstrate",
    "some robustness. Note that due to this and stochastic behaviour of the density estimator, it is",
    "not possible to analytically compute expected results. Therefore, we rather look at the relations here.",
    "Same idea for the second sample, but here, it is the second variable that is anomalous.",
    "In the fourth sample, there are 2 anomalous variables. Therefore, the contribution of these 2 variables should be",
    "\"significantly\" higher than the contribution of the other variables. The contribution of both anomalous variables",
    "should be equal (approximately).",
    "Defining an anomaly scorer that handles multidimensional inputs.",
    "Seeing that the expectation of the noise in all nodes is 0, we introduce anomalies by setting some of them to 3.",
    "reduce the score.",
    "The contributions should add up to g(x) - E[g(X)]",
    "The contributions should add up to g(x) - E[g(X)]",
    "The contributions should add up to g(x) - E[g(X)]",
    "Three examples:",
    "1. X1 is the root cause (+ 10 to the noise)",
    "2. X0 is the root cause (+ 10 to the noise)",
    "3. X0 and X3 are both root causes (+ 10 to both noises)",
    "The sum of the scores should add up to the anomaly score of the target (here, X3).",
    "Three examples:",
    "1. X1 is the root cause (+ 10 to the noise)",
    "2. X0 is the root cause (+ 10 to the noise)",
    "3. X0 and X3 are both root causes (+ 10 to both noises)",
    "The sum of the scores should add up to the anomaly score of the target (here, X3).",
    "Defining ground truth model to avoid SCM learning issues and, hence, to focus on the anomaly attribution",
    "algorithm.",
    "1. X0 is the root cause (+ 10 to the noise)",
    "2. X0 and X1 are the root causes (+ 10 to both noise)",
    "3. X2 and X3 are both root causes (+ 10 to both noises)",
    "The sum of the scores should add up to the anomaly score of the target (here, X3).",
    "Copyright 2021 Amazon.com, Inc. or its affiliates. All Rights Reserved.",
    "collider: X->Z<-Y",
    "collider: X->Z<-Y",
    "chain: X->Y->Z",
    "chain: X->Y->Z",
    "Empty graph",
    "Full DAG",
    "DAG with single node",
    "DAG with single edge",
    "DAG with single edge",
    "chain: X->Z->Y",
    "Setup data",
    "Test LinearDML",
    "Test ContinuousTreatmentOrthoForest",
    "Test LinearDRLearner",
    "Setup data",
    "Test DeepIV",
    "Treatment model,",
    "Response model",
    "Test IntentToTreatDRIV",
    "Observed data",
    "assumed graph",
    "Identify effect",
    "Estimate effect",
    "A model where X is also a common cause",
    "A model where X is also a common cause",
    "The case where effect modifier is not a common cause",
    "A model where X is also a common cause",
    "Not using testsuite from .base/TestEstimtor, custom code below",
    "More cases where Exception  is expected",
    "Compute confidence intervals, standard error and significance tests",
    "Defined a linear dataset with a given set of properties",
    "Create a model that captures the same",
    "Identify the effects within the model",
    "Defined a linear dataset with a given set of properties",
    "Create a model that captures the same",
    "Identify the effects within the model",
    "Defined a linear dataset with a given set of properties",
    "Create a model that captures the same",
    "Identify the effects within the model",
    "Defined a linear dataset with a given set of properties",
    "Create a model that captures the same",
    "Identify the effects within the model",
    "Defined a linear dataset with a given set of properties",
    "Create a model that captures the same",
    "Identify the effects within the model",
    "Check if calling the method causes some import or runtime errors",
    "TODO: Plotting undirected graphs with networkx causes an error when an older networkx version is used with a newer",
    "newer matplotlib version:",
    "AttributeError: module 'matplotlib.cbook' has no attribute 'is_numlike'",
    "Networkx 2.4+ should fix this issue.",
    "plot_adjacency_matrix(causal_graph, is_directed=False)",
    "Can use these lists to specify the models/estimators/refuters that a particular interpreter supports.  Throw a ValueError if the user provides an incompatible object to intepret.",
    "Unpacking the keyword arguments",
    "todo: add docstring for common parameters here and remove from child refuter classes",
    "Default value for the number of simulations to be conducted",
    "joblib params for parallel processing",
    "Concatenate the confounders, instruments and effect modifiers",
    "Shuffle the confounders",
    "Check if all are select or deselect variables",
    "Check if all the required_variables belong to confounders, instrumental variables or effect",
    "This calculates a two-sided percentile p-value",
    "See footnotes in https://journals.sagepub.com/doi/full/10.1177/2515245920911881",
    "Get the mean for the simulations",
    "Get the standard deviation for the simulations",
    "Get the Z Score [(val - mean)/ std_dev ]",
    "Initializing the p_value",
    "Perform Bootstrap Significance Test with the original estimate and the set of refutations",
    "Perform Normal Tests of Significance with the original estimate and the set of refutations",
    "Perform Bootstrap Significance Test with the original estimate and the set of refutations",
    "Perform Normal Tests of Significance with the original estimate and the set of refutations",
    "re.sub only takes string parameter so the first if is to avoid error",
    "if the input is a text file, convert the contained data into string",
    "load dot file",
    "Adding node attributes",
    "adding penwidth to make the edge bold",
    "Adding common causes",
    "Adding instruments",
    "Adding effect modifiers",
    "Assuming the simple form of effect modifier",
    "that directly causes the outcome.",
    "self._graph.add_edge(node_name, outcome, style = \"dotted\", headport=\"s\", tailport=\"n\")",
    "self._graph.add_edge(outcome, node_name, style = \"dotted\", headport=\"n\", tailport=\"s\") # TODO make the ports more general so that they apply not just to top-bottom node configurations",
    "Adding columns in the dataframe as confounders that were not in the graph",
    "Adding unobserved confounders",
    "removal of only direct edges wrt a target is not implemented for incoming edges",
    "also return the number of backdoor paths blocked by observed nodes",
    "Assume that nodes1 is the treatment",
    "ignores new_graph parameter, always uses self._graph",
    "remove paths that have nodes1\\node1 or nodes2\\node2 as intermediate nodes",
    "TODO Refactor to remove this from here and only implement this logic in causalIdentifier. Unnecessary assumption of nodes1 to be causing nodes2.",
    "Cannot simply compute ancestors, since that will also include nodes1 and its parents (e.g. instruments)",
    "Return effect modifiers according to the graph",
    "removing all mediators",
    "Also add any effect modifiers that could not be auto-detected (e.g., they are also common causes)",
    "[TODO: double check these work with multivariate implementation:]",
    "Exclusion",
    "As-if-random setup",
    "As-if-random",
    "convert the outputted generator into a list",
    "dpaths = self.get_all_directed_paths(nodes1, nodes2)",
    "return len(dpaths) > 0",
    "Condition 1: node 1 ---> node 2 is intercepted by candidate_nodes",
    "Emit a `UserWarning` if there are any unobserved graph variables and",
    "and log a message highlighting data variables that are not part of the graph.",
    "Create causal graph object",
    "Sometimes, effect modifiers from the graph may not match those provided by the user.",
    "(Because some effect modifiers may also be common causes)",
    "In such cases, the user-provided modifiers are used.",
    "If no effect modifiers are provided,  then the ones from the graph are used.",
    "Import causal discovery class",
    "Initialize causal graph object",
    "TODO add propensity score as default backdoor method, iv as default iv method, add an informational message to show which method has been selected.",
    "TODO add dowhy as a prefix to all dowhy estimators",
    "This is done as all dowhy estimators have two parts and external ones have two or more parts",
    "Define the third-party estimation method to be used",
    "Process the dowhy estimators",
    "Check if estimator's target estimand is identified",
    "Note that while the name of the variable is the same,",
    "\"self.causal_estimator\", this estimator takes in less",
    "parameters than the same from the",
    "estimate_effect code. It is not advisable to use the",
    "estimator from this function to call estimate_effect",
    "with fit_estimator=False.",
    "Estimator had been computed in a previous call",
    "The default number of simulations for statistical testing",
    "The default number of simulations to obtain confidence intervals",
    "This should be at least 399 for a 5% error rate:",
    "https://www.econstor.eu/bitstream/10419/67820/1/587473266.pdf",
    "The portion of the total size that should be taken each time to find the confidence intervals",
    "1 is the recommended value",
    "https://ocw.mit.edu/courses/mathematics/18-05-introduction-to-probability-and-statistics-spring-2014/readings/MIT18_05S14_Reading24.pdf",
    "https://projecteuclid.org/download/pdf_1/euclid.ss/1032280214",
    "The default Confidence Level",
    "Number of quantiles to discretize continuous columns, for applying groupby",
    "Prefix to add to temporary categorical variables created after discretization",
    "std args to be removed from locals() before being passed to args_dict",
    "Setting the default interpret method",
    "Check if some parameters were set, otherwise set to default values",
    "Estimate conditional estimates by default",
    "TODO Only works for binary treatment",
    "Defaulting to class default values if parameters are not provided",
    "Checking that there is at least one effect modifier",
    "Making sure that effect_modifier_names is a list",
    "Making a copy since we are going to be changing effect modifier names",
    "For every numeric effect modifier, adding a temp categorical column",
    "Grouping by effect modifiers and computing effect separately",
    "Deleting the temporary categorical columns",
    "The array that stores the results of all estimations",
    "Find the sample size the proportion with the population size",
    "Perform the set number of simulations",
    "names of treatment and outcome",
    "Using class default parameters if not specified",
    "Checking if bootstrap_estimates are already computed",
    "Checked if any parameter is changed from the previous std error estimate",
    "Now use the data obtained from the simulations to get the value of the confidence estimates",
    "Get the variations of each bootstrap estimate and sort",
    "Now we take the (1- p)th and the (p)th variations, where p is the chosen confidence level",
    "Get the lower and upper bounds by subtracting the variations from the estimate",
    "Use existing params, if new user defined params are not present",
    "Checking if bootstrap_estimates are already computed",
    "Check if any parameter is changed from the previous std error estimate",
    "Use existing params, if new user defined params are not present",
    "self._outcome = self._data[\"dummy_outcome\"]",
    "Processing the null hypothesis estimates",
    "Doing a two-sided test",
    "Being conservative with the p-value reported",
    "Being conservative with the p-value reported",
    "If the estimate_index is 0, it depends on the number of simulations",
    "Need to test r-squared before supporting",
    "effect_r_squared = self._evaluate_effect_strength(estimate, method=\"r-squared\")",
    "'r-squared': effect_r_squared",
    "elif method == \"r-squared\":",
    "outcome_mean = np.mean(self._outcome)",
    "total_variance = np.sum(np.square(self._outcome - outcome_mean))",
    "Assuming a linear model with one variable: the treatment",
    "Currently only works for continuous y",
    "causal_model = outcome_mean + estimate.value*self._treatment",
    "squared_residual = np.sum(np.square(self._outcome - causal_model))",
    "r_squared = 1 - (squared_residual/total_variance)",
    "return r_squared",
    "Check if estimator's target estimand is identified",
    "Store parameters inside estimate object for refutation methods",
    "TODO: This add_params needs to move to the estimator class",
    "inside estimate_effect and estimate_conditional_effect",
    "TODO: Remove _data, _treatment_name and _outcome_name from this object",
    "we save them here to enable the methods that required these properties saved in the estimator",
    "eventually we should call those methods and just save the results in this object",
    "instead of having this object invoke the estimator methods with the data.",
    "No estimand was identified (identification failed)",
    "s += \"Variance in outcome explained by treatment: {}\\n\".format(self.effect_strength[\"r-squared\"])",
    "can't use nx.node_connected_component, because it doesn't work with DiGraphs.",
    "Hence, a manual loop:",
    "Below loop assumes that the last indices of W are alwawys converted to discrete",
    "one-hot encode discrete W",
    "Now deleting the old continuous value",
    "create a dataset with 10 observations one binary treatment and a continuous outcome affected by one common cause",
    "Two continuous treatments, no common cause, an instrumental variable and two effect modifiers - linearly added appropriately",
    "One Hot Encoding",
    "TODO Ensure that we do not generate weak instruments",
    "TODO - test all our methods with random noise added to covariates (instead of the stochastic treatment assignment)",
    "Converting treatment to binary if required",
    "Generating frontdoor variables if asked for",
    "NOTE: We are assuming a linear relationship *even when t is categorical* and integer coded.",
    "For categorical t, this example dataset has the effect size for category 2 being exactly",
    "double the effect for category 1",
    "This could be changed at this stage by one-hot encoding t and using a custom beta that",
    "sets a different effect for each category {0, 1, 2}",
    "Computing ATE",
    "constructing column names for one-hot encoded discrete features",
    "Specifying the correct dtypes",
    "Now specifying the corresponding graph strings",
    "Now writing the gml graph",
    "creating data frame",
    "Specifying the correct dtypes",
    "Now specifying the corresponding graph strings",
    "Now writing the gml graph",
    "Adding edges between common causes and the frontdoor mediator",
    "Error terms",
    "else:",
    "V = 6 + W0 + tterm + E1",
    "Y = 12 + W0*W0 + W0*W0 + yterm + E2  # E2_new",
    "Generating a random normal distribution of integers",
    "Generating data for nodes which have no incoming edges",
    "\"currset\" variable currently has all the successors of the nodes which had no incoming edges",
    "assuming that all unobserved common causes are numerical and are not affected by one hot encoding",
    "Creating a NN to simulate the nuisance function",
    "strength of unobserved confounding",
    "Computing ATE",
    "Specifying the correct dtypes",
    "Now writing the gml graph",
    "The following code for loading the Lalonde dataset was copied from",
    "https://github.com/wayfair/pylift/blob/5afc9088e96f25672423663f5c9b4bb889b4dfc0/examples/Lalonde/Lalonde_sample.ipynb?short_path=b1d451f#L94-L99).",
    "",
    "Copyright 2018, Wayfair, Inc.",
    "",
    "Redistribution and use in source and binary forms, with or without modification, are permitted provided that",
    "the following conditions are met:",
    "",
    "1. Redistributions of source code must retain the above copyright notice, this list of conditions and the",
    "following disclaimer.",
    "",
    "2. Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the",
    "following disclaimer in the documentation and/or other materials provided with the distribution.",
    "",
    "THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND ANY EXPRESS OR IMPLIED",
    "WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A",
    "PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY",
    "DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,",
    "PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER",
    "CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR",
    "OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH",
    "DAMAGE.",
    "The following code is a slight modification of",
    "https://github.com/wayfair/pylift/blob/5afc9088e96f25672423663f5c9b4bb889b4dfc0/examples/Lalonde/Lalonde_sample.ipynb?short_path=b1d451f#L94-L99).",
    "",
    "Copyright 2018, Wayfair, Inc.",
    "",
    "Redistribution and use in source and binary forms, with or without modification, are permitted provided that",
    "the following conditions are met:",
    "",
    "1. Redistributions of source code must retain the above copyright notice, this list of conditions and the",
    "following disclaimer.",
    "",
    "2. Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the",
    "following disclaimer in the documentation and/or other materials provided with the distribution.",
    "",
    "THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND ANY EXPRESS OR IMPLIED",
    "WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A",
    "PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY",
    "DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,",
    "PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER",
    "CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR",
    "OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH",
    "DAMAGE.",
    "",
    "0.0.0 is standard placeholder for poetry-dynamic-versioning",
    "any changes to this should not be checked in",
    "",
    "Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved",
    "Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved",
    "Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved",
    "adapt number of channels",
    "save memory",
    "Keep same dimensions",
    "Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved",
    "single-attribute Causal",
    "test environment",
    "Subsample 2x for computational convenience",
    "Assign a binary label based on the digit",
    "Flip label with probability 0.25",
    "Assign a color based on the label; flip the color with probability environment",
    "Apply the color to the image by zeroing out the other color channel",
    "single-attribute Independent",
    "test environment",
    "Subsample 2x for computational convenience",
    "Assign a binary label based on the digit",
    "Flip label with probability 0.25",
    "multi-attribute Causal + Independent",
    "test environment",
    "Subsample 2x for computational convenience",
    "rotate the image by angle in parameter",
    "Assign a binary label based on the digit",
    "Flip label with probability 0.25",
    "Assign a color based on the label; flip the color with probability environment",
    "Apply the color to the image by zeroing out the other color channel",
    "Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved",
    "Acause regularization",
    "Aconf regularization",
    "Aind regularization",
    "Asel regularization",
    "Compile loss",
    "Check if the optimizer is currently supported",
    "Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved",
    "The currently supported estimators",
    "The default standard deviation for noise",
    "The default scaling factor to determine the bucket size",
    "The minimum number of points for the estimator to run",
    "The Default Transformation, when no arguments are given, or if the number of data points are insufficient for an estimator",
    "The Default True Causal Effect, this is taken to be ZERO by default",
    "The Default split for the number of data points that fall into the training and validation sets",
    "Assuming that outcome is one-dimensional",
    "We need to change the identified estimand",
    "We thus, make a copy. This is done as we don't want",
    "to change the original DataFrame",
    "We use collections.OrderedDict to maintain the order in which the data is stored",
    "Check if we are using an estimator in the transformation list",
    "The rationale behind ordering of the loops is the fact that we induce randomness everytime we create the",
    "Train and the Validation Datasets. Thus, we run the simulation loop followed by the training and the validation",
    "loops. Thus, we can get different values everytime we get the estimator.",
    "for _ in range( self._num_simulations ):",
    "Warn the user that the specified parameter is not applicable when no estimator is present in the transformation",
    "Adding an unobserved confounder if provided by the user",
    "We set X_train = 0 and outcome_train to be 0",
    "Get the final outcome, after running through all the values in the transformation list",
    "Check if the value of true effect has been already stored",
    "We use None as the key as we have no base category for this refutation",
    "As we currently support only one treatment",
    "If the number of data points is too few, run the default transformation: [(\"zero\",\"\"),(\"noise\", {'std_dev':1} )]",
    "Check if the value of true effect has been already stored",
    "This ensures that we calculate the causal effect only once.",
    "We use key_train as we map data with respect to the base category of the data",
    "As we currently support only one treatment",
    "Add h(t) to f(W) to get the dummy outcome",
    "We convert to ndarray for ease in indexing",
    "The data is of the form",
    "sim1: cat1 cat2 ... catn",
    "sim2: cat1 cat2 ... catn",
    "Note: We would like the causal_estimator to find the true causal estimate that we have specified through this",
    "refuter. Let the value of the true causal effect be h(t). In the following section of code, we wish to find out if h(t) falls in the",
    "distribution of the refuter.",
    "True Causal Effect list",
    "Iterating through the refutation for each category",
    "We use string arguments to account for both 32 and 64 bit varaibles",
    "action for continuous variables",
    "Action for categorical variables",
    "Find the set difference for each row",
    "Choose one out of the remaining",
    "We want to see if the estimate falls in the same distribution as the one generated by the refuter",
    "Ideally that should be the case as running bootstrap should not have a significant effect on the ability",
    "of the treatment to affect the outcome",
    "TODO: Check that the target estimand has backdoor variables?",
    "Standardizing the data",
    "Fit a model containing all confounders and compare predictions",
    "using all features compared to all features except a given",
    "confounder.",
    "Estimating the regression coefficient from standardized features to t",
    "By default, return a plot with 10 points",
    "consider 10 values of the effect of the unobserved confounder",
    "Standardizing the data",
    "Fit a model containing all confounders and compare predictions",
    "using all features compared to all features except a given",
    "confounder.",
    "By default, return a plot with 10 points",
    "consider 10 values of the effect of the unobserved confounder",
    "By default, we add the effect of simulated confounder for treatment.",
    "But subtract it from outcome to create a negative correlation",
    "assuming that the original confounder's effect was positive on both.",
    "This is to remove the effect of the original confounder.",
    "By default, we add the effect of simulated confounder for treatment.",
    "But subtract it from outcome to create a negative correlation",
    "assuming that the original confounder's effect was positive on both.",
    "This is to remove the effect of the original confounder.",
    "Obtaining the list of observed variables",
    "Taking a subset of the dataframe that has only observed variables",
    "Residuals from the outcome model obtained by fitting a linear model",
    "Residuals from the treatment model obtained by fitting a linear model",
    "Initialising product_cor_metric_observed with a really low value as finding maximum",
    "The user has an option to give the the effect_strength_on_y and effect_strength_on_t which can be then used instead of maximum correlation with treatment and outcome in the observed variables as it specifies the desired effect.",
    "Choosing a c_star based on the data.",
    "The correlations stop increasing upon increasing c_star after a certain value, that is it plateaus and we choose the value of c_star to be the value it plateaus.",
    "Choosing c1 and c2 based on the hyperbolic relationship once c_star is chosen by going over various combinations of c1 and c2 values and choosing the combination which",
    "which maintains the minimum distance between the product of correlations of the simulated variable and the product of maximum correlations of one of the observed variables",
    "and additionally checks if the ratio of the weights are such that they maintain the ratio of the maximum possible observed coefficients within some confidence interval",
    "c1_final and c2_final are initialised to the values on the hyperbolic curve such that c1_final = c2_final  and c1_final*c2_final = c_star",
    "initialising min_distance_between_product_cor_metrics to be a value greater than 1",
    "TODO (petergtz): This should introduce a Protocol defining the interface this returns instead of the concrete types",
    "If the estimator used is LinearDML, partially linear sensitivity analysis will be automatically chosen",
    "Get a 2D matrix of values",
    "x,y =  np.meshgrid(self.kappa_t, self.kappa_y) # x,y are both MxN",
    "Store the values into the refute object",
    "Adding a label on the contour line for the original estimate",
    "Label every other level using strings",
    "Default value of the p value taken for the distribution",
    "Number of Trials: Number of cointosses to understand if a sample gets the treatment",
    "Mean of the Normal Distribution",
    "Standard Deviation of the Normal Distribution",
    "Create a new column in the data by the name of placebo",
    "Sanity check the data",
    "only permute is supported for iv methods",
    "For IV methods, the estimating_instrument_names should also be",
    "changed. Create a copy to avoid modifying original object",
    "We need to change the identified estimand",
    "We make a copy as a safety measure, we don't want to change the",
    "original DataFrame",
    "Run refutation in parallel",
    "Note: We hardcode the estimate value to ZERO as we want to check if it falls in the distribution of the refuter",
    "Ideally we should expect that ZERO should fall in the distribution of the effect estimates as we have severed any causal",
    "relationship between the treatment and the outcome.",
    "new estimator",
    "new effect estimate",
    "observed covariate E-value",
    "see Table 2 and p.37 in https://dash.harvard.edu/bitstream/handle/1/36874927/EValue_FinalSubmission.pdf",
    "if CI crosses null, set its E-value to 1",
    "only report E-value for CI limit closer to null",
    "see Table 2 and p.37 in https://dash.harvard.edu/bitstream/handle/1/36874927/EValue_FinalSubmission.pdf",
    "whether the DGP is assumed to be partially linear",
    "features are the observed confounders",
    "Now code for benchmarking using covariates begins",
    "R^2 of outcome with observed common causes and treatment",
    "R^2 of treatment with observed common causes",
    "Partial R^2 of outcome after regressing over unobserved confounder, observed common causes and treatment",
    "Assuming that the difference in R2 is the same for wj and new unobserved confounder",
    "for treatment,  Calpha is not a function of the partial R2. So we need a different assumption.",
    "Assuming that the ratio of variance of alpha^2 is the same for wj and new unobserved confounder",
    "(1-ratio_var_alpha_wj) is the numerator of Calpha2, similar to the partial R2 for treatment",
    "wrt unobserved confounders in partial-linear models",
    "Do the support characterization",
    "Recover the samples that are in the support",
    "Assess overlap using propensity scores with cross-fitting",
    "Check if all supported units are considered to be in the overlap set",
    "NOTE: The original paper implements both DNF and CNF rules, but for simplicity, this code only implements DNF rules",
    "For DNF rules, a sample is covered if *any* rule applies",
    "whether the DGP is assumed to be partially linear",
    "can change this to allow default values that are same as the other parameter",
    "Strength of confounding that omitted variables generate in treatment regression",
    "computing the point estimate for the bounds",
    "common causes after removing the benchmark causes",
    "dataframe with treatment and observed common causes after removing benchmark causes",
    "R^2 of treatment with observed common causes removing benchmark causes",
    "return the variance of alpha_s",
    "R^2 of outcome with observed common causes and treatment after removing benchmark causes",
    "Obtaining theta_s (the obtained estimate)",
    "Creating numpy arrays",
    "Setting up cross-validation parameters",
    "tuple of residuals from first stage estimation [0,1], and the confounders [2]",
    "We need to estimate, sigma^2 = (Y-g_s)^2. We use the following derivation.",
    "Yres = Y - E[Y|W]",
    "E[Y|W] = f(x) + theta_s * E[T|W]",
    "Yres = Y - f(x) - theta_s * E[T|W]",
    "g(s) = theta_s * T + f(x)",
    "g(s) = theta_s * (T - E[T|W]) + f(x) + theta_s * E[T|W]",
    "g(s) = theta_s * Tres +f(x) + theta_s * E[T|W]",
    "Y - g(s) = Y - [theta_s * Tres + f(x) + theta_s * E[T|W] )",
    "Y - g(s) = ( Y - f(x) -  theta_s * E[T|W]) - theta_s * Tres",
    "Y - g(s) = Yres - theta_s * Tres",
    "nu_2 is E[alpha_s^2]",
    "Now computing scores for finding the (1-a) confidence interval",
    "R^2 of treatment with observed common causes",
    "R^2 of outcome with treatment and observed common causes",
    "Partial R^2 of outcome after regressing over unobserved confounder, observed common causes and treatment",
    "Partial R^2 of treatment after regressing over unobserved confounder and observed common causes",
    "adding 1.1 as plotting margin  ensure that the benchmarked part is shown fully in plot",
    "adding 1.1 as plotting margin  ensure that the benchmarked part is shown fully in plot",
    "Adding unadjusted point estimate",
    "Adding bounds to partial R^2 values for given strength of confounders",
    "Adding a new backdoor variable to the identified estimand",
    "Run refutation in parallel",
    "We want to see if the estimate falls in the same distribution as the one generated by the refuter",
    "Ideally that should be the case as choosing a subset should not have a significant effect on the ability",
    "of the treatment to affect the outcome",
    "TODO: Sensitivity Analyzers excluded from list due to different return type",
    "Reject H0",
    "a, b and c are all continuous variables",
    "a, b and c are all discrete variables",
    "c is set of continuous and binary variables and",
    "1. either a and b is continuous and the other is binary",
    "2. both a and b are binary",
    "c is discrete and",
    "either a or b is continuous and the other is discrete",
    "a and b are discrete and c is a mixture of discrete and continuous variables. We discretize c and calculate conditional mutual information",
    "original_treatment_name: : stores original variable names for labelling",
    "common_causes_map : maps the original variable names to variable names in OLS regression",
    "benchmark_common_causes: stores variable names in terms of regression model variables",
    "original_benchmark_covariates: stores original variable names for labelling",
    "estimate: estimate of regression",
    "degree_of_freedom: degree of freedom of error in regression",
    "standard_error: standard error in regression",
    "t_stats: Treatment coefficient t-value - measures how many standard errors the estimate is away from zero.",
    "partial_f2: value to determine if a regression model and a nested version of it have a statistically significant difference between them",
    "r2tu_w: partial R^2  of unobserved confounder \"u\" with treatment \"t\", after conditioning on observed covariates \"w\"",
    "r2yu_tw: partial R^2  of unobserved confounder \"u\" with outcome \"y\", after conditioning on observed covariates \"w\" and treatment \"t\"",
    "r2twj_w: partial R^2 of observed covariate wj with treatment \"t\", after conditioning on observed covariates \"w\" excluding wj",
    "r2ywj_tw:  partial R^2 of observed covariate wj with outcome \"y\", after conditioning on observed covariates \"w\" (excluding wj) and treatment \"t\"",
    "benchmarking_results: dataframe containing information about bounds and bias adjusted terms",
    "stats: dictionary containing information like robustness value, partial R^2, estimate, standard error , degree of freedom, partial f^2, t-statistic",
    "partial R^2 (r2yt_w) is the proportion of variation in outcome uniquely explained by treatment",
    "build a new regression model by considering treatment variables as outcome",
    "r2twj_w is partial R^2 of covariate wj with treatment \"t\", after conditioning on covariates w(excluding wj)",
    "r2ywj_tw is partial R^2 of covariate wj with outcome \"y\", after conditioning on covariates w(excluding wj) and treatment \"t\"",
    "r2tu_w is the partial r^2 from regressing u on t after conditioning on w",
    "Compute bias adjusted terms",
    "Plotting the contour plot",
    "Adding contours",
    "Adding threshold contour line",
    "Adding unadjusted point estimate",
    "Adding bounds to partial R^2 values for given strength of confounders",
    "from https://www.bnmetrics.com/blog/factory-pattern-in-python3-simple-version",
    "The default subset of the data to be used",
    "Run refutation in parallel",
    "We want to see if the estimate falls in the same distribution as the one generated by the refuter",
    "Ideally that should be the case as choosing a subset should not have a significant effect on the ability",
    "of the treatment to affect the outcome",
    "Parameters",
    "Bookkeeping",
    "Initialize estimators",
    "Convert to dataframe if not",
    "Format labels",
    "Sample from reference measure and construct features",
    "Add reference samples",
    "Binarize features (fit to data only)",
    "Fit estimator",
    "Store reference volume",
    "Construct features dataframe",
    "Construct features dataframe",
    "Iterate over columns",
    "logging.info(\"Using provided reference range for {}\".format(c))",
    "number of unique values",
    "Constant column",
    "Binary column",
    "Ordinal column (seed = counter so not correlated)",
    "For get_params / set_params",
    "Overlap / Support (y = +1), non-overlap (y = 0), and uniform background (y = -1) samples",
    "We should always have overlap samples, and either background or non-overlap samples",
    "This will throw an error if, for example, all samples are considered to",
    "be in the overlap region",
    "Initialize with empty and singleton conjunctions, i.e. X plus all-ones feature",
    "Feature indicator and conjunction matrices",
    "Iteration counter",
    "Formulate master LP",
    "Variables",
    "Objective function (no penalty on empty conjunction)",
    "Constraints",
    "This gets activated for DNF",
    "Solve problem",
    "Extract dual variables",
    "Beam search for conjunctions with negative reduced cost",
    "Most negative reduced cost among current variables",
    "Negative reduced costs found",
    "Add to existing conjunctions",
    "Reformulate master LP",
    "Variables",
    "Objective function",
    "Constraints",
    "Solve problem",
    "Extract dual variables",
    "Beam search for conjunctions with negative reduced cost",
    "Most negative reduced cost among current variables",
    "print('UB.min():', UB.min())",
    "Save generated conjunctions and coefficients",
    "Restrict conjunctions to those used by LP",
    "NOTE: This is a greedy approach, so it does not incorporate lambda0 explicitly",
    "Similarly, it will prefer a larger number of smaller rules if lambda1 is set",
    "to a larger value, because the incremental cost will be lower.",
    "Fraction of reference samples that each conjunction covers",
    "Regularization (for each conjunction)",
    "Positive samples newly covered (for each conjunction)",
    "Costs (for each conjunction)",
    "Zero out the rules and only take those which are used",
    "Small tolerance on comparisons",
    "This can be useful to break ties and favor larger values of xi",
    "Compute conjunctions of features",
    "Predict labels",
    "Use helper function",
    "Use helper function",
    "Lower bound specific to each singleton solution",
    "Initialize output",
    "Remove redundant rows by grouping by unique feature combinations and summing residual",
    "Initialize queue with root instance",
    "Separate data according to positive and negative residuals",
    "Iterate over increasing degree while queue is non-empty",
    "Initialize list of children to process",
    "Process instances in queue",
    "inst = instCurr[0]",
    "Evaluate all singleton solutions",
    "Best solutions that also improve on current output (allow for duplicate removal)",
    "Append to current output",
    "Remove duplicates",
    "Update output",
    "Compute lower bounds on higher-degree solutions",
    "Evaluate children using weighted average of their costs and LBs",
    "Best children with potential to improve on current output and current candidates (allow for duplicate removal)",
    "Iterate through best children",
    "New \"zero\" solution",
    "Check if duplicate",
    "Add to candidates for further processing",
    "Create pricing instance",
    "Remove covered rows",
    "Remove redundant features",
    "Track number of candidates added",
    "Update candidates",
    "Instances to process in next iteration",
    "Conjunctions corresponding to solutions",
    "List of categorical columns",
    "Number of quantile thresholds used to binarize ordinal features",
    "whether to append negations",
    "whether to convert thresholds on ordinal features to strings",
    "Quantile probabilities",
    "Initialize",
    "Iterate over columns",
    "number of unique values",
    "Constant or binary column",
    "Mapping to 0, 1",
    "Categorical column",
    "OneHotEncoder object",
    "Fit to observed categories",
    "Ordinal column",
    "Few unique values",
    "Thresholds are sorted unique values excluding maximum",
    "Many unique values",
    "Thresholds are quantiles excluding repetitions",
    "Contains NaN values",
    "Initialize dataframe",
    "Iterate over columns",
    "Constant or binary column",
    "Rename values to 0, 1",
    "Categorical column",
    "Apply OneHotEncoder",
    "Append negations",
    "Concatenate",
    "Ordinal column",
    "Threshold values to produce binary arrays",
    "Append negations",
    "Convert to dataframe with column labels",
    "Ensure that rows corresponding to NaN values are zeroed out",
    "Add NaN indicator column",
    "Concatenate",
    "from https://www.bnmetrics.com/blog/factory-pattern-in-python3-simple-version",
    "If the relative change of the score is less than the given threshold, we stop the estimation early.",
    "Note, the output of score_samples are log values.",
    "Note, the output of score_samples are log values.",
    "Currently only support continuous distributions for auto selection.",
    "Estimate distribution parameters from data.",
    "Ignore warnings from fitting process.",
    "Fit distribution to data.",
    "Some distributions might not be compatible with the data.",
    "Separate parts of parameters.",
    "Check the KL divergence between the distribution of the given and fitted distribution.",
    "Identify if this distribution is better.",
    "This error is typically raised when the data is discrete and all points are assigned to less cluster than",
    "specified. It can also happen due to duplicated points. In these cases, the current best solution should",
    "be sufficient.",
    "Usual feature relevance using the mean deviation as set function, i.e. g(x) - E[g(X)]",
    "A convenience function when computing confidence intervals specifically for non-deterministic causal queries. This",
    "function evaluates the provided causal query multiple times to build a confidence interval based on the returned",
    "results.",
    "Note that this function does not re-fit the causal model(s) and only executes the provided query as it is. In order",
    "to re-refit the graphical causal model on random subsets of the data before executing the query, consider using the",
    "fit_and_compute function.",
    "",
    "**Example usage:**",
    "",
    ">>> gcm.fit(causal_model, data)",
    ">>> strength_medians, strength_intervals = gcm.confidence_intervals(",
    ">>>     gcm.bootstrap_sampling(gcm.arrow_strength, causal_model, target_node='Y'))",
    "",
    "In this example, gcm.confidence_intervals is expecting a callable with non-deterministic outputs for building the",
    "confidence intervals. Since each causal query potentially expects a different set of parameters, we use 'partial'",
    "here to configure the function call. In this case,",
    "gcm.bootstrap_sampling(gcm.arrow_strength, causal_model, target_node='Y') would be equivalent to",
    "lambda : gcm.arrow_strength(causal_model, target_node='Y').",
    "",
    "In order to incorporate uncertainties coming from fitting the causal model(s), we can use",
    "gcm.fit_and_compute instead:",
    ">>>  strength_medians, strength_intervals = gcm.confidence_intervals(",
    ">>>        gcm.fit_and_compute(gcm.arrow_strength,",
    ">>>                                            causal_model,",
    ">>>                                            bootstrap_training_data=data,",
    ">>>                                            target_node='Y'))",
    "This would refit the provided causal_model on a subset of the data first before executing gcm.arrow_strength in each",
    "run.",
    "Simulating interventions by propagating the effects through the graph. For this, we iterate over the nodes based",
    "on their topological order.",
    "After drawing samples of the node based on the data generation process, we apply the corresponding",
    "intervention. The inputs of downstream nodes are therefore based on the outcome of the intervention in this",
    "node.",
    "Abduction: For invertible SCMs, we recover exact noise values from data.",
    "Action + Prediction: Propagate the intervention downstream using recovered noise values.",
    "Check if we need to apply an intervention on the given node.",
    "Apply intervention function to the data of the node.",
    "Check if the intervention function changes the shape of the data.",
    "For estimating the effect, we only need to consider the nodes that have a directed path to the target node, i.e.",
    "all ancestors of the target.",
    "The target node can be a continuous real-valued variable or a categorical variable with at most two classes",
    "(i.e. binary).",
    "Avoid too many features",
    "Making sure there are at least 30% test samples.",
    "Making sure that there are at least 2 samples from one class (here, simply duplicate the point).",
    "Compare number of correct classifications.",
    "Making sure that X and Y have no overlapping values, which would lead to a distance of 0 with k=1 and, thus, to",
    "a division by zero.",
    "For estimating Shapley values for multiple samples (e.g. in feature relevance) and the number of samples",
    "is unknown beforehand.",
    "The method stops if either the change between some consecutive runs is below the given threshold or the",
    "maximum number of runs is reached.",
    "In each run, we create one random permutation of players. For instance, given 4 players, a permutation",
    "could be [3,1,4,2].",
    "Generate k random permutations by sorting the indices of the Halton sequence",
    "Create all subsets belonging to the generated permutation. This is, if we have [3,1,4,2], then the",
    "subsets are [3], [3,1], [3,1,4] [3,1,4,2].",
    "The result for each subset is cached such that if a subset that has already been evaluated appears again,",
    "we can take this result directly.",
    "To improve the runtime, multiple permutations are evaluated in each run.",
    "The current Shapley values are the average of the estimated values, i.e. we need to divide by the number",
    "of generated permutations here.",
    "In case Shapley values are estimated for multiple samples, e.g., in feature relevance. So, we have a",
    "matrix of Shapley values instead of a vector.",
    "Here, the change between consecutive runs is below the minimum threshold, but to reduce the",
    "likelihood that this just happened by chance, we require that this happens at least for",
    "num_consecutive_converged_runs times in a row.",
    "Check if change in percentage is below threshold",
    "Check for values that are exactly zero. If they don't change between two runs, we consider it as converging.",
    "Create all (unique) subsets)",
    "Assigning a 'high' weight, since this resembles \"infinity\".",
    "The weight for a subset with a specific length (see paper mentioned in the docstring for more",
    "information).",
    "TODO: Add method for auto select a bin_width/width based on the data. Make sure that the auto selection method is",
    "theoretically sound, i.e. make entropy results from different data comparable.",
    "Extremely small values can somehow result in negative values.",
    "Creating a smaller subgraph, which only contains upstream nodes that are connected to the target node.",
    "Sampling from the conditional distribution based on the current sample.",
    "Sampling from the conditional based on the current sample, but randomizing the inputs of all variables that",
    "are in the given subset. By this, we can simulate the impact on the conditional distribution when removing",
    "only the incoming edges of the variables in the subset.",
    "Creating a smaller subgraph, which only contains upstream nodes that are connected to the target node.",
    "In case of the full subset (no randomization), we get the same predictions as when we apply the",
    "prediction method to the samples of interest, since all noise samples are replaced with a sample of",
    "interest.",
    "In case of the empty subset (all are jointly randomize), it boils down to taking the average over all",
    "predictions, seeing that the randomization yields the same values for each sample of interest (none of the",
    "samples of interest are used to replace a (jointly) 'randomized' sample).",
    "Exact model",
    "This constant is used as key when storing/accessing models as causal mechanisms in graph node attributes",
    "This constant is used as key when storing the parents of a node during fitting. It's used for validation purposes",
    "afterwards.",
    "Smallest possible value. This is used in various algorithm for numerical stability.",
    "Make copy to avoid manipulating the original matrix.",
    "The given prediction method has to be evaluated multiple times on a large amount of different inputs. Typically,",
    "the batch evaluation of a prediction model on multiple inputs at the same time is significantly faster",
    "than evaluating it on single simples in a for-loop. To make use of this, we try to evaluate as many samples as",
    "possible in one batch call of the prediction method. However, this also requires a lot of memory for many samples.",
    "To overcome potential memory issues, multiple batch calls are performed, each with at most batch_size many",
    "samples. The number of samples that are evaluated is normally",
    "baseline_noise_samples.shape[0] * feature_samples.shape[0]. Here, we reduce it to",
    "batch_size * feature_samples.shape[0]. If the batch_size would be set 1, then each baseline_noise_samples is",
    "evaluated one by one in a for-loop.",
    "Each batch consist of at most batch_size * feature_samples.shape[0] many samples. If there are multiple",
    "batches, the offset indicates the index of the current baseline_noise_samples that has not been evaluated yet.",
    "If the batch size would be larger than the remaining amount of samples, it is reduced to only include the",
    "remaining baseline_noise_samples.",
    "The inputs consist of batch_size many copies of feature_samples. Here, we set the columns of the features",
    "in baseline_feature_indices to their respective values in baseline_noise_samples.",
    "After creating the (potentially large) input data matrix, we can evaluate the prediction method.",
    "Here, offset + index now indicates the sample index in baseline_noise_samples.",
    "This would average all prediction results obtained for the 'offset + index'-th sample in",
    "baseline_noise_samples. This is, y^(offset + index) = E[Y | do(x^(offset + index)_s)].",
    "This would return all prediction results obtained for the 'offset + index'-th sample in",
    "baseline_noise_samples, i.e. the results are not averaged.",
    "Making copy to ensure that the original object is not modified.",
    "Permute samples jointly. This still represents an interventional distribution.",
    "Permute samples independently.",
    "Looks for the first index where the cumulative sum of the probabilities is larger than the threshold.",
    "Note that if there are multiple indices with the same maximum value (as in this case here), the argmax",
    "function returns the first index.",
    "test local Markov condition, null hypothesis: conditional independence",
    "test edge dependence, null hypothesis: independence",
    "The order of the p-values added to the list is deterministic.",
    "To be able to validate that the graph structure did not change between fitting and causal query, we store the",
    "parents of a node during fit. That way, before sampling, we can verify the parents are still the same. While",
    "this would automatically fail when the number of parents is different, there are other more subtle cases,",
    "where the number is still the same, but it's different parents, and therefore different data. That would yield",
    "wrong results, but would not fail.",
    "Independence tests are symmetric",
    "Find out which tests to do",
    "Parallelize over tests",
    "Gather results",
    "Summarize",
    "Find out which tests to do",
    "Parallelize over tests",
    "Gather results",
    "Summarize",
    "Find out which tests to do",
    "Parallelize over tests",
    "Gather results",
    "Summarize",
    "DAG Evaluation",
    "Suggestions",
    "Append list of violations (node, non_desc) to get local information",
    "Plot histograms",
    "Plot given violations",
    "For LMC we highlight X for which X _|/|_ Y \\in ND_X | Pa_X",
    "For PD we highlight the edge (if Y\\in Anc_X -> X are adjacent)",
    "For causal minimality we highlight the edge Y \\in Pa_X -> X",
    "Create Validation header",
    "Create Validation summary",
    "Close Validation",
    "Create Suggestions header",
    "Iterate over suggestions",
    "Test if we have data for X and Y",
    "Test if we have data for Z",
    "Eq. (1) in https://arxiv.org/pdf/1810.11363.pdf",
    "Subtracting Y here since the cumulative sum includes the current element. The same reason we subtract 1",
    "from the count.",
    "Using the negative value here seeing that the Shapley estimation evaluates v(S u {i}) - v(S) for a subset S. In",
    "case of variance, we have v(S u {i}) <= v(S), which would result in a negative contribution of players to the",
    "target quantity (here, variance).",
    "Using the negative value here seeing that the Shapley estimation evaluates v(S u {i}) - v(S) for a subset S. In",
    "case of variance, we have v(S u {i}) <= v(S), which would result in a negative contribution of players to the",
    "target quantity (here, variance).",
    "Calculate Ri, the product of the residuals",
    "Standard deviation of the residuals",
    "Either X and/or Y is constant.",
    "If Z is empty, we are in the pairwise setting.",
    "Either X and/or Y is constant.",
    "If Z is empty, we are in the pairwise setting.",
    "Estimate test statistic multiple times on different permutations of the data. The p-value is then the",
    "probability (i.e. fraction) of obtaining a test statistic that is greater than statistic on the non-permuted",
    "data.",
    "Take the lower dimensional variable as target.",
    "First stage statistical model",
    "Second stage statistical model",
    "Check if the treatment is one-dimensional",
    "First stage",
    "Second Stage",
    "Combining the two estimates",
    "This same estimate is valid for frontdoor as well as mediation (NIE)",
    "Bulding the feature matrix",
    "features = sm.add_constant(features, has_constant='add') # to add an intercept term",
    "TODO move this to the identification step",
    "Obtain estimate by Wald Estimator",
    "Obtain estimate by 2SLS estimator: Cov(y,z) / Cov(x,z)",
    "More than 1 instrument. Use 2sls.",
    "Checking if Y is binary",
    "Enable the user to pass params for a custom propensity model",
    "Convert the categorical variables into dummy/indicator variables",
    "Basically, this gives a one hot encoding for each category",
    "The first category is taken to be the base line.",
    "Check if the treatment is one-dimensional",
    "Checking if the treatment is binary",
    "The model is always built on the entire data",
    "TODO make treatment_value and control value also as local parameters",
    "All treatments are set to the same constant value",
    "Fixing treatment value to the specified value, if provided",
    "treatment_vals and data_df should have same number of rows",
    "Bulding the feature matrix",
    "The model is always built on the entire data",
    "Replacing treatment values by given x",
    "First, create interventional tensor in original space",
    "Then, use pandas to ensure that the dummies are assigned correctly for a categorical treatment",
    "The average treatment effect is a combination of different",
    "regression coefficients. Complicated to compute the confidence",
    "interval analytically. For example, if y=a + b1.t + b2.tx, then",
    "the average treatment effect is b1+b2.mean(x).",
    "Refer Gelman, Hill. ARM Book. Chapter 9",
    "http://www.stat.columbia.edu/~gelman/arm/chap9.pdf",
    "TODO: Looking for contributions",
    "For a linear regression model, the causal effect of a variable is equal to the coefficient corresponding to the",
    "variable. Hence, the model by default outputs the confidence interval corresponding to treatment=1 and control=0.",
    "So for custom treatment and control values, we must multiply the confidence interval by the difference of the two.",
    "For a linear regression model, the causal effect of a variable is equal to the coefficient corresponding to the",
    "variable. Hence, the model by default outputs the standard error corresponding to treatment=1 and control=0.",
    "So for custom treatment and control values, we must multiply the standard error by the difference of the two.",
    "setting method-specific parameters",
    "Infer the right strata based on clipping threshold",
    "0.5 because there are two values for the treatment",
    "To be conservative and allow most strata to be included in the",
    "analysis",
    "At least 90% of the strata should be included in analysis",
    "sum weighted outcomes over all strata  (weight by treated population)",
    "TODO - how can we add additional information into the returned estimate?",
    "such as how much clipping was done, or per-strata info for debugging?",
    "sort the dataframe by propensity score",
    "create a column 'strata' for each element that marks what strata it belongs to",
    "for each strata, count how many treated and control units there are",
    "throw away strata that have insufficient treatment or control",
    "TODO -- fix: we are actually conditioning on positive treatment (d=1)",
    "Setting method specific parameters",
    "trim propensity score weights",
    "ips ==> (isTreated(y)/ps(y)) + ((1-isTreated(y))/(1-ps(y)))",
    "nips ==> ips / (sum of ips over all units)",
    "icps ==> ps(y)/(1-ps(y)) / (sum of (ps(y)/(1-ps(y))) over all control units)",
    "itps ==> ps(y)/(1-ps(y)) / (sum of (ps(y)/(1-ps(y))) over all treatment units)",
    "Vanilla IPS estimator",
    "The Hajek estimator (or the self-normalized estimator)",
    "Stabilized weights (from Robins, Hernan, Brumback (2000))",
    "Paper: Marginal Structural Models and Causal Inference in Epidemiology",
    "Calculating the effect",
    "Subtracting the weighted means",
    "TODO - how can we add additional information into the returned estimate?",
    "TODO -- fix: we are actually conditioning on positive treatment (d=1)",
    "Save parameters for later refutter fitting",
    "Enforcing this ordering is necessary to feed through the propensity values from dataset",
    "For metalearners only--issue a warning if w contains variables not in x",
    "Override the effect_modifiers set in CausalEstimator.__init__()",
    "Also only update self._effect_modifiers, and create a copy of self._effect_modifier_names",
    "the latter can be used by other estimator methods later",
    "Instrumental variables names, if present",
    "choosing the instrumental variable to use",
    "Calling the econml estimator's fit method",
    "As of v0.9, econml has some kewyord only arguments",
    "from https://www.bnmetrics.com/blog/factory-pattern-in-python3-simple-version",
    "Changing shape to a list for a singleton value",
    "Note that self._control_value is assumed to be a singleton value",
    "TODO -- fix: we are actually conditioning on positive treatment (d=1)",
    "For each unit, return the estimated effect of the treatment value",
    "that was actually applied to the unit",
    "this assumes a binary treatment regime",
    "TODO remove neighbors that are more than a given radius apart",
    "estimate ATT on treated by summing over difference between matched neighbors",
    "Now computing ATC",
    "TODO -- fix: we are actually conditioning on positive treatment (d=1)",
    "from https://www.bnmetrics.com/blog/factory-pattern-in-python3-simple-version",
    "Handle externally provided estimator classes",
    "allowed types of distance metric",
    "Dictionary of any user-provided params for the distance metric",
    "that will be passed to sklearn nearestneighbors",
    "Check if the treatment is one-dimensional",
    "Checking if the treatment is binary",
    "Convert the categorical variables into dummy/indicator variables",
    "Basically, this gives a one hot encoding for each category",
    "The first category is taken to be the base line.",
    "this assumes a binary treatment regime",
    "TODO remove neighbors that are more than a given radius apart",
    "estimate ATT on treated by summing over difference between matched neighbors",
    "Return indices in the original dataframe",
    "self.matched_indices_att[treated_df_index[i]] = control.iloc[indices[i]].index.tolist()",
    "Now computing ATC",
    "Return indices in the original dataframe",
    "Add the identification method used in the estimator",
    "Check the backdoor variables being used",
    "Add the observed confounders and one hot encode the categorical variables",
    "Get the data of the unobserved confounders",
    "One hot encode the data if they are categorical",
    "Check the instrumental variables involved",
    "Perform the same actions as the above",
    "Both the outcome and the treatment have to be 1D arrays according to the CausalML API",
    "We want to pass 'v0' rather than ['v0'] to prevent a shape mismatch",
    "For CATEs",
    "TODO we are conditioning on a postive treatment",
    "TODO create an expression corresponding to each estimator used",
    "Generating data with equal 0 and 1 (since ranks are uniformly distributed)",
    "Flipping some values",
    "Wrapping labels if they are too long",
    "This layout tries to mimic the graphviz layout in a simpler form. The depth grows horizontally here instead of",
    "vertically.",
    "Set the figure size based on the number of nodes",
    "Nodes that are vertically connected, but not neighbors should be connected via a curved edge.",
    "All other nodes should be connected with a straight line.",
    "Draw labels node labels",
    "Each node gets a depth assigned, based on the distance to the closest root node.",
    "In case of undirected graphs, we just take any node as root node.",
    "No path to root node, ignore this connection then.",
    "Counts the number of vertical nodes in the same layers.",
    "Creates a matrix indicating whether two nodes are vertical neighbors.",
    "Get all y coordinates per layer",
    "Sort the y-coordinates",
    "Finding p-value using student T test",
    "Only consider edges have absolute edge weight > 0.01",
    "Modify graph such that it only contains bidirected edges",
    "Find c components by finding connected components on the undirected graph",
    "Understanding Neural Network weights",
    "Refer to this link:https://stackoverflow.com/questions/50937628/mlp-classifier-neurons-weights",
    "add weight column",
    "before weights are applied we count number rows in each category",
    "which is equivalent to summing over weight=1",
    "after weights are applied we need to sum over the given weights",
    "First, calculating mean differences by strata",
    "Second, without strata",
    "Third, concatenating them and plotting",
    "Setting estimator attribute for convenience",
    "Outcome is numeric",
    "Treatments are also numeric or binary",
    "Outcome is categorical",
    "Treatments are numeric or binary",
    "TODO: A common way to show all plots",
    "from https://www.bnmetrics.com/blog/factory-pattern-in-python3-simple-version",
    "Get adjacency list",
    "If node pair has been fully explored",
    "Add node1 to backdoor set of node_pair",
    "Check if path is backdoor and does not have nodes1\\node1 or nodes2\\node2 as intermediate nodes",
    "True if arrow incoming, False if arrow outgoing",
    "Mark pair (node1, node2) complete",
    "Modify variable count and indices covered",
    "Average total effect",
    "Natural direct effect",
    "Natural indirect effect",
    "Controlled direct effect",
    "Backdoor method names",
    "First, check if there is a directed path from action to outcome",
    "## 1. BACKDOOR IDENTIFICATION",
    "Pick algorithm to compute backdoor sets according to method chosen",
    "First, checking if there are any valid backdoor adjustment sets",
    "Setting default \"backdoor\" identification adjustment set",
    "## 2. INSTRUMENTAL VARIABLE IDENTIFICATION",
    "Now checking if there is also a valid iv estimand",
    "## 3. FRONTDOOR IDENTIFICATION",
    "Now checking if there is a valid frontdoor variable",
    "Finally returning the estimand object",
    "Pick algorithm to compute backdoor sets according to method chosen",
    "Setting default \"backdoor\" identification adjustment set",
    "Finally returning the estimand object",
    "## 1. FIRST DOING BACKDOOR IDENTIFICATION",
    "First, checking if there are any valid backdoor adjustment sets",
    "Setting default \"backdoor\" identification adjustment set",
    "## 2. SECOND, CHECKING FOR MEDIATORS",
    "Now checking if there are valid mediator variables",
    "Finally returning the estimand object",
    "## 1. FIRST DOING BACKDOOR IDENTIFICATION",
    "First, checking if there are any valid backdoor adjustment sets",
    "Setting default \"backdoor\" identification adjustment set",
    "## 2. SECOND, CHECKING FOR MEDIATORS",
    "Now checking if there are valid mediator variables",
    "Finally returning the estimand object",
    "First, checking if empty set is a valid backdoor set",
    "If the method is `minimal-adjustment`, return the empty set right away.",
    "Second, checking for all other sets of variables. If include_unobserved is false, then only observed variables are eligible.",
    "only remove descendants of Y",
    "also allow any causes of Y that are not caused by T (for lower variance)",
    "remove descendants of T (mediators) and descendants of Y",
    "If var is d-separated from both treatment or outcome, it cannot",
    "be a part of the backdoor set",
    "repeat the above search with BACKDOOR_MIN",
    "If `minimal-adjustment` method is specified, start the search from the set with minimum size. Otherwise, start from the largest.",
    "If the backdoor method is `maximal-adjustment` or `minimal-adjustment`, return the first found adjustment set.",
    "If all variables are observed, and the biggest eligible set",
    "does not satisfy backdoor, then none of its subsets will.",
    "Adding a None estimand if no backdoor set found",
    "Default set contains minimum possible number of instrumental variables, to prevent lowering variance in the treatment variable.",
    "Default set is the one with the least number of adjustment variables (optimizing for efficiency)",
    "For simplicity, assuming a one-variable frontdoor set",
    "Cond 1: All directed paths intercepted by candidate_var",
    "Cond 2: No confounding between treatment and candidate var",
    "Cond 3: treatment blocks all confounding between candidate_var and outcome",
    "For simplicity, assuming a one-variable mediation set",
    "Create estimands dict as per the API for backdoor, but do not return it",
    "Setting default \"backdoor\" identification adjustment set",
    "Create estimands dict as per the API for backdoor, but do not return it",
    "Setting default \"backdoor\" identification adjustment set",
    "TODO: outputs string for now, but ideally should do symbolic",
    "expressions Mon 19 Feb 2018 04:54:17 PM DST",
    "TODO Better support for multivariate treatments",
    "TODO: support multivariate treatments better.",
    "TODO: support multivariate treatments better.",
    "TODO: support multivariate treatments better.",
    "For direct effect",
    "If no costs are passed, use uniform costs",
    "restriction to ancestors",
    "back-door graph",
    "moralization",
    "Estimators list for returning after identification",
    "Line 1",
    "If no action has been taken, the effect on Y is just the marginal of the observational distribution P(v) on Y.",
    "Line 2",
    "If we are interested in the effect on Y, it is sufficient to restrict our attention on the parts of the model ancestral to Y.",
    "Modify list of valid nodes",
    "Line 3 - forces an action on any node where such an action would have no effect on Y \u2013 assuming we already acted on X.",
    "Modify adjacency matrix to obtain that corresponding to do(X)",
    "Line 4 - Decomposes the problem into a set of smaller problems using the key property of C-component factorization of causal models.",
    "If the entire graph is a single C-component already, further problem decomposition is impossible, and we must provide base cases.",
    "Modify adjacency matrix to remove treatment variables",
    "Line 5 - The algorithms fails due to the presence of a hedge - the graph G, and a subgraph S that does not contain any X nodes.",
    "Line 6 - If there are no bidirected arcs from X to the other nodes in the current subproblem under consideration, then we can replace acting on X by conditioning, and thus solve the subproblem.",
    "Line 7 - This is the most complicated case in the algorithm. Explain in the second last paragraph on Pg 41 of the link provided in the docstring above.",
    "Do not show backdoor key unless it is the only backdoor set.",
    "Just show the default backdoor set",
    "self._identified_estimand = self._causal_model.identify_effect()",
    "self._identified_estimand,",
    "self._causal_model._treatment,",
    "self._causal_model._outcome,",
    "If labels provided",
    "Return in valid DOT format",
    "Get adjacency matrix",
    "If labels not provided",
    "Obtain valid DOT format",
    "If labels provided",
    "Return in valid DOT format",
    "from https://www.bnmetrics.com/blog/factory-pattern-in-python3-simple-version",
    "from https://www.bnmetrics.com/blog/factory-pattern-in-python3-simple-version"
  ],
  "v0.9.1": [
    "-*- coding: utf-8 -*-",
    "",
    "Configuration file for the Sphinx documentation builder.",
    "",
    "This file does only contain a selection of the most common options. For a",
    "full list see the documentation:",
    "http://www.sphinx-doc.org/en/stable/config",
    "-- Path setup --------------------------------------------------------------",
    "If extensions (or modules to document with autodoc) are in another directory,",
    "add these directories to sys.path here. If the directory is relative to the",
    "documentation root, use os.path.abspath to make it absolute, like shown here.",
    "",
    "-- Project information -----------------------------------------------------",
    "Version Information (for version-switcher)",
    "-- General configuration ---------------------------------------------------",
    "If your documentation needs a minimal Sphinx version, state it here.",
    "",
    "needs_sphinx = '1.0'",
    "Add any Sphinx extension module names here, as strings. They can be",
    "extensions coming with Sphinx (named 'sphinx.ext.*') or your custom",
    "ones.",
    "sphinx-panels shouldn't add bootstrap css since the pydata-sphinx-theme",
    "already loads it",
    "Add any paths that contain templates here, relative to this directory.",
    "The suffix(es) of source filenames.",
    "You can specify multiple suffix as a list of string:",
    "",
    "source_suffix = ['.rst', '.md']",
    "The master toctree document.",
    "The language for content autogenerated by Sphinx. Refer to documentation",
    "for a list of supported languages.",
    "",
    "This is also used if you do content translation via gettext catalogs.",
    "Usually you set \"language\" from the command line for these cases.",
    "List of patterns, relative to source directory, that match files and",
    "directories to ignore when looking for source files.",
    "This pattern also affects html_static_path and html_extra_path .",
    "The name of the Pygments (syntax highlighting) style to use.",
    "-- Options for HTML output -------------------------------------------------",
    "The theme to use for HTML and HTML Help pages.  See the documentation for",
    "a list of builtin themes.",
    "",
    "Theme options are theme-specific and customize the look and feel of a theme",
    "further.  For a list of options available for each theme, see the",
    "documentation.",
    "",
    "Add any paths that contain custom static files (such as style sheets) here,",
    "relative to this directory. They are copied after the builtin static files,",
    "so a file named \"default.css\" will overwrite the builtin \"default.css\".",
    "Custom sidebar templates, must be a dictionary that maps document names",
    "to template names.",
    "",
    "The default sidebars (for documents that don't match any pattern) are",
    "defined by theme itself.  Builtin themes are using these templates by",
    "default: ``['localtoc.html', 'relations.html', 'sourcelink.html',",
    "'searchbox.html']``.",
    "",
    "html_sidebars = {}",
    "-- Options for HTMLHelp output ---------------------------------------------",
    "Output file base name for HTML help builder.",
    "-- Options for LaTeX output ------------------------------------------------",
    "The paper size ('letterpaper' or 'a4paper').",
    "",
    "'papersize': 'letterpaper',",
    "The font size ('10pt', '11pt' or '12pt').",
    "",
    "'pointsize': '10pt',",
    "Additional stuff for the LaTeX preamble.",
    "",
    "'preamble': '',",
    "Latex figure (float) alignment",
    "",
    "'figure_align': 'htbp',",
    "Grouping the document tree into LaTeX files. List of tuples",
    "(source start file, target name, title,",
    "author, documentclass [howto, manual, or own class]).",
    "-- Options for manual page output ------------------------------------------",
    "One entry per manual page. List of tuples",
    "(source start file, name, description, authors, manual section).",
    "-- Options for Texinfo output ----------------------------------------------",
    "Grouping the document tree into Texinfo files. List of tuples",
    "(source start file, target name, title, author,",
    "dir menu entry, description, category)",
    "-- Options for Epub output -------------------------------------------------",
    "Bibliographic Dublin Core info.",
    "The unique identifier of the text. This can be a ISBN number",
    "or the project homepage.",
    "",
    "epub_identifier = ''",
    "A unique identification for the text.",
    "",
    "epub_uid = ''",
    "A list of files that should not be packed into the epub file.",
    "-- Extension configuration -------------------------------------------------",
    "-- Options for todo extension ----------------------------------------------",
    "If true, `todo` and `todoList` produce output, else they produce nothing.",
    "init docstrings should also be included in class",
    "Patch all of the published versions",
    "check old RST version (<= v0.8)",
    "Remove old version links",
    "Append updated version links",
    "requires stdin input for identify in weighting sampler",
    "requires Rpy2 for lalonde",
    "will be removed",
    "applied notebook, not necessary to test each time",
    "needs xgboost too",
    "Slow Notebooks",
    "requires Rpy2 for causal discovery",
    "daily tests of dowhy_causal_discovery_example.ipynb are failing due to cdt/rpy2 config.",
    "comment out, since we are switching causal discovery implementations",
    "Adding the dowhy root folder to the python path so that jupyter notebooks",
    "can import dowhy",
    "\"--ExecutePreprocessor.timeout=600\",",
    "self.df = pd.read_csv(os.path.join(DATA_PATH,'dgp_1/acic_1_1_data.csv'))",
    "self.ate = np.mean(self.df['y1'] - self.df['y0'])",
    "treated = self.df[self.df['z']==1]",
    "self.att = np.mean(treated['y1'] - treated['y0'])",
    "def test_average_treatment_effect(self):",
    "est_ate = 1",
    "bias = est_ate - self.ate",
    "print(bias)",
    "self.assertAlmostEqual(self.ate, est_ate)",
    "def test_average_treatment_effect_on_treated(self):",
    "est_att = 1",
    "self.att=1",
    "bias = est_att - self.att",
    "print(bias)",
    "self.assertAlmostEqual(self.att, est_att)",
    "removing two common causes",
    "removing two common causes",
    "removing two common causes",
    "removing two common causes",
    "check if all partial R^2 values are between 0 and 1",
    "We calculate adjusted estimates for two sets of partial R^2 values.",
    "Test if hypothetical confounding by unobserved confounder u1 leads to an adjusted effect that is farther from the original estimate as compared to u2",
    "Creating a model with no unobserved confounders",
    "check if all partial R^2 values are between 0 and 1",
    "for a dataset with no confounders, the robustness value should be higher than a given threshold (0.95 in our case)",
    "Non Parametric estimator",
    "We calculate adjusted estimates for two sets of partial R^2 values.",
    "adjusted lower ate bound for confounder u1 where r2tu_w = 0.7 and r2yu_tw = 0.9",
    "adjusted lower ate bound for confounder u2 where r2tu_w = 0.2 and r2yu_tw = 0.3",
    "adjusted upper ate bound for confounder u1 where r2tu_w = 0.7 and r2yu_tw = 0.9",
    "adjusted upper ate bound for confounder u2 where r2tu_w = 0.2 and r2yu_tw = 0.3",
    "adjusted lower confidence bound for confounder u1 where r2tu_w = 0.7 and r2yu_tw = 0.9",
    "adjusted lower confidence bound for confounder u2 where r2tu_w = 0.2 and r2yu_tw = 0.3",
    "adjusted upper confidence bound for confounder u1 where r2tu_w = 0.7 and r2yu_tw = 0.9",
    "adjusted upper confidence bound for confounder u2 where r2tu_w = 0.2 and r2yu_tw = 0.3",
    "Test if hypothetical confounding by unobserved confounder u1 leads to an adjusted effect that is farther from the original estimate as compared to u2",
    "we patched figure plotting call to avoid drawing plots during tests",
    "We calculate adjusted estimates for two sets of partial R^2 values.",
    "adjusted lower ate bound for confounder u1 where r2tu_w = 0.7 and r2yu_tw = 0.9",
    "adjusted lower ate bound for confounder u2 where r2tu_w = 0.2 and r2yu_tw = 0.3",
    "adjusted upper ate bound for confounder u1 where r2tu_w = 0.7 and r2yu_tw = 0.9",
    "adjusted upper ate bound for confounder u2 where r2tu_w = 0.2 and r2yu_tw = 0.3",
    "adjusted lower confidence bound for confounder u1 where r2tu_w = 0.7 and r2yu_tw = 0.9",
    "adjusted lower confidence bound for confounder u2 where r2tu_w = 0.2 and r2yu_tw = 0.3",
    "adjusted upper confidence bound for confounder u1 where r2tu_w = 0.7 and r2yu_tw = 0.9",
    "adjusted upper confidence bound for confounder u2 where r2tu_w = 0.2 and r2yu_tw = 0.3",
    "Test if hypothetical confounding by unobserved confounder u1 leads to an adjusted effect that is farther from the original estimate as compared to u2",
    "we patched figure plotting call to avoid drawing plots during tests",
    "comparing test examples from R E-Value package",
    "check implementation of Observed Covariate E-value against R package",
    "The outcome is a linear function of the confounder",
    "The slope is 1,2 and the intercept is 3",
    "As we run with only one common cause and one instrument variable we run with (?, 2)",
    "Supports user-provided dataset object",
    "To test if there are any exceptions",
    "To test if the estimate is identical if refutation parameters are zero",
    "This value is hardcoded to be zero as we are runnning this on a linear dataset.",
    "Ordinarily, we should expect this value to be zero.",
    "This value is hardcoded to be zero as we are runnning this on a linear dataset.",
    "Ordinarily, we should expect this value to be zero.",
    "Calculate causal effect twice: once for unit (t=1, c=0), once for specific increase (t=100, c=50)",
    "Only P(Y|T) should be present for test to succeed.",
    "Calculate causal effect twice: once for unit (t=1, c=0), once for specific increase (t=100, c=50)",
    "Since undirected graph, identify effect must throw an error.",
    "Calculate causal effect twice: once for unit (t=1, c=0), once for specific increase (t=100, c=50)",
    "Compare with ground truth",
    "Calculate causal effect twice: once for unit (t=1, c=0), once for specific increase (t=100, c=50)",
    "Compare with ground truth",
    "Calculate causal effect twice: once for unit (t=1, c=0), once for specific increase (t=100, c=50)",
    "Compare with ground truth",
    "Calculate causal effect twice: once for unit (t=1, c=0), once for specific increase (t=100, c=50)",
    "Compare with ground truth",
    "Causal model initialization",
    "Obtain backdoor sets",
    "Check if backdoor sets are valid i.e. if they block all paths between the treatment and the outcome",
    "Causal model initialization",
    "Obtain backdoor sets",
    "Causal model initialization",
    "Obtain backdoor sets",
    "Check if backdoor sets are valid i.e. if they block all paths between the treatment and the outcome",
    "Causal model initialization",
    "Obtain backdoor sets",
    "Check if backdoor sets are valid i.e. if they block all paths between the treatment and the outcome",
    "Causal model initialization",
    "Obtain backdoor sets",
    "Check if backdoor sets are valid i.e. if they block all paths between the treatment and the outcome",
    "Causal model initialization",
    "Obtain backdoor sets",
    "Check if backdoor sets are valid i.e. if they block all paths between the treatment and the outcome",
    "Example is selected from Pearl J. \"Causality\" 2nd Edition, from chapter 3.3.1 on backoor criterion.",
    "The following simpsons paradox examples are taken from Pearl, J {2013}. \"Understanding Simpson\u2019s Paradox\" - http://ftp.cs.ucla.edu/pub/stat_ser/r414.pdf",
    "The following are examples given in the \"Book of Why\" by Judea Pearl, chapter \"The Do-operator and the Back-Door Criterion\"",
    "For all examples from these papers we use X for the treatment variable",
    "instead of A.",
    "Figure 6 from Smucler, Sapienza and Rotnitzky (2021), Biometrika",
    "Figure 5 from Smucler, Sapienza and Rotnitzky (2021), Biometrika",
    "Figure 4 from Smucler, Sapienza and Rotnitzky (2021), Biometrika",
    "Figure 3 from Smucler, Sapienza and Rotnitzky (2021), Biometrika",
    "Figure 2 from Smucler and Rotnitzky (2022), Journal of Causal Inference",
    "L replaces X as the conditional variable",
    "Figure 2 from Smucler and Rotnitzky (2022), Journal of Causal Inference",
    "L replaces X as the conditional variable. Uses different costs",
    "Figure 3 from Smucler and Rotnitzky (2022), Journal of Causal Inference",
    "A graph where optimal, optimal minimal and optimal min cost are different",
    "The graph from Shrier and Platt (2008)",
    "A graph for which the algorithm was producing wrong result due to a bug reported by Sara Taheri",
    "Another graph for which the algorithm was producing wrong result due to a bug reported by Sara Taheri",
    "cov_mat = np.diag(np.ones(num_features))",
    "collider: X->Z<-Y",
    "chain: X->Z->Y",
    "fork: X<-Z->Y",
    "general DAG: X<-Z->Y, X->Y",
    "fork: X<-Z->Y",
    "Contributions should add up to Var(X2)",
    "Contributions should add up to Var(X2)",
    "H(P(Y)) -- Can be precomputed",
    "-(H(P(Y | do(x_S)) - H(P(Y))) = H(P(Y)) - H(P(Y | do(x_S))",
    "H(P(Y | do(x_S)) = H(E[P(Y | x_S, X'_\\S)])",
    "E[P(Y | x_S, X'_\\S)]",
    "H(E[P(Y | x_S, X'_\\S)])",
    "Using H(P(Y)) based on the origina data, i.e. ignoring baseline_predictions.",
    "E[H(P(Y)) - H(P(Y | do(X_U))] = H(P(Y)) - E[H(P(Y | X))]",
    "Just checking formats, i.e. no need for correlation.",
    "Just checking formats, i.e. no need for correlation.",
    "Mean from the categorical part is: (-5 + 5+ 10) / 3 = 10/3",
    "C2 = 3 * A2 + 2 * B2",
    "By default, the strength is measure with respect to the variance.",
    "Here, we misspecified the mechanism on purpose by setting scale to 1 instead of 2.",
    "If we provide the observational data here, we can mitigate the misspecification of the causal mechanism.",
    "Here, changing the mechanism.",
    "Here, changing the mechanism.",
    "Here, changing the mechanism.",
    "Here, changing the mechanism.",
    "Defining an anomaly scorer that handles multidimensional inputs.",
    "Seeing that the expectation of the noise in all nodes is 0, we introduce anomalies by setting some of them to 3.",
    "In the first sample, only the first variable is anomalous. Therefore, it should have the highest contribution",
    "and it should be \"significantly\" higher than the contribution of the other ones (here, we just arbitrarily say",
    "it should be 3x higher. Due to the confounding factor Z, the reconstructed noise variables are pairwise dependent,",
    "which is a violation of our causal sufficiency assumption. However, a confounder is included here to demonstrate",
    "some robustness. Note that due to this and stochastic behaviour of the density estimator, it is",
    "not possible to analytically compute expected results. Therefore, we rather look at the relations here.",
    "Same idea for the second sample, but here, it is the second variable that is anomalous.",
    "In the fourth sample, there are 2 anomalous variables. Therefore, the contribution of these 2 variables should be",
    "\"significantly\" higher than the contribution of the other variables. The contribution of both anomalous variables",
    "should be equal (approximately).",
    "Defining an anomaly scorer that handles multidimensional inputs.",
    "Seeing that the expectation of the noise in all nodes is 0, we introduce anomalies by setting some of them to 3.",
    "reduce the score.",
    "The contributions should add up to g(x) - E[g(X)]",
    "The contributions should add up to g(x) - E[g(X)]",
    "The contributions should add up to g(x) - E[g(X)]",
    "Three examples:",
    "1. X1 is the root cause (+ 10 to the noise)",
    "2. X0 is the root cause (+ 10 to the noise)",
    "3. X0 and X3 are both root causes (+ 10 to both noises)",
    "The sum of the scores should add up to the anomaly score of the target (here, X3).",
    "Three examples:",
    "1. X1 is the root cause (+ 10 to the noise)",
    "2. X0 is the root cause (+ 10 to the noise)",
    "3. X0 and X3 are both root causes (+ 10 to both noises)",
    "The sum of the scores should add up to the anomaly score of the target (here, X3).",
    "Defining ground truth model to avoid SCM learning issues and, hence, to focus on the anomaly attribution",
    "algorithm.",
    "1. X0 is the root cause (+ 10 to the noise)",
    "2. X0 and X1 are the root causes (+ 10 to both noise)",
    "3. X2 and X3 are both root causes (+ 10 to both noises)",
    "The sum of the scores should add up to the anomaly score of the target (here, X3).",
    "Check if calling the method causes some import or runtime errors",
    "TODO: Plotting undirected graphs with networkx causes an error when an older networkx version is used with a newer",
    "newer matplotlib version:",
    "AttributeError: module 'matplotlib.cbook' has no attribute 'is_numlike'",
    "Networkx 2.4+ should fix this issue.",
    "plot_adjacency_matrix(causal_graph, is_directed=False)",
    "Setup data",
    "Test LinearDML",
    "Test ContinuousTreatmentOrthoForest",
    "Test LinearDRLearner",
    "Setup data",
    "Test DeepIV",
    "Treatment model,",
    "Response model",
    "Test IntentToTreatDRIV",
    "Observed data",
    "assumed graph",
    "Identify effect",
    "Estimate effect",
    "Not using testsuite from .base/TestEstimtor, custom code below",
    "More cases where Exception  is expected",
    "Compute confidence intervals, standard error and significance tests",
    "Defined a linear dataset with a given set of properties",
    "Create a model that captures the same",
    "Identify the effects within the model",
    "Defined a linear dataset with a given set of properties",
    "Create a model that captures the same",
    "Identify the effects within the model",
    "Defined a linear dataset with a given set of properties",
    "Create a model that captures the same",
    "Identify the effects within the model",
    "Defined a linear dataset with a given set of properties",
    "Create a model that captures the same",
    "Identify the effects within the model",
    "Defined a linear dataset with a given set of properties",
    "Create a model that captures the same",
    "Identify the effects within the model",
    "Can use these lists to specify the models/estimators/refuters that a particular interpreter supports.  Throw a ValueError if the user provides an incompatible object to intepret.",
    "Unpacking the keyword arguments",
    "todo: add docstring for common parameters here and remove from child refuter classes",
    "Default value for the number of simulations to be conducted",
    "joblib params for parallel processing",
    "Concatenate the confounders, instruments and effect modifiers",
    "Shuffle the confounders",
    "Check if all are select or deselect variables",
    "Check if all the required_variables belong to confounders, instrumental variables or effect",
    "Get the number of simulations",
    "Sort the simulations",
    "Obtain the median value",
    "Performing a two sided test",
    "np.searchsorted tells us the index if it were a part of the array",
    "We select side to be left as we want to find the first value that matches",
    "We subtact 1 as we are finding the value from the right tail",
    "We take the side to be right as we want to find the last index that matches",
    "We get the probability with respect to the left tail.",
    "return twice the determined quantile as this is a two sided test",
    "Get the mean for the simulations",
    "Get the standard deviation for the simulations",
    "Get the Z Score [(val - mean)/ std_dev ]",
    "Initializing the p_value",
    "Perform Bootstrap Significance Test with the original estimate and the set of refutations",
    "Perform Normal Tests of Significance with the original estimate and the set of refutations",
    "Perform Bootstrap Significance Test with the original estimate and the set of refutations",
    "Perform Normal Tests of Significance with the original estimate and the set of refutations",
    "re.sub only takes string parameter so the first if is to avoid error",
    "if the input is a text file, convert the contained data into string",
    "load dot file",
    "Adding node attributes",
    "adding penwidth to make the edge bold",
    "Adding common causes",
    "Adding instruments",
    "Adding effect modifiers",
    "Assuming the simple form of effect modifier",
    "that directly causes the outcome.",
    "self._graph.add_edge(node_name, outcome, style = \"dotted\", headport=\"s\", tailport=\"n\")",
    "self._graph.add_edge(outcome, node_name, style = \"dotted\", headport=\"n\", tailport=\"s\") # TODO make the ports more general so that they apply not just to top-bottom node configurations",
    "Adding columns in the dataframe as confounders that were not in the graph",
    "Adding unobserved confounders",
    "removal of only direct edges wrt a target is not implemented for incoming edges",
    "also return the number of backdoor paths blocked by observed nodes",
    "Assume that nodes1 is the treatment",
    "ignores new_graph parameter, always uses self._graph",
    "remove paths that have nodes1\\node1 or nodes2\\node2 as intermediate nodes",
    "TODO Refactor to remove this from here and only implement this logic in causalIdentifier. Unnecessary assumption of nodes1 to be causing nodes2.",
    "Cannot simply compute ancestors, since that will also include nodes1 and its parents (e.g. instruments)",
    "removing all mediators",
    "[TODO: double check these work with multivariate implementation:]",
    "Exclusion",
    "As-if-random setup",
    "As-if-random",
    "convert the outputted generator into a list",
    "dpaths = self.get_all_directed_paths(nodes1, nodes2)",
    "return len(dpaths) > 0",
    "Condition 1: node 1 ---> node 2 is intercepted by candidate_nodes",
    "Create causal graph object",
    "Sometimes, effect modifiers from the graph may not match those provided by the user.",
    "(Because some effect modifiers may also be common causes)",
    "In such cases, the user-provided modifiers are used.",
    "If no effect modifiers are provided,  then the ones from the graph are used.",
    "Import causal discovery class",
    "Initialize causal graph object",
    "TODO add propensity score as default backdoor method, iv as default iv method, add an informational message to show which method has been selected.",
    "TODO add dowhy as a prefix to all dowhy estimators",
    "This is done as all dowhy estimators have two parts and external ones have two or more parts",
    "Define the third-party estimation method to be used",
    "Process the dowhy estimators",
    "Check if estimator's target estimand is identified",
    "Note that while the name of the variable is the same,",
    "\"self.causal_estimator\", this estimator takes in less",
    "parameters than the same from the",
    "estimate_effect code. It is not advisable to use the",
    "estimator from this function to call estimate_effect",
    "with fit_estimator=False.",
    "Estimator had been computed in a previous call",
    "The default number of simulations for statistical testing",
    "The default number of simulations to obtain confidence intervals",
    "The portion of the total size that should be taken each time to find the confidence intervals",
    "1 is the recommended value",
    "https://ocw.mit.edu/courses/mathematics/18-05-introduction-to-probability-and-statistics-spring-2014/readings/MIT18_05S14_Reading24.pdf",
    "https://projecteuclid.org/download/pdf_1/euclid.ss/1032280214",
    "The default Confidence Level",
    "Number of quantiles to discretize continuous columns, for applying groupby",
    "Prefix to add to temporary categorical variables created after discretization",
    "std args to be removed from locals() before being passed to args_dict",
    "Setting the default interpret method",
    "Check if some parameters were set, otherwise set to default values",
    "Estimate conditional estimates by default",
    "TODO Only works for binary treatment",
    "Defaulting to class default values if parameters are not provided",
    "Checking that there is at least one effect modifier",
    "Making sure that effect_modifier_names is a list",
    "Making a copy since we are going to be changing effect modifier names",
    "For every numeric effect modifier, adding a temp categorical column",
    "Grouping by effect modifiers and computing effect separately",
    "Deleting the temporary categorical columns",
    "The array that stores the results of all estimations",
    "Find the sample size the proportion with the population size",
    "Perform the set number of simulations",
    "names of treatment and outcome",
    "Using class default parameters if not specified",
    "Checking if bootstrap_estimates are already computed",
    "Checked if any parameter is changed from the previous std error estimate",
    "Now use the data obtained from the simulations to get the value of the confidence estimates",
    "Get the variations of each bootstrap estimate and sort",
    "Now we take the (1- p)th and the (p)th variations, where p is the chosen confidence level",
    "Get the lower and upper bounds by subtracting the variations from the estimate",
    "Use existing params, if new user defined params are not present",
    "Checking if bootstrap_estimates are already computed",
    "Check if any parameter is changed from the previous std error estimate",
    "Use existing params, if new user defined params are not present",
    "self._outcome = self._data[\"dummy_outcome\"]",
    "Processing the null hypothesis estimates",
    "Doing a two-sided test",
    "Being conservative with the p-value reported",
    "Being conservative with the p-value reported",
    "If the estimate_index is 0, it depends on the number of simulations",
    "Need to test r-squared before supporting",
    "effect_r_squared = self._evaluate_effect_strength(estimate, method=\"r-squared\")",
    "'r-squared': effect_r_squared",
    "elif method == \"r-squared\":",
    "outcome_mean = np.mean(self._outcome)",
    "total_variance = np.sum(np.square(self._outcome - outcome_mean))",
    "Assuming a linear model with one variable: the treatment",
    "Currently only works for continuous y",
    "causal_model = outcome_mean + estimate.value*self._treatment",
    "squared_residual = np.sum(np.square(self._outcome - causal_model))",
    "r_squared = 1 - (squared_residual/total_variance)",
    "return r_squared",
    "Check if estimator's target estimand is identified",
    "Store parameters inside estimate object for refutation methods",
    "TODO: This add_params needs to move to the estimator class",
    "inside estimate_effect and estimate_conditional_effect",
    "No estimand was identified (identification failed)",
    "s += \"Variance in outcome explained by treatment: {}\\n\".format(self.effect_strength[\"r-squared\"])",
    "Below loop assumes that the last indices of W are alwawys converted to discrete",
    "one-hot encode discrete W",
    "Now deleting the old continuous value",
    "create a dataset with 10 observations one binary treatment and a continuous outcome affected by one common cause",
    "Two continuous treatments, no common cause, an instrumental variable and two effect modifiers - linearly added appropriately",
    "One Hot Encoding",
    "Making beta an array",
    "TODO Ensure that we do not generate weak instruments",
    "TODO - test all our methods with random noise added to covariates (instead of the stochastic treatment assignment)",
    "Converting treatment to binary if required",
    "Generating frontdoor variables if asked for",
    "NOTE: We are assuming a linear relationship *even when t is categorical* and integer coded.",
    "For categorical t, this example dataset has the effect size for category 2 being exactly",
    "double the effect for category 1",
    "This could be changed at this stage by one-hot encoding t and using a custom beta that",
    "sets a different effect for each category {0, 1, 2}",
    "Computing ATE",
    "constructing column names for one-hot encoded discrete features",
    "Specifying the correct dtypes",
    "Now specifying the corresponding graph strings",
    "Now writing the gml graph",
    "Making beta an array",
    "creating data frame",
    "Specifying the correct dtypes",
    "Now specifying the corresponding graph strings",
    "Now writing the gml graph",
    "Adding edges between common causes and the frontdoor mediator",
    "Error terms",
    "else:",
    "V = 6 + W0 + tterm + E1",
    "Y = 12 + W0*W0 + W0*W0 + yterm + E2  # E2_new",
    "Generating a random normal distribution of integers",
    "Generating data for nodes which have no incoming edges",
    "\"currset\" variable currently has all the successors of the nodes which had no incoming edges",
    "Making beta an array",
    "assuming that all unobserved common causes are numerical and are not affected by one hot encoding",
    "Creating a NN to simulate the nuisance function",
    "strength of unobserved confounding",
    "Computing ATE",
    "Specifying the correct dtypes",
    "Now writing the gml graph",
    "The following code for loading the Lalonde dataset was copied from",
    "https://github.com/wayfair/pylift/blob/5afc9088e96f25672423663f5c9b4bb889b4dfc0/examples/Lalonde/Lalonde_sample.ipynb?short_path=b1d451f#L94-L99).",
    "",
    "Copyright 2018, Wayfair, Inc.",
    "",
    "Redistribution and use in source and binary forms, with or without modification, are permitted provided that",
    "the following conditions are met:",
    "",
    "1. Redistributions of source code must retain the above copyright notice, this list of conditions and the",
    "following disclaimer.",
    "",
    "2. Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the",
    "following disclaimer in the documentation and/or other materials provided with the distribution.",
    "",
    "THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND ANY EXPRESS OR IMPLIED",
    "WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A",
    "PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY",
    "DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,",
    "PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER",
    "CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR",
    "OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH",
    "DAMAGE.",
    "",
    "0.0.0 is standard placeholder for poetry-dynamic-versioning",
    "any changes to this should not be checked in",
    "",
    "The currently supported estimators",
    "The default standard deviation for noise",
    "The default scaling factor to determine the bucket size",
    "The minimum number of points for the estimator to run",
    "The Default Transformation, when no arguments are given, or if the number of data points are insufficient for an estimator",
    "The Default True Causal Effect, this is taken to be ZERO by default",
    "The Default split for the number of data points that fall into the training and validation sets",
    "Assuming that outcome is one-dimensional",
    "We need to change the identified estimand",
    "We thus, make a copy. This is done as we don't want",
    "to change the original DataFrame",
    "We use collections.OrderedDict to maintain the order in which the data is stored",
    "Check if we are using an estimator in the transformation list",
    "The rationale behind ordering of the loops is the fact that we induce randomness everytime we create the",
    "Train and the Validation Datasets. Thus, we run the simulation loop followed by the training and the validation",
    "loops. Thus, we can get different values everytime we get the estimator.",
    "for _ in range( self._num_simulations ):",
    "Warn the user that the specified parameter is not applicable when no estimator is present in the transformation",
    "Adding an unobserved confounder if provided by the user",
    "We set X_train = 0 and outcome_train to be 0",
    "Get the final outcome, after running through all the values in the transformation list",
    "Check if the value of true effect has been already stored",
    "We use None as the key as we have no base category for this refutation",
    "As we currently support only one treatment",
    "If the number of data points is too few, run the default transformation: [(\"zero\",\"\"),(\"noise\", {'std_dev':1} )]",
    "Check if the value of true effect has been already stored",
    "This ensures that we calculate the causal effect only once.",
    "We use key_train as we map data with respect to the base category of the data",
    "As we currently support only one treatment",
    "Add h(t) to f(W) to get the dummy outcome",
    "We convert to ndarray for ease in indexing",
    "The data is of the form",
    "sim1: cat1 cat2 ... catn",
    "sim2: cat1 cat2 ... catn",
    "Note: We would like the causal_estimator to find the true causal estimate that we have specified through this",
    "refuter. Let the value of the true causal effect be h(t). In the following section of code, we wish to find out if h(t) falls in the",
    "distribution of the refuter.",
    "True Causal Effect list",
    "Iterating through the refutation for each category",
    "We use string arguments to account for both 32 and 64 bit varaibles",
    "action for continuous variables",
    "Action for categorical variables",
    "Find the set difference for each row",
    "Choose one out of the remaining",
    "We want to see if the estimate falls in the same distribution as the one generated by the refuter",
    "Ideally that should be the case as running bootstrap should not have a significant effect on the ability",
    "of the treatment to affect the outcome",
    "Standardizing the data",
    "Fit a model containing all confounders and compare predictions",
    "using all features compared to all features except a given",
    "confounder.",
    "Estimating the regression coefficient from standardized features to t",
    "By default, return a plot with 10 points",
    "consider 10 values of the effect of the unobserved confounder",
    "Standardizing the data",
    "Fit a model containing all confounders and compare predictions",
    "using all features compared to all features except a given",
    "confounder.",
    "By default, return a plot with 10 points",
    "consider 10 values of the effect of the unobserved confounder",
    "By default, we add the effect of simulated confounder for treatment.",
    "But subtract it from outcome to create a negative correlation",
    "assuming that the original confounder's effect was positive on both.",
    "This is to remove the effect of the original confounder.",
    "By default, we add the effect of simulated confounder for treatment.",
    "But subtract it from outcome to create a negative correlation",
    "assuming that the original confounder's effect was positive on both.",
    "This is to remove the effect of the original confounder.",
    "Obtaining the list of observed variables",
    "Taking a subset of the dataframe that has only observed variables",
    "Residuals from the outcome model obtained by fitting a linear model",
    "Residuals from the treatment model obtained by fitting a linear model",
    "Initialising product_cor_metric_observed with a really low value as finding maximum",
    "The user has an option to give the the effect_strength_on_y and effect_strength_on_t which can be then used instead of maximum correlation with treatment and outcome in the observed variables as it specifies the desired effect.",
    "Choosing a c_star based on the data.",
    "The correlations stop increasing upon increasing c_star after a certain value, that is it plateaus and we choose the value of c_star to be the value it plateaus.",
    "Choosing c1 and c2 based on the hyperbolic relationship once c_star is chosen by going over various combinations of c1 and c2 values and choosing the combination which",
    "which maintains the minimum distance between the product of correlations of the simulated variable and the product of maximum correlations of one of the observed variables",
    "and additionally checks if the ratio of the weights are such that they maintain the ratio of the maximum possible observed coefficients within some confidence interval",
    "c1_final and c2_final are initialised to the values on the hyperbolic curve such that c1_final = c2_final  and c1_final*c2_final = c_star",
    "initialising min_distance_between_product_cor_metrics to be a value greater than 1",
    "If the estimator used is LinearDML, partially linear sensitivity analysis will be automatically chosen",
    "Get a 2D matrix of values",
    "x,y =  np.meshgrid(self.kappa_t, self.kappa_y) # x,y are both MxN",
    "Store the values into the refute object",
    "Adding a label on the contour line for the original estimate",
    "Label every other level using strings",
    "Default value of the p value taken for the distribution",
    "Number of Trials: Number of cointosses to understand if a sample gets the treatment",
    "Mean of the Normal Distribution",
    "Standard Deviation of the Normal Distribution",
    "Create a new column in the data by the name of placebo",
    "Sanity check the data",
    "only permute is supported for iv methods",
    "For IV methods, the estimating_instrument_names should also be",
    "changed. Create a copy to avoid modifying original object",
    "We need to change the identified estimand",
    "We make a copy as a safety measure, we don't want to change the",
    "original DataFrame",
    "Run refutation in parallel",
    "Note: We hardcode the estimate value to ZERO as we want to check if it falls in the distribution of the refuter",
    "Ideally we should expect that ZERO should fall in the distribution of the effect estimates as we have severed any causal",
    "relationship between the treatment and the outcome.",
    "new estimator",
    "new effect estimate",
    "observed covariate E-value",
    "see Table 2 and p.37 in https://dash.harvard.edu/bitstream/handle/1/36874927/EValue_FinalSubmission.pdf",
    "if CI crosses null, set its E-value to 1",
    "only report E-value for CI limit closer to null",
    "see Table 2 and p.37 in https://dash.harvard.edu/bitstream/handle/1/36874927/EValue_FinalSubmission.pdf",
    "whether the DGP is assumed to be partially linear",
    "features are the observed confounders",
    "Now code for benchmarking using covariates begins",
    "R^2 of outcome with observed common causes and treatment",
    "R^2 of treatment with observed common causes",
    "Partial R^2 of outcome after regressing over unobserved confounder, observed common causes and treatment",
    "Assuming that the difference in R2 is the same for wj and new unobserved confounder",
    "for treatment,  Calpha is not a function of the partial R2. So we need a different assumption.",
    "Assuming that the ratio of variance of alpha^2 is the same for wj and new unobserved confounder",
    "(1-ratio_var_alpha_wj) is the numerator of Calpha2, similar to the partial R2 for treatment",
    "wrt unobserved confounders in partial-linear models",
    "whether the DGP is assumed to be partially linear",
    "can change this to allow default values that are same as the other parameter",
    "Strength of confounding that omitted variables generate in treatment regression",
    "computing the point estimate for the bounds",
    "common causes after removing the benchmark causes",
    "dataframe with treatment and observed common causes after removing benchmark causes",
    "R^2 of treatment with observed common causes removing benchmark causes",
    "return the variance of alpha_s",
    "R^2 of outcome with observed common causes and treatment after removing benchmark causes",
    "Obtaining theta_s (the obtained estimate)",
    "Creating numpy arrays",
    "Setting up cross-validation parameters",
    "tuple of residuals from first stage estimation [0,1], and the confounders [2]",
    "We need to estimate, sigma^2 = (Y-g_s)^2. We use the following derivation.",
    "Yres = Y - E[Y|W]",
    "E[Y|W] = f(x) + theta_s * E[T|W]",
    "Yres = Y - f(x) - theta_s * E[T|W]",
    "g(s) = theta_s * T + f(x)",
    "g(s) = theta_s * (T - E[T|W]) + f(x) + theta_s * E[T|W]",
    "g(s) = theta_s * Tres +f(x) + theta_s * E[T|W]",
    "Y - g(s) = Y - [theta_s * Tres + f(x) + theta_s * E[T|W] )",
    "Y - g(s) = ( Y - f(x) -  theta_s * E[T|W]) - theta_s * Tres",
    "Y - g(s) = Yres - theta_s * Tres",
    "nu_2 is E[alpha_s^2]",
    "Now computing scores for finding the (1-a) confidence interval",
    "R^2 of treatment with observed common causes",
    "R^2 of outcome with treatment and observed common causes",
    "Partial R^2 of outcome after regressing over unobserved confounder, observed common causes and treatment",
    "Partial R^2 of treatment after regressing over unobserved confounder and observed common causes",
    "adding 1.1 as plotting margin  ensure that the benchmarked part is shown fully in plot",
    "adding 1.1 as plotting margin  ensure that the benchmarked part is shown fully in plot",
    "Adding unadjusted point estimate",
    "Adding bounds to partial R^2 values for given strength of confounders",
    "Adding a new backdoor variable to the identified estimand",
    "Run refutation in parallel",
    "We want to see if the estimate falls in the same distribution as the one generated by the refuter",
    "Ideally that should be the case as choosing a subset should not have a significant effect on the ability",
    "of the treatment to affect the outcome",
    "TODO: Sensitivity Analyzers excluded from list due to different return type",
    "Reject H0",
    "a, b and c are all continuous variables",
    "a, b and c are all discrete variables",
    "c is set of continuous and binary variables and",
    "1. either a and b is continuous and the other is binary",
    "2. both a and b are binary",
    "c is discrete and",
    "either a or b is continuous and the other is discrete",
    "a and b are discrete and c is a mixture of discrete and continuous variables. We discretize c and calculate conditional mutual information",
    "original_treatment_name: : stores original variable names for labelling",
    "common_causes_map : maps the original variable names to variable names in OLS regression",
    "benchmark_common_causes: stores variable names in terms of regression model variables",
    "original_benchmark_covariates: stores original variable names for labelling",
    "estimate: estimate of regression",
    "degree_of_freedom: degree of freedom of error in regression",
    "standard_error: standard error in regression",
    "t_stats: Treatment coefficient t-value - measures how many standard errors the estimate is away from zero.",
    "partial_f2: value to determine if a regression model and a nested version of it have a statistically significant difference between them",
    "r2tu_w: partial R^2  of unobserved confounder \"u\" with treatment \"t\", after conditioning on observed covariates \"w\"",
    "r2yu_tw: partial R^2  of unobserved confounder \"u\" with outcome \"y\", after conditioning on observed covariates \"w\" and treatment \"t\"",
    "r2twj_w: partial R^2 of observed covariate wj with treatment \"t\", after conditioning on observed covariates \"w\" excluding wj",
    "r2ywj_tw:  partial R^2 of observed covariate wj with outcome \"y\", after conditioning on observed covariates \"w\" (excluding wj) and treatment \"t\"",
    "benchmarking_results: dataframe containing information about bounds and bias adjusted terms",
    "stats: dictionary containing information like robustness value, partial R^2, estimate, standard error , degree of freedom, partial f^2, t-statistic",
    "partial R^2 (r2yt_w) is the proportion of variation in outcome uniquely explained by treatment",
    "build a new regression model by considering treatment variables as outcome",
    "r2twj_w is partial R^2 of covariate wj with treatment \"t\", after conditioning on covariates w(excluding wj)",
    "r2ywj_tw is partial R^2 of covariate wj with outcome \"y\", after conditioning on covariates w(excluding wj) and treatment \"t\"",
    "r2tu_w is the partial r^2 from regressing u on t after conditioning on w",
    "Compute bias adjusted terms",
    "Plotting the contour plot",
    "Adding contours",
    "Adding threshold contour line",
    "Adding unadjusted point estimate",
    "Adding bounds to partial R^2 values for given strength of confounders",
    "from https://www.bnmetrics.com/blog/factory-pattern-in-python3-simple-version",
    "The default subset of the data to be used",
    "Run refutation in parallel",
    "We want to see if the estimate falls in the same distribution as the one generated by the refuter",
    "Ideally that should be the case as choosing a subset should not have a significant effect on the ability",
    "of the treatment to affect the outcome",
    "from https://www.bnmetrics.com/blog/factory-pattern-in-python3-simple-version",
    "If the relative change of the score is less than the given threshold, we stop the estimation early.",
    "Note, the output of score_samples are log values.",
    "Note, the output of score_samples are log values.",
    "Currently only support continuous distributions for auto selection.",
    "Estimate distribution parameters from data.",
    "Ignore warnings from fitting process.",
    "Fit distribution to data.",
    "Some distributions might not be compatible with the data.",
    "Separate parts of parameters.",
    "Check the KL divergence between the distribution of the given and fitted distribution.",
    "Identify if this distribution is better.",
    "This error is typically raised when the data is discrete and all points are assigned to less cluster than",
    "specified. It can also happen due to duplicated points. In these cases, the current best solution should",
    "be sufficient.",
    "Usual feature relevance using the mean deviation as set function, i.e. g(x) - E[g(X)]",
    "A convenience function when computing confidence intervals specifically for non-deterministic causal queries. This",
    "function evaluates the provided causal query multiple times to build a confidence interval based on the returned",
    "results.",
    "Note that this function does not re-fit the causal model(s) and only executes the provided query as it is. In order",
    "to re-refit the graphical causal model on random subsets of the data before executing the query, consider using the",
    "fit_and_compute function.",
    "",
    "**Example usage:**",
    "",
    ">>> gcm.fit(causal_model, data)",
    ">>> strength_medians, strength_intervals = gcm.confidence_intervals(",
    ">>>     gcm.bootstrap_sampling(gcm.arrow_strength, causal_model, target_node='Y'))",
    "",
    "In this example, gcm.confidence_intervals is expecting a callable with non-deterministic outputs for building the",
    "confidence intervals. Since each causal query potentially expects a different set of parameters, we use 'partial'",
    "here to configure the function call. In this case,",
    "gcm.bootstrap_sampling(gcm.arrow_strength, causal_model, target_node='Y') would be equivalent to",
    "lambda : gcm.arrow_strength(causal_model, target_node='Y').",
    "",
    "In order to incorporate uncertainties coming from fitting the causal model(s), we can use",
    "gcm.fit_and_compute instead:",
    ">>>  strength_medians, strength_intervals = gcm.confidence_intervals(",
    ">>>        gcm.fit_and_compute(gcm.arrow_strength,",
    ">>>                                            causal_model,",
    ">>>                                            bootstrap_training_data=data,",
    ">>>                                            target_node='Y'))",
    "This would refit the provided causal_model on a subset of the data first before executing gcm.arrow_strength in each",
    "run.",
    "Simulating interventions by propagating the effects through the graph. For this, we iterate over the nodes based",
    "on their topological order.",
    "After drawing samples of the node based on the data generation process, we apply the corresponding",
    "intervention. The inputs of downstream nodes are therefore based on the outcome of the intervention in this",
    "node.",
    "Abduction: For invertible SCMs, we recover exact noise values from data.",
    "Action + Prediction: Propagate the intervention downstream using recovered noise values.",
    "Check if we need to apply an intervention on the given node.",
    "Apply intervention function to the data of the node.",
    "Check if the intervention function changes the shape of the data.",
    "For estimating the effect, we only need to consider the nodes that have a directed path to the target node, i.e.",
    "all ancestors of the target.",
    "The target node can be a continuous real-valued variable or a categorical variable with at most two classes",
    "(i.e. binary).",
    "Avoid too many features",
    "Making sure there are at least 30% test samples.",
    "Making sure that there are at least 2 samples from one class (here, simply duplicate the point).",
    "Compare number of correct classifications.",
    "This constant is used as key when storing/accessing models as causal mechanisms in graph node attributes",
    "This constant is used as key when storing the parents of a node during fitting. It's used for validation purposes",
    "afterwards.",
    "can't use nx.node_connected_component, because it doesn't work with DiGraphs.",
    "Hence a manual loop:",
    "For estimating Shapley values for multiple samples (e.g. in feature relevance) and the number of samples",
    "is unknown beforehand.",
    "The method stops if either the change between some consecutive runs is below the given threshold or the",
    "maximum number of runs is reached.",
    "In each run, we create one random permutation of players. For instance, given 4 players, a permutation",
    "could be [3,1,4,2].",
    "Create all subsets belonging to the generated permutation. This is, if we have [3,1,4,2], then the",
    "subsets are [3], [3,1], [3,1,4] [3,1,4,2].",
    "The result for each subset is cached such that if a subset that has already been evaluated appears again,",
    "we can take this result directly.",
    "To improve the runtime, multiple permutations are evaluated in each run.",
    "The current Shapley values are the average of the estimated values, i.e. we need to divide by the number",
    "of generated permutations here.",
    "Here, the change between two runs is below the minimum threshold, but to reduce the likelihood",
    "that this just happened by chance, we require that this happens at least for two runs in a row.",
    "Create all (unique) subsets)",
    "Assigning a 'high' weight, since this resembles \"infinity\".",
    "The weight for a subset with a specific length (see paper mentioned in the docstring for more",
    "information).",
    "TODO: Add method for auto select a bin_width/width based on the data. Make sure that the auto selection method is",
    "theoretically sound, i.e. make entropy results from different data comparable.",
    "Extremely small values can somehow result in negative values.",
    "Creating a smaller subgraph, which only contains upstream nodes that are connected to the target node.",
    "Sampling from the conditional distribution based on the current sample.",
    "Sampling from the conditional based on the current sample, but randomizing the inputs of all variables that",
    "are in the given subset. By this, we can simulate the impact on the conditional distribution when removing",
    "only the incoming edges of the variables in the subset.",
    "Creating a smaller subgraph, which only contains upstream nodes that are connected to the target node.",
    "In case of the full subset (no randomization), we get the same predictions as when we apply the",
    "prediction method to the samples of interest, since all noise samples are replaced with a sample of",
    "interest.",
    "In case of the empty subset (all are jointly randomize), it boils down to taking the average over all",
    "predictions, seeing that the randomization yields the same values for each sample of interest (none of the",
    "samples of interest are used to replace a (jointly) 'randomized' sample).",
    "Smallest possible value. This is used in various algorithm for numerical stability.",
    "Make copy to avoid manipulating the original matrix.",
    "The given prediction method has to be evaluated multiple times on a large amount of different inputs. Typically,",
    "the batch evaluation of a prediction model on multiple inputs at the same time is significantly faster",
    "than evaluating it on single simples in a for-loop. To make use of this, we try to evaluate as many samples as",
    "possible in one batch call of the prediction method. However, this also requires a lot of memory for many samples.",
    "To overcome potential memory issues, multiple batch calls are performed, each with at most batch_size many",
    "samples. The number of samples that are evaluated is normally",
    "baseline_noise_samples.shape[0] * feature_samples.shape[0]. Here, we reduce it to",
    "batch_size * feature_samples.shape[0]. If the batch_size would be set 1, then each baseline_noise_samples is",
    "evaluated one by one in a for-loop.",
    "Each batch consist of at most batch_size * feature_samples.shape[0] many samples. If there are multiple",
    "batches, the offset indicates the index of the current baseline_noise_samples that has not been evaluated yet.",
    "If the batch size would be larger than the remaining amount of samples, it is reduced to only include the",
    "remaining baseline_noise_samples.",
    "The inputs consist of batch_size many copies of feature_samples. Here, we set the columns of the features",
    "in baseline_feature_indices to their respective values in baseline_noise_samples.",
    "After creating the (potentially large) input data matrix, we can evaluate the prediction method.",
    "Here, offset + index now indicates the sample index in baseline_noise_samples.",
    "This would average all prediction results obtained for the 'offset + index'-th sample in",
    "baseline_noise_samples. This is, y^(offset + index) = E[Y | do(x^(offset + index)_s)].",
    "This would return all prediction results obtained for the 'offset + index'-th sample in",
    "baseline_noise_samples, i.e. the results are not averaged.",
    "Making copy to ensure that the original object is not modified.",
    "Permute samples jointly. This still represents an interventional distribution.",
    "Permute samples independently.",
    "test local Markov condition, null hypothesis: conditional independence",
    "test edge dependence, null hypothesis: independence",
    "The order of the p-values added to the list is deterministic.",
    "To be able to validate that the graph structure did not change between fitting and causal query, we store the",
    "parents of a node during fit. That way, before sampling, we can verify the parents are still the same. While",
    "this would automatically fail when the number of parents is different, there are other more subtle cases,",
    "where the number is still the same, but it's different parents, and therefore different data. That would yield",
    "wrong results, but would not fail.",
    "Wrapping labels if they are too long",
    "Using the negative value here seeing that the Shapley estimation evaluates v(S u {i}) - v(S) for a subset S. In",
    "case of variance, we have v(S u {i}) <= v(S), which would result in a negative contribution of players to the",
    "target quantity (here, variance).",
    "Using the negative value here seeing that the Shapley estimation evaluates v(S u {i}) - v(S) for a subset S. In",
    "case of variance, we have v(S u {i}) <= v(S), which would result in a negative contribution of players to the",
    "target quantity (here, variance).",
    "Calculate Ri, the product of the residuals",
    "Standard deviation of the residuals",
    "Either X and/or Y is constant.",
    "If Z is empty, we are in the pairwise setting.",
    "Either X and/or Y is constant.",
    "If Z is empty, we are in the pairwise setting.",
    "Estimate test statistic multiple times on different permutations of the data. The p-value is then the",
    "probability (i.e. fraction) of obtaining a test statistic that is greater than statistic on the non-permuted",
    "data.",
    "First stage statistical model",
    "Second stage statistical model",
    "Check if the treatment is one-dimensional",
    "First stage",
    "Second Stage",
    "Combining the two estimates",
    "This same estimate is valid for frontdoor as well as mediation (NIE)",
    "Bulding the feature matrix",
    "features = sm.add_constant(features, has_constant='add') # to add an intercept term",
    "TODO move this to the identification step",
    "Obtain estimate by Wald Estimator",
    "Obtain estimate by 2SLS estimator: Cov(y,z) / Cov(x,z)",
    "More than 1 instrument. Use 2sls.",
    "Checking if Y is binary",
    "Enable the user to pass params for a custom propensity model",
    "Convert the categorical variables into dummy/indicator variables",
    "Basically, this gives a one hot encoding for each category",
    "The first category is taken to be the base line.",
    "Check if the treatment is one-dimensional",
    "Checking if the treatment is binary",
    "The model is always built on the entire data",
    "TODO make treatment_value and control value also as local parameters",
    "All treatments are set to the same constant value",
    "Using all data by default",
    "Fixing treatment value to the specified value, if provided",
    "treatment_vals and data_df should have same number of rows",
    "Bulding the feature matrix",
    "The model is always built on the entire data",
    "Replacing treatment values by given x",
    "First, create interventional tensor in original space",
    "Then, use pandas to ensure that the dummies are assigned correctly for a categorical treatment",
    "The average treatment effect is a combination of different",
    "regression coefficients. Complicated to compute the confidence",
    "interval analytically. For example, if y=a + b1.t + b2.tx, then",
    "the average treatment effect is b1+b2.mean(x).",
    "Refer Gelman, Hill. ARM Book. Chapter 9",
    "http://www.stat.columbia.edu/~gelman/arm/chap9.pdf",
    "TODO: Looking for contributions",
    "For a linear regression model, the causal effect of a variable is equal to the coefficient corresponding to the",
    "variable. Hence, the model by default outputs the confidence interval corresponding to treatment=1 and control=0.",
    "So for custom treatment and control values, we must multiply the confidence interval by the difference of the two.",
    "For a linear regression model, the causal effect of a variable is equal to the coefficient corresponding to the",
    "variable. Hence, the model by default outputs the standard error corresponding to treatment=1 and control=0.",
    "So for custom treatment and control values, we must multiply the standard error by the difference of the two.",
    "setting method-specific parameters",
    "Infer the right strata based on clipping threshold",
    "0.5 because there are two values for the treatment",
    "To be conservative and allow most strata to be included in the",
    "analysis",
    "At least 90% of the strata should be included in analysis",
    "sum weighted outcomes over all strata  (weight by treated population)",
    "TODO - how can we add additional information into the returned estimate?",
    "such as how much clipping was done, or per-strata info for debugging?",
    "sort the dataframe by propensity score",
    "create a column 'strata' for each element that marks what strata it belongs to",
    "for each strata, count how many treated and control units there are",
    "throw away strata that have insufficient treatment or control",
    "TODO -- fix: we are actually conditioning on positive treatment (d=1)",
    "Setting method specific parameters",
    "trim propensity score weights",
    "ips ==> (isTreated(y)/ps(y)) + ((1-isTreated(y))/(1-ps(y)))",
    "nips ==> ips / (sum of ips over all units)",
    "icps ==> ps(y)/(1-ps(y)) / (sum of (ps(y)/(1-ps(y))) over all control units)",
    "itps ==> ps(y)/(1-ps(y)) / (sum of (ps(y)/(1-ps(y))) over all treatment units)",
    "Vanilla IPS estimator",
    "The Hajek estimator (or the self-normalized estimator)",
    "Stabilized weights (from Robins, Hernan, Brumback (2000))",
    "Paper: Marginal Structural Models and Causal Inference in Epidemiology",
    "Calculating the effect",
    "Subtracting the weighted means",
    "TODO - how can we add additional information into the returned estimate?",
    "TODO -- fix: we are actually conditioning on positive treatment (d=1)",
    "Save parameters for later refutter fitting",
    "Enforcing this ordering is necessary to feed through the propensity values from dataset",
    "For metalearners only--issue a warning if w contains variables not in x",
    "Override the effect_modifiers set in CausalEstimator.__init__()",
    "Also only update self._effect_modifiers, and create a copy of self._effect_modifier_names",
    "the latter can be used by other estimator methods later",
    "Instrumental variables names, if present",
    "choosing the instrumental variable to use",
    "Calling the econml estimator's fit method",
    "As of v0.9, econml has some kewyord only arguments",
    "from https://www.bnmetrics.com/blog/factory-pattern-in-python3-simple-version",
    "Changing shape to a list for a singleton value",
    "TODO -- fix: we are actually conditioning on positive treatment (d=1)",
    "For each unit, return the estimated effect of the treatment value",
    "that was actually applied to the unit",
    "this assumes a binary treatment regime",
    "TODO remove neighbors that are more than a given radius apart",
    "estimate ATT on treated by summing over difference between matched neighbors",
    "Now computing ATC",
    "TODO -- fix: we are actually conditioning on positive treatment (d=1)",
    "from https://www.bnmetrics.com/blog/factory-pattern-in-python3-simple-version",
    "Handle externally provided estimator classes",
    "allowed types of distance metric",
    "Dictionary of any user-provided params for the distance metric",
    "that will be passed to sklearn nearestneighbors",
    "Check if the treatment is one-dimensional",
    "Checking if the treatment is binary",
    "Convert the categorical variables into dummy/indicator variables",
    "Basically, this gives a one hot encoding for each category",
    "The first category is taken to be the base line.",
    "this assumes a binary treatment regime",
    "TODO remove neighbors that are more than a given radius apart",
    "estimate ATT on treated by summing over difference between matched neighbors",
    "Return indices in the original dataframe",
    "self.matched_indices_att[treated_df_index[i]] = control.iloc[indices[i]].index.tolist()",
    "Now computing ATC",
    "Return indices in the original dataframe",
    "Add the identification method used in the estimator",
    "Check the backdoor variables being used",
    "Add the observed confounders and one hot encode the categorical variables",
    "Get the data of the unobserved confounders",
    "One hot encode the data if they are categorical",
    "Check the instrumental variables involved",
    "Perform the same actions as the above",
    "Both the outcome and the treatment have to be 1D arrays according to the CausalML API",
    "We want to pass 'v0' rather than ['v0'] to prevent a shape mismatch",
    "For CATEs",
    "TODO we are conditioning on a postive treatment",
    "TODO create an expression corresponding to each estimator used",
    "Generating data with equal 0 and 1 (since ranks are uniformly distributed)",
    "Flipping some values",
    "Finding p-value using student T test",
    "Only consider edges have absolute edge weight > 0.01",
    "Modify graph such that it only contains bidirected edges",
    "Find c components by finding connected components on the undirected graph",
    "Understanding Neural Network weights",
    "Refer to this link:https://stackoverflow.com/questions/50937628/mlp-classifier-neurons-weights",
    "add weight column",
    "before weights are applied we count number rows in each category",
    "which is equivalent to summing over weight=1",
    "after weights are applied we need to sum over the given weights",
    "First, calculating mean differences by strata",
    "Second, without strata",
    "Third, concatenating them and plotting",
    "Setting estimator attribute for convenience",
    "Outcome is numeric",
    "Treatments are also numeric or binary",
    "Outcome is categorical",
    "Treatments are numeric or binary",
    "TODO: A common way to show all plots",
    "from https://www.bnmetrics.com/blog/factory-pattern-in-python3-simple-version",
    "Get adjacency list",
    "If node pair has been fully explored",
    "Add node1 to backdoor set of node_pair",
    "Check if path is backdoor and does not have nodes1\\node1 or nodes2\\node2 as intermediate nodes",
    "True if arrow incoming, False if arrow outgoing",
    "Mark pair (node1, node2) complete",
    "Modify variable count and indices covered",
    "Average total effect",
    "Natural direct effect",
    "Natural indirect effect",
    "Controlled direct effect",
    "Backdoor method names",
    "First, check if there is a directed path from action to outcome",
    "## 1. BACKDOOR IDENTIFICATION",
    "Pick algorithm to compute backdoor sets according to method chosen",
    "First, checking if there are any valid backdoor adjustment sets",
    "Setting default \"backdoor\" identification adjustment set",
    "## 2. INSTRUMENTAL VARIABLE IDENTIFICATION",
    "Now checking if there is also a valid iv estimand",
    "## 3. FRONTDOOR IDENTIFICATION",
    "Now checking if there is a valid frontdoor variable",
    "Finally returning the estimand object",
    "Pick algorithm to compute backdoor sets according to method chosen",
    "Setting default \"backdoor\" identification adjustment set",
    "Finally returning the estimand object",
    "## 1. FIRST DOING BACKDOOR IDENTIFICATION",
    "First, checking if there are any valid backdoor adjustment sets",
    "Setting default \"backdoor\" identification adjustment set",
    "## 2. SECOND, CHECKING FOR MEDIATORS",
    "Now checking if there are valid mediator variables",
    "Finally returning the estimand object",
    "## 1. FIRST DOING BACKDOOR IDENTIFICATION",
    "First, checking if there are any valid backdoor adjustment sets",
    "Setting default \"backdoor\" identification adjustment set",
    "## 2. SECOND, CHECKING FOR MEDIATORS",
    "Now checking if there are valid mediator variables",
    "Finally returning the estimand object",
    "First, checking if empty set is a valid backdoor set",
    "If the method is `minimal-adjustment`, return the empty set right away.",
    "Second, checking for all other sets of variables. If include_unobserved is false, then only observed variables are eligible.",
    "only remove descendants of Y",
    "also allow any causes of Y that are not caused by T (for lower variance)",
    "remove descendants of T (mediators) and descendants of Y",
    "If var is d-separated from both treatment or outcome, it cannot",
    "be a part of the backdoor set",
    "repeat the above search with BACKDOOR_MIN",
    "If `minimal-adjustment` method is specified, start the search from the set with minimum size. Otherwise, start from the largest.",
    "If the backdoor method is `maximal-adjustment` or `minimal-adjustment`, return the first found adjustment set.",
    "If all variables are observed, and the biggest eligible set",
    "does not satisfy backdoor, then none of its subsets will.",
    "Adding a None estimand if no backdoor set found",
    "Default set contains minimum possible number of instrumental variables, to prevent lowering variance in the treatment variable.",
    "Default set is the one with the least number of adjustment variables (optimizing for efficiency)",
    "For simplicity, assuming a one-variable frontdoor set",
    "Cond 1: All directed paths intercepted by candidate_var",
    "Cond 2: No confounding between treatment and candidate var",
    "Cond 3: treatment blocks all confounding between candidate_var and outcome",
    "For simplicity, assuming a one-variable mediation set",
    "Create estimands dict as per the API for backdoor, but do not return it",
    "Setting default \"backdoor\" identification adjustment set",
    "Create estimands dict as per the API for backdoor, but do not return it",
    "Setting default \"backdoor\" identification adjustment set",
    "TODO: outputs string for now, but ideally should do symbolic",
    "expressions Mon 19 Feb 2018 04:54:17 PM DST",
    "TODO Better support for multivariate treatments",
    "TODO: support multivariate treatments better.",
    "TODO: support multivariate treatments better.",
    "TODO: support multivariate treatments better.",
    "For direct effect",
    "If no costs are passed, use uniform costs",
    "restriction to ancestors",
    "back-door graph",
    "moralization",
    "Estimators list for returning after identification",
    "Line 1",
    "If no action has been taken, the effect on Y is just the marginal of the observational distribution P(v) on Y.",
    "Line 2",
    "If we are interested in the effect on Y, it is sufficient to restrict our attention on the parts of the model ancestral to Y.",
    "Modify list of valid nodes",
    "Line 3 - forces an action on any node where such an action would have no effect on Y \u2013 assuming we already acted on X.",
    "Modify adjacency matrix to obtain that corresponding to do(X)",
    "Line 4 - Decomposes the problem into a set of smaller problems using the key property of C-component factorization of causal models.",
    "If the entire graph is a single C-component already, further problem decomposition is impossible, and we must provide base cases.",
    "Modify adjacency matrix to remove treatment variables",
    "Line 5 - The algorithms fails due to the presence of a hedge - the graph G, and a subgraph S that does not contain any X nodes.",
    "Line 6 - If there are no bidirected arcs from X to the other nodes in the current subproblem under consideration, then we can replace acting on X by conditioning, and thus solve the subproblem.",
    "Line 7 - This is the most complicated case in the algorithm. Explain in the second last paragraph on Pg 41 of the link provided in the docstring above.",
    "Do not show backdoor key unless it is the only backdoor set.",
    "Just show the default backdoor set",
    "self._identified_estimand = self._causal_model.identify_effect()",
    "self._identified_estimand,",
    "self._causal_model._treatment,",
    "self._causal_model._outcome,",
    "If labels provided",
    "Return in valid DOT format",
    "Get adjacency matrix",
    "If labels not provided",
    "Obtain valid DOT format",
    "If labels provided",
    "Return in valid DOT format",
    "from https://www.bnmetrics.com/blog/factory-pattern-in-python3-simple-version",
    "from https://www.bnmetrics.com/blog/factory-pattern-in-python3-simple-version"
  ],
  "v0.9": [
    "-*- coding: utf-8 -*-",
    "",
    "Configuration file for the Sphinx documentation builder.",
    "",
    "This file does only contain a selection of the most common options. For a",
    "full list see the documentation:",
    "http://www.sphinx-doc.org/en/stable/config",
    "-- Path setup --------------------------------------------------------------",
    "If extensions (or modules to document with autodoc) are in another directory,",
    "add these directories to sys.path here. If the directory is relative to the",
    "documentation root, use os.path.abspath to make it absolute, like shown here.",
    "",
    "-- Project information -----------------------------------------------------",
    "Version Information (for version-switcher)",
    "-- General configuration ---------------------------------------------------",
    "If your documentation needs a minimal Sphinx version, state it here.",
    "",
    "needs_sphinx = '1.0'",
    "Add any Sphinx extension module names here, as strings. They can be",
    "extensions coming with Sphinx (named 'sphinx.ext.*') or your custom",
    "ones.",
    "sphinx-panels shouldn't add bootstrap css since the pydata-sphinx-theme",
    "already loads it",
    "Add any paths that contain templates here, relative to this directory.",
    "The suffix(es) of source filenames.",
    "You can specify multiple suffix as a list of string:",
    "",
    "source_suffix = ['.rst', '.md']",
    "The master toctree document.",
    "The language for content autogenerated by Sphinx. Refer to documentation",
    "for a list of supported languages.",
    "",
    "This is also used if you do content translation via gettext catalogs.",
    "Usually you set \"language\" from the command line for these cases.",
    "List of patterns, relative to source directory, that match files and",
    "directories to ignore when looking for source files.",
    "This pattern also affects html_static_path and html_extra_path .",
    "The name of the Pygments (syntax highlighting) style to use.",
    "-- Options for HTML output -------------------------------------------------",
    "The theme to use for HTML and HTML Help pages.  See the documentation for",
    "a list of builtin themes.",
    "",
    "Theme options are theme-specific and customize the look and feel of a theme",
    "further.  For a list of options available for each theme, see the",
    "documentation.",
    "",
    "Add any paths that contain custom static files (such as style sheets) here,",
    "relative to this directory. They are copied after the builtin static files,",
    "so a file named \"default.css\" will overwrite the builtin \"default.css\".",
    "Custom sidebar templates, must be a dictionary that maps document names",
    "to template names.",
    "",
    "The default sidebars (for documents that don't match any pattern) are",
    "defined by theme itself.  Builtin themes are using these templates by",
    "default: ``['localtoc.html', 'relations.html', 'sourcelink.html',",
    "'searchbox.html']``.",
    "",
    "html_sidebars = {}",
    "-- Options for HTMLHelp output ---------------------------------------------",
    "Output file base name for HTML help builder.",
    "-- Options for LaTeX output ------------------------------------------------",
    "The paper size ('letterpaper' or 'a4paper').",
    "",
    "'papersize': 'letterpaper',",
    "The font size ('10pt', '11pt' or '12pt').",
    "",
    "'pointsize': '10pt',",
    "Additional stuff for the LaTeX preamble.",
    "",
    "'preamble': '',",
    "Latex figure (float) alignment",
    "",
    "'figure_align': 'htbp',",
    "Grouping the document tree into LaTeX files. List of tuples",
    "(source start file, target name, title,",
    "author, documentclass [howto, manual, or own class]).",
    "-- Options for manual page output ------------------------------------------",
    "One entry per manual page. List of tuples",
    "(source start file, name, description, authors, manual section).",
    "-- Options for Texinfo output ----------------------------------------------",
    "Grouping the document tree into Texinfo files. List of tuples",
    "(source start file, target name, title, author,",
    "dir menu entry, description, category)",
    "-- Options for Epub output -------------------------------------------------",
    "Bibliographic Dublin Core info.",
    "The unique identifier of the text. This can be a ISBN number",
    "or the project homepage.",
    "",
    "epub_identifier = ''",
    "A unique identification for the text.",
    "",
    "epub_uid = ''",
    "A list of files that should not be packed into the epub file.",
    "-- Extension configuration -------------------------------------------------",
    "-- Options for todo extension ----------------------------------------------",
    "If true, `todo` and `todoList` produce output, else they produce nothing.",
    "init docstrings should also be included in class",
    "Patch all of the published versions",
    "check old RST version (<= v0.8)",
    "Remove old version links",
    "Append updated version links",
    "requires stdin input for identify in weighting sampler",
    "requires Rpy2 for lalonde",
    "will be removed",
    "applied notebook, not necessary to test each time",
    "needs xgboost too",
    "Slow Notebooks",
    "requires Rpy2 for causal discovery",
    "daily tests of dowhy_causal_discovery_example.ipynb are failing due to cdt/rpy2 config.",
    "comment out, since we are switching causal discovery implementations",
    "Adding the dowhy root folder to the python path so that jupyter notebooks",
    "can import dowhy",
    "\"--ExecutePreprocessor.timeout=600\",",
    "self.df = pd.read_csv(os.path.join(DATA_PATH,'dgp_1/acic_1_1_data.csv'))",
    "self.ate = np.mean(self.df['y1'] - self.df['y0'])",
    "treated = self.df[self.df['z']==1]",
    "self.att = np.mean(treated['y1'] - treated['y0'])",
    "def test_average_treatment_effect(self):",
    "est_ate = 1",
    "bias = est_ate - self.ate",
    "print(bias)",
    "self.assertAlmostEqual(self.ate, est_ate)",
    "def test_average_treatment_effect_on_treated(self):",
    "est_att = 1",
    "self.att=1",
    "bias = est_att - self.att",
    "print(bias)",
    "self.assertAlmostEqual(self.att, est_att)",
    "removing two common causes",
    "removing two common causes",
    "removing two common causes",
    "removing two common causes",
    "check if all partial R^2 values are between 0 and 1",
    "We calculate adjusted estimates for two sets of partial R^2 values.",
    "Test if hypothetical confounding by unobserved confounder u1 leads to an adjusted effect that is farther from the original estimate as compared to u2",
    "Creating a model with no unobserved confounders",
    "check if all partial R^2 values are between 0 and 1",
    "for a dataset with no confounders, the robustness value should be higher than a given threshold (0.95 in our case)",
    "Non Parametric estimator",
    "We calculate adjusted estimates for two sets of partial R^2 values.",
    "adjusted lower ate bound for confounder u1 where r2tu_w = 0.7 and r2yu_tw = 0.9",
    "adjusted lower ate bound for confounder u2 where r2tu_w = 0.2 and r2yu_tw = 0.3",
    "adjusted upper ate bound for confounder u1 where r2tu_w = 0.7 and r2yu_tw = 0.9",
    "adjusted upper ate bound for confounder u2 where r2tu_w = 0.2 and r2yu_tw = 0.3",
    "adjusted lower confidence bound for confounder u1 where r2tu_w = 0.7 and r2yu_tw = 0.9",
    "adjusted lower confidence bound for confounder u2 where r2tu_w = 0.2 and r2yu_tw = 0.3",
    "adjusted upper confidence bound for confounder u1 where r2tu_w = 0.7 and r2yu_tw = 0.9",
    "adjusted upper confidence bound for confounder u2 where r2tu_w = 0.2 and r2yu_tw = 0.3",
    "Test if hypothetical confounding by unobserved confounder u1 leads to an adjusted effect that is farther from the original estimate as compared to u2",
    "we patched figure plotting call to avoid drawing plots during tests",
    "We calculate adjusted estimates for two sets of partial R^2 values.",
    "adjusted lower ate bound for confounder u1 where r2tu_w = 0.7 and r2yu_tw = 0.9",
    "adjusted lower ate bound for confounder u2 where r2tu_w = 0.2 and r2yu_tw = 0.3",
    "adjusted upper ate bound for confounder u1 where r2tu_w = 0.7 and r2yu_tw = 0.9",
    "adjusted upper ate bound for confounder u2 where r2tu_w = 0.2 and r2yu_tw = 0.3",
    "adjusted lower confidence bound for confounder u1 where r2tu_w = 0.7 and r2yu_tw = 0.9",
    "adjusted lower confidence bound for confounder u2 where r2tu_w = 0.2 and r2yu_tw = 0.3",
    "adjusted upper confidence bound for confounder u1 where r2tu_w = 0.7 and r2yu_tw = 0.9",
    "adjusted upper confidence bound for confounder u2 where r2tu_w = 0.2 and r2yu_tw = 0.3",
    "Test if hypothetical confounding by unobserved confounder u1 leads to an adjusted effect that is farther from the original estimate as compared to u2",
    "we patched figure plotting call to avoid drawing plots during tests",
    "comparing test examples from R E-Value package",
    "check implementation of Observed Covariate E-value against R package",
    "The outcome is a linear function of the confounder",
    "The slope is 1,2 and the intercept is 3",
    "As we run with only one common cause and one instrument variable we run with (?, 2)",
    "Supports user-provided dataset object",
    "To test if there are any exceptions",
    "To test if the estimate is identical if refutation parameters are zero",
    "This value is hardcoded to be zero as we are runnning this on a linear dataset.",
    "Ordinarily, we should expect this value to be zero.",
    "This value is hardcoded to be zero as we are runnning this on a linear dataset.",
    "Ordinarily, we should expect this value to be zero.",
    "Calculate causal effect twice: once for unit (t=1, c=0), once for specific increase (t=100, c=50)",
    "Only P(Y|T) should be present for test to succeed.",
    "Calculate causal effect twice: once for unit (t=1, c=0), once for specific increase (t=100, c=50)",
    "Since undirected graph, identify effect must throw an error.",
    "Calculate causal effect twice: once for unit (t=1, c=0), once for specific increase (t=100, c=50)",
    "Compare with ground truth",
    "Calculate causal effect twice: once for unit (t=1, c=0), once for specific increase (t=100, c=50)",
    "Compare with ground truth",
    "Calculate causal effect twice: once for unit (t=1, c=0), once for specific increase (t=100, c=50)",
    "Compare with ground truth",
    "Calculate causal effect twice: once for unit (t=1, c=0), once for specific increase (t=100, c=50)",
    "Compare with ground truth",
    "Causal model initialization",
    "Obtain backdoor sets",
    "Check if backdoor sets are valid i.e. if they block all paths between the treatment and the outcome",
    "Causal model initialization",
    "Obtain backdoor sets",
    "Causal model initialization",
    "Obtain backdoor sets",
    "Check if backdoor sets are valid i.e. if they block all paths between the treatment and the outcome",
    "Causal model initialization",
    "Obtain backdoor sets",
    "Check if backdoor sets are valid i.e. if they block all paths between the treatment and the outcome",
    "Causal model initialization",
    "Obtain backdoor sets",
    "Check if backdoor sets are valid i.e. if they block all paths between the treatment and the outcome",
    "Causal model initialization",
    "Obtain backdoor sets",
    "Check if backdoor sets are valid i.e. if they block all paths between the treatment and the outcome",
    "Example is selected from Pearl J. \"Causality\" 2nd Edition, from chapter 3.3.1 on backoor criterion.",
    "The following simpsons paradox examples are taken from Pearl, J {2013}. \"Understanding Simpson\u2019s Paradox\" - http://ftp.cs.ucla.edu/pub/stat_ser/r414.pdf",
    "The following are examples given in the \"Book of Why\" by Judea Pearl, chapter \"The Do-operator and the Back-Door Criterion\"",
    "For all examples from these papers we use X for the treatment variable",
    "instead of A.",
    "Figure 6 from Smucler, Sapienza and Rotnitzky (2021), Biometrika",
    "Figure 5 from Smucler, Sapienza and Rotnitzky (2021), Biometrika",
    "Figure 4 from Smucler, Sapienza and Rotnitzky (2021), Biometrika",
    "Figure 3 from Smucler, Sapienza and Rotnitzky (2021), Biometrika",
    "Figure 2 from Smucler and Rotnitzky (2022), Journal of Causal Inference",
    "L replaces X as the conditional variable",
    "Figure 2 from Smucler and Rotnitzky (2022), Journal of Causal Inference",
    "L replaces X as the conditional variable. Uses different costs",
    "Figure 3 from Smucler and Rotnitzky (2022), Journal of Causal Inference",
    "A graph where optimal, optimal minimal and optimal min cost are different",
    "The graph from Shrier and Platt (2008)",
    "A graph for which the algorithm was producing wrong result due to a bug reported by Sara Taheri",
    "Another graph for which the algorithm was producing wrong result due to a bug reported by Sara Taheri",
    "cov_mat = np.diag(np.ones(num_features))",
    "collider: X->Z<-Y",
    "chain: X->Z->Y",
    "fork: X<-Z->Y",
    "general DAG: X<-Z->Y, X->Y",
    "fork: X<-Z->Y",
    "Contributions should add up to Var(X2)",
    "Contributions should add up to Var(X2)",
    "H(P(Y)) -- Can be precomputed",
    "-(H(P(Y | do(x_S)) - H(P(Y))) = H(P(Y)) - H(P(Y | do(x_S))",
    "H(P(Y | do(x_S)) = H(E[P(Y | x_S, X'_\\S)])",
    "E[P(Y | x_S, X'_\\S)]",
    "H(E[P(Y | x_S, X'_\\S)])",
    "Using H(P(Y)) based on the origina data, i.e. ignoring baseline_predictions.",
    "E[H(P(Y)) - H(P(Y | do(X_U))] = H(P(Y)) - E[H(P(Y | X))]",
    "Just checking formats, i.e. no need for correlation.",
    "Just checking formats, i.e. no need for correlation.",
    "Mean from the categorical part is: (-5 + 5+ 10) / 3 = 10/3",
    "C2 = 3 * A2 + 2 * B2",
    "By default, the strength is measure with respect to the variance.",
    "Here, we misspecified the mechanism on purpose by setting scale to 1 instead of 2.",
    "If we provide the observational data here, we can mitigate the misspecification of the causal mechanism.",
    "Here, changing the mechanism.",
    "Here, changing the mechanism.",
    "Here, changing the mechanism.",
    "Here, changing the mechanism.",
    "Defining an anomaly scorer that handles multidimensional inputs.",
    "Seeing that the expectation of the noise in all nodes is 0, we introduce anomalies by setting some of them to 3.",
    "In the first sample, only the first variable is anomalous. Therefore, it should have the highest contribution",
    "and it should be \"significantly\" higher than the contribution of the other ones (here, we just arbitrarily say",
    "it should be 3x higher. Due to the confounding factor Z, the reconstructed noise variables are pairwise dependent,",
    "which is a violation of our causal sufficiency assumption. However, a confounder is included here to demonstrate",
    "some robustness. Note that due to this and stochastic behaviour of the density estimator, it is",
    "not possible to analytically compute expected results. Therefore, we rather look at the relations here.",
    "Same idea for the second sample, but here, it is the second variable that is anomalous.",
    "In the fourth sample, there are 2 anomalous variables. Therefore, the contribution of these 2 variables should be",
    "\"significantly\" higher than the contribution of the other variables. The contribution of both anomalous variables",
    "should be equal (approximately).",
    "Defining an anomaly scorer that handles multidimensional inputs.",
    "Seeing that the expectation of the noise in all nodes is 0, we introduce anomalies by setting some of them to 3.",
    "reduce the score.",
    "The contributions should add up to g(x) - E[g(X)]",
    "The contributions should add up to g(x) - E[g(X)]",
    "The contributions should add up to g(x) - E[g(X)]",
    "Three examples:",
    "1. X1 is the root cause (+ 10 to the noise)",
    "2. X0 is the root cause (+ 10 to the noise)",
    "3. X0 and X3 are both root causes (+ 10 to both noises)",
    "The sum of the scores should add up to the anomaly score of the target (here, X3).",
    "Three examples:",
    "1. X1 is the root cause (+ 10 to the noise)",
    "2. X0 is the root cause (+ 10 to the noise)",
    "3. X0 and X3 are both root causes (+ 10 to both noises)",
    "The sum of the scores should add up to the anomaly score of the target (here, X3).",
    "Defining ground truth model to avoid SCM learning issues and, hence, to focus on the anomaly attribution",
    "algorithm.",
    "1. X0 is the root cause (+ 10 to the noise)",
    "2. X0 and X1 are the root causes (+ 10 to both noise)",
    "3. X2 and X3 are both root causes (+ 10 to both noises)",
    "The sum of the scores should add up to the anomaly score of the target (here, X3).",
    "Check if calling the method causes some import or runtime errors",
    "TODO: Plotting undirected graphs with networkx causes an error when an older networkx version is used with a newer",
    "newer matplotlib version:",
    "AttributeError: module 'matplotlib.cbook' has no attribute 'is_numlike'",
    "Networkx 2.4+ should fix this issue.",
    "plot_adjacency_matrix(causal_graph, is_directed=False)",
    "Setup data",
    "Test LinearDML",
    "Test ContinuousTreatmentOrthoForest",
    "Test LinearDRLearner",
    "Setup data",
    "Test DeepIV",
    "Treatment model,",
    "Response model",
    "Test IntentToTreatDRIV",
    "Observed data",
    "assumed graph",
    "Identify effect",
    "Estimate effect",
    "Not using testsuite from .base/TestEstimtor, custom code below",
    "More cases where Exception  is expected",
    "Compute confidence intervals, standard error and significance tests",
    "Defined a linear dataset with a given set of properties",
    "Create a model that captures the same",
    "Identify the effects within the model",
    "Defined a linear dataset with a given set of properties",
    "Create a model that captures the same",
    "Identify the effects within the model",
    "Defined a linear dataset with a given set of properties",
    "Create a model that captures the same",
    "Identify the effects within the model",
    "Defined a linear dataset with a given set of properties",
    "Create a model that captures the same",
    "Identify the effects within the model",
    "Defined a linear dataset with a given set of properties",
    "Create a model that captures the same",
    "Identify the effects within the model",
    "Can use these lists to specify the models/estimators/refuters that a particular interpreter supports.  Throw a ValueError if the user provides an incompatible object to intepret.",
    "Unpacking the keyword arguments",
    "todo: add docstring for common parameters here and remove from child refuter classes",
    "Default value for the number of simulations to be conducted",
    "joblib params for parallel processing",
    "Concatenate the confounders, instruments and effect modifiers",
    "Shuffle the confounders",
    "Check if all are select or deselect variables",
    "Check if all the required_variables belong to confounders, instrumental variables or effect",
    "Get the number of simulations",
    "Sort the simulations",
    "Obtain the median value",
    "Performing a two sided test",
    "np.searchsorted tells us the index if it were a part of the array",
    "We select side to be left as we want to find the first value that matches",
    "We subtact 1 as we are finding the value from the right tail",
    "We take the side to be right as we want to find the last index that matches",
    "We get the probability with respect to the left tail.",
    "return twice the determined quantile as this is a two sided test",
    "Get the mean for the simulations",
    "Get the standard deviation for the simulations",
    "Get the Z Score [(val - mean)/ std_dev ]",
    "Initializing the p_value",
    "Perform Bootstrap Significance Test with the original estimate and the set of refutations",
    "Perform Normal Tests of Significance with the original estimate and the set of refutations",
    "Perform Bootstrap Significance Test with the original estimate and the set of refutations",
    "Perform Normal Tests of Significance with the original estimate and the set of refutations",
    "re.sub only takes string parameter so the first if is to avoid error",
    "if the input is a text file, convert the contained data into string",
    "load dot file",
    "Adding node attributes",
    "adding penwidth to make the edge bold",
    "Adding common causes",
    "Adding instruments",
    "Adding effect modifiers",
    "Assuming the simple form of effect modifier",
    "that directly causes the outcome.",
    "self._graph.add_edge(node_name, outcome, style = \"dotted\", headport=\"s\", tailport=\"n\")",
    "self._graph.add_edge(outcome, node_name, style = \"dotted\", headport=\"n\", tailport=\"s\") # TODO make the ports more general so that they apply not just to top-bottom node configurations",
    "Adding columns in the dataframe as confounders that were not in the graph",
    "Adding unobserved confounders",
    "removal of only direct edges wrt a target is not implemented for incoming edges",
    "also return the number of backdoor paths blocked by observed nodes",
    "Assume that nodes1 is the treatment",
    "ignores new_graph parameter, always uses self._graph",
    "remove paths that have nodes1\\node1 or nodes2\\node2 as intermediate nodes",
    "TODO Refactor to remove this from here and only implement this logic in causalIdentifier. Unnecessary assumption of nodes1 to be causing nodes2.",
    "Cannot simply compute ancestors, since that will also include nodes1 and its parents (e.g. instruments)",
    "removing all mediators",
    "[TODO: double check these work with multivariate implementation:]",
    "Exclusion",
    "As-if-random setup",
    "As-if-random",
    "convert the outputted generator into a list",
    "dpaths = self.get_all_directed_paths(nodes1, nodes2)",
    "return len(dpaths) > 0",
    "Condition 1: node 1 ---> node 2 is intercepted by candidate_nodes",
    "Create causal graph object",
    "Sometimes, effect modifiers from the graph may not match those provided by the user.",
    "(Because some effect modifiers may also be common causes)",
    "In such cases, the user-provided modifiers are used.",
    "If no effect modifiers are provided,  then the ones from the graph are used.",
    "Import causal discovery class",
    "Initialize causal graph object",
    "TODO add propensity score as default backdoor method, iv as default iv method, add an informational message to show which method has been selected.",
    "TODO add dowhy as a prefix to all dowhy estimators",
    "This is done as all dowhy estimators have two parts and external ones have two or more parts",
    "Define the third-party estimation method to be used",
    "Process the dowhy estimators",
    "Check if estimator's target estimand is identified",
    "Note that while the name of the variable is the same,",
    "\"self.causal_estimator\", this estimator takes in less",
    "parameters than the same from the",
    "estimate_effect code. It is not advisable to use the",
    "estimator from this function to call estimate_effect",
    "with fit_estimator=False.",
    "Estimator had been computed in a previous call",
    "The default number of simulations for statistical testing",
    "The default number of simulations to obtain confidence intervals",
    "The portion of the total size that should be taken each time to find the confidence intervals",
    "1 is the recommended value",
    "https://ocw.mit.edu/courses/mathematics/18-05-introduction-to-probability-and-statistics-spring-2014/readings/MIT18_05S14_Reading24.pdf",
    "https://projecteuclid.org/download/pdf_1/euclid.ss/1032280214",
    "The default Confidence Level",
    "Number of quantiles to discretize continuous columns, for applying groupby",
    "Prefix to add to temporary categorical variables created after discretization",
    "std args to be removed from locals() before being passed to args_dict",
    "Setting the default interpret method",
    "Check if some parameters were set, otherwise set to default values",
    "Estimate conditional estimates by default",
    "TODO Only works for binary treatment",
    "Defaulting to class default values if parameters are not provided",
    "Checking that there is at least one effect modifier",
    "Making sure that effect_modifier_names is a list",
    "Making a copy since we are going to be changing effect modifier names",
    "For every numeric effect modifier, adding a temp categorical column",
    "Grouping by effect modifiers and computing effect separately",
    "Deleting the temporary categorical columns",
    "The array that stores the results of all estimations",
    "Find the sample size the proportion with the population size",
    "Perform the set number of simulations",
    "names of treatment and outcome",
    "Using class default parameters if not specified",
    "Checking if bootstrap_estimates are already computed",
    "Checked if any parameter is changed from the previous std error estimate",
    "Now use the data obtained from the simulations to get the value of the confidence estimates",
    "Get the variations of each bootstrap estimate and sort",
    "Now we take the (1- p)th and the (p)th variations, where p is the chosen confidence level",
    "Get the lower and upper bounds by subtracting the variations from the estimate",
    "Use existing params, if new user defined params are not present",
    "Checking if bootstrap_estimates are already computed",
    "Check if any parameter is changed from the previous std error estimate",
    "Use existing params, if new user defined params are not present",
    "self._outcome = self._data[\"dummy_outcome\"]",
    "Processing the null hypothesis estimates",
    "Doing a two-sided test",
    "Being conservative with the p-value reported",
    "Being conservative with the p-value reported",
    "If the estimate_index is 0, it depends on the number of simulations",
    "Need to test r-squared before supporting",
    "effect_r_squared = self._evaluate_effect_strength(estimate, method=\"r-squared\")",
    "'r-squared': effect_r_squared",
    "elif method == \"r-squared\":",
    "outcome_mean = np.mean(self._outcome)",
    "total_variance = np.sum(np.square(self._outcome - outcome_mean))",
    "Assuming a linear model with one variable: the treatment",
    "Currently only works for continuous y",
    "causal_model = outcome_mean + estimate.value*self._treatment",
    "squared_residual = np.sum(np.square(self._outcome - causal_model))",
    "r_squared = 1 - (squared_residual/total_variance)",
    "return r_squared",
    "Check if estimator's target estimand is identified",
    "Store parameters inside estimate object for refutation methods",
    "TODO: This add_params needs to move to the estimator class",
    "inside estimate_effect and estimate_conditional_effect",
    "No estimand was identified (identification failed)",
    "s += \"Variance in outcome explained by treatment: {}\\n\".format(self.effect_strength[\"r-squared\"])",
    "Below loop assumes that the last indices of W are alwawys converted to discrete",
    "one-hot encode discrete W",
    "Now deleting the old continuous value",
    "create a dataset with 10 observations one binary treatment and a continuous outcome affected by one common cause",
    "Two continuous treatments, no common cause, an instrumental variable and two effect modifiers - linearly added appropriately",
    "One Hot Encoding",
    "Making beta an array",
    "TODO Ensure that we do not generate weak instruments",
    "TODO - test all our methods with random noise added to covariates (instead of the stochastic treatment assignment)",
    "Converting treatment to binary if required",
    "Generating frontdoor variables if asked for",
    "NOTE: We are assuming a linear relationship *even when t is categorical* and integer coded.",
    "For categorical t, this example dataset has the effect size for category 2 being exactly",
    "double the effect for category 1",
    "This could be changed at this stage by one-hot encoding t and using a custom beta that",
    "sets a different effect for each category {0, 1, 2}",
    "Computing ATE",
    "constructing column names for one-hot encoded discrete features",
    "Specifying the correct dtypes",
    "Now specifying the corresponding graph strings",
    "Now writing the gml graph",
    "Making beta an array",
    "creating data frame",
    "Specifying the correct dtypes",
    "Now specifying the corresponding graph strings",
    "Now writing the gml graph",
    "Adding edges between common causes and the frontdoor mediator",
    "Error terms",
    "else:",
    "V = 6 + W0 + tterm + E1",
    "Y = 12 + W0*W0 + W0*W0 + yterm + E2  # E2_new",
    "Generating a random normal distribution of integers",
    "Generating data for nodes which have no incoming edges",
    "\"currset\" variable currently has all the successors of the nodes which had no incoming edges",
    "Making beta an array",
    "assuming that all unobserved common causes are numerical and are not affected by one hot encoding",
    "Creating a NN to simulate the nuisance function",
    "strength of unobserved confounding",
    "Computing ATE",
    "Specifying the correct dtypes",
    "Now writing the gml graph",
    "The following code for loading the Lalonde dataset was copied from",
    "https://github.com/wayfair/pylift/blob/5afc9088e96f25672423663f5c9b4bb889b4dfc0/examples/Lalonde/Lalonde_sample.ipynb?short_path=b1d451f#L94-L99).",
    "",
    "Copyright 2018, Wayfair, Inc.",
    "",
    "Redistribution and use in source and binary forms, with or without modification, are permitted provided that",
    "the following conditions are met:",
    "",
    "1. Redistributions of source code must retain the above copyright notice, this list of conditions and the",
    "following disclaimer.",
    "",
    "2. Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the",
    "following disclaimer in the documentation and/or other materials provided with the distribution.",
    "",
    "THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND ANY EXPRESS OR IMPLIED",
    "WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A",
    "PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY",
    "DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,",
    "PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER",
    "CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR",
    "OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH",
    "DAMAGE.",
    "",
    "0.0.0 is standard placeholder for poetry-dynamic-versioning",
    "any changes to this should not be checked in",
    "",
    "The currently supported estimators",
    "The default standard deviation for noise",
    "The default scaling factor to determine the bucket size",
    "The minimum number of points for the estimator to run",
    "The Default Transformation, when no arguments are given, or if the number of data points are insufficient for an estimator",
    "The Default True Causal Effect, this is taken to be ZERO by default",
    "The Default split for the number of data points that fall into the training and validation sets",
    "Assuming that outcome is one-dimensional",
    "We need to change the identified estimand",
    "We thus, make a copy. This is done as we don't want",
    "to change the original DataFrame",
    "We use collections.OrderedDict to maintain the order in which the data is stored",
    "Check if we are using an estimator in the transformation list",
    "The rationale behind ordering of the loops is the fact that we induce randomness everytime we create the",
    "Train and the Validation Datasets. Thus, we run the simulation loop followed by the training and the validation",
    "loops. Thus, we can get different values everytime we get the estimator.",
    "for _ in range( self._num_simulations ):",
    "Warn the user that the specified parameter is not applicable when no estimator is present in the transformation",
    "Adding an unobserved confounder if provided by the user",
    "We set X_train = 0 and outcome_train to be 0",
    "Get the final outcome, after running through all the values in the transformation list",
    "Check if the value of true effect has been already stored",
    "We use None as the key as we have no base category for this refutation",
    "As we currently support only one treatment",
    "If the number of data points is too few, run the default transformation: [(\"zero\",\"\"),(\"noise\", {'std_dev':1} )]",
    "Check if the value of true effect has been already stored",
    "This ensures that we calculate the causal effect only once.",
    "We use key_train as we map data with respect to the base category of the data",
    "As we currently support only one treatment",
    "Add h(t) to f(W) to get the dummy outcome",
    "We convert to ndarray for ease in indexing",
    "The data is of the form",
    "sim1: cat1 cat2 ... catn",
    "sim2: cat1 cat2 ... catn",
    "Note: We would like the causal_estimator to find the true causal estimate that we have specified through this",
    "refuter. Let the value of the true causal effect be h(t). In the following section of code, we wish to find out if h(t) falls in the",
    "distribution of the refuter.",
    "True Causal Effect list",
    "Iterating through the refutation for each category",
    "We use string arguments to account for both 32 and 64 bit varaibles",
    "action for continuous variables",
    "Action for categorical variables",
    "Find the set difference for each row",
    "Choose one out of the remaining",
    "We want to see if the estimate falls in the same distribution as the one generated by the refuter",
    "Ideally that should be the case as running bootstrap should not have a significant effect on the ability",
    "of the treatment to affect the outcome",
    "Standardizing the data",
    "Fit a model containing all confounders and compare predictions",
    "using all features compared to all features except a given",
    "confounder.",
    "Estimating the regression coefficient from standardized features to t",
    "By default, return a plot with 10 points",
    "consider 10 values of the effect of the unobserved confounder",
    "Standardizing the data",
    "Fit a model containing all confounders and compare predictions",
    "using all features compared to all features except a given",
    "confounder.",
    "By default, return a plot with 10 points",
    "consider 10 values of the effect of the unobserved confounder",
    "By default, we add the effect of simulated confounder for treatment.",
    "But subtract it from outcome to create a negative correlation",
    "assuming that the original confounder's effect was positive on both.",
    "This is to remove the effect of the original confounder.",
    "By default, we add the effect of simulated confounder for treatment.",
    "But subtract it from outcome to create a negative correlation",
    "assuming that the original confounder's effect was positive on both.",
    "This is to remove the effect of the original confounder.",
    "Obtaining the list of observed variables",
    "Taking a subset of the dataframe that has only observed variables",
    "Residuals from the outcome model obtained by fitting a linear model",
    "Residuals from the treatment model obtained by fitting a linear model",
    "Initialising product_cor_metric_observed with a really low value as finding maximum",
    "The user has an option to give the the effect_strength_on_y and effect_strength_on_t which can be then used instead of maximum correlation with treatment and outcome in the observed variables as it specifies the desired effect.",
    "Choosing a c_star based on the data.",
    "The correlations stop increasing upon increasing c_star after a certain value, that is it plateaus and we choose the value of c_star to be the value it plateaus.",
    "Choosing c1 and c2 based on the hyperbolic relationship once c_star is chosen by going over various combinations of c1 and c2 values and choosing the combination which",
    "which maintains the minimum distance between the product of correlations of the simulated variable and the product of maximum correlations of one of the observed variables",
    "and additionally checks if the ratio of the weights are such that they maintain the ratio of the maximum possible observed coefficients within some confidence interval",
    "c1_final and c2_final are initialised to the values on the hyperbolic curve such that c1_final = c2_final  and c1_final*c2_final = c_star",
    "initialising min_distance_between_product_cor_metrics to be a value greater than 1",
    "If the estimator used is LinearDML, partially linear sensitivity analysis will be automatically chosen",
    "Get a 2D matrix of values",
    "x,y =  np.meshgrid(self.kappa_t, self.kappa_y) # x,y are both MxN",
    "Store the values into the refute object",
    "Adding a label on the contour line for the original estimate",
    "Label every other level using strings",
    "Default value of the p value taken for the distribution",
    "Number of Trials: Number of cointosses to understand if a sample gets the treatment",
    "Mean of the Normal Distribution",
    "Standard Deviation of the Normal Distribution",
    "Create a new column in the data by the name of placebo",
    "Sanity check the data",
    "only permute is supported for iv methods",
    "For IV methods, the estimating_instrument_names should also be",
    "changed. Create a copy to avoid modifying original object",
    "We need to change the identified estimand",
    "We make a copy as a safety measure, we don't want to change the",
    "original DataFrame",
    "Run refutation in parallel",
    "Note: We hardcode the estimate value to ZERO as we want to check if it falls in the distribution of the refuter",
    "Ideally we should expect that ZERO should fall in the distribution of the effect estimates as we have severed any causal",
    "relationship between the treatment and the outcome.",
    "new estimator",
    "new effect estimate",
    "observed covariate E-value",
    "see Table 2 and p.37 in https://dash.harvard.edu/bitstream/handle/1/36874927/EValue_FinalSubmission.pdf",
    "if CI crosses null, set its E-value to 1",
    "only report E-value for CI limit closer to null",
    "see Table 2 and p.37 in https://dash.harvard.edu/bitstream/handle/1/36874927/EValue_FinalSubmission.pdf",
    "whether the DGP is assumed to be partially linear",
    "features are the observed confounders",
    "Now code for benchmarking using covariates begins",
    "R^2 of outcome with observed common causes and treatment",
    "R^2 of treatment with observed common causes",
    "Partial R^2 of outcome after regressing over unobserved confounder, observed common causes and treatment",
    "Assuming that the difference in R2 is the same for wj and new unobserved confounder",
    "for treatment,  Calpha is not a function of the partial R2. So we need a different assumption.",
    "Assuming that the ratio of variance of alpha^2 is the same for wj and new unobserved confounder",
    "(1-ratio_var_alpha_wj) is the numerator of Calpha2, similar to the partial R2 for treatment",
    "wrt unobserved confounders in partial-linear models",
    "whether the DGP is assumed to be partially linear",
    "can change this to allow default values that are same as the other parameter",
    "Strength of confounding that omitted variables generate in treatment regression",
    "computing the point estimate for the bounds",
    "common causes after removing the benchmark causes",
    "dataframe with treatment and observed common causes after removing benchmark causes",
    "R^2 of treatment with observed common causes removing benchmark causes",
    "return the variance of alpha_s",
    "R^2 of outcome with observed common causes and treatment after removing benchmark causes",
    "Obtaining theta_s (the obtained estimate)",
    "Creating numpy arrays",
    "Setting up cross-validation parameters",
    "tuple of residuals from first stage estimation [0,1], and the confounders [2]",
    "We need to estimate, sigma^2 = (Y-g_s)^2. We use the following derivation.",
    "Yres = Y - E[Y|W]",
    "E[Y|W] = f(x) + theta_s * E[T|W]",
    "Yres = Y - f(x) - theta_s * E[T|W]",
    "g(s) = theta_s * T + f(x)",
    "g(s) = theta_s * (T - E[T|W]) + f(x) + theta_s * E[T|W]",
    "g(s) = theta_s * Tres +f(x) + theta_s * E[T|W]",
    "Y - g(s) = Y - [theta_s * Tres + f(x) + theta_s * E[T|W] )",
    "Y - g(s) = ( Y - f(x) -  theta_s * E[T|W]) - theta_s * Tres",
    "Y - g(s) = Yres - theta_s * Tres",
    "nu_2 is E[alpha_s^2]",
    "Now computing scores for finding the (1-a) confidence interval",
    "R^2 of treatment with observed common causes",
    "R^2 of outcome with treatment and observed common causes",
    "Partial R^2 of outcome after regressing over unobserved confounder, observed common causes and treatment",
    "Partial R^2 of treatment after regressing over unobserved confounder and observed common causes",
    "adding 1.1 as plotting margin  ensure that the benchmarked part is shown fully in plot",
    "adding 1.1 as plotting margin  ensure that the benchmarked part is shown fully in plot",
    "Adding unadjusted point estimate",
    "Adding bounds to partial R^2 values for given strength of confounders",
    "Adding a new backdoor variable to the identified estimand",
    "Run refutation in parallel",
    "We want to see if the estimate falls in the same distribution as the one generated by the refuter",
    "Ideally that should be the case as choosing a subset should not have a significant effect on the ability",
    "of the treatment to affect the outcome",
    "TODO: Sensitivity Analyzers excluded from list due to different return type",
    "Reject H0",
    "a, b and c are all continuous variables",
    "a, b and c are all discrete variables",
    "c is set of continuous and binary variables and",
    "1. either a and b is continuous and the other is binary",
    "2. both a and b are binary",
    "c is discrete and",
    "either a or b is continuous and the other is discrete",
    "a and b are discrete and c is a mixture of discrete and continuous variables. We discretize c and calculate conditional mutual information",
    "original_treatment_name: : stores original variable names for labelling",
    "common_causes_map : maps the original variable names to variable names in OLS regression",
    "benchmark_common_causes: stores variable names in terms of regression model variables",
    "original_benchmark_covariates: stores original variable names for labelling",
    "estimate: estimate of regression",
    "degree_of_freedom: degree of freedom of error in regression",
    "standard_error: standard error in regression",
    "t_stats: Treatment coefficient t-value - measures how many standard errors the estimate is away from zero.",
    "partial_f2: value to determine if a regression model and a nested version of it have a statistically significant difference between them",
    "r2tu_w: partial R^2  of unobserved confounder \"u\" with treatment \"t\", after conditioning on observed covariates \"w\"",
    "r2yu_tw: partial R^2  of unobserved confounder \"u\" with outcome \"y\", after conditioning on observed covariates \"w\" and treatment \"t\"",
    "r2twj_w: partial R^2 of observed covariate wj with treatment \"t\", after conditioning on observed covariates \"w\" excluding wj",
    "r2ywj_tw:  partial R^2 of observed covariate wj with outcome \"y\", after conditioning on observed covariates \"w\" (excluding wj) and treatment \"t\"",
    "benchmarking_results: dataframe containing information about bounds and bias adjusted terms",
    "stats: dictionary containing information like robustness value, partial R^2, estimate, standard error , degree of freedom, partial f^2, t-statistic",
    "partial R^2 (r2yt_w) is the proportion of variation in outcome uniquely explained by treatment",
    "build a new regression model by considering treatment variables as outcome",
    "r2twj_w is partial R^2 of covariate wj with treatment \"t\", after conditioning on covariates w(excluding wj)",
    "r2ywj_tw is partial R^2 of covariate wj with outcome \"y\", after conditioning on covariates w(excluding wj) and treatment \"t\"",
    "r2tu_w is the partial r^2 from regressing u on t after conditioning on w",
    "Compute bias adjusted terms",
    "Plotting the contour plot",
    "Adding contours",
    "Adding threshold contour line",
    "Adding unadjusted point estimate",
    "Adding bounds to partial R^2 values for given strength of confounders",
    "from https://www.bnmetrics.com/blog/factory-pattern-in-python3-simple-version",
    "The default subset of the data to be used",
    "Run refutation in parallel",
    "We want to see if the estimate falls in the same distribution as the one generated by the refuter",
    "Ideally that should be the case as choosing a subset should not have a significant effect on the ability",
    "of the treatment to affect the outcome",
    "from https://www.bnmetrics.com/blog/factory-pattern-in-python3-simple-version",
    "If the relative change of the score is less than the given threshold, we stop the estimation early.",
    "Note, the output of score_samples are log values.",
    "Note, the output of score_samples are log values.",
    "Currently only support continuous distributions for auto selection.",
    "Estimate distribution parameters from data.",
    "Ignore warnings from fitting process.",
    "Fit distribution to data.",
    "Some distributions might not be compatible with the data.",
    "Separate parts of parameters.",
    "Check the KL divergence between the distribution of the given and fitted distribution.",
    "Identify if this distribution is better.",
    "This error is typically raised when the data is discrete and all points are assigned to less cluster than",
    "specified. It can also happen due to duplicated points. In these cases, the current best solution should",
    "be sufficient.",
    "Usual feature relevance using the mean deviation as set function, i.e. g(x) - E[g(X)]",
    "A convenience function when computing confidence intervals specifically for non-deterministic causal queries. This",
    "function evaluates the provided causal query multiple times to build a confidence interval based on the returned",
    "results.",
    "Note that this function does not re-fit the causal model(s) and only executes the provided query as it is. In order",
    "to re-refit the graphical causal model on random subsets of the data before executing the query, consider using the",
    "fit_and_compute function.",
    "",
    "**Example usage:**",
    "",
    ">>> gcm.fit(causal_model, data)",
    ">>> strength_medians, strength_intervals = gcm.confidence_intervals(",
    ">>>     gcm.bootstrap_sampling(gcm.arrow_strength, causal_model, target_node='Y'))",
    "",
    "In this example, gcm.confidence_intervals is expecting a callable with non-deterministic outputs for building the",
    "confidence intervals. Since each causal query potentially expects a different set of parameters, we use 'partial'",
    "here to configure the function call. In this case,",
    "gcm.bootstrap_sampling(gcm.arrow_strength, causal_model, target_node='Y') would be equivalent to",
    "lambda : gcm.arrow_strength(causal_model, target_node='Y').",
    "",
    "In order to incorporate uncertainties coming from fitting the causal model(s), we can use",
    "gcm.fit_and_compute instead:",
    ">>>  strength_medians, strength_intervals = gcm.confidence_intervals(",
    ">>>        gcm.fit_and_compute(gcm.arrow_strength,",
    ">>>                                            causal_model,",
    ">>>                                            bootstrap_training_data=data,",
    ">>>                                            target_node='Y'))",
    "This would refit the provided causal_model on a subset of the data first before executing gcm.arrow_strength in each",
    "run.",
    "Simulating interventions by propagating the effects through the graph. For this, we iterate over the nodes based",
    "on their topological order.",
    "After drawing samples of the node based on the data generation process, we apply the corresponding",
    "intervention. The inputs of downstream nodes are therefore based on the outcome of the intervention in this",
    "node.",
    "Abduction: For invertible SCMs, we recover exact noise values from data.",
    "Action + Prediction: Propagate the intervention downstream using recovered noise values.",
    "Check if we need to apply an intervention on the given node.",
    "Apply intervention function to the data of the node.",
    "Check if the intervention function changes the shape of the data.",
    "For estimating the effect, we only need to consider the nodes that have a directed path to the target node, i.e.",
    "all ancestors of the target.",
    "The target node can be a continuous real-valued variable or a categorical variable with at most two classes",
    "(i.e. binary).",
    "Avoid too many features",
    "Making sure there are at least 30% test samples.",
    "Making sure that there are at least 2 samples from one class (here, simply duplicate the point).",
    "Compare number of correct classifications.",
    "This constant is used as key when storing/accessing models as causal mechanisms in graph node attributes",
    "This constant is used as key when storing the parents of a node during fitting. It's used for validation purposes",
    "afterwards.",
    "can't use nx.node_connected_component, because it doesn't work with DiGraphs.",
    "Hence a manual loop:",
    "For estimating Shapley values for multiple samples (e.g. in feature relevance) and the number of samples",
    "is unknown beforehand.",
    "The method stops if either the change between some consecutive runs is below the given threshold or the",
    "maximum number of runs is reached.",
    "In each run, we create one random permutation of players. For instance, given 4 players, a permutation",
    "could be [3,1,4,2].",
    "Create all subsets belonging to the generated permutation. This is, if we have [3,1,4,2], then the",
    "subsets are [3], [3,1], [3,1,4] [3,1,4,2].",
    "The result for each subset is cached such that if a subset that has already been evaluated appears again,",
    "we can take this result directly.",
    "To improve the runtime, multiple permutations are evaluated in each run.",
    "The current Shapley values are the average of the estimated values, i.e. we need to divide by the number",
    "of generated permutations here.",
    "Here, the change between two runs is below the minimum threshold, but to reduce the likelihood",
    "that this just happened by chance, we require that this happens at least for two runs in a row.",
    "Create all (unique) subsets)",
    "Assigning a 'high' weight, since this resembles \"infinity\".",
    "The weight for a subset with a specific length (see paper mentioned in the docstring for more",
    "information).",
    "TODO: Add method for auto select a bin_width/width based on the data. Make sure that the auto selection method is",
    "theoretically sound, i.e. make entropy results from different data comparable.",
    "Extremely small values can somehow result in negative values.",
    "Creating a smaller subgraph, which only contains upstream nodes that are connected to the target node.",
    "Sampling from the conditional distribution based on the current sample.",
    "Sampling from the conditional based on the current sample, but randomizing the inputs of all variables that",
    "are in the given subset. By this, we can simulate the impact on the conditional distribution when removing",
    "only the incoming edges of the variables in the subset.",
    "Creating a smaller subgraph, which only contains upstream nodes that are connected to the target node.",
    "In case of the full subset (no randomization), we get the same predictions as when we apply the",
    "prediction method to the samples of interest, since all noise samples are replaced with a sample of",
    "interest.",
    "In case of the empty subset (all are jointly randomize), it boils down to taking the average over all",
    "predictions, seeing that the randomization yields the same values for each sample of interest (none of the",
    "samples of interest are used to replace a (jointly) 'randomized' sample).",
    "Smallest possible value. This is used in various algorithm for numerical stability.",
    "Make copy to avoid manipulating the original matrix.",
    "The given prediction method has to be evaluated multiple times on a large amount of different inputs. Typically,",
    "the batch evaluation of a prediction model on multiple inputs at the same time is significantly faster",
    "than evaluating it on single simples in a for-loop. To make use of this, we try to evaluate as many samples as",
    "possible in one batch call of the prediction method. However, this also requires a lot of memory for many samples.",
    "To overcome potential memory issues, multiple batch calls are performed, each with at most batch_size many",
    "samples. The number of samples that are evaluated is normally",
    "baseline_noise_samples.shape[0] * feature_samples.shape[0]. Here, we reduce it to",
    "batch_size * feature_samples.shape[0]. If the batch_size would be set 1, then each baseline_noise_samples is",
    "evaluated one by one in a for-loop.",
    "Each batch consist of at most batch_size * feature_samples.shape[0] many samples. If there are multiple",
    "batches, the offset indicates the index of the current baseline_noise_samples that has not been evaluated yet.",
    "If the batch size would be larger than the remaining amount of samples, it is reduced to only include the",
    "remaining baseline_noise_samples.",
    "The inputs consist of batch_size many copies of feature_samples. Here, we set the columns of the features",
    "in baseline_feature_indices to their respective values in baseline_noise_samples.",
    "After creating the (potentially large) input data matrix, we can evaluate the prediction method.",
    "Here, offset + index now indicates the sample index in baseline_noise_samples.",
    "This would average all prediction results obtained for the 'offset + index'-th sample in",
    "baseline_noise_samples. This is, y^(offset + index) = E[Y | do(x^(offset + index)_s)].",
    "This would return all prediction results obtained for the 'offset + index'-th sample in",
    "baseline_noise_samples, i.e. the results are not averaged.",
    "Making copy to ensure that the original object is not modified.",
    "Permute samples jointly. This still represents an interventional distribution.",
    "Permute samples independently.",
    "test local Markov condition, null hypothesis: conditional independence",
    "test edge dependence, null hypothesis: independence",
    "The order of the p-values added to the list is deterministic.",
    "To be able to validate that the graph structure did not change between fitting and causal query, we store the",
    "parents of a node during fit. That way, before sampling, we can verify the parents are still the same. While",
    "this would automatically fail when the number of parents is different, there are other more subtle cases,",
    "where the number is still the same, but it's different parents, and therefore different data. That would yield",
    "wrong results, but would not fail.",
    "Wrapping labels if they are too long",
    "Using the negative value here seeing that the Shapley estimation evaluates v(S u {i}) - v(S) for a subset S. In",
    "case of variance, we have v(S u {i}) <= v(S), which would result in a negative contribution of players to the",
    "target quantity (here, variance).",
    "Using the negative value here seeing that the Shapley estimation evaluates v(S u {i}) - v(S) for a subset S. In",
    "case of variance, we have v(S u {i}) <= v(S), which would result in a negative contribution of players to the",
    "target quantity (here, variance).",
    "Calculate Ri, the product of the residuals",
    "Standard deviation of the residuals",
    "Either X and/or Y is constant.",
    "If Z is empty, we are in the pairwise setting.",
    "Either X and/or Y is constant.",
    "If Z is empty, we are in the pairwise setting.",
    "Estimate test statistic multiple times on different permutations of the data. The p-value is then the",
    "probability (i.e. fraction) of obtaining a test statistic that is greater than statistic on the non-permuted",
    "data.",
    "First stage statistical model",
    "Second stage statistical model",
    "Check if the treatment is one-dimensional",
    "First stage",
    "Second Stage",
    "Combining the two estimates",
    "This same estimate is valid for frontdoor as well as mediation (NIE)",
    "Bulding the feature matrix",
    "features = sm.add_constant(features, has_constant='add') # to add an intercept term",
    "TODO move this to the identification step",
    "Obtain estimate by Wald Estimator",
    "Obtain estimate by 2SLS estimator: Cov(y,z) / Cov(x,z)",
    "More than 1 instrument. Use 2sls.",
    "Checking if Y is binary",
    "Enable the user to pass params for a custom propensity model",
    "Convert the categorical variables into dummy/indicator variables",
    "Basically, this gives a one hot encoding for each category",
    "The first category is taken to be the base line.",
    "Check if the treatment is one-dimensional",
    "Checking if the treatment is binary",
    "The model is always built on the entire data",
    "TODO make treatment_value and control value also as local parameters",
    "All treatments are set to the same constant value",
    "Using all data by default",
    "Fixing treatment value to the specified value, if provided",
    "treatment_vals and data_df should have same number of rows",
    "Bulding the feature matrix",
    "The model is always built on the entire data",
    "Replacing treatment values by given x",
    "First, create interventional tensor in original space",
    "Then, use pandas to ensure that the dummies are assigned correctly for a categorical treatment",
    "The average treatment effect is a combination of different",
    "regression coefficients. Complicated to compute the confidence",
    "interval analytically. For example, if y=a + b1.t + b2.tx, then",
    "the average treatment effect is b1+b2.mean(x).",
    "Refer Gelman, Hill. ARM Book. Chapter 9",
    "http://www.stat.columbia.edu/~gelman/arm/chap9.pdf",
    "TODO: Looking for contributions",
    "For a linear regression model, the causal effect of a variable is equal to the coefficient corresponding to the",
    "variable. Hence, the model by default outputs the confidence interval corresponding to treatment=1 and control=0.",
    "So for custom treatment and control values, we must multiply the confidence interval by the difference of the two.",
    "For a linear regression model, the causal effect of a variable is equal to the coefficient corresponding to the",
    "variable. Hence, the model by default outputs the standard error corresponding to treatment=1 and control=0.",
    "So for custom treatment and control values, we must multiply the standard error by the difference of the two.",
    "setting method-specific parameters",
    "Infer the right strata based on clipping threshold",
    "0.5 because there are two values for the treatment",
    "To be conservative and allow most strata to be included in the",
    "analysis",
    "At least 90% of the strata should be included in analysis",
    "sum weighted outcomes over all strata  (weight by treated population)",
    "TODO - how can we add additional information into the returned estimate?",
    "such as how much clipping was done, or per-strata info for debugging?",
    "sort the dataframe by propensity score",
    "create a column 'strata' for each element that marks what strata it belongs to",
    "for each strata, count how many treated and control units there are",
    "throw away strata that have insufficient treatment or control",
    "TODO -- fix: we are actually conditioning on positive treatment (d=1)",
    "Setting method specific parameters",
    "trim propensity score weights",
    "ips ==> (isTreated(y)/ps(y)) + ((1-isTreated(y))/(1-ps(y)))",
    "nips ==> ips / (sum of ips over all units)",
    "icps ==> ps(y)/(1-ps(y)) / (sum of (ps(y)/(1-ps(y))) over all control units)",
    "itps ==> ps(y)/(1-ps(y)) / (sum of (ps(y)/(1-ps(y))) over all treatment units)",
    "Vanilla IPS estimator",
    "The Hajek estimator (or the self-normalized estimator)",
    "Stabilized weights (from Robins, Hernan, Brumback (2000))",
    "Paper: Marginal Structural Models and Causal Inference in Epidemiology",
    "Calculating the effect",
    "Subtracting the weighted means",
    "TODO - how can we add additional information into the returned estimate?",
    "TODO -- fix: we are actually conditioning on positive treatment (d=1)",
    "Save parameters for later refutter fitting",
    "Enforcing this ordering is necessary to feed through the propensity values from dataset",
    "For metalearners only--issue a warning if w contains variables not in x",
    "Override the effect_modifiers set in CausalEstimator.__init__()",
    "Also only update self._effect_modifiers, and create a copy of self._effect_modifier_names",
    "the latter can be used by other estimator methods later",
    "Instrumental variables names, if present",
    "choosing the instrumental variable to use",
    "Calling the econml estimator's fit method",
    "As of v0.9, econml has some kewyord only arguments",
    "from https://www.bnmetrics.com/blog/factory-pattern-in-python3-simple-version",
    "Changing shape to a list for a singleton value",
    "TODO -- fix: we are actually conditioning on positive treatment (d=1)",
    "For each unit, return the estimated effect of the treatment value",
    "that was actually applied to the unit",
    "this assumes a binary treatment regime",
    "TODO remove neighbors that are more than a given radius apart",
    "estimate ATT on treated by summing over difference between matched neighbors",
    "Now computing ATC",
    "TODO -- fix: we are actually conditioning on positive treatment (d=1)",
    "from https://www.bnmetrics.com/blog/factory-pattern-in-python3-simple-version",
    "Handle externally provided estimator classes",
    "allowed types of distance metric",
    "Dictionary of any user-provided params for the distance metric",
    "that will be passed to sklearn nearestneighbors",
    "Check if the treatment is one-dimensional",
    "Checking if the treatment is binary",
    "Convert the categorical variables into dummy/indicator variables",
    "Basically, this gives a one hot encoding for each category",
    "The first category is taken to be the base line.",
    "this assumes a binary treatment regime",
    "TODO remove neighbors that are more than a given radius apart",
    "estimate ATT on treated by summing over difference between matched neighbors",
    "Return indices in the original dataframe",
    "self.matched_indices_att[treated_df_index[i]] = control.iloc[indices[i]].index.tolist()",
    "Now computing ATC",
    "Return indices in the original dataframe",
    "Add the identification method used in the estimator",
    "Check the backdoor variables being used",
    "Add the observed confounders and one hot encode the categorical variables",
    "Get the data of the unobserved confounders",
    "One hot encode the data if they are categorical",
    "Check the instrumental variables involved",
    "Perform the same actions as the above",
    "Both the outcome and the treatment have to be 1D arrays according to the CausalML API",
    "We want to pass 'v0' rather than ['v0'] to prevent a shape mismatch",
    "For CATEs",
    "TODO we are conditioning on a postive treatment",
    "TODO create an expression corresponding to each estimator used",
    "Generating data with equal 0 and 1 (since ranks are uniformly distributed)",
    "Flipping some values",
    "Finding p-value using student T test",
    "Only consider edges have absolute edge weight > 0.01",
    "Modify graph such that it only contains bidirected edges",
    "Find c components by finding connected components on the undirected graph",
    "Understanding Neural Network weights",
    "Refer to this link:https://stackoverflow.com/questions/50937628/mlp-classifier-neurons-weights",
    "add weight column",
    "before weights are applied we count number rows in each category",
    "which is equivalent to summing over weight=1",
    "after weights are applied we need to sum over the given weights",
    "First, calculating mean differences by strata",
    "Second, without strata",
    "Third, concatenating them and plotting",
    "Setting estimator attribute for convenience",
    "Outcome is numeric",
    "Treatments are also numeric or binary",
    "Outcome is categorical",
    "Treatments are numeric or binary",
    "TODO: A common way to show all plots",
    "from https://www.bnmetrics.com/blog/factory-pattern-in-python3-simple-version",
    "Get adjacency list",
    "If node pair has been fully explored",
    "Add node1 to backdoor set of node_pair",
    "Check if path is backdoor and does not have nodes1\\node1 or nodes2\\node2 as intermediate nodes",
    "True if arrow incoming, False if arrow outgoing",
    "Mark pair (node1, node2) complete",
    "Modify variable count and indices covered",
    "Average total effect",
    "Natural direct effect",
    "Natural indirect effect",
    "Controlled direct effect",
    "Backdoor method names",
    "First, check if there is a directed path from action to outcome",
    "## 1. BACKDOOR IDENTIFICATION",
    "Pick algorithm to compute backdoor sets according to method chosen",
    "First, checking if there are any valid backdoor adjustment sets",
    "Setting default \"backdoor\" identification adjustment set",
    "## 2. INSTRUMENTAL VARIABLE IDENTIFICATION",
    "Now checking if there is also a valid iv estimand",
    "## 3. FRONTDOOR IDENTIFICATION",
    "Now checking if there is a valid frontdoor variable",
    "Finally returning the estimand object",
    "Pick algorithm to compute backdoor sets according to method chosen",
    "Setting default \"backdoor\" identification adjustment set",
    "Finally returning the estimand object",
    "## 1. FIRST DOING BACKDOOR IDENTIFICATION",
    "First, checking if there are any valid backdoor adjustment sets",
    "Setting default \"backdoor\" identification adjustment set",
    "## 2. SECOND, CHECKING FOR MEDIATORS",
    "Now checking if there are valid mediator variables",
    "Finally returning the estimand object",
    "## 1. FIRST DOING BACKDOOR IDENTIFICATION",
    "First, checking if there are any valid backdoor adjustment sets",
    "Setting default \"backdoor\" identification adjustment set",
    "## 2. SECOND, CHECKING FOR MEDIATORS",
    "Now checking if there are valid mediator variables",
    "Finally returning the estimand object",
    "First, checking if empty set is a valid backdoor set",
    "If the method is `minimal-adjustment`, return the empty set right away.",
    "Second, checking for all other sets of variables. If include_unobserved is false, then only observed variables are eligible.",
    "only remove descendants of Y",
    "also allow any causes of Y that are not caused by T (for lower variance)",
    "remove descendants of T (mediators) and descendants of Y",
    "If var is d-separated from both treatment or outcome, it cannot",
    "be a part of the backdoor set",
    "repeat the above search with BACKDOOR_MIN",
    "If `minimal-adjustment` method is specified, start the search from the set with minimum size. Otherwise, start from the largest.",
    "If the backdoor method is `maximal-adjustment` or `minimal-adjustment`, return the first found adjustment set.",
    "If all variables are observed, and the biggest eligible set",
    "does not satisfy backdoor, then none of its subsets will.",
    "Adding a None estimand if no backdoor set found",
    "Default set contains minimum possible number of instrumental variables, to prevent lowering variance in the treatment variable.",
    "Default set is the one with the least number of adjustment variables (optimizing for efficiency)",
    "For simplicity, assuming a one-variable frontdoor set",
    "Cond 1: All directed paths intercepted by candidate_var",
    "Cond 2: No confounding between treatment and candidate var",
    "Cond 3: treatment blocks all confounding between candidate_var and outcome",
    "For simplicity, assuming a one-variable mediation set",
    "Create estimands dict as per the API for backdoor, but do not return it",
    "Setting default \"backdoor\" identification adjustment set",
    "Create estimands dict as per the API for backdoor, but do not return it",
    "Setting default \"backdoor\" identification adjustment set",
    "TODO: outputs string for now, but ideally should do symbolic",
    "expressions Mon 19 Feb 2018 04:54:17 PM DST",
    "TODO Better support for multivariate treatments",
    "TODO: support multivariate treatments better.",
    "TODO: support multivariate treatments better.",
    "TODO: support multivariate treatments better.",
    "For direct effect",
    "If no costs are passed, use uniform costs",
    "restriction to ancestors",
    "back-door graph",
    "moralization",
    "Estimators list for returning after identification",
    "Line 1",
    "If no action has been taken, the effect on Y is just the marginal of the observational distribution P(v) on Y.",
    "Line 2",
    "If we are interested in the effect on Y, it is sufficient to restrict our attention on the parts of the model ancestral to Y.",
    "Modify list of valid nodes",
    "Line 3 - forces an action on any node where such an action would have no effect on Y \u2013 assuming we already acted on X.",
    "Modify adjacency matrix to obtain that corresponding to do(X)",
    "Line 4 - Decomposes the problem into a set of smaller problems using the key property of C-component factorization of causal models.",
    "If the entire graph is a single C-component already, further problem decomposition is impossible, and we must provide base cases.",
    "Modify adjacency matrix to remove treatment variables",
    "Line 5 - The algorithms fails due to the presence of a hedge - the graph G, and a subgraph S that does not contain any X nodes.",
    "Line 6 - If there are no bidirected arcs from X to the other nodes in the current subproblem under consideration, then we can replace acting on X by conditioning, and thus solve the subproblem.",
    "Line 7 - This is the most complicated case in the algorithm. Explain in the second last paragraph on Pg 41 of the link provided in the docstring above.",
    "Do not show backdoor key unless it is the only backdoor set.",
    "Just show the default backdoor set",
    "self._identified_estimand = self._causal_model.identify_effect()",
    "self._identified_estimand,",
    "self._causal_model._treatment,",
    "self._causal_model._outcome,",
    "If labels provided",
    "Return in valid DOT format",
    "Get adjacency matrix",
    "If labels not provided",
    "Obtain valid DOT format",
    "If labels provided",
    "Return in valid DOT format",
    "from https://www.bnmetrics.com/blog/factory-pattern-in-python3-simple-version",
    "from https://www.bnmetrics.com/blog/factory-pattern-in-python3-simple-version"
  ],
  "v0.8": [
    "Get the long description from the README file",
    "Get the required packages",
    "Plotting packages are optional to install",
    "Loading version number",
    "-*- coding: utf-8 -*-",
    "",
    "Configuration file for the Sphinx documentation builder.",
    "",
    "This file does only contain a selection of the most common options. For a",
    "full list see the documentation:",
    "http://www.sphinx-doc.org/en/stable/config",
    "-- Path setup --------------------------------------------------------------",
    "If extensions (or modules to document with autodoc) are in another directory,",
    "add these directories to sys.path here. If the directory is relative to the",
    "documentation root, use os.path.abspath to make it absolute, like shown here.",
    "",
    "-- Project information -----------------------------------------------------",
    "The short X.Y version",
    "The full version, including alpha/beta/rc tags",
    "-- General configuration ---------------------------------------------------",
    "If your documentation needs a minimal Sphinx version, state it here.",
    "",
    "needs_sphinx = '1.0'",
    "Add any Sphinx extension module names here, as strings. They can be",
    "extensions coming with Sphinx (named 'sphinx.ext.*') or your custom",
    "ones.",
    "Add any paths that contain templates here, relative to this directory.",
    "The suffix(es) of source filenames.",
    "You can specify multiple suffix as a list of string:",
    "",
    "source_suffix = ['.rst', '.md']",
    "The master toctree document.",
    "The language for content autogenerated by Sphinx. Refer to documentation",
    "for a list of supported languages.",
    "",
    "This is also used if you do content translation via gettext catalogs.",
    "Usually you set \"language\" from the command line for these cases.",
    "List of patterns, relative to source directory, that match files and",
    "directories to ignore when looking for source files.",
    "This pattern also affects html_static_path and html_extra_path .",
    "The name of the Pygments (syntax highlighting) style to use.",
    "-- Options for HTML output -------------------------------------------------",
    "The theme to use for HTML and HTML Help pages.  See the documentation for",
    "a list of builtin themes.",
    "",
    "html_theme = 'sphinx-rtd-theme'",
    "on_rtd is whether we are on readthedocs.org",
    "only import and set the theme if we're building docs locally",
    "otherwise, readthedocs.org uses their theme by default, so no need to specify it",
    "Theme options are theme-specific and customize the look and feel of a theme",
    "further.  For a list of options available for each theme, see the",
    "documentation.",
    "",
    "html_theme_options = {}",
    "Add any paths that contain custom static files (such as style sheets) here,",
    "relative to this directory. They are copied after the builtin static files,",
    "so a file named \"default.css\" will overwrite the builtin \"default.css\".",
    "Custom sidebar templates, must be a dictionary that maps document names",
    "to template names.",
    "",
    "The default sidebars (for documents that don't match any pattern) are",
    "defined by theme itself.  Builtin themes are using these templates by",
    "default: ``['localtoc.html', 'relations.html', 'sourcelink.html',",
    "'searchbox.html']``.",
    "",
    "html_sidebars = {}",
    "-- Options for HTMLHelp output ---------------------------------------------",
    "Output file base name for HTML help builder.",
    "-- Options for LaTeX output ------------------------------------------------",
    "The paper size ('letterpaper' or 'a4paper').",
    "",
    "'papersize': 'letterpaper',",
    "The font size ('10pt', '11pt' or '12pt').",
    "",
    "'pointsize': '10pt',",
    "Additional stuff for the LaTeX preamble.",
    "",
    "'preamble': '',",
    "Latex figure (float) alignment",
    "",
    "'figure_align': 'htbp',",
    "Grouping the document tree into LaTeX files. List of tuples",
    "(source start file, target name, title,",
    "author, documentclass [howto, manual, or own class]).",
    "-- Options for manual page output ------------------------------------------",
    "One entry per manual page. List of tuples",
    "(source start file, name, description, authors, manual section).",
    "-- Options for Texinfo output ----------------------------------------------",
    "Grouping the document tree into Texinfo files. List of tuples",
    "(source start file, target name, title, author,",
    "dir menu entry, description, category)",
    "-- Options for Epub output -------------------------------------------------",
    "Bibliographic Dublin Core info.",
    "The unique identifier of the text. This can be a ISBN number",
    "or the project homepage.",
    "",
    "epub_identifier = ''",
    "A unique identification for the text.",
    "",
    "epub_uid = ''",
    "A list of files that should not be packed into the epub file.",
    "-- Extension configuration -------------------------------------------------",
    "-- Options for todo extension ----------------------------------------------",
    "If true, `todo` and `todoList` produce output, else they produce nothing.",
    "init docstrings should also be included in class",
    "requires stdin input for identify in weighting sampler",
    "requires Rpy2 for lalonde",
    "requires Rpy2 for causal discovery",
    "very slow",
    "will be removed",
    "applied notebook, not necessary to test each time",
    "Adding the dowhy root folder to the python path so that jupyter notebooks",
    "can import dowhy",
    "\"--ExecutePreprocessor.timeout=600\",",
    "self.df = pd.read_csv(os.path.join(DATA_PATH,'dgp_1/acic_1_1_data.csv'))",
    "self.ate = np.mean(self.df['y1'] - self.df['y0'])",
    "treated = self.df[self.df['z']==1]",
    "self.att = np.mean(treated['y1'] - treated['y0'])",
    "def test_average_treatment_effect(self):",
    "est_ate = 1",
    "bias = est_ate - self.ate",
    "print(bias)",
    "self.assertAlmostEqual(self.ate, est_ate)",
    "def test_average_treatment_effect_on_treated(self):",
    "est_att = 1",
    "self.att=1",
    "bias = est_att - self.att",
    "print(bias)",
    "self.assertAlmostEqual(self.att, est_att)",
    "removing two common causes",
    "removing two common causes",
    "removing two common causes",
    "removing two common causes",
    "check if all partial R^2 values are between 0 and 1",
    "We calculate adjusted estimates for two sets of partial R^2 values.",
    "Test if hypothetical confounding by unobserved confounder u1 leads to an adjusted effect that is farther from the original estimate as compared to u2",
    "Creating a model with no unobserved confounders",
    "check if all partial R^2 values are between 0 and 1",
    "for a dataset with no confounders, the robustness value should be higher than a given threshold (0.95 in our case)",
    "The outcome is a linear function of the confounder",
    "The slope is 1,2 and the intercept is 3",
    "As we run with only one common cause and one instrument variable we run with (?, 2)",
    "Supports user-provided dataset object",
    "To test if there are any exceptions",
    "To test if the estimate is identical if refutation parameters are zero",
    "This value is hardcoded to be zero as we are runnning this on a linear dataset.",
    "Ordinarily, we should expect this value to be zero.",
    "This value is hardcoded to be zero as we are runnning this on a linear dataset.",
    "Ordinarily, we should expect this value to be zero.",
    "Calculate causal effect twice: once for unit (t=1, c=0), once for specific increase (t=100, c=50)",
    "Only P(Y|T) should be present for test to succeed.",
    "Calculate causal effect twice: once for unit (t=1, c=0), once for specific increase (t=100, c=50)",
    "Since undirected graph, identify effect must throw an error.",
    "Calculate causal effect twice: once for unit (t=1, c=0), once for specific increase (t=100, c=50)",
    "Compare with ground truth",
    "Calculate causal effect twice: once for unit (t=1, c=0), once for specific increase (t=100, c=50)",
    "Compare with ground truth",
    "Calculate causal effect twice: once for unit (t=1, c=0), once for specific increase (t=100, c=50)",
    "Compare with ground truth",
    "Calculate causal effect twice: once for unit (t=1, c=0), once for specific increase (t=100, c=50)",
    "Compare with ground truth",
    "Causal model initialization",
    "Causal identifier identification",
    "Obtain backdoor sets",
    "Check if backdoor sets are valid i.e. if they block all paths between the treatment and the outcome",
    "Causal model initialization",
    "Causal identifier identification",
    "Obtain backdoor sets",
    "Causal model initialization",
    "Causal identifier identification",
    "Obtain backdoor sets",
    "Check if backdoor sets are valid i.e. if they block all paths between the treatment and the outcome",
    "Causal model initialization",
    "Causal identifier identification",
    "Obtain backdoor sets",
    "Check if backdoor sets are valid i.e. if they block all paths between the treatment and the outcome",
    "Causal model initialization",
    "Causal identifier identification",
    "Obtain backdoor sets",
    "Check if backdoor sets are valid i.e. if they block all paths between the treatment and the outcome",
    "Causal model initialization",
    "Causal identifier identification",
    "Obtain backdoor sets",
    "Check if backdoor sets are valid i.e. if they block all paths between the treatment and the outcome",
    "Example is selected from Pearl J. \"Causality\" 2nd Edition, from chapter 3.3.1 on backoor criterion.",
    "The following simpsons paradox examples are taken from Pearl, J {2013}. \"Understanding Simpson\u2019s Paradox\" - http://ftp.cs.ucla.edu/pub/stat_ser/r414.pdf",
    "The following are examples given in the \"Book of Why\" by Judea Pearl, chapter \"The Do-operator and the Back-Door Criterion\"",
    "cov_mat = np.diag(np.ones(num_features))",
    "collider: X->Z<-Y",
    "chain: X->Z->Y",
    "fork: X<-Z->Y",
    "general DAG: X<-Z->Y, X->Y",
    "fork: X<-Z->Y",
    "Contributions should add up to Var(X2)",
    "Contributions should add up to Var(X2)",
    "H(P(Y)) -- Can be precomputed",
    "-(H(P(Y | do(x_S)) - H(P(Y))) = H(P(Y)) - H(P(Y | do(x_S))",
    "H(P(Y | do(x_S)) = H(E[P(Y | x_S, X'_\\S)])",
    "E[P(Y | x_S, X'_\\S)]",
    "H(E[P(Y | x_S, X'_\\S)])",
    "Using H(P(Y)) based on the origina data, i.e. ignoring baseline_predictions.",
    "E[H(P(Y)) - H(P(Y | do(X_U))] = H(P(Y)) - E[H(P(Y | X))]",
    "Just checking formats, i.e. no need for correlation.",
    "Just checking formats, i.e. no need for correlation.",
    "By default, the strength is measure with respect to the variance.",
    "Here, we misspecified the mechanism on purpose by setting scale to 1 instead of 2.",
    "If we provide the observational data here, we can mitigate the misspecification of the causal mechanism.",
    "Here, changing the mechanism.",
    "Here, changing the mechanism.",
    "Here, changing the mechanism.",
    "Here, changing the mechanism.",
    "Defining an anomaly scorer that handles multidimensional inputs.",
    "Seeing that the expectation of the noise in all nodes is 0, we introduce anomalies by setting some of them to 3.",
    "In the first sample, only the first variable is anomalous. Therefore, it should have the highest contribution",
    "and it should be \"significantly\" higher than the contribution of the other ones (here, we just arbitrarily say",
    "it should be 3x higher. Due to the confounding factor Z, the reconstructed noise variables are pairwise dependent,",
    "which is a violation of our causal sufficiency assumption. However, a confounder is included here to demonstrate",
    "some robustness. Note that due to this and stochastic behaviour of the density estimator, it is",
    "not possible to analytically compute expected results. Therefore, we rather look at the relations here.",
    "Same idea for the second sample, but here, it is the second variable that is anomalous.",
    "In the fourth sample, there are 2 anomalous variables. Therefore, the contribution of these 2 variables should be",
    "\"significantly\" higher than the contribution of the other variables. The contribution of both anomalous variables",
    "should be equal (approximately).",
    "Defining an anomaly scorer that handles multidimensional inputs.",
    "Seeing that the expectation of the noise in all nodes is 0, we introduce anomalies by setting some of them to 3.",
    "reduce the score.",
    "The contributions should add up to g(x) - E[g(X)]",
    "The contributions should add up to g(x) - E[g(X)]",
    "The contributions should add up to g(x) - E[g(X)]",
    "Three examples:",
    "1. X1 is the root cause (+ 10 to the noise)",
    "2. X0 is the root cause (+ 10 to the noise)",
    "3. X0 and X3 are both root causes (+ 10 to both noises)",
    "The sum of the scores should add up to the anomaly score of the target (here, X3).",
    "Three examples:",
    "1. X1 is the root cause (+ 10 to the noise)",
    "2. X0 is the root cause (+ 10 to the noise)",
    "3. X0 and X3 are both root causes (+ 10 to both noises)",
    "The sum of the scores should add up to the anomaly score of the target (here, X3).",
    "Defining ground truth model to avoid SCM learning issues and, hence, to focus on the anomaly attribution",
    "algorithm.",
    "1. X0 is the root cause (+ 10 to the noise)",
    "2. X0 and X1 are the root causes (+ 10 to both noise)",
    "3. X2 and X3 are both root causes (+ 10 to both noises)",
    "The sum of the scores should add up to the anomaly score of the target (here, X3).",
    "Check if calling the method causes some import or runtime errors",
    "TODO: Plotting undirected graphs with networkx causes an error when an older networkx version is used with a newer",
    "newer matplotlib version:",
    "AttributeError: module 'matplotlib.cbook' has no attribute 'is_numlike'",
    "Networkx 2.4+ should fix this issue.",
    "plot_adjacency_matrix(causal_graph, is_directed=False)",
    "Setup data",
    "Test LinearDML",
    "Test ContinuousTreatmentOrthoForest",
    "Test LinearDRLearner",
    "Setup data",
    "Test DeepIV",
    "Treatment model,",
    "Response model",
    "Test IntentToTreatDRIV",
    "Not using testsuite from .base/TestEstimtor, custom code below",
    "More cases where Exception  is expected",
    "Compute confidence intervals, standard error and significance tests",
    "Defined a linear dataset with a given set of properties",
    "Create a model that captures the same",
    "Identify the effects within the model",
    "Defined a linear dataset with a given set of properties",
    "Create a model that captures the same",
    "Identify the effects within the model",
    "Defined a linear dataset with a given set of properties",
    "Create a model that captures the same",
    "Identify the effects within the model",
    "Defined a linear dataset with a given set of properties",
    "Create a model that captures the same",
    "Identify the effects within the model",
    "Defined a linear dataset with a given set of properties",
    "Create a model that captures the same",
    "Identify the effects within the model",
    "Backdoor method names",
    "First, check if there is a directed path from action to outcome",
    "## 1. BACKDOOR IDENTIFICATION",
    "First, checking if there are any valid backdoor adjustment sets",
    "Setting default \"backdoor\" identification adjustment set",
    "## 2. INSTRUMENTAL VARIABLE IDENTIFICATION",
    "Now checking if there is also a valid iv estimand",
    "## 3. FRONTDOOR IDENTIFICATION",
    "Now checking if there is a valid frontdoor variable",
    "Finally returning the estimand object",
    "## 1. FIRST DOING BACKDOOR IDENTIFICATION",
    "First, checking if there are any valid backdoor adjustment sets",
    "Setting default \"backdoor\" identification adjustment set",
    "## 2. SECOND, CHECKING FOR MEDIATORS",
    "Now checking if there are valid mediator variables",
    "Finally returning the estimand object",
    "## 1. FIRST DOING BACKDOOR IDENTIFICATION",
    "First, checking if there are any valid backdoor adjustment sets",
    "Setting default \"backdoor\" identification adjustment set",
    "## 2. SECOND, CHECKING FOR MEDIATORS",
    "Now checking if there are valid mediator variables",
    "Finally returning the estimand object",
    "First, checking if empty set is a valid backdoor set",
    "If the method is `minimal-adjustment`, return the empty set right away.",
    "Second, checking for all other sets of variables. If include_unobserved is false, then only observed variables are eligible.",
    "If var is d-separated from both treatment or outcome, it cannot",
    "be a part of the backdoor set",
    "repeat the above search with BACKDOOR_MIN",
    "If `minimal-adjustment` method is specified, start the search from the set with minimum size. Otherwise, start from the largest.",
    "If the backdoor method is `maximal-adjustment` or `minimal-adjustment`, return the first found adjustment set.",
    "If all variables are observed, and the biggest eligible set",
    "does not satisfy backdoor, then none of its subsets will.",
    "Adding a None estimand if no backdoor set found",
    "Default set contains minimum possible number of instrumental variables, to prevent lowering variance in the treatment variable.",
    "Default set is the one with the least number of adjustment variables (optimizing for efficiency)",
    "For simplicity, assuming a one-variable frontdoor set",
    "Cond 1: All directed paths intercepted by candidate_var",
    "Cond 2: No confounding between treatment and candidate var",
    "Cond 3: treatment blocks all confounding between candidate_var and outcome",
    "For simplicity, assuming a one-variable mediation set",
    "Create estimands dict as per the API for backdoor, but do not return it",
    "Setting default \"backdoor\" identification adjustment set",
    "Create estimands dict as per the API for backdoor, but do not return it",
    "Setting default \"backdoor\" identification adjustment set",
    "TODO: outputs string for now, but ideally should do symbolic",
    "expressions Mon 19 Feb 2018 04:54:17 PM DST",
    "TODO Better support for multivariate treatments",
    "TODO: support multivariate treatments better.",
    "TODO: support multivariate treatments better.",
    "TODO: support multivariate treatments better.",
    "For direct effect",
    "Do not show backdoor key unless it is the only backdoor set.",
    "Just show the default backdoor set",
    "Can use these lists to specify the models/estimators/refuters that a particular interpreter supports.  Throw a ValueError if the user provides an incompatible object to intepret.",
    "Unpacking the keyword arguments",
    "Default value for the number of simulations to be conducted",
    "Concatenate the confounders, instruments and effect modifiers",
    "Shuffle the confounders",
    "Check if all are select or deselect variables",
    "Check if all the required_variables belong to confounders, instrumental variables or effect",
    "Initializing the p_value",
    "Perform Bootstrap Significance Test with the original estimate and the set of refutations",
    "Perform Normal Tests of Significance with the original estimate and the set of refutations",
    "Perform Bootstrap Significance Test with the original estimate and the set of refutations",
    "Perform Normal Tests of Significance with the original estimate and the set of refutations",
    "Get the number of simulations",
    "Sort the simulations",
    "Obtain the median value",
    "Performing a two sided test",
    "np.searchsorted tells us the index if it were a part of the array",
    "We select side to be left as we want to find the first value that matches",
    "We subtact 1 as we are finding the value from the right tail",
    "We take the side to be right as we want to find the last index that matches",
    "We get the probability with respect to the left tail.",
    "return twice the determined quantile as this is a two sided test",
    "Get the mean for the simulations",
    "Get the standard deviation for the simulations",
    "Get the Z Score [(val - mean)/ std_dev ]",
    "re.sub only takes string parameter so the first if is to avoid error",
    "if the input is a text file, convert the contained data into string",
    "load dot file",
    "Adding node attributes",
    "adding penwidth to make the edge bold",
    "Adding common causes",
    "Adding instruments",
    "Adding effect modifiers",
    "Assuming the simple form of effect modifier",
    "that directly causes the outcome.",
    "self._graph.add_edge(node_name, outcome, style = \"dotted\", headport=\"s\", tailport=\"n\")",
    "self._graph.add_edge(outcome, node_name, style = \"dotted\", headport=\"n\", tailport=\"s\") # TODO make the ports more general so that they apply not just to top-bottom node configurations",
    "Adding columns in the dataframe as confounders that were not in the graph",
    "Adding unobserved confounders",
    "also return the number of backdoor paths blocked by observed nodes",
    "Assume that nodes1 is the treatment",
    "ignores new_graph parameter, always uses self._graph",
    "remove paths that have nodes1\\node1 or nodes2\\node2 as intermediate nodes",
    "TODO Refactor to remove this from here and only implement this logic in causalIdentifier. Unnecessary assumption of nodes1 to be causing nodes2.",
    "Cannot simply compute ancestors, since that will also include nodes1 and its parents (e.g. instruments)",
    "removing all mediators",
    "[TODO: double check these work with multivariate implementation:]",
    "Exclusion",
    "As-if-random setup",
    "As-if-random",
    "convert the outputted generator into a list",
    "dpaths = self.get_all_directed_paths(nodes1, nodes2)",
    "return len(dpaths) > 0",
    "Condition 1: node 1 ---> node 2 is intercepted by candidate_nodes",
    "Create causal graph object",
    "Sometimes, effect modifiers from the graph may not match those provided by the user.",
    "(Because some effect modifiers may also be common causes)",
    "In such cases, the user-provided modifiers are used.",
    "If no effect modifiers are provided,  then the ones from the graph are used.",
    "Import causal discovery class",
    "Initialize causal graph object",
    "TODO add propensity score as default backdoor method, iv as default iv method, add an informational message to show which method has been selected.",
    "TODO add dowhy as a prefix to all dowhy estimators",
    "This is done as all dowhy estimators have two parts and external ones have two or more parts",
    "Define the third-party estimation method to be used",
    "Process the dowhy estimators",
    "Check if estimator's target estimand is identified",
    "Estimator had been computed in a previous call",
    "Store parameters inside estimate object for refutation methods",
    "TODO: This add_params needs to move to the estimator class",
    "inside estimate_effect and estimate_conditional_effect",
    "Check if estimator's target estimand is identified",
    "Note that while the name of the variable is the same,",
    "\"self.causal_estimator\", this estimator takes in less",
    "parameters than the same from the",
    "estimate_effect code. It is not advisable to use the",
    "estimator from this function to call estimate_effect",
    "with fit_estimator=False.",
    "Estimator had been computed in a previous call",
    "The default number of simulations for statistical testing",
    "The default number of simulations to obtain confidence intervals",
    "The portion of the total size that should be taken each time to find the confidence intervals",
    "1 is the recommended value",
    "https://ocw.mit.edu/courses/mathematics/18-05-introduction-to-probability-and-statistics-spring-2014/readings/MIT18_05S14_Reading24.pdf",
    "https://projecteuclid.org/download/pdf_1/euclid.ss/1032280214",
    "The default Confidence Level",
    "Number of quantiles to discretize continuous columns, for applying groupby",
    "Prefix to add to temporary categorical variables created after discretization",
    "std args to be removed from locals() before being passed to args_dict",
    "Currently estimation methods only support univariate treatment and outcome",
    "Setting the default interpret method",
    "Setting treatment and outcome values",
    "Now saving the effect modifiers",
    "only add the observed nodes",
    "Check if some parameters were set, otherwise set to default values",
    "Estimate conditional estimates by default",
    "names of treatment and outcome",
    "TODO Only works for binary treatment",
    "Defaulting to class default values if parameters are not provided",
    "Checking that there is at least one effect modifier",
    "Making sure that effect_modifier_names is a list",
    "Making a copy since we are going to be changing effect modifier names",
    "For every numeric effect modifier, adding a temp categorical column",
    "Grouping by effect modifiers and computing effect separately",
    "Deleting the temporary categorical columns",
    "The array that stores the results of all estimations",
    "Find the sample size the proportion with the population size",
    "Perform the set number of simulations",
    "names of treatment and outcome",
    "Using class default parameters if not specified",
    "Checking if bootstrap_estimates are already computed",
    "Checked if any parameter is changed from the previous std error estimate",
    "Now use the data obtained from the simulations to get the value of the confidence estimates",
    "Get the variations of each bootstrap estimate and sort",
    "Now we take the (1- p)th and the (p)th variations, where p is the chosen confidence level",
    "Get the lower and upper bounds by subtracting the variations from the estimate",
    "Use existing params, if new user defined params are not present",
    "Checking if bootstrap_estimates are already computed",
    "Check if any parameter is changed from the previous std error estimate",
    "Use existing params, if new user defined params are not present",
    "self._outcome = self._data[\"dummy_outcome\"]",
    "Processing the null hypothesis estimates",
    "Doing a two-sided test",
    "Being conservative with the p-value reported",
    "Being conservative with the p-value reported",
    "If the estimate_index is 0, it depends on the number of simulations",
    "Need to test r-squared before supporting",
    "effect_r_squared = self._evaluate_effect_strength(estimate, method=\"r-squared\")",
    "'r-squared': effect_r_squared",
    "elif method == \"r-squared\":",
    "outcome_mean = np.mean(self._outcome)",
    "total_variance = np.sum(np.square(self._outcome - outcome_mean))",
    "Assuming a linear model with one variable: the treatment",
    "Currently only works for continuous y",
    "causal_model = outcome_mean + estimate.value*self._treatment",
    "squared_residual = np.sum(np.square(self._outcome - causal_model))",
    "r_squared = 1 - (squared_residual/total_variance)",
    "return r_squared",
    "No estimand was identified (identification failed)",
    "s += \"Variance in outcome explained by treatment: {}\\n\".format(self.effect_strength[\"r-squared\"])",
    "Below loop assumes that the last indices of W are alwawys converted to discrete",
    "one-hot encode discrete W",
    "Now deleting the old continuous value",
    "Making beta an array",
    "TODO Ensure that we do not generate weak instruments",
    "TODO - test all our methods with random noise added to covariates (instead of the stochastic treatment assignment)",
    "Converting treatment to binary if required",
    "Generating frontdoor variables if asked for",
    "NOTE: We are assuming a linear relationship *even when t is categorical* and integer coded.",
    "For categorical t, this example dataset has the effect size for category 2 being exactly",
    "double the effect for category 1",
    "This could be changed at this stage by one-hot encoding t and using a custom beta that",
    "sets a different effect for each category {0, 1, 2}",
    "Computing ATE",
    "constructing column names for one-hot encoded discrete features",
    "Specifying the correct dtypes",
    "Now specifying the corresponding graph strings",
    "Now writing the gml graph",
    "Making beta an array",
    "creating data frame",
    "Specifying the correct dtypes",
    "Now specifying the corresponding graph strings",
    "Now writing the gml graph",
    "Adding edges between common causes and the frontdoor mediator",
    "Error terms",
    "else:",
    "V = 6 + W0 + tterm + E1",
    "Y = 12 + W0*W0 + W0*W0 + yterm + E2  # E2_new",
    "Generating a random normal distribution of integers",
    "Generating data for nodes which have no incoming edges",
    "\"currset\" variable currently has all the successors of the nodes which had no incoming edges",
    "Loading version number",
    "The currently supported estimators",
    "The default standard deviation for noise",
    "The default scaling factor to determine the bucket size",
    "The minimum number of points for the estimator to run",
    "The Default Transformation, when no arguments are given, or if the number of data points are insufficient for an estimator",
    "The Default True Causal Effect, this is taken to be ZERO by default",
    "The Default split for the number of data points that fall into the training and validation sets",
    "Assuming that outcome is one-dimensional",
    "We need to change the identified estimand",
    "We thus, make a copy. This is done as we don't want",
    "to change the original DataFrame",
    "We use collections.OrderedDict to maintain the order in which the data is stored",
    "Check if we are using an estimator in the transformation list",
    "The rationale behind ordering of the loops is the fact that we induce randomness everytime we create the",
    "Train and the Validation Datasets. Thus, we run the simulation loop followed by the training and the validation",
    "loops. Thus, we can get different values everytime we get the estimator.",
    "Warn the user that the specified parameter is not applicable when no estimator is present in the transformation",
    "Adding an unobserved confounder if provided by the user",
    "We set X_train = 0 and outcome_train to be 0",
    "Get the final outcome, after running through all the values in the transformation list",
    "Check if the value of true effect has been already stored",
    "We use None as the key as we have no base category for this refutation",
    "As we currently support only one treatment",
    "If the number of data points is too few, run the default transformation: [(\"zero\",\"\"),(\"noise\", {'std_dev':1} )]",
    "Check if the value of true effect has been already stored",
    "This ensures that we calculate the causal effect only once.",
    "We use key_train as we map data with respect to the base category of the data",
    "As we currently support only one treatment",
    "Add h(t) to f(W) to get the dummy outcome",
    "We convert to ndarray for ease in indexing",
    "The data is of the form",
    "sim1: cat1 cat2 ... catn",
    "sim2: cat1 cat2 ... catn",
    "Note: We would like the causal_estimator to find the true causal estimate that we have specified through this",
    "refuter. Let the value of the true causal effect be h(t). In the following section of code, we wish to find out if h(t) falls in the",
    "distribution of the refuter.",
    "True Causal Effect list",
    "Iterating through the refutation for each category",
    "We use string arguments to account for both 32 and 64 bit varaibles",
    "action for continuous variables",
    "Action for categorical variables",
    "Find the set difference for each row",
    "Choose one out of the remaining",
    "We want to see if the estimate falls in the same distribution as the one generated by the refuter",
    "Ideally that should be the case as running bootstrap should not have a significant effect on the ability",
    "of the treatment to affect the outcome",
    "Standardizing the data",
    "Fit a model containing all confounders and compare predictions",
    "using all features compared to all features except a given",
    "confounder.",
    "Estimating the regression coefficient from standardized features to t",
    "By default, return a plot with 10 points",
    "consider 10 values of the effect of the unobserved confounder",
    "Standardizing the data",
    "Fit a model containing all confounders and compare predictions",
    "using all features compared to all features except a given",
    "confounder.",
    "By default, return a plot with 10 points",
    "consider 10 values of the effect of the unobserved confounder",
    "Get a 2D matrix of values",
    "x,y =  np.meshgrid(self.kappa_t, self.kappa_y) # x,y are both MxN",
    "Store the values into the refute object",
    "Adding a label on the contour line for the original estimate",
    "Label every other level using strings",
    "By default, we add the effect of simulated confounder for treatment.",
    "But subtract it from outcome to create a negative correlation",
    "assuming that the original confounder's effect was positive on both.",
    "This is to remove the effect of the original confounder.",
    "By default, we add the effect of simulated confounder for treatment.",
    "But subtract it from outcome to create a negative correlation",
    "assuming that the original confounder's effect was positive on both.",
    "This is to remove the effect of the original confounder.",
    "Obtaining the list of observed variables",
    "Taking a subset of the dataframe that has only observed variables",
    "Residuals from the outcome model obtained by fitting a linear model",
    "Residuals from the treatment model obtained by fitting a linear model",
    "Initialising product_cor_metric_observed with a really low value as finding maximum",
    "The user has an option to give the the effect_strength_on_y and effect_strength_on_t which can be then used instead of maximum correlation with treatment and outcome in the observed variables as it specifies the desired effect.",
    "Choosing a c_star based on the data.",
    "The correlations stop increasing upon increasing c_star after a certain value, that is it plateaus and we choose the value of c_star to be the value it plateaus.",
    "Choosing c1 and c2 based on the hyperbolic relationship once c_star is chosen by going over various combinations of c1 and c2 values and choosing the combination which",
    "which maintains the minimum distance between the product of correlations of the simulated variable and the product of maximum correlations of one of the observed variables",
    "and additionally checks if the ratio of the weights are such that they maintain the ratio of the maximum possible observed coefficients within some confidence interval",
    "c1_final and c2_final are initialised to the values on the hyperbolic curve such that c1_final = c2_final  and c1_final*c2_final = c_star",
    "initialising min_distance_between_product_cor_metrics to be a value greater than 1",
    "Default value of the p value taken for the distribution",
    "Number of Trials: Number of cointosses to understand if a sample gets the treatment",
    "Mean of the Normal Distribution",
    "Standard Deviation of the Normal Distribution",
    "only permute is supported for iv methods",
    "We need to change the identified estimand",
    "We make a copy as a safety measure, we don't want to change the",
    "original DataFrame",
    "For IV methods, the estimating_instrument_names should also be",
    "changed. So we change it inside the estimate and then restore it",
    "back at the end of this method.",
    "Create a new column in the data by the name of placebo",
    "Sanity check the data",
    "Restoring the value of iv_instrument_name",
    "Note: We hardcode the estimate value to ZERO as we want to check if it falls in the distribution of the refuter",
    "Ideally we should expect that ZERO should fall in the distribution of the effect estimates as we have severed any causal",
    "relationship between the treatment and the outcome.",
    "Adding a new backdoor variable to the identified estimand",
    "We want to see if the estimate falls in the same distribution as the one generated by the refuter",
    "Ideally that should be the case as choosing a subset should not have a significant effect on the ability",
    "of the treatment to affect the outcome",
    "Reject H0",
    "a, b and c are all continuous variables",
    "a, b and c are all discrete variables",
    "c is set of continuous and binary variables and",
    "1. either a and b is continuous and the other is binary",
    "2. both a and b are binary",
    "c is discrete and",
    "either a or b is continuous and the other is discrete",
    "a and b are discrete and c is a mixture of discrete and continuous variables. We discretize c and calculate conditional mutual information",
    "original_treatment_name: : stores original variable names for labelling",
    "common_causes_map : maps the original variable names to variable names in OLS regression",
    "benchmark_common_causes: stores variable names in terms of regression model variables",
    "original_benchmark_covariates: stores original variable names for labelling",
    "estimate: estimate of regression",
    "degree_of_freedom: degree of freedom of error in regression",
    "standard_error: standard error in regression",
    "t_stats: Treatment coefficient t-value - measures how many standard errors the estimate is away from zero.",
    "partial_f2: value to determine if a regression model and a nested version of it have a statistically significant difference between them",
    "r2tu_w: partial R^2  of unobserved confounder \"u\" with treatment \"t\", after conditioning on observed covariates \"w\"",
    "r2yu_tw: partial R^2  of unobserved confounder \"u\" with outcome \"y\", after conditioning on observed covariates \"w\" and treatment \"t\"",
    "r2twj_w: partial R^2 of observed covariate wj with treatment \"t\", after conditioning on observed covariates \"w\" excluding wj",
    "r2ywj_tw:  partial R^2 of observed covariate wj with outcome \"y\", after conditioning on observed covariates \"w\" (excluding wj) and treatment \"t\"",
    "benchmarking_results: dataframe containing information about bounds and bias adjusted terms",
    "stats: dictionary containing information like robustness value, partial R^2, estimate, standard error , degree of freedom, partial f^2, t-statistic",
    "partial R^2 (r2yt_w) is the proportion of variation in outcome uniquely explained by treatment",
    "build a new regression model by considering treatment variables as outcome",
    "r2twj_w is partial R^2 of covariate wj with treatment \"t\", after conditioning on covariates w(excluding wj)",
    "r2ywj_tw is partial R^2 of covariate wj with outcome \"y\", after conditioning on covariates w(excluding wj) and treatment \"t\"",
    "r2tu_w is the partial r^2 from regressing u on t after conditioning on w",
    "Compute bias adjusted terms",
    "Plotting the contour plot",
    "Adding contours",
    "Adding threshold contour line",
    "Adding unadjusted point estimate",
    "Adding bounds to partial R^2 values for given strength of confounders",
    "from https://www.bnmetrics.com/blog/factory-pattern-in-python3-simple-version",
    "The default subset of the data to be used",
    "We want to see if the estimate falls in the same distribution as the one generated by the refuter",
    "Ideally that should be the case as choosing a subset should not have a significant effect on the ability",
    "of the treatment to affect the outcome",
    "Get adjacency list",
    "If node pair has been fully explored",
    "Add node1 to backdoor set of node_pair",
    "Check if path is backdoor and does not have nodes1\\node1 or nodes2\\node2 as intermediate nodes",
    "True if arrow incoming, False if arrow outgoing",
    "Mark pair (node1, node2) complete",
    "Modify variable count and indices covered",
    "Estimators list for returning after identification",
    "Line 1",
    "If no action has been taken, the effect on Y is just the marginal of the observational distribution P(v) on Y.",
    "Line 2",
    "If we are interested in the effect on Y, it is sufficient to restrict our attention on the parts of the model ancestral to Y.",
    "Modify list of valid nodes",
    "Line 3 - forces an action on any node where such an action would have no effect on Y \u2013 assuming we already acted on X.",
    "Modify adjacency matrix to obtain that corresponding to do(X)",
    "Line 4 - Decomposes the problem into a set of smaller problems using the key property of C-component factorization of causal models.",
    "If the entire graph is a single C-component already, further problem decomposition is impossible, and we must provide base cases.",
    "Modify adjacency matrix to remove treatment variables",
    "Line 5 - The algorithms fails due to the presence of a hedge - the graph G, and a subgraph S that does not contain any X nodes.",
    "Line 6 - If there are no bidirected arcs from X to the other nodes in the current subproblem under consideration, then we can replace acting on X by conditioning, and thus solve the subproblem.",
    "Line 7 - This is the most complicated case in the algorithm. Explain in the second last paragraph on Pg 41 of the link provided in the docstring above.",
    "from https://www.bnmetrics.com/blog/factory-pattern-in-python3-simple-version",
    "If the relative change of the score is less than the given threshold, we stop the estimation early.",
    "Note, the output of score_samples are log values.",
    "Note, the output of score_samples are log values.",
    "Currently only support continuous distributions for auto selection.",
    "Estimate distribution parameters from data.",
    "Ignore warnings from fitting process.",
    "Fit distribution to data.",
    "Some distributions might not be compatible with the data.",
    "Separate parts of parameters.",
    "Check the KL divergence between the distribution of the given and fitted distribution.",
    "Identify if this distribution is better.",
    "This error is typically raised when the data is discrete and all points are assigned to less cluster than",
    "specified. It can also happen due to duplicated points. In these cases, the current best solution should",
    "be sufficient.",
    "Usual feature relevance using the mean deviation as set function, i.e. g(x) - E[g(X)]",
    "A convenience function when computing confidence intervals specifically for non-deterministic causal queries. This",
    "function evaluates the provided causal query multiple times to build a confidence interval based on the returned",
    "results.",
    "Note that this function does not re-fit the causal model(s) and only executes the provided query as it is. In order",
    "to re-refit the graphical causal model on random subsets of the data before executing the query, consider using the",
    "bootstrap_training_and_sampling function.",
    "",
    "**Example usage:**",
    "",
    ">>> gcm.fit(causal_model, data)",
    ">>> strength_medians, strength_intervals = gcm.confidence_intervals(",
    ">>>     gcm.bootstrap_sampling(gcm.arrow_strength, causal_model, target_node='Y'))",
    "",
    "In this example, gcm.confidence_intervals is expecting a callable with non-deterministic outputs for building the",
    "confidence intervals. Since each causal query potentially expects a different set of parameters, we use 'partial'",
    "here to configure the function call. In this case,",
    "gcm.bootstrap_sampling(gcm.arrow_strength, causal_model, target_node='Y') would be equivalent to",
    "lambda : gcm.arrow_strength(causal_model, target_node='Y').",
    "",
    "In order to incorporate uncertainties coming from fitting the causal model(s), we can use",
    "gcm.bootstrap_training_and_sampling instead:",
    ">>>  strength_medians, strength_intervals = gcm.confidence_intervals(",
    ">>>        gcm.bootstrap_training_and_sampling(gcm.arrow_strength,",
    ">>>                                            causal_model,",
    ">>>                                            bootstrap_training_data=data,",
    ">>>                                            target_node='Y'))",
    "This would refit the provided causal_model on a subset of the data first before executing gcm.arrow_strength in each",
    "run.",
    "Simulating interventions by propagating the effects through the graph. For this, we iterate over the nodes based",
    "on their topological order.",
    "After drawing samples of the node based on the data generation process, we apply the corresponding",
    "intervention. The inputs of downstream nodes are therefore based on the outcome of the intervention in this",
    "node.",
    "Abduction: For invertible SCMs, we recover exact noise values from data.",
    "Action + Prediction: Propagate the intervention downstream using recovered noise values.",
    "Check if we need to apply an intervention on the given node.",
    "Apply intervention function to the data of the node.",
    "Check if the intervention function changes the shape of the data.",
    "For estimating the effect, we only need to consider the nodes that have a directed path to the target node, i.e.",
    "all ancestors of the target.",
    "The target node can be a continuous real-valued variable or a categorical variable with at most two classes",
    "(i.e. binary).",
    "Making sure there are at least 30% test samples.",
    "Making sure that there are at least 2 samples from one class (here, simply duplicate the point).",
    "Compare number of correct classifications.",
    "This constant is used as key when storing/accessing models as causal mechanisms in graph node attributes",
    "This constant is used as key when storing the parents of a node during fitting. It's used for validation purposes",
    "afterwards.",
    "can't use nx.node_connected_component, because it doesn't work with DiGraphs.",
    "Hence a manual loop:",
    "For estimating Shapley values for multiple samples (e.g. in feature relevance) and the number of samples",
    "is unknown beforehand.",
    "The method stops if either the change between some consecutive runs is below the given threshold or the",
    "maximum number of runs is reached.",
    "In each run, we create one random permutation of players. For instance, given 4 players, a permutation",
    "could be [3,1,4,2].",
    "Create all subsets belonging to the generated permutation. This is, if we have [3,1,4,2], then the",
    "subsets are [3], [3,1], [3,1,4] [3,1,4,2].",
    "The result for each subset is cached such that if a subset that has already been evaluated appears again,",
    "we can take this result directly.",
    "To improve the runtime, multiple permutations are evaluated in each run.",
    "The current Shapley values are the average of the estimated values, i.e. we need to divide by the number",
    "of generated permutations here.",
    "Here, the change between two runs is below the minimum threshold, but to reduce the likelihood",
    "that this just happened by chance, we require that this happens at least for two runs in a row.",
    "Create all (unique) subsets)",
    "Assigning a 'high' weight, since this resembles \"infinity\".",
    "The weight for a subset with a specific length (see paper mentioned in the docstring for more",
    "information).",
    "TODO: Add method for auto select a bin_width/width based on the data. Make sure that the auto selection method is",
    "theoretically sound, i.e. make entropy results from different data comparable.",
    "Extremely small values can somehow result in negative values.",
    "Creating a smaller subgraph, which only contains upstream nodes that are connected to the target node.",
    "Sampling from the conditional distribution based on the current sample.",
    "Sampling from the conditional based on the current sample, but randomizing the inputs of all variables that",
    "are in the given subset. By this, we can simulate the impact on the conditional distribution when removing",
    "only the incoming edges of the variables in the subset.",
    "Creating a smaller subgraph, which only contains upstream nodes that are connected to the target node.",
    "In case of the full subset (no randomization), we get the same predictions as when we apply the",
    "prediction method to the samples of interest, since all noise samples are replaced with a sample of",
    "interest.",
    "In case of the empty subset (all are jointly randomize), it boils down to taking the average over all",
    "predictions, seeing that the randomization yields the same values for each sample of interest (none of the",
    "samples of interest are used to replace a (jointly) 'randomized' sample).",
    "Smallest possible value. This is used in various algorithm for numerical stability.",
    "Make copy to avoid manipulating the original matrix.",
    "The given prediction method has to be evaluated multiple times on a large amount of different inputs. Typically,",
    "the batch evaluation of a prediction model on multiple inputs at the same time is significantly faster",
    "than evaluating it on single simples in a for-loop. To make use of this, we try to evaluate as many samples as",
    "possible in one batch call of the prediction method. However, this also requires a lot of memory for many samples.",
    "To overcome potential memory issues, multiple batch calls are performed, each with at most batch_size many",
    "samples. The number of samples that are evaluated is normally",
    "baseline_noise_samples.shape[0] * feature_samples.shape[0]. Here, we reduce it to",
    "batch_size * feature_samples.shape[0]. If the batch_size would be set 1, then each baseline_noise_samples is",
    "evaluated one by one in a for-loop.",
    "Each batch consist of at most batch_size * feature_samples.shape[0] many samples. If there are multiple",
    "batches, the offset indicates the index of the current baseline_noise_samples that has not been evaluated yet.",
    "If the batch size would be larger than the remaining amount of samples, it is reduced to only include the",
    "remaining baseline_noise_samples.",
    "The inputs consist of batch_size many copies of feature_samples. Here, we set the columns of the features",
    "in baseline_feature_indices to their respective values in baseline_noise_samples.",
    "After creating the (potentially large) input data matrix, we can evaluate the prediction method.",
    "Here, offset + index now indicates the sample index in baseline_noise_samples.",
    "This would average all prediction results obtained for the 'offset + index'-th sample in",
    "baseline_noise_samples. This is, y^(offset + index) = E[Y | do(x^(offset + index)_s)].",
    "This would return all prediction results obtained for the 'offset + index'-th sample in",
    "baseline_noise_samples, i.e. the results are not averaged.",
    "Making copy to ensure that the original object is not modified.",
    "Permute samples jointly. This still represents an interventional distribution.",
    "Permute samples independently.",
    "test local Markov condition, null hypothesis: conditional independence",
    "test edge dependence, null hypothesis: independence",
    "The order of the p-values added to the list is deterministic.",
    "To be able to validate that the graph structure did not change between fitting and causal query, we store the",
    "parents of a node during fit. That way, before sampling, we can verify the parents are still the same. While",
    "this would automatically fail when the number of parents is different, there are other more subtle cases,",
    "where the number is still the same, but it's different parents, and therefore different data. That would yield",
    "wrong results, but would not fail.",
    "Wrapping labels if they are too long",
    "Using the negative value here seeing that the Shapley estimation evaluates v(S u {i}) - v(S) for a subset S. In",
    "case of variance, we have v(S u {i}) <= v(S), which would result in a negative contribution of players to the",
    "target quantity (here, variance).",
    "Using the negative value here seeing that the Shapley estimation evaluates v(S u {i}) - v(S) for a subset S. In",
    "case of variance, we have v(S u {i}) <= v(S), which would result in a negative contribution of players to the",
    "target quantity (here, variance).",
    "TODO: This is a temporary workaround.",
    "Under some circumstances, the KCI test throws a \"numpy.linalg.LinAlgError: SVD did not converge\"",
    "error, depending on the data samples. This is related to the utilized algorithms by numpy for SVD.",
    "There is actually a robust version for SVD, but it is not included in numpy.",
    "This can either be addressed by some augmenting the data, using a different SVD implementation or",
    "wait until numpy updates the used algorithm.",
    "Not dividing by n, seeing that the expectation and variance are also not divided by n and n**2, respectively.",
    "Taking the sum, because due to numerical issues, the matrices might not be symmetric.",
    "Filter out eigenvalues that are too small.",
    "Test statistic is given as np.trace(K @ H @ L @ H) / n. Below computes without matrix products.",
    "Dividing by n not required since we do not divide the test statistical_tools by n.",
    "Estimate test statistic multiple times on different permutations of the data. The p-value is then the",
    "probability (i.e. fraction) of obtaining a test statistic that is greater than statistic on the non-permuted",
    "data.",
    "First stage statistical model",
    "Second stage statistical model",
    "Required to ensure that self.method_params contains all the",
    "parameters needed to create an object of this class",
    "Check if the treatment is one-dimensional",
    "First stage",
    "Second Stage",
    "Combining the two estimates",
    "This same estimate is valid for frontdoor as well as mediation (NIE)",
    "Total  effect of treatment",
    "Bulding the feature matrix",
    "features = sm.add_constant(features, has_constant='add') # to add an intercept term",
    "Required to ensure that self.method_params contains all the information",
    "to create an object of this class",
    "choosing the instrumental variable to use",
    "TODO move this to the identification step",
    "Obtain estimate by Wald Estimator",
    "Obtain estimate by 2SLS estimator: Cov(y,z) / Cov(x,z)",
    "More than 1 instrument. Use 2sls.",
    "Required to ensure that self.method_params contains all the",
    "parameters needed to create an object of this class",
    "Checking if Y is binary",
    "Required to ensure that self.method_params contains all the",
    "parameters to create an object of this class",
    "Enable the user to pass params for a custom propensity model",
    "Check if the treatment is one-dimensional",
    "Checking if the treatment is binary",
    "Convert the categorical variables into dummy/indicator variables",
    "Basically, this gives a one hot encoding for each category",
    "The first category is taken to be the base line.",
    "check if user provides the propensity score column",
    "Required to ensure that self.method_params contains all the information",
    "to create an object of this class",
    "TODO make treatment_value and control value also as local parameters",
    "Checking if the model is already trained",
    "The model is always built on the entire data",
    "All treatments are set to the same constant value",
    "Using all data by default",
    "Fixing treatment value to the specified value, if provided",
    "treatment_vals and data_df should have same number of rows",
    "Bulding the feature matrix",
    "The model is always built on the entire data",
    "Replacing treatment values by given x",
    "First, create interventional tensor in original space",
    "Then, use pandas to ensure that the dummies are assigned correctly for a categorical treatment",
    "Required to ensure that self.method_params contains all the",
    "parameters to create an object of this class",
    "The average treatment effect is a combination of different",
    "regression coefficients. Complicated to compute the confidence",
    "interval analytically. For example, if y=a + b1.t + b2.tx, then",
    "the average treatment effect is b1+b2.mean(x).",
    "Refer Gelman, Hill. ARM Book. Chapter 9",
    "http://www.stat.columbia.edu/~gelman/arm/chap9.pdf",
    "TODO: Looking for contributions",
    "For a linear regression model, the causal effect of a variable is equal to the coefficient corresponding to the",
    "variable. Hence, the model by default outputs the confidence interval corresponding to treatment=1 and control=0.",
    "So for custom treatment and control values, we must multiply the confidence interval by the difference of the two.",
    "For a linear regression model, the causal effect of a variable is equal to the coefficient corresponding to the",
    "variable. Hence, the model by default outputs the standard error corresponding to treatment=1 and control=0.",
    "So for custom treatment and control values, we must multiply the standard error by the difference of the two.",
    "Required to ensure that self.method_params contains all the information",
    "to create an object of this class",
    "setting method-specific parameters",
    "Infer the right strata based on clipping threshold",
    "0.5 because there are two values for the treatment",
    "To be conservative and allow most strata to be included in the",
    "analysis",
    "At least 90% of the strata should be included in analysis",
    "sum weighted outcomes over all strata  (weight by treated population)",
    "TODO - how can we add additional information into the returned estimate?",
    "such as how much clipping was done, or per-strata info for debugging?",
    "sort the dataframe by propensity score",
    "create a column 'strata' for each element that marks what strata it belongs to",
    "for each strata, count how many treated and control units there are",
    "throw away strata that have insufficient treatment or control",
    "TODO -- fix: we are actually conditioning on positive treatment (d=1)",
    "Required to ensure that self.method_params contains all the information",
    "to create an object of this class",
    "Setting method specific parameters",
    "trim propensity score weights",
    "ips ==> (isTreated(y)/ps(y)) + ((1-isTreated(y))/(1-ps(y)))",
    "nips ==> ips / (sum of ips over all units)",
    "icps ==> ps(y)/(1-ps(y)) / (sum of (ps(y)/(1-ps(y))) over all control units)",
    "itps ==> ps(y)/(1-ps(y)) / (sum of (ps(y)/(1-ps(y))) over all treatment units)",
    "Vanilla IPS estimator",
    "The Hajek estimator (or the self-normalized estimator)",
    "Stabilized weights (from Robins, Hernan, Brumback (2000))",
    "Paper: Marginal Structural Models and Causal Inference in Epidemiology",
    "Calculating the effect",
    "Subtracting the weighted means",
    "TODO - how can we add additional information into the returned estimate?",
    "TODO -- fix: we are actually conditioning on positive treatment (d=1)",
    "Required to ensure that self.method_params contains all the",
    "parameters to create an object of this class",
    "For metalearners only--issue a warning if w contains variables not in x",
    "Override the effect_modifiers set in CausalEstimator.__init__()",
    "Also only update self._effect_modifiers, and create a copy of self._effect_modifier_names",
    "the latter can be used by other estimator methods later",
    "Instrumental variables names, if present",
    "choosing the instrumental variable to use",
    "from https://www.bnmetrics.com/blog/factory-pattern-in-python3-simple-version",
    "Calling the econml estimator's fit method",
    "As of v0.9, econml has some kewyord only arguments",
    "Changing shape to a list for a singleton value",
    "TODO -- fix: we are actually conditioning on positive treatment (d=1)",
    "this assumes a binary treatment regime",
    "TODO remove neighbors that are more than a given radius apart",
    "estimate ATT on treated by summing over difference between matched neighbors",
    "Now computing ATC",
    "TODO -- fix: we are actually conditioning on positive treatment (d=1)",
    "from https://www.bnmetrics.com/blog/factory-pattern-in-python3-simple-version",
    "Handle externally provided estimator classes",
    "allowed types of distance metric",
    "Required to ensure that self.method_params contains all the",
    "parameters to create an object of this class",
    "Check if the treatment is one-dimensional",
    "Checking if the treatment is binary",
    "Convert the categorical variables into dummy/indicator variables",
    "Basically, this gives a one hot encoding for each category",
    "The first category is taken to be the base line.",
    "Dictionary of any user-provided params for the distance metric",
    "that will be passed to sklearn nearestneighbors",
    "this assumes a binary treatment regime",
    "TODO remove neighbors that are more than a given radius apart",
    "estimate ATT on treated by summing over difference between matched neighbors",
    "Return indices in the original dataframe",
    "self.matched_indices_att[treated_df_index[i]] = control.iloc[indices[i]].index.tolist()",
    "Now computing ATC",
    "Return indices in the original dataframe",
    "Required to ensure that self.method_params contains all the information",
    "to create an object of this class",
    "Add the identification method used in the estimator",
    "Check the backdoor variables being used",
    "Add the observed confounders and one hot encode the categorical variables",
    "Get the data of the unobserved confounders",
    "One hot encode the data if they are categorical",
    "Check the instrumental variables involved",
    "Perform the same actions as the above",
    "Check if effect modifiers are used",
    "Get the class corresponding the the estimator to be used",
    "Initialize the object",
    "Both the outcome and the treatment have to be 1D arrays according to the CausalML API",
    "We want to pass 'v0' rather than ['v0'] to prevent a shape mismatch",
    "For CATEs",
    "TODO we are conditioning on a postive treatment",
    "TODO create an expression corresponding to each estimator used",
    "Generating data with equal 0 and 1 (since ranks are uniformly distributed)",
    "Flipping some values",
    "Finding p-value using student T test",
    "Only consider edges have absolute edge weight > 0.01",
    "Modify graph such that it only contains bidirected edges",
    "Find c components by finding connected components on the undirected graph",
    "Understanding Neural Network weights",
    "Refer to this link:https://stackoverflow.com/questions/50937628/mlp-classifier-neurons-weights",
    "add weight column",
    "before weights are applied we count number rows in each category",
    "which is equivalent to summing over weight=1",
    "after weights are applied we need to sum over the given weights",
    "First, calculating mean differences by strata",
    "Second, without strata",
    "Third, concatenating them and plotting",
    "Setting estimator attribute for convenience",
    "Outcome is numeric",
    "Treatments are also numeric or binary",
    "Outcome is categorical",
    "Treatments are numeric or binary",
    "TODO: A common way to show all plots",
    "from https://www.bnmetrics.com/blog/factory-pattern-in-python3-simple-version",
    "self._identified_estimand = self._causal_model.identify_effect()",
    "self._identified_estimand,",
    "self._causal_model._treatment,",
    "self._causal_model._outcome,",
    "If labels provided",
    "Return in valid DOT format",
    "Get adjacency matrix",
    "If labels not provided",
    "Obtain valid DOT format",
    "If labels provided",
    "Return in valid DOT format",
    "from https://www.bnmetrics.com/blog/factory-pattern-in-python3-simple-version",
    "from https://www.bnmetrics.com/blog/factory-pattern-in-python3-simple-version"
  ],
  "v0.7.1": [
    "Get the long description from the README file",
    "Get the required packages",
    "Plotting packages are optional to install",
    "Loading version number",
    "-*- coding: utf-8 -*-",
    "",
    "Configuration file for the Sphinx documentation builder.",
    "",
    "This file does only contain a selection of the most common options. For a",
    "full list see the documentation:",
    "http://www.sphinx-doc.org/en/stable/config",
    "-- Path setup --------------------------------------------------------------",
    "If extensions (or modules to document with autodoc) are in another directory,",
    "add these directories to sys.path here. If the directory is relative to the",
    "documentation root, use os.path.abspath to make it absolute, like shown here.",
    "",
    "-- Project information -----------------------------------------------------",
    "The short X.Y version",
    "The full version, including alpha/beta/rc tags",
    "-- General configuration ---------------------------------------------------",
    "If your documentation needs a minimal Sphinx version, state it here.",
    "",
    "needs_sphinx = '1.0'",
    "Add any Sphinx extension module names here, as strings. They can be",
    "extensions coming with Sphinx (named 'sphinx.ext.*') or your custom",
    "ones.",
    "Add any paths that contain templates here, relative to this directory.",
    "The suffix(es) of source filenames.",
    "You can specify multiple suffix as a list of string:",
    "",
    "source_suffix = ['.rst', '.md']",
    "The master toctree document.",
    "The language for content autogenerated by Sphinx. Refer to documentation",
    "for a list of supported languages.",
    "",
    "This is also used if you do content translation via gettext catalogs.",
    "Usually you set \"language\" from the command line for these cases.",
    "List of patterns, relative to source directory, that match files and",
    "directories to ignore when looking for source files.",
    "This pattern also affects html_static_path and html_extra_path .",
    "The name of the Pygments (syntax highlighting) style to use.",
    "-- Options for HTML output -------------------------------------------------",
    "The theme to use for HTML and HTML Help pages.  See the documentation for",
    "a list of builtin themes.",
    "",
    "html_theme = 'sphinx-rtd-theme'",
    "on_rtd is whether we are on readthedocs.org",
    "only import and set the theme if we're building docs locally",
    "otherwise, readthedocs.org uses their theme by default, so no need to specify it",
    "Theme options are theme-specific and customize the look and feel of a theme",
    "further.  For a list of options available for each theme, see the",
    "documentation.",
    "",
    "html_theme_options = {}",
    "Add any paths that contain custom static files (such as style sheets) here,",
    "relative to this directory. They are copied after the builtin static files,",
    "so a file named \"default.css\" will overwrite the builtin \"default.css\".",
    "Custom sidebar templates, must be a dictionary that maps document names",
    "to template names.",
    "",
    "The default sidebars (for documents that don't match any pattern) are",
    "defined by theme itself.  Builtin themes are using these templates by",
    "default: ``['localtoc.html', 'relations.html', 'sourcelink.html',",
    "'searchbox.html']``.",
    "",
    "html_sidebars = {}",
    "-- Options for HTMLHelp output ---------------------------------------------",
    "Output file base name for HTML help builder.",
    "-- Options for LaTeX output ------------------------------------------------",
    "The paper size ('letterpaper' or 'a4paper').",
    "",
    "'papersize': 'letterpaper',",
    "The font size ('10pt', '11pt' or '12pt').",
    "",
    "'pointsize': '10pt',",
    "Additional stuff for the LaTeX preamble.",
    "",
    "'preamble': '',",
    "Latex figure (float) alignment",
    "",
    "'figure_align': 'htbp',",
    "Grouping the document tree into LaTeX files. List of tuples",
    "(source start file, target name, title,",
    "author, documentclass [howto, manual, or own class]).",
    "-- Options for manual page output ------------------------------------------",
    "One entry per manual page. List of tuples",
    "(source start file, name, description, authors, manual section).",
    "-- Options for Texinfo output ----------------------------------------------",
    "Grouping the document tree into Texinfo files. List of tuples",
    "(source start file, target name, title, author,",
    "dir menu entry, description, category)",
    "-- Options for Epub output -------------------------------------------------",
    "Bibliographic Dublin Core info.",
    "The unique identifier of the text. This can be a ISBN number",
    "or the project homepage.",
    "",
    "epub_identifier = ''",
    "A unique identification for the text.",
    "",
    "epub_uid = ''",
    "A list of files that should not be packed into the epub file.",
    "-- Extension configuration -------------------------------------------------",
    "-- Options for todo extension ----------------------------------------------",
    "If true, `todo` and `todoList` produce output, else they produce nothing.",
    "init docstrings should also be included in class",
    "requires stdin input for identify in weighting sampler",
    "requires Rpy2 for lalonde",
    "requires Rpy2 for causal discovery",
    "very slow",
    "will be removed",
    "applied notebook, not necessary to test each time",
    "Adding the dowhy root folder to the python path so that jupyter notebooks",
    "can import dowhy",
    "\"--ExecutePreprocessor.timeout=600\",",
    "self.df = pd.read_csv(os.path.join(DATA_PATH,'dgp_1/acic_1_1_data.csv'))",
    "self.ate = np.mean(self.df['y1'] - self.df['y0'])",
    "treated = self.df[self.df['z']==1]",
    "self.att = np.mean(treated['y1'] - treated['y0'])",
    "def test_average_treatment_effect(self):",
    "est_ate = 1",
    "bias = est_ate - self.ate",
    "print(bias)",
    "self.assertAlmostEqual(self.ate, est_ate)",
    "def test_average_treatment_effect_on_treated(self):",
    "est_att = 1",
    "self.att=1",
    "bias = est_att - self.att",
    "print(bias)",
    "self.assertAlmostEqual(self.att, est_att)",
    "removing two common causes",
    "removing two common causes",
    "removing two common causes",
    "removing two common causes",
    "The outcome is a linear function of the confounder",
    "The slope is 1,2 and the intercept is 3",
    "As we run with only one common cause and one instrument variable we run with (?, 2)",
    "Supports user-provided dataset object",
    "To test if there are any exceptions",
    "To test if the estimate is identical if refutation parameters are zero",
    "This value is hardcoded to be zero as we are runnning this on a linear dataset.",
    "Ordinarily, we should expect this value to be zero.",
    "This value is hardcoded to be zero as we are runnning this on a linear dataset.",
    "Ordinarily, we should expect this value to be zero.",
    "Calculate causal effect twice: once for unit (t=1, c=0), once for specific increase (t=100, c=50)",
    "Only P(Y|T) should be present for test to succeed.",
    "Calculate causal effect twice: once for unit (t=1, c=0), once for specific increase (t=100, c=50)",
    "Since undirected graph, identify effect must throw an error.",
    "Calculate causal effect twice: once for unit (t=1, c=0), once for specific increase (t=100, c=50)",
    "Compare with ground truth",
    "Calculate causal effect twice: once for unit (t=1, c=0), once for specific increase (t=100, c=50)",
    "Compare with ground truth",
    "Calculate causal effect twice: once for unit (t=1, c=0), once for specific increase (t=100, c=50)",
    "Compare with ground truth",
    "Calculate causal effect twice: once for unit (t=1, c=0), once for specific increase (t=100, c=50)",
    "Compare with ground truth",
    "Causal model initialization",
    "Causal identifier identification",
    "Obtain backdoor sets",
    "Check if backdoor sets are valid i.e. if they block all paths between the treatment and the outcome",
    "Causal model initialization",
    "Causal identifier identification",
    "Obtain backdoor sets",
    "Causal model initialization",
    "Causal identifier identification",
    "Obtain backdoor sets",
    "Check if backdoor sets are valid i.e. if they block all paths between the treatment and the outcome",
    "Causal model initialization",
    "Causal identifier identification",
    "Obtain backdoor sets",
    "Check if backdoor sets are valid i.e. if they block all paths between the treatment and the outcome",
    "Causal model initialization",
    "Causal identifier identification",
    "Obtain backdoor sets",
    "Check if backdoor sets are valid i.e. if they block all paths between the treatment and the outcome",
    "Causal model initialization",
    "Causal identifier identification",
    "Obtain backdoor sets",
    "Check if backdoor sets are valid i.e. if they block all paths between the treatment and the outcome",
    "Example is selected from Pearl J. \"Causality\" 2nd Edition, from chapter 3.3.1 on backoor criterion.",
    "The following simpsons paradox examples are taken from Pearl, J {2013}. \"Understanding Simpson\u2019s Paradox\" - http://ftp.cs.ucla.edu/pub/stat_ser/r414.pdf",
    "The following are examples given in the \"Book of Why\" by Judea Pearl, chapter \"The Do-operator and the Back-Door Criterion\"",
    "cov_mat = np.diag(np.ones(num_features))",
    "Setup data",
    "Test LinearDML",
    "Test ContinuousTreatmentOrthoForest",
    "Test LinearDRLearner",
    "Setup data",
    "Test DeepIV",
    "Treatment model,",
    "Response model",
    "Test IntentToTreatDRIV",
    "Not using testsuite from .base/TestEstimtor, custom code below",
    "More cases where Exception  is expected",
    "Compute confidence intervals, standard error and significance tests",
    "Defined a linear dataset with a given set of properties",
    "Create a model that captures the same",
    "Identify the effects within the model",
    "Defined a linear dataset with a given set of properties",
    "Create a model that captures the same",
    "Identify the effects within the model",
    "Defined a linear dataset with a given set of properties",
    "Create a model that captures the same",
    "Identify the effects within the model",
    "Defined a linear dataset with a given set of properties",
    "Create a model that captures the same",
    "Identify the effects within the model",
    "Defined a linear dataset with a given set of properties",
    "Create a model that captures the same",
    "Identify the effects within the model",
    "Backdoor method names",
    "First, check if there is a directed path from action to outcome",
    "## 1. BACKDOOR IDENTIFICATION",
    "First, checking if there are any valid backdoor adjustment sets",
    "Setting default \"backdoor\" identification adjustment set",
    "## 2. INSTRUMENTAL VARIABLE IDENTIFICATION",
    "Now checking if there is also a valid iv estimand",
    "## 3. FRONTDOOR IDENTIFICATION",
    "Now checking if there is a valid frontdoor variable",
    "Finally returning the estimand object",
    "## 1. FIRST DOING BACKDOOR IDENTIFICATION",
    "First, checking if there are any valid backdoor adjustment sets",
    "Setting default \"backdoor\" identification adjustment set",
    "## 2. SECOND, CHECKING FOR MEDIATORS",
    "Now checking if there are valid mediator variables",
    "Finally returning the estimand object",
    "## 1. FIRST DOING BACKDOOR IDENTIFICATION",
    "First, checking if there are any valid backdoor adjustment sets",
    "Setting default \"backdoor\" identification adjustment set",
    "## 2. SECOND, CHECKING FOR MEDIATORS",
    "Now checking if there are valid mediator variables",
    "Finally returning the estimand object",
    "First, checking if empty set is a valid backdoor set",
    "If the method is `minimal-adjustment`, return the empty set right away.",
    "Second, checking for all other sets of variables. If include_unobserved is false, then only observed variables are eligible.",
    "If var is d-separated from both treatment or outcome, it cannot",
    "be a part of the backdoor set",
    "repeat the above search with BACKDOOR_MIN",
    "If `minimal-adjustment` method is specified, start the search from the set with minimum size. Otherwise, start from the largest.",
    "If the backdoor method is `maximal-adjustment` or `minimal-adjustment`, return the first found adjustment set.",
    "If all variables are observed, and the biggest eligible set",
    "does not satisfy backdoor, then none of its subsets will.",
    "Adding a None estimand if no backdoor set found",
    "Default set contains minimum possible number of instrumental variables, to prevent lowering variance in the treatment variable.",
    "Default set is the one with the least number of adjustment variables (optimizing for efficiency)",
    "For simplicity, assuming a one-variable frontdoor set",
    "Cond 1: All directed paths intercepted by candidate_var",
    "Cond 2: No confounding between treatment and candidate var",
    "Cond 3: treatment blocks all confounding between candidate_var and outcome",
    "For simplicity, assuming a one-variable mediation set",
    "Create estimands dict as per the API for backdoor, but do not return it",
    "Setting default \"backdoor\" identification adjustment set",
    "Create estimands dict as per the API for backdoor, but do not return it",
    "Setting default \"backdoor\" identification adjustment set",
    "TODO: outputs string for now, but ideally should do symbolic",
    "expressions Mon 19 Feb 2018 04:54:17 PM DST",
    "TODO Better support for multivariate treatments",
    "TODO: support multivariate treatments better.",
    "TODO: support multivariate treatments better.",
    "TODO: support multivariate treatments better.",
    "For direct effect",
    "Do not show backdoor key unless it is the only backdoor set.",
    "Just show the default backdoor set",
    "Can use these lists to specify the models/estimators/refuters that a particular interpreter supports.  Throw a ValueError if the user provides an incompatible object to intepret.",
    "Unpacking the keyword arguments",
    "Default value for the number of simulations to be conducted",
    "Concatenate the confounders, instruments and effect modifiers",
    "Shuffle the confounders",
    "Check if all are select or deselect variables",
    "Check if all the required_variables belong to confounders, instrumental variables or effect",
    "Initializing the p_value",
    "Perform Bootstrap Significance Test with the original estimate and the set of refutations",
    "Perform Normal Tests of Significance with the original estimate and the set of refutations",
    "Perform Bootstrap Significance Test with the original estimate and the set of refutations",
    "Perform Normal Tests of Significance with the original estimate and the set of refutations",
    "Get the number of simulations",
    "Sort the simulations",
    "Obtain the median value",
    "Performing a two sided test",
    "np.searchsorted tells us the index if it were a part of the array",
    "We select side to be left as we want to find the first value that matches",
    "We subtact 1 as we are finding the value from the right tail",
    "We take the side to be right as we want to find the last index that matches",
    "We get the probability with respect to the left tail.",
    "Get the mean for the simulations",
    "Get the standard deviation for the simulations",
    "Get the Z Score [(val - mean)/ std_dev ]",
    "re.sub only takes string parameter so the first if is to avoid error",
    "if the input is a text file, convert the contained data into string",
    "load dot file",
    "Adding node attributes",
    "adding penwidth to make the edge bold",
    "Adding common causes",
    "Adding instruments",
    "Adding effect modifiers",
    "Assuming the simple form of effect modifier",
    "that directly causes the outcome.",
    "self._graph.add_edge(node_name, outcome, style = \"dotted\", headport=\"s\", tailport=\"n\")",
    "self._graph.add_edge(outcome, node_name, style = \"dotted\", headport=\"n\", tailport=\"s\") # TODO make the ports more general so that they apply not just to top-bottom node configurations",
    "Adding columns in the dataframe as confounders that were not in the graph",
    "Adding unobserved confounders",
    "also return the number of backdoor paths blocked by observed nodes",
    "Assume that nodes1 is the treatment",
    "ignores new_graph parameter, always uses self._graph",
    "remove paths that have nodes1\\node1 or nodes2\\node2 as intermediate nodes",
    "TODO Refactor to remove this from here and only implement this logic in causalIdentifier. Unnecessary assumption of nodes1 to be causing nodes2.",
    "Cannot simply compute ancestors, since that will also include nodes1 and its parents (e.g. instruments)",
    "removing all mediators",
    "[TODO: double check these work with multivariate implementation:]",
    "Exclusion",
    "As-if-random setup",
    "As-if-random",
    "convert the outputted generator into a list",
    "dpaths = self.get_all_directed_paths(nodes1, nodes2)",
    "return len(dpaths) > 0",
    "Condition 1: node 1 ---> node 2 is intercepted by candidate_nodes",
    "Create causal graph object",
    "Sometimes, effect modifiers from the graph may not match those provided by the user.",
    "(Because some effect modifiers may also be common causes)",
    "In such cases, the user-provided modifiers are used.",
    "If no effect modifiers are provided,  then the ones from the graph are used.",
    "Import causal discovery class",
    "Initialize causal graph object",
    "TODO add propensity score as default backdoor method, iv as default iv method, add an informational message to show which method has been selected.",
    "TODO add dowhy as a prefix to all dowhy estimators",
    "This is done as all dowhy estimators have two parts and external ones have two or more parts",
    "Define the third-party estimation method to be used",
    "Process the dowhy estimators",
    "Check if estimator's target estimand is identified",
    "Estimator had been computed in a previous call",
    "Store parameters inside estimate object for refutation methods",
    "TODO: This add_params needs to move to the estimator class",
    "inside estimate_effect and estimate_conditional_effect",
    "Check if estimator's target estimand is identified",
    "Note that while the name of the variable is the same,",
    "\"self.causal_estimator\", this estimator takes in less",
    "parameters than the same from the",
    "estimate_effect code. It is not advisable to use the",
    "estimator from this function to call estimate_effect",
    "with fit_estimator=False.",
    "Estimator had been computed in a previous call",
    "The default number of simulations for statistical testing",
    "The default number of simulations to obtain confidence intervals",
    "The portion of the total size that should be taken each time to find the confidence intervals",
    "1 is the recommended value",
    "https://ocw.mit.edu/courses/mathematics/18-05-introduction-to-probability-and-statistics-spring-2014/readings/MIT18_05S14_Reading24.pdf",
    "https://projecteuclid.org/download/pdf_1/euclid.ss/1032280214",
    "The default Confidence Level",
    "Number of quantiles to discretize continuous columns, for applying groupby",
    "Prefix to add to temporary categorical variables created after discretization",
    "std args to be removed from locals() before being passed to args_dict",
    "Currently estimation methods only support univariate treatment and outcome",
    "Setting the default interpret method",
    "Setting treatment and outcome values",
    "Now saving the effect modifiers",
    "only add the observed nodes",
    "Check if some parameters were set, otherwise set to default values",
    "Estimate conditional estimates by default",
    "names of treatment and outcome",
    "TODO Only works for binary treatment",
    "Defaulting to class default values if parameters are not provided",
    "Checking that there is at least one effect modifier",
    "Making sure that effect_modifier_names is a list",
    "Making a copy since we are going to be changing effect modifier names",
    "For every numeric effect modifier, adding a temp categorical column",
    "Grouping by effect modifiers and computing effect separately",
    "Deleting the temporary categorical columns",
    "The array that stores the results of all estimations",
    "Find the sample size the proportion with the population size",
    "Perform the set number of simulations",
    "names of treatment and outcome",
    "Using class default parameters if not specified",
    "Checking if bootstrap_estimates are already computed",
    "Checked if any parameter is changed from the previous std error estimate",
    "Now use the data obtained from the simulations to get the value of the confidence estimates",
    "Get the variations of each bootstrap estimate and sort",
    "Now we take the (1- p)th and the (p)th variations, where p is the chosen confidence level",
    "Get the lower and upper bounds by subtracting the variations from the estimate",
    "Use existing params, if new user defined params are not present",
    "Checking if bootstrap_estimates are already computed",
    "Check if any parameter is changed from the previous std error estimate",
    "Use existing params, if new user defined params are not present",
    "self._outcome = self._data[\"dummy_outcome\"]",
    "Processing the null hypothesis estimates",
    "Doing a two-sided test",
    "Being conservative with the p-value reported",
    "Being conservative with the p-value reported",
    "If the estimate_index is 0, it depends on the number of simulations",
    "Need to test r-squared before supporting",
    "effect_r_squared = self._evaluate_effect_strength(estimate, method=\"r-squared\")",
    "'r-squared': effect_r_squared",
    "elif method == \"r-squared\":",
    "outcome_mean = np.mean(self._outcome)",
    "total_variance = np.sum(np.square(self._outcome - outcome_mean))",
    "Assuming a linear model with one variable: the treatment",
    "Currently only works for continuous y",
    "causal_model = outcome_mean + estimate.value*self._treatment",
    "squared_residual = np.sum(np.square(self._outcome - causal_model))",
    "r_squared = 1 - (squared_residual/total_variance)",
    "return r_squared",
    "No estimand was identified (identification failed)",
    "s += \"Variance in outcome explained by treatment: {}\\n\".format(self.effect_strength[\"r-squared\"])",
    "Below loop assumes that the last indices of W are alwawys converted to discrete",
    "one-hot encode discrete W",
    "Now deleting the old continuous value",
    "Making beta an array",
    "TODO Ensure that we do not generate weak instruments",
    "TODO - test all our methods with random noise added to covariates (instead of the stochastic treatment assignment)",
    "Converting treatment to binary if required",
    "Generating frontdoor variables if asked for",
    "NOTE: We are assuming a linear relationship *even when t is categorical* and integer coded.",
    "For categorical t, this example dataset has the effect size for category 2 being exactly",
    "double the effect for category 1",
    "This could be changed at this stage by one-hot encoding t and using a custom beta that",
    "sets a different effect for each category {0, 1, 2}",
    "Computing ATE",
    "constructing column names for one-hot encoded discrete features",
    "Specifying the correct dtypes",
    "Now specifying the corresponding graph strings",
    "Now writing the gml graph",
    "Making beta an array",
    "creating data frame",
    "Specifying the correct dtypes",
    "Now specifying the corresponding graph strings",
    "Now writing the gml graph",
    "Adding edges between common causes and the frontdoor mediator",
    "Error terms",
    "else:",
    "V = 6 + W0 + tterm + E1",
    "Y = 12 + W0*W0 + W0*W0 + yterm + E2  # E2_new",
    "Generating a random normal distribution of integers",
    "Generating data for nodes which have no incoming edges",
    "\"currset\" variable currently has all the successors of the nodes which had no incoming edges",
    "Loading version number",
    "The currently supported estimators",
    "The default standard deviation for noise",
    "The default scaling factor to determine the bucket size",
    "The minimum number of points for the estimator to run",
    "The Default Transformation, when no arguments are given, or if the number of data points are insufficient for an estimator",
    "The Default True Causal Effect, this is taken to be ZERO by default",
    "The Default split for the number of data points that fall into the training and validation sets",
    "Assuming that outcome is one-dimensional",
    "We need to change the identified estimand",
    "We thus, make a copy. This is done as we don't want",
    "to change the original DataFrame",
    "We use collections.OrderedDict to maintain the order in which the data is stored",
    "Check if we are using an estimator in the transformation list",
    "The rationale behind ordering of the loops is the fact that we induce randomness everytime we create the",
    "Train and the Validation Datasets. Thus, we run the simulation loop followed by the training and the validation",
    "loops. Thus, we can get different values everytime we get the estimator.",
    "Warn the user that the specified parameter is not applicable when no estimator is present in the transformation",
    "Adding an unobserved confounder if provided by the user",
    "We set X_train = 0 and outcome_train to be 0",
    "Get the final outcome, after running through all the values in the transformation list",
    "Check if the value of true effect has been already stored",
    "We use None as the key as we have no base category for this refutation",
    "As we currently support only one treatment",
    "If the number of data points is too few, run the default transformation: [(\"zero\",\"\"),(\"noise\", {'std_dev':1} )]",
    "Check if the value of true effect has been already stored",
    "This ensures that we calculate the causal effect only once.",
    "We use key_train as we map data with respect to the base category of the data",
    "As we currently support only one treatment",
    "Add h(t) to f(W) to get the dummy outcome",
    "We convert to ndarray for ease in indexing",
    "The data is of the form",
    "sim1: cat1 cat2 ... catn",
    "sim2: cat1 cat2 ... catn",
    "Note: We would like the causal_estimator to find the true causal estimate that we have specified through this",
    "refuter. Let the value of the true causal effect be h(t). In the following section of code, we wish to find out if h(t) falls in the",
    "distribution of the refuter.",
    "True Causal Effect list",
    "Iterating through the refutation for each category",
    "We use string arguments to account for both 32 and 64 bit varaibles",
    "action for continuous variables",
    "Action for categorical variables",
    "Find the set difference for each row",
    "Choose one out of the remaining",
    "We want to see if the estimate falls in the same distribution as the one generated by the refuter",
    "Ideally that should be the case as running bootstrap should not have a significant effect on the ability",
    "of the treatment to affect the outcome",
    "Standardizing the data",
    "Fit a model containing all confounders and compare predictions",
    "using all features compared to all features except a given",
    "confounder.",
    "Estimating the regression coefficient from standardized features to t",
    "By default, return a plot with 10 points",
    "consider 10 values of the effect of the unobserved confounder",
    "Standardizing the data",
    "Fit a model containing all confounders and compare predictions",
    "using all features compared to all features except a given",
    "confounder.",
    "By default, return a plot with 10 points",
    "consider 10 values of the effect of the unobserved confounder",
    "Get a 2D matrix of values",
    "x,y =  np.meshgrid(self.kappa_t, self.kappa_y) # x,y are both MxN",
    "Store the values into the refute object",
    "Adding a label on the contour line for the original estimate",
    "Label every other level using strings",
    "By default, we add the effect of simulated confounder for treatment.",
    "But subtract it from outcome to create a negative correlation",
    "assuming that the original confounder's effect was positive on both.",
    "This is to remove the effect of the original confounder.",
    "By default, we add the effect of simulated confounder for treatment.",
    "But subtract it from outcome to create a negative correlation",
    "assuming that the original confounder's effect was positive on both.",
    "This is to remove the effect of the original confounder.",
    "Obtaining the list of observed variables",
    "Taking a subset of the dataframe that has only observed variables",
    "Residuals from the outcome model obtained by fitting a linear model",
    "Residuals from the treatment model obtained by fitting a linear model",
    "Initialising product_cor_metric_observed with a really low value as finding maximum",
    "The user has an option to give the the effect_strength_on_y and effect_strength_on_t which can be then used instead of maximum correlation with treatment and outcome in the observed variables as it specifies the desired effect.",
    "Choosing a c_star based on the data.",
    "The correlations stop increasing upon increasing c_star after a certain value, that is it plateaus and we choose the value of c_star to be the value it plateaus.",
    "Choosing c1 and c2 based on the hyperbolic relationship once c_star is chosen by going over various combinations of c1 and c2 values and choosing the combination which",
    "which maintains the minimum distance between the product of correlations of the simulated variable and the product of maximum correlations of one of the observed variables",
    "and additionally checks if the ratio of the weights are such that they maintain the ratio of the maximum possible observed coefficients within some confidence interval",
    "c1_final and c2_final are initialised to the values on the hyperbolic curve such that c1_final = c2_final  and c1_final*c2_final = c_star",
    "initialising min_distance_between_product_cor_metrics to be a value greater than 1",
    "Default value of the p value taken for the distribution",
    "Number of Trials: Number of cointosses to understand if a sample gets the treatment",
    "Mean of the Normal Distribution",
    "Standard Deviation of the Normal Distribution",
    "only permute is supported for iv methods",
    "We need to change the identified estimand",
    "We make a copy as a safety measure, we don't want to change the",
    "original DataFrame",
    "For IV methods, the estimating_instrument_names should also be",
    "changed. So we change it inside the estimate and then restore it",
    "back at the end of this method.",
    "Create a new column in the data by the name of placebo",
    "Sanity check the data",
    "Restoring the value of iv_instrument_name",
    "Note: We hardcode the estimate value to ZERO as we want to check if it falls in the distribution of the refuter",
    "Ideally we should expect that ZERO should fall in the distribution of the effect estimates as we have severed any causal",
    "relationship between the treatment and the outcome.",
    "Adding a new backdoor variable to the identified estimand",
    "We want to see if the estimate falls in the same distribution as the one generated by the refuter",
    "Ideally that should be the case as choosing a subset should not have a significant effect on the ability",
    "of the treatment to affect the outcome",
    "Reject H0",
    "a, b and c are all continuous variables",
    "a, b and c are all discrete variables",
    "c is set of continuous and binary variables and",
    "1. either a and b is continuous and the other is binary",
    "2. both a and b are binary",
    "c is discrete and",
    "either a or b is continuous and the other is discrete",
    "a and b are discrete and c is a mixture of discrete and continuous variables. We discretize c and calculate conditional mutual information",
    "from https://www.bnmetrics.com/blog/factory-pattern-in-python3-simple-version",
    "The default subset of the data to be used",
    "We want to see if the estimate falls in the same distribution as the one generated by the refuter",
    "Ideally that should be the case as choosing a subset should not have a significant effect on the ability",
    "of the treatment to affect the outcome",
    "Get adjacency list",
    "If node pair has been fully explored",
    "Add node1 to backdoor set of node_pair",
    "Check if path is backdoor and does not have nodes1\\node1 or nodes2\\node2 as intermediate nodes",
    "True if arrow incoming, False if arrow outgoing",
    "Mark pair (node1, node2) complete",
    "Modify variable count and indices covered",
    "Estimators list for returning after identification",
    "Line 1",
    "If no action has been taken, the effect on Y is just the marginal of the observational distribution P(v) on Y.",
    "Line 2",
    "If we are interested in the effect on Y, it is sufficient to restrict our attention on the parts of the model ancestral to Y.",
    "Modify list of valid nodes",
    "Line 3 - forces an action on any node where such an action would have no effect on Y \u2013 assuming we already acted on X.",
    "Modify adjacency matrix to obtain that corresponding to do(X)",
    "Line 4 - Decomposes the problem into a set of smaller problems using the key property of C-component factorization of causal models.",
    "If the entire graph is a single C-component already, further problem decomposition is impossible, and we must provide base cases.",
    "Modify adjacency matrix to remove treatment variables",
    "Line 5 - The algorithms fails due to the presence of a hedge - the graph G, and a subgraph S that does not contain any X nodes.",
    "Line 6 - If there are no bidirected arcs from X to the other nodes in the current subproblem under consideration, then we can replace acting on X by conditioning, and thus solve the subproblem.",
    "Line 7 - This is the most complicated case in the algorithm. Explain in the second last paragraph on Pg 41 of the link provided in the docstring above.",
    "from https://www.bnmetrics.com/blog/factory-pattern-in-python3-simple-version",
    "First stage statistical model",
    "Second stage statistical model",
    "Required to ensure that self.method_params contains all the",
    "parameters needed to create an object of this class",
    "Check if the treatment is one-dimensional",
    "First stage",
    "Second Stage",
    "Combining the two estimates",
    "This same estimate is valid for frontdoor as well as mediation (NIE)",
    "Total  effect of treatment",
    "Bulding the feature matrix",
    "features = sm.add_constant(features, has_constant='add') # to add an intercept term",
    "Required to ensure that self.method_params contains all the information",
    "to create an object of this class",
    "choosing the instrumental variable to use",
    "TODO move this to the identification step",
    "Obtain estimate by Wald Estimator",
    "Obtain estimate by 2SLS estimator: Cov(y,z) / Cov(x,z)",
    "More than 1 instrument. Use 2sls.",
    "Required to ensure that self.method_params contains all the",
    "parameters needed to create an object of this class",
    "Checking if Y is binary",
    "Required to ensure that self.method_params contains all the",
    "parameters to create an object of this class",
    "Enable the user to pass params for a custom propensity model",
    "Check if the treatment is one-dimensional",
    "Checking if the treatment is binary",
    "Convert the categorical variables into dummy/indicator variables",
    "Basically, this gives a one hot encoding for each category",
    "The first category is taken to be the base line.",
    "Required to ensure that self.method_params contains all the information",
    "to create an object of this class",
    "TODO make treatment_value and control value also as local parameters",
    "Checking if the model is already trained",
    "The model is always built on the entire data",
    "All treatments are set to the same constant value",
    "Using all data by default",
    "Fixing treatment value to the specified value, if provided",
    "treatment_vals and data_df should have same number of rows",
    "Bulding the feature matrix",
    "The model is always built on the entire data",
    "Replacing treatment values by given x",
    "First, create interventional tensor in original space",
    "Then, use pandas to ensure that the dummies are assigned correctly for a categorical treatment",
    "Required to ensure that self.method_params contains all the",
    "parameters to create an object of this class",
    "The average treatment effect is a combination of different",
    "regression coefficients. Complicated to compute the confidence",
    "interval analytically. For example, if y=a + b1.t + b2.tx, then",
    "the average treatment effect is b1+b2.mean(x).",
    "Refer Gelman, Hill. ARM Book. Chapter 9",
    "http://www.stat.columbia.edu/~gelman/arm/chap9.pdf",
    "TODO: Looking for contributions",
    "For a linear regression model, the causal effect of a variable is equal to the coefficient corresponding to the",
    "variable. Hence, the model by default outputs the confidence interval corresponding to treatment=1 and control=0.",
    "So for custom treatment and control values, we must multiply the confidence interval by the difference of the two.",
    "For a linear regression model, the causal effect of a variable is equal to the coefficient corresponding to the",
    "variable. Hence, the model by default outputs the standard error corresponding to treatment=1 and control=0.",
    "So for custom treatment and control values, we must multiply the standard error by the difference of the two.",
    "Required to ensure that self.method_params contains all the information",
    "to create an object of this class",
    "setting method-specific parameters",
    "check if the user provides the propensity score column",
    "Infer the right strata based on clipping threshold",
    "0.5 because there are two values for the treatment",
    "To be conservative and allow most strata to be included in the",
    "analysis",
    "At least 90% of the strata should be included in analysis",
    "sum weighted outcomes over all strata  (weight by treated population)",
    "TODO - how can we add additional information into the returned estimate?",
    "such as how much clipping was done, or per-strata info for debugging?",
    "sort the dataframe by propensity score",
    "create a column 'strata' for each element that marks what strata it belongs to",
    "for each strata, count how many treated and control units there are",
    "throw away strata that have insufficient treatment or control",
    "TODO -- fix: we are actually conditioning on positive treatment (d=1)",
    "Required to ensure that self.method_params contains all the information",
    "to create an object of this class",
    "Setting method specific parameters",
    "check if user provides the propensity score column",
    "trim propensity score weights",
    "ips ==> (isTreated(y)/ps(y)) + ((1-isTreated(y))/(1-ps(y)))",
    "nips ==> ips / (sum of ips over all units)",
    "icps ==> ps(y)/(1-ps(y)) / (sum of (ps(y)/(1-ps(y))) over all control units)",
    "itps ==> ps(y)/(1-ps(y)) / (sum of (ps(y)/(1-ps(y))) over all treatment units)",
    "Vanilla IPS estimator",
    "The Hajek estimator (or the self-normalized estimator)",
    "Stabilized weights (from Robins, Hernan, Brumback (2000))",
    "Paper: Marginal Structural Models and Causal Inference in Epidemiology",
    "Calculating the effect",
    "Subtracting the weighted means",
    "TODO - how can we add additional information into the returned estimate?",
    "TODO -- fix: we are actually conditioning on positive treatment (d=1)",
    "Required to ensure that self.method_params contains all the",
    "parameters to create an object of this class",
    "For metalearners only--issue a warning if w contains variables not in x",
    "Override the effect_modifiers set in CausalEstimator.__init__()",
    "Also only update self._effect_modifiers, and create a copy of self._effect_modifier_names",
    "the latter can be used by other estimator methods later",
    "Instrumental variables names, if present",
    "choosing the instrumental variable to use",
    "from https://www.bnmetrics.com/blog/factory-pattern-in-python3-simple-version",
    "Calling the econml estimator's fit method",
    "As of v0.9, econml has some kewyord only arguments",
    "Changing shape to a list for a singleton value",
    "TODO -- fix: we are actually conditioning on positive treatment (d=1)",
    "check if the user provides a propensity score column",
    "this assumes a binary treatment regime",
    "TODO remove neighbors that are more than a given radius apart",
    "estimate ATT on treated by summing over difference between matched neighbors",
    "Now computing ATC",
    "TODO -- fix: we are actually conditioning on positive treatment (d=1)",
    "from https://www.bnmetrics.com/blog/factory-pattern-in-python3-simple-version",
    "Handle externally provided estimator classes",
    "allowed types of distance metric",
    "Required to ensure that self.method_params contains all the",
    "parameters to create an object of this class",
    "Check if the treatment is one-dimensional",
    "Checking if the treatment is binary",
    "Convert the categorical variables into dummy/indicator variables",
    "Basically, this gives a one hot encoding for each category",
    "The first category is taken to be the base line.",
    "Dictionary of any user-provided params for the distance metric",
    "that will be passed to sklearn nearestneighbors",
    "this assumes a binary treatment regime",
    "TODO remove neighbors that are more than a given radius apart",
    "estimate ATT on treated by summing over difference between matched neighbors",
    "Return indices in the original dataframe",
    "self.matched_indices_att[treated_df_index[i]] = control.iloc[indices[i]].index.tolist()",
    "Now computing ATC",
    "Return indices in the original dataframe",
    "Required to ensure that self.method_params contains all the information",
    "to create an object of this class",
    "Add the identification method used in the estimator",
    "Check the backdoor variables being used",
    "Add the observed confounders and one hot encode the categorical variables",
    "Get the data of the unobserved confounders",
    "One hot encode the data if they are categorical",
    "Check the instrumental variables involved",
    "Perform the same actions as the above",
    "Check if effect modifiers are used",
    "Get the class corresponding the the estimator to be used",
    "Initialize the object",
    "Both the outcome and the treatment have to be 1D arrays according to the CausalML API",
    "We want to pass 'v0' rather than ['v0'] to prevent a shape mismatch",
    "For CATEs",
    "TODO we are conditioning on a postive treatment",
    "TODO create an expression corresponding to each estimator used",
    "Generating data with equal 0 and 1 (since ranks are uniformly distributed)",
    "Flipping some values",
    "Finding p-value using student T test",
    "Only consider edges have absolute edge weight > 0.01",
    "Modify graph such that it only contains bidirected edges",
    "Find c components by finding connected components on the undirected graph",
    "Understanding Neural Network weights",
    "Refer to this link:https://stackoverflow.com/questions/50937628/mlp-classifier-neurons-weights",
    "add weight column",
    "before weights are applied we count number rows in each category",
    "which is equivalent to summing over weight=1",
    "after weights are applied we need to sum over the given weights",
    "First, calculating mean differences by strata",
    "Second, without strata",
    "Third, concatenating them and plotting",
    "Setting estimator attribute for convenience",
    "Outcome is numeric",
    "Treatments are also numeric or binary",
    "Outcome is categorical",
    "Treatments are numeric or binary",
    "TODO: A common way to show all plots",
    "from https://www.bnmetrics.com/blog/factory-pattern-in-python3-simple-version",
    "self._identified_estimand = self._causal_model.identify_effect()",
    "self._identified_estimand,",
    "self._causal_model._treatment,",
    "self._causal_model._outcome,",
    "If labels provided",
    "Return in valid DOT format",
    "Get adjacency matrix",
    "If labels not provided",
    "Obtain valid DOT format",
    "If labels provided",
    "Return in valid DOT format",
    "from https://www.bnmetrics.com/blog/factory-pattern-in-python3-simple-version",
    "from https://www.bnmetrics.com/blog/factory-pattern-in-python3-simple-version"
  ],
  "v0.7": [
    "Get the long description from the README file",
    "Get the required packages",
    "Plotting packages are optional to install",
    "Loading version number",
    "-*- coding: utf-8 -*-",
    "",
    "Configuration file for the Sphinx documentation builder.",
    "",
    "This file does only contain a selection of the most common options. For a",
    "full list see the documentation:",
    "http://www.sphinx-doc.org/en/stable/config",
    "-- Path setup --------------------------------------------------------------",
    "If extensions (or modules to document with autodoc) are in another directory,",
    "add these directories to sys.path here. If the directory is relative to the",
    "documentation root, use os.path.abspath to make it absolute, like shown here.",
    "",
    "-- Project information -----------------------------------------------------",
    "The short X.Y version",
    "The full version, including alpha/beta/rc tags",
    "-- General configuration ---------------------------------------------------",
    "If your documentation needs a minimal Sphinx version, state it here.",
    "",
    "needs_sphinx = '1.0'",
    "Add any Sphinx extension module names here, as strings. They can be",
    "extensions coming with Sphinx (named 'sphinx.ext.*') or your custom",
    "ones.",
    "Add any paths that contain templates here, relative to this directory.",
    "The suffix(es) of source filenames.",
    "You can specify multiple suffix as a list of string:",
    "",
    "source_suffix = ['.rst', '.md']",
    "The master toctree document.",
    "The language for content autogenerated by Sphinx. Refer to documentation",
    "for a list of supported languages.",
    "",
    "This is also used if you do content translation via gettext catalogs.",
    "Usually you set \"language\" from the command line for these cases.",
    "List of patterns, relative to source directory, that match files and",
    "directories to ignore when looking for source files.",
    "This pattern also affects html_static_path and html_extra_path .",
    "The name of the Pygments (syntax highlighting) style to use.",
    "-- Options for HTML output -------------------------------------------------",
    "The theme to use for HTML and HTML Help pages.  See the documentation for",
    "a list of builtin themes.",
    "",
    "html_theme = 'sphinx-rtd-theme'",
    "on_rtd is whether we are on readthedocs.org",
    "only import and set the theme if we're building docs locally",
    "otherwise, readthedocs.org uses their theme by default, so no need to specify it",
    "Theme options are theme-specific and customize the look and feel of a theme",
    "further.  For a list of options available for each theme, see the",
    "documentation.",
    "",
    "html_theme_options = {}",
    "Add any paths that contain custom static files (such as style sheets) here,",
    "relative to this directory. They are copied after the builtin static files,",
    "so a file named \"default.css\" will overwrite the builtin \"default.css\".",
    "Custom sidebar templates, must be a dictionary that maps document names",
    "to template names.",
    "",
    "The default sidebars (for documents that don't match any pattern) are",
    "defined by theme itself.  Builtin themes are using these templates by",
    "default: ``['localtoc.html', 'relations.html', 'sourcelink.html',",
    "'searchbox.html']``.",
    "",
    "html_sidebars = {}",
    "-- Options for HTMLHelp output ---------------------------------------------",
    "Output file base name for HTML help builder.",
    "-- Options for LaTeX output ------------------------------------------------",
    "The paper size ('letterpaper' or 'a4paper').",
    "",
    "'papersize': 'letterpaper',",
    "The font size ('10pt', '11pt' or '12pt').",
    "",
    "'pointsize': '10pt',",
    "Additional stuff for the LaTeX preamble.",
    "",
    "'preamble': '',",
    "Latex figure (float) alignment",
    "",
    "'figure_align': 'htbp',",
    "Grouping the document tree into LaTeX files. List of tuples",
    "(source start file, target name, title,",
    "author, documentclass [howto, manual, or own class]).",
    "-- Options for manual page output ------------------------------------------",
    "One entry per manual page. List of tuples",
    "(source start file, name, description, authors, manual section).",
    "-- Options for Texinfo output ----------------------------------------------",
    "Grouping the document tree into Texinfo files. List of tuples",
    "(source start file, target name, title, author,",
    "dir menu entry, description, category)",
    "-- Options for Epub output -------------------------------------------------",
    "Bibliographic Dublin Core info.",
    "The unique identifier of the text. This can be a ISBN number",
    "or the project homepage.",
    "",
    "epub_identifier = ''",
    "A unique identification for the text.",
    "",
    "epub_uid = ''",
    "A list of files that should not be packed into the epub file.",
    "-- Extension configuration -------------------------------------------------",
    "-- Options for todo extension ----------------------------------------------",
    "If true, `todo` and `todoList` produce output, else they produce nothing.",
    "requires stdin input for identify in weighting sampler",
    "requires Rpy2 for lalonde",
    "requires Rpy2 for causal discovery",
    "very slow",
    "will be removed",
    "Adding the dowhy root folder to the python path so that jupyter notebooks",
    "can import dowhy",
    "\"--ExecutePreprocessor.timeout=600\",",
    "self.df = pd.read_csv(os.path.join(DATA_PATH,'dgp_1/acic_1_1_data.csv'))",
    "self.ate = np.mean(self.df['y1'] - self.df['y0'])",
    "treated = self.df[self.df['z']==1]",
    "self.att = np.mean(treated['y1'] - treated['y0'])",
    "def test_average_treatment_effect(self):",
    "est_ate = 1",
    "bias = est_ate - self.ate",
    "print(bias)",
    "self.assertAlmostEqual(self.ate, est_ate)",
    "def test_average_treatment_effect_on_treated(self):",
    "est_att = 1",
    "self.att=1",
    "bias = est_att - self.att",
    "print(bias)",
    "self.assertAlmostEqual(self.att, est_att)",
    "removing two common causes",
    "The outcome is a linear function of the confounder",
    "The slope is 1,2 and the intercept is 3",
    "As we run with only one common cause and one instrument variable we run with (?, 2)",
    "Supports user-provided dataset object",
    "To test if there are any exceptions",
    "To test if the estimate is identical if refutation parameters are zero",
    "This value is hardcoded to be zero as we are runnning this on a linear dataset.",
    "Ordinarily, we should expect this value to be zero.",
    "This value is hardcoded to be zero as we are runnning this on a linear dataset.",
    "Ordinarily, we should expect this value to be zero.",
    "Calculate causal effect twice: once for unit (t=1, c=0), once for specific increase (t=100, c=50)",
    "Only P(Y|T) should be present for test to succeed.",
    "Calculate causal effect twice: once for unit (t=1, c=0), once for specific increase (t=100, c=50)",
    "Since undirected graph, identify effect must throw an error.",
    "Calculate causal effect twice: once for unit (t=1, c=0), once for specific increase (t=100, c=50)",
    "Compare with ground truth",
    "Calculate causal effect twice: once for unit (t=1, c=0), once for specific increase (t=100, c=50)",
    "Compare with ground truth",
    "Calculate causal effect twice: once for unit (t=1, c=0), once for specific increase (t=100, c=50)",
    "Compare with ground truth",
    "Calculate causal effect twice: once for unit (t=1, c=0), once for specific increase (t=100, c=50)",
    "Compare with ground truth",
    "Causal model initialization",
    "Causal identifier identification",
    "Obtain backdoor sets",
    "Check if backdoor sets are valid i.e. if they block all paths between the treatment and the outcome",
    "Causal model initialization",
    "Causal identifier identification",
    "Obtain backdoor sets",
    "Causal model initialization",
    "Causal identifier identification",
    "Obtain backdoor sets",
    "Check if backdoor sets are valid i.e. if they block all paths between the treatment and the outcome",
    "Causal model initialization",
    "Causal identifier identification",
    "Obtain backdoor sets",
    "Check if backdoor sets are valid i.e. if they block all paths between the treatment and the outcome",
    "Causal model initialization",
    "Causal identifier identification",
    "Obtain backdoor sets",
    "Check if backdoor sets are valid i.e. if they block all paths between the treatment and the outcome",
    "Causal model initialization",
    "Causal identifier identification",
    "Obtain backdoor sets",
    "Check if backdoor sets are valid i.e. if they block all paths between the treatment and the outcome",
    "Example is selected from Pearl J. \"Causality\" 2nd Edition, from chapter 3.3.1 on backoor criterion.",
    "The following simpsons paradox examples are taken from Pearl, J {2013}. \"Understanding Simpson\u2019s Paradox\" - http://ftp.cs.ucla.edu/pub/stat_ser/r414.pdf",
    "The following are examples given in the \"Book of Why\" by Judea Pearl, chapter \"The Do-operator and the Back-Door Criterion\"",
    "cov_mat = np.diag(np.ones(num_features))",
    "Setup data",
    "Test LinearDML",
    "Test ContinuousTreatmentOrthoForest",
    "Test LinearDRLearner",
    "Setup data",
    "Test DeepIV",
    "Treatment model,",
    "Response model",
    "Test IntentToTreatDRIV",
    "Not using testsuite from .base/TestEstimtor, custom code below",
    "More cases where Exception  is expected",
    "Compute confidence intervals, standard error and significance tests",
    "Defined a linear dataset with a given set of properties",
    "Create a model that captures the same",
    "Identify the effects within the model",
    "Defined a linear dataset with a given set of properties",
    "Create a model that captures the same",
    "Identify the effects within the model",
    "Defined a linear dataset with a given set of properties",
    "Create a model that captures the same",
    "Identify the effects within the model",
    "Defined a linear dataset with a given set of properties",
    "Create a model that captures the same",
    "Identify the effects within the model",
    "Defined a linear dataset with a given set of properties",
    "Create a model that captures the same",
    "Identify the effects within the model",
    "Backdoor method names",
    "First, check if there is a directed path from action to outcome",
    "## 1. BACKDOOR IDENTIFICATION",
    "First, checking if there are any valid backdoor adjustment sets",
    "Setting default \"backdoor\" identification adjustment set",
    "## 2. INSTRUMENTAL VARIABLE IDENTIFICATION",
    "Now checking if there is also a valid iv estimand",
    "## 3. FRONTDOOR IDENTIFICATION",
    "Now checking if there is a valid frontdoor variable",
    "Finally returning the estimand object",
    "## 1. FIRST DOING BACKDOOR IDENTIFICATION",
    "First, checking if there are any valid backdoor adjustment sets",
    "Setting default \"backdoor\" identification adjustment set",
    "## 2. SECOND, CHECKING FOR MEDIATORS",
    "Now checking if there are valid mediator variables",
    "Finally returning the estimand object",
    "## 1. FIRST DOING BACKDOOR IDENTIFICATION",
    "First, checking if there are any valid backdoor adjustment sets",
    "Setting default \"backdoor\" identification adjustment set",
    "## 2. SECOND, CHECKING FOR MEDIATORS",
    "Now checking if there are valid mediator variables",
    "Finally returning the estimand object",
    "First, checking if empty set is a valid backdoor set",
    "If the method is `minimal-adjustment`, return the empty set right away.",
    "Second, checking for all other sets of variables. If include_unobserved is false, then only observed variables are eligible.",
    "If var is d-separated from both treatment or outcome, it cannot",
    "be a part of the backdoor set",
    "repeat the above search with BACKDOOR_MIN",
    "If `minimal-adjustment` method is specified, start the search from the set with minimum size. Otherwise, start from the largest.",
    "If the backdoor method is `maximal-adjustment` or `minimal-adjustment`, return the first found adjustment set.",
    "If all variables are observed, and the biggest eligible set",
    "does not satisfy backdoor, then none of its subsets will.",
    "Adding a None estimand if no backdoor set found",
    "Default set contains minimum possible number of instrumental variables, to prevent lowering variance in the treatment variable.",
    "Default set is the one with the least number of adjustment variables (optimizing for efficiency)",
    "For simplicity, assuming a one-variable frontdoor set",
    "Cond 1: All directed paths intercepted by candidate_var",
    "Cond 2: No confounding between treatment and candidate var",
    "Cond 3: treatment blocks all confounding between candidate_var and outcome",
    "For simplicity, assuming a one-variable mediation set",
    "Create estimands dict as per the API for backdoor, but do not return it",
    "Setting default \"backdoor\" identification adjustment set",
    "Create estimands dict as per the API for backdoor, but do not return it",
    "Setting default \"backdoor\" identification adjustment set",
    "TODO: outputs string for now, but ideally should do symbolic",
    "expressions Mon 19 Feb 2018 04:54:17 PM DST",
    "TODO Better support for multivariate treatments",
    "TODO: support multivariate treatments better.",
    "TODO: support multivariate treatments better.",
    "TODO: support multivariate treatments better.",
    "For direct effect",
    "Do not show backdoor key unless it is the only backdoor set.",
    "Just show the default backdoor set",
    "Can use these lists to specify the models/estimators/refuters that a particular interpreter supports.  Throw a ValueError if the user provides an incompatible object to intepret.",
    "Unpacking the keyword arguments",
    "Default value for the number of simulations to be conducted",
    "Concatenate the confounders, instruments and effect modifiers",
    "Shuffle the confounders",
    "Check if all are select or deselect variables",
    "Check if all the required_variables belong to confounders, instrumental variables or effect",
    "Initializing the p_value",
    "Perform Bootstrap Significance Test with the original estimate and the set of refutations",
    "Perform Normal Tests of Significance with the original estimate and the set of refutations",
    "Perform Bootstrap Significance Test with the original estimate and the set of refutations",
    "Perform Normal Tests of Significance with the original estimate and the set of refutations",
    "Get the number of simulations",
    "Sort the simulations",
    "Obtain the median value",
    "Performing a two sided test",
    "np.searchsorted tells us the index if it were a part of the array",
    "We select side to be left as we want to find the first value that matches",
    "We subtact 1 as we are finding the value from the right tail",
    "We take the side to be right as we want to find the last index that matches",
    "We get the probability with respect to the left tail.",
    "Get the mean for the simulations",
    "Get the standard deviation for the simulations",
    "Get the Z Score [(val - mean)/ std_dev ]",
    "load dot file",
    "Adding node attributes",
    "adding penwidth to make the edge bold",
    "Adding common causes",
    "Adding instruments",
    "Adding effect modifiers",
    "Assuming the simple form of effect modifier",
    "that directly causes the outcome.",
    "self._graph.add_edge(node_name, outcome, style = \"dotted\", headport=\"s\", tailport=\"n\")",
    "self._graph.add_edge(outcome, node_name, style = \"dotted\", headport=\"n\", tailport=\"s\") # TODO make the ports more general so that they apply not just to top-bottom node configurations",
    "Adding columns in the dataframe as confounders that were not in the graph",
    "Adding unobserved confounders",
    "also return the number of backdoor paths blocked by observed nodes",
    "Assume that nodes1 is the treatment",
    "ignores new_graph parameter, always uses self._graph",
    "remove paths that have nodes1\\node1 or nodes2\\node2 as intermediate nodes",
    "TODO Refactor to remove this from here and only implement this logic in causalIdentifier. Unnecessary assumption of nodes1 to be causing nodes2.",
    "Cannot simply compute ancestors, since that will also include nodes1 and its parents (e.g. instruments)",
    "removing all mediators",
    "[TODO: double check these work with multivariate implementation:]",
    "Exclusion",
    "As-if-random setup",
    "As-if-random",
    "convert the outputted generator into a list",
    "dpaths = self.get_all_directed_paths(nodes1, nodes2)",
    "return len(dpaths) > 0",
    "Condition 1: node 1 ---> node 2 is intercepted by candidate_nodes",
    "Create causal graph object",
    "Sometimes, effect modifiers from the graph may not match those provided by the user.",
    "(Because some effect modifiers may also be common causes)",
    "In such cases, the user-provided modifiers are used.",
    "If no effect modifiers are provided,  then the ones from the graph are used.",
    "Import causal discovery class",
    "Initialize causal graph object",
    "TODO add propensity score as default backdoor method, iv as default iv method, add an informational message to show which method has been selected.",
    "TODO add dowhy as a prefix to all dowhy estimators",
    "This is done as all dowhy estimators have two parts and external ones have two or more parts",
    "Define the third-party estimation method to be used",
    "Process the dowhy estimators",
    "Check if estimator's target estimand is identified",
    "Estimator had been computed in a previous call",
    "Store parameters inside estimate object for refutation methods",
    "TODO: This add_params needs to move to the estimator class",
    "inside estimate_effect and estimate_conditional_effect",
    "Check if estimator's target estimand is identified",
    "Note that while the name of the variable is the same,",
    "\"self.causal_estimator\", this estimator takes in less",
    "parameters than the same from the",
    "estimate_effect code. It is not advisable to use the",
    "estimator from this function to call estimate_effect",
    "with fit_estimator=False.",
    "Estimator had been computed in a previous call",
    "The default number of simulations for statistical testing",
    "The default number of simulations to obtain confidence intervals",
    "The portion of the total size that should be taken each time to find the confidence intervals",
    "1 is the recommended value",
    "https://ocw.mit.edu/courses/mathematics/18-05-introduction-to-probability-and-statistics-spring-2014/readings/MIT18_05S14_Reading24.pdf",
    "https://projecteuclid.org/download/pdf_1/euclid.ss/1032280214",
    "The default Confidence Level",
    "Number of quantiles to discretize continuous columns, for applying groupby",
    "Prefix to add to temporary categorical variables created after discretization",
    "Currently estimation methods only support univariate treatment and outcome",
    "Setting the default interpret method",
    "Unpacking the keyword arguments",
    "Setting treatment and outcome values",
    "Now saving the effect modifiers",
    "only add the observed nodes",
    "Checking if some parameters were set, otherwise setting to default values",
    "Estimate conditional estimates by default",
    "names of treatment and outcome",
    "TODO Only works for binary treatment",
    "Defaulting to class default values if parameters are not provided",
    "Checking that there is at least one effect modifier",
    "Making sure that effect_modifier_names is a list",
    "Making a copy since we are going to be changing effect modifier names",
    "For every numeric effect modifier, adding a temp categorical column",
    "Grouping by effect modifiers and computing effect separately",
    "Deleting the temporary categorical columns",
    "The array that stores the results of all estimations",
    "Find the sample size the proportion with the population size",
    "Perform the set number of simulations",
    "names of treatment and outcome",
    "Using class default parameters if not specified",
    "Checking if bootstrap_estimates are already computed",
    "Checked if any parameter is changed from the previous std error estimate",
    "Now use the data obtained from the simulations to get the value of the confidence estimates",
    "Get the variations of each bootstrap estimate and sort",
    "Now we take the (1- p)th and the (p)th variations, where p is the chosen confidence level",
    "Get the lower and upper bounds by subtracting the variations from the estimate",
    "Use existing params, if new user defined params are not present",
    "Checking if bootstrap_estimates are already computed",
    "Check if any parameter is changed from the previous std error estimate",
    "Use existing params, if new user defined params are not present",
    "self._outcome = self._data[\"dummy_outcome\"]",
    "Processing the null hypothesis estimates",
    "Doing a two-sided test",
    "Being conservative with the p-value reported",
    "Being conservative with the p-value reported",
    "If the estimate_index is 0, it depends on the number of simulations",
    "Need to test r-squared before supporting",
    "effect_r_squared = self._evaluate_effect_strength(estimate, method=\"r-squared\")",
    "'r-squared': effect_r_squared",
    "elif method == \"r-squared\":",
    "outcome_mean = np.mean(self._outcome)",
    "total_variance = np.sum(np.square(self._outcome - outcome_mean))",
    "Assuming a linear model with one variable: the treatment",
    "Currently only works for continuous y",
    "causal_model = outcome_mean + estimate.value*self._treatment",
    "squared_residual = np.sum(np.square(self._outcome - causal_model))",
    "r_squared = 1 - (squared_residual/total_variance)",
    "return r_squared",
    "No estimand was identified (identification failed)",
    "s += \"Variance in outcome explained by treatment: {}\\n\".format(self.effect_strength[\"r-squared\"])",
    "Below loop assumes that the last indices of W are alwawys converted to discrete",
    "one-hot encode discrete W",
    "Now deleting the old continuous value",
    "Making beta an array",
    "TODO Ensure that we do not generate weak instruments",
    "TODO - test all our methods with random noise added to covariates (instead of the stochastic treatment assignment)",
    "Converting treatment to binary if required",
    "Generating frontdoor variables if asked for",
    "Computing ATE",
    "constructing column names for one-hot encoded discrete features",
    "Specifying the correct dtypes",
    "Now specifying the corresponding graph strings",
    "Now writing the gml graph",
    "Making beta an array",
    "creating data frame",
    "Specifying the correct dtypes",
    "Now specifying the corresponding graph strings",
    "Now writing the gml graph",
    "Adding edges between common causes and the frontdoor mediator",
    "Error terms",
    "else:",
    "V = 6 + W0 + tterm + E1",
    "Y = 12 + W0*W0 + W0*W0 + yterm + E2  # E2_new",
    "Loading version number",
    "The currently supported estimators",
    "The default standard deviation for noise",
    "The default scaling factor to determine the bucket size",
    "The minimum number of points for the estimator to run",
    "The Default Transformation, when no arguments are given, or if the number of data points are insufficient for an estimator",
    "The Default True Causal Effect, this is taken to be ZERO by default",
    "The Default split for the number of data points that fall into the training and validation sets",
    "Assuming that outcome is one-dimensional",
    "We need to change the identified estimand",
    "We thus, make a copy. This is done as we don't want",
    "to change the original DataFrame",
    "We use collections.OrderedDict to maintain the order in which the data is stored",
    "Check if we are using an estimator in the transformation list",
    "The rationale behind ordering of the loops is the fact that we induce randomness everytime we create the",
    "Train and the Validation Datasets. Thus, we run the simulation loop followed by the training and the validation",
    "loops. Thus, we can get different values everytime we get the estimator.",
    "Warn the user that the specified parameter is not applicable when no estimator is present in the transformation",
    "Adding an unobserved confounder if provided by the user",
    "We set X_train = 0 and outcome_train to be 0",
    "Get the final outcome, after running through all the values in the transformation list",
    "Check if the value of true effect has been already stored",
    "We use None as the key as we have no base category for this refutation",
    "As we currently support only one treatment",
    "If the number of data points is too few, run the default transformation: [(\"zero\",\"\"),(\"noise\", {'std_dev':1} )]",
    "Check if the value of true effect has been already stored",
    "This ensures that we calculate the causal effect only once.",
    "We use key_train as we map data with respect to the base category of the data",
    "As we currently support only one treatment",
    "Add h(t) to f(W) to get the dummy outcome",
    "We convert to ndarray for ease in indexing",
    "The data is of the form",
    "sim1: cat1 cat2 ... catn",
    "sim2: cat1 cat2 ... catn",
    "Note: We would like the causal_estimator to find the true causal estimate that we have specified through this",
    "refuter. Let the value of the true causal effect be h(t). In the following section of code, we wish to find out if h(t) falls in the",
    "distribution of the refuter.",
    "True Causal Effect list",
    "Iterating through the refutation for each category",
    "We use string arguments to account for both 32 and 64 bit varaibles",
    "action for continuous variables",
    "Action for categorical variables",
    "Find the set difference for each row",
    "Choose one out of the remaining",
    "We want to see if the estimate falls in the same distribution as the one generated by the refuter",
    "Ideally that should be the case as running bootstrap should not have a significant effect on the ability",
    "of the treatment to affect the outcome",
    "Standardizing the data",
    "Fit a model containing all confounders and compare predictions",
    "using all features compared to all features except a given",
    "confounder.",
    "Estimating the regression coefficient from standardized features to t",
    "By default, return a plot with 10 points",
    "consider 10 values of the effect of the unobserved confounder",
    "Standardizing the data",
    "Fit a model containing all confounders and compare predictions",
    "using all features compared to all features except a given",
    "confounder.",
    "By default, return a plot with 10 points",
    "consider 10 values of the effect of the unobserved confounder",
    "Get a 2D matrix of values",
    "x,y =  np.meshgrid(self.kappa_t, self.kappa_y) # x,y are both MxN",
    "Store the values into the refute object",
    "Adding a label on the contour line for the original estimate",
    "Label every other level using strings",
    "By default, we add the effect of simulated confounder for treatment.",
    "But subtract it from outcome to create a negative correlation",
    "assuming that the original confounder's effect was positive on both.",
    "This is to remove the effect of the original confounder.",
    "By default, we add the effect of simulated confounder for treatment.",
    "But subtract it from outcome to create a negative correlation",
    "assuming that the original confounder's effect was positive on both.",
    "This is to remove the effect of the original confounder.",
    "Obtaining the list of observed variables",
    "Taking a subset of the dataframe that has only observed variables",
    "Residuals from the outcome model obtained by fitting a linear model",
    "Residuals from the treatment model obtained by fitting a linear model",
    "Initialising product_cor_metric_observed with a really low value as finding maximum",
    "The user has an option to give the the effect_strength_on_y and effect_strength_on_t which can be then used instead of maximum correlation with treatment and outcome in the observed variables as it specifies the desired effect.",
    "Choosing a c_star based on the data.",
    "The correlations stop increasing upon increasing c_star after a certain value, that is it plateaus and we choose the value of c_star to be the value it plateaus.",
    "Choosing c1 and c2 based on the hyperbolic relationship once c_star is chosen by going over various combinations of c1 and c2 values and choosing the combination which",
    "which maintains the minimum distance between the product of correlations of the simulated variable and the product of maximum correlations of one of the observed variables",
    "and additionally checks if the ratio of the weights are such that they maintain the ratio of the maximum possible observed coefficients within some confidence interval",
    "c1_final and c2_final are initialised to the values on the hyperbolic curve such that c1_final = c2_final  and c1_final*c2_final = c_star",
    "initialising min_distance_between_product_cor_metrics to be a value greater than 1",
    "Default value of the p value taken for the distribution",
    "Number of Trials: Number of cointosses to understand if a sample gets the treatment",
    "Mean of the Normal Distribution",
    "Standard Deviation of the Normal Distribution",
    "only permute is supported for iv methods",
    "We need to change the identified estimand",
    "We make a copy as a safety measure, we don't want to change the",
    "original DataFrame",
    "For IV methods, the estimating_instrument_names should also be",
    "changed. So we change it inside the estimate and then restore it",
    "back at the end of this method.",
    "Create a new column in the data by the name of placebo",
    "Sanity check the data",
    "Restoring the value of iv_instrument_name",
    "Note: We hardcode the estimate value to ZERO as we want to check if it falls in the distribution of the refuter",
    "Ideally we should expect that ZERO should fall in the distribution of the effect estimates as we have severed any causal",
    "relationship between the treatment and the outcome.",
    "Adding a new backdoor variable to the identified estimand",
    "We want to see if the estimate falls in the same distribution as the one generated by the refuter",
    "Ideally that should be the case as choosing a subset should not have a significant effect on the ability",
    "of the treatment to affect the outcome",
    "from https://www.bnmetrics.com/blog/factory-pattern-in-python3-simple-version",
    "The default subset of the data to be used",
    "We want to see if the estimate falls in the same distribution as the one generated by the refuter",
    "Ideally that should be the case as choosing a subset should not have a significant effect on the ability",
    "of the treatment to affect the outcome",
    "Get adjacency list",
    "If node pair has been fully explored",
    "Add node1 to backdoor set of node_pair",
    "Check if path is backdoor and does not have nodes1\\node1 or nodes2\\node2 as intermediate nodes",
    "True if arrow incoming, False if arrow outgoing",
    "Mark pair (node1, node2) complete",
    "Modify variable count and indices covered",
    "Estimators list for returning after identification",
    "Line 1",
    "If no action has been taken, the effect on Y is just the marginal of the observational distribution P(v) on Y.",
    "Line 2",
    "If we are interested in the effect on Y, it is sufficient to restrict our attention on the parts of the model ancestral to Y.",
    "Modify list of valid nodes",
    "Line 3 - forces an action on any node where such an action would have no effect on Y \u2013 assuming we already acted on X.",
    "Modify adjacency matrix to obtain that corresponding to do(X)",
    "Line 4 - Decomposes the problem into a set of smaller problems using the key property of C-component factorization of causal models.",
    "If the entire graph is a single C-component already, further problem decomposition is impossible, and we must provide base cases.",
    "Modify adjacency matrix to remove treatment variables",
    "Line 5 - The algorithms fails due to the presence of a hedge - the graph G, and a subgraph S that does not contain any X nodes.",
    "Line 6 - If there are no bidirected arcs from X to the other nodes in the current subproblem under consideration, then we can replace acting on X by conditioning, and thus solve the subproblem.",
    "Line 7 - This is the most complicated case in the algorithm. Explain in the second last paragraph on Pg 41 of the link provided in the docstring above.",
    "from https://www.bnmetrics.com/blog/factory-pattern-in-python3-simple-version",
    "Check if the treatment is one-dimensional",
    "first_stage_features = self.build_first_stage_features()",
    "fs_model = self.first_stage_model()",
    "if self._target_estimand.identifier_method==\"frontdoor\":",
    "first_stage_outcome = self._frontdoor_variables",
    "elif self._target_estimand.identifier_method==\"mediation\":",
    "first_stage_outcome = self._mediators",
    "fs_model.fit(first_stage_features, self._frontdoor_variables)",
    "self.logger.debug(\"Coefficients of the fitted model: \" +",
    "\",\".join(map(str, fs_model.coef_)))",
    "residuals = self._frontdoor_variables - fs_model.predict(first_stage_features)",
    "self._data[\"residual\"] = residuals",
    "First stage",
    "Second Stage",
    "Combining the two estimates",
    "This same estimate is valid for frontdoor as well as mediation (NIE)",
    "Total  effect of treatment",
    "Bulding the feature matrix",
    "features = sm.add_constant(features, has_constant='add') # to add an intercept term",
    "choosing the instrumental variable to use",
    "TODO move this to the identification step",
    "Obtain estimate by Wald Estimator",
    "Obtain estimate by 2SLS estimator: Cov(y,z) / Cov(x,z)",
    "More than 1 instrument. Use 2sls.",
    "Enable the user to pass params for a custom propensity model",
    "Check if the treatment is one-dimensional",
    "Checking if the treatment is binary",
    "Convert the categorical variables into dummy/indicator variables",
    "Basically, this gives a one hot encoding for each category",
    "The first category is taken to be the base line.",
    "TODO make treatment_value and control value also as local parameters",
    "Checking if the model is already trained",
    "The model is always built on the entire data",
    "All treatments are set to the same constant value",
    "Using all data by default",
    "Fixing treatment value to the specified value, if provided",
    "treatment_vals and data_df should have same number of rows",
    "Bulding the feature matrix",
    "The model is always built on the entire data",
    "Replacing treatment values by given x",
    "The average treatment effect is a combination of different",
    "regression coefficients. Complicated to compute the confidence",
    "interval analytically. For example, if y=a + b1.t + b2.tx, then",
    "the average treatment effect is b1+b2.mean(x).",
    "Refer Gelman, Hill. ARM Book. Chapter 9",
    "http://www.stat.columbia.edu/~gelman/arm/chap9.pdf",
    "TODO: Looking for contributions",
    "For a linear regression model, the causal effect of a variable is equal to the coefficient corresponding to the",
    "variable. Hence, the model by default outputs the confidence interval corresponding to treatment=1 and control=0.",
    "So for custom treatment and control values, we must multiply the confidence interval by the difference of the two.",
    "For a linear regression model, the causal effect of a variable is equal to the coefficient corresponding to the",
    "variable. Hence, the model by default outputs the standard error corresponding to treatment=1 and control=0.",
    "So for custom treatment and control values, we must multiply the standard error by the difference of the two.",
    "check if the user provides the propensity score column",
    "Infer the right strata based on clipping threshold",
    "0.5 because there are two values for the treatment",
    "To be conservative and allow most strata to be included in the",
    "analysis",
    "At least 90% of the strata should be included in analysis",
    "sum weighted outcomes over all strata  (weight by treated population)",
    "TODO - how can we add additional information into the returned estimate?",
    "such as how much clipping was done, or per-strata info for debugging?",
    "sort the dataframe by propensity score",
    "create a column 'strata' for each element that marks what strata it belongs to",
    "for each strata, count how many treated and control units there are",
    "throw away strata that have insufficient treatment or control",
    "TODO -- fix: we are actually conditioning on positive treatment (d=1)",
    "'ips_weight', 'ips_normalized_weight', 'ips_stabilized_weight'",
    "check if user provides the propensity score column",
    "trim propensity score weights",
    "ips ==> (isTreated(y)/ps(y)) + ((1-isTreated(y))/(1-ps(y)))",
    "nips ==> ips / (sum of ips over all units)",
    "icps ==> ps(y)/(1-ps(y)) / (sum of (ps(y)/(1-ps(y))) over all control units)",
    "itps ==> ps(y)/(1-ps(y)) / (sum of (ps(y)/(1-ps(y))) over all treatment units)",
    "Vanilla IPS estimator",
    "The Hajek estimator (or the self-normalized estimator)",
    "Stabilized weights (from Robins, Hernan, Brumback (2000))",
    "Paper: Marginal Structural Models and Causal Inference in Epidemiology",
    "Calculating the effect",
    "Subtracting the weighted means",
    "TODO - how can we add additional information into the returned estimate?",
    "TODO -- fix: we are actually conditioning on positive treatment (d=1)",
    "For metalearners only--issue a warning if w contains variables not in x",
    "Override the effect_modifiers set in CausalEstimator.__init__()",
    "Also only update self._effect_modifiers, and create a copy of self._effect_modifier_names",
    "the latter can be used by other estimator methods later",
    "Instrumental variables names, if present",
    "choosing the instrumental variable to use",
    "from https://www.bnmetrics.com/blog/factory-pattern-in-python3-simple-version",
    "Calling the econml estimator's fit method",
    "As of v0.9, econml has some kewyord only arguments",
    "Changing shape to a list for a singleton value",
    "TODO -- fix: we are actually conditioning on positive treatment (d=1)",
    "check if the user provides a propensity score column",
    "this assumes a binary treatment regime",
    "TODO remove neighbors that are more than a given radius apart",
    "estimate ATT on treated by summing over difference between matched neighbors",
    "Now computing ATC",
    "TODO -- fix: we are actually conditioning on positive treatment (d=1)",
    "from https://www.bnmetrics.com/blog/factory-pattern-in-python3-simple-version",
    "Check if the treatment is one-dimensional",
    "Checking if the treatment is binary",
    "Setting the number of matches per data point",
    "Default distance metric if not provided by the user",
    "Convert the categorical variables into dummy/indicator variables",
    "Basically, this gives a one hot encoding for each category",
    "The first category is taken to be the base line.",
    "Dictionary of any user-provided params for the distance metric",
    "that will be passed to sklearn nearestneighbors",
    "this assumes a binary treatment regime",
    "TODO remove neighbors that are more than a given radius apart",
    "estimate ATT on treated by summing over difference between matched neighbors",
    "Return indices in the original dataframe",
    "self.matched_indices_att[treated_df_index[i]] = control.iloc[indices[i]].index.tolist()",
    "Now computing ATC",
    "Return indices in the original dataframe",
    "Add the identification method used in the estimator",
    "Check the backdoor variables being used",
    "Add the observed confounders and one hot encode the categorical variables",
    "Get the data of the unobserved confounders",
    "One hot encode the data if they are categorical",
    "Check the instrumental variables involved",
    "Perform the same actions as the above",
    "Check if effect modifiers are used",
    "Get the class corresponding the the estimator to be used",
    "Initialize the object",
    "Both the outcome and the treatment have to be 1D arrays according to the CausalML API",
    "We want to pass 'v0' rather than ['v0'] to prevent a shape mismatch",
    "For CATEs",
    "TODO we are conditioning on a postive treatment",
    "TODO create an expression corresponding to each estimator used",
    "Generating data with equal 0 and 1 (since ranks are uniformly distributed)",
    "Flipping some values",
    "Only consider edges have absolute edge weight > 0.01",
    "Modify graph such that it only contains bidirected edges",
    "Find c components by finding connected components on the undirected graph",
    "Understanding Neural Network weights",
    "Refer to this link:https://stackoverflow.com/questions/50937628/mlp-classifier-neurons-weights",
    "add weight column",
    "before weights are applied we count number rows in each category",
    "which is equivalent to summing over weight=1",
    "after weights are applied we need to sum over the given weights",
    "First, calculating mean differences by strata",
    "Second, without strata",
    "Third, concatenating them and plotting",
    "Setting estimator attribute for convenience",
    "Outcome is numeric",
    "Treatments are also numeric or binary",
    "Outcome is categorical",
    "Treatments are numeric or binary",
    "TODO: A common way to show all plots",
    "from https://www.bnmetrics.com/blog/factory-pattern-in-python3-simple-version",
    "self._identified_estimand = self._causal_model.identify_effect()",
    "self._identified_estimand,",
    "self._causal_model._treatment,",
    "self._causal_model._outcome,",
    "If labels provided",
    "Return in valid DOT format",
    "Get adjacency matrix",
    "If labels not provided",
    "Obtain valid DOT format",
    "If labels provided",
    "Return in valid DOT format",
    "from https://www.bnmetrics.com/blog/factory-pattern-in-python3-simple-version",
    "from https://www.bnmetrics.com/blog/factory-pattern-in-python3-simple-version"
  ],
  "v0.6": [
    "Get the long description from the README file",
    "Get the required packages",
    "Plotting packages are optional to install",
    "Loading version number",
    "-*- coding: utf-8 -*-",
    "",
    "Configuration file for the Sphinx documentation builder.",
    "",
    "This file does only contain a selection of the most common options. For a",
    "full list see the documentation:",
    "http://www.sphinx-doc.org/en/stable/config",
    "-- Path setup --------------------------------------------------------------",
    "If extensions (or modules to document with autodoc) are in another directory,",
    "add these directories to sys.path here. If the directory is relative to the",
    "documentation root, use os.path.abspath to make it absolute, like shown here.",
    "",
    "-- Project information -----------------------------------------------------",
    "The short X.Y version",
    "The full version, including alpha/beta/rc tags",
    "-- General configuration ---------------------------------------------------",
    "If your documentation needs a minimal Sphinx version, state it here.",
    "",
    "needs_sphinx = '1.0'",
    "Add any Sphinx extension module names here, as strings. They can be",
    "extensions coming with Sphinx (named 'sphinx.ext.*') or your custom",
    "ones.",
    "Add any paths that contain templates here, relative to this directory.",
    "The suffix(es) of source filenames.",
    "You can specify multiple suffix as a list of string:",
    "",
    "source_suffix = ['.rst', '.md']",
    "The master toctree document.",
    "The language for content autogenerated by Sphinx. Refer to documentation",
    "for a list of supported languages.",
    "",
    "This is also used if you do content translation via gettext catalogs.",
    "Usually you set \"language\" from the command line for these cases.",
    "List of patterns, relative to source directory, that match files and",
    "directories to ignore when looking for source files.",
    "This pattern also affects html_static_path and html_extra_path .",
    "The name of the Pygments (syntax highlighting) style to use.",
    "-- Options for HTML output -------------------------------------------------",
    "The theme to use for HTML and HTML Help pages.  See the documentation for",
    "a list of builtin themes.",
    "",
    "html_theme = 'sphinx-rtd-theme'",
    "on_rtd is whether we are on readthedocs.org",
    "only import and set the theme if we're building docs locally",
    "otherwise, readthedocs.org uses their theme by default, so no need to specify it",
    "Theme options are theme-specific and customize the look and feel of a theme",
    "further.  For a list of options available for each theme, see the",
    "documentation.",
    "",
    "html_theme_options = {}",
    "Add any paths that contain custom static files (such as style sheets) here,",
    "relative to this directory. They are copied after the builtin static files,",
    "so a file named \"default.css\" will overwrite the builtin \"default.css\".",
    "Custom sidebar templates, must be a dictionary that maps document names",
    "to template names.",
    "",
    "The default sidebars (for documents that don't match any pattern) are",
    "defined by theme itself.  Builtin themes are using these templates by",
    "default: ``['localtoc.html', 'relations.html', 'sourcelink.html',",
    "'searchbox.html']``.",
    "",
    "html_sidebars = {}",
    "-- Options for HTMLHelp output ---------------------------------------------",
    "Output file base name for HTML help builder.",
    "-- Options for LaTeX output ------------------------------------------------",
    "The paper size ('letterpaper' or 'a4paper').",
    "",
    "'papersize': 'letterpaper',",
    "The font size ('10pt', '11pt' or '12pt').",
    "",
    "'pointsize': '10pt',",
    "Additional stuff for the LaTeX preamble.",
    "",
    "'preamble': '',",
    "Latex figure (float) alignment",
    "",
    "'figure_align': 'htbp',",
    "Grouping the document tree into LaTeX files. List of tuples",
    "(source start file, target name, title,",
    "author, documentclass [howto, manual, or own class]).",
    "-- Options for manual page output ------------------------------------------",
    "One entry per manual page. List of tuples",
    "(source start file, name, description, authors, manual section).",
    "-- Options for Texinfo output ----------------------------------------------",
    "Grouping the document tree into Texinfo files. List of tuples",
    "(source start file, target name, title, author,",
    "dir menu entry, description, category)",
    "-- Options for Epub output -------------------------------------------------",
    "Bibliographic Dublin Core info.",
    "The unique identifier of the text. This can be a ISBN number",
    "or the project homepage.",
    "",
    "epub_identifier = ''",
    "A unique identification for the text.",
    "",
    "epub_uid = ''",
    "A list of files that should not be packed into the epub file.",
    "-- Extension configuration -------------------------------------------------",
    "-- Options for todo extension ----------------------------------------------",
    "If true, `todo` and `todoList` produce output, else they produce nothing.",
    "requires stdin input for identify in weighting sampler",
    "requires Rpy2 for lalonde",
    "very slow",
    "Adding the dowhy root folder to the python path so that jupyter notebooks",
    "can import dowhy",
    "\"--ExecutePreprocessor.timeout=600\",",
    "self.df = pd.read_csv(os.path.join(DATA_PATH,'dgp_1/acic_1_1_data.csv'))",
    "self.ate = np.mean(self.df['y1'] - self.df['y0'])",
    "treated = self.df[self.df['z']==1]",
    "self.att = np.mean(treated['y1'] - treated['y0'])",
    "def test_average_treatment_effect(self):",
    "est_ate = 1",
    "bias = est_ate - self.ate",
    "print(bias)",
    "self.assertAlmostEqual(self.ate, est_ate)",
    "def test_average_treatment_effect_on_treated(self):",
    "est_att = 1",
    "self.att=1",
    "bias = est_att - self.att",
    "print(bias)",
    "self.assertAlmostEqual(self.att, est_att)",
    "removing two common causes",
    "The outcome is a linear function of the confounder",
    "The slope is 1,2 and the intercept is 3",
    "As we run with only one common cause and one instrument variable we run with (?, 2)",
    "Supports user-provided dataset object",
    "To test if there are any exceptions",
    "To test if the estimate is identical if refutation parameters are zero",
    "This value is hardcoded to be zero as we are runnning this on a linear dataset.",
    "Ordinarily, we should expect this value to be zero.",
    "This value is hardcoded to be zero as we are runnning this on a linear dataset.",
    "Ordinarily, we should expect this value to be zero.",
    "cov_mat = np.diag(np.ones(num_features))",
    "Setup data",
    "Test LinearDML",
    "Test ContinuousTreatmentOrthoForest",
    "Test LinearDRLearner",
    "Setup data",
    "Test DeepIV",
    "Treatment model,",
    "Response model",
    "Test IntentToTreatDRIV",
    "Not using testsuite from .base/TestEstimtor, custom code below",
    "More cases where Exception  is expected",
    "Compute confidence intervals, standard error and significance tests",
    "Defined a linear dataset with a given set of properties",
    "Create a model that captures the same",
    "Identify the effects within the model",
    "Defined a linear dataset with a given set of properties",
    "Create a model that captures the same",
    "Identify the effects within the model",
    "Defined a linear dataset with a given set of properties",
    "Create a model that captures the same",
    "Identify the effects within the model",
    "Defined a linear dataset with a given set of properties",
    "Create a model that captures the same",
    "Identify the effects within the model",
    "Defined a linear dataset with a given set of properties",
    "Create a model that captures the same",
    "Identify the effects within the model",
    "## 1. BACKDOOR IDENTIFICATION",
    "First, checking if there are any valid backdoor adjustment sets",
    "Setting default \"backdoor\" identification adjustment set",
    "## 2. INSTRUMENTAL VARIABLE IDENTIFICATION",
    "Now checking if there is also a valid iv estimand",
    "## 3. FRONTDOOR IDENTIFICATION",
    "Now checking if there is a valid frontdoor variable",
    "Finally returning the estimand object",
    "## 1. FIRST DOING BACKDOOR IDENTIFICATION",
    "First, checking if there are any valid backdoor adjustment sets",
    "Setting default \"backdoor\" identification adjustment set",
    "## 2. SECOND, CHECKING FOR MEDIATORS",
    "Now checking if there are valid mediator variables",
    "Finally returning the estimand object",
    "## 1. FIRST DOING BACKDOOR IDENTIFICATION",
    "First, checking if there are any valid backdoor adjustment sets",
    "Setting default \"backdoor\" identification adjustment set",
    "## 2. SECOND, CHECKING FOR MEDIATORS",
    "Now checking if there are valid mediator variables",
    "Finally returning the estimand object",
    "First, checking if empty set is a valid backdoor set",
    "Second, checking for all other sets of variables",
    "causes_t = self._graph.get_causes(self.treatment_name)",
    "causes_y = self._graph.get_causes(self.outcome_name, remove_edges={'sources':self.treatment_name, 'targets':self.outcome_name})",
    "common_causes = list(causes_t.intersection(causes_y))",
    "self.logger.info(\"Common causes of treatment and outcome:\" + str(common_causes))",
    "Adding a None estimand if no backdoor set found",
    "Default set is the one with the most number of adjustment variables (optimizing for minimum (unknown) bias not for efficiency)",
    "For simplicity, assuming a one-variable frontdoor set",
    "For simplicity, assuming a one-variable mediation set",
    "Create estimands dict as per the API for backdoor, but do not return it",
    "Setting default \"backdoor\" identification adjustment set",
    "Create estimands dict as per the API for backdoor, but do not return it",
    "Setting default \"backdoor\" identification adjustment set",
    "Adding a None estimand if no backdoor set found",
    "Default set is the one with the most number of adjustment variables (optimizing for minimum (unknown) bias not for efficiency)",
    "TODO: outputs string for now, but ideally should do symbolic",
    "expressions Mon 19 Feb 2018 04:54:17 PM DST",
    "TODO Better support for multivariate treatments",
    "TODO: support multivariate treatments better.",
    "TODO: support multivariate treatments better.",
    "TODO: support multivariate treatments better.",
    "For direct effect",
    "Do not show backdoor key unless it is the only backdoor set.",
    "Just show the default backdoor set",
    "Can use these lists to specify the models/estimators/refuters that a particular interpreter supports.  Throw a ValueError if the user provides an incompatible object to intepret.",
    "Unpacking the keyword arguments",
    "Default value for the number of simulations to be conducted",
    "Concatenate the confounders, instruments and effect modifiers",
    "Shuffle the confounders",
    "Check if all are select or deselect variables",
    "Check if all the required_variables belong to confounders, instrumental variables or effect",
    "Initializing the p_value",
    "Perform Bootstrap Significance Test with the original estimate and the set of refutations",
    "Perform Normal Tests of Significance with the original estimate and the set of refutations",
    "Perform Bootstrap Significance Test with the original estimate and the set of refutations",
    "Perform Normal Tests of Significance with the original estimate and the set of refutations",
    "Get the number of simulations",
    "Sort the simulations",
    "Obtain the median value",
    "Performing a two sided test",
    "np.searchsorted tells us the index if it were a part of the array",
    "We select side to be left as we want to find the first value that matches",
    "We subtact 1 as we are finding the value from the right tail",
    "We take the side to be right as we want to find the last index that matches",
    "We get the probability with respect to the left tail.",
    "Get the mean for the simulations",
    "Get the standard deviation for the simulations",
    "Get the Z Score [(val - mean)/ std_dev ]",
    "load dot file",
    "Adding node attributes",
    "adding penwidth to make the edge bold",
    "Adding common causes",
    "Adding instruments",
    "Adding effect modifiers",
    "Adding columns in the dataframe as confounders that were not in the graph",
    "Adding unobserved confounders",
    "also return the number of backdoor paths blocked by observed nodes",
    "remove paths that have nodes1\\node1 or nodes2\\node2 as intermediate nodes",
    "TODO Refactor to remove this from here and only implement this logic in causalIdentifier. Unnecessary assumption of nodes1 to be causing nodes2.",
    "Cannot simply compute ancestors, since that will also include nodes1 and its parents (e.g. instruments)",
    "removing all mediators",
    "[TODO: double check these work with multivariate implementation:]",
    "Exclusion",
    "As-if-random setup",
    "As-if-random",
    "convert the outputted generator into a list",
    "Sometimes, effect modifiers from the graph may not match those provided by the user.",
    "(Because some effect modifiers may also be common causes)",
    "In such cases, the user-provided modifiers are used.",
    "If no effect modifiers are provided,  then the ones from the graph are used.",
    "TODO add propensity score as default backdoor method, iv as default iv method, add an informational message to show which method has been selected.",
    "TODO add dowhy as a prefix to all dowhy estimators",
    "This is done as all dowhy estimators have two parts and external ones have two or more parts",
    "Define the third-party estimation method to be used",
    "Process the dowhy estimators",
    "Check if estimator's target estimand is identified",
    "Store parameters inside estimate object for refutation methods",
    "TODO: This add_params needs to move to the estimator class",
    "inside estimate_effect and estimate_conditional_effect",
    "Check if estimator's target estimand is identified",
    "The default number of simulations for statistical testing",
    "The default number of simulations to obtain confidence intervals",
    "The portion of the total size that should be taken each time to find the confidence intervals",
    "1 is the recommended value",
    "https://ocw.mit.edu/courses/mathematics/18-05-introduction-to-probability-and-statistics-spring-2014/readings/MIT18_05S14_Reading24.pdf",
    "https://projecteuclid.org/download/pdf_1/euclid.ss/1032280214",
    "The default Confidence Level",
    "Number of quantiles to discretize continuous columns, for applying groupby",
    "Prefix to add to temporary categorical variables created after discretization",
    "Currently estimation methods only support univariate treatment and outcome",
    "Setting the default interpret method",
    "Unpacking the keyword arguments",
    "Checking if some parameters were set, otherwise setting to default values",
    "Estimate conditional estimates by default",
    "Setting more values",
    "Now saving the effect modifiers",
    "TODO Only works for binary treatment",
    "Defaulting to class default values if parameters are not provided",
    "Checking that there is at least one effect modifier",
    "Making sure that effect_modifier_names is a list",
    "Making a copy since we are going to be changing effect modifier names",
    "For every numeric effect modifier, adding a temp categorical column",
    "Grouping by effect modifiers and computing effect separately",
    "Deleting the temporary categorical columns",
    "The array that stores the results of all estimations",
    "Find the sample size the proportion with the population size",
    "Perform the set number of simulations",
    "Using class default parameters if not specified",
    "Checking if bootstrap_estimates are already computed",
    "Checked if any parameter is changed from the previous std error estimate",
    "Now use the data obtained from the simulations to get the value of the confidence estimates",
    "Sort the simulations",
    "Now we take the (1- p)th and the (p)th values, where p is the chosen confidence level",
    "get the values",
    "Use existing params, if new user defined params are not present",
    "Checking if bootstrap_estimates are already computed",
    "Check if any parameter is changed from the previous std error estimate",
    "Use existing params, if new user defined params are not present",
    "self._outcome = self._data[\"dummy_outcome\"]",
    "Processing the null hypothesis estimates",
    "Doing a two-sided test",
    "Being conservative with the p-value reported",
    "Being conservative with the p-value reported",
    "If the estimate_index is 0, it depends on the number of simulations",
    "Need to test r-squared before supporting",
    "effect_r_squared = self._evaluate_effect_strength(estimate, method=\"r-squared\")",
    "'r-squared': effect_r_squared",
    "elif method == \"r-squared\":",
    "outcome_mean = np.mean(self._outcome)",
    "total_variance = np.sum(np.square(self._outcome - outcome_mean))",
    "Assuming a linear model with one variable: the treatment",
    "Currently only works for continuous y",
    "causal_model = outcome_mean + estimate.value*self._treatment",
    "squared_residual = np.sum(np.square(self._outcome - causal_model))",
    "r_squared = 1 - (squared_residual/total_variance)",
    "return r_squared",
    "s += \"Variance in outcome explained by treatment: {}\\n\".format(self.effect_strength[\"r-squared\"])",
    "Below loop assumes that the last indices of W are alwawys converted to discrete",
    "one-hot encode discrete W",
    "Now deleting the old continuous value",
    "Making beta an array",
    "TODO Ensure that we do not generate weak instruments",
    "TODO - test all our methods with random noise added to covariates (instead of the stochastic treatment assignment)",
    "Converting treatment to binary if required",
    "Generating frontdoor variables if asked for",
    "Computing ATE",
    "constructing column names for one-hot encoded discrete features",
    "Specifying the correct dtypes",
    "Now specifying the corresponding graph strings",
    "Now writing the gml graph",
    "Making beta an array",
    "creating data frame",
    "Specifying the correct dtypes",
    "Now specifying the corresponding graph strings",
    "Now writing the gml graph",
    "Adding edges between common causes and the frontdoor mediator",
    "Error terms",
    "else:",
    "V = 6 + W0 + tterm + E1",
    "Y = 12 + W0*W0 + W0*W0 + yterm + E2  # E2_new",
    "Loading version number",
    "The currently supported estimators",
    "The default standard deviation for noise",
    "The default scaling factor to determine the bucket size",
    "The minimum number of points for the estimator to run",
    "The Default Transformation, when no arguments are given, or if the number of data points are insufficient for an estimator",
    "The Default True Causal Effect, this is taken to be ZERO by default",
    "The Default split for the number of data points that fall into the training and validation sets",
    "Assuming that outcome is one-dimensional",
    "We need to change the identified estimand",
    "We thus, make a copy. This is done as we don't want",
    "to change the original DataFrame",
    "We use collections.OrderedDict to maintain the order in which the data is stored",
    "Check if we are using an estimator in the transformation list",
    "The rationale behind ordering of the loops is the fact that we induce randomness everytime we create the",
    "Train and the Validation Datasets. Thus, we run the simulation loop followed by the training and the validation",
    "loops. Thus, we can get different values everytime we get the estimator.",
    "Warn the user that the specified parameter is not applicable when no estimator is present in the transformation",
    "Adding an unobserved confounder if provided by the user",
    "We set X_train = 0 and outcome_train to be 0",
    "Get the final outcome, after running through all the values in the transformation list",
    "Check if the value of true effect has been already stored",
    "We use None as the key as we have no base category for this refutation",
    "As we currently support only one treatment",
    "If the number of data points is too few, run the default transformation: [(\"zero\",\"\"),(\"noise\", {'std_dev':1} )]",
    "Check if the value of true effect has been already stored",
    "This ensures that we calculate the causal effect only once.",
    "We use key_train as we map data with respect to the base category of the data",
    "As we currently support only one treatment",
    "Add h(t) to f(W) to get the dummy outcome",
    "We convert to ndarray for ease in indexing",
    "The data is of the form",
    "sim1: cat1 cat2 ... catn",
    "sim2: cat1 cat2 ... catn",
    "Note: We would like the causal_estimator to find the true causal estimate that we have specified through this",
    "refuter. Let the value of the true causal effect be h(t). In the following section of code, we wish to find out if h(t) falls in the",
    "distribution of the refuter.",
    "True Causal Effect list",
    "Iterating through the refutation for each category",
    "We use string arguments to account for both 32 and 64 bit varaibles",
    "action for continuous variables",
    "Action for categorical variables",
    "Find the set difference for each row",
    "Choose one out of the remaining",
    "We want to see if the estimate falls in the same distribution as the one generated by the refuter",
    "Ideally that should be the case as running bootstrap should not have a significant effect on the ability",
    "of the treatment to affect the outcome",
    "Get a 2D matrix of values",
    "Store the values into the refute object",
    "Obtaining the list of observed variables",
    "Taking a subset of the dataframe that has only observed variables",
    "Residuals from the outcome model obtained by fitting a linear model",
    "Residuals from the treatment model obtained by fitting a linear model",
    "Initialising product_cor_metric_observed with a really low value as finding maximum",
    "The user has an option to give the the effect_strength_on_y and effect_strength_on_t which can be then used instead of maximum correlation with treatment and outcome in the observed variables as it specifies the desired effect.",
    "Choosing a c_star based on the data.",
    "The correlations stop increasing upon increasing c_star after a certain value, that is it plateaus and we choose the value of c_star to be the value it plateaus.",
    "Choosing c1 and c2 based on the hyperbolic relationship once c_star is chosen by going over various combinations of c1 and c2 values and choosing the combination which",
    "which maintains the minimum distance between the product of correlations of the simulated variable and the product of maximum correlations of one of the observed variables",
    "and additionally checks if the ratio of the weights are such that they maintain the ratio of the maximum possible observed coefficients within some confidence interval",
    "c1_final and c2_final are initialised to the values on the hyperbolic curve such that c1_final = c2_final  and c1_final*c2_final = c_star",
    "initialising min_distance_between_product_cor_metrics to be a value greater than 1",
    "Default value of the p value taken for the distribution",
    "Number of Trials: Number of cointosses to understand if a sample gets the treatment",
    "Mean of the Normal Distribution",
    "Standard Deviation of the Normal Distribution",
    "only permute is supported for iv methods",
    "We need to change the identified estimand",
    "We make a copy as a safety measure, we don't want to change the",
    "original DataFrame",
    "For IV methods, the estimating_instrument_names should also be",
    "changed. So we change it inside the estimate and then restore it",
    "back at the end of this method.",
    "Create a new column in the data by the name of placebo",
    "Sanity check the data",
    "Restoring the value of iv_instrument_name",
    "Note: We hardcode the estimate value to ZERO as we want to check if it falls in the distribution of the refuter",
    "Ideally we should expect that ZERO should fall in the distribution of the effect estimates as we have severed any causal",
    "relationship between the treatment and the outcome.",
    "Adding a new backdoor variable to the identified estimand",
    "from https://www.bnmetrics.com/blog/factory-pattern-in-python3-simple-version",
    "The default subset of the data to be used",
    "We want to see if the estimate falls in the same distribution as the one generated by the refuter",
    "Ideally that should be the case as choosing a subset should not have a significant effect on the ability",
    "of the treatment to affect the outcome",
    "from https://www.bnmetrics.com/blog/factory-pattern-in-python3-simple-version",
    "Check if the treatment is one-dimensional",
    "first_stage_features = self.build_first_stage_features()",
    "fs_model = self.first_stage_model()",
    "if self._target_estimand.identifier_method==\"frontdoor\":",
    "first_stage_outcome = self._frontdoor_variables",
    "elif self._target_estimand.identifier_method==\"mediation\":",
    "first_stage_outcome = self._mediators",
    "fs_model.fit(first_stage_features, self._frontdoor_variables)",
    "self.logger.debug(\"Coefficients of the fitted model: \" +",
    "\",\".join(map(str, fs_model.coef_)))",
    "residuals = self._frontdoor_variables - fs_model.predict(first_stage_features)",
    "self._data[\"residual\"] = residuals",
    "First stage",
    "Second Stage",
    "Combining the two estimates",
    "This same estimate is valid for frontdoor as well as mediation (NIE)",
    "Total  effect of treatment",
    "Bulding the feature matrix",
    "features = sm.add_constant(features, has_constant='add') # to add an intercept term",
    "choosing the instrumental variable to use",
    "TODO move this to the identification step",
    "Obtain estimate by Wald Estimator",
    "Obtain estimate by 2SLS estimator: Cov(y,z) / Cov(x,z)",
    "More than 1 instrument. Use 2sls.",
    "We need to initialize the model when we create any propensity score estimator",
    "Check if the treatment is one-dimensional",
    "Checking if the treatment is binary",
    "Convert the categorical variables into dummy/indicator variables",
    "Basically, this gives a one hot encoding for each category",
    "The first category is taken to be the base line.",
    "TODO make treatment_value and control value also as local parameters",
    "Checking if the model is already trained",
    "The model is always built on the entire data",
    "All treatments are set to the same constant value",
    "Using all data by default",
    "Fixing treatment value to the specified value, if provided",
    "treatment_vals and data_df should have same number of rows",
    "Bulding the feature matrix",
    "The model is always built on the entire data",
    "Replacing treatment values by given x",
    "sort the dataframe by propensity score",
    "create a column 'strata' for each element that marks what strata it belongs to",
    "for each strata, count how many treated and control units there are",
    "throw away strata that have insufficient treatment or control",
    "print(\"before clipping, here is the distribution of treatment and control per strata\")",
    "print(self._data.groupby(['strata',self._treatment_name])[self._outcome_name].count())",
    "sum weighted outcomes over all strata  (weight by treated population)",
    "TODO - how can we add additional information into the returned estimate?",
    "such as how much clipping was done, or per-strata info for debugging?",
    "TODO -- fix: we are actually conditioning on positive treatment (d=1)",
    "trim propensity score weights",
    "ips ==> (isTreated(y)/ps(y)) + ((1-isTreated(y))/(1-ps(y)))",
    "nips ==> ips / (sum of ips over all units)",
    "icps ==> ps(y)/(1-ps(y)) / (sum of (ps(y)/(1-ps(y))) over all control units)",
    "itps ==> ps(y)/(1-ps(y)) / (sum of (ps(y)/(1-ps(y))) over all treatment units)",
    "Vanilla IPS estimator",
    "Also known as the Hajek estimator",
    "Stabilized weights",
    "Simple normalized estimator (commented out for now)",
    "ips_sum = self._data['ips_weight'].sum()",
    "self._data['nips_weight'] = self._data['ips_weight'] / ips_sum",
    "self._data['ips2'] = self._data['ps'] / (1 - self._data['ps'])",
    "treated_ips_sum = (self._data['ips2'] * self._data[self._treatment_name[0]]).sum()",
    "control_ips_sum = (self._data['ips2'] * (1 - self._data[self._treatment_name[0]])).sum()",
    "self._data['itps_weight'] = self._data['ips2'] / treated_ips_sum",
    "self._data['icps_weight'] = self._data['ips2'] / control_ips_sum",
    "Calculating the effect",
    "TODO - how can we add additional information into the returned estimate?",
    "TODO -- fix: we are actually conditioning on positive treatment (d=1)",
    "For metalearners only--issue a warning if w contains variables not in x",
    "Override the effect_modifiers set in CausalEstimator.__init__()",
    "Also only update self._effect_modifiers, and create a copy of self._effect_modifier_names",
    "the latter can be used by other estimator methods later",
    "Instrumental variables names, if present",
    "choosing the instrumental variable to use",
    "from https://www.bnmetrics.com/blog/factory-pattern-in-python3-simple-version",
    "Calling the econml estimator's fit method",
    "As of v0.9, econml has some kewyord only arguments",
    "Changing shape to a list for a singleton value",
    "TODO -- fix: we are actually conditioning on positive treatment (d=1)",
    "this assumes a binary treatment regime",
    "TODO remove neighbors that are more than a given radius apart",
    "estimate ATT on treated by summing over difference between matched neighbors",
    "Now computing ATC",
    "TODO -- fix: we are actually conditioning on positive treatment (d=1)",
    "from https://www.bnmetrics.com/blog/factory-pattern-in-python3-simple-version",
    "Add the identification method used in the estimator",
    "Check the backdoor variables being used",
    "Add the observed confounders and one hot encode the categorical variables",
    "Get the data of the unobserved confounders",
    "One hot encode the data if they are categorical",
    "Check the instrumental variables involved",
    "Perform the same actions as the above",
    "Check if effect modifiers are used",
    "Get the class corresponding the the estimator to be used",
    "Initialize the object",
    "Both the outcome and the treatment have to be 1D arrays according to the CausalML API",
    "We want to pass 'v0' rather than ['v0'] to prevent a shape mismatch",
    "For CATEs",
    "TODO we are conditioning on a postive treatment",
    "TODO create an expression corresponding to each estimator used",
    "Generating data with equal 0 and 1 (since ranks are uniformly distributed)",
    "Flipping some values",
    "Understanding Neural Network weights",
    "Refer to this link:https://stackoverflow.com/questions/50937628/mlp-classifier-neurons-weights",
    "add weight column",
    "before weights are applied we count number rows in each category",
    "which is equivalent to summing over weight=1",
    "after weights are applied we need to sum over the given weights",
    "First, calculating mean differences by strata",
    "Second, without strata",
    "Third, concatenating them and plotting",
    "Setting estimator attribute for convenience",
    "Outcome is numeric",
    "Treatments are also numeric or binary",
    "Outcome is categorical",
    "Treatments are numeric or binary",
    "TODO: A common way to show all plots",
    "from https://www.bnmetrics.com/blog/factory-pattern-in-python3-simple-version",
    "self._identified_estimand = self._causal_model.identify_effect()",
    "self._identified_estimand,",
    "self._causal_model._treatment,",
    "self._causal_model._outcome,"
  ],
  "v0.5.1": [
    "Get the long description from the README file",
    "Get the required packages",
    "Loading version number",
    "-*- coding: utf-8 -*-",
    "",
    "Configuration file for the Sphinx documentation builder.",
    "",
    "This file does only contain a selection of the most common options. For a",
    "full list see the documentation:",
    "http://www.sphinx-doc.org/en/stable/config",
    "-- Path setup --------------------------------------------------------------",
    "If extensions (or modules to document with autodoc) are in another directory,",
    "add these directories to sys.path here. If the directory is relative to the",
    "documentation root, use os.path.abspath to make it absolute, like shown here.",
    "",
    "-- Project information -----------------------------------------------------",
    "The short X.Y version",
    "The full version, including alpha/beta/rc tags",
    "-- General configuration ---------------------------------------------------",
    "If your documentation needs a minimal Sphinx version, state it here.",
    "",
    "needs_sphinx = '1.0'",
    "Add any Sphinx extension module names here, as strings. They can be",
    "extensions coming with Sphinx (named 'sphinx.ext.*') or your custom",
    "ones.",
    "Add any paths that contain templates here, relative to this directory.",
    "The suffix(es) of source filenames.",
    "You can specify multiple suffix as a list of string:",
    "",
    "source_suffix = ['.rst', '.md']",
    "The master toctree document.",
    "The language for content autogenerated by Sphinx. Refer to documentation",
    "for a list of supported languages.",
    "",
    "This is also used if you do content translation via gettext catalogs.",
    "Usually you set \"language\" from the command line for these cases.",
    "List of patterns, relative to source directory, that match files and",
    "directories to ignore when looking for source files.",
    "This pattern also affects html_static_path and html_extra_path .",
    "The name of the Pygments (syntax highlighting) style to use.",
    "-- Options for HTML output -------------------------------------------------",
    "The theme to use for HTML and HTML Help pages.  See the documentation for",
    "a list of builtin themes.",
    "",
    "html_theme = 'sphinx-rtd-theme'",
    "on_rtd is whether we are on readthedocs.org",
    "only import and set the theme if we're building docs locally",
    "otherwise, readthedocs.org uses their theme by default, so no need to specify it",
    "Theme options are theme-specific and customize the look and feel of a theme",
    "further.  For a list of options available for each theme, see the",
    "documentation.",
    "",
    "html_theme_options = {}",
    "Add any paths that contain custom static files (such as style sheets) here,",
    "relative to this directory. They are copied after the builtin static files,",
    "so a file named \"default.css\" will overwrite the builtin \"default.css\".",
    "Custom sidebar templates, must be a dictionary that maps document names",
    "to template names.",
    "",
    "The default sidebars (for documents that don't match any pattern) are",
    "defined by theme itself.  Builtin themes are using these templates by",
    "default: ``['localtoc.html', 'relations.html', 'sourcelink.html',",
    "'searchbox.html']``.",
    "",
    "html_sidebars = {}",
    "-- Options for HTMLHelp output ---------------------------------------------",
    "Output file base name for HTML help builder.",
    "-- Options for LaTeX output ------------------------------------------------",
    "The paper size ('letterpaper' or 'a4paper').",
    "",
    "'papersize': 'letterpaper',",
    "The font size ('10pt', '11pt' or '12pt').",
    "",
    "'pointsize': '10pt',",
    "Additional stuff for the LaTeX preamble.",
    "",
    "'preamble': '',",
    "Latex figure (float) alignment",
    "",
    "'figure_align': 'htbp',",
    "Grouping the document tree into LaTeX files. List of tuples",
    "(source start file, target name, title,",
    "author, documentclass [howto, manual, or own class]).",
    "-- Options for manual page output ------------------------------------------",
    "One entry per manual page. List of tuples",
    "(source start file, name, description, authors, manual section).",
    "-- Options for Texinfo output ----------------------------------------------",
    "Grouping the document tree into Texinfo files. List of tuples",
    "(source start file, target name, title, author,",
    "dir menu entry, description, category)",
    "-- Options for Epub output -------------------------------------------------",
    "Bibliographic Dublin Core info.",
    "The unique identifier of the text. This can be a ISBN number",
    "or the project homepage.",
    "",
    "epub_identifier = ''",
    "A unique identification for the text.",
    "",
    "epub_uid = ''",
    "A list of files that should not be packed into the epub file.",
    "-- Extension configuration -------------------------------------------------",
    "-- Options for todo extension ----------------------------------------------",
    "If true, `todo` and `todoList` produce output, else they produce nothing.",
    "requires stdin input for identify in weighting sampler",
    "requires Rpy2 for lalonde",
    "very slow",
    "Adding the dowhy root folder to the python path so that jupyter notebooks",
    "can import dowhy",
    "\"--ExecutePreprocessor.timeout=600\",",
    "self.df = pd.read_csv(os.path.join(DATA_PATH,'dgp_1/acic_1_1_data.csv'))",
    "self.ate = np.mean(self.df['y1'] - self.df['y0'])",
    "treated = self.df[self.df['z']==1]",
    "self.att = np.mean(treated['y1'] - treated['y0'])",
    "def test_average_treatment_effect(self):",
    "est_ate = 1",
    "bias = est_ate - self.ate",
    "print(bias)",
    "self.assertAlmostEqual(self.ate, est_ate)",
    "def test_average_treatment_effect_on_treated(self):",
    "est_att = 1",
    "self.att=1",
    "bias = est_att - self.att",
    "print(bias)",
    "self.assertAlmostEqual(self.att, est_att)",
    "removing two common causes",
    "The outcome is a linear function of the confounder",
    "The slope is 1,2 and the intercept is 3",
    "As we run with only one common cause and one instrument variable we run with (?, 2)",
    "Supports user-provided dataset object",
    "To test if there are any exceptions",
    "To test if the estimate is identical if refutation parameters are zero",
    "This value is hardcoded to be zero as we are runnning this on a linear dataset.",
    "Ordinarily, we should expect this value to be zero.",
    "This value is hardcoded to be zero as we are runnning this on a linear dataset.",
    "Ordinarily, we should expect this value to be zero.",
    "cov_mat = np.diag(np.ones(num_features))",
    "Setup data",
    "Test LinearDMLCateEstimator",
    "Test ContinuousTreatmentOrthoForest",
    "Test LinearDRLearner",
    "Setup data",
    "Test DeepIV",
    "TODO: Test IntentToTreatDRIV when EconML v0.7 comes out",
    "Not using testsuite from .base/TestEstimtor, custom code below",
    "More cases where Exception  is expected",
    "Compute confidence intervals, standard error and significance tests",
    "Defined a linear dataset with a given set of properties",
    "Create a model that captures the same",
    "Identify the effects within the model",
    "Defined a linear dataset with a given set of properties",
    "Create a model that captures the same",
    "Identify the effects within the model",
    "Defined a linear dataset with a given set of properties",
    "Create a model that captures the same",
    "Identify the effects within the model",
    "Defined a linear dataset with a given set of properties",
    "Create a model that captures the same",
    "Identify the effects within the model",
    "Defined a linear dataset with a given set of properties",
    "Create a model that captures the same",
    "Identify the effects within the model",
    "## 1. BACKDOOR IDENTIFICATION",
    "First, checking if there are any valid backdoor adjustment sets",
    "Setting default \"backdoor\" identification adjustment set",
    "## 2. INSTRUMENTAL VARIABLE IDENTIFICATION",
    "Now checking if there is also a valid iv estimand",
    "## 3. FRONTDOOR IDENTIFICATION",
    "Now checking if there is a valid frontdoor variable",
    "Finally returning the estimand object",
    "## 1. FIRST DOING BACKDOOR IDENTIFICATION",
    "First, checking if there are any valid backdoor adjustment sets",
    "Setting default \"backdoor\" identification adjustment set",
    "## 2. SECOND, CHECKING FOR MEDIATORS",
    "Now checking if there are valid mediator variables",
    "Finally returning the estimand object",
    "## 1. FIRST DOING BACKDOOR IDENTIFICATION",
    "First, checking if there are any valid backdoor adjustment sets",
    "Setting default \"backdoor\" identification adjustment set",
    "## 2. SECOND, CHECKING FOR MEDIATORS",
    "Now checking if there are valid mediator variables",
    "Finally returning the estimand object",
    "First, checking if empty set is a valid backdoor set",
    "Second, checking for all other sets of variables",
    "causes_t = self._graph.get_causes(self.treatment_name)",
    "causes_y = self._graph.get_causes(self.outcome_name, remove_edges={'sources':self.treatment_name, 'targets':self.outcome_name})",
    "common_causes = list(causes_t.intersection(causes_y))",
    "self.logger.info(\"Common causes of treatment and outcome:\" + str(common_causes))",
    "Adding a None estimand if no backdoor set found",
    "Default set is the one with the most number of adjustment variables (optimizing for minimum (unknown) bias not for efficiency)",
    "For simplicity, assuming a one-variable frontdoor set",
    "For simplicity, assuming a one-variable mediation set",
    "Create estimands dict as per the API for backdoor, but do not return it",
    "Setting default \"backdoor\" identification adjustment set",
    "Create estimands dict as per the API for backdoor, but do not return it",
    "Setting default \"backdoor\" identification adjustment set",
    "Adding a None estimand if no backdoor set found",
    "Default set is the one with the most number of adjustment variables (optimizing for minimum (unknown) bias not for efficiency)",
    "TODO: outputs string for now, but ideally should do symbolic",
    "expressions Mon 19 Feb 2018 04:54:17 PM DST",
    "TODO Better support for multivariate treatments",
    "TODO: support multivariate treatments better.",
    "TODO: support multivariate treatments better.",
    "TODO: support multivariate treatments better.",
    "For direct effect",
    "Do not show backdoor key unless it is the only backdoor set.",
    "Can use these lists to specify the models/estimators/refuters that a particular interpreter supports.  Throw a ValueError if the user provides an incompatible object to intepret.",
    "Unpacking the keyword arguments",
    "Default value for the number of simulations to be conducted",
    "Concatenate the confounders, instruments and effect modifiers",
    "Shuffle the confounders",
    "Check if all are select or deselect variables",
    "Check if all the required_variables belong to confounders, instrumental variables or effect",
    "Initializing the p_value",
    "Perform Bootstrap Significance Test with the original estimate and the set of refutations",
    "Perform Normal Tests of Significance with the original estimate and the set of refutations",
    "Perform Bootstrap Significance Test with the original estimate and the set of refutations",
    "Perform Normal Tests of Significance with the original estimate and the set of refutations",
    "Get the number of simulations",
    "Sort the simulations",
    "Obtain the median value",
    "Performing a two sided test",
    "np.searchsorted tells us the index if it were a part of the array",
    "We select side to be left as we want to find the first value that matches",
    "We subtact 1 as we are finding the value from the right tail",
    "We take the side to be right as we want to find the last index that matches",
    "We get the probability with respect to the left tail.",
    "Get the mean for the simulations",
    "Get the standard deviation for the simulations",
    "Get the Z Score [(val - mean)/ std_dev ]",
    "load dot file",
    "Adding node attributes",
    "TODO do not add it here. CausalIdentifier should call causal_graph to add an unobserved common cause if needed. This also ensures that we do not need get_common_causes in this class.",
    "adding penwidth to make the edge bold",
    "Adding common causes",
    "Adding instruments",
    "Adding effect modifiers",
    "Adding columns in the dataframe as confounders that were not in the graph",
    "Adding unobserved confounders",
    "also return the number of backdoor paths blocked by observed nodes",
    "remove paths that have nodes1\\node1 or nodes2\\node2 as intermediate nodes",
    "TODO Refactor to remove this from here and only implement this logic in causalIdentifier. Unnecessary assumption of nodes1 to be causing nodes2.",
    "Cannot simply compute ancestors, since that will also include nodes1 and its parents (e.g. instruments)",
    "removing all mediators",
    "[TODO: double check these work with multivariate implementation:]",
    "Exclusion",
    "As-if-random setup",
    "As-if-random",
    "TODO: move the logging level argument to a json file. Tue 20 Feb 2018 06:56:27 PM DST",
    "Sometimes, effect modifiers from the graph may not match those provided by the user.",
    "(Because some effect modifiers may also be common causes)",
    "In such cases, the user-provided modifiers are used.",
    "If no effect modifiers are provided,  then the ones from the graph are used.",
    "TODO add propensity score as default backdoor method, iv as default iv method, add an informational message to show which method has been selected.",
    "TODO add dowhy as a prefix to all dowhy estimators",
    "This is done as all dowhy estimators have two parts and external ones have two or more parts",
    "Define the third-party estimation method to be used",
    "Process the dowhy estimators",
    "Check if estimator's target estimand is identified",
    "Store parameters inside estimate object for refutation methods",
    "Check if estimator's target estimand is identified",
    "The default number of simulations for statistical testing",
    "The default number of simulations to obtain confidence intervals",
    "The portion of the total size that should be taken each time to find the confidence intervals",
    "1 is the recommended value",
    "https://ocw.mit.edu/courses/mathematics/18-05-introduction-to-probability-and-statistics-spring-2014/readings/MIT18_05S14_Reading24.pdf",
    "https://projecteuclid.org/download/pdf_1/euclid.ss/1032280214",
    "The default Confidence Level",
    "Number of quantiles to discretize continuous columns, for applying groupby",
    "Prefix to add to temporary categorical variables created after discretization",
    "Currently estimation methods only support univariate treatment and outcome",
    "Setting the default interpret method",
    "Unpacking the keyword arguments",
    "Checking if some parameters were set, otherwise setting to default values",
    "Estimate conditional estimates by default",
    "Setting more values",
    "Now saving the effect modifiers",
    "TODO Only works for binary treatment",
    "Defaulting to class default values if parameters are not provided",
    "Checking that there is at least one effect modifier",
    "Making sure that effect_modifier_names is a list",
    "Making a copy since we are going to be changing effect modifier names",
    "For every numeric effect modifier, adding a temp categorical column",
    "Grouping by effect modifiers and computing effect separately",
    "Deleting the temporary categorical columns",
    "The array that stores the results of all estimations",
    "Find the sample size the proportion with the population size",
    "Perform the set number of simulations",
    "Using class default parameters if not specified",
    "Checking if bootstrap_estimates are already computed",
    "Checked if any parameter is changed from the previous std error estimate",
    "Now use the data obtained from the simulations to get the value of the confidence estimates",
    "Sort the simulations",
    "Now we take the (1- p)th and the (p)th values, where p is the chosen confidence level",
    "get the values",
    "Use existing params, if new user defined params are not present",
    "Checking if bootstrap_estimates are already computed",
    "Check if any parameter is changed from the previous std error estimate",
    "Use existing params, if new user defined params are not present",
    "self._outcome = self._data[\"dummy_outcome\"]",
    "Processing the null hypothesis estimates",
    "Doing a two-sided test",
    "Being conservative with the p-value reported",
    "Being conservative with the p-value reported",
    "If the estimate_index is 0, it depends on the number of simulations",
    "Need to test r-squared before supporting",
    "effect_r_squared = self._evaluate_effect_strength(estimate, method=\"r-squared\")",
    "'r-squared': effect_r_squared",
    "elif method == \"r-squared\":",
    "outcome_mean = np.mean(self._outcome)",
    "total_variance = np.sum(np.square(self._outcome - outcome_mean))",
    "Assuming a linear model with one variable: the treatment",
    "Currently only works for continuous y",
    "causal_model = outcome_mean + estimate.value*self._treatment",
    "squared_residual = np.sum(np.square(self._outcome - causal_model))",
    "r_squared = 1 - (squared_residual/total_variance)",
    "return r_squared",
    "s += \"Variance in outcome explained by treatment: {}\\n\".format(self.effect_strength[\"r-squared\"])",
    "Below loop assumes that the last indices of W are alwawys converted to discrete",
    "one-hot encode discrete W",
    "Now deleting the old continuous value",
    "Making beta an array",
    "TODO Ensure that we do not generate weak instruments",
    "TODO - test all our methods with random noise added to covariates (instead of the stochastic treatment assignment)",
    "Converting treatment to binary if required",
    "Generating frontdoor variables if asked for",
    "Computing ATE",
    "constructing column names for one-hot encoded discrete features",
    "Specifying the correct dtypes",
    "Now specifying the corresponding graph strings",
    "Now writing the gml graph",
    "Making beta an array",
    "creating data frame",
    "Specifying the correct dtypes",
    "Now specifying the corresponding graph strings",
    "Now writing the gml graph",
    "Adding edges between common causes and the frontdoor mediator",
    "Error terms",
    "else:",
    "V = 6 + W0 + tterm + E1",
    "Y = 12 + W0*W0 + W0*W0 + yterm + E2  # E2_new",
    "Loading version number",
    "The currently supported estimators",
    "The default standard deviation for noise",
    "The default scaling factor to determine the bucket size",
    "The minimum number of points for the estimator to run",
    "The Default Transformation, when no arguments are given, or if the number of data points are insufficient for an estimator",
    "The Default True Causal Effect, this is taken to be ZERO by default",
    "The Default split for the number of data points that fall into the training and validation sets",
    "We need to change the identified estimand",
    "We thus, make a copy. This is done as we don't want",
    "to change the original DataFrame",
    "We use collections.OrderedDict to maintain the order in which the data is stored",
    "Check if we are using an estimator in the transformation list",
    "The rationale behind ordering of the loops is the fact that we induce randomness everytime we create the",
    "Train and the Validation Datasets. Thus, we run the simulation loop followed by the training and the validation",
    "loops. Thus, we can get different values everytime we get the estimator.",
    "Warn the user that the specified parameter is not applicable when no estimator is present in the transformation",
    "We set X_train = 0 and outcome_train to be 0",
    "Get the final outcome, after running through all the values in the transformation list",
    "Check if the value of true effect has been already stored",
    "We use None as the key as we have no base category for this refutation",
    "As we currently support only one treatment",
    "If the number of data points is too few, run the default transformation: [(\"zero\",\"\"),(\"noise\", {'std_dev':1} )]",
    "Check if the value of true effect has been already stored",
    "This ensures that we calculate the causal effect only once.",
    "We use key_train as we map data with respect to the base category of the data",
    "As we currently support only one treatment",
    "Add h(t) to f(W) to get the dummy outcome",
    "We convert to ndarray for ease in indexing",
    "The data is of the form",
    "sim1: cat1 cat2 ... catn",
    "sim2: cat1 cat2 ... catn",
    "Note: We would like the causal_estimator to find the true causal estimate that we have specified through this",
    "refuter. Let the value of the true causal effect be h(t). In the following section of code, we wish to find out if h(t) falls in the",
    "distribution of the refuter.",
    "True Causal Effect list",
    "Iterating through the refutation for each category",
    "We use string arguments to account for both 32 and 64 bit varaibles",
    "action for continuous variables",
    "Action for categorical variables",
    "Find the set difference for each row",
    "Choose one out of the remaining",
    "We want to see if the estimate falls in the same distribution as the one generated by the refuter",
    "Ideally that should be the case as running bootstrap should not have a significant effect on the ability",
    "of the treatment to affect the outcome",
    "Get a 2D matrix of values",
    "Store the values into the refute object",
    "Default value of the p value taken for the distribution",
    "Number of Trials: Number of cointosses to understand if a sample gets the treatment",
    "Mean of the Normal Distribution",
    "Standard Deviation of the Normal Distribution",
    "We need to change the identified estimand",
    "We make a copy as a safety measure, we don't want to change the",
    "original DataFrame",
    "Create a new column in the data by the name of placebo",
    "Sanity check the data",
    "Note: We hardcode the estimate value to ZERO as we want to check if it falls in the distribution of the refuter",
    "Ideally we should expect that ZERO should fall in the distribution of the effect estimates as we have severed any causal",
    "relationship between the treatment and the outcome.",
    "Adding a new backdoor variable to the identified estimand",
    "from https://www.bnmetrics.com/blog/factory-pattern-in-python3-simple-version",
    "The default subset of the data to be used",
    "We want to see if the estimate falls in the same distribution as the one generated by the refuter",
    "Ideally that should be the case as choosing a subset should not have a significant effect on the ability",
    "of the treatment to affect the outcome",
    "from https://www.bnmetrics.com/blog/factory-pattern-in-python3-simple-version",
    "Check if the treatment is one-dimensional",
    "first_stage_features = self.build_first_stage_features()",
    "fs_model = self.first_stage_model()",
    "if self._target_estimand.identifier_method==\"frontdoor\":",
    "first_stage_outcome = self._frontdoor_variables",
    "elif self._target_estimand.identifier_method==\"mediation\":",
    "first_stage_outcome = self._mediators",
    "fs_model.fit(first_stage_features, self._frontdoor_variables)",
    "self.logger.debug(\"Coefficients of the fitted model: \" +",
    "\",\".join(map(str, fs_model.coef_)))",
    "residuals = self._frontdoor_variables - fs_model.predict(first_stage_features)",
    "self._data[\"residual\"] = residuals",
    "First stage",
    "Second Stage",
    "Combining the two estimates",
    "This same estimate is valid for frontdoor as well as mediation (NIE)",
    "Total  effect of treatment",
    "Bulding the feature matrix",
    "features = sm.add_constant(features, has_constant='add') # to add an intercept term",
    "choosing the instrumental variable to use",
    "TODO move this to the identification step",
    "Obtain estimate by Wald Estimator",
    "Obtain estimate by 2SLS estimator: Cov(y,z) / Cov(x,z)",
    "More than 1 instrument. Use 2sls.",
    "We need to initialize the model when we create any propensity score estimator",
    "Check if the treatment is one-dimensional",
    "Checking if the treatment is binary",
    "Convert the categorical variables into dummy/indicator variables",
    "Basically, this gives a one hot encoding for each category",
    "The first category is taken to be the base line.",
    "TODO make treatment_value and control value also as local parameters",
    "Checking if the model is already trained",
    "The model is always built on the entire data",
    "All treatments are set to the same constant value",
    "Using all data by default",
    "Fixing treatment value to the specified value, if provided",
    "treatment_vals and data_df should have same number of rows",
    "Bulding the feature matrix",
    "The model is always built on the entire data",
    "Replacing treatment values by given x",
    "sort the dataframe by propensity score",
    "create a column 'strata' for each element that marks what strata it belongs to",
    "for each strata, count how many treated and control units there are",
    "throw away strata that have insufficient treatment or control",
    "print(\"before clipping, here is the distribution of treatment and control per strata\")",
    "print(self._data.groupby(['strata',self._treatment_name])[self._outcome_name].count())",
    "sum weighted outcomes over all strata  (weight by treated population)",
    "TODO - how can we add additional information into the returned estimate?",
    "such as how much clipping was done, or per-strata info for debugging?",
    "TODO -- fix: we are actually conditioning on positive treatment (d=1)",
    "trim propensity score weights",
    "ips ==> (isTreated(y)/ps(y)) + ((1-isTreated(y))/(1-ps(y)))",
    "nips ==> ips / (sum of ips over all units)",
    "icps ==> ps(y)/(1-ps(y)) / (sum of (ps(y)/(1-ps(y))) over all control units)",
    "itps ==> ps(y)/(1-ps(y)) / (sum of (ps(y)/(1-ps(y))) over all treatment units)",
    "Vanilla IPS estimator",
    "Also known as the Hajek estimator",
    "Stabilized weights",
    "Simple normalized estimator (commented out for now)",
    "ips_sum = self._data['ips_weight'].sum()",
    "self._data['nips_weight'] = self._data['ips_weight'] / ips_sum",
    "self._data['ips2'] = self._data['ps'] / (1 - self._data['ps'])",
    "treated_ips_sum = (self._data['ips2'] * self._data[self._treatment_name[0]]).sum()",
    "control_ips_sum = (self._data['ips2'] * (1 - self._data[self._treatment_name[0]])).sum()",
    "self._data['itps_weight'] = self._data['ips2'] / treated_ips_sum",
    "self._data['icps_weight'] = self._data['ips2'] / control_ips_sum",
    "Calculating the effect",
    "TODO - how can we add additional information into the returned estimate?",
    "TODO -- fix: we are actually conditioning on positive treatment (d=1)",
    "Checking if effect modifiers are a subset of common causes",
    "For metalearners only--issue a warning if w contains variables not in x",
    "Override the effect_modifiers set in CausalEstimator.__init__()",
    "Also only update self._effect_modifiers, and create a copy of self._effect_modifier_names",
    "the latter can be used by other estimator methods later",
    "Instrumental variables names, if present",
    "from https://www.bnmetrics.com/blog/factory-pattern-in-python3-simple-version",
    "Calling the econml estimator's fit method",
    "Changing shape to a list for a singleton value",
    "TODO -- fix: we are actually conditioning on positive treatment (d=1)",
    "this assumes a binary treatment regime",
    "TODO remove neighbors that are more than a given radius apart",
    "estimate ATT on treated by summing over difference between matched neighbors",
    "Now computing ATC",
    "TODO -- fix: we are actually conditioning on positive treatment (d=1)",
    "from https://www.bnmetrics.com/blog/factory-pattern-in-python3-simple-version",
    "Add the identification method used in the estimator",
    "Check the backdoor variables being used",
    "Add the observed confounders and one hot encode the categorical variables",
    "Get the data of the unobserved confounders",
    "One hot encode the data if they are categorical",
    "Check the instrumental variables involved",
    "Perform the same actions as the above",
    "Check if effect modifiers are used",
    "Get the class corresponding the the estimator to be used",
    "Initialize the object",
    "Both the outcome and the treatment have to be 1D arrays according to the CausalML API",
    "We want to pass 'v0' rather than ['v0'] to prevent a shape mismatch",
    "For CATEs",
    "TODO we are conditioning on a postive treatment",
    "TODO create an expression corresponding to each estimator used",
    "add weight column",
    "before weights are applied we count number rows in each category",
    "which is equivalent to summing over weight=1",
    "after weights are applied we need to sum over the given weights",
    "First, calculating mean differences by strata",
    "Second, without strata",
    "Third, concatenating them and plotting",
    "Setting estimator attribute for convenience",
    "Outcome is numeric",
    "Treatments are also numeric or binary",
    "Outcome is categorical",
    "Treatments are numeric or binary",
    "TODO: A common way to show all plots",
    "from https://www.bnmetrics.com/blog/factory-pattern-in-python3-simple-version",
    "self._identified_estimand = self._causal_model.identify_effect()",
    "self._identified_estimand,",
    "self._causal_model._treatment,",
    "self._causal_model._outcome,"
  ],
  "v0.5": [
    "Get the long description from the README file",
    "Get the required packages",
    "Loading version number",
    "-*- coding: utf-8 -*-",
    "",
    "Configuration file for the Sphinx documentation builder.",
    "",
    "This file does only contain a selection of the most common options. For a",
    "full list see the documentation:",
    "http://www.sphinx-doc.org/en/stable/config",
    "-- Path setup --------------------------------------------------------------",
    "If extensions (or modules to document with autodoc) are in another directory,",
    "add these directories to sys.path here. If the directory is relative to the",
    "documentation root, use os.path.abspath to make it absolute, like shown here.",
    "",
    "-- Project information -----------------------------------------------------",
    "The short X.Y version",
    "The full version, including alpha/beta/rc tags",
    "-- General configuration ---------------------------------------------------",
    "If your documentation needs a minimal Sphinx version, state it here.",
    "",
    "needs_sphinx = '1.0'",
    "Add any Sphinx extension module names here, as strings. They can be",
    "extensions coming with Sphinx (named 'sphinx.ext.*') or your custom",
    "ones.",
    "Add any paths that contain templates here, relative to this directory.",
    "The suffix(es) of source filenames.",
    "You can specify multiple suffix as a list of string:",
    "",
    "source_suffix = ['.rst', '.md']",
    "The master toctree document.",
    "The language for content autogenerated by Sphinx. Refer to documentation",
    "for a list of supported languages.",
    "",
    "This is also used if you do content translation via gettext catalogs.",
    "Usually you set \"language\" from the command line for these cases.",
    "List of patterns, relative to source directory, that match files and",
    "directories to ignore when looking for source files.",
    "This pattern also affects html_static_path and html_extra_path .",
    "The name of the Pygments (syntax highlighting) style to use.",
    "-- Options for HTML output -------------------------------------------------",
    "The theme to use for HTML and HTML Help pages.  See the documentation for",
    "a list of builtin themes.",
    "",
    "html_theme = 'sphinx-rtd-theme'",
    "on_rtd is whether we are on readthedocs.org",
    "only import and set the theme if we're building docs locally",
    "otherwise, readthedocs.org uses their theme by default, so no need to specify it",
    "Theme options are theme-specific and customize the look and feel of a theme",
    "further.  For a list of options available for each theme, see the",
    "documentation.",
    "",
    "html_theme_options = {}",
    "Add any paths that contain custom static files (such as style sheets) here,",
    "relative to this directory. They are copied after the builtin static files,",
    "so a file named \"default.css\" will overwrite the builtin \"default.css\".",
    "Custom sidebar templates, must be a dictionary that maps document names",
    "to template names.",
    "",
    "The default sidebars (for documents that don't match any pattern) are",
    "defined by theme itself.  Builtin themes are using these templates by",
    "default: ``['localtoc.html', 'relations.html', 'sourcelink.html',",
    "'searchbox.html']``.",
    "",
    "html_sidebars = {}",
    "-- Options for HTMLHelp output ---------------------------------------------",
    "Output file base name for HTML help builder.",
    "-- Options for LaTeX output ------------------------------------------------",
    "The paper size ('letterpaper' or 'a4paper').",
    "",
    "'papersize': 'letterpaper',",
    "The font size ('10pt', '11pt' or '12pt').",
    "",
    "'pointsize': '10pt',",
    "Additional stuff for the LaTeX preamble.",
    "",
    "'preamble': '',",
    "Latex figure (float) alignment",
    "",
    "'figure_align': 'htbp',",
    "Grouping the document tree into LaTeX files. List of tuples",
    "(source start file, target name, title,",
    "author, documentclass [howto, manual, or own class]).",
    "-- Options for manual page output ------------------------------------------",
    "One entry per manual page. List of tuples",
    "(source start file, name, description, authors, manual section).",
    "-- Options for Texinfo output ----------------------------------------------",
    "Grouping the document tree into Texinfo files. List of tuples",
    "(source start file, target name, title, author,",
    "dir menu entry, description, category)",
    "-- Options for Epub output -------------------------------------------------",
    "Bibliographic Dublin Core info.",
    "The unique identifier of the text. This can be a ISBN number",
    "or the project homepage.",
    "",
    "epub_identifier = ''",
    "A unique identification for the text.",
    "",
    "epub_uid = ''",
    "A list of files that should not be packed into the epub file.",
    "-- Extension configuration -------------------------------------------------",
    "-- Options for todo extension ----------------------------------------------",
    "If true, `todo` and `todoList` produce output, else they produce nothing.",
    "self.df = pd.read_csv(os.path.join(DATA_PATH,'dgp_1/acic_1_1_data.csv'))",
    "self.ate = np.mean(self.df['y1'] - self.df['y0'])",
    "treated = self.df[self.df['z']==1]",
    "self.att = np.mean(treated['y1'] - treated['y0'])",
    "def test_average_treatment_effect(self):",
    "est_ate = 1",
    "bias = est_ate - self.ate",
    "print(bias)",
    "self.assertAlmostEqual(self.ate, est_ate)",
    "def test_average_treatment_effect_on_treated(self):",
    "est_att = 1",
    "self.att=1",
    "bias = est_att - self.att",
    "print(bias)",
    "self.assertAlmostEqual(self.att, est_att)",
    "removing two common causes",
    "The outcome is a linear function of the confounder",
    "The slope is 1,2 and the intercept is 3",
    "As we run with only one common cause and one instrument variable we run with (?, 2)",
    "Supports user-provided dataset object",
    "To test if there are any exceptions",
    "To test if the estimate is identical if refutation parameters are zero",
    "This value is hardcoded to be zero as we are runnning this on a linear dataset.",
    "Ordinarily, we should expect this value to be zero.",
    "This value is hardcoded to be zero as we are runnning this on a linear dataset.",
    "Ordinarily, we should expect this value to be zero.",
    "cov_mat = np.diag(np.ones(num_features))",
    "Setup data",
    "Test LinearDMLCateEstimator",
    "Test ContinuousTreatmentOrthoForest",
    "Test LinearDRLearner",
    "Setup data",
    "Test DeepIV",
    "TODO: Test IntentToTreatDRIV when EconML v0.7 comes out",
    "Not using testsuite from .base/TestEstimtor, custom code below",
    "More cases where Exception  is expected",
    "Compute confidence intervals, standard error and significance tests",
    "Defined a linear dataset with a given set of properties",
    "Create a model that captures the same",
    "Identify the effects within the model",
    "Defined a linear dataset with a given set of properties",
    "Create a model that captures the same",
    "Identify the effects within the model",
    "Defined a linear dataset with a given set of properties",
    "Create a model that captures the same",
    "Identify the effects within the model",
    "Defined a linear dataset with a given set of properties",
    "Create a model that captures the same",
    "Identify the effects within the model",
    "Defined a linear dataset with a given set of properties",
    "Create a model that captures the same",
    "Identify the effects within the model",
    "## 1. BACKDOOR IDENTIFICATION",
    "First, checking if there are any valid backdoor adjustment sets",
    "Setting default \"backdoor\" identification adjustment set",
    "## 2. INSTRUMENTAL VARIABLE IDENTIFICATION",
    "Now checking if there is also a valid iv estimand",
    "## 3. FRONTDOOR IDENTIFICATION",
    "Now checking if there is a valid frontdoor variable",
    "Finally returning the estimand object",
    "## 1. FIRST DOING BACKDOOR IDENTIFICATION",
    "First, checking if there are any valid backdoor adjustment sets",
    "Setting default \"backdoor\" identification adjustment set",
    "## 2. SECOND, CHECKING FOR MEDIATORS",
    "Now checking if there are valid mediator variables",
    "Finally returning the estimand object",
    "## 1. FIRST DOING BACKDOOR IDENTIFICATION",
    "First, checking if there are any valid backdoor adjustment sets",
    "Setting default \"backdoor\" identification adjustment set",
    "## 2. SECOND, CHECKING FOR MEDIATORS",
    "Now checking if there are valid mediator variables",
    "Finally returning the estimand object",
    "First, checking if empty set is a valid backdoor set",
    "Second, checking for all other sets of variables",
    "causes_t = self._graph.get_causes(self.treatment_name)",
    "causes_y = self._graph.get_causes(self.outcome_name, remove_edges={'sources':self.treatment_name, 'targets':self.outcome_name})",
    "common_causes = list(causes_t.intersection(causes_y))",
    "self.logger.info(\"Common causes of treatment and outcome:\" + str(common_causes))",
    "Adding a None estimand if no backdoor set found",
    "Default set is the one with the most number of adjustment variables (optimizing for minimum (unknown) bias not for efficiency)",
    "For simplicity, assuming a one-variable frontdoor set",
    "For simplicity, assuming a one-variable mediation set",
    "Create estimands dict as per the API for backdoor, but do not return it",
    "Setting default \"backdoor\" identification adjustment set",
    "Create estimands dict as per the API for backdoor, but do not return it",
    "Setting default \"backdoor\" identification adjustment set",
    "Adding a None estimand if no backdoor set found",
    "Default set is the one with the most number of adjustment variables (optimizing for minimum (unknown) bias not for efficiency)",
    "TODO: outputs string for now, but ideally should do symbolic",
    "expressions Mon 19 Feb 2018 04:54:17 PM DST",
    "TODO Better support for multivariate treatments",
    "sym_common_causes = [sp.stats.Normal(common_cause, sym_mu, sym_sigma) for common_cause in common_causes]",
    "TODO: support multivariate treatments better.",
    "TODO: support multivariate treatments better.",
    "TODO: support multivariate treatments better.",
    "Do not show backdoor key unless it is the only backdoor set.",
    "Can use these lists to specify the models/estimators/refuters that a particular interpreter supports.  Throw a ValueError if the user provides an incompatible object to intepret.",
    "Unpacking the keyword arguments",
    "Default value for the number of simulations to be conducted",
    "Concatenate the confounders, instruments and effect modifiers",
    "Shuffle the confounders",
    "Check if all are select or deselect variables",
    "Check if all the required_variables belong to confounders, instrumental variables or effect",
    "Initializing the p_value",
    "Perform Bootstrap Significance Test with the original estimate and the set of refutations",
    "Perform Normal Tests of Significance with the original estimate and the set of refutations",
    "Perform Bootstrap Significance Test with the original estimate and the set of refutations",
    "Perform Normal Tests of Significance with the original estimate and the set of refutations",
    "Get the number of simulations",
    "Sort the simulations",
    "Obtain the median value",
    "Performing a two sided test",
    "np.searchsorted tells us the index if it were a part of the array",
    "We select side to be left as we want to find the first value that matches",
    "We subtact 1 as we are finding the value from the right tail",
    "We take the side to be right as we want to find the last index that matches",
    "We get the probability with respect to the left tail.",
    "Get the mean for the simulations",
    "Get the standard deviation for the simulations",
    "Get the Z Score [(val - mean)/ std_dev ]",
    "load dot file",
    "Adding node attributes",
    "TODO do not add it here. CausalIdentifier should call causal_graph to add an unobserved common cause if needed. This also ensures that we do not need get_common_causes in this class.",
    "Adding common causes",
    "Adding instruments",
    "Adding effect modifiers",
    "Adding columns in the dataframe as confounders that were not in the graph",
    "Adding unobserved confounders",
    "also return the number of backdoor paths blocked by observed nodes",
    "remove paths that have nodes1\\node1 or nodes2\\node2 as intermediate nodes",
    "TODO Refactor to remove this from here and only implement this logic in causalIdentifier. Unnecessary assumption of nodes1 to be causing nodes2.",
    "Cannot simply compute ancestors, since that will also include nodes1 and its parents (e.g. instruments)",
    "removing all mediators",
    "[TODO: double check these work with multivariate implementation:]",
    "Exclusion",
    "As-if-random setup",
    "As-if-random",
    "TODO: move the logging level argument to a json file. Tue 20 Feb 2018 06:56:27 PM DST",
    "Sometimes, effect modifiers from the graph may not match those provided by the user.",
    "(Because some effect modifiers may also be common causes)",
    "In such cases, the user-provided modifiers are used.",
    "If no effect modifiers are provided,  then the ones from the graph are used.",
    "TODO add propensity score as default backdoor method, iv as default iv method, add an informational message to show which method has been selected.",
    "TODO add dowhy as a prefix to all dowhy estimators",
    "This is done as all dowhy estimators have two parts and external ones have two or more parts",
    "Define the third-party estimation method to be used",
    "Process the dowhy estimators",
    "Check if estimator's target estimand is identified",
    "Store parameters inside estimate object for refutation methods",
    "Check if estimator's target estimand is identified",
    "The default number of simulations for statistical testing",
    "The default number of simulations to obtain confidence intervals",
    "The portion of the total size that should be taken each time to find the confidence intervals",
    "1 is the recommended value",
    "https://ocw.mit.edu/courses/mathematics/18-05-introduction-to-probability-and-statistics-spring-2014/readings/MIT18_05S14_Reading24.pdf",
    "https://projecteuclid.org/download/pdf_1/euclid.ss/1032280214",
    "The default Confidence Level",
    "Number of quantiles to discretize continuous columns, for applying groupby",
    "Prefix to add to temporary categorical variables created after discretization",
    "Currently estimation methods only support univariate treatment and outcome",
    "Setting the default interpret method",
    "Unpacking the keyword arguments",
    "Checking if some parameters were set, otherwise setting to default values",
    "Estimate conditional estimates by default",
    "Setting more values",
    "Now saving the effect modifiers",
    "TODO Only works for binary treatment",
    "Defaulting to class default values if parameters are not provided",
    "Checking that there is at least one effect modifier",
    "Making sure that effect_modifier_names is a list",
    "Making a copy since we are going to be changing effect modifier names",
    "For every numeric effect modifier, adding a temp categorical column",
    "Grouping by effect modifiers and computing effect separately",
    "Deleting the temporary categorical columns",
    "The array that stores the results of all estimations",
    "Find the sample size the proportion with the population size",
    "Perform the set number of simulations",
    "Using class default parameters if not specified",
    "Checking if bootstrap_estimates are already computed",
    "Checked if any parameter is changed from the previous std error estimate",
    "Now use the data obtained from the simulations to get the value of the confidence estimates",
    "Sort the simulations",
    "Now we take the (1- p)th and the (p)th values, where p is the chosen confidence level",
    "get the values",
    "Use existing params, if new user defined params are not present",
    "Checking if bootstrap_estimates are already computed",
    "Check if any parameter is changed from the previous std error estimate",
    "Use existing params, if new user defined params are not present",
    "self._outcome = self._data[\"dummy_outcome\"]",
    "Processing the null hypothesis estimates",
    "Doing a two-sided test",
    "Being conservative with the p-value reported",
    "Being conservative with the p-value reported",
    "If the estimate_index is 0, it depends on the number of simulations",
    "Need to test r-squared before supporting",
    "effect_r_squared = self._evaluate_effect_strength(estimate, method=\"r-squared\")",
    "'r-squared': effect_r_squared",
    "elif method == \"r-squared\":",
    "outcome_mean = np.mean(self._outcome)",
    "total_variance = np.sum(np.square(self._outcome - outcome_mean))",
    "Assuming a linear model with one variable: the treatment",
    "Currently only works for continuous y",
    "causal_model = outcome_mean + estimate.value*self._treatment",
    "squared_residual = np.sum(np.square(self._outcome - causal_model))",
    "r_squared = 1 - (squared_residual/total_variance)",
    "return r_squared",
    "s += \"Variance in outcome explained by treatment: {}\\n\".format(self.effect_strength[\"r-squared\"])",
    "Below loop assumes that the last indices of W are alwawys converted to discrete",
    "one-hot encode discrete W",
    "Now deleting the old continuous value",
    "Making beta an array",
    "TODO Ensure that we do not generate weak instruments",
    "TODO - test all our methods with random noise added to covariates (instead of the stochastic treatment assignment)",
    "Converting treatment to binary if required",
    "Generating frontdoor variables if asked for",
    "Computing ATE",
    "constructing column names for one-hot encoded discrete features",
    "Specifying the correct dtypes",
    "Now specifying the corresponding graph strings",
    "Now writing the gml graph",
    "Making beta an array",
    "creating data frame",
    "Specifying the correct dtypes",
    "Now specifying the corresponding graph strings",
    "Now writing the gml graph",
    "Adding edges between common causes and the frontdoor mediator",
    "Error terms",
    "else:",
    "V = 6 + W0 + tterm + E1",
    "Y = 12 + W0*W0 + W0*W0 + yterm + E2  # E2_new",
    "Loading version number",
    "The currently supported estimators",
    "The default standard deviation for noise",
    "The default scaling factor to determine the bucket size",
    "The minimum number of points for the estimator to run",
    "The Default Transformation, when no arguments are given, or if the number of data points are insufficient for an estimator",
    "The Default True Causal Effect, this is taken to be ZERO by default",
    "The Default split for the number of data points that fall into the training and validation sets",
    "We need to change the identified estimand",
    "We thus, make a copy. This is done as we don't want",
    "to change the original DataFrame",
    "We use collections.OrderedDict to maintain the order in which the data is stored",
    "Check if we are using an estimator in the transformation list",
    "The rationale behind ordering of the loops is the fact that we induce randomness everytime we create the",
    "Train and the Validation Datasets. Thus, we run the simulation loop followed by the training and the validation",
    "loops. Thus, we can get different values everytime we get the estimator.",
    "Warn the user that the specified parameter is not applicable when no estimator is present in the transformation",
    "We set X_train = 0 and outcome_train to be 0",
    "Get the final outcome, after running through all the values in the transformation list",
    "Check if the value of true effect has been already stored",
    "We use None as the key as we have no base category for this refutation",
    "As we currently support only one treatment",
    "If the number of data points is too few, run the default transformation: [(\"zero\",\"\"),(\"noise\", {'std_dev':1} )]",
    "Check if the value of true effect has been already stored",
    "This ensures that we calculate the causal effect only once.",
    "We use key_train as we map data with respect to the base category of the data",
    "As we currently support only one treatment",
    "Add h(t) to f(W) to get the dummy outcome",
    "We convert to ndarray for ease in indexing",
    "The data is of the form",
    "sim1: cat1 cat2 ... catn",
    "sim2: cat1 cat2 ... catn",
    "Note: We would like the causal_estimator to find the true causal estimate that we have specified through this",
    "refuter. Let the value of the true causal effect be h(t). In the following section of code, we wish to find out if h(t) falls in the",
    "distribution of the refuter.",
    "True Causal Effect list",
    "Iterating through the refutation for each category",
    "We use string arguments to account for both 32 and 64 bit varaibles",
    "action for continuous variables",
    "Action for categorical variables",
    "Find the set difference for each row",
    "Choose one out of the remaining",
    "We want to see if the estimate falls in the same distribution as the one generated by the refuter",
    "Ideally that should be the case as running bootstrap should not have a significant effect on the ability",
    "of the treatment to affect the outcome",
    "Get a 2D matrix of values",
    "Store the values into the refute object",
    "Default value of the p value taken for the distribution",
    "Number of Trials: Number of cointosses to understand if a sample gets the treatment",
    "Mean of the Normal Distribution",
    "Standard Deviation of the Normal Distribution",
    "We need to change the identified estimand",
    "We make a copy as a safety measure, we don't want to change the",
    "original DataFrame",
    "Create a new column in the data by the name of placebo",
    "Sanity check the data",
    "Note: We hardcode the estimate value to ZERO as we want to check if it falls in the distribution of the refuter",
    "Ideally we should expect that ZERO should fall in the distribution of the effect estimates as we have severed any causal",
    "relationship between the treatment and the outcome.",
    "Adding a new backdoor variable to the identified estimand",
    "from https://www.bnmetrics.com/blog/factory-pattern-in-python3-simple-version",
    "The default subset of the data to be used",
    "We want to see if the estimate falls in the same distribution as the one generated by the refuter",
    "Ideally that should be the case as choosing a subset should not have a significant effect on the ability",
    "of the treatment to affect the outcome",
    "from https://www.bnmetrics.com/blog/factory-pattern-in-python3-simple-version",
    "Check if the treatment is one-dimensional",
    "first_stage_features = self.build_first_stage_features()",
    "fs_model = self.first_stage_model()",
    "if self._target_estimand.identifier_method==\"frontdoor\":",
    "first_stage_outcome = self._frontdoor_variables",
    "elif self._target_estimand.identifier_method==\"mediation\":",
    "first_stage_outcome = self._mediators",
    "fs_model.fit(first_stage_features, self._frontdoor_variables)",
    "self.logger.debug(\"Coefficients of the fitted model: \" +",
    "\",\".join(map(str, fs_model.coef_)))",
    "residuals = self._frontdoor_variables - fs_model.predict(first_stage_features)",
    "self._data[\"residual\"] = residuals",
    "First stage",
    "Second Stage",
    "Combining the two estimates",
    "Total  effect of treatment",
    "Bulding the feature matrix",
    "features = sm.add_constant(features, has_constant='add') # to add an intercept term",
    "choosing the instrumental variable to use",
    "TODO move this to the identification step",
    "Obtain estimate by Wald Estimator",
    "Obtain estimate by 2SLS estimator: Cov(y,z) / Cov(x,z)",
    "More than 1 instrument. Use 2sls.",
    "We need to initialize the model when we create any propensity score estimator",
    "Check if the treatment is one-dimensional",
    "Checking if the treatment is binary",
    "Convert the categorical variables into dummy/indicator variables",
    "Basically, this gives a one hot encoding for each category",
    "The first category is taken to be the base line.",
    "TODO make treatment_value and control value also as local parameters",
    "Checking if the model is already trained",
    "The model is always built on the entire data",
    "All treatments are set to the same constant value",
    "Using all data by default",
    "Fixing treatment value to the specified value, if provided",
    "treatment_vals and data_df should have same number of rows",
    "Bulding the feature matrix",
    "The model is always built on the entire data",
    "Replacing treatment values by given x",
    "sort the dataframe by propensity score",
    "create a column 'strata' for each element that marks what strata it belongs to",
    "for each strata, count how many treated and control units there are",
    "throw away strata that have insufficient treatment or control",
    "print(\"before clipping, here is the distribution of treatment and control per strata\")",
    "print(self._data.groupby(['strata',self._treatment_name])[self._outcome_name].count())",
    "sum weighted outcomes over all strata  (weight by treated population)",
    "TODO - how can we add additional information into the returned estimate?",
    "such as how much clipping was done, or per-strata info for debugging?",
    "TODO -- fix: we are actually conditioning on positive treatment (d=1)",
    "trim propensity score weights",
    "ips ==> (isTreated(y)/ps(y)) + ((1-isTreated(y))/(1-ps(y)))",
    "nips ==> ips / (sum of ips over all units)",
    "icps ==> ps(y)/(1-ps(y)) / (sum of (ps(y)/(1-ps(y))) over all control units)",
    "itps ==> ps(y)/(1-ps(y)) / (sum of (ps(y)/(1-ps(y))) over all treatment units)",
    "Vanilla IPS estimator",
    "Also known as the Hajek estimator",
    "Stabilized weights",
    "Simple normalized estimator (commented out for now)",
    "ips_sum = self._data['ips_weight'].sum()",
    "self._data['nips_weight'] = self._data['ips_weight'] / ips_sum",
    "self._data['ips2'] = self._data['ps'] / (1 - self._data['ps'])",
    "treated_ips_sum = (self._data['ips2'] * self._data[self._treatment_name[0]]).sum()",
    "control_ips_sum = (self._data['ips2'] * (1 - self._data[self._treatment_name[0]])).sum()",
    "self._data['itps_weight'] = self._data['ips2'] / treated_ips_sum",
    "self._data['icps_weight'] = self._data['ips2'] / control_ips_sum",
    "Calculating the effect",
    "TODO - how can we add additional information into the returned estimate?",
    "TODO -- fix: we are actually conditioning on positive treatment (d=1)",
    "Checking if effect modifiers are a subset of common causes",
    "Instrumental variables names, if present",
    "from https://www.bnmetrics.com/blog/factory-pattern-in-python3-simple-version",
    "Calling the econml estimator's fit method",
    "Changing shape to a list for a singleton value",
    "TODO -- fix: we are actually conditioning on positive treatment (d=1)",
    "this assumes a binary treatment regime",
    "TODO remove neighbors that are more than a given radius apart",
    "estimate ATT on treated by summing over difference between matched neighbors",
    "Now computing ATC",
    "TODO -- fix: we are actually conditioning on positive treatment (d=1)",
    "from https://www.bnmetrics.com/blog/factory-pattern-in-python3-simple-version",
    "Add the identification method used in the estimator",
    "Check the backdoor variables being used",
    "Add the observed confounders and one hot encode the categorical variables",
    "Get the data of the unobserved confounders",
    "One hot encode the data if they are categorical",
    "Check the instrumental variables involved",
    "Perform the same actions as the above",
    "Check if effect modifiers are used",
    "Get the class corresponding the the estimator to be used",
    "Initialize the object",
    "Both the outcome and the treatment have to be 1D arrays according to the CausalML API",
    "We want to pass 'v0' rather than ['v0'] to prevent a shape mismatch",
    "For CATEs",
    "TODO we are conditioning on a postive treatment",
    "TODO create an expression corresponding to each estimator used",
    "add weight column",
    "before weights are applied we count number rows in each category",
    "which is equivalent to summing over weight=1",
    "after weights are applied we need to sum over the given weights",
    "First, calculating mean differences by strata",
    "Second, without strata",
    "Third, concatenating them and plotting",
    "Setting estimator attribute for convenience",
    "Outcome is numeric",
    "Treatments are also numeric or binary",
    "Outcome is categorical",
    "Treatments are numeric or binary",
    "TODO: A common way to show all plots",
    "from https://www.bnmetrics.com/blog/factory-pattern-in-python3-simple-version",
    "self._identified_estimand = self._causal_model.identify_effect()",
    "self._identified_estimand,",
    "self._causal_model._treatment,",
    "self._causal_model._outcome,"
  ],
  "v0.4": [
    "Get the long description from the README file",
    "Get the required packages",
    "Loading version number",
    "-*- coding: utf-8 -*-",
    "",
    "Configuration file for the Sphinx documentation builder.",
    "",
    "This file does only contain a selection of the most common options. For a",
    "full list see the documentation:",
    "http://www.sphinx-doc.org/en/stable/config",
    "-- Path setup --------------------------------------------------------------",
    "If extensions (or modules to document with autodoc) are in another directory,",
    "add these directories to sys.path here. If the directory is relative to the",
    "documentation root, use os.path.abspath to make it absolute, like shown here.",
    "",
    "-- Project information -----------------------------------------------------",
    "The short X.Y version",
    "The full version, including alpha/beta/rc tags",
    "-- General configuration ---------------------------------------------------",
    "If your documentation needs a minimal Sphinx version, state it here.",
    "",
    "needs_sphinx = '1.0'",
    "Add any Sphinx extension module names here, as strings. They can be",
    "extensions coming with Sphinx (named 'sphinx.ext.*') or your custom",
    "ones.",
    "Add any paths that contain templates here, relative to this directory.",
    "The suffix(es) of source filenames.",
    "You can specify multiple suffix as a list of string:",
    "",
    "source_suffix = ['.rst', '.md']",
    "The master toctree document.",
    "The language for content autogenerated by Sphinx. Refer to documentation",
    "for a list of supported languages.",
    "",
    "This is also used if you do content translation via gettext catalogs.",
    "Usually you set \"language\" from the command line for these cases.",
    "List of patterns, relative to source directory, that match files and",
    "directories to ignore when looking for source files.",
    "This pattern also affects html_static_path and html_extra_path .",
    "The name of the Pygments (syntax highlighting) style to use.",
    "-- Options for HTML output -------------------------------------------------",
    "The theme to use for HTML and HTML Help pages.  See the documentation for",
    "a list of builtin themes.",
    "",
    "html_theme = 'sphinx-rtd-theme'",
    "on_rtd is whether we are on readthedocs.org",
    "only import and set the theme if we're building docs locally",
    "otherwise, readthedocs.org uses their theme by default, so no need to specify it",
    "Theme options are theme-specific and customize the look and feel of a theme",
    "further.  For a list of options available for each theme, see the",
    "documentation.",
    "",
    "html_theme_options = {}",
    "Add any paths that contain custom static files (such as style sheets) here,",
    "relative to this directory. They are copied after the builtin static files,",
    "so a file named \"default.css\" will overwrite the builtin \"default.css\".",
    "Custom sidebar templates, must be a dictionary that maps document names",
    "to template names.",
    "",
    "The default sidebars (for documents that don't match any pattern) are",
    "defined by theme itself.  Builtin themes are using these templates by",
    "default: ``['localtoc.html', 'relations.html', 'sourcelink.html',",
    "'searchbox.html']``.",
    "",
    "html_sidebars = {}",
    "-- Options for HTMLHelp output ---------------------------------------------",
    "Output file base name for HTML help builder.",
    "-- Options for LaTeX output ------------------------------------------------",
    "The paper size ('letterpaper' or 'a4paper').",
    "",
    "'papersize': 'letterpaper',",
    "The font size ('10pt', '11pt' or '12pt').",
    "",
    "'pointsize': '10pt',",
    "Additional stuff for the LaTeX preamble.",
    "",
    "'preamble': '',",
    "Latex figure (float) alignment",
    "",
    "'figure_align': 'htbp',",
    "Grouping the document tree into LaTeX files. List of tuples",
    "(source start file, target name, title,",
    "author, documentclass [howto, manual, or own class]).",
    "-- Options for manual page output ------------------------------------------",
    "One entry per manual page. List of tuples",
    "(source start file, name, description, authors, manual section).",
    "-- Options for Texinfo output ----------------------------------------------",
    "Grouping the document tree into Texinfo files. List of tuples",
    "(source start file, target name, title, author,",
    "dir menu entry, description, category)",
    "-- Options for Epub output -------------------------------------------------",
    "Bibliographic Dublin Core info.",
    "The unique identifier of the text. This can be a ISBN number",
    "or the project homepage.",
    "",
    "epub_identifier = ''",
    "A unique identification for the text.",
    "",
    "epub_uid = ''",
    "A list of files that should not be packed into the epub file.",
    "-- Extension configuration -------------------------------------------------",
    "-- Options for todo extension ----------------------------------------------",
    "If true, `todo` and `todoList` produce output, else they produce nothing.",
    "self.df = pd.read_csv(os.path.join(DATA_PATH,'dgp_1/acic_1_1_data.csv'))",
    "self.ate = np.mean(self.df['y1'] - self.df['y0'])",
    "treated = self.df[self.df['z']==1]",
    "self.att = np.mean(treated['y1'] - treated['y0'])",
    "def test_average_treatment_effect(self):",
    "est_ate = 1",
    "bias = est_ate - self.ate",
    "print(bias)",
    "self.assertAlmostEqual(self.ate, est_ate)",
    "def test_average_treatment_effect_on_treated(self):",
    "est_att = 1",
    "self.att=1",
    "bias = est_att - self.att",
    "print(bias)",
    "self.assertAlmostEqual(self.att, est_att)",
    "removing two common causes",
    "The outcome is a linear function of the confounder",
    "The slope is 2 and the intercept is 3",
    "As we run with only one common cause and one instrument variable we run with (?, 2)",
    "Supports user-provided dataset object",
    "To test if there are any exceptions",
    "To test if the estimate is identical if refutation parameters are zero",
    "This value is hardcoded to be zero as we are runnning this on a linear dataset.",
    "Ordinarily, we should expect this value to be zero.",
    "This value is hardcoded to be zero as we are runnning this on a linear dataset.",
    "Ordinarily, we should expect this value to be zero.",
    "cov_mat = np.diag(np.ones(num_features))",
    "Setup data",
    "Test LinearDMLCateEstimator",
    "Test ContinuousTreatmentOrthoForest",
    "Test LinearDRLearner",
    "Setup data",
    "Test DeepIV",
    "TODO: Test IntentToTreatDRIV when EconML v0.7 comes out",
    "Not using testsuite from .base/TestEstimtor, custom code below",
    "More cases where Exception  is expected",
    "Compute confidence intervals, standard error and significance tests",
    "Defined a linear dataset with a given set of properties",
    "Create a model that captures the same",
    "Identify the effects within the model",
    "Defined a linear dataset with a given set of properties",
    "Create a model that captures the same",
    "Identify the effects within the model",
    "Defined a linear dataset with a given set of properties",
    "Create a model that captures the same",
    "Identify the effects within the model",
    "Defined a linear dataset with a given set of properties",
    "Create a model that captures the same",
    "Identify the effects within the model",
    "Defined a linear dataset with a given set of properties",
    "Create a model that captures the same",
    "Identify the effects within the model",
    "Now checking if there is also a valid iv estimand",
    "TODO: outputs string for now, but ideally should do symbolic",
    "expressions Mon 19 Feb 2018 04:54:17 PM DST",
    "TODO Better support for multivariate treatments",
    "sym_common_causes = [sp.stats.Normal(common_cause, sym_mu, sym_sigma) for common_cause in common_causes]",
    "TODO: support multivariate treatments better.",
    "Default value for the number of simulations to be conducted",
    "Concatenate the confounders, instruments and effect modifiers",
    "Shuffle the confounders",
    "Check if all are select or deselect variables",
    "Check if all the required_variables belong to confounders, instrumental variables or effect",
    "Initializing the p_value",
    "Perform Bootstrap Significance Test with the original estimate and the set of refutations",
    "Perform Normal Tests of Significance with the original estimate and the set of refutations",
    "Perform Bootstrap Significance Test with the original estimate and the set of refutations",
    "Perform Normal Tests of Significance with the original estimate and the set of refutations",
    "Get the number of simulations",
    "Sort the simulations",
    "Obtain the median value",
    "Performing a two sided test",
    "np.searchsorted tells us the index if it were a part of the array",
    "We select side to be left as we want to find the first value that matches",
    "We subtact 1 as we are finding the value from the right tail",
    "We take the side to be right as we want to find the last index that matches",
    "We get the probability with respect to the left tail.",
    "Get the mean for the simulations",
    "Get the standard deviation for the simulations",
    "Get the Z Score [(val - mean)/ std_dev ]",
    "load dot file",
    "Adding node attributes",
    "TODO do not add it here. CausalIdentifier should call causal_graph to add an unobserved common cause if needed. This also ensures that we do not need get_common_causes in this class.",
    "Adding common causes",
    "Adding instruments",
    "Adding effect modifiers",
    "Adding columns in the dataframe as confounders that were not in the graph",
    "Adding unobserved confounders",
    "TODO Refactor to remove this from here and only implement this logic in causalIdentifier. Unnecessary assumption of nodes1 to be causing nodes2.",
    "Cannot simply compute ancestors, since that will also include nodes1 and its parents (e.g. instruments)",
    "[TODO: double check these work with multivariate implementation:]",
    "Exclusion",
    "As-if-random setup",
    "As-if-random",
    "TODO: move the logging level argument to a json file. Tue 20 Feb 2018 06:56:27 PM DST",
    "Sometimes, effect modifiers from the graph may not match those provided by the user.",
    "(Because some effect modifiers may also be common causes)",
    "In such cases, the user-provided modifiers are used.",
    "If no effect modifiers are provided,  then the ones from the graph are used.",
    "TODO add propensity score as default backdoor method, iv as default iv method, add an informational message to show which method has been selected.",
    "TODO add dowhy as a prefix to all dowhy estimators",
    "This is done as all dowhy estimators have two parts and external ones have two or more parts",
    "Define the third-party estimation method to be used",
    "Process the dowhy estimators",
    "Check if estimator's target estimand is identified",
    "Store parameters inside estimate object for refutation methods",
    "Check if estimator's target estimand is identified",
    "The default number of simulations for statistical testing",
    "The default number of simulations to obtain confidence intervals",
    "The portion of the total size that should be taken each time to find the confidence intervals",
    "1 is the recommended value",
    "https://ocw.mit.edu/courses/mathematics/18-05-introduction-to-probability-and-statistics-spring-2014/readings/MIT18_05S14_Reading24.pdf",
    "https://projecteuclid.org/download/pdf_1/euclid.ss/1032280214",
    "The default Confidence Level",
    "Number of quantiles to discretize continuous columns, for applying groupby",
    "Prefix to add to temporary categorical variables created after discretization",
    "Currently estimation methods only support univariate treatment and outcome",
    "Unpacking the keyword arguments",
    "Checking if some parameters were set, otherwise setting to default values",
    "Setting more values",
    "Now saving the effect modifiers",
    "TODO Only works for binary treatment",
    "Defaulting to class default values if parameters are not provided",
    "Checking that there is at least one effect modifier",
    "Making sure that effect_modifier_names is a list",
    "Making a copy since we are going to be changing effect modifier names",
    "For every numeric effect modifier, adding a temp categorical column",
    "Grouping by effect modifiers and computing effect separately",
    "Deleting the temporary categorical columns",
    "The array that stores the results of all estimations",
    "Find the sample size the proportion with the population size",
    "Perform the set number of simulations",
    "Using class default parameters if not specified",
    "Checking if bootstrap_estimates are already computed",
    "Checked if any parameter is changed from the previous std error estimate",
    "Now use the data obtained from the simulations to get the value of the confidence estimates",
    "Sort the simulations",
    "Now we take the (1- p)th and the (p)th values, where p is the chosen confidence level",
    "get the values",
    "Use existing params, if new user defined params are not present",
    "Checking if bootstrap_estimates are already computed",
    "Check if any parameter is changed from the previous std error estimate",
    "Use existing params, if new user defined params are not present",
    "self._outcome = self._data[\"dummy_outcome\"]",
    "Processing the null hypothesis estimates",
    "Doing a two-sided test",
    "Being conservative with the p-value reported",
    "Being conservative with the p-value reported",
    "If the estimate_index is 0, it depends on the number of simulations",
    "Need to test r-squared before supporting",
    "effect_r_squared = self._evaluate_effect_strength(estimate, method=\"r-squared\")",
    "'r-squared': effect_r_squared",
    "elif method == \"r-squared\":",
    "outcome_mean = np.mean(self._outcome)",
    "total_variance = np.sum(np.square(self._outcome - outcome_mean))",
    "Assuming a linear model with one variable: the treatment",
    "Currently only works for continuous y",
    "causal_model = outcome_mean + estimate.value*self._treatment",
    "squared_residual = np.sum(np.square(self._outcome - causal_model))",
    "r_squared = 1 - (squared_residual/total_variance)",
    "return r_squared",
    "s += \"Variance in outcome explained by treatment: {}\\n\".format(self.effect_strength[\"r-squared\"])",
    "Below loop assumes that the last indices of W are alwawys converted to discrete",
    "one-hot encode discrete W",
    "Now deleting the old continuous value",
    "Making beta an array",
    "TODO Ensure that we do not generate weak instruments",
    "TODO - test all our methods with random noise added to covariates (instead of the stochastic treatment assignment)",
    "Converting treatment to binary if required",
    "Specifying the correct dtypes",
    "Now specifying the corresponding graph strings",
    "Now writing the gml graph",
    "Making beta an array",
    "creating data frame",
    "Specifying the correct dtypes",
    "Now specifying the corresponding graph strings",
    "Now writing the gml graph",
    "Loading version number",
    "The currently supported estimators",
    "The default standard deviation for noise",
    "The default scaling factor to determine the bucket size",
    "The minimum number of points for the estimator to run",
    "The Default Transformation, when no arguments are given, or if the number of data points are insufficient for an estimator",
    "We need to change the identified estimand",
    "We thus, make a copy. This is done as we don't want",
    "to change the original DataFrame",
    "We set X_train = 0 and outcome_train to be 0",
    "Get the final outcome, after running through all the values in the transformation list",
    "If the number of data points is too few, run the default transformation: [(\"zero\",\"\"),(\"noise\", {'std_dev':1} )]",
    "We convert to ndarray for ease in indexing",
    "The data is of the form",
    "sim1: cat1 cat2 ... catn",
    "sim2: cat1 cat2 ... catn",
    "Note: We hardcode the estimate value to ZERO as we want to check if it falls in the distribution of the refuter",
    "Ideally we should expect that ZERO should fall in the distribution of the effect estimates as we have severed any causal",
    "relationship between the treatment and the outcome.",
    "We use string arguments to account for both 32 and 64 bit varaibles",
    "action for continuous variables",
    "Action for categorical variables",
    "Find the set difference for each row",
    "Choose one out of the remaining",
    "We want to see if the estimate falls in the same distribution as the one generated by the refuter",
    "Ideally that should be the case as running bootstrap should not have a significant effect on the ability",
    "of the treatment to affect the outcome",
    "Get a 2D matrix of values",
    "Store the values into the refute object",
    "Default value of the p value taken for the distribution",
    "Number of Trials: Number of cointosses to understand if a sample gets the treatment",
    "Mean of the Normal Distribution",
    "Standard Deviation of the Normal Distribution",
    "We need to change the identified estimand",
    "We make a copy as a safety measure, we don't want to change the",
    "original DataFrame",
    "Create a new column in the data by the name of placebo",
    "Sanity check the data",
    "Note: We hardcode the estimate value to ZERO as we want to check if it falls in the distribution of the refuter",
    "Ideally we should expect that ZERO should fall in the distribution of the effect estimates as we have severed any causal",
    "relationship between the treatment and the outcome.",
    "Adding a new backdoor variable to the identified estimand",
    "from https://www.bnmetrics.com/blog/factory-pattern-in-python3-simple-version",
    "We want to see if the estimate falls in the same distribution as the one generated by the refuter",
    "Ideally that should be the case as choosing a subset should not have a significant effect on the ability",
    "of the treatment to affect the outcome",
    "from https://www.bnmetrics.com/blog/factory-pattern-in-python3-simple-version",
    "choosing the instrumental variable to use",
    "TODO move this to the identification step",
    "Obtain estimate by Wald Estimator",
    "Obtain estimate by 2SLS estimator: Cov(y,z) / Cov(x,z)",
    "More than 1 instrument. Use 2sls.",
    "We need to initialize the model when we create any propensity score estimator",
    "Check if the treatment is one-dimensional",
    "Checking if the treatment is binary",
    "Convert the categorical variables into dummy/indicator variables",
    "Basically, this gives a one hot encoding for each category",
    "The first category is taken to be the base line.",
    "TODO make treatment_value and control value also as local parameters",
    "Checking if the model is already trained",
    "The model is always built on the entire data",
    "All treatments are set to the same constant value",
    "Using all data by default",
    "Fixing treatment value to the specified value, if provided",
    "treatment_vals and data_df should have same number of rows",
    "Bulding the feature matrix",
    "The model is always built on the entire data",
    "Replacing treatment values by given x",
    "sort the dataframe by propensity score",
    "create a column 'strata' for each element that marks what strata it belongs to",
    "for each strata, count how many treated and control units there are",
    "throw away strata that have insufficient treatment or control",
    "print(\"before clipping, here is the distribution of treatment and control per strata\")",
    "print(self._data.groupby(['strata',self._treatment_name])[self._outcome_name].count())",
    "print(\"after clipping at threshold, now we have:\" )",
    "print(clipped.groupby(['strata',self._treatment_name])[self._outcome_name].count())",
    "sum weighted outcomes over all strata  (weight by treated population)",
    "TODO - how can we add additional information into the returned estimate?",
    "such as how much clipping was done, or per-strata info for debugging?",
    "TODO -- fix: we are actually conditioning on positive treatment (d=1)",
    "trim propensity score weights",
    "ips ==> (isTreated(y)/ps(y)) + ((1-isTreated(y))/(1-ps(y)))",
    "nips ==> ips / (sum of ips over all units)",
    "icps ==> ps(y)/(1-ps(y)) / (sum of (ps(y)/(1-ps(y))) over all control units)",
    "itps ==> ps(y)/(1-ps(y)) / (sum of (ps(y)/(1-ps(y))) over all treatment units)",
    "Vanilla IPS estimator",
    "Also known as the Hajek estimator",
    "Stabilized weights",
    "Simple normalized estimator (commented out for now)",
    "ips_sum = self._data['ips_weight'].sum()",
    "self._data['nips_weight'] = self._data['ips_weight'] / ips_sum",
    "self._data['ips2'] = self._data['ps'] / (1 - self._data['ps'])",
    "treated_ips_sum = (self._data['ips2'] * self._data[self._treatment_name[0]]).sum()",
    "control_ips_sum = (self._data['ips2'] * (1 - self._data[self._treatment_name[0]])).sum()",
    "self._data['itps_weight'] = self._data['ips2'] / treated_ips_sum",
    "self._data['icps_weight'] = self._data['ips2'] / control_ips_sum",
    "Calculating the effect",
    "TODO - how can we add additional information into the returned estimate?",
    "TODO -- fix: we are actually conditioning on positive treatment (d=1)",
    "Checking if effect modifiers are a subset of common causes",
    "Instrumental variables names, if present",
    "from https://www.bnmetrics.com/blog/factory-pattern-in-python3-simple-version",
    "Calling the econml estimator's fit method",
    "Changing shape to a list for a singleton value",
    "TODO -- fix: we are actually conditioning on positive treatment (d=1)",
    "this assumes a binary treatment regime",
    "TODO remove neighbors that are more than a given radius apart",
    "estimate ATT on treated by summing over difference between matched neighbors",
    "Now computing ATC",
    "TODO -- fix: we are actually conditioning on positive treatment (d=1)",
    "from https://www.bnmetrics.com/blog/factory-pattern-in-python3-simple-version",
    "Add the identification method used in the estimator",
    "Check the backdoor variables being used",
    "Add the observed confounders and one hot encode the categorical variables",
    "Get the data of the unobserved confounders",
    "One hot encode the data if they are categorical",
    "Check the instrumental variables involved",
    "Perform the same actions as the above",
    "Check if effect modifiers are used",
    "Get the class corresponding the the estimator to be used",
    "Initialize the object",
    "Both the outcome and the treatment have to be 1D arrays according to the CausalML API",
    "We want to pass 'v0' rather than ['v0'] to prevent a shape mismatch",
    "TODO we are conditioning on a postive treatment",
    "TODO create an expression corresponding to each estimator used",
    "self._identified_estimand = self._causal_model.identify_effect()",
    "self._identified_estimand,",
    "self._causal_model._treatment,",
    "self._causal_model._outcome,"
  ],
  "v0.2": [
    "Get the long description from the README file",
    "Get the required packages",
    "Loading version number",
    "-*- coding: utf-8 -*-",
    "",
    "Configuration file for the Sphinx documentation builder.",
    "",
    "This file does only contain a selection of the most common options. For a",
    "full list see the documentation:",
    "http://www.sphinx-doc.org/en/stable/config",
    "-- Path setup --------------------------------------------------------------",
    "If extensions (or modules to document with autodoc) are in another directory,",
    "add these directories to sys.path here. If the directory is relative to the",
    "documentation root, use os.path.abspath to make it absolute, like shown here.",
    "",
    "-- Project information -----------------------------------------------------",
    "The short X.Y version",
    "The full version, including alpha/beta/rc tags",
    "-- General configuration ---------------------------------------------------",
    "If your documentation needs a minimal Sphinx version, state it here.",
    "",
    "needs_sphinx = '1.0'",
    "Add any Sphinx extension module names here, as strings. They can be",
    "extensions coming with Sphinx (named 'sphinx.ext.*') or your custom",
    "ones.",
    "Add any paths that contain templates here, relative to this directory.",
    "The suffix(es) of source filenames.",
    "You can specify multiple suffix as a list of string:",
    "",
    "source_suffix = ['.rst', '.md']",
    "The master toctree document.",
    "The language for content autogenerated by Sphinx. Refer to documentation",
    "for a list of supported languages.",
    "",
    "This is also used if you do content translation via gettext catalogs.",
    "Usually you set \"language\" from the command line for these cases.",
    "List of patterns, relative to source directory, that match files and",
    "directories to ignore when looking for source files.",
    "This pattern also affects html_static_path and html_extra_path .",
    "The name of the Pygments (syntax highlighting) style to use.",
    "-- Options for HTML output -------------------------------------------------",
    "The theme to use for HTML and HTML Help pages.  See the documentation for",
    "a list of builtin themes.",
    "",
    "html_theme = 'sphinx-rtd-theme'",
    "on_rtd is whether we are on readthedocs.org",
    "only import and set the theme if we're building docs locally",
    "otherwise, readthedocs.org uses their theme by default, so no need to specify it",
    "Theme options are theme-specific and customize the look and feel of a theme",
    "further.  For a list of options available for each theme, see the",
    "documentation.",
    "",
    "html_theme_options = {}",
    "Add any paths that contain custom static files (such as style sheets) here,",
    "relative to this directory. They are copied after the builtin static files,",
    "so a file named \"default.css\" will overwrite the builtin \"default.css\".",
    "Custom sidebar templates, must be a dictionary that maps document names",
    "to template names.",
    "",
    "The default sidebars (for documents that don't match any pattern) are",
    "defined by theme itself.  Builtin themes are using these templates by",
    "default: ``['localtoc.html', 'relations.html', 'sourcelink.html',",
    "'searchbox.html']``.",
    "",
    "html_sidebars = {}",
    "-- Options for HTMLHelp output ---------------------------------------------",
    "Output file base name for HTML help builder.",
    "-- Options for LaTeX output ------------------------------------------------",
    "The paper size ('letterpaper' or 'a4paper').",
    "",
    "'papersize': 'letterpaper',",
    "The font size ('10pt', '11pt' or '12pt').",
    "",
    "'pointsize': '10pt',",
    "Additional stuff for the LaTeX preamble.",
    "",
    "'preamble': '',",
    "Latex figure (float) alignment",
    "",
    "'figure_align': 'htbp',",
    "Grouping the document tree into LaTeX files. List of tuples",
    "(source start file, target name, title,",
    "author, documentclass [howto, manual, or own class]).",
    "-- Options for manual page output ------------------------------------------",
    "One entry per manual page. List of tuples",
    "(source start file, name, description, authors, manual section).",
    "-- Options for Texinfo output ----------------------------------------------",
    "Grouping the document tree into Texinfo files. List of tuples",
    "(source start file, target name, title, author,",
    "dir menu entry, description, category)",
    "-- Options for Epub output -------------------------------------------------",
    "Bibliographic Dublin Core info.",
    "The unique identifier of the text. This can be a ISBN number",
    "or the project homepage.",
    "",
    "epub_identifier = ''",
    "A unique identification for the text.",
    "",
    "epub_uid = ''",
    "A list of files that should not be packed into the epub file.",
    "-- Extension configuration -------------------------------------------------",
    "-- Options for todo extension ----------------------------------------------",
    "If true, `todo` and `todoList` produce output, else they produce nothing.",
    "self.df = pd.read_csv(os.path.join(DATA_PATH,'dgp_1/acic_1_1_data.csv'))",
    "self.ate = np.mean(self.df['y1'] - self.df['y0'])",
    "treated = self.df[self.df['z']==1]",
    "self.att = np.mean(treated['y1'] - treated['y0'])",
    "def test_average_treatment_effect(self):",
    "est_ate = 1",
    "bias = est_ate - self.ate",
    "print(bias)",
    "self.assertAlmostEqual(self.ate, est_ate)",
    "def test_average_treatment_effect_on_treated(self):",
    "est_att = 1",
    "self.att=1",
    "bias = est_att - self.att",
    "print(bias)",
    "self.assertAlmostEqual(self.att, est_att)",
    "removing two common causes",
    "Supports user-provided dataset object",
    "To test if there are any exceptions",
    "To test if the estimate is identical if refutation parameters are zero",
    "beta=1,",
    "num_instruments=2, num_samples=100000,",
    "cov_mat = np.diag(np.ones(num_features))",
    "Not using testsuite from .base/TestEstimtor, custom code below",
    "More cases where Exception  is expected",
    "Now checking if there is also a valid iv estimand",
    "TODO: outputs string for now, but ideally should do symbolic",
    "expressions Mon 19 Feb 2018 04:54:17 PM DST",
    "TODO Better support for multivariate treatments",
    "sym_common_causes = [sp.stats.Normal(common_cause, sym_mu, sym_sigma) for common_cause in common_causes]",
    "TODO: support multivariate treatments better.",
    "load dot file",
    "TODO do not add it here. CausalIdentifier should call causal_graph to add an unobserved common cause if needed. This also ensures that we do not need get_common_causes in this class.",
    "nx.draw_networkx(self._graph, pos=nx.shell_layout(self._graph))",
    "Adding common causes",
    "Adding instruments",
    "Adding effect modifiers",
    "Adding unobserved confounders",
    "Adding unobserved confounders",
    "TODO Refactor to remove this from here and only implement this logic in causalIdentifier. Unnecessary assumption of nodes1 to be causing nodes2.",
    "Cannot simply compute ancestors, since that will also include nodes1 and its parents (e.g. instruments)",
    "[TODO: double check these work with multivariate implementation:]",
    "Exclusion",
    "As-if-random setup",
    "As-if-random",
    "TODO: move the logging level argument to a json file. Tue 20 Feb 2018 06:56:27 PM DST",
    "TODO add propensity score as default backdoor method, iv as default iv method, add an informational message to show which method has been selected.",
    "Check if estimator's target estimand is identified",
    "Store parameters inside estimate object for refutation methods",
    "Check if estimator's target estimand is identified",
    "Currently estimation methods only support univariate treatment and outcome",
    "Checking if some parameters were set, otherwise setting to default values",
    "Setting more values",
    "Now saving the effect modifiers",
    "TODO Only works for binary treatment",
    "Doing a two-sided test",
    "Being conservative with the p-value reported",
    "Being conservative with the p-value reported",
    "Need to test r-squared before supporting",
    "effect_r_squared = self._evaluate_effect_strength(estimate, method=\"r-squared\")",
    "'r-squared': effect_r_squared",
    "elif method == \"r-squared\":",
    "outcome_mean = np.mean(self._outcome)",
    "total_variance = np.sum(np.square(self._outcome - outcome_mean))",
    "Assuming a linear model with one variable: the treatment",
    "Currently only works for continuous y",
    "causal_model = outcome_mean + estimate.value*self._treatment",
    "squared_residual = np.sum(np.square(self._outcome - causal_model))",
    "r_squared = 1 - (squared_residual/total_variance)",
    "return r_squared",
    "s += \"Variance in outcome explained by treatment: {}\\n\".format(self.effect_strength[\"r-squared\"])",
    "Making beta an array",
    "TODO Ensure that we do not generate weak instruments",
    "TODO - test all our methods with random noise added to covariates (instead of the stochastic treatment assignment)",
    "Converting treatment to binary if required",
    "Specifying the correct dtypes",
    "Now specifying the corresponding graph strings",
    "Now writing the gml graph",
    "Making beta an array",
    "creating data frame",
    "Specifying the correct dtypes",
    "Now specifying the corresponding graph strings",
    "Now writing the gml graph",
    "Loading version number",
    "Adding a new backdoor variable to the identified estimand",
    "from https://www.bnmetrics.com/blog/factory-pattern-in-python3-simple-version",
    "from https://www.bnmetrics.com/blog/factory-pattern-in-python3-simple-version",
    "choosing the instrumental variable to use",
    "TODO move this to the identification step",
    "Obtain estimate by Wald Estimator",
    "Obtain estimate by 2SLS estimator: Cov(y,z) / Cov(x,z)",
    "More than 1 instrument. Use 2sls.",
    "Instrumental variables names, if present",
    "from https://www.bnmetrics.com/blog/factory-pattern-in-python3-simple-version",
    "Calling the econml estimator's fit method",
    "Changing shape to a list for a singleton value",
    "TODO -- fix: we are actually conditioning on positive treatment (d=1)",
    "All treatments are set to the same constant value",
    "Checking if treatment is one-dimensional",
    "Checking if treatment is binary",
    "sort the dataframe by propensity score",
    "create a column 'strata' for each element that marks what strata it belongs to",
    "for each strata, count how many treated and control units there are",
    "throw away strata that have insufficient treatment or control",
    "print(\"before clipping, here is the distribution of treatment and control per strata\")",
    "print(self._data.groupby(['strata',self._treatment_name])[self._outcome_name].count())",
    "print(\"after clipping at threshold, now we have:\" )",
    "print(clipped.groupby(['strata',self._treatment_name])[self._outcome_name].count())",
    "sum weighted outcomes over all strata  (weight by treated population)",
    "TODO - how can we add additional information into the returned estimate?",
    "such as how much clipping was done, or per-strata info for debugging?",
    "TODO -- fix: we are actually conditioning on positive treatment (d=1)",
    "Checking if treatment is one-dimensional",
    "Checking if treatment is binary",
    "trim propensity score weights",
    "ips ==> (isTreated(y)/ps(y)) + ((1-isTreated(y))/(1-ps(y)))",
    "nips ==> ips / (sum of ips over all units)",
    "icps ==> ps(y)/(1-ps(y)) / (sum of (ps(y)/(1-ps(y))) over all control units)",
    "itps ==> ps(y)/(1-ps(y)) / (sum of (ps(y)/(1-ps(y))) over all treatment units)",
    "Vanilla IPS estimator",
    "Also known as the Hajek estimator",
    "Stabilized weights",
    "Simple normalized estimator (commented out for now)",
    "ips_sum = self._data['ips_weight'].sum()",
    "self._data['nips_weight'] = self._data['ips_weight'] / ips_sum",
    "self._data['ips2'] = self._data['ps'] / (1 - self._data['ps'])",
    "treated_ips_sum = (self._data['ips2'] * self._data[self._treatment_name[0]]).sum()",
    "control_ips_sum = (self._data['ips2'] * (1 - self._data[self._treatment_name[0]])).sum()",
    "self._data['itps_weight'] = self._data['ips2'] / treated_ips_sum",
    "self._data['icps_weight'] = self._data['ips2'] / control_ips_sum",
    "Calculating the effect",
    "TODO - how can we add additional information into the returned estimate?",
    "TODO -- fix: we are actually conditioning on positive treatment (d=1)",
    "Checking if treatment is one-dimensional",
    "Checking if treatment is binary",
    "this assumes a binary treatment regime",
    "TODO remove neighbors that are more than a given radius apart",
    "estimate ATT on treated by summing over difference between matched neighbors",
    "Now computing ATC",
    "TODO -- fix: we are actually conditioning on positive treatment (d=1)",
    "from https://www.bnmetrics.com/blog/factory-pattern-in-python3-simple-version",
    "self._identified_estimand = self._causal_model.identify_effect()",
    "self._identified_estimand,",
    "self._causal_model._treatment,",
    "self._causal_model._outcome,"
  ],
  "v0.1.1-alpha": [
    "Get the long description from the README file",
    "Get the required packages",
    "-*- coding: utf-8 -*-",
    "",
    "Configuration file for the Sphinx documentation builder.",
    "",
    "This file does only contain a selection of the most common options. For a",
    "full list see the documentation:",
    "http://www.sphinx-doc.org/en/stable/config",
    "-- Path setup --------------------------------------------------------------",
    "If extensions (or modules to document with autodoc) are in another directory,",
    "add these directories to sys.path here. If the directory is relative to the",
    "documentation root, use os.path.abspath to make it absolute, like shown here.",
    "",
    "-- Project information -----------------------------------------------------",
    "The short X.Y version",
    "The full version, including alpha/beta/rc tags",
    "-- General configuration ---------------------------------------------------",
    "If your documentation needs a minimal Sphinx version, state it here.",
    "",
    "needs_sphinx = '1.0'",
    "Add any Sphinx extension module names here, as strings. They can be",
    "extensions coming with Sphinx (named 'sphinx.ext.*') or your custom",
    "ones.",
    "Add any paths that contain templates here, relative to this directory.",
    "The suffix(es) of source filenames.",
    "You can specify multiple suffix as a list of string:",
    "",
    "source_suffix = ['.rst', '.md']",
    "The master toctree document.",
    "The language for content autogenerated by Sphinx. Refer to documentation",
    "for a list of supported languages.",
    "",
    "This is also used if you do content translation via gettext catalogs.",
    "Usually you set \"language\" from the command line for these cases.",
    "List of patterns, relative to source directory, that match files and",
    "directories to ignore when looking for source files.",
    "This pattern also affects html_static_path and html_extra_path .",
    "The name of the Pygments (syntax highlighting) style to use.",
    "-- Options for HTML output -------------------------------------------------",
    "The theme to use for HTML and HTML Help pages.  See the documentation for",
    "a list of builtin themes.",
    "",
    "html_theme = 'sphinx-rtd-theme'",
    "on_rtd is whether we are on readthedocs.org",
    "only import and set the theme if we're building docs locally",
    "otherwise, readthedocs.org uses their theme by default, so no need to specify it",
    "Theme options are theme-specific and customize the look and feel of a theme",
    "further.  For a list of options available for each theme, see the",
    "documentation.",
    "",
    "html_theme_options = {}",
    "Add any paths that contain custom static files (such as style sheets) here,",
    "relative to this directory. They are copied after the builtin static files,",
    "so a file named \"default.css\" will overwrite the builtin \"default.css\".",
    "Custom sidebar templates, must be a dictionary that maps document names",
    "to template names.",
    "",
    "The default sidebars (for documents that don't match any pattern) are",
    "defined by theme itself.  Builtin themes are using these templates by",
    "default: ``['localtoc.html', 'relations.html', 'sourcelink.html',",
    "'searchbox.html']``.",
    "",
    "html_sidebars = {}",
    "-- Options for HTMLHelp output ---------------------------------------------",
    "Output file base name for HTML help builder.",
    "-- Options for LaTeX output ------------------------------------------------",
    "The paper size ('letterpaper' or 'a4paper').",
    "",
    "'papersize': 'letterpaper',",
    "The font size ('10pt', '11pt' or '12pt').",
    "",
    "'pointsize': '10pt',",
    "Additional stuff for the LaTeX preamble.",
    "",
    "'preamble': '',",
    "Latex figure (float) alignment",
    "",
    "'figure_align': 'htbp',",
    "Grouping the document tree into LaTeX files. List of tuples",
    "(source start file, target name, title,",
    "author, documentclass [howto, manual, or own class]).",
    "-- Options for manual page output ------------------------------------------",
    "One entry per manual page. List of tuples",
    "(source start file, name, description, authors, manual section).",
    "-- Options for Texinfo output ----------------------------------------------",
    "Grouping the document tree into Texinfo files. List of tuples",
    "(source start file, target name, title, author,",
    "dir menu entry, description, category)",
    "-- Options for Epub output -------------------------------------------------",
    "Bibliographic Dublin Core info.",
    "The unique identifier of the text. This can be a ISBN number",
    "or the project homepage.",
    "",
    "epub_identifier = ''",
    "A unique identification for the text.",
    "",
    "epub_uid = ''",
    "A list of files that should not be packed into the epub file.",
    "-- Extension configuration -------------------------------------------------",
    "-- Options for todo extension ----------------------------------------------",
    "If true, `todo` and `todoList` produce output, else they produce nothing.",
    "self.df = pd.read_csv(os.path.join(DATA_PATH,'dgp_1/acic_1_1_data.csv'))",
    "self.ate = np.mean(self.df['y1'] - self.df['y0'])",
    "treated = self.df[self.df['z']==1]",
    "self.att = np.mean(treated['y1'] - treated['y0'])",
    "def test_average_treatment_effect(self):",
    "est_ate = 1",
    "bias = est_ate - self.ate",
    "print(bias)",
    "self.assertAlmostEqual(self.ate, est_ate)",
    "def test_average_treatment_effect_on_treated(self):",
    "est_att = 1",
    "self.att=1",
    "bias = est_att - self.att",
    "print(bias)",
    "self.assertAlmostEqual(self.att, est_att)",
    "cov_mat = np.diag(np.ones(num_features))",
    "Now checking if there is also a valid iv estimand",
    "TODO: outputs string for now, but ideally should do symbolic",
    "expressions Mon 19 Feb 2018 04:54:17 PM DST",
    "[TODO: support multivariate states]",
    "sym_common_causes = [sp.stats.Normal(common_cause, sym_mu, sym_sigma) for common_cause in common_causes]",
    "[TODO: support multivariate states]",
    "TODO: move the logging level argument to a json file. Tue 20 Feb 2018 06:56:27 PM DST",
    "Check if estimator's target estimand is identified",
    "Check if estimator's target estimand is identified",
    "load dot file",
    "Adding common causes",
    "Adding instruments",
    "Adding unobserved confounders",
    "[TODO: double check these work with multivariate implementation:]",
    "Exclusion",
    "As-if-random setup",
    "As-if-random",
    "Currently estimation methods only support univariate treatment and outcome",
    "self._estimate = est",
    "Doing a two-sided test",
    "Being conservative with the p-value reported",
    "Being conservative with the p-value reported",
    "TODO - test all our methods with random noise added to covariates (instead of the stochastic treatment assignment)",
    "identified_estimand = IdentifiedEstimand(",
    "treatment_variable = self._treatment_name,",
    "outcome_variable = self._outcome_name,",
    "backdoor_variables = new_backdoor_variables)#self._target_estimand.backdoor_variables)#new_backdoor_variables)",
    "from https://www.bnmetrics.com/blog/factory-pattern-in-python3-simple-version",
    "from https://www.bnmetrics.com/blog/factory-pattern-in-python3-simple-version",
    "choosing the instrumental variable to use",
    "Obtain estimate by Wald Estimator",
    "Obtain estimate by Pearl (1995) ratio estimator.",
    "y = x+ u; multiply both sides by z and take expectation.",
    "sort the dataframe by propensity score",
    "create a column 'strata' for each element that marks what strata it belongs to",
    "for each strata, count how many treated and control units there are",
    "throw away strata that have insufficient treatment or control",
    "print(\"before clipping, here is the distribution of treatment and control per strata\")",
    "print(self._data.groupby(['strata',self._treatment_name])[self._outcome_name].count())",
    "print(\"after clipping at threshold, now we have:\" )",
    "print(clipped.groupby(['strata',self._treatment_name])[self._outcome_name].count())",
    "sum weighted outcomes over all strata  (weight by treated population)",
    "TODO - how can we add additional information into the returned estimate?",
    "such as how much clipping was done, or per-strata info for debugging?",
    "TODO -- fix: we are actually conditioning on positive treatment (d=1)",
    "trim propensity score weights",
    "ips ==> (isTreated(y)/ps(y)) + ((1-isTreated(y))/(1-ps(y)))",
    "nips ==> ips / (sum of ips over all units)",
    "icps ==> ps(y)/(1-ps(y)) / (sum of (ps(y)/(1-ps(y))) over all control units)",
    "itps ==> ps(y)/(1-ps(y)) / (sum of (ps(y)/(1-ps(y))) over all treatment units)",
    "TODO - how can we add additional information into the returned estimate?",
    "TODO -- fix: we are actually conditioning on positive treatment (d=1)",
    "this assumes a binary treatment regime",
    "TODO remove neighbors that are more than a given radius apart",
    "estimate ATE on treated by summing over difference between matched neighbors",
    "TODO -- fix: we are actually conditioning on positive treatment (d=1)",
    "from https://www.bnmetrics.com/blog/factory-pattern-in-python3-simple-version",
    "self._identified_estimand = self._causal_model.identify_effect()",
    "self._identified_estimand,",
    "self._causal_model._treatment,",
    "self._causal_model._outcome,"
  ]
}